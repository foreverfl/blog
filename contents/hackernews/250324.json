[
  {
    "id": "e5bcbc9609f02ec9",
    "title": {
      "en": "The earliest versions of the first C compiler known to exist",
      "ko": "ìµœì´ˆì˜ C ì»´íŒŒì¼ëŸ¬",
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/mortdeus/legacy-cc",
    "score": 66,
    "by": "diginova",
    "time": 1742545039,
    "content": "legacy-cc\nThe earliest versions of the very first c compiler known to exist in the wild written by the late legend himself dmr.\nThese are not capable of being compiled today with modern c compilers like gcc. I am only posting these here for the enjoyment and reminiscing of the spark that ignited a soon to be trillion dollar industry. Enjoy. :)\nYou guys might be able to play around building this compilier with Aiju's pdp-11/unix emulator. I havent tried building this myself so I cant confirm it works, but I posted the link to her emulator for anybody wanting to hack around with it.\nSource: [https://www.bell-labs.com/usr/dmr/www/primevalC.html] alt-link [http://cm.bell-labs.co/who/dmr/primevalC.html]\nUseful Links\nPDP-11 Emulator: [http://pdp11.aiju.de/] (so i've been playing around with this to see if it still works, and it clearly has problems right now. if somebody wants to try and create like a VM image of unix running on PDP-7 hardware, id happily link to it here.)\nResearch Unix Repository: [https://www.tuhs.org/Archive/Distributions/Research/]",
    "summary": {
      "en": "The text discusses the earliest versions of the first C compiler, created by the late computer pioneer Dennis Ritchie (dmr). These versions cannot be compiled using modern C compilers like GCC. The author shares these old compilers for nostalgic enjoyment, highlighting their role in starting a significant industry. \n\nThey mention that users can try building the compiler using Aiju's PDP-11/Unix emulator, although the author has not tested it themselves. Links to the emulator and a Research Unix repository are provided for those interested in exploring this further.",
      "ko": "ì´ ê¸€ì—ì„œëŠ” ê³ (æ•…) ì»´í“¨í„° ì„ êµ¬ì ë´ë‹ˆìŠ¤ ë¦¬ì¹˜ê°€ ë§Œë“  ìµœì´ˆì˜ C ì»´íŒŒì¼ëŸ¬ì˜ ì´ˆê¸° ë²„ì „ë“¤ì— ëŒ€í•´ ë‹¤ë£¹ë‹ˆë‹¤. ì´ ë²„ì „ë“¤ì€ í˜„ëŒ€ì˜ C ì»´íŒŒì¼ëŸ¬ì¸ GCCë¥¼ ì‚¬ìš©í•´ ì»´íŒŒì¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì €ìëŠ” ì´ëŸ¬í•œ ì˜¤ë˜ëœ ì»´íŒŒì¼ëŸ¬ë¥¼ í–¥ìˆ˜ ì–´ë¦° ì¦ê±°ì›€ì„ ìœ„í•´ ê³µìœ í•˜ë©°, ì´ë“¤ì´ ì¤‘ìš”í•œ ì‚°ì—…ì˜ ì‹œì‘ì— ê¸°ì—¬í•œ ì ì„ ê°•ì¡°í•©ë‹ˆë‹¤.\n\nì‚¬ìš©ìë“¤ì€ Aijuì˜ PDP-11/Unix ì—ë®¬ë ˆì´í„°ë¥¼ ì´ìš©í•´ ì»´íŒŒì¼ëŸ¬ë¥¼ êµ¬ì¶•í•´ ë³¼ ìˆ˜ ìˆë‹¤ê³  ì–¸ê¸‰í•˜ì§€ë§Œ, ì €ìëŠ” ì§ì ‘ í…ŒìŠ¤íŠ¸í•´ ë³´ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê´€ì‹¬ ìˆëŠ” ì‚¬ëŒë“¤ì„ ìœ„í•´ ì—ë®¬ë ˆì´í„°ì™€ Research Unix ì €ì¥ì†Œì— ëŒ€í•œ ë§í¬ë„ ì œê³µë©ë‹ˆë‹¤.",
      "ja": null
    }
  },
  {
    "id": "059ded92dec3320a",
    "title": {
      "en": "Project Aardvark: reimagining AI weather prediction",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.turing.ac.uk/blog/project-aardvark-reimagining-ai-weather-prediction",
    "score": 214,
    "by": "bentobean",
    "time": 1742772819,
    "content": "Section page\n                            Events from around the Turing Network\n\n                                    Conferences, workshops, and other events from around the Turing Network\n\n    Events from around the Turing Network",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "19e0e815410e9b28",
    "title": {
      "en": "A proof checker meant for education",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://jsiek.github.io/deduce/index.html",
    "score": 30,
    "by": "alabhyajindal",
    "time": 1742557657,
    "content": "A proof checker meant for education\n                Teaching correctness proofs of functional programs to students.\n\n                    Get Started\n                    Live Code",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "cf047133cd50e135",
    "title": {
      "en": "Quadlet: Running Podman containers under systemd",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://mo8it.com/blog/quadlet/",
    "score": 142,
    "by": "gjvc",
    "time": 1742776025,
    "content": "Quadlet: Running Podman containers under systemd\n\n        2024-01-02, updated: 2024-02-19\n            Tags:\n            #container,#linux,#selfhosting\n\n        Reading time: ~11min\n\n        Quadlet lets you run your Podman containers as systemd services.\nThis is especially useful for running containers in the background and automatically starting them after a server reboot.\nRunning Podman containers under systemd isn't new.\nIndeed, this was supported by Podman for a long time with the command podman generate systemd.\nBut this command now shows a deprecation warning to migrate to Quadlet.\nFor some months, I was too lazy to do that migration on my home server.\nWhy even touch a working system?\nBut now that I finally found time for it, I really appreciate Quadlet!\nI think that Podman finally has a Docker Compose alternative which is even more flexible and powerful!\nIn this blog post, I will explain how to use Quadlet with rootless Podman and migrate from the old method with podman generate systemd.\n\n        Note\n        If you wonder why systemd: Something has to start containers when there is no daemon (since Podman is daemonless).\nIf you are part of the vocal minority hating against systemd, then please leave instead of trolling.\n\n    Landscape mode recommended on mobile devices\n\n            Contents\n\n                            The deprecated method\n\n                            The problem\n\n                            Quadlet\n\n                            Is it any better?\n\n                            Dependencies\n\n                            Too many files?\n\n                            Updating images\n\n                            What about podman-compose?\n\n                            Further resources\n\nThe deprecated method\nLet's see how the old method worked before comparing it to Quadlet.\nYou can skip to the Quadlet section though.\nFirst, you had to create a container.\nIn an older post about Containerized PostgreSQL with rootless Podman, I created a container with a command similar to the following:\npodman create \\\n  --name test-db \\\n  -p 5432:5432 \\\n  -v ~/volumes/test-db:/var/lib/postgresql/data:Z \\\n  -e POSTGRES_PASSWORD=CHANGE_ME \\\n  --label \"io.containers.autoupdate=registry\" \\\n  docker.io/library/postgres:16\n\nThe command details are explained in the older post.\nThe only option that isn't explained there is --label \"io.containers.autoupdate=registry\".\nThis option enables updating the container image using podman auto-update which will be explained later in this post.\nAfter creating the container, you can run the following command:\npodman generate systemd test-db -fn --new\n\nIt creates the systemd service file container-test-db.service in your current path.\nThe options of the command aren't relevant for now, but they are also explained in the older post.\nTo use this generated service file, you had to place it in the directory ~/.config/systemd/user.\nTo enable and start it, you had to run the following command:\nsystemctl --user enable --now container-test-db\n\nThe problem\nThe problem with the old method is that it required you to run commands toâ€¦\n\ncreate a container\ngenerate a service file\nmove the service file if not already in the mentioned directory\nenable the service\n\nEspecially the command for creating the container is often lengthy.\nThis means that you had to create a shell script with these commands if you wanted to be able to rerun them later.\nTo reduce duplication, I created the following fish function to be called in my fish scripts that create the containers:\nfunction podman-default-create\n    set -l container_name $argv[1]\n\n    podman create \\\n        --name $container_name \\\n        --replace \\\n        $argv[2..]; or return 1\n\n    podman generate systemd --no-header --new --name $container_name >~/.config/systemd/user/container-$container_name.service; or return 1\n\n    systemctl --user enable --now container-$container_name\nend\n\nYou don't have to understand the details of the function above.\nWhat I want to demonstrate with it is that the old method was too hacky and involved the usage of redundant commands.\nThere must be an easier way, you might think.\nEspecially if you experienced the convenience that Docker Compose provides.\nBut this is not the only problem.\nThe old method is very inflexible!\nIf you want to cutomize the service file and use all systemd features, you need to manually edit it after each generation!\nQuadlet\nLet's take a look at the new method with Quadlet.\nFirst, you create the directory ~/.config/containers/systemd.\nThen, you place a .container file inside it.\nFor example, here is the test-db.container file:\n[Container]\nImage=docker.io/library/postgres:16\nAutoUpdate=registry\nPublishPort=5432:5432\nVolume=%h/volumes/test-db:/var/lib/postgresql/data:Z\nEnvironment=POSTGRES_PASSWORD=CHANGE_ME\n\n[Service]\nRestart=always\n\n[Install]\nWantedBy=default.target\n\nIt is a normal systemd service file but with the special section [Container].\nThis section has many documented options.\nAlmost all these options map to command line options that can be used to create a container with Podman (podman create).\nThe ones that we are interested in for the example are the following:\n\nImage specifies the image (with tag) to use\nAutoUpdate=registry maps to --label \"io.containers.autoupdate=registry\" (explained later in this post)\nPublishPort maps to -p\nVolume maps to -v\nEnvironment maps to -e\n\nIt is important to use the systemd specifier %h instead of ~ for the user home directory.\nIn the [Service] section, we use the Restart option and set it to always to always restart the container (unless stopped manually).\nTo automatically start the container on boot, we set the WantedBy option in the [Install] section to default.target.\n\n        Note\n        I thought that setting WantedBy to multi-user.target would work because it is the default target on servers.\nBut it doesn't work in the case of rootless containers.\nmulti-user.target is not defined in the user mode in systemd.\nYou can verify this by running the command systemctl --user status multi-user.target.\nIt is only defined in the system mode (systemctl status multi-user.target without --user).\n\nSince we use user services for systemd, we have to enable the linger for our user to start the containers without the user being logged in:\nloginctl enable-linger\n\n        âš ï¸ Warning âš ï¸\n        Enabling the linger is required for the container to be automatically started after a server reboot!\n\nFor systemd to discover the new service file, run systemctl --user daemon-reload.\nNow, you can start the container with systemctl --user start test-db.\nYou can check the status of the container service by running systemctl --user status test-db.\nYou can also verify that the Podman container is running by running podman ps.\nYou should find the container systemd-test-db.\nThe container has the name of the service file (test-db.container without the .container extension) prefixed by systemd- to avoid collisions with containers not managed by systemd.\nBut you can manually set the name of the container using the ContainerName option in the [Container] section.\nIs it any better?\nMy first impression was: \"Well, now I have to map all the podman create options to their equivalents in the [Container] section. Where is the benefit?\".\nBut after migrating all containers, I found the following benefits:\n\nYou have only one file (the container service file) instead of a script that generates a service file.\nYou can use all options possible in systemd. All options that you know about in the [Unit] and [Service] sections are supported. For example, you can specify a command to run before starting the container by setting the StartExecPre option. No more manual editing of generated files anymore!\nRather subjective: Writing configuration files is easier than writing and debugging shell scripts.\nIt is easier to handle dependencies as we will see in the next section.\n\nDependencies\nLet's assume that we have an app container that depends on the database container that we created.\nYou want the database container to be automatically started when the app container is started.\nYou also want to make sure that the app container is started after the database container.\nOtherwise, the app container might fail to start.\nHow can we express this dependency?\nLet's use OxiTraffic as an example (shameless plug ğŸ˜…).\nHere is the container service file oxitraffic.container that should be placed in ~/.config/containers/systemd:\n[Container]\nImage=docker.io/mo8it/oxitraffic:0.9.2\nAutoUpdate=registry\nVolume=%h/volumes/oxitraffic/config.toml:/volumes/config.toml:Z,ro\nVolume=%h/volumes/oxitraffic/logs:/var/log/oxitraffic:Z\n\n[Unit]\nRequires=test-db.service\nAfter=test-db.service\n\n[Service]\nRestart=always\n\n[Install]\nWantedBy=default.target\n\nThe new section is [Unit].\nWe set the Requires option to test-db.service to only start the app when the database is started.\nWe also set the After option to make sure that both containers aren't started in parallel.\nNote that we use test-db.service when referencing this container service and not test-db.container.\n\ntest-db.container is the file name.\ntest-db.service is the service name.\nsystemd-test-db is the default container name.\n\nFor the app to communicate with the database, a network should be added to both containers with the Network option in the [Container] section, but networking is out of the scope of this post.\nToo many files?\nIn our example, we created two files, one for the app container and one for the database container.\nDoes this mean that multi-container apps are more complex with Quadlet because you can't just put them in one file like with Docker Compose?\nIt depends on how you define complexity in this context.\nDoes splitting content over multiple files always result in a higher complexity?\nFor me, it is more complex to have everything in the same file.\nI had to maintain Docker Compose files with hundreds of lines and dozens of containersâ€¦\nThat wasn't fun!\nHaving each container in its own file has less mental overhead for me because I just have to think about this single container when I am in its file.\nOf course, you need to specify its dependencies on other containers, but you don't need to think about the details of these other containers.\nThe Docker Compose file of Mailcow is a terrifying example of huge Docker Compose files.\n\n        Note\n        Docker Compose supports splitting to multiple files.\n\nSo we need multiple files.\nBut we should still group related ones together!\nQuadlet supports placing unit files inside directories in the ~/.config/containers/systemd directory.\nFor our example, you would create an oxitraffic directory and place both files inside it.\nUpdating images\nNow, we have containers running in the background and automatically started after a server reboot.\nWouldn't it be nice to have an easy method to update the images of these containers without running podman pull for every container and then restarting the updated ones?\nFor example, if a new image is uploaded for PostgreSQL 16 (with the image tag 16 that we used), then the image should be updated and the container should be restarted.\nWith Docker, you would need something like Watchtower.\nBut Podman provides a tool out of the box!\nIf you set AutoUpdate=registry, you can just run podman auto-update and Podman will check if the registry has a newer image which is compatible with the used tag.\nIn that case, the image will be pulled and the container will be restarted.\nIt is that easy ğŸ˜\nOf course, this could be dangerous if you use a tag like latest for OxiTraffic instead of a concrete version like 0.9.2.\nBecause the next version pushed to the latest tag might include a breaking change!\nIt is even more dramatic if you use the latest tag for the PostgreSQL image because manual migrations are always required when upgrading PostgreSQL to a new major version.\nTherefore, always use a tag that can't lead to a breaking change!\nTrust me, this is not only a problem with Podman updates.\nI learned this the hard way when trying to deploy Docker containers a while ago that used the latest tag.\nPersonally, I run podman auto-update manually on the server every couple of days to see what has been updated and make sure that the containers are still healthy afterwards.\nWhat about podman-compose?\nThere is the Python script podman-compose which runs Compose files with Podman.\nBut I don't consider it a long-term alternative to Docker Compose for multiple reasons:\n\npodman-compose is just a translation layer between the Compose spec and Podman with systemd. Why yet another layer of abstraction that sacrifices flexibility? It doesn't let you use all features of systemd.\nI consider Python scripts a prototype because of Python's interpreted nature. Official Podman projects are written in a compiled language like Rust or Go.\nIt is not actively maintained. The last commit was 5 months ago.\n\nQuadlet aligns much better with the rootless, daemonless design of Podman.\nIf you are stuck with Compose files and would like to try out Quadlet, check out podlet that can help you during the migration.\nFurther resources\nTake this post as an introduction.\nI highly recommend reading the man page podman-systemd.unit to get a deeper understanding of Quadlet.\nYou don't need to read the documentation of every supported option though.\nQuadlet doesn't only work with containers.\nIt can also manage pods, networks and volumes (see man page).\nIf you are new to writing systemd unit files (like me), I also recommend checking out the man pages systemd.unit and systemd.service.\npodlet is a wonderful Rust tool that can help you during the migration.\nIt can create Quadlet files out of Podman commands or even (Docker) Compose files.\nCheck out the similar blog post on blog.while-true-do for another perspective and a second example.\nIt is my favorite blog related to Linux ğŸ¥°\nFinally, if you want to see my migration as an example, then you can compare before and after.",
    "summary": {
      "en": "**Summary of Quadlet: Running Podman Containers under systemd**\n\nQuadlet allows you to run Podman containers as systemd services, making it easier to manage containers in the background and ensuring they restart automatically after a server reboot. While the previous method using `podman generate systemd` is deprecated, Quadlet offers a more flexible and powerful alternative.\n\n**Key Points:**\n\n- **Transition to Quadlet:** Users are encouraged to migrate from the old method to Quadlet due to its advantages.\n  \n- **Old Method Limitations:** The previous process was cumbersome, requiring multiple commands to create containers, generate service files, and enable services. It was also inflexible, as any customization needed manual edits to generated files.\n\n- **Using Quadlet:** To use Quadlet:\n  1. Create a directory at `~/.config/containers/systemd`.\n  2. Place a `.container` file in that directory, specifying container details like image, ports, volumes, and environment variables.\n  3. Enable user linger to allow services to start without a logged-in user.\n\n- **Benefits of Quadlet:**\n  - Single service file for each container simplifies management.\n  - Full access to systemd features without the need for manual edits.\n  - Easier handling of dependencies between containers.\n\n- **Updates and Maintenance:** Quadlet supports automatic updates for container images, allowing you to run `podman auto-update` to check for newer images.\n\n- **Comparison with Docker Compose:** Unlike Docker Compose, Quadlet provides a direct way to manage services without an additional abstraction layer, aligning better with Podman's design.\n\n- **Further Learning:** For deeper understanding, users are encouraged to read relevant man pages and consider tools like `podlet` for transitioning from Docker Compose.\n\nOverall, Quadlet enhances the user experience of managing Podman containers by streamlining the process and integrating with systemd more effectively.",
      "ko": "**ì¿¼ë“œë ›: ì‹œìŠ¤í…œ ë°ëª¬ìœ¼ë¡œ Podman ì»¨í…Œì´ë„ˆ ì‹¤í–‰í•˜ê¸°**\n\nì¿¼ë“œë ›(Quadlet)ì€ Podman ì»¨í…Œì´ë„ˆë¥¼ ì‹œìŠ¤í…œ ë°ëª¬(systemd) ì„œë¹„ìŠ¤ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì»¨í…Œì´ë„ˆë¥¼ ë” ì‰½ê²Œ ê´€ë¦¬í•  ìˆ˜ ìˆìœ¼ë©°, ì„œë²„ ì¬ë¶€íŒ… í›„ì—ë„ ìë™ìœ¼ë¡œ ì¬ì‹œì‘ë˜ë„ë¡ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ì „ì— ì‚¬ìš©í•˜ë˜ `podman generate systemd` ë°©ë²•ì€ ë” ì´ìƒ ê¶Œì¥ë˜ì§€ ì•Šì§€ë§Œ, ì¿¼ë“œë ›ì€ ë” ìœ ì—°í•˜ê³  ê°•ë ¥í•œ ëŒ€ì•ˆì„ ì œê³µí•©ë‹ˆë‹¤.\n\n**ì£¼ìš” ë‚´ìš©:**\n\n- **ì¿¼ë“œë ›ìœ¼ë¡œì˜ ì „í™˜:** ì‚¬ìš©ìë“¤ì€ ì¿¼ë“œë ›ì˜ ì¥ì  ë•ë¶„ì— ì´ì „ ë°©ë²•ì—ì„œ ì¿¼ë“œë ›ìœ¼ë¡œì˜ ì „í™˜ì„ ê¶Œì¥ë°›ê³  ìˆìŠµë‹ˆë‹¤.\n\n- **êµ¬ì‹ ë°©ë²•ì˜ í•œê³„:** ì´ì „ ë°©ì‹ì€ ì—¬ëŸ¬ ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•´ì•¼ í–ˆê³ , ì»¨í…Œì´ë„ˆë¥¼ ë§Œë“¤ê³  ì„œë¹„ìŠ¤ íŒŒì¼ì„ ìƒì„±í•˜ë©° ì„œë¹„ìŠ¤ë¥¼ í™œì„±í™”í•˜ëŠ” ê³¼ì •ì´ ë²ˆê±°ë¡œì› ìŠµë‹ˆë‹¤. ë˜í•œ, ì»¤ìŠ¤í„°ë§ˆì´ì§•ì´ í•„ìš”í•  ê²½ìš° ìƒì„±ëœ íŒŒì¼ì„ ìˆ˜ë™ìœ¼ë¡œ ìˆ˜ì •í•´ì•¼ í•´ì„œ ìœ ì—°ì„±ì´ ë–¨ì–´ì¡ŒìŠµë‹ˆë‹¤.\n\n- **ì¿¼ë“œë › ì‚¬ìš©ë²•:** ì¿¼ë“œë ›ì„ ì‚¬ìš©í•˜ë ¤ë©´ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ë”°ë¥´ì„¸ìš”:\n  1. `~/.config/containers/systemd` ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n  2. í•´ë‹¹ ë””ë ‰í† ë¦¬ì— `.container` íŒŒì¼ì„ ì¶”ê°€í•˜ê³ , ì´ë¯¸ì§€, í¬íŠ¸, ë³¼ë¥¨, í™˜ê²½ ë³€ìˆ˜ ë“±ì˜ ì»¨í…Œì´ë„ˆ ì„¸ë¶€ ì •ë³´ë¥¼ ëª…ì‹œí•©ë‹ˆë‹¤.\n  3. ì‚¬ìš©ì ë§ê±°(user linger)ë¥¼ í™œì„±í™”í•˜ì—¬ ë¡œê·¸ì¸í•˜ì§€ ì•Šì€ ìƒíƒœì—ì„œë„ ì„œë¹„ìŠ¤ê°€ ì‹œì‘ë  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.\n\n- **ì¿¼ë“œë ›ì˜ ì¥ì :**\n  - ê° ì»¨í…Œì´ë„ˆì— ëŒ€í•´ ë‹¨ì¼ ì„œë¹„ìŠ¤ íŒŒì¼ì„ ì‚¬ìš©í•˜ì—¬ ê´€ë¦¬ê°€ ê°„í¸í•´ì§‘ë‹ˆë‹¤.\n  - ìˆ˜ë™ ìˆ˜ì • ì—†ì´ ì‹œìŠ¤í…œ ë°ëª¬ì˜ ëª¨ë“  ê¸°ëŠ¥ì„ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n  - ì»¨í…Œì´ë„ˆ ê°„ì˜ ì˜ì¡´ì„±ì„ ë” ì‰½ê²Œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n- **ì—…ë°ì´íŠ¸ ë° ìœ ì§€ ê´€ë¦¬:** ì¿¼ë“œë ›ì€ ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€ì˜ ìë™ ì—…ë°ì´íŠ¸ë¥¼ ì§€ì›í•˜ì—¬, `podman auto-update` ëª…ë ¹ì–´ë¡œ ìµœì‹  ì´ë¯¸ì§€ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n- **ë„ì»¤ ì»´í¬ì¦ˆì™€ì˜ ë¹„êµ:** ë„ì»¤ ì»´í¬ì¦ˆì™€ ë‹¬ë¦¬ ì¿¼ë“œë ›ì€ ì¶”ê°€ì ì¸ ì¶”ìƒí™” ê³„ì¸µ ì—†ì´ ì„œë¹„ìŠ¤ë¥¼ ì§ì ‘ ê´€ë¦¬í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì œê³µí•˜ì—¬, Podmanì˜ ì„¤ê³„ì™€ ë” ì˜ ë§ìŠµë‹ˆë‹¤.\n\n- **ë” ì•Œì•„ë³´ê¸°:** ë” ê¹Šì´ ìˆëŠ” ì´í•´ë¥¼ ì›í•˜ì‹ ë‹¤ë©´ ê´€ë ¨ ë§¤ë‰´ì–¼ í˜ì´ì§€ë¥¼ ì½ê³ , ë„ì»¤ ì»´í¬ì¦ˆì—ì„œ ì¿¼ë“œë ›ìœ¼ë¡œ ì „í™˜í•  ë•Œ `podlet`ê³¼ ê°™ì€ ë„êµ¬ë¥¼ ê³ ë ¤í•´ ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤.\n\nê²°ë¡ ì ìœ¼ë¡œ, ì¿¼ë“œë ›ì€ Podman ì»¨í…Œì´ë„ˆ ê´€ë¦¬ì˜ ì‚¬ìš©ì ê²½í—˜ì„ í–¥ìƒì‹œí‚¤ë©°, í”„ë¡œì„¸ìŠ¤ë¥¼ ê°„ì†Œí™”í•˜ê³  ì‹œìŠ¤í…œ ë°ëª¬ê³¼ì˜ í†µí•©ì„ ë”ìš± íš¨ê³¼ì ìœ¼ë¡œ ë§Œë“¤ì–´ ì¤ë‹ˆë‹¤.",
      "ja": "# Quadletã§Podmanã‚³ãƒ³ãƒ†ãƒŠã‚’systemdã‚µãƒ¼ãƒ“ã‚¹ã¨ã—ã¦ç°¡å˜ã«ç®¡ç†ã—ã‚ˆã†ï¼\n\nQuadletã¯ã€Podmanã‚³ãƒ³ãƒ†ãƒŠã‚’systemdã‚µãƒ¼ãƒ“ã‚¹ã¨ã—ã¦å®Ÿè¡Œã§ãã‚‹ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ã®ã‚³ãƒ³ãƒ†ãƒŠç®¡ç†ãŒå®¹æ˜“ã«ãªã‚Šã€ã‚µãƒ¼ãƒãƒ¼å†èµ·å‹•å¾Œã«è‡ªå‹•çš„ã«å†èµ·å‹•ã™ã‚‹ã“ã¨ãŒä¿è¨¼ã•ã‚Œã¾ã™ã€‚å¾“æ¥ã®`podman generate systemd`ã‚’ä½¿ã£ãŸæ–¹æ³•ã¯å»ƒæ­¢ã•ã‚Œã¤ã¤ã‚ã‚Šã¾ã™ãŒã€Quadletã¯ã‚ˆã‚ŠæŸ”è»Ÿã§å¼·åŠ›ãªä»£æ›¿æ‰‹æ®µã‚’æä¾›ã—ã¾ã™ã€‚\n\n## ä¸»ãªãƒã‚¤ãƒ³ãƒˆ\n\n- **Quadletã¸ã®ç§»è¡Œ:** æ—§æ¥ã®æ–¹æ³•ã‹ã‚‰Quadletã¸ã®ç§»è¡Œã‚’æ¨å¥¨ã—ã¾ã™ã€‚ãã®åˆ©ç‚¹ãŒå¤šã„ãŸã‚ã§ã™ã€‚\n\n- **æ—§æ–¹æ³•ã®åˆ¶ç´„:** å¾“æ¥ã®ãƒ—ãƒ­ã‚»ã‚¹ã¯ç…©é›‘ã§ã€ã‚³ãƒ³ãƒ†ãƒŠã®ä½œæˆã€ã‚µãƒ¼ãƒ“ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®ç”Ÿæˆã€ã‚µãƒ¼ãƒ“ã‚¹ã®æœ‰åŠ¹åŒ–ã«è¤‡æ•°ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å¿…è¦ã¨ã—ã¾ã—ãŸã€‚ã¾ãŸã€ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã‚’è¡Œã†ã«ã¯ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ‰‹å‹•ã§ç·¨é›†ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã€æŸ”è»Ÿæ€§ã«æ¬ ã‘ã¦ã„ã¾ã—ãŸã€‚\n\n- **Quadletã®ä½¿ã„æ–¹:** Quadletã‚’ä½¿ã†ã«ã¯ã€ä»¥ä¸‹ã®æ‰‹é †ã‚’è¸ã¿ã¾ã™ã€‚\n  1. `~/.config/containers/systemd`ã«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã—ã¾ã™ã€‚\n  2. ãã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«`.container`ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç½®ãã€ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚„ãƒãƒ¼ãƒˆã€ãƒœãƒªãƒ¥ãƒ¼ãƒ ã€ç’°å¢ƒå¤‰æ•°ãªã©ã®ã‚³ãƒ³ãƒ†ãƒŠã®è©³ç´°ã‚’æŒ‡å®šã—ã¾ã™ã€‚\n  3. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã€Œãƒªãƒ³ã‚¬ãƒ¼ã€ã‚’æœ‰åŠ¹ã«ã—ã¦ã€ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ã„ãªã„ãƒ¦ãƒ¼ã‚¶ãƒ¼ã§ã‚‚ã‚µãƒ¼ãƒ“ã‚¹ãŒèµ·å‹•ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚\n\n- **Quadletã®åˆ©ç‚¹:**\n  - å„ã‚³ãƒ³ãƒ†ãƒŠã«å¯¾ã—ã¦1ã¤ã®ã‚µãƒ¼ãƒ“ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€ç®¡ç†ãŒç°¡ç´ åŒ–ã•ã‚Œã¾ã™ã€‚\n  - æ‰‹å‹•ã§ã®ç·¨é›†ãªã—ã«systemdã®æ©Ÿèƒ½ã‚’ãƒ•ãƒ«æ´»ç”¨ã§ãã¾ã™ã€‚\n  - ã‚³ãƒ³ãƒ†ãƒŠé–“ã®ä¾å­˜é–¢ä¿‚ã®å‡¦ç†ãŒå®¹æ˜“ã«ãªã‚Šã¾ã™ã€‚\n\n- **æ›´æ–°ã¨ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹:** Quadletã¯ã‚³ãƒ³ãƒ†ãƒŠã‚¤ãƒ¡ãƒ¼ã‚¸ã®è‡ªå‹•æ›´æ–°ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ãŠã‚Šã€`podman auto-update`ã‚’å®Ÿè¡Œã™ã‚‹ã“ã¨ã§æ–°ã—ã„ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’ãƒã‚§ãƒƒã‚¯ã§ãã¾ã™ã€‚\n\n- **Docker Composeã¨ã®æ¯”è¼ƒ:** Docker Composeã¨ã¯ç•°ãªã‚Šã€Quadletã¯è¿½åŠ ã®æŠ½è±¡åŒ–ãƒ¬ã‚¤ãƒ¤ãƒ¼ãªã—ã§ã‚µãƒ¼ãƒ“ã‚¹ã‚’ç›´æ¥ç®¡ç†ã§ãã€Podmanã®è¨­è¨ˆã«ã‚ˆã‚Šé©ã—ã¦ã„ã¾ã™ã€‚\n\n- **ã•ã‚‰ãªã‚‹å­¦ã³:** ã‚ˆã‚Šæ·±ãç†è§£ã™ã‚‹ãŸã‚ã«ã€é–¢é€£ã™ã‚‹ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ãƒšãƒ¼ã‚¸ã‚’èª­ã‚€ã“ã¨ã‚„ã€Docker Composeã‹ã‚‰ã®ç§»è¡Œã«å½¹ç«‹ã¤ãƒ„ãƒ¼ãƒ«ã€Œpodletã€ã‚’æ¤œè¨ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚\n\nå…¨ä½“ã¨ã—ã¦ã€Quadletã¯Podmanã‚³ãƒ³ãƒ†ãƒŠã®ç®¡ç†ä½“é¨“ã‚’å‘ä¸Šã•ã›ã€ãƒ—ãƒ­ã‚»ã‚¹ã‚’ç°¡ç´ åŒ–ã—ã€systemdã¨ã®çµ±åˆã‚’ã‚ˆã‚ŠåŠ¹æœçš„ã«ã—ã¦ã„ã¾ã™ã€‚"
    }
  },
  {
    "id": "14118e59492a099a",
    "title": {
      "en": "Harfbuzz Release 11.0.0",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/harfbuzz/harfbuzz/releases/tag/11.0.0",
    "score": 6,
    "by": "rossant",
    "time": 1742816230,
    "content": "harfbuzz\n\n    /\n\n      harfbuzz\n\n    Public\n\nNotifications\n    You must be signed in to change notification settings\n\nFork\n    649\n\n          Star\n          4.5k\n\n        Code\n\n        Issues\n          81\n\n        Pull requests\n          9\n\n        Discussions\n\n        Actions\n\n        Projects\n          0\n\n        Wiki\n\n        Security\n            1\n\n        Insights\n\nAdditional navigation options\n\n          Code\n\n          Issues\n\n          Pull requests\n\n          Discussions\n\n          Actions\n\n          Projects\n\n          Wiki\n\n          Security\n\n          Insights\n\n      Releases\n      11.0.0\n\n        11.0.0\n\n        Latest\n\n      Latest\n\n        Compare\n\n        Choose a tag to compare\n\n    Loading\n\n          View all tags\n\n      khaledhosny\n\n    released this\n\n        23 Mar 22:36\n\n          Â·\n\n            5 commits\n\n          to main\n          since this release\n\n                11.0.0\n\n                This tag was signed with the committerâ€™s verified signature.\n\n            khaledhosny\n            Khaled Hosny\n\n            GPG key ID: 9AB24930C0B997A2\n\n            Verified\n\n          Learn about vigilant mode.\n\n              ea6a172\n\n        What's Changed\n\nThere are three new font-functions implementations (integrations) in this release:\n\nhb-coretext has gained one, calling into the CoreText library,\nhb-directwrite has gained one, calling into the DirectWrite library.\nhb-fontations has gained one, calling into the Skrifa Rust library.\nAll three are mostly useful for performance and correctness testing, but some clients might find them useful.\nAn API is added to use them from a single API by providing a backend name string:\nhb_font_set_funcs_using()\n\nSeveral new APIs are added, to load a font-face using different \"face-loaders\", and a single entry point to them all using a loader name string:\n\nhb_ft_face_create_from_file_or_fail() and hb_ft_face_create_from_blob_or_fail()\nhb_coretext_face_create_from_file_or_fail() and hb_coretext_face_create_from_blob_or_fail()\nhb_directwrite_face_create_from_file_or_fail() and hb_directwrite_face_create_from_blob_or_fail()\nhb_face_create_from_file_or_fail_using()\n\nAll drawing and painting operations using the default, hb-ot functions have become memory allocation-free.\n\nSeveral performance optimizations have been implemented.\n\nApplication of the trak table during shaping has been improved.\n\nThe directwrite shaper now supports font variations, and correctly applies user features.\n\nThe hb-directwrite API and shaper has graduated from experimental.\n\nVarious bug fixes and other improvements.\n\nNew API:\n+hb_malloc\n+hb_calloc\n+hb_realloc\n+hb_free\n+hb_face_list_loaders\n+hb_face_create_or_fail_using\n+hb_face_create_from_file_or_fail_using\n+hb_font_list_funcs\n+hb_font_set_funcs_using\n+hb_coretext_face_create_from_blob_or_fail\n+hb_directwrite_face_create_from_file_or_fail\n+hb_directwrite_face_create_from_blob_or_fail\n+hb_directwrite_font_create\n+hb_directwrite_font_get_dw_font_face\n+hb_directwrite_font_set_funcs\n+hb_fontations_font_set_funcs\n+hb_ft_face_create_from_blob_or_fail\n+hb_paint_push_font_transform\n+hb_paint_push_inverse_font_transform\n+HB_BUFFER_CLUSTER_LEVEL_GRAPHEMES\n+HB_BUFFER_CLUSTER_LEVEL_IS_MONOTONE\n+HB_BUFFER_CLUSTER_LEVEL_IS_GRAPHEMES\n+HB_BUFFER_CLUSTER_LEVEL_IS_CHARACTERS\n\nDeprecated API:\n+hb_directwrite_font_get_dw_font\n\nNew Contributors\n\n@parona-source made their first contribution in #5090\n@pweiskircher made their first contribution in #5107\n\nFull Changelog: 10.4.0...11.0.0\n\n            Contributors\n\n    pweiskircher and parona-source\n\n          Assets\n    5\n\n    Loading\n\n            2\n  stevleibelt and valgur reacted with thumbs up emoji\n\n    All reactions\n\n                  2 reactions\n\n    2 people reacted",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "c1017d760737a62f",
    "title": {
      "en": "Osgint â€“ OSINT tool to find information about GitHub user",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/hippiiee/osgint",
    "score": 25,
    "by": "CHEF-KOCH",
    "time": 1742795502,
    "content": "OSGINT\n\n  Retrieve informations about a github username/email\n\nFeatures\n\n Find github username from an email\n Find email from github username (not working all the time)\n Find informations about a profile (account creation date, public gists, id, public pgp, public ssh ...)\n\nRequirements\npip3 install -r requirements.txt\n\nUsage\n$ python3 osgint.py -h\n\n .d88888b.                    d8b          888\nd88P\" \"Y88b                   Y8P          888\n888     888                                888\n888     888 .d8888b   .d88b.  888 88888b.  888888\n888     888 88K      d88P\"88b 888 888 \"88b 888\n888     888 \"Y8888b. 888  888 888 888  888 888\nY88b. .d88P      X88 Y88b 888 888 888  888 Y88b.\n \"Y88888P\"   88888P'  \"Y88888 888 888  888  \"Y888\n                          888  v1.0.0\n                     Y8b d88P\n                      \"Y88P\"\nBy Hippie | https://twitter.com/hiippiiie\n\nusage: osgint.py [-h] [-u USERNAME] [-e EMAIL] [--json]\n\noptions:\n  -h, --help            show this help message and exit\n  -u USERNAME, --username USERNAME\n                        Github username of the account to search for (default: None)\n  -e EMAIL, --email EMAIL\n                        Email of the account to search for github username (default: None)\n  --json                Return a json output (default: False)\n\nExample output\nusername\n$ ./osgint.py -u hippiiee\n[+] login : hippiiee\n[+] id : 41185722\n[+] avatar_url : https://avatars.githubusercontent.com/u/41185722?v=4\n[+] name : Hippie\n[+] blog : https://hippie.cat\n[+] bio : Hi !\n[+] public_repos : 10\n[+] public_gists : 0\n[+] followers : 8\n[+] following : 9\n[+] created_at : 2018-07-13T08:28:00Z\n[+] updated_at : 2022-08-21T13:11:36Z\n[+] public_gists : https://gist.github.com/hippiiee\n[+] GPG_keys : https://github.com/hippiiee.gpg\n[+] GPG_key_id : 27cbb171ff857c58\n[+] email : hquere@e3r4p3.42.fr hippolyte.q@gmail.com\n\n$ ./osgint.py -u hippiiee --json\n{\n    \"GPG_key_id\": \"27cbb171ff857c58\",\n    \"GPG_keys\": \"https://github.com/hippiiee.gpg\",\n    \"avatar_url\": \"https://avatars.githubusercontent.com/u/41185722?v=4\",\n    \"bio\": \"Hi !\",\n    \"blog\": \"https://hippie.cat\",\n    \"created_at\": \"2018-07-13T08:28:00Z\",\n    \"email\": [\n        \"hquere@e3r4p3.42.fr\",\n        \"hippolyte.q@gmail.com\"\n    ],\n    \"followers\": 8,\n    \"following\": 9,\n    \"id\": 41185722,\n    \"login\": \"hippiiee\",\n    \"name\": \"Hippie\",\n    \"public_gists\": \"https://gist.github.com/hippiiee\",\n    \"public_repos\": 10,\n    \"updated_at\": \"2022-08-21T13:11:36Z\"\n}\n\nEmail\n$ ./osgint.py -e chrisadr@gentoo.org\n[+] username : ChrisADR\n\n$ ./osgint.py -e chrisadr@gentoo.org --json\n{\n    \"username\": \"ChrisADR\"\n}\n\nHow does it works ?\nTo get a user email, osingt is checking :\n\nall the public commits of the user, if the email is not hidden in one of the commit it will be added to the list\nif the user have a GPG key, if he has one, it's getting the email from the content of the GPG after a base64 decode\ngithub user API\n\nTo get a user email, osgint is checking :\n\ngithub user API\nğŸš§ spoofing a commit with the email, then checking the name in the commit history (working every time) ğŸš§ (Work In Progress)\n\nProject inspired from Zen",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "55139fbaabff0af1",
    "title": {
      "en": "Aiter: AI Tensor Engine for ROCm",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://rocm.blogs.amd.com/software-tools-optimization/aiter:-ai-tensor-engine-for-rocmâ„¢/README.html",
    "score": 75,
    "by": "hochmartinez",
    "time": 1742724713,
    "content": "AITER: AI Tensor Engine For ROCm#\n\n.author_string {\n    font-size: 14px;\n    font-weight: 300;\n    line-height: 22.5px;\n    text-transform: none;\n    margin-bottom: 8px;\n    font-family: var(--pst-font-family-base);\n    display: inline-block;\n    vertical-align: middle;\n}\n\n.bd-main .bd-content .bd-article-container {\n    max-width: 1750px !important;\n    width: 100% !important;\n    margin: 0 auto !important;\n}\n\n@media (min-width: 1200px) {\n    .bd-main .bd-content .bd-article-container .bd-article {\n        padding-left: 2rem;\n    }\n}\n\n.bd-container {\n    max-width: 1750px !important;\n    width: 100% !important;\n    margin: 0 auto !important;\n}\n\n.author_string {\n\n    font-size: 14px;\n\n    font-weight: 300;\n\n    line-height: 22.5px;\n\n    text-transform: none;\n\n    margin-bottom: 8px;\n\n    font-family: var(--pst-font-family-base);\n\n    display: inline-block;\n\n    vertical-align: middle;\n\n}\n\n.author_string_right {\n    font-size: 14px;\n    font-weight: 300;\n    line-height: 22.5px;\n    text-transform: none;\n    margin-bottom: 8px;\n    font-family: var(--pst-font-family-base);\n    display: inline-block;\n    vertical-align: middle;\n    flex-direction: row-reverse;\n}\n\n.author_string:last-child {\n    margin-bottom: 0px;\n}\n\n.author_string svg {\n    margin-right: 5px;\n}\n\n.bd-sidebar-primary {\n    display: none;\n}\n\n.sidebar-toggle {\n    display: none;\n}\n\nlabel.sidebar-toggle.primary-toggle.btn.btn-sm {\n    display: none;\n}\n\n.author_row {\n    display: flex;\n    justify-content: space-between;\n    align-items: flex-start;\n    width: 100%;\n    margin-bottom: 3px;\n    padding: 2px;\n}\n\n.author_row_right {\n    display: flex;\n    justify-content: space-between;\n    align-items: flex-start;\n    width: 100%;\n    margin-bottom: 3px;\n    padding: 2px;\n    flex-direction: row-reverse;\n}\n\n.author_right {\n    display: flex;\n    flex-direction: column-reverse;\n    max-width: 50%;\n    word-wrap: break-word;\n    align-items: right;\n}\n\n.author_left {\n    display: flex;\n    flex-direction: column-reverse;\n    max-width: 50%;\n    word-wrap: break-word;\n    align-items: left;\n}\n\n.image-container {\n    display: flex;\n    align-items: center;\n    justify-content: center;\n    padding: 5px;\n}\n\n.responsive-image {\n    width: 100%;\n    height: 480px;\n    object-fit: cover;\n    border: 5px solid #333;\n    box-shadow: 0 0 10px rgba(0,0,0,0.5);\n}\n\n             March 21, 2025 by Shekhar Pandey, Liz Li, Carlus Huang, Lingpeng Jin, Anshul Gupta.\n\n            5 min read. | 1331 total words.\n\n            Software tools & optimizations\n\n            AI/ML\n\n.icon-bar.fixed,\n.icon-bar.horizontal {\n    display: none;\n}\n\n@media screen and (min-width: 1520px) {\n    .icon-bar.fixed {\n        display: flex;\n        position: fixed;\n        top: 50%;\n        right: 0;\n        transform: translateY(-50%);\n        flex-direction: column;\n        z-index: 1000;\n    }\n    .icon-bar.fixed a {\n        display: block;\n        text-align: center;\n        padding: 16px;\n        font-size: 20px;\n        color: white;\n        transition: background-color 0.3s, color 0.3s;\n        background-size: 25px 25px;\n    }\n    .icon-bar.fixed a svg {\n        width: 25px;\n        height: 25px;\n    }\n    .icon-bar.fixed a:hover {\n        background-color: #000;\n    }\n}\n\n@media screen and (max-width: 1520px) {\n    .icon-bar.horizontal {\n        display: flex;\n        flex-direction: row;\n        justify-content: flex-start;\n        margin: 20px 0;\n    }\n    .icon-bar.horizontal a {\n        width: 30px;\n        height: 30px;\n        display: flex;\n        align-items: center;\n        justify-content: center;\n        font-size: 14px;\n        color: white;\n        margin-left: 8px;\n        transition: background-color 0.3s, color 0.3s;\n        background-size: 30px 30px;\n        padding: 4px;\n    }\n    .icon-bar.horizontal a svg {\n        width: 20px;\n        height: 20px;\n    }\n    .icon-bar.horizontal a:first-child {\n        margin-left: 0;\n    }\n    .icon-bar.horizontal a:hover {\n        background-color: #000;\n    }\n}\n\n.facebook {\n    background: #3B5998;\n}\n.twitter {\n    background: #55ACEE;\n}\n.reddit {\n    background: #dd4b39;\n}\n.linkedin {\n    background: #007bb5;\n}\n.google {\n    background: #bb0000;\n}\n\nPerformance optimization is critical when working with GPUs, especially for tasks involving artificial intelligence, which can be extremely demanding. To fully leverage the capabilities of advanced hardware, itâ€™s essential to master optimization strategies and ensure every available resource is utilized efficiently. In this blog we will provide an overview ofAMDâ€™s AI Tensor Engine for ROCm (AITER) and show you how easy it is to integrate AITER kernels in basic LLM training and inference workload.AITER helps developers to focus on creating operators while allowing customers to seamlessly integrate this operator collection into their own private, public, or any custom framework.\n\nWhat is AI Tensor Engine for ROCm (AITER)#\nAMD is introducing the AI Tensor Engine for ROCm (AITER), acentralized repository filled with high-performance AI operators [1]designed to accelerate various AI workloads. AITER serves as a unified platform where customers can easily find and integrate optimized operators into their existing frameworksâ€”be it private, public, or custom-built as you can see in Figure 1 below. With AITER, AMD simplifies the complexity of optimization, enabling users to maximize performance while providing flexibility to meet diverse AI requirements.\n\nFigure 1: Block level diagram of AITER#\n\nKey Features#\n\nVersatile and User-Friendly Design: AITERâ€™s architecture is carefully crafted for versatility and ease of use, allowing seamless integration into various workflows and systems.\nDual Programming Interfaces: At the highest abstraction level, AITER supports two primary interfacesâ€”C++ and Python (Torch API). This dual-interface approach makes AITER highly accessible, catering to developers with different programming preferences and skillsets.\nRobust Kernel Infrastructure: Underneath the user-level APIs, AITER employs a powerful and robust kernel infrastructure. This infrastructure is built upon a variety of underlying technologies, including Triton, CK (Compute Kernel), ASM (Assembly), and HIP (Heterogeneous Interface for Portability).\nComprehensive Kernel Support: The AITER kernel ecosystem efficiently supports diverse computational tasks such as inference workloads, training kernels, GEMM (General Matrix Multiplication) operations, and communication kernels. Such comprehensive kernel support ensures that users can confidently handle complex and resource-intensive AI tasks.\nCustomizable and Optimizable Kernel Ecosystem: With its rich kernel environment, AITER allows developers to perform customized optimizations tailored specifically to their applications. This flexibility helps developers to bypass or overcome architectural limitations, resulting in significantly enhanced performance and adaptability.\nSeamless Integration with AMD ROCm: At its core, AITER leverages AMDâ€™s ROCm, ensuring efficient bridging between optimized kernels and AMD GPUs. This integration unlocks the full potential and peak performance of AMD GPUs, delivering optimal efficiency across a wide range of AI workloads.\n\nBy combining user-friendly interfaces, extensive kernel capabilities, and robust GPU integration, AITER empowers developers to achieve maximum efficiency and performance in their AI applications.\n\nPerformance Gains with AITER#\nBy leveraging AITERâ€™s advanced optimizations, users can experience significant performance improvements across various AI operations:\n\nAITER block-scale GEMM: Achieves up to 2x performance boost [2], substantially accelerating general matrix multiplication tasks.\nAITER block-scale fused MoE: Delivers up to 3x performance boost [3], optimizing the efficiency of Mixture of Experts (MoE) operations.\nAITER MLA for decode: Provides an impressive up to 17x performance boost[4], dramatically enhancing decoding efficiency.\nAITER MHA for prefill: Realizes up to 14x performance boost[5], significantly improving Multi-Head Attention (MHA) performance during prefill stages.\n\nAITERâ€™s Integration in vLLM/SGLang for DeepSeek V3/R1#\nThe integration of AITER into vLLM/SGLang for the DeepSeek v3/r1 model has led to remarkable improvements in total token throughput (tokens per second, tok/s). Before AITERâ€™s integration, the throughput stood at 6484.76 tok/s. After incorporating AITERâ€™s optimizations, throughput dramatically increased to 13704.36 tok/s, marking more than a 2x improvement[6] in processing speed as shown in Figure 2 below.\n\nFigure 2.Throughput Comparison: Before and After Integrating AITER in SGLang on DeepSeek Models on AMD Instinctâ„¢ MI300X.#\n\nRunning Deepseek with AITER#\nusing vLLM\nVLLM_SEED=42 VLLM_MLA_DISABLE=0 VLLM_USE_TRITON_FLASH_ATTN=0 \\\nVLLM_USE_ROCM_FP8_FLASH_ATTN=0 VLLM_FP8_PADDING=1 VLLM_USE_AITER_MOE=1 \\\nVLLM_USE_AITER_BLOCK_GEMM=1 VLLM_USE_AITER_MLA=0 vllm serve \\\n/workspace/models/DeepSeek-R1 \\\n  --host 0.0.0.0 \\\n  --port 8000 \\\n  --api-key abc-123 \\\n  --tensor-parallel-size 8 \\\n  --trust-remote-code \\\n  --seed 42\n\n  Copy to clipboard\n\nusing SGLang\nCK_BLOCK_GEMM=1 SGLANG_ROCM_AITER_BLOCK_MOE=1 RCCL_MSCCL_ENABLE=0 \\\nDEBUG_HIP_BLOCK_SYN=1024 GPU_FORCE_BLIT_COPY_SIZE=64 \\\npython3 -m sglang.launch_server  --model /models/DeepSeek-V3/ \\\n --tp 8 --trust-remote-code\n\n  Copy to clipboard\n\nGetting Started with AITER#\nTo begin working with AITER, follow these simple installation steps:\n\nClone the repository:\n\ngit clone https://github.com/ROCm/aiter.git\ncd AITER\n\n  Copy to clipboard\n\nUnder the AITER root directory, run the following command to install the library in development mode:\n\npython3 setup.py develop\n\n  Copy to clipboard\n\nImplementing a Simple Linear Layer Using AITER#\nLetâ€™s demonstrate how you can implement a simple replica of PyTorchâ€™s linear layer using AITERâ€™s tgemm function.\nfrom aiter.tuned_gemm import tgemm\nimport torch\n\nclass LinearLayer(torch.nn.Module):\n\tdef **init**(self, in_features, out_features):\n\t\tsuper(LinearLayer, self).**init**()\n\t\tself.weight = torch.nn.Parameter(torch.randn(out_features, in_features).cuda())\n\t\tself.bias = torch.nn.Parameter(torch.randn(out_features).cuda())\n\n\tdef forward(self, input):\n\t\tinput = input.cuda()\n\t\treturn [tgemm.mm](http://tgemm.mm/)(input, self.weight, self.bias, None, None)\n\n# Define input size and layer size\nin_features = 128\nout_features = 64\nbatch_size = 32\n\n# Create custom AITER linear layer\nlayer = LinearLayer(in_features, out_features).cuda()\ninput_tensor = torch.randn(batch_size, in_features).cuda()\n\n# Get output from AITER linear layer\noutput_aiter = layer(input_tensor)\n\n# Create PyTorch linear layer with same weights and bias\npytorch_layer = torch.nn.Linear(in_features, out_features).cuda()\npytorch_layer.weight = torch.nn.Parameter(layer.weight.clone())\npytorch_layer.bias = torch.nn.Parameter(layer.bias.clone())\n\n# Get output from PyTorch linear layer\noutput_pytorch = pytorch_layer(input_tensor)\n\n# Compare outputs\nprint(\"Output difference (max absolute error):\", torch.max(torch.abs(output_aiter - output_pytorch)))\nprint(\"Output difference (mean absolute error):\", torch.mean(torch.abs(output_aiter - output_pytorch)))\n\n  Copy to clipboard\n\nIt can be very simple using AITER in daily workload, some of the other low level kernel APIs are mentioned as below which can be used to integrate in your architecture.\n\nKernel\nAPI\n\nMHA (Flash Attention)\naiter.flash_attn_func()\n\nLayerNorm\naiter.layer_norm()\n\nLayerNormFusedResidualAdd\naiter.layernorm2d_with_add_asm()\n\nRoPE forward\naiter.rope_fwd()\n\nRoPE backward\naiter.rope_bwd()\n\nRMSNorm\naiter.rms_norm()\n\nMLA Decode\naiter.ops.triton.mla_decode()\n\nAITER is not just limited to the above mentioned APIs, there are a lot of features available as mentioned in the below table and a lot are coming very soon.\n\nFeature\nType (F=Forward, B=Backward)\nDetails\n\nPrefill Attention\nF/B\nFav3 FWD FP16/BF16  Fav3 BWD FP16/BF16  MLA FP16/BF16  FA FP8 FWD+BWD (Block-Scale)  Chunked-prefill\n\nDecode Attention\nF\nPaged Attention FP16/BF16  Paged Attention FP8 per-tensor quant  Paged Attention FP8/INT8 with KV per-token quant  KVCache Update & Rotary  Batched Decoding  MLA Decoding\n\nFused-Moe\nF\nMoe-Sorting kernel and tiling solution  FP16/BF16 per-token Fused-Moe  FP8/INT8 per-token Fused-Moe  FP8 per-tensor Fused-Moe  FP8/INT4 per-tensor Fused-Moe  Fused-FFN\n\nLow Precision Gemm\nF\nFP8 per-token/channel Gemm  FP8 Block Scale Gemm  INT8 weight-only Gemm\n\nDistributed Gemm\nF/B\nDistributed GEMM\n\nNormalization and Fusion\nF\nLayernorm+quant/shortcut  RMSNorm+quant/shortcut\n\nCustom Comm.\nF\nAR/AG fused with normalization  AR/AG quantized, Optimized hipgraph support\n\nConv2d/2d\nF/B\nFP16/BF16 fwd/bwd/wrw  Fusion with bias/activation, etc.\n\nSummary#\nIn this blog we introduced AMDâ€™s AI Tensor Engine for ROCm (AITER), our centralized high performance AI operators repository, designed to significantly accelerate AI workloads on AMD GPUs. AITER has already demonstrated its value by substantially accelerating AI workloads and significantly improving efficiency and performance. AMD remains committed to continuous innovation, with numerous further enhancements and optimization efforts currently underway. The roadmap includes even greater advancements, which promise to set new standards in AI computation. Stay tuned as AMD continues to push the boundaries of performance, ensuring machine learning engineers can consistently achieve faster, more efficient, and more powerful AI solutions.\n\nAdditional Resources#\nAITER Github: ROCm/aiter\n\nDisclaimers#\nThird-party content is licensed to you directly by the third party that owns the\ncontent and is not licensed to you by AMD. ALL LINKED THIRD-PARTY CONTENT IS\nPROVIDED â€œAS ISâ€ WITHOUT A WARRANTY OF ANY KIND. USE OF SUCH THIRD-PARTY CONTENT\nIS DONE AT YOUR SOLE DISCRETION AND UNDER NO CIRCUMSTANCES WILL AMD BE LIABLE TO\nYOU FOR ANY THIRD-PARTY CONTENT. YOU ASSUME ALL RISK AND ARE SOLELY RESPONSIBLE\nFOR ANY DAMAGES THAT MAY ARISE FROM YOUR USE OF THIRD-PARTY CONTENT.\n\n[1]\nAI operators :  optimized mathematical functions or computational kernels that perform fundamental AI and machine learning tasks, such as matrix multiplications, convolutions, and activations, which are crucial for accelerating AI workloads.\n\n[2]\nOn average,â€¯a system configured with an AMD Instinctâ„¢ MI300X GPU shows that AITER block-scale GEMM offers a 2x performance boost substantially accelerating general matrix multiplication tasks. Testing done by AMD on 03/011/2025, results may vary based on configuration, usage, software version, and optimizations.\nSYSTEM CONFIGURATION: AMD Instinctâ„¢ MI300X platform\n- System Model: Supermicro GPU A+ Server AS - 8125GS-TNMR2\n- CPU: 2x AMD EPYC 9654 96-core Processor (2 sockets, 96 cores per socket, 2 threads per core)\n- NUMA Config: 2 NUMA node per socket\n- Memory: 2.3 TiB (24 DIMMs, 4800 mts, 96 GiB/DIMM)\n- Disk: Root drive + Data drive combined:\n- 2x 960GB Samsung MZ1L2960HCJR-00A07\n- 4x 3.84TB Samsung MZQL23T8HCLS-00A07\n- GPU: 8x AMD MI300X 192GB HBM3 750W\n- Host OS: Ubuntu 22.04.4 LTS with Linux kernel 5.15.0-116-generic.\n- System BIOS: 3.2\n- System Bios Vendor:   American Megatrends International, LLC.\n- Host GPU Driver (amdgpu version): 6.10.5\n\n[3]\nOn average,â€¯a system configured with an AMD Instinctâ„¢ MI300X GPU shows AITER block-scale fused MoE offers a 3x performance boost, optimizing the efficiency of Mixture of Experts (MoE) operations.â€¯ Testing done by AMD on 03/011/2025, results may vary based on configuration, usage, software version, and optimizations.\nSYSTEM CONFIGURATION: AMD Instinctâ„¢ MI300X platform\n- System Model: Supermicro GPU A+ Server AS - 8125GS-TNMR2\n- CPU: 2x AMD EPYC 9654 96-core Processor (2 sockets, 96 cores per socket, 2 threads per core)\n- NUMA Config: 2 NUMA node per socket\n- Memory: 2.3 TiB (24 DIMMs, 4800 mts, 96 GiB/DIMM)\n- Disk: Root drive + Data drive combined:\n- 2x 960GB Samsung MZ1L2960HCJR-00A07\n- 4x 3.84TB Samsung MZQL23T8HCLS-00A07\n- GPU: 8x AMD MI300X 192GB HBM3 750W\n- Host OS: Ubuntu 22.04.4 LTS with Linux kernel 5.15.0-116-generic.\n- System BIOS: 3.2\n- System Bios Vendor:   American Megatrends International, LLC.\n- Host GPU Driver (amdgpu version): 6.10.5\n\n[4]\nOn average,â€¯a system configured with an AMD Instinctâ„¢ MI300X GPU shows that AITER MLA for decode offers a 17x performance boost enhancing decoding efficiency.â€¯ Testing done by AMD on 03/011/2025, results may vary based on configuration, usage, software version, and optimizations.\nSYSTEM CONFIGURATION: AMD Instinctâ„¢ MI300X platform\n- System Model: Supermicro GPU A+ Server AS - 8125GS-TNMR2\n- CPU: 2x AMD EPYC 9654 96-core Processor (2 sockets, 96 cores per socket, 2 threads per core)\n- NUMA Config: 2 NUMA node per socket\n- Memory: 2.3 TiB (24 DIMMs, 4800 mts, 96 GiB/DIMM)\n- Disk: Root drive + Data drive combined:\n- 2x 960GB Samsung MZ1L2960HCJR-00A07\n- 4x 3.84TB Samsung MZQL23T8HCLS-00A07\n- GPU: 8x AMD MI300X 192GB HBM3 750W\n- Host OS: Ubuntu 22.04.4 LTS with Linux kernel 5.15.0-116-generic.\n- System BIOS: 3.2\n- System Bios Vendor:   American Megatrends International, LLC.\n- Host GPU Driver (amdgpu version): 6.10.5\n\n[5]\nOn average,â€¯a system configured with an AMD Instinctâ„¢ MI300X GPU with AITER MHA for prefill shows a14x performance boost, improving Multi-Head Attention (MHA) performance during prefill stages.Testing done by AMD on 03/011/2025, results may vary based configuration, usage, software version, and optimizations.\nSYSTEM CONFIGURATION: AMD Instinctâ„¢ MI300X platform\n- System Model: Supermicro GPU A+ Server AS - 8125GS-TNMR2\n- CPU: 2x AMD EPYC 9654 96-core Processor (2 sockets, 96 cores per socket, 2 threads per core)\n- NUMA Config: 2 NUMA node per socket\n- Memory: 2.3 TiB (24 DIMMs, 4800 mts, 96 GiB/DIMM)\n- Disk: Root drive + Data drive combined:\n- 2x 960GB Samsung MZ1L2960HCJR-00A07\n- 4x 3.84TB Samsung MZQL23T8HCLS-00A07\n- GPU: 8x AMD MI300X 192GB HBM3 750W\n- Host OS: Ubuntu 22.04.4 LTS with Linux kernel 5.15.0-116-generic.\n- System BIOS: 3.2\n- System Bios Vendor:   American Megatrends International, LLC.\n- Host GPU Driver (amdgpu version): 6.10.5\n\n[6]\nOn average,â€¯a system configured with an AMD Instinctâ„¢ MI300X GPU shows that before AITERâ€™s integration, the throughput stood at 6484.76 tok/s. After incorporating AITERâ€™s optimizations, throughput dramatically increased to 13704.36 tok/s, marking more than a 2x improvement in processing speed. Testing done by AMD on 03/11/2025, results may vary based on configuration, usage, software version, and optimizations.\nSYSTEM CONFIGURATION: AMD Instinctâ„¢ MI300X platform\n- System Model: Supermicro GPU A+ Server AS - 8125GS-TNMR2\n- CPU: 2x AMD EPYC 9654 96-core Processor (2 sockets, 96 cores per socket, 2 threads per core)\n- NUMA Config: 2 NUMA node per socket\n- Memory: 2.3 TiB (24 DIMMs, 4800 mts, 96 GiB/DIMM)\n- Disk: Root drive + Data drive combined:\n- 2x 960GB Samsung MZ1L2960HCJR-00A07\n- 4x 3.84TB Samsung MZQL23T8HCLS-00A07\n- GPU: 8x AMD MI300X 192GB HBM3 750W\n- Host OS: Ubuntu 22.04.4 LTS with Linux kernel 5.15.0-116-generic.\n- System BIOS: 3.2\n- System Bios Vendor:   American Megatrends International, LLC.\n- Host GPU Driver (amdgpu version): 6.10.5",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "13d4eb798a832725",
    "title": {
      "en": "The Vatican's Latinist (2017)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://newcriterion.com/article/the-vaticans-latinist/",
    "score": 68,
    "by": "bookofjoe",
    "time": 1742779791,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "542f250f45f190b5",
    "title": {
      "en": "Langfuse (YC W23) Is Hiring in Berlin, Germany",
      "ko": null,
      "ja": null
    },
    "type": "job",
    "url": "https://langfuse.com/careers",
    "score": 1,
    "by": "clemo_ra",
    "time": 1742810172,
    "content": "Join Us!\nLangfuse is building the leading open-source LLM engineering platform (Why Langfuse?).\nWhile LLMs improve a lot, we donâ€™t see enough applications in production. Building these applications requires a new workflow of continuous monitoring and evaluation that we enable with Langfuse.\nWe are seeing strong traction over recent months (see metrics below), thus it is the right time to grow the team to build out our backend systems, product, and how we communicate with developers.\nWe are hiring across for engineering and technical go-to-market roles. We work in-person in Berlin, Germany & are building an excellent, small team. If you are considering new opportunities â€” even if you might not be looking immediately, we would love to hear from you & get coffee.\nWe have raised a $4M seed round from Lightspeed, General Catalyst, Y Combinator and angels. We are growing fast (see metrics below) and work with some of the best AI teams such as Samsara, Twilio, KhanAcademy and Rocket Money.\nLearn more about Langfuse\nDocumentation\n10 min product walkthrough\nInteractive demo\nOrigin story\nLangfuse 2.0 scope\nEngineering blog on Langfuse v3\n\nTeam\n\nClemens Rawert, @rawert, Linkedin\nMarc Klingen, @marcklingen, GitHub, Linkedin\nMarlies Mayerhofer, @marliessophie, GitHub, LinkedIn\nMax Deichmann, @maxdeichmann, GitHub, Linkedin\nHassieb Pakzad, @hassiebpakzad, GitHub, LinkedIn\nSteffen Schmitz, GitHub, LinkedIn\nJannik MaierhÃ¶fer, @JMaierhoefer, GitHub, LinkedIn\n\nA few tidbits that might help you understand what kind of company we are:\n\nWe have 2 scheduled meetings per week: 15 min planning on Monday, 60 min demo on Friday\nWe are a very high-trust environment, Reed Hastingsâ€™ â€˜No Rules Rulesâ€™ inspires our culture, hereâ€™s a summary\nMost team members have led teams before and are excited about being an IC again to ship more\nWe were heavily influenced by our time at Y Combinator and follow its ethos. Weâ€™re a Silicon Valley Startup in Berlin - more here\n\nPS: We are based in Berlin (Germany) and work in person. Come join us build one of the most exciting open source devtool companies in Europe!\nRoles\nCanâ€™t find a role thatâ€™s a perfect fit? If you are excited about delivering\nexceptional developer experiences and building a high-quality open-source\nproduct alongside an insanely motivated team that ships, feel\nfree to reach out: careers@langfuse.com\n\nDesign Engineer\n\nBerlin, â‚¬70K - â‚¬130K EUR, 0.1% - 0.35%\nYou are a design-minded frontend engineer who is passionate about building beautiful, functional, and user-friendly interfaces. You own UI/UX in the Langfuse application to help our users achieve their goals and create how thousands of teams build their LLM applications.\n\nBackend Engineer\n\nBerlin, â‚¬70K - â‚¬130K EUR, 0.1% - 0.35%\nYou will work on our backend systems, powering the core infrastructure of Langfuse. You work on a data-intensive application that is used by hundreds of companies and thousands of developers ingesting hundreds of millions of events.\nThe technical blog post on the v3 release is a great example of what you could have worked on.\n\nProduct Engineer\n\nBerlin, â‚¬70K - â‚¬130K EUR, 0.1% - 0.35%\nDesign, scope, ship, and launch new full-stack (UI, backend, SDKs, integrations) features on top of our existing data platform. Youâ€™ll own what you build truly end-to-end from first conversations with users to operating it in production.\nSee launch weeks (#1, #2) and changelog for a good selection of things you could have worked on.\n\nDeveloper Advocate\n\nBerlin, â‚¬70K - â‚¬130K EUR, 0.1% - 0.35%\nRun our bottom-up go-to-market via developer-focused content, documentation, partnerships, events, and community-building. Langfuse is a platform of many building blocks and there are lots of common workflows we can educate on to drive awareness and adoption.\nExamples: Hugging Face and Posthog partnerships, LLM Security Monitoring guides/examples, AWS partnership, our YouTube channel\n\nFounding GTM Engineer\n\nBerlin, â‚¬70K - â‚¬130K EUR, 0.1% - 0.35%\nWork closely with our enterprise customers throughout the inbound sales cycle. Help them resolve complex technical questions, succeed in POCs, and expand after initial adoption.\nBuild ressources and improve the process to further increase scalability of our GTM.\n\nPlease apply via Work At a Startup or send us an email with your CV/LinkedIn to careers@langfuse.com\nYou need to be located in Berlin, Germany or willing to relocate. The first few interviews can be done remotely via Zoom. In the final stage, we will work one day onsite together (usually on an actual high-priority project).\nWhy Join Langfuse?\n\nLangfuse HQ, Berlin (Mitte), Germany\nImpactful work/product for awesome OSS community and users. We get to work with extremely smart users in our OSS community who share a ton of great feedback, and it feels great to help them build exciting products. Youâ€™ll also get to work with many large enterprises and YC companies who monitor and improve their LLM products with Langfuse. Community channels: GitHub Discussions, GitHub Issues, Discord, recordings of community town halls\nStrong in-person engineering culture. Almost everyone on the team works on the product; our go-to-market (also Enterprise) is exclusively inbound, and we enjoy building a developer tool together with our open-source community.\nInteresting challenges. Building an open source developer tool with a strong observability component is engineering-wise highly interesting due the various integrations and SDKs, the chance to design the best LLM Engineering workflows for these teams, and the necessity to scale our backend-logic and infrastructure.\nGrowth & Ownership. We believe that a product like ours is best built by a small group of excellent engineers who fully own the components that they build. This includes running them in production, product feedback and roadmap planning, and design. We are a small team thatâ€™s moving fast (changelog) with a focus on great developer experience.\nShared curiosity and interest. If you are interested in working with engineering teams who rethink whatâ€™s possible when using LLMs to power new use cases, come work with us as we all share this excitement and get to see the leading edge of what great teams build every day. Next to shipping quickly this is the most fun part for sure.\nPublic Metrics\n\nOpen dashboard in new tab â†—Last updated on March 7, 2025Was this page useful?YesCould be betterQuestions? We're here to helpGitHub Q&AGitHubChat EmailTalk to salesSubscribe to updatesGetupdates",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "b97d55479f95d8e5",
    "title": {
      "en": "Show HN: I built website for sharing Drum Patterns",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "http://drumpatterns.onether.com",
    "score": 340,
    "by": "wesz",
    "time": 1742735121,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "8aa9f568f782b2a4",
    "title": {
      "en": "HP avoids monetary damages over bricked printers in class-action settlement",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://arstechnica.com/gadgets/2025/03/hp-avoids-monetary-damages-over-bricked-printers-in-class-action-settlement/",
    "score": 43,
    "by": "JSR_FDED",
    "time": 1742806818,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "85899ffe934cf308",
    "title": {
      "en": "The SeL4 Microkernel: An Introduction [pdf]",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://sel4.systems/About/seL4-whitepaper.pdf",
    "score": 229,
    "by": "snvzz",
    "time": 1742728168,
    "content": "The seL4 Â®   Foundation https://sel4.systems/Foundation  The seL4 Microkernel An Introduction  Gernot Heiser  gernot@sel4.systems Revision 1.4 of 2025-01-08\n\nAbstract  This whitepaper provides an introduction to and overview of seL4. We explain what seL4 is (and is not) and explore its defining features. We explain what makes seL4 uniquely qualified as the operating-system kernel of choice for security- and safety-critical systems, and generally embedded and cyber-physical systems. In particular, we explain seL4â€™s assurance story, its security- and safety-relevant features, and its benchmark-setting performance. We also discuss typical usage scenarios, including incremental cyber retrofit of legacy systems.  CCS Concepts  â€¢   Software and its engineering   â†’   Operating Systems  â€¢   Security and privacy   â†’   Systems security  â€¢   Security and privacy   â†’   Formal methods and theory of security  â€¢   Computer systems organization   â†’   Real-time systems   â†’   Real-time operating systems  â€¢   Computer systems organization   â†’   Real-time systems   â†’   Dependable and fault-tolerant systems and networks  Keywords  seL4, microkernel, performance  Reference Format:  Gernot Heiser. The seL4 Microkernel â€“ An Introduction. White paper. The seL4 Foundation, Revision 1.4 of 2025-01-08.  Copyright   Â©   2020 seL4 Project a Series of LF Projects, LLC. Distributed under the Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) License. seL4 is a trademark of LF Projects, LLC.\n\nContents  List of Figures   2 1   What Is seL4?   3 2   seL4 Is a Microkernel and a Hypervisor, It Is Not an OS   5  2.1   Monolithic kernels vs microkernels   . . . . . . . . . . . . . . . . . . . . .   5 2.2   seL4 Is a microkernel, not an OS . . . . . . . . . . . . . . . . . . . . . . .   7 2.3   seL4 is also a hypervisor   . . . . . . . . . . . . . . . . . . . . . . . . . . .   8 2.4   seL4 is not seLinux   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   10  3   seL4â€™s Verification Story   11  3.1   Correctness and security enforcement   . . . . . . . . . . . . . . . . . . .   12 3.2   The seL4 Microkit   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   15  4   About Capabilities   19  4.1   What are capabilities? . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   19 4.2   Why Capabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   20  5   Support for Hard Real-Time Systems   24  5.1   General real-time support . . . . . . . . . . . . . . . . . . . . . . . . . . .   24 5.2   Mixed-criticality systems   . . . . . . . . . . . . . . . . . . . . . . . . . . .   25  6   Security is No Excuse for Poor Performance   29 7   Real-World Deployment and Incremental Cyber Retrofit   30  7.1   General considerations   . . . . . . . . . . . . . . . . . . . . . . . . . . . .   30 7.2   Retrofitting existing systems   . . . . . . . . . . . . . . . . . . . . . . . . .   30  8   Conclusions   32 Bibliography   34  1\n\nList of Figures  2.1   Operating-system structure.   . . . . . . . . . . . . . . . . . . . . . . . . .   5 2.2   L4 microkernel family tree.   . . . . . . . . . . . . . . . . . . . . . . . . . .   7 2.3   VM-provided services   . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   9 2.4   seL4 virtualisation support with usermode VMMs. . . . . . . . . . . . . .   9 3.1   seL4â€™s proof chain. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   11 3.2   Translation validation proof chain. . . . . . . . . . . . . . . . . . . . . . .   13 3.3   Microkit abstractions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   15 3.4   Verified architecture mapping and system generation.   . . . . . . . . . .   17 4.1   Capabilities are keys to objects . . . . . . . . . . . . . . . . . . . . . . . .   19 4.2   The compiler as a confused deputy. . . . . . . . . . . . . . . . . . . . . .   22 5.1   Simplified example of a mixed-criticality system. . . . . . . . . . . . . . .   26 7.1   Incremental cyber-retrofit on the ULB. . . . . . . . . . . . . . . . . . . . .   31 2\n\nChapter 1 What Is seL4?  seL4 is an operating system microkernel  An operating system (OS) is the low-level system software that controls a computer systemâ€™s resources and enforces security. Unlike application software, the OS has exclusive access to a more privileged execution mode of the processor ( kernel mode ) that gives it direct access to hardware. Applications only ever execute in   user mode   and can only access hardware as permitted by the OS. An OS microkernel is a minimal core of an OS, reducing the code executing at higher privilege to a minimum. seL4 is a member of the L4 family of microkernels that goes back to the mid-1990s. (And no,   seL4 has nothing to do with seLinux. )  seL4 is also a hypervisor  seL4 supports virtual machines that can run a fully fledged guest OS such as Linux. Subject to seL4â€™s enforcement of communication channels, guests and their applications can communicate with each other as well as with native applications. Learn more about what it means that seL4 is a microkernel and its use as a hypervisor in Chapter 2. And learn about real-world deployment scenarios, including approaches for retrofitting security into legacy systems in Chapter 7.  seL4 is proved correct  seL4 comes with a formal, mathematical, machine-checked   proof of implementation correctness , meaning the kernel is in a very strong sense â€œbug freeâ€ with respect to its specification. In fact, seL4 is the worldâ€™s first OS kernel with such a proof at the code level [Klein et al., 2009].  seL4 is provably secure  Besides implementation correctness, seL4 comes with further proofs of   security enforcement   [Klein et al., 2014]. They say that in a correctly configured seL4-based system, the kernel guarantees the classical security properties of  confidentiality, integrity and availability . More about these proofs in Chapter 3.  seL4 improves security with fine-grained access control through capabilities  Capabilities are access tokens which support very fine-grained control over which entity can access a particular resource in a system. They support strong 3\n\nsecurity according to the principle of least privilege (also called principle of least authority, POLA). This is a core design principle of highly secure system, and is impossible to achieve with the way access control happens in mainstream systems such as Linux or Windows. seL4 is still the   worldâ€™s only OS that is both capability-based and formally verified , and as such has a defensible claim of being the worldâ€™s most secure OS. More about capabilities in Chapter 4.  seL4 ensures safety of time-critical systems  seL4 is the worldâ€™s only OS kernel (at least in the open literature) that has undergone a complete and sound analysis of its   worst-case execution time  (WCET) [Blackham et al., 2011, Sewell et al., 2017]. This means, if the kernel is configured appropriately, all kernel operations are bounded in time, and the bound is known. This is a prerequisite for building   hard real-time systems , where failure to react to an event within a strictly bounded time period is catastrophic.  seL4 is the worldâ€™s most advanced mixed-criticality OS  seL4 provides strong support for mixed criticality real-time systems (MCS), where the timeliness of critical activities must be ensured even if they co-exist with less trusted code executing on the same platform. seL4 achieves this with a flexible model that retains good resource utilisation, unlike the more established MCS OSes that use strict (and inflexible) time and space partitioning [Lyons et al., 2018]. More on seL4â€™s real-time and MCS support in Chapter 5.  seL4 is the worldâ€™s fastest microkernel  Traditionally, systems are either (sort-of) secure, or they are fast. seL4 is unique in that it is both. seL4 is designed to support a wide range of real-world use cases, whether they are security- (or safety-)critical or not, and excellent performance is a requirement. More on seL4â€™s performance in Chapter 6.  seL4 is pronounced â€œess-e-ell-fourâ€  The pronunciation â€œsell-fourâ€ is deprecated.  How to read this document  This document is meant to be approachable by a wide audience. However, for completeness, we will add some deeper technical detail in places.  Such detail will be marked with a chilli, like the one on the left. If you see this then you know you can safely skip the marked passage if you think the technical description is too â€œspicyâ€ for your taste, or if you are simply not interested in this level of detail. Only other chillied passages will assume you have read it.  Technical section  Where the chilli appears in a section title, such as here, this indicates that the whole section is fairly technical and can be skipped. 4\n\nChapter 2 seL4 Is a Microkernel and a Hypervisor, It Is Not an OS  2.1   Monolithic kernels vs microkernels  To understand the difference between a mainstream OS, such as Linux, and a microkernel, such as seL4, letâ€™s look at Figure 2.1. 10   kSLOC  Hardware  VFS  IPC, File System  Scheduler, Virtual Memory  Device Drivers, Dispatcher  Syscall  Hardware  IPC, Threads, Virtual Memory  Application  NW  Proto -  col  Stack  File  Server  Device  Driver  IPC  Kernel  Mode  User  Mode  20,000  kSLOC  Figure 2.1: Operating-system structure: Monolithic kernel (left) vs microkernel (right). The left side presents a (fairly abstracted) view of the architecture of a system such as Linux. The yellow part is the OS   kernel , it offers services such as file storage and networking to applications. All the code that implements those services executes in the   privileged mode   of the hardware, also called   kernel mode   or   supervisor mode   â€“ the execution mode that has unfettered access and control of all resources in the system. In contrast, applications run in unprivileged, or   user mode , and do not have direct access to many hardware resources, which must be accessed through the OS. The OS is internally structured in a number of layers, where each layer provides abstractions implemented by layers below. The problem with privileged-mode code is that it is dangerous: If anything goes wrong here, thereâ€™s nothing to stop the damage. In particular, if this code has a bug that can be exploited by an attacker to run the attackerâ€™s code in privileged mode (called a privilege-escalation or arbitrary code-execution attack) then the attacker can do what they want with the system. Such flaws are the root problem of the many system compromises we experience in mainstream systems. 5\n\nOf course, software bugs are mostly a fact of life, and OSes are not different. For example, the Linux kernel comprises of the order of 20 million lines of source code (20 MSLOC); we can estimate that it contains literally tens of thousands of bugs [Biggs et al., 2018]. This is obviously a huge attack surface! This idea is captured by saying that Linux has a large   trusted computing base   (TCB), which is defined as the subset of the overall system that must be trusted to operate correctly for the system to be secure. The idea behind a microkernel design is to drastically reduce the TCB and thus the attack surface. As schematically shown at the right of Figure 2.1, the kernel, i.e. the part of the system executing in privileged mode, is much smaller. In a well-designed microkernel, such as seL4, it is of the order of ten thousand lines of source code (10 kSLOC). This is literally three orders of magnitude smaller than the Linux kernel, and the attack surface shrinks accordingly (maybe more, as the density of bugs probably grows more than linearly with code size). Obviously, it is not possible to provide the same functionality, in terms of OS services, in such a small code base. In fact, the microkernel provides almost no services: it is just a thin wrapper around hardware, just enough to securely multiplex hardware resources. What the microkernel mostly provides is isolation, sandboxes in which programs can execute without interference from other programs. And, critically, it provides a   protected procedure call   (PPC) mechanism, which is a form of inter-process communication (IPC). For historical reasons the term IPC lives on, but I recommend avoiding it as it leads to misconceptions that result in poor designs.  For a deeper explanation of what seL4 IPC is and is not, I recommend reading my blog How to (and how not to) use seL4 IPC. The PPC mechanism allows one program to securely call a function in a different program, where the microkernel transports function inputs and outputs between the programs and, importantly, enforces interfaces: the â€œremoteâ€ (contained in a different sandbox) function can only be called at an exported entrypoint, and only by explicitly authorised clients (who have been given the appropriate capability, see Chapter 4). The microkernel system uses this approach to provide the services the monolithic OS implements in the kernel. In the microkernel world, these services are just programs, no different from applications, that run in their own sandboxes, and provide a PPC interface for applications to call. Should a server be compromised, that compromise is confined to the server, its sandbox protects the rest of the system. This is in stark contrast to the monolithic case, where a compromise of an OS service compromises the complete system. This effect can be quantified: Our recent study shows that of the known Linux compromises classified as   critical , i.e. most severe, 29% would be fully eliminated by a microkernel design, and another 55% would be mitigated enough to no longer qualify as critical [Biggs et al., 2018]. 6\n\n93   94   95   96   97   98   99   00   01   02   03   04   05   06   07   08   09   10   11   12   13 L3   â†’   L4   â€œXâ€   Hazelnut   Pistachio L4/Alpha L4/MIPS seL4 OKL4 Microkernel OKL4 Microvisor Codezero P4   â†’   PikeOS Fiasco   Fiasco.OC L4-embedded NOVA API Inheritance Code Inheritance GMD/IBM/TU Karlsruhe   UNSW/NICTA/Data61 TU Dresden   Commercial Clone OK Labs Creators: Figure 2.2: L4 microkernel family tree.  2.2   seL4 Is a microkernel, not an OS  seL4 is a microkernel, and designed for generality while minimising the TCB. It is a member of the L4 microkernel family, which goes back to the mid-â€™90s; Figure 2.2 shows seL4â€™s provenance. It was developed by our group at UNSW/NICTA, these days known as Trustworthy Systems (TS). At the time we had 15 years of experience in developing high-performance microkernels, and a track-record of real-world deployments: Our   OKL4 Microkernel   shipped on billions of Qualcomm cellular modem chips, and our   L4-embedded   kernel from the mid-Noughties runs on the secure enclave of all recent iOS devices (iPhones etc). Being a microkernel, seL4 contains none of the usual OS services; such services are provided by programs running in user mode. Besides the great advantages elaborated above, there are downsides to the microkernel design: These components must come from somewhere. Some can be ported from open-source OSes, such as FreeBSD or Linux, or they can be written from scratch. But in any case, this is significant work. To scale up we need the help of the community, and the seL4 Foundation is the key mechanism for enabling the community to cooperate and develop or port such services for seL4-based systems. The most important ones are device drivers, network protocol stacks, and file systems. We have a fair number of these, but much more is needed. Even compared to other microkernels, seL4â€™s API is very low-level, with only the minimum abstraction as required to securely manage the hardware. As such, building systems on seL4 is particularly difficult. A good way to look at this is to think of seL4 as the â€œassembly language of operating systemsâ€: very primitive. No-one in their right mind would write a complex system, say a database or web server, in assembly language, we use higher-level programming languages for that. These simplify the task by providing higher-level constructs and abstractions, at the 7\n\nexpense of losing some of the power the hardware provides. In almost all cases, that is a winning trade-off. Similarly, one should not try to build a complex system directly on seL4, but should use a higher-level framework that provides more appropriate abstractions, at the expense of introducing some policy and taming the power of seL4. Specifically, such a framework should allows developers to focus on the code that implements the services, ignore hardware complexities, and automate much of the system integration. There are presently three main component frameworks for seL4, all open source: The Microkit, CAmkES and Genode. The seL4 Microkit reduces the complex seL4 API to literally a handful of simple abstractions, designed around components called   protection domains . It also provides a   software development kit   (SDK) that makes it easy to build and integrate separately compiled modules with a kernel binary to produce a bootable image. The Microkitâ€™s simplicity is achieved by requiring the system architecture to be static, meaning the set of modules and their communications is defined at system configuration time â€“ a model that seems to match the requirements of most (if not all) embedded systems, including complex cyberphysical systems such as cars and aircraft. CAmkES, a predecessor of the Microkit, is also a component framework for a statically-architected system. Its abstractions are higher level yet more complex than those of the Microkit. It also lacks an SDK, resulting in a much more painful build process that forces use of the kernelâ€™s complex build system. CAmkES also introduces significant overheads. Genode is in many ways a more powerful and general framework, that supports multiple microkernels and already comes with a wealth of services and device drivers, especially for x86 platforms, and does not enforce a static system architecture. It is arguably more convenient to work with than CAmkES, and is certainly the way to get a complex system up quickly. However, Genode has drawbacks: 1. As it supports multiple microkernels, not all as powerful as seL4, Genode is based on the least common denominator. In particular, it cannot use all of seL4â€™s security and safety features. 2. It has no assurance story. More on this in Section 3.2. For these reasons, the Microkit is the recommended framework for building systems on seL4, at least as long as the static architecture works. Section 3.2 presents it in more detail.  2.3   seL4 is also a hypervisor  seL4 is a microkernel, but it is also a hypervisor: It is possible to run virtual machines on seL4, and inside the virtual machine (VM) a mainstream OS, such as Linux. This enables an alternative way of provisioning system services, by having a Linux VM provide them. Such a setup is shown in Figure 2.3, which shows how some services are borrowed from multiple Linux instances running as guest OSes in separate VMs. In this example, we provide two system services: networking and storage. Networking is provided by a native protocol stack running directly on seL4, lwIP or PicoTCP are 8",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "b78601fed6f74ae5",
    "title": {
      "en": "A USB Interface to the \"Mother of All Demos\" Keyset",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.righto.com/2025/03/mother-of-all-demos-usb-keyset-interface.html",
    "score": 258,
    "by": "zdw",
    "time": 1742743886,
    "content": "Ken Shirriff's blog\n\nComputer history, restoring vintage computers, IC reverse engineering, and whatever\n\nA USB interface to the \"Mother of All Demos\" keyset\n\nIn the early 1960s, Douglas Engelbart started investigating how computers could augment human intelligence:\n\"If, in your office, you as an intellectual worker\nwere supplied with a computer display backed up by a computer that was alive for you all day and was instantly responsive to every\naction you had, how much value could you derive from that?\"\nEngelbart developed many features of modern computing that we now take for granted: the mouse,1 hypertext, shared documents, windows,\nand a graphical user interface.\nAt the 1968 Joint Computer Conference, Engelbart demonstrated these innovations in a groundbreaking presentation, now known as\n\"The Mother of All Demos.\"\n\nThe keyset with my prototype USB interface.\nEngelbart's demo also featured an input device known as the keyset, but unlike his other innovations, the keyset failed to catch on.\nThe 5-finger keyset lets you type without moving your hand, entering characters by pressing multiple keys simultaneously as a chord.\nChristina Englebart, his daughter, loaned one of Engelbart's keysets to me.\nI constructed an interface to connect the keyset to USB, so that it can be used with a modern computer.\nThe video below shows me typing with the keyset, using the mouse buttons to select upper case and special characters.2\n\nI wrote this blog post to describe my USB keyset interface.\nAlong the way, however, I got sidetracked by the history of The Mother of All Demos and how it obtained that name.\nIt turns out that Engelbart's demo isn't the first demo to be called \"The Mother of All Demos\".\nEngelbart and The Mother of All Demos\n\nEngelbart's work has its roots in\nVannevar Bush's 1945 visionary essay, \"As We May Think.\"\nBush envisioned thinking machines, along with the \"memex\", a compact machine holding a library of collective knowledge with hypertext-style links: \"The Encyclopedia Britannica could be reduced to the volume of a matchbox.\"\nThe memex could search out information based on associative search, building up a hypertext-like trail of connections.\nIn the early 1960s, Engelbart was inspired by Bush's essay and set out\nto develop means to augment human intellect: \"increasing the capability of a man to approach a complex problem situation, to gain comprehension to suit his particular needs, and to derive solutions to problems.\"3\nEngelbart founded the Augmentation Research Center at the Stanford Research Institute (now SRI), where\nhe and his team created a system called NLS (oN-Line System).\nEngelbart editing a hierarchical shopping list.\nIn 1968, Engelbart demonstrated NLS to a crowd of two thousand people\nat the Fall Joint Computer Conference.\nEngelbart gave the demo from the stage, wearing a crisp shirt and tie and a headset microphone.\nEngelbart created hierarchical documents, such as the shopping list above, and moved around them with hyperlinks.\nHe demonstrated how text could be created, moved, and edited with the keyset and mouse.\nOther documents included graphics, crude line drawing by today's standards but cutting-edge for the time.\nThe computer's output was projected onto a giant screen, along with video of Engelbart.\nEngelbart using the keyset to edit text. Note that the display doesn't support lowercase text; instead, uppercase is indicated by a line above the character. Adapted from The Mother of All Demos.\nEngelbart sat at a specially-designed Herman Miller desk6 that held the\nkeyset, keyboard, and mouse, shown above.\nWhile Engelbart was on stage in San Francisco,\nthe SDS 9404 computer that ran the NLS software was 30 miles to the south in Menlo Park.5\nTo the modern eye, the demo resembles a PowerPoint presentation over Zoom, as\nEngelbart collaborated with\nJeff Rulifson and Bill Paxton, miles away in Menlo Park.\n(Just like a modern Zoom call, the remote connection started with \"We're not hearing you. How about now?\")\nJeff Rulifson browsed the NLS code, jumping between code files with hyperlinks and expanding subroutines by clicking on them.\nNLS was written in custom high-level languages, which they developed\nwith a \"compiler compiler\" called TREE-META.\nThe NLS system held interactive documentation as well as tracking bugs and changes.\nBill Paxton interactively drew a diagram and then demonstrated how NLS could be used as a database, retrieving information by searching on keywords.\n(Although Engelbart was stressed by the live demo, Paxton told me that he was \"too young and inexperienced to be concerned.\")\nBill Paxton, in Menlo Park, communicating with the conference in San Francisco.\nBill English, an electrical engineer, not only built the first mouse for Engelbart but was also the hardware mastermind behind the demo.\nIn San Francisco, the screen images were projected on a 20-foot screen by a Volkswagen-sized\nEidophor projector, bouncing light off a modulated oil film.\nNumerous cameras, video switchers and mixers created the video image.\nTwo leased microwave links and half a dozen antennas connected SRI in Menlo Park to the demo in San Francisco.\nHigh-speed modems send the mouse, keyset, and keyboard signals from the demo back to SRI.\nBill English spent months assembling the hardware and network for the demo and then managed the demo behind the scenes, assisted by a team of about 17 people.\nAnother participant was the famed counterculturist Stewart Brand, known for the Whole Earth Catalog\nand the WELL, one of the oldest online virtual communities.\nBrand advised Engelbart on the presentation, as well as running a camera. He'd often point the camera at a monitor to generate swirling psychedelic\nfeedback patterns, reminiscent of the LSD that he and Engelbart had experimented with.\nThe demo received press attention such as\na San Francisco Chronicle article titled \"Fantastic World of Tomorrow's Computer\".\nIt stated, \"The most fantastic glimpse into the computer future was taking place in a windowless room on the third floor of the Civic Auditorium\"\nwhere Engelbart \"made a computer in Menlo Park do secretarial work for him that ten efficient secretaries couldn't do in twice the time.\"\nHis goal: \"We hope to help man do better what he doesâ€”perhaps by as much as 50 per cent.\"\nHowever, the demo received little attention in the following decades.7\nEngelbart continued his work at SRI for almost a decade, but as Engelbart commented with frustration,\nâ€œThere was a slightly less than universal perception of our value at SRIâ€.8\nIn 1977, SRI sold the Augmentation Research Center to Tymshare, a time-sharing computing company.\n(Timesharing was the cloud computing of the 1970s and 1980s,\nwhere companies would use time on a centralized computer.)\nAt Tymshare, Engelbart's system was renamed AUGMENT and marketed as an office automation service, but Engelbart himself was sidelined from development,\na situation that he described as\nsitting in a corner and becoming invisible.\nMeanwhile, Bill English and some other SRI researchers9 migrated four miles south to Xerox PARC and worked on the Xerox Alto computer.\nThe Xerox Alto incorporated many ideas from the Augmentation Research Center including the graphical user interface, the mouse, and the keyset.\nThe Alto's keyset\nwas almost identical to the Engelbart keyset, as can be seen in the photo below.\nThe Alto's keyset was most popular for the networked 3D shooter game \"Maze War\", with the clicking of keysets echoing through the hallways of Xerox PARC.\nA Xerox Alto with a keyset on the left.\nXerox famously failed to commercialize the ideas from the Xerox Alto, but Steve Jobs recognized the importance of interactivity, the graphical user interface, and the mouse\nwhen he visited Xerox PARC in 1979.\nSteve Jobs provided the Apple Lisa and Macintosh ended up with a graphical user interface and the mouse (streamlined to one button instead of three), but he left the keyset behind.10\nWhen McDonnell Douglas acquired Tymshare in 1984, Engelbart and his softwareâ€”now called Augmentâ€”had a new home.11\nIn 1987, McDonnell Douglas released a text editor and outline processor for the IBM PC called\nMiniBASE,\none of the few PC applications that supported a keyset.\nThe functionality of MiniBASE was almost identical to Engelbart's 1968 demo, but in 1987, MiniBASE\nwas competing against GUI-based word processors such as MacWrite and Microsoft Word, so MiniBASE had little impact.\nEngelbart left McDonnell Douglas in 1988, forming a research foundation called the Bootstrap Institute to continue his research independently.\nThe name: \"The Mother of All Demos\"\nThe name \"The Mother of All Demos\" has its roots in the Gulf War.\nIn August 1990, Iraq invaded Kuwait, leading to war between Iraq and a coalition of the United States and 41 other countries.\nDuring the months of buildup prior to active conflict, Iraq's leader, Saddam Hussein,\nexhorted the Iraqi people to prepare for \"the mother of all battles\",12 a phrase that caught the attention of the media.\nThe battle didn't proceed as Hussein hoped: during exactly 100 hours of ground combat, the US-led coalition liberated Kuwait, pushed into Iraq, crushed the Iraqi forces,\nand declared a ceasefire.13\nHussein's mother of all battles became the mother of all surrenders.\nThe phrase \"mother of all ...\" became the 1990s equivalent of a meme, used as a slightly-ironic superlative.\nIt was applied to everything\nfrom The Mother of All Traffic Jams to The Mother of All Windows Books, from The Mother of All Butter Cookies to Apple calling mobile devices\nThe Mother of All Markets.14\nIn 1991, this superlative was applied to a computer demo, but it wasn't Engelbart's demo.\nAndy Grove, Intel's president, gave a keynote speech at Comdex 1991 entitled The Second Decade: Computer-Supported Collaboration,\na live demonstration of his vision for PC-based video conferencing and wireless communication in the PC's second decade.\nThis complex hour-long demo required almost six months to prepare, with 15 companies collaborating.\nIntel called this demo \"The Mother of All Demos\", a name repeated in the New York Times, San Francisco Chronicle, Fortune, and PC Week.15\nAndy Grove's demo was a hit, with over 20,000 people requesting a video tape, but the demo was soon forgotten.\nOn the eve of Comdex, the New York Times wrote about Intel's \"Mother of All Demos\". Oct 21, 1991, D1-D2.\nIn 1994, Wired writer Steven Levy wrote Insanely Great: The Life and Times of Macintosh, the Computer that Changed Everything.8\nIn the second chapter of this comprehensive book, Levy explained how Vannevar Bush and Doug Engelbart \"sparked a chain reaction\" that led to the Macintosh.\nThe chapter described Engelbart's 1968 demo in detail including a throwaway line saying, \"It was the mother of all demos.\"16\nBased on my research, I think this is the source of the name \"The Mother of All Demos\" for Engelbart's demo.\nBy the end of the century, multiple publications echoed Levy's catchy phrase.\nIn February 1999, the San Jose Mercury News had a special article on Engelbart, saying that the demonstration was \"still called 'the mother of all demos'\", a description echoed by\nthe industry publication Computerworld.17\nThe book Nerds: A Brief History of the Internet stated that the demo \"has entered legend as 'the mother of all demos'\".\nBy this point, Engelbart's fame for the \"mother of all demos\" was cemented and the phrase became near-obligatory when writing about him.\nThe classic Silicon Valley history Fire in the Valley (1984), for example,\ndidn't even mention Engelbart but in the second edition (2000),\n\"The Mother of All Demos\" had its own chapter.\nInterfacing the keyset to USB\nGetting back to the keyset interface,\nthe keyset consists of five microswitches, triggered by the five levers.\nThe switches are wired to a standard DB-25 connector.\nI used a Teensy 3.6 microcontroller board for the interface, since this board can act both as a USB device\nand as a USB host.\nAs a USB device, the Teensy can emulate a standard USB keyboard.\nAs a USB host, the Teensy can receive input from a standard USB mouse.\nConnecting the keyset to the Teensy is (almost) straightforward, wiring the switches to five data inputs on the Teensy and the common line connected to ground.\nThe Teensy's input lines can be configured with pullup resistors inside the microcontroller. The result is that a data line shows 1 by default and\n0 when the corresponding key is pressed.\nOne complication is that the keyset apparently has a 1.5 kÎ© between the leftmost button and ground, maybe to indicate that the device is plugged in.\nThis resistor caused that line to always appear low to the Teensy.\nTo counteract this and allow the Teensy to read the pin, I connected a 1 kÎ© pullup resistor to that one line.\nThe interface code\nReading the keyset and sending characters over USB is mostly straightforward, but there are a few complications.\nFirst, it's unlikely that the user will press multiple keyset buttons at exactly the same time. Moreover, the button contacts may bounce.\nTo deal with this, I wait until the buttons have a stable value for 100 ms (a semi-arbitrary delay) before sending a key over USB.\nThe second complication is that with five keys, the keyset only supports 32 characters. To obtain upper case, numbers, special characters, and control\ncharacters, the keyset is designed to be used in conjunction with mouse buttons.\nThus, the interface needs to act as a USB host, so I can plug in a USB mouse to the interface.\nIf I want the mouse to be usable as a mouse, not just buttons in conjunction with the keyset, the interface mus forward mouse events over USB.\nBut it's not that easy, since mouse clicks in conjunction with the keyset shouldn't be forwarded. Otherwise, unwanted clicks will happen while\nusing the keyset.\nTo emulate a keyboard, the code uses the Keyboard library. This library provides\nan API to send characters to the destination computer.\nInconveniently, the simplest method, print(), supports only regular characters, not special characters like ENTER or BACKSPACE. For those, I needed to\nuse the lower-level press() and release() methods.\nTo read the mouse buttons,\nthe code uses the USBHost_t36 library, the Teensy version of the USB Host library.\nFinally, to pass mouse motion through to the destination computer, I use the Mouse library.\nConclusions\nEngelbart claimed  that learning a keyset wasn't\ndifficultâ€”a six-year-old kid could learn it in less than a weekâ€”but I'm not willing to invest much time into learning it. In my brief use of the keyset, I found it very difficult to use physically.\nPressing four keys at once is difficult, with the worst being all fingers except the ring finger. Combining this with a mouse button or two at the same time\ngave me the feeling that I was sight-reading a difficult piano piece.\nMaybe it becomes easier with use, but I noticed that Alto programs tended to treat the keyset as function keys, rather than a mechanism for typing with chords.18\nDavid Liddle of Xerox PARC said, \"We found that [the keyset] was tending to slow people down, once you got away from really hot [stuff] system programmers.\nIt wasn't quite so good if you were giving it to other engineers, let alone clerical people and so on.\"\nIf anyone else has a keyset that they want to connect via USB (unlikely as it may be), my code is on\ngithub.19  Thanks to Christina Engelbart for loaning me the keyset. Thanks to Bill Paxton for answering my questions.\nFollow me on Bluesky (@righto.com) or RSS for updates.\nFootnotes and references\n\nEngelbart's use of the mouse wasn't arbitrary, but based on research.\nIn 1966, shortly after inventing the mouse, Engelbart carried out a\nNASA-sponsored study\nthat evaluated six input devices: two types of joysticks, a Graphacon positioner, the mouse,\na light pen, and a control operated by the knees (leaving the hands free).\nThe mouse, knee control, and light pen performed best, with users finding the mouse satisfying to use. Although inexperienced subjects had some trouble with the mouse, experienced subjects considered\nit the best device.\nA joystick, Graphacon, mouse, knee control, and light pen were examined as input devices. Photos from the study.\nâ†©\n\nThe information sheet below from the Augmentation Research Center shows what keyset chords correspond to each character.\nI used this encoding for my interface software.\nEach column corresponds to a different combination of mouse buttons.\nThe information sheet for the keyset specifies how to obtain each character.\nThe special characters above are <CD> (Command Delete, i.e. cancel a partially-entered command), <BC> (Backspace Character), <OK> (confirm command), <BW>(Backspace Word), <RC> (Replace Character), <ESC> (which does filename completion).\nNLS and the Augment software have the concept of a viewspec, a view specification that controls the\nview of a file.\nFor instance, viewspecs can expand or collapse an outline to show more or less detail, filter the content, or show authorship of sections.\nThe keyset can select viewspecs, as shown below.\nBack of the keyset information sheet.\nViewsets are explained in more detail in The Mother of All Demos.\nFor my keyset interface, I ignored viewspecs since I don't have software to use these inputs, but\nit would be easy to modify the code to output the desired viewspec characters.\nâ†©\n\nSee Augmenting Human Intellect: A Conceptual Framework, Engelbart's 1962 report.â†©\n\nEngelbart used an SDS 940 computer running the Berkeley Timesharing System.\nThe computer had 64K words of core memory, with 4.5 MB of drum storage for swapping and 96 MB of disk storage for files.\nFor displays, the computer drove twelve 5\" high-resolution CRTs, but these weren't viewed directly.\nInstead, each CRT had a video camera pointed at it and the video was redisplayed on a larger display in a work station in each office.\nThe SDS 940 was a large 24-bit scientific computer, built by Scientific Data Systems.\nAlthough SDS built the first integrated-circuit-based commercial computer in 1965 (the SDS 92),\nthe SDS 940 was a transistorized system.\nIt consisted of multiple refrigerator-sized cabinets, as shown below. Since each memory cabinet held 16K words and the computer at SRI had 64K,\nSRI's computer had two additional cabinets of memory.\nFront view of an SDS 940 computer. From the Theory of Operation manual.\nIn the late 1960s, Xerox wanted to get into the computer industry, so Xerox\nbought Scientific Data Systems in 1969 for $900 million (about $8 billion in current dollars).\nThe acquisition was a disaster. After steadily losing money, Xerox decided to exit the mainframe computer business in 1975.\nXerox's CEO summed up the purchase: \"With hindsight, we would not have done the same thing.\"â†©\n\nThe Mother of All Demos is on YouTube,\nas well as a five-minute summary for the impatient.â†©\n\nThe desk for the keyset and mouse was designed by Herman Miller, the office furniture company.\nHerman Miller worked with SRI to design the\ndesks, chairs, and office walls as part of their plans for the office of the future.\nHerman Miller invented the cubicle office in 1964, creating a modern replacement for the commonly used open office arrangement.â†©\n\nEngelbart's demo is famous now, but for many years it was ignored.\nFor instance, Electronic Design had a long\narticle\non Engelbart's work in 1969 (putting the system on the cover), but there was no mention of the demo.\nEngelbart's system was featured on the cover of Electronic Design. Feb 1, 1969. (slightly retouched)\nBut by the 1980s, the Engelbart demo started getting attention.\nThe 1986 documentary Silicon Valley Boomtown had a long\nsection on Engelbart's work and the demo. By 1988, the New York Times was referring to the demo as legendary.â†©\n\nLevy had written about Engelbart a decade earlier, in the May 1984 issue of the magazine Popular Computing.\nThe article focused on the mouse, recently available to the public through the Apple Lisa and the IBM PC (as an option).\nThe big issue at the time was how many buttons a mouse should have: three like Engelbart's mouse, the one button that Apple used, or two buttons\nas Bill Gates preferred.\nBut Engelbart's larger vision also came through in Levy's interview along with his frustration that most of his research had been ignored,\novershadowed by the mouse.\nNotably, there was no mention of Engelbart's 1968 demo in the article.â†©â†©\n\nThe SRI researchers who moved to Xerox include Bill English, Charles Irby, Jeff Rulifson, Bill Duval, and Bill Paxton (details).â†©\n\nIn 2023, Xerox donated the entire Xerox PARC research center to SRI. The research center remained in Palo Alto but became part of SRI.\nIn a sense, this closed the circle, since many of the people and ideas from SRI had gone to PARC in the 1970s.\nHowever, both PARC and SRI had changed radically since the 1970s, with the cutting edge of computer research moving elsewhere.â†©\n\nFor a detailed discussion of the Augment system, see Tymshare's Augment: Heralding a New Era, Oct 1978.\nAugment provided a \"broad range of information handling capability\" that was not available elsewhere.\nUnlike other word processing systems, Augment was targeted at the professional, not clerical workers,\npeople who were \"eager to explore the open-ended possibilities\" of the interactive process.\nThe main complaints about Augment were its price and that it was not easy to use. Accessing Engelbart's NLS system over ARPANET cost an eye-watering $48,000 a year (over $300,000 a year in current dollars).\nTymshare's Augment service was cheaper (about $80 an hour in current dollars), but still much more expensive than a standard word processing\nservice.\nOverall, the article found that Augment users were delighted with the system: \"It is stimulating to belong to the electronic intelligentsia.\"\nUsers found it to be \"a way of lifeâ€”an absorbing, enriching experience\".â†©\n\nWilliam Safire provided background in the New York Times, explaining\nthat \"the mother of all battles\"\noriginally referred to the battle of Qadisiya in A.D. 636, and Saddam Hussein was referencing that ancient battle.\nA translator responded, however,\nthat the Arabic expression would be better translated as \"the great battle\" than \"the mother of all battles.\"â†©\n\nThe end of the Gulf War left Saddam Hussein in control of Iraq and left thousands of US troops in Saudi Arabia.\nThese factors would turn out to be catastrophic in the following years.â†©\n\nAt the Mobile '92 conference, Apple's CEO, John Sculley, said personal communicators could be \"the mother of all markets,\"\nwhile Andy Grove of Intel said that the idea of a wireless personal communicator in every pocket is \"a pipe dream driven by greed\"\n(link).\nIn hindsight, Sculley was completely right and Grove was completely wrong.â†©\n\nSome references to Intel's \"Mother of all demos\" are\nComputer Industry Gathers Amid Chaos, New York Times, Oct 21, 1991\nand \"Intel's High-Tech Vision of the Future: Chipmaker proposes using computers to dramatically improve productivity\", San Francisco Chronicle, Oct 21, 1991, p24.\nThe title of an article in Microprocessor Report, \"Intel Declares Victory in the Mother of All Demos\" (Nov. 20, 1991), alluded to the recently-ended war.\nFortune wrote about Intel's demo in the  Feb 17, 1997 issue.\nA longer description of Intel's demo is in the book Strategy is Destiny.â†©\n\nSeveral sources claim that Andy van Dam was the first to call Engelbart's demo \"The Mother of All Demos.\" Although van Dam attended the 1968 demo,\nI couldn't find any evidence that he coined the phrase.\nJohn Markoff, a technology journalist for The New York Times, wrote a book What the Dormouse Said: How the Sixties Counterculture Shaped the Personal Computer Industry.\nIn this book, Markoff wrote about Engelbart's demo, saying \"Years later, his talk remained 'the mother of all demos' in the words of Andries van Dam, a Brown University computer scientist.\"\nAs far as I can tell, van Dam used the phrase but only after it had already been popularized by Levy.â†©\n\nIt's curious to write that the demonstration was still called the \"mother of all demos\" when the phrase was just a few years old.â†©\n\nThe photo below shows a keyset from the Xerox Alto.\nThe five keys are labeled with separate functionsâ€”Copy, Undelete, Move, Draw, and Fineâ€”\nfor use with ALE,\na program for IC design.\nALE supported\nkeyset chording\nin combination with the mouse.\nâ†©Keyset from a Xerox Alto, courtesy of Digibarn.\n\nAfter I implemented this interface, I came across a project that constructed a 3D-printed chording keyset, also using a Teensy for the USB interface. You can find that project here.â†©\n\nEmail ThisBlogThis!Share to XShare to FacebookShare to Pinterest\n\nLabels:\nalto,\nelectronics,\nreverse-engineering\n\n10 comments:\n\nMartin Haeberli\nsaid...\n\nKen - cool project ! I have a (few?) Xerox Optical Mice that I'd like to interface to USB as well - do you know of any off-the shelf solutions? Or maybe this could be adapted? Best, Martin\n\nMarch 23, 2025 at 8:54â€¯AM\n\nKen Shirriff\nsaid...\n\nHi Martin! I don't know of any off-the-shelf solutions, but it would probably be easy to modify my interface. (Famous last words :)\n\nMarch 23, 2025 at 9:29â€¯AM\n\nAnonymous\nsaid...\n\nCharles Moore, creator of the FORTH programming language, famously used a 5-button chording puck for programming in the 1970s.\n\nMarch 23, 2025 at 10:56â€¯AM\n\n<img src=\"//blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglgI5BrNqTAwhKW-P2yd3IezmYF6xIss6PS08E5EYw5JA1g37XT-ua8QrbPkhen-laj2FYOc6-DP4U5H3YyboN9QiKE2XHZuILzMAxl9-3DWkxm3I78bme2aCCItOyU-w/s45-c/Still+of+Teaching+video+headshot.png\" width=\"35\" height=\"35\" class=\"photo\" alt=\"\">\nGeoff Fellows\nsaid...\n\nJust a minor correction 1K ohm is 1024 ohms. Please use S.I. prefixes in this case lowercase k for x1000. See https://www.nist.gov/pml/owm/metric-si-prefixes\n\nMarch 23, 2025 at 1:27â€¯PM\n\nPouya Kary\nsaid...\n\nHi! Your work is amazing! Recently I tried to create the keyset with a midi controller and so since the community of those who care about keyset is so small, I wished to show it to you. https://github.com/pouyakary/nls-keyset-with-midi-keys\n\nMarch 24, 2025 at 12:19â€¯AM\n\nPouya Kary\nsaid...\n\nWow I didn't know herman miller was involved, how cool\n\nMarch 24, 2025 at 12:21â€¯AM\n\nAnonymous\nsaid...\n\nThe existence of Morse code should have told Engelbart that using straight ASCII encoding might not be the optimal solution.Also, why not embrace the fact that you can't press (and release) keys at the same time? You get 24 different codes that way, pressing and releasing 2 of 4 keys in some kind of order. Add single-key presses for 28. Add the thumb key (why only one?) and you've got more possible codes than you'll be able to remember.\n\nMarch 24, 2025 at 2:09â€¯AM\n\nAnonymous\nsaid...\n\nUppercase K would be written with a line on top of it, so it was indeed lowercase k. ;)Also, isn't it really \"Ki\" to indicate 1024, with K representing Kelvin?\n\nMarch 24, 2025 at 2:43â€¯AM\n\nAnonymous\nsaid...\n\nI wonder if you could have gone with mouse scroll wheel for upper/lower case?Have a \"natural gravity\" to middle and when you push up, you get all uppercase, and push back down and you get all lowercase. A quick, short scroll up only uppercases the next letter.Not sure of the best way to provide visual feedback (status tray icon? just look at the letters coming out), but I think it can be tuned to work very nicely.\n\nMarch 24, 2025 at 2:47â€¯AM\n\nAnonymous\nsaid...\n\nI know that the Robot in a Can team has a clean esp and laser cut wood keyset build. I don'tbknow the link to the files and they're currently out of stock, but I'll reach out and see if they can restock and republish the open source design.\n\nMarch 24, 2025 at 5:13â€¯AM\n\nPost a Comment\n\nOlder Post\n\nHome\n\n      @import url('https://fonts.googleapis.com/css?family=Montserrat:300,400,500,700');\n      .form-preview {\n      display: flex;\n      flex-direction: column;\n      justify-content: center;\n      margin-top: 30px;\n      padding: clamp(17px, 5%, 40px) clamp(17px, 7%, 50px);\n      max-width: 350px;\n      min-height: 200px;\n      border-radius: 6px;\n      box-shadow: 0 5px 25px rgba(34, 60, 47, 0.25);\n      }\n      .form-preview,\n      .form-preview *{\n        box-sizing: border-box;\n      }\n      .form-preview .preview-heading {\n      width: 100%;\n      }\n      .form-preview .preview-heading h5{\n        margin-top: 0;\n        margin-bottom: 0;\n      }\n      .form-preview .preview-input-field {\n      margin-top: 20px;\n      width: 100%;\n      }\n      .form-preview .preview-input-field input {\n      width: 100%;\n      height: 40px;\n      border-radius: 6px;\n      border: 2px solid #e9e8e8;\n      background-color: #fff;\n      outline: none;\n      }\n      .form-preview .preview-input-field input::placeholder, .form-preview .preview-input-field input {\n      opacity: 0.5;\n      color: #000;\n      font-family: \"Montserrat\";\n      font-size: 14px;\n      font-weight: 500;\n      line-height: 20px;\n      text-align: center;\n      }\n      .form-preview .preview-submit-button {\n      margin-top: 10px;\n      width: 100%;\n      }\n      .form-preview .preview-submit-button button {\n      width: 100%;\n      height: 40px;\n      border: 0;\n      border-radius: 6px;\n      line-height: 0px;\n      }\n      .form-preview .preview-submit-button button:hover {\n      cursor: pointer;\n      }\n\n      Get new posts by email:  Subscribe\n\nAbout the site\n\nContact info and site index\n\nPopular Posts\n\nThe Pentium contains a complicated circuit to multiply by three\n\nA USB interface to the \"Mother of All Demos\" keyset\n\nInside a vintage aerospace navigation computer of uncertain purpose\n\nA Multi-Protocol Infrared Remote Library for the Arduino\n\nApple iPhone charger teardown: quality in a tiny expensive package\n\nA dozen USB chargers in the lab: Apple is very good, but not quite the best\n\nMining Bitcoin with pencil and paper: 0.67 hashes per day\n\nTeardown and exploration of Apple's Magsafe connector\n\nSearch This Blog\n\nLabels\n\n386\n\n6502\n\n8008\n\n8085\n\n8086\n\n8087\n\n8088\n\naerospace\n\nalto\n\nanalog\n\nApollo\n\napple\n\narc\n\narduino\n\narm\n\nbeaglebone\n\nbitcoin\n\nc#\n\ncadc\n\ncalculator\n\nchips\n\ncss\n\ndatapoint\n\ndx7\n\nelectronics\n\nf#\n\nfairchild\n\nfpga\n\nfractals\n\ngenome\n\nglobus\n\nhaskell\n\nHP\n\nhtml5\n\nibm\n\nibm1401\n\nibm360\n\nintel\n\nipv6\n\nir\n\njava\n\njavascript\n\nmath\n\nmicrocode\n\noscilloscope\n\nPentium\n\nphoto\n\npower supply\n\nrandom\n\nreverse-engineering\n\nsheevaplug\n\nsnark\n\nspace\n\nspanish\n\nsynth\n\nteardown\n\ntheory\n\nunicode\n\nZ-80\n\nBlog Archive\n\n        â–¼\n\n2025\n\n(7)\n\n        â–¼\n\nMarch\n\n(2)\n\nA USB interface to the \"Mother of All Demos\" keyset\nThe Pentium contains a complicated circuit to mult...\n\n        â–º\n\nFebruary\n\n(1)\n\n        â–º\n\nJanuary\n\n(4)\n\n        â–º\n\n2024\n\n(21)\n\n        â–º\n\nDecember\n\n(1)\n\n        â–º\n\nNovember\n\n(1)\n\n        â–º\n\nOctober\n\n(1)\n\n        â–º\n\nSeptember\n\n(3)\n\n        â–º\n\nAugust\n\n(2)\n\n        â–º\n\nJuly\n\n(2)\n\n        â–º\n\nJune\n\n(1)\n\n        â–º\n\nMay\n\n(1)\n\n        â–º\n\nApril\n\n(1)\n\n        â–º\n\nMarch\n\n(2)\n\n        â–º\n\nFebruary\n\n(3)\n\n        â–º\n\nJanuary\n\n(3)\n\n        â–º\n\n2023\n\n(35)\n\n        â–º\n\nDecember\n\n(4)\n\n        â–º\n\nNovember\n\n(2)\n\n        â–º\n\nOctober\n\n(3)\n\n        â–º\n\nSeptember\n\n(1)\n\n        â–º\n\nAugust\n\n(2)\n\n        â–º\n\nJuly\n\n(3)\n\n        â–º\n\nMay\n\n(1)\n\n        â–º\n\nApril\n\n(2)\n\n        â–º\n\nMarch\n\n(4)\n\n        â–º\n\nFebruary\n\n(5)\n\n        â–º\n\nJanuary\n\n(8)\n\n        â–º\n\n2022\n\n(18)\n\n        â–º\n\nNovember\n\n(3)\n\n        â–º\n\nAugust\n\n(1)\n\n        â–º\n\nJuly\n\n(1)\n\n        â–º\n\nJune\n\n(1)\n\n        â–º\n\nMay\n\n(1)\n\n        â–º\n\nApril\n\n(4)\n\n        â–º\n\nMarch\n\n(2)\n\n        â–º\n\nFebruary\n\n(3)\n\n        â–º\n\nJanuary\n\n(2)\n\n        â–º\n\n2021\n\n(26)\n\n        â–º\n\nDecember\n\n(4)\n\n        â–º\n\nNovember\n\n(2)\n\n        â–º\n\nSeptember\n\n(1)\n\n        â–º\n\nAugust\n\n(1)\n\n        â–º\n\nJuly\n\n(2)\n\n        â–º\n\nJune\n\n(2)\n\n        â–º\n\nMay\n\n(1)\n\n        â–º\n\nApril\n\n(2)\n\n        â–º\n\nMarch\n\n(4)\n\n        â–º\n\nFebruary\n\n(4)\n\n        â–º\n\nJanuary\n\n(3)\n\n        â–º\n\n2020\n\n(33)\n\n        â–º\n\nDecember\n\n(2)\n\n        â–º\n\nNovember\n\n(3)\n\n        â–º\n\nOctober\n\n(2)\n\n        â–º\n\nSeptember\n\n(4)\n\n        â–º\n\nAugust\n\n(5)\n\n        â–º\n\nJuly\n\n(2)\n\n        â–º\n\nJune\n\n(3)\n\n        â–º\n\nMay\n\n(4)\n\n        â–º\n\nApril\n\n(2)\n\n        â–º\n\nMarch\n\n(5)\n\n        â–º\n\nJanuary\n\n(1)\n\n        â–º\n\n2019\n\n(18)\n\n        â–º\n\nNovember\n\n(3)\n\n        â–º\n\nOctober\n\n(2)\n\n        â–º\n\nSeptember\n\n(3)\n\n        â–º\n\nAugust\n\n(1)\n\n        â–º\n\nJuly\n\n(4)\n\n        â–º\n\nApril\n\n(2)\n\n        â–º\n\nFebruary\n\n(1)\n\n        â–º\n\nJanuary\n\n(2)\n\n        â–º\n\n2018\n\n(17)\n\n        â–º\n\nDecember\n\n(1)\n\n        â–º\n\nSeptember\n\n(4)\n\n        â–º\n\nAugust\n\n(1)\n\n        â–º\n\nJune\n\n(1)\n\n        â–º\n\nMay\n\n(1)\n\n        â–º\n\nApril\n\n(1)\n\n        â–º\n\nMarch\n\n(3)\n\n        â–º\n\nFebruary\n\n(1)\n\n        â–º\n\nJanuary\n\n(4)\n\n        â–º\n\n2017\n\n(21)\n\n        â–º\n\nDecember\n\n(5)\n\n        â–º\n\nNovember\n\n(2)\n\n        â–º\n\nOctober\n\n(3)\n\n        â–º\n\nAugust\n\n(1)\n\n        â–º\n\nJuly\n\n(2)\n\n        â–º\n\nJune\n\n(2)\n\n        â–º\n\nApril\n\n(2)\n\n        â–º\n\nMarch\n\n(2)\n\n        â–º\n\nFebruary\n\n(1)\n\n        â–º\n\nJanuary\n\n(1)\n\n        â–º\n\n2016\n\n(34)\n\n        â–º\n\nDecember\n\n(2)\n\n        â–º\n\nOctober\n\n(5)\n\n        â–º\n\nSeptember\n\n(8)\n\n        â–º\n\nAugust\n\n(2)\n\n        â–º\n\nJuly\n\n(3)\n\n        â–º\n\nJune\n\n(4)\n\n        â–º\n\nMay\n\n(1)\n\n        â–º\n\nApril\n\n(1)\n\n        â–º\n\nMarch\n\n(1)\n\n        â–º\n\nFebruary\n\n(4)\n\n        â–º\n\nJanuary\n\n(3)\n\n        â–º\n\n2015\n\n(12)\n\n        â–º\n\nDecember\n\n(2)\n\n        â–º\n\nNovember\n\n(1)\n\n        â–º\n\nOctober\n\n(3)\n\n        â–º\n\nAugust\n\n(1)\n\n        â–º\n\nMay\n\n(2)\n\n        â–º\n\nMarch\n\n(2)\n\n        â–º\n\nFebruary\n\n(1)\n\n        â–º\n\n2014\n\n(13)\n\n        â–º\n\nDecember\n\n(1)\n\n        â–º\n\nOctober\n\n(1)\n\n        â–º\n\nSeptember\n\n(3)\n\n        â–º\n\nMay\n\n(2)\n\n        â–º\n\nMarch\n\n(1)\n\n        â–º\n\nFebruary\n\n(5)\n\n        â–º\n\n2013\n\n(24)\n\n        â–º\n\nNovember\n\n(2)\n\n        â–º\n\nSeptember\n\n(4)\n\n        â–º\n\nAugust\n\n(4)\n\n        â–º\n\nJuly\n\n(4)\n\n        â–º\n\nJune\n\n(2)\n\n        â–º\n\nApril\n\n(1)\n\n        â–º\n\nMarch\n\n(2)\n\n        â–º\n\nFebruary\n\n(2)\n\n        â–º\n\nJanuary\n\n(3)\n\n        â–º\n\n2012\n\n(10)\n\n        â–º\n\nDecember\n\n(1)\n\n        â–º\n\nNovember\n\n(5)\n\n        â–º\n\nOctober\n\n(1)\n\n        â–º\n\nMay\n\n(1)\n\n        â–º\n\nMarch\n\n(1)\n\n        â–º\n\nFebruary\n\n(1)\n\n        â–º\n\n2011\n\n(11)\n\n        â–º\n\nDecember\n\n(2)\n\n        â–º\n\nJuly\n\n(2)\n\n        â–º\n\nMay\n\n(2)\n\n        â–º\n\nApril\n\n(1)\n\n        â–º\n\nMarch\n\n(1)\n\n        â–º\n\nFebruary\n\n(3)\n\n        â–º\n\n2010\n\n(22)\n\n        â–º\n\nDecember\n\n(2)\n\n        â–º\n\nNovember\n\n(4)\n\n        â–º\n\nOctober\n\n(3)\n\n        â–º\n\nAugust\n\n(1)\n\n        â–º\n\nJune\n\n(1)\n\n        â–º\n\nMay\n\n(2)\n\n        â–º\n\nApril\n\n(3)\n\n        â–º\n\nMarch\n\n(4)\n\n        â–º\n\nJanuary\n\n(2)\n\n        â–º\n\n2009\n\n(22)\n\n        â–º\n\nDecember\n\n(2)\n\n        â–º\n\nNovember\n\n(5)\n\n        â–º\n\nSeptember\n\n(1)\n\n        â–º\n\nAugust\n\n(3)\n\n        â–º\n\nJuly\n\n(1)\n\n        â–º\n\nJune\n\n(3)\n\n        â–º\n\nApril\n\n(1)\n\n        â–º\n\nMarch\n\n(3)\n\n        â–º\n\nFebruary\n\n(2)\n\n        â–º\n\nJanuary\n\n(1)\n\n        â–º\n\n2008\n\n(27)\n\n        â–º\n\nJuly\n\n(3)\n\n        â–º\n\nJune\n\n(1)\n\n        â–º\n\nMay\n\n(3)\n\n        â–º\n\nApril\n\n(4)\n\n        â–º\n\nMarch\n\n(10)\n\n        â–º\n\nFebruary\n\n(6)\n\nPowered by Blogger.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "f955dda4b78101e9",
    "title": {
      "en": "Tesla sales drop 35% in San Diego County",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://fox5sandiego.com/news/business/tesla-sales-drop-35-in-san-diego-county/",
    "score": 50,
    "by": "doener",
    "time": 1742806811,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "ef3de44116e99591",
    "title": {
      "en": "Building and Publishing Games to Steam Directly from GitHub Actions",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://nullonerror.org/2025/03/23/building-and-publishing-games-to-steam-directly-from-gitHub-actions/",
    "score": 41,
    "by": "delduca",
    "time": 1742739200,
    "content": "Building and Publishing Games to Steam Directly from GitHub Actions\n\n  23 Mar 2025\n  I have been using GitHub Actions extensively both at work and in personal projects, as shown in this post What Iâ€™ve been automating with GitHub Actions, an automated life.\n\nIn my free time, Iâ€™m working on a 2D hide-and-seek game, and as you might expect, Iâ€™ve automated the entire release pipeline for publishing on Steam. After a few attempts, when it finally worked, it felt like magic: all I had to do was create a new tag, and within minutes, the Steam client was downloading the update.\n\nAs I mentioned earlier, I have a 2D engine that, while simple, is quite comprehensive. With each new tag, I compile it in parallel for Windows, macOS, Linux, and WebAssembly. Once compilation is complete, I create a release and publish it on GitHub. Releases Â· willtobyte/carimbo\n\nThis way\n\nname: Release\n\non:\n  push:\n    tags:\n      - \"v*.*.*\"\n\njobs:\n  release:\n    runs-on: ${{ matrix.config.os }}\n\n    permissions:\n      contents: write\n\n    strategy:\n      fail-fast: true\n\n      matrix:\n        config:\n          - name: macOS\n            os: macos-latest\n          - name: Ubuntu\n            os: ubuntu-latest\n          - name: WebAssembly\n            os: ubuntu-latest\n          - name: Windows\n            os: windows-latest\n\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Cache Dependencies\n        uses: actions/cache@v4\n        with:\n          path: |\n            ~/.conan2/p\n            C:/Users/runneradmin/.conan2/p\n          key: ${{ matrix.config.name }}-${{ hashFiles('**/conanfile.py') }}\n          restore-keys: |\n            ${{ matrix.config.name }}-\n\n      - name: Prepare Build Directory\n        run: mkdir build\n\n      - name: Setup Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: \"3.12\"\n\n      - name: Install Conan\n        run: pip install conan\n\n      - name: Detect Conan Profile\n        run: conan profile detect --force\n\n      - name: Set Conan Center\n        run: conan remote update conancenter --url https://center2.conan.io\n\n      - name: Detect WebAssembly Conan Profile\n        if: matrix.config.name == 'WebAssembly'\n        run: |\n          cat > ~/.conan2/profiles/webassembly <<EOF\n          include(default)\n\n          [settings]\n          arch=wasm\n          os=Emscripten\n\n          [tool_requires]\n          *: emsdk/3.1.73\n          EOF\n\n      - name: Install Windows Or macOS Dependencies\n        if: matrix.config.name == 'Windows' || matrix.config.name == 'macOS'\n        run: conan install . --output-folder=build --build=missing --settings compiler.cppstd=20 --settings build_type=Release\n\n      - name: Install Ubuntu Dependencies\n        if: matrix.config.name == 'Ubuntu'\n        run: conan install . --output-folder=build --build=missing --settings compiler.cppstd=20 --settings build_type=Release --conf \"tools.system.package_manager:mode=install\" --conf \"tools.system.package_manager:sudo=True\"\n\n      - name: Install WebAssembly Dependencies\n        if: matrix.config.name == 'WebAssembly'\n        run: conan install . --output-folder=build --build=missing --profile=webassembly --settings compiler.cppstd=20 --settings build_type=Release\n\n      - name: Configure\n        run: cmake .. -DCMAKE_TOOLCHAIN_FILE=\"conan_toolchain.cmake\" -DCMAKE_BUILD_TYPE=Release\n        working-directory: build\n\n      - name: Build\n        run: cmake --build . --parallel 8 --config Release --verbose\n        working-directory: build\n\n      - name: Create Artifacts Directory\n        run: mkdir artifacts\n\n      - name: Compress Artifacts\n        if: matrix.config.name == 'macOS'\n        working-directory: build\n        run: |\n          chmod -R a+rwx carimbo\n          tar -cpzvf macOS.tar.gz carimbo\n          mv macOS.tar.gz ../artifacts\n\n      - name: Compress Artifacts\n        if: matrix.config.name == 'Ubuntu'\n        working-directory: build\n        run: |\n          chmod +x carimbo\n          tar -czvf Ubuntu.tar.gz --mode='a+rwx' carimbo\n          mv Ubuntu.tar.gz ../artifacts\n\n      - name: Compress Artifacts\n        if: matrix.config.name == 'WebAssembly'\n        working-directory: build\n        run: |\n          zip -jr WebAssembly.zip carimbo.wasm carimbo.js\n          mv WebAssembly.zip ../artifacts\n\n      - name: Compress Artifacts\n        if: matrix.config.name == 'Windows'\n        working-directory: build\n        shell: powershell\n        run: |\n          Compress-Archive -LiteralPath 'Release/carimbo.exe' -DestinationPath \"../artifacts/Windows.zip\"\n\n      - name: Release\n        uses: softprops/action-gh-release@v1\n        with:\n          tag_name: ${{ github.event.inputs.tagName }}\n          prerelease: ${{ github.events.inputs.prerelease }}\n          files: artifacts/*\n\nPublishing on Steam is quite simple. First, you need a developer account with the correct documentation and fees paid.\n\nAfter that, youâ€™ll need to generate some secret keys as follows:\n\nsteamcmd +login <username> <password> +quit\n\nIf you donâ€™t have the steamcmd application installed, youâ€™ll need to install it using:\n\ncast install --cask steamcmd\n\nCopy the contents of the authentication file:\n\ncat ~/Library/Application\\ Support/Steam/config/config.vdf | base64 | pbcopy\n\nNote: You must have MFA enabled. After logging in, run the command below and copy the output into a GitHub Action variable named STEAM_CONFIG_VDF.\n\nAlso, create the variables STEAM_USERNAME with your username and STEAM_APP_ID with your gameâ€™s ID.\n\nAdditionally, the Action downloads the latest Carimbo release for Windows only (sorry Linux and macOS users, my time is limited). Ideally, I should pin the runtime version (the Carimbo version) using something like a runtime.txt file. Maybe Iâ€™ll implement this in the future, but for now, everything runs on the bleeding edge. :-)\n\non:\n  push:\n    tags:\n      - \"v*.*.*\"\n\njobs:\n  publish:\n    runs-on: ubuntu-latest\n    env:\n      CARIMBO_TAG: \"v1.0.65\"\n\n    permissions:\n      contents: write\n\n    steps:\n      - name: Clone the repository\n        uses: actions/checkout@v4\n\n      - name: Install 7zip\n        run: sudo apt install p7zip-full\n\n      - name: Create bundle\n        run: 7z a -xr'!.git/*' -xr'!.git' -xr'!.*' -t7z -m0=lzma -mx=6 -mfb=64 -md=32m -ms=on bundle.7z .\n\n      - name: Download Carimbo runtime\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n        run: gh release download ${{ env.CARIMBO_TAG }} --repo willtobyte/carimbo --pattern \"Windows.zip\"\n\n      - name: Extract Carimbo runtime\n        run: 7z x Windows.zip -o.\n\n      - name: Copy files\n        run: |\n          mkdir -p output\n          mv bundle.7z output/\n          mv carimbo.exe output/\n\n      - name: Upload build to Steam\n        uses: game-ci/steam-deploy@v3\n        with:\n          username: ${{ secrets.STEAM_USERNAME }}\n          configVdf: ${{ secrets.STEAM_CONFIG_VDF }}\n          appId: ${{ secrets.STEAM_APP_ID }}\n          rootPath: output\n          depot1Path: \".\"\n          releaseBranch: prerelease\n\nBoom! If everything is correct, your game should appear in your Steam client under the list of owned games.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "72c8f64ceeefa7f5",
    "title": {
      "en": "The Mystery of the Oldest Writing System Remained Unsolved Until 1856",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.smithsonianmag.com/history/mystery-worlds-oldest-writing-system-remained-unsolved-until-four-scholars-raced-decipher-it-180985954/",
    "score": 25,
    "by": "wyclif",
    "time": 1742729075,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "449a2c2ef5726266",
    "title": {
      "en": "Japanese scientists use stem cell treatment to restore movement in spinal injury",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://medicalxpress.com/news/2025-03-japanese-scientists-stem-cell-treatment.html",
    "score": 15,
    "by": "pseudolus",
    "time": 1742811919,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "5a6b64c7bd1b498a",
    "title": {
      "en": "Supply Chain Attacks on Linux Distributions â€“ Fedora Pagure",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://fenrisk.com/pagure",
    "score": 160,
    "by": "akyuu",
    "time": 1742414317,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "ac72d66503620b34",
    "title": {
      "en": "First Known Photographs of Living Specimens",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.inaturalist.org/projects/first-known-photographs-of-living-specimens",
    "score": 95,
    "by": "Morizero",
    "time": 1742501985,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "2159168a750d76f2",
    "title": {
      "en": "Autology: A Lisp with access to its own interpreter",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/Kimbsy/autology",
    "score": 11,
    "by": "simonpure",
    "time": 1742553436,
    "content": "Autology\nA hot take on Lisp metaprogramming, Autology is a Lisp with access\nto its own interpreter.\nWhat is it?\nAutology is a functional interpreted Lisp language, written in Clojure.\nThe Autology interpreter function is defined as a data structure which is available to the Autology program, it is bound to the variable *i*.\nWhenever an expression needs to be evaluated, Autology will retrieve the current value of *i* from the current lexically scoped execution environment and use it to evaluate the expression.\nBy binding new values of *i* we can modify the behaviour of the Autology language while our program is running.\nWhy have you done this?\nMainly for fun and to explore a neat idea.\nAutology is not a particularly useful language as-is, its very slow and resource intensive.\nMacros are the normal tool for Lisp languages to modify their syntax or behaviour, but these are normally only available at compile time. Autology allows dynamic rewriting of the language at runtime.\nEven other non-Lisp languages are able to define some form of Domain Specific Language to allow programmers to express problems more clearly, however these are generally quite restrictive in scope. Autology allows full ad-hoc re-writing of the syntax of the language, as well as defining new features or removing existing ones.\nWhat can you do with it?\nWell to start with you might like to add functions to the language. Autology doesn't have them by default, but we can rebind the *i* symbol to a data structure representing a new interpreter function, one that does have functions.\n(bind (;; grab a copy of the `:atl/eval-list` section of the\n       ;; interpreter which is responsible for evaluating lists.\n       original (get-marker *i* :atl/eval-list)\n\n       ;; define a case test+body for use when the list expression\n       ;; starts with our function special form, in this case `Î»`.\n       Î»-form (qu (Î» (let [[_Î» params body] e]\n                       (fn [& values]\n                         (autology.core/evaluate\n                          body\n                          (reduce (fn [acc-env [s v]]\n                                    (assoc acc-env s v))\n                                  env\n                                  (zipmap params values)))))))\n\n       ;; rebind `*i*` to be a new interpreter with the\n       ;; `:atl/eval-list` section replaced with a version that\n       ;; includes our lambda handling special form.\n       *i* (replace-marker *i* :atl/eval-list\n                           (list :atl/eval-list\n                                 (concat (butlast original)\n                                         Î»-form\n                                         (list (last original)))))\n\n       ;; We can now immediately define functions since the\n       ;; interpreter will have already been updated to evaluate the\n       ;; remaining bindings like this one.\n       double (Î» (n)\n                 (+ n n)))\n\n      ;; Finally we can invoke our new function!\n      (double (double (double (double (double 1.3125))))))\n\nAfter that, pretty much anything you want!\n\nWhy not switch form applicative order evaluation to normal order evaluation?\nMaybe modify the language so it uses a continuation passing style?\nDefine a metacircular Lisp inside your program which has access to it's own interpreter as well at the Autology interpreter.\n\nRun tests\nclojure -X:test",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "e3fa46eac06efa4b",
    "title": {
      "en": "Improving recommendation systems and search in the age of LLMs",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://eugeneyan.com/writing/recsys-llm/",
    "score": 379,
    "by": "7d7n",
    "time": 1742701205,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "e3f4dd00492cfe4a",
    "title": {
      "en": "Play the Virtual Organ from Arp Schnitger",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.orgelstadt-hamburg.de/play-arp/",
    "score": 103,
    "by": "ohjeez",
    "time": 1742754706,
    "content": "Â© Hauptkirche St. Jacobi Hamburg\n\n          â€ºplay arpâ€¹Virtuelle Orgel St. Jacobi\n          Hier haben Sie die weltweit einmalige Gelegenheit, eine der  international\n            wertvollsten und klangschÃ¶nsten Barockorgeln selbst zum Klingen zu bringen.\n            Viele Orgelfans und Experten pilgern nach Hamburg, um die berÃ¼hmte Arp-Schnitger\n            Orgel in der Hauptkirche St. Jacobi zu erleben. Wir bringen eine besondere\n            Auswahl klanglicher Perlen dieses Instrumentes exklusiv zu Ihnen. Spielen\n            Sie selbst!\n            (FÃ¼r das klanglich beste Erlebnis empfehlen wir einen guten KopfhÃ¶rer zu nutzen)",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "4899e5e1fbb3dfa5",
    "title": {
      "en": "23andMe files for bankruptcy to sell itself",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.reuters.com/business/healthcare-pharmaceuticals/dna-testing-firm-23andme-files-chapter-11-bankruptcy-sell-itself-2025-03-24/",
    "score": 329,
    "by": "healsdata",
    "time": 1742788104,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "16e73a0bb33acc13",
    "title": {
      "en": "You need to know about bootc",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://sean.thrailkill.cloud/posts/you-need-to-know-about-bootc/",
    "score": 48,
    "by": "nikodunk",
    "time": 1742775524,
    "content": "You Need To Know About Bootc\n\n        Posted\n\n  Jan  3, 2025\n\n          Updated\n\n  Jan  5, 2025\n\n          By\n\n  3 min read\n\n      You Need To Know About Bootc\n\n      Contents\n\n        You Need To Know About Bootc\n\n    At the risk of sounding like every YouTuber out there who wants you to click the â€œSubscribeâ€ button right now, I really believe everyone who cares about Linux should start learning about Bootc. When looking at the future of Linux and what that could look like thereâ€™s already a lot of talk of immutable distos like SteamOS, Fedora Silverblue, or NixOS being the future.\n\nStarting off let me be clear, traditional distros arenâ€™t going anywhere. They have their place and people already know and love them. Businesses rely on them. Yet people arenâ€™t using them and weâ€™ve been talking about â€œYear of the Linux Desktopâ€ for longer than I have been alive. Immutable distros (or â€œAtomicâ€ as they now like to be called) have been around for a while as well, but are relatively new in the Linux Desktop space and allow end users to be able a Linux desktop like an appliance. They can turn it on and know it will boot properly without having to worry about things users had to worry about in the past like drivers, kernel mods, or new packages breaking things â€“ the ghosts of Linux Desktop Past. If you want to see what can be possible, look at the Steam Deck.\n\nAlright, you get it by now â€“ atomic distros are cool. What makes them even cooler is how you can make your own atomic distro. Enter Bootc! Bootc allows you to make an OS the same way you make an application, using containers! There is no denying that most applications are shipped today as a Docker container and most developers have some experience with them. Bootc takes this existing developer experience and directly translates it to your OS. Lets look at an example to see how easy it can be:\n\n      1\n2\n3\nFROM quay.io/fedora/fedora-bootc:41\nCOPY nginx.container /usr/share/containers/systemd\nCOPY nginx.conf /etc/nginx\n\nSimple, right? This copies a file to run a Nginx container as a quadlet and a config into the /etc folder. This brings GitOps to my OS. With this whenever my machine starts, whether for the first time or millionth time, its going to be configured to work exactly as expected with no extra work or additional configuration. Day 1/2 configuration with other tools like Ansible or Chef are no longer necessary. Everything together in the same repo.\n\nBut that was a simple example for an Nginx server, what about something complex like a Linux desktop with all the software I want like VSCode, OpenRazr, and Nvidia drivers? Look no further than Universal Blue. This project is the epitome of â€œI want everything to work out of the box with no extra workâ€. In fact The Verge recently published an article saying that one of the Universal Blue projects, Bazzite, was a better experience than SteamOS. High praise! They put a lot of work into streamlining the user experience so anything you could want is just a simple command away and it always â€œJust Worksâ€ which is exactly the kind of experience normal non-technical users want. Something I can give to my parents and know theyâ€™ll have access to all the applications they need and wonâ€™t be able to break. Bootc is the way weâ€™re going to be able to achieve that.\n\nBootc isnâ€™t the only piece to the puzzle and Iâ€™m just scratching the surface with what Bootc can do. There are more pieces like ComposeFS & Systemd-sysext but Bootc is where I think people should start. If you want to see more examples feel free to check out my repo on GitLab.\n\n          Bootc\n\n          Atomic\n\n          UBlue\n\n          SteamOS\n\n          This post is licensed under\n\n          CC BY 4.0\n\n         by the author.\n\n  Share",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "4e1d365d65298255",
    "title": {
      "en": "BeeFormer: CF and CBF hybrid approach for recommendation systems",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/recombee/beeformer",
    "score": 23,
    "by": "klaussilveira",
    "time": 1742732424,
    "content": "beeFormer\nThis is the official implementation provided with our paper beeFormer: Bridging the Gap Between Semantic and Interaction Similarity in Recommender Systems.\nmain idea of beeFormer\n\nCollaborative filtering (CF) methods can capture patterns from interaction data that are not obvious at first sight. For example, when buying a printer, users can also buy toners, papers, or cables to connect the printer, and collaborative filtering can take such patterns into account. However, in the cold-start recommendation setup, where new items do not have any interaction at all, collaborative filtering methods cannot be used, and recommender systems are forced to use other approaches, like content-based filtering (CBF). The problem with content-based filtering is that it relies on item attributes, such as text descriptions. In our printer example, semantic similarity-trained language models will put other printers closer than accessories that users might be searching for. Our method is training language models to learn these user behavior patterns from interaction data to transfer that knowledge to previously unseen items. Our experiments show that performance benefits from this approach are enormous.\nSteps to start training the models:\n\ncreate virtual environment python3.10 -m venv beef and activate it source beef/bin/activate\nclone this repository and navigate to it cd beeformer\ninstall packages pip install -r requirements.txt\ndownload the data for movielens: navigate to the _dataset/ml20m folder and run source download_data\ndownload the data for goodbooks: navigate to the _dataset/goodbooks folder and run source download_data\ndownload the data for amazonbooks: navigate to the _dataset/amazonbooks folder and run source download_data && python preprocess.py\nin the root folder of the project run the train.py, for example like this:\n\npython train.py --seed 42 --scheduler None --lr 1e-5 --epochs 5 --dataset goodbooks --sbert \"sentence-transformers/all-mpnet-base-v2\" --max_seq_length 384 --batch_size 1024 --max_output 10000 --sbert_batch_size 200 --use_cold_start true --save_every_epoch true --model_name my_model\n\nEvaluate the results. To reproduce numbers from the paper using our hugginface repository, run for example:\n\npython evaluate_itemsplit.py --seed 42 --dataset goodbooks --sbert beeformer/Llama-goodbooks-mpnet\n\nor\npython evaluate_timesplit.py --seed 42 --dataset amazon-books --sbert beeformer/Llama-amazbooks-mpnet\n\nDatasets and preprocessing\nPreprocessing information\nWe consider ratings of 4.0 and higher as an interaction. We only keep the users with at least 5 interactions.\nLLM Data augmentations\nSince there are no text descriptions in the original data, we manually connect several datasets with the original data and train our models on it. However, this approach has several limitations: texts from different sources have different styles and different lengths, and this might influence the results. Therefore, we use the Llama-3.1-8b-instruct model to generate item descriptions for us. We use the following conversation template:\nimport pandas as pd\n\nfrom tqdm import tqdm\nfrom vllm import LLM, SamplingParams\n\nitems = pd.read_feather(\"items_with_gathered_side_info.feather\")\n\nllm = LLM(model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",dtype=\"float16\")\n\ntokenizer = llm.get_tokenizer()\nconversation = [ tokenizer.apply_chat_template(\n        [\n            {'role': 'system','content':\"You are ecomerce shop designer. Given a item description create one paragraph long summarization of the product.\"},\n            {'role': 'user', 'content': \"Item description: \"+x},\n            {'role': 'assistant', 'content': \"Sure, here is your one paragraph summary of your product:\"},\n        ],\n        tokenize=False,\n    ) for x in tqdm(items.gathered_features.to_list())]\n\noutput = llm.generate(\n    conversation,\n    SamplingParams(\n        temperature=0.1,\n        top_p=0.9,\n        max_tokens=512,\n        stop_token_ids=[tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")],\n    )\n)\n\nitems_descriptions = [o.outputs[0].text for o in output]\n\nHowever, LLM refused to generate descriptions for some items (For example, because it refuses to generate explicit content). We removed such items from the dataset. We also removed items for which we were not able to connect meaningful descriptions from other datasets, which led to LLM completely hallucinating item descriptions.\nWe share the resulting LLM-generated item descriptions in datasets/ml20m, dataset/goodbooks and datasets/amazonbooks folders.\nStatistics of datasets used for evaluation\n\nGoodBooks-10k\nMovieLens-20M\nAmazon Books\n\n# of items in X\n9975\n16902\n63305\n\n# of users in X\n53365\n136589\n634964\n\n# of interactions in X\n4119623\n9694668\n8290500\n\ndensity of X [%]\n0.7739\n0.4199\n0.0206\n\ndensity of X^TX [%]\n41.22\n26.93\n7.59\n\nPretrained models\nWe share pretrained models at https://huggingface.co/beeformer.\nHyperparameters\nWe used hyperparameters for training our models as follows.\n\nhyperparameter\ndescription\nbeeformer/Llama-goodbooks-mpnet\nbeeformer/Llama-movielens-mpnet\nbeeformer/Llama-goodlens-mpnet\nbeeformer/Llama-amazbooks-mpnet\n\nseed\nrandom seed used during training\n42\n42\n42\n42\n\nscheduler\nlearning rate scheduling strategy\nconstant learning rate\nconstant learning rate\nconstant learning rate\nconstant learning rate\n\nlr\nlearning rate\n1e-5\n1e-5\n1e-5\n1e-5\n\nepochs\nnumber of trained epochs\n5\n5\n10\n5\n\ndevices\ntraining script allow to train on multiple gpus in parallel - we used 4xV100\n[0,1,2,3]\n[0,1,2,3]\n[0,1,2,3]\n[0,1,2,3]\n\ndataset\ndataset used for training\ngoodbooks\nml20m\ngoodlens\namazon-books\n\nsbert\noriginal sentence transformer model used as an initial model for training\nsentence-transformers/all-mpnet-base-v2\nsentence-transformers/all-mpnet-base-v2\nsentence-transformers/all-mpnet-base-v2\nsentence-transformers/all-mpnet-base-v2\n\nmax_seq_length\nlimitation of sequence length; shorter sequences trains faster original mpnet model uses max 512 tokens in. sequence\n384\n384\n384\n384\n\nbatch_size\nnumber of users sampled in random batch from interaction matrix\n1024\n1024\n1024\n1024\n\nmax_output\nnegative sampling hyperparameter (m in the paper). Negatives are sampled uniformly at random.\n10000\n10000\n10000\n12500\n\nsbert_batch_size\nnumber of items processed together during training step (gradient accumulation step size)\n200\n200\n200\n200\n\nuse_cold_start\nsplit the dataset item-wise (some items are hidden to test the genralization towards new items)\ntrue\ntrue\ntrue\nfalse\n\nuse_time_split\nsort interactions by timestamp and use last 20% of interactions as a test set (generalization from the past to the future)\nfalse\nfalse\nfalse\ntrue\n\nRecSys 2024 poster\n\nCitation\nIf you find this repository helpful, feel free to cite our paper:\n@inproceedings{10.1145/3640457.3691707,\n        author = {Van\\v{c}ura, Vojt\\v{e}ch and Kord\\'{\\i}k, Pavel and Straka, Milan},\n        title = {beeFormer: Bridging the Gap Between Semantic and Interaction Similarity in Recommender Systems},\n        year = {2024},\n        isbn = {9798400705052},\n        publisher = {Association for Computing Machinery},\n        address = {New York, NY, USA},\n        url = {https://doi.org/10.1145/3640457.3691707},\n        doi = {10.1145/3640457.3691707},\n        booktitle = {Proceedings of the 18th ACM Conference on Recommender Systems},\n        pages = {1102â€“1107},\n        numpages = {6},\n        keywords = {Cold-start recommendation, Recommender systems, Sentence embeddings, Text mining, Zero-shot recommendation},\n        location = {Bari, Italy},\n        series = {RecSys '24}\n}",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "2349423c5fce781f",
    "title": {
      "en": "Total lunar eclipse over Teide crater, Tenerife â€“ a project with many obstacles",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://lrtimelapse.com/news/total-lunar-eclipse-over-teide-crater-tenerife/",
    "score": 123,
    "by": "elijahparker",
    "time": 1742569456,
    "content": "â† LRTimelapse 7.3 released!Total lunar eclipse over the Teide crater, Tenerife â€“ a project with many obstaclesâ€¦ Posted on 2025-03-21 by Gunther WegnerRarely have I thought so often about abandoning a photography project as during this lunar eclipse on Tenerife. Somehow, everything seemed to conspire against us this time. This is the story of an elaborately planned photo project where almost everything went wrong, but perseverance ultimately paid off. But letâ€™s start from the beginningâ€¦Germans: Hier gibt es eine Deutsche Version.We, that is Uli and I, have made it almost a tradition to meet on Tenerife (Canary Islands, Spain) once a year. This time, Uli said, hey, on March 14th thereâ€™s a total lunar eclipse, and Iâ€™ve calculated a location from which we can photograph the fully eclipsed moon setting directly over the crater of Teide at dawn!It sounded almost too good to be trueâ€”because typical lunar eclipses are rather dull photographically, especially after youâ€™ve seen and photographed a few. Without an interesting foreground, pictures of a red moon look the same anywhere in the world. Technically challenging, perhaps, but not very creative.A Unique ProjectNow, several exciting factors came together in Uliâ€™s idea:We would experience a total eclipse on Tenerife, while in Germany, for example, the moon would only be partially eclipsed.We would have a foregroundâ€”the impressive crater of Teide, Tenerifeâ€™s landmark and Spainâ€™s highest mountain at 3,715 meters.The totality would occur during dawn, meaning the sky would already be somewhat blue, and the moon would be low enough for us to use the crater as a foreground. We expected very aesthetic images from the contrast of the red eclipsed moon against the blue sky.According to my research, this opportunity to photograph a totally eclipsed moon over Teide wouldnâ€™t occur again within the next 10 yearsâ€”or possibly even much longer; I didnâ€™t look further. These images would be genuinely unique.The PlanningUli used the smartphone app Planit for planning. This app not only displays exact eclipse timings but also shows the moonâ€™s position relative to the observerâ€™s locationâ€”in three dimensions. The app has quite a learning curve, but roughly speaking:You define the summit of Teide as your â€œtarget,â€ jump to the appâ€™s day and time of moonset, and primarily search for a camera position reachable by foot.On the left, Iâ€™ve aligned the â€œsceneâ€ with the peak of Teide. The blue line on the right marks the spot where weâ€™d need to be standing for the shot, based on where it intersects the trail.This means looking for an intersection on the blue line with a hiking trail or road. On Tenerife, there was actually only one reachable location, on a hiking path in the Canadas, below the caldera rimâ€”about an hourâ€™s hike from the nearest parking lot. This spot becomes your virtual camera position in Planit.Once youâ€™ve found this spot, you can switch Planit to the VR camera mode to see Teideâ€™s peak, the moon, and the Earthâ€™s shadow through a virtual camera. You can even specify the focal length.We need to move further to the right to get the moon lined up over the crater. You can use the arrows to shift the camera position accordingly.Now, you can finely tune both the cameraâ€™s time and positionâ€”we want the fully shadowed moon precisely descending into the crater.Finally, you save the GPS coordinates for your chosen location.Leaving Nothing to ChanceRather than navigating blindly at night, we decided to scout the location during daylight first.Beautiful Landscapes in the Canadas del TeideOn a sunny morning, we drove an hour and a half up to Teide National Park and hiked to the spot. The route was quite strenuous, but the sun was shining, it was about 10 degrees Celsiusâ€”not too coldâ€”and the solitude and landscape were fantastic.We didnâ€™t want to think about the night yet, knowing temperatures would drop below freezing, coupled with the wind.In the background you can see the edge of the Caldera, the giant outer crater that surrounds the Pico del Teide.â€œA few meters more to the right,â€ said Uli, and I drew a line in the dust with my foot.Hereâ€™s where it would happen.At the moment, the sky was bright blue, and there was no sign of the moon. Hardly imaginable that in a few days, from exactly this spot, it would be precisely above the crater and eclipsed.We felt optimistic. This could really work out. On our way back, a wind began to pick up, funneling down from the direction of the IzaÃ±a observatories into our valley, like through a jet.The temperature dropped rapidly, and we already had to push a bit against the wind. A first, small taste of what was still ahead of usâ€¦Equipment ShortcomingsI was not entirely happy with the equipment I had available on the island. Usually, I donâ€™t photograph much with telephoto lenses here, so my longest focal length was a 70-200mm f/2.8.Uliâ€”always hardcoreâ€”had traveled to the island with 60 kg of gear, prepared for everything. Besides all his Sony lenses (200-600mm) and his Nikon Coolpix P1000 (24-3000mm!), he even had a 300mm and a Nikon 400mm f/2.8 with him. Crazy.â€œSince you have so many lenses â€“ could you lend me your Nikon?â€ (The only lens of his that would fit my cameras.) Uli agreed. One less worry, as 200mm was quite short for moon photography.Another issue was my tripod situation. I had only one reliable tripod on the island. The second one didnâ€™t deserve the nameâ€”I usually just use it as a holder for my tablet.Since this was a unique opportunity, I wanted to use two cameras. I had my Nikon Z8 and the Nikon Z50 II with me.So I decided to ask a friendâ€”who was planning to visit us a few days laterâ€”to bring me a tripod. Unfortunately, he had to cancel the trip last minute, so once again, I was left tripod-less. Ordering stuff here on the island is a bit tricky, and good camera stores are few and far between. So I figured Iâ€™d head into the capital to buy one there. The selection I had briefly checked online wasnâ€™t huge, but there was one tripod that actually deserved the name (and to be honest, Iâ€™m pretty into it now).After that little half-day trip, I could check off another item from the list. Now everything should finally go smoothly! (â€¦ though hindsight is always 20/20â€¦ ;-))Bad weather is comingâ€¦Weather ChallengesAlright, but the weather was still a big question mark. For daysâ€”actually, weeksâ€”thereâ€™d been this weird weather pattern hanging over the Atlantic. Weird meaning strong winds from the west (instead of the usual northeast trade winds), rain, even snow up in the mountains, really bad weather in the north of the island, and high cloudsâ€”yep, even over the rim of the caldera, which was exactly where we wanted to shoot.And sure enough, the evening of the event was super cloudy. We kept checking the webcams and forecasts over and over.Uli wanted to head up the night before the lunar eclipse to set up a few wide-angle cameras for some time-lapse shots, planning to pick them up again the next morning.I was hoping to catch a bit of sleep beforehand. The alarm was set for 2 a.m.Worst-case scenario kicks inJust as I had packed everything and was about to head to bed, Uli called: â€œIâ€™ve got good news and bad news. The goodâ€”weatherâ€™s actually not that bad. Not as windy as we feared.â€â€œSounds promisingâ€”and the bad?â€â€œWhile I was out setting up my cameras, someone broke into my car and stole my second backpackâ€”with all my telephoto lenses and other camerasâ€¦â€â€œWhat??!!â€Uli was surprisingly calm, considering heâ€™d just lost two-thirds of his gear: â€œItâ€™s insuredâ€¦â€ â€” but still, how were we supposed to shoot the lunar eclipse now, without any long lenses?It really felt like the universe was telling us it wasnâ€™t meant to be. I only had the 70â€“200 mm, and Uliâ€™s longest lens left was a 135 mm. Not exactly ideal for capturing a lunar eclipse.If it had happened just a day earlier, we mightâ€™ve been able to borrow some lenses. But just hours before? No chance.We went back and forth. â€œSo should we just call it off? Weatherâ€™s supposed to be unstable anywayâ€¦â€Thinking about calling it offThat little inner voiceâ€”the one that already had zero interest in getting up at 2 a.m. and hiking for hours in the coldâ€”kept chiming in: â€œThis is going to be a bust anyway, itâ€™s way too cold and probably all clouded over. Why donâ€™t you just shoot it from down here in town, at the pier? You could sleep in two extra hours, itâ€™s less windy, and way warmer.â€Uli said heâ€™d drive back down first and stop by a couple of electronics stores along the wayâ€”maybe, just maybe, he could find a telephoto lens there. Luckily, shops stay open pretty late on Tenerifeâ€¦â€œAlright, letâ€™s talk again once youâ€™re back.â€I kept refreshing the weather apps and checking webcams. Ran the numbers in Planit to figure out if we could at least capture the event down here on the coast. Sure, it would workâ€¦ but it just wouldnâ€™t be that once-in-a-lifetime shot.â€¦or maybe not?At some point, Uli called back: â€œThey donâ€™t even carry camera gear at all anymore at places like MediaMarktâ€”but I just remembered I actually have another 300 mm lens lying aroundâ€¦â€ Me: â€œğŸ™ˆ speechlessâ€¦â€ Alright thenâ€”300 mm for him, 200 mm for meâ€¦ not ideal, but worth a shot. Inner slacker, pipe down. â€œAlright, letâ€™s do this. Iâ€™ll pick you up at 2:30,â€ I said. As you can probably guess, it was a short night. I showed up right on time at Uliâ€™s: â€œWow, traveling light, huh?â€ â€” honestly, without some dark humor, this whole thing wouldâ€™ve been unbearableâ€¦Letâ€™s goâ€”up the mountain!Already in Vilaflor, the last village before the national park, the car beeped at 4Â°C. A few curves later, the roads were wet, and a little further up, they even started to sparkle with frost. I crawled up into the national park at 20 km/h. As soon as we entered the Caldera, we could see clouds spilling over the rim from the northâ€”yep, exactly the direction we needed to head. â€œThe observatories are still completely buried in clouds,â€ Uli said. Yeahâ€¦ I figured as much. We drove past the massive â€œRoques de Garcia.â€ The full moon was so bright, we could actually see the landscape with the naked eye. At one point, I switched off the headlightsâ€”crazy how bright that full moon is.Is the weather about to ruin everything?â€œWhat if we just shoot the eclipse here, with the silhouette of the Roques in the foreground?â€We thought about it for a moment. The downside: the moon would dip behind them later, since the rocks arenâ€™t that tall, and the sky would already be getting brighter with dawn. Plus, thereâ€™d be a much higher chance of bumping into other people hereâ€”people who might wander straight into our timelapses. And honestly, it just wouldnâ€™t be the shot weâ€™d spent so much time planning for.So, we kept going. From the Teide cable car station onwards, the road got damp again, and I had to slow way down. These curves are no jokeâ€”and there are no winter tires on Tenerife.Wisps of clouds started forming above us. Just 4 km to go. The clouds kept thickening. Oh man, this is not looking goodâ€¦Eventually, we reached the parking lot near El Portillo. The clouds were spilling into the caldera from the left and drifting right over us.Down there, where the clouds are driftingâ€”thatâ€™s where we need to go. (Shot on smartphone)â€œLetâ€™s walk over to the viewpoint and check things outâ€”worst case, we head back to the Roques. Timing-wise, weâ€™d still make it.â€But honestly, the viewpoint didnâ€™t really give us any new insights. The clouds were still there, and we couldnâ€™t tell whether theyâ€™d get better or worse. We figured we had about a 50:50 chance of getting our shot. At the Roques? Pretty much a guaranteed 100%. But of courseâ€¦ it just wouldnâ€™t be the same. Soâ€”what to do?We decided to take the riskIn the end, we went with the more uncertain, but potentially way more rewarding optionâ€”and started the hike.Inside the caldera (Smartphone)Thanks to the full moon, we didnâ€™t need a headlamp or any other light. Even the initial bit of scrambling over some rocks worked just fine in moonlight alone. As timelapse and astro photographers, we naturally have a bit of an aversion to those insanely bright LED floodlights people call â€œheadlampsâ€ these days. And speaking of whichâ€”we actually saw quite a few of them, despite the unstable weather, making their way up the slopes of Teide. It never ceases to amaze me how many people head up there, no matter the conditions.And if it was already -4Â°C and pretty windy where we were at 2,200 meters, it mustâ€™ve been closer to -10Â°C up there, more than 1,000 meters higherâ€”with some seriously brutal wind.Teide in the full moonlight (Smartphone)But we had layered up properly, and while we were hiking, we werenâ€™t cold at all. The wind was at our backs, too.Setting upThe moon was still fairly high, so we had plenty of time to set everything up without rushing. Uli planned to use his main camera with the 300 mm on the Polarie star tracker. Getting the mount perfectly aligned with the North Star always takes a little timeâ€”but luckily, he managed to catch a clear glimpse of Polaris through a gap in the clouds.I had the 24â€“70 on the Z50II set to about 50 mm, aiming for a wide shot with the whole Teide in frame. Once the moon moved a bit lower, I was going to switch over to the 70â€“200 on my new tripod, dialed in at 200 mm, and aim it right at the craterâ€™s summit. But first, I shot a â€œboringâ€ sequenceâ€”just the already-eclipsed moon on its own, no background.A layer of ice formed on our gear in no timeI controlled the exposure of both cameras using qDslrDashboardâ€”but mostly manually, not with the â€œAuto Holy Grailâ€ mode. The reason? During a lunar eclipse, you need to be able to adjust exposure very quickly, while the â€œAuto Holy Grailâ€ algorithm is designed to do the opposite: it smooths out short-term brightness changes and only adapts gradually over timeâ€”perfect for sunrises and sunsets, but not so much for a fast-changing eclipse.At one point, the moon drifted out of frame on the 200 mm, so I decided it was time to set up the final shot. The idea was to frame the crater at the bottom and start shooting just as the moon was entering the frameâ€”this way, I wouldnâ€™t have to touch the camera again until the moon had completely moved out.Challenge: ExposureA quick word about exposure during a lunar eclipseâ€”because honestly, itâ€™s one of the biggest challenges.While even just a sliver of the full moon is still visible, not even todayâ€™s sensorsâ€”with their 14 or 15 stops of dynamic rangeâ€”can capture both the moon and the landscape without compromise. Either you expose for the landscape and the moon is totally blown out, or you expose for the moon and the landscape turns to pure black.The trick is finding that sweet spot: a balance where you can still pull some detail out of the landscape in post without everything being a noisy mess, and where the moon doesnâ€™t just show up as a giant white blob.Test shots and histograms? Not much help eitherâ€”the moon just shows up as a bright dot on the screen, everything else around it looks pitch black.This is what the image preview looks like on the monitorâ€¦At that point, you really have to rely on experience to judge the exposureâ€”trying to nail it so thereâ€™s still just enough hidden in the shadows to pull the landscape out later. Even with a flat camera profile, thereâ€™s just no way to tell from the JPG or histogram what youâ€™ll actually be able to recover from the RAW file later on.The same image (Nikon Z50 II) after RAW processing â€“ you can now see the moon is already slightly eclipsed, and thereâ€™s detail in the landscape too.As totality approaches, exposure gets easier since the contrast drops. The Z50II performed well here, though of course it canâ€™t quite keep up with the dynamic range of the Z8. 30mm, f/2.8, 2 sec, ISO 800.Everything changes suddenly the moment totality begins. In our case, the eclipse was right on the edge of the umbra, so even toward the end of totality, we still had a faint glow on the right side of the moon in the images.To get the best possible timelapse of a lunar eclipse, you have to react fast when totality hits and adjust your exposure accordingly. For the 50 mm wide shot, I decided to let the moon blow out just a littleâ€”as little as possible, but enough to still keep some detail in the landscape. For the telephoto shot, I was only capturing totality anyway, which made it way easier to expose properly.Meanwhile, clouds had drifted in front of Teide and we found ourselves standing in freezing rain. Thankfully, the crater and the moon were still more or less visible. Our camera bags were crusted with ice crystals by now, and during my final camera adjustments, I couldnâ€™t feel my fingers anymore. I couldnâ€™t even manage to turn off the displayâ€”the button just wouldnâ€™t press. Oh well, the battery would survive. Hopefully.Would the moon actually fall into the crater? Shot with the Nikon Z8 at 200mm, f/2.8, ISO 800, and 2 sec exposure.So, we were all set: my wide shot was rolling, the telephoto sequence was up and running, and both of Uliâ€™s cameras were clicking away too. I had set the interval to 3 seconds, by the way.Unexpected interruptionFinally, we could shove our hands into our warm pockets and just enjoy the show. Just as we were starting to relax, we suddenly saw two of those ultra-bright â€œsearchlightâ€ headlamps making their way down the rocky slopeâ€”right between us and Teide. WTF?? Thereâ€™s no trail there, and definitely no parking spot up topâ€”where the heck did they come from?They just kept weaving their way down toward us, totally unfazed, and their insanely bright beams kept sweeping straight in our directionâ€”almost certainly hitting our cameras, too.Youâ€™ve got to be kidding me. â€œTurn off the lights!â€ I shouted into the night. â€œÂ¡Apaguen las luces!â€ â€” no reaction. I called again, louder this time. Still nothing. Eventually, they settled on some rocks in front of usâ€”but left their lights on the entire time.Who are these people? I mean, who else would be able to pinpoint this exact location so precisely? Probably only photographers, right? I mean, who else knows how to use Planit? ğŸ˜‰ But then again, photographersâ€”especially astro nerdsâ€”usually know better than to blast everything with full-beam headlamps all night. We didnâ€™t get the feeling they were even shooting photosâ€¦ so what were they doing there?Guess weâ€™ll never know. But seriously, running into someone out here, at this hour, was the last thing we expected.In the end, we just had to roll with it. Whether those lights would show up in our footageâ€¦ well, weâ€™d find out later. For now, we just wanted to soak in the magic of the total lunar eclipse.The freezing rain had stopped, and the clouds decided to give the summit a breakâ€”drifting by just beneath the peak.Totality approachesFrom the moment totality began to the point where the moon disappeared behind the crater, we had just 6 minutes. The sky had already started to take on a faint blue hue with the approaching dawn. And now, more clouds were loomingâ€”this time coming in from the right. Just after the moon touched the edge of the crater, one of those clouds drifted right in front of itâ€”because of course it did. Luckily, it was a small one and didnâ€™t mess up the final result too much. Andâ€”miraculouslyâ€”the lights from those ridiculously bright headlamps didnâ€™t show up in the timelapse at all. Sometimes, you just need a bit of luck!We did it!And here it isâ€”the single frame from the timelapse that made all of this effort totally worth it:It worked! ğŸ™‚ Nikon Z8, 200mm, f/2.8, 2 sec, ISO 800.Once the moon had disappeared, we briefly considered running off to the right to catch one last glimpse of it beside the mountainâ€”but it didnâ€™t take long to realize that wasnâ€™t going to happen. We still had to de-ice and pack up all the gear, and with how quickly the moon was setting, there was just no way weâ€™d make it in time. So instead, we let the cameras keep rolling a little longer and simply watched as the day slowly began to break.Photo: Uli FehrOne more timelapse with the ice-covered camera, this time facing the edge of the caldera, where clouds were now spilling in from the south as well.We did it! ğŸ™‚The way backFrozen to the bone but happy, we packed up everything calmly and started the hike back to the car. Straight into the thick clouds that soon wrapped around us. We battled against the wind, and a thin layer of ice had formed on the ground. I really didnâ€™t want to think about the drive back just yetâ€¦Somewhere out there in that fog bankâ€¦ is our car.Every now and then, Teide lit up in a beautiful orange glow from the rising sun.Everything frozenâ€¦ğŸ™‚When we finally got back to the car, it was completely frozen over too. Ice scrapers in Tenerife? Yeahâ€¦ not exactly a booming market. So with hands that were finally starting to thaw, we scraped the ice off the windshield. My phone still showed -4Â°C.And once again, we got lucky: the road wasnâ€™t too icy, so we carefully began the drive backâ€”at a snailâ€™s pace. The higher the sun climbed and the farther south we got, the drier the road became, and before long the thermometer crept back into positive territory.We did it!A little riddle for youTired but seriously happy, we started our descentâ€”and ended up witnessing a super interesting physical-chemical phenomenon. Check out this photo:  In the spots where there was shadowâ€”or just recently had beenâ€”the road had already dried. But in the sun? Still wet. Howâ€™s that even possible? If you know the answer, drop it in the comments! ğŸ™‚Uli picked up the camera heâ€™d set up the night before, and then we headed back down to the warm coast for a well-deserved late breakfastâ€”though not without quickly grabbing our laptops first to offload the sequences. And while we were still waiting for our food and coffee, we couldnâ€™t help ourselvesâ€”we already started reviewing the footage. The excitement was just too much!Do you want to learn how to shoot and edit such timelapses? Get my E-Book Timelapse Photography and learn it step by step the easy way!My timelapseAnd to wrap things up, hereâ€™s a little compilation of the timelapse sequences I captured. Enjoy the show!\n\n      Total Lunar Eclipse over Crater of Pico del Teide, Tenerife (HDR Timelapse)\n    Video can&#8217;t be loaded because JavaScript is disabled: <a href=\"https://youtu.be/dSuCsWTxZ6U\" title=\"Total Lunar Eclipse over Crater of Pico del Teide, Tenerife (HDR Timelapse)\">Total Lunar Eclipse over Crater of Pico del Teide, Tenerife (HDR Timelapse) (https://youtu.be/dSuCsWTxZ6U)</a>To watch the video in HDR, youâ€™ll need an HDR-capable TV or, on a computer, an HDR-capable monitor and browser (Chromium-based), with HDR enabled in your operating system. Make sure to watch the video directly on YouTube!P.S. Have you ever planned and pulled off a photo project this elaborate? Iâ€™d love to hear about itâ€”drop me a comment!P.P.S. And be sure to check out Uli on Instagram and Facebookâ€”heâ€™s always up to some wild photo adventures. ğŸ™‚This might also be of interest to you:LA Timelapse Film Festival - the award ceremony now on Youtube! The Los Angeles Timelapse Film Festival (LATFF), first of its kind, took place last Saturday (Jan. 28, 2023) as a live stream and wasâ€¦Northern Skies 4K (Cello Version, 2024) This is an update of my Film Northern Skies where the Cello in the Music track was played by Georg Roither, a professional cellistâ€¦Shorter Basic Tutorial (no Lightroom) This is the simplified, shorter basic tutorial for LRTimelapse without Lightroom Classic. You will learn how to edit your timelapse using LRTimelapse only: keyframeâ€¦LRTimelapse 7 released and new Tutorials! Dear Community, I've now released the final version of LRTimelapse 7, the latest \"major\" upgrade of LRTimelapse. I've spent an enormous amount of timeâ€¦LA Timelapse Film Festival Audience Voting and Livestream Next Saturday, Jan. 28, 2023, the award ceremony for the Los Angeles Timelapse Film Festival will take place in an online ceremony - andâ€¦Basic Tutorial This is the complete basic tutorial for LRTimelapse. Additionally to the internal editing and processing in LRTimelapse it also includes editing and exporting viaâ€¦â† LRTimelapse 7.3 released!6 Responses to Total lunar eclipse over the Teide crater, Tenerife â€“ a project with many obstaclesâ€¦ Constantin says: 2025-03-22 at 3:06 pmOn the topic of the road: Parts still in shade have humidity collected from the night in form of very light frost, barely visible. This melts when the sun hits and thereby becomes clearly visible as wet road.Reply Gunther Wegner says: 2025-03-22 at 5:15 pmYes, but here the road is wet in the sun while it is dry in the shade. Thatâ€™s paradoxical and the other way around, as you would expect. Iâ€™ll wait for a few more answers, then Iâ€™ll solve it! ğŸ¤—Reply Gavin says: 2025-03-22 at 7:27 amI had a think about this phenomenon on the road and I suspect that the road heated by the sun is much closer to the dew point than the road in the shade. The clear night sky has kept the road dry, but with the advent of some warming, the moisture in the air is attracted to objects at or close to the dew point. Best way to find out the answer is to have a stab in the dark, like I have, and then the experts will show up to give us the science.Reply Matthew Wheeler says: 2025-03-22 at 5:07 amThank you for a great finger-nail biting account of this expensive adventure, with such a beautiful, spectacular outcome after all the work and obstaclesâ€“it is so well presented and I had to read to the end right away!  About the shadow â€œphotographyâ€ on the road: I have seen this fair bit in Canada, where the snow can be etched away by the sun, leaving quite intricate images of the shadows. Frozen mud can become quite light in tone, and darken when it thaws. In this case it is possible the pavement looks dry because drizzle had frozen into it, and the width of the gap outside the current shadow shows the time it took to melt and darken the road surface.Reply Silvana Della Camera says: 2025-03-22 at 12:27 amWonderful! I am glad you didnâ€™t give up. I have found that the battle within is often harder than the difficult task itself. Thank you for sharing your story and the remarkable results.Reply John Edwards says: 2025-03-21 at 7:48 pmLove the planning and perseverance.  Thanks for sharing your story and your excellent timelapse of the lunar eclipse.ReplyLeave a Reply Cancel replyYour email address will not be published. Required fields are marked *Comment *document.getElementById(\"comment\").setAttribute( \"id\", \"adbea64b2fde5b75f2ce12dc1519c796\" );document.getElementById(\"c60cb81b37\").setAttribute( \"id\", \"comment\" );Name * Email * Website",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "2577d1b6b5eb6922",
    "title": {
      "en": "LangManus: An Open-Source Manus Agent with LangChain + LangGraph",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/langmanus/langmanus",
    "score": 71,
    "by": "gfortaine",
    "time": 1742554860,
    "content": "ğŸ¦œğŸ¤– LangManus\n\nEnglish | ç®€ä½“ä¸­æ–‡ | æ—¥æœ¬èª\n\nCome From Open Source, Back to Open Source\n\nLangManus is a community-driven AI automation framework that builds upon the incredible work of the open source community. Our goal is to combine language models with specialized tools for tasks like web search, crawling, and Python code execution, while giving back to the community that made this possible.\nDemo\nTask: Calculate the influence index of DeepSeek R1 on HuggingFace. This index can be designed using a weighted sum of factors such as followers, downloads, and likes.\nLangManus's Fully Automated Plan and Solution:\n\nGather the latest information about \"DeepSeek R1\", \"HuggingFace\", and related topics through online searches.\nInteract with a Chromium instance to visit the HuggingFace official website, search for \"DeepSeek R1\" and retrieve the latest data, including followers, likes, downloads, and other relevant metrics.\nFind formulas for calculating model influence using search engines and web scraping.\nUse Python to compute the influence index of DeepSeek R1 based on the collected data.\nPresent a comprehensive report to the user.\n\nView on YouTube\n\nTable of Contents\n\nQuick Start\nProject Statement\nArchitecture\nFeatures\nWhy LangManus?\nSetup\n\nPrerequisites\nInstallation\nConfiguration\n\nUsage\nDocker\nWeb UI\nDevelopment\nFAQ\nContributing\nLicense\nAcknowledgments\n\nQuick Start\n# Clone the repository\ngit clone https://github.com/langmanus/langmanus.git\ncd langmanus\n\n# Install dependencies, uv will take care of the python interpreter and venv creation\nuv sync\n\n# Playwright install to use Chromium for browser-use by default\nuv run playwright install\n\n# Configure environment\n# Windows: copy .env.example .env\ncp .env.example .env\n# Edit .env with your API keys\n\n# Run the project\nuv run main.py\n\nProject Statement\nThis is an academically driven open-source project, developed by a group of former colleagues in our spare time. It aims to explore and exchange ideas in the fields of Multi-Agent and DeepResearch.\n\nPurpose: The primary purpose of this project is academic research, participation in the GAIA leaderboard, and the future publication of related papers.\nIndependence Statement: This project is entirely independent and unrelated to our primary job responsibilities. It does not represent the views or positions of our employers or any organizations.\nNo Association: This project has no association with Manus (whether it refers to a company, organization, or any other entity).\nClarification Statement: We have not promoted this project on any social media platforms. Any inaccurate reports related to this project are not aligned with its academic spirit.\nContribution Management: Issues and PRs will be addressed during our free time and may experience delays. We appreciate your understanding.\nDisclaimer: This project is open-sourced under the MIT License. Users assume all risks associated with its use. We disclaim any responsibility for any direct or indirect consequences arising from the use of this project.\n\né¡¹ç›®å£°æ˜\næœ¬é¡¹ç›®æ˜¯ä¸€ä¸ªå­¦æœ¯é©±åŠ¨çš„å¼€æºé¡¹ç›®ï¼Œç”±ä¸€ç¾¤å‰åŒäº‹åœ¨ä¸šä½™æ—¶é—´å¼€å‘ï¼Œæ—¨åœ¨æ¢ç´¢å’Œäº¤æµ Multi-Agent å’Œ DeepResearch ç›¸å…³é¢†åŸŸçš„æŠ€æœ¯ã€‚\n\né¡¹ç›®ç›®çš„ï¼šæœ¬é¡¹ç›®çš„ä¸»è¦ç›®çš„æ˜¯å­¦æœ¯ç ”ç©¶ã€å‚ä¸ GAIA æ’è¡Œæ¦œï¼Œå¹¶è®¡åˆ’åœ¨æœªæ¥å‘è¡¨ç›¸å…³è®ºæ–‡ã€‚\nç‹¬ç«‹æ€§å£°æ˜ï¼šæœ¬é¡¹ç›®å®Œå…¨ç‹¬ç«‹ï¼Œä¸æˆ‘ä»¬çš„æœ¬èŒå·¥ä½œæ— å…³ï¼Œä¸ä»£è¡¨æˆ‘ä»¬æ‰€åœ¨å…¬å¸æˆ–ä»»ä½•ç»„ç»‡çš„ç«‹åœºæˆ–è§‚ç‚¹ã€‚\næ— å…³è”å£°æ˜ï¼šæœ¬é¡¹ç›®ä¸ Manusï¼ˆæ— è®ºæ˜¯å…¬å¸ã€ç»„ç»‡è¿˜æ˜¯å…¶ä»–å®ä½“ï¼‰æ— ä»»ä½•å…³è”ã€‚\næ¾„æ¸…å£°æ˜ï¼šæˆ‘ä»¬æœªåœ¨ä»»ä½•ç¤¾äº¤åª’ä½“å¹³å°ä¸Šå®£ä¼ è¿‡æœ¬é¡¹ç›®ï¼Œä»»ä½•ä¸æœ¬é¡¹ç›®ç›¸å…³çš„ä¸å®æŠ¥é“å‡ä¸æœ¬é¡¹ç›®çš„å­¦æœ¯ç²¾ç¥æ— å…³ã€‚\nè´¡çŒ®ç®¡ç†ï¼šIssue å’Œ PR å°†åœ¨æˆ‘ä»¬ç©ºé—²æ—¶é—´å¤„ç†ï¼Œå¯èƒ½å­˜åœ¨å»¶è¿Ÿï¼Œæ•¬è¯·è°…è§£ã€‚\nå…è´£å£°æ˜ï¼šæœ¬é¡¹ç›®åŸºäº MIT åè®®å¼€æºï¼Œä½¿ç”¨è€…éœ€è‡ªè¡Œæ‰¿æ‹…ä½¿ç”¨é£é™©ã€‚æˆ‘ä»¬å¯¹å› ä½¿ç”¨æœ¬é¡¹ç›®äº§ç”Ÿçš„ä»»ä½•ç›´æ¥æˆ–é—´æ¥åæœä¸æ‰¿æ‹…è´£ä»»ã€‚\n\nArchitecture\nLangManus implements a hierarchical multi-agent system where a supervisor coordinates specialized agents to accomplish complex tasks:\n\nThe system consists of the following agents working together:\n\nCoordinator - The entry point that handles initial interactions and routes tasks\nPlanner - Analyzes tasks and creates execution strategies\nSupervisor - Oversees and manages the execution of other agents\nResearcher - Gathers and analyzes information\nCoder - Handles code generation and modifications\nBrowser - Performs web browsing and information retrieval\nReporter - Generates reports and summaries of the workflow results\n\nFeatures\nCore Capabilities\n\nğŸ¤– LLM Integration\n\nIt supports the integration of most models through litellm.\nSupport for open source models like Qwen\nOpenAI-compatible API interface\nMulti-tier LLM system for different task complexities\n\nTools and Integrations\n\nğŸ” Search and Retrieval\n\nWeb search via Tavily API\nNeural search with Jina\nAdvanced content extraction\n\nDevelopment Features\n\nğŸ Python Integration\n\nBuilt-in Python REPL\nCode execution environment\nPackage management with uv\n\nWorkflow Management\n\nğŸ“Š Visualization and Control\n\nWorkflow graph visualization\nMulti-agent orchestration\nTask delegation and monitoring\n\nWhy LangManus?\nWe believe in the power of open source collaboration. This project wouldn't be possible without the amazing work of projects like:\n\nQwen for their open source LLMs\nTavily for search capabilities\nJina for crawl search technology\nBrowser-use for control browser\nAnd many other open source contributors\n\nWe're committed to giving back to the community and welcome contributions of all kinds - whether it's code, documentation, bug reports, or feature suggestions.\nSetup\nPrerequisites\n\nuv package manager\n\nInstallation\nLangManus leverages uv as its package manager to streamline dependency management.\nFollow the steps below to set up a virtual environment and install the necessary dependencies:\n# Step 1: Create and activate a virtual environment through uv\nuv python install 3.12\nuv venv --python 3.12\n\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Step 2: Install project dependencies\nuv sync\n\nBy completing these steps, you'll ensure your environment is properly configured and ready for development.\nConfiguration\nLangManus uses a three-layer LLM system, which are respectively used for reasoning, basic tasks, and vision-language tasks. Configuration is done using the conf.yaml file in the root directory of the project. You can copy conf.yaml.example to conf.yaml to start the configuration:\ncp conf.yaml.example conf.yaml\n\n# Setting it to true will read the conf.yaml configuration, and setting it to false will use the original .env configuration. The default is false (compatible with existing configurations)\nUSE_CONF: true\n\n# LLM Config\n## Follow the litellm configuration parameters: https://docs.litellm.ai/docs/providers. You can click on the specific provider document to view the completion parameter examples\nREASONING_MODEL:\n  model: \"volcengine/ep-xxxx\"\n  api_key: $REASONING_API_KEY # Supports referencing the environment variable ENV_KEY in the.env file through $ENV_KEY\n  api_base: $REASONING_BASE_URL\n\nBASIC_MODEL:\n  model: \"azure/gpt-4o-2024-08-06\"\n  api_base: $AZURE_API_BASE\n  api_version: $AZURE_API_VERSION\n  api_key: $AZURE_API_KEY\n\nVISION_MODEL:\n  model: \"azure/gpt-4o-2024-08-06\"\n  api_base: $AZURE_API_BASE\n  api_version: $AZURE_API_VERSION\n  api_key: $AZURE_API_KEY\n\nYou can create a .env file in the root directory of the project and configure the following environment variables. You can copy the.env.example file as a template to start:\ncp .env.example .env\n\n# Tool API Key\nTAVILY_API_KEY=your_tavily_api_key\nJINA_API_KEY=your_jina_api_key  # Optional\n\n# Browser Configuration\nCHROME_INSTANCE_PATH=/Applications/Google Chrome.app/Contents/MacOS/Google Chrome  # Optional, the path to the Chrome executable file\nCHROME_HEADLESS=False  # Optional, the default is False\nCHROME_PROXY_SERVER=http://127.0.0.1:10809  # Optional, the default is None\nCHROME_PROXY_USERNAME=  # Optional, the default is None\nCHROME_PROXY_PASSWORD=  # Optional, the default is None\n\nNote:\n\nThe system uses different models for different types of tasks:\n\nThe reasoning LLM is used for complex decision-making and analysis.\nThe basic LLM is used for simple text tasks.\nThe vision-language LLM is used for tasks involving image understanding.\n\nThe configuration of all LLMs can be customized independently.\nThe Jina API key is optional. Providing your own key can obtain a higher rate limit (you can obtain this key at jina.ai).\nThe default configuration for Tavily search is to return up to 5 results (you can obtain this key at app.tavily.com).\n\nConfigure Pre-commit Hook\nLangManus includes a pre-commit hook that runs linting and formatting checks before each commit. To set it up:\n\nMake the pre-commit script executable:\n\nchmod +x pre-commit\n\nInstall the pre-commit hook:\n\nln -s ../../pre-commit .git/hooks/pre-commit\n\nThe pre-commit hook will automatically:\n\nRun linting checks (make lint)\nRun code formatting (make format)\nAdd any reformatted files back to staging\nPrevent commits if there are any linting or formatting errors\n\nUsage\nBasic Execution\nTo run LangManus with default settings:\nuv run main.py\n\nAPI Server\nLangManus provides a FastAPI-based API server with streaming support:\n# Start the API server\nmake serve\n\n# Or run directly\nuv run server.py\n\nThe API server exposes the following endpoints:\n\nPOST /api/chat/stream: Chat endpoint for LangGraph invoke with streaming support\n\nRequest body:\n\n{\n  \"messages\": [{ \"role\": \"user\", \"content\": \"Your query here\" }],\n  \"debug\": false\n}\n\nReturns a Server-Sent Events (SSE) stream with the agent's responses\n\nAdvanced Configuration\nLangManus can be customized through various configuration files in the src/config directory:\n\nenv.py: Configure LLM models, API keys, and base URLs\ntools.py: Adjust tool-specific settings (e.g., Tavily search results limit)\nagents.py: Modify team composition and agent system prompts\n\nAgent Prompts System\nLangManus uses a sophisticated prompting system in the src/prompts directory to define agent behaviors and responsibilities:\nCore Agent Roles\n\nSupervisor (src/prompts/supervisor.md): Coordinates the team and delegates tasks by analyzing requests and determining which specialist should handle them. Makes decisions about task completion and workflow transitions.\n\nResearcher (src/prompts/researcher.md): Specializes in information gathering through web searches and data collection. Uses Tavily search and web crawling capabilities while avoiding mathematical computations or file operations.\n\nCoder (src/prompts/coder.md): Professional software engineer role focused on Python and bash scripting. Handles:\n\nPython code execution and analysis\nShell command execution\nTechnical problem-solving and implementation\n\nFile Manager (src/prompts/file_manager.md): Handles all file system operations with a focus on properly formatting and saving content in markdown format.\n\nBrowser (src/prompts/browser.md): Web interaction specialist that handles:\n\nWebsite navigation\nPage interaction (clicking, typing, scrolling)\nContent extraction from web pages\n\nPrompt System Architecture\nThe prompts system uses a template engine (src/prompts/template.py) that:\n\nLoads role-specific markdown templates\nHandles variable substitution (e.g., current time, team member information)\nFormats system prompts for each agent\n\nEach agent's prompt is defined in a separate markdown file, making it easy to modify behavior and responsibilities without changing the underlying code.\nDocker\nLangManus can be run in a Docker container. default serve api on port 8000.\nBefore run docker, you need to prepare environment variables in .env file.\ndocker build -t langmanus .\ndocker run --name langmanus -d --env-file .env -e CHROME_HEADLESS=True -p 8000:8000 langmanus\n\nYou can also just run the cli with docker.\ndocker build -t langmanus .\ndocker run --rm -it --env-file .env -e CHROME_HEADLESS=True langmanus uv run python main.py\n\nWeb UI\nLangManus provides a default web UI.\nPlease refer to the langmanus/langmanus-web-ui project for more details.\nDocker Compose (include both backend and frontend)\nLangManus provides a docker-compose setup to easily run both the backend and frontend together:\n# Start both backend and frontend\ndocker-compose up -d\n\n# The backend will be available at http://localhost:8000\n# The frontend will be available at http://localhost:3000, which could be accessed through web browser\n\nThis will:\n\nBuild and start the LangManus backend container\nBuild and start the LangManus web UI container\nConnect them using a shared network\n\n** Make sure you have your .env file prepared with the necessary API keys before starting the services. **\nDevelopment\nTesting\nRun the test suite:\n# Run all tests\nmake test\n\n# Run specific test file\npytest tests/integration/test_workflow.py\n\n# Run with coverage\nmake coverage\n\nCode Quality\n# Run linting\nmake lint\n\n# Format code\nmake format\n\nFAQ\nPlease refer to the FAQ.md for more details.\nContributing\nWe welcome contributions of all kinds! Whether you're fixing a typo, improving documentation, or adding a new feature, your help is appreciated. Please see our Contributing Guide for details on how to get started.\nLicense\nThis project is open source and available under the MIT License.\nStar History\n\nAcknowledgments\nSpecial thanks to all the open source projects and contributors that make LangManus possible. We stand on the shoulders of giants.\nIn particular, we want to express our deep appreciation for:\n\nLangChain for their exceptional framework that powers our LLM interactions and chains\nLangGraph for enabling our sophisticated multi-agent orchestration\nBrowser-use for control browser\n\nThese amazing projects form the foundation of LangManus and demonstrate the power of open source collaboration.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "c32c7963edc80997",
    "title": {
      "en": "Show HN: LinkedIn sucks, so I built a better one",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://heyopenspot.com/",
    "score": 170,
    "by": "fliellerjulian",
    "time": 1742755960,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "b6280d3a3041786d",
    "title": {
      "en": "Technicalities of Homeworld 2 Backgrounds",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://simonschreibt.de/gat/homeworld-2-backgrounds/",
    "score": 176,
    "by": "todsacerdoti",
    "time": 1742735655,
    "content": "I made a game! Buy on Steam\n\n\t\t\t\t\t\tA Game Art TrickHomeworld 2 â€“ Backgrounds\n\nRussian Version by Eliiah\n\nKorean Version by Woodorl Kim\n\nWhat you see here\n\nis the stunning background art\n\nof one of the most beautiful sci-fi games.\n\n H o m e w o r l d 2\nThanks for reading.\nJust kidding. Of course i have something to say about this. In the company we look at the art of Homeworld from time to time and bow to the creators of this masterpiece. Once we talked about how great the background look and how interesting this sketched style is. There is somethingâ€¦some details seemâ€¦special to us.\n\nI mentioned, that this looks a bit likeâ€¦ a vertex color gradient. But they wouldnâ€™t paint the background on geometry, right? I meanâ€¦that would has to be a highly tessellated sphere.\nThe discussion was over but I wasnâ€™t satisfied and wanted at least see the textures. So i used some mod tools to extract the Homeworld 2 Demo data but there were no textures. Only some .HOD files. I used Google and found a thread how to generate these .HOD files from a .TGA. It was said:\n\nâ€œâ€¦scans every pixel of the image then based on contrast\nit decides whether or not to add a new vertex and colorâ€¦â€\n\nWhat?\nCould it really be, that this is vertex color? Luckily you can watch at .HOD file with CFHodEdit. And another tool can force a wireframe mode. And now look what this brought to light:\nThis is one\n\nof the most brave\n\nsolutions for game art i ever saw.\n\nAnd here you can see how this influences the sky sphere geometry of the game. Do you see how low the resolution is in the low contrast areas? And how round the sphere is where details were painted?\n\nI never ever had thought, that this can produce such good results. Oh and donâ€™t forget that this technique solves two major problems.\n#1 You donâ€™t have any problems with DDS texture compression artifacts.\n#2 More important from composition perspective: since you canâ€™t get too fine detail (it was said in the tutorial that the base TGA shouldnâ€™t contain too sharp details), the background stays were it should:\nIn the background.\nToo often i see games where the background contains so much noise and details, that you canâ€™t really separate fore-/midground from background.\nThe last time i saw this perfect combination of tech & composition was in Diablo 3. I talk about the 2.5D tree article.\nIf you want know more about how these spheres are generated, read my next article about this topic.\nThanks for reading.\n\nUpdate 1\n\nOskar StÃ¥lberg used the Homeworld-Background-Idea in his personal project which looks soooo gorgeous! :,)\nSource: Planet Demo by Oskar StÃ¥lberg\n\nUpdate 2\n\nChris Correia works on a space game and asked me about the stars in the Homeworld-Backgrounds because they are super-sharp. I remembered having seen a thread like this a while ago and here it is!\nIn fact, the stars are single textures/billboards:\nSource: Background Star Fields\n\n\t\t\t\t\t23 Replies\n\n\t\t\t\tWant more?\n\t\t\t\tRead more articles like this one by checking out the Game Art Trick Section! By the way: I've a Ko-fi, a Gumroad and a Discord. Have a very nice day! <3\n\n\t\t\tThis entry was posted in Game Art Tricks and tagged Backgrounds, Homeworld 2 on 15. March 2013 by Simon.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "84af1fd35b76d30e",
    "title": {
      "en": "The Wright brothers invented the airplane, right? Not if you're in Brazil",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.washingtonpost.com/world/2025/03/21/brazil-airplane-wright-brothers-santos-dumont/",
    "score": 41,
    "by": "benbreen",
    "time": 1742608551,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "d3f6e0fbaea8de1a",
    "title": {
      "en": "Spaghetti science: What pasta reveals about the universe",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.bbc.com/future/article/20250319-spaghetti-science-what-pasta-reveals-about-the-universe",
    "score": 38,
    "by": "pseudolus",
    "time": 1742489441,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "ca5bdbe4dfbd282b",
    "title": {
      "en": "Show HN: I Got Tired of Looking Up Words While Reading, So I Built This",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://chromewebstore.google.com/detail/wordship/mdegpebfloegabmllebkomaflfoaojpm",
    "score": 5,
    "by": "yw2365",
    "time": 1742815057,
    "content": "Switch to Chrome to install extensions and themesInstall ChromeWordship5.0(2 ratings)ShareExtensionTools16 usersAdd to Chrome\n\nWordship5.0(2 ratings)ShareExtensionTools16 usersAdd to Chrome\n\nOverviewSelect any text and get its contextual translation. Save it to review later.Ever looked up a word and still had no clue what it actually means?\n\nTraditional dictionaries give you a literal definition, but they miss the bigger pictureâ€”the context. Words change meaning depending on how theyâ€™re used.\n\nThatâ€™s why I built this extension that provides contextual translations for any selected text. Instead of just definitions, you get real-world meaning, making it easier to truly understand any language.\n\nTry it out and never second-guess a translation again! ğŸš€See more\n\n5 out of 52 ratingsGoogle doesn't verify reviews. Learn more about results and reviews.See all reviews\n\nDetailsVersion1.0.9UpdatedMarch 23, 2025Flag concernSize59.5KiBLanguagesEnglishDeveloper Email gw16@codeswitch.usNon-traderThis developer has not identified itself as a trader. For consumers in the European Union, please note that consumer rights do not apply to contracts between you and this developer.\n\nPrivacyThe developer has disclosed that it will not collect or use your data. To learn more, see the developerâ€™s privacy policy.This developer declares that your data isNot being sold to third parties, outside of the approved use casesNot being used or transferred for purposes that are unrelated to the item's core functionalityNot being used or transferred to determine creditworthiness or for lending purposes\n\nRelatedTranslate - Translator, Dictionary, TTS4.5(2.6K)Average rating 4.5 out of 5 stars. 2.6K ratings.Google doesn't verify reviews. Learn more about results and reviews.Accurate translate words, phrases and texts using Google Translate. Full page translator, dictionary and text to speech (TTS).Translater - translation of selected text4.0(8)Average rating 4 out of 5 stars. 8 ratings.Google doesn't verify reviews. Learn more about results and reviews.Translation selected text with Google Translate with light and dark themeTranslator, dictionary - accurate translate4.4(2K)Average rating 4.4 out of 5 stars. 2K ratings.Google doesn't verify reviews. Learn more about results and reviews.Translate text on any webpage instantly using Google Translate. Online Translator, accurate Translation.Multilanguage translator4.7(464)Average rating 4.7 out of 5 stars. 464 ratings.Google doesn't verify reviews. Learn more about results and reviews.Multilanguage translator, translate words, translate phrases. Full page translator and dictionary, text to speech (TTS).Translate for Chrome -Translator, Dictionary4.7(579)Average rating 4.7 out of 5 stars. 579 ratings.Google doesn't verify reviews. Learn more about results and reviews.Translate words, phrases or text on any webpage. Full page translator and dictionary, text to speech (TTS) using Google Translate.Quick Translate: Reading & writing translator4.6(118)Average rating 4.6 out of 5 stars. 118 ratings.Google doesn't verify reviews. Learn more about results and reviews.Translate what you read or write with Google Translate, the best translator in the world.Simple Translator: Instantly Unlock All World Languages!4.7(4.4K)Average rating 4.7 out of 5 stars. 4.4K ratings.Google doesn't verify reviews. Learn more about results and reviews.Translate texts instantly with one click! Fast translator + dictionary for Chrome. Perfect for travel & learning! ğŸš€Lingvanex - Online Text and Voice Translator4.1(254)Average rating 4.1 out of 5 stars. 254 ratings.Google doesn't verify reviews. Learn more about results and reviews.Lingvanex online text and voice translator provides instant translations into the language of your choice. Over 108 languages.Translator4.3(58)Average rating 4.3 out of 5 stars. 58 ratings.Google doesn't verify reviews. Learn more about results and reviews.Translate words and phrases while browsing the web, using Google Translate.Multitran popup4.9(25)Average rating 4.9 out of 5 stars. 25 ratings.Google doesn't verify reviews. Learn more about results and reviews.Translate words and phrases using multitran.com dictionaryGoogle Translate Plus4.6(486)Average rating 4.6 out of 5 stars. 486 ratings.Google doesn't verify reviews. Learn more about results and reviews.Translate the definitions by google translate.Translator4.3(38)Average rating 4.3 out of 5 stars. 38 ratings.Google doesn't verify reviews. Learn more about results and reviews.Translate words and phrases while browsing the web, using Google translator.Translate - Translator, Dictionary, TTS4.5(2.6K)Average rating 4.5 out of 5 stars. 2.6K ratings.Google doesn't verify reviews. Learn more about results and reviews.Accurate translate words, phrases and texts using Google Translate. Full page translator, dictionary and text to speech (TTS).Translater - translation of selected text4.0(8)Average rating 4 out of 5 stars. 8 ratings.Google doesn't verify reviews. Learn more about results and reviews.Translation selected text with Google Translate with light and dark themeTranslator, dictionary - accurate translate4.4(2K)Average rating 4.4 out of 5 stars. 2K ratings.Google doesn't verify reviews. Learn more about results and reviews.Translate text on any webpage instantly using Google Translate. Online Translator, accurate Translation.Multilanguage translator4.7(464)Average rating 4.7 out of 5 stars. 464 ratings.Google doesn't verify reviews. Learn more about results and reviews.Multilanguage translator, translate words, translate phrases. Full page translator and dictionary, text to speech (TTS).Translate for Chrome -Translator, Dictionary4.7(579)Average rating 4.7 out of 5 stars. 579 ratings.Google doesn't verify reviews. Learn more about results and reviews.Translate words, phrases or text on any webpage. Full page translator and dictionary, text to speech (TTS) using Google Translate.Quick Translate: Reading & writing translator4.6(118)Average rating 4.6 out of 5 stars. 118 ratings.Google doesn't verify reviews. Learn more about results and reviews.Translate what you read or write with Google Translate, the best translator in the world.Simple Translator: Instantly Unlock All World Languages!4.7(4.4K)Average rating 4.7 out of 5 stars. 4.4K ratings.Google doesn't verify reviews. Learn more about results and reviews.Translate texts instantly with one click! Fast translator + dictionary for Chrome. Perfect for travel & learning! ğŸš€Lingvanex - Online Text and Voice Translator4.1(254)Average rating 4.1 out of 5 stars. 254 ratings.Google doesn't verify reviews. Learn more about results and reviews.Lingvanex online text and voice translator provides instant translations into the language of your choice. Over 108 languages.\n\nTranslate - Translator, Dictionary, TTS4.5(2.6K)Average rating 4.5 out of 5 stars. 2.6K ratings.Google doesn't verify reviews. Learn more about results and reviews.Accurate translate words, phrases and texts using Google Translate. Full page translator, dictionary and text to speech (TTS).Translater - translation of selected text4.0(8)Average rating 4 out of 5 stars. 8 ratings.Google doesn't verify reviews. Learn more about results and reviews.Translation selected text with Google Translate with light and dark themeTranslator, dictionary - accurate translate4.4(2K)Average rating 4.4 out of 5 stars. 2K ratings.Google doesn't verify reviews. Learn more about results and reviews.Translate text on any webpage instantly using Google Translate. Online Translator, accurate Translation.Multilanguage translator4.7(464)Average rating 4.7 out of 5 stars. 464 ratings.Google doesn't verify reviews. Learn more about results and reviews.Multilanguage translator, translate words, translate phrases. Full page translator and dictionary, text to speech (TTS).Translate for Chrome -Translator, Dictionary4.7(579)Average rating 4.7 out of 5 stars. 579 ratings.Google doesn't verify reviews. Learn more about results and reviews.Translate words, phrases or text on any webpage. Full page translator and dictionary, text to speech (TTS) using Google Translate.Quick Translate: Reading & writing translator4.6(118)Average rating 4.6 out of 5 stars. 118 ratings.Google doesn't verify reviews. Learn more about results and reviews.Translate what you read or write with Google Translate, the best translator in the world.Simple Translator: Instantly Unlock All World Languages!4.7(4.4K)Average rating 4.7 out of 5 stars. 4.4K ratings.Google doesn't verify reviews. Learn more about results and reviews.Translate texts instantly with one click! Fast translator + dictionary for Chrome. Perfect for travel & learning! ğŸš€Lingvanex - Online Text and Voice Translator4.1(254)Average rating 4.1 out of 5 stars. 254 ratings.Google doesn't verify reviews. Learn more about results and reviews.Lingvanex online text and voice translator provides instant translations into the language of your choice. Over 108 languages.Translator4.3(58)Average rating 4.3 out of 5 stars. 58 ratings.Google doesn't verify reviews. Learn more about results and reviews.Translate words and phrases while browsing the web, using Google Translate.Multitran popup4.9(25)Average rating 4.9 out of 5 stars. 25 ratings.Google doesn't verify reviews. Learn more about results and reviews.Translate words and phrases using multitran.com dictionaryGoogle Translate Plus4.6(486)Average rating 4.6 out of 5 stars. 486 ratings.Google doesn't verify reviews. Learn more about results and reviews.Translate the definitions by google translate.Translator4.3(38)Average rating 4.3 out of 5 stars. 38 ratings.Google doesn't verify reviews. Learn more about results and reviews.Translate words and phrases while browsing the web, using Google translator.Translate - Translator, Dictionary, TTS4.5(2.6K)Average rating 4.5 out of 5 stars. 2.6K ratings.Google doesn't verify reviews. Learn more about results and reviews.Accurate translate words, phrases and texts using Google Translate. Full page translator, dictionary and text to speech (TTS).Translater - translation of selected text4.0(8)Average rating 4 out of 5 stars. 8 ratings.Google doesn't verify reviews. Learn more about results and reviews.Translation selected text with Google Translate with light and dark themeTranslator, dictionary - accurate translate4.4(2K)Average rating 4.4 out of 5 stars. 2K ratings.Google doesn't verify reviews. Learn more about results and reviews.Translate text on any webpage instantly using Google Translate. Online Translator, accurate Translation.Multilanguage translator4.7(464)Average rating 4.7 out of 5 stars. 464 ratings.Google doesn't verify reviews. Learn more about results and reviews.Multilanguage translator, translate words, translate phrases. Full page translator and dictionary, text to speech (TTS).Translate for Chrome -Translator, Dictionary4.7(579)Average rating 4.7 out of 5 stars. 579 ratings.Google doesn't verify reviews. Learn more about results and reviews.Translate words, phrases or text on any webpage. Full page translator and dictionary, text to speech (TTS) using Google Translate.Quick Translate: Reading & writing translator4.6(118)Average rating 4.6 out of 5 stars. 118 ratings.Google doesn't verify reviews. Learn more about results and reviews.Translate what you read or write with Google Translate, the best translator in the world.Simple Translator: Instantly Unlock All World Languages!4.7(4.4K)Average rating 4.7 out of 5 stars. 4.4K ratings.Google doesn't verify reviews. Learn more about results and reviews.Translate texts instantly with one click! Fast translator + dictionary for Chrome. Perfect for travel & learning! ğŸš€Lingvanex - Online Text and Voice Translator4.1(254)Average rating 4.1 out of 5 stars. 254 ratings.Google doesn't verify reviews. Learn more about results and reviews.Lingvanex online text and voice translator provides instant translations into the language of your choice. Over 108 languages.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "fb99d9f458752edd",
    "title": {
      "en": "argp: GNU-style command line argument parser for Go",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/tdewolff/argp",
    "score": 163,
    "by": "networked",
    "time": 1742733921,
    "content": "GNU command line argument parser\nCommand line argument parser following the GNU standard.\n./test -vo out.png --size 256 input.txt\n\nwith the following features:\n\nbuild-in help (-h and --help) message\nscan arguments into struct fields with configuration in tags\nscan into composite field types (arrays, slices, structs)\nallow for nested sub commands\n\nGNU command line argument rules:\n\narguments are options when they begin with a hyphen -\nmultiple options can be combined: -abc is the same as -a -b -c\nlong options start with two hyphens: --abc is one option\noption names are alphanumeric characters\noptions can have a value: -a 1 means that a has value 1\noption values can be separated by a space, equal sign, or nothing: -a1 -a=1 -a 1 are all equal\noptions and non-options can be interleaved\nthe argument -- terminates all options so that all following arguments are treated as non-options\na single - argument is a non-option usually used to mean standard in or out streams\noptions may be specified multiple times, only the last one determines its value\noptions can have multiple values: -a 1 2 3 means that a is an array/slice/struct of three numbers of value [1,2,3]\n\nSee also github.com/tdewolff/prompt for a command line prompter.\nInstallation\nMake sure you have Git and Go (1.22 or higher) installed, run\nmkdir Project\ncd Project\ngo mod init\ngo get -u github.com/tdewolff/argp\n\nThen add the following import\nimport (\n    \"github.com/tdewolff/argp\"\n)\n\nExamples\nDefault usage\nA regular command with short and long options.\nSee cmd/test/main.go.\npackage main\n\nimport \"github.com/tdewolff/argp\"\n\nfunc main() {\n    var verbose int\n    var input string\n    var output string\n    var files []string\n    size := 512 // default value\n\n    cmd := argp.New(\"CLI tool description\")\n    cmd.AddOpt(argp.Count{&verbose}, \"v\", \"verbose\", \"Increase verbosity, eg. -vvv\")\n    cmd.AddOpt(&output, \"o\", \"output\", \"Output file name\")\n    cmd.AddOpt(&size, \"\", \"size\", \"Image size\")\n    cmd.AddArg(&input, \"input\", \"Input file name\")\n    cmd.AddRest(&files, \"files\", \"Additional files\")\n    cmd.Parse()\n\n    // ...\n}\n\nwith help output\nUsage: test [options] input files...\n\nOptions:\n  -h, --help          Help\n  -o, --output string Output file name\n      --size=512 int  Image size\n  -v, --verbose int   Increase verbosity, eg. -vvv\n\nArguments:\n  input     Input file name\n  files     Additional files\n\nSub commands\nExample with sub commands using a main command for when no sub command is used, and a sub command named \"cmd\". For the main command we can also use New and AddOpt instead and process the command after argp.Parse().\npackage main\n\nimport \"github.com/tdewolff/argp\"\n\nfunc main() {\n    cmd := argp.NewCmd(&Main{}, \"CLI tool description\")\n    cmd.AddCmd(&Command{}, \"cmd\", \"Sub command\")\n    cmd.Parse()\n}\n\ntype Main struct {\n    Version bool `short:\"v\"`\n}\n\nfunc (cmd *Main) Run() error {\n    // ...\n}\n\ntype Command struct {\n    Verbose bool `short:\"v\" name:\"\"`\n    Output string `short:\"o\" desc:\"Output file name\"`\n    Size int `default:\"512\" desc:\"Image size\"`\n}\n\nfunc (cmd *Command) Run() error {\n    // ...\n}\n\nArguments\nvar input string\ncmd.AddArg(&input, \"input\", \"Input file name\")\n\nvar files []string\ncmd.AddRest(&files, \"files\", \"Additional input files\")\n\nOptions\nBasic types\nvar v string = \"default\"\ncmd.AddOpt(&v, \"v\", \"var\", \"description\")\n\nvar v bool = true\ncmd.AddOpt(&v, \"v\", \"var\", \"description\")\n\nvar v int = 42 // also: int8, int16, int32, int64\ncmd.AddOpt(&v, \"v\", \"var\", \"description\")\n\nvar v uint = 42 // also: uint8, uint16, uint32, uint64\ncmd.AddOpt(&v, \"v\", \"var\", \"description\")\n\nvar v float64 = 4.2 // also: float32\ncmd.AddOpt(&v, \"v\", \"var\", \"description\")\n\nComposite types\nv := [2]int{4, 2} // element can be any valid basic or composite type\ncmd.AddOpt(&v, \"v\", \"var\", \"description\")\n// --var [4 2]  =>  [2]int{4, 2}\n// or: --var 4,2  =>  [2]int{4, 2}\n\nv := []int{4, 2, 1} // element can be any valid basic or composite type\ncmd.AddOpt(&v, \"v\", \"var\", \"description\")\n// --var [4 2 1]  =>  []int{4, 2, 1}\n// or: --var 4,2,1  =>  []int{4, 2, 1}\n\nv := map[int]string{1:\"one\", 2:\"two\"} // key and value can be any valid basic or composite type\ncmd.AddOpt(&v, \"v\", \"var\", \"description\")\n// --var {1:one 2:two}  =>  map[int]string{1:\"one\", 2:\"two\"}\n\nv := struct { // fields can be any valid basic or composite type\n    S string\n    I int\n    B [2]bool\n}{\"string\", 42, [2]bool{0, 1}}\ncmd.AddOpt(&v, \"v\", \"var\", \"description\")\n// --var {string 42 [0 1]}  =>  struct{S string, I int, B [2]bool}{\"string\", 42, false, true}\n\nCount\nCount the number of time a flag has been passed.\nvar c int\ncmd.AddOpt(argp.Count{&c}, \"c\", \"count\", \"Count\")\n// Count the number of times flag is present\n// -c -c / -cc / --count --count  =>  2\n// or: -c 5  =>  5\n\nAppend\nAppend each flag to a list.\nvar v []int\ncmd.AddOpt(argp.Append{&v}, \"v\", \"value\", \"Values\")\n// Append values for each flag\n// -v 1 -v 2  =>  [1 2]\n\nConfig\nLoad all arguments from a configuration file. Currently only TOML is supported.\ncmd.AddOpt(&argp.Config{cmd, \"config.toml\"}, \"\", \"config\", \"Configuration file\")\n\nList\nUse a list source specified as type:list. Default supported types are: inline.\n\nInline takes a []string, e.g. inline:[foo bar]\n\nlist := argp.NewList(il)\ndefer list.Close()\n\ncmd.AddOpt(&list, \"\", \"list\", \"List\")\n\nYou can add a MySQL source:\ntype mysqlList struct {\n\tHosts    string\n\tUser     string\n\tPassword string\n\tDbname   string\n\tQuery    string\n}\n\nfunc newMySQLList(s []string) (argp.ListSource, error) {\n\tif len(s) != 1 {\n\t\treturn nil, fmt.Errorf(\"invalid path\")\n\t}\n\n\tt := mysqlList{}\n\tif err := argp.LoadConfigFile(&t, s[0]); err != nil {\n\t\treturn nil, err\n\t}\n\n\turi := fmt.Sprintf(\"%s:%s@%s/%s\", t.User, t.Password, t.Hosts, t.Dbname)\n\tdb, err := sqlx.Open(\"mysql\", uri)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdb.SetConnMaxLifetime(time.Minute)\n\tdb.SetConnMaxIdleTime(time.Minute)\n\tdb.SetMaxOpenConns(10)\n\tdb.SetMaxIdleConns(10)\n\treturn argp.NewSQLList(db, t.Query, \"\")\n}\n\n// ...\nlist.AddSource(\"mysql\", newMySQLList)\n// ...\n\nUse as ./bin -list mysql:list-config.toml.\nDict\nUse a dict source specified as type:dict. Default supported types are: static and inline.\n\nStatic takes a string and will return that as a value for all keys, e.g. static:foobar\nInline takes a map[string]string, e.g. inline:{foo:1 bar:2}\n\ndict := argp.NewDict([]string{\"static:value\"})\ndefer dict.Close()\n\ncmd.AddOpt(&dict, \"\", \"dict\", \"Dict\")\n\nYou can add custom sources must like the mysqlList example above.\nOption tags\nThe following struct will accept the following options and arguments:\n\n-v or --var with a default value of 42\nThe first argument called first with a default value of 4.2\nThe other arguments called rest\n\ntype Command struct {\n    Var1 int `short:\"v\" name:\"var\" default:\"42\" desc:\"Description\"`\n    Var2 float64 `name:\"first\" index:\"0\" default:\"4.2\"`\n    Var3 []string `name:\"rest\" index:\"*\"`\n}\n\nfunc (cmd *Command) Run() error {\n    // run command\n    return nil\n}\n\nLicense\nReleased under the MIT license.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "616488f05e0e9d76",
    "title": {
      "en": "Show HN: My iOS app to practice sight reading (10 years in the App Store)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://apps.apple.com/us/app/notes-sight-reading-trainer/id874386416",
    "score": 211,
    "by": "rooster117",
    "time": 1742765108,
    "content": ".ember3, #ember3::before {\n           width: 246px;\n           height: 246px;\n         }\n         .ember3::before {\n           padding-top: 100%;\n         }\n@media (min-width: 735px) {\n           .ember3, #ember3::before {\n           width: 217px;\n           height: 217px;\n         }\n         .ember3::before {\n           padding-top: 100%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember3, #ember3::before {\n           width: 230px;\n           height: 230px;\n         }\n         .ember3::before {\n           padding-top: 100%;\n         }\n         }\n\n          Notes - Sight Reading Trainer\n            4+\n\n            Piano sheet music\n\n            Ryan Newsome\n\n              Designed for iPad\n\n                      #169 in Education\n\n    4.8 â€¢ 2.7K Ratings\n\n                Free\n\n                Offers In-App Purchases\n\nScreenshots\n\n                  iPad\n\n                  iPhone\n\n    .ember6, #ember6::before {\n           width: 643px;\n           height: 857px;\n         }\n         .ember6::before {\n           padding-top: 133.28149300155522%;\n         }\n@media (min-width: 735px) {\n           .ember6, #ember6::before {\n           width: 217px;\n           height: 289px;\n         }\n         .ember6::before {\n           padding-top: 133.17972350230414%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember6, #ember6::before {\n           width: 313px;\n           height: 417px;\n         }\n         .ember6::before {\n           padding-top: 133.22683706070288%;\n         }\n         }\n\n    .ember7, #ember7::before {\n           width: 643px;\n           height: 857px;\n         }\n         .ember7::before {\n           padding-top: 133.28149300155522%;\n         }\n@media (min-width: 735px) {\n           .ember7, #ember7::before {\n           width: 217px;\n           height: 289px;\n         }\n         .ember7::before {\n           padding-top: 133.17972350230414%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember7, #ember7::before {\n           width: 313px;\n           height: 417px;\n         }\n         .ember7::before {\n           padding-top: 133.22683706070288%;\n         }\n         }\n\n    .ember8, #ember8::before {\n           width: 643px;\n           height: 857px;\n         }\n         .ember8::before {\n           padding-top: 133.28149300155522%;\n         }\n@media (min-width: 735px) {\n           .ember8, #ember8::before {\n           width: 217px;\n           height: 289px;\n         }\n         .ember8::before {\n           padding-top: 133.17972350230414%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember8, #ember8::before {\n           width: 313px;\n           height: 417px;\n         }\n         .ember8::before {\n           padding-top: 133.22683706070288%;\n         }\n         }\n\n    .ember9, #ember9::before {\n           width: 643px;\n           height: 857px;\n         }\n         .ember9::before {\n           padding-top: 133.28149300155522%;\n         }\n@media (min-width: 735px) {\n           .ember9, #ember9::before {\n           width: 217px;\n           height: 289px;\n         }\n         .ember9::before {\n           padding-top: 133.17972350230414%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember9, #ember9::before {\n           width: 313px;\n           height: 417px;\n         }\n         .ember9::before {\n           padding-top: 133.22683706070288%;\n         }\n         }\n\n    .ember10, #ember10::before {\n           width: 643px;\n           height: 857px;\n         }\n         .ember10::before {\n           padding-top: 133.28149300155522%;\n         }\n@media (min-width: 735px) {\n           .ember10, #ember10::before {\n           width: 217px;\n           height: 289px;\n         }\n         .ember10::before {\n           padding-top: 133.17972350230414%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember10, #ember10::before {\n           width: 313px;\n           height: 417px;\n         }\n         .ember10::before {\n           padding-top: 133.22683706070288%;\n         }\n         }\n\n    .ember11, #ember11::before {\n           width: 643px;\n           height: 857px;\n         }\n         .ember11::before {\n           padding-top: 133.28149300155522%;\n         }\n@media (min-width: 735px) {\n           .ember11, #ember11::before {\n           width: 217px;\n           height: 289px;\n         }\n         .ember11::before {\n           padding-top: 133.17972350230414%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember11, #ember11::before {\n           width: 313px;\n           height: 417px;\n         }\n         .ember11::before {\n           padding-top: 133.22683706070288%;\n         }\n         }\n\nDescription\n\n  Introducing Notes - Sight Reading Trainer, the ultimate iOS app for mastering sight reading in music! Whether you're a beginner or an experienced musician, Notes is your is your tool to become the musician you were meant to be. When you are ready to learn your instrument and music rather than just learning a song this app is for you.\n\nKey Features:\nEngaging Exercises: Explore a variety of interactive exercises designed to strengthen your sight reading abilities with a piano interface.\n\nWays to use the app: Connect with Midi, use the on screen keyboard or your devices microphone for input.\n\nCustomizable Difficulty: Tailor the difficulty level to match your proficiency, allowing you to progress at your own pace and challenge yourself when ready.\n\nComprehensive Music Notation: Learn to read sheet music, decipher key signatures, note durations and augmentations (sharp/flat) with ease.\n\nProgress Tracking: Keep track of your improvements over time with detailed performance analytics and history.\n\nPractice with songs: This app is focused on randomized practice but also has a handful of songs to practice your developing skills with. More to be added soon!\n\nGoal Setting: Set achievable note goals and receive reminders to maintain consistency in your practice.\n\nUnlock the world of sight reading and elevate your musical journey with Notes - Sight Reading Trainer. Download the app now and get serious about learning music!\n\n        more\n\nWhatâ€™s New\n\n    Version History\n\n            Feb 26, 2025\n            Version 7.3\n\n  Fix to allow using midi inputs and microphone for the keyboarding learning lessons\n\nRatings and Reviews\n\n        See All\n\n      4.8 out of 5\n        2.7K Ratings\n\n        2.7K Ratings\n\n    Quality reviewer\n\n    ,\n\n    04/22/2023\n\n    Great app!\n\n  This is one of the best apps Iâ€™ve used to study notes. I only sing but Iâ€™m trying to learn notes and this is fantastic. You will learn how to use a piano in the process, which is key to making sure you learn to sing in tune. My only objection is that the lessons do NOT include training on sharps & flats. I was hoping the in app purchase for additional lessons (happens after youâ€™ve learned quite a lot, no worries) included this, and thereâ€™s no info given on what these lessons contain, only that â€œmore will be addedâ€. The practice mode thankfully does allow you to train flats but not in an organized, piece by piece way like the lessons do. But unfortunately you cannot see flats and sharps labeled on the keys like the other ones.\n\nI have to knock off a starâ€¦ sharps and flats are a big part of the music that I do and so this app didnâ€™t teach me everything I wanted to know.\n\n        more\n\n    FPPMEDIA\n\n    ,\n\n    01/18/2018\n\n    Thank you for educational app.\n\n  This app and the ease of use has introduced a whole new world to me.\nI never thought I would understand how to play a piano.. My life has been filled with homelessness and bad fortune, so I learned a long time ago that music had a medicinal effect on me.. A personal escape of sorts... I always wanted to understand how to make beautiful sounds with a piano, but I donâ€™t learn so well.. This app made it easy for me to understand the keyboard layout and knowing what each note is from each key.. It seems like this app will help me learn how to write songs and that will make life so happy for me.\nIâ€™m not trying to be sad here.. I just wanted to explain how much this app can help someone like me. I canâ€™t afford the full version but this version, I appreciate none the less.\nâ€œMusic may have to save the world somedayâ€.\n\n        more\n\n    Churple\n\n    ,\n\n    03/26/2018\n\n    Absolutely Stunning\n\n  I rarely write reviews, but this app is so beyond amazing Iâ€™ll try my best to share my thoughts. First of all, everything is absolutely FREE, NO premium that you have to buy to basically unlock everything, I feel the developers of this app were truly trying to help instead of take all of your money. Secondly, Iâ€™ve had this app for one day and Iâ€™ve already learned more than I did searching for weeks for a good app thatâ€™s free. Iâ€™ve started to be able to play simple songs on my keyboard, like Jingle Bells, completely looking at the sheet music. I almost gave up on trying to learn how to read sheet music, but this has given me a great start. Sorry for bad grammar.\n\n        more\n\nApp Privacy\n\n    See Details\n\n    The developer, Ryan Newsome, indicated that the appâ€™s privacy practices may include handling of data as described below. For more information, see the developerâ€™s privacy policy.\n\n        Data Not Linked to You\n        The following data may be collected but it is not linked to your identity:\n\n                  Usage Data\n\n                  Diagnostics\n\n    Privacy practices may vary, for example, based on the features you use or your age. LearnMore\n\nInformation\n\n          Seller\n\n              Ryan Newsome\n\n          Size\n          32 MB\n\n          Category\n\n                Education\n\n        Compatibility\n\n                  iPhone\n\n                Requires iOS 14.0 or later.\n\n                  iPad\n\n                Requires iPadOS 14.0 or later.\n\n                  iPodtouch\n\n                Requires iOS 14.0 or later.\n\n                  Mac\n\n                Requires macOS11.0 or later and a Mac with AppleM1chip or later.\n\n                  Apple Vision\n\n                Requires visionOS 1.0 or later.\n\n        Languages\n\n  English, French, German, Italian, Polish, Simplified Chinese, Spanish, Traditional Chinese\n\n          Age Rating\n\n              4+\n\n        Copyright\n        Â© Ryan Newsome\n\n          Price\n          Free\n\n          In-App Purchases\n\n                  Premium\n                  $3.99\n\n            App Support\n\n            Privacy Policy\n\nApp Support\n\n          Privacy Policy\n\nMore By This Developer\n\n    .ember21, #ember21::before {\n           width: 320px;\n           height: 320px;\n         }\n         .ember21::before {\n           padding-top: 100%;\n         }\n@media (min-width: 735px) {\n           .ember21, #ember21::before {\n           width: 157px;\n           height: 157px;\n         }\n         .ember21::before {\n           padding-top: 100%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember21, #ember21::before {\n           width: 146px;\n           height: 146px;\n         }\n         .ember21::before {\n           padding-top: 100%;\n         }\n         }\n\n        Violin Sight Reading Trainer\n\n    Music\n\nYou Might Also Like\n\n        See All\n\n    .ember23, #ember23::before {\n           width: 320px;\n           height: 320px;\n         }\n         .ember23::before {\n           padding-top: 100%;\n         }\n@media (min-width: 735px) {\n           .ember23, #ember23::before {\n           width: 157px;\n           height: 157px;\n         }\n         .ember23::before {\n           padding-top: 100%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember23, #ember23::before {\n           width: 146px;\n           height: 146px;\n         }\n         .ember23::before {\n           padding-top: 100%;\n         }\n         }\n\n        Learn Music Notes Sight Read\n\n    Education\n\n    .ember24, #ember24::before {\n           width: 320px;\n           height: 320px;\n         }\n         .ember24::before {\n           padding-top: 100%;\n         }\n@media (min-width: 735px) {\n           .ember24, #ember24::before {\n           width: 157px;\n           height: 157px;\n         }\n         .ember24::before {\n           padding-top: 100%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember24, #ember24::before {\n           width: 146px;\n           height: 146px;\n         }\n         .ember24::before {\n           padding-top: 100%;\n         }\n         }\n\n        MyMusicTheory - Music Theory\n\n    Education\n\n    .ember25, #ember25::before {\n           width: 320px;\n           height: 320px;\n         }\n         .ember25::before {\n           padding-top: 100%;\n         }\n@media (min-width: 735px) {\n           .ember25, #ember25::before {\n           width: 157px;\n           height: 157px;\n         }\n         .ember25::before {\n           padding-top: 100%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember25, #ember25::before {\n           width: 146px;\n           height: 146px;\n         }\n         .ember25::before {\n           padding-top: 100%;\n         }\n         }\n\n        Piano Marvel - Lessons & Music\n\n    Education\n\n    .ember26, #ember26::before {\n           width: 320px;\n           height: 320px;\n         }\n         .ember26::before {\n           padding-top: 100%;\n         }\n@media (min-width: 735px) {\n           .ember26, #ember26::before {\n           width: 157px;\n           height: 157px;\n         }\n         .ember26::before {\n           padding-top: 100%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember26, #ember26::before {\n           width: 146px;\n           height: 146px;\n         }\n         .ember26::before {\n           padding-top: 100%;\n         }\n         }\n\n        Perfect Ear - Ear Trainer\n\n    Education\n\n    .ember27, #ember27::before {\n           width: 320px;\n           height: 320px;\n         }\n         .ember27::before {\n           padding-top: 100%;\n         }\n@media (min-width: 735px) {\n           .ember27, #ember27::before {\n           width: 157px;\n           height: 157px;\n         }\n         .ember27::before {\n           padding-top: 100%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember27, #ember27::before {\n           width: 146px;\n           height: 146px;\n         }\n         .ember27::before {\n           padding-top: 100%;\n         }\n         }\n\n        Sheet Music Reader Halbestunde\n\n    Education\n\n    .ember28, #ember28::before {\n           width: 320px;\n           height: 320px;\n         }\n         .ember28::before {\n           padding-top: 100%;\n         }\n@media (min-width: 735px) {\n           .ember28, #ember28::before {\n           width: 157px;\n           height: 157px;\n         }\n         .ember28::before {\n           padding-top: 100%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember28, #ember28::before {\n           width: 146px;\n           height: 146px;\n         }\n         .ember28::before {\n           padding-top: 100%;\n         }\n         }\n\n        Skoove: Learn to Play Piano\n\n    Education",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "9e4f0badd03a1ead",
    "title": {
      "en": "Shift-to-Middle Array: A Faster Alternative to Std:Deque?",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/attilatorda/Shift-To-Middle_Array",
    "score": 91,
    "by": "AttilaT",
    "time": 1742772027,
    "content": "Shift-To-Middle Array\nThe Shift-To-Middle Array is a dynamic array designed to optimize insertions and deletions at both ends, offering a high-performance alternative to std::deque, std::vector, and linked lists. It achieves this while maintaining contiguous memory storage, improving cache locality and enabling efficient parallel processing.\n\nğŸŒŸ Features\nâœ… Amortized O(1) insertions & deletions at both ends\nâœ… Fast random access (O(1))\nâœ… Better cache locality than linked lists\nâœ… Supports SIMD & parallel optimizations\nâœ… Efficient memory usage compared to std::deque\nğŸ“Œ How It Works\nUnlike std::deque, which uses a fragmented block structure, the Shift-To-Middle Array dynamically redistributes space to avoid costly shifts. When resizing, elements are moved toward the middle, ensuring efficient insertions at both ends without excessive copying.\nğŸš€ Time Complexity Comparison\nThe following table compares the time complexity of Shift-To-Middle Array operations with other common data structures:\n\nOperation\nArrayList (std::vector)\nLinked List\nShift-To-Middle Array\n\nAccess (by index)\nO(1)\nO(n)\nO(1)\n\nInsertion at head\nO(n)\nO(1)\nO(1) amortized\n\nInsertion at tail\nO(1) amortized\nO(1)\nO(1) amortized\n\nInsertion in middle\nO(n)\nO(n)\nO(n)\n\nDeletion at head\nO(n)\nO(1)\nO(1) amortized\n\nDeletion at tail\nO(1)\nO(1)\nO(1) amortized\n\nDeletion in middle\nO(n)\nO(n)\nO(n)\n\nCache Locality\nExcellent\nPoor\nExcellent\n\nğŸ† Performance Benchmarks\nBenchmarks comparing Shift-To-Middle Array vs. std::deque vs. ExpandingRingBuffer vs. std::queue demonstrate that performance improvements depend on CPU and GPU capabilities, such as multi-core parallelism, SIMD optimizations, and cache efficiency.\nThe benchmarks were compiled using GCC with the -O3 optimization flag, ensuring high-performance execution. Results vary based on hardware specifications and workload characteristics.\nğŸ“‚ Installation & Usage\nTo use Shift-To-Middle Array in your project:\n#include \"ShiftToMiddleArray.h\"\nShiftToMiddleArray<int> stmArray;\nstmArray.insert_head(42);\nstmArray.insert_tail(99);\nint value = stmArray.get_head();\nstmArray.remove_head();\n\nğŸ”¬ When To Use\n\nHigh-performance queue structures\nGame engines & real-time applications\nNetworking (packet buffering, event queues)\nDynamic sequences in computational geometry & physics\n\nğŸ“– Documentation\nRunning Java Benchmarks\nTo run the Java benchmarks, ensure you have the Trove library installed. Compile and execute using:\njavac -cp trove-3.0.3.jar; ShiftToMiddleArrayBenchmarkTrove.java\njava -cp trove-3.0.3.jar; ShiftToMiddleArrayBenchmarkTrove\n\nFull API reference and benchmarks are available in the Wiki!\nğŸ“Š Benchmarks & Results\nFor full benchmark details, check out the benchmarks report. The provided Python scripts can be used to visualize performance metrics from CSV benchmark results.\nğŸ› History\nThe Shift-To-Middle Array was developed as part of an effort to create a more efficient implementation strategy for lists and deques. Traditional data structures, such as std::deque and linked lists, suffer from poor cache locality or fragmented memory allocations, leading to inefficiencies. By leveraging contiguous memory, dynamic mid-shifting, and modern CPU optimizations, Shift-To-Middle Array provides a balanced solution for insertion, deletion, and access performance.\nğŸ“œ License\nThis project is licensed under the MIT License.\nğŸ¤ Contributing\nContributions are welcome! Feel free to open an issue or pull request.\nğŸš€ Try Shift-To-Middle Array today and optimize your data structures!",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "87b9e4639e85eaba",
    "title": {
      "en": "Ask HN: Do programmers generally enjoy some amount of complexity?",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": null,
    "score": 12,
    "by": "keepamovin",
    "time": 1742807758,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "d1858b63d26df576",
    "title": {
      "en": "The Worst Programmer I Know (2023)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://dannorth.net/the-worst-programmer/",
    "score": 440,
    "by": "rbanffy",
    "time": 1742735325,
    "content": "We can help your organisation to go faster â€” ask us how",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "f6eb26c0893ff9a4",
    "title": {
      "en": "Euclid opens data treasure trove, offers glimpse of deep fields",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.esa.int/Science_Exploration/Space_Science/Euclid/Euclid_opens_data_treasure_trove_offers_glimpse_of_deep_fields",
    "score": 98,
    "by": "layer8",
    "time": 1742490431,
    "content": "Science & Exploration\n\tEuclid opens data treasure trove, offers glimpse of deep fields\n\n\t\t\t\t\t\t19/03/2025\n\t\t\t\t53918 views\n\t\t\t\t\t\t\t\t\t\t148 likes\n\nESA /\t\t\tScience & Exploration /\t\t\tSpace Science /\t\t\tEuclid\n\n\tOn 19 March 2025, the European Space Agencyâ€™s Euclid mission released its first batch of survey data, including a preview of its deep fields. Here, hundreds of thousands of galaxies in different shapes and sizes take centre stage and show a glimpse of their large-scale organisation in the cosmic web.\n\n\t\t\tEuclid is back â€“ 26 million galaxies and counting\n\t\t\tAccess the video\n\n\t\tCovering a huge area of the sky in three mosaics, the data release also includes numerous galaxy clusters, active galactic nuclei and transient phenomena, as well as the first classification survey of more than 380 000 galaxies and 500 gravitational lens candidates compiled through combined artificial intelligence and citizen science efforts. All of this sets the scene for the broad range of topics that the dark Universe detective Euclid is set to address with its rich dataset.â€œEuclid shows itself once again to be the ultimate discovery machine. It is surveying galaxies on the grandest scale, enabling us to explore our cosmic history and the invisible forces shaping our Universe,â€ says ESAâ€™s Director of Science, Prof. Carole Mundell.â€œWith the release of the first data from Euclidâ€™s survey, we are unlocking a treasure trove of information for scientists to dive into and tackle some of the most intriguing questions in modern science. With this, ESA is delivering on its commitment to enable scientific progress for generations to come.â€\n\n\t\t\t\t\t\t\t\tEuclid Deep Field North â€“ preview\n\n\t\t\t\t\t\t\t\tEuclid Deep Field Fornax â€“ preview\n\n\t\t\t\t\t\t\t\tEuclid Deep Field South â€“ preview\n\n\t\t\t123PreviousNext\n\n\t\t\t\t\t\t\tLocation of Euclid Deep Fields on Gaia and Planck sky map\n\n\t\tEuclid has scouted out the three areas in the sky where it will eventually provide the deepest observations of its mission. In just one week of observations, with one scan of each region so far, Euclid already spotted 26 million galaxies. The farthest of those are up to 10.5 billion light-years away.The fields also contain a small population of bright quasars that can be seen much farther away. In the coming years, Euclid will pass over these three regions tens of times, capturing many more faraway galaxies, making these fields truly â€˜deepâ€™ by the end of the nominal mission in 2030.But the first glimpse of 63 square degrees of the sky, the equivalent area of more than 300 times the full Moon, already gives an impressive preview of the scale of Euclidâ€™s grand cosmic atlas when the mission is complete. This atlas will cover one-third of the entire sky â€“ 14 000 square degrees â€“ in this high-quality detail.\n\n\tTracing out the cosmic web in Euclidâ€™s deep fields\n\n\t\t\t\t\t\t\tEuclid Deep Field South, 16x zoom\n\n\t\tâ€œItâ€™s impressive how one observation of the deep field areas has already given us a wealth of data that can be used for a variety of purposes in astronomy: from galaxy shapes, to strong lenses, clusters, and star formation, among others,â€ says Valeria Pettorino, ESAâ€™s Euclid project scientist. â€œWe will observe each deep field between 30 and 52 times over Euclidâ€™s six year mission, each time improving the resolution of how we see those areas, and the number of objects we manage to observe. Just think of the discoveries that await us.â€\n\n\t\t\t\t\t\t\tEuclid Deep Field South, 70x zoom\n\n\t\tTo answerthe mysteries it is designed for, Euclid measures the huge variety of shapes and the distribution of billions of galaxies very precisely with its high-resolution imagingvisible instrument (VIS), while its near-infrared instrument (NISP)is essential for unravelling galaxy distances and masses. The new images already showcase this capability for hundreds of thousands of galaxies, and start to hint at the large-scale organisation of these galaxies in thecosmic web. These filaments of ordinary matter and dark matter weave through the cosmos, and from these, galaxies formed and evolved. This is an essential piece in the puzzle towards understanding themysterious nature of dark matter and dark energy, which together appear to make up 95% of the Universe.â€œThe full potential of Euclid to learn more about dark matter and dark energy from the large-scale structure of the cosmic web will be reached only when it has completed its entire survey. Yet the volume of this first data release already offers us a unique first glance at the large-scale organisation of galaxies, which we can use to learn more about galaxy formation over time,\" says Clotilde Laigle, Euclid Consortium scientist and data processing expert based at the Institut d'Astrophysique de Paris, France.\n\n\t\t\t\t\t\t\tEuclidâ€™s view of the Catâ€™s Eye Nebula\n\n\tHumans and AI classify more than 380 000 galaxies\n\n\t\tEuclid is expected to capture images of more than 1.5 billion galaxies over six years, sending back around 100 GB of data every day. Such an impressively large dataset creates incredible discovery opportunities, but huge challenges when it comes to searching for, analysing and cataloguing galaxies. The advancement of artificial intelligence (AI) algorithms, in combination with thousands of human citizen science volunteers and experts, is playing a critical role.â€œWeâ€™re at a pivotal moment in terms of how we tackle large-scale surveys in astronomy. AI is a fundamental and necessary part of our process in order to fully exploit Euclidâ€™s vast dataset,â€ says Mike Walmsley, Euclid Consortium scientist based at the University of Toronto, Canada, who has been heavily involved in astronomical deep learning algorithms for the last decade.â€œWeâ€™re building the tools as well as providing the measurements. In this way we can deliver cutting-edge science in a matter of weeks, compared with the years-long process of analysing big surveys like these in the past,â€ he adds.A major milestone in this effort is the first detailed catalogue of more than 380 000 galaxies, which have been classified according to features such as spiral arms, central bars, and tidal tails that infer merging galaxies. The catalogue is created by the â€˜Zoobotâ€™ AI algorithm.During an intensive one-month campaign on Galaxy Zoo last year, 9976 human volunteers worked together to teach Zoobot to recognise galaxy features by classifying Euclid images.\n\n\t\t\t\t\t\t\tGalaxies in different shapes captured by Euclid\n\n\t\tThis first catalogue released today represents just 0.4% of the total number of galaxies of similar resolution expected to be imaged over Euclidâ€™s lifetime. The final catalogue will present the detailed morphology of at least an order of magnitude more galaxies than ever measured before, helping scientists answer questions like how spiral arms form and how supermassive black holes grow.â€œWeâ€™re looking at galaxies from inside to out, from how their internal structures govern their evolution to how the external environment shapes their transformation over time,â€ adds Clotilde.â€œEuclid is a goldmine of data and its impact will be far-reaching, from galaxy evolution to the bigger-picture cosmology goals of the mission.â€\n\n\tGravitational lensing discovery engine\n\n\t\tLight travelling towards us from distant galaxies is bent and distorted by normal and dark matter in the foreground. This effect is calledgravitational lensingand it is one of the tools that Euclid uses to reveal how dark matter is distributed through the Universe.When the distortions are very apparent, it is known as â€˜strong lensingâ€™, which can result in features such asEinstein rings, arcs, and multiple imaged lenses.Using an initial sweep by AI models, followed by citizen science inspection, expert vetting and modelling, a first catalogue of 500 galaxy-galaxy strong lens candidates is released today, almost all of which were previously unknown.This type of lensing happens when a foreground galaxy and its halo of dark matter act as a lens, distorting the image of a background galaxy along the line of sight towards Euclid.\n\n\t\t\t\t\t\t\tStrong gravitational lenses captured by Euclid\n\n\t\tWith the help of these models, Euclid will capture some 7000 candidates in the major cosmology data release planned for the end of 2026, and in the order of 100 000 galaxy-galaxy strong lenses by the end of the mission, around 100 times more than currently known.Euclid will also be able to measureâ€˜weakâ€™ lensing, when the distortions of background sources are much smaller. Such subtle distortions can only be detected by analysing large numbers of galaxies in a statistical way. In the coming years, Euclid will measure the distorted shapes of billions of galaxies over 10 billion years of cosmic history, thus providing a 3D view of the distribution of dark matter in our Universe.â€œEuclid is very quickly covering larger and larger areas of the sky thanks to its unprecedented surveying capabilities,â€ says Pierre Ferruit, ESAâ€™s Euclid mission manager, who is based at ESAâ€™sEuropean Space Astronomy Centre (ESAC)in Spain, home of theAstronomy Science Archivewhere Euclidâ€™s data will be made available.â€œThis data release highlights the incredible potential we have by combining the strengths of Euclid, AI, citizen science and experts into a single discovery engine that will be essential in tackling the vast volume of data returned by Euclid.â€Notes to editorsAs of 19 March 2025, Euclid has observed about 2000 square degrees, approximately 14% of the total survey area (14 000 square degrees). The three deep fields together comprise 63.1 square degrees.Euclid â€˜quickâ€™ releases, such as the one of 19 March, are of selected areas, intended to demonstrate the data products to be expected in the major data releases that follow, and to allow scientists to sharpen their data analysis tools in preparation. The missionâ€™s first cosmology data will be released to the community in October 2026. Data accumulated over additional, multiple passes of the deep field locations will be included in the 2026 release.The three deep field previews can now be explored in ESASky from 19 March 12:00 CET onwards:\n\nEuclid Deep Field South\n\nEuclid Deep Field Fornax\n\nEuclid Deep Field North\n\nThe data release of 19 March 2025 is described in multiple scientific papers which have not yet been through the peer-review process, but which will be submitted to the journal Astronomy & Astrophysics. A preprint of the papers is available here from 19 March 12:00 CET.Find more detailed information about the data release here.About EuclidEuclid was launched in July 2023 and started its routine science observations on 14 February 2024. In November 2023 and May 2024, the world got its first glimpses of the quality of Euclidâ€™s images, and in October 2024 the first piece of its great map of the Universe was released.Euclid is a European mission, built and operated by ESA, with contributions from its Member States and NASA. The Euclid Consortium â€“ consisting of more than 2000 scientists from 300 institutes in 15 European countries, the USA, Canada and Japan â€“ is responsible for providing the scientific instruments and scientific data analysis. ESA selected Thales Alenia Space as prime contractor for the construction of the satellite and its service module, with Airbus Defence and Space chosen to develop the payload module, including the telescope. NASA provided the detectors of the Near-Infrared Spectrometer and Photometer, NISP. Euclid is a medium-class mission in ESAâ€™s Cosmic Vision Programme.Contact\nESA Media relationsmedia@esa.int\n\nLike\nThank you for liking\nYou have already liked this page, you can only like it once!",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "b01e1938a4e4ad6a",
    "title": {
      "en": "Andromeda XXXV: The Faintest Dwarf Satellite of the Andromeda Galaxy",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://iopscience.iop.org/article/10.3847/2041-8213/adb433",
    "score": 38,
    "by": "PaulHoule",
    "time": 1742449604,
    "content": "The American Astronomical Society (AAS), established in 1899 and based in Washington, DC, is the major organization of professional astronomers in North America. Its membership of about 7,000 individuals also includes physicists, mathematicians, geologists, engineers, and others whose research and educational interests lie within the broad spectrum of subjects comprising contemporary astronomy. The mission of the AAS is to enhance and share humanity's scientific understanding of the universe.\n\n                The Institute of Physics (IOP) is a leading scientific society promoting physics and bringing physicists together for the benefit of all. It has a worldwide membership of around 50 000 comprising physicists from all sectors, as well as those with an interest in physics. It works to advance physics research, application and education; and engages with policy makers and the public to develop awareness and understanding of physics. Its publishing company, IOP Publishing, is a world leader in professional scientific communications.\n            A publishing partnershipThe following article is Open accessAndromeda XXXV: The Faintest Dwarf Satellite of the Andromeda GalaxyJose Marco Arias, Eric F. Bell, Katya Gozman, In Sung Jang, Saxon Stockton, Oleg Y. Gnedin, Richard D'Souza, Antonela Monachesi, Jeremy Bailin, David Nidever, and Roelof S. de JongShow full author listPublished 2025 March 11 â€¢\n       \t  Â© 2025. The Author(s). Published by the American Astronomical Society.\n        The Astrophysical Journal Letters,\n\tVolume 982,\nNumber 1Citation Jose Marco Arias et al 2025 ApJL 982 L3DOI 10.3847/2041-8213/adb433 Download Article PDFDownloadArticle ePub\n            You need an eReader or compatible software to experience the benefits of the ePub3 file format.Authors\n                                    Jose Marco AriasAFFILIATIONSDepartment of Astronomy, University of Michigan, 1085 S. University Ave., Ann Arbor, MI 48109-1107, USA; joarias@umich.eduEMAILjoarias@umich.eduhttps://orcid.org/0009-0002-9085-5928Eric F. BellAFFILIATIONSDepartment of Astronomy, University of Michigan, 1085 S. University Ave., Ann Arbor, MI 48109-1107, USA; joarias@umich.eduhttps://orcid.org/0000-0002-5564-9873Katya GozmanAFFILIATIONSDepartment of Astronomy, University of Michigan, 1085 S. University Ave., Ann Arbor, MI 48109-1107, USA; joarias@umich.eduhttps://orcid.org/0000-0003-2294-4187In Sung JangAFFILIATIONSDepartment of Astronomy and Astrophysics, University of Chicago, Chicago, IL 60637, USAhttps://orcid.org/0000-0002-2502-0070Saxon StocktonAFFILIATIONSDepartment of Physics, Utah Valley University, 800 W. University Pkwy., Orem, UT 84058, USAhttps://orcid.org/0000-0003-1677-0213Oleg Y. GnedinAFFILIATIONSDepartment of Astronomy, University of Michigan, 1085 S. University Ave., Ann Arbor, MI 48109-1107, USA; joarias@umich.eduhttps://orcid.org/0000-0001-9852-9954Richard D'SouzaAFFILIATIONSVatican Observatory, Specola Vaticana, V-00120, Vatican City Statehttps://orcid.org/0000-0001-9269-8167Antonela MonachesiAFFILIATIONSDepartamento de AstronomÃ­a, Universidad de La Serena, Avda. RÃ¡ul BitrÃ¡n 1305, La Serena, Chilehttps://orcid.org/0000-0003-2325-9616Jeremy BailinAFFILIATIONSDepartment of Physics and Astronomy, University of Alabama, Box 870324, Tuscaloosa, AL 35487-0324, USAhttps://orcid.org/0000-0001-6380-010XDavid NideverAFFILIATIONSDepartment of Physics, Montana State University, P.O. Box 173840, Bozeman, MT 59717-3840, USAhttps://orcid.org/0000-0002-1793-3689Roelof S. de JongAFFILIATIONSLeibniz-Institut fÃ¼r Astrophysik Potsdam (AIP), An der Sternwarte 16, 14482 Potsdam, Germanyhttps://orcid.org/0000-0001-6982-4081\n                                FiguresSkip to each figure in the articleTablesSkip to each table in the articleReferencesCitationsArticle dataSkip to each data item in the articleWhat\n                                        is article data?Open science Download Article PDFDownloadArticle ePub\n            Authors\n                    Jose Marco AriasAFFILIATIONSDepartment of Astronomy, University of Michigan, 1085 S. University Ave., Ann Arbor, MI 48109-1107, USA; joarias@umich.eduEMAILjoarias@umich.eduhttps://orcid.org/0009-0002-9085-5928Eric F. BellAFFILIATIONSDepartment of Astronomy, University of Michigan, 1085 S. University Ave., Ann Arbor, MI 48109-1107, USA; joarias@umich.eduhttps://orcid.org/0000-0002-5564-9873Katya GozmanAFFILIATIONSDepartment of Astronomy, University of Michigan, 1085 S. University Ave., Ann Arbor, MI 48109-1107, USA; joarias@umich.eduhttps://orcid.org/0000-0003-2294-4187In Sung JangAFFILIATIONSDepartment of Astronomy and Astrophysics, University of Chicago, Chicago, IL 60637, USAhttps://orcid.org/0000-0002-2502-0070Saxon StocktonAFFILIATIONSDepartment of Physics, Utah Valley University, 800 W. University Pkwy., Orem, UT 84058, USAhttps://orcid.org/0000-0003-1677-0213Oleg Y. GnedinAFFILIATIONSDepartment of Astronomy, University of Michigan, 1085 S. University Ave., Ann Arbor, MI 48109-1107, USA; joarias@umich.eduhttps://orcid.org/0000-0001-9852-9954Richard D'SouzaAFFILIATIONSVatican Observatory, Specola Vaticana, V-00120, Vatican City Statehttps://orcid.org/0000-0001-9269-8167Antonela MonachesiAFFILIATIONSDepartamento de AstronomÃ­a, Universidad de La Serena, Avda. RÃ¡ul BitrÃ¡n 1305, La Serena, Chilehttps://orcid.org/0000-0003-2325-9616Jeremy BailinAFFILIATIONSDepartment of Physics and Astronomy, University of Alabama, Box 870324, Tuscaloosa, AL 35487-0324, USAhttps://orcid.org/0000-0001-6380-010XDavid NideverAFFILIATIONSDepartment of Physics, Montana State University, P.O. Box 173840, Bozeman, MT 59717-3840, USAhttps://orcid.org/0000-0002-1793-3689Roelof S. de JongAFFILIATIONSLeibniz-Institut fÃ¼r Astrophysik Potsdam (AIP), An der Sternwarte 16, 14482 Potsdam, Germanyhttps://orcid.org/0000-0001-6982-4081\n                Article metrics2960 Total downloads0 Video abstract viewsShare this articleArticle informationDatesReceived 2024 October 30 Revised 2025 February 8 Accepted 2025 February 8 Published 2025 March 11 Unified Astronomy Thesaurus conceptsDwarf galaxies; Andromeda GalaxyJournal RSS\n\n               Create or edit your corridor alerts\n\n                        Corridor alerts\n\n                            Receive alerts on all new research papers in American Astronomical Society\n                            (AAS) journals as soon as they are published.\n                            Select your desired journals and corridors below. You will need to select a minimum of one corridor.\n\n                                Corridors\n\n                                Journals\n\n                                Please note, The Planetary Science Journal (PSJ) does not currently use the corridors.\n\n                            Create alert\n\n                    What are corridors?opens in new tab\n\n    2041-8205/982/1/L3Abstract\n            We present the discovery of Andromeda XXXV, the faintest Andromeda satellite galaxy discovered to date, identified as an overdensity of stars in the Pan-Andromeda Archaeological Survey and confirmed via Hubble Space Telescope imaging. Located at a heliocentric distance of 927âˆ’63+76 kpc and 158âˆ’45+57 kpc from Andromeda, Andromeda XXXV is an extended (rh=53âˆ’11+13 pc), elliptical (=0.4Â±0.2), metal-poor ([Fe/H]âˆ¼âˆ’1.9) system, and the least luminous (MV=âˆ’5.2Â±0.3) of Andromeda's dwarf satellites discovered so far. Andromeda XXXV's properties are consistent with the known population of dwarf galaxies around the Local Group, bearing close structural resemblance to the Canes Venatici II and Hydra II Milky Way (MW) dwarf satellite galaxies. Its stellar population, characterized by a red horizontal branch or a red clump feature, mirrors that of other Andromeda satellite galaxies in showing evidence for a spread in age and metallicity, with no signs of younger stellar generations. This ageâ€“metallicity spread is not observed in MW satellites of comparable stellar mass, ",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "3f8268237786294b",
    "title": {
      "en": "The polar vortex is hitting the brakes",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.climate.gov/news-features/blogs/polar-vortex/polar-vortex-hitting-brakes",
    "score": 261,
    "by": "bryanrasmussen",
    "time": 1742671898,
    "content": "Polar Vortex Blog\nA blog about monitoring and forecasting the stratospheric polar vortexes and how they do and donâ€™tâ€”mostly donâ€™t!*â€”affect extreme winter weather and seasonal climate. *Seriously, weâ€™ll probably spend a lot of time explaining how the latest wintry weather blast bearing down on us from Canada probably has nothing to do with the polar vortex.\nDisclaimer\nClimate.govâ€™s Polar Vortex Blog is written, edited, and moderated by Amy Butler (NOAA Chemical Sciences Laboratory) and Laura Ciasto (NOAA Climate Prediction Center), with editorial and graphics support from the Climate.gov team (NOAA Climate Program Office). These are blog posts by subject matter experts, not official agency communications; if you quote from these posts or from the comments section, you should attribute the quoted material to the individual blogger or commenter, not to NOAA or or Climate.gov.\n\n  Latest Blogs\n\n                The polar vortex is hitting the brakes\n\n                This cold air? Itâ€™s probably not the polar vortex\n\n                Polar Vortex Intensifications: the overlooked influencer\n\n                Another blast of Arctic air: this time, with a stretched but strong polar vortex\n\n  Related Content\n\n                RSS Feed\n\n                                                      Polar Vortex\n\n                The polar vortex is hitting the brakes\n\n  By         Laura Ciasto AND Amy Butler\n\n                    Published March 6, 2025\n\n                    12Comments\n\n                facebook\n\n                twitter\n\n                envelope\n\n                print\n\n            For much of this winter season, the polar vortex winds at 60Â°N have been racing around the stratospheric polar region. During February alone, these west-to-east winds were two times stronger than normal for that time of year. However, the latest forecasts suggest that the polar vortex is about to switch gears with a major vortex disruption to happen this weekend. Read on to find out why the polar vortex could be bottoming out early this season.\n\n            Observed and forecasted (NOAA GEFSv12) polar vortex wind speeds at 60Â°N (bold blue line) compared to the natural range of variability (faint blue shading). Since mid-November, these stratospheric winds have been stronger than normal (thin blue line). However, thatâ€™s about to change as the latest forecasts (issued March 3, 2025) indicate the winds at 60Â°N are going to dramatically decrease over the next few days (bold purple line), indicating a polar vortex disruption. The big question is whether these winds will rebound toward their normal strength before the end of the season. NOAA Climate.gov image, adapted from original by Laura Ciasto.\n\n      Stratospheric pit stop\nAt the time of writing this post, the polar stratospheric west-to-east winds are still speeding around the Arctic [footnote #1], but forecasts suggest they are not only going to come to a screeching halt by the weekend, but they are then going to strongly reverse direction. When this wind reversal (i.e., winds become east-to-west) occurs at 60Â°N and 10 hPa (~19 mi/30 km above us), itâ€™s called a sudden stratospheric warming. As the name suggests, these major polar vortex disruptions are linked to incredible stratospheric temperature increases over a short period of time [footnote #2]. For this upcoming event, temperatures in the mid-stratosphere could increase as much as 45Â°F (25Â°C) in less than 5 days.\n\n            Observed and forecasted (NOAA GEFSv12) polar cap temperatures compared to the natural range of variability (faint orange shading). Since October, these stratospheric temperatures (bold red line) have been colder than normal (thin red line). This is expected because strong polar vortex winds act as a barrier between cold Arctic air and warmer mid-latitude air. As the polar vortex becomes disrupted, the stratosphere will warm quickly and intensely (bold pink line), hence the name sudden stratospheric warming. NOAA Climate.gov image, adapted from original by Laura Ciasto.\n\n      Sudden stratospheric warming events usually come in two possible flavors in which the polar vortex either displaces off the pole or splits into two smaller vortexes. This particular event may be a bit of both. The initial warming event kicks off with the polar vortex shifted toward Europe, but the forecasts also show pieces of the vortex splitting off from the main lobes several days later.\n\n            Evolution and forecast of stratospheric conditions. Earlier this week (March 4 2025; left panel), the polar vortex winds (vectors) were situated closer to the pole keeping the relatively cold air (light shading) isolated from the warmer surrounding air (orange/red shading). By March 10, 2025 (middle panel), the GFS forecast indicates the polar vortex will be nudged farther off the pole, with warmer air flooding the Arctic. The average winds around 60Â°N will become east-to-west, characterizing a sudden stratospheric warming. This disruption to the polar vortex is expected to continue through at least the next two weeks with smaller lobes of the vortex periodically splitting off (e.g., March 13, 2025, right panel). Current forecasts suggest that the stratospheric winds will not recover this spring and become west-to-east again. If so, this event will be classified as a final warming instead of a mid-winter sudden stratospheric warming. NOAA Climate.gov image, based on Global Forecast System data provided by Laura Ciasto.\n\n      Will the polar vortex rev its engine again?\nOne of the big questions regarding this polar vortex disruption is whether the stratospheric winds at 60Â°N will recover and become west-to-east again, extending the polar vortex season (and its ability to influence weather patterns) into late spring. Forecasts [footnote #3] do not currently show a recovery, so this pit stop may be the end of the vortexâ€™s racing season. If this turns out to be the case, then it would be classified as a â€œfinal stratospheric warmingâ€ rather than a major sudden stratospheric warming.\nAs we discussed in last seasonâ€™s post, final warmings occur every spring as sunlight returns to the North Pole and the temperature differences between the equator and pole decrease. As a result, the west-to-east winds that are maintained by that temperature difference decrease and transition to east-to-west winds. This transition usually happens sometime in mid-April, but there have been 5 years since 1958 when final warmings occurred before March 15. Like this year, those years corresponded to winters without a mid-winter sudden stratospheric warming [footnote #4].\nA potential stratosphere-troposphere fender bender\n\n            Differences from average atmospheric thickness (â€œstandardized geopotential height anomaliesâ€) in the column of air over the Arctic for the stratosphere and troposphere. Since the beginning of the year, low-thickness anomalies (purple shading indicative of a stronger than average polar vortex) have dominated the stratosphere but only periodically coupled down to the troposphere. Latest forecasts show a dramatic change with thickness anomalies increasing (orange shading), consistent with a polar vortex disruption. These stratospheric anomalies are preceded by tropospheric anomalies of the same sign, hinting at a nudge from below. However, itâ€™s too soon to tell whether these stratospheric anomalies will then drip down into the troposphere again. Standardized anomalies are based on departures from the 1991-2020 Climate Forecast System Reanalysis climatologies and have been divided by the standard deviation. Data are from the Global Forecast System observational analysis and forecast.\n\n      Regardless of whether this is the final warming or the vortex decides to ride again, both have the potential to impact our weather this spring. Disruptions to the polar vortex can communicate down to the troposphere and disrupt the jet stream. These disruptions to the jet stream can bring colder than normal Arctic air down into the eastern United States.\nNow this doesnâ€™t mean you need to bring your winter tires back out while your garden tools continue to collect dust. First, itâ€™s too soon to tell whether this vortex disruption will make its way down to the troposphere as the latest forecast doesnâ€™t show much stratosphere-troposphere interaction after the onset of the warming event. Second, though the impacts of March sudden warmings are very similar to those in mid-winter, spring is coming, so any Arctic air brought down in the US won't \"feel\" as cold compared to if it happened in January because we are in a warmer part of the year.\nEven if the polar vortex season ends early this year, weâ€™re hoping to have at least 1 or 2 more posts (including a guest author) so stay tuned!\nFootnotes\n[1] We spent several posts this winter talking about the strong, but sometimes stretchy, polar vortex and what that has meant for our winter weather. If youâ€™re interested, please read more here, here, and here.\n[2] The sudden increase in temperature over such a short period of time occurs for a couple of reasons. As the polar winds weaken and reverse direction during a major sudden stratospheric warming, there is a component of the air that moves poleward and descends rapidly over the Arctic and pressure increases. As the air descends it warms: this is one of the reasons why the temperatures can increase so impressively during a major warming event. Furthermore, the polar vortex winds act as a barrier between cold Arctic air and warmer mid-latitude air. When the winds/barrier weaken, warmer mid-latitude winds can enter the polar stratosphere and contribute to increasing temperatures.\n[3] We show the American GEFS model in these posts, but the ECMWF model currently doesnâ€™t show a vortex recovery in the next several weeks either.\n[4] The link between winters with a sudden warming and late season final warmings (and correspondingly, years without a sudden warming and early season final warmings) is thought to be due to the tug of war in the stratosphere between dynamic and radiative processes that control the strength of the polar vortex. In particular, if a sudden warming occurs during mid-winter, the polar stratospheric winds will be pulled towards returning to a west-to-east flowing state to balance the stratospheric temperature gradient created by lack of sunlight over the pole. If this recovery of the stratospheric winds to west-to-east flow occurs, it provides potentially weeks to months of additional time for planetary waves to interact with the winds, extending the timing of the final warming until much later. On the other hand, if the sudden warming occurs near the spring equinox, when sunlight has returned to the pole, the stratospheric winds feel no radiative force to return to a west-to-east state, and so often the winds will stay east-to-west (corresponding to an early season final warming).\n\n    Add new comment\n\n  Comments\n\n      Comment Policy\n\n        Permalink\n\nAll comments are placed in moderation and must be approved by one of the blogging team. We do not monitor comments 24/7. Please be patient and do not post your comment multiple times.\nWe will not publish comments that are off topic, or that contain profanity, name-calling, personal attacks, or misinformation.\nWe will not post comments that include links to blogs or websites unrelated to the post topic.\nOn occasion we may edit a comment before we post it. We will always indicate if we have done so.\n\n      Reply\n\n        Submitted by laura.ciasto on Thu, 03/06/2025 - 17:12\n\n      Vortex\n\n        Permalink\n\n            Thanks for your info and work on this. Sure hope it's a minimal effect and short term. Have mercy already, thanks.\n\n      Reply\n\n        Submitted by Dan on Fri, 03/07/2025 - 07:49\n              In reply to Comment Policy by laura.ciasto\n\n      Vortex and tropical weather\n\n        Permalink\n\n            Just curious if the votes has decided to go for the season and the weather does start warming up early. What are the impacts that this can have around June when hurricane season starts? Does this warming event actually contribute more to a possibly more active tropical activity?\n\n      Reply\n\n        Submitted by Steve  on Fri, 03/07/2025 - 08:19\n\n      polar vortex and hurricanes\n\n        Permalink\n\n            Great question!\nTo my knowledge, there's a not an established link between the stratospheric polar vortex and hurricanes. This is partly due to the fact that the hurricane season doesn't really coincide with the times when the polar vortex is most active. So I don't think there'd be a huge impact on the overall activity of the hurricane season.\nThat said, there's the possibility that the timing of the final warming could impact hurricanes in June. One of the main impacts of the stratospheric warmings on the troposphere is to disrupt the jet stream. If there was a late final warming (say early May) and any resulting disruptions to the jet stream persisted for several weeks, then that could impact hurricanes formed in June. But I'm mostly speculating here. It'd be an interesting area to research in the future!\n\n      Reply\n\n        Submitted by laura.ciasto on Mon, 03/10/2025 - 10:41\n              In reply to Vortex and tropical weather by Steve\n\n      Thank you\n\n        Permalink\n\n            I really appreciate this information, it is facinating to learn more about the weather. Thank you.\n\n      Reply\n\n        Submitted by Bob on Fri, 03/07/2025 - 10:35\n\n      Thanks, glad you areâ€¦\n\n        Permalink\n\n            Thanks, glad you are enjoying it!\n\n      Reply\n\n        Submitted by laura.ciasto on Mon, 03/10/2025 - 09:26\n              In reply to Thank you by Bob\n\n      Many thanks!\n\n        Permalink\n\n            Thank you for this very informative blog - wonderfully written and illustrated as always. Do you know which 5 years had an earlier FWE before 15th March please? 2016 I have but dataset is limited to > 1980\n\n      Reply\n\n        Submitted by Katie Greening on Fri, 03/07/2025 - 13:56\n\n      Early final warming dates\n\n        Permalink\n\n            Thanks!\nThe years are: Mar-11-1961, Mar-12-1974, Mar-12-1978, Mar-13-2005, Mar-5-2016.\nI based this information off of Butler et al (2021):https://wcd.copernicus.org/articles/2/453/2021/\n\n      Reply\n\n        Submitted by laura.ciasto on Mon, 03/10/2025 - 09:35\n              In reply to Many thanks! by Katie Greening\n\n      polar jet stream\n\n        Permalink\n\n            What has happened to the polar jet stream? It doesn't look anything like it used to. Will it ever look like those official images, such as the one you posted on this blog on 12/5/23 as if it were one solid band? Some days it looks more like a bowl of spaghetti.\n\n      Reply\n\n        Submitted by Julia  on Fri, 03/14/2025 - 19:56\n\n      jet stream\n\n        Permalink\n\n            The jet stream (not to be confused with the stratospheric polar vortex) never looks as simple as it does in the schematic. It also depends on what level in the atmosphere you are looking at. If you're interested, go to:https://earth.nullschool.net and play around with the different levels such ass 500 and 250-hPa.\nThat said, it has seen a lot of action this winter. One way we can measure the jet stream stream is through an index called the Arctic Oscillation (also called the AO). When the AO is negative, it means the jet stream is much wavier. This season has seen several instances of a negative AO.\n\n      Reply\n\n        Submitted by laura.ciasto on Wed, 03/19/2025 - 08:56\n              In reply to polar jet stream by Julia\n\n      polar vortex well doesnt look like the united states will get it\n\n        Permalink\n\n            vortex is not gona hit us looks like its march 18/2025\n\n      Reply\n\n        Submitted by jeff  on Tue, 03/18/2025 - 15:05\n\n      U.S. impacts\n\n        Permalink\n\n            The latest forecasts do show some downward propagation, which suggests the US might feel some impacts of the stratospheric warming by the end of March/early April. But like we said, the impacts of the polar vortex won't \"feel\" as strong as they do in January because temperatures are warmer this time of year.\n\n      Reply\n\n        Submitted by laura.ciasto on Wed, 03/19/2025 - 09:14\n              In reply to polar vortex well doesnt look like the united states will get it by jeff\n\n      Add new comment\n\n      Your name\n\n      Subject\n\n      Comment\n\n  Rich Text Editor, Comment fieldEditor toolbarsToolsMaximizeSourceCutKeyboard shortcut Ctrl+XCopyKeyboard shortcut Ctrl+CPasteKeyboard shortcut Ctrl+VPaste as plain textKeyboard shortcut Ctrl+Shift+VPaste from WordInsert Special CharacterFormattingBoldKeyboard shortcut Ctrl+BItalicKeyboard shortcut Ctrl+IStrikethroughUnderlineKeyboard shortcut Ctrl+USubscriptSuperscriptDecrease IndentIncrease IndentRemove FormatLinksLinkKeyboard shortcut Ctrl+KUnlinkListsInsert/Remove Bulleted ListInsert/Remove Numbered ListTablePress ALT 0 for helpElements path\n\nAbout text formats\n\n      Leave this field blank",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "56e5a0c00e5785e6",
    "title": {
      "en": "A Brief History of the Miracle Bacterium",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.asimov.press/p/miracle-bacterium",
    "score": 65,
    "by": "mailyk",
    "time": 1742745552,
    "content": "Share this postAsimov PressA Brief History of the Miracle BacteriumCopy linkFacebookEmailNotesMoreDiscover more from Asimov PressScience and technology for a flourishing future. Website: press.asimov.comSubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.Already have an account? Sign inThe Long ReadA Brief History of the Miracle BacteriumSerratia marcescens, a pathogen with an uncanny resemblance to blood, has had an outsized influence on modern science.Corrado NaiMar 24, 202517Share this postAsimov PressA Brief History of the Miracle BacteriumCopy linkFacebookEmailNotesMore2ShareArticle voiceover1Ã—0:00-24:31Audio playback is not supported on your browser. Please upgrade.At 1:15 p.m. on Monday, August 8th, 1904, a British physician named M. H. Gordon took some soil he had â€œrichly impregnated with a living emulsionâ€ of the virulent bacterium,1 Serratia marcescens, and sprinkled it near a lamp post in front of the U.K. House of Commons. Gordon knew that was the exact spot Members of Parliament had to cross before their 2 p.m. session. His hope was for the politicians to step on the contaminated ground and spread the bacteria inside the Debating Chamber.Gordon wasnâ€™t executing a terrorist attack. Rather, he had been appointed by a committee to study how germs spread inside the House of Commons following an outbreak of influenza among its members. Gordon had chosen Serratia marcescens because the bacterium forms easily recognizable red colonies. For his experiment, he placed numerous open Petri dishes inside the Debating Chamber on which the colonies could grow. Gordonâ€™s idea was simple: politiciansâ€™ boots would carry contaminated soil and spread Serratia marcescens into the building; he would go away with his Petri dishes and reveal colonies of the bacterium to point at faults in the House of Commonsâ€™ ventilation system.But despite placing hundreds of Petri dishes around the chamber, Gordon wasnâ€™t able to retrieve more than a handful of colonies. Suspecting the microbe might not spread easily by means of boots, he followed this up with a theatrical experiment: Inside the Debating Chamber, he gargled a suspension of Serratia marcescens and recited Shakespeareâ€™s â€œKing Henry Vâ€ and â€œJulius Caesarâ€ for one hour to no one but an audience of open Petri dishes. This time, copious colonies of Serratia marcescens appeared, leading him to conclude that speech can transmit microbes as far as 70 feet (21 meters) away.2Indeed, Serratia marcescensâ€™ vivid blood-red color has prompted its use in a wide range of experiments that have increased our understanding of how germs disperse within human bodies, buildings, and populations. Sightings of the striking microbe outside the lab have awakened both fear and awe in the general population.The awareness that certain strains of Serratia marcescens can cause severe harm to humans â€” counter-intuitively, the paler varieties are most dangerous â€” only became evident decades after Gordonâ€™s investigations. Before then, hospitals deliberately sprayed Serratia marcescens inside their facilities to investigate microbial dispersion, and laboratory handbooks demonstrated transmission by handshake by having students coat their fingers in the microbe. While greater awareness of its dangers eventually led to its discontinuation in tracing experiments, Serratia marcescens remains an important subject of biomedical research.Its scientific journey began over 200 years ago with a bloody polenta.Subscribe to Asimov Press.SubscribeBlood MasqueradeWhen growing on solid substances, like foodstuffs or Petri dishes filled with agar, Serratia marcescens forms red colonies reminiscent of blood, earning it the informal moniker â€œmasquerader of blood.â€ Its appearance on the polenta of a wealthy Paduan farmer during a particularly hot and humid summer in 1819 â€” ideal conditions for its flourishing â€” prompted the flurry of investigation that gave it its scientific name.Serratia marcescens looks like blood droplets when grown at room temperature on solid media. Mature colonies are mucilaginous, viscous, and tinged bright red or pink with an \"uncanny resemblance to blood.â€ Credit: Benutzer:BrudersohnAfter the farmer, Antonio Pittarello, discovered the red-spotted food in his house near Padua, the resultant hubbub of curiosity seekers and the spiritually appalled so disturbed the neighborhood that the local District Commissioner appointed Dr. Vicenze Sette (a physician, surgeon, and district health officer) to investigate the discovery. Also taking an interest in the mysterious â€œbloody polentaâ€ was Bartolomeo Bizio (1791-1862), a young pharmacy student who joined the investigation on his own. In the meantime, red spots started to appear on food in hundreds of homes throughout Pittarelloâ€™s village as well as in villages close by.Through a series of ingenious experiments, Sette and Bizio reproduced the red spots on fresh polenta independently in their own home laboratories. Both concluded (erroneously) that the cause was a microscopic fungus.3 Sette called it Zaogalactina imetrofa (from the Latin, â€œslime living on foodâ€); Bizio called it Serratia4 to honor an Italian Benedictine monk and physicist, Serafino Serrati (whom he felt had not gotten adequate credit for his contributions to the invention of the steamboat) and marcescens (from the Latin, â€œto decayâ€) since, much to his disappointment, the microbeâ€™s red color fades easily.Racing to gain acclaim for identifying the source of the â€œbloody polentaâ€ before Sette, Bizio made two deft moves. First, by citing Lazzaro Spallanzani, a pioneering biologist known for having worked to disprove the theory of spontaneous generation, he hoped to lend credibility to his findings; indeed, by demonstrating a biological origin for the â€œbloody polenta,â€ Bizio again refuted spontaneous generation decades before Louis Pasteur. And second, Bizio quickly published his discoveries in a Venetian newspaper.5 Sette was furious about being scooped by Bizioâ€™s article, although Bizioâ€™s results didnâ€™t contradict his own.Bartolomeo BizioAs Bizio reported in a letter to a priest, Angelo Bellani, three years after the event:For several days succeeding its publication, there was much discussion of my experiments in the columns of this newspaper [The Official Gazette]; and to make them still more well known, an enterprising publisher printed them in a small pamphlet which he sold in the streets, so that the general public as well as educated persons came to know about them.By recreating red spots of Serratia marcescens on fresh polenta, Bizio grew pure colonies of microbes more than half a century before the â€œculture plate techniqueâ€ developed by Robert Koch, Walther and Fanny Angelina Hesse, and Julius Petri.6 The significance of Bizioâ€™s experiments has survived over the centuries, showing the falsehood of a dismissive remark made by renowned bacterial systematist S. T. Cowan, who said: â€œI believe we shall not lose anything by ignoring all work before the pioneer [sic] work of Pasteur.â€The bacteriological investigations of another predecessor of Pasteur, the naturalist Christian Gottfried Ehrenberg (1795-1876), have survived as well. In 1848, several decades after the bloody polenta first appeared in Padua, red spots began popping up on boiled potatoes in Germany. Unaware of Bizioâ€™s work, Ehrenberg observed the phenomenon and called the organism responsible Monas prodigiosa, one of the many names that Serratia marcescens has held over the centuries. Ehrenberg studied historical records and concluded that the bacterium was most likely responsible for more than 100 documented cases of so-called â€œmiraculous blood.â€!function(){\"use strict\";window.addEventListener(\"message\",(function(e){if(void 0!==e.data[\"datawrapper-height\"]){var t=document.querySelectorAll(\"iframe\");for(var a in e.data[\"datawrapper-height\"])for(var r=0;r<t.length;r++){if(t[r].contentWindow===e.source)t[r].style.height=e.data[\"datawrapper-height\"][a]+\"px\"}}}))}();Specifically, Serratia marcescens holds a large but unintended place in two of the worldâ€™s religions due to this shocking resemblance to blood. The 1264 â€œMiracle of Bolsenaâ€ of a Host â€œbleedingâ€ with Serratia marcescens was believed to have contributed to the establishment of the Holy Communion, a central sacrament of the Greek Orthodox and Roman Catholic church. Additionally, the infamous antisemitic â€œblood libelsâ€ might have been due to this microbe, with thousands of Jews executed as heretics following accusations of having stabbed holy wafers in what was most likely a naturally occurring outbreak of Serratia marcescens on the starchy substratum. It is difficult to verify if Serratia marcescens was the precise biological cause of such â€œmiraclesâ€ or â€œheresies,â€ however, since many microorganisms, such as mold or yeast, are often red or pink and grow readily on food. But it seems likely, for as researchers have pointed out: â€œWe know of no organism [ â€¦ ] looking more like drops of fresh blood than Serratia marcescens.â€With its centuries-long history, itâ€™s perhaps no surprise that Serratia marcescens has had so many different names and presents â€œone of the most confusing taxonomies in the bacterial world.â€ In 1924, bacteriologists rehabilitated Bizioâ€™s Linnean appellation, retaining Ehrenbergâ€™s â€œterm of miracle (prodigium) bacterium for use as a trivial or common name.â€ By the end of the 19th century, however, due to its vivid hue, Serratia marcescens had become one of â€œthe cornerstones of modern bacteriology.â€ Fake Blood, Real ThreatSpecifically, Serratia marcescensâ€™ striking color made it the microbe of choice for several decades amongst bacteriologists studying how pathogens spread through buildings and cities.In 1897, German bacteriologist Carl FlÃ¼gge used the bacteria to perform experiments which served as a template for Gordonâ€™s (but without the Shakespearean flair).7 FlÃ¼gge was the first to demonstrate that mouth droplets carry bacteria. His findings spurred surgeon Johann Mikulicz to develop a precursor of todayâ€™s face mask, and were a major driver of the six feet (two meters) â€œsocial distancingâ€ policy during the COVID-19 pandemic.In 1919, military doctors proved that utensils can indirectly transmit microbes by applying Serratia marcescens on the mouth and lips of â€œdonor soldiersâ€ before their meals. In 1926, bacteriologists traced how handshakes transmit microbes by smearing the bacterium on the hands of test subjects. In 1937, dentists detected bacteremia (entry of bacteria into the blood system) following dental extraction by spreading the pathogen around the gum of teeth. In 1945, military staff correlated air quality with the progression of illness by exposing test subjects to huge quantities of aerosolized Serratia marcescens. In 1957, doctors demonstrated urinary tract infections through catheters by applying Serratia marcescens on genitalia of semi-comatose patients, one of whom died shortly after.8More ominously, Nazi Germany studied the spread of the pathogen in the Paris MÃ©tro and the London Underground, as reported in a 1934 article by investigative journalist Henry Wickham Steed, Aerial Warfare: Secret German Plans. The French and U.K. governments took Steedâ€™s article very seriously, as the underground systems in both Paris and London had functioned as shelters during WWI and would again in WWII.London residents take refuge in an Underground station during intense bombing from the Luftwaffe.The U.S. military also deployed Serratia marcescens in a spectacular way in September 1950. In a secret project called â€œOperation Sea Spray,â€ the Navy sprayed enormous quantities of the pathogen along the coast of San Francisco to study large-scale, open-air transmission of germs in biological warfare. Navy scientists also set up monitoring stations and traced the microbe up to 50 miles inland, but experts disputed any scientific and epidemiological merit of the experiments. Notably, â€œOperation Sea Sprayâ€ coincided with the first recorded nosocomial outbreak of Serratia marcescens, with eleven inpatients infected at Stanford Hospital between September 1950 and February 1951. Two of them got bacteremia; one died of heart failure.9(In November 1976, the Long Island newspaper Newsday leaked information about â€œOperation Sea Spray.â€ Contrary to other biowarfare experiments that flew under the radar, this led to a public Hearing at the Senate in March and May of 1977.10 Surprisingly, however, in August 1977, an investigation by the U.S. Centers for Disease Control concluded that the strain used by the U.S. Military was not related to any infection within the population.)That Serratia marcescens was so widely used in experiments involving human subjects should startle us; especially since the scientific community was accumulating evidence that the bacterium could be deadly. As early as 1903, infectologist E. Bertarelli showed that the bacterium was lethal to mice, rats, and guinea pigs â€œfollowing inoculation of massive doses of Bacillus prodigiosusâ€ (one of the microbeâ€™s many names). The first report of human infection by Serratia marcescens followed in 1913, when a healthy young man sought medical help after becoming troubled by a foul-smelling red sputum, which he mistakenly took for blood.11 Doctors found no sign of blood cells in the sputum but saw a large number of intestinal bacteria, which grew on agar into distinctive red colonies. The patient later recovered without complications.Among those with weakened immune systems, Serratia marcescens has been involved â€œin every conceivable kind of infection.â€ It can affect an individualâ€™s mouth and throat, lungs, gut, urinary tract, blood, heart, wounded skin, eyes, and central nervous system. When susceptible patients are infected by a virulent form of the pathogen, symptoms may include discoloration of extremities, shock, convulsions, deafness, delirium, and coma. Mortality of affected patients (especially in the case of sepsis, or the spread of bacteria in the bloodstream) runs as high as 30-40 percent. Indeed, by the late 1960s, the bacterium was unequivocally declared as a cause of serious infections, including death.Infections with Serratia marcescens havenâ€™t ceased to be a cause for concern,12 and clinicians have wondered if there arenâ€™t many more infections with Serratia marcescens going unrecognized. As recently as 2017, the World Health Organization listed Serratia among bacteria for which new antibiotics are urgently needed.Modern WondersSerratia marcescens striking red color comes from a pigment aptly called â€œprodigiosin.â€ Clinicians have observed an inverse correlation between a strainâ€™s high levels of prodigiosin and its ability to cause infections. This might explain why past studies designed to take advantage of Serratia marcescensâ€™ vivid appearance have caused little harm. By using bright red colonies as markers for their experiments, like M. H. Gordonâ€™s in the U.K. Parliament, researchers might have unwittingly selected a less virulent strain of Serratia marcescens.Prodigiosin is an alkaloid with immunosuppressive, anticarcinogenic, and antimicrobial properties. Scientists do not yet fully understand why the bacterium produces prodigiosin, but have noticed that environmental conditions influence red pigmentation (for example, a rise in temperature reduces the bacteriumâ€™s vivid color). They speculate that the molecule might be important for cell dormancy, dispersal in the environment, or for gaining advantages over competing microbes.Prodigiosin has many uses as a biomedicine, including the inhibition of microbes such as bacteria, fungi, algae, and viruses. Researchers are still investigating the mechanisms by which prodigiosin acts on a target cell, and have observed adverse effects on the cell membrane as well as in organelles within the cell. Prodigiosin has also contributed to research on microbial â€œsecondary metabolites;â€ molecules, including antibiotics, which are not involved in primary physiological functions like growth or reproduction, but nonetheless help protect an organism. Researchers showed before the discovery of penicillin, the first antibiotic, that Serratia marcescens inhibits Vibrio bacteria. In 1983, prodigiosin was also used as a molecular marker to clone, for the first time, a gene involved in the biosynthesis of an antibiotic.A formula named Coleyâ€™s Toxin containing Serratia marcescens has been used for over a century to stimulate the immune system to fight off cancers. (Its namesake, William Coley, is considered the â€œFather of Immunotherapy.â€) Prodigiosin has shown activity against cancer cells, among others, by inducing DNA cleavage and apoptosis (cell death). Prodigiosin is also being investigated for therapies that require inhibition of the immune system.In nature, Serratia marcescens is part of a complex web of ecological interactions. It can inhibit parasites (like the malaria agent Plasmodium falciparum), kill insects (like the cockroach Blattella germanica by acting synergistically with a fungus), or enhance growth of some plants while killing others. Researchers have also used Serratia marcescens as a model to study how environmental species evolve into pathogens. When confronted with a natural predator (like the protist Tetrahymena thermophila), the bacterium acquires traits that contribute to its virulence. In 2011, researchers also observed that Serratia marcescens had begun to infect corals, the first instance of a â€œreverse zoonosisâ€ in marine habitats, a situation in which a pathogen moves from humans to animals.As of early 2025, Serratia marcescens stands among the top 30 most studied microbes, topping species such as Agrobacterium tumefaciens (a widely used model microbe to genetically engineer plants), Legionella pneumophila (the cause of Legionnaireâ€™s disease), Clostridioides difficile (a common diarrhea-causing bacterium affecting the intestinal tract), and Mycobacterium smegmatis (a model organism for the study of the pathogen causing tuberculosis, M. tuberculosis). Through surprising sightings and curious experiments, this microbe has left an enormous mark culturally, clinically, and scientifically. Its vivid blood-red color has elicited both wonder and alarm in the eyes of the beholder even as its actual dangers have often been ignored.It is no wonder, then, that Alexander Fleming, that giant of microbial investigation, also paid notice. Fleming used it in a painting method he developed, called microbial art (or â€œagar artâ€). When working in this medium, artists take a Petri dish filled with agar and, using a lab tool called a loop, inoculate sections of the plate with various species of microbes that produce different hues. As the microbes grow, their living pigments form an image. For yellow, Fleming used Staphylococcus; for blue, Bacillus violaceus; and for brilliant red, of course, Serratia marcescens.Agar art paintings by Alexander Fleming. Credit: American Society for MicrobiologyThis article is accompanied by an interview with the author. Listen on Spotify or Apple Music.Corrado Nai is a science writer with a Ph.D. in microbiology. He has published articles in Smithsonian Magazine, New Scientist, Reactor (formerly Tor.com), Small Things Considered, and elsewhere. He is writing a graphic novel about the forgotten woman who introduced agar to microbiology, Fanny Angelina Hesse (1850-1934), based on unpublished historical material he helped resurface. Corrado lives in Jakarta with his wife and daughter.Cite: Nai C. â€œA Brief History of the Miracle Bacterium.â€ Asimov Press (2025). DOI: 10.62211/48yk-73gfLead image by Ella Watkins-Dulaney, adapted from Benutzer:Brudersohn (CC BY3.0).1I use â€œvirulent bacteriumâ€ and â€œpathogenâ€ interchangeably to refer to a disease-causing microbe. Usually, â€œpathogenâ€ refers to a species whereas strains (that is, lineages of bacteria) within a species can be more or less â€œvirulent.â€2Gordonâ€™s 250+ page report is dramatic. In some experiments he made bacteriological analyses of the Ministerial versus the Opposition side of the Chamber; in others he placed open Petri dishes even on the top of the Clock Tower. In his speech experiment, he retrieved more colonies when he repeated the test with the ventilation system on than with the ventilation system off. Despite inconclusive bacteriology findings, he suggested 14 substantive modifications to the ventilation system.3Christian Gottfried Ehrenberg thought it was an animal, botanist Camille Montagne an algae.4Serratia marcescens is the first bacterium to be ever named after a person. The genus name Serratia is only predated by Vibrio (Otto MÃ¼ller, 1773) and Polyangium (Heinrich Link, 1809).5The modern counterpart of Bizioâ€™s newspaper article, published in the Gazzetta Privilegiata di Venezia on 24 August 1819, would be a preprint.6Koch was the first to streak bacteria into single colonies in the 1870s; the Hesse couple introduced agar to the laboratory in 1881; and Petri perfected his Petri dishes in 1887. The â€œculture plate techniqueâ€ continues to be used unaltered to this day.7The original German article is available here: Carl FlÃ¼gge (1897), Ueber Luftinfection. Zeitschrift fÃ¼r Hygiene 25:179-224.8Itâ€™s uncertain if Serratia marcescens was the cause of death, as doctors were not granted the rights for an autopsy.9The Stanford Hospital case marked the first case of endocarditis (inflammation of the heart) by Serratia marcescens.10The Hearing discussed classified Military biowarfare experiments that took place using Serratia marcescens until 1968, including in the New York City Subway. The Military admitted knowing about the Stanford Hospital outbreak.11This appearance of red sputum without the presence of blood was later named â€œpseudohaemoptysis.â€12A dramatic outbreak happened in the mid-1970s and involved epidemics in four different hospitals in Nashville, Tennessee, caused by rotating staff likely transmitting the pathogen by hand. The virulent strain of Serratia marcescens was resistant to all antibiotics and infected over 200 patients, killing eight. Then, in one of those hospitals, a much more severe epidemic with Klebsiella pneumoniae infected about 400 patients, killing 18. Clinicians attributed this outbreak to co-infection and in vivo transmission of antibiotic-resistant genes from Serratia to Klebsiella. More recently, outbreaks of Serratia marcescens have occurred within prisons due to contaminated disinfectants and amongst intravenous opioid users due to contaminated syringes. In 2004, biotech company Chiron Corporation lost an enormous quantity of flu vaccines due to contamination with Serratia marcescens.Subscribe to Asimov PressScience and technology for a flourishing future. Website: press.asimov.comSubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.17 Likesâˆ™2 Restacks17Share this postAsimov PressA Brief History of the Miracle BacteriumCopy linkFacebookEmailNotesMore2SharePreviousA guest post byCorrado NaiScience writer, PhD fungal ecology. Currently writing a graphic novel about the forgotten woman who introduced agar to the lab, Fanny Angelina Hesse (1850-1934): https://fanny-hesse-graphic-novel.site/",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "2f636b83b97fa83c",
    "title": {
      "en": "Feral pig meat transmits rare bacteria",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://arstechnica.com/health/2025/03/florida-man-eats-feral-pig-meat-contracts-rare-biothreat-bacteria/",
    "score": 170,
    "by": "abawany",
    "time": 1742404626,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "4e2d1faf56b6ad1c",
    "title": {
      "en": "63 Chinese Cuisines: The Complete Guide (2024)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://chinesecookingdemystified.substack.com/p/63-chinese-cuisines-the-complete",
    "score": 117,
    "by": "mastax",
    "time": 1742779414,
    "content": "Share this postChinese Cooking Demystified Substack63 Chinese Cuisines: the Complete GuideCopy linkFacebookEmailNotesMoreDiscover more from Chinese Cooking Demystified SubstackRecipes and other stuff maybe.Over 7,000 subscribersSubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.Already have an account? Sign in63 Chinese Cuisines: the Complete GuideNot one cuisine, not eight, but many more. Our best shot at a comprehensive-ish guide.Chinese Cooking DemystifiedNov 19, 2024394Share this postChinese Cooking Demystified Substack63 Chinese Cuisines: the Complete GuideCopy linkFacebookEmailNotesMore3144ShareOne country, one cuisine.Itâ€™s a model of food that really underpins how people tend to think about, organize, and categorize food. Search Google Maps, and your local eateries are bound to be arranged into â€œThaiâ€ or â€œItalianâ€. Hop over to Taste Atlas, and youâ€™ll find little country flags next to each article. On Redditâ€™s /r/cooking, itâ€™s practically a semi-annual tradition to see someone attempting to make â€˜food around the worldâ€™, dutifully hopping around the globe nation-state by nation-state. When Good Mythical Morning guesses where a certain dish is around the world, they throw a dart at a map divided intoâ€¦ countries. These divisions can even turn into a political project â€“ food slotting in right there next to the national anthem and the flag emoji.And in fairness, itâ€™s certainly not complete bullshit. Borders set trading networks, so they often set what ingredients are available. Borders (often) set a language, so they set the linguistic pathways of how recipes spread.But at the same time, itâ€™sâ€¦ kind of bullshit, right? Like, people in New York eat much more similarly to people in Toronto (or maybe even Sydney!) than they do to people in New Orleans. Geography matters. Culture matters. And these things quite often donâ€™t neatly align to the various colors on the map.And itâ€™s in China, perhaps, where the one country, one cuisine model goes to die most violently. Sit Cantonese food next to Sichuan food, and the absurdity becomes aggressively spectacular:Left, Dim Sum in Hong Kong; Right, A Baijiu-Soaked Meal of 'Cold-Eaten Beef' in Zigong, SichuanAnd so, if you satiate your curiosity with a quick Google, youâ€™ll often find people saying that â€œChina doesnâ€™t have one cuisine, but eightâ€.â€œThe Big Eightâ€We could maybe even turn it into a little jingle.â€œCantonese, Sichuan, Hunan, Jiangsu, Zhejiang, Fujian, Shandong, and Anhui.â€The â€˜eight cuisines of Chinaâ€™ - itâ€™s a fun little bit of food trivia. I feel like when youâ€™re new to the sport of Chinese food, itâ€™s a fresh new model that can easily graft in and replace that tired, old one country, one system framework.Chinese Cooking Demystified Substack is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.SubscribeThereâ€™s other resources online that go into detail about The Big Eight. Personally, I donâ€™t really want to dwell on the thing â€“ after all, even if youâ€™re fresh to a sport, Iâ€™m not a big believer in starting with bad form. As you might be able to gather, Iâ€™m not exactly the most massive fan of the trope. I mean, the foodâ€™s good, of course. But thereâ€™sâ€¦ some issues:The most obvious, glaring problem is the incompleteness. Take, for example, one of the most famous Chinese restaurants in America: Xiâ€™an Famous Foods. The Anthony Bourdain favorite, the Millennial Hipster darling. People fell in love with their Roujiamo (â€œlamb burgersâ€); their slippery liangpi became a New York food sensation. They put Biang Biang noodles on the American food world map.So where is Xiâ€™an on this map?Oh.Likeâ€¦ the mapâ€™s overlooking not just Guizhou and Guangxi, itâ€™s overlookingâ€¦ literally the entire North of the country. No Lanzhou Hand-pulled noodles, no Peking Duck. Whatâ€™s going on here?The missing context is that The Big Eight was never meant to be an exhaustive list of all the cuisines of the country. It was a somewhat haphazard extension of the so-called â€œBig Fourâ€ banquet traditions of imperial China: Cantonese (ç²¤), Sichuan (å·), Shandong (é²), and Jiangsu (è‹). The reason why The Big Eight contains Cantonese and not Yunnan isnâ€™t because people in Yunnan donâ€™t have a unique cuisine â€“ itâ€™s because Cantonese had an establish system of banquet presentation that was enjoyed by the merchants and the Mandarins of the Qing dynasty, and Yunnan didnâ€™t.This cultural background is sort ofâ€¦ implicit knowledge for a lot of foodies in China â€“ implicit knowledge that I donâ€™t usually see translated into English. So, there you have it.But at the same time, I donâ€™t think itâ€™s exactly fair to tear down The Big Eight without building something to replace it. After all, itâ€™s still a better starting point than One Country, One Cuisine! Itâ€™s just incomplete.But thenâ€¦ if not eight, how many cuisines are there?Iâ€™ve never seen anyone attempt to count.So, we attempted to count.The 63 Cuisines of Mainland ChinaWelcome to the project that drove me to the edge of my sanity.Hereâ€™s our map:Hereâ€™s a full(er) res copy. Save it, peruse it, share it, whatever.This post isâ€¦ a little bit of a knowledge dump. I hope it can be a resource. Itâ€™s definitely gunna be too long for email, and probably too long to read in a single sitting. Think of this as the coffee table book of the Chinese Cooking Demystified Substack â€” itâ€™s here to open, browse, and poke around from time to time.Further, I also want to emphasize that we tried to come to this project from a position of humility. How to define the boundary of a â€˜cuisineâ€™ is not obvious. I went into (probably overly excruciating) detail about our methodology in the accompanying video, so I wonâ€™t re-hash too much of it here. Our rules of thumb were:Passing The 50% Rule. If youâ€™d estimate that more that 50% of the dishes are â€˜uniqueâ€™, and the dishes that remain often have different versions, itâ€™s a separate cuisine.Culinary Self-Determination. Do the people themselves (particularly the food world) make a distinction between cuisines? E.g. people in Louisiana are quick to make a distinction between Cajun food and Creole food, so to us this would be two distinct cuisines - even if the differences may not be overly obvious to an outsider.Failing the â€˜Mutual Intelligibilityâ€™ test. Imagine an old, talented home cook from one area. Would they be able to recreate a dish from another area solely from taste, without looking anything up?â€˜Culinary Continuumsâ€™ must be broken somewhere. A little like dialect continuums in linguistics, there can often be small changes in food between neighboring towns and cities - that then morph into large differences if you zoom out and look at either end of the continuum. Boundaries will ultimately arbitrary.There will likely be a lot of contention about some of these boundaries, and thereâ€™s absolutely stuff that we missed. Weâ€™re also personally the most familiar with the South of China, so differing opinions and viewpoints are more than welcome.All we wanted to do was improve on the big eight.Note: Taiwan is included in the below maps because the lowland regions have been part of the China proper social-ecological system since the 1600s. As we discuss in our section on Taiwan â€˜outside of mainland Chinaâ€™, this was then layered with other provincial Chinese cuisines as KMT forces fled to the island. It implies nothing about the political status of Taiwan. (we also donâ€™t get in any trouble this way)Guangdong ProvinceLeft, Location of Guangdong in China; Right, Breakdown of Guangdong CuisinesWe divided Guangdong into six cuisines (nine if you include Macau and Hong Kong). Cantonese is, of course, by far the most well known of the six and needs little introduction. And yet, at the same time, I think the cuisine can sometimes be a bit misunderstood. If you ask ten Cantonese chefs what the â€˜essenceâ€™ of Cantonese cuisine is, practically to the man theyâ€™ll reply â€˜keeping the original flavorâ€™. Itâ€™s a motto that works pretty well within a Cantonese cultural context: as a way to proudly distinguish yourself from the Sichuan and the Hunan. Less realized, I think, is that in an international 21st century context, the trope communicates as a self-own more than anything â€” a bit like  bragging about how wide your cityâ€™s downtown freeway is: â€œokay, got it, Cantonese food is a stuffy old cuisine that doesnâ€™t know how to season thingsâ€.And while itâ€™s certainly true that Cantonese food opts not to impress with chili peppers by the fistful, this isnâ€™t exactly Scandinavian hospital food either. Black fermented soybeans feature prominently, ditto with other fermented sauces. The use of dried seafood and mushrooms â€” practically umami concentrates â€”  is ubiquitous. Hell, on an international scale, they might even be less afraid of the chili pepper than their reputation might suggestâ€¦Instead, to me, Cantonese cuisine is all about The Democratization of the Haute. In the age of Imperial China, there was a sort of â€˜national cuisineâ€™ of the elite called Guanfu cuisine, which weâ€™ll touch more on in our discussion on Beijing. During the 19th century, the merchant class in Canton grew in breadth and depth, and the food they ate ended up borrowing quite a bit philosophically from that Guanfu tradition. And what was that tradition all about? Well, outside of simply showing off (the food of the elite forever rhymes), that cuisine particularly loved the playful manipulation of form.To illustrate, the Cantonese Pearl River is the land of two fish - the lingyu (Dace) and the wanyu (Asian Carp). And so, yes, they do sometimes opt to keep that â€˜original flavorâ€™ to make sashimi â€” and Shunde-style Sashimi is quite delicious. But you can also chop some fish up real fine and use it as a stuffing (e.g. for chilis). Or you can take that paste, mix it with egg white, deep fry it and get a dead wringer for tofu. Or you could  pan-fry that fish, shred it, and toss it in a water chestnut starch-thickened soup and make a â€˜congee without riceâ€™. You see this sort of thing again and again in Cantonese food â€” itâ€™s a fun cuisine. I mean, on some level isnâ€™t the entire Dim Sum cart â€œstuff stuffed in stuff, stuffed inside of other stuffâ€? What makes Cantonese food awesome is that every time youâ€™re lounging al fresco at a Daipaidong, every time youâ€™re grabbing steamer baskets from the Dim Sum houseâ€¦ youâ€™re eating an echo of the Qing Dynasty elite. And that, I think, is a beautiful thing.The Pearl River system. Classically, Cantonese culture is considered to follow west along the River Xi until roughly Nanning.Of course, geographically speaking, this cuisine â€” together with Cantonese language and culture writ large â€” is quite bound to the Pearl River system. It starts from the delta smack dab in the middle of the province and continues westward. You can find Cantonese speakers all the way into the Guangxi province next to the Vietnam border, but hop one or two peninsula to the east of the Pearl River Delta and youâ€™re already starting to get into a very different linguistic and culinary zone.To the east, there are two primary cuisines: Teochew and Hakka. Teochew (Chaozhou, in Mandarin) may appear similar to Cantonese at first blush â€” neither cuisine features the chili pepper very prominently â€” but any apparent similarities are quite surface level. Thereâ€™s not much crossover in dishes, and what dishes they do share often have different versions. Cantonese meals are based around white rice; Teochew meals can be served with rice or they can equally be served with mue (ç³œ), a thin rice porridge often paired with salty preserves. Teochew language is mutually unintelligible with Cantonese and more closely resembles the Min languages next door in the Fujian province.With that said, Teochew and Cantonese have obviously had more than a little bit of contact over the centuries. Besides being not all that far apart, both cities were famed for their (rival) merchant classes. Both groups helped build the twin economic engines of Hong Kong (a bit more Cantonese than Teochew) and Shenzhen (a bit more Teochew than Cantonese). This is why Hong Kong fishballs are generally the Teochew style, and why you can find Teochew-style Cheong Fun variants on the streets of Chaozhou and Shenzhen. Hakka, meanwhile, are a fascinating Chinese subgroup thought to have migrated over the centuries from the north of the country down into the south. Precisely why that was is hotly debated, but by the late Qing they nonetheless ended up settling down in the mountainous bits of Jiangxi, Fujian, and Guangdong. As migrants, they were often confined to land that was less productive agriculturally, and the sweet potato formed the foundation of the traditional Hakka diet. Historically, there was no love lost between Hakka and Cantonese - there were even wars fought over land in the mid 19th century.These days however, that history is a distant memory. But Hakka food still bears some imprint of it - while rice is the modern staple, sweet potato features much more heavily than in Teochew and Cantonese. And - apocryphally, at least - a number of classic Hakka dishes are thought to have been forged from attempting to make wheat-flour based northern Chinese foods in a new land that didnâ€™t really have much wheat (e.g. their stuffed tofu is said to be a dumpling equivalent, though I think itâ€™d be fair to register that claim asâ€¦ â€˜debatableâ€™).Dong River Hakka, meanwhile, consist of Hakka communities in Huizhou, Dapeng (Shenzhen), and other bits scattered around the Pearl River Delta1. This is one of those â€˜culinary self determinationâ€™ internal divisions, as Dong River Hakka is generally considered a distinct by many Hakka chefs. A reasonable argument could be made that the food in this area is really more of a cross between Hakka and Cantonese than a distinct cuisine per se, but admittedly we havenâ€™t spent much time in the epicenter of Huizhou.Returning back to the Pearl River system, as Cantonese moves to the west and into the hills, the food begins to change. Flavors become stronger, you start to see some echoes of non-Han influence (e.g. steaming in bamboo), and fermented ingredients start to appear more often than the Cantonese mainstay of dried ingredients. We drew the line between Cantonese proper and its more strongly flavored cousin (â€œXi Riverâ€ cuisine) at the province border, but a little east or west would also make sense. In the hills, we also chose to separate Lianzhou - an area with a good chunk of Yao people influence - out as a distinct cuisine. The Coastal west of Guangdong shares a number of similarities with Cantonese, and could perhaps be lumped in with Cantonese if you were feeling lumpy. That said, the south of Guangxi along the gulf of Tonkin (Beihai), the lowlands of Hainan (Haikou), and the Leizhou peninsula in Guangdong (Zhanjiang) bear some striking similarities - this region sometimes being referred to as â€˜Qin-Lian-Lei-Qiongâ€™ (é’¦å»‰é›·ç¼). The prominence of seafood would probably be the most obvious element of the cuisine to a traveler, but things are also starting to get a bit more heavily seasoned, ala the food in neighboring Guangxi. Their mixed rice dishes are also renowned - most famously the Haikou areaâ€™s chicken rice, which over the years became a favorite throughout Southeast Asia as well. Also interspersed in this area are also communities of Teochew speakers (lost on their way to Southeast Asia, perhaps?), which also influenced the food in a number of ways, most obviously with various rice products called he (ç±º).CantoneseRepresentative dishes:Wonton Mee (äº‘åé¢). Top left. Wonton noodles, a Cantonese classic. Often served in soup, this version was ordered dry with soup on the side - smothered with dried shrimp roe, and oyster sauce to mix.Dim Sum (æ—©èŒ¶). Top middle. This particular dish is Siu Mai (çƒ§éº¦), a pork and shrimp dumpling. Cheong Fun (è‚ ç²‰). Top right. Breakfast rice noodle rolls, topped with a seasoned soy sauce. There can be various stuffings, but are often rather subdued - this specific one is a bit of pork mince. Soy milk is a more common drink to go along with it, but if youâ€™ve got a nice coffeeshop next door, why not?Claypot Rice (ç…²ä»”é¥­). Bottom left. Much beloved Cantonese dish, complete with crispy rice (i.e. socarrat) at the very bottom. This one was topped with pork ribs and Cantonese sausage.Char Siu Rice Bowl (å‰çƒ§é¥­). Bottom middle. Cantonese roast meats are iconic - from Siu Yuk Pork Belly, to Roast Goose, to Roast Duck. This is Char Siu BBQ pork, served with egg in a rice bowl and topped with seasoned soy sauce.Double Boiled Soups (ç‚–å“). Bottom right. Cantonese cuisine is quite renowned for their tradition of soup making - this soup in particular is scallop with hairy gourd (ç‘¶æŸ±ç‚–èŠ‚ç“œ).TeochewRepresentative dishes:Teochew Lushui (æ½®å·å¤æ°´). Top left. Teochew Lushui (master stock) has a unique  spice mix - including additions like lemongrass and galangal. Stewed goose is a very classic item (å¤é¹…) and the â€œlion-head gooseâ€ (ç‹®å¤´é¹…) is particularly prized. This one is a plate of goose breast.Mue (ç³œ). Top middle. Pictured is a simple bowl of mue with its classic pickled pairings. The preserves that goes with Teochew mue congee are called giamzap (å’¸æ‚), meaning â€œvarious salty thingsâ€. Pickled daikon (èœè„¯) is the most classic.Chinese Black Olive with preserved vegetable (æ©„æ¦„èœ). Top right. The most famous giamzap outside of the region is Chinese Black Olive. Pictured is a bottle of pickled mustard green with Chinese black olive (which, I have to say, is also fucking incredible topped over white rice as well). Oyster pancake (èšçƒ™). Bottom right. The coastal southeast - from Chaozhou to Fujian - has a tradition of using sweet potato starch to make various kinds of pancakes, of which oyster pancake is probably the most famous. The most common variant in Chaozhou is (1) a round pancake with (2) eggs added and (3) fried until the edges are crispy.Seafood congee restaurants. Bottom middle. Being a rocky coastal area, historically the region didnâ€™t have much land for rice cultivation, leaning instead on the ocean. Pictured is a local congee restaurant in Jieyang city with fish and seafood laid out front. Choose your seafood, the kitchen whips it up, and you enjoy it next to some mue congee.Guorou (è£¹è‚‰). Bottom right. Common to the area are various forms of â€˜rollsâ€™ containing meat or seafood, rolled up in thin tofu sheets then steamed to form. This one in particular is a type of roll called guorou in Mandarin (è£¹è‚‰, lit. â€˜wrapped meatâ€™) - cut into pieces, deep fried, and served with a sweet fruit sauce.HakkaRepresentative dishes:Stuffed Tofu (in soup) (é…¿è±†è…). Top left. Hakka cuisine loves its stuffed stuff, with tofu being the most famous. It comes in many different shapes, fillings, and ways of serving. Pictured is a variety stuffed with minced pork and salted fish, served in an (excellent) pork broth.Mixed â€˜Yanâ€™ Hakka Noodles (è…Œé¢). Top middle. While you can find various types of â€œHakka Noodlesâ€ all around the world, pictured is the OG version - a simple and delicious affair of noodles mixed with lard, fish sauce, and deep-fried garlic, served along side a bowl of pork bone soup. Mixed â€˜Yanâ€™ Beef (è…Œç‰›è‚‰). Top right. Mixed, or â€˜yanâ€™ dishes feature heavily in Hakka cuisine. In addition to the mixed noodles above, mixed beef is another common dish of its category. There are regional variations - pictured is a Meizhou sort mixed with garlic, chili oil, and Chinese fermented black soybeans.Salt-Baked Chicken (ç›ç„—é¸¡). Bottom right. Salt baked chicken is a Hakka classic that has a couple variants. The most traditional way of making it is baking an entire chicken (wrapped in paper) in a large wok of salt. The meat keeps well and is quite firm, as shown.Taro balls and Daikon balls (èŠ‹å¤´ä¸¸/èåœä¸¸). Bottom middle. Hakka cuisine loves its balls. Meatballs are obviously classic as well, but pictured is (1) a steamed Daikon ball made from mixing shredded Daikon with sweet potato starch and (2) Taro ball made in a similar fashion. Deep frying is also a popular route.Faban, rice cupcake (å‘ç²„). Bottom right.  Pictured is a fluffy rice cupcake called faban (å‘ç²„), colored with red yeast rice. In Hakka language, the latter character, â€˜banâ€™ (ç²„), is a catch-all term that can be used to describe anything made from dough or batter - from cupcakes, to pancakes (éº¦ç²„), to dumplings (è‰¾ç²„), to noodles (è€é¼ ç²„), and even jelly (è‰ç²„). Dong River HakkaRepresentative dishes:Dongjiang-style Stuffed Tofu (ä¸œæ±Ÿé…¿è±†è…). Left. In the Dongjiang region, this dish is most often panfried then quickly simmered sauce. In Hakka restaurants within more Cantonese cultural areas like Guangzhou, this is generally the version that youâ€™ll find on restaurant menus.Pork Belly with Preserved Vegetable, Meicai Kourou (æ¢…èœæ‰£è‚‰). Middle. Meicai Kourou, preserved mustard green with pork belly, is a Hakka classic dish for banquets and festivals. The pork belly is poached, deep fried, sliced into large pieces, then steamed with preserved mustard greens. The one shown here also has taro pieces steamed in between the slabs of pork belly to help absorb the entirely of the flavorful sauce.Three Cup Duck (ä¸‰æ¯é¸­). Right. â€œThree cupâ€™ is a flavor profile originating in Hakka cuisine, which classically braising with one cup each of rice wine, soy sauce, and toasted sesame oil. Three cup chicken is probably the most famous one outside of Hakka region but other poultry can also be used such as duck, goose, or pigeon. Lianzhou (Yao)Representative dishes:Beef Pancake (ç‰›è‚‰ç³). Left. A deep-fried stuffed pancake with a beef filling mixed with shredded daikon, chili, and garlic scape. It uses a shallow ladle as a mold, sandwiching beef in between layers of batter (made from a mix of mashed rice, sweet potato, and taro).Smoked jerky (ç‰›è‚‰å¹²). Middle. The Yao people traditionally would preserve meat via smoking, an approach much more common to Hunanese than Cantonese cooking. Popular in the area is a beef jerky, made by grilling, smoking, then drying marinated beef. The jerky is then eaten by thinly slicing and steaming, or alternatively stir frying with chilis, pickles, and/or black fermented soybean.Chili beef noodle soup (æ˜Ÿå­ç‰›è‚‰é¢). Right. A spicy and sour beef noodle soup topped with lacto-fermented mustard green, pickled chilis, pickled daikon, minced garlic, and chili sauce.Coastal (Qin-Lian-Lei-Qiong)Representative dishes:Duck rice (é¸­ä»”é¥­). Top left. In addition to the famed Chicken Rice (more on this below), other poultry like duck or goose also feature prominently in the category. Cooking method is similar: the poultry is poached, then the rice is cooked together with the birdâ€™s rendered fat and the cooking liquid from poaching. In Leizhou, the set comes with a bowl of duck soup as well. Wet Spicy Beef (æ¹¿è¾£ç‰›è‚‰). Top middle. A newer addition to the areaâ€™s BBQ scene, â€˜wet spicy beefâ€™ are grilled beef skewers dipped in a spicy (sometimes coconut milk-laced) sauce.Seafood Rice Noodles (æµ·é²œæç²‰). Top right. Seafood a major aspect of the areaâ€™s cuisine - from stir fries, braises, to snacks, and rice noodles. Pictured is a flat rice noodle (hor fun) served in a shellfish-based stock topped with various seafood.Chicken rice (é¸¡é¥­). Bottom left. The origin of the world-famed Hainan chicken rice. The variants in the mainland tend to be simpler than its Southeast Asian cousins, opting for somewhat different dipping sauces. The version in the picture was served alongside a sand ginger-soy sauce dip.Beef & Wild Betel Rice Cake (æœºç²½é¥¼). Bottom middle. This chewy rice cake originated from Teochew migrants to the area. While some of these cakes can be quite simple, pictured is a sweet and savory version with beef and wild betel (è›¤è’Œå¶), an herb which adds an herbaceous and peppery punch.Layered rice cake (ç°¸ç®•ç‚Š/æ°´ç±º). Bottom right. This plain layered rice cake is popular throughout the region, with different names and different toppings. Itâ€™s made by steaming white rice batter in layers using a bamboo tray or metal sheet. It can be eaten alongside garlic soy sauce, bean paste, garlic oil, Chinese chive oil, or peanut oil; and topped with toasted nuts, preserved mustard green, pickled daikon, minced pork, and/or dried shrimp.Hainan ProvinceLeft, Location of Hainan in China; Right, Breakdown of Hainan CuisinesThe largest city in Hainan, Haikou, is very much within the Coastal (Qin-Lian-Lei-Qiong) cuisine discussed above. That cuisine was listed under Guangdong mostly for convenience, as Northern Hainan might very well be where the center of gravity is there.In the south - around the tourist city of Sanya - is a very mountainous region that was traditionally the home of the Hlai ethic group, a Taiâ€“Kadai speaking ethnic group thatâ€™s fascinating in their own right. While these days most of the food commercialized along the coast will serve up tourist-focused fair like coconut chicken hotpot (a Shenzhen invention), you can still find Hlai restaurants within the mountainous interior.Around Wanning - Xinglong, specifically - thereâ€™s a fascinating community of Indonesian Chinese that fled to the mainland to escape Suhartoâ€™s purges in the 1960s. Weâ€™ve personally never been to the city, so itâ€™s a little difficult to parse how organic the cuisine is to that pocket - or if itâ€™s simply a sort of tourism production. Weâ€™ll discuss Straits Chinese later in the post. HlaiRepresentative dishes:Leaf Banquet (é•¿æ¡Œå®´).Left. Leaf banquet or â€œlong table banquetâ€ is a festive ritual for Hlai people, with colored rice serving as the centerpiece. Dishes to serve alongside would be their local style of blood sausage, braised palm hearts, grilled fish, etc.Stir fried banana blossoms (ç‚’èŠ­è•‰èŠ±).Middle. Banana blossomâ€™s an ingredient eaten widely throughout the tropical Southeast Asia, and Hainan is no exception. In addition to stir frying (pictured), it can also be braised or stewed with meat.Fermented fish (é±¼èŒ¶).Right. Similar to other Taiâ€“Kadai groups, Hlai also has a penchant for the sour - notably, fermented fish and fermented meat. This version is freshwater fish fermented with salt and sticky rice - after fermentation, it can be eaten straight or steamed.GuangxiLeft, Location of Guangxi in China; Right, Breakdown of Cuisines in GuangxiThe Xi River is the major western portion of the Pearl River system, and as such shares some techniques and philosophies with Cantonese. I sometimes refer to the cuisine as â€˜Cantoneseâ€™s wilder, kinkier cousinâ€™, and itâ€™s comparatively much less afraid of spicy and sour flavors than the eastern portion of said river system.Internationally, itâ€™s a cuisine thatâ€™s a bit infamous for its tradition of dog meat - particularly the city of Yulin. And while I donâ€™t partake in eating dog myself, I do have to hand it to the Yulin people: lifting one culinary middle finger high and proud against the collective indignation of the entirely of the west, and your other against the collective disdain of the cosmopolitan cities of Shanghai and Shenzhen? For a small city, that takes some guts. We should all try to learn a little from Yulin. Mov",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "6bf5f1d4fb0b83cb",
    "title": {
      "en": "Map Features in OpenStreetMap with Computer Vision",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://blog.mozilla.ai/map-features-in-openstreetmap-with-computer-vision/",
    "score": 300,
    "by": "Brysonbw",
    "time": 1742665330,
    "content": "Map Features in OpenStreetMap with Computer Vision\n                Mozilla.ai developed and released the OpenStreetMap AI Helper Blueprint. If you love maps and are interested in training your own computer vision model, youâ€™ll enjoy diving into this Blueprint.\n\n                    David de la Iglesia Castro\n\n                        Mar 4, 2025\n                            â€” 5 min read\n\n            Photo by Stefancu Iulian / Unsplash\n\n            MotivationAt Mozilla.ai, we believe that there are a lot of opportunities where artificial intelligence (AI) can empower communities driven by open collaboration.These opportunities need to be designed carefully, though, as many members of these communities (and people in general) are increasingly worried about the amount of AI slop flooding the internet.With this idea in mind we developed and released the OpenStreetMap AI Helper Blueprint. If you love maps and are interested in training your own computer vision model, youâ€™ll enjoy diving into this Blueprint.Why OpenStreetMap?Data is one of the most important components of any AI application, and OpenStreetMap has a vibrant community that collaborates to maintain and extend the most complete open map database available. If you havenâ€™t heard of it, OpenStreetMap is an open, editable map of the world created by a community of mappers who contribute and maintain data about roads, trails, cafÃ©s, railway stations, and more.Combined with other sources, like satellite imagery, this database offers infinite possibilities to train different AI models.As a long-time user and contributor to OpenStreetMap , I wanted to build an end-to-end application where a model is first trained with this data and then used to contribute back.The idea is to use AI to speed up the slower parts of the mapping process (roaming around the map, drawing polygons) while keeping a human in the loop for the critical parts (verifying that the generated data is correct).Why Computer Vision?Large Language Models (LLM) and, more recently, Visual Language Models (VLM) are sucking all the oxygen out of the AI room, but there are a lot of interesting applications that donâ€™t (need to) use this type of models.Many of the Map Features you can find in OpenStreetMap are represented with a polygon ('Area'). It turns out that finding and drawing these polygons is a very time consuming task for a human, but Computer Vision models can be easily trained for the task (when provided with enough data).We chose to split the work of finding and drawing map features into 2 computer vision tasks using state-of-the-art non-LLM models: Object Detection with YOLOv11, by Ultralytics, which identifies where relevant features exist in an image.Segmentation with SAM2, by Meta, which refines the detected features by outlining their exact shape.These models are lightweight, fast, and local-friendly â€“ itâ€™s refreshing to work with models that donâ€™t demand a high-end GPU just to function. As an example, the combined weights of YOLOv11 and SAM2 take much less disk space (<250MB) than any of the smallest Visual Language Models available, like SmolVLM (4.5GB).By combining these models, we can automate much of the mapping process while keeping humans in control for final verification.The OpenStreetMap AI Helper BlueprintThe Blueprint can be divided into 3 stages:Stage 1: Create an Object Detection dataset from OpenStreetMapThe first stage involves fetching data from OpenStreetMap, combining it with satellite images, and transforming it into a format suitable for training.You can run it yourself in the Create Dataset Colab.For fetching OpenStreetMap data, we use:The Nominatim API to provide users with a flexible way of selecting an area of interest. In our swimming pool example, we use Galicia for training and Viana do Castelo for validation.The Overpass API to download all the relevant polygons using specific tags within the selected area of interest. In our swimming pool example, we use leisure=swimming_pool discarding the ones also tagged with location=indoor.Once all the polygons have been downloaded, you can choose a zoom level. We use this zoom level to first identify all the tiles that contain a polygon and then download them using the Static Tiles API from Mapbox.The polygons in latitude and longitude coordinates are transformed to a bounding box in pixel coordinates relative to each tile and then saved in the Ultralytics YOLO format.Finally, the dataset is uploaded to the Hugging Face Hub. You can check our example mozilla-ai/osm-swimming-pools.Stage 2 - Finetune an Object Detection modelOnce the dataset is uploaded in the right format, finetuning a YOLOv11 (or any other model supported by Ultralytics) is quite easy. You can run it yourself in the Finetune Model Colab and check all the available hyperparameters.Once the model is trained, it is also uploaded to the Hugging Face Hub. You can check our example mozilla-ai/swimming-pool-detector.Stage 3 - Contributing to OpenStreetMapOnce you have a finetuned Object Detection model, you can use it to run inference across multiple tiles. You can run inference yourself in the Run Inference Colab. We also provide a hosted demo where you can try our example swimming pool detector: HuggingFace Demo.The inference requires a couple of human interactions. First, you need to first pick a point of interest in the map:After a point is selected, a bounding box is computed around it based on the margin argument.All the existing elements of interest are downloaded from OpenStreetMap, and all the tiles are downloaded from Mapbox and joined to create a stacked image.The stacked image is divided into overlapping tiles. For each tile, we run the Object Detection model (YOLOv11). If an object of interest is detected (e.g. a swimming pool), we pass the bounding box to the Segmentation model (SAM2) to obtain a segmentation mask.All the predicted polygons are checked against the existing ones, downloaded from OpenStreetMap, in order to avoid duplicates.All those identified as new are displayed one by one for manual verification and filtering.The ones you chose to keep will be then uploaded to OpenStreetMap in a single changeset.Closing thoughtsOpenStreetMap is a powerful example of open collaboration to create a rich, community-driven map of the world. The OpenStreatMap AI Helper Blueprint shows that, with the right approach, AI can enhance human contributions while keeping human verification at the core.In the fully manual process it takes about 1 min to map 2-3 swimming pools, whereas using the blueprint, even without an optimized UX, I can map about 10-15 in the same time (~5x more).It also highlights the value of high-quality data from projects like OpenStreetMap, which enables to easily train models like YOLOv11 to perform object detection â€“ proving that you shouldnâ€™t always throw an LLM at the problem.Weâ€™d love for you to try the OpenStreetMap AI Helper Blueprint and experiment with training a model on a different map feature. If youâ€™re interested, feel free to contribute to the repo to help improve it, or fork it to extend it even further!To find other Blueprints weâ€™ve released, check out the Blueprints Hub.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "caba56a5a2afb958",
    "title": {
      "en": "The Software Engineering Identity Crisis",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://annievella.com/posts/the-software-engineering-identity-crisis/",
    "score": 77,
    "by": "napolux",
    "time": 1742755045,
    "content": "The Software Engineering Identity Crisis\n\n        March 21, 2025\n\n      Many of us became software engineers because we found our identity in building things. Not managing things. Not overseeing things. Building things. With our own hands, our own minds, our own code.\nBut that identity is being challenged.\nAI coding assistants arenâ€™t just changing how we write software - theyâ€™re fundamentally transforming who we are. Weâ€™re shifting from creators to orchestrators, from builders to overseers. From engineers to something that looks suspiciously likeâ€¦ managers.\nThe irony cuts deep: for years, weâ€™ve said that software engineering transcends mere coding. Requirements, design, testing, operations - these were all supposedly part of our craft. Yet the industry pushed us in the opposite direction. We handed these responsibilities to specialists - Product Owners, Architects, Quality Engineers, Platform Engineers - while we doubled down on our coding expertise. We became masters of our code, proud wielders of a modern magic.\nAnd now, just as weâ€™ve perfected this craft, AI is threatening to take it away from us.\nThe Joy Weâ€™re About to Lose\nLetâ€™s be honest about whatâ€™s at stake here. Many of us donâ€™t just write code - we love writing code. Our identity is woven into every elegant solution we craft, every test we make pass, every problem we solve through pure logic and creativity. Itâ€™s not just work, not just a craft - itâ€™s who we are.\nThink about those moments of deep satisfaction: when you finally track down that elusive bug thatâ€™s been haunting production, when you work out how to optimise that slow algorithm and watch response times drop from seconds to milliseconds, when you transform a maze of legacy code into something clean and maintainable. These arenâ€™t just achievements - theyâ€™re expressions of who we are as engineers. Theyâ€™re the moments that remind us why we chose this path.\nNow imagine AI taking over these moments of craftsmanship. The creators of these tools paint an optimistic picture - they say weâ€™ll spend more time on defining intent, high-level architecture, and systems thinking. But listen carefully to what theyâ€™re really saying: weâ€™ll become overseers rather than creators, managers rather than builders.\n\n      Are software engineers evolving into system thinkers and high-level architects?\n\nThis shift raises important questions about our identity as builders: Is oversight what drives us? Is it what makes us leap out of bed in the morning, eager to solve the next puzzle?\nThe Identity Shift: Itâ€™s Already Here\nThis isnâ€™t some theoretical future weâ€™re debating - itâ€™s the reality weâ€™re living right now. When Google recently revealed that AI generates more than a quarter of their new code, it was just the beginning. Y Combinator CEO Garry Tan reveals that for about a quarter of their startups, 95% of their code is now written by AI - marking a truly significant shift. My own Masterâ€™s research revealed a similar picture: 77% of us are spending less time writing code, and almost half believe our core skill might become secondary to prompt engineering. Think about that transition: from crafting solutions to crafting prompts.\n\n      Will Prompt Engineering Replace Traditional Coding Skills?\n\nWhen asked how to develop prompt engineering skills, software engineers emphasise refining communication skills. Getting an AI to do what you want means being able to articulate things well - providing just the right amount of context along with a clear description of the task. The better you can communicate with Gen AI, the more likely the output will match your expectations. Some even suggest being polite to these tools, treating them like team members - as if youâ€™re leading another human being to do something for you.\nThe changes run so deep that weâ€™re inventing new terms just to describe who weâ€™re becoming. Take vibe coding, a term that Andrej Karpathy recently coined in a tweet. It captures a profound shift in how we write software.\nAt one end of the spectrum lies the traditional way - the craftspersonâ€™s way. We write each line with intent, every function name and architectural decision reflecting our deep understanding of the system.\nAt the other end? We let AI fill in the blanks, â€œvibingâ€ with its suggestions. We focus on the what, not the how. As Karpathy puts it: â€œfully give in to the vibes, embrace exponentials, and forget that the code even exists.â€\nThat last part gives us pause - are we still engineers if we forget all about the code?\nIn a recent pairing session, engineering thought leaders Gene Kim and Steve Yegge demonstrated what this looks like in practice. Using an AI coding assistant, they ported a 3,500-line legacy Ruby script to Kotlin - a task that would normally take a week - in just one hour. The AI didnâ€™t just translate the code; it improved it, adding the modular architecture and unit tests theyâ€™d wanted for years but couldnâ€™t justify the time for.\nEven Patrick Debois, the godfather of DevOps, sees this transformation reshaping our identity. In his recent analysis of AI Native Development patterns, he outlines four fundamental shifts in how we work:\n\n      Patrick Debois: The 4 Patterns of AI Native Dev\n\nThese patterns reveal a profound shift: weâ€™re moving from producers to managers of AI systems, from detailed implementation to expressing intent, from delivery to discovery through rapid experimentation, and from content creation to knowledge curation. Our role is evolving to blend creation with orchestration, building with oversight.\nOverall, I think itâ€™s fair to say that the nature of our professional identity is changing at its core.\nThe Craft That Shaped Our Identity\nTo understand this identity crisis, we need to look at how deeply the craft of coding has shaped who we are. At its core, writing code is about mastery and control - skills weâ€™ve spent years perfecting. Modern programming languages are much higher-level than those of days gone by, but they still require deep technical understanding. Few developers today deal with the nitty-gritty of pointers and memory management, yet we still take pride in knowing how things work under the hood. Even as frameworks do more heavy lifting, weâ€™ve maintained our identity as artisans who understand our tools intimately.\nProgramming today is much more about stitching together APIs, frameworks, and libraries in creative ways to build something meaningful. In fact, recent research at Google suggests that creativity in software engineering centres on the concept of clever reuse over pure novelty. This makes sense to me - Iâ€™ve often commented that weâ€™re all just â€˜integrationâ€™ engineers nowadays, really.\nStill, thereâ€™s a strange pride that we take in knowing all the weird syntax required to build something. Itâ€™s like a secret language that only we understand. Becoming proficient with a programming language gives you a lot of control to make it do precisely what you want it to do. Itâ€™s so detailed - just one wrong character will break the entire thing and it can take a lot of time and patience to make it do what you want.\n First, one must perform perfectly. The computer resembles the magic of legend in this respect, too. If one character, one pause, of the incantation is not strictly in proper form, the magic doesnâ€™t work.\nâ€” Frederick P. Brooks, The Mythical Man-Month, Chapter 1, Addison-Wesley, 1975\nThe other 99% of the population thinks weâ€™re magicians for understanding code and indeed, it can take years of deliberate practice to master it. Those who master more than one programming language have the privilege of being referred to as polyglots. Many of us take great pride in writing clean, elegant code. We passionately argue over different styles and best practices, often taking it all too seriously.\nA Reluctant Managerâ€™s Tale\nLet me share a story about identity evolution that might resonate.\nAfter a decade of being an individual contributor, I hit the infamous ceiling of the technical track. Senior Lead Software Engineer - that was as far as the technical track went. Staff+ Engineering wasnâ€™t a thing yet, and the sole Architecture position at the company I was working for was filled. I faced a choice that would change who I was: stay a builder or become an overseer.\nI chose management. Reluctantly. Thatâ€™s where the path led me. I told myself it was still engineering, just at a different level. That managing systems wasnâ€™t so different from managing people. That I could still keep my hands in the code in between other tasks.\nSound familiar? The parallels are uncanny. Just as I had to trade direct problem-solving for meetings and documentation, weâ€™re now being asked to trade coding for prompt engineering. The skills that defined us as engineers - mastering syntax, architecting our code elegantly, catching and handling the edge cases, debugging complex issues - are being relegated to AI. Instead, weâ€™re told to focus on skills that sound suspiciously like management: clear communication, systems thinking, problem definition.\nBut hereâ€™s what no oneâ€™s talking about: the identity crisis. That deep sense of loss when you realise youâ€™re no longer building things with your own hands. When your technical mastery becomes less relevant than your ability to â€œmanageâ€ the tools. When your craft becomes oversight.\nCan orchestrating AI ever give us that same sense of identity? Of being a builder, a creator, a problem solver?\nWhen Machines Challenge Our Identity\nBy now the source of our identity crisis becomes clear. The craft we spent years perfecting - the one that gave us purpose, meaning, and pride - is now being done faster, cheaper, and at scale by a machine. Sure, the quality isnâ€™t as good as your hand-written code (yet). But the speed at which code can now be written is staggering and businesses are tripping over themselves to get in on the action.\nThis is where a glimmer of hope emerges. Remember that irony - how we gave away the broader aspects of our craft to specialists? AI is pushing us to reclaim what we once knew: that software engineering transcends mere coding. That core truth remains - ultimately, software engineering is about solving problems, creating solutions, building things that matter.\nThese broader skills - what Addy Osmani calls â€œdurable engineering skillsâ€ in his article on the human 30% of AI-assisted coding - have always separated great engineers from good ones. Communication, big-picture thinking, handling ambiguity - these become even more crucial in an AI-driven world.\nYet this emphasis on broader skills has sparked debate in our community. For some, it sounds suspiciously like management repackaged. And theyâ€™re not entirely wrong - a recent CIO article confirms that development teams are already being restructured to focus on oversight rather than creation. The article envisions future teams consisting of a product manager, a UX designer, and a software architect who primarily uses AI to generate prototypes. These architects, or senior developers, must â€œunderstand the contentâ€¦ who the customer is and what weâ€™re trying to achieveâ€ - classic management responsibilities repackaged as technical work.\n\n      Management in a Cloak\n\nThis evolution raises fundamental questions about our identity as engineers: As the traditional career ladder transforms, how will the next generation of software engineers develop their skills? How do we preserve the deep technical understanding and craftsmanship that shaped our profession while embracing these new tools? And perhaps most unsettling - as AI capabilities advance exponentially, will our role as craftspeople become as obsolete as the manual weaverâ€™s did during the Industrial Revolution?\nThe Path Forward\nPerhaps the answer lies not in resisting this transformation, but in understanding it through the lens of history. These identity crises - these fundamental shifts in how we define ourselves through our work - arenâ€™t new. Theyâ€™re part of a pattern that repeats whenever technology reshapes a profession.\nDuring the Industrial Revolution, craftspeople faced a similar crisis. Their traditional skills - honed over generations - were being replaced by machines. But what happened next is fascinating: many adapted, becoming specialist professionals who could fix and improve these same machines that threatened to replace them. Others found ways to apply their deep understanding of materials and processes to improve overall factory operations.\nIf we draw this parallel to our AI era, a similar path emerges. The core of software engineering - solving problems and creating value - remains unchanged. Our tools are evolving, and with them, the skills needed to wield them effectively.\nThe question isnâ€™t whether weâ€™ll become managers of machines - itâ€™s whether we can find the same satisfaction in this evolution of our craft.\nThe Engineerâ€™s Dilemma\nSo where does this leave us? Are we all destined to become overseers of AI agents rather than writers of code? Is this a future to resist or embrace?\nThe truth, as always, is nuanced. Just as some engineers naturally gravitate toward management while others prefer to remain hands-on, weâ€™ll likely see a similar spectrum emerge in how we interact with AI. Some will excel at orchestrating AI systems, focusing on high-level design and making these systems more efficient and reliable - conducting a technical symphony rather than performing solo. Others will find their calling in domains where human expertise remains crucial - perhaps in security-sensitive applications, novel areas where AI lacks training data, or systems where performance and reliability are paramount. The key isnâ€™t to resist this evolution, but to find our place within it.\nWhatâ€™s clear is that the definition of â€œsoftware engineerâ€ is expanding, not contracting. The skills that make someone valuable are diversifying. And this creates both challenges and opportunities.\nFor those who love the craft of coding, this shift can feel threatening. But remember that AI tools are still just that - tools. They donâ€™t understand the â€œwhyâ€ behind the code, the business context, or the human needs being served. They canâ€™t innovate in the true sense of the word, at least not yet. And as far as we know, they canâ€™t feel the satisfaction of solving a complex problem or the joy of creating something new.\nPerhaps the most valuable skill in this new landscape isnâ€™t prompt engineering or systems architecture, but adaptability - the willingness to evolve, to learn new skills, and to find your unique place in a rapidly changing field.\nThe Bright Side\nDespite these challenges, thereâ€™s something important we need to acknowledge: these AI tools can be incredibly empowering. With agentic IDEs like Windsurf and Cursor taking software development to a whole new level, itâ€™s like having a supportive pair-programming partner whoâ€™s always there, ready to help you tackle problems that might have seemed daunting before.\nFor junior developers or those of us who might feel a bit rusty, AI assistants can be a confidence booster - helping you get started when youâ€™re staring at a blank file, validating your approach when youâ€™re unsure, or explaining complex concepts in a way that makes sense to you. For experienced developers, theyâ€™re like having a tireless assistant who can handle the routine tasks while you focus on the more challenging aspects of the problem.\nThe speed at which we can now prototype ideas, explore different approaches, and learn new technologies is truly remarkable. What might have taken weeks of research and trial-and-error can often be accomplished in hours or even minutes. Itâ€™s like having a superpower - the ability to amplify our capabilities and turn our ideas into reality faster than ever before.\nThe Reality Check\nBut with great power comes great responsibility. A recent comprehensive GitClear study analysing 211 million lines of code revealed some concerning trends as AI code generation tools became more prevalent:\n\nA 17.1% increase in copy-pasted code, marking the first time AI-assisted code duplication surpassed refactored (moved) code.\nAn 8-fold rise in duplicated code blocks, with 6.66% of commits now containing repeated code sections.\nA 26% increase in code churn, with 5.7% of all code changes being revised or deleted within two weeks.\n\n      GitClear: Trends in Code Changes\n\nWhile weâ€™re producing code faster than ever, weâ€™re also spending more time fixing AI-generated mistakes and dealing with code thatâ€™s harder to maintain. This isnâ€™t just about speed - itâ€™s about the craft of writing sustainable, maintainable software.\nThe Hidden Identity Crisis\nYet beneath these surface-level changes lies a deeper challenge - one that strikes at the heart of who we are as engineers. The emerging field of Human-AI teaming is revealing uncomfortable truths about our future. A 2024 study shows that when humans and AI work together, the results often fall short of expectations. Not because AI lacks capability, but because trust works differently with machines than with humans.\nWe donâ€™t build trust with AI the way we do with human teammates.\nWith humans, trust grows gradually, through shared success. Each problem solved together strengthens the bond. Even failures can deepen trust when handled well. With AI, trust often starts high and erodes fast.\nEvery incorrect response, every hallucinated bug fix, every misplaced confidence weakens our faith in the machine. Unlike human relationships where trust typically grows over time, AI trust often peaks early and declines.\nAnd when trust erodes, so does productivity.\nThe research reveals why:\n\nThereâ€™s an inherent unpredictability in how AI interprets our intentions\nIt lacks the contextual awareness that makes human collaboration fluid\nIts decisions often lack transparency, making it hard to rebuild trust once lost\n\nThese challenges mirror what many of us experience when transitioning into technical leadership. Just as new engineering managers must learn to trust their teamâ€™s work without doing it themselves, we now face a similar transition with AI - learning to guide and verify rather than write every line ourselves.\nThe reality is stark: despite AIâ€™s raw capabilities, teams often perform worse with AI than without it. Just as a teamâ€™s productivity suffers under ineffective leadership, our effectiveness diminishes when we donâ€™t understand how to work with our AI tools.\nReclaiming Your Identity\nDrawing from both my journey as a reluctant manager and my research into this AI transformation, I see three ways we might preserve our identity as builders:\n\nResist - Some will choose to focus on domains where human creativity and deep technical expertise remain essential\nAdapt - Others will embrace AI orchestration, becoming conductors of a new kind of technical symphony\nBalance - And many, like myself, will seek a middle path - using AI for routine tasks while preserving the joy of direct problem-solving\n\nThen I had a realisation that changed my perspective: We donâ€™t have to choose just one path.\nThe Identity Pendulum\nPerhaps the answer to our identity crisis lies in the Engineer/Manager pendulum. My own journey between these roles taught me something crucial about identity:\n\nManagement didnâ€™t replace my engineering identity - it expanded it\nReturns to hands-on work werenâ€™t steps backward - they were identity renewals\nThe pendulum swing itself became part of who I am - adaptable, growing, evolving\n\nAnd thatâ€™s when it hit me: This is exactly the model we need for the AI age.\nInstead of being forced into permanent â€œAI managers,â€ what if we could swing between:\n\nDeep technical work where we write and refine code directly\nStrategic orchestration where we guide AI systems\nCreative problem-solving that combines both approaches\n\nThis balanced approach resonates deeply with what Iâ€™m hearing from fellow engineers. My research shows a clear message: maintaining strong engineering fundamentals is more crucial than ever. We need deep technical knowledge to effectively review, verify, and adjust AI-generated code - because itâ€™s often not quite right. When asked about their concerns with AI coding assistants, software engineers ranked code quality and security well above job security.\n\n      Top Concerns About AI Coding Assistants Among Software Engineers\n\nThat tells me something profound: we see ourselves as guardians of engineering excellence, ensuring that AI-generated solutions adhere to solid software engineering principles. Weâ€™re not looking to delegate our expertise to AI - weâ€™re evolving to apply our craft in new ways.\nYour Move\nAs we navigate this transformation, a fundamental truth emerges: our identity crisis isnâ€™t really about AI at all. The research into Human-AI teaming, the parallels with management transitions, the pendulum of roles - they all point to something deeper. Beyond choosing between builder or overseer lies the heart of who we are: creators.\nAnd now we come full circle: AI isnâ€™t taking our jobs; itâ€™s giving us a chance to reclaim those broader aspects of our role that we gave away to specialists. To return to a time when software engineering meant more than just writing code. When it meant understanding the whole problem space, from user needs to business impact, from system design to operational excellence.\nThe pendulum metaphor offers us wisdom here. Just as many of us have swung between engineering and management roles, we can embrace a similar fluidity with AI. Some periods weâ€™ll dive deep into the code, experiencing that thrill of crafting elegant solutions. Other times weâ€™ll step back to guide AI systems - not as overseers, but as master builders who understand every part of their craft. Like the Industrial Revolutionâ€™s workers who became experts at optimising the machines that transformed their craft, we can master these AI systems - making them instruments of our creativity, not replacements for it.\nIn the AI era, what matters most is preserving the essence of who we are: that pure joy of building things, of solving hard problems, of making something work exactly right. Our engineering excellence runs deeper than just verifying AIâ€™s work - it flows from knowing systems so intimately that we can shape them, improve them, transform them.\nThe choice isnâ€™t whether AI will change our industry - it already is. The real choice is how we evolve with it. Will we cling to an outdated sense of what it means to be an engineer? Or will we reclaim our craft, not as mere coders, but as master builders of AI-augmented systems?\nThe pendulum is swinging - will you hold your ground, or move with it?",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "0a93402f82a14892",
    "title": {
      "en": "European Cloud, Global Reach",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://upcloud.com/blog/european-cloud-global-reach",
    "score": 209,
    "by": "Sami_Lehtinen",
    "time": 1742806194,
    "content": "The shifting global geopolitical landscape is forcing many organizations to revisit their cloud strategies to ensure stability, security, and future scalability. As the world looks to migrate data and services to Europe, organisations should take care to evaluate cloud service providers (CSPs) on their certifications, security controls, and ability to support regulatory requirements relevant to their industry.Not every European CSP is the same and itâ€™s worth digging deeper into what features and guarantees to look for, to fast-track business growth.Read more about best practices in Cloud Compliance and download our free check listData ResidencyBeing able to choose the jurisdiction where you host your data is one of the greatest benefits of cloud. You control your data, server storage location, and determine the retention period â€“ your data, your rules.However, understanding applicable data residency requirements is crucial when selecting cloud regions to ensure compliance with the data transfer restrictions imposed by data protection laws, authority guidelines or your customersâ€™ requirements. While most CSPs offer European cloud regions, itâ€™s important to ensure the whole supply chain, including possible sub-processors of your CSP, comply with your data residency requirements.Because UpCloudâ€™s systems and operations are based in Finland, your data is protected by European regulations when you choose one of our European data centres. We never move your data from the selected data centre unless you specifically request it. And with data centres in 13 global locations, at UpCloud you can easily choose which country you want to store your data via the UpCloud control panel.To strengthen our European data residency, we also have an EU Access Management Policy in place, ensuring only EU-based employees have privileged full remote access to operating systems within our EU data centres.Data SecurityISO/IEC 27001:2022 is the worldâ€™s best-known standard for information security management systems. It provides guidance for establishing, implementing, maintaining and continually improving an information security management system. This standard is not a one-off certification. Holders are regularly audited by an independent third party to ensure adherence to high standards and the efficiency of security controls. Choosing a provider with an up-to-date ISO 27001 certification is a vote for robust data security management.At UpCloud, we are committed to complying with European data protection laws and compliance with ISO 27001. This international standard not only signifies our dedication to maintaining a high level of information security but also ensures that we adhere to recognized best practices in managing and safeguarding your data.To ensure a comprehensive and multi-faceted approach to security, we are also aligned with ISO31000:2018 and NIST CSF. We also have a bug bounty program and offer a public Vulnerability Disclosure Program for reporting vulnerabilities. Alongside, we are certified and audited annually to ensure we remain committed and aligned to the CISPE Code of Conduct.The Cloud Infrastructure Services Providers in Europe (CISPE) is a non-profit organisation with members that include OVH, Hetzner, Leaseweb, Aruba, and UpCloud. The CISPE Code of Conduct focuses on data protection principles, and adhering to this ensures that your data remains within your control, isnâ€™t used for anything other than what youâ€™ve authorized, and remains in the EEA (EU countries, Norway, Liechtenstein, and Iceland), providing an additional layer of protection given the stringent data protection laws in place.Go Beyond ComplianceThe General Data Protection Regulation (GDPR) is the best-known European standard. After all, itâ€™s hard to miss the opportunity to reject cookies on European websites. But thereâ€™s far more to data security.Europe enforces the most robust data protection regulations in the world. When migrating to a European Cloud, choose a CSP that complies with the EU Regulatory Framework on Data, not just GDPR.UpCloud prioritises compliance with the EU regulatory framework on data, adhering to:Digital Services Act (2024) â€“ prevents illegal and harmful activities and content, protects fundamental rights, and obligates the removal of illegal content.NIS2 (2024) â€“ the Network and Information Security directive established a unified legal framework to uphold cybersecurity in 18 critical sectors across the EU.DORA (2025) â€“ governs ICT risk management, mandating cyber resilience for the financial sector and their service providers.Data Act (2025) â€“ prevents vendor-locks and prohibits unfair contract terms.Data Governance Act (2023) â€“ provides structures and rules for data sharing.Digital Markets Act (2023) â€“ governs large online platforms to ensure fair business practices.ESG directive â€“ while the Corporate Sustainability Reporting directive governs data service providers, we choose to operate transparently and openly share our ESG reporting.Why choose a European cloud service provider?When considering a move to Europe, organisations should establish clear compliance requirements based on industry regulations, legal obligations, and internal policies. This ensures a structured approach to maintaining security and regulatory adherence in the cloud.UpCloud is a proudly European-owned organization. With 13 global data centers, and a cloud native product stack engineered for exceptional scalability, speed, and security, weâ€™re the European force challenging what the cloud should be.The heart of our promise at UpCloud is the unwavering security of your data. Certifications like ISO 27001 and adherence to the CISPE Code of Conduct demonstrate our commitment to this promise. UpCloud is the trusted European partner for your cloud infrastructure needs.Read more about how UpCloud supports customer compliance and protects customer data.Planet icons created by Freepik â€“ Flaticon",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "a357dc585a05941e",
    "title": {
      "en": "Did you spot a fish? Press the Fish Doorbell",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://visdeurbel.nl/en/",
    "score": 77,
    "by": "rguiscard",
    "time": 1742801480,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "3afe2dbf1082bde5",
    "title": {
      "en": "EmptyEpsilon open source spaceship bridge simulator",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://daid.github.io/EmptyEpsilon/",
    "score": 149,
    "by": "mmooss",
    "time": 1742486151,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "af6f19d8a0fd65d6",
    "title": {
      "en": "300-year-old Polish beech voted Tree of the Year",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.bbc.co.uk/news/articles/c20dd6yk55yo",
    "score": 211,
    "by": "n1b0m",
    "time": 1742416678,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "cabaa32c45bba776",
    "title": {
      "en": "The persistent mischaracterization of Google and Facebook A/B tests",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.sciencedirect.com/science/article/pii/S0167811624001149",
    "score": 43,
    "by": "PaulHoule",
    "time": 1742784882,
    "content": "There was a problem providing the content you requested\nPlease\n contact our support team\nfor more information and provide the details below.\n  Reference number: 9256308c4e33ea0b IP Address: 175.196.243.166 User Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/134.0.6998.35 Safari/537.36 Timestamp: 2025-03-24 12:32:09 UTC\n::CLOUDFLARE_ERROR_1000S_BOX::",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "ead20f3f44763bd9",
    "title": {
      "en": "Hitts.cc â€“ Advanced Text to Speech with GPT-4o Mini TTS",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://hitts.cc",
    "score": 4,
    "by": "marytaylor",
    "time": 1742816717,
    "content": "This tool uses OpenAI's GPT-4o mini TTS API to convert your text into natural-sounding speech. The process works in three simple steps:\n\n                            Input your text: Type or paste the content you want to convert to speech.\n\n                            Customize settings: Choose from six different voices and adjust the speech speed.\n\n                            Generate audio: Our system sends your text to the GPT-4o mini TTS model, which processes it and returns high-quality audio.\n\n                        The generated audio is streamed directly to your browser without being stored on our servers. Try different voices and speeds to find the perfect combination for your content!",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "dc82356fa3518944",
    "title": {
      "en": "Most AI value will come from broad automation, not from R & D",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://epoch.ai/gradient-updates/most-ai-value-will-come-from-broad-automation-not-from-r-d",
    "score": 172,
    "by": "ydnyshhh",
    "time": 1742668500,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "2852f55e961e65fa",
    "title": {
      "en": "Next.js version 15.2.3 has been released to address a security vulnerability",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://nextjs.org/blog/cve-2025-29927",
    "score": 226,
    "by": "makepanic",
    "time": 1742678347,
    "content": "Back to BlogSaturday, March 22nd 2025CVE-2025-29927Posted byLee Robinson@leerobNext.js version 15.2.3 has been released to address a security vulnerability (CVE-2025-29927). Additionally, backported patches are available.\nWe recommend that all self-hosted Next.js deployments using next start and output: 'standalone' should update immediately.\nContinue reading for more details on the CVE.\nTimeline\n\n2025-02-27T06:03Z: Disclosure to Next.js team via GitHub private vulnerability reporting\n2025-03-14T17:13Z: Next.js team started triaging the report\n2025-03-14T19:08Z: Patch pushed for Next.js 15.x\n2025-03-14T19:26Z: Patch pushed for Next.js 14.x\n2025-03-17T22:44Z: Next.js 14.2.25 released\n2025-03-18T00:23Z: Next.js 15.2.3 released\n2025-03-18T18:03Z: CVE-2025-29927 issued by GitHub\n2025-03-21T10:17Z: Security Advisory published\n2025-03-22T21:21Z: Next.js 13.5.9 released\n2025-03-23T06:44Z: Next.js 12.3.5 released\n\nVulnerability details\n\nNext.js uses an internal header x-middleware-subrequest to prevent recursive requests from triggering infinite loops. The security report showed it was possible to skip running Middleware, which could allow requests to skip critical checksâ€”such as authorization cookie validationâ€”before reaching routes.\nImpact scope\n\nAffected\n\nSelf-hosted Next.js applications using Middleware (next start with output: standalone)\nThis affects you if you rely on Middleware for auth or security checks, which are not then validated later in your application.\nApplications using Cloudflare can turn on a Managed WAF rule\n\nNot affected\n\nApplications hosted on Vercel\nApplications hosted on Netlify\nApplications deployed as static exports (Middleware not executed)\n\nPatched versions\n\nFor Next.js 15.x, this issue is fixed in 15.2.3\nFor Next.js 14.x, this issue is fixed in 14.2.25\nFor Next.js 13.x, this issue is fixed in 13.5.9\nFor Next.js 12.x, this issue is fixed in 12.3.5\n\nIf patching to a safe version is infeasible, it is recommended that you prevent external user requests which contain the x-middleware-subrequest header from reaching your Next.js application.\nOur security responsibility\n\nNext.js has published 16 security advisories since 2016. Over time, we've continued to improve how we gather, patch, and disclose vulnerabilities.\nGitHub Security Advisories and CVEs are industry-standard approaches to notifying users, vendors, and companies of vulnerabilities in software. While we have published a CVE, we missed the mark on partner communications.\nTo help us more proactively work with partners depending on Next.js, and other infrastructure providers, we are opening a partner mailing list. Please reach out to partners@nextjs.org to be included.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "633618d629b017a8",
    "title": {
      "en": "Show HN: Interactive Map of Tom Scott Videos",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.sambaumann.com/tom-scott-map/",
    "score": 4,
    "by": "sambaumann",
    "time": 1742816337,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "7a42a657c8f2f87d",
    "title": {
      "en": "The Myth of Liquid Detection",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://eclecticlight.co/2025/03/16/last-week-on-my-mac-the-myth-of-liquid-detection/",
    "score": 52,
    "by": "frizlab",
    "time": 1742484766,
    "content": "hoakley\n\nMarch 16, 2025\n\t\t\t\t\tMacs, Technology\n\n\t\tLast Week on My Mac: The myth of liquiddetection\n\n\t\tMacs have developed their own mythology, and this week I unintentionally came across a myth that developed over a year ago. Like so many it was born from a chance observation, this time of a new background process that appeared on 25 October 2023 in macOS Sonoma 14.1, and was reported in 9to5Mac on 3 November 2023.\nDiscovery\nThe process in question is liquiddetectiond, and as 9to5Macâ€™s headline claimed, it meant that â€œMacs can now inform Apple if any liquids have been detected in the USB-C portsâ€. That article argued that â€œit seems more likely that the data collected by this daemon will be used for technicians to determine whether a Mac is eligible for free repair.â€ â€œPutting a digital liquid detector on USB-C ports is just another way to ensure that technicians are right about claiming that a Mac has been exposed to liquids.â€\nThat, and a couple of linked reports elsewhere, brought a small flurry of comments about how typical this was of Apple, then all went quiet until 9to5Macâ€™s article was picked up by Hacker News on 9 January 2024 and generated 340 comments. Predictably, most either castigated Appleâ€™s behaviour or disappeared down rabbit holes about unrelated topics. Among them, though, was one precious insight: â€œIt prevents the device from applying/draining power from any pin in such a state, mainly to reduce corrosion of the contacts and increase longevity.â€\nBy the middle of January last year, the story had gone cold, and everyone must have gone away with their worst fears confirmed. You couldnâ€™t even get a USB-C port damp in your Mac any more, as Apple would use that as an excuse to void your Macâ€™s warranty.\nDocumentation\nAppleâ€™s first word on the subject seems to have been in a support note published on 23 November 2024, which passed largely unnoticed. This announced liquid detection as a feature new to macOS Sequoia when running on only the following models:\n\nMacBook Air M3 and later\nMacBook Pro with M3 Pro or Max\nMacBook Pro with M4 base, Pro or Max\n\nnone of which had been released at the time of 9to5Macâ€™s report, although the second were released four days later, on 7 November 2023.\nIf thereâ€™s liquid in one of their USB-C receptacles (ports) when a USB-C cable is connected to it, this new sensor should detect it and alert the user, advising them to shut the Mac down, disconnect all cables and leave it to dry.\nThis is in addition to, and separate from, what Apple terms Liquid Contact Indicators (LCI), that have long been fitted to laptop Macs and some Apple wired and wireless keyboards â€œto help determine if these products have been exposed to liquid,â€ according to this support note.\nWas Apple just making excuses, or was this new liquiddetectiond service intended to benefit the user?\nEvidence\nI stumbled into this innocently last week when I was looking at Accessory Security, a feature confined to laptop Apple silicon models. By chance, the laptop I was using was a MacBook Pro M3 Pro, one of the few in which liquid detection works. There, on several occasions in its log, after connecting a Thunderbolt cable, its liquid detection system checked that the USB-C port was dry, in a series of log entries like:\n0.887 liquiddetectiond Starting LDCM Now\n0.887 liquiddetectiond LDCM Discovery is enabled.\n0.889 liquiddetectiond LDCM - Matched with V4...\n0.890 liquiddetectiond LDCM - checkIsReceptacleEmpty: 0\n0.890 liquiddetectiond LDCM - Handling LDCM interrupt event for port 2\n0.890 IOAccessoryManager IOPortFeatureLDCMUserClient::_copyData(): Copying LDCM data... (target: Port-USB-C@2/LDCM)\n0.890 liquiddetectiond LDCM - Feature Status: 0, Completion Status: 0, Measurement Pin: 0 Mitigations Status: 0, Wet: 0, Wet State Duration: 0\n0.890 liquiddetectiond LDCM - checkIsReceptacleEmpty: 0\n0.890 liquiddetectiond LDCM: liquidDetected: 0, receptacleEmpty: 0, shouldShow: 0\n(Times given in seconds elapsed.)\nBut on my more recent Mac mini M4 Pro running Sequoia, all I saw was that LDCM is not supported on this device.\nAttempts to connect over the network are obvious in the log, and on not one of the occasions that liquid detection was performed did that MacBook Pro try to connect to any remote site. Maybe its reports could have been embedded in other analytics data passed to Apple later, but there was absolutely no evidence that the results of liquiddetectiond went beyond the confines of my Mac.\nThis demonstrates the importance of testing out hypotheses, and of reading the log. Even without the benefit of Appleâ€™s recent support note, it should have been easy to demonstrate this behaviour, yet no one seems to have attempted to.\nExplanation\nClaims made of the role of liquid detection in USB-C ports also donâ€™t make sense. As with most laptop manufacturers, Apple already builds Liquid Contact Indicators into components of laptop Macs within their case. These are most frequently affected by spillage of drinks on a laptopâ€™s keyboard, resulting in any of a wide range of water-based liquids from coffee to cognac entering the case. That often results in extensive damage to the logic board and other components, that are expensive to replace.\nBut a damp USB-C port is quite a different matter. It could occur in a laptop that had been out in the cold and was then brought into a warm and more humid environment, the same sort of conditions that steam up your spectacles. Over time, that could lead to corrosion of the contacts in the USB-C ports, and unreliable connections.\nBecause each release of macOS is identical across all models of Mac, although only a few of the most recent models feature liquid detection sensors in their USB-C ports, the liquiddetectiond service runs in the background of all Macs running Sequoia. Itâ€™s to be found inside /System/Library/CoreServices/liquiddetectiond.app, which isnâ€™t even a bundle, just its Mach-O binary and an image of the warning sign displayed. Itâ€™s run through its LaunchDaemon com.apple.liquiddetectiond.plist, which youâ€™ll also find in the SSV of every Mac.\nAs is so often the case, the truth behind the myth is more prosaic, and doesnâ€™t involve Apple secretly capturing data from your Mac, nor conspiring to dodge warranty repairs. In fact, if you look at the warranty terms of pretty well every other laptop manufacturer, they too exclude damage caused by liquid ingress, as demonstrated by their Liquid Contact Indicators. And some are also starting to fit similar liquid detection sensors in their USB-C ports. But donâ€™t let those get in the way of a good myth.\nShare this:TwitterFacebookRedditPinterestEmailPrintLike Loading...\n\n\tRelated\nUSB ports on Apple Silicon Macs: Accessory Security and liquiddetectionMarch 13, 2025In \"Macs\"A lexicon of Mac power adaptors, from MagSafe toUSB-CJanuary 1, 2019In \"Macs\"External bootable disks work better with M1 Macs in Big Sur11.3May 3, 2021In \"Macs\"\n\n\t\tPosted in Macs, Technology and tagged Apple, liquid detection, liquiddetectiond, MacBook Air, MacBook Pro, macOS 15, Sequoia, USB-C. Bookmark the permalink.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "94a6b8257e7f1c12",
    "title": {
      "en": "Achieving Great Privacy with Safari",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://matanabudy.com/achieving-great-privacy-with-safari/",
    "score": 111,
    "by": "matanabudy",
    "time": 1742741849,
    "content": "Achieving Great Privacy with Safari\n\n                    23 Mar, 2025\n\n    Browsers have always been an arena for fighting and flaming over why your browser is the best and has the most features, while all others are inferior and unusable. Iâ€™ve never liked to fight those wars (not with browsers and not with code editors, another hotly debated topic), and Iâ€™ve never really understood how people get so religious about their tech choices.\nSo, first of all, use whatever you want and have fun with whatever you chooseâ€”thereâ€™s no need to be mad at someone elseâ€™s browser preference.\nWith that being said, browsers have indeed been all over the news lately, with the latest updates about Chrome introducing the new Manifest V3 and uBlock Origin being turned off for new Chrome versions. Privacy is once again a hot topic, and browser choices are back in the headlines. â€œUse Firefox,â€ they say. â€œNo! Brave is the only browser that will save you!â€ Blah, blah, blah.\nI wonâ€™t say anything about that because, well, Iâ€™m not into these wars, remember? I try to stay positive about my choices, and therefore, Iâ€™m using this post to show how Iâ€™m taking care of my privacy with Safari, my browser of choice. Is it perfect? Of course not. Can anything be privacy-perfect these days? Come on.\n\niCloud Private Relay. As someone deeply embedded in the Apple ecosystem with many of my devices being Apple products, Iâ€™m subscribed to iCloud+ and have activated iCloud Private Relay. This does two key things: it hides your IP address from websites via a relay and encrypts your DNS queries.\n\nWipr. One of the best pieces of software Iâ€™ve encountered. Itâ€™s a simple and effective ad-blocker, developed by a single indie developer, with a one-time purchaseâ€”and then you can forget about it.\n\nStopTheMadness Pro. An extension thatâ€™s all about stopping annoying tricks on the web. This includes removing URL tracking parameters, preventing websites from changing your context menu, and much more. Itâ€™s was also created by an indie developer.\n\nHush. Blocks annoying cookies and pop-ups. Itâ€™s great and open-source.\n\nThatâ€™s it. Three extensions and one serviceâ€”and honestly, you can do well with just Wipr installed. How effective is this setup? Using the EFF online test, this configuration achieved â€œstrong protection against web tracking,â€ but it also includes a unique fingerprint among the 272,373 tested in the past 45 days, which is not so good:\n\nUsing an online ad-block test, this setup achieved a 96% block rate:\n\nJust for fun, I compared it to two other â€œprivateâ€ setups (all with iCloud Private Relay still enabled):\n\nFirefox, withuBlock Origin,Privacy Badger, andI Still Donâ€™t Care About Cookies. It still got â€œstrong protection against web trackingâ€ from EFF but with an improvement of the fingerprint being only a nearly unique fingerprintâ€”not a fully unique one (â€œone in 136,187 browsers have the same fingerprint as yoursâ€). In the online test, only 52% of the websites were blockedâ€”not so great:\n\nBrave Browser, with its built-in Brave shields. It seems to randomize its fingerprint, which according to EFF â€œprovides very strong protection against tracking companies trying to fingerprint your browser, surely better than a unique fingerprint. It also achieved 68% on the online test:\n\nSo, to conclude, Iâ€™m pretty happy with my setup. Is it the most comprehensive test and comparison? Probably not. Do I want to fight with others over their setups? Nope. I do hope that if you use Safari and didnâ€™t know about these extensions, Iâ€™ve helped you improve your setup (if you wanted to).\nReply via email\n\n            26\n\n    document.querySelector('#upvote-form').addEventListener('submit', (e) => {\n        e.preventDefault();\n        const form = e.target;\n        fetch(form.action, {\n            method: form.method,\n            body: new FormData(form),\n        });\n        const button = form.querySelector('button')\n        button.disabled = true\n        button.style.color = \"salmon\"\n        const upvoteCount = document.querySelector('.upvote-count')\n        upvoteCount.innerHTML = `${(parseInt(upvoteCount.innerHTML.split(\" \")[0]) + 1)}`\n    });",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "b60fc90e8ce27c81",
    "title": {
      "en": "The Compressed Book Edition",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://solar.lowtechmagazine.com/2025/03/the-compressed-book-edition/",
    "score": 34,
    "by": "skowalak",
    "time": 1742751084,
    "content": "The Compressed Book Edition\n\n\t\t\t\t\tHow to make books more sustainable? Inspired by the image compression on our solar-powered website, we squeezed the article catalog of our three-volume book series into just one book.\n\nMarch 20, 2025\n\nWritten by\nKris De Decker\n\nImage: The Compressed Book Edition. Image by Marie Verdeil and Hugo Lopez.\n\n              View original image\n\n            View dithered image\n\nSubscribe to our newsletter.\n Support Low-tech Magazine via Paypal or Patreon\nRead Low-tech Magazine offline.\n\nThe Compressed Edition is available in our bookshop as a paperback and hardcover.\nIn 2018, Low-tech Magazine launched a low-energy website that runs on solar power. To reduce energy use and make the content accessible for readers with old computers and slow internet connections, we opted for a back-to-basics web design, optimising image and file sizes, as well as using a static site generator instead of a database-driven content management system. In 2019, we also launched a book edition of Low-tech Magazine, which consists of three volumes with articles and one volume with comments.\nThe Compressed Book Edition\nWhile a book looks and feels more low-tech than a website, it has an environmental footprint as well. Industrial book publishing and distribution involves wood harvesting, pulp and paper production, printing, ink-making, and lots of shipping throughout the supply chain. Even if the wood for paper production is harvested sustainably, which is rarely the case, all these processes require energy and produce carbon emissions.\nTo address these issues, and to keep practicing what we preach, Low-tech Magazine has now made a â€œcompressed editionâ€ of the chronological book series. Inspired by the image compression on our website, we squeezed the article catalog of three volumes into just one book. Consequently, we reduced the paper consumption and carbon emissions by almost a factor of three. The compressed edition contains 84 articles and over 700 images on slightly more than 600 pages.\nWe did this by switching to a smaller font size (similar to the one used in the comments book), by downsizing most images, and by opting for a two-column layout. I rewrote some articles, especially older ones, resulting not only in fewer pages but also in better articles. Laia Comellas and Marie Verdeil collaborated on the design of the compressed edition.\n\nImage: The original, â€œuncompressedâ€ book series. Image by Marie Verdeil and Hugo Lopez.\n\n              View original image\n\n            View dithered image\n\nBooks or Website?\nEver since the launch of the book collection, readers have asked what is most sustainable: reading Low-tech Magazine online or on paper? While itâ€™s a relevant question, comparing the carbon emissions of books and websites is complex and somewhat pointless. There are so many variables influencing this calculation that you could tilt the result toward your preferred answer.\n\nComparing the carbon emissions of books and websites is complex and somewhat pointless.\n\nA crucial factor is the time spent reading. If you only read a few articles, reading online likely has a lower carbon footprint. But, if you read all articles, and maybe even go back to them regularly afterward, the difference between online and offline reading becomes smaller. For the books, all carbon emissions occur before the reading process starts. In contrast, the more time you spend on the website, the higher the carbon emissions. 1\nThereâ€™s another difference between online and offline reading: Unlike a website, a book can be read by more than one person without raising its carbon emissions â€” for example, when itâ€™s available in a library. 2 Books usually have very long lifetimes, between 25 and 500 years. 3 Therefore, they can be read by dozens of people. That is especially the case for hardcover books, which take a bit more resources to produce than paperbacks but are more resistant to abuse.\nCarbon Emissions & Energy Use\nTo estimate the carbon emissions of Low-tech Magazineâ€™s books, I used a study of a 320-page hardcover book weighing 0.75 kg. 3 According to the researchers, the complete production process of the book has a carbon footprint of between 2 and 3 kg of CO2-equivalents, depending on what happens when the book is discarded after 25 years (landfill or recycling).\nThe three â€œuncompressedâ€ paperback books together weigh 2,531 grams, which corresponds to an estimated carbon footprint of between 6.7 and 10.1 kg CO2-equivalents. For the compressed edition, at 929 grams, the carbon footprint comes down to between 2.48 and 3.70 kg of CO2-equivalents. These numbers are surprisingly high compared to the carbon footprint of Low-tech Magazineâ€™s web server, which we calculated to be 9 kg CO2-equivalents per year. 4 Although our web server runs on solar power, these carbon emissions result from producing the solar panel, the battery, the solar charge controller, and the printed circuit boards, measured across their estimated lifetime.\nHowever, the book does not require any infrastructure to be read, while the carbon footprint of our web server is only one part of the total footprint of the website. Readers of the website need a computer to access it, and the resulting energy use and carbon emissions of powering and manufacturing that device should also be included. Assuming 60 hours to read all the articles, a laptop power use of 25-50 watts, and the average power grid carbon intensity in Europe (300g/kWh), the power use of the laptop would add between 0.45 and 0.90 kg of carbon emissions for reading content online.\n\nImage: The Compressed Book Edition (hardcover). Image by Marie Verdeil and Hugo Lopez.\n\n              View original image\n\n            View dithered image\n\nTo this should be added the energy that was required to manufacture the laptop and the power grid (of which in both cases only a part can be attributed to reading Low-tech Magazine). Because life cycle analyses show that the energy used for manufacturing a laptop surpasses its operational energy use, we can â€” conservatively â€” double this result to between 0.90 kg and 1.8 kg of CO2-equivalents. 5 Because our website uses very little energy, almost the complete carbon footprint of Low-tech Magazine is due to the devices of our readers.\n\nThe book needs to be read by at least two to three people before its environmental footprint becomes smaller than the one caused by reading all Low-tech Magazine articles online.\n\nNevertheless, this carbon footprint is still lower than the 3 kg of CO2-equivalents for the compressed book edition. Although this is a very rough estimate, it seems to suggest that this book needs to be read by at least two to three people before its environmental footprint becomes smaller than the one caused by reading all Low-tech Magazine articles online. Of course, we have set the bar very high for ourselves because of our light-weight website. Compared to the old blog, which was much more carbon-intensive than the solar-powered website, the compressed book edition â€” and perhaps even the uncompressed book edition â€” would be the more sustainable option even if it is read only by one person.\nHow Many Trees Have We Cut Down?\nThe environmental footprint of books does not only show in carbon emissions. Books are made from paper, and paper is â€” nowadays â€” almost exclusively made from dead trees. With close to 10,000 Low-tech Magazine books sold, an uncomfortable question pops up: how many trees did I kill? That is not an easy question to answer, because the only reference I could find says that â€œone tree can produce 25 booksâ€ 6, without specifying what size of tree or type of book these numbers refer to.\nNevertheless, if this estimation is valid, the Low-tech Magazine books would be responsible for chopping down roughly 400 unspecified trees. Whether or not these trees were replaced by other trees, I have no way of knowing. However, by reducing the page number from 1,700 to a little over 600 pages, at least the Compressed Edition attempts to limit this resource use.\n\nCompressing the content â€” an editorial and design choice â€” produces a larger reduction in resource use than printing on recycled paper could ever do.\n\nLow-tech Magazineâ€™s â€œtree consumptionâ€ could be further reduced by printing on recycled paper, and we would probably do so if our book distributor and printer â€” Lulu â€” would offer that option. However, printing on recycled paper is not a panacea. Paper can only be recycled a couple of times before it needs to be incinerated or landfilled.\nPartly because of this, and partly because of economic growth, there is not enough recycled paper available to print the ever-increasing number of books that are published each year. If Low-tech Magazine prints on recycled paper, it means that someone else wonâ€™t. Moreover, printing on recycled paper often increases the carbon emissions of paper production. 3 Compressing the content â€” an editorial and design choice â€” produces a larger reduction in resource use than printing on recycled paper could ever do.\nHow Many Books are Thrown Away?\nThe carbon footprint and wood consumption of an individual book only tell a part of the story. Most environmental damage in the book industry is done by overproduction. A very large number of printed books are not sold but discarded before anyone can read them. Overproduction occurs in two ways. First, most books that come on the market fail commercially, which is a consequence of the business strategies of book publishers. 7 Large publishers invest in a massive number of titles in the hope that one will become a bestseller.\n\nImage: The Compressed Book Edition (paperback). Image by Marie Verdeil and Hugo Lopez.\n\n              View original image\n\n            View dithered image\n\nImage: The Compressed Book Edition (paperback). Image by Marie Verdeil and Hugo Lopez.\n\n              View original image\n\n            View dithered image\n\nSecond, higher print runs significantly lower the printing costs per copy, and thus encourage overproduction. To give an example, printing 100 copies of a 600-page book costs 7,3 euros per copy, while printing 1,000 copies costs only 4,30 euros per copy. 8 As a result, it can be more profitable to print more books and discard the unsold copies. Even books that are in demand can be thrown away. For example, unsold copies at events are usually destroyed rather than sent back to publishers because itâ€™s cheaper. 9\nA less wasteful approach is printing on demand, in which a copy only gets printed once it is bought. In this case, there is no waste unless a book is printed badly. However, the energy use and carbon emissions per printed copy are probably higher, with the printing equipment taking a larger share in the total resource use. The printing costs are much higher, too (around 15 euros per copy for a 600-page book). 10 Almost all Low-tech Magazine books are printed on demand, but we can only do this because we have our own direct sales channel (the website). If books are sold through Amazon or in bookstores, print-on-demand results in a very high sales price or a very low profit for the book publisher.\nOther Low-tech Magazine Books\nThe launch of the compressed edition does not mean that the â€œuncompressedâ€ books will no longer be for sale. They will be redesigned in the next months, reflecting the same changes in articles (shorter and better) but printed with a larger font, larger images, and a one-column layout. Their carbon footprint will decrease compared to the earlier volumes but in a less spectacular way. Nevertheless, while those uncompressed books remain the most comfortable option for reading Low-tech Magazine, we expect a significant part of potential buyers to opt for the more compact edition, as it provides the most bang for the buck.\nObviously, â€œcompressingâ€ the books also addresses printing costs, which have skyrocketed in the last few years. Higher printing costs result in a higher sales price and a lower profit, or both. The compressed edition allows us to roughly halve the sales price for the whole archive while maintaining two-thirds of the profit. For the redesigned uncompressed editions, a more modest reduction in the page number will allow us to keep the current sales price but restore the profit per copy to pre-pandemic levels â€” important for the survival of the magazine. After all, people buy books for the content they contain, not for their weight on paper.\nEbooks and Thematic Books\nThe Compressed Edition is part of an ongoing research project into the sustainability of Low-tech Magazineâ€™s publishing operations. Apart from the solar-powered website, we have introduced ebooks (2024), and these are perhaps the most sustainable option to read Low-tech Magazine. We also started the publication of a thematic books series (2023), aimed at people who are only interested in certain topics and themes of Low-tech Magazine. The ebooks and thematic books are made in collaboration with Marie Verdeil.\nBook Launch Event\nWe hold a book launch event in Barcelona on March 28.\n\nImage: Book launch event in Barcelona. Poster by Hugo Lopez.\n\n              View original image\n\n            View dithered image\n\nSubscribe to our newsletter.\n Support Low-tech Magazine via Paypal or Patreon\nRead Low-tech Magazine offline.\n\nThat is mainly because of the energy use of the end-use devices that people use to access the website. Surprisingly, the number of website visitors has no influence on the power use of our web server. That is likely due to the fact that it is a very lightweight, static website. For â€œnormalâ€, dynamic websites, the energy use is closely related to the number of visitors.â†©ï¸\n\nLow-tech Magazineâ€™s books are available in several libraries. See: https://search.worldcat.org. If it is not available in your local library, you can file an acquisition request there.â†©ï¸\n\nWells, Jeanâ€Robert, et al. â€œCarbon footprint assessment of a paperback book: Can planned integration of deinked market pulp be detrimental to climate?.â€ Journal of Industrial Ecology 16.2 (2012): 212-222.â†©ï¸â†©ï¸â†©ï¸\n\nSee: â€œHow sustainable is a solar powered website?â€, in this volume.â†©ï¸\n\nThereâ€™s also the power use of the router. However, while the grid-powered router uses 240 Wh of electricity per day, it is shared by all website visitors (and by the author). Even on a calm day (around 2,000 unique visitors), extra energy use due to the router is only 0.12 watt-hour per visitor. That is much lower than the energy use of the laptop, even during a relatively short visit.â†©ï¸\n\nhttps://publishyourpurpose.com/blog/environmental-impact-book-publishing/â†©ï¸\n\nBiel, Joe. Peopleâ€™s Guide to Publishing: Building a Successful, Sustainable, Meaningful Book Business From the Ground Up. Microcosm Publishing, 2018.â†©ï¸\n\nBased on the prices of expresta.euâ†©ï¸\n\nBookstores need to prove that they destroyed the books by ripping off the covers and sending them to the publisher. Source: personal communication with book sellers at events.â†©ï¸\n\nBook cost calculator, Lulu. https://www.lulu.com/pricingâ†©ï¸\n\nRelated Articles\n\nThe Printed Website\nThematic Book Series\nBrowse By  Themes ...\n\nLow-tech Magazine: The Ebooks\nLow-tech Magazine now offers its complete book collection in epub format.\nJuly 23, 2024\n\nThematic Book Series: How Circular is the Circular Economy?\nThe fourth volume in a series of books opening up Low-tech Magazineâ€™s archive by theme.\nJuly 22, 2024\n\nThematic Book Series: Heating People, not Spaces\nWe have launched the third volume in a new series of books opening up Low-tech Magazineâ€™s archive by theme.\nDecember 7, 2023\n\nThematic Book Series: How to Downsize a Transport Network?\nWe have launched the second volume in a new series of books opening up Low-tech Magazineâ€™s archive by theme.\nOctober 7, 2023",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "61b77cc47900b019",
    "title": {
      "en": "Satshot Founder Lanny Faleide Takes Global Imagery to New Heights",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.croplife.com/precision-tech/satshot/",
    "score": 27,
    "by": "rmason",
    "time": 1742420465,
    "content": "By Lara Sowinski|March 12, 2025\n\n\t\tSatshot founder Lanny Faleide accepts the 2024 CropLife Ag Tech Award of Excellence in the Legacy category from Editor Lara Sowinski during Tech Hub LIVE in Des Moines, IA.\nEditorâ€™s Note: The annual CropLife Ag Tech Awards of Excellence are presented each year during the Tech Hub LIVE Conference and Expo. Over the next several months, CropLife will profile each of the 2024 winners to see what they are working on currently and how being recognized as a CropLife Ag Tech Awards of Excellence winner has benefitted them personally and professionally. Here we profile Satshot founder Lanny Faleide, winner of the 2024 Legacy Award.\nIt was nearly 30 years ago that Lanny Faleide launched Satshot, a global imagery service for the precision agriculture industry.\nFrom an early age, Faleide loved imagery, and he loved space. Seeing the TV broadcast of the first moonwalk made a profound impression on him.\nShortly after starting college in the early 1970s, Faleide quit school to return home and help his father farm. At the time, the U.S.-Soviet Union grain deal had tripled the price of wheat.\nLike many others in his age group who grew up on family farms, Faleide thought this might be a once-in-a-lifetime chance to â€œbecome millionaires,â€ he recalls.\nAlthough he didnâ€™t achieve millionaire status, Faleide did earn his private pilotâ€™s license.\nâ€œIn 1977, I was taking pictures of our crops with near-infrared film and my 35 mm camera,â€ he says. â€œBecause I was looking at the crops from above and was seeing all the variation that were going unaddressed, that stayed in my head.â€\nFrom that moment on, imagery was at the forefront for Faleide.\n\nRelated: 2025 Ag Tech Awards of Excellence: Nominate Today!\n\nLikewise, the satellite imagery sector was also beginning to take off, thanks to interest generated by the Apollo moon missions and the stunning images of earth taken from space.\nNASAâ€™s Landsat 1 satellite launched in 1972, marking a new age of remote sensing of land from space.\nThe U.S. farm crisis of the 1980s proved to be a â€œvery difficult financial decadeâ€ for Faleide and his wife. After many years in the agriculture business, they decided to change careers. He gave up his rented farmland and concentrated farming on his and his familyâ€™s land.\nFaleide enrolled in GIS classes at North Dakota State University.\nIn 1986, France launched its first commercial satellite, SPOT 1.\nShortly thereafter, Faleide started â€œdabblingâ€ in satellites, and the idea to launch his own satellite imagery company soon followed.\nA big break came during a routine trip to the grocery store where Faleide ran into a childhood friend who was working with the Crystal Sugar company. Faleide mentioned his budding satellite imagery company and asked his friend if he could connect Faleide with someone at Crystal Sugar so he could pitch his services.\nIt turns out, his friend was that person.\nâ€œHe gave me my first $92,000 contract within a year. That was my start-up money,â€ says Faleide.\nOwning the Space\nFaleide has pioneered the use of satellite imagery in agriculture â€” and made it commercially viable.\nThe Guggenheim commissioned Lanny Faleide to contribute his work to â€œCountryside, The Future.â€ The exhibit was on display in New York City from February 2020-2021.\nToday, there are hundreds of companies and researchers with funding from grants, private equity, and venture capital, doing work similar to Satshot. However, Faleide was the first to combine propriety knowledge and technology with satellite imagery and offer these products and services to the agriculture industry.\nâ€œWe were ahead of everybody in the beginning. And in many ways, we still are today,â€ he says.\nOne advantage was Faleideâ€™s nephew, who was head of programming for flight simulators at Northwest Airlines in Minneapolis in the mid-1990s.\nâ€œHe built our first software,â€ says Faleide, which helped established Satshotâ€™s business model of using satellite imagery to support precision agriculture, agronomy, and provide a variety of analytics.\nIn basic terms, Satshot uses satellite imagery from multiple sources â€” commercial companies and government, and private and open-source data â€” which is then uploaded to Satshotâ€™s hub in the cloud. Itâ€™s then joined with a mapping system framework, like Google or Apple.\nSatellite receiving stations on Lanny Faleideâ€™s land in North Dakota.\nSubscribers can use Satshot to view their field and look at satellite imagery every five days, whether weather conditions were cloudy or clear, with the added capability to go back multiple years for historical purposes.\nFrom there, the subscriber chooses the type of analysis he/she wants, such as NDVI (Normalized Difference Vegetation Index).\nSimply put, Satshotâ€™s imagery identifies the â€œgood, bad, and uglyâ€ in the field, says Faleide, which helps optimize a plan by accurately evaluating the variability in field conditions and determining whatâ€™s required with regards to crop protection products and application. Itâ€™s taking data and putting it in a form thatâ€™s workable on a farmerâ€™s field.\nNot only has the technology changed over the years, but so too has the cost.\nIn the early days, one satellite image that covered 120 miles wide by 120 miles long was about $4,400.\nToday, Faleide can access very high-resolution images at a more affordable price, and he downloads a lot of them.\nâ€œWe automatically download between 400 and 500 satellite scenes daily and each one is about 300 megabytes in size,â€ he notes.\nA petabyte is one million times larger than a gigabyte, and Satshot has about two-thirds of a petabyte of data stored in its cloud infrastructure.\nThe company is expanding in other ways.\nâ€œNot only do we provide data and analytics for agronomy and precision ag, weâ€™re also renting land to space companies who put up equipment to collect imagery from space. Itâ€™s kind of an interesting business model,â€ says Faleide.\nReflecting on his legacy and his work, Faleide says he always has faith in what he set out to do, namely helping farmers by using satellite imagery and technology.\nWhile his â€œstubborn Norwegianâ€ tenacity has gotten him through some trying times, the key to success is â€œdonâ€™t quit,â€ Faleide emphasizes, â€œand believe in what youâ€™re doing.â€\nAdmittedly, â€œIt can take its toll every now and then,â€ he says, â€œbut itâ€™s been a very fun ride.â€\n\n        1\n        1\n        5\n\n            Satshot Founder Lanny Faleide Takes Global Imagery to New Heights\n\nYou may also like...\n\nâ€¢ Top 10 Ag Retailers With the Most Fertilizer Revenue in 2024 - CropLife\n\nâ€¢ Nutrien: Big Company, Big Plans - CropLife\n\nâ€¢ 14 Iowa Ag Retailers Earn Spot in Latest CropLife 100 - CropLife\n\n                                Subscribe Today For\n\n                        Subscribe\n\n\t\t\t\t\t\tLara Sowinski is Group Editor for the CropLife Media Group at Meister Media Worldwide. Lara is also Co-Chair of the VISION Conference.\t\t\t\t\t\tSee all author stories here.\n\n\t\t\tLeave a ReplyCancel reply\n\n\t\t\t\t\t\tdocument.addEventListener('DOMContentLoaded', function () {\n\t\t\t\t\t\t\tvar commentForms = document.getElementsByClassName('jetpack_remote_comment');\n\t\t\t\t\t\t\tfor (var i = 0; i < commentForms.length; i++) {\n\t\t\t\t\t\t\t\tcommentForms[i].allowTransparency = false;\n\t\t\t\t\t\t\t\tcommentForms[i].scrolling = 'no';\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t});\n\n\t\t\t\tAdvertisement\n\n\t\t\tRelated posts\n\n\t\t\tTrimble and PTx Trimble Expand Innovative Technology to Maintain Precision and Continuous Operations in Agriculture\n\n\t\t\t\tBy AGCO|March 20, 2025\n\n\t\t\tAI in Agriculture Advancing Rapidly\n\n\t\t\t\tBy Eric Sfiligoj|March 19, 2025",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "1e51268ed4d16a02",
    "title": {
      "en": "Quitting an Intel x86 Hypervisor",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://halobates.de/blog/p/446",
    "score": 112,
    "by": "todsacerdoti",
    "time": 1742676124,
    "content": "Quitting an Intel x86 hypervisor\nwith one comment\n\nThis is an esoteric topic that might be of interest to people implementing Intel hypervisors. It assumes you know the basics of the Intel virtualization architecture, see Hypervisor from scratch for a tutorial. The actual full VT architecture is described in Volume 3 of the Intel SDMLetâ€™s say we write an x86 hypervisor that starts in the UEFI environment and virtualizes the initialization phase of an OS. But the hypervisor wants to eventually quit itself to not cause extra overhead during OS run time.The way the hypervisor works is that it runs in its own memory and with its own page tables which are switched atomically on every VM exit by the VT-x implementation. This way it is isolated from the main OS.At some vm exit with the hypervisor running in its own context it decides that it is not needed anymore and wants to quit. To disable VT support the VMXOFF instruction can be used. But what we really need is an atomic VMXOFF + switch to the original OS page tables plus a jump, and all that without using any registers which need to be already restored to the original state of the OS. One trick is to use the MOV to CR3 instruction that reloads the page table as a jump. As soon as the page table is reloaded the CPU will fetch the next instruction with the translations from the freshly loaded page table, so we can transfer execution to the guest context. However to do that the MOV CR3 needs to be located just before the page offset of the target instruction. This can be done by copying a trampoline to the right page offset (potentially overlapping into the previous page). The trampoline is located in a special transfer page table mapping that places writable code pages overlapping the target mapping.But there are some complications. The hypervisor also needs to load the segmentation state (like GDT/LDT/IDT) of the guest. In theory they could just be loaded by mapping these guest pages into the transfer mapping and loading them before the transfer.  But what happens if the GDT/LDT/IDT is on the same page as the target address? This is common in real OSâ€™ assembler startup code which is implemented in a small assembler file without any page separation between code and data. One option would be to copy them to the transfer page too and load it there, or the hypervisor first copies them to a temporary buffer and loads it from there. In the second option the base addresses of these structures will be incorrect, but in practice you can often rely on them getting reloaded eventually anyways.Another problem is the register state of the target. MOV to CR3 needs a register as the source of the reload, and it needs to be the last instruction of the trampoline. So it is impossible to restore the register it uses. But remember the hypervisor is doing this as the result of a VM exit. If we chose an exit for a condition that already clobbers a register we can use the same register for the reload and the next instruction executed in the original target (and which caused the exit originally) will just overwrite it again.\n\nA very convenient instruction for this is CPUID. It is executed multiple times in OS startup and overwrites multiple registers. In fact VMX always intercepts CPUID so it has to handle these exits in any case. So the trick to quit an hypervisor is to wait for the next CPUID exit and then use one of the registers clobbered by CPUID for the final CR3 reload.  This will have inconsistent register state for one instruction in the target, but unless the original OS is currently running a debugger it will never notice. In principle any exit as a result of an instruction that clobbers a register can be used for this.There is another potential complication if the target address of the OS conflicts with where the hypervisor is running before entering the transfer mapping. The transfer mapping needs to map the original code so that it can be jumped to. This could be solved with a third auxiliary mapping that is used before jumping to the transfer trampoline. In practice it doesnâ€™t seem to be a problem because x86 OS typically run in a 1:1 mapping for startup, and that cannot conflict with the 1:1 mapping used by  UEFI programs as our hypervisor.\n\nHappy hypervisor hacking!\n\n    Written by therapsid\n    March 18th, 2025 at 9:34 pm\n\n    Posted in curiosities,kernel\n\n    Â« The browser circle closes again\n\nOne Response to 'Quitting an Intel x86 hypervisor'\nSubscribe to comments with RSS\n\n\tp9p7bz\n\n\t\t\t\tMessage; You got a transfer #QH51. NEXT => https://graph.org/GET-BITCOIN-TRANSFER-02-23-2?hs=6dc5fe145591a5bd6997b09abe9717df&\n\t24 Mar 25 at 06:37\n\nLeave a Reply\n\nName\n\nMail (will not be published)\n\nWebsite\n\nÎ”document.getElementById( \"ak_js_1\" ).setAttribute( \"value\", ( new Date() ).getTime() );",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "560a71ebb93a40ea",
    "title": {
      "en": "Millions are visiting the European Alternatives site. What trends are we seeing?",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://plausible.io/blog/european-alternatives-trends-privacy-tech",
    "score": 388,
    "by": "themeaningist",
    "time": 1742803396,
    "content": "â† All posts  Millions are visiting the European Alternatives siteâ€“â€“what trends are we seeing?  Mar 24, 2025 â€¢ Written by Hricha Shandily    â˜ï¸Thatâ€™s a screenshot from the publicly open Plausible Analytics dashboard of the European Alternatives website, featuring independent, privacy-friendly digital tools. With almost 2M visitors, 2.3M total visits, and 8.5M pageviews, this library is no longer a niche projectâ€”itâ€™s the go-to destination for users looking for independent, privacy-friendly tools. Over 1 million of the ~2 million all-time visitors came in 2025 aloneâ€”and weâ€™re only in March. Letâ€™s use the analytics dashboard to see what trends are emerging and what is the demand looking like for EU-built, privacy-friendly tech.  1100% surge in 2025 traffic Reddit surpasses Google, while privacy-friendly search engines gain traction Top 5 categories: What are people looking for? Not just a European trendâ€”demand goes beyond the EU borders European countries leading (and lagging) in the shift to privacy-friendly tools Which European tools are winning? Need a hand in deciding which tools to go with? Privacy-friendly web analytics is on the rise  1100% surge in 2025 traffic If we filter the dashboard by â€œYear to dateâ€ period, i.e. 2025 so far, we see there have been 1.3 million visitors to the website, with a surge of 1.1K%.  Another interesting number is the â€œtime on page,â€ which is 2m47s spent on average. This goes on to show that people are interested and engaging well with the library. Reddit surpasses Google, while privacy-friendly search engines gain traction Look at the Top Sources report for 2025 so far:  Google is typically the dominant search driver for most websitesâ€”but not here. Reddit ranks as the second-largest traffic source, bringing in 311K visitorsâ€”significantly more than Google (193K). This isnâ€™t a regular thing we see with sites. This suggests that people arenâ€™t just searching for alternatives but are actively discussing and recommending them in forums, subreddits, and privacy-conscious communities. But hereâ€™s another interesting insight: Privacy-friendly search engines are also key traffic sources:  DuckDuckGo: 33.8K visitors Ecosia: 13.5K visitors Qwant: 10.8K visitors  This says something about the increasing preference for privacy-friendly tools, i.e., more people are actively using privacy-friendly search engines to discover other privacy-friendly alternatives. Top 5 categories: What are people looking for? A quick look at the â€œTop Pagesâ€ report tells us about the most popular top 5 categories: - Email providers - Search Engines - Cloud Computing Platforms - Navigation Apps - Web Analytics Services Upon expanding the report, we can take a deeper look at the most popular categories in terms of unique visitors, while comparing the engagement metrics.  B2B categories win? While categories like email providers, search engines, and navigation apps attract a large B2C audience, B2B categories are more diverse and consistently rank high. Businesses are actively seeking privacy-friendly analytics, hosting, and infrastructure alternatives, driving growth in EU-built tools. Not just a European trendâ€”demand goes beyond the EU borders Looking at the all-time Countries report: European countries make up the bulk of visitors (as expected), but the United States ranks fourth, sending over 100K visitors to the site! Even Canada and India make an appearance in the top 25, contributing decent amounts of visitors to the site. If you scroll all the way down, or simply look at the â€œMap view,â€ thereâ€™s hardly any part of the globe thatâ€™s not participating in the shift towards privacy-focused digital tools.  Privacy-conscious users are everywhere. European countries leading (and lagging) in the shift to privacy-friendly tools Germany leads in visitor numbers, with 379K, and contributing 20.5% of the total traffic all-time! This is followed by the Netherlands (141K) and France (135K). These countries show significant engagement with European digital alternatives. On the lower end, Guernsey recorded 130 visitors, while regions like Vatican City State and Saint Martin (French part) had 3 each. The variation in engagement levels may reflect differences in population size, internet adoption rates, etc. Which European tools are winning? The dashboard has an Outbound link click goal configured, which is useful in understanding which listed tools eventually attract real visitors. Once we filter the dashboard by this goal (see it here), we can see which URLs were clicked. From these results, these are the top 10 tools receiving traffic from the EU alternatives website:  Mailbox.org (email provider) ProtonMail (email provider) Soverin (email provider) Startpage (search engine) Qwant (search engine) Posteo (email provider) Ecosia (search engine) Scaleway (cloud provider) Startmail (email provider) Good search (search engine)  And if we look at the all-time data, thereâ€™s a whopping 1.2M total outbound clicks from the website to such tools, thatâ€™s more than half of the total all-time visits to the library.  And conversions arenâ€™t just from Europeâ€”the U.S. audience alone has a 16.8% conversion rate (check here)! Need a hand in deciding which tools to go with? We recently vetted many EU-built B2B tools and picked out a few of our favourites. Theyâ€™re all:  Built in the EU â€“ Companies headquartered in a European country. Hosted in the EU â€“ Ensuring your data doesnâ€™t leave the European borders and stays compliant with European privacy laws. GDPR-compliant â€“ Tools that align with European data protection laws. High quality â€“ Competitive with mainstream solutions. Privacy-focused â€“ Respecting user data and following GDPR regulations.  Check out our handpicked list of 16 privacy-focused European tools. Privacy-friendly web analytics is on the rise We started Plausible Analytics six years ago as a privacy-first, GDPR-compliant, cookieless and muuuch easier-to-use alternative to Google Analytics. So, we took a look at how our category is doing:  5th most popular category in the library 9th most visited page overall More than a 2,700% increase in unique visitors in 2025  The demand for privacy-friendly web analytics is booming. If youâ€™re looking for an alternative, check out our privacy-first approach. ğŸ‘‹   Written by Hricha Shandily",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "4578f2434d6be9e1",
    "title": {
      "en": "Do viruses trigger Alzheimer's?",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.economist.com/science-and-technology/2025/03/17/do-viruses-trigger-alzheimers",
    "score": 188,
    "by": "helsinkiandrew",
    "time": 1742714943,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "1cdb780613f6dfd7",
    "title": {
      "en": "All Placebos are not created equal (2021)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.samstack.io/p/all-placebos-are-not-created-equal",
    "score": 67,
    "by": "sebg",
    "time": 1742556486,
    "content": "Share this postSamstackAll Placebos are not created equalCopy linkFacebookEmailNotesMoreDiscover more from SamstackA newsletter about social science, the future, and why people behave in the way that they do. Over 4,000 subscribersSubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.Already have an account? Sign inAll Placebos are not created equalSam AtisNov 16, 202123Share this postSamstackAll Placebos are not created equalCopy linkFacebookEmailNotesMore13ShareA pair of Perkins TractorsHave you ever heard of a Perkins Tractor? If you lived in London in the 18th Century, you could buy a pair of Perkins Tractors (shown above) for the steep price of five guineas, and wave them over an aching part of your body for about twenty minutes as a way of relieving your pain. They worked, sort of. People who bought and used Perkins Tractors really did report feeling that their pain had gone away, even though there was no physiological reason they should work. John Haygarth, a British doctor, was intrigued. He organised a trial at a British hospital with several patients who were treated with fake Perkins Tractors - instead of being made of the expensive metal alloys, they were made with cheap wood. They worked just as well as the Perkins Tractors that were selling in London street stalls for five guineas, and thus the Placebo effect was demonstrated for the first time - â€œSuch is the wonderful force of imagination!â€, remarked Haygarth.Wooden Rods arenâ€™t often used as placebos these days, youâ€™re much more likely to receive some kind of oral placebo (like sugar pills), a topical placebo (such as a cream containing no active ingredients), or an intra-articular placebo (injected directly into the joints). However, this presents a tricky dilemma - what do we do about the fact that different types of placebo can have different effect sizes? Hereâ€™s one study thatâ€™s interesting - researchers from Tufts Medical Center looked into how patients with osteoarthritis react differently depending on which type of Placebo they receive. An intra-articular placebo is more effective at relieving pain than topical placebos, and topical placebos are more effective than oral placebos. This is perhaps fairly intuitive - my impression is that, in some sense, I take an injection more seriously than I do taking a pill. What is interesting is that the difference in effect size between an intra-articular placebo and an oral placebo is sometimes larger than the difference in effect size between active pain relief drugs and oral placebo. The difference in effect size between an intra-articular placebo and an oral placebo is 0.29 (CrI 0.09 to 0.49), whereas the difference in effect size between acetaminophen (paracetamol) and an oral placebo is only 0.18 (CrI 0.05 to 0.30). Interaction Effect between Expectations and Physician CharacteristicsBut the type of placebo used isnâ€™t the only thing that can affect how effective it is - characteristics of the physician administering the placebo can also make a difference. In this study, doctors injected histamine into patients to induce an allergic response, and then gave patients a placebo cream. Patients were either told that the cream would alleviate the response (positive expectations) or that it would exacerbate the response (negative expectations). The doctors administering the placebo changed how warm/cold they were to patients, as well as how competent/incompetent they came across. To appear warm, doctors called patients by their names, made more eye contact, and so on. To appear more competent, they made no mistakes in the procedure (as opposed to those appearing incompetent, who put the blood pressure cuff on incorrectly), made sure the room was tidy, etc. Patients perceived these differences - they rated the doctors attempting to appear warm as more warm, and they rated the doctors attempting to appear competent as more competent. The result can be seen in the chart above - there is an interaction effect between expectations and doctor warmth/competence on the size of the allergic reaction - among patients who were told the cream would alleviate their allergic response, the warmth/competence of the doctor made a difference to their allergic response, whereas among patients with negative expectations, the doctor characteristics made no difference. Characteristics of the patient can also make a difference to how effective a placebo is - children are more receptive than adults (study can be found here). The study here gestures at a problem that all of these findings may lead to: in RCTs comparing how responsive adults and children were to anti-epileptic drugs, the treatment effect was significantly lower in children. The reason for this was not actually that the drugs were any less effective in children, but that because children were more receptive to the placebo, the active drug looked less effective in comparison. Charts showing the increase effectiveness of placebosThe implications of this are pretty serious - the placebo effect in the United States has actually become quite a lot stronger over time, meaning that drugs that once would have been approved may not be now - because their performance relative to that of placebo is less convincing. This study makes the point clearly - by 2013, drugs produced 8.9% more pain relief than placebos, compared to 27.3% in 1996. In the charts above, it can be seen that the effect of placebo drugs has increased a lot, whereas the effectiveness of pain relief drugs has barely changed, meaning that the treatment advantage (the effectiveness of active drugs as opposed to placebos) has fallen dramatically. Weirdly, it seems like this is only happening in the United States, whereas other countries havenâ€™t seen particularly large increases in the effect size of placebos. Maybe I just hadnâ€™t thought about the Placebo effect enough, but I hadnâ€™t really considered that the size of the effect could be so different depending on whether the patient was taking a pill as opposed to using a cream, the characteristics of the doctor, and so on. The obvious result of this is that certain medications appear less effective than they would do had a different type of placebo been used (or had the physician administering the placebo behaved differently, or had the patient been older/younger, etc.) - given that, over the past ten years, 90% of potential drugs for the treatment of cancer pain have failed at advanced stages of clinical trials, this seems like itâ€™s worth thinking about a bit more. If you enjoyed this post, consider subscribing. The newsletters cover a range of interesting topics, and subscriptions are free (and will remain free for the foreseeable future)!Subscribe23 Likes23Share this postSamstackAll Placebos are not created equalCopy linkFacebookEmailNotesMore13Share",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "115bd37799a4b693",
    "title": {
      "en": "Stop using the elbow criterion for k-means",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://arxiv.org/abs/2212.12189",
    "score": 70,
    "by": "Anon84",
    "time": 1742698298,
    "content": "A major challenge when using k-means clustering often is how to choose the parameter k, the number of clusters. In this letter, we want to point out that it is very easy to draw poor conclusions from a common heuristic, the \"elbow method\". Better alternatives have been known in literature for a long time, and we want to draw attention to some of these easy to use options, that often perform better. This letter is a call to stop using the elbow method altogether, because it severely lacks theoretic support, and we want to encourage educators to discuss the problems of the method -- if introducing it in class at all -- and teach alternatives instead, while researchers and reviewers should reject conclusions drawn from the elbow method.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "227bc50b349e54e4",
    "title": {
      "en": "Bitter Lesson is about AI agents",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://ankitmaloo.com/bitter-lesson/",
    "score": 129,
    "by": "ankit219",
    "time": 1742721389,
    "content": "The Bitter Lesson: Rethinking How We Build AI Systems\n\n      Mar 20, 2025\n\n    The Race for AI Progress\nIn 2019, Richard Sutton, wrote his groundbreaking essay titled â€˜The Bitter Lessonâ€™. Simply put, the essay concludes that systems which get better with higher compute beat the systems that do not. Or specifically in AI: raw computing power consistently wins over intricate human-designed solutions. I used to believe that clever orchestrations and sophisticated rules were the key to building better AI systems. That was a typical sofware dev mentality. You build a system, look for edgecases, cover them and you are good to go. Boy, was I wrong.\n\nThink of it like training for a marathon. You could spend months perfecting your running form and buying the latest gear, but nothing beats putting in the miles. In AI, those miles are compute cycles.\n\nNatureâ€™s Blueprint\nRecently, I was tending to my small garden when it hit me - a perfect analogy for this principle. My plants donâ€™t need detailed instructions to grow. Given the basics (water, sunlight, and nutrients), they figure out the rest on their own. This is exactly how effective AI systems work.\n\nWhen we over-engineer AI solutions, weâ€™re essentially trying to micromanage that plant, telling it exactly how to grow each leaf. Not only is this inefficient, but it often leads to brittle systems that canâ€™t adapt to new situations.\n\nA Tale of Three Approaches\nToday, one of the most common enterprise usecase for AI agents is customer support. Let me share a real-world scenario I encountered while building a customer service automation system:\n\n    The Rule-Based Approach: Initially, everyone built an extensive decision tree with hundreds of rules to handle customer queries. It worked for common cases but broke down with slight variations. Maintenance became a nightmare.\n\n    The Limited-Compute Agent: Next, with the dawn of ChatGPT, there were AI powered customer agents with modest computing resources. You could write prompts based on patterns you saw in historical data or SOP guidelines. Worked well on simple enough questions, but struggled with complex queries and needed constant human oversight.\n\n    Many AI agents are here at this point. One path is to constrain it even further, branch out, bring in different frameworks and guardrails, so that the agent sticks to the goal. Inadventently, the compute is somehow fixed. Or you could try:\n\n    The Scale-Out Solution: Then we tried something different - what if we threw more compute at it? Not just bigger GPUs, but fundamentally rethinking how we use AI. We had the agent generate multiple responses in parallel, run several reasoning paths simultaneously, and pick the best outcomes. Each customer interaction could spawn dozens of AI calls exploring different approaches. The system would generate multiple potential responses, evaluate them, and even simulate how the conversation might unfold. Sure, it was computationally expensive - but it worked surprisingly well. The system started handling edge cases we hadnâ€™t even thought of, and more importantly, it discovered interaction patterns that emerged naturally from having the freedom to explore multiple paths.\n\nwhich brings us to:\n\nThe RL Revolution\nIn 2025, this pattern becomes even more evident with Reinforcement Learning agents. While many companies are focused on building wrappers around generic models, essentially constraining the model to follow specific workflow paths, the real breakthrough would come from companies investing in post-training RL compute. These RL-enhanced models wouldnâ€™t just follow predefined patterns; they are discovering entirely new ways to solve problems. Take OpenAIâ€™s Deep Research or Claudeâ€™s computer-use capabilities - they demonstrate how investing in compute-heavy post-training processes yields better results than intricate orchestration layers. Itâ€™s not that the wrappers are wrong; they just know one way to solve the problem. RL agents, with their freedom to explore and massive compute resources, found better ways we hadnâ€™t even considered.\n\nThe beauty of RL agents lies in how naturally they learn. Imagine teaching someone to ride a bike - you wouldnâ€™t give them a 50-page manual on the physics of cycling. Instead, they try, fall, adjust, and eventually master it. RL agents work similarly but at massive scale. They attempt thousands of approaches to solve a problem, receiving feedback on what worked and what didnâ€™t. Each success strengthens certain neural pathways, each failure helps avoid dead ends.\n\nFor instance, in customer service, an RL agent might discover that sometimes asking a clarifying question early in the conversation, even when seemingly obvious, leads to much better resolution rates. This isnâ€™t something we would typically program into a wrapper, but the agent found this pattern through extensive trial and error. The key is having enough computational power to run these experiments and learn from them.\n\nWhat makes this approach powerful is that the agent isnâ€™t limited by our preconceptions. While wrapper solutions essentially codify our current best practices, RL agents can discover entirely new best practices. They might find that combining seemingly unrelated approaches works better than our logical, step-by-step solutions. This is the bitter lesson in action - given enough compute power, learning through exploration beats hand-crafted rules every time.\n\nIndeed, you see this play out in â€“soon to be bigâ€“ competition between Claude code and Cursor. Currently users say Cursor does not work well with Claude Sonnet 3.7, but it works flawlessly with Sonnet 3.5. On the other hand, people complain that Claude code (which uses Sonnet 3.7 under the hood) consumes a lot of tokens. However, it works amazingly well. Cursor, reportedly will launch as version with usage based pricing which will make more use of 3.7â€™s agentic behavior1. We will see this in more domains, especially outside of code where the an agent could think of multiple approaches, while humans have codified a single workflow.\n\nWhat this means for AI Engineers\nThis insight fundamentally changes how we should approach AI system design:\n\n    Start Simple, Scale Big: Begin with the simplest possible learning architecture that can capture the essence of your problem. Then scale it up with compute rather than adding complexity.\n\n  Design for Scale: Build systems that can effectively utilize additional compute. This means:\n\n      Parallelizable architectures\n      Flexible learning frameworks that can grow with more data and compute\n      Infrastructure that can handle distributed processing\n\n  Avoid Premature Optimization: Donâ€™t spend weeks optimizing algorithms before youâ€™ve maxed out your compute potential. The returns from clever engineering often pale in comparison to simply adding more computational resources.\n\nThe Real â€œSo Whatâ€\nThe implications are profound and somewhat uncomfortable for us engineers:\n\n    Investment Strategy: Organizations should invest more in computing infrastructure than in complex algorithmic development.\n\n    Competitive Advantage: The winners in AI wonâ€™t be those with the cleverest algorithms, but those who can effectively harness the most compute power.\n\n    Career Focus: As AI engineers, our value lies not in crafting perfect algorithms but in building systems that can effectively leverage massive computational resources. That is a fundamental shift in mental models of how to build software.\n\nLooking Forward\nThis lesson might seem to diminish the role of the AI engineer, but it actually elevates it. Our job is to:\n\n  Design systems that can effectively utilize increasing compute resources\n  Build robust learning environments that scale\n  Create architectures that can grow without requiring fundamental redesigns\n\nThe future belongs to those who can build systems that learn and adapt through computational force, not those who try to encode human knowledge into rigid rules.\n\nRemember: In the race between clever engineering and raw compute, compute wins. Our role is to build the race track, not to design the runnerâ€™s every move.\n\n      I mean the source is their Community Manager. So, not exactly reportedly. In this thread, they call it more sync vs more delegated work, but in reality, it is a fight between constraints and compute. This post pretty much admits it. At this point, they have already released a version where every Sonnet 3.7 Max call costs about $0.05.â†©",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "31d5f84f7b5d10d1",
    "title": {
      "en": "Janito â€“ A Claude-powered CLI assistant for developers",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/joaompinto/janito",
    "score": 22,
    "by": "joaompinto",
    "time": 1742733707,
    "content": "ğŸ¤– Janito\nJanito is a powerful AI-assisted command-line interface (CLI) tool built with Python, leveraging Anthropic's Claude for intelligent code and file management.\n\nâœ¨ Features\n\nğŸ§  Intelligent AI assistant powered by Claude\nğŸ“ File management capabilities with real-time output\nğŸ” Smart code search and editing\nğŸ’» Interactive terminal interface with rich formatting\nğŸ“Š Detailed token usage tracking and cost reporting with cache savings analysis\nğŸ›‘ Token and tool usage reporting even when interrupted with Ctrl+C\nğŸŒ Web page fetching with content extraction capabilities\nğŸ”„ Parameter profiles for optimizing Claude's behavior for different tasks\nğŸ“‹ Line delta tracking to monitor net changes in files\nğŸ’¬ Conversation history with ability to resume previous conversations\nğŸ”‡ Trust mode for concise output without tool details\n\nğŸ› ï¸ System Requirements\n\nPython 3.8+ - Janito requires Python 3.8 or higher\nOperating Systems:\n\nLinux/macOS: Native support\nWindows: Requires Git Bash for proper operation of CLI tools\n\nAnthropic API Key - Required for Claude AI integration\n\nğŸ› ï¸ Installation\n# Install directly from PyPI\npip install janito\n\nSetting up your API Key\nJanito requires an Anthropic API key to function. You can:\n\nSet the API key: janito --set-api-key your_api_key\n\nFor development or installation from source, please see README_DEV.md.\nğŸš€ Usage Tutorial\nAfter installation, you can start using Janito right away. Let's walk through a simple tutorial:\nGetting Started\nFirst, let's check that everything is working:\n# Get help and see available commands\njanito --help\n\nTutorial: Creating a Simple Project\nLet's create a simple HTML project with Janito's help:\nAfter installing Janito, using your prefered editor and/or terminal, go to a new empty folder.\nUse the janito command to create a new project.\n# Step 1: Create a new project structure\njanito \"Create a simple HTML page with a calculator and 3 columns with text for the 3 main activities of the Kazakh culture\"\n\nBrowse the resulting html page.\nTutorial: Adding Features\nNow, let's enhance our example\n# Step 2: Add multiplication and division features\njanito \"Add some svg icons and remove the calculator\"\n\nRefresh the page\nExploring More Features\nJanito offers many more capabilities:\n# Show detailed token usage and cost information\njanito --show-tokens \"Explain what is in the project\"\n\n# Use a specific parameter profile for creative tasks\njanito --profile creative \"Write a fun description for our project\"\n\n# Use trust mode for concise output without tool details\njanito --trust \"Optimize the HTML code\"\n# Or use the short alias\njanito -t \"Optimize the HTML code\"\n\n# Continue the most recent conversation\njanito --continue \"Please add one more line\"\n\n# Continue a specific conversation using its message ID\n# (Janito displays the message ID after each conversation)\njanito --continue abc123def \"Let's refine that code\"\n\n# Show current configuration and available profiles\njanito --show-config\n\n# You can press Ctrl+C at any time to interrupt a query\n# Janito will still display token and tool usage information\n# Even interrupted conversations can be continued with --continue\n\nğŸ”§ Available Tools\nJanito comes with several built-in tools:\n\nğŸ“„ str_replace_editor - View, create, and edit files\nğŸ” find_files - Find files matching patterns\nğŸ—‘ï¸ delete_file - Delete files\nğŸ” search_text - Search for text patterns in files\nğŸŒ fetch_webpage - Fetch and extract content from web pages\nğŸ“‹ move_file - Move files from one location to another\nğŸ’» bash - Execute bash commands with real-time output display\n\nğŸ“Š Usage Tracking\nJanito includes a comprehensive token usage tracking system that helps you monitor API costs:\n\nBasic tracking: By default, Janito displays a summary of token usage and cost after each query\nDetailed reporting: Use the --show-tokens flag to see detailed breakdowns including:\n\nInput and output token counts\nPer-tool token usage statistics\nPrecise cost calculations\nCache performance metrics with savings analysis\nLine delta tracking for file modifications\n\n# Show detailed token usage and cost information\njanito --show-tokens \"Write a Python function to sort a list\"\n\n# Basic usage (shows simplified token usage summary)\njanito \"Explain Docker containers\"\n\n# Use trust mode for concise output without tool details\njanito --trust \"Create a simple Python script\"\n# Or use the short alias\njanito -t \"Create a simple Python script\"\n\nThe usage tracker automatically calculates cache savings, showing you how much you're saving by reusing previous responses.\nğŸ“‹ Parameter Profiles\nJanito offers predefined parameter profiles to optimize Claude's behavior for different tasks:\n\nprecise: Factual answers, documentation, structured data (temperature: 0.2)\nbalanced: Professional writing, summarization, everyday tasks (temperature: 0.5)\nconversational: Natural dialogue, educational content (temperature: 0.7)\ncreative: Storytelling, brainstorming, marketing copy (temperature: 0.9)\ntechnical: Code generation, debugging, technical problem-solving (temperature: 0.3)\n\n# Use a specific profile\njanito --profile creative \"Write a poem about coding\"\n\n# View available profiles\njanito --show-config\n\nğŸ”‡ Trust Mode\nJanito offers a trust mode that suppresses tool outputs for a more concise execution experience:\nHow It Works\n\nWhen enabled with --trust or -t, Janito suppresses informational and success messages from tools\nOnly essential output and error messages are displayed\nThe final result from Claude is still shown in full\nTrust mode is a per-session setting and not saved to your configuration\n\nUsing Trust Mode\n# Enable trust mode with the full flag\njanito --trust \"Create a Python script that reads a CSV file\"\n\n# Or use the short alias\njanito -t \"Create a Python script that reads a CSV file\"\n\nThis feature is particularly useful for:\n\nExperienced users who don't need to see every step of the process\nBatch processing or scripting where concise output is preferred\nFocusing on results rather than the process\nCreating cleaner output for documentation or sharing\n\nğŸ’¬ Conversation History\nJanito automatically saves your conversation history, allowing you to continue previous discussions:\nHow It Works\n\nEach conversation is saved with a unique message ID in .janito/last_messages/\nThe most recent conversation is also saved as .janito/last_message.json for backward compatibility\nAfter each conversation, Janito displays the command to continue that specific conversation\n\nUsing the Continue Feature\n# Continue the most recent conversation\njanito --continue \"Add more details to your previous response\"\n\n# Continue a specific conversation using its ID\njanito --continue abc123def \"Let's modify that code you suggested\"\n\nThe --continue flag (or -c for short) allows you to:\n\nResume the most recent conversation when used without an ID\nResume a specific conversation when provided with a message ID\nMaintain context across multiple interactions for complex tasks\n\nThis feature is particularly useful for:\n\nMulti-step development tasks\nIterative code improvements\nContinuing discussions after system interruptions\nMaintaining context when working on complex problems\n\nâš™ï¸ Dependencies\nJanito automatically installs the following dependencies:\n\ntyper (>=0.9.0) - For CLI interface\nrich (>=13.0.0) - For rich text formatting\nclaudine - For Claude AI integration\nAdditional packages for file handling and web content extraction\n\nğŸ› ï¸ Command-Line Options\nJanito offers a variety of command-line options to customize its behavior:\n--verbose, -v                 Enable verbose mode with detailed output\n--show-tokens                 Show detailed token usage and pricing information\n--workspace, -w TEXT          Set the workspace directory\n--set-config TEXT             Configuration string in format 'key=value', e.g., 'temperature=0.7'\n--show-config                 Show current configuration\n--reset-config                Reset configuration by removing the config file\n--set-api-key TEXT            Set the Anthropic API key globally in the user's home directory\n--ask                         Enable ask mode which disables tools that perform changes\n--trust, -t                   Enable trust mode which suppresses tool outputs for concise execution\n--temperature FLOAT           Set the temperature for model generation (0.0 to 1.0)\n--profile TEXT                Use a predefined parameter profile (precise, balanced, conversational, creative, technical)\n--role TEXT                   Set the assistant's role (default: 'software engineer')\n--version                     Show the version and exit\n--continue, -c TEXT           Continue a previous conversation, optionally with a specific message ID\n--help                        Show the help message and exit\n\nğŸ”‘ API Key Configuration\nYou can configure your Anthropic API key in several ways:\n# Option 1: Set as environment variable\nexport ANTHROPIC_API_KEY=your_api_key\n\n# Option 2: Configure globally within Janito\njanito --set-api-key your_api_key\n\n# Option 3: Let Janito prompt you on first use\njanito \"Hello, I'm new to Janito!\"\n\nYour API key is securely stored and used for all future sessions.\nğŸ’» Development\nFor development instructions, please refer to README_DEV.md.\nğŸ“œ License\nThis project is licensed under the MIT License - see the LICENSE file for details.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "5605dab30cc5cb7a",
    "title": {
      "en": "How one of the worldâ€™s major money laundering networks operates",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.nytimes.com/2025/03/23/world/asia/cambodia-money-laundering-huione.html",
    "score": 100,
    "by": "bookofjoe",
    "time": 1742751402,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "09e9f98019f60115",
    "title": {
      "en": "PyTorch Internals: Ezyang's Blog",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://blog.ezyang.com/2019/05/pytorch-internals/",
    "score": 427,
    "by": "Anon84",
    "time": 1742654344,
    "content": "PyTorch internals\n\nThis post is a long form essay version of a talk about PyTorch internals, that I gave at the PyTorch NYC meetup on May 14, 2019.\n\nHi everyone!  Today I want to talk about the internals of PyTorch.\n\nThis talk is for those of you who have used PyTorch, and thought to yourself, \"It would be great if I could contribute to PyTorch,\" but were scared by PyTorch's behemoth of a C++ codebase.  I'm not going to lie: the PyTorch codebase can be a bit overwhelming at times. The purpose of this talk is to put a map in your hands: to tell you about the basic conceptual structure of a \"tensor library that supports automatic differentiation\", and give you some tools and tricks for finding your way around the codebase.  I'm going to assume that you've written some PyTorch before, but haven't necessarily delved deeper into how a machine learning library is written.\n\nThe talk is in two parts: in the first part, I'm going to first introduce you to the conceptual universe of a tensor library.  I'll start by talking about the tensor data type you know and love, and give a more detailed discussion about what exactly this data type provides, which will lead us to a better understanding of how it is actually implemented under the hood.  If you're an advanced user of PyTorch, you'll be familiar with most of this material.  We'll also talk about the trinity of \"extension points\", layout, device and dtype, which guide how we think about extensions to the tensor class.  In the live talk at PyTorch NYC, I skipped the slides about autograd, but I'll talk a little bit about them in these notes as well.\nThe second part grapples with the actual nitty gritty details involved with actually coding in PyTorch.  I'll tell you how to cut your way through swaths of autograd code, what code actually matters and what is legacy, and also all of the cool tools that PyTorch gives you for writing kernels.\n\nThe tensor is the central data structure in PyTorch.  You probably have a pretty good idea about what a tensor intuitively represents: its an n-dimensional data structure containing some sort of scalar type, e.g., floats, ints, et cetera.  We can think of a tensor as consisting of some data, and then some metadata describing the size of the tensor, the type of the elements in contains (dtype), what device the tensor lives on (CPU memory? CUDA memory?)\n\nThere's also a little piece of metadata you might be less familiar with: the stride.  Strides are actually one of the distinctive features of PyTorch, so it's worth discussing them a little more.\n\nA tensor is a mathematical concept.  But to represent it on our computers, we have to define some sort of physical representation for them.  The most common representation is to lay out each element of the tensor contiguously in memory (that's where the term contiguous comes from), writing out each row to memory, as you see above. In the example above, I've specified that the tensor contains 32-bit integers, so you can see that each integer lies in a physical address, each offset four bytes from each other.  To remember what the actual dimensions of the tensor are, we have to also record what the sizes are as extra metadata.\nSo, what do strides have to do with this picture?\n\nSuppose that I want to access the element at position tensor[1, 0] in my logical representation.  How do I translate this logical position into a location in physical memory?  Strides tell me how to do this: to find out where any element for a tensor lives, I multiply each index with the respective stride for that dimension, and sum them all together.  In the picture above, I've color coded the first dimension blue and the second dimension red, so you can follow the index and stride in the stride calculation.  Doing this sum, I get two (zero-indexed), and indeed, the number three lives two below the beginning of the contiguous array.\n(Later in the talk, I'll talk about TensorAccessor, a convenience class that handles the indexing calculation.  When you use TensorAccessor, rather than raw pointers, this calculation is handled under the covers for you.)\nStrides are the fundamental basis of how we provide views to PyTorch users.  For example, suppose that I want to extract out a tensor that represents the second row of the tensor above:\n\nUsing advanced indexing support, I can just write tensor[1, :] to get this row.  Here's the important thing: when I do this, I don't create a new tensor; instead, I just return a tensor which is a different view on the underlying data.  This means that if I, for example, edit the data in that view, it will be reflected in the original tensor.  In this case, it's not too hard to see how to do this: three and four live in contiguous memory, and all we need to do is record an offset saying that the data of this (logical) tensor lives two down from the top.  (Every tensor records an offset, but most of the time it's zero, and I'll omit it from my diagrams when that's the case.)\n\nQuestion from the talk: If I take a view on a tensor, how do I free the memory of the underlying tensor?\nAnswer: You have to make a copy of the view, thus disconnecting it from the original physical memory.  There's really not much else you can do.  By the way, if you have written Java in the old days, taking substrings of strings has a similar problem, because by default no copy is made, so the substring retains the (possibly very large string). Apparently, they fixed this in Java 7u6.\n\nA more interesting case is if I want to take the first column:\n\nWhen we look at the physical memory, we see that the elements of the column are not contiguous: there's a gap of one element between each one.  Here, strides come to the rescue: instead of specifying a stride of one, we specify a stride of two, saying that between one element and the next, you need to jump two slots.  (By the way, this is why it's called a \"stride\": if we think of an index as walking across the layout, the stride says how many locations we stride forward every time we take a step.)\nThe stride representation can actually let you represent all sorts of interesting views on tensors; if you want to play around with the possibilities, check out the Stride Visualizer.\nLet's step back for a moment, and think about how we would actually implement this functionality (after all, this is an internals talk.)  If we can have views on tensor, this means we have to decouple the notion of the tensor (the user-visible concept that you know and love), and the actual physical data that stores the data of the tensor (called storage):\n\nThere may be multiple tensors which share the same storage.  Storage defines the dtype and physical size of the tensor, while each tensor records the sizes, strides and offset, defining the logical interpretation of the physical memory.\nOne thing to realize is that there is always a pair of Tensor-Storage, even for \"simple\" cases where you don't really need a storage (e.g., you just allocated a contiguous tensor with torch.zeros(2, 2)).\n\nBy the way, we're interested in making this picture not true; instead of having a separate concept of storage, just define a view to be a tensor that is backed by a base tensor.  This is a little more complicated, but it has the benefit that contiguous tensors get a much more direct representation without the Storage indirection.  A change like this would make PyTorch's internal representation a bit more like Numpy's.\n\nWe've talked quite a bit about the data layout of tensor (some might say, if you get the data representation right, everything else falls in place).  But it's also worth briefly talking about how operations on the tensor are implemented.  At the very most abstract level, when you call torch.mm, two dispatches happen:\n\nThe first dispatch is based on the device type and layout of a tensor: e.g., whether or not it is a CPU tensor or a CUDA tensor (and also, e.g., whether or not it is a strided tensor or a sparse one).  This is a dynamic dispatch: it's a virtual function call (exactly where that virtual function call occurs will be the subject of the second half of this talk).  It should make sense that you need to do a dispatch here: the implementation of CPU matrix multiply is quite different from a CUDA implementation.  It is a dynamic dispatch because these kernels may live in separate libraries (e.g., libcaffe2.so versus libcaffe2_gpu.so), and so you have no choice: if you want to get into a library that you don't have a direct dependency on, you have to dynamic dispatch your way there.\nThe second dispatch is a dispatch on the dtype in question.  This dispatch is just a simple switch-statement for whatever dtypes a kernel chooses to support.  Upon reflection, it should also make sense that we need to a dispatch here: the CPU code (or CUDA code, as it may) that implements multiplication on float is different from the code for int.  It stands to reason you need separate kernels for each dtype.\nThis is probably the most important mental picture to have in your head, if you're trying to understand the way operators in PyTorch are invoked.  We'll return to this picture when it's time to look more at code.\n\nSince we have been talking about Tensor, I also want to take a little time to the world of tensor extensions.  After all, there's more to life than dense, CPU float tensors.  There's all sorts of interesting extensions going on, like XLA tensors, or quantized tensors, or MKL-DNN tensors, and one of the things we have to think about, as a tensor library, is how to accommodate these extensions.\n\nOur current model for extensions offers four extension points on tensors.  First, there is the trinity three parameters which uniquely determine what a tensor is:\n\nThe device, the description of where the tensor's physical memory is actually stored, e.g., on a CPU, on an NVIDIA GPU (cuda), or perhaps on an AMD GPU (hip) or a TPU (xla).  The distinguishing characteristic of a device is that it has its own allocator, that doesn't work with any other device.\nThe layout, which describes how we logically interpret this physical memory.  The most common layout is a strided tensor, but sparse tensors have a different layout involving a pair of tensors, one for indices, and one for data; MKL-DNN tensors may have even more exotic layout, like blocked layout, which can't be represented using merely strides.\nThe dtype, which describes what it is that is actually stored in each element of the tensor.  This could be floats or integers, or it could be, for example, quantized integers.\n\nIf you want to add an extension to PyTorch tensors (by the way, if that's what you want to do, please talk to us!  None of these things can be done out-of-tree at the moment), you should think about which of these parameters you would extend.  The Cartesian product of these parameters define all of the possible tensors you can make.  Now, not all of these combinations may actually have kernels (who's got kernels for sparse, quantized tensors on FPGA?) but in principle the combination could make sense, and thus we support expressing it, at the very least.\nThere's one last way you can make an \"extension\" to Tensor functionality, and that's write a wrapper class around PyTorch tensors that implements your object type.  This perhaps sounds obvious, but sometimes people reach for extending one of the three parameters when they should have just made a wrapper class instead.  One notable merit of wrapper classes is they can be developed entirely out of tree.\nWhen should you write a tensor wrapper, versus extending PyTorch itself?  The key test is whether or not you need to pass this tensor along during the autograd backwards pass.  This test, for example, tells us that sparse tensor should be a true tensor extension, and not just a Python object that contains an indices and values tensor: when doing optimization on networks involving embeddings, we want the gradient generated by the embedding to be sparse.\n\nOur philosophy on extensions also has an impact of the data layout of tensor itself.  One thing we really want out of our tensor struct is for it to have a fixed layout: we don't want fundamental (and very frequently called) operations like \"What's the size of a tensor?\" to require virtual dispatches.  So when you look at the actual layout of a Tensor (defined in the TensorImpl struct),  what we see is a common prefix of all fields that we consider all \"tensor\"-like things to universally have, plus a few fields that are only really applicable for strided tensors, but are so important that we've kept them in the main struct, and then a suffix of custom fields that can be done on a per-Tensor basis.  Sparse tensors, for example, store their indices and values in this suffix.\n\nI told you all about tensors, but if that was the only thing PyTorch provided, we'd basically just be a Numpy clone.  The distinguishing characteristic of PyTorch when it was originally released was that it provided automatic differentiation on tensors (these days, we have other cool features like TorchScript; but back then, this was it!)\nWhat does automatic differentiation do?  It's the machinery that's responsible for taking a neural network:\n\n...and fill in the missing code that actually computes the gradients of your network:\n\nTake a moment to study this diagram.  There's a lot to unpack; here's what to look at:\n\nFirst, rest your eyes on the variables in red and blue.  PyTorch implements reverse-mode automatic differentiation, which means that we effectively walk the forward computations \"backward\" to compute the gradients.  You can see this if you look at the variable names: at the bottom of the red, we compute loss; then, the first thing we do in the blue part of the program is compute grad_loss.  loss was computed from next_h2, so we compute grad_next_h2.  Technically, these variables which we call grad_ are not really gradients; they're really Jacobians left-multiplied by a vector, but in PyTorch we just call them grad and mostly everyone knows what we mean.\nIf the structure of the code stays the same, the behavior doesn't: each line from forwards is replaced with a different computation, that represents the derivative of the forward operation.  For example, the tanh operation is translated into a tanh_backward operation (these two lines are connected via a grey line on the left hand side of the diagram).  The inputs and outputs of the forward and backward operations are swapped: if the forward operation produced next_h2, the backward operation takes grad_next_h2 as an input.\n\nThe whole point of autograd is to do the computation that is described by this diagram, but without actually ever generating this source.  PyTorch autograd doesn't do a source-to-source transformation (though PyTorch JIT does know how to do symbolic differentiation).\n\nTo do this, we need to store more metadata when we carry out operations on tensors.  Let's adjust our picture of the tensor data structure: now instead of just a tensor which points to a storage, we now have a variable which wraps this tensor, and also stores more information (AutogradMeta), which is needed for performing autograd when a user calls loss.backward() in their PyTorch script.\n\nThis is yet another slide which will hopefully be out of date in the near future.  Will Feng is working on a Variable-Tensor merge in C++, following a simple merge which happened to PyTorch's frontend interface.\nWe also have to update our picture about dispatch:\n\nBefore we dispatch to CPU or CUDA implementations, there is another dispatch on variables, which is responsible for unwrapping variables, calling the underlying implementation (in green), and then rewrapping the results into variables and recording the necessary autograd metadata for backwards.\nSome implementations don't unwrap; they just call into other variable implementations.  So you might spend a while in the Variable universe.  However, once you unwrap and go into the non-Variable Tensor universe, that's it; you never go back to Variable (except by returning from your function.)\n\nIn my NY meetup talk, I skipped the following seven slides.  I'm also going to delay writeup for them; you'll have to wait for the sequel for some text.\n\nEnough about concepts, let's look at some code.\n\nPyTorch has a lot of folders, and there is a very detailed description of what they are in the CONTRIBUTING document, but really, there are only four directories you really need to know about:\n\nFirst, torch/ contains what you are most familiar with: the actual Python modules that you import and use.  This stuff is Python code and easy to hack on (just make a change and see what happens).  However, lurking not too deep below the surface is...\ntorch/csrc/, the C++ code that implements what you might call the frontend of PyTorch.  In more descriptive terms, it implements the binding code that translates between the Python and C++ universe, and also some pretty important pieces of PyTorch, like the autograd engine and the JIT compiler.  It also contains the C++ frontend code.\naten/, short for \"A Tensor Library\" (coined by Zachary DeVito), is a C++ library that implements the operations of Tensors.  If you're looking for where some kernel code lives, chances are it's in ATen.  ATen itself bifurcates into two neighborhoods of operators: the \"native\" operators, which are modern, C++ implementations of operators, and the \"legacy\" operators (TH, THC, THNN, THCUNN), which are legacy, C implementations.  The legacy operators are the bad part of town; try not to spend too much time there if you can.\nc10/, which is a pun on Caffe2 and A\"Ten\" (get it? Caffe 10) contains the core abstractions of PyTorch, including the actual implementations of the Tensor and Storage data structures.\n\nThat's a lot of places to look for code; we should probably simplify the directory structure, but that's how it is.  If you're trying to work on operators, you'll spend most of your time in aten.\nLet's see how this separation of code breaks down in practice:\n\nWhen you call a function like torch.add, what actually happens?  If you remember the discussion we had about dispatching, you already have the basic picture in your head:\n\nWe have to translate from Python realm to the C++ realm (Python argument parsing)\nWe handle variable dispatch (VariableType--Type, by the way, doesn't really have anything to do programming language types, and is just a gadget for doing dispatch.)\nWe handle device type / layout dispatch (Type)\nWe have the actual kernel, which is either a modern native function, or a legacy TH function.\n\nEach of these steps corresponds concretely to some code.  Let's cut our way through the jungle.\n\nOur initial landing point in the C++ code is the C implementation of a Python function, which we've exposed to the Python side as something like torch._C.VariableFunctions.add.  THPVariable_add is the implementation of one such implementation.\nOne important thing to know about this code is that it is auto-generated.  If you search in the GitHub repository, you won't find it, because you have to actually build PyTorch to see it.  Another important thing is, you don't have to really deeply understand what this code is doing; the idea is to skim over it and get a sense for what it is doing.  Above, I've annotated some of the most important bits in blue: you can see that there is a use of a class PythonArgParser to actually pull out C++ objects out of the Python args and kwargs; we then call a dispatch_add function (which I've inlined in red); this releases the global interpreter lock and then calls a plain old method on the C++ Tensor self.  On its way back, we rewrap the returned Tensor back into a PyObject.\n(At this point, there's an error in the slides: I'm supposed to tell you about the Variable dispatch code.  I haven't fixed it here yet.  Some magic happens, then...)\n\nWhen we call the add method on the Tensor class, no virtual dispatch happens yet.  Instead, we have an inline method which calls a virtual method on a \"Type\" object.  This method is the actual virtual method (this is why I say Type is just a \"gadget\" that gets you dynamic dispatch.)  In the particular case of this example, this virtual call dispatches to an implementation of add on a class named TypeDefault.  This happens to be because we have an implementation of add that is the same for every device type (both CPU and CUDA); if we had happened to have different implementations, we might have instead landed on something like CPUFloatType::add.  It is this implementation of the virtual method that finally gets us to the actual kernel code.\n\nHopefully, this slide will be out-of-date very soon too; Roy Li is working on replacing Type dispatch with another mechanism which will help us better support PyTorch on mobile.\nIt's worth reemphasizing that all of the code, until we got to the kernel, is automatically generated.\n\nIt's a bit twisty and turny, so once you have some basic orientation about what's going on, I recommend just jumping straight to the kernels.\n\nPyTorch offers a lot of useful tools for prospective kernel writers.  In this section, we'll walk through a few of them.  But first of all, what do you need to write a kernel?\n\nWe generally think of a kernel in PyTorch consisting of the following parts:\n\nFirst, there's some metadata which we write about the kernel, which powers the code generation and lets you get all the bindings to Python, without having to write a single line of code.\nOnce you've gotten to the kernel, you're past the device type / layout dispatch. The first thing you need to write is error checking, to make sure the input tensors are the correct dimensions.  (Error checking is really important!  Don't skimp on it!)\nNext, we generally have to allocate the result tensor which we are going to write the output into.\nTime for the kernel proper.  At this point, you now should do the second, dtype dispatch, to jump into a kernel which is specialized per dtype it operates on.  (You don't want to do this too early, because then you will be uselessly duplicating code that looks the same in any case.)\nMost performant kernels need some sort of parallelization, so that you can take advantage of multi-CPU systems.  (CUDA kernels are \"implicitly\" parallelized, since their programming model is built on top of massive parallelization).\nFinally, you need to access the data and do the computation you wanted to do!\n\nIn the subsequent slides, we'll walk through some of the tools PyTorch has for helping you implementing these steps.\n\nTo take advantage of all of the code generation which PyTorch brings, you need to write a schema for your operator.  The schema gives a mypy-esque type of your function, and also controls whether or not we generate bindings for methods or functions on Tensor.  You also tell the schema what implementations of your operator should be called for given device-layout combinations.  Check out the README in native is for more information about this format.\n\nYou also may need to define a derivative for your operation in derivatives.yaml.\n\nError checking can be done by way of either a low level or a high level API.  The low level API is just a macro, TORCH_CHECK, which takes a boolean, and then any number of arguments to make up the error string to render if the boolean is not true.  One nice thing about this macro is that you can intermix strings with non-string data; everything is formatted using their implementation of operator<<, and most important data types in PyTorch have operator<< implementations.\nThe high level API saves you from having to write up repetitive error messages over and over again.  The way it works is you first wrap each Tensor into a TensorArg, which contains information about where the tensor came from (e.g., its argument name).  It then provides a number of pre-canned functions for checking various properties; e.g., checkDim() tests if the tensor's dimensionality is a fixed number.  If it's not, the function provides a user-friendly error message based on the TensorArg metadata.\n\nOne important thing to be aware about when writing operators in PyTorch, is that you are often signing up to write three operators: abs_out, which operates on a preallocated output (this implements the out= keyword argument), abs_, which operates inplace, and abs, which is the plain old functional version of an operator.\nMost of the time, abs_out is the real workhorse, and abs and abs_ are just thin wrappers around abs_out; but sometimes writing specialized implementations for each case are warranted.\n\nTo do dtype dispatch, you should use the AT_DISPATCH_ALL_TYPES macro.  This takes in the dtype of the tensor you want to dispatch over, and a lambda which will be specialized for each dtype that is dispatchable from the macro.  Usually, this lambda just calls a templated helper function.\nThis macro doesn't just \"do dispatch\", it also decides what dtypes your kernel will support.  As such, there are actually quite a few versions of this macro, which let you pick different subsets of dtypes to generate specializations for.  Most of the time, you'll just want AT_DISPATCH_ALL_TYPES, but keep an eye out for situations when you might want to dispatch to some more types.  There's guidance in Dispatch.h for how to select the correct one for your use-case.\n\nOn CPU, you frequently want to parallelize your code.  In the past, this was usually done by directly sprinkling OpenMP pragmas in your code.\n\nAt some point, we have to actually access the data.  PyTorch offers quite a few options for doing this.\n\nIf you just want to get a value at some specific location, you should use TensorAccessor.  A tensor accessor is like a tensor, but it hard codes the dimensionality and dtype of the tensor as template parameters.  When you retrieve an accessor like x.accessor<float, 3>();, we do a runtime test to make sure that the tensor really is this format; but after that, every access is unchecked.  Tensor accessors handle strides correctly, so you should prefer using them over raw pointer access (which, unfortunately, some legacy kernels do.)  There is also a PackedTensorAccessor, which is specifically useful for sending an accessor over a CUDA launch, so that you can get accessors from inside your CUDA kernel.  (One notable gotcha: TensorAccessor defaults to 64-bit indexing, which is much slower than 32-bit indexing in CUDA!)\nIf you're writing some sort of operator with very regular element access, for example, a pointwise operation, you are much better off using a higher level of abstraction, the TensorIterator.   This helper class automatically handles broadcasting and type promotion for you, and is quite handy.\nFor true speed on CPU, you may need to write your kernel using vectorized CPU instructions.  We've got helpers for that too!  The Vec256 class represents a vector of scalars and provides a number of methods which perform vectorized operations on them all at once.  Helpers like binary_kernel_vec then let you easily run vectorized operations, and then finish everything that doesn't round nicely into vector instructions using plain old instructions.  The infrastructure here also manages compiling your kernel multiple times under different instruction sets, and then testing at runtime what instructions your CPU supports, and using the best kernel in those situations.\n\nA lot of kernels in PyTorch are still written in the legacy TH style.  (By the way, TH stands for TorcH.  It's a pretty nice acronym, but unfortunately it is a bit poisoned; if you see TH in the name, assume that it's legacy.)  What do I mean by the legacy TH style?\n\nIt's written in C style, no (or very little) use of C++.\nIt's manually refcounted (with manual calls to THTensor_free to decrease refcounts when you're done using tensors), and\nIt lives in generic/ directory, which means that we are actually going to compile the file multiple times, but with different #define scalar_t.\n\nThis code is pretty crazy, and we hate reviewing it, so please don't add to it.  One of the more useful tasks that you can do, if you like to code but don't know too much about kernel writing, is to port some of these TH functions to ATen.\n\nTo wrap up, I want to talk a little bit about working efficiently on PyTorch.  If the largeness of PyTorch's C++ codebase is the first gatekeeper that stops people from contributing to PyTorch, the efficiency of your workflow is the second gatekeeper.  If you try to work on C++ with Python habits, you will have a bad time: it will take forever to recompile PyTorch, and it will take you forever to tell if your changes worked or not.\nHow to work efficiently could probably be a talk in and of itself, but this slide calls out some of the most common anti-patterns I've seen when someone complains: \"It's hard to work on PyTorch.\"\n\nIf you edit a header, especially one that is included by many source files (and especially if it is included by CUDA files), expect a very long rebuild.  Try to stick to editing cpp files, and edit headers sparingly!\nOur CI is a very wonderful, zero-setup way to test if your changes worked or not. But expect to wait an hour or two before you get back signal.  If you are working on a change that will require lots of experimentation, spend the time setting up a local development environment.  Similarly, if you run into a hard to debug problem on a specific CI configuration, set it up locally.  You can download and run the Docker images locally\nThe CONTRIBUTING guide explains how to setup ccache; this is highly recommended, because sometimes it will help you get lucky and avoid a massive recompile when you edit a header.  It also helps cover up bugs in our build system, when we recompile files when we shouldn't.\nAt the end of the day, we have a lot of C++ code, and you will have a much more pleasant experience if you build on a beefy server with CPUs and RAM.  In particular, I don't recommend doing CUDA builds on a laptop; building CUDA is sloooooow and laptops tend to not have enough juice to turnaround quickly enough.\n\nSo that's it for a whirlwind tour of PyTorch's internals!  Many, many things have been omitted; but hopefully the descriptions and explanations here can help you get a grip on at least a substantial portion of the codebase.\nWhere should you go from here?  What kinds of contributions can you make?  A good place to start is our issue tracker.  Starting earlier this year, we have been triaging issues; issues labeled triaged mean that at least one PyTorch developer has looked at it and made an initial assessment about the issue.  You can use these labels to find out what issues we think are high priority or look up issues specific to some module, e.g., autograd or find issues which we think are small (word of warning: we're sometimes wrong!)\nEven if you don't want to get started with coding right away, there are many other useful activities like improving documentation (I love merging documentation PRs, they are so great), helping us reproduce bug reports from other users, and also just helping us discuss RFCs on the issue tracker. PyTorch would not be where it is today without our open source contributors; we hope you can join us too!\n\n          May 16, 2019\n          PyTorch",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "26d559c47369fac8",
    "title": {
      "en": "The Race to Decipher Cuneiform in the 19th Century",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.smithsonianmag.com/history/mystery-worlds-oldest-writing-system-remained-unsolved-until-four-scholars-raced-decipher-it-180985954/",
    "score": 37,
    "by": "Petiver",
    "time": 1742481030,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "7ebe6c4ba0f90ac6",
    "title": {
      "en": "Derivatives and Logarithms of 3D Transforms",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://nosferalatu.com/./DerivativesLogarithmsTransforms.html",
    "score": 96,
    "by": "nosferalatu123",
    "time": 1742515596,
    "content": "Derivatives, Logarithms, andTransforms\n\n    Date\n\n         Tue 07 February 2023\n\nTags\n\tProgramming\n\n                Given a transform TTT and a point x, we can find the transformed point with Tâˆ—xTâˆ—xT * x. But what if we want to smoothly interpolate TTT so it moves xxx along the path from its initial position to its position transformed by TTT?\nWhat we want to find is the point xxx at time ttt:\nx(t)=T(t)âˆ—x(0)x(t) = T(t) *x(0)\nwhere x(0)x(0) is the pointâ€™s initial position, and T(t)T(t) is the transform at time tt. Since we have only a single transform TT, we need to find a way to interpolate it overtime.\nOne way to accomplish this is to raise TT to the power of tt, which can be done using the exponential and logarithm of the transform. Interestingly, the logarithm of a transform can also be used to easily find the velocity of a point xx in space: the velocity vector (also called the tangent vector) is just log(T)âˆ—xlog(T) * x. This blog post shows the relationship between the logarithm andvelocity.\nExample\nCheck out this interactive example to see how the vector field changes as you manipulate the gizmo to translate and rotate the transform. The vector field represents the velocity vector at each point in space during thetransformation.\nAs you move the gizmo, youâ€™ll notice a white curve that traces the path from the origin to the gizmoâ€™s transform. Along this curve, youâ€™ll see the interpolated transform as it travels from the origin to the gizmo. As you can see, the interpolation follows the flow of the velocity vector field. The appletâ€™s code is using the exponential and logarithm of the transform to compute the curve, interpolated transform, and vectorfield.\n\n   ControlsGizmoModeRotateTranslateRotateShowVectorFieldShowOriginAxesShowGizmoShowInterpolatedTransform\n\nThe source code for the applet can be found here, which includes an implementation of closed-form log() and exp() for rigid bodytransforms.\nNext, Iâ€™ll describe how to compute the interpolated transform and the velocity vector field you see in thisexample.\nWhatâ€™s T(t)T(t)?\nWe have TT, but not T(t)T(t), which changes with time. Assuming that multiplying two transforms represents the composition of those transforms, we can find T(t)T(t) bysaying\nT(0)=IT(0) = I (the identitytransform)\nT(1)=TT(1) =T\nT(2)=Tâˆ—TT(2) = T *T\nT(3)=Tâˆ—Tâˆ—TT(3) = T * T *T\nMore generally, we can find TT at any time bysaying\nT(t)=TtT(t) = T^t.\nThe above trick is from a blog post by Fabian Giesen here but works for any transform that uses multiplication forcomposition.\nNow that we know T(t)=TtT(t) = T^t, the original equation can be rearrangedto\nx(t)=Ttâˆ—x(0)x(t) = T^t * x(0).\nWhatâ€™s TtT^t?\nTo compute TtT^t, we need to use the matrix exponential and matrixlogarithm.\nLetâ€™s start with two facts about a matrixX:\nelog(X)=Xe^{log(X)} = Xand\nlog(Xy)=log(X)âˆ—ylog(X^y) = log(X) * y.\nPut together, we can saythat\nTt=elog(Tt)=elog(T)âˆ—tT^t = e^{log(T^t)} =e^{log(T)*t}\nwhich we can plug into the earlier equation, givingus\nx(t)=elog(T)âˆ—tâˆ—x(0)x(t) = e^{log(T) * t} * x(0).\nThis says that to find the point x at time t, find the transform at time t using elog(T)âˆ—te^{log(T) * t}, and use that to transform the point at its initial position (at time0).\nWhatâ€™s thederivative?\nIn calculus, we learnedthat\nddteat=aeat\\dfrac{d}{dt}e^{a t} = a e^{at}\nwhich holds true for matrices aswell:\nddteAt=AeAt\\dfrac{d}{dt}e^{A t} = A e^{At}\nThis relationship is explained in more detail in the section the derivative of the matrix exponential.\nWe can use this property to find the derivative of our earlier equation x(t)=elog(T)tx(0)x(t) = e^{log(T)t} x(0) with respect tot:\nddtx(t)=log(T)elog(T)tx(0)\\dfrac{d}{dt}x(t) = log(T) e^{log(T) t} x(0).\nThis equation states that to find the first derivative (the velocity vector, also called the tangent vector) of the point at time t, you first transform the pointâ€™s initial position x(0)x(0) with the interpolated transform elog(T)te^{log(T)t} and then multiply it by the logarithm of the transform log(T)log(T). This expression follows the right-to-left convention of column vectors, so you would start with the initial position x(0)x(0), then apply the interpolated transform elog(T)te^{log(T)t}, and finally multiply by the logarithm log(T)log(T).\nelog(T)te^{log(T) t} acts as an operator that maps points from their initial position to their new position at time t. The matrix exponential can be thought of as like integration. At time 0, elog(T)te^{log(T) t} is the identity matrix (e0=Ie^0=I for matrix exponentials), and at time 1.0, elog(T)te^{log(T) t} is equal to the original transform matrix T (elog(T)=Te^{log(T)}=T).\nWhatâ€™s this allmean?\nIf we take the equation at the end of â€œWhatâ€™s TtT^t?â€\nx(t)=elog(T)tx(0)x(t) = e^{log(T) t}x(0)\nand substitute that into the equation at the end of â€œWhatâ€™s thederivative?â€\nddtx(t)=log(T)elog(T)tx(0)\\dfrac{d}{dt}x(t) = log(T) e^{log(T) t} x(0),\nthen wehave:\nddtx(t)=log(T)x(t)\\dfrac{d}{dt}x(t) = log(T) x(t).\nThis relates the derivative of a moving point to the logarithm of the transformation moving thatpoint.\nOne way to think of log(T)log(T) is as a vector field of tangent vectors for the transformation. In other words, itâ€™s the field of first derivatives. This vector field is independent of time and shows the velocity for every point inspace.\nThat equation is saying that if you transform any point in space by the logarithm of the transform, you will get the first derivative at that point. The first derivative is the velocity, so log(T)log(T) defines the velocity field (the field of tangent vectors at every point inspace).\nAs a point moves through space by the transform, it forms a curve. The tangent vector at time t is tangent to the pointâ€™s position on the curve at timet.\nYou can think of the logarithm of a matrix as the velocity field of the action performed by that matrix. The velocity field visualized in the interactive example above is thisfield.\nA more informal way of looking at this is tosay\nvelocity=log(transform)âˆ—positionvelocity = log(transform) *position\nmeaning, to understand how a point will move in time, look at the vector field of the log of the transform as a velocity field. As the point flows along that velocity field, it moves intime.\nWhatâ€™s the differentialequation?\nWe can also reformulate all of this as a differential equation. Earlier, wehad\nddtx(t)=log(T)x(t)\\dfrac{d}{dt} x(t) = log(T)x(t)\nwhich is a differential equation. Because log(T)log(T) is a matrix, it is more specifically a matrix differentialequation.\nScalar ordinary differential equations of theform\nyâ€²(t)=ay(t)y'(t)=ay(t)\nhave the generalsolution\ny(t)=eaty(0)y(t)=e^{at}y(0).\nSimilarly, matrix differential equations of theform\nxâ€²(t)=Ax(t)x'(t)=Ax(t)\nhave the generalsolution\nx(t)=eAtx(0)x(t)=e^{At}x(0).\nTherefore, given our equation fromearlier\nddtx(t)=log(T)x(t)\\dfrac{d}{dt} x(t) = log(T)x(t)\nwe have thesolution\nx(t)=elog(T)tx(0)x(t) = e^{log(T) t} x(0).\nThis is the same as our original equation, but we started with a differential equation and found a solution. To prove this solution is correct, just take the derivative of it, which is what we did earlier in the Whatâ€™s the derivative?section.\nThe exponential map and logarithmmap\nThe exponential map is defined as the infiniteseries\neAt=I+At+12(At)2+13!(At)3+...=âˆâˆ‘i=0(At)ii!e^{A t} = I + A t + \\frac{1}{2}(A t)^2 + \\frac{1}{3!}(A t)^3 + ... = \\sum_{i=0}^{\\infty} \\frac{(At)^i}{i!}\nand can be used to find the exponential of real numbers, complex numbers, quaternions, matrices, and more. For example, when square matrices are plugged in to the series, the result is called the matrix exponential.\nSimilarly, the logarithm is defined as the infiniteseries\nlog(A)=âˆâˆ‘i=1(âˆ’1)i+1(Aâˆ’I)iilog(A) = \\sum_{i=1}^{\\infty} (-1)^{i+1} \\frac{(A - I)^i}{i}\nIf you want to know more, search for the exponential map and logarithm map. Youâ€™ll find that these are important concepts in Lie group theory. The exponential map and logarithm map are inverses of each other. In Lie theory, the exponential map maps a tangent vector at a point p to a point on the manifold. The logarithm map does the opposite, mapping a point on the manifold back to the tangent vector atp.\nWhen reading about Lie groups, youâ€™ll come across many different kinds of groups. There are only a few groups that are related to transforms, though. SO(3) is a 3D rotation matrix, SU(2) is a quaternion, SE(3) is a 3D rigid body transform (rotation and translation), SIM(3) is rotation, translation, and (positive) uniform scale, and GL(n) is an nxnmatrix.\nThere are several options for how to practically compute the exponential and logarithm map for a matrix or otherobject:\n1) Use a math library like Eigen or Armadillo. These have functions to compute the matrix exponential and matrixlogarithm.\n2) The library Sophus has code for a closed form exp/log for the groups SO(3), SE(3), and SIM(3). Beware that it clamps its quaternions to a 3D rotation angle in -Ï€\\pi â€¦ +Ï€\\pithough.\n3) There is an excellent PDF at the web site of Ethan Eade here which contains the closed form equations for the groups SO(3), SE(3), and SIM(3).\n4) Compute the matrix exponential and logarithm by using the infinite series definitions above, and truncating after some number of terms. In my experience, this is not robust when working with floating point numbers, as you quickly start to deal with very small and very large numbers, depending on your inputmatrix.\n5) Compute the exponential with numerical integration. Given a starting point xx, integrating it for time t is the same thing as the exponential. There are many ways to compute numerical integration, from Euler to Runge-Kutta to adaptivemethods.\nPitfalls\nThere are a few issues that you should be awareof.\nPitfall#1\nThe logarithm of a rotation matrix will return a 3D rotation angle in -Ï€\\pi â€¦ +Ï€\\pi. More technically, there are an infinite number of logarithms for a matrix, each corresponding to a rotation angle that is  2Ï€\\pi greater than the previous one. Generally matrix logarithm code will return the principal logarithm, which is the logarithm in -Ï€\\pi â€¦ +Ï€\\pi. This can lead to discontinuities when interpolating transforms with rotations in them, such rotations from human joints (you can move your head from looking over your left shoulder to over your right shoulder and rotate a little more than 180degrees).\nOn the other hand, the logarithm of a quaternion returns a 3D rotation angle in the larger range of -2Ï€\\pi â€¦ +2Ï€\\pi, which makes quaternions nicer to workwith.\nPitfall#2\nWhen working with logarithms, be aware that that theproperty\nlog(AB)=log(A)+log(B)log(AB) = log(A) +log(B)\nis only true when A and B commute, which is not the case for most transforms. Real numbers always commute, though, so the property does apply to them. Itâ€™s tempting to apply the property to transforms, but itâ€™s important to remember it only applies when A and Bcommute.\nPitfall#3\nRelated to pitfall #2, you might want to interpolate two transforms A and Bwith\ninterpolate(A,B,t)=e(1âˆ’t)âˆ—log(A)+tâˆ—log(B)interpolate(A, B, t) = e^{(1-t)*log(A) +t*log(B)}\nBut be careful: this only works if A and B commute, which is not usually the case for transforms. Otherwise, this interpolation is neither shortest path nor constantspeed.\nInstead, interpolate the relative (also called delta) transform from A to B, likethis:\ninterpolate(A,B,t)=elog(BAâˆ’1)tAinterpolate(A, B, t) = e^{log(B A^{-1}) t}A\nHowever, this method only works for interpolating between two transforms and not for blending more than twotransforms.\nVisualizing a matrix as a vectorfield\nIf you are wondering how you can visualize a matrix as a vector field, an eloquent explanation is in 3Blue1Brownâ€™s video about matrix exponentiation. This part about matrices as vector fields explains that verywell:\n\nThe derivative of the matrixexponential\nEarlier we used the property ddteAt=AeAt\\dfrac{d}{dt}e^{A t} = A e^{A t}. Itâ€™s not obvious why this property is true, but itâ€™s an important part of unlocking all ofthis.\nA good reference for this derivation is in the textbook Modern Robotics. A free copy of that book can be found here. See equation (3.43) in thatbook.\nThe matrix exponential is definedas\neAt=I+At+12(At)2+13!(At)3+...e^{A t} = I + A t + \\frac{1}{2}(A t)^2 + \\frac{1}{3!}(A t)^3 +...\nWhat then is ddteAt\\dfrac{d}{dt}e^{A t}? If we take the derivative of each term of the matrix exponentialâ€™s expanded definition, wehave\nddteAt=0+A+A2t+12A3t2+...\\dfrac{d}{dt}e^{A t} = 0 + A + A^2 t + \\frac{1}{2} A^3 t^2 +...\nPull out A, and then wehave\nddteAt=Aâˆ—(I+At+12(At)2+...)=AeAt\\dfrac{d}{dt}e^{A t} = A*(I + A t + \\frac{1}{2} (A t)^2 + ...) = A e^{A t}.\nItâ€™s worth noting that the matrix AA can go on the left or right, and it always holds truethat\nAeAt=eAtAAe^{A t} = e^{At}A\nfor any square matrix, as stated in equation (3.44) in ModernRobotics.\nComments\nLeave comments on this post with Github Issues here.\nif (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {\n    var align = \"center\",\n        indent = \"0em\",\n        linebreak = \"false\";\n\n    if (false) {\n        align = (screen.width < 768) ? \"left\" : align;\n        indent = (screen.width < 768) ? \"0em\" : indent;\n        linebreak = (screen.width < 768) ? 'true' : linebreak;\n    }\n\n    var mathjaxscript = document.createElement('script');\n    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';\n    mathjaxscript.type = 'text/javascript';\n    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';\n\n    var configscript = document.createElement('script');\n    configscript.type = 'text/x-mathjax-config';\n    configscript[(window.opera ? \"innerHTML\" : \"text\")] =\n        \"MathJax.Hub.Config({\" +\n        \"    config: ['MMLorHTML.js'],\" +\n        \"    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" +\n        \"    jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" +\n        \"    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" +\n        \"    displayAlign: '\"+ align +\"',\" +\n        \"    displayIndent: '\"+ indent +\"',\" +\n        \"    showMathMenu: true,\" +\n        \"    messageStyle: 'normal',\" +\n        \"    tex2jax: { \" +\n        \"        inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" +\n        \"        displayMath: [ ['$$','$$'] ],\" +\n        \"        processEscapes: true,\" +\n        \"        preview: 'TeX',\" +\n        \"    }, \" +\n        \"    'HTML-CSS': { \" +\n        \"        availableFonts: ['STIX', 'TeX'],\" +\n        \"        preferredFont: 'STIX',\" +\n        \"        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" +\n        \"        linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" +\n        \"    }, \" +\n        \"}); \" +\n        \"if ('default' !== 'default') {\" +\n            \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n            \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n        \"}\";\n\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);\n}",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "b1540f1f5052b18b",
    "title": {
      "en": "How arrow-rs is able to decode JSON so fast",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.arroyo.dev/blog/fast-arrow-json-decoding",
    "score": 13,
    "by": "necubi",
    "time": 1742749827,
    "content": "My day job is working on the Arroyo\nstream processing engine, which executes complex, stateful queries over\nhigh-scale streams of events. Computing things like windowed aggregates, stream\njoins, and incrementally-computed SQL involves, as you might imagine, a lot of\nsophisticated algorithms and systems. Doing it much faster than existing systems\nlike Apache Flink took careful performance engineering at every level of the\nstack.\nButâ€¦I'm going to let you in on a little secret. The sad truth of this industry,\nwhich no one will tell you, is that so, so many data pipelines spend the bulk of their\nCPU timeâ€¦deserializing JSON1.\nThat's right. Everybody's favorite human-readable and sorta-human-writable data\ninterchange format has been responsible for >50% of CPU usage on several\nhigh-scale Flink pipelines I've worked on. And so if you want to build a\nfast stream processing engine, you need to start with a fast JSON\ndeserialization strategy2.\nArroyo's internal data representation is built on top of\nArrow, a columnar in-memory format designed for\nfast analytical processing3. So our challenge is to take row-oriented JSON;\nparse it, and build out columns from the corresponding field in each\ndocument. And do this as fast as possible.\nFortunately, we don't have to start from scratch: the\narrow-rs project includes a very fast JSON\ndecoder, largely written by\nRaphael Taylor-Davies. I found the approach quite\ninteresting and thought it was worth writing up. So we'll dig into how\narrow-json works and then talk through Arroyo's extensions to support raw\nJSONâ€”leaving parts of the JSON serializedâ€”and the ability to skip records in a\nbatch that don't match the schema. Along the way you should pick up some\nintuition for how to work with columnar data, which many engineers find\nchallenging and unfamiliar in our typically row-oriented world.\nTable of contents\n\nBut is this fast?\nBatched decoding\nStealing from the best\nRun the tape\n\nStrings\nLists\nAn example\n\nColumn building\n\nStruct decoding\nPrimitive and String decoding\n\nStreaming extensions\n\nRaw JSON\nInvalid data handling\n\nWrapping up\n\nBut is this fast?\nBefore we get into the details, we might want to answer the questionâ€¦is this\nactually fast? And the answer is: yes! Benchmarking against the Jackson-based\ndeserializer used for Flink SQL, arrow-json is up to 2.3x faster:\nBenchmarkJava (Âµs/record)Rust (Âµs/record)SpeedupNexmark0.5780.586-0.14%Bids0.5170.39630.6%Logs0.6090.50420.8%Tweets11.7305.108229%\n(See https://github.com/ArroyoSystems/json-benchmarks for the benchmark source code and test documents.)\nArrow-json does particularly well with large, deeply nested records, and poorly\nwith enum-like structures where there are many null fields.\nBatched decoding\nArrow, like other columnar formats, operates in batches. We're almost never\ndealing with just one record (or â€œrowâ€) of data, because columnar formats\nachieve their high performance by storing many values together in a single\narray. If you just have one row, the columnar overhead eats any performance\ngains.\nSo our goal is to take a bunch of JSON documents (generally with a top-level\nobject) and turn them into a set of arrays representing each column of the\nschema.\nTo make this concrete, let's take data representing HTTP server logs:\nField NameData TypeipUtf8identityUtf8user_idUtf8timestampTimestampdateUtf8requestUtf8codeUInt32sizeUInt64\nAn example document looks like  this:\n{\n  \"ip\": \"131.62.139.111\",\n  \"identity\": \"-\",\n  \"user_id\": \"jimmy_voluptas\",\n  \"timestamp\": \"2025-02-19T09:15:21.884163-08:00\",\n  \"request\": \"POST /tmp/high/own.zip\",\n  \"status_code\": 404,\n  \"size\": 4980,\n  \"referer\": \"-\",\n  \"user_agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36\"\n}\nAnd we'll be decoding each column into arrays like this:\nip137.100.60.1653.249.253.103118.64.108.216113.243.167.149133.131.152.130121.87.92.182identity------user_iddeven_quikiara_nihileladio_cupiditalucile_temporarae_etenrico_dolorumtimestamp2025-02-19T09:15:22.281400-08:002025-02-19T09:15:22.303590-08:002025-02-19T09:15:22.325585-08:002025-02-19T09:15:22.347106-08:002025-02-19T09:15:22.369079-08:002025-02-19T09:15:22.391311-08:00requestGET /sbin.tarGET /var/problem/first.zipGET /part/public.pptGET /etc/harper/man/few.xlsGET /usr/aria/elijah/man.rarGET /var/week/life.xlsstatus_code401500403200500403size95126965910928864592706referer------\nIn general, operations are much more efficient if we can perform them along the\ngrain of columns; in other words we want to decode all of the ips, then all of\nthe identities, then all of the user_ids, as this allows us to avoid repeatedly\ndowncasting (interpreting as a concrete type) our generically-typed arrays.\nStealing from the best\nTo do so, arrow-json uses an idea from simdjson, one of\nthe fastest JSON decoders out there4. While most JSON libraries parse\n(understanding the structure of the document) and deserialize (converting to a\ntree representation) as a single step, simdjson uses two passes. In the first\npass, it identifies all of the structural tokens ([, {, ], } ,:, ,)\nand stores their locations to a separate array5. In the second pass each\npreviously-identified node is parsed into the tree. For simdjson this strategy\nenables SIMD (single-instruction multiple data) processing to quickly identify\nthe token locations, then allows parallelizing the tree constructions because\nfields can be handled without needing to finish previous fields.\nIt turns out this structure is quite helpful when doing columnar parsing as\nwell, as we can use the output of the first pass to deserialize entire columns\nat once, as we start knowing where all of the fields corresponding to that column are\nin the buffer.\nWe do need to make some changes to the simdjson approach to support efficiently\noperating over many JSON documents. The arrow-json interface looks like this:\nimpl Decoder {\n    pub fn decode(&mut self, buf: &[u8]) -> Result<usize, ArrowError>;\n\n    pub fn flush(&mut self) -> Result<Option<RecordBatch>, ArrowError>;\n}\nThis is a function on the Decoder struct (which stores the state of the\nin-progress batch) which takes a byte sliceâ€”an immutable view of a sequence\nof bytesâ€”then returns either the number of processed bytes or an error if the\nJSON was malformed.\nThe API supports streaming. We can just keep piping in bytes representing\npotentially partial JSON documents without having to frame them (figure out\nwhere one ends and the next begins). When we're ready to consume the outputs, we\ncall flush which returns an Arrow RecordBatch (a series of columns with a\nschema) with all of the data we'd sent to decode.\nAs we're just given slices (which are references to data we don't own) and we're\ndeferring the actual deserialization to the flush call, we need to store the\nunderlying data ourselves in order to reference it in the second pass.\nRun the tape\nWith that background, we can describe the arrow-json deserialization algorithm.\nWe'll start with the first pass, where we parse the JSON into a flattened â€œtapeâ€\ndata structure, containing an array of structural components. While we process\nbytes, we're building up three data structures\n\ntape: Vec<TapeElement>, a list of structural elements in the data, like {,\n} , [, as well as strings, numbers, and literals etc. Structural elements\nstore an int identifying their opposite element (so a { points to the index\nof the corresponding }), while strings and numbers are stored as indices\ninto the offsets array\nbytes: Vec<u8>, a buffer containing raw string data and numeric literals (as\nwe don't have access to the original inputs when it comes time to building the\nRecordBatch, we have to store the raw data ourselves)\noffsets: Vec<usize>, a list of offsets into the bytes buffer, pointing to\nwhere individual elements begin (and, by looking at the next offset, where\nthey end).\n\nWe also keep a stack of our parse state, as in typical recursive-descent\nparsing, and use a state machine to drive the parsing process.\nTo start decoding, we push a Value token onto the parse stack, which indicates\nwe're looking for a valid JSON Valueâ€”one of a null, true, or false\nliteral, a string, a number, an array, or an object; these are all\ndistinguishable by looking at a single character. Whichever we find, the\ncorresponding state is pushed onto the stack and on the next iteration of the\ndecode loop we will begin to decode that value.\nTo help understand how this proceeds, we'll walk through a few value parsing\nstrategies.\nStrings\nA JSON string starts with a double-quote \", then continues through any valid\nunicode codepoints until an unescaped closing \". It may also contain special\ncharacters and arbitrary unicode via escapes (\\b, \\f, \\n, \\r, \\t,\n\\\\, \\\", \\uXXXX). We search for the next double-quote or backslash (using a\nSIMD-optimized memchr function), and\npush those bytes onto our buffer.\nWe then look at which byte we matched. If it's a backslash, we move into the\nescape state, decoding the escaped value and pushing it into our buffer. If it's\na double quote, we're done. We push buffer.len() into the offsets list to\nindicate where this string ends, then a String element onto the tape, along with\nthe second-to-last index of the offset array, which marks the start.\nLists\nList decoding starts when we encounter a\n[ token. We push StartList to the tape, then we skip ahead to the next non-whitespace, non-comma token, and check if it's a ].\nIf so, we're done with the list. We push an EndList to the tape, and update\nthe corresponding StartList to point to the index of the EndList. Otherwise,\nwe begin value parsing for the list element.\nAn example\nTo see how this works, we can walk through the example of decoding this log\nline:\n{\"ip\":\"34.127.44.91\",\"identity\":\"-\",\"user_id\":\"carmela_enim\",\n\"timestamp\":\"2025-02-19T09:15:21.839430-08:00\",\n\"request\":\"GET /sbin/early.csv\",\"status_code\":401,\"size\":3833,\"referer\":\"-\"}\nWe start, as always, in the Value state. We find the first token is a {,\nmoving us to the Object state, causing us to push StartObject(MAX_INT) to\nthe tape. Then we encounter the \", moving to the String state. We search for\nthe next \" or \\\\ , finding a\". We push the bytes between the quotes (ip)\nto the buffer, push 2 onto the offsets array, and String(0) to the tape.\nFrom there we continue through the various parse rules until we reach the };\nwe push EndObject(1) and update the initial StartObject from the placeholder\nMAX_INT to point to the end index (18).\nWhen fully parsed, we end up with the following datastructures:\n\nColumn building\nOnce we have the tape built we can perform the second pass, which builds the\nfinal columns of Arrow arrays.\nWe start by constructing a tree of decoders according to our schema. Each\ndecoder is responsible for implementing a decode function:\nfn decode(&mut self, tape: &Tape<'_>, pos: &[u32])\n   -> Result<ArrayData, ArrowError>\n(if you're not familiar with Rust definitions, this is a method, taking a\nmutable reference to the structâ€”self in Rust terms, equivalent to Java's\nthisâ€”an immutable reference to the tape datastructure we built previously, and\nan immutable slice of an array of ints, and returning either an error or\nArrayData).\nThis might look a bit funnyâ€”why are we getting an array of positions? This is\nwhere the the columnar part of this comes in, and much of the performance\njustification for the process of building up the tape before deserializing.\nBecause we've identified all of the components of the document and we know the\nschema, we can determine up-front the tape indices of all of the data that will\ngo into a particular arrayâ€”meaning we can process them all at once, in a tight,\nefficient loop, and only needing to downcast the array a single time.\nWhat does this look like? In our example, the top-level decoder is a\nStructArrayDecoder, containing as fields all of the fields of our top-level\nobject:\nStructArrayDecoer {\n  decoders: [\n    StringArrayDecoder { .. }, // ip\n    StringArrayDecoder { .. }, // identity\n    StringArrayDecoder { .. }, // user_id\n    TimestampArrayDecoder { .. }, // timestamp\n    StringArrayDecoder { .. }, // request\n    PrimitiveArrayDecoder { .. } // status_code\n    PrimitiveArraydecoder { .. } // size\n    StringArrayDecoder { .. } // referer\n  ]\n}\nStruct decoding\nDecoding starts by finding all of the tape positions of our top level value. If\nthis is a primitive value, like a string or a number, this is trivialâ€”every\nelement in our tape will be our primitive.\nIn the more common case that this is a struct, we make use of the metadata we\nstored along with the StartObject tape elementâ€”the tape position of the\ncorresponding EndObject element. So to find all of the StartObjects, we can\nuse this simple algorithm:\nlet mut cur_idx = 1; // The first offset is a null\nlet mut pos = vec![];\n\nfor _ in 0..tape.num_rows() {\n    offsets.push(cur_idx);\n\n    match tape.elements[cur_idx] {\n        TapeElement::StartObject(end_idx) => {\n            cur_idx = end_idx + 1;\n        }\n        TapeElement::Null => {\n            cur_idx += 1;\n        }\n        ..\n    }\n}\nWith our top-level positions found, we can call the decode method for our\nStructArrayDecoder, which must then find the positions array for each struct\nfield. There are three potential complications here6:\n\nExpected fields may be missing\nAdditional fields may be present\nWe might have a null literal instead of an object\n\nFor each struct position, then, we first determine whether it's an object or\nNull by querying the tape. If it's Null, we just append that to a null\nbuffer7 and continue on. If it's an object, we go field-by-field by iterating\nthrough every tape element between the object start and object end. Starting at\nthe next element, we read it as a String (if it's not, this is not a valid JSON\ndocumentâ€”since Object keys must be stringsâ€”and we can exit).\nWe look up the key in our schema and find the corresponding field, and add the\nnext tape element to its position array, then advance to the next field.\nOnce we've found the position arrays for all our fields, we simply pass them\ninto the decode method on each of our child decoders, producing arrays for\neach, which we can construct into our StructArray according to the\nArrow Struct layout.\nPrimitive and String decoding\nFor non-nested types like primitives (nulls, ints, and floats) and strings,\ndecoding is much simpler. We simply iterate through the positions array, looking\nup each element in the tape, and parsing it as the corresponding Arrow type, and\npushing it into our array (which we are able to construct and size up front,\nsince we know how many elements there will be). A bit of complexity creeps in\nhere, as JSON typing can be a bitâ€¦looseâ€¦in practice. So if we're expecting a\nnumber and find a string, we'll attempt to parse it as a number anyways8.\nIf you remember the tape encoding from earlier, there isn't much distinction\nbetween strings and numbersâ€”they're both stored as bytes in our buffer, copied\ndirectly from the input. The tape element points to an index in our offset\narray, which then indexes into our buffer. So numeric decoding in either case\nlooks like this:\nmatch tape.get(pos) {\n    TapeElement::String(idx) => {\n        let s = tape.get_string(idx);\n        let value = P::parse(s).ok_or_else(|| {\n            ArrowError::JsonError(format!(\"failed to parse \\\"{s}\\\" as {d}\",))\n        })?;\n\n        builder.append_value(value)\n    }\n    TapeElement::Number(idx) => {\n        let s = tape.get_string(idx);\n        let value = ParseJsonNumber::parse(s.as_bytes()).ok_or_else(|| {\n            ArrowError::JsonError(format!(\"failed to parse {s} as {d}\",))\n        })?;\n\n        builder.append_value(value)\n    }\n    ...\n}\nwith Tape::get_string implemented something like\nfn get_string(&self, idx: u32) -> &str {\n    self.buffer[self.offsets[idx]..self.offsets[idx + 1]\n}\nOnce we've parsed each of our positions, we build and return the final array.\nStreaming extensions\nSo that's how the standard arrow-json decoder works. We parse the input text\ninto a flattened tape data structure, find the positions of each row, then build\nup each Arrow array in one go.\nBut for Arroyo, we found we needed a couple of additional features to support\nstreaming data ingestion, as event data is often messier than properly\nschematized and cleaned batch data.\nRaw JSON\nDecoding JSON into Arrow can provide some impedance mismatch: JSON is, by\nitself, untyped, unschematized, and pretty much an anything goes sort of data\nserialization format. Attempts to add structure to JSONâ€”like\nJSON Schemaâ€”end up with incredible complexity,\nwith features like conditionals, pattern properties, and self-modifying schemas.\nWe're never going to be able to support all of that in a relatively rigid\ntypesystem like Arrow. In other cases, the data fundamentally does not follow a\nschema, like an unstructured â€œpropertiesâ€ field.\nSo sometimes you just need an escape hatch, the ability to throw up your hands and say,\nsure, just give us whatever you got. In Arroyo, we call that raw JSONâ€”a\nspecial\nArrow extension type\nthat will accept any valid JSON value and store it as an encoded string.\nHow do we efficiently decode arbitrary JSON into a string? The tape decoding\napproach actually makes this quite simple. In each case, we are building up a\nstring. We have a few cases to handle depending on what kind of TapeElement we\nfind:\n\nLiterals (true/false/null): we write that literal to the string\nStrings: we push \", followed by the contents of the buffer corresponding to\nthe string, followed by \"\nNumbers: we push the contents of the buffer corresponding to that number\nLists: we push [, then go element by element, recursively calling our decode method,\npushing , (except for the last one), then  ]\nObjects: we first push {, go two elements at a time, first decoding the key,\npushing :, then the value pushing , (except for the last one), then }\n\nOnce we've reached the end of the element we end up with a compact\nrepresentation of the original JSON (notably missing the original whitespace,\nwhich is lost during the tape building process).\nInvalid data handling\nA reality of JSON data pipelines is that the JSON in them is not always exactly\nas it's supposed to be. Some application adds a field, emits a new type of\nevent, or drops a required field and now there's bad data in our Kafka topic. So\nArroyo, like other streaming engines, offers a\nâ€œbad dataâ€ handling setting,\nthat allows users to ignore incorrect data rather than failing the pipeline.\nFor row-based decoding this is trivial to implementâ€”you try to decode a row, and\nthat fails you just move on. As an application, you don't need any special\nsupport from the decoder. But for columnar decoding, we process an entire batch\nof records at once, and a single bad document will cause us to lose the entire\nbatch. Adding in the ability to handle fine-grained errors requires modifying\nthe decoder itself.\nWe'll consider two ways our input JSON might be invalid, corresponding to our\ntwo passes. The first is if the JSON fails to parseâ€”for example, there's a {\nwithout a corresponding }, an unterminated string, or an illegal escape\nsequence. These errors we handle when passing bytes to the TapeDecoder. For\nexample, if we start parsing an object we expect to find the next character is a\n\"â€”if it's not we'll return an error.\n{\n  \"ip\": \"131.62.139.111,\n  \"identity\": \"-\"\n}\nOnce an error has occurred, we roll back the state of the tape to where it was\nwhen we started parsing the current record. This requires storing the indices of\nour three arrays when we start each new record, and unwinding the stack.\nThe second type of error occurs when we receive valid JSON which mismatches the\nschema. How strict we are about validation is configurable (users can choose to\nallow extra fields, to coerce primitives, and mark fields as nullable), but\nthere are some errors that will always fail decoding, like incorrect and\nun-coercable types\n{\n  \"ip\": \"131.62.139.111\",\n  ...\n  \"size\": \"pretty big\"\n}\nThe columnar decoding strategy makes handling schema errors harder. In this\nexample, we don't know the record is bad until we get to the final column, at\nwhich point we've already built all of the other columns. We can't merely skip\nit at this point, because that will lead to different columns being misaligned\n(we could mark it null, assuming the column is nullable, but that has different\nand undesirable semantics).\nWe really need to know which rows are invalid before we start constructing any\nof our columns. Our solution was to add an addition method to each decoder:\nfn validate_row(&self, tape: &Tape<'_>, pos: u32) -> bool\nThis method should follow the validation logic in the actual decoder, returning\ntrue for valid rows and false for invalid ones.\nBefore decoding a batch, we then call this for our root decoder with each\nposition, determining which rows are good and which are invalid. We filter out\nany bad rows from our positions array, and proceed to decode with the good rows.\nIf desired we can also return the bad JSON (possibly for logging or reporting to\nthe user) by using the raw JSON decoder described above.\nWrapping up\nSo that is how Arroyo deserializes JSON into Arrow arrays. As discussed at the\nbeginning, this is quite fast, but there are still many possible performance\nimprovements. While writing this post, I spent some time profiling and\noptimizing the tape decoder and\nfound 30-40% of performance improvements.\nThat PR added SIMD optimizations to a couple of operations (finding string ends\nand UTF-8 validation), but most of the input processing work is still done in a\nscalar manner. There's also room to eliminate the CPU spent building the string\nbuffer by using the input data directly as in simdjson (albeit at the cost of\nextra memory usage).\nThe array-building pass is already quite efficient so long as our data is dense.\nIn cases where we have many fields that are null (possibly to express an enum)\nwe pay the cost of filling in all of those fields (and subfields, if they are\nobjects) with null values.\n\nFootnotes\n\nhttps://dl.acm.org/doi/10.14778/3236187.3236207 â†©\n\nSome may interject at the point that the real solution is to stop using\nJSON for your data pipelines. Other formats like Protobuf or Avro can be\ndeserialized much faster,\nand are schematized to prevent bad data. But you try tell the 1000 developers\nat your company that they should spend a quarter on data format migrations. â†©\n\nColumnar formats like Arrow store data as arrays of columns (for example,\nevery user_id field will be stored together) rather than rows (storing the\nuser_id, created_at, name, etc. fields for a single user together). They have\nmany advantages for analytical processing. â†©\n\nsimdjson has a lot of other tricks up its sleeves to get its outrageous\nperformance, as detailed in\nthis highly-readable VLDB paper â†©\n\nThis is significantly more complex than it sounds, because structural\ntokens may be contained within quotesâ€”in which case they're no longer\nstructuralâ€”but quotes may also be escaped. â†©\n\nIf all fields are non-nullable and we're operating in â€œstrictâ€ mode where\nadditional fields are not allowed, we could just compute the fixed offsets, like\nevery field congruent to 1 mod 8 is identity â†©\n\nArrow uses a dedicated bitmap, the â€œnullâ€ or â€œvalidityâ€ buffer, to\nindicate whether the corresponding element in the data buffer is null; this\nallows us to store, for example, primitive ints without needing to reserve a\nspecial null value. â†©\n\nIn fact, because many JSON encoders use JavaScript numeric types and lack\nsupport for full 64-bit integers, string encoding is the only reliable way to\npass large numbers around â†©",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "f0278c8dffc99c32",
    "title": {
      "en": "Through a Glass Lushly: Michalina Janoszanka's Reverse Paintings (Ca. 1920s)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://publicdomainreview.org/collection/michalina-janoszanka/",
    "score": 61,
    "by": "prismatic",
    "time": 1742418939,
    "content": "Michalina Janoszanka (1889â€“1952) is an artist better known for her role on the other side of the canvas, as the muse and mentee of famed Polish painter Jacek Malczewski (1854â€“1929). She posed for countless symbolist paintings, appearing alone, alongside Malczewski in double-portraits, among satyrs, and as Medusa. However, Janoszanka was more than a muse. She was also an artist in her own right. Trained in KrakÃ³w and Vienna, she became a strong oil painter. Her themes were traditional: portraits, still lifes, and religious scenes. But what most captured her mentorâ€™s excitement, not to mention the attention of the Young Poland modernist art movement, was something else: the surreal, kaleidoscopic landscapes she painted directly onto glass.Reverse painting, achieved by building up layers of pigment onto the backside of a pane of glass, has a long history. Having spread as far as China from its birthplace in fifteenth-century Italy, the technique was favored in Eastern Europe for devotional images and icons. These paintings could be produced quickly and cheaply, and â€œhad the added attraction of reflecting the scant light in the dark rooms of the peasant homesâ€, writes scholar Phyllis Granoff. For all their strengths, however, reverse painting was not counted among European fine art. By Janoszankaâ€™s time, the medium was firmly in the realm of folk art. What Michalina did with reverse painting was entirely distinct from the stiff religious imagery of traditional Polish glass painting. Brilliant gem tones and metallic lines frame her subjects â€” lush flowers and trees, stylized birds and frogs â€” in shadowy landscapes. She achieves textures and tones that call to mind other mediums. Dense linework becomes lace-like butterfly wings. In Winter, swirled, watercolor blues form a marbled field reminiscent of stained glass. At the birdsâ€™ breasts, Janoszanka leaves visible brush strokes, evoking individual feathers. Butterfly juxtaposes traditional flower patterns â€” simplified, geometric â€” with life-like dry leaves. The result is whimsical and stylized, but not childish. In Spring, where Janoszanka sets glowing trees against an exploding coral sky, the effect is plainly psychedelic. Across the Atlantic, reverse paintingâ€™s heyday had come and gone by the early 1900s. By layering not just paint but also crumpled metal foil over glass, American â€œtinsel paintingâ€ had stretched the limits of technique â€” but the medium, adopted mainly by middle-class women, was viewed as â€œfeminineâ€. The craft had come to be seen as hackneyed and outdated, a Victorian throwback, and the twentieth century saw many such paintings â€œcracked and broken and thrown awayâ€, writes curator Karli Wurzelbacher. But at that very moment, modernist painters began experimenting with the old technique. Artists like Marsden Hartley (1877â€“1943) and Rebecca Salsbury James (1891â€“1968) expanded on the traditional themes (flowers, still life, birds) and pushed reverse painting in novel directions: new color palettes, new approaches to line and space, new abstractions. These American experiments, beginning in the 1910s, coincided with Janoszankaâ€™s work in Poland.Viewed side by side, the artistsâ€™ respective experiments in glass painting could not have been more different. But in Maine, New Mexico, and KrakÃ³w, reverse painting presented new possibilities to a generation of modernist artists. There is no telling whether Janoszanka was aware of these other experimenters in avant-garde glass painting. It is also unclear if she faced the same gendered critique: the critic Paul Rosenfeld, argues Wurzelbacher, â€œcast Hartleyâ€™s â€˜canvases and rectangles of glassâ€™ as effeminateâ€, comparing them to â€œsome sweet bit of handiwork . . . the design of a sampler, a piece of embroidery.â€ Though her name remains overshadowed by Malczewski, Janoszankaâ€™s work presents a compelling case for reassessment. Her dreamlike glass paintings are a bridge between folk tradition and modernist experimentation. They raise unanswered questions not just about Janoszankaâ€™s career, but also about modernismâ€™s engagement with glass painting across cultural boundaries. Questions that can be answered, ultimately, only through a return to the archive.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "4258f6f0a35dcd6a",
    "title": {
      "en": "Happy 18th Birthday CUDA",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://thechipletter.substack.com/p/happy-18th-birthday-cuda",
    "score": 12,
    "by": "chmaynard",
    "time": 1742710877,
    "content": "Share this postThe Chip LetterHappy 18th Birthday CUDA!Copy linkFacebookEmailNotesMoreHappy 18th Birthday CUDA!CUDA reaches maturity and AlexNet's source code is releasedBabbageMar 23, 202519Share this postThe Chip LetterHappy 18th Birthday CUDA!Copy linkFacebookEmailNotesMore23ShareThis post is a brief â€˜warm upâ€™ to a short series of posts on interesting aspects of Nvidiaâ€™s and competitors histories and technology. Weâ€™ll next look at Nvidiaâ€™s PTX â€˜assembly languageâ€™ (actually an intermediate representation) that has been making headlines due to its use by DeepSeek. Iâ€™ll argue that PTX is part of Nvidiaâ€™s â€˜secret sauceâ€™ and weâ€™ll look at its place in Nvidiaâ€™s software ecosystem and the firmâ€™s strategy.First though, itâ€™s worth doing a recap on Nvidiaâ€™s CUDA, which is built on top of PTX. Understanding the role that CUDA plays and itâ€™s crucial importance is a useful prelude to any discussion of PTX.ShareNvidia has once again been making headlines with several important announcements at GTC 2025. Here is the companyâ€™s own summary on their blog and Jensen Huangâ€™s Keynote.Whilst the new hardware might grab many of the headlines CUDA remains central to Nvidiaâ€™s proposition.Jensen Huang talks about CUDA libraries at GTC 2025â€¦ this slide is is genuinely my favorite â€¦Jensen Huang on the CUDA-X slideThe first version of CUDA appeared in February 2007 so itâ€™s just passed its 18th birthday. In many places CUDA would be old enough to buy some champagne to celebrate its own success (although not in its home state!)CUDA may be all grown up but it hasnâ€™t forgotten its early years. That first 1.0 version of CUDA is still available for download and the documentation is still online.Despite itâ€™s importance and maturity, there is still some confusion as to what â€˜CUDAâ€™ refers to though. Nvidia tries to clear things up in a recent blog post What is CUDA?So, What Is CUDA?Some people confuse CUDA, launched in 2006, for a programming language â€” or maybe an API. With over 150 CUDA-based libraries, SDKs, and profiling and optimization tools, it represents far more than that.Wikipedia defines CUDA as:â€¦ a software layer that gives direct access to the GPU's virtual instruction set and parallel computational elements for the execution of compute kernels. In addition to drivers and runtime kernels, the CUDA platform includes compilers, libraries and developer tools to help programmers accelerate their applications.Perhaps we should see CUDA as a (powerful) â€˜brandâ€™ for Nvidiaâ€™s software ecosystem.Weâ€™ve discussed the background to Nvidiaâ€™s investment in CUDA back in 2023 (post previously paid and now free):Nvidia's Embarrassingly Parallel SuccessBabbageÂ·2023ë…„ 6ì›” 25ì¼Disclaimer: For once I have to add that this isnâ€™t investment advice. I donâ€™t hold or intend to hold Nvidia stock!Read full storyFrom that post:â€¦ Jensen Huang and Nvidia could have had no idea that either of these two major use cases [AI and Crypto] for CUDA would be so important, let alone that they would be as valuable for Nvidia as now seems likely.But then CUDA was always to enable General Purpose computing on GPUs.There are a whole class of computing problems that are embarrassingly parallel (meaning that itâ€™s obvious that they can be split into sub tasks, given to a number of processors and run in parallel). Modelling Neural Networks and mining Bitcoin are just the two most important so far.There are many important classes of problems that fall into this category. And we are discovering more all the time.Computational biology, fluid dynamics, financial modelling, cryptography are all important fields where computational work is often embarrassingly parallel.So we shouldnâ€™t say that Nvidia has built a â€˜moatâ€™ around AI. Rather that it has a moat around â€˜embarassingly parallel computingâ€™.â€¦This was always a sensible bet for Nvidia to take. Why? Because the world is very, very often embarrassingly parallel.Itâ€™s worth noting that whilst this was a â€˜sensibleâ€™ bet in 2007 it wasnâ€™t a â€˜pain-freeâ€™ one. CUDA would have come with both a financial cost and an opportunity cost for Nvidia. The firmâ€™s net income for 2007 was less than $450m. The investment in CUDA would have been material relative to that income.And crucially the firm stuck with that bet.This is all in contrast with Nvidiaâ€™s competitors. AMD was actually the first choice for some key early machine learning projects. In 2004 there was the paper â€˜GPU implementation of neural networksâ€™ by Kyoung-Su Oh and Keechul Jung. From the abstract:Graphics processing unit (GPU) is used for a faster artificial neural network. It is used to implement the matrix multiplication of a neural network to enhance the time performance of a text detection system. Preliminary results produced a 20-fold performance enhancement using an ATI RADEON 9700 PRO board. The parallelism of a GPU is fully utilized by accumulating a lot of input feature vectors and weight vectors, then converting the many inner-product operations into one matrix operation.AMD was in some financial trouble at this time though. The firm lost more than $3bn in each of fiscal 2007 and 2008. By contrast Intel made net profits of over $7bn in 2007 so has no such excuse.By 2012 Nvidia and CUDA were the first choice for machine learning research:2012 saw AlexNet, demonstrating the power of Convolutional Neural Networks (CNNs) in image recognition. AlexNet was a major step forward but wasnâ€™t the first to use GPUs for image recognitionAlexNet was not the first fast GPU-implementation of a CNN to win an image recognition contest. A CNN on GPU by K. Chellapilla et al. (2006) was 4 times faster than an equivalent implementation on CPU.But by 2012 CUDA was available and so AlexNet used CUDA. From then on GPUs became an essential tool for machine leaning and CUDA became by far the most popular way of using GPUs for machine learning.GitHub Page for the original AlexNet codeSpeaking of AlexNet, the Computer History Museum has just released the programâ€™s original source code along with a fascinating blog post.In 2020, I reached out to Alex Krizhevsky to ask about the possibility of allowing CHM to release the AlexNet source code, due to its historical significance. He connected me to Geoff Hinton, who was working at Google at the time. As Google had acquired Hinton, Sutskever, and Krizhevskyâ€™s company DNNresearch, they now owned the AlexNet IP. Hinton got the ball rolling by connecting us to the right team at Google. CHM worked with them for five years in a group effort to negotiate the release. This team also helped us identify which specific version of the AlexNet source code to release. In fact, there have been many versions of AlexNet over the years. There are also repositories of code named â€œAlexNetâ€ on GitHub, though many of these are not the original code, but recreations based on the famous paper.CHM is proud to present the source code to the 2012 version of Alex Krizhevsky, Ilya Sutskever, and Geoffery Hintonâ€™s AlexNet, which transformed the field of artificial intelligence. Access the source code on CHMâ€™s GitHub page.This original AlexNet source contains roughly 6,000 lines of CUDA code.According to Wikipedia:The model was trained for 90 epochs over a period of five to six days using two Nvidia GTX 580 GPUs (3GB each).These GPUs have a theoretical performance of 1.581 TFLOPS in float32 and were priced at US$500 upon release. Each forward pass of AlexNet required approximately 1.43 GFLOPs. Based on these values, the two GPUs together were theoretically capable of performing over 2,200 forward passes per second under ideal conditions.Two GPUs that cost $1,000!We can fast forward to 2024 and CUDA is now on version 12.8.As SemiAnalysis has described, Nvidiaâ€™s leading competitor AMD struggles to get core AI software running reliably on its hardware:AMDâ€™s software experience is riddled with bugs rendering out of the box training with AMD is impossible. We were hopeful that AMD could emerge as a strong competitor to NVIDIA in training workloads, but, as of today, this is unfortunately not the case. The CUDA moat has yet to be crossed by AMD due to AMDâ€™s weaker-than-expected software Quality Assurance (QA) culture and its challenging out of the box experience. As fast as AMD tries to fill in the CUDA moat, NVIDIA engineers are working overtime to deepen said moat with new features, libraries, and performance updates.Iâ€™d just like to end with what is an underestimated factor - at least in some discussions - in CUDAâ€™s success. We now have eighteen years evidence of Nvidiaâ€™s commitment and consistent delivery. Thatâ€™s something that no-one else can match.SubscribeFor more, two previous posts (both part paid part free) discuss CUDA in more detail:Demystifying GPU Compute ArchitecturesBabbageÂ·2024ë…„ 2ì›” 4ì¼The complexity of modern GPUs neednâ€™t obscure the essence of how these machines work. If we are to make the most of the advanced technology in these GPUs then we need a little less mystique.Read full storyDemystifying GPU Compute SoftwareBabbageÂ·2024ë…„ 3ì›” 10ì¼This post looks at the major tools in the ecosystem including CUDA, ROCm, OpenCL, SYCL, oneAPI, and others. What are they? Who supports them? What is their history? How do they relate to each other?Read full story19 Likesâˆ™3 Restacks19Share this postThe Chip LetterHappy 18th Birthday CUDA!Copy linkFacebookEmailNotesMore23SharePrevious",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "99e9a4ff0d6d0812",
    "title": {
      "en": "Landrun: Sandbox any Linux process using Landlock, no root or containers",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/Zouuup/landrun",
    "score": 479,
    "by": "Zoup",
    "time": 1742651819,
    "content": "landrun\nA lightweight, secure sandbox for running Linux processes using Landlock LSM. Think firejail, but with kernel-level security and minimal overhead.\nLinux Landlock is a kernel-native security module that lets unprivileged processes sandbox themselves - but nobody uses it because the API is ... hard!\nLandrun is designed to make it practical to sandbox any command with fine-grained filesystem and network access controls. No root. No containers. No SELinux/AppArmor configs.\nIt's lightweight, auditable, and wraps Landlock v5 features (file access + TCP restrictions).\nFeatures\n\nğŸ”’ Kernel-level security using Landlock LSM\nğŸš€ Lightweight and fast execution\nğŸ›¡ï¸ Fine-grained access control for directories\nğŸ”„ Support for read and write paths\nâš¡ Path-specific execution permissions\nğŸŒ TCP network access control (binding and connecting)\n\nDemo\n\nRequirements\n\nLinux kernel 5.13 or later with Landlock LSM enabled\nLinux kernel 6.7 or later for network restrictions (TCP bind/connect)\nGo 1.18 or later (for building from source)\n\nInstallation\nQuick Install\ngo install github.com/zouuup/landrun/cmd/landrun@latest\n\nFrom Source\ngit clone https://github.com/zouuup/landrun.git\ncd landrun\ngo build -o landrun cmd/landrun/main.go\nsudo cp landrun /usr/local/bin/\n\nUsage\nBasic syntax:\nlandrun [options] <command> [args...]\n\nOptions\n\n--ro <path>: Allow read-only access to specified path (can be specified multiple times or as comma-separated values)\n--rox <path>: Allow read-only access with execution to specified path (can be specified multiple times or as comma-separated values)\n--rw <path>: Allow read-write access to specified path (can be specified multiple times or as comma-separated values)\n--rwx <path>: Allow read-write access with execution to specified path (can be specified multiple times or as comma-separated values)\n--bind-tcp <port>: Allow binding to specified TCP port (can be specified multiple times or as comma-separated values)\n--connect-tcp <port>: Allow connecting to specified TCP port (can be specified multiple times or as comma-separated values)\n--env <var>: Environment variable to pass to the sandboxed command (format: KEY=VALUE or just KEY to pass current value)\n--best-effort: Use best effort mode, falling back to less restrictive sandbox if necessary [default: disabled]\n--log-level <level>: Set logging level (error, info, debug) [default: \"error\"]\n\nImportant Notes\n\nYou must explicitly add the directory to the command you want to run with --rox flag\nFor system commands, you typically need to include /usr/bin, /usr/lib, and other system directories\nUse --rwx for directories where you need both write access and the ability to execute files\nNetwork restrictions require Linux kernel 6.7 or later with Landlock ABI v4\nBy default, no environment variables are passed to the sandboxed command. Use --env to explicitly pass environment variables\nThe --best-effort flag allows graceful degradation on older kernels that don't support all requested restrictions\nPaths can be specified either using multiple flags or as comma-separated values (e.g., --ro /usr,/lib,/home)\n\nEnvironment Variables\n\nLANDRUN_LOG_LEVEL: Set logging level (error, info, debug)\n\nExamples\n\nRun a command with read-only access to a directory:\n\nlandrun --rox /usr/ --ro /path/to/dir ls /path/to/dir\n\nRun a command with write access to a directory:\n\nlandrun --rox /usr/bin --ro /lib --rw /path/to/dir touch /path/to/dir/newfile\n\nRun a command with execution permissions:\n\nlandrun --rox /usr/ --ro /lib,/lib64 /usr/bin/bash\n\nRun with debug logging:\n\nlandrun --log-level debug --rox /usr/ --ro /lib,/lib64,/path/to/dir ls /path/to/dir\n\nRun with network restrictions:\n\nlandrun --rox /usr/ --ro /lib,/lib64 --bind-tcp 8080 --connect-tcp 80 /usr/bin/my-server\n\nThis will allow the program to only bind to TCP port 8080 and connect to TCP port 80.\n\nRun a DNS client with appropriate permissions:\n\nlandrun --log-level debug --ro /etc,/usr --rox /usr/ --connect-tcp 443 nc kernel.org 443\n\nThis allows connections to port 443, requires access to /etc/resolv.conf for resolving DNS.\n\nRun a web server with selective network permissions:\n\nlandrun --rox /usr/bin --ro /lib,/lib64,/var/www --rwx /var/log --bind-tcp 80,443 /usr/bin/nginx\n\nRunning anything without providing parameters is... maximum security jail!\n\nlandrun ls\n\nIf you keep getting permission denied without knowing what exactly going on, best to use strace with it.\n\nlandrun --rox /usr strace -f -e trace=all ls\n\nRun with specific environment variables:\n\nlandrun --rox /usr --ro /etc --env HOME --env PATH --env CUSTOM_VAR=my_value -- env\n\nThis example passes the current HOME and PATH variables, plus a custom variable named CUSTOM_VAR.\nSecurity\nlandrun uses Linux's Landlock LSM to create a secure sandbox environment. It provides:\n\nFile system access control\nDirectory access restrictions\nExecution control\nTCP network restrictions\nProcess isolation\n\nLandlock is an access-control system that enables processes to securely restrict themselves and their future children. As a stackable Linux Security Module (LSM), it creates additional security layers on top of existing system-wide access controls, helping to mitigate security impacts from bugs or malicious behavior in applications.\nLandlock Access Control Rights\nlandrun leverages Landlock's fine-grained access control mechanisms, which include:\nFile-specific rights:\n\nExecute files (LANDLOCK_ACCESS_FS_EXECUTE)\nWrite to files (LANDLOCK_ACCESS_FS_WRITE_FILE)\nRead files (LANDLOCK_ACCESS_FS_READ_FILE)\nTruncate files (LANDLOCK_ACCESS_FS_TRUNCATE) - Available since Landlock ABI v3\n\nDirectory-specific rights:\n\nRead directory contents (LANDLOCK_ACCESS_FS_READ_DIR)\nRemove directories (LANDLOCK_ACCESS_FS_REMOVE_DIR)\nRemove files (LANDLOCK_ACCESS_FS_REMOVE_FILE)\nCreate various filesystem objects (char devices, directories, regular files, sockets, etc.)\nRefer/reparent files across directories (LANDLOCK_ACCESS_FS_REFER) - Available since Landlock ABI v2\n\nNetwork-specific rights (requires Linux 6.7+ with Landlock ABI v4):\n\nBind to specific TCP ports (LANDLOCK_ACCESS_NET_BIND_TCP)\nConnect to specific TCP ports (LANDLOCK_ACCESS_NET_CONNECT_TCP)\n\nLimitations\n\nLandlock must be supported by your kernel\nNetwork restrictions require Linux kernel 6.7 or later with Landlock ABI v4\nSome operations may require additional permissions\nFiles or directories opened before sandboxing are not subject to Landlock restrictions\n\nKernel Compatibility Table\n\nFeature\nMinimum Kernel Version\nLandlock ABI Version\n\nBasic filesystem sandboxing\n5.13\n1\n\nFile referring/reparenting control\n5.19\n2\n\nFile truncation control\n6.2\n3\n\nNetwork TCP restrictions\n6.7\n4\n\nIOCTL on special files\n6.10\n5\n\nTroubleshooting\nIf you receive \"permission denied\" or similar errors:\n\nEnsure you've added all necessary paths with --ro or --rw\nTry running with --log-level debug to see detailed permission information\nCheck that Landlock is supported and enabled on your system:\ngrep -E 'landlock|lsm=' /boot/config-$(uname -r)\n# alternatively, if there are no /boot/config-* files\nzgrep -iE 'landlock|lsm=' /proc/config.gz\n# another alternate method\ngrep -iE 'landlock|lsm=' /lib/modules/$(uname -r)/config\n\nYou should see CONFIG_SECURITY_LANDLOCK=y and lsm=landlock,... in the output\nFor network restrictions, verify your kernel version is 6.7+ with Landlock ABI v4:\nuname -r\n\nTechnical Details\nImplementation\nThis project uses the landlock-lsm/go-landlock package for sandboxing, which provides both filesystem and network restrictions. The current implementation (v0.1.3) supports:\n\nRead/write/execute restrictions for files and directories\nTCP port binding restrictions\nTCP port connection restrictions\nBest-effort mode for graceful degradation on older kernels\n\nBest-Effort Mode\nWhen using --best-effort (disabled by default), landrun will gracefully degrade to using the best available Landlock version on the current kernel. This means:\n\nOn Linux 6.7+: Full filesystem and network restrictions\nOn Linux 6.2-6.6: Filesystem restrictions including truncation, but no network restrictions\nOn Linux 5.19-6.1: Basic filesystem restrictions including file reparenting, but no truncation control or network restrictions\nOn Linux 5.13-5.18: Basic filesystem restrictions without file reparenting, truncation control, or network restrictions\nOn older Linux: No restrictions (sandbox disabled)\n\nFuture Features\nBased on the Linux Landlock API capabilities, we plan to add:\n\nğŸ”’ Enhanced filesystem controls with more fine-grained permissions\nğŸŒ Support for UDP and other network protocol restrictions (when supported by Linux kernel)\nğŸ”„ Process scoping and resource controls\nğŸ›¡ï¸ Additional security features as they become available in the Landlock API\n\nLicense\nThis project is licensed under the GNU General Public License v2\nContributing\nContributions are welcome! Please feel free to submit a Pull Request.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "57ba15739b7f3a4e",
    "title": {
      "en": "NYPD sergeant is suspended over 'spying for the Chinese'",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.dailymail.co.uk/news/article-14528793/chinese-spy-nypd-zhu-jiang-suspended.html",
    "score": 5,
    "by": "suraci",
    "time": 1742814201,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "414579601aa1b820",
    "title": {
      "en": "Show HN: Search and chat with millions of court cases using AI.",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.courtsearch.ai/",
    "score": 8,
    "by": "ashr_",
    "time": 1742757680,
    "content": "Frequently Asked Questions\n\nHow does Courtsearch work?Simply search for a court case and Courtsearch will present the court records and legal opinions related to the case. You can then chat with the documents to get answers to your legal questions.What types of court records can I search?You can search for federal and state court records, including district courts, appellate courts, and supreme courts.Can I chat with multiple court records at once?Yes, you can chat with multiple court records simultaneously. Toggle the 'Selected' field to add and remove documents from the chat.What file formats can I export the data to?You can export the data to CSV, Excel, JSON and more. Ask Courtsearch to export the data to the format you need.How can I verify the accuracy of the responses?You can verify the accuracy of the responses by cross-referencing the data with the original court record. Courtsearch provides a link to the original document in each response.\n\nHow does Courtsearch work?Simply search for a court case and Courtsearch will present the court records and legal opinions related to the case. You can then chat with the documents to get answers to your legal questions.\n\nWhat types of court records can I search?You can search for federal and state court records, including district courts, appellate courts, and supreme courts.\n\nCan I chat with multiple court records at once?Yes, you can chat with multiple court records simultaneously. Toggle the 'Selected' field to add and remove documents from the chat.\n\nWhat file formats can I export the data to?You can export the data to CSV, Excel, JSON and more. Ask Courtsearch to export the data to the format you need.\n\nHow can I verify the accuracy of the responses?You can verify the accuracy of the responses by cross-referencing the data with the original court record. Courtsearch provides a link to the original document in each response.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "713dddb4ea35db01",
    "title": {
      "en": "A single-fibre computer enables textile networks and distributed inference",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.nature.com/articles/s41586-024-08568-6",
    "score": 55,
    "by": "walterbell",
    "time": 1742384341,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "29d209b338460f29",
    "title": {
      "en": "Paul A. M. Dirac, Interview by Friedrich Hund (1982) [video]",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.youtube.com/watch?v=xJzrU38pGWc",
    "score": 116,
    "by": "mdp2021",
    "time": 1742657448,
    "content": "Back\n\n  KR\n\n    Skip navigation\n\n        Search\n\n  Search with your voice\n\nSign in\n\n  KR\n\n{\"@context\":\"https://schema.org\",\"@type\":\"VideoObject\",\"description\":\"Interview with Paul Adrien Maurice Dirac (1902â€“1984), Nobel Prize in Physics 1933, \\\"for the discovery of new productive forms of atomic theory\\\". Topics discussed: \\nSymmetry as central concept in theoretical physics. \\nSpace and time according to Lorentz. \\nMatter and anti-matter. \\nDirac's definition of symmetry. \\nFermions, Negative energy levels. \\nEinstein's general theory of relativity. \\nNatural times and lengths. \\nCosmology and gravitation. \\nRelativistic quantum mechanics. \\nAtomic constants.\",\"duration\":\"PT1234S\",\"embedUrl\":\"https://www.youtube.com/embed/xJzrU38pGWc\",\"interactionCount\":\"325998\",\"name\":\"Paul A. M. Dirac, Interview by Friedrich Hund (1982)\",\"thumbnailUrl\":[\"https://i.ytimg.com/vi/xJzrU38pGWc/hqdefault.jpg\"],\"uploadDate\":\"2023-02-17T12:49:35-08:00\",\"@id\":\"https://www.youtube.com/watch?v=xJzrU38pGWc\",\"genre\":\"Science & Technology\",\"author\":\"mehranshargh\"}\n\nPaul A. M. Dirac, Interview by Friedrich Hund (1982)SearchWatch laterShareCopy linkInfoShoppingTap to unmute2xIf playback doesn't begin shortly, try restarting your device.â€¢Up nextLiveUpcomingCancelPlay NowYou're signed outVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.CancelConfirmShareInclude playlistAn error occurred while retrieving sharing information. Please try again later.0:000:00 / 20:33â€¢Watch full videoLiveâ€¢â€¢Scroll for details\n\n      Paul A. M. Dirac, Interview by Friedrich Hund (1982)\n\n    mehranshargh\n\n    mehranshargh\n\n42.5K subscribers\n      SubscribeSubscribed\n\n        6.6KShareDownload\n  Download\n\nSave\n\n  325K views  2 years ago\n\n  325,998 views â€¢ Feb 17, 2023\n\n          Show less\n\n        Interview with Paul Adrien Maurice Dirac (1902â€“1984), Nobel Prize in Physics 1933, \"for the discovery of new productive forms of atomic theory\". Topics discussed:\nSymmetry as central concept in theoretical physics. â€¦...more\n...more\n\nPeople mentioned1 personAlbert EinsteinGerman-born physicist (1879â€“1955)\n\n  Transcript\n\n  Follow along using the transcript.\n\n  Show transcript\n\n    mehranshargh\n\n      42.5K subscribers\n\n  Videos\n  About\n\n    VideosAbout\n\n          Show less\n\n            Paul A. M. Dirac, Interview by Friedrich Hund (1982)325,998 views325K viewsFeb 17, 20236.6KShareDownload\n  Download\n\nSave\n\n      Comments\n      444\n\n              Top comments\n\n              Newest first\n\n      In this video\n\nTranscript\n\n      Description\n\n  Paul A. M. Dirac, Interview by Friedrich Hund (1982)\n\n6.6KLikes325,998Views2023Feb 17\nInterview with Paul Adrien Maurice Dirac (1902â€“1984), Nobel Prize in Physics 1933, \"for the discovery of new productive forms of atomic theory\". Topics discussed:\nSymmetry as central concept in theoretical physics.\nSpace and time according to Lorentz.\nMatter and anti-matter.\nDirac's definition of symmetry.\nFermions, Negative energy levels.\nEinstein's general theory of relativity.\nNatural times and lengths.\nCosmology and gravitation.\nRelativistic quantum mechanics.\nAtomic constants.â€¦...more\n...more\nShow less\n\nPeople mentioned1 personAlbert EinsteinGerman-born physicist (1879â€“1955)\n\n  Transcript\n\n  Follow along using the transcript.\n\n  Show transcript\n\n    mehranshargh\n\n      42.5K subscribers\n\n  Videos\n  About\n\n    VideosAbout\n\n      Transcript\n\nNaN / NaN\n\n14:58The Scientist Who Discovered the World's Most Beautiful EquationNewsthink961K views â€¢ 11 months ago19:34Roger Penrose on quantum mechanics and consciousness | Full interviewThe Institute of Art and Ideas818K views â€¢ 1 year ago16:34Deriving the Dirac EquationRichard Behiel128K views â€¢ 1 year ago21:44Feynman's Lost Lecture (ft. 3Blue1Brown)minutephysics3.5M views â€¢ 6 years ago21:08Great Physicists: Paul A.M. Dirac - The Taciturn GeniusUnzicker's Real Physics100K views â€¢ 4 years ago26:29How Feynman did quantum mechanics (and you should too)Physics with Elliot559K views â€¢ 1 year ago27:15The Most Misunderstood Concept in PhysicsVeritasium20M views â€¢ 1 year ago13:22Bell's Inequality: The weirdest theorem in the world | Nobel Prize 2022Qiskit2.3M views â€¢ 2 years ago59:43Dirac's belt trick, Topology,  and Spin Â½ particlesNoahExplainsPhysics457K views â€¢ 3 years ago15:01L4.4 Dirac equation for the electron and hydrogen HamiltonianMIT OpenCourseWare213K views â€¢ 6 years ago1:00:18Quantum Fields: The Real Building Blocks of the Universe - with David TongThe Royal Institution6.5M views â€¢ 8 years ago22:44Field Theory Fundamentals in 20 Minutes!Physics with Elliot706K views â€¢ 3 years ago1:06:50The complete FUN TO IMAGINE with Richard FeynmanChristopher Sykes4.7M views â€¢ 6 years ago13:51Louis De Broglie, interviewed by Pierre Grivet (1967)mehranshargh69K views â€¢ 2 years ago29:06Discovering the relativistic Dirac equation with Paul Dirac and grapheneProfessor Nano50K views â€¢ 1 year ago6:15Paul Steinhardt: My Mentor Richard FeynmanDr Brian Keating24K views â€¢ 10 months ago1:09:42The Mystery of SpinorsRichard Behiel1.2M views â€¢ 1 year ago46:00Dirac Conversation: Edward WittenInt'l Centre for Theoretical Physics223K views â€¢ 11 months ago15:53Quantum Field Theory visualizedScienceClic English2.2M views â€¢ 4 years ago4:00Legendary Physicist and Florida State Professor Paul DiracFloridaState210K views â€¢ 15 years ago\n\nShow more",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "2faa9e49893d33b8",
    "title": {
      "en": "Understanding R1-Zero-Like Training: A Critical Perspective",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/sail-sg/understand-r1-zero",
    "score": 152,
    "by": "pama",
    "time": 1742654112,
    "content": "Understanding R1-Zero-Like Training: A Critical Perspective\nZichen Liu*â€ , Changyu Chen*, Wenjun Li*, Penghui Qi*\nTianyu Pang, Chao Du, Wee Sun Lee, Min Lin\n*Core Contributors, â€ Project Lead\n\n    ğŸ‰ Updates â€¢\n    ğŸ”— Links â€¢\n    ğŸ“– TL;DR\n\n    ğŸ’» Usage  â€¢\n    ğŸŠ Citation â€¢\n    ğŸŒ» Acknowledgement\n\nUpdates\n\n21/03/2025: ğŸ‰ We release our paper, models and codebase. Our R1-Zero training is implemented with ğŸŒ¾ Oat, a highly modular, research-friendly and efficient LLM RL framework.\n\nLinks\n\nUnderstanding R1-Zero-Like Training\n\nğŸ“„ Paper\nğŸ¤— Models\n\nThere May Not Be Aha Moment in R1-Zero-like Training â€” A Pilot Study\n\nğŸ“„ Blog\nğŸ’» Code\n\nOAT: A research-friendly framework for LLM online alignment\n\nğŸ’» Codebase\n\nTL;DR\nTo understand R1-Zero-like training, we critically examine two core components: base models\nand reinforcement learning. We highlight our findings below.\nOn base models:\n\nDeepSeek-V3-Base already exhibit \"Aha moment\".\n\nAs the popular choice for R1-Zero-like training, Qwen2.5 base models demonstrate strong reasoning capabilities\neven without prompt templates: the average benchmark scores improve by ~60% (compared to the traditional 4-shot prompting)!\n\nOn reinforcement learning:\n\nGRPO leads to biased optimization! We propose a simple fix that improves token efficiency\nwhile maintaining reasoning performance, termed as Dr. GRPO (GRPO Done Right).\n\nIn R1-Zero-like training, the template and the question set perform a duet to affect the RL dynamics\n\n(Left Plot) For Qwen2.5-Math-1.5B, a mismatched template (e.g., R1 template) in fact destructs the reasoning capabilities before RL reconstructing it. This makes the improvement impressive on the surface.\n(Middle Plot) However, if a template does not deviate from the pretraining distribution too far, even a small and completely o.o.d. question set (e.g., GSM8K) could induce the reasoning ability equally well, by reinforcing correct reasoning behaviors instead of infusing new knowledge.\n\nBeyond Qwen, Llama can also be RL-tuned from base models. In this case, domain-specific pretraining will improves RL ceiling.\n\n(Right Plot) GRPO can even make Llama with math knowledge \"Aha\" by increasing the output length; however, it is likely due to its length bias, which can be removed by Dr. GRPO.\n\nOur minimalist R1-Zero recipe:\nOur analysis suggests a minimalist recipe for R1-Zero-like training:\nWe RL-tune Qwen2.5-\nMath-7B using the (unbiased) Dr. GRPO algorithm on MATH level 3-5 questions with the Qwen-Math template, and achieve state-of-the-art performance with only 27 hours compute on 8Ã— A100 GPUs.\n\nIf you are interested in more details, please check out our paper!\nUsage\nInstall\nWe recommend a clean python==3.10 environment for development.\n# Install vllm & oat, the LLM RL framework we developed r1-zero training on.\npip install vllm==0.7.2 && pip install oat-llm==0.0.9\n\n# Install this package locally to use the math grader.\ngit clone git@github.com:sail-sg/understand-r1-zero.git && cd understand-r1-zero\npip install -e .\n\nTraining\nWe implement R1-Zero training by extending Oat's Learner and Actor components. Please see train_zero_math.py for a step-by-step guide.\n# Patch LD_LIBRARY_PATH to avoid dependency errors:\nexport LD_LIBRARY_PATH=$(python -c \"import sysconfig; print(sysconfig.get_config_var('LIBDIR'))\"):$LD_LIBRARY_PATH\n\n# Run the experiment (tested on 8 x A100-40G) with Dr. GRPO:\n# (change to `--critic_type grpo` for running GRPO)\npython train_zero_math.py \\\n    --critic_type drgrpo \\\n    --gpus 8 \\\n    --enable_prefix_caching \\\n    --collocate \\\n    --vllm_sleep \\\n    --vllm_gpu_ratio 0.35 \\\n    --gradient-checkpointing \\\n    --flash-attn \\\n    --bf16 \\\n    --rnd-seed \\\n    --learning_rate 0.000001 \\\n    --lr_scheduler constant \\\n    --num_ppo_epochs 1 \\\n    --beta 0 \\\n    --oracle_type reward \\\n    --oracle math \\\n    --pretrain Qwen/Qwen2.5-Math-1.5B \\\n    --prompt_template r1 \\\n    --zero-stage 2 \\\n    --ref_offload \\\n    --prompt_data ./datasets/train/math_12k \\\n    --train_split train \\\n    --input_key problem \\\n    --output_key answer \\\n    --max-train 9999999 \\\n    --num_prompt_epoch 20 \\\n    --prompt_max_length 1024 \\\n    --num_samples 8 \\\n    --temperature 1 \\\n    --top_p 1 \\\n    --generate_max_length 3000 \\\n    --save_steps -1 \\\n    --train_batch_size 128 \\\n    --train_batch_size_per_device 1 \\\n    --mini_train_batch_size_per_device 1 \\\n    --rollout_batch_size 128 \\\n    --rollout_batch_size_per_device 16 \\\n    --pi_buffer_maxlen_per_device 128 \\\n    --eval_batch_size 200 \\\n    --eval_steps 16 \\\n    --eval_temperature 0 \\\n    --eval_generate_max_length 3000 \\\n    --eval_data ./datasets/evaluation_suite \\\n    --eval_input_key input \\\n    --use-wb \\\n    --wb-run-name qwen2.5-Math-1.5b-r1-zero \\\n    --wb_project oat-zero\n\nPlease see here for more example scripts.\nEvaluation\n# Evaluate our models:\npython evaluate_model.py --model_name sail/Qwen2.5-Math-7B-Oat-Zero\npython evaluate_model.py --model_name sail/Qwen2.5-Math-1.5B-Oat-Zero\npython evaluate_model.py --model_name sail/Llama-3.2-3B-Oat-Zero --template r1\n\n# Evaluate baseline models:\npython evaluate_model.py --model_name Qwen/Qwen2.5-Math-1.5B\npython evaluate_model.py --model_name Qwen/Qwen2.5-Math-7B\npython evaluate_model.py --model_name hkust-nlp/Qwen-2.5-Math-7B-SimpleRL-Zero\npython evaluate_model.py --model_name PRIME-RL/Eurus-2-7B-PRIME-Zero\npython evaluate_model.py --model_name Open-Reasoner-Zero/Open-Reasoner-Zero-7B\n\nCitation\nIf you find our work useful for your research, please consider citing:\n@misc{liu2025understanding,\n  title={Understanding R1-Zero-Like Training: A Critical Perspective},\n  author={Zichen Liu and Changyu Chen and Wenjun Li and Penghui Qi and Tianyu Pang and Chao Du and Wee Sun Lee and Min Lin},\n  year={2025},\n  howpublished={\\url{https://github.com/sail-sg/understand-r1-zero}},\n}\n\nAcknowledgement\n\nThis work is supported by Sea AI Lab for computing resources.\nThe training codes are built on Oat, which employs vLLM, DeepSpeed and launchpad.\nThe base models are from Qwen2.5-Math, Llama, and DeepSeek.\nWe thank Qingfeng Lan for his time in thoroughly reviewing our code.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "2a9c1e6024029864",
    "title": {
      "en": "Bra and KET: String Interpolation in AmigaDOS",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.datagubbe.se/braket/",
    "score": 76,
    "by": "ingve",
    "time": 1742663122,
    "content": "BRA and KET: String Interpolation in AmigaDOS\n\nExploring the unnecessary, part 3735928559\nSpring 2025\n\nA Short Word on AmigaDOS Scripts\n\nWithout getting caught up in semantics, AmigaDOS in this text refers to the command line portion of the Amiga computers' operating system. AmigaDOS is based on TRIPOS, but was expanded and added to by both the original Amiga team and then Commodore. One thing that remains basically the same is the parser for shell scripts, or sequence files as they're called in TRIPOS. The script parser is reasonably competent for a 1980:s home computer OS and can be used for solving real world made up programming problems.\n\nThe parser accepts a few special directives, such as .KEY, which is used to describe the template for arguments passed to the script file. For example, .KEY FILENAME/A tells the parser that the script accepts a string argument called FILENAME. These directives are placed at the start of the script file.\n\nBy default, the parser does string interpolation using the characters < (less than) and > (greater than). The cleverness of this is debatable: these characters are also used for I/O redirection, which can quickly make things confusing. Luckily, the parser also accepts the directives .BRA and .KET, each followed by a single character, which lets the programmer override the default interpolation characters. The terms BRA and KET are most likely borrowed from Dirac notation.\n\nA Simple Example\n\nUsually, AmigaDOS scripts using interpolation replace the default characters with curly braces, { and }, respectively. This trend may have originated at Commodore. Consider the following script:\n\n.BRA {\n.KET }\n.KEY var\nEcho Hello {var}!\n\nHere, < and > have been substituted with { and }. The script takes a single argument, var, and the string interpolation then replaces {var} with the supplied argument. If saved as the file myscript and executed, the following AmigaShell session can be achieved:\n\nAmigaShell> myscript datagubbe\nHello datagubbe!\nAmigaShell> _\n\nInterpolate This!\n\nThis of course begs the question: What characters can be used for string interpolation in shell scripts? Thanks to the tireless efforts of talented researchers at Datagubbe Labs, an answer can be presented - and programmers worldwide can finally get a good night's sleep. Without further ado, let's get down to brass tacks!\n\nTest Tooling\n\nThere are text editors on the Amiga that'll happily insert any ASCII character into a text file, but why use an existing solution when you can build your own? An ARexx program - makescript - was quickly hacked together meticulously constructed for the purpose. This program generates an AmigaDOS script file with arbitrary characters for .BRA and .KET, supplied either as plain text or ASCII char codes. The source code for makescript is available here.\n\nBread and Butter\n\nThe defaults and the standard curly substitutes both work very well, as expected. Datagubbe Labs has taken the liberty of preparing screenshots, to further emphasize the validity and rigor of this important research. The scripts can also be downloaded, for transparency and reproducibility. Witness:\n\n  Default (no .BRA/.KET directives)\n  Standard (curly braces)\n\nOdds and Alphas\n\nWhat about non-matching character pairs, such as [ and }? That works, too, which was expected. As does standard letters, such as A and B.\n\n  Odd pair ([ and })\n  Letters (A and B)\n\nBells and Whistles\n\nFinally, non-printable characters were introduced. First, shift out (SO, decimal char code 14) and shift in (SI, decimal char code 15) were tested. These worked, too. Then, upping the ante, our relentless researchers decided to try the ASCII BEL character (decimal char code 7). AmigaDOS handles the BEL character by flashing the screen when it's encountered in a text. How would this fare in script execution? For closing the interpolation, negative acknowledgement (NAK, decimal char code 21), was chosen. This too, proved to work flawlessly - and no screen flashing was encountered when executing the script.\n\n  Shift In and Shift Out\n  BEL and NAK\n\nAnother Job Well Done\n\nNaturally, when printing the script file contents to the shell window using the AmigaDOS command Type, these characters cause a bit of commotion. BEL, for example, results in a screen flash as expected. In order to verify that the characters are indeed present in the script files, feel free to download them above - or study this screenshot of MicroEMACS, displaying the non-printable characters as control sequences.\n\nIn conclusion, AmigaDOS string interpolation offers both resilience and a wide variety of choice - a cause for celebration as good as any. Thanks for your attention, and happy hacking!",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "8dd6872cca0d0d91",
    "title": {
      "en": "Metabolism Can Shape Cells' Destinies",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.quantamagazine.org/how-metabolism-can-shape-cells-destinies-20250321/",
    "score": 50,
    "by": "isaacfrond",
    "time": 1742570015,
    "content": "Quanta Homepage\n\n                                        Physics\n\n                                        Mathematics\n\n                                        Biology\n\n                                        Computer Science\n\n                                        Topics\n\n                                        Archive\n\n                                        Blog\n\n                                        Columns\n\n                                        Interviews\n\n                                        Podcasts\n\n                                        Puzzles\n\n                                        Multimedia\n\n                                        Videos\n\n                                        About Quanta\n\n                                    An editorially independent publication supported by the Simons Foundation.\n\n                                    Follow Quanta\n\n    Facebook\n\n        Youtube\n\n        Instagram\n\n    RSS\n\n                Newsletter\n\n                    Get the latest news delivered to your inbox.\n\n                            Email\n\n                        Subscribe\n\n                        Recent newsletters\n\n                                    Gift Store\n\n                                        Shop Quanta gear\n\nNewsletter\n\n                    Get the latest news delivered to your inbox.\n\n                            Email\n\n                        Subscribe\n\n                        Recent newsletters\n\nQuanta Homepage\n\n                                        Physics\n\n                                        Mathematics\n\n                                        Biology\n\n                                        Computer Science\n\n                                        Topics\n\n                                        Archive\n\n        Saved articles\n\n                    Saved Articles\n                                            Create a reading list by clicking the Read Later icon next to the articles you wish to save.\n\n                            See all saved articles\n\n        Login\n\n                    Log out\n\n                    Change password\n\n                                Search\n\nHome\n\n                How Metabolism Can Shape Cellsâ€™ Destinies\n\n        Comment\n\n        Save Article\n\n                    Read Later\n\n                                                Share\n\n    Facebook\n\n                            Copied!\n\n    Copy link\n         (opens a new tab)\n\n    Email\n\n    Pocket\n\n    Reddit\n\n    Ycombinator\n\ndevelopmental biology\n    How Metabolism Can Shape Cellsâ€™ Destinies\n\n        By\n\n                Viviane Callier\n\nMarch 21, 2025\n\n            A growing body of work suggests that cell metabolism â€” the chemical reactions that provide energy and building materials â€” plays a vital, overlooked role in the first steps of life.\n\n        Comment\n\n        Save Article\n\n                    Read Later\n\ndevelopmental biology\n    How Metabolism Can Shape Cellsâ€™ Destinies\n\n        By\n\n                Viviane Callier\n\nMarch 21, 2025\n\n            A growing body of work suggests that cell metabolism â€” the chemical reactions that provide energy and building materials â€” plays a vital, overlooked role in the first steps of life.\n\n        Comment\n\n        Save Article\n\n                    Read Later\n\nThe embryo of a fruit fly grows and then forms layers of specialized cells. New research shows how metabolism influences these early developmental steps.\n\n                            Bruno C. Vellutini\n\nEach of us starts life as a single cell. To develop into a complex, multicellular being, that cell must divide, and then those cells must divide again, and again â€” and then these stem cells start to specialize into different types, with different destinies in our bodies. In the first week, our cells reach their first turning point: They must become either placenta or embryo. Then, in the developing embryo, cells form three primary layers â€” ectoderm, mesoderm and endoderm â€” which, over time, become skin, neurons, heart, gut, and so on.\nThese determinations of cellsâ€™ fates â€” what type of specialized cell they will become â€” occur in stages throughout embryonic development. Because each cell type has a characteristic pattern of gene activity, scientists assumed that the decisions cells make are dictated by genes: specifically, networks of genes that turn each other on and off, initiating a cascade that forms the correct types of cells in the correct order.\nBut genes are not the whole story. New research has shown the extent to which cell metabolism â€” the chemical reactions within a cell that provide energy and materials for growth â€” has an important, underappreciated role in directing cell fates.\nâ€œMetabolism is more than just housekeeping in stem cells, especially embryonic stem cells,â€ said Jan Å»ylicz (opens a new tab), a developmental biologist at the University of Copenhagen. â€œItâ€™s a crucial pathway that regulates decision-making processes.â€\nIn the course of their whirring biochemical activity, cells not only produce energy but also synthesize metabolites: molecular biological building blocks, such as amino acids, nucleotides, carbohydrates and lipids. In the last decade or two, with the development of better methods for measuring metabolites in cells, there has been a surge of interest in the various ways these small molecules regulate gene activity, and in particular cell fate and development. Now, studies suggest that their presence or absence â€” which can be influenced by external factors, such as environment and diet â€” can determine the fate of a cell and, in turn, the development of an embryo.\n\nThe developmental biologist Jan Å»ylicz has seen how a single type of metabolite can change the fate of a cell during the earliest stages of human development.\n\n    Courtesy of Jan Zylicz/UCPH, Denmark\n\nâ€œBeyond the bioenergetics, these by-products of metabolism are used for regulating specialized programs as well,â€ such as cell differentiation and the formation of an embryoâ€™s three layers, said Berna Sozen (opens a new tab), a developmental biologist at Yale University who recently published research in Nature showing how glucose metabolism (opens a new tab) influences the earliest stages of embryonic development. â€œThe possibilities are so exciting. It really changes the way we think about developmental biology, the way we think about how our own life starts.â€\nScientists have traditionally believed that all the instructions a cell needs to become a particular type are encoded in its DNA. In that case, when a stem cell differentiates, part of that execution involves turning on the genes that encode that cell typeâ€™s metabolism, said Jared Rutter (opens a new tab), a biochemist at the University of Utah. But studies now show that the operation can run backward: The cell tests whether it has the materials in its environment. If it cannot execute the metabolism, then it wonâ€™t become that cell type, in spite of signals to differentiate. â€œItâ€™s a revolution in my thinking of how metabolism influences things,â€ Rutter said.\nThe body of work overturns assumptions about the pure dominance of genes during development and helps us understand the factors that contribute to an embryoâ€™s survival, cell death and even cancer.\nâ€œAlmost any question is on the table,â€ said Lydia Finley (opens a new tab), a cancer biologist at Memorial Sloan Kettering Cancer Center in New York. â€œThe field of metabolism and development is really developing now, which is super exciting, because itâ€™s early, early days.â€\n\nLydia Finley of Memorial Sloan Kettering Cancer Center discovered that the anti-tumor protein p53 helps prevent cancer by managing a cellâ€™s metabolic state.\n\n    Memorial Sloan Kettering Cancer Center\n\nEarly Signals\nOne of the most striking examples of how metabolism can drive cell differentiation comes from a humble slime mold. When Dictyostelium has plenty of nutrients in its environment, it happily grows and divides as a group of single cells. But when food dries up, a change occurs: Individual cells aggregate and form a sort of multicellular slug, which crawls as a single unit and forms fruiting bodies to reproduce. While food availability is the obvious trigger for the change, until recently no one knew how exactly it flips the switch from unicellularity to multicellularity â€” a form of cell fate â€” at the molecular level.\nFour years ago, the immunologist Erika Pearce (opens a new tab) and her team studying cell metabolism at Johns Hopkins University discovered how this switch is metabolically driven (opens a new tab). Under starvation conditions, Dictyostelium mitochondria generate a burst of reactive oxygen species â€” small, unstable molecules that can damage proteins and DNA, and can also act as signaling molecules. To protect itself from its own mitochondria, the cell produces an antioxidant called glutathione.\nGlutathione doesnâ€™t come out of nowhere: It requires the nutrient sulfur. A starving slime mold cell shunts all of its sulfur into glutathione production. That means thereâ€™s no sulfur left to build iron-sulfur complexes, without which the cell canâ€™t make new mitochondria. Therefore, the slime mold â€œhas no choice but to become multicellular,â€ Pearce said. It canâ€™t grow and spread on its own anymore, so it forms a slug and heads off in search of food.\n\n                    When food is scarce, the Dictyostelium slime mold abandons a unicellular lifestyle and switches into a sluglike multicellular form with fruiting bodies (pictured here). At a cellular level, this change is driven by the lack of a single nutrient: sulfur.\n\n    Eye of Science/ Science Source\n\nâ€œMetabolism was driving that entire phenotype, and whether or not you had food there â€” that is probably still the most fundamental driving force,â€ Pearce said. â€œEvery single one of our cells is probably subjected to that as well.â€\nThis discovery showed that a cellâ€™s metabolic state can trigger a signaling cascade that completely changes an organismâ€™s form and behavior. However, understanding how cell metabolism is translated into a developmental signal in organisms more complex than slime molds had taken decades of work.\nBack in the 1990s, the biologist Navdeep Chandel (opens a new tab) was a graduate student working on a mitochondrial enzyme called cytochrome c oxidase. â€œI was a pretty confident young lad thinking I know what cytochrome c oxidase does: It takes an electron from cytochrome c and gives it to oxygen,â€ he said â€” a key part of the mitochondriaâ€™s process for creating cellular energy in the form of adenosine triphosphate (ATP). But surprisingly, in 1996, researchers discovered that if cytochrome c is released from the mitochondria, it sets off a cascade of signals (opens a new tab) that triggers cell death â€” also a kind of cell fate decision.\nâ€œSo it [cytochrome c] has a second function, a moonlighting function, you could call it,â€ Chandel said. That was the first hint that mitochondria were doing more than just providing ATP: They were also influencing cell decision-making. Chandel, now a mitochondrial biologist at Northwestern University Feinberg School of Medicine, has been working on elucidating mitochondrial signaling ever since.\nWorking with human stem cells more than a decade ago, he discovered that mutating a critical mitochondrial enzyme prevented the cells from differentiating (opens a new tab) into fat cells as they should have. In 2013, his lab showed that reactive oxygen species generated by mitochondria were essential signals (opens a new tab) in mouse skin development. Then, in 2023, in an experiment published in Nature, he and his team again found that cell specialization could not happen without healthy, functioning mitochondria. In a mouse model, stem cells with defective mitochondria unleashed a stress response â€” a cascade of molecular signals that activated stress-response genes in the nucleus â€” and then the cells stalled out, unable to become lung cells (opens a new tab). The miceâ€™s lungs failed to develop, and they died.\n\nNavdeep Chandel, a biologist at Northwestern University, has spent his career elucidating how mitochondrial signals affect cell specialization and animal development.\n\n    Northwestern Medicine\n\nThe stress response, Chandel concluded, was an emergency message to the nucleus to stop development when the mitochondria encountered a metabolic problem.\nâ€œWhen we started these experiments, most people would say, â€˜Oh my God, what a dumb experiment, youâ€™re going to get dead cells,â€™â€ he said. â€œBut hang on. We havenâ€™t seen that. Weâ€™ve seen specific defects â€” the defects being [cells] not differentiated. I think thatâ€™s quite cool.â€\nOver the past few years, other research projects have independently tied mitochondriaâ€™s emergency stress response to cellsâ€™ failure to differentiate. For example, in fruit flies, defects in metabolic enzymes in a subset of tissues can trigger a stress response that halts the growth and development (opens a new tab) of the entire animal. By genetically blocking the stress response, the researchers reversed the effect.\nMost recently, in February 2025 in Science, the endocrinologist Scott Soleimanpour (opens a new tab) at the University of Michigan found that in mice with defective mitochondria, beta cells (special cells that produce insulin) were de-differentiating â€” losing their identity as beta cells (opens a new tab) and reverting to a more immature state. By inhibiting the stress response, his team could get the beta cells to re-differentiate, much as Chandel could restore the lung cells in his mice if he suppressed their stress response.\n\n                    What Makes Life Tick? Mitochondria May Keep Time for Cells\n\n                developmental biology\n\n                    What Makes Life Tick? Mitochondria May Keep Time for Cells\n\n            September 18, 2023\n\n        Comment\n\n        Save Article\n\n                    Read Later\n\nResearchers already knew that mitochondria under stress can send signals to other parts of the cell. These studies help clarify the message. â€œThe animal knows thereâ€™s something wrong at a metabolic level, and it releases signals to slow down development,â€ said the geneticist Jason Tennessen (opens a new tab) of Indiana University, who led the fruit fly studies.\nThe research has flipped how Tennessen thinks about the relationship between genetics and metabolism. â€œInstead of thinking about the gene expression networks just happening to interact with metabolism, itâ€™s really metabolism driving [developmental decision-making],â€ he said, â€œand gene expression networks are the tools by which that occurs.â€\nThis idea â€” that cell metabolism is an integral but unheralded part of the developmental process â€” isnâ€™t fantastical. In another field of biology, epigenetics, researchers have already detailed the process by which metabolites turn genes on and off. But they needed the work of developmental biologists to connect more of the dots.\nThe Metabolic Nucleus\nNearly all the different cell types in your body â€” liver cells, heart cells, skin cells, beta cells, and so on â€” contain the same genome in their nuclei. What differentiates them is how the gene activity is regulated. In each cell type, a different set of genes is expressed to make the proteins and RNA that allow them to function properly in their respective roles in a mature body.\nEpigeneticists who study this process have, over the past few decades, elucidated a complex system by which proteins and enzymes activate or repress certain genes. The meters-long strand of DNA in every cell is wound around proteins called histones. With the help of specific enzymes, molecules that scientists call â€œchemical modificationsâ€ or â€œepigenetic marksâ€ attach to the histones and cause the DNA to unspool, exposing different genes for activation. These modifications can thereby activate some genes and deactivate others, influencing the biochemical processes in a cell and therefore the functions that cell performs.\n\nMark Belan/Quanta Magazine\n\nâ€œThose chemical modifications that decorate [histones] and modify gene expression â€” theyâ€™re metabolites, full stop,â€ said Finley, the cancer biologist. â€œChemical modifications themselves are metabolites, and their removal is dependent on metabolites.â€\nFifteen years ago, when Kathryn Wellen (opens a new tab) was a postdoc studying cancer cells, she discovered that the epigenetic marks on histones change in response to the presence of nutrients. When food is plentiful, mitochondria make a metabolite called acetyl-CoA. It diffuses into the nucleus, where the genome resides, through large pores. There, enzymes break down the metabolites into epigenetic marks known as acetyl groups and place them on histones to activate one set of genes. However, when the cells are starving, enzymes strip off the acetyl groups. Some of those acetyl groups are turned back into acetyl-CoA and consumed for energy, while others are recycled to activate a different set of genes.\nClearly thereâ€™s a lot of metabolic activity occurring in the nucleus. Wellen wondered whether the nucleus had its own unique metabolism and could therefore be considered a â€œmetabolic compartment.â€ Working with Nate Snyder (opens a new tab), a biochemist at the Lewis Katz School of Medicine at Temple University, Wellen and other researchers developed new methods to measure metabolites in different parts of the cell and saw that metabolic activity in the nucleus (opens a new tab) is not identical to activity occurring elsewhere.\n\n                    Kathryn Wellen, who studies metabolism and epigenetics, discovered that the nucleus is a metabolic compartment with unique metabolic activity.\n\n    Penn Medicine\n\nâ€œAlthough that may sound obvious, it was not,â€ Wellen said. The nucleusâ€™s metabolic activity was specific to the functions in that compartment, including epigenetic activity. â€œThere are a lot of metabolic enzymes that are actually in the nucleus and are dynamically regulated in the nucleus,â€ said Wellen, who now heads a lab at the University of Pennsylvania. â€œWe were really excited to find that.â€\nThis idea of the nucleus as a metabolic compartment was foundational to understanding how metabolism impacts embryonic development. In early embryonic cells, as developmental decisions are made that direct cells to become ectoderm, mesoderm and endoderm, all of the epigenetic marks on the histones get repositioned. They can be removed, added and relocated to activate certain genes and repress others.\nâ€œWhat is intriguing is that all of this is associated with a massive accumulation of metabolic enzymes in the nucleus,â€ said Å»ylicz, the developmental biologist. These enzymes make molecules, which then activate other enzymes that remove epigenetic marks and lay down new ones as cells grow, divide and take on different fates.\nDuring this period, the cell moves many enzymes (opens a new tab) from the cytoplasm and mitochondria to the nucleus. That way, the metabolites necessary for gene activity can be produced locally, in the nucleus, where they are needed, Å»ylicz said. â€œThe moment where you reprogram the epigenome â€” that happens to be the same time when youâ€™re also really using this nucleus as a metabolic compartment.â€\nEarly in human development, the embryo is a ball of cells. The cells on the outside form the placenta; the cells on the inside form the embryo. The major difference between these two types of cells is in the activity of metabolic genes. Recently, Å»yliczâ€™s team pinpointed differences between these cells in alpha-ketoglutarate, a well-studied metabolite, and showed that the metabolite accelerated the differentiation (opens a new tab) of stem cells into cells that will become the placenta.\n\n                Related:\n\n                                    Cell by Cell, Scientists Map the Genetic Steps as Eggs Become Animals\n\n                                    Cells Across the Body Talk to Each Other About Aging\n\n                                    In the Ticking of the Embryonic Clock, She Finds Answers\n\n                                    Cellular Self-Destruction May Be Ancient. But Why?\n\nAlpha-ketoglutarate not only controls differentiation in stem cells; it does the same in cancer cells, Finleyâ€™s team and other groups found a few years ago. They were studying p53, a protein that is well known for its anticancer effects; its gene is the most commonly mutated gene in human cancer. Their study, published inNature, found that p53 caused alpha-ketoglutarate to accumulate; this alpha-ketoglutaratealtered the fate of the cancer cells (opens a new tab) so that they were less likely to form tumors. This was striking and unexpected because researchers had assumed that p53 has an anticancer effect by directly regulating the activity of genes. It also works by altering metabolism.\nâ€œThis is particularly exciting because if changing metabolism can change cell fate in a meaningful way, there is the possibility that you might be able to manipulate that therapeutically, where aberrant decisions of differentiation are causal for the disease â€” like in many forms of cancer,â€ said Rutter, who was not involved in the study.\nIn some ways, this interplay between metabolism and genes is obvious: We know that life is influenced by both its genes and its environment. This new, exciting field of research shows at a molecular level how the materials available to our cells influence their fates, and ours.\n\nBy Viviane Callier\n                Contributing Writer\n\n                March 21, 2025\n\n                    View PDF/Print Mode\n\n                            biology\n\n                            cancer\n\n                            cells\n\n                            development\n\n                            developmental biology\n\n                            DNA\n\n                            epigenetics\n\n                            gene regulation\n\n                            metabolism\n\n                            mitochondria\n\n                            stem cells\n\n                    All topics\n\nShare this article\n\n    Facebook\n\n                            Copied!\n\n    Copy link\n         (opens a new tab)\n\n    Email\n\n    Pocket\n\n    Reddit\n\n    Ycombinator\n\n                    Newsletter\n\n                    Get Quanta Magazine delivered to your inbox\n\n                    Subscribe now\n\n                    Recent newsletters\n\n             (opens a new tab)\n\nThe Quanta Newsletter\n\n                    Get highlights of the most important news delivered to your email inbox\n\n                            Email\n\n                        Subscribe\n\n                        Recent newsletters\n                                             (opens a new tab)\n\nAlso in Biology\n\n                    How Did Multicellular Life Evolve?\n\n                The Joy of Why\n\n                    How Did Multicellular Life Evolve?\n\n        By\n\n                    Janna Levin\n\n                 +1 authors\n\n                        Steven Strogatz\n\n            March 20, 2025\n\n        Comment\n\n        Save Article\n\n                    Read Later\n\n                    A New, Chemical View of Ecosystems\n\n                ecology\n\n                    A New, Chemical View of Ecosystems\n\n        By\n\n                Molly Herring\n\n            March 5, 2025\n\n        Comment\n                1\n\n        Save Article\n\n                    Read Later\n\n                    The â€˜Elegantâ€™ Math Model That Could Help Rescue Coral Reefs\n\n                mathematical biology\n\n                    The â€˜Elegantâ€™ Math Model That Could Help Rescue Coral Reefs\n\n        By\n\n                Max G. Levy\n\n            February 26, 2025\n\n        Comment\n                1\n\n        Save Article\n\n                    Read Later\n\nComment on this article\n\n                    Quanta Magazine moderates comments tofacilitate an informed, substantive, civil conversation. Abusive, profane, self-promotional, misleading, incoherent or off-topic comments will be rejected. Moderators are staffed during regular business hours (New York time) and can only accept comments written in English.\n\n        Show comments\n\nNext article\n                How Did Multicellular Life Evolve?\n\nQuanta Homepage\n\n    Facebook\n\n        Youtube\n\n        Instagram\n\n                                        About Quanta\n\n                                        Archive\n\n                                        Contact Us\n\n                                        Terms & Conditions\n\n                                        Privacy Policy\n\n                        All Rights Reserved Â© 2025\n\n                    An editorially independent publication supported by the Simons Foundation.\n\n                        Simons Foundation\n\nQuanta Homepage\n\n    Facebook\n\n        Youtube\n\n        Instagram\n\nAbout Quanta\n\n                                        Archive\n\n                                        Contact Us\n\n                                        Terms & Conditions\n\n                                        Privacy Policy\n\nAll Rights Reserved Â© 2025\n\n                    An editorially independent publication supported by the Simons Foundation.\n\n                        Simons Foundation\n\nClose\n\n    Log in to Quanta\n\n            Use your social network\n\n                        Facebook                        Connect with Facebook\n\n                                                Connect with Google\n\n            or\n\n                    email\n\n                    password\n\n                    Remember me\n\n                        Forgot your password ?\n\n    Don't have an account yet?\n         Sign up\n\nClose\n\n        Forgot your password?\n        Weâ€™ll email you instructions to reset your password\n\n                email\n\nClose\n\n        Change your password\n        Enter your new password\n\n                Password\n\n                Retype new password\n\nClose\n\n        Sign Up\n\n                    First Name\n\n                    Last Name\n\n                    Email\n\n                    Password\n\n                    Retype Password\n\n            Creating an account means you accept Quanta Magazine's\n\n            Terms & Conditions and Privacy Policy",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "29e3a9556d84a434",
    "title": {
      "en": "High-Performance PNG Decoding",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://blend2d.com/blog/png-image-codec.html",
    "score": 28,
    "by": "PaulHoule",
    "time": 1742710934,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "b8e7e2ce380474ed",
    "title": {
      "en": "RDNA 4's â€œOut-of-Orderâ€ Memory Accesses",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://chipsandcheese.com/p/rdna-4s-out-of-order-memory-accesses",
    "score": 152,
    "by": "ingve",
    "time": 1742768309,
    "content": "Share this postChips and CheeseRDNA 4's \"Out-of-Order\" Memory AccessesCopy linkFacebookEmailNotesMoreRDNA 4's \"Out-of-Order\" Memory AccessesExamining RDNA 4's out-of-order memory accesses in detail, and investigating with testingChester LamMar 24, 202515Share this postChips and CheeseRDNA 4's \"Out-of-Order\" Memory AccessesCopy linkFacebookEmailNotesMore43ShareAMD's RDNA 4 brings a variety of memory subsystem enhancements. Among those, one slide stood out because it dealt with out-of-order memory accesses. According to the slide, RDNA 4 allows requests from different shaders to be satisfied out-of-order, and adds new out-of-order queues for memory requests.Cross-Wave Out-of-Order Memory AccessesAMD apparently had a false dependency case in the memory subsystem prior to RDNA 4. One wave could wait for a memory loads made by another wave. A \"wavefront\", \"wave\", or \"warp\" on a GPU is the rough equivalent of a CPU thread. It has its own register state, and can run out of sync with other waves. Each wave's instructions are independent from those in other waves with very few exceptions (like atomic operations).In RDNA 3, there was a strict ordering on the return of data, such that effectively a request that was made later in time was not permitted to pass a request made earlier in time, even if the data for it was ready much sooner.Navi 4 Architecture Deep Dive, Andrew Pomianowski, CVP, Silicon Design Engineering (AMD)A fundamental tenet of multithreaded programming is that you get no ordering guarantees between threads unless you make it happen via locks or other mechanisms. That's what makes multithreaded performance scaling work. AMD's slide took me by surprise because there's no reason memory reads should be an exception. I re-watched the video several times and stared at the slide for a while to see if that's really what they meant. They clearly meant it, but I still didn't believe my eyes and ears. So I took time to craft a test for it.Testing ItAMD's slide describes a scenario where one wave's cache misses prevent another wave from quickly consuming data from cache hits. Causing cache misses is easy. I can pointer chase through a large array with a random pattern (\"wave Y\"). Similarly, I can keep accesses within a small memory footprint to get cache hits (\"wave X\"). But doing both at the same time is problematic. Wave Y may evict data used by wave X, causing cache misses on wave X.Focusing on this scenario, and trying to create a Wave X and Wave Y that might hold each other upInstead of going for cache hits and misses, I tested by seeing whether waiting on memory accesses in one wave would falsely wait in on memory accesses made by another. My \"wave Y\" is basically a memory latency test, and makes a fixed number of accesses. Each access depends on the previous one's result, and I have the wave pointer chase through a 1 GB array to ensure cache misses. My \"wave X\" makes four independent memory accesses per loop iteration. It then consumes the load data, which means waiting for data to arrive from memory.Once wave Y completes all of its accesses, it sets a flag in local memory. Wave X makes as many memory accesses as it can until it sees the flag set, after which it writes out its â€œscoreâ€ and terminates. I run both waves in the same workgroup to ensure they share a WGP, and therefore share as much of the memory subsystem as possible. Keeping both waves in the same workgroup also lets me place the â€œfinishedâ€ flag in local memory. Wave X has to check that flag every iteration, and itâ€™s best to have flag checks not go through the same caches that wave Y is busy contaminating.If each wave X access gets delayed by a wave Y one, I should see approximately the same number of accesses from both. Instead on RDNA 3, I see wave X make more accesses than wave Y by exactly the loop unroll factor on wave X. AMD's compiler statically schedules instructions and sends out all four accesses before waiting on data. It then waits on load completion with s_waitcnt vmcnt(...) instructions.Annotated RDNA 3 assembly generated by AMD's compiler for wave X. Note that unrolling the loop to use four memory accesses per iteration lets the compiler issue those four accesses before waiting on themAccesses tracked by vmcnt always return in-order, letting the compiler wait on specific accesses by waiting until vmcnt decrements to a certain value or lower. In wave Y, I make all accesses dependent so the compiler only waits for vmcnt to reach 0.Annotated RDNA 3 assembly for wave y, for completenessOn RDNA 3, s_waitcnt vmcnt(...) seems to wait for requests to complete not only from its wave, but from other waves too. That explains why wave X makes exactly four accesses for each access that wave Y makes. If I unroll the loop more, letting the compiler schedule more independent accesses before waiting, the ratio goes up to match the unroll factor.On RDNA 4, the two waves donâ€™t care what the other is doing. Thatâ€™s the way it should be. RDNA 4 also displays more run-to-run variation, which is also expected because cache behavior is highly unpredictable in this test. Iâ€™m surprised by the results, but itâ€™s convincing evidence that AMD indeed had false cross-wave memory delays on RDNA 3 and older GPU architectures. I also tested on Renoirâ€™s Vega iGPU, and saw the same behavior as RDNA 3.At a simplistic level, you can imagine that requests from the shaders go into a queue to be serviced, and many of those requests can be in flightNavi 4 Architecture Deep Dive, Andrew Pomianowski, CVP, Silicon Design Engineering (AMD)AMD's presentation hints that RDNA 3 and older GPUs had multiple waves sharing a memory access queue. As mentioned above, AMD GPUs since GCN handle memory dependencies with hardware counters that software waits on. By keeping vmcnt returns in-order, the compiler can wait on the specific load that produces data needed by the next instruction, without also waiting on every other load the wave has pending. RDNA 3 and prior AMD GPUs possibly had a shared memory access queue, with each entry tagged with its wave's ID. As each memory access leaves the queue in-order, hardware decrements the counter for its wave.Perhaps RDNA 4 divides the shared queue into per-thread queues. That would align with the point on AMD's slide saying RDNA 4 introduces \"additional out-of-order queues\" for memory requests. Or perhaps RDNA 4 retains a shared queue, but can drain entries out-of-order. That would require tracking extra info, like whether a memory access is the oldest one for its wave.Does it Happen to Others?Sharing a memory access queue and returning data in-order seems like a natural hardware simplification. That raises the question of whether GPU architectures from Intel and Nvidia had similar limitations.Intel's Xe-LPG does not have false cross-wave memory dependencies. Running the same test on Meteor Lake's iGPU shows variation depending on where the two waves end up. If wave X and wave Y run on XVEs with shared instruction control logic, wave X's performance is lower than in other cases. Regardless, it's clear Xe-LPG doesn't force a wave to wait on another's accesses. Intel's subsequent Battlemage (Xe2) architecture shows similar behavior, and the same applies to Intel's Gen 9 (Skylake) graphics from a while ago.I also checked generated assembly to ensure Intel's compiler wasn't unrolling the loop further.Generated assembly on Meteor Lake's iGPU for Wave X. UGM = untyped global memory, SLM = shared local memory. The rest is trivial, just remember that Intel GPUs have registers that are full of registers...never mindNvidia's Pascal has varying behavior depending on where waves are located within a SM. Each Pascal SM has four partitions, which are arranged in pairs that share a texture unit and a 24 KB texture cache. Waves are assigned to partitions within a pair first. It's as if the partitions are numbered [0,1]-> tex, [2,3]-> tex. Waves in the same sub-partition pair have the false dependency issue. Evidently they share some general load/store logic besides the texture unit, because I don't touch textures in this test.If a wave is not offset from another one by a multiple of four or multiple of 4 plus one, it doesn't have the false dependency problem. Turing, as tested on the GTX 1660 Ti, doesn't have a problem either.Better Nonblocking LoadsBesides removing false cross-wave delays, AMD also improved memory request handling within a wave. Much like in-order CPU cores, like Arm's Cortex A510, GPUs can execute independent instructions while waiting on memory access. A thread only stalls when it tries to use the memory access's result. GPUs have done this for decades, though the implementation details differ. Intel and Nvidia's GPUs use a software managed scoreboard. AMD used pending request counters from GCN onward.RDNA 4 uses the same scheme but splits out the vmcnt category into several counters. A thread can interleave global memory, texture sampling, and raytracing intersection test requests, and wait on them separately. That gives the compiler more flexibility to move work ahead of a wait for memory access completion. Another interpretation of AMD's slide is that the each counter corresponds to a separate queue, each of which has out-of-order behavior across waves (but may have in-order behavior within a wave).Example of RDNA 4 assembly from 3DMark's raytracing feature test, showing a basic block separately waiting on global memory loads and texture sampling requests issued by other basic blocksSimilarly, lgkmcnt gets separated into kmcnt for scalar memory loads and dscnt for LDS accesses. Scalar memory loads are out-of-order, which means the compiler must wait for all scalar memory loads to complete (kmcnt=0 or lgmkcnt=0) before using results from any pending scalar memory load. On RDNA 4, the compiler can interleave scalar memory and LDS accesses without having to wait for lgkmcnt=0.Intel and Nvidia's GPUs use software managed scoreboards. A scoreboard entry can be set or and waited on by any instruction, regardless of memory access type. Therefore RDNA 4's optimization isn't applicable to those other GPU architectures. A cost to Intel/Nvidia's approach is that utilizing a big memory request queue would require a correspondingly large scoreboard. AMD can extend a counter by one bit and double the number of queue entries a wave can use.Final WordsRDNA 4's memory subsystem enhancements are exciting and improve performance across a variety of workloads compared to RDNA 3. AMD specifically calls out benefits in raytracing workloads, where traversal and result handling may occur simultaneously on the same WGP. Traversal involves pointer chasing, while result handling might involve more cache friendly data lookups and texture sampling. Breaking cross-wave memory dependencies would prevent different memory access patterns in those tasks from delaying each other.Likely this wasn't an issue with rasterization because waves assigned to a WGP probably work on pixels in close proximity. Those waves may sample the same textures, and even take samples in close proximity to each other within the same texture. If one wave misses in cache, the others likely do too.Breaking up vmcnt and lgmkcnt probably helps raytracing too. Raytracing shaders make BVH intersection and LDS stack management requests during traversal. Then they might sample textures or access global memory buffers during result handling. Giving the compiler flexibility to interleave those request types and still wait on a specific request is a good thing.Radeon logo on the RX 9070 graciously provided by AMD for reviewBut RDNA 4's scheme for handling memory dependencies isn't fundamentally different from that of GCN many years ago. While the implementation details differ, RDNA 4, GCN, and Intel and Nvidia's GPUs can all absorb cache misses without immediately stalling a thread. Each GPU maker has improved their ability to do so, whether it's with more scoreboard tokens or more counters. RDNA 4 indeed can do Cortex A510 style nonblocking loads, but it's far from a new feature in the world of GPUs.Resolving false cross-wave dependencies isn't new either. Nvidia had \"out-of-order\" cross-wave memory access handling in Turing, and presumably their newer architectures too. Intel had the same at least as far back as Gen 9 (Skylake) graphics. Therefore RDNA 4's \"out-of-order\" memory subsystem enhancements are best seen as generational tweaks, rather than new game changing techniques. Still, AMD's engineers deserve credit for making them happen. RDNA 4â€™s  arguably makes the most significant change to AMDâ€™s GPU memory subsystem since RDNA launched in 2019. I'm glad to see the company continue to improve their GPU architecture and make it better suited to emerging workloads like raytracing.Subscribe to Chips and CheeseLaunched a year agoThe Devil is in the Details! Deep dives into computer hardware and software and the wider industry...SubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.15 Likesâˆ™3 Restacks15Share this postChips and CheeseRDNA 4's \"Out-of-Order\" Memory AccessesCopy linkFacebookEmailNotesMore43SharePrevious",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "79d3b9dd2357223c",
    "title": {
      "en": "Blue Ghost lander captures sunset shots on moon before falling silent",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://phys.org/news/2025-03-blue-ghost-lander-captures-stunning.html",
    "score": 97,
    "by": "pseudolus",
    "time": 1742381137,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "b5943bb62ae43b9d",
    "title": {
      "en": "Move on to ESM-Only",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://antfu.me/posts/move-on-to-esm-only",
    "score": 79,
    "by": "bpierre",
    "time": 1742776525,
    "content": "The Toolings are Ready Modern Tools Top-Down & Bottom-Up Requiring ESM in Node.js The Troubles with Dual Formats Interop Issues Dependency Resolution Package Size When Should We Move to ESM-only? New Packages Browser-targeted Packages Standalone CLI Node.js Support Know Your Consumers How Far We Are? Moving Forward Three years ago, I wrote a post about shipping ESM & CJS in a single package, advocating for dual CJS/ESM formats to ease user migration and trying to make the best of both worlds. Back then, I didnâ€™t fully agree with aggressively shipping ESM-only, as I considered the ecosystem wasnâ€™t ready, especially since the push was mostly from low-level libraries. Over time, as tools and the ecosystem have evolved, my perspective has gradually shifted towards more and more on adopting ESM-only.As of 2025, a decade has passed since ESM was first introduced in 2015. Modern tools and libraries have increasingly adopted ESM as the primary module format. According to WOOORMâ€™s script, the packages that ships ESM on npm in 2021 was 7.8%, and by the end of 2024, it had reached 25.8%. Although a significant portion of packages still use CJS, the trend clearly shows a good shift towards ESM.ESM adoption over time, generated by the npm-esm-vs-cjs script. Last updated at 2024-11-27Here in this post, Iâ€™d like to share my thoughts on the current state of the ecosystem and why I believe itâ€™s time to move on to ESM-only.The Toolings are Ready #Modern Tools #With the rise of Vite as a popular modern frontend build tool, many meta-frameworks like Nuxt, SvelteKit, Astro, SolidStart, Remix, Storybook, Redwood, and many others are all built on top of Vite nowadays, that treating ESM as a first-class citizen.As a complement, we have also testing library Vitest, which was designed for ESM from the day one with powerful module mocking capability and efficient fine-grain caching support.CLI tools like tsx and jiti offer a seamless experience for running TypeScript and ESM code without requiring additional configuration. This simplifies the development process and reduces the overhead associated with setting up a project to use ESM.Other tools, for example, ESLint, in the recent v9.0, introduced a new flat config system that enables native ESM support with eslint.config.mjs, even in CJS projects.Top-Down & Bottom-Up #Back in 2021, when SINDRESORHUS first started migrating all his packages to ESM-only, for example, find-up and execa, it was a bold move. I consider this move as a bottom-up approach, as the packages that rather low-level and many their dependents are not ready for ESM yet. I was worried that this would force those dependents to stay on the old version of the packages, which might result in the ecosystem being fragmented. (As of today, I actually appreciate that move bringing us quite a lot of high-quality ESM packages, regardless that the process wasnâ€™t super smooth).Itâ€™s way easier for an ESM or Dual formats package to depend on CJS packages, but not the other way around. In terms of smooth adoption, I believe the top-down approach is more effective in pushing the ecosystem forward. With the support of high-level frameworks and tools from top-down, itâ€™s no longer a significant obstacle to use ESM-only packages. The remaining challenges in terms of ESM adoption primarily lie with package authors needing to migrate and ship their code in ESM format.Requiring ESM in Node.js #The capability to require() ESM modules in Node.js, initiated by JOYEECHEUNG, marks an incredible milestone. This feature allows packages to be published as ESM-only while still being consumable by CJS codebases with minimal modifications. It helps avoid the async infection (also known as Red Functions) introduced by dynamic import() ESM, which can be pretty hard, if not impossible in some cases, to migrate and adapt.This feature was recently unflagged and backported to Node.js v22 (and soon v20), which means it should be available to many developers already. Consider the top-down or bottom-up metaphor, this feature actually makes it possible to start ESM migration also from middle-out, as it allows import chains like ESM â†’ CJS â†’ ESM â†’ CJS to work seamlessly.To solve the interop issue between CJS and ESM in this case, Node.js also introduced a new export { Foo as 'module.exports' } syntax in ESM to export CJS-compatible exports (by this PR). This allows package authors to publish ESM-only packages while still supporting CJS consumers, without even introducing breaking changes (expcet for changing the required Node.js version).For more details on the progress and discussions around this feature, keep track on this issue.The Troubles with Dual Formats #While dual CJS/ESM packages have been a quite helpful transition mechanism, they come with their own set of challenges. Maintaining two separate formats can be cumbersome and error-prone, especially when dealing with complex codebases. Here are some of the issues that arise when maintaining dual formats:Interop Issues #Fundamentally, CJS and ESM are different module systems with distinct design philosophies. Although Node.js has made it possible to import CJS modules in ESM, dynamically import ESM in CJS, and even require() ESM modules, there are still many tricky cases that can lead to interop issues.One key difference is that CJS typically uses a single module.exports object, while ESM supports both default and named exports. When authoring code in ESM and transpiling to CJS, handling exports can be particularly challenging, especially when the exported value is a non-object, such as a function or a class. Additionally, to make the types correct, we also need to introduce further complications with .d.mts and .d.cts declaration files. And so onâ€¦As I am trying to explain this problem deeper, I found that I actually wish you didnâ€™t even need to be bothered with this problem at all. Itâ€™s frankly too complicated and frustrating. If you are just a user of packages, let alone the package authors to worry about that. This is one of the reasons I advocate for the entire ecosystem to transition to ESM, to leave these problems behind and spare everyone from this unnecessary hassle.Dependency Resolution #When a package has both CJS and ESM formats, the resolution of dependencies can become convoluted. For example, if a package depends on another package that only ships ESM, the consumer must ensure that the ESM version is used. This can lead to version conflicts and dependency resolution issues, especially when dealing with transitive dependencies.Also for packages that are designed to used with singleton pattern, this might introduce multiple copies of the same package and cause unexpected behaviors.Package Size #Shipping dual formats essentially doubles the package size, as both CJS and ESM bundles need to be included. While a few extra kilobytes might not seem significant for a single package, the overhead can quickly add up in projects with hundreds of dependencies, leading to the infamous node_modules bloat. Therefore, package authors should keep an eye on their package size. Moving to ESM-only is a way to optimize it, especially if the package doesnâ€™t have strong requirements on CJS.When Should We Move to ESM-only? #This post does not intend to diminish the value of dual-format publishing. Instead, I want to encourage evaluating the current state of the ecosystem and the potential benefits of transitioning to ESM-only.There are several factors to consider when deciding whether to move to ESM-only:New Packages #I strongly recommend that all new packages be released as ESM-only, as there are no legacy dependencies to consider. New adopters are likely already using a modern, ESM-ready stack, there being ESM-only should not affect the adoption. Additionally, maintaining a single module system simplifies development, reduces maintenance overhead, and ensures that your package benefits from future ecosystem advancements.Browser-targeted Packages #If a package is primarily targeted for the browser, it makes total sense to ship ESM-only. In most cases, browser packages go through a bundler, where ESM provides significant advantages in static analysis and tree-shaking. This leads to smaller and more optimized bundles, which would also improve loading performance and reduce bandwidth consumption for end users.Standalone CLI #For a standalone CLI tool, itâ€™s no difference to end users whether itâ€™s ESM or CJS. However, using ESM would enable your dependencies to also be ESM, facilitating the ecosystemâ€™s transition to ESM from a top-down approach.Node.js Support #If a package is targeting the evergreen Node.js versions, itâ€™s a good time to consider ESM-only, especially with the recent require(ESM) support.Know Your Consumers #If a package already has certain users, itâ€™s essential to understand the dependentsâ€™ status and requirements. For example, for an ESLint plugin/utils that requires ESLint v9, while ESLint v9â€™s new config system supports ESM natively even in CJS projects, there is no blocker for it to be ESM-only.Definitely, there are different factors to consider for different projects. But in general, I believe the ecosystem is ready for more packages to move to ESM-only, and itâ€™s a good time to evaluate the benefits and potential challenges of transitioning.How Far We Are? #The transition to ESM is a gradual process that requires collaboration and effort from the entire ecosystem. Which I believe we are on a good track moving forward.To improve the transparency and visibility of the ESM adoption, I recently built a visualized tool called Node Modules Inspector for analyzing your packagesâ€™s dependencies. It provides insights into the ESM adoption status of your dependencies and helps identify potential issues when migrating to ESM.Here are some screenshots of the tool to give you a quick impression:Node Modules Inspector - OverviewNode Modules Inspector - Dependency GraphNode Modules Inspector - Reports like ESM Adoptions and Duplicated PackagesThis tool is still in its early stages, but I hope it will be a valuable resource for package authors and maintainers to track the ESM adoption progress of their dependencies and make informed decisions about transitioning to ESM-only.To learn more about how to use it and inspect your projects, check the repository node-modules-inspector.Moving Forward #I am planning to gradually transition the packages I maintain to ESM-only and take a closer look at the dependencies we rely on. We also have plenty of exciting ideas for the Node Modules Inspector, aiming to provide more useful insights and help find the best path forward.I look forward to a more portable, resilient, and optimized JavaScript/TypeScript ecosystem.I hope this post has shed some light on the benefits of moving to ESM-only and the current state of the ecosystem. If you have any thoughts or questions, feel free to reach out using the links below. Thank you for reading!",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "bc798ab915ad080e",
    "title": {
      "en": "The case of the critical section that let multiple threads enter a block of code",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://devblogs.microsoft.com/oldnewthing/20250321-00/?p=110984",
    "score": 119,
    "by": "luu",
    "time": 1742717665,
    "content": "March 20, 2025\n      What could cause a memory corruption bug to disappear in safe mode?\n\n        Raymond Chen",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "d209cefdecea71a4",
    "title": {
      "en": "Monster Cables picked the wrong guy to threaten (2008)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.oncontracts.com/monster-cables-picked-the-wrong-guy-to-threaten/",
    "score": 548,
    "by": "wallflower",
    "time": 1742603437,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "59c3d3c0405f7a06",
    "title": {
      "en": "The belay test and the modern American climbing gym",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.climbing.com/people/peter-mayfield-belay-test-modern-american-climbing-gym-history/",
    "score": 133,
    "by": "vasco",
    "time": 1742408381,
    "content": "Meet the Guy Who Invented the Belay Testâ€”and the Modern American Climbing Gym\n\n          The story of Californiaâ€™s City Rock\n\n      Updated\n\n        Mar 18, 2025\n\n          Bruce Hildenbrand\n\n      Comment\n\n    Share\n\n            Share to Outside Feed\n            Create a new post with the article attached\n\n          Copy link\n\n          Email\n\n          Share on X\n\n          Share on Facebook\n\n          Share on Reddit\n\n              Photo: Peter Mayfield Collection\n\n                Peter Mayfield bouldering at City Rock Photo: Peter Mayfield Collection\n\n      Heading out the door? Read this article on the new Outside+ app available now on iOS devices for members!\n      Download the app.\n\n      Forty years ago, there was not a single purpose-built climbing gym in the U.S. In 1987, Seattleâ€™s Vertical Club opened and a year later, the Portland Rock Gym launched. But the concept of a profitable indoor climbing gym was far from proven. This is the story of the person whose vision and determination would revolutionize climbing gyms as we know them today.\nPeter Mayfield was a climbing prodigy. At 16 years old, in 1978, he was repeating the hardest free climbs in Yosemite. He went on to establish some of the hardest aid routes on the Valleyâ€™s big walls. By the age of 19, he was guiding for the prestigious Yosemite Mountaineering School, and became chief guide a year later. When sport climbing competitions began in the late 1980s, Mayfield participated in the first ever Snowbird climbing competition as well as Jeff Loweâ€™s national competitions.\nIn 1988, Mayfield formed a guiding cooperative with legends Kim Schmitz and Jim Bridwell, but by his mid-twenties, he was looking for a job that would keep him closer to home. So when Bridwell returned from a trip to Europe with posters and magazines of climbing walls going up in France, Mayfield knew heâ€™d found his next venture.\n\nPeter Mayfield at 19 years old on his first ascent in 1981 of Zenyatta Mondatta on El Cap with Jim Bridwell. Twenty years after his FA, Mayfield repeated the route with his 19-year-old son Braden. (Photo: Peter Mayfield Collection)\nAt the time, the walls in France were closer to artwork in public spaces than purpose-built climbing structures. It was around that time that the Vertical Clubâ€”a space for experienced climbers to trainâ€”went up, but Mayfield had something completely different in mind. As chief guide of the Yosemite Mountaineering School, he didnâ€™t just want to create a place for himself and his buddies to get stronger. â€œI had this serious career of turning people onto this sport,â€ reflects Mayfield. â€œI was really thinking [about] kids, corporate programs â€¦ The word gym wasnâ€™t even in my mind. It was more like an indoor climbing institute.â€\nWith the popularity of indoor climbing gyms these days, itâ€™s hard to imagine what it was like in the late â€™80s, when climbing itself was still very much a niche activity. Mayfield knew that to keep the gym lights on, heâ€™d need to attract more than just his hardcore climbing buddies.\nThen there was the problem with Californiaâ€™s legendary great weather. Mayfield was skeptical that climbers would want to climb indoors on days when the sun was shining, because unlike Seattle and Portland, you could climb outdoors virtually year-round. So his marketing plan was also designed to get non-climbers to come to his gym. He wanted to build a place where â€œ10,000 eight-year-old girls could try something they never dreamed of.â€\nâ€œThat was my pitch,â€ Mayfield says.\nThe economics of the first commercial climbing gym\nMayfield found his first investor while guiding an ascent of Alaskaâ€™s Mooseâ€™s Tooth. Tent-bound by storms for days, he discussed his climbing gym dreams with his clients. At the end of the expedition, as they were splitting up at the Oakland Airport, one of the clients asked him what he needed to start his business. He replied that he needed money for a phone line. The client pulled out his wallet and gave him $200. â€œI literally got handed two C-notes to start my business,â€ he remembers. He used that money to put in a Pacific Bell phone line in his motherâ€™s basement in Berkeley and got to work.\nâ€œI got no positive reaction from the [climbing] industry at all,â€ remembers Mayfield. â€œI was pretty connected because I climbed with Bridwell. I met with the CEO of The North Face. I met with the CEO of Marmot. I met with the president of REI. They all said â€˜it will never work. It makes sense in Seattle where it rains. Who would ever climb indoors in California?â€\nMayfield even sent his business plan to Yvon Chouinard. â€œHe was very polite and gave me some good advice,â€ Mayfield recalls. â€œHe did not invest. None of them invested. But he kind of got what I was talking about.â€ (Chouinard did tell Mayfield that he should try to sell milkshakes: â€œIf you can nail it with the youth, you will kick ass.â€)\nSo Mayfield widened his net, contacting â€everybody in Silicon Valley who had ever climbedâ€ and raised half a million dollars with 48 limited partners, essentially a large group of $5,000 investors. He stalled for a while, dealing with the mountain of paperwork associated with creating such a high volume of limited partnerships. But in the midst of his fundraising, he produced the Great Outdoor Adventure Fair in San Francisco. To promote his concept to the general public, he set up a portable climbing wall at the fair. John Gage, high up in management at one of Silicon Valleyâ€™s most successful startups, Sun Microsystems, was impressed and tossed in $40,000, re-energizing Mayfield.\nBut at that time, building a climbing gym was a huge, complex undertaking. Mayfield relied on a suite of competent advisers to help him through. He eventually hooked up with Armand Tanny of Goldâ€™s Gym, who explained the new, revolutionary process of selling and managing memberships via an automatic fund transfer. They also impressed upon him the importance of aesthetics. The other two climbing gyms had spent no more than $18,000 building their gyms. Mayfield spent $40,000 on just the bathrooms.\nChristian Griffith (designer of City Rock walls) and Peter Mayfield (Photo: Beth Wald)\nBuilding a gym for climbersâ€”and non-climbers\nâ€œSo then I was like, â€˜Okay, this is a gym. This is not an indoor sculpture garden that we climb on thatâ€™s a climbing institute. I am going to build a business model with monthly memberships,'â€ adds Mayfield. He decided to call it City Rock Gym. The gym was located in Emeryville, between Oakland and Berkeley.\nâ€œI made the first gym you could take your kid to and trust that they would be well taken care of,â€ he explains.\nA number of climbing wall companies, such as Entre-Prises, were pressing Mayfield to help him build his gym. Instead, he turned to Christian Griffith, who he had climbed with on the competition circuit, and who felt that they could design an â€incredible spaceâ€ together. â€œChristian posted up in south Berkeley in my momâ€™s yard and carved foam for two weeks and he completely created that gym,â€ recalls Mayfield. â€œHe took foam core and glued graph paper to it and he did a beautiful model. He was inspired by the gritstone cliffs in England where you are always like â€˜whatâ€™s around the corner?â€™ So City Rock had these really protuberant buttresses.â€\nOverall, the City Rock Gym design was both intentional and approachable. â€œWhen people walked in the door I wanted them to see the friendly slab. I wanted them to think they can do this,â€ he explains. â€œI want them to see the six year olds climbing. I really felt that I had to prove the concept. Itâ€™s not me and my bros throwing down at the boulders.â€\nInstruction served as another key pioneering element of Mayfieldâ€™s gym concept. He began teaching the climbing safety techniques heâ€™d learned from the Sierra Club, an organization which had been studying and teaching belay practices since the 1930s. â€œWe really did invent the belay test,â€ he recalls, citing the need for procedure in such a high-risk business. â€œI thought somebody would die at our business. I ran the statistics in my head. So many people tying knots. So many people belaying. I really took it seriously.â€\nGiven the magnitude of risk, Mayfield says he really â€œleaned intoâ€ establishing a belay test and enforcing safety. â€œI had some really good, famous, climbers come in and fail the belay test,â€ he recalls. â€œThese good climbers were just a little too casual with their hands. You donâ€™t vaguely slide your hands around. They had to tighten their act up a little bit.â€\nOn City Rock Gymâ€™s grand opening weekend in 1990, 900 people came in to climb. Mayfield watched brand-new climbing ropes wear out in front of his eyes. â€œWe installed these half-pipe sealed bearing things so the ropes wouldnâ€™t wear out, but on Sunday morning: â€˜Oh my God! I see cores in that rope. Quick, replace that,â€™â€ remembers Mayfield. â€œI took the climbing school really seriously. I hired the best and they were well trained â€¦ It was really high level with really good technique coaching.â€\nCity Rock quickly gained a loyal following. With his emphasis on introducing new participants to the sport, Mayfield pioneered programs like a kidâ€™s belay clinic and the first climbing-themed birthday parties. He created an event called Women on Rock and flew in Lynn Hill to teach it. The vertical dance team Bandaloop got its start at City Rock in 1991.\nThe vertical dance company Bandaloop was born at City Rock (Photo: Peter Mayfield Collection)\nBut could a commercial climbing gym keep elite climbers stoked?\nWhile he was intent on attracting non-climbers to the gym, Mayfield didnâ€™t turn his back on his climbing buddies. He created routes for their level of difficulty and also gave some of them jobs. Many experienced climbers including Scott Cosgrove, Steve Schneider, Bird Lew, and Don Welch worked at City Rock.\nâ€œWe set some hard routes,â€ remembers Schneider, a 5.14 climber and one of the routesetters at City Rock. â€œWe did some competitions there which were very successful and those routes would be left up for others to climb.â€\nAs the initial buzz of City Rockâ€™s grand opening wore off, two vital questions remained: Were established climbers willing to climb indoors on sunny days? And were the routes difficult enough to keep experienced climbers satisfied? â€œI was initially a little skeptical but found I really enjoyed it,â€ recalls Greg Murphy who, together with Chan Harrell, held the fastest known time on the Northwest Face of Half Dome. â€œThere was nothing super steep as I recall, so the harder routes were kind of finger intensive. But it always seemed like there was a good mix of routes and some harder projects to work on.â€\nAnne Smith, an excellent climber and the mother of Connor Herson, agrees: â€œCR [City Rock] was very much geared towards experienced climbers. [It] hosted National Championship events and set at least as many harder climbers as easier ones.â€\nAs a competitive sport climber himself, Mayfield eagerly hosted national championship competitions five years in a row. Hans Florine had set the speed record on the Nose with Steve Schnieder the week before he won the 1990 nationals in both difficulty and speed at City Rock. For the next three years, Florine partnered with Mayfield and City Rock to stage the national championships there. The gym played host to top climbers such as Robyn Erbesfield, Bobbi Bensman, Dale Goddard, and Jim Karn.\nHank Levine competing in US Nationals at City Rock (Photo: Peter Mayfield Collection)\nThe inclusive climbing gym concept takes off\nThree years after City Rockâ€™s opening, California got its second climbing gym via Tom Davisâ€™s now-famous Pacific Edge, in Santa Cruz. (Chris Sharma and Natalia Grossman both learned to climb there.) Davis was inspired by what Mayfield had created: â€œIt was visionary to open a gym in California at the timeâ€”but that is really hard to imagine now. From a dream to a nationwide industry.â€\nBut Davis recalls that perhaps the most important thing City Rock did for the climbing gym industry was establish safety standards to manage risk, including legitimate belay tests. â€œIt could easily have been a dirtbag climbersâ€™ training hangout,â€ Davis explains. â€œBut they pulled off something that demonstrated the true value of climbing: risk taking with the correct skillset, and on a level that a discerning public could understand.â€\nIn 1992, Mayfield and Casey Newman, who ran the Colorado Rock Gym, along with several others, founded the Climbing Gym Association, which has since become the Climbing Wall Association. The purpose of this organization was to regulate the burgeoning climbing gym industry, hopefully keeping the government from stepping in with its own set of regulations. They built a gym accreditation program and traveled around the country accrediting the dozens of climbing gyms that existed by that time in the U.S. But thatâ€™s a story for another time.\nWhat ever happened to the original City Rockâ€”and Mayfield?\nSo how did the original design of City Rock hold up? Jim Collins, a notable climber, bestselling author, and business guru who taught MBA students at Stanford University, studied City Rock as a class project. The academic analysis found that City Rock was â€œgoing to get blown out of the water by the next wave of climbing gyms,â€ in Mayfieldâ€™s words. Collins and his students had two major reasons to back up their conclusion. The first was that City Rockâ€™s walls werenâ€™t steep enough. And the second reason was that they hypothesized other entrepreneurs would come along and operate gyms more efficiently at a lower cost.\nMayfield explains the limited steepness of his gymâ€™s original walls: â€œ[City Rock] was designed and built before America had super steep climbing. It was before there was a Rifle. It was before there was a Jailhouse Rock. Two or three years later, the next wave of gyms did have arches and bigger, steeper, upside-down climbing structures because the cliffs that people were climbing on were like that. The hard climbing in 1989 was like Smith Rock. Dead vertical or maybe a bit past vertical, like 100 degrees.â€\nIn 1997, Mayfield sold City Rock to Mark Melvin, who founded Mission Cliffs, now Touchstone Climbing. He then moved back to Yosemite, where he started an eco-tourism business to educate people about nature and conservation in the Valley.\nWhile the decision to sell the gym he poured his heart and soul into for the better part of 10 years was difficult, he made some money and his investors did well trading up into the Touchstone world. â€œI couldnâ€™t care less about making money. I am just not wired that way to manipulate the world to make money.â€ He says that in retrospect, he was â€œfaking itâ€ as a business person and finds himself better adapted to social entrepreneurship.\nAfter the sale, City Rock fell victim to the skyrocketing rents associated with the Internet boom in the Bay Area. When rent tripled in the highly desirable Emeryville area, it was time to look for another location. The new owners of City Rock honored its memberships and opened Berkeley Ironworks a few miles away in 2000. It was three times the size of City Rock and also featured a large fitness area, a yoga studio, and an indoor cycling room. It represented the next generation of climbing gyms.\nNowadays, Mayfield lives in Truckee, CA, where he founded the nonprofit Gateway Mountain Center, which focuses on nature-based therapy for kids. â€œEven to this day, I stay interested in the effect [of climbing gyms] on youth, especially high-need youth,â€ says Mayfield. He also works with kids from local drug treatment centers and juvenile detention camps. â€œI am watching these kids just light up,â€ he says. â€œThey face incredible challenges in life, but they get to climb. This is transformative for high-need youth.â€\nThirty-five years have passed since Mayfield first started City Rock. While his impacts on the climbing gym industry are clear, he also continues to abide by his personal commitment to benefit youth through climbing. Mayfield still climbs, but these days, prefers Berkeleyâ€™s iconic Indian Rock to a climbing gym.\nReflecting on his pioneering foray into climbing gyms, he says: â€œI had kind of a classic business school case study of being the pioneer who spends all the money on R&D to prove the concept, then gets blown out of the water by the next wave, the well-funded MBAs who come after.â€\n\n    Similar Reads\n\n          A Climbing Graphic Novel? Competitive Climbing Becomes Art in This New Book.\n\n          The Horrors of the Gym Belay Test\n\n          Want More From Your Climbing Gym? These 7 US Facilities Are Getting Creative.\n\n          Decolonizing Climbing in Malawi and Beyond\n\n      Tags\n\n        climbing gyms\n\n        Peter Mayfield\n\n        Recycle",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "cf48a834519f255b",
    "title": {
      "en": "Don't Be Afraid of Types",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://lmika.org/2025/03/18/dont-be-afraid-of-types.html",
    "score": 122,
    "by": "speckx",
    "time": 1742323980,
    "content": "I found that thereâ€™s a slight aversion to creating new types in the codebases I work in. I saw it during my early days while I was working in Java projects, and I see it today in the occasional Go project. Function bodies with lots of local variables, functions that take a large number of arguments or returning a large number of results, extensions to existing types rather than making new ones. Itâ€™s a strange phenomenon.\nI canâ€™t explain why this is. Maybe itâ€™s a fear of feeling like youâ€™re tampering with the â€œgrand designâ€ of the codebase. This is plausible as it was the feeling I had as a junior dev. Afraid to create new classes in Java thinking that Iâ€™m introducing a new concept to the project that others had to deal with going forward. _I can add all the verbs I want, but who am _ I to introduce a new noun?\nThis is obviously a ridiculous notion when you think about it for more than a few seconds. If you come up with a concept or a series of values that naturally go together, so much so that youâ€™re carrying them together as a series of arguments through multiple function calls, itâ€™s probably in your interested to make a type for it. Thatâ€™s what the type system is for: a means of grouping similar bits of information into an easy-to-use whole.\nThis makes total sense for the application models: the entities to which youâ€™re softwareâ€™s reason for being hinges on. But Iâ€™ve found it useful to make types for the lesser bits of information: requests from handlers passed through to the service layer, for instance. Just now, Iâ€™m working on some code that deals with creating subscriptions. I need to carry the office ID, customer ID, price ID, the subscription quantity, the tax settings, and the subscription metadata from the API handler all the way through to the Stripe client. This is less than what the subscription model deals with, but itâ€™s still a pain to carry these six bits of information separately through the unmarshalling logic, the validation logic, and then through to the server.\nSo what did I do? I made a â€œCreateSubscriptionRequestâ€ struct, a new type. Yes, itâ€™s not going to be reusable, but who cares? It makes the code and my life simpler. And honestly, I think the whole â€œobject-orientated approachâ€ to software design really screwed up our thinking here. There was this feeling in the zeitgeist that types and classes are sacred, and that to create a new one is a privilege bestowed only to the leads, architects, and anyone else that had write access to the UML diagrams. Each type was to be an artefact of design, probably because of how much baggage came from defining a new one: they had to be in a separate file, must have seven different constructors, and the fields must be mediated through the use of getters and setters. And if you need something similar to what youâ€™re working on, you didnâ€™t â€œcopy-and-pasteâ€ like some animal; you inherited or composed what was there. Given all this, itâ€™s probably understandable that creating new types felt like a decision with a significant bit of â€œweightâ€; and who are you, mere lowly junior developer, to make such a decision to create a type just to make it easier to handle data from your handler?\nI think the culture around C and Go have got it right. Need to carry a few things for a single function? Create a new type. Donâ€™t worry that itâ€™s used only for a single function. Donâ€™t worry that it only contains a subset of fields of the model youâ€™re operating on.1\nNow obviously itâ€™s possible to go too far, and start having way too many types than is necessary. Donâ€™t forget that a new type is a bit more cognitive load, as the person maintaining you application will now need to unpack and reference your type when they need to work on it. Just stick with what you need, and make it clear what the purpose of the type is. â€œCreateSubscriptionRequestâ€ makes it plan that this type only deals with the areas of a code that creates subscriptions, and will probably only make sense through those code paths.\nBut take it from someone thatâ€™s had do deal with codes passing through and returning several values of strings, ints, and bools through a series of function calls: a single struct value is much easier to work with. All it takes is the courage for someone to say â€œyes, that should be a type.â€\nDonâ€™t be afraid for that someone to be you.\n\nIn fact, that might actually better than using the model type and adding â€œthis field is ignored, that field must be zero, etc. etc.â€ in the function docs.â†©ï¸",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "d2395062a202fc9f",
    "title": {
      "en": "The HTML Review",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://thehtml.review/04/",
    "score": 10,
    "by": "bookofjoe",
    "time": 1742731531,
    "content": "ASCII bedroom archive\n\n                    `'::.\n    _________H ,%%&%,\n   /\\     _   \\%&&%%&%\n  /  \\___/^\\___\\%&%%&&\n  |  | []   [] |%\\Y&%'\n  |  |   .-.   | ||\n ~~@._|@@_|||_@@|~||~~~~~~~~~~~~~\n     `\"\"\") )\"\"\"`     welcome home\n\n                Eileen Ahn",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "bfd23bb5d8d8073e",
    "title": {
      "en": "Tencent's 'Hunyuan-T1'â€“The First Mamba-Powered Ultra-Large Model",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://llm.hunyuan.tencent.com/#/blog/hy-t1?lang=en",
    "score": 288,
    "by": "marban",
    "time": 1742664332,
    "content": "Introduction\n            Reinforcement learning has pioneered a new Scaling paradigm in the post-training phase of large language models, a breakthrough that is increasingly attracting attention from the industry. With the successive release of OpenAI's O-series models and DeepSeek R1, the excellent performance demonstrated by the models fully proves the crucial role of reinforcement learning in the optimization process.\n\n            In mid-February this year, the Hunyuan team launched the Hunyuan T1-Preview (Hunyuan-Thinker-1-Preview) reasoning model based on the medium-scale Hunyuan base on the Tencent Yuanbao APP, bringing users an ultimate and rapid in-depth thinking experience.\n\n            Today, we are very pleased to announce that the in-depth thinking model of the Hunyuan large model series has been successfully upgraded to the Hunyuan-T1 official version. This model is based on the TurboS fast-thinking base, the world's first ultra-large-scale Hybrid-Transformer-Mamba MoE large model released by us at the beginning of March. Through large-scale post-training, its reasoning ability has been significantly expanded and further aligned with human preferences.\n\n            Compared with the previous T1-preview model, Hunyuan-T1 has shown a significant overall performance improvement and is a leading cutting-edge strong reasoning large model in the industry.\n\n            Based on TurboS, T1 shows unique advantages in the direction of in-depth reasoning. TurboS's long-text capture ability helps Turbo-S effectively solve the problems of context loss and long-distance information dependence often encountered in long-text reasoning. Secondly, its Mamba architecture specifically optimizes the processing ability of long sequences. Through an efficient computing method, it can ensure the ability to capture long-text information while significantly reducing the consumption of computing resources. Under the same deployment conditions, the decoding speed is 2 times faster.\n\n            In the post-training phase of the model, we invested 96.7% of our computing power in reinforcement learning training, focusing on improving pure reasoning ability and optimizing alignment with human preferences.\n\n            We collected world science and reasoning problems, covering mathematics/logic reasoning/science/code, etc. These data sets cover everything from basic mathematical reasoning to complex scientific problem solving. Combined with ground-truth real feedback, we ensure that the model can demonstrate excellent capabilities when facing various reasoning tasks.\n\n            In terms of training plans, we adopted a curriculum learning approach to gradually increase data difficulty while expanding the model's context length in a step-by-step manner, enabling the model to improve its reasoning ability while learning to use tokens efficiently for reasoning.\n\n            Regarding the training strategy, we referred to classic reinforcement learning strategies such as data replay and periodic policy resetting, which significantly improved the long-term stability of model training by over 50%. During the alignment with human preferences phase, we adopted a unified reward system feedback scheme of self-rewarding (based on an early version of T1-preview to comprehensively evaluate and score the model's output) + reward mode, guiding the model to self-improve. The model shows richer content details and more efficient information in its responses.\n\n            In addition to achieving comparable or slightly better results than R1 on various public benchmarks such as MMLU-pro, CEval, AIME, Zebra Logic, and other Chinese and English knowledge and competition-level math and logical reasoning indicators, Hunyuan-T1 also performs on par with R1 in internal human evaluation datasets. It has a slight edge in cultural and creative instruction following, text summarization, and agent capabilities.\n\n            From the perspective of comprehensive evaluation metrics, the overall performance of Hunyuan-T1 can be on a par with first-class cutting-edge inference models.\n            In terms of comprehensive ability evaluation, T1 ranks second only to O1 on MMLU-PRO, with a high score of 87.2. This test set covers questions from 14 fields such as humanities,\n            social sciences, and science and engineering, mainly testing the model's memory and understanding of extensive knowledge.\n            Additionally, there is GPQA-diamond, which focuses on professional domain knowledge and complex scientific reasoning, mainly including doctoral-level difficult problems in physics, chemistry, and biology. T1 achieved a score of 69.3.\n\n            In the field of science and engineering, we tested scenarios that require strong reasoning abilities, such as coding, mathematics, and logical reasoning.\n            In the code evaluation of LiveCodeBench, T1 reached a score of 64.9. Meanwhile, T1 also performs excellently in mathematics.\n            Especially on MATH-500, it achieved an excellent score of 96.2, closely following DeepSeek R1, demonstrating T1's comprehensive ability in solving math problems.\n            Besides, T1 has shown very strong adaptability in multiple alignment tasks, instruction-following tasks, and tool utilization tasks. For example, T1 achieved a score of 91.9 in the ArenaHard task.\n           Model Effect   Note: The evaluation metrics of other models in the table are from official evaluation results. For the parts not included in the official evaluation results, they are from the results of the Hunyuan internal evaluation platform.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "60e5facc16b537eb",
    "title": {
      "en": "I want a good parallel computer",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://raphlinus.github.io/gpu/2025/03/21/good-parallel-computer.html",
    "score": 221,
    "by": "raphlinus",
    "time": 1742586942,
    "content": "I want a good parallel computer\n\n      Mar 21, 2025\n\n    The GPU in your computer is about 10 to 100 times more powerful than the CPU, depending on workload. For real-time graphics rendering and machine learning, you are enjoying that power, and doing those workloads on a CPU is not viable. Why arenâ€™t we exploiting that power for other workloads? What prevents a GPU from being a more general purpose computer?\n\nI believe there are two main things holding it back. One is an impoverished execution model, which makes certain tasks difficult or impossible to do efficiently; GPUs excel at big blocks of data with predictable shape, such as dense matrix multiplication, but struggle when the workload is dynamic. Second, our languages and tools are inadequate. Programming a parallel computer is just a lot harder.\n\nModern GPUs are also extremely complex, and getting more so rapidly. New features such as mesh shaders and work graphs are two steps forward one step back; for each new capability there is a basic task that isnâ€™t fully supported.\n\nI believe a simpler, more powerful parallel computer is possible, and that there are signs in the historical record. In a slightly alternate universe, we would have those computers now, and be doing the work of designing algorithms and writing programs to run well on them, for a very broad range of tasks.\n\nLast April, I gave a colloquium (video) at the UCSC CSE program with the same title. This blog is a companion to that.\n\nMemory efficiency of sophisticated GPU programs\n\nIâ€™ve been working on Vello, an advanced 2D vector graphics renderer, for many years. The CPU uploads a scene description in a simplified binary SVG-like format, then the compute shaders take care of the rest, producing a 2D rendered image at the end. The compute shaders parse tree structures, do advanced computational geometry for stroke expansion, and sorting-like algorithms for binning. Internally, itâ€™s essentially a simple compiler, producing a separate optimized byte-code like program for each 16x16 pixel tile, then interpreting those programs. What it cannot do, a problem I am increasingly frustrated by, is run in bounded memory. Each stage produces intermediate data structures, and the number and size of these structures depends on the input in an unpredictable way. For example, changing a single transform in the encoded scene can result in profoundly different rendering plans.\n\nThe problem is that the buffers for the intermediate results need to be allocated (under CPU control) before kicking off the pipeline. There are a number of imperfect potential solutions. We could estimate memory requirements on the CPU before starting a render, but thatâ€™s expensive and may not be precise, resulting either in failure or waste. We could try a render, detect failure, and retry if buffers were exceeded, but doing readback from GPU to CPU is a big performance problem, and creates a significant architectural burden on other engines weâ€™d interface with.\n\nThe details of the specific problem are interesting but beyond the scope of this blog post. The interested reader is directed to the Potato design document, which explores the question of how far you can get doing scheduling on CPU, respecting bounded GPU resources, while using the GPU for actual pixel wrangling. It also touches on several more recent extensions to the standard GPU execution model, all of which are complex and non-portable, and none of which quite seem to solve the problem.\n\nFundamentally, it shouldnâ€™t be necessary to allocate large buffers to store intermediate results. Since they will be consumed by downstream stages, itâ€™s far more efficient to put them in queues, sized large enough to keep enough items in flight to exploit available parallelism. Many GPU operations internally work as queues (the standard vertex shader / fragment shader / rasterop pipeline being the classic example), so itâ€™s a question of exposing that underlying functionality to applications. The GRAMPS paper from 2009 suggests this direction, as did the Brook project, a predecessor to CUDA.\n\nThere are a lot of potential solutions to running Vello-like algorithms in bounded memory; most have a fatal flaw on hardware today. Itâ€™s interesting to speculate about changes that would unlock the capability. Itâ€™s worth emphasizing, Iâ€™m not feeling held back by the amount of parallelism I can exploit, as my approach of breaking the problem into variants of prefix sum easily scales to hundreds of thousands of threads. Rather, itâ€™s the inability to organize the overall as stages operating in parallel, connected through queues tuned to use only the amount of buffer memory needed to keep everything smoothly, as opposed to the compute shader execution model of large dispatches separated by pipeline barriers.\n\nParallel computers of the past\n\nThe lack of a good parallel computer today is especially frustrating because there were some promising designs in the past, which failed to catch on for various complex reasons, leaving us with overly complex and limited GPUs, and extremely limited, though efficient, AI accelerators.\n\nConnection Machine\n\nIâ€™m listing this not because itâ€™s a particularly promising design, but because it expressed the dream of a good parallel computer in the clearest way. The first Connection Machine shipped in 1985, and contained up to 64k processors, connected in a hypercube network. The number of individual threads is large even by todayâ€™s standards, though each individual processor was extremely underpowered.\n\nPerhaps more than anything else, the CM spurred tremendous research into parallel algorithms. The pioneering work by Blelloch on prefix sum was largely done on the Connection Machine, and I find early paper on sorting on CM-2 to be quite fascinating.\n\nConnection Machine 1 (1985) at KIT / Informatics / TECO â€¢ by KIT TECO â€¢ CC0\n\nCell\n\nAnother important pioneering parallel computer was Cell, which shipped as part of the PlayStation 3 in 2006. That device shipped in fairly good volume (about 87.4 million units), and had fascinating applications including high performance computing, but was a dead end; the Playstation 4 switched to a fairly vanilla rendering pipeline based on a Radeon GPU.\n\nProbably one of the biggest challenges in the Cell was the programming model. In the version shipped on the PS3, there were 8 parallel cores, each with 256kB of static RAM, and each with 128 bit wide vector SIMD. The programmer had to manually copy data into local SRAM, where a kernel would then do some computation. There was little or no support for high level programming; thus people wanting to target this platform had to painstakingly architect and implement parallel algorithms.\n\nAll that said, the Cell basically met my requirements for a â€œgood parallel computer.â€ The individual cores could run effectively arbitrary programs, and there was a global job queue.\n\nThe Cell had approximately 200 GFLOPS of total throughput, which was impressive at the time, but pales in comparison to modern GPUs or even a modern CPU (Intel i9-13900K is approximately 850 GFLOPS, with a medium-high end Ryzen 7 is 3379 GFLOPS).\n\nLarrabee\n\nPerhaps the most poignant road not taken in the history of GPU design is Larrabee. The 2008 SIGGRAPH paper makes a compelling case, but ultimately the project failed. Itâ€™s hard to say why exactly, but I think itâ€™s possible it was just poor execution on Intelâ€™s part, and with more persistence and a couple of iterations to improve the shortcomings in the original version, it might well have succeeded. At heart, Larrabee is a standard x86 computer with wide (512 bit) SIMD units and just a bit of special hardware to optimize graphics tasks. Most graphics functions are implemented in software. If it had succeeded, it would very easily fulfill my wishes; work creation and queuing is done in software and can be entirely dynamic at a fine level of granularity.\n\nBits of Larrabee live on. The upcoming AVX10 instruction set is an evolution of Larrabeeâ€™s AVX-512, and supports 32 lanes of f16 operations. In fact, Tom Forsyth, one of its creators, argues that Larrabee did not indeed fail but that its legacy is a success. It did ship in modest volumes as Xeon Phi. Another valuable facet of legacy is ISPC, and Matt Pharrâ€™s blog on The story of ispc sheds light on the Larrabee project.\n\nLikely one of the problems of Larrabee was power consumption, which has emerged as one of the limiting factors in parallel computer performance. The fully coherent (total store order) memory hierarchy, while making software easier, also added to the cost of the system, and since then weâ€™ve gained a lot of knowledge in how to write performant software in weaker memory models.\n\nAnother aspect that definitely held Larrabee back was the software, which is always challenging, especially for innovative directions. The drivers didnâ€™t expose the special capabilities of the highly programmable hardware, and performance on traditional triangle-based 3D graphics scenes was underwhelming. Even so, it did quite well on CAD workloads involving lots of antialiased lines, driven by a standard OpenGL interface.\n\nThe changing workload\n\nEven within games, compute is becoming a much larger fraction of the total workload (for AI, itâ€™s everything). Analysis of Starfield by Chips and Cheese shows that about half the time is in compute. The Nanite renderer also uses compute even for rasterization of small triangles, as hardware is only more efficient for triangles above a certain size. As games do more image filtering, global illumination, and primitives such as Gaussian splatting, the trend will almost certainly continue.\n\nIn 2009, Tim Sweeney gave a thought-provoking talk entitled The end of the GPU roadmap, in which he proposed that the concept of GPU would go away entirely, replaced by a highly parallel general purpose computer. That has not come to pass, though there has been some movement in that direction: the Larrabee project (as described above), the groundbreaking cudaraster paper from 2011 implemented the traditional 3D rasterization pipeline entirely in compute, and found (simplifying quite a bit) that it was about 2x slower than using fixed function hardware, and more recent academic GPU designs based on grids of RISC-V cores. Itâ€™s worth noting, a more recent update from Tellusim suggests that cudaraster-like rendering in compute can be close to parity on modern hardware.\n\nAn excellent 2017 presentation, Future Directions for Compute-for-Graphics by Andrew Lauritzen, highlighted many of the challenges of incorporating advanced compute techniques into graphics pipelines. Thereâ€™s been some progress since then, but it speaks to many of the same problems Iâ€™m raising in this blog post. Also see comments by Josh Barczak, which also links the GRAMPS work and discusses issues with language support.\n\nPaths forward\n\nI can see a few ways to get from the current state to a good parallel computer. Each basically picks a starting point that might have been on the right track but got derailed.\n\nBig grid of cores: Cell reborn\n\nThe original promise of the Cell still has some appeal. A modern high end CPU chip has north of 100 billion transistors, while a reasonably competent RISC CPU can be made with orders of magnitude fewer. Why not place hundreds or even thousands of cores on a chip? For maximum throughput, put a vector (SIMD) unit on each core. Indeed, there are at least two AI accelerator chips based on this idea: Esperanto and Tenstorrent. Iâ€™m particularly interested in the latter because its software stack is open source.\n\nThat said, there are most definitely challenges. A CPU by itself isnâ€™t enough, it also needs high bandwidth local memory and communications with other cores. One reason the Cell was so hard to program is that the local memory was small and needed to be managed explicitly - your program needed to do explicit transfers through the network to get data in and out. The trend in CPU (and GPU) design is to virtualize everything, so that thereâ€™s an abstraction of a big pool of memory that all the cores share. Youâ€™ll still want to make your algorithm cache-aware for performance, but if not, the program will still run. Itâ€™s possible a sufficiently smart compiler can adapt a high-level description of the problem to the actual hardware (and this is the approach taken by Tenstorrentâ€™s TT-Buda stack, specialized to AI workloads). In analogy to exploiting instruction-level parallelism through VLIW, the Itanium stands as a cautionary tale.\n\nFrom my read of the Tenstorrent docs, the matrix unit is limited to just matrix multiplication and a few supporting operations such as transpose, so itâ€™s not clear it would be a significant speedup for complex algorithms as needed in 2D rendering. But I think itâ€™s worth exploring, to see how far it can be pushed, and perhaps whether practical extensions to the matrix unit to support permutations and so on would unlock more algorithms.\n\nMost of the â€œbig grid of coresâ€ designs are targeted toward AI acceleration, and for good reason: it is hungry for raw throughput with low power costs, so alternatives to traditional CPU approaches are appealing. See the New Silicon for Supercomputers talk by Ian Cutress for a great survey of the field.\n\nRunning Vulkan commands from GPU-side\n\nA relatively small delta to existing GPUs would be the ability to dispatch work from a controller mounted on the GPU and sharing address space with the shaders. In its most general form, users would be able to run threads on this controller that could run the full graphics API (for example, Vulkan). The programming model could be similar to now, just that the thread submitting work is running close to the compute units and therefore has dramatically lower latency.\n\nIn their earliest form, GPUâ€™s were not distributed systems, they were co-processors, tightly coupled to the host CPUâ€™s instruction stream. These days, work is issued to the GPU by the equivalent of async remote procedure calls, with end-to-end latency often as high as 100Âµs. This proposal essentially calls for a return to less of a distributed system model, where work can efficiently be issued on a much finer grain and with much more responsiveness to the data. For dynamic work creation, latency is the most important blocker.\n\nNote that GPU APIs are slowly inventing a more complex, more limited version of this anyway. While itâ€™s not possible to run the Vulkan API directly from a shader, with a recent Vulkan extension (VK_EXT_device_generated_commands) it is possible to encode some commands into a command buffer from a shader. Metal has this capability as well (see gpuweb#431 for more details about portability). Itâ€™s worth noting that the ability to run indirect commands to recursively generate more work is one of the missing functions; it seems that the designers did not take Hofstadter to heart.\n\nIt is interesting to contemplate actually running Vulkan API directly from a shader. Since the Vulkan API is expressed in terms of C, one of the requirements is the ability to run C. This is being done on an experimental basis (see the vcc project), but is not yet practical. Of course, CUDA can run C. CUDA 12.4 also has support for conditional nodes, and as of 12.0 it had support for device graph launch, which reduces latency considerably.\n\nWork graphs\n\nWork graphs are a recent new extension to the GPU execution model. Briefly, the program is structured as a graph of nodes (kernel programs) and edges (queues) all running in parallel. As a node generates output, filling its output queues, the GPU dispatches kernels (at workgroup granularity) to process those outputs further. To a large extent, this is a modern reinvention of the GRAMPS idea.\n\nWhile exciting, and very likely useful for an increasing range of graphics tasks, work graphs also have serious limitations; I researched whether I could use them for the existing Vello design and found three major problems. First, they cannot easily express joins, where progress of a node is dependent on synchronized input from two different queues. Vello uses joins extensively, for example one kernel to compute a bounding box of a draw object (aggregating multiple path segments), and another to process the geometry within that bounding box. Second, there is no ordering guarantee between the elements pushed into a queue, and 2D graphics ultimately does require ordering (the whiskers of the tiger must be drawn over the tigerâ€™s face). Third, work graphs donâ€™t support variable-size elements.\n\nThe lack of an ordering guarantee is particularly frustrating, because the traditional 3D pipeline does maintain ordering, among other reasons, to prevent Z-fighting artifacts (for a fascinating discussion of how GPU hardware preserves the blend order guarantee, see A trip through the Graphics Pipeline part 9). It is not possible to faithfully emulate the traditional vertex/fragment pipeline using the new capability. Obviously, maintaining ordering guarantees in parallel systems is expensive, but ideally there is a way to opt in when needed, or at least couple work graphs with another mechanism (some form of sorting, which is possible to implement efficiently on GPUs) to re-establish ordering as needed. Thus, I see work graphs as two steps forward, one step back.\n\nCPU convergent evolution\n\nIn theory, when running highly parallel workloads, a traditional multi-core CPU design is doing the same thing as a GPU, and if fully optimized for efficiency, should be competitive. That, arguably, is the design brief for Larrabee, and also motivation for more recent academic work like Vortex. Probably the biggest challenge is power efficiency. As a general trend, CPU designs are diverging into those optimizing single-core performance (performance cores) and those optimizing power efficiency (efficiency cores), with cores of both types commonly present on the same chip. As E-cores become more prevalent, algorithms designed to exploit parallelism at scale may start winning, incentivizing provision of even larger numbers of increasingly efficient cores, even if underpowered for single-threaded tasks.\n\nAn advantage of this approach is that it doesnâ€™t change the execution model, so existing languages and tools can still be used. Unfortunately, most existing languages are poor at expressing and exploiting parallelism at both the SIMD and thread level â€“ shaders have a more limited execution model but at least itâ€™s clear how to execute them in parallel efficiently. And for thread-level parallelism, avoiding performance loss from context switches is challenging. Hopefully, newer languages such as Mojo will help, and potentially can be adapted to GPU-like execution models as well.\n\nIâ€™m skeptical this approach will actually become competitive with GPUs and AI accelerators, as there is just a huge gap in throughput per watt compared with GPUs â€“ about an order of magnitude. Also, GPUs and AI accelerators wonâ€™t be standing still either.\n\nMaybe the hardware is already there?\n\nItâ€™s possible that there is hardware currently shipping that meets my criteria for a good parallel computer, but its potential is held back by software. GPUs generally have a â€œcommand processorâ€ onboard, which, in cooperation with the host-side driver, breaks down the rendering and compute commands into chunks to be run by the actual execution units. Invariably, this command processor is hidden and cannot run user code. Opening that up could be quite interesting. A taste of that is in Hans-Kristian Arntzenâ€™s notes on implementing work graphs in open source drivers: Workgraphs in vkd3d-proton.\n\nGPU designs vary in how much is baked into the hardware and how much is done by a command processor. Programmability is a good way to make things more flexible. The main limiting factor is the secrecy around such designs. Even in GPUs with open source drivers, the firmware (which is what runs on the command processor) is very locked down. Of course, a related challenge is security; opening up the command processor to user code increases the vulnerability surface area considerably. But from a research perspective, it should be interesting to explore whatâ€™s possible aside from security concerns.\n\nAnother interesting direction is the rise of â€œAccelerated Processing Unitsâ€ which integrate GPUs and powerful CPUs in the same address space. Conceptually, these are similar to integrated graphics chips, but those rarely have enough performance to be interesting. From what Iâ€™ve seen, running existing APIs on such hardware (Vulkan for compute shaders, or one of the modern variants of OpenCL) would not have significant latency advantages for synchronizing work back to the CPU, due to context switching overhead. Itâ€™s possible a high priority or dedicated thread might quickly process items placed in a queue by GPU-side tasks. The key idea is queues running at full throughput, rather than async remote procedure calls with potentially huge latency.\n\nComplexity\n\nTaking a step back, one of the main features of the GPU ecosystem is a dizzying level of complexity. Thereâ€™s the core parallel computer, then lots of special function hardware (and the scope of this is increasing, especially with newer features such as ray tracing), then big clunky mechanisms to get work scheduled and run. Those start with the basic compute shader dispatch mechanism (a 3D grid of x, y, z dimensions, 16 bits each), and then augment that with various indirect command encoding extensions.\n\nWork graphs also fit into the category of complexifying the execution model to work around the limitations of the primitive 3D grid. I was initially excited about their prospect, but when I took a closer look, I found they were inadequate for expressing any of the producer/consumer relationships in Vello.\n\nThereâ€™s a lot of accidental complexity as well. There are multiple competing APIs, each with subtly different semantics, which makes it especially hard to write code once and have it just work.\n\nCUDA is adding lots of new features, some of which improve autonomy as Iâ€™ve been wanting, and there is a tendency for graphics APIs to adopt features from CUDA. However, thereâ€™s also a lot of divergence between these ecosystems (work graphs canâ€™t be readily adapted to CUDA, and itâ€™s very unlikely graphics shaders will get independent thread scheduling any time soon).\n\nThe complexity of the GPU ecosystem has many downstream effects. Drivers and shader compilers are buggy and insecure, and there is probably no path to really fixing that. Core APIs tend to be very limited in functionality and performance, so thereâ€™s a dazzling array of extensions that need to be detected at runtime, and the most appropriate permutation selected. This in turn makes it far more likely to run into bugs that appear only with specific combinations of features, or on particular hardware.\n\nAll this is in fairly stark contrast to the CPU world. A modern CPU is also dazzlingly complex, with billions of transistors, but it is rooted in a much simpler computational model. From a programmer perspective, writing code for a 25 billion transistor Apple M3 isnâ€™t that different from, say, a Cortex M0, which can be made with about 48,000 transistors. Similarly, a low performance RISC-V implementation is a reasonable student project. Obviously the M3 is doing a lot more with branch prediction, superscalar issue, memory hierarchies, op fusion, and other performance tricks, but itâ€™s recognizably doing the same thing as a vastly smaller and simpler chip.\n\nIn the past, there were economic pressures towards replacing special-purpose circuitry with general purpose compute performance, but those incentives are shifting. Basically, if youâ€™re optimizing for number of transistors, then somewhat less efficient general purpose compute can be kept busy almost all the time, while special purpose hardware is only justified if there is high enough utilization in the workload. However, as Dennard scaling has ended and weâ€™re more constrained by power than transistor count, special purpose hardware starts winning more; it can simply be powered down if it isnâ€™t used by the workload. The days of a purely RISC computational model are probably over. What Iâ€™d like to see replacing it is an agile core (likely RISC-V) serving as the control function for a bunch of special-purpose accelerator extensions. That certainly is the model of the Vortex project among others.\n\nConclusion\n\nIn his talk shortly before retirement, Nvidia GPU architect Erik Lindholm said (in the context of work creation and queuing systems), â€œmy career has been about making things more flexible, more programmable. Itâ€™s not finished yet. Thereâ€™s one more step that I feel that needs to be done, and Iâ€™ve been pursuing this at Nvidia Research for many years.â€ I agree, and my own work would benefit greatly. Now that he has retired, it is not clear who will take up the mantle. It may be Nvidia disrupting their previous product line with a new approach as they have in the past. It may be an upstart AI accelerator making a huge grid of low power processors with vector units, that just happens to be programmable. It might be CPU efficiency cores evolving to become so efficient they compete with GPUs.\n\nOr it might not happen at all. On the current trajectory, GPUs will squeeze out incremental improvements on existing graphics workloads at the cost of increasing complexity, and AI accelerators will focus on improving the throughput of slop generation to the exclusion of everything else.\n\nIn any case, there is an opportunity for intellectually curious people to explore the alternate universe in which the good parallel computer exists; architectures can be simulated on FPGA like Vortex, and algorithms can be prototyped on multicore wide-SIMD CPUs. We can also start to think about what a proper programming language for such a machine might look like, as frustrating as it is to not have real hardware to run it on.\n\nProgress on a good parallel computer would help my own little sliver of work, trying to make a fully parallel 2D renderer with modest resource requirements. Iâ€™ve got to imagine it would in addition help AI efforts, potentially unlocking sparse techniques that canâ€™t run on existing hardware. I also think thereâ€™s a golden era of algorithms that can be parallel but arenâ€™t a win on current GPUs, waiting to be developed.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "22d55cc5566c84c7",
    "title": {
      "en": "Show HN: A terminal emulator in pure PHP",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/soloterm/screen",
    "score": 189,
    "by": "aarondf",
    "time": 1742579005,
    "content": "Solo Screen\nScreen is a terminal renderer written in pure PHP. It powers Solo for Laravel and\ncan be used to build rich text-based user interfaces in any PHP application.\nNoteScreen is a library intended to be integrated into PHP applications. It is not a standalone terminal application.\n\nAbout terminal renderers\nA terminal renderer processes text and ANSI escape sequences to create a virtual representation of terminal output.\nUnlike a full terminal emulator, Screen focuses specifically on correctly interpreting and rendering text content with\nformatting rather than handling input, interactive sessions, or process management.\nTerminal renderers interpret escape sequences to:\n\nTrack cursor position\nApply text colors and styles (bold, underline, etc.)\nManage screen content\nHandle special character sets\nGenerate a final rendered output\n\nScreen implements this functionality in pure PHP, allowing developers to build terminal user interfaces without relying\non external dependencies or native code.\nWhy this exists\nScreen was originally created to solve a specific problem in Solo for Laravel.\nSolo provides a TUI (Text User Interface) that runs multiple processes simultaneously in separate panels, similar to\ntmux. However, when these processes output ANSI escape codes for cursor movement and screen manipulation, they could\npotentially \"break out\" of their visual containers and interfere with other parts of the interface.\nTo solve this problem, Screen creates a virtual terminal buffer where:\n\nAll ANSI operations (cursor movements, color changes, screen clears) are safely interpreted within an isolated\nenvironment\nThe final rendered state is captured after all operations are processed\nOnly the final visual output is displayed to the user's terminal\n\nThis approach provides complete control over how terminal output is rendered, ensuring that complex ANSI operations stay\ncontained within their designated areas. While initially built for Solo, Screen has evolved into a standalone library\nthat can be used in any PHP application requiring terminal rendering.\nFeatures\n\nPure PHP Implementation: Only one dependency (Grapheme, another Solo\nlibrary)\nComprehensive ANSI Support: Handles cursor positioning, text styling, and screen manipulation\nUnicode/Multibyte Support: Properly handles UTF-8 characters including emojis and wide characters\nBuffer Management: Maintains separate buffers for text content and styling\nCharacter Width Handling: Correctly calculates display width for CJK and other double-width characters\nScrolling: Support for vertical scrolling with proper content management\n\nInstallation\nInstall via Composer:\ncomposer require soloterm/screen\n\nRequirements\n\nPHP 8.1 or higher\nmbstring extension\n\nBasic usage\nHere's a simple example of using Screen:\nuse SoloTerm\\Screen\\Screen;\n\n// Create a screen with dimensions (columns, rows)\n$screen = new Screen(80, 24);\n\n// Write text and ANSI escape sequences\n$screen->write(\"Hello, \\e[1;32mWorld!\\e[0m\");\n\n// Move cursor and add more text\n$screen->write(\"\\e[5;10HPositioned text\");\n\n// Get the rendered content\necho $screen->output();\n\nCore concepts\nScreen operates with several key components:\nScreen\nThe main class that coordinates all functionality. It takes care of cursor positioning, content writing, and rendering\nthe final output.\n$screen = new Screen(80, 24); // width, height\n$screen->write(\"Text and ANSI codes\");\n\nBuffers\nScreen uses multiple buffer types to track content and styling:\n\nPrintableBuffer: Stores visible characters and handles width calculations\nAnsiBuffer: Tracks styling information (colors, bold, underline, etc.)\n\nANSI processing\nScreen correctly interprets ANSI escape sequences for:\n\nCursor movement (up, down, left, right, absolute positioning)\nText styling (colors, bold, italic, underline)\nScreen clearing and line manipulation\nScrolling\n\nAdvanced features\nCursor positioning\n// Move cursor to position (row 5, column 10)\n$screen->write(\"\\e[5;10H\");\n\n// Move cursor up 3 lines\n$screen->write(\"\\e[3A\");\n\n// Save and restore cursor position\n$screen->write(\"\\e7\"); // Save\n$screen->write(\"More text\");\n$screen->write(\"\\e8\"); // Restore\n\nText styling\n// Bold red text\n$screen->write(\"\\e[1;31mImportant message\\e[0m\");\n\n// Background colors\n$screen->write(\"\\e[44mBlue background\\e[0m\");\n\n// 256-color support\n$screen->write(\"\\e[38;5;208mOrange text\\e[0m\");\n\n// RGB colors\n$screen->write(\"\\e[38;2;255;100;0mCustom color\\e[0m\");\n\nScreen manipulation\n// Clear screen\n$screen->write(\"\\e[2J\");\n\n// Clear from cursor to end of line\n$screen->write(\"\\e[0K\");\n\n// Insert lines\n$screen->write(\"\\e[2L\");\n\n// Scroll up\n$screen->write(\"\\e[2S\");\n\nCustom integrations\nYou can respond to terminal queries by setting a callback:\n$screen->respondToQueriesVia(function($response) {\n    // Process response (like cursor position)\n    echo $response;\n});\n\nNoteThis is still a work in progress. We need some more tests / use cases here.\n\nExample: building a simple UI\nuse SoloTerm\\Screen\\Screen;\n\n$screen = new Screen(80, 24);\n\n// Draw a border\n$screen->write(\"â”Œ\" . str_repeat(\"â”€\", 78) . \"â”\\n\");\nfor ($i = 0; $i < 22; $i++) {\n    $screen->write(\"â”‚\" . str_repeat(\" \", 78) . \"â”‚\\n\");\n}\n$screen->write(\"â””\" . str_repeat(\"â”€\", 78) . \"â”˜\");\n\n// Add a title\n$screen->write(\"\\e[1;30H\\e[1;36mMy Application\\e[0m\");\n\n// Add some content\n$screen->write(\"\\e[5;5HWelcome to the application!\");\n$screen->write(\"\\e[7;5HPress 'q' to quit.\");\n\n// Render\necho $screen->output();\n\nHandling unicode and wide characters\nScreen properly handles Unicode characters including emoji and CJK characters that take up multiple columns:\n$screen->write(\"Regular text: Hello\");\n$screen->write(\"\\nWide characters: ä½ å¥½ä¸–ç•Œ\");\n$screen->write(\"\\nEmoji: ğŸš€ ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ ğŸŒ\");\n\nTesting\nScreen includes a comprehensive testing suite that features a unique visual comparison system:\ncomposer test\n\nVisual testing\nScreen employs an innovative screenshot-based testing approach (see ComparesVisually trait) that validates the visual\noutput:\n\nThe test renders content in a real terminal (iTerm)\nIt captures a screenshot of the terminal output\nIt runs the same content through the Screen renderer\nIt captures a screenshot of the rendered output\nIt compares the screenshots pixel-by-pixel to ensure accuracy\n\nThis testing strategy ensures that Screen's rendering accurately matches real terminal behavior, especially for complex\nscenarios involving:\n\nMulti-byte characters\nComplex ANSI formatting\nCursor movements\nScrolling behavior\nLine wrapping\n\nFor environments without screenshot capabilities, tests can fall back to fixture-based comparison, making the test suite\nversatile for CI/CD pipelines.\nTo enable screenshots for all tests, use the following command:\nENABLE_SCREENSHOT_TESTING=1 composer test\n\nTo enable screenshots for only the tests that don't already have fixtures, use the following command:\nENABLE_SCREENSHOT_TESTING=2 composer test\n\nContributing\nContributions are welcome! Please feel free to submit a pull request.\nLicense\nThe MIT License (MIT).\nSupport\nThis is free! If you want to support me:\n\nSponsor my open source work: aaronfrancis.com/backstage\nCheck out my courses:\n\nMastering Postgres\nHigh Performance SQLite\nScreencasting\n\nHelp spread the word about things I make\n\nCredits\nSolo Screen was developed by Aaron Francis. If you like it, please let me know!\n\nTwitter: https://twitter.com/aarondfrancis\nWebsite: https://aaronfrancis.com\nYouTube: https://youtube.com/@aarondfrancis\nGitHub: https://github.com/aarondfrancis",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "0aeb24033dd0cbba",
    "title": {
      "en": "Scientists program stem cells to mimic first days of embryonic development",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://news.ucsc.edu/2025/03/shariati-cellstemcell-25.html",
    "score": 37,
    "by": "StemCells",
    "time": 1742521519,
    "content": "UC Santa Cruz\n\nMyUCSC\nPeople\nCalendars\nMaps\nA-Z Index\n\n Search\n\n          Newscenter\n\nAwards & Honors\nEvents\nVideos\nSubscribe\nContact Us\nHelpful Links\n\nNews by topic\nNews archives\nE-Newsletter\nTuesday Newsday\nRSS feeds\nUCSC Magazine\nAdministrative Messages\n\n        Home / 2025 / March / Scientists program stem cells to mimic first days of embryonic development\n            Scientists program stem cells to mimic first days of embryonic developmentMarch 20, 2025\n            By\n            Emily CerfShare this story:TwitterFacebookLinkedInRedditProgrammable cellular models of embryos, known as embryoids, allow scientists to mimic the first few day of embryonic development.The project team, left to right: Ali Shariati,Sayaka Kozuki, andGerrald Lodewijk.The earliest days after fertilization, once a sperm cell meets an egg, are shrouded in scientific mystery.The process of how a humble single cell becomes an organism fascinates scientists across disciplines. For some animals, the entire process of cellular multiplication, generation of specialized cells, and their organization into an ordered multicellular embryo takes place in the protective environment of the uterus, making direct observation and studies challenging. This makes it difficult for scientists to understand what can go wrong during that process, and how specific risk factors and the surrounding environment may prevent embryo formation.Scientists at UC Santa Cruz were able to engineer cellular models of embryos without ever experimenting with any actual embryos, allowing them to mimic the first few days after two sexually reproductive cells meet. They use CRISPR-based engineering methods to prompt stem cells to organize into â€œprogrammableâ€ embryo-like structures, also known as embryoids, which can be used to study the role of certain genes in early development. These structures are not actual embryos but assemblies of lab-grown cells that self-organize in ways that mimic some aspects of early developmental stages. Their results are published in the leading stem cell journal Cell Stem Cell.â€œWe as scientists are interested in recreating and repurposing natural phenomena, such as formation of an embryo, in the dish to enable studies that are otherwise challenging to do with natural systems,â€ said Ali Shariati, assistant professor of biomolecular engineering and the studyâ€™s senior author. â€œWe want to know how cells organize themselves into an embryo-like model, and what could go wrong when there are pathological conditions that prevent an embryo from successfully developing.â€\nCell co-development\nShariati is an expert in stem cell engineering, a field that uses stem cells â€” unspecialized cells that can form any type of cell such as gut or brain cells â€” to study and solve biological and health problems.This project, led by UCSC postdoctoral scholar Gerrald Lodewijk and biomolecular engineering alumna and current Caltech graduate student Sayaka Kozuki, used mouse stem cells that are commonly grown in the lab to guide them to form basic building blocks of the embryo.The team used a version of CRISPR technology known as an epigenome editor, which does not cut DNA but instead modifies how it is expressed. They targeted regions of the genome known to be involved in the development of an early embryo. This allowed them to control which genes were activated, and induce the creation of main types of cells needed for early development.â€œWe use the stem cells, which are like a blank canvas, and use them to induce different cell types using our CRISPR tools,â€ Lodewijik said.This method had the advantage of allowing different cell types to â€œco-develop,â€ which more closely resembles the natural embryo formation than the chemical approaches other scientists have used to develop different cell types.â€œThese cells co-develop together, just like they would in an actual embryo, and establish that history of being neighbors,â€ Shariati said. â€œWe do not change their genome or expose them to specific signaling molecules, but rather activate the existing genes.â€The team found that 80% of the stem cells organize themselves into a structure that mimics the most basic form of an embryo after a few days, and most undergo gene activation that reflects the development process that occurs in living organisms.â€œThe similarity is remarkable in the way the cells organize themselves, as well as the molecular composition,â€ Shariati said. â€œ[The cells require] very little input from us â€” itâ€™s as if the cells already know what to do, and we just give them a little bit of guidance.â€The researchers observed that the cells showed a collective behavior in moving and organizing together.â€œSome of them start doing this rotational migration, almost like the collective behavior of birds or other species,â€ Shariati said. â€œThrough this collective behavior and migration they can form these fascinating embryonic patterns.â€\nâ€œProgrammableâ€ models\nHaving an accurate baseline model that reflects a living organismsâ€™ early embryo could allow scientists to better study and learn how to treat developmental disorders or mutations.â€œThese models have a more complete representation of whatâ€™s going on in early stages of development, and can capture the background,â€ Lodewijik said.The CRISPR programming not only allows the scientists to activate the genes at the beginning of the experimentation process, but also enables them to activate or modify genes important for other parts of development. This allows the embryo models to be â€œprogrammable,â€ meaning they can be relatively easily influenced with a high level of control to target and test the impact of multiple genes as the embryo model develops, illuminating which have deleterious effects when turned on or off.As an example, the researchers demonstrated how certain tissues form or are hindered during early development, but their methods could be used to study a wide range of genes and their cascading effects on the cell types.â€œI think this is the pioneering work of this study â€” the programmability and that we donâ€™t rely on extrinsic factors to do this, but rather have a lot of control inside the cell,â€ Shariati said.The researchers are interested in how this approach might be used to study other species, allowing for a look into their embryo formation without ever using their actual embryos.This research could allow for the study of the bottlenecks that lead reproduction to fail in early stages. Among mammals, humans have more reproduction challenges in that human embryos often fail to implant or establish the correct early organizational form. Understanding why this is the case could help make progress toward improving human fertility.Share this story:TwitterFacebookLinkedInReddit\n\n          Recent articlesMarch 20, 2025New exhibition examines the messier side of life through art and environmental researchMarch 19, 2025Long-accepted theory explaining the nature of our universe may need updating, new analysis indicatesMarch 18, 2025UCSC Dining Serves Up Success at Clam Chowder Cook-OffMarch 17, 2025Student Health Services Welcomes New Medical Leadership, Reaffirms Commitment to Student HealthcareMarch 18, 2025New lactation spaces open for nursing mothers on campusMarch 17, 2025Workshop aims to create speech AI for all\n\nUniversity News\n\nUniversity News Home\nMonthly Newsletter\n\nOther News & Events\n\nCampus Calendars\nUC News\n\nFeatures\n\n UC Santa Cruz Magazine\n\nResources\n\nAdministrative Messages\nUCSC Chancellor\nPress Releases\nContacts for Reporters\nSend us an email\n\nUC Santa Cruz, 1156 High Street, Santa Cruz, Ca 95064\nÂ©2025 Regents of the University of California. All Rights Reserved.\n\n        Feedback\n\nReport an accessibility barrier\nLand Acknowledgment\nEmployment\nPrivacy\nAccreditation\n\n    Last modified: March 21, 2025  175.196.243.166",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "8a18267be5bd00f8",
    "title": {
      "en": "North American spider species alters its webs to deal with urban noise pollution",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.nytimes.com/2025/03/22/science/spiders-webs-noise-pollution.html",
    "score": 18,
    "by": "perihelions",
    "time": 1742721103,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "39898832b7a562bd",
    "title": {
      "en": "Everything I learned from (finally) completing The Artist's Way",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://extraordinaryroutines.com/musings/the-artists-way-complete-guide",
    "score": 8,
    "by": "cf100clunk",
    "time": 1742752696,
    "content": "Everything I learned from (finally) completing The Artistâ€™s Way\n\n  Words by Madeleine DoreMaybe youâ€™ve heard people referring to the morning pages or talking about going on an artist date. Maybe youâ€™ve just started reading the first few chapters, or adopted some of its core practices into your daily routine. Maybe youâ€™ve abandoned it halfway through, or dipped in and out, or even completed the entire twelve-week course. For those not familiar, The Artistâ€™s Way is a seminal text for creative unblocking and was published in 1992 by Julia Cameron. It has sold over five million copies and has helped demystify the creative process and transform countless lives, from philosophers to illustrators and authors. In many of the hundreds of conversations Iâ€™ve had with creatives, Iâ€™ve asked or been asked the question, â€œHave you read The Artistâ€™s Way?â€ The answer that follows is often some iteration of, â€œI enjoyed some of the exercises, but have never finished the book,â€ or â€œitâ€™s been sitting on my shelf for years.â€\n\n    â€œâ€œIâ€™m completely changed, and simultaneously unchanged. That, to me, is the wonderful thing about it Â­â€“ change is imperfect, itâ€™s two steps forward and one step back, and often vice versa. When itâ€™s lasting, itâ€™s almost undetectable as your new self and your old self merge and stroll and stumble together.â€ â€\n\n  Five years ago, while on a break after finishing studies I bought the Kindle version and began the week by week tasks with gusto, filling three pages with stream-of-consciousness writing each morning and taking myself on artist dates. By week three it came to an abrupt stop. Itâ€™s difficult to determine why I didnâ€™t keep stumbling through â€“ perhaps it was feeling daunted by the â€œhomeworkâ€ or the religious undertones that didnâ€™t resonate. In the proceeding years, I would stop and start the habit of the morning pages, what Julia Cameron describes as the â€˜primary tool for creative recoveryâ€™.With a particularly rocky start to the year, I had decided to be more diligent with filling three pages of brain drain each morning, but hadnâ€™t yet picked up the text I put down five years ago. It wasnâ€™t until an afternoon spent reading by the pool with a dear friend that I spotted her copy of The Artistâ€™s Way and began to flip through the pages again. Snippets like â€œget out of the way, let it work through youâ€ and â€œyou have to stop telling yourself itâ€™s too lateâ€ were a balm for this particular period of stuckness I was experiencing. It was time. I moved into a new sublet in Melbourne and with the new environment came a clean slate to create new habits. I found a copy in a second-hand bookstore, the owner tapping on the cover saying â€œnow this, this will change your lifeâ€ while she put it through the cash register. Later that evening, I began from the beginning, the homework now viewed as a bedrock for a new routine, and the religious undertones swapped out for words like â€˜flowâ€™ and â€˜energyâ€™.Twelve weeks, a dozen artist dates and three Moleksine journals filled with morning pages later, I closed the final page of The Artistâ€™s Way and find myself in New York City. Iâ€™m completely changed, and simultaneously unchanged. That, to me, is the wonderful thing about it Â­â€“ change is imperfect, itâ€™s two steps forward and one step back, and often vice versa. When it's lasting, it's almost undetectable as your new self and your old self merge and stroll and stumble together.I learned what can only be learned through going inward, even if that means looking like youâ€™re doing nothing from the outside. Here are my week by week notes and learnings â€“ from listening to yourself to reframing lost opportunities â€“ from the book I encourage any individual feeling blocked to dip into. Lessons from making it all the way through The Artistâ€™s WayWeek 1: Progress not perfectionThe first time I began The Artistâ€™s Way it helped spark the idea for my labour of love, Extraordinary Routines. This second attempt is exactly five yearâ€™s after the first interview was published in 2014.So many of this weekâ€™s lessons resonate with what Iâ€™ve unearthed in the conversations Iâ€™ve had with creatives and own personal principles â€“ creativityâ€™s chief need is support and the key is to keep trying and experimenting. Iâ€™m stuck by the description of â€˜shadow artistsâ€™ â€“ those that are drawn to other artists, but are blocked artists themselves and â€œcaught between the dream of action and the feature of failure.â€Iâ€™ve spent been five years of inspecting how other people are creative and yet I havenâ€™t written that book I wanted to write, or launched that podcast I wanted to launch. I canâ€™t linger on that point for too long without spiralling. â€œVery often audacity, not talent, makes one person an artist and another a shadow artist â€“ hiding in the shadows, afraid to step out and expose the dream to the light, fearful that it will disintegrate to the torch.While Iâ€™ve possessed audacity in some areas â€“ launching an event series and my freelance writing for example â€“ I can see how a lack of it has let certain projects languish. I havenâ€™t been audacious enough â€“ I have failed. This is a belief, not a fact. â€œWe are the thinker of the thoughts, not the thoughts,â€ as Julia writes. We can get trapped in either/or thinking instead of taking things gently and slowly, and moving towards being unblocked. â€œNurturing is key. To recover, we need solitude, self-nurturing, self-intimacy.â€I take myself to Readings bookshop, to relish in the solitude and the self-intimacy of walking and thinking and looking and flicking through pages and ideas. Taking note of Juliaâ€™s words â€˜progress not perfectionâ€™, I make a list of my project timeline for the podcast and do a callout for test listeners on Instagram. Beginning feels good. Week 2: People can be blocks This week Iâ€™m confronted by feelings of envy, self-doubt and feeling like Iâ€™ve missed certain opportunities. â€œErratic is part of getting unstuck,â€ Julia writes. â€œDo not let self-doubt turn into self-sabotage.â€Unblocking also comes with big changes, and we see how our personal lives might contain traces of being stuck. We can attach ourselves to what Julia refers to as â€˜crazymakersâ€™, or people who we choose (perhaps subconsciously) to act as blocks for ourselves and our creative desires. Itâ€™s easier to obsess about someone who broke your heart or annoys you at work than to think about your own creative potential. This week, I opened the door to my â€˜crazymakerâ€™ who I had been caught thinking â€˜what if, what if, what ifâ€™ for a few months while my book proposal gathered dust. The door was swiftly shut again later that week and this romantic obsession had come to a final end just when I was reading this very chapter. The book describes this as synchronicity and a sign of unblocking and the syncing between the chapters and my everyday life happened time and time again as I read the book. The day after things ended between myself and my â€˜crazymakerâ€™ for the second time, I get a cold â€“ what Julia describes as a Kriya, or â€œthe bad case of the flu right after youâ€™ve broken up with your love. Itâ€™s the rotten head cold and bronchial cough that announces youâ€™ve abused your health to meet an unreachable work deadline.â€ There is no longer a fantasy of what could, would or should be with this person, no longer an addiction to the fantasy. It hurts, but the antidote to the pain of letting go is opening up to what delight there is in your life.â€œThe quality of life is in proportion, always to the capacity for delight,â€ writes Julia. â€œIn the exact now, we are all, always, alright.â€I now have autonomy with my time and a capacity for paying attention to something else, something delightful, something that I feel truly connected to. Maybe even that book proposal. Week 3: Anger and jealousy are a mapWhen you feel like you are sliding backwards, remember that growth occurs in spurts. We take one step forward and two steps back and thatâ€™s okay â€“ we cannot change through judgement but self-love. Emotions that might feel counterintuitive such as anger are actually a map. â€œIt tells us we canâ€™t get away with our old life any longer. It tells us that old life is dying,â€ writes Julia. What angers us can also guide us if it is acted upon, it can help us say no, speak up, or create in response to something that has been stirred within us â€“ anger is an invitation to take action. It also points us towards what we want to do, and as Julia Cameron writes, â€œthe how follows the what.â€We cannot obsess over how we will make something happen, we just need to say what we are doing, and take action. â€œAction has magic so simply begin,â€ writes Julia. This week, I record my first interview for the podcast and put together a pitch deck for a podcast producer. Week 4: â€œIf you want to work on your art, work on your lifeâ€ â€“ Anton ChekovIn order to shake a creative rut, we must puncture the denial and refuse to keep saying itâ€™s okay that we havenâ€™t attended to our most important work. Itâ€™s no longer okay that we have allowed for distraction, interruption, or being led by someone elseâ€™s agenda.This week was about tossing out the old and unworkable, and noticing changes in tastes, judgments and personal identity. For me, I found myself leaving a talk I wasnâ€™t enjoying when previously I would have continued to sit through out of politeness. I found that I surprised myself with my actions or with my responses in conversations, which is a sign of starting to shake the habits and trappings that can keep us stuck.â€œYou are no longer stuck but you cannot tell where you are going,â€ writes Julia. Week 5: open up to opportunity Often we experience doubt because we do not take ourselves seriously enough, writes Julia, but I also find that taking myself too seriously or things too personally can present a block. I can easily put things into the â€œimpossibleâ€ category, but as this week explored, often something is impossible only because we are looking too ahead or too narrowly at the things we desire. â€œVery often, when we cannot seem to find an adequate supply it is because we are insisting on a particular human source of supply,â€ writes Julia. Tasked with asking myself what next steps I am evading, I realised I had to make decision about a trip to New York City I had talked about since the beginning of the year. Money, timing, and fear all got in the way, but the next step was to decide, so I booked the flight. â€œFind the river and say yes to the flow,â€ writes Julia, and so the river will be NYC.Just like the bends of a river, things donâ€™t pay off in a linear fashion â€“ there is no neat sequence of events once youâ€™ve made a step, but the important thing is to become internally clear on dreams, desires and delights. Working out my dreams, desires and delights is still a work in progress, but from this weekâ€™s readings I know I need time, space and quiet to become clear on those â€“ and that often means saying no to others or to our own expectations and too-rigid plans. It can difficult to say no, especially to people we love, but itâ€™s an important practice. â€œMany recovering creatives sabotage themselves most frequently by making nice,â€ as Julia writes. We need to protect our solitude and flow when we are unblocking. â€œWe strive to be good, to be nice, to be helpful, to be un-selfish. We want to be generous, of service, of the world. But what we really want is to be left alone. When we canâ€™t get others to leave us alone, we eventually abandon ourselves. To others, we may look like weâ€™re there. We may act like weâ€™re there. But our true self has gone to ground.â€Week 6: Money anxiety and wasted timeThis week, I committed to writing a short article every day to test the idea of taking care of the quantity and letting an external force take care of the quality. Each time I sat down to write I took note of an internal objections and negative beliefs the subconscious mind blurts out. One reoccurring blurt was â€œI should be workingâ€ â€“ even though I was writing, it felt like I wasnâ€™t work because it wasnâ€™t paid work. It didnâ€™t feel like enough to simply write for my own sake, as if there has to be some other external recipient of the words rather than my own contentedness in writing them.â€œWhat we really want to do is what we are meant to do,â€ writes Julia. Why do I so often deny myself the luxury to do what it is I want to do? Often itâ€™s my anxiety around money that presents a block â€“ self-conscious about earning less than many of my peers means that even when I might have the time, space and quiet to write, there is a lingering shame or guilt in using that time when I really should be trying to make more money. While I might earn comparatively less, what we need is entirely subjective and Iâ€™m privileged to be able to cover my living expenses. As Julia writes, we â€˜deny ourselves the luxury of timeâ€™ â€“ money anxiety is the excuse I use to erode my spare time.Week 7: perfecting perfectionismWhile I love lists, I can also be entrapped by them. â€œArt is about getting something down, not planning,â€ writes Julia. Often planning for me is a symptom of perfectionism, which we can have false ideas about. â€œPerfectionism is not a quest for the best, but a pursuit of the worst in ourselves.â€This week, my artist date was to go to The Moth storytelling event solo, and I was in awe of how strangers got up on stage and told a story with no notes and full hearts. They were funny, touching, imperfect and the embodiment of the question in this chapter: â€œWhat would you do if you didnâ€™t have to do it perfectly?â€Often we wait for the perfect time, the perfect condition, the perfect state because we think it will lead to a guaranteed success or some sense of safety and security, but as the book explains, â€œsafety is a very expensive illusion.â€Waiting for perfect can lead to things like jealousy and envy, where we see others doing what we know we can but have kept a wish or pipe dream. As Julia Cameron, â€œBlocked artists deny success from ourselves and others.â€The antidote is to take a risk and focus on the doing. â€œA risk is worth taking simply for the sake of taking it.â€I open up the first test podcast and begin to stumble through the editing software I have no experience with. I judge it as bad, unworkable, and un-shareable â€“ I still have a long way to go with my perfectionism it seems. Week 8: Real change occurs in tiny incrementsFor me, this chapter was the most mind altering of all and I continue to return to it because the lesson that comes from this will take a long time to fully inhale. As previously described, when commencing the twelve weeks, I internalised a lack of audacity as a personal failure, when perhaps more accurately, criticism pinches at our audacity reserves. Throughout the creative process, Julia explains, artists face loss of hope, money and self-belief, and it is encouragement that can so often help pick us back up. With encouragement â€“ from others and importantly ourselves â€“ we can begin to reframe any loss, rejection of criticism. â€œEvery loss must be framed as a potential gain, every end a beginning,â€ writes Julia. This week, I get an email from my book agent to say that she is moving to a different company and can no longer represent me. The previous week, I received news that my regular newspaper will no longer run. Other freelancing ties seemed to be losing momentum â€“ I felt like I had lost what Iâ€™d built over the last few years in one fell swoop.This words could not have come at a better time: â€œStop complaining about the lousy curves you get thrown and stretch, reach for what you really want,â€ writes Julia. Instead of â€˜why meâ€™ I needed to ask what next. I needed to see the potential of these endings and what could be opened up in their place â€“ a fresh start, a shake-up of my routine, and push from complacency. â€œCreativity is in the doing, not the done,â€ writes Julia. I often focus on the done â€“ the book only when it gets the deal, the columns being published, the podcast being praised when itâ€™s launched. I had ignored the doing, the writing, the recording and learning that all these dream projects contained. Iâ€™d skipped over process and progress, straight into perfection. â€œFocused on process, our creative life creates a sense of adventure,â€ writes Julia. Instead of thinking about the big end goal or outcome, I needed to focus on the next small step and swap â€˜whatâ€™s the useâ€™ for â€˜what is nextâ€™.â€œMost of the time, the next right thing to do is small: washing out your paintbrushes, stopping by the art-supply store and getting your clay, checking the local paper for a list of acting classesâ€¦ as a rule of thumb, it is best to just admit that there is always one action you can take for your creativity daily. This daily-action commitment fills the form.â€But yet, here is where our minds can play tricks â€“ instead of taking a small step, we become anxious over the big ones. â€œOne of our favourite things to do â€“ instead of art â€“ is to contemplate the odds,â€ writes Julia. This is often called anxiety in the lieu of action. â€œWatch yourself for a week and notice the way you will pick up an anxious thought, almost like a joint, to blow off or at least delay, your next creative action.â€I do this incessantly â€“ worry about the big project rather than pause my thoughts to take a small step. I see with full clarity my addiction when I read Juliaâ€™s words: â€œMost blocked creatives have an active addiction to anxiety. We prefer the low-grade pain and occasional heart-stopping panic attack to the drudgery of small and simple daily steps in the right direction.â€I logged my anxious thoughts for one day and it was helpful for staying focused on the task at hand â€“ redoing my chapter outline for the book. Work begets work and taking one small step in action instead of indulging in the big questions can help us further along.Week 9: Fear is not lazinessI often joke that Iâ€™m a lazy-overachiever, but after reading this weekâ€™s chapter I wasnâ€™t sure if I was really either. â€œBlocked artists are not lazy, they are blocked,â€ writes Julia. We spend energy on self-doubt, self-hatred, regret, grief, jealousy and think in terms of great big scary impossible tasks. My high expectations and lofty plans often mean that I want to leap over the small, incremental steps and dive straight into the impossible tasks. â€œThe need to produce a great work of art makes it hard to produce at all,â€ she writes. We can also fall into asking whatâ€™s the point and berating ourselves for only just starting, worrying that everyone is so much further ahead, we will never quite catch up. When I donâ€™t succeed or finish my to do list or meet my ideals, I call it laziness, ignoring the fact I set myself up to fail â€“ perhaps out of self-protection or self-sabotage. â€˜Do not call the inability to start laziness, call it fear,â€™ writes Julia. This fear can often come in the form of a â€˜Creative U-Turnâ€™, and itâ€™s best to reach out for someone who can help when you are stuck. â€œThe glare of success can send the recovering artist scurrying back into the cave of self-defeat.â€To tackle fear, we need to use love â€“ not pushing or hustling but rather leading from joy not duty. When we start with joy, the discipline will follow. The question bubbles up again â€“ what do I enjoy? What do I desire? Why does this continue to elude me? This week felt like a lot of question-asking and meetings, but no step taking. Does this mean I am searching for the joy?Week 10: overwork v. zestful workâ€œWhen we are clear about who we are and what we are doing, the energy flows freely and we experience no strain,â€ writes Julia. I still donâ€™t feel clear on what I am doing â€“ am I writing a book? Am I building on a freelance career? Am I making a podcast? Am I growing an event series? I spend a lot of time thinking about the doing in most of the aforementioned cases, but have no idea overall what I am doing. Do people actually know the answer to this?Iâ€™ve removed blocks, in particular I have removed alcohol after seeing how it had a negative domino effect on my daily habits and havenâ€™t had a drink for almost five months by this point in the book. I take this as a clear sign of the beginning of my unblocking. â€œWhen we become unblocked, we will experience a withdrawal from our old life and what has kept us stuck â€“ habits, workholism, relationships, addictions. We find that we are able to articulate our own boundaries and desires and become less malleability to the whims of others,â€ writes Julia. Even without alcohol, there is still room for improvement in respect to how Iâ€™m using my time â€“ which is often being the busy worker bee and feeling overwhelmed or worries about money rather than attending to important creative work. As Julia writes, â€œIt is more likely that you have the time and are misspending it.â€I donâ€™t feel particularly busy, I feel overwhelmed by what feels like a lack of zestful work. I know what I need to do, but I still feel stuck, unable to move towards it. As Julia puts it, I am in a creative drought.What do we do when we are in the drought? We stumble through because it is â€œthe time in the desert brings us clarity.â€ Sometimes it is our expectations that has created a drought. We have to watch for delusions such as fame and external validation which are often short-cut to self-approval. â€œThe desire to be better than can choke off the desire to be,â€ writes Julia. What we return to is the process of creating itself rather than the outcome, because â€œwanting more will always snap at our heels, erode our joy at ours or anotherâ€™s accomplishments.â€Once again, itâ€™s joy. â€œOnly when we are being joyful creative can we release the obsession with others and how they are doing.â€Week 11: The true purpose of exerciseA resounding message throughout the chapters has been that itâ€™s important to experiment with what works for you. For me, thatâ€™s been experimenting with not drinking, itâ€™s been doing the morning pages diligently, itâ€™s been taking myself on artist dates, and itâ€™s been running. When starting out, I could run for less than five minutes on a treadmill. By week eight of the book, I hit my goal of running 5km without stopping. When you try something and keep trying, it might just work. I reached that goal, and I keep running, for clarity and focus and steadiness. â€œWe learn by going where we have to go. Exercise is often the going that moves us from stagnation to inspiration, from problem to solution, from self-pity to self-respect. We learn we are stronger than we thought. We learn to look at things with a new perspective. We learn to solve our problems by tapping our own inner resources and listening for inspiration, not only from others but from ourselves. Seemingly without effort, our answers come while we swim or strike or ride or run. By definition, this is one of the fruits of exercise: the act of bringing into play or realising in action,â€ writes Julia. Rather than a tool for vanity, exercise teaches us about the rewards of the process, not the outcome.As Julia writes, â€œAny regular, repeatable action primes the wellâ€ and for me the seemingly non-creative act of running puts me into step with myself.Each time I tell myself I will run and then I take action and do it, I am building self-respect, which comes from doing the work. Iâ€™ve also learned that the goal is not the point, itâ€™s the running. Once I reached 5km, the treadmill did not evaporate â€“ opportunity to keep running, keep taking strides remained.â€œWhen we get â€˜thereâ€™, there disappearsâ€ writes Julia, so we may as well focus on the running, not the end. Week 12: Letting goItâ€™s the final chapter of the book and I have arrived in New York City with the intention of setting aside three months to work on the personal projects that I have carried with me the last few months â€“ the podcast and the book. I expect to hit the ground running immediately, but I can hear a familiar voice inside my mind telling me this has been a mistake, that I canâ€™t afford to be here financially, that Iâ€™ve taken a wrong turn and I should focus on finding a job, a real job. â€œWe throw up roadblocks to maintain a sense of control,â€ writes Julia. In place of trying to grasp time, I need to take myself more lightly and less seriously.â€œTo be creative is to be productive â€“ but by cooperating with creating, not by forcing it.â€ Gentleness, and trusting in the darkness, the unknown, is what Iâ€™m really here to learn. Itâ€™s what the twelve weeks have led me to and I change my phone background to say â€œlet go.â€Cooperation with our creativity takes time, and we have to remember that we can often sense our own changing and experience a sense of grief for our old life. Thereâ€™s something new opening up, I just donâ€™t know what, and thatâ€™s okay. In fact, itâ€™s extraordinary. Where to from here? Notes on the elusiveness of delight and desire Itâ€™s been three weeks since I finished The Artistâ€™s Way. I have kept the habit of the morning pages, I take myself on artist dates in New York City, and I continue to explore what it is that delights and brings joy, because in truth Iâ€™m not quite sure â€“ I often still feel like Iâ€™m in the creative desert, even though Iâ€™m in the most creatively vibrant city in the world. I want to watch for what delights me, I want to be alive to joy, I want to pay attention, I want to experiment and see if Julia Cameronâ€™s resounding message is true. â€œThe quality of life is in proportion, always to the capacity for delight.â€â€œBecome internally clear on dreams, desires and delights.â€â€œWhen we are clear about who we are and what we are doing, the energy flows freely and we experience no strain.â€â€œWhat we really want to do is what we are meant to doâ€ â€œOnly when we are being joyfully creative can we release the obsession with others and how they are doing.â€Itâ€™s interesting, to finish a book Iâ€™ve heard time and time again will change your life, and to not feel completely drastically changed. My book proposal is still being reworked, the podcast is still just in the pipeline, and Iâ€™m still unsure about what truly, deeply, delights me. A difference is that Iâ€™m committed to being okay with what isnâ€™t yet finished because things take time. I think thatâ€™s the beauty in finally being open to incremental change, to know that letting go of old habits and ways of thinking takes time, and that things donâ€™t pay off in a linear fashion, and there is often a plateau after any big internal shift.\n\n    â€œâ€œIâ€™m committed to being okay with what isnâ€™t yet finished because things take time and donâ€™t pay off in a linear fashion.â€â€\n\n  Julia Cameron describes creatives droughts as times in between dreams, listless, dry seasons. This drought has lasted much longer that I would have liked or predicted, but I am remaining upright and sitting with the listlessness because itâ€™s teaching me something. I can see how far I still have to go in my creative unblocking, but there is something within me that I had lost â€“ a simmering, a curiosity, an opening. Itâ€™s always there, in the background, and I keep it alive by attending to my morning pages, by checking in with my mind, by admitting I donâ€™t yet know, by allowing myself to stumble. â€œDroughts end because we have kept writing our pages. They end because we have not collapsed to the floor of our despair and refused to move. We have doubted, yes, but we have stumbled on,â€ writes Julia Cameron. I wish you all the best as you stumbleâ€”through the pages, through the process, through your days. * * *Update: In the proceeding months and years, I stumbled on and launched a podcast, wrote my first book, and also concluded this very project. You can now find my writing on Substack at Madeleine Doreâ€”On Things\n\n      Madeleine DoreOctober 5, 2019",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "11859a6c056f214a",
    "title": {
      "en": "The Vectrex Computer",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.amigalove.com/viewtopic.php?t=2887",
    "score": 91,
    "by": "erickhill",
    "time": 1742683941,
    "content": "If you're a Vectrex fan, be prepared to mourn what never came to beâ€¦\n\nSince I have nothing better to do as I get over an annoying head cold, I've been lying in bed today downloading every issue of Electronic Games magazine and converting each one into OCR searchable text. For decades I've had this nagging memory of a Vectrex computer announcement that never made it to production, and I was convinced it was from that publication. I owned a Vectrex at the time and would scour the game magazines for any tiny shred of news they might provide about my system. To their credit they seemed to bend over backwards to mention when a new Vectrex game hit store shelves, it just didn't happen that often enough for the Vectrex. I was usually lucky to find a couple inches of text per issue, and these were over 100 pages long each month.\n\nBut I remember reading about this supposed computer \"add-on\" and that the article was actually more than just a blurb and had some images. I never forgot about it. But, all these decades later I was never able to find that announcement. Every few years I'd look around and give up. I honestly started to wonder recently if I'd invented the memory.\n\nSome of the magazine scans on Archive.org are giant unsearchable image-based PDFs so not even Google could see inside them. So, today I started to download issues one at a time, convert them to 600 dpi for OCR scanning in Adobe Acrobat and started searching for \"vectrex\". Over, and over, and over. Amazing what illness can spawn sometimes.\n\nAfter a few hours I FOUND IT. AFTER ALL THESE YEARS!!!\n\nBehold (in text) - the mythological Vectrex Computer. Supposedly it even had 5 titles ready to ship in early 1984!\n\n\t\t\tBoth the light pen and goggles mentioned in the article were in fact produced and sold for a brief time.\n\nSince not everyone can see images, here is the text that matters from the article:\n\nBelieve it or not, the Vectrex stand-alone unit can now be transformed into a home computer!\nWith the assistance of a new keyboard developed by General Consumer Electronics, the plug-in adds 16K ROM and 16K RAM to the unit, and even incorporates a 65-key typewriter-style keyboard. Software for the computer is made on wafer-tapes, providing for 128K bytes of storage. BASIC's built into the system, and the keyboard is just the first step toward a word processor that'll be introduced in early 1984.\n\nFive cartridges for the 1983 computer include:\nThe Sound Studio, a music composition aid\nExploring the Solar System\nCreate-A-Game/Maze, which teaches the user to program his very own video-entertainment\nBasic Animation Domination\nBasic Science, a game teaching the fundamental concepts of physics\n\nA 65-key keyboard would have been one less key than a Commodore 64. I have no idea what \"wafer-tape\" storage is, but 128K storage sounds pretty interesting for the timeframe. A typical 5.25\" floppy disk could hold ~170KB per side and a datasette cassette could hold about 100KB per side.\n\nThe Vectrex has 1KB of RAM and 8KB of ROM. So this computer would have beefed it up to a knee-buckling 17KB of RAM. That seems a little light given the fact the C64 and Atari 800xl were already on the market with 64KB. Regardless, the games the Vectrex had already had impressive visuals and sound even with such meager system stats thanks to cartridge storage, so who knows what may have been possible?\n\nI remember being so excited as a kid when I read that news. Sad it never came to pass, but I'm really glad I finally rediscovered the article and it wasn't a human intelligence (HI) hallucination!",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "bfaa7281ec987c04",
    "title": {
      "en": "The New Three-Tier Application",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.dbos.dev/blog/new-three-tier-application",
    "score": 79,
    "by": "qianli_cs",
    "time": 1742303075,
    "content": "Recent articlesMaking Apps Durable with 10x Less CodeBenchmarksMarch 20, 2025The New Three-Tier ApplicationDBOS ArchitectureMarch 5, 2025Durable Execution for Building Crashproof AI AgentsHow ToFebruary 24, 2025",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "f6626717cb52e977",
    "title": {
      "en": "CSS Form Control Styling Level 1 â€“ W3C First Public Working Draft",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://drafts.csswg.org/css-forms/",
    "score": 25,
    "by": "tambourine_man",
    "time": 1742691330,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "877cfaddbe3399f5",
    "title": {
      "en": "Hubble Captures Vivid Auroras in Jupiter's Atmosphere (2016)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://science.nasa.gov/missions/hubble/hubble-captures-vivid-auroras-in-jupiters-atmosphere/",
    "score": 56,
    "by": "mooreds",
    "time": 1742426968,
    "content": "Explore HubbleScienceHubble Space TelescopeHubble Captures Vivid Auroras...Hubble HomeOverviewAbout HubbleThe History of HubbleHubble TimelineWhy Have a Telescope in Space?Hubble by the NumbersAt the MuseumFAQsImpact & BenefitsHubble's Impact & BenefitsScience ImpactsCultural ImpactTechnology BenefitsImpact on Human SpaceflightAstro Community ImpactsScienceHubble ScienceScience ThemesScience HighlightsScience Behind DiscoveriesHubble's Partners in ScienceUniverse UncoveredExplore the Night SkyObservatoryHubble ObservatoryHubble DesignMission OperationsMissions to HubbleHubble vs WebbTeamHubble TeamCareer AspirationsHubble AstronautsNewsHubble NewsHubble News ArchiveSocial MediaMedia ResourcesMultimediaMultimediaImagesVideosSonificationsPodcastse-BooksOnline ActivitiesLithographsFact SheetsPosters Hubble on the NASA AppGlossary More35th AnniversaryOnline Activities4 min readHubble Captures Vivid Auroras in Jupiterâ€™s AtmosphereNASA Hubble Mission TeamGoddard Space Flight CenterJun 30, 2016   Article\n\nAstronomers are using NASA's Hubble Space Telescope to study auroras â€“ stunning light shows in a planet's atmosphere â€“ on the poles of the largest planet in the solar system, Jupiter. The auroras were photographed during a series of Hubble Space Telescope Imaging Spectrograph...\n\nAstronomers are using the NASA/ESA Hubble Space Telescope to study auroras â€” stunning light shows in a planetâ€™s atmosphere â€” on the poles of the largest planet in the solar system, Jupiter. This observation program is supported by measurements made by NASAâ€™s Juno spacecraft, currently on its way to Jupiter.\n\nJupiter, the largest planet in the solar system, is best known for its colorful storms, the most famous being the Great Red Spot. Now astronomers have focused on another beautiful feature of the planet, using Hubble's ultraviolet capabilities.\n\nThe extraordinary vivid glows shown in the new observations are known as auroras. They are created when high-energy particles enter a planetâ€™s atmosphere near its magnetic poles and collide with atoms of gas. As well as producing beautiful images, this program aims to determine how various components of Jupiterâ€™s auroras respond to different conditions in the solar wind, a stream of charged particles ejected from the sun.\n\nThis observation program is perfectly timed as NASAâ€™s Juno spacecraft is currently in the solar wind near Jupiter and will enter the orbit of the planet in early July 2016. While Hubble is observing and measuring the auroras on Jupiter, Juno is measuring the properties of the solar wind itself; a perfect collaboration between a telescope and a space probe.\n\nâ€œThese auroras are very dramatic and among the most active I have ever seenâ€, said Jonathan Nichols from the University of Leicester, U.K., and principal investigator of the study. â€œIt almost seems as if Jupiter is throwing a firework party for the imminent arrival of Juno.â€\n\nTo highlight changes in the auroras Hubble is observing Jupiter almost daily for several months. Using this series of far-ultraviolet images from Hubble's Space Telescope Imaging Spectrograph, it is possible for scientists to create videos that demonstrate the movement of the vivid auroras, which cover areas bigger than Earth.\n\nNot only are the auroras huge in size, they are also hundreds of times more energetic than auroras on Earth. And, unlike those on Earth, they never cease. While on Earth the most intense auroras are caused by solar storms â€” when charged particles rain down on the upper atmosphere, excite gases and cause them to glow red, green and purple â€” Jupiter has an additional source for its auroras.\n\nThe strong magnetic field of the gas giant grabs charged particles from its surroundings. This includes not only the charged particles within the solar wind but also the particles thrown into space by its orbiting moon Io, known for its numerous and large volcanoes.\n\nThe new observations and measurements made with Hubble and Juno will help to better understand how the sun and other sources influence auroras. While the observations with Hubble are still ongoing and the analysis of the data will take several more months, the first images and videos are already available and show the auroras on Jupiterâ€™s north pole in their full beauty.\n\nThe Jet Propulsion Laboratory (JPL) in Pasadena, California, manages the Juno mission for Southwest Research Institute in San Antonio, Texas. Juno is part of NASA's New Frontiers Program, which is managed at NASA's Marshall Space Flight Center in Huntsville, Alabama, for NASA's Science Mission Directorate in Washington, D.C. Lockheed Martin Space Systems, Denver, built the spacecraft. The California Institute of Technology in Pasadena manages JPL for NASA.\n\nThe Hubble Space Telescope is a project of international cooperation between NASA and the European Space Agency. NASA's Goddard Space Flight Center in Greenbelt, Maryland, manages the telescope. The Space Telescope Science Institute (STScI) in Baltimore, Maryland, conducts Hubble science operations. STScI is operated for NASA by the Association of Universities for Research in Astronomy (AURA) in Washington, D.C.\n\n\t\t\t\tRelated Images & Videos\n\n\t\t\t\t\t\t\t\tAuroras on Jupiter\n\n\t\t\t\t\t\t\t\tAstronomers are using NASA's Hubble Space Telescope to study auroras â€“ stunning light shows in a planet's atmosphere â€“ on the poles of the largest planet in the solar system, Jupiter. The auroras were photographed during a series of Hubble Space Telescope Imaging Spectrograph...\n\n\t\t\t\t\t\t\t\tJupiter WFC3/UVIS\n\n\t\t\t\t\t\t\t\tThis visible-light image of Jupiter was taken with the Wide Field Camera 3 aboard NASA's Hubble Space Telescope on April 21, 2014.\n\n\t\t\t\t\t\t\t\tScale and Compass Image for Jupiter Aurora\n\n\t\t\t\t\t\t\t\tHubble Time-Lapse of Aurora on Jupiter (May 19, 2016)\n\n\t\t\t\t\t\t\t\tThis time-lapse video of the vivid auroras in Jupiter's atmosphere was created using far-ultraviolet-light observations made on May 19, 2016, with the Space Telescope Imaging Spectrograph aboard NASA's Hubble Space Telescope. Hubble is particularly suited to observing and...\n\n\t\t\t\t\t\t\t\tHubble Time-Lapse of Aurora on Jupiter (June 2, 2016)\n\n\t\t\t\t\t\t\t\tThis time-lapse video of the vivid auroras in Jupiter's atmosphere was created using far-ultraviolet-light observations made on June 2, 2016, with the Space Telescope Imaging Spectrograph aboard NASA's Hubble Space Telescope. Hubble is particularly suited to observing and...\n\n\t\t\t\t\t\t\t\tHubble Tracks Bright Auroras on Jupiter (Composite Video Sequence 1)\n\n\t\t\t\t\t\t\t\tThis composite video illustrates the auroras on Jupiter relative to their position on the giant planet. As on Earth, auroras are produced by the interaction of a planet's magnetic field with its atmosphere. The Jupiter auroras observed by NASA's Hubble Space Telescope are some...\n\n\t\t\t\t\t\t\t\tHubble Tracks Bright Auroras on Jupiter (Composite Video Sequence 2)\n\n\t\t\t\t\t\t\t\tThis composite video illustrates the auroras on Jupiter relative to their position on the giant planet. As on Earth, auroras are produced by the interaction of a planet's magnetic field with its atmosphere. The Jupiter auroras observed by NASA's Hubble Space Telescope are some...\n\n\t\t\t\t\tShare\n\n\t\t\t\t\tDetails\n\n\t\t\t\t\t\tLast Updated\n\n\t\t\t\t\tMar 20, 2025\n\n\t\t\tLocationNASA Goddard Space Flight Center\n\n\t\t\t\t\t\t\tContact\n\n\t\t\t\t\t\tMediaClaire Andreoli\nNASAâ€™s Goddard Space Flight Center\nGreenbelt, Maryland\nclaire.andreoli@nasa.gov\n\n\t\t\t\t\t\t\tCredits\n\n\t\t\t\t\t\tNASA, ESA, and J. Nichols (University of Leicester);\nAcknowledgment: A. Simon (NASA/GSFC) and the OPAL team\n\n\tRelated TermsHubble Space TelescopeAstrophysicsExoplanet AtmosphereGoddard Space Flight CenterJupiterPlanetary Environments & AtmospheresPlanetsThe Solar System\n\t\t\t\t\t\tRelated Links and Documents\n\nHubble-Europeâ€™s Release\nNASAâ€™s Hubble Portal\n\n\t\t\t\t\tKeep Exploring\n\t\t\t\t\tDiscover More Topics From Hubble\n\n\t\t\t\t\t\t\t\tHubble Space Telescope\n\n\t\t\t\t\t\t\tSince its 1990 launch, the Hubble Space Telescope has changed our fundamental understanding of the universe.\n\n\t\t\t\t\t\t\t\tHubble Science Highlights\n\n\t\t\t\t\t\t\t\tHubble Images\n\n\t\t\t\t\t\t\t\tHubble News",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "bb7728a51670dda3",
    "title": {
      "en": "Show HN: We made an MCP server so Cursor can debug Node.js on its own",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.npmjs.com/package/@hyperdrive-eng/mcp-nodejs-debugger",
    "score": 132,
    "by": "arthurgousset",
    "time": 1742659046,
    "content": "MCP Node.js Debugger\nAn MCP server that gives Cursor or Claude Code access to Node.js at runtime to help you debug: @hyperdrive-eng/mcp-nodejs-debugger.\nDemo\nCursor\nhttps://github.com/user-attachments/assets/c193a17e-b0e6-4c51-82aa-7f3f0de17e1a\nClaude Code\nhttps://github.com/user-attachments/assets/adb7321b-3a6a-459b-a5c9-df365710d4d8\nQuick start\nCursor\n\nAdd to Cursor (~/.cursor/mcp.json)\n\n+ {\n+   \"mcpServers\": {\n+   \"nodejs-debugger\": {\n+      \"command\": \"npx\",\n+       \"args\": [\"@hyperdrive-eng/mcp-nodejs-debugger\"]\n+     }\n+   }\n+ }\n\nRun a Node.js server in debug mode (i.e. with the --inspect flat)\nnode --inspect {file.js}\n\nAsk Cursor to debug your Node.js server at runtime\n\nClaude Code\n\nAdd to Claude Code\nclaude mcp add nodejs-debugger npx @hyperdrive-eng/mcp-nodejs-debugger\n\nStart Claude Code\nclaude\nâ•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\nâ”‚ âœ» Welcome to Claude Code research preview!            â”‚\nâ”‚                                                       â”‚\nâ”‚   /help for help                                      â”‚\nâ”‚                                                       â”‚\nâ”‚   Found 1 MCP server (use /mcp for status)            â”‚\nâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n\nRun a Node.js server in debug mode (i.e. with the --inspect flat)\n# In another terminal\nnode --inspect {file.js}\n\nAsk Claude Code to debug your Node.js server at runtime\n> I'm getting a runtime error in Node.js\n\n  {YOUR_RUNTIME_ERROR}\n\n  Please help me debug this error at runtime using the nodejs-debugger mcp.\n\nUsage\nClaude Code\n\nAdd to Claude Code\nclaude mcp add nodejs-debugger npx mcp-nodejs-debugger\n\nVerify connection\n> /mcp\n  â¿  MCP Server Status\n\n     â€¢ nodejs-debugger: connected\n\nRemove from Claude Code\nclaude remove nodejs-debugger\n\nCursor\n\nAdd to Cursor (~/.cursor/mcp.json)\n+ {\n+   \"mcpServers\": {\n+   \"nodejs-debugger\": {\n+      \"command\": \"npx\",\n+       \"args\": [\"@hyperdrive-eng/mcp-nodejs-debugger\"]\n+     }\n+   }\n+ }\n\nVerify connection:\n\nRemove from Cursor (~/.cursor/mcp.json):\n- {\n-   \"mcpServers\": {\n-   \"nodejs-debugger\": {\n-      \"command\": \"npx\",\n-       \"args\": [\"@hyperdrive-eng/mcp-nodejs-debugger\"]\n-     }\n-   }\n- }\n\nExample\nCursor\n\nRun this simple Node.js app: github.com/mdn/express-locallibrary-tutorial:\nnode --inspect ./bin/www\n\nAsk Cursor to set a breakpoint\n\nCreate a book at localhost:3000/catalog/book/create\n\nWatch Cursor capture runtime state\n\nClaude Code\n\nHere is a buggy Node.js server:\nnode --inspect index.js\n\nDebugger listening on ws://127.0.0.1:9229/2862f5a2-8618-4516-8429-1248b397e9b4\n\n#######################\n## THE RUNTIME ERROR ##\n#######################\nMongooseServerSelectionError: Could not connect to any servers in your MongoDB Atlas cluster. One common reason is that you're trying to access the database from an IP that isn't whitelisted. Make sure your current IP address is on your Atlas cluster's IP whitelist: https://www.mongodb.com/docs/atlas/security-whitelist/\n    at _handleConnectionErrors (/Users/arthur/Documents/thirdparties/mdn/express-locallibrary-tutorial/node_modules/mongoose/lib/connection.js:1110:11)\n    at NativeConnection.openUri (/Users/arthur/Documents/thirdparties/mdn/express-locallibrary-tutorial/node_modules/mongoose/lib/connection.js:1041:11)\n    at async main (/Users/arthur/Documents/thirdparties/mdn/express-locallibrary-tutorial/app.js:35:3) {\n  reason: TopologyDescription {\n    type: 'ReplicaSetNoPrimary',\n    servers: Map(3) {\n      'ac-8e6hgk7-shard-00-01.cojoign.mongodb.net:27017' => [ServerDescription],\n      'ac-8e6hgk7-shard-00-02.cojoign.mongodb.net:27017' => [ServerDescription],\n      'ac-8e6hgk7-shard-00-00.cojoign.mongodb.net:27017' => [ServerDescription]\n    },\n    stale: false,\n    compatible: true,\n    heartbeatFrequencyMS: 10000,\n    localThresholdMS: 15,\n    setName: 'atlas-5syy8t-shard-0',\n    maxElectionId: null,\n    maxSetVersion: null,\n    commonWireVersion: 0,\n    logicalSessionTimeoutMinutes: null\n  },\n  code: undefined\n}\n\nClaude Code debugs the Node.js server at runtime:\n> I'm getting a runtime error connecting to mongoose. Please help me debug by checking the value of\n  the mongodb configs at runtime using the nodejs-debugger mcp.\n\n  âº I'll help you debug the mongoose connection issue using the Node.js debugger. Let's first understand where the\n  MongoDB connection is configured.\n\n  âº Search(pattern: \"**/*.js\")â€¦\n    â¿ Found 100 files\n\n  âº Let's set a breakpoint at the mongoose connection point and continue:\n\n  âº nodejs-debugger:set_breakpoint (MCP)(file:                                                                         â€¦\n        \"/Users/arthur/Documents/thirdparties/mdn/express-locallibrary-tutorial/app.js\",\n         line: 35)\n  â¿ Breakpoint set successfully. ID:\n       2:34:0:file:///Users/arthur/Documents/thirdparties/mdn/express-locallibrary-tutorial/app\\.js\n\n  âº Let's list all our breakpoints:\n\n  âº nodejs-debugger:list_breakpoints (MCP)\n  â¿ [\n         {\n           \"file\": \"/Users/arthur/Documents/thirdparties/mdn/express-locallibrary-tutorial/app.js\",\n           \"line\": 35,\n           \"id\": \"2:34:0:file:///Users/arthur/Documents/thirdparties/mdn/express-locallibrary-tutorial/app\\\\.js\"\n         }\n       ]\n\n  âº Let's try to execute a specific piece of code to check the connection variables:\n\n  âº nodejs-debugger:nodejs_inspect (MCP)(js_code: \"// Get the mongoose module and inspect connection\\nconst mongoose = â€¦\n                                        require('mongoose');\\nconsole.log('Current mongoose version:',\n                                        mongoose.version);\\nconsole.log('Current mongoose connection state:',\n                                        mongoose.connection.readyState);\\n// Log connection string safely (without\n                                        password)\\nconst connectionString = process.env.MONGODB_URI || \\n\n                                        \\\"mongodb+srv://cooluser:coolpassword@cluster0.cojoign.mongodb.net/local_library?retryWr\n                                        ites=true&w=majority&appName=Cluster0\\\";\\nconsole.log('Connection string\n                                        (masked):', connectionString.replace(/(?<=mongodb\\\\+srv:\\\\/\\\\/\\\\w+:)(.+?)(?=@)/,\n                                         '*****'));\")\n\n  â¿ Console output:\n       [log] Current mongoose version: 8.9.5\n       [log] Current mongoose connection state: 0\n       [log] Connection string (masked): mongodb+srv://cooluser:coolpassword@cluster0.cojoign.mongodb.net/local_library?retryWri\n       tes=true&w=majority&appName=Cluster0\n\n  âº The runtime error you're seeing in your application is related to MongoDB\n    Atlas connection issues. Based on your error message, the problem is:\n\n    1. You're using placeholder credentials (\"cooluser:coolpassword\")\n\n    To fix this, you have two options:\n\n    1. Use a local MongoDB instance by changing line 29-30 in app.js to:\n    const dev_db_url = \"mongodb://127.0.0.1:27017/local_library\";\n    2. Or correctly set up MongoDB Atlas:\n      - Log into Atlas at https://cloud.mongodb.com\n      - Go to Network Access and add your current IP to whitelist\n      - Create proper database credentials\n      - Update the connection string with real credentials\n\n    Which option would you like to proceed with?",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  }
]