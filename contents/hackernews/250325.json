[
  {
    "id": "94dd7a5cb320f1fa",
    "title": {
      "en": "Spammers are better at SPF, DKIM, and DMARC than everyone else",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://toad.social/@grumpybozo/114213600922816869",
    "score": 101,
    "by": "pabs3",
    "time": 1742890490,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "781d9ca14c79dac7",
    "title": {
      "en": "Writing your own C++ standard library from scratch",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://nibblestew.blogspot.com/2025/03/writing-your-own-c-standard-library.html",
    "score": 70,
    "by": "JNRowe",
    "time": 1742890236,
    "content": "Nibble Stew\n\nA gathering of development thoughts of Jussi Pakkanen. Some of you may know him as the creator of the Meson build system. jpakkane at gmail dot com\n\nMonday, March 24, 2025\n\nWriting your own C++ standard library from scratch\n\nThe C++ standard library (also know as the STL) is, without a doubt, an astounding piece of work. Its scope, performance and incredible backwards compatibility have taken decades of work by many of the world's best programmers. My hat's off to all those people who have contributed to it.All of that is not to say that it is not without its problems. The biggest one being the absolutely abysmal compile times but unreadability, and certain unoptimalities caused by strict backwards compatibility are also at the top of the list. In fact, it could be argued that most of the things people really dislike about C++ are features of the STL rather than the language itself. Fortunately, using the STL is not mandatory. If you are crazy enough, you can disable it completely and build your own standard library in the best Bender style.One of the main advantages of being an unemployed-by-choice open source developer is that you can do all of that if you wish. There are no incompetent middle damagers hovering over your shoulder to ensure you are \"producing immediate customer value\" rather than \"wasting time on useless polishing that does not produce immediate customer value\".It's my time, and I'll waste it if I want to!What's in it?The biggest design questions of a standard library are scope and the \"feel\" of the API. Rather than spending time on design, we steal it. Thus, when in doubt, read the Python stdlib documentation and replicate it. Thus the name of the library is pystd.The test appTo keep the scope meaningful, we start by writing only enough of stdlib to build an app that reads a text file, validates it as UTF-8, splits the contents into words, counts how many time each word appears in the file and prints all words and how many times it appears sorted by decreasing count.This requires, at least:File handlingStringsUTF8 validationA hash mapA vectorSortingThe training wheels come offThe code is available in this Github repo for those who want to follow along at home.Disabling the STL is fairly easy (with Linux+GCC at least) and requires only these two Meson statements:add_global_arguments('-nostdinc++', language: 'cpp')\nadd_global_link_arguments('-nostdlib++', '-lsupc++', language: 'cpp')The supc++ library is (according to stackoverflow) a support library GCC needs to implement core language features. Now the stdlib is off and it is time to implement everything with sticks, stones and duct tape.The outcomeOnce you have implemented everything discussed above and auxiliary stuff like a hashing framework the main application looks like this.The end result is both Valgrind and Asan clean. There is one chunk of unreleased memory, but that comes from supc++. There is probably UB in the implementation. But it should be the good kind of UB that, if it would actually not work, would break the entire Linux userspace because everything depends on it working \"as expected\".All of this took fewer than 1000 lines of code in the library itself (including a regex implementation that is not actually used). For comparison merely including vector from the STL brings in 27 thousand lines of code.Comparison to an STL versionConverting this code to use the STL is fairly simple and only requires changing some types and fine tuning the API. The main difference is that the STL version does not validate that the input is UTF-8 as there is no builtin function for that. Now we can compare the two.Runtime for both is 0.001 to 0.002 seconds on the small test file I used. Pystd is not noticeably slower than the STL version, which is enough for our purposes. It almost certainly scales worse because there has been zero performance work on it.Compiling the pystd version with -O2 takes 0.3 seconds whereas the STL version takes 1.2 seconds. The measurements were done on a Ryzen 7 3700X processor.The executable's unstripped size is 349k for STL and 309k for pystd. The stripped sizes are 23k and 135k. Approximately 100 k of the pystd executable comes from supc++. In the STL version that probably comes dynamically from libstdc++(which, on this machine, takes 2.5 MB).Perfect ABI stabilityDesigning a standard library is exceedingly difficult because you can't ever really change it. Someone, somewhere, is depending on every misfeature in it so they can never be changed.Pystd has been designed to both support perfect ABI stability and make it possible to change it in arbitrary ways in the future. If you start from scratch this turned out to be fairly simple.The sample code above used the pystd namespace. It does not actually exist. Instead it is defined like this in the cpp file:#include <pystd2025.hpp>namespace pystd = pystd2025;In pystd all code is in a namespace with a year and is stored in a header file with the same year. The idea is, then, that every year you create a new release. This involves copying all stdlib header files to a file with the new year and regexping the namespace declarations to match. The old code is now frozen forever (except for bug fixes) whereas the new code can be changed at will because there arezero existing lines of code that depend on it.End users now have the choice of when to update their code to use newer pystd versions. Even better, if there is an old library that can not be updated, any of the old versions can be used in parallel. For example:pystd2030::SomeType foo;pystd2025::SomeType bar(foo.something(), foo.something_else());Thus if no code is ever updated, everything keeps working. If all code is updated at once, everything works. If only parts of the code are updated, things can still be made to work with some glue code. This puts the maintenance burden on the people whose projects can not be updated as opposed to every other developer in the world. This is as it should be, and also would motivate people with broken deps to spend some more effort to get them fixed.\n\nPosted by\n\nJussi\n\nat\n\n5:03 PM\n\nEmail ThisBlogThis!Share to XShare to FacebookShare to Pinterest\n\nNo comments:\n\nPost a Comment\n\n      BLOG_CMT_createIframe('https://www.blogger.com/rpc_relay.html');\n\nOlder Post\n\nHome\n\nSubscribe to:\nPost Comments (Atom)\n\nAbout Me\n\nJussi\n\nView my complete profile\n\nBlog Archive\n\n        ▼\n\n2025\n\n(5)\n\n        ▼\n\nMarch\n\n(1)\n\nWriting your own C++ standard library from scratch\n\n        ►\n\nFebruary\n\n(3)\n\n        ►\n\nJanuary\n\n(1)\n\n        ►\n\n2024\n\n(21)\n\n        ►\n\nDecember\n\n(4)\n\n        ►\n\nNovember\n\n(1)\n\n        ►\n\nOctober\n\n(2)\n\n        ►\n\nSeptember\n\n(1)\n\n        ►\n\nAugust\n\n(2)\n\n        ►\n\nJuly\n\n(1)\n\n        ►\n\nJune\n\n(1)\n\n        ►\n\nMay\n\n(1)\n\n        ►\n\nApril\n\n(3)\n\n        ►\n\nMarch\n\n(2)\n\n        ►\n\nFebruary\n\n(1)\n\n        ►\n\nJanuary\n\n(2)\n\n        ►\n\n2023\n\n(41)\n\n        ►\n\nDecember\n\n(5)\n\n        ►\n\nNovember\n\n(2)\n\n        ►\n\nOctober\n\n(2)\n\n        ►\n\nSeptember\n\n(3)\n\n        ►\n\nAugust\n\n(1)\n\n        ►\n\nJuly\n\n(3)\n\n        ►\n\nJune\n\n(4)\n\n        ►\n\nMay\n\n(4)\n\n        ►\n\nApril\n\n(4)\n\n        ►\n\nMarch\n\n(6)\n\n        ►\n\nFebruary\n\n(5)\n\n        ►\n\nJanuary\n\n(2)\n\n        ►\n\n2022\n\n(34)\n\n        ►\n\nDecember\n\n(4)\n\n        ►\n\nNovember\n\n(3)\n\n        ►\n\nOctober\n\n(2)\n\n        ►\n\nSeptember\n\n(4)\n\n        ►\n\nAugust\n\n(2)\n\n        ►\n\nJuly\n\n(3)\n\n        ►\n\nJune\n\n(2)\n\n        ►\n\nMay\n\n(2)\n\n        ►\n\nApril\n\n(4)\n\n        ►\n\nFebruary\n\n(5)\n\n        ►\n\nJanuary\n\n(3)\n\n        ►\n\n2021\n\n(29)\n\n        ►\n\nDecember\n\n(1)\n\n        ►\n\nNovember\n\n(1)\n\n        ►\n\nOctober\n\n(1)\n\n        ►\n\nSeptember\n\n(2)\n\n        ►\n\nAugust\n\n(1)\n\n        ►\n\nJuly\n\n(5)\n\n        ►\n\nJune\n\n(3)\n\n        ►\n\nMay\n\n(5)\n\n        ►\n\nApril\n\n(2)\n\n        ►\n\nMarch\n\n(4)\n\n        ►\n\nFebruary\n\n(3)\n\n        ►\n\nJanuary\n\n(1)\n\n        ►\n\n2020\n\n(42)\n\n        ►\n\nDecember\n\n(1)\n\n        ►\n\nNovember\n\n(5)\n\n        ►\n\nOctober\n\n(5)\n\n        ►\n\nSeptember\n\n(2)\n\n        ►\n\nAugust\n\n(4)\n\n        ►\n\nJuly\n\n(5)\n\n        ►\n\nJune\n\n(2)\n\n        ►\n\nMay\n\n(4)\n\n        ►\n\nApril\n\n(3)\n\n        ►\n\nMarch\n\n(4)\n\n        ►\n\nFebruary\n\n(5)\n\n        ►\n\nJanuary\n\n(2)\n\n        ►\n\n2019\n\n(27)\n\n        ►\n\nDecember\n\n(4)\n\n        ►\n\nNovember\n\n(3)\n\n        ►\n\nOctober\n\n(1)\n\n        ►\n\nSeptember\n\n(2)\n\n        ►\n\nAugust\n\n(3)\n\n        ►\n\nJuly\n\n(2)\n\n        ►\n\nJune\n\n(2)\n\n        ►\n\nMay\n\n(3)\n\n        ►\n\nApril\n\n(1)\n\n        ►\n\nFebruary\n\n(6)\n\n        ►\n\n2018\n\n(24)\n\n        ►\n\nNovember\n\n(4)\n\n        ►\n\nOctober\n\n(1)\n\n        ►\n\nSeptember\n\n(1)\n\n        ►\n\nAugust\n\n(3)\n\n        ►\n\nJuly\n\n(3)\n\n        ►\n\nJune\n\n(1)\n\n        ►\n\nApril\n\n(3)\n\n        ►\n\nMarch\n\n(4)\n\n        ►\n\nFebruary\n\n(4)\n\n        ►\n\n2017\n\n(35)\n\n        ►\n\nDecember\n\n(4)\n\n        ►\n\nNovember\n\n(1)\n\n        ►\n\nSeptember\n\n(1)\n\n        ►\n\nAugust\n\n(3)\n\n        ►\n\nJuly\n\n(3)\n\n        ►\n\nJune\n\n(3)\n\n        ►\n\nMay\n\n(5)\n\n        ►\n\nApril\n\n(4)\n\n        ►\n\nMarch\n\n(3)\n\n        ►\n\nFebruary\n\n(2)\n\n        ►\n\nJanuary\n\n(6)\n\n        ►\n\n2016\n\n(20)\n\n        ►\n\nDecember\n\n(4)\n\n        ►\n\nNovember\n\n(2)\n\n        ►\n\nOctober\n\n(1)\n\n        ►\n\nSeptember\n\n(2)\n\n        ►\n\nAugust\n\n(1)\n\n        ►\n\nJuly\n\n(1)\n\n        ►\n\nJune\n\n(2)\n\n        ►\n\nMay\n\n(2)\n\n        ►\n\nApril\n\n(4)\n\n        ►\n\nMarch\n\n(1)\n\n        ►\n\n2015\n\n(19)\n\n        ►\n\nDecember\n\n(2)\n\n        ►\n\nNovember\n\n(3)\n\n        ►\n\nOctober\n\n(3)\n\n        ►\n\nSeptember\n\n(2)\n\n        ►\n\nAugust\n\n(3)\n\n        ►\n\nJuly\n\n(5)\n\n        ►\n\nJune\n\n(1)\n\nSimple theme. Powered by Blogger.",
    "summary": {
      "en": "**Summary of \"Writing Your Own C++ Standard Library from Scratch\"**\n\nJussi Pakkanen discusses the challenges and decisions involved in creating a custom C++ standard library, referred to as \"pystd.\" Although the existing C++ standard library (STL) is impressive, it has issues like long compile times and readability concerns. Pakkanen proposes that it is possible to disable the STL and build a new library that addresses these problems.\n\nThe library aims to offer essential features such as file handling, string manipulation, UTF-8 validation, and sorting while keeping the codebase minimal—under 1,000 lines compared to the STL’s extensive code. The author emphasizes that creating a library from scratch allows for flexibility in design and the ability to ensure compatibility with future versions.\n\nOne unique aspect of pystd is its approach to versioning. Each year, a new version of the library is created, allowing old and new versions to coexist. This ensures that users can maintain compatibility with older code while adopting newer features at their own pace.\n\nOverall, the project highlights the potential for creating a more efficient and user-friendly alternative to the traditional C++ standard library.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "29803211a6f61815",
    "title": {
      "en": "German parliament votes as a Git contribution graph",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://abstimmung.eu/git/2024",
    "score": 424,
    "by": "mxschll",
    "time": 1742858943,
    "content": "Abstimmungen 2024Namentliche Abstimmungen im BundestagJanFebMärAprMaiJunJulAugSepOktNovDezMoMiFrWenigerMehr17. Januar 2024Jahresbericht 2022 der Wehrbeauftragten (Entschließungsantrag)VerteidigungAußenpolitikUkraineBundeswehrMilitärDie Abstimmung betrifft den Entschließungsantrag der CDU/CSU-Fraktion zum Jahresbericht 2022 der Wehrbeauftragten, der die Bundesregierung auffordert, einsatzbereite TAURUS-Marschflugkörper an die Ukraine zu liefern und diese unverzüglich nachzubeschaffen.Ergebnisse des BundestagsJa: 178(27%)Nein: 485(73%)Enthaltung: 3(0%)17. Jan. 2024Gesamtstimmen: 66618. Januar 2024Entlastung der LandwirtschaftLandwirtschaftWirtschaftSteuerpolitikAgrarpolitikInnovationBürokratieDer Antrag der CDU/CSU-Fraktion betrifft die Unterstützung der Landwirtschaft und zielt darauf ab, die Wettbewerbsfähigkeit der Landwirtschaft zu erhalten und zu verbessern, anstatt sie durch politische Maßnahmen zu schwächen. Der Antrag fordert die Bundesregierung auf, Maßnahmen zu ergreifen, um die Landwirtschaft zu fördern und zu entlasten.Ergebnisse des BundestagsJa: 188(28%)Nein: 420(62%)Enthaltung: 69(10%)18. Jan. 2024Gesamtstimmen: 677Friedensinitiative für die Ukraine und Russland (Beschlussempfehlung)AußenpolitikSicherheitspolitikFriedenspolitikUkraineRusslandDie Abstimmung betrifft einen Antrag der AfD-Fraktion, der eine Friedensinitiative mit Sicherheitsgarantien für die Ukraine und Russland fordert, um Deutschlands Verantwortung für den Frieden in Europa gerecht zu werden.Ergebnisse des BundestagsJa: 605(89%)Nein: 75(11%)Enthaltung: 2(0%)18. Jan. 2024Gesamtstimmen: 68219. Januar 2024Modernisierung des Staatsangehörigkeitsrechts (GesEntw BReg)StaatsangehörigkeitIntegrationRechtGesellschaftMigrationDie Abstimmung betrifft die Modernisierung des Staatsangehörigkeitsrechts (StARModG), um den Zugang zur deutschen Staatsangehörigkeit zu erleichtern und die Integration von Ausländern zu fördern.Ergebnisse des BundestagsJa: 382(60%)Nein: 234(37%)Enthaltung: 23(3%)19. Jan. 2024Gesamtstimmen: 63901. Februar 2024Änderung des BundeswahlgesetzesWahlrechtInnenpolitikBevölkerungsentwicklungKommunalpolitikGesetzgebungDie Abstimmung betrifft die Änderung des Bundeswahlgesetzes, um die Wahlkreiseinteilung an die aktuelle Bevölkerungsentwicklung anzupassen und kommunale Gebietsänderungen zu berücksichtigen. Ziel ist es, sicherzustellen, dass die Wahl zum Deutschen Bundestag auf einer fairen und gleichmäßigen Grundlage stattfindet.Ergebnisse des BundestagsJa: 382(57%)Nein: 276(41%)Enthaltung: 12(2%)1. Feb. 2024Gesamtstimmen: 67022. Februar 2024Zehn Jahre russischer Krieg gegen die UkraineUkraineRusslandSicherheitspolitikAußenpolitikMilitärVölkerrechtDer Antrag betrifft die Unterstützung der Ukraine angesichts des russischen Krieges und zielt darauf ab, die deutsche und europäische Sicherheit zu stärken sowie die Verantwortlichen für Kriegsverbrechen zur Rechenschaft zu ziehen.Ergebnisse des BundestagsJa: 381(57%)Nein: 284(43%)Enthaltung: 2(0%)22. Feb. 2024Gesamtstimmen: 667Deutsche Außen- und Sicherheitspolitik (Antrag CDU/CSU)AußenpolitikSicherheitspolitikVerteidigungInternationale BeziehungenBevölkerungsschutzDer Antrag der CDU/CSU betrifft eine Neuausrichtung der deutschen Außen- und Sicherheitspolitik, um auf die veränderte Bedrohungslage durch Russland zu reagieren und Deutschlands Rolle in Europa und der Welt zu stärken. Er fordert eine \"echte Zeitenwende\" mit konkreten Maßnahmen zur Stärkung der Verteidigungsfähigkeit, des Bevölkerungsschutzes und der internationalen Zusammenarbeit.Ergebnisse des BundestagsJa: 182(27%)Nein: 479(72%)Enthaltung: 5(1%)22. Feb. 2024Gesamtstimmen: 66623. Februar 2024Wachstumschancengesetz;Kfz-Haftpflichtversicherung (BeschlEmpf Vermittlungsausschuss)WirtschaftSteuerrechtInvestitionInnovationFinanzenDie Beschlussempfehlung des Vermittlungsausschusses betrifft das Gesetz zur Stärkung von Wachstumschancen, Investitionen und Innovation sowie Steuervereinfachung und Steuerfairness (Wachstumschancengesetz). Sie zielt darauf ab, Änderungen an diesem Gesetz vorzunehmen, um Wachstum, Investitionen und Innovation zu fördern sowie das Steuersystem zu vereinfachen und fairer zu gestalten.Ergebnisse des BundestagsJa: 376(58%)Nein: 267(41%)Enthaltung: 1(1%)23. Feb. 2024Gesamtstimmen: 644Cannabisgesetz (GesEntwurf BReg)GesundheitDrogenpolitikJugendschutzJustizSicherheitDas Cannabisgesetz regelt den kontrollierten Umgang mit Cannabis und ändert weitere Vorschriften. Es zielt darauf ab, den Gesundheitsschutz zu verbessern, den illegalen Markt einzudämmen und den Kinder- und Jugendschutz zu stärken.Ergebnisse des BundestagsJa: 404(64%)Nein: 226(36%)Enthaltung: 4(0%)23. Feb. 2024Gesamtstimmen: 634Bundeswehreinsatz EUNAVFOR ASPIDES (BeschlEmpf)AußenpolitikSicherheitspolitikEuropäische UnionMilitärInternationaler HandelDie Abstimmung betrifft die Beteiligung bewaffneter deutscher Streitkräfte an der von der Europäischen Union geführten Operation EUNAVFOR ASPIDES, die zum Schutz der Schifffahrt im Roten Meer und angrenzenden Gebieten beitragen soll. Der Bundestag stimmt über den Antrag der Bundesregierung zur Entsendung von bis zu 700 Soldaten bis maximal zum 28. Februar 2025 ab.Ergebnisse des BundestagsJa: 538(94%)Nein: 31(5%)Enthaltung: 4(1%)23. Feb. 2024Gesamtstimmen: 57314. März 2024Ablehnung von Lieferung des Taurus-Marschflugkörpers (Beschlussempfehlung)AußenpolitikVerteidigungSicherheitspolitikUkraineMilitärDie Abstimmung betrifft den Antrag der CDU/CSU, die Bundesregierung aufzufordern, der Ukraine Taurus-Marschflugkörper aus Bundeswehrbeständen zu liefern, die Integration des Waffensystems in ukrainische Flugzeuge zu unterstützen, ukrainische Soldaten auszubilden, Hemmnisse bei der Weitergabe sensibler Informationen zu beseitigen, Ausrüstungslücken bei der Bundeswehr durch Nachbeschaffung zu schließen, die Produktionskapazitäten der Industrie zu erhöhen und weitere Taurus-Marschflugkörper zu beschaffen. Der Antrag wird abgelehnt.Ergebnisse des BundestagsJa: 494(72%)Nein: 188(27%)Enthaltung: 5(1%)14. März 2024Gesamtstimmen: 68720. März 2024Steuervergünstigung für Agrardiesel (Antrag AfD)LandwirtschaftSteuerpolitikSubventionenHaushaltspolitikAgrardieselDie Abstimmung betrifft den Antrag der AfD-Fraktion, die Steuervergünstigung für Agrardiesel beizubehalten, um die deutsche Landwirtschaft nicht zusätzlich zu belasten und die Lebensmittelpreise nicht künstlich zu verteuern. Der Antrag sieht vor, Kürzungen in anderen Bereichen des Haushalts vorzunehmen, falls Einsparungen notwendig sind, und die Bundesregierung aufzufordern, Beschlüsse der Parlamentsgremien zu respektieren.Ergebnisse des BundestagsJa: 553(90%)Nein: 64(10%)Enthaltung: 0(0%)20. März 2024Gesamtstimmen: 61721. März 2024Bundeswehreinsatz SEA GUARDIAN im Mittelmeer (Antrag BReg)SicherheitspolitikMilitärNATOMittelmeerTerrorismusbekämpfungInternationale BeziehungenDie Abstimmung betrifft die Fortsetzung der Beteiligung bewaffneter deutscher Streitkräfte an der NATO-geführten maritimen Sicherheitsoperation SEA GUARDIAN (MSO SG) im Mittelmeer, um zur Sicherheit und Stabilität in der Region beizutragen. Der Deutsche Bundestag stimmt über den Antrag der Bundesregierung zur Verlängerung dieses Einsatzes ab.Ergebnisse des BundestagsJa: 563(84%)Nein: 109(16%)Enthaltung: 2(0%)21. März 2024Gesamtstimmen: 674Bundeswehreinsatz in der Republik Südsudan (UNMISS)AußenpolitikSicherheitspolitikVerteidigungHumanitäre HilfeFriedensmissionUNDie Abstimmung betrifft die Fortsetzung der Beteiligung bewaffneter deutscher Streitkräfte an der UN-Friedensmission UNMISS (United Nations Mission in South Sudan) in der Republik Südsudan. Es geht darum, ob der Bundestag der von der Bundesregierung beschlossenen Verlängerung des Einsatzes zustimmt.Ergebnisse des BundestagsJa: 610(92%)Nein: 48(7%)Enthaltung: 2(1%)21. März 2024Gesamtstimmen: 66012. April 2024Selbstbestimmung in Bezug auf den GeschlechtseintragGesellschaftspolitikGleichstellungSelbstbestimmungTransgenderRechtDie Abstimmung betrifft den Entwurf eines Gesetzes über die Selbstbestimmung in Bezug auf den Geschlechtseintrag und zur Änderung weiterer Vorschriften. Das Gesetz zielt darauf ab, die Regelungen zur Änderung des Geschlechtseintrags zu vereinfachen und zu vereinheitlichen.Ergebnisse des BundestagsJa: 372(59%)Nein: 251(40%)Enthaltung: 11(1%)12. Apr. 2024Gesamtstimmen: 634Änderung des EnergiewirtschaftsgesetzesEnergiewirtschaftWasserstoffInfrastrukturNetzentwicklungFinanzierungDie Abstimmung betrifft ein Gesetz zur Änderung des Energiewirtschaftsgesetzes (EnWG), um den Aufbau einer nationalen Wasserstoffinfrastruktur zu fördern und die Finanzierung des Wasserstoff-Kernnetzes zu regeln.Ergebnisse des BundestagsJa: 347(61%)Nein: 202(36%)Enthaltung: 16(3%)12. Apr. 2024Gesamtstimmen: 56525. April 2024Operation EUNAVFOR MED IRINI (Beschlussempfehlung)AußenpolitikSicherheitspolitikEuropäische UnionMilitärLibyenDie Abstimmung betrifft die Fortsetzung der Beteiligung bewaffneter deutscher Streitkräfte an der Operation EUNAVFOR MED IRINI der Europäischen Union. Ziel ist es, das Waffenembargo gegen Libyen weiterhin durchzusetzen und zur Stabilisierung des Landes beizutragen.Ergebnisse des BundestagsJa: 554(84%)Nein: 100(15%)Enthaltung: 2(1%)25. Apr. 2024Gesamtstimmen: 65626. April 2024Erneuerbare-Energien-Gesetz (Solarpaket)Erneuerbare EnergienPhotovoltaikKlimaschutzEnergiewendeBürokratieabbauNetzausbauDas Gesetz zur Änderung des Erneuerbare-Energien-Gesetzes (EEG) zielt darauf ab, den Ausbau der Photovoltaik (PV) in Deutschland zu beschleunigen und zu steigern, um die Klimaziele zu erreichen. Es werden Anreize für Solaranlagen geschaffen und Bürokratie abgebaut.Ergebnisse des BundestagsJa: 384(58%)Nein: 79(12%)Enthaltung: 200(30%)26. Apr. 2024Gesamtstimmen: 66316. Mai 2024Ablehnung eines Antrags gegen den WHO-PandemievertragGesundheitspolitikInternationale BeziehungenSouveränitätWHOPandemieDemokratieDie Abstimmung betrifft den Antrag der AfD-Fraktion, den WHO-Pandemievertrag und die überarbeiteten Internationalen Gesundheitsvorschriften abzulehnen. Der Antrag zielt darauf ab, die Souveränität Deutschlands in Gesundheitsfragen zu wahren und eine vermeintliche Machtausweitung der WHO zu verhindern.Ergebnisse des BundestagsJa: 578(89%)Nein: 71(11%)Enthaltung: 1(0%)16. Mai 2024Gesamtstimmen: 65005. Juni 2024Ablehnung eines Antrags zur Abschaffung des SolidaritätszuschlagsSteuerpolitikFinanzenWirtschaftSolidaritätszuschlagAbgabenlastSteuerreformDie Abstimmung betrifft den Antrag der AfD-Fraktion zur Abschaffung des Solidaritätszuschlags als ersten Schritt einer umfassenden Steuerreform zur Entlastung von Mittelstand, Unternehmen und Arbeitnehmern. Der Finanzausschuss empfiehlt die Ablehnung des Antrags.Ergebnisse des BundestagsJa: 537(90%)Nein: 62(10%)Enthaltung: 0(0%)5. Juni 2024Gesamtstimmen: 59906. Juni 2024Ablehnung eines Antrags zur Bekämpfung des politischen IslamInneres und HeimatPolitischer IslamDemokratieRechtSicherheitMigrationDie Abstimmung betrifft den Antrag der CDU/CSU-Fraktion, den politischen Islam als Gefahr für die freiheitliche Demokratie wirksam zu bekämpfen. Der Ausschuss für Inneres und Heimat empfiehlt die Ablehnung des Antrags.Ergebnisse des BundestagsJa: 406(63%)Nein: 241(37%)Enthaltung: 1(0%)6. Juni 2024Gesamtstimmen: 648Beschleunigung immissionsschutzrechtlicher GenehmigungsverfahrenUmweltKlimaschutzEnergieWirtschaftGenehmigungsverfahrenErneuerbare EnergienDie Abstimmung betrifft ein Gesetz zur Verbesserung des Klimaschutzes, zur Beschleunigung von Genehmigungsverfahren im Immissionsschutz und zur Umsetzung von EU-Recht, mit dem Ziel, Verfahren zu vereinfachen und den Ausbau erneuerbarer Energien zu fördern.Ergebnisse des BundestagsJa: 377(59%)Nein: 257(40%)Enthaltung: 9(1%)6. Juni 2024Gesamtstimmen: 64313. Juni 2024Antrag zu Konsequenzen aus dem Terror von MannheimInnere SicherheitMigrationIntegrationTerrorismusbekämpfungRechtspolitikDer Antrag der CDU/CSU-Fraktion betrifft die Forderung nach klaren Konsequenzen aus dem Terroranschlag von Mannheim und zielt darauf ab, die Migrations- und Integrationspolitik zu verschärfen sowie die Sicherheitsmaßnahmen zu erhöhen.Ergebnisse des BundestagsJa: 253(38%)Nein: 399(60%)Enthaltung: 11(2%)13. Juni 2024Gesamtstimmen: 663Ablehnung eines Antrags zum Verbot des Vereins Muslim InteraktivInneres und HeimatVereinsrechtExtremismusIslamismusSicherheitspolitikDie Abstimmung betrifft den Antrag der AfD-Fraktion, den Verein Muslim Interaktiv zu verbieten, da dieser als extremistisch und gegen die freiheitliche demokratische Grundordnung gerichtet angesehen wird. Der Bundestag lehnt den Antrag ab.Ergebnisse des BundestagsJa: 577(88%)Nein: 68(10%)Enthaltung: 11(2%)13. Juni 2024Gesamtstimmen: 656Antrag zum Eintritt in die zweite Beratung zur Aufhebung des LieferkettengesetzesWirtschaftRechtSozialesEuropäische UnionDie Abstimmung betrifft einen Antrag der CDU/CSU-Fraktion, direkt in die zweite Beratung eines Gesetzentwurfs zur Aufhebung des Lieferkettensorgfaltspflichtengesetzes (LkSG) einzutreten.Ergebnisse des BundestagsJa: 250(38%)Nein: 401(62%)Enthaltung: 1(0%)13. Juni 2024Gesamtstimmen: 65227. Juni 2024Bundeswehreinsatz EUFOR ALTHEAAußenpolitikSicherheitspolitikEuropäische UnionBundeswehrBosnien und HerzegowinaDie Abstimmung betrifft die Fortsetzung der Beteiligung bewaffneter deutscher Streitkräfte an der EU-geführten Sicherheitsoperation EUFOR ALTHEA in Bosnien und Herzegowina bis zum 30. Juni 2025.Ergebnisse des BundestagsJa: 553(87%)Nein: 74(12%)Enthaltung: 8(1%)27. Juni 2024Gesamtstimmen: 635Bundeswehreinsatz im Kosovo (KFOR)AußenpolitikSicherheitspolitikVerteidigungKosovoBundeswehrInternationale BeziehungenDie Abstimmung betrifft die Fortsetzung der Beteiligung bewaffneter deutscher Streitkräfte an der internationalen Sicherheitspräsenz im Kosovo (KFOR). Es geht darum, das Mandat für den Einsatz der Bundeswehr im Rahmen von KFOR zu verlängern.Ergebnisse des BundestagsJa: 535(85%)Nein: 91(14%)Enthaltung: 2(1%)27. Juni 2024Gesamtstimmen: 628Bundeswehreinsatz im Libanon (UNIFIL)AußenpolitikSicherheitspolitikBundeswehrLibanonUNIFILFriedensmissionDie Abstimmung betrifft die Fortsetzung der Beteiligung bewaffneter deutscher Streitkräfte an der \"United Nations Interim Force in Lebanon\" (UNIFIL), einer Friedensmission der Vereinten Nationen im Libanon. Es geht um die Verlängerung des Mandats und die damit verbundenen Aufgaben und Befugnisse der deutschen Soldaten.Ergebnisse des BundestagsJa: 534(85%)Nein: 88(14%)Enthaltung: 3(1%)27. Juni 2024Gesamtstimmen: 62505. Juli 2024Änderung des SchwangerschaftskonfliktgesetzesSozialesFamilieFrauenrechteGesundheitRechtDie Abstimmung betrifft die Änderung des Schwangerschaftskonfliktgesetzes (SchKG) und zielt darauf ab, einen bundeseinheitlichen und rechtssicheren Umgang mit sogenannten Gehsteigbelästigungen vor Schwangerschaftsberatungsstellen und Einrichtungen zur Vornahme von Schwangerschaftsabbrüchen sicherzustellen.Ergebnisse des BundestagsJa: 381(69%)Nein: 169(31%)Enthaltung: 0(0%)5. Juli 2024Gesamtstimmen: 55017. Oktober 2024Entwurf zur Aufhebung des LieferkettengesetzesWirtschaftMenschenrechteUmweltLieferkettenGesetzgebungEuropäische UnionDie Abstimmung betrifft den Gesetzentwurf der CDU/CSU zur Aufhebung des Lieferkettensorgfaltspflichtengesetzes (LkSG), das Unternehmen zur Einhaltung von Menschenrechten in ihren Lieferketten verpflichtet.Ergebnisse des BundestagsJa: 247(37%)Nein: 412(62%)Enthaltung: 1(1%)17. Okt. 2024Gesamtstimmen: 660Gesetzentwurf zur KrankenhausreformGesundheitKrankenhausVersorgungFinanzierungQualitätssicherungDie Abstimmung betrifft den Entwurf eines Gesetzes zur Verbesserung der Versorgungsqualität im Krankenhaus und zur Reform der Vergütungsstrukturen (KHVVG), das darauf abzielt, die Krankenhausversorgung zu verbessern und die Finanzierung zu reformieren.Ergebnisse des BundestagsJa: 373(57%)Nein: 285(43%)Enthaltung: 1(0%)17. Okt. 2024Gesamtstimmen: 659Bundeswehreinsatz im IrakAußenpolitikSicherheitspolitikMilitärIrakTerrorismusbekämpfungDie Abstimmung betrifft die Fortsetzung des Einsatzes bewaffneter deutscher Streitkräfte im Irak, um die Stabilisierung des Landes zu sichern, ein Wiedererstarken des IS (Islamischer Staat) zu verhindern und die Versöhnung im Irak zu fördern. Der Bundestag stimmt über den Antrag der Bundesregierung ab, diesen Einsatz bis zum 31. Januar 2026 mit bis zu 500 Soldatinnen und Soldaten zu verlängern.Ergebnisse des BundestagsJa: 539(85%)Nein: 93(15%)Enthaltung: 2(0%)17. Okt. 2024Gesamtstimmen: 63418. Oktober 2024Änderungsantrag zur „Verbesserung der inneren Sicherheit“Innere SicherheitAsylrechtMigrationspolitikGrenzschutzTerrorismusbekämpfungWaffenrechtDie Abstimmung betrifft einen Änderungsantrag der CDU/CSU-Fraktion zu einem Gesetzentwurf, der die innere Sicherheit und das Asylsystem verbessern soll, insbesondere durch Änderungen im Asylgesetz und im Ausweisungsrecht.Ergebnisse des BundestagsJa: 254(39%)Nein: 404(61%)Enthaltung: 1(0%)18. Okt. 2024Gesamtstimmen: 659Artikel 5 des Entwurfs „Verbesserung der inneren Sicherheit“ (2. Beratung)InneresSicherheitWaffenrechtExtremismusTerrorismusÖffentliche SicherheitDie Abstimmung betrifft Artikel 5 eines Gesetzentwurfs, der darauf abzielt, das Waffenrecht zu verschärfen, um Extremisten und Terroristen den Zugang zu Waffen zu erschweren und die öffentliche Sicherheit zu erhöhen.Ergebnisse des BundestagsJa: 375(57%)Nein: 275(42%)Enthaltung: 4(1%)18. Okt. 2024Gesamtstimmen: 654Gesetzentwurf „Verbesserung der Terrorismusbekämpfung“Innere SicherheitTerrorismusbekämpfungExtremismuspräventionDatenschutzBürgerrechteWaffenrechtDer Gesetzentwurf zur Verbesserung der Terrorismusbekämpfung zielt darauf ab, den Sicherheitsbehörden im digitalen Raum zusätzliche Befugnisse zu geben und die Zusammenarbeit zwischen Bund, Ländern und zivilgesellschaftlichen Akteuren zu stärken, um extremistischen und islamistischen Tendenzen entgegenzuwirken.Ergebnisse des BundestagsJa: 367(56%)Nein: 280(43%)Enthaltung: 4(1%)18. Okt. 2024Gesamtstimmen: 651Gesetzentwurf „Verbesserung der inneren Sicherheit“Innere SicherheitAsylrechtWaffenrechtExtremismusTerrorismusbekämpfungDer Gesetzentwurf zur Verbesserung der inneren Sicherheit und des Asylsystems betrifft die Anpassung von Gesetzen, um die innere Sicherheit Deutschlands zu stärken und das Asylsystem effektiver zu gestalten. Er reagiert auf aktuelle Bedrohungen und soll den Behörden mehr Handlungsspielraum geben.Ergebnisse des BundestagsJa: 357(55%)Nein: 289(44%)Enthaltung: 8(1%)18. Okt. 2024Gesamtstimmen: 654Erster Entschließungsantrag zur „Verbesserung der inneren Sicherheit“Innere SicherheitAsylpolitikMigrationspolitikWaffenrechtIslamismusExtremismusbekämpfungDie Abstimmung betrifft den Entschließungsantrag der CDU/CSU zum Gesetzentwurf zur Verbesserung der inneren Sicherheit und des Asylsystems, der sich mit der Migrationspolitik, dem Waffenrecht und der Bekämpfung des Islamismus auseinandersetzt.Ergebnisse des BundestagsJa: 237(36%)Nein: 405(62%)Enthaltung: 8(2%)18. Okt. 2024Gesamtstimmen: 650Zweiter Entschließungsantrag zur „Verbesserung der inneren Sicherheit“Innere SicherheitAsylpolitikWaffenrechtMigrationspolitikExtremismuspräventionDie Abstimmung betrifft einen Entschließungsantrag der CDU/CSU zum Gesetzentwurf zur Verbesserung der inneren Sicherheit und des Asylsystems, der sich gegen die Schwerpunkte der Regierungsfraktionen im Bereich Waffenrecht richtet und stattdessen eine andere Migrations- und Sicherheitspolitik fordert.Ergebnisse des BundestagsJa: 185(29%)Nein: 373(59%)Enthaltung: 73(12%)18. Okt. 2024Gesamtstimmen: 631Dritter Entschließungsantrag zur „Verbesserung der inneren Sicherheit“Innere SicherheitAsylrechtMigrationGrenzschutzEU-RechtDie Abstimmung betrifft einen Entschließungsantrag der CDU/CSU-Fraktion zu einem Gesetzentwurf, der die innere Sicherheit und das Asylsystem verbessern soll. Der Antrag zielt darauf ab, die Bundesregierung zu umfassenden Zurückweisungen von Personen ohne Einreiserecht an den deutschen Grenzen aufzufordern.Ergebnisse des BundestagsJa: 249(39%)Nein: 394(61%)Enthaltung: 1(0%)18. Okt. 2024Gesamtstimmen: 644Ablehnung eines Antrags zu MesserangriffenInneresRechtKriminalitätWaffenrechtMigrationSicherheitDie Beschlussempfehlung des Ausschusses für Inneres und Heimat betrifft die Ablehnung eines Antrags der AfD-Fraktion, der eine gezielte Sanktionierung von Messerangriffen anstelle von Verschärfungen im Waffenrecht fordert.Ergebnisse des BundestagsJa: 567(89%)Nein: 62(10%)Enthaltung: 7(1%)18. Okt. 2024Gesamtstimmen: 636Ablehnung eines Antrags zu DrittstaatenangehörigenAsylrechtMigrationspolitikGrenzkontrolleRücknahmeabkommenDie Abstimmung betrifft den Antrag der AfD-Fraktion, die Bundesregierung zu verpflichten, Drittstaatsangehörige an den Außengrenzen Deutschlands zurückzuweisen und bilaterale Rücknahmeabkommen mit Nachbarstaaten anzuwenden.Ergebnisse des BundestagsJa: 572(89%)Nein: 62(10%)Enthaltung: 8(1%)18. Okt. 2024Gesamtstimmen: 64216. Dezember 2024Antrag gemäß Artikel 68 des Grundgesetzes (Vertrauensfrage)PolitikRegierungGrundgesetzVertrauensfrageDie Abstimmung betrifft die Vertrauensfrage gemäß Artikel 68 des Grundgesetzes, bei der der Bundeskanzler das Vertrauen des Bundestages ausspricht. Der Bundeskanzler beabsichtigt, vor der Abstimmung eine Erklärung abzugeben.Ergebnisse des BundestagsJa: 207(29%)Nein: 394(55%)Enthaltung: 116(16%)16. Dez. 2024Gesamtstimmen: 71719. Dezember 2024Gesetzentwurf zur Änderung von Artikel 93 und 94 des Grundgesetzes (Bundesverfassungsgericht)VerfassungsrechtBundesverfassungsgerichtRechtsstaatPolitikJustizDer Gesetzentwurf betrifft die Änderung des Grundgesetzes in Bezug auf die Stellung und Struktur des Bundesverfassungsgerichts, indem Artikel 93 und 94 punktuell ergänzt und deren Inhalte systematisch neu geordnet werden. Ziel ist es, die Organqualität des Bundesverfassungsgerichts sowie die unmittelbare Bindung der öffentlichen Gewalt an seine Entscheidungen stärker hervorzuheben.Ergebnisse des BundestagsJa: 600(90%)Nein: 69(10%)Enthaltung: 0(0%)19. Dez. 2024Gesamtstimmen: 669",
    "summary": {
      "en": "Here’s a simplified summary of the key votes in the Bundestag for 2024:\n\n1. **January 17**: \n   - **Wehrbeauftragten's Annual Report**: CDU/CSU requested arms for Ukraine. Results: 178 Yes, 485 No.\n   - **Support for Agriculture**: CDU/CSU aimed to improve agricultural competitiveness. Results: 188 Yes, 420 No.\n   - **Peace Initiative for Ukraine/Russia**: AfD proposed a peace initiative. Results: 605 Yes, 75 No.\n\n2. **January 19**: \n   - **Modernization of Citizenship Law**: Aimed to ease citizenship access. Results: 382 Yes, 234 No.\n\n3. **February 1**: \n   - **Amendment of Federal Election Law**: Aimed for fair election districting. Results: 382 Yes, 276 No.\n\n4. **February 22**: \n   - **Support for Ukraine Against Russia**: Aimed to strengthen support for Ukraine. Results: 381 Yes, 284 No.\n   - **CDU/CSU Proposal for Foreign and Security Policy**: Called for a shift in policy due to threats from Russia. Results: 182 Yes, 479 No.\n\n5. **February 23**: \n   - **Growth Opportunities Law**: Aimed to promote investments and simplify taxes. Results: 376 Yes, 267 No.\n   - **Cannabis Law**: Regulated cannabis use for health and safety. Results: 404 Yes, 226 No.\n\n6. **March 14**: \n   - **Taurus Missiles for Ukraine**: CDU/CSU proposed arms support. Results: 494 Yes, 188 No.\n\n7. **March 20**: \n   - **Tax Relief for Agricultural Diesel**: AfD sought to maintain tax relief. Results: 553 Yes, 64 No.\n\n8. **March 21**: \n   - **Bundeswehr Deployment in the Mediterranean**: Approved continued military participation. Results: 563 Yes, 109 No.\n\n9. **April 12**: \n   - **Self-determination in Gender Registration**: Simplified gender registration laws. Results: 372 Yes, 251 No.\n\n10. **April 25**: \n    - **Operation EUNAVFOR MED IRINI**: Approved continued military involvement in Libya. Results: 554 Yes, 100 No.\n\n11. **April 26**: \n    - **Renewable Energy Law**: Aimed to speed up solar energy expansion. Results: 384 Yes, 79 No.\n\n12. **May 16**: \n    - **Opposition to WHO Pandemic Treaty**: AfD sought to reject the treaty. Results: 578 Yes, 71 No.\n\n13. **June 5**: \n    - **Opposition to Abolishing Solidarity Surcharge**: Results: 537 Yes, 62 No.\n\n14. **June 6**: \n    - **Opposition to Combating Political Islam**: Results: 406 Yes, 241 No.\n\n15. **June 13**: \n    - **Response to Terrorism in Mannheim**: CDU/CSU proposed stricter immigration policies. Results: 253 Yes, 399 No.\n\n16. **June 27**: \n    - **Bundeswehr Deployment in Bosnia**: Approved continued military mission. Results: 553 Yes, 74 No.\n\n17. **July 5**: \n    - **Amendment to Pregnancy Conflict Law**: Improved handling of pregnancy-related counseling. Results: 381 Yes, 169 No.\n\n18. **October 17**: \n    - **Repeal of Supply Chain Law**: Results: 247 Yes, 412 No.\n\n19. **October 18**: \n    - **Improvement of Internal Security**: Proposed changes to asylum laws. Results: 254 Yes, 404 No.\n\n20. **December 16**: \n    - **Confidence Vote for Chancellor**: Results: 207 Yes, 394 No.\n\n21. **December 19**: \n    - **Amendment to Constitutional Court Laws**: Aimed to enhance the court's role. Results: 600 Yes, 69 No.\n\nThis summary captures the main votes and their outcomes, highlighting key legislative themes throughout the year.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "9328671e85b8c779",
    "title": {
      "en": "Closing Keynote with Leslie Lamport at Scale 22x: Coding Isn't Programming",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.socallinuxexpo.org/scale/22x/presentations/closing-keynote-leslie-lamport",
    "score": 19,
    "by": "todsacerdoti",
    "time": 1742898863,
    "content": "Subscribe to announcements\n\nSubscribe\n\nClosing Keynote with Leslie Lamport\n\n          Leslie Lamport\n      Audience:     Everyone\n      Topic:     Keynote\n          Join us for a captivating closing keynote with the legendary Leslie Lamport, Turing Award winner and pioneer in the field of distributed computing. We'll discuss computing history, open source and distributed systems.\n\n      Presentation:      linux-expo (1).pdf\n      Room:     Ballroom DE\n      Time:     Sunday, March 9, 2025 - 15:00 to 16:00\n      Audio/Video:\n\nSubscribe to announcements\n\nSubscribe\n\nPlatinum Sponsors\n\n      Gold Sponsors\n\n      Silver Sponsors\n\n      Network Sponsors\n\n      Registration Sponsors\n\n      Reception Sponsors\n\n      Special Event Sponsors\n\n      Speaker Track Sponsors\n\n      Travel Sponsors\n\n      Media Sponsors\n\nAbout SCALE\nCode of Conduct\nContact Us\nPolicies",
    "summary": {
      "en": "Join us for the closing keynote by Leslie Lamport, a Turing Award winner known for his work in distributed computing. The talk will cover topics like computing history, open source, and distributed systems. \n\n**Details:**\n- **Date:** Sunday, March 9, 2025\n- **Time:** 15:00 to 16:00\n- **Location:** Ballroom DE\n\nVarious sponsors support the event, and more information is available about policies and conduct.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "dd21d9bbd9e6583f",
    "title": {
      "en": "Search My Site – open-source search engine for personal and independent websites",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://searchmysite.net",
    "score": 135,
    "by": "OuterVale",
    "time": 1742869143,
    "content": "Browse\n\n\t\t\t\t\t\t\t\t\tNewest\n\n\t\t\t\t\t\t\t\t\t\tRandom\n\n\t\t\t\t\t\t\tSearch real content by real people from the indieweb / the small web / digital gardens.\n\t\t\t\t\t\t\tSites are ranked higher when they have no ads.\n\t\t\t\t\t\t\tFully open source. Also a search as a service.\n\n\t\t\t\t\tAbout searchmysite.net\n\n\t\t\t\t\t\tWhat is the search engine and why should I use it?\n\n\t\t\t\t\t\tThe searchmysite.net search engine is a niche search, focussing on the \"indieweb\" or \"small web\" or \"digital gardens\", i.e. non-commercial content, primarily personal and independent websites.\n\n\t\t\t\t\t\tIf you want to research people's personal experiences of or deep-dives into certain topics, hobbies or interests, then you may find the searchmysite.net public search useful to avoid having to wade through all the marketing websites and blog spam that fill the big search engines. It differs from the commercial search engines because it:\n\n\t\t\t\t\t\tIndexes only user-submitted and moderated sites, rather than indexing the entire internet with all of its spam, \"search engine optimisation\" and \"click-bait\" content.\n\t\t\t\t\t\tDoes not show adverts and promotes advert-free results pages, to remove the incentives for spam and surveillance capitalism.\n\t\t\t\t\t\tAims to have a sustainable and user-aligned operating model, planning to pay running costs via the \"search as a service\" features, rather than relying on advertising which could put it in conflict with user needs.\n\t\t\t\t\t\tOffers an unusually high level of privacy for a search engine, thanks to the operating model which is not based on advertising or collecting personal information for sale to advertisers (see the Privacy Policy for full details).\n\t\t\t\t\t\tIs fully open source, for even greater transparency of the searching, ranking and indexing processes, and for greater community involvement in improving the service.",
    "summary": {
      "en": "**Summary of searchmysite.net**\n\nSearchmysite.net is a niche search engine focused on the \"indieweb,\" which includes personal and independent websites. It is designed for users who want to explore personal experiences and in-depth topics without the clutter of commercial ads and spam found on larger search engines. \n\nKey features include:\n\n- **User-Submitted Content**: It only indexes sites that are submitted and moderated by users, avoiding the vast spam of the entire internet.\n- **Ad-Free Experience**: There are no ads, promoting cleaner search results and reducing spam incentives.\n- **Sustainable Model**: It plans to cover operational costs through \"search as a service,\" not advertising, ensuring alignment with user needs.\n- **Privacy Focus**: It offers high privacy levels, not selling personal data to advertisers.\n- **Open Source**: The platform is fully open source, allowing for transparency and community involvement in its development.\n\nOverall, searchmysite.net is geared towards users looking for authentic, non-commercial content.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "dece63737e377a4b",
    "title": {
      "en": "Status as a Service (2019)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.eugenewei.com/blog/2019/2/19/status-as-a-service",
    "score": 42,
    "by": "simonebrunozzi",
    "time": 1742885984,
    "content": "Status as a Service (StaaS)\n\n          February 26, 2019\n           by Eugene Wei\n\n  Editor's Note 1: I have no editor. Editor’s Note 2: I would like to assure new subscribers to this blog that most my posts are not as long as this one. Or as long as my previous one. My long break from posting here means that this piece is a collection of what would’ve normally been a series of shorter posts. I put section titles below, so skip any that don’t interest you. My short takes are on Twitter. All that said, I apologize for nothing. Editor's Note 3: I lied, I apologize for one thing, and that is my long writing hiatus. Without a work computer, I had to resort to using my 7 year old 13\" Macbook Pro as my main computer, and sometime last year my carpal tunnel syndrome returned with a vengeance and left my wrists debilitated with pain. I believe all of you who say your main computer is a laptop or, shudder, an iPad, but goodness gracious I cannot type on a compact keyboard for long periods of time without having my hands turn into useless stumps. It was only the return to typing almost exclusively on my old friend the Kinesis Advantage 2 ergo keyboard that put me back in the game. Editor’s Note 4: I was recently on Patrick O'Shaughnessy's podcast Invest Like the Best, and near the end of that discussion, I mentioned a new essay I'd been working on about the similarities between social networks and ICO's. This is that piece.Status-Seeking Monkeys\"It is a truth universally acknowledged, that a person in possession of little fortune, must be in want of more social capital.\"So wrote Jane Austen, or she would have, I think, if she were chronicling our current age (instead we have Taylor Lorenz, and thank goodness for that).Let's begin with two principles:People are status-seeking monkeys*People seek out the most efficient path to maximizing social capital* Status-Seeking Monkeys will also be the name of my indie band, if I ever learn to play the guitar and start a bandI begin with these two observations of human nature because few would dispute them, yet I seldom see social networks, some of the largest and fastest-growing companies in the history of the world, analyzed on the dimension of status or social capital.It’s in part a measurement issue. Numbers lend an air of legitimacy and credibility. We have longstanding ways to denominate and measure financial capital and its flows. Entire websites, sections of newspapers, and a ton of institutions report with precision on the prices and movements of money.We have no such methods for measuring the values and movement of social capital, at least not with anywhere near the accuracy or precision. The body of research feels both broad and yet meager. If we had better measures besides user counts, this piece and many others would be full of charts and graphs that added a sense of intellectual heft to the analysis. There would be some annual presentation called the State of Social akin to Meeker's Internet Trends Report, or perhaps it would be a fifty page sub-section of her annual report.Despite this, most of the social media networks we study generate much more social capital than actual financial capital, especially in their early stages; almost all such companies have internalized one of the popular truisms of Silicon Valley, that in the early days, companies should postpone revenue generation in favor of rapid network scaling. Social capital has much to say about why social networks lose heat, stall out, and sometimes disappear altogether. And, while we may not be able to quantify social capital, as highly attuned social creatures, we can feel it.Social capital is, in many ways, a leading indicator of financial capital, and so its nature bears greater scrutiny. Not only is it good investment or business practice, but analyzing social capital dynamics can help to explain all sorts of online behavior that would otherwise seem irrational.In the past few years, much progress has been made analyzing Software as a Service (SaaS) businesses. Not as much has been made on social networks. Analysis of social networks still strikes me as being like economic growth theory long before Paul Romer's paper on endogenous technological change. However, we can start to demystify social networks if we also think of them as SaaS businesses, but instead of software, they provide status. This post is a deep dive into what I refer to as Status as a Service (StaaS) businesses.Think of this essay as a series of strongly held hypotheses; without access to the types of data which i’m not even sure exists, it’s difficult to be definitive. As ever, my wise readers will add or push back as they always do.Traditional Network Effects Model of Social NetworksOne of the fundamental lessons of successful social networks is that they must first appeal to people when they have few users. Typically this is done through some form of single-user utility. This is the classic cold start problem of social.  The answer to the traditional chicken-and-egg question is actually answerable: what comes first is a single chicken, and then another chicken, and then another chicken, and so on. The harder version of the question is why the first chicken came and stayed when no other chickens were around, and why the others followed.The second fundamental lessons is that social networks must have strong network effects so that as more and more users come aboard, the network enters a positive flywheel of growth, a compounding value from positive network effects that leads to hockey stick growth that puts dollar signs in the eyes of investors and employees  alike. \"Come for the tool, stay for the network\" wrote Chris Dixon, in perhaps the most memorable maxim for how this works.Even before social networks, we had Metcalfe's Law on telecommunications networks:The value of a telecommunications network is proportional to the square of the number of connected users of the system (n^2)This ported over to social networks cleanly. It is intuitive, and it includes that tantalizing math formula that explains why growth curves for social networks bends up sharply at the ankle of the classic growth S-curve.But dig deeper and many many questions remain. Why do some large social networks suddenly fade away, or lose out to new tiny networks? Why do some new social networks with great single-player tools fail to transform into networks, while others with seemingly frivolous purposes make the leap? Why do some networks sometimes lose value when they add more users? What determines why different networks stall out at different user base sizes? Why do some networks cross international borders easily while others stay locked within specific countries? Why, if Metcalfe's Law holds, do many of Facebook's clones of other social network features fail, while some succeed, like Instagram Stories?What ties many of these explanations together is social capital theory, and how we analyze social networks should include a study of a social network's accumulation of social capital assets and the nature and structure of its status games. In other words, how do such companies capitalize, either consciously or not, on the fact that people are status-seeking monkeys, always trying to seek more of it in the most efficient way possible?To paraphrase Nicki Minaj, “If I'm fake I ain't notice cause my followers ain't.”[Editor’s note: sometimes the followers actually are fake.]Utility vs. Social Capital FrameworkClassic network effects theory still holds, I’m not discarding it. Instead, let's append some social capital theory. Together, those form the two axes on which I like to analyze social network health.Actually, I tend to use three axes to dissect social networks.\n\n              View fullsize\n\n            The three axes on which I evaluate social network strength\n\n  For this post, though, I'm only going to look at two of them, utility and social capital, as the entertainment axis adds a whole lot of complexity which I'll perhaps explain another time.\n\n              View fullsize\n\n            The basic two axis framework guiding much of the social network analysis in this piece\n\n  Utility doesn't require much explanation, though we often use the term very loosely and categorize too many things as utility when they aren't that useful (we generally confuse circuses for bread and not the reverse; Fox News, for example, is more entertainment than utility, as is common of many news outlets). A social network like Facebook allows me to reach lots of people I would otherwise have a harder time tracking down, and that is useful. A messaging app like WhatsApp allows me to communicate with people all over the world without paying texting or incremental data fees, which is useful. Quora and Reddit and Discord and most every social network offer some forms of utility.The other axis is, for a lack of a more precise term, the social capital axis, or the status axis. Can I use the social network to accumulate social capital? What forms? How is it measured? And how do I earn that status?There are several different paths to success for social networks, but those which compete on the social capital axis are often more mysterious than pure utilities. Competition on raw utility tends to be Darwinian, ruthless, and highly legible. This is the world, for example, of communication services like messaging and video conferencing.  Investing in this space also tends to be a bit more straightforward: how useful is your app or service, can you get distribution, etc. When investors send me decks on things in this category, I am happy to offer an opinion, but I enjoy puzzling over the world of artificial prestige even more.The creation of a successful status game is so mysterious that it often smacks of alchemy. For that reason, entrepreneurs who succeed in this space are thought of us a sort of shaman, perhaps because most investors are middle-aged white men who are already so high status they haven't the first idea why people would seek virtual status (more on that later).With the rise of Instagram, with its focus on photos and filters, and Snapchat, with its ephemeral messaging, and Vine, with its 6-second video limit, for a while there was a thought that new social networks would be built on some new modality of communications. That's a piece of it, but it's not the complete picture, and not for the reasons many people think, which is why we have seen a whole bunch of strange failed experiments in just about every odd combinations of features and filters and artificial constraints in how we communicate with each other through our phones. Remember Facebook's Snapchat competitor Slingshot, in which you had to unlock any messages you received by responding with a message? It felt like product design by mad libs.When modeling how successful social networks create a status game worth playing, a useful metaphor is one of the trendiest technologies: cryptocurrency.Social Networks as ICO'sHow is a new social network analogous to an ICO? Each new social network issues a new form of social capital, a token.You must show proof of work to earn the token.Over time it becomes harder and harder to mine new tokens on each social network, creating built-in scarcity. Many people, especially older folks, scoff at both social networks and cryptocurrencies. [\"Why does anyone care what you ate for lunch?\" is the canonical retort about any social network, though it’s fading with time. Both social networks and ICO's tend to drive skeptics crazy because they seem to manufacture value out of nothing. The shifting nature of scarcity will always leave a wake of skepticism and disbelief.]Years ago, I stayed at the house of a friend whose high school daughter was home upstairs with a classmates. As we adults drank wine in the kitchen downstairs while waiting for dinner to finish in the oven, we heard lots of music and stomping and giggling coming from upstairs. When we finally called them down for dinner, I asked them what all the ruckus had been. My friend's daughter proudly held up her phone to show me a recording they'd posted to an app called Musical.ly. It was a lip synch and dance routine replete with their own choreography. They'd rehearsed the piece more times than they could count. It showed. Their faces were shiny with sweat, and they were still breathing hard from the exertion. Proof of work indeed.I spent the rest of the dinner scrolling through the app, fascinated, interviewing the girls about what they liked about the app, why they were on it, what share of their free time it had captured. I can't tell if parents are offended or glad when I spend much of the time visiting them interviewing their sons and daughters instead, but in the absence of good enough metrics with which to analyze this space, I subscribe to the Jane Goodall theory of how to study your subject. Besides, status games of adults are already well covered by the existing media, from literature to film. Children's status games, once familiar to us, begin to fade from our memory as time passes, and its modern forms have been drastically altered by social media.Other examples abound. Perhaps you've read a long and thoughtful response by a random person on Quora or Reddit, or watched YouTube vloggers publishing night after night, or heard about popular Vine stars living in houses together, helping each other shoot and edit 6-second videos. While you can outsource Bitcoin mining to a computer, people still mine for social capital on social networks largely through their own blood, sweat, and tears.[Aside: if you yourself are not an aspiring social network star, living with one is...not recommended.]Perhaps, if you've spent time around today's youth, you've watched with a mixture of horror and fascination as a teen snaps dozens of selfies before publishing the most flattering one to Instagram, only to pull it down if it doesn't accumulate enough likes within the first hour. It’s another example of proof of work, or at least vigorous market research.Almost every social network of note had an early signature proof of work hurdle. For Facebook it was posting some witty text-based status update. For Instagram, it was posting an interesting square photo. For Vine, an entertaining 6-second video. For Twitter, it was writing an amusing bit of text of 140 characters or fewer. Pinterest? Pinning a compelling photo. You can likely derive the proof of work for other networks like Quora and Reddit and Twitch and so on. Successful social networks don't pose trick questions at the start, it’s usually clear what they want from you.[An aside about exogenous social capital: you might complain that your tweets are more interesting and  grammatical than those of, say, Donald Trump (you're probably right!). Or that your photos are better composed and more interesting at a deep level of photographic craft than those of Kim Kardashian. The difference is, they bring a massive supply of exogenous pre-existing social capital from another status game, the fame game, to every table, and some forms of social capital transfer quite well across platforms. Generalized fame is one of them. More specific forms of fame or talent might not retain their value as easily: you might follow Paul Krugman on Twitter, for example, but not have any interest in his Instagram account. I don't know if he has one, but I probably wouldn't follow it if he did, sorry Paul, it’s nothing personal.]If you've ever joined one of these social networks early enough, you know that, on a relative basis, getting ahead of others in terms of social capital (followers, likes, etc.) is easier in the early days. Some people who were featured on recommended follower lists in the early days of Twitter have follower counts in the 7-figures, just as early masters of Musical.ly and Vine were accumulated massive and compounding follower counts. The more people who follow you, the more followers you gain because of leaderboards and recommended follower algorithms and other such common discovery mechanisms.It's true that as more people join a network, more social capital is up for grabs in the aggregate. However, in general, if you come to a social network later, unless you bring incredible exogenous social capital (Taylor Swift can join any social network on the planet and collect a massive following immediately), the competition for attention is going to be more intense than it was in the beginning. Everyone has more of an understanding of how the game works so the competition is stiffer.Why Proof of Work MattersWhy does proof of work matter for a social network? If people want to maximize social capital, why not make that as easy as possible?As with cryptocurrency, if it were so easy, it wouldn't be worth anything. Value is tied to scarcity, and scarcity on social networks derives from proof of work. Status isn't worth much if there's no skill and effort required to mine it. It's not that a social network that makes it easy for lots of users to perform well can't be a useful one, but competition for relative status still motivates humans. Recall our first tenet: humans are status-seeking monkeys. Status is a relative ladder. By definition, if everyone can achieve a certain type of status, it’s no status at all, it’s a participation trophy.Musical.ly created a hurdle for gaining followers and status that wasn't easily cleared by many people. However, for some, especially teens, and especially girls, it was a status game at which they were particularly suited to win. And so they flocked there, because, according to my second tenet, people look for the most efficient ways to accumulate the most social capital.Recall Twitter in the early days, when it was somewhat of a harmless but somewhat inert status update service. I went back to look at my first few tweets on the service from some 12 years ago and my first two, spaced about a year apart, were both about doing my taxes. Looking back at them, I bore even myself. Early Twitter consisted mostly of harmless but dull life status updates, a lot of “is this thing on?” tapping on the virtual microphone. I guess I am in the camp of not caring about what you had for lunch after all. Get off my lawn, err, phone screen!What changed Twitter, for me, was the launch of Favstar and Favrd (both now defunct, ruthlessly murdered by Twitter), these global leaderboards that suddenly turned the service into a competition to compose the most globally popular tweets. Recall, the Twitter graph was not as dense then as it was now, nor did distribution accelerants like one-click retweeting and Moments exist yet.What Favstar and Favrd did was surface really great tweets and rank them on a scoreboard, and that, to me, launched the performative revolution in Twitter. It added needed feedback to the feedback loop, birthing a new type of comedian, the master of the 140 character or less punchline (the internet has killed the joke, humor is all punchline now that the setup of the joke is assumed to be common knowledge thanks to Google).The launch of these global tweet scoreboards reminds me of the moment in the now classic film** Battle Royale when Beat Takeshi Kitano informs a bunch of troublemaking school kids that they’ve been deported to an island are to fight to the death, last student standing wins, and that those who try to sneak out of designated battle zones will be killed by explosive collars. I'm not saying that Twitter is a life-or-death struggle, but you need only time trav",
    "summary": {
      "en": "### Summary of \"Status as a Service (StaaS)\" by Eugene Wei\n\nThis essay explores the concept of \"Status as a Service\" (StaaS), likening social networks to businesses that provide social capital instead of software. The author argues that people are inherently motivated to seek social status, and social networks thrive by facilitating this pursuit.\n\n**Key Points:**\n\n1. **Human Nature and Status:** People naturally seek social capital, which is akin to status. Social networks are often analyzed in terms of financial capital, but understanding their dynamics through the lens of social capital is crucial.\n\n2. **Measurement Challenges:** Unlike financial capital, social capital is harder to quantify, leading to less precise analysis of social networks. Current metrics often focus on user counts rather than deeper social dynamics.\n\n3. **Network Effects:** Successful social networks grow by appealing to users initially with unique single-user benefits and then leveraging strong network effects to accelerate growth.\n\n4. **Utility vs. Social Capital:** Social networks can be evaluated on two axes: utility (how useful they are) and social capital (how they help users gain status). A successful social network balances both aspects.\n\n5. **Proof of Work:** Similar to cryptocurrencies, social networks require users to demonstrate effort (proof of work) to gain status, which creates scarcity and value for social capital.\n\n6. **Competition and Scarcity:** As more users join a network, the competition for social capital becomes fiercer. Early adopters often have an easier time gaining status, as the competition is less intense.\n\n7. **Dynamic Nature of Social Capital:** The essay emphasizes that social capital is not static and can change based on user interaction and network structure, influencing the overall health and sustainability of social networks.\n\nIn summary, Wei introduces a framework for analyzing social networks through the lens of status-seeking behavior, illustrating the importance of social capital in understanding their growth and dynamics.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "2c6f01f2468a616d",
    "title": {
      "en": "Qwen2.5-VL-32B: Smarter and Lighter",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://qwenlm.github.io/blog/qwen2.5-vl-32b/",
    "score": 479,
    "by": "tosh",
    "time": 1742841312,
    "content": "QWEN CHAT\nGITHUB\nHUGGING FACE\nMODELSCOPE\nDISCORDIntroduction#At the end of January this year, we launched the Qwen2.5-VL series of models, which received widespread attention and positive feedback from the community. Building on the Qwen2.5-VL series, we continued to optimize the model using reinforcement learning and open-sourced the new VL model with the beloved 32B parameter scale under the Apache 2.0 license — Qwen2.5-VL-32B-Instruct. Compared to the previously released Qwen2.5-VL series models, the features of this 32B VL model are as follows:Responses More Aligned with Human Preferences: Adjusted the output style to provide more detailed, better-formatted answers that align more closely with human preferences.Mathematical Reasoning: Significant improvement in the accuracy of solving complex mathematical problems.Fine-grained Image Understanding and Reasoning: Enhanced accuracy and detailed analysis in tasks such as image parsing, content recognition, and visual logic deduction.Performance#Extensive benchmarking against state-of-the-art (SoTA) models of comparable scale, Qwen2.5-VL-32B-Instruct has demonstrated superiority over baselines, e.g., Mistral-Small-3.1-24B and Gemma-3-27B-IT, even surpassing the larger Qwen2-VL-72B-Instruct. Notably, it achieves significant advantages in multimodal tasks such as MMMU, MMMU-Pro, and MathVista, which focus on complex, multi-step reasoning. On MM-MT-Bench, a benchmark emphasizing subjective user experience evaluation, Qwen2.5-VL-32B-Instruct outperforms its predecessor Qwen2-VL-72B-Instruct by a substantial margin.In addition to excelling in visual capabilities, Qwen2.5-VL-32B-Instruct has also achieved top-tier performance in pure text capabilities at the same scale.Demo Cases#Fine-grained Image Understanding and Reasoning (1/5)\nNextUserI am driving a large truck on this road, and it is now 12 o’clock. Can I reach a place 110 kilometers away before 13 o’clock?\nQwen2.5-VL-32BTo determine whether you can reach a place 110 kilometers away before 13:00, we need to analyze the information provided:Current Time: It is 12:00.Distance to Destination: 110 kilometers.Speed Limit for Trucks: The speed limit sign indicates that the maximum speed for trucks is 100 km/h.Step-by-Step Analysis:#1. Calculate the Required Travel Time#The formula to calculate travel time is:\nTime=DistanceSpeed\n\\text{Time} = \\frac{\\text{Distance}}{\\text{Speed}}\nTime=SpeedDistance\nGiven:Distance = 110 kmMaximum Speed = 100 km/hSubstitute the values:\nTime=110km100km/h=1.1hours\n\\text{Time} = \\frac{110 \\text{ km}}{100 \\text{ km/h}} = 1.1 \\text{ hours}\nTime=100km/h110km=1.1hours2. Convert Time to Hours and Minutes#1.1 hours is equivalent to:\n1hour+0.1×60minutes=1hour+6minutes=1hourand6minutes\n1 \\text{ hour} + 0.1 \\times 60 \\text{ minutes} = 1 \\text{ hour} + 6 \\text{ minutes} = 1 \\text{ hour and } 6 \\text{ minutes}\n1hour+0.1×60minutes=1hour+6minutes=1hourand6minutes3. Determine Arrival Time#If you start at 12:00 and it takes 1 hour and 6 minutes to travel 110 kilometers, your arrival time will be:\n12:00+1hourand6minutes=13:06\n12:00 + 1 \\text{ hour and } 6 \\text{ minutes} = 13:06\n12:00+1hourand6minutes=13:064. Compare with Target Time#You want to arrive before 13:00. However, based on the calculations, you will arrive at 13:06, which is after 13:00.Conclusion:#Given the speed limit of 100 km/h for trucks, it is not possible to reach a place 110 kilometers away before 13:00.Thus, the final answer is:\nNo\n\\boxed{\\text{No}}\nNoMathematical Reasoning (2/5)\nNextUserAs shown in the figure, the straight lines AB and CD intersect at point O, OD bisects ∠AOE, ∠BOC = 50.0, then ∠EOB = ()\nQwen2.5-VL-32BProblem Analysis#We are given the following information:Straight lines AB AB AB and CD CD CD intersect at point O O O.OD OD OD bisects ∠AOE \\angle AOE ∠AOE.∠BOC=50∘ \\angle BOC = 50^\\circ ∠BOC=50∘.We need to find ∠EOB \\angle EOB ∠EOB.Step-by-Step Solution#Step 1: Understand the given information and relationships#Since AB AB AB and CD CD CD are straight lines intersecting at O O O, the angles around point O O O sum up to 360∘ 360^\\circ 360∘.The vertically opposite angles are equal. Therefore:\n∠AOD=∠BOCand∠DOB=∠COA.\n\\angle AOD = \\angle BOC \\quad \\text{and} \\quad \\angle DOB = \\angle COA.\n∠AOD=∠BOCand∠DOB=∠COA.\nGiven ∠BOC=50∘ \\angle BOC = 50^\\circ ∠BOC=50∘, we have:\n∠AOD=50∘.\n\\angle AOD = 50^\\circ.\n∠AOD=50∘.Step 2: Use the angle bisector property#It is given that OD OD OD bisects ∠AOE \\angle AOE ∠AOE. This means:\n∠AOD=∠DOE.\n\\angle AOD = \\angle DOE.\n∠AOD=∠DOE.\nSince ∠AOD=50∘ \\angle AOD = 50^\\circ ∠AOD=50∘, we have:\n∠DOE=50∘.\n\\angle DOE = 50^\\circ.\n∠DOE=50∘.Step 3: Determine ∠AOE \\angle AOE ∠AOE#The angle ∠AOE \\angle AOE ∠AOE is the sum of ∠AOD \\angle AOD ∠AOD and ∠DOE \\angle DOE ∠DOE:\n∠AOE=∠AOD+∠DOE=50∘+50∘=100∘.\n\\angle AOE = \\angle AOD + \\angle DOE = 50^\\circ + 50^\\circ = 100^\\circ.\n∠AOE=∠AOD+∠DOE=50∘+50∘=100∘.Step 4: Use the straight line property#Since AB AB AB is a straight line, the angles on a straight line sum up to 180∘ 180^\\circ 180∘. Therefore:\n∠AOE+∠EOB=180∘.\n\\angle AOE + \\angle EOB = 180^\\circ.\n∠AOE+∠EOB=180∘.\nSubstituting ∠AOE=100∘ \\angle AOE = 100^\\circ ∠AOE=100∘, we get:\n100∘+∠EOB=180∘.\n100^\\circ + \\angle EOB = 180^\\circ.\n100∘+∠EOB=180∘.\nSolving for ∠EOB \\angle EOB ∠EOB:\n∠EOB=180∘−100∘=80∘.\n\\angle EOB = 180^\\circ - 100^\\circ = 80^\\circ.\n∠EOB=180∘−100∘=80∘.Final Answer#80\n\\boxed{80}\n80Mathematical Reasoning (3/5)\nNextUser侏罗纪蜘蛛网是一种非常有规律的蜘蛛网，如图，它是由无数个正方形环绕而成的，且最外边第一个正方形A_{1}B_{1}C_{1}D_{1}的面积为a_{1}=1，分别取正方形A_{1}B_{1}C_{1}D_{1}各边的三等分点A_{2}，B_{2}，C_{2}，D_{2}，作第二个正方形A_{2}B_{2}C_{2}D_{2}，且正方形A_{2}B_{2}C_{2}D_{2}的面积为a_{2}，\\ldots \\ldots ，依次类推，第n个正方形A_{n}B_{n}C_{n}D_{n}的面积为a_{n}. $求{a_{n}}的通项公式\nQwen2.5-VL-32B问题分析#我们需要找到第 n n n 个正方形的面积 an a_n an 的通项公式。已知：第一个正方形 A1B1C1D1 A_1B_1C_1D_1 A1B1C1D1 的面积为 a1=1 a_1 = 1 a1=1。每次取前一个正方形各边的三等分点，构造一个新的正方形。我们需要确定每次构造新正方形时，其面积与前一个正方形面积之间的关系。第一步：设第一个正方形的边长#设第一个正方形 A1B1C1D1 A_1B_1C_1D_1 A1B1C1D1 的边长为 s1 s_1 s1。由于其面积为 a1=1 a_1 = 1 a1=1，因此：\ns1=a1=1=1.\ns_1 = \\sqrt{a_1} = \\sqrt{1} = 1.\ns1=a1=1=1.第二步：构造第二个正方形#取 A1B1C1D1 A_1B_1C_1D_1 A1B1C1D1 各边的三等分点，得到新的正方形 A2B2C2D2 A_2B_2C_2D_2 A2B2C2D2。我们需要计算这个新正方形的边长。1. 确定新正方形的顶点坐标#假设第一个正方形 A1B1C1D1 A_1B_1C_1D_1 A1B1C1D1 的顶点坐标如下：A1=(0,1) A_1 = (0, 1) A1=(0,1),B1=(1,1) B_1 = (1, 1) B1=(1,1),C1=(1,0) C_1 = (1, 0) C1=(1,0),D1=(0,0) D_1 = (0, 0) D1=(0,0).取各边的三等分点：A2 A_2 A2 是 A1B1 A_1B_1 A1B1 的三等分点，坐标为 (13,1) \\left( \\frac{1}{3}, 1 \\right) (31,1),B2 B_2 B2 是 B1C1 B_1C_1 B1C1 的三等分点，坐标为 (1,23) \\left( 1, \\frac{2}{3} \\right) (1,32),C2 C_2 C2 是 C1D1 C_1D_1 C1D1 的三等分点，坐标为 (23,0) \\left( \\frac{2}{3}, 0 \\right) (32,0),D2 D_2 D2 是 D1A1 D_1A_1 D1A1 的三等分点，坐标为 (0,13) \\left( 0, \\frac{1}{3} \\right) (0,31).2. 计算新正方形的边长#新正方形 A2B2C2D2 A_2B_2C_2D_2 A2B2C2D2 的边长可以通过计算任意两点之间的距离得到。例如，计算 A2 A_2 A2 和 B2 B_2 B2 之间的距离：\nA2=(13,1),B2=(1,23).\nA_2 = \\left( \\frac{1}{3}, 1 \\right), \\quad B_2 = \\left( 1, \\frac{2}{3} \\right).\nA2=(31,1),B2=(1,32).\n两点间的距离公式为：\ns2=(x2−x1)2+(y2−y1)2.\ns_2 = \\sqrt{\\left( x_2 - x_1 \\right)^2 + \\left( y_2 - y_1 \\right)^2}.\ns2=(x2−x1)2+(y2−y1)2.\n代入坐标：\ns2=(1−13)2+(23−1)2=(23)2+(−13)2=49+19=59=53.\ns_2 = \\sqrt{\\left( 1 - \\frac{1}{3} \\right)^2 + \\left( \\frac{2}{3} - 1 \\right)^2} = \\sqrt{\\left( \\frac{2}{3} \\right)^2 + \\left( -\\frac{1}{3} \\right)^2} = \\sqrt{\\frac{4}{9} + \\frac{1}{9}} = \\sqrt{\\frac{5}{9}} = \\frac{\\sqrt{5}}{3}.\ns2=(1−31)2+(32−1)2=(32)2+(−31)2=94+91=95=35.3. 计算新正方形的面积#新正方形的面积 a2 a_2 a2 为：\na2=s22=(53)2=59.\na_2 = s_2^2 = \\left( \\frac{\\sqrt{5}}{3} \\right)^2 = \\frac{5}{9}.\na2=s22=(35)2=95.第三步：归纳规律#从上述计算可以看出：第一个正方形的边长为 s1=1 s_1 = 1 s1=1，面积为 a1=1 a_1 = 1 a1=1。第二个正方形的边长为 s2=53 s_2 = \\frac{\\sqrt{5}}{3} s2=35，面积为 a2=59 a_2 = \\frac{5}{9} a2=95。接下来，我们猜测每次构造新正方形时，边长和面积的变化规律。1. 边长变化规律#假设第 n n n 个正方形的边长为 sn s_n sn，则第 n+1 n+1 n+1 个正方形的边长 sn+1 s_{n+1} sn+1 可以通过相似三角形的性质得出。每次取三等分点构造的新正方形，其边长是前一个正方形边长的 53 \\frac{\\sqrt{5}}{3} 35 倍。因此：\nsn+1=53sn.\ns_{n+1} = \\frac{\\sqrt{5}}{3} s_n.\nsn+1=35sn.2. 面积变化规律#由于面积与边长的平方成正比，第 n+1 n+1 n+1 个正方形的面积 an+1 a_{n+1} an+1 为：\nan+1=sn+12=(53sn)2=(53)2an=59an.\na_{n+1} = s_{n+1}^2 = \\left( \\frac{\\sqrt{5}}{3} s_n \\right)^2 = \\left( \\frac{\\sqrt{5}}{3} \\right)^2 a_n = \\frac{5}{9} a_n.\nan+1=sn+12=(35sn)2=(35)2an=95an.这表明 {an} \\{a_n\\} {an} 是一个等比数列，首项为 a1=1 a_1 = 1 a1=1，公比为 59 \\frac{5}{9} 95。第四步：写出通项公式#等比数列的通项公式为：\nan=a1⋅qn−1,\na_n = a_1 \\cdot q^{n-1},\nan=a1⋅qn−1,\n其中 a1=1 a_1 = 1 a1=1，q=59 q = \\frac{5}{9} q=95。因此：\nan=1⋅(59)n−1=(59)n−1.\na_n = 1 \\cdot \\left( \\frac{5}{9} \\right)^{n-1} = \\left( \\frac{5}{9} \\right)^{n-1}.\nan=1⋅(95)n−1=(95)n−1.最终答案#(59)n−1\n\\boxed{\\left( \\frac{5}{9} \\right)^{n-1}}\n(95)n−1Mathematical Reasoning (4/5)\nNextUser如图，椭圆M:\\frac{y^2}{a^2}+\\frac{x^2}{b^2}=1(a>b>0)的两顶点为A(－2,0)，B(2,0)，离心率e=\\frac{\\sqrt{3}}{2}，过y轴上的点F(0,t)(|t|<4,t\neq 0)的直线l与椭圆交于C，D两点，并与x轴交于点P，直线AC与直线BD交于点Q. $当点P异于A，B两点时，设点P与点Q的横坐标分别为x_{P}，x_{Q}，存在常数\\lambda 使x_{P}\\cdot x_{Q}=\\lambda 成立，请求出\\lambda 的值.\nQwen2.5-VL题目分析#我们有一个椭圆 M:y2a2+x2b2=1 M: \\frac{y^2}{a^2} + \\frac{x^2}{b^2} = 1 M:a2y2+b2x2=1，其中 a>b>0 a > b > 0 a>b>0。已知椭圆的两顶点为 A(−2,0) A(-2, 0) A(−2,0) 和 B(2,0) B(2, 0) B(2,0)，离心率为 e=32 e = \\frac{\\sqrt{3}}{2} e=23。我们需要找到常数 λ \\lambda λ，使得点 P P P（直线 l l l 与 x x x-轴的交点）和点 Q Q Q（直线 AC AC AC 与直线 BD BD BD 的交点）的横坐标 xP x_P xP 和 xQ x_Q xQ 满足 xP⋅xQ=λ x_P \\cdot x_Q = \\lambda xP⋅xQ=λ。第一步：确定椭圆的参数#顶点信息：椭圆的顶点为 A(−2,0) A(-2, 0) A(−2,0) 和 B(2,0) B(2, 0) B(2,0)，说明椭圆的长轴在 x x x-轴上，且半长轴长度为 b=2 b = 2 b=2。因此，b=2 b = 2 b=2。离心率信息：离心率 e=32 e = \\frac{\\sqrt{3}}{2} e=23。离心率的公式为 e=ca e = \\frac{c}{a} e=ac，其中 c=a2−b2 c = \\sqrt{a^2 - b^2} c=a2−b2。代入 e=32 e = \\frac{\\sqrt{3}}{2} e=23，得到：\nca=32  ⟹  c=32a.\n\\frac{c}{a} = \\frac{\\sqrt{3}}{2} \\implies c = \\frac{\\sqrt{3}}{2} a.\nac=23⟹c=23a.又因为 c=a2−b2 c = \\sqrt{a^2 - b^2} c=a2−b2，代入 b=2 b = 2 b=2，得到：\na2−4=32a.\n\\sqrt{a^2 - 4} = \\frac{\\sqrt{3}}{2} a.\na2−4=23a.平方两边：\na2−4=34a2  ⟹  14a2=4  ⟹  a2=16  ⟹  a=4.\na^2 - 4 = \\frac{3}{4} a^2 \\implies \\frac{1}{4} a^2 = 4 \\implies a^2 = 16 \\implies a = 4.\na2−4=43a2⟹41a2=4⟹a2=16⟹a=4.因此，椭圆的方程为：\ny216+x24=1.\n\\frac{y^2}{16} + \\frac{x^2}{4} = 1.\n16y2+4x2=1.第二步：直线 l l l 的方程#直线 l l l 过点 F(0,t) F(0, t) F(0,t)，设其斜率为 k k k，则直线 l l l 的方程为：\ny=kx+t.\ny = kx + t.\ny=kx+t.第三步：直线 l l l 与椭圆的交点#将直线方程 y=kx+t y = kx + t y=kx+t 代入椭圆方程 y216+x24=1 \\frac{y^2}{16} + \\frac{x^2}{4} = 1 16y2+4x2=1，得到：\n(kx+t)216+x24=1.\n\\frac{(kx + t)^2}{16} + \\frac{x^2}{4} = 1.\n16(kx+t)2+4x2=1.\n整理得：\n(kx+t)2+4x2=16  ⟹  k2x2+2ktx+t2+4x2=16  ⟹  (k2+4)x2+2ktx+t2−16=0.\n(kx + t)^2 + 4x^2 = 16 \\implies k^2x^2 + 2ktx + t^2 + 4x^2 = 16 \\implies (k^2 + 4)x^2 + 2ktx + t^2 - 16 = 0.\n(kx+t)2+4x2=16⟹k2x2+2ktx+t2+4x2=16⟹(k2+4)x2+2ktx+t2−16=0.\n这是关于 x x x 的二次方程，设其两根为 x1 x_1 x1 和 x2 x_2 x2，则交点 C(x1,y1) C(x_1, y_1) C(x1,y1) 和 D(x2,y2) D(x_2, y_2) D(x2,y2) 的横坐标分别为 x1 x_1 x1 和 x2 x_2 x2。由韦达定理，有：\nx1+x2=−2ktk2+4,x1x2=t2−16k2+4.\nx_1 + x_2 = -\\frac{2kt}{k^2 + 4}, \\quad x_1 x_2 = \\frac{t^2 - 16}{k^2 + 4}.\nx1+x2=−k2+42kt,x1x2=k2+4t2−16.第四步：点 P P P 的横坐标#直线 l l l 与 x x x-轴的交点 P P P 的纵坐标为 0，代入直线方程 y=kx+t y = kx + t y=kx+t，得到：\n0=kxP+t  ⟹  xP=−tk.\n0 = kx_P + t \\implies x_P = -\\frac{t}{k}.\n0=kxP+t⟹xP=−kt.第五步：直线 AC AC AC 和 BD BD BD 的方程#直线 AC AC AC 的方程：点 A(−2,0) A(-2, 0) A(−2,0) 和点 C(x1,y1) C(x_1, y_1) C(x1,y1)，直线 AC AC AC 的斜率为：\ny1−0x1+2=y1x1+2.\n\\frac{y_1 - 0}{x_1 + 2} = \\frac{y_1}{x_1 + 2}.\nx1+2y1−0=x1+2y1.因此，直线 AC AC AC 的方程为：\ny=y1x1+2(x+2).\ny = \\frac{y_1}{x_1 + 2}(x + 2).\ny=x1+2y1(x+2).直线 BD BD BD 的方程：点 B(2,0) B(2, 0) B(2,0) 和点 D(x2,y2) D(x_2, y_2) D(x2,y2)，直线 BD BD BD 的斜率为：\ny2−0x2−2=y2x2−2.\n\\frac{y_2 - 0}{x_2 - 2} = \\frac{y_2}{x_2 - 2}.\nx2−2y2−0=x2−2y2.因此，直线 BD BD BD 的方程为：\ny=y2x2−2(x−2).\ny = \\frac{y_2}{x_2 - 2}(x - 2).\ny=x2−2y2(x−2).第六步：点 Q Q Q 的横坐标#点 Q Q Q 是直线 AC AC AC 和直线 BD BD BD 的交点，联立方程：\ny1x1+2(x+2)=y2x2−2(x−2).\n\\frac{y_1}{x_1 + 2}(x + 2) = \\frac{y_2}{x_2 - 2}(x - 2).\nx1+2y1(x+2)=x2−2y2(x−2).\n将 y1=kx1+t y_1 = kx_1 + t y1=kx1+t 和 y2=kx2+t y_2 = kx_2 + t y2=kx2+t 代入，得到：\nkx1+tx1+2(x+2)=kx2+tx2−2(x−2).\n\\frac{kx_1 + t}{x_1 + 2}(x + 2) = \\frac{kx_2 + t}{x_2 - 2}(x - 2).\nx1+2kx1+t(x+2)=x2−2kx2+t(x−2).\n交叉相乘得：\n(kx1+t)(x2−2)(x+2)=(kx2+t)(x1+2)(x−2).\n(kx_1 + t)(x_2 - 2)(x + 2) = (kx_2 + t)(x_1 + 2)(x - 2).\n(kx1+t)(x2−2)(x+2)=(kx2+t)(x1+2)(x−2).\n展开并整理，可以解出 xQ x_Q xQ。经过复杂但标准的代数运算，可以得到：\nxQ=4⋅x1x2−2(x1+x2)x1+x2−4.\nx_Q = 4 \\cdot \\frac{x_1 x_2 - 2(x_1 + x_2)}{x_1 + x_2 - 4}.\nxQ=4⋅x1+x2−4x1x2−2(x1+x2).\n将韦达定理的结果 x1+x2=−2ktk2+4 x_1 + x_2 = -\\frac{2kt}{k^2 + 4} x1+x2=−k2+42kt 和 x1x2=t2−16k2+4 x_1 x_2 = \\frac{t^2 - 16}{k^2 + 4} x1x2=k2+4t2−16 代入，化简后得到：\nxQ=−4tk.\nx_Q = -\\frac{4t}{k}.\nxQ=−k4t.第七步：计算 xP⋅xQ x_P \\cdot x_Q xP⋅xQ#点 P P P 的横坐标为 xP=−tk x_P = -\\frac{t}{k} xP=−kt，点 Q Q Q 的横坐标为 xQ=−4tk x_Q = -\\frac{4t}{k} xQ=−k4t。因此：\nxP⋅xQ=(−tk)(−4tk)=4t2k2.\nx_P \\cdot x_Q = \\left(-\\frac{t}{k}\\right) \\left(-\\frac{4t}{k}\\right) = \\frac{4t^2}{k^2}.\nxP⋅xQ=(−kt)(−k4t)=k24t2.\n注意到 k k k 和 t t t 的具体值不影响最终的乘积形式，且根据对称性和几何性质，可以确定 xP⋅xQ x_P \\cdot x_Q xP⋅xQ 为一个常数。通过进一步分析或直接利用椭圆的几何性质，可以得出：\nxP⋅xQ=4.\nx_P \\cdot x_Q = 4.\nxP⋅xQ=4.最终答案#4\n\\boxed{4}\n4Fine-grained Image Understanding (5/5)\nNextUser这是什么Qwen2.5-VL-32B这张图片展示的是一道中式火锅，具体来说是四川麻辣火锅。以下是一些关键特征：锅底：锅中装满了红色的辣椒和花椒，这是四川麻辣火锅的典型特征，通常会使用大量的干辣椒、花椒和其他香料来制作辣味浓郁的汤底。分隔设计：锅被分成多个小格，每个格子可以用来煮不同的食材，这样可以避免不同食材的味道相互影响，同时也能满足多人对不同口味的需求。周围配菜：可以看到一些切好的肉类（如牛肉片）、蔬菜和其他食材，这些都是火锅常见的涮菜。氛围感：整体画面呈现出一种热气腾腾、色彩鲜艳的感觉，符合火锅的热闹和美味的氛围。火锅简介：#四川麻辣火锅是中国火锅的一种，起源于四川省，以其独特的麻辣风味闻名。主要特点包括：使用大量辣椒和花椒，味道辛辣且麻。汤底通常由牛油或清油制成，加入各种香料熬制。配菜丰富多样，包括肉类、海鲜、蔬菜等，适合多人一起享用。这种火锅不仅是一种美食，也是一种社交方式，非常适合家庭聚会或朋友聚餐。User图中为什么是四川火锅Qwen2.5-VL-32B从图片中可以看出，这是一道典型的四川火锅，主要依据以下几个特征：1. 锅底的麻辣风格#图中的锅底颜色鲜艳，呈现出明显的红色，这是四川火锅的典型特征。四川火锅以“麻辣”著称，通常会使用大量的干辣椒、花椒等调料来制作辣味浓郁的汤底。红色的辣椒和花椒是四川火锅的灵魂，它们不仅赋予了火锅独特的风味，还体现了四川人对“麻”和“辣”的热爱。2. 分隔设计#锅被分成了多个小格，这种设计在四川火锅中非常常见。每个格子可以用来煮不同的食材，避免不同食材的味道相互影响，同时也能满足多人对不同口味的需求。这种设计也符合四川火锅的多样性和灵活性，适合多人一起享用。3. 配菜种类#周围可以看到一些切好的肉类（如牛肉片）和其他食材，这些都是四川火锅常见的涮菜。四川火锅的配菜种类丰富，包括肉类、海鲜、蔬菜等，能够满足不同人的口味需求。4. 氛围感#整体画面呈现出一种热气腾腾、色彩鲜艳的感觉，充满了火锅的热闹和美味氛围。四川火锅以其麻辣鲜香的特点，深受人们喜爱，尤其是在寒冷的季节或聚会时，更是不可或缺的美食。5. 地域文化#四川火锅是中国火锅的一种，起源于四川省，以其独特的麻辣风味闻名。四川人对辣椒和花椒有着深厚的情感，这也体现在他们的饮食文化中。四川火锅正是这种文化的体现。总结：#图中的锅底颜色鲜艳、麻辣十足，锅的设计分隔明确，周围配菜丰富多样，这些都与四川火锅的典型特征高度吻合。因此，可以确定这是一道四川火锅。Next Step#While Qwen2.5-VL-32B has focused on optimizing subjective experience and mathematical reasoning through reinforcement learning—operating within the paradigm of “fast thinking”. Our next research direction will prioritize long and effective reasoning processes to push the boundaries of visual models in tackling highly complex, multi-step visual reasoning tasks.Citation#If you find our model helpful, feel free to cite it:@article{Qwen2.5-VL,\n  title={Qwen2.5-VL Technical Report},\n  author={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and Zhong, Humen and Zhu, Yuanzhi and Yang, Mingkun and Li, Zhaohai and Wan, Jianqiang and Wang, Pengfei and Ding, Wei and Fu, Zheren and Xu, Yiheng and Ye, Jiabo and Zhang, Xi and Xie, Tianbao and Cheng, Zesen and Zhang, Hang and Yang, Zhibo and Xu, Haiyang and Lin, Junyang},\n  journal={arXiv preprint arXiv:2502.13923},\n  year={2025}\n}",
    "summary": {
      "en": "In January, the Qwen2.5-VL series models were launched, receiving positive feedback. The new model, Qwen2.5-VL-32B-Instruct, has been optimized with reinforcement learning and is open-sourced under the Apache 2.0 license. Key features of the 32B model include:\n\n1. **Human-like Responses**: Outputs are more detailed and formatted to match human preferences.\n2. **Improved Math Skills**: Enhanced accuracy in solving complex mathematical problems.\n3. **Better Image Understanding**: Higher accuracy in image tasks like content recognition and visual reasoning.\n\nPerformance benchmarks show that Qwen2.5-VL-32B-Instruct outperforms comparable models, especially in multimodal tasks and subjective user experience evaluations.\n\nThe model also excels in text processing and has demonstrated strong capabilities in various demo cases, such as fine-grained image understanding and mathematical reasoning.\n\nFuture research will focus on developing long and effective reasoning processes for complex visual tasks to further enhance the model's capabilities.\n\nIf you find this model useful, you can cite it as a technical report.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "2fe01a4dc45a027a",
    "title": {
      "en": "Triforce – a beamformer for Apple Silicon laptops",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/chadmed/triforce",
    "score": 536,
    "by": "tosh",
    "time": 1742827534,
    "content": "Triforce - a beamformer for Apple Silicon laptops\nTriforce implements a Minimum Variance Distortionless Response adaptive beamformer\nfor the microphone array found in the following Apple Silicon laptops:\n\nMacBook Pro 13\" (M1/M2)\nMacBook Air 13\" (M1/M2)\nMacBook Pro 14\" (M1 Pro/Max, M2 Pro/Max)\nMacBook Pro 16\" (M1 Pro/Max, M2 Pro/Max)\nMacBook Air 15\" (M2)\n\nDependencies\nTriforce tries to keep dependencies to a minimum. On top of the\ncrates specified in Cargo.lock, you will only require:\n\nLV2\n\nWhy?\nThe Apple Silicon laptops mentioned above have an array of microphones arranged\neither in a triangle or in a straight line. Much like with the speakers, Apple are trying way too hard to be\nfancy here, and implement an adaptive beamformer in userspace to try and isolate\nthe desired signal from background noise. Without beamforming, the array is far\ntoo sensitive and omnidirectional to be at all useful. Thus, to make it useful outside\nof macOS, we need a beamformer.\nExpectation management\nFinding accessible literature on any DSP-related topics, let alone something like\nwideband adaptive beamforming, is challenging to say the least. This is an attempt\nat a beamformer armed only with first year undergrad level engineering maths and some vague\nidea of the principles gleaned from various webpages and PDFs. Do not expect it to\noutperform Apple's implementation at this time. Patches to improve it are always welcome!\nKnown limitations\n\nnalgebra does not do any explicit SIMD optimisation, relying only on LLVM auto-vectorisation.\nPerformance and efficiency of matrix math routines are not very good.\nFollowing from that, we are not doing wideband decomposition due to the added computational\nburden. Without SIMD/NEON support, this is simply too slow for a realtime audio plugin.\nOutput is mono only. Much like with wideband decomposition, adding additional matrix processing\nto fake stereo output would be too computationally intensive",
    "summary": {
      "en": "**Summary of Triforce - Beamformer for Apple Silicon Laptops**\n\nTriforce is a software tool designed to improve microphone performance on certain Apple Silicon laptops, specifically:\n\n- MacBook Pro 13\" (M1/M2)\n- MacBook Air 13\" (M1/M2)\n- MacBook Pro 14\" (M1 Pro/Max, M2 Pro/Max)\n- MacBook Pro 16\" (M1 Pro/Max, M2 Pro/Max)\n- MacBook Air 15\" (M2)\n\nIt uses a technique called Minimum Variance Distortionless Response (MVDR) adaptive beamforming, which helps isolate desired sounds from background noise. This is important because the built-in microphones can pick up too much noise without such processing.\n\n**Key Points:**\n- Triforce has minimal dependencies, mainly requiring LV2.\n- The software aims to make the microphones more useful outside of macOS.\n- It is a work in progress and may not perform as well as Apple's built-in solution.\n- There are limitations, including:\n  - No advanced optimization for performance.\n  - It can only output mono sound.\n  - It does not handle wideband processing due to high computational demands.\n\nThe creator encourages contributions to improve the project.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "e49db46a233384da",
    "title": {
      "en": "Reflecting on WikiTok",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.aizk.sh/posts/reflecting-on-wikitok",
    "score": 63,
    "by": "aizk",
    "time": 1742882951,
    "content": "Reflecting on WikiTok\n\nThree weeks ago, I launched Wikitok at 1:53 AM EST from the corner of my Brooklyn bedroom.\nAll the great software engineers I admire take time to write out their thoughts, so I'd like to do the same for WikiTok, while also sharing my experience in software in general. So much has transpired in a very short amount of time - I'm certain the lessons I learned will be valuable to anyone reading this.\nThe Background\nI built wikitok.io in about 2 hours (but not the iphone app that doesn't work, nor the play store rip off, nor wikitok.net but I'm getting ahead of myself). It all came from this tweet. Simply put it's infinite scrolling wikipedia. The details of the initial process can be found in this Ars Technica article - instead of rehashing those details too much I'm going to focus more on the code as well as the aftermath / what I learned from virality.\nThe Codebase\nThe tech stack is bun, react and typescript. I combined the code from the very first working commit and pasted it below, and we'll go through it line by line.\nimport React, { useState, useEffect, useRef, useCallback } from 'react';\n\n// Types\ninterface WikiArticle {\n   title: string;\n   extract: string;\n   pageid: number;\n   thumbnail?: {\n       source: string;\n       width: number;\n       height: number;\n   };\n}\n\ninterface WikiCardProps {\n   article: WikiArticle;\n   key?: React.Key;\n}\nRight now we're just defining the shape of the wiki data. Thumbnails are technically optional, which is why we have a ? after the type definition, but in later iterations I ditched articles without thumbnails altogether. The key helps React understand which items are new or have been removed from the list, but is actually just extra code from Claude in this instance.\n// WikiCard Component\nfunction WikiCard({ article }: WikiCardProps) {\n   return (\n       <div className=\"h-screen w-full flex items-center justify-center snap-start\">\n           <div className=\"bg-white max-w-lg w-full mx-4 rounded-xl shadow-xl p-6\">\n               {article.thumbnail && (\n                   <img\n                       src={article.thumbnail.source}\n                       alt={article.title}\n                       className=\"w-full h-48 object-cover rounded-lg mb-4\"\n                   />\n               )}\n               <h2 className=\"text-2xl font-bold mb-3\">{article.title}</h2>\n               <p className=\"text-gray-700\">{article.extract}</p>\n               <a\n                   href={`https://en.wikipedia.org/?curid=${article.pageid}`}\n                   target=\"_blank\"\n                   rel=\"noopener noreferrer\"\n                   className=\"mt-4 inline-block text-blue-500 hover:text-blue-600\"\n               >\n                   Read more →\n               </a>\n           </div>\n       </div>\n   );\n}\nThis component is the heart of the UI. We are essentially saying, \"for each article, render a div with a thumbnail, title, extract, and a link to the full article\", and it must be in the shape of the WikiCardProps interface. The only minor issue with this is that the curid parameter is technically the wrong way to query articles - as pointed out by an engineer from Wikipedia here.\nSo, what does it look like?\n\nBesides the accidental white text on the white background, not bad! It is however way too tiny, but we can fix that later. It seems Claude designed it from the start to only be TikTok sized.\n// Custom Hook for Wiki Articles\nfunction useWikiArticles() {\n   const [articles, setArticles] = useState<WikiArticle[]>([]);\n   const [loading, setLoading] = useState(false);\n\n   const fetchArticles = async () => {\n       setLoading(true);\n       try {\n           const response = await fetch(\n               \"https://en.wikipedia.org/w/api.php?\" +\n               new URLSearchParams({\n                   action: \"query\",\n                   format: \"json\",\n                   generator: \"random\",\n                   grnnamespace: \"0\",\n                   prop: \"extracts|pageimages\",\n                   grnlimit: \"5\",\n                   exchars: \"200\",\n                   exlimit: \"max\",\n                   explaintext: \"1\",\n                   piprop: \"thumbnail\",\n                   pithumbsize: \"400\",\n                   origin: \"*\",\n               })\n           );\n           const data = await response.json();\n           const newArticles = Object.values(data.query.pages).map((page: any) => ({\n               title: page.title,\n               extract: page.extract,\n               pageid: page.pageid,\n               thumbnail: page.thumbnail,\n           }));\n           setArticles((prev) => [...prev, ...newArticles]);\n       } catch (error) {\n           console.error(\"Error fetching articles:\", error);\n       }\n       setLoading(false);\n   };\n\n   return { articles, loading, fetchArticles };\n}\nThis is a custom hook that fetches the articles from Wikipedia's API.\nWe call the Wikipedia API for 5 new articles every fetch, parse the JSON response, and extend our existing article list using the spread operator.\nThe only thing that immediately jumps out is the \"any\" type - but I've spent a lot of time agonizing over needless type complexity in typescript projects without actually moving the project forward at all, essentially just using it as some sort of procrastination tool. This is a really devious trap to find yourself in.\nThe \"proper\" solution would be to define an interface like WikiApiResponse that describes the shape of the response, and then use that to type the response, but in a project this simple, it's clear that the use of \"any\" won't make a difference to the safety of the code - so don't worry too hard!\nWhat matters above all is that you ship something, anything at all.\nGet feedback, get users, and then iterate. The codebase has changed significantly since this initial commit, shaped by user feedback and bug fixes - your users will tell you what needs to be changed.\nI mentioned before that the hook helps us fetch articles, but how do we know when to fetch more articles?\n// Main App Component\nfunction App() {\n   const { articles, loading, fetchArticles } = useWikiArticles();\n   const observerTarget = useRef(null);\n\n   const handleObserver = useCallback(\n       (entries: IntersectionObserverEntry[]) => {\n           const [target] = entries;\n           if (target.isIntersecting && !loading) {\n               fetchArticles();\n           }\n       },\n       [loading, fetchArticles]\n   );\n\n   useEffect(() => {\n       const observer = new IntersectionObserver(handleObserver, {\n           threshold: 0.5,\n       });\n\n       if (observerTarget.current) {\n           observer.observe(observerTarget.current);\n       }\n\n       return () => observer.disconnect();\n   }, [handleObserver]);\n\n   useEffect(() => {\n       fetchArticles();\n   }, []);\n\n   return (\n       <div className=\"h-screen w-full bg-black text-white overflow-y-scroll snap-y snap-mandatory\">\n           {articles.map((article: WikiArticle) => (\n               <WikiCard key={article.pageid} article={article} />\n           ))}\n           <div ref={observerTarget} className=\"h-10\" />\n           {loading && (\n               <div className=\"h-screen w-full flex items-center justify-center\">\n                   Loading...\n               </div>\n           )}\n       </div>\n   );\n}\n\nexport default App;\nThis is the most interesting bit of code in the project. In my head, the way I conceptualized this was that there should always be a buffer of articles underneath the current article, and it's triggered as the user continues to scroll. Not too many fetched where it slows the user experience but enough so that they never reach the \"end\" of wikipedia.\nClaude's idea was to create an invisible div at the bottom of our article list and attach an observer to it. When this div becomes visible (meaning the user has scrolled near the bottom), we fetch more articles. All in all, pretty clever. Truthfully, I've never designed an infinite scrolling interface before, so I was completely winging it.\nIs this code excellent? No. But was this code ready to be shipped? No not even close, I had to go back and make a dozen more fast commits before I felt comfortable publishing the project.\nBut did this get the ball rolling and made me realize what I could build? Yes.\nThe Development Process\nMy thought process working on wikitok was completely backwards to what standard software engineering principles would tell you - I let Claude with Cursor do the vast majority of the work, where I gave it the specs I wanted and I prompted away. I maintained a running list of all the features I wanted, testing and checking things off one by one, committing quick wins. Get the feature, take a peek at the code, move on to the next requirement, maybe compartmentalize some of the code here and there so it's nice and functional (I mean that both in a programming sense and literal sense). It worked out well - for instance Claude surprised me at the start letting me know I wouldn't even need a backend to access Wikipedia's API (no CORS restriction) which made deployment far easier for me. Interestingly, Wikimedia themselves later asked about my experience using their API, noting that many developers struggle with finding and parsing their documentation - they were surprised when I mentioned I hadn't used their docs for the MVP at all. You can find their docs here.\nThis approach can backfire if you try it in a professional setting. So why did I roll with it?\nWell for a start - I've had thousands of conversations with different AIs, and I've developed a pretty decent radar for their capabilities, when they make things up, when to completely ignore them, etc. The caveat to this is I'd imagine this wouldn't be the case if I started building in a language I've never used before. I also had a gut feeling from the start the MVP would be less than 500 lines, which is more than enough for the context of any modern LLM. The last point is that speed was the real priority here. There was already a tweet gaining momentum asking for \"All of wikipedia on one page\", which is what inspired the entire project. Building wikitok with that exact tweet in mind, knowing people might see the two tweets side by side proved to be wildly helpful.\nTo be clear to any junior engineers reading this (I'm kinda talking to myself here) - with current AI models, vibing your way to success will completely break down once you try to contribute to a serious production codebase. I think about this a lot, and I learned this the hard way when it came time to contribute to the Minecraft Benchmark, as the project is thousands and thousands of lines of code. But for simple, small projects? Some random super specific, monotonous office task? Lean in on the AI.\nUltimately, even with AI, software engineering will still be the job of giving computers precise instructions to do things, but how we do that will change. The field reinvents itself each decade, and AI is no exception.\nLessons from Going Viral\nGoing viral is stressful. It's a bit like winning the lottery, except there's a catch. As you improve your skills, you get to enter more tickets into the lottery. This is really critical - it's still the lottery, it's unfair, but your odds can just keep getting better and better, and eventually one day everyone will notice what you've built. I've had viral tweets before, and I've made a bunch of quick websites, but this is the first time I combined the two.\nThe other point in this is that you have to know what to do when you get lucky. If this was the first ever React project I made, I am certain I would've fumbled the launch and completely screwed it up, but thankfully it wasn't! I quite like this point from Brandon Sanderson, he mentioned in his writing lectures that if you haven't written 3 novels, your job is not to sell books. You job is to write enough books to learn your process and what works for you.\nSo you're viral, now what? Focus your energy on the platform that helped you achieve virality because that's where the most eyeballs are. You won't be able to answer every question and comment people throw at you. I made that mistake, getting way too caught up in the frenzy.\nOn answering comments - not all comment sections are equal. I spent quite a lot of time responding to questions in the initial hacker news post because I knew going viral on hacker news and making a good positive impression on that site would push my post up, and from there I'd get an enormous amount of traffic. Hacker news also tends to be more critical of specific technical details than lets say a random news subreddit, so you might get more value out of it (well, not always). The general flow of information was something like twitter → hacker news → mainstream news outlets → reddit / youtube.\nAlso be prepared to not sleep much for the first day of the launch. You're trading a bit of your health for the experience, but it's worth it. I got 4 and a half hours of sleep when I launched WikiTok.\nSorry Bryan Johnson\nAt the height of the frenzy, I had two people knock on my door and I briefly freaked out, wondering if they were journalists that somehow decided it would be a good idea to show up to my house. Thankfully they were just 2 Verizon technicians conducting routine surveys.\nDealing with Journalists\nI would advise anyone who goes viral to only do interviews with journalists via email or dms (unless it's an actual in-person news interview). There's nothing explicitly wrong with a phone call, but it's far easier to be extremely precise with your words when it's in a written format. It will take longer to write and more often than not most of it will get discarded, but the writing process is extremely valuable.\nWhen a journalist does contact you, do some research on their account. Reminder: a blue checkmark doesn't mean anything on Twitter anymore, as I would later be reminded when dealing with a scammer posing as a journalist. Ask to chat only via email and dms if you have the time. You really want to collect your thoughts and make sure you say exactly what you want to say, compared to freewheeling a conversation. I learned that first when I had a conversation with Business Insider, and it was a pleasant experience, even though I don't think I relayed my message and ideas as well as I could have.\nI wrote about 2000 words for the Washington Post, which took me roughly 3 and a half hours (longer than WikiTok did to make). They only used a paragraph, but now I have a detailed written record of my process and reasoning.\nStill that wasn't enough, and even after explicitly ignoring a question about Elon Musk, they took an angle of \"Insert Elon Musk for clicks,\" which I found to be a bit annoying. There's a lesson here that there's only so much you can control when you go viral.\n(Illustration by Elena Lacey/The Washington Post; Tom Brenner/The Washington Post; iStock)\nYou'll need that precision because information degrades like a game of telephone as it spreads. I counted at least 5 international articles that had the wrong URL (somebody sniped wikitok.net, and people ran with it saying it was mine). You won't be able to correct every mistake, but call out the big ones (like journalists screwing up coverage, which I promise will happen). Focus on the main sources of information - that's all you can really control. Ultimately that's what it means for something to go viral - spreading rapidly across the internet, partially out of your control.\nThe other thing about virality that shocked me, and this is more specific to software engineering - is how strange of an experience it is to have an open source project go viral. It felt like I was on a fishing boat, and suddenly a dozen people climbed aboard and offered their advice on how I should steer the ship. Some people had good points, but others wanted to steer the ship completely in the wrong direction, and I had to be the ones to shut them down. A very weird feeling. A frequent request was adding some sort of \"algorithm\" to the project, which I'm sure would be neat and all, but it's not what I set out to build.\nSome journalists picked up on my sentiment and frustration from dealing with so many requests (specifically, the ones who wanted to add an algorithm to the project), and eventually labeled wikitok as a \"cure\" for doomscrolling, or saying that I built it out explicitly due to frustration with algorithms. I wouldn't really agree with that, but it's a good conversation to have. Personally, I felt it would be strange to add an algorithm and store user data on a project that's based off of Wikipedia's data - it just feels wrong and probably violates their creative commons license. How would I even ethically monetize that? Maybe everyone 100 scrolls you get a giant, annoying popup of Jimmy Wales asking for a donation or something. The omni-presence of algorithms, good and bad, deserves a blog post all of its own.\nCoincidentally, Alec Watson of Technology Connections just uploaded a fantastic video talking about the pitfalls of living lives served by algorithms, I'd encourage you to watch it here. He refers to the issue as \"Algorithmic Complacency\" and goes into great detail about it.\nWatch Out for Scams\nWatch out for phishing attempts. I had a sophisticated phishing attempt show up in my inbox.\n\nA TechCrunch journalist DM'd saying they wanted to chat, and they offered a calendar link to allow me to schedule something on their end. The link will send you to a twitter oauth screen asking if you wish to allow Calendly to access your account, but it's completely fake! It's a clever man-in-the-middle attack, completely bypassing your password or 2FA, and they get permission to post on your behalf. The last step from their end is to use your account as a crypto pump and dump scheme, making thousands. An example of such coin would be something like this random wikitok coin somebody made, but with 99.8% of the coin being held by the owner, I don't think even the most degenerate crypto bros would invest in that.\nFor me, I googled the TechCrunch journalist and realized that wasn't actually their account - and I also got lucky that I had happened to see a tweet about someone's experience with a phishing attempt on my feed a few hours earlier. I wonder if I hadn't seen that tweet, would I have fallen for it? The best thing I can do is post a tweet about it, write up this blog, and make sure people know. I'll leave two more very good threads here from Alex Banks and Danny Postma who both dealt with the same exact scam.\nIn retrospect, it seems like an obvious scam. But when you have 5 different real journalists DMing you and asking for interviews, on top of maintaining the site itself, I could have very easily fell for it, and I'm sure others have.\nI went through about maybe 200 DMs total. The vast majority were really positive, mostly people who were just excited and reaching out to me about Wikitok, which was wonderful! But I had to filter through the craziest shit imaginable. Here's a really unhinged crypto scam:\n\nBut those aren't the worst ones. The worst ones are the death threats. Not because I'm scared of them, but it's just sad to see what insane things people's brains were pretrained on as they grew up. I'm Jewish, I was cancelled before I was even born, 3000 years ago. I didn't get as many death threats as I usually do when I get viral tweet or something, but I did get a few. I'm not going to share them in this post but you can use your imagination. Maybe one day I'll write a detailed blog post about the Jewish experience in the 21st century.\nAbout Me\nI'm actually in the middle of a career transition. Less than a year ago, I was a civil engineer, doing everything from AutoCAD drafting to literally being in the trenches. I quit 2 years out of college because everyday I came home with a deep seated feeling that I chose the wrong career path. I dropped everything, moved to Brooklyn, and found communities where I could really learn to ship. Why did I decide to quit, even in the face of AI and a perfectly fine degree in civil engineering? One reason - I already had some actual programming experience at an internship in college. I wasn't hired to code at all, I just picked it up on my own at the company and started to make serious contributions, one stack overflow page at a time, so I knew it must be possible. The downside to this was that it made the tail end of college rather depressing, as this deep-seated internal conflict started to brew. Am I a software engineer? A civil engineer? Do these classes matter in the slightest? I suppose college really did prepare me for the real world.\nAnd with regards to AI taking jobs - it obviously will become a serious problem in future. But being a doomer right now is like lying down in a parking lot waiting to get run over - you're surrendering to a pointless outcome while the rest of the world keeps moving. There's still so much to build and accomplish. We don't know when AGI is coming, so why stand around and wait for it? Even when some sort of thing that we can agree on is AGI gets released, do you think it will be able to solve problems like creating peace in the Middle East? AGI does not encompass the human condition - there will still be work to do.\nWhat's Next?\nFor wikitok - not much. The website is mostly complete and currently I'm only reviewing simple pull requests. Before I wrote code, I used to make music as a hobby. In the world of music, when you put a song out there, it's set in stone. I'm still adjusting to the fact that software is the opposite - endlessly evolving, 24/7. I did create a sequel to wikitok called spacetok.io that allows you to scroll through images from NASA, after seeing some tweets talking about the idea. I knew the sequel wouldn't go viral, but I made it simply because it was fun. If you know anyone at NASA who'd like it, let them know!\nWhat's next for me? I'll be working on more projects like mcbench.ai. I'll also be diving into theoretical study to fill in my knowledge gaps that Claude is not able to provide, but my big priority is looking for a job. That was one reason I made wikitok - it seemed like fun and something I could put on my resume (so hey, why not check it out and send me an email if you're hiring).\nThanks for reading!Share",
    "summary": {
      "en": "### Summary of \"Reflecting on WikiTok\"\n\nThree weeks ago, the author launched WikiTok, a website offering infinite scrolling access to Wikipedia articles. They created the site in about two hours using a tech stack of Bun, React, and TypeScript. The project stemmed from a tweet and aimed to provide a simple way to explore Wikipedia content.\n\n**Key Points:**\n\n1. **Development Journey:**\n   - The author used a custom hook to fetch articles from Wikipedia's API, creating a UI component to display the articles.\n   - The development process was unconventional, relying heavily on AI assistance and quick iterations.\n\n2. **Viral Experience:**\n   - Going viral is both exciting and overwhelming, requiring quick responses and adjustments based on user feedback.\n   - The author emphasized focusing on the platform that brought them attention and managing interactions with users and journalists effectively.\n\n3. **Handling Media Attention:**\n   - The author advises communicating with journalists via email to ensure clarity and precision.\n   - They also experienced challenges with misinformation spreading about their project and had to navigate various requests and suggestions from users.\n\n4. **Phishing Awareness:**\n   - The author shared experiences with phishing attempts and the importance of verifying contacts, especially during heightened visibility.\n\n5. **Personal Background:**\n   - The author transitioned from civil engineering to software development, motivated by a desire for a more fulfilling career.\n\n6. **Future Plans:**\n   - The author intends to continue developing projects and is currently seeking job opportunities in software engineering.\n\nOverall, the author reflects on the rapid growth of WikiTok and the lessons learned about development, virality, and media engagement.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "08222b77479d8480",
    "title": {
      "en": "Palantir suggests 'common operating system' for UK govt data",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.theregister.com/2025/03/25/palantir_covid_inquiry_comments/",
    "score": 15,
    "by": "rntn",
    "time": 1742902335,
    "content": "Databases\n\n            9\n\nPalantir suggests 'common operating system' for UK govt data\n\n            9\n\n'Don't wait for another pandemic or civil challenge,' says US spy-tech biz\n\nLindsay Clark\n\nTue25Mar2025                                     //\n11:00UTC\n\nComment It might take a particularly shameless company to grasp the opportunity presented by the UK's coronavirus pandemic and step in with a sales pitch. US spy-tech biz Palantir is willing to give it a go.\nIn a witness statement to the UK COVID-19 Inquiry [PDF], an ongoing independent public inquiry into the nation's response to the pandemic (in which around 208,000 people died), Louis Mosley, executive veep of Palantir Technologies UK, said the government should invest in a \"common operating system\" for its data, encompassing departments such as the Department for Work and Pensions and local authorities.\nThe government should \"deploy this common operating system capability immediately and not wait until the next pandemic or civil challenge on the scale of COVID-19 is already underway. An investment of this kind is already long overdue,\" he added.\n\nHealth crusaders prep legal challenge over NHS mega contract with Palantir\nREAD MORE\nPalantir was founded by Peter Thiel, who made his money and name by co-founding PayPal. It attracted early investment from the US Central Intelligence Agency's venture capital arm, In-Q-Tel, and won early contracts in US defense and intelligence applications of its data analytics technologies, along the way supporting US Immigration and Customs Enforcement, an agency that has been accused of separating children from their families during enforcement actions.\nHealthcare quango NHS England first started working with the US data analytics corporation in March 2020, during the height of the pandemic.\n\n            <a href=\"https://pubads.g.doubleclick.net/gampad/jump?co=1&amp;iu=/6978/reg_software/databases&amp;sz=300x50%7C300x100%7C300x250%7C300x251%7C300x252%7C300x600%7C300x601&amp;tile=2&amp;c=2Z-Kc9dqy3ORbe3gXuv-yiQAAAII&amp;t=ct%3Dns%26unitnum%3D2%26raptor%3Dcondor%26pos%3Dtop%26test%3D0\" target=\"_blank\">\n                <img src=\"https://pubads.g.doubleclick.net/gampad/ad?co=1&amp;iu=/6978/reg_software/databases&amp;sz=300x50%7C300x100%7C300x250%7C300x251%7C300x252%7C300x600%7C300x601&amp;tile=2&amp;c=2Z-Kc9dqy3ORbe3gXuv-yiQAAAII&amp;t=ct%3Dns%26unitnum%3D2%26raptor%3Dcondor%26pos%3Dtop%26test%3D0\" alt=\"\">\n            </a>\n\nPalantir was initially handed a nominal £1 contract to work on a COVID-19 data store, along with cloud providers AWS, Google, and Microsoft Azure, and Faculty, a UK AI company. Without open competition, its contract was expanded to a £1 million ($1.29 million) agreement, then a £23 million ($29.7 million) arrangement was signed in December 2020.\n\nPalantir designed to 'power the West to its obvious innate superiority,' says CEO\n\nWe can't make this stuff up: Palantir, Anduril form fellowship for AI adventures\n\nKey aspects of Palantir's Federated Data Platform lack legal basis, lawyers tell NHS England\n\nPrior UK government planned £485M four-year budget for Palantir-based healthcare system\n\nThat deal was subject to the threat of judicial review from campaigners, who argued the contract represented such a change in data usage it warranted public consultation under British data protection law.\nNHS England then extended the contract by six months for £11.5 million ($14.8 million), and awarded a £24.9 million ($32.1 million) deal to cover the one-year transition to a new Federated Data Platform, the £330 million ($426 million) project which Palantir won following an open competition.\n\nIn his submission, Mosley was keen to underscore Palantir's involvement in the government's response to the COVID emergency, including work to distribute ventilators, vaccines, and personal protective equipment.\nAmong the benefits of introducing the imagined data \"operating system\" across government would be the ability to \"optimize procurement, among other things.\"\n\n                <a href=\"https://pubads.g.doubleclick.net/gampad/jump?co=1&amp;iu=/6978/reg_software/databases&amp;sz=300x50%7C300x100%7C300x250%7C300x251%7C300x252%7C300x600%7C300x601&amp;tile=4&amp;c=44Z-Kc9dqy3ORbe3gXuv-yiQAAAII&amp;t=ct%3Dns%26unitnum%3D4%26raptor%3Dfalcon%26pos%3Dmid%26test%3D0\" target=\"_blank\">\n                    <img src=\"https://pubads.g.doubleclick.net/gampad/ad?co=1&amp;iu=/6978/reg_software/databases&amp;sz=300x50%7C300x100%7C300x250%7C300x251%7C300x252%7C300x600%7C300x601&amp;tile=4&amp;c=44Z-Kc9dqy3ORbe3gXuv-yiQAAAII&amp;t=ct%3Dns%26unitnum%3D426raptor%3Dfalcon%26pos%3Dmid%26test%3D0\" alt=\"\">\n                </a>\n\nMaybe the company should be careful what it wishes for, though. The UK procurement for data analytics software seemed to be pretty well optimized toward Palantir winning contracts at the time of the COVID outbreak. ®\n                                    <strong>Get our</strong> <a href=\"https://whitepapers.theregister.com/\" style=\"text-transform:uppercase\">Tech Resources</a>\n\n        Share\n\n            More about\n\n                Government of the United Kingdom\n\n                Palantir\n\n        More like these\n\n            ×\n\n                    More about\n\n                Government of the United Kingdom\n\n                Palantir\n\n                    Narrower topics\n\n                Cabinet Office\n\n                Competition and Markets Authority\n\n                Computer Misuse Act\n\n                DCMS\n\n                GCHQ\n\n                HMRC\n\n                Home Office\n\n                ICO\n\n                NCSC\n\n                NHS\n\n                RPA\n\n                    Broader topics\n\n                Government\n\n                United Kingdom\n\n                    More about\n\n        Share\n\n                9\n\n                    COMMENTS\n\n            More about\n\n                Government of the United Kingdom\n\n                Palantir\n\n        More like these\n\n            ×\n\n                    More about\n\n                Government of the United Kingdom\n\n                Palantir\n\n                    Narrower topics\n\n                Cabinet Office\n\n                Competition and Markets Authority\n\n                Computer Misuse Act\n\n                DCMS\n\n                GCHQ\n\n                HMRC\n\n                Home Office\n\n                ICO\n\n                NCSC\n\n                NHS\n\n                RPA\n\n                    Broader topics\n\n                Government\n\n                United Kingdom\n\n        TIP US OFF\n        Send us news",
    "summary": {
      "en": "Palantir, a US data analytics company, has proposed that the UK government create a \"common operating system\" for data management across various departments to better prepare for future crises, like pandemics. Louis Mosley, an executive at Palantir, emphasized the need for immediate action rather than waiting for another emergency.\n\nPalantir began working with NHS England during the COVID-19 pandemic in March 2020 and initially received a small contract that grew significantly without open competition. Their involvement included assisting with the distribution of critical medical supplies. The company argues that a unified data system could improve procurement and other government operations.\n\nHowever, their contracts have faced legal scrutiny, with campaigners arguing that changes in data usage require public consultation. Despite this, Palantir's role in the UK's pandemic response highlights its influence and the growing reliance on its data services.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "4ea41d504e487ba9",
    "title": {
      "en": "The Lost Towers of the Guelph-Ghibelline Wars",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.exurbe.com/the-lost-towers-of-the-guelph-ghibelline-wars/",
    "score": 43,
    "by": "wallflower",
    "time": 1742734721,
    "content": "The Lost Towers of the Guelph-Ghibelline Wars\n\n\t\t Posted by\n\n\t\t\t\t exurbe\n\n\t\t\ton\n\n\t\t\t\t\t January 15, 2025\n\n\t\t\t\t Florence, History, Inventing the Renaissance, Italy\n\n\t\t\t\t 12\n\nLooks fake, doesn’t it? This implausible Medieval forest of towers, as dense as Manhattan skyscrapers, is our best reconstruction of the town of Bologna at its height, toward the end of the Medieval Guelph-Ghibelline wars. We don’t see many such towers today… or think we don’t, but actually their remnants are all over Italy.\nOften when in Florence one sees buildings like this, where one section is rough stone standing out amid stucco neighbors.\n\nThese are actually the bottom nubs of Medieval stone towers. The town of San Gimigniano (below) is famous for having several still intact. Wealthy families built these as mini-fortresses within the city, where they could defend against riots, enemy families (think Montagues and Capulets) and invasion:\n\nSigns of wealth and prestige, these all-stone buildings were also fireproof, leading to a terrible but effective tactic: take your family, treasures & goods up into your tower then set fire to enemies’ homes and let the city burn around you while you sit safe above. This was VERY BAD for cities.\nstreet corner in San Gimigniano\nAfter many disasters, Florence’s solution was to BAN private buildings over a certain height, forcing everyone who had a tower to knock the top off down to regulation height, leaving these recognizable stone nubs all around the city. This round one below is the oldest (now a restaurant).\n\nMy favorite tower stub is this one, in Via dei Cerchi. I lived on the top floor for a year as a grad student, up 111 steps! I had calves of steel by spring, but the views from the top looked like someone had put a poster of Florence on the wall except it was a window!\n\nOnly city buildings were allowed to exceed the mandated height, which is why Florence’s skyline is now all special buildings: monastery bell towers, the cathedral & baptistery, Orsanmichele the city’s granary (tall to keep grain away from water & mice), the seat of government, and one special guy…\n\nThe tower on the right here is part of Bargello, the prison & police fortress, but it didn’t start that way. It was built by a private family, who sold it to the city when the law banning towers was passed, and the city incorporated it into their prison fort.\n\nThe city jail had to be a fortress in case someone from a powerful family was arrested and the family sent goons to break them out (those guys who bite their thumbs in the opening scene of Romeo & Juliet would *totally* have stormed the jail to bust Romeo out!).\n\nIn this photo you can see how the brick battlements are a later addition, added to the tower as part of its transformation from private fortress to public.\n\nWhat did Florence look like back when it had all its towers? Its long-time ally across the mountains Bologna is famous for still having two intact towers, but in the Middle Ages Bologna was known as the City of 100 Towers because so many families built them. The reconstructions look absolutely incredible. Florence didn’t have so many but did have dozens, so the richest part of the city center would have looked much like this. Much to the despair of the city fire brigade!\n\nSo, whether in a film or on the street, if you ever see a historic Italian city and walk along a block where for some reason one chunk of wall is stone and all the others smooth, you’re probably looking at a relic of the faction feuds that Guido Ruggiero aptly calls “The Italian 300 Years’ War.”\n\nI talk about this long war in “Inventing the Renaissance,” one of many points of continuity which show how the supposed difference between a bad “Dark Ages” and Renaissance “golden age” is 100% propaganda, but fascinating propaganda with a deep history.\nAnd I’ll share more tidbits like over the coming days as we countdown to the book’s release!\n\n                FacebookLiveJournalTwitterRedditLinkedInTumblrPush to KindleShare\n\n\t Bologna, Florence, Guelph, Inventing the Renaissance\n\n\t\t 12 Responses to “The Lost Towers of the Guelph-Ghibelline Wars”\n\n\t\t\t\tOllie said:\n\n\t\t\t 01-16-2025 11:00 AM\n\n\t\t\t\tYour photography is pretty great.\n\n\t\t\t\tJasonium said:\n\n\t\t\t 01-16-2025 12:49 PM\n\n\t\t\t\tI would like to commend you on your restraint. You had the perfect opportunity to write, “Looks like a bunch of baloney, doesn’t it?” in your opening, and you resisted.\n\n\t\t\t\tAbout that history you were taught… – Pylimitics said:\n\n\t\t\t 01-20-2025 11:55 AM\n\n\t\t\t\t[…] an era we know much less about than we think we do, and most of what we do think we know is wrong. The photo is from her site Ex Urbe, where you can read more about these things, and see some photos of modern-day Bologna and the […]\n\n\t\t\t\tMedieval Skyscrapers, Cars for Justice, and Russian Noir - JSTOR Daily said:\n\n\t\t\t 01-21-2025 1:45 PM\n\n\t\t\t\t[…] The Skyscrapers of Medieval Italy (Ex Urbe) by Ada Palmer In medieval Italy, factional wars led powerful families to build huge towers that served as little urban fortresses. Their stubs are still there today. […]\n\n\t\t\t\t2:00PM Water Cooler 1/21/2025 | naked capitalism said:\n\n\t\t\t 01-21-2025 7:12 PM\n\n\t\t\t\t[…] Lost Towers of the Guelph-Ghibelline Wars” [Ex Urbe]. “Looks fake, doesn’t it? This implausible Medieval forest of towers, as dense as […]\n\n\t\t\t\t2:00PM Water Cooler 1/21/2025 | naked capitalism – Buddy CRM News Test said:\n\n\t\t\t 01-21-2025 11:33 PM\n\n\t\t\t\t[…] Lost Towers of the Guelph-Ghibelline Wars” [Ex Urbe]. “Looks fake, doesn’t it? This implausible Medieval forest of towers, as dense as Manhattan […]\n\n\t\t\t\t2:00PM Water Cooler 1/21/2025 | bare capitalism - Lucky to know | All rights reserved. said:\n\n\t\t\t 01-22-2025 12:22 AM\n\n\t\t\t\t[…] Misplaced Towers of the Guelph-Ghibelline Wars” [Ex Urbe]. “Seems to be faux, doesn’t it? This implausible Medieval forest of towers, as dense as […]\n\n\t\t\t\tGorgeous Gary said:\n\n\t\t\t 01-22-2025 2:06 AM\n\n\t\t\t\tI feel like a certain structural engineer of your acquaintance will find this post quite intriguing! (LOL)\n\n\t\t\t\t2:00PM Water Cooler 1/21/2025 | naked capitalism - Neclink said:\n\n\t\t\t 01-22-2025 2:42 AM\n\n\t\t\t\t[…] Lost Towers of the Guelph-Ghibelline Wars” [Ex Urbe]. “Looks fake, doesn’t it? This implausible Medieval forest of towers, as dense as Manhattan […]\n\n\t\t\t\t2:00PM Water Cooler 1/21/2025 | naked capitalism - Agoku said:\n\n\t\t\t 01-22-2025 5:30 AM\n\n\t\t\t\t[…] Lost Towers of the Guelph-Ghibelline Wars” [Ex Urbe]. “Looks fake, doesn’t it? This implausible Medieval forest of towers, as dense as Manhattan […]\n\n\t\t\t\tThe Lost Towers of the Guelph-Ghibelline Wars – Mirabilis.ca said:\n\n\t\t\t 01-25-2025 10:37 PM\n\n\t\t\t\t[…] From Ex Urbe: The Lost Towers of the Guelph-Ghibelline Wars. […]\n\n\t\t\t\tEx Urbe said:\n\n\t\t\t 01-27-2025 4:04 PM\n\n\t\t\t\t[…] a post last week, I talked about how Renaissance towns used to be full of tall stone towers, built by rich families […]\n\n\t\t\t\tComments are closed.",
    "summary": {
      "en": "The article discusses the historical towers of Italy, particularly during the Guelph-Ghibelline wars. It highlights Bologna, known for having many towers, and Florence, where remnants of these towers can still be seen today as stone bases among other buildings. Wealthy families built these tall stone structures as defensive fortresses against conflicts and invasions. \n\nFlorence eventually banned the construction of private towers over a certain height to reduce fire risks and maintain safety, leading to the characteristic stubs of towers seen in the city today. These towers were symbols of wealth and power, and their presence reflected the intense family rivalries of the time. \n\nThe article also hints at the broader historical narrative, suggesting that the supposed divide between the \"Dark Ages\" and the Renaissance is more complex than commonly believed. The author plans to share more insights as they promote their upcoming book.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "36cbb80ba911a295",
    "title": {
      "en": "WWI's 'Dazzle' Camouflage Seemed Effective Due to Unexpected Optical Trick",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://gizmodo.com/wwis-famous-dazzle-camouflage-seemed-effective-due-to-unexpected-optical-trick-study-finds-2000577568",
    "score": 80,
    "by": "ilamont",
    "time": 1742680378,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "39377b593bb66aa1",
    "title": {
      "en": "A study reveals that deciduous trees' roots remain active in winter",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.creaf.cat/en/articules/deciduous-trees-roots-remain-active-winter",
    "score": 15,
    "by": "bryanrasmussen",
    "time": 1742671996,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "8049440cfd9b45fd",
    "title": {
      "en": "Three Hundred Years Later, a Tool from Isaac Newton Gets an Update",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.quantamagazine.org/three-hundred-years-later-a-tool-from-isaac-newton-gets-an-update-20250324/",
    "score": 105,
    "by": "rbanffy",
    "time": 1742854616,
    "content": "Quanta Homepage\n\n                                        Physics\n\n                                        Mathematics\n\n                                        Biology\n\n                                        Computer Science\n\n                                        Topics\n\n                                        Archive\n\n                                        Blog\n\n                                        Columns\n\n                                        Interviews\n\n                                        Podcasts\n\n                                        Puzzles\n\n                                        Multimedia\n\n                                        Videos\n\n                                        About Quanta\n\n                                    An editorially independent publication supported by the Simons Foundation.\n\n                                    Follow Quanta\n\n    Facebook\n\n        Youtube\n\n        Instagram\n\n    RSS\n\n                Newsletter\n\n                    Get the latest news delivered to your inbox.\n\n                            Email\n\n                        Subscribe\n\n                        Recent newsletters\n\n                                    Gift Store\n\n                                        Shop Quanta gear\n\nNewsletter\n\n                    Get the latest news delivered to your inbox.\n\n                            Email\n\n                        Subscribe\n\n                        Recent newsletters\n\nQuanta Homepage\n\n                                        Physics\n\n                                        Mathematics\n\n                                        Biology\n\n                                        Computer Science\n\n                                        Topics\n\n                                        Archive\n\n        Saved articles\n\n                    Saved Articles\n                                            Create a reading list by clicking the Read Later icon next to the articles you wish to save.\n\n                            See all saved articles\n\n        Login\n\n                    Log out\n\n                    Change password\n\n                                Search\n\nHome\n\n                Three Hundred Years Later, a Tool from Isaac Newton Gets an Update\n\n        Comment\n\n        Save Article\n\n                    Read Later\n\n                                                Share\n\n    Facebook\n\n                            Copied!\n\n    Copy link\n         (opens a new tab)\n\n    Email\n\n    Pocket\n\n    Reddit\n\n    Ycombinator\n\nalgorithms\n    Three Hundred Years Later, a Tool from Isaac Newton Gets an Update\n\n        By\n\n                Kevin Hartnett\n\nMarch 24, 2025\n\n            A simple, widely used mathematical technique can finally be applied to boundlessly complex problems.\n\n        Comment\n\n        Save Article\n\n                    Read Later\n\nalgorithms\n    Three Hundred Years Later, a Tool from Isaac Newton Gets an Update\n\n        By\n\n                Kevin Hartnett\n\nMarch 24, 2025\n\n            A simple, widely used mathematical technique can finally be applied to boundlessly complex problems.\n\n        Comment\n\n        Save Article\n\n                    Read Later\n\nMichele Sclafani for Quanta Magazine\n\nEvery day, researchers search for optimal solutions. They might want to figure out where to build a major airline hub. Or to determine how to maximize return while minimizing risk in an investment portfolio. Or to develop self-driving cars that can distinguish between traffic lights and stop signs.\nMathematically, these problems get translated into a search for the minimum values of functions. But in all these scenarios, the functions are too complicated to assess directly. Researchers have to approximate the minimal values instead.\nIt turns out that one of the best ways to do this is by using an algorithm that Isaac Newton developed over 300 years ago. This algorithm is fairly simple. It’s a little like searching, blindfolded, for the lowest point in an unfamiliar landscape. As you put one foot in front of the other, the only information you need is whether you’re going uphill or downhill, and whether the grade is increasing or decreasing. Using that information, you can get a good approximation of the minimum relatively quickly.\nAlthough enormously powerful — centuries later, Newton’s method is still crucial for solving present-day problems in logistics, finance, computer vision and even pure math — it also has a significant shortcoming. It doesn’t work well on all functions. So mathematicians have continued to study the technique, figuring out different ways to broaden its scope without sacrificing efficiency.\nLast summer, three researchers announced the latest improvement (opens a new tab) to Newton’s method. Amir Ali Ahmadi (opens a new tab) of Princeton University, along with his former students Abraar Chaudhry (opens a new tab) (now at the Georgia Institute of Technology) and Jeffrey Zhang (opens a new tab) (now at Yale University), extended Newton’s method to work efficiently on the broadest class of functions yet.\n“Newton’s method has 1,000 different applications in optimization,” Ahmadi said. “Potentially our algorithm can replace it.”\n\n                    In the 1680s, Isaac Newton developed an algorithm for finding optimal solutions. Three centuries later, mathematicians are still using and honing his method.\n\n    Godfrey Kneller/Public Domain\n\nA Centuries-Old Technique\nMathematical functions transform inputs into outputs. Often, the most important feature of a function is its minimum value — the combination of inputs that produces the smallest possible output.\nBut finding the minimum is hard. Functions can have dozens of variables raised to high powers, defying formulaic analysis; graphs of their solutions form high-dimensional landscapes that are impossible to explore from a bird’s-eye view. In those higher-dimensional landscapes, said Coralia Cartis (opens a new tab) of the University of Oxford, “We want to find a valley. Some are local valleys; others are the lowest point. You’re trying to find these things, and the question is: What info do you have to guide you to that?”\nIn the 1680s, Newton recognized that even when you’re dealing with a very complicated function, you’ll still always have access to at least two pieces of information to help you find its deepest valley. First, you can calculate the function’s so-called first derivative, or slope: the steepness of the function at a given point. Second, you can compute the rate at which the slope itself is changing (the function’s second derivative).\n\n                    Amir Ali Ahmadi sees optimization problems everywhere he looks.\n\n    Archives of the Mathematisches Forschungsinstitut Oberwolfach\n\nSay you’re trying to find the minimum of some complicated function. First, choose a point on the function that you think might be close to the true minimum. Compute the function’s first and second derivatives at that point. These derivatives can be used to construct a special quadratic equation — a parabola if your function lives in a 2D plane, and a cuplike shape called a paraboloid if your function is higher dimensional. This quadratic equation, which is called a Taylor approximation, roughly resembles your function at the point you chose.\nNow calculate the minimum of the quadratic equation instead of the original — something you can do easily, using a well-known formula. (That’s because quadratic equations are simple; it’s when equations get more complicated that calculating the minimum becomes prohibitive.) You’ll get a point. Then plug the coordinates of that point back into your original function, and you’ll get a new point on the function that is, hopefully, closer to its true minimum. Start the entire process again.\nNewton proved that if you keep on repeating this process, you’ll eventually home in on the minimum value of the original, more complicated function. The method doesn’t always work, especially if you start at a point that’s too far away from the true minimum. But for the most part, it does. And it has some desirable attributes.\n\n    Mark Belan/Quanta Magazine;Source:arxiv:2305.07512 (opens a new tab)\n\nOther iterative methods, like gradient descent — the algorithm used in today’s machine learning models — converge toward the true minimum at a linear rate. Newton’s method converges toward it much faster: at a “quadratic” rate. In other words, it can identify the minimum value in fewer iterations than gradient descent. (Each iteration of Newton’s method is more computationally expensive than an iteration of gradient descent, which is why researchers prefer gradient descent for certain applications, like training neural networks. But Newton’s method is still enormously efficient, making it useful in all sorts of contexts.)\nNewton could have written his method to converge toward the true minimum value even faster if, instead of taking just the first and second derivatives at each point, he had also taken, say, the third and fourth derivatives. That would have given him more complicated Taylor approximations, with exponents greater than 2. But the whole crux of his strategy was to transform a complicated function into a simpler one. These more complicated Taylor equations were more than Newton could handle mathematically.\n\n                    Jeffrey Zhang and his co-authors wiggled functions in just the right way, allowing them to broaden the scope of a powerful optimization technique.\n\n    Courtesy of Jeffrey Zhang\n\n“Newton did it for degree 2. He did that because nobody knew how to minimize higher-order polynomials,” Ahmadi said.\nIn the centuries since, mathematicians have worked to extend his method, to probe how much information they can squeeze out of more complicated Taylor approximations of their functions.\nIn the 19th century, for instance, the Russian mathematician Pafnuty Chebyshev proposed a version of Newton’s method that approximated functions with cubic equations (which have an exponent of 3). But his algorithm didn’t work when the original function involved multiple variables. Much more recently, in 2021, Yurii Nesterov (now at Corvinus University of Budapest) demonstrated how to approximate functions (opens a new tab) of any number of variables efficiently with cubic equations. But his method couldn’t be extended to approximate functions using quartic equations, quintics and so on without losing its efficiency. Nevertheless, the proof was a major breakthrough in the field.\nNow Ahmadi, Chaudhry and Zhang have taken Nesterov’s result another step further. Their algorithm works for any number of variables and arbitrarily many derivatives. Moreover, it remains efficient for all these cases — something that until now wasn’t possible.\nBut first, they had to find a way to make a hard math problem a lot easier.\nFinding Wiggle Room\nThere is no fast, general purpose method for finding the minima of functions raised to high exponents. That’s always been the main limitation of Newton’s method. But there are certain types of functions that have characteristics that make them easy to minimize. In the new work, Ahmadi, Chaudhry and Zhang prove that it’s always possible to find approximating equations that have these characteristics. They then show how to adapt these equations to run Newton’s method efficiently.\nWhat properties make an equation easy to minimize? Two things: The first is that the equation should be bowl-shaped, or “convex.” Rather than having many valleys, it has just one — meaning that when you try to minimize it, you don’t have to worry about mistaking an arbitrary valley for the lowest one.\n\n                    Abraar Chaudhry and two colleagues recently found a way to improve a centuries-old method for finding the minima of functions.\n\n    Camille Carpenter Henriquez\n\nThe second property is that the equation can be written as a sum of squares. For example, 5x2 + 16x + 13 can be written as the sum (x + 2)2 + (2x + 3)2. In recent years, mathematicians have developed techniques for minimizing equations with arbitrarily large exponents so long as they are both convex and a sum of squares. However, those techniques were of little help when it came to Newton’s method. Most of the time, the Taylor approximation you use won’t have these nice properties.\nBut Ahmadi, Chaudhry and Zhang figured out how to use a technique called semidefinite programming to wiggle the Taylor approximation just enough to make it both a sum of squares and convex, though not so much that it became unmoored from the original function it was supposed to resemble.\nThey essentially added a fudge factor to the Taylor expansion, turning it into an equation that had the two desired properties. “We can change the Taylor expansion a bit to make it simpler to minimize. Think of the Taylor expansion, but modified a little bit,” Ahmadi said. He and his colleagues then showed that, using this modified version of the Taylor expansion — which involved arbitrarily many derivatives — their algorithm would still converge on the true minimum of the original function. Moreover, the rate of convergence would scale with the number of derivatives used: Just as using two derivatives allowed Newton to approach the true minimum at a quadratic rate, using three derivatives enabled the researchers to approach it at a cubic rate, and so on.\nAhmadi, Chaudhry and Zhang had created a more powerful version of Newton’s method that could reach the true minimum value of a function in fewer iterations than previous techniques.\n\n                Related:\n\n                                    Surprising Limits Discovered in Quest for Optimal Solutions\n\n                                    Risky Giant Steps Can Solve Optimization Problems Faster\n\n                                    How We Can Make Sense of Chaos\n\nLike the original version of Newton’s method, each iteration of this new algorithm is still computationally more expensive than methods such as gradient descent. As a result, for the moment, the new work won’t change the way self-driving cars, machine learning algorithms or air traffic control systems work. The best bet in these cases is still gradient descent.\n“Many ideas in optimization take years before they are made fully practical,” said Jason Altschuler (opens a new tab) of the University of Pennsylvania. “But this seems like a fresh perspective.”\nIf, over time, the underlying computational technology needed to run Newton’s method becomes more efficient — making each iteration less computationally expensive — then the algorithm developed by Ahmadi, Chaudhry and Zhang could eventually surpass gradient descent for all sorts of applications, including machine learning.\n“Our algorithm right now is provably faster, in theory,” Ahmadi said. He’s hopeful, he added, that in 10 to 20 years, it will also be so in practice.\n\nBy Kevin Hartnett\n                Contributing Writer\n\n                March 24, 2025\n\n                    View PDF/Print Mode\n\n                            algorithms\n\n                            applied math\n\n                            dynamical systems\n\n                            machine learning\n\n                            mathematics\n\n                    All topics\n\nShare this article\n\n    Facebook\n\n                            Copied!\n\n    Copy link\n         (opens a new tab)\n\n    Email\n\n    Pocket\n\n    Reddit\n\n    Ycombinator\n\n                    Newsletter\n\n                    Get Quanta Magazine delivered to your inbox\n\n                    Subscribe now\n\n                    Recent newsletters\n\n             (opens a new tab)\n\nThe Quanta Newsletter\n\n                    Get highlights of the most important news delivered to your email inbox\n\n                            Email\n\n                        Subscribe\n\n                        Recent newsletters\n                                             (opens a new tab)\n\nAlso in Mathematics\n\n                    ‘Once in a Century’ Proof Settles Math’s Kakeya Conjecture\n\n                geometry\n\n                    ‘Once in a Century’ Proof Settles Math’s Kakeya Conjecture\n\n        By\n\n                Joseph Howlett\n\n            March 14, 2025\n\n        Comment\n                2\n\n        Save Article\n\n                    Read Later\n\n                    Years After the Early Death of a Math Genius, Her Ideas Gain New Life\n\n                geometry\n\n                    Years After the Early Death of a Math Genius, Her Ideas Gain New Life\n\n        By\n\n                Joseph Howlett\n\n            March 3, 2025\n\n        Comment\n                3\n\n        Save Article\n\n                    Read Later\n\n                    After 20 Years, Math Couple Solves Major Group Theory Problem\n\n                group theory\n\n                    After 20 Years, Math Couple Solves Major Group Theory Problem\n\n        By\n\n                Leila Sloman\n\n            February 19, 2025\n\n        Comment\n                8\n\n        Save Article\n\n                    Read Later\n\nComment on this article\n\n                    Quanta Magazine moderates comments tofacilitate an informed, substantive, civil conversation. Abusive, profane, self-promotional, misleading, incoherent or off-topic comments will be rejected. Moderators are staffed during regular business hours (New York time) and can only accept comments written in English.\n\n        Show comments\n\nNext article\n                How Metabolism Can Shape Cells’ Destinies\n\nQuanta Homepage\n\n    Facebook\n\n        Youtube\n\n        Instagram\n\n                                        About Quanta\n\n                                        Archive\n\n                                        Contact Us\n\n                                        Terms & Conditions\n\n                                        Privacy Policy\n\n                        All Rights Reserved © 2025\n\n                    An editorially independent publication supported by the Simons Foundation.\n\n                        Simons Foundation\n\nQuanta Homepage\n\n    Facebook\n\n        Youtube\n\n        Instagram\n\nAbout Quanta\n\n                                        Archive\n\n                                        Contact Us\n\n                                        Terms & Conditions\n\n                                        Privacy Policy\n\nAll Rights Reserved © 2025\n\n                    An editorially independent publication supported by the Simons Foundation.\n\n                        Simons Foundation\n\nClose\n\n    Log in to Quanta\n\n            Use your social network\n\n                        Facebook                        Connect with Facebook\n\n                                                Connect with Google\n\n            or\n\n                    email\n\n                    password\n\n                    Remember me\n\n                        Forgot your password ?\n\n    Don't have an account yet?\n         Sign up\n\nClose\n\n        Forgot your password?\n        We’ll email you instructions to reset your password\n\n                email\n\nClose\n\n        Change your password\n        Enter your new password\n\n                Password\n\n                Retype new password\n\nClose\n\n        Sign Up\n\n                    First Name\n\n                    Last Name\n\n                    Email\n\n                    Password\n\n                    Retype Password\n\n            Creating an account means you accept Quanta Magazine's\n\n            Terms & Conditions and Privacy Policy",
    "summary": {
      "en": "Quanta Magazine focuses on various scientific topics like physics, mathematics, biology, and computer science. It aims to provide insightful articles, interviews, and multimedia content, supported by the Simons Foundation.\n\nA recent article discusses a significant update to Isaac Newton's algorithm, which helps find optimal solutions in complex mathematical problems. Researchers, led by Amir Ali Ahmadi, have improved Newton's method to work more efficiently across a broader range of functions. \n\nNewton's original algorithm uses the first and second derivatives of a function to find its minimum value, which is crucial for solving problems in logistics, finance, and AI. However, it has limitations when applied to more complex functions.\n\nThe new improvement enables the algorithm to handle functions with multiple variables and higher derivatives, making it more versatile while maintaining efficiency. This advancement may not immediately change applications like machine learning but holds promise for future improvements in computational technology, potentially outpacing traditional methods like gradient descent in the long run.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "d9f28312a418739d",
    "title": {
      "en": "The Great Barefoot Running Hysteria of 2010",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://runningshoescore.com/blog/barefoot-running-hysteria-of-2010",
    "score": 70,
    "by": "jonnycat",
    "time": 1742898502,
    "content": "Something went wrong!\n\n    Hang in there while we get back on track\n\n    Home\n\n    The Great Barefoot Running Hysteria of 2010\n\n03/25/25 • 9 minute read\n\nThe year was 2010.  “Ke$ha's Tik Tok”  was topping the Billboard charts.  Steve Jobs has just introduced a goofy new oversized iPhone called an “iPad”.  And in running forums across the internet far and wide, hoards of enthusiasts preached the gospel of a new way of running: without shoes.Google Trends interest in \"Barefoot Running\" from 2004 to presentThe “Great Barefoot Running Hysteria of 2010”, as I call it, took the amateur running world by storm.  Propelled by dramatic claims of performance improvements and injury prevention, barefoot running gave rise to a vocal (and often militant) contingent of enthusiasts and entirely new classes of footwear.  And then over the course of a several years, it faded away almost as quickly as it came, leaving behind changes in running shoes and culture forever.  In this post, we'll explore the history and legacy of the barefoot running movement.The Barefoot Running Movement & How It StartedBarefoot running is–of course–as old as humanity itself. In fact, people have run barefoot throughout most of human history, with the practice continuing today in several cultures, such as Kenya and indigenous peoples in Mexico.  In this sense, before addressing the modern origins of barefoot running, we need to talk about the origins of shod running.  The concept of running shoes as we understand them today, specifically designed to improve running efficiency and comfort, did not emerge until the late 19th and early 20th centuries. This evolution coincided with the rise of organized sports and recreational running, which spurred the development of footwear tailored to the specific needs of runners. The first breakthrough came in 1865 when an English shoemaker brilliantly suggested adding spikes to otherwise normal-looking dress shoes to make them suitable for cross-country running. Later, in the early 20th century, shoes with rubber soles were introduced, offering improved grip and shock absorption, marking a significant advancement in the design of running shoes.The result of these running shoe trends was running shoe stores filled with bulky, over-engineered clown shoes that promised to prevent running injuries, but probably didn't.Into more modern times, running shoes continued to evolve past their humble beginnings into heavier, more complex footwear with exotic materials like plastics & EVA foam.  In the 80s and 90s, many shoemakers became fixated on stability and the notion that pronation, the natural roll of the foot after it lands, was a cause for running injuries.  Running shoes were increasingly designed to try to prevent this movement with wedges of foam that support and stabilize the arch of the foot, a high heel-to-toe drop, and other questionable features like plastic air bubbles in the heel to cushion the foot with each step.The result of these running shoe trends was running shoe stores filled with bulky, over-engineered clown shoes that promised to prevent running injuries, but probably didn't, and were mostly just uncomfortable to use. You can see an example of this kind of design in action in the Nike Air Max 90, which is now also available in a more tasteful re-issue.  While the original was marketed as a running shoe, the modern re-issue is a nostalgic fashion shoe that you should definitely not run in.The original Nike Air Max 90A modern re-issue of the Nike Air Max 90 (from Nike.com)The barefoot running renaissance can thus be understood as a backlash to the dominant running shoe trends at the time.  The revival was driven by a growing dissatisfaction with traditional running shoes, which many believed contributed to injuries and impeded natural foot movement. Proponents of barefoot running argued that it encouraged a more natural gait, reducing the impact on the legs and back and enhancing the overall running experience.In 2004, inspired by Stanford athletes training barefoot, Nike introduced the Nike Free, a running shoe that bucked the bulky running shoe trends of the time with a super flexible sole and a minimal heel-to-toe offset.The original Nike Free (via Wikipedia)Then, in 2006, Vibram launched the 5-Fingers: a shoe intended to be as close to barefoot as possible, with a thin rubber sole and glove-like fit for the toes.  This was the shoe that truly ushered in the minimalist mindset: running shoes should enable a \"natural\" gait and that less is more. Vibram 5 Fingers (via Amazon)The barefoot-inspired Nike Free and Vibram 5 Fingers set the stage, but the real barefoot running revolution began not with a pair of shoes, but with a book about an indigenous tribe of Native Mexicans.Christopher McDougall's \"Born to Run\"While the over-engineered running shoe backlash had already started with shoes like the Nike Free and the Vibram 5 Fingers, the spark that lit the barefoot running powder keg was Christopher McDougall's 2009 bestseller, \"Born to Run\". The book, which explores the running habits of the Tarahumara Native Mexican tribe, known for their long-distance running ability, captivated the imagination of runners–and non-runners–everywhere. Book cover for \"Born to Run\" by Christopher McDougallThe Tarahumara, or Rarámuri, as they refer to themselves, are an indigenous people who reside in the rugged and remote Copper Canyon region of Northwestern Mexico. Renowned for their extraordinary long-distance running abilities, the Tarahumara have garnered international attention and admiration from runners and researchers alike. Their running habits, deeply embedded in their culture and lifestyle, are not merely for sport but serve practical and ceremonial purposes as well.McDougall's narrative suggested that modern running injuries were virtually non-existent among the Tarahumara.  They often ran barefoot or in minimal footwear, sandals crafted from leather and tire strips called huaraches.  The sandals provide minimal cushioning and protection, promoting a natural running form that many attribute to their low incidence of running-related injuries according to McDougall.  The popularity of McDougall's book sparked widespread curiosity and enthusiasm for barefoot running, and many readers happily ditched their bulky running shoes for new, minimalist alternatives.  As the barefoot running movement evolved, so did the market for running footwear, leading to the development of minimalist shoes designed to mimic the barefoot running experience while providing some protection from the hazards of rough terrain. The Influx of (Mostly Newbie) Barefoot RunnersBolstered by the popularity of \"Born to Run\", barefoot running was suddenly everywhere.  This period saw a surge in barefoot running clinics, forums, and social media groups where enthusiasts shared tips and experiences.  And as often happens, what started as a perfectly reasonable idea took on a life of its own and became dogmatic: barefoot running was the way to run.  The movement gave rise to a series of increasingly lofty and strongly worded claims: barefoot running prevented injuries; barefoot running was more efficient; heel striking was evil; barefoot running was the natural and therefore \"correct\" way to run.  The idea transcended running communities and seeped into popular culture prompting lofty headlines like this New York Times article, The Once and Future Way to Run.The idea even transcended the sport of running itself.  To many, the barefoot running movement was not merely about the act of running without shoes; it represented a broader philosophy seeking to embrace simplicity, natural form, and mindfulness in the pursuit of physical fitness and well-being.  The fervor around barefoot running bordered on religious.Despite the smug sense of superiority that some barefoot proponents projected, many of the most enthusiastic adopters of barefoot running were, in fact, novice and inexperienced runners.  While barefoot running enthusiasts were eager to point out elite runners training or racing barefoot, like Zola Budd, these barefoot elites are the exception rather than the rule.  Most serious and elite runners generally sat out the barefoot trend, or at least took a more nuanced approach.  Many advanced runners already gravitated towards less bulky shoes, and many that did adopt barefoot running did so as part of isolated workouts on soft surfaces.It's difficult to quantify this claim, but my personal memory of this period was that online discourse around running form and footwear was dominated by an aggressive mob mentality around barefoot running.   If you were running with shoes on, you were–according to the online mob–\"doin' it wrong\", as the internet was fond of saying at the time.  Though it generally didn't represent the opinions of more experienced runners, the barefoot running crowd was certainly the loudest. Opinions that didn't completely jive with the all-minimalist approach were often brutally and violently downvoted.The Backlash and The Downfall of Barefoot RunningDespite the enthusiasm and claims of the benefits of barefoot running, research into the practice was thin.  While some of the claims seemed logical, most of the evidence around the benefits of barefoot running was anecdotal.  Much of the argument hinged around barefoot running being the natural and thus correct way to run, and that running with over-engineered running shoes was unnatural and thus incorrect.Some will recognize this line of reasoning as the appeal to nature fallacy: a logical fallacy in which a subject is claimed to be good simply because it is natural.  The fallacy pops up frequently in health & medical settings when people extol the virtues of \"all-natural\" products or alternative medicines.  Sure, some natural products are healthy and beneficial; but so are a lot of deadly poisons.  Similarly, diet, where the appeal to nature is used to justify all sorts of sometimes conflicting food choices (throughout history humans have eaten wildly versatile diets).  This doesn't mean that natural is bad, it simply means that it's not a valid argument in and of itself for the benefits of barefoot running.As time went on and the research caught up with the trend, the results were mixed at best: some studies showed potential benefits, while others highlighted increased risks. Critics of barefoot running point to evidence showing an increased incidence of certain types of injuries, such as Achilles tendinitis and metatarsal stress fractures, among runners transitioning to barefoot or minimalist running without proper adaptation. Additionally, there's an obvious concern about the lack of protection from environmental hazards (e.g., sharp objects, rough terrain) when running without traditional footwear, which can lead to acute injuries. At the same time, many fledgling barefoot runners soon faced a reality check. Reports of injuries began to surface, casting doubt on the benefits of running without shoes. Podiatrists and sports medicine professionals started warning about the potential risks, especially for those with pre-existing foot conditions, those who transitioned too quickly, or those who tried to run too much. Many who stuck with the sport and aspired to run longer distances such as the marathon soon learned that it's simply difficult to put in the mileage required to excel at these distances without proper footwear.  Though you're sure to find counterexamples (I still see a small handful of barefoot runners at major running events like the Boston Marathon), most runners who are putting in 50, 60, 70, or more miles a week to race a marathon are doing so with shoes designed to help cushion the impact.Long-Lasting Changes to Running ShoesDespite the decline in barefoot running's popularity, its impact on the running shoe industry was undeniable. Recognizing the demand for a more natural running experience, shoe manufacturers began developing more lightweight, minimalist running shoes with less cushioning and a lower heel-to-toe drop. More importantly, the minimalist movement helped end the dominance of needlessly overbuilt running shoes.  Of course, highly supportive & motion control running shoes are still available, and some runners prefer to run in them, but it's no longer the dominant paradigm.Ironically, the minimalist shoe movement triggered its own backlash with the maximalist shoe movement of mega-cushioned shoes, ushered in by brands like HOKA.  But the maximalist trend is not a return to the overbuilt shoes of the 90s–in fact, it still incorporates minimalist concepts like lower heel-to-toe drop, lightweight materials, and placing less emphasis on the support and motion control that earlier running shoes relied on.  These innovations aimed to combine the benefits of barefoot running with the protection and support of traditional running shoes. Today, many of these features remain integral in modern running shoe design, marking a lasting legacy of the barefoot running movement.So in the end, while the barefoot running hysteria of 2010 may have been a short-lived trend, it sparked crucial conversations about running health and led to significant advancements in running footwear. Though no longer in the limelight, its impact continues to influence how we run and think about our running gear.\n\n    by Jon K.\n\n  Jon has been an avid runner for over 20 years with a marathon PR of 2:46.  He loves everything running & helping others find love for the sport as well.\n\n    Trending Deals\n    See all trending deals...\n\n      Men's Nike Pegasus 41 Ohio State\n\n          $94.97\n\n      Men's Nike Pegasus 41 Ravens\n\n          $94.97\n\n      Women's Nike Pegasus 41 Virginia\n\n          $94.97\n\n      Men's Nike Pegasus 41 Broncos\n\n          $94.97\n\n  Running Shoe Score tracks the pricing and availability of high quality running shoes at a variety of\n  retailers.  Compare prices & find deals on running shoes from top brands.  Pricing & availabilty updated daily!\n\n    Nike\n    | New Balance\n    | Brooks\n    | Asics\n    | Saucony\n    | HOKA\n    | ... and more!\n\n  Pages on this site may contain affiliate links",
    "summary": {
      "en": "In 2010, the barefoot running movement gained popularity, driven by claims of improved performance and injury prevention. This trend emerged as a reaction against bulky, over-engineered running shoes that were thought to cause injuries. \n\nHistorically, humans have run barefoot for most of our existence. Modern running shoes began developing in the late 19th century, evolving into complex designs in the 80s and 90s that prioritized stability over natural foot movement. This dissatisfaction with traditional shoes led to minimalist options, like the Nike Free and Vibram 5-Fingers, which aimed to promote a more natural running style.\n\nThe movement was further energized by Christopher McDougall's 2009 book, \"Born to Run,\" which showcased the Tarahumara tribe's remarkable long-distance running abilities, often done barefoot or in minimal footwear. This sparked a surge in barefoot running clinics and communities, although many newcomers to the sport embraced it dogmatically, often overlooking the potential risks.\n\nAs evidence emerged about the downsides of barefoot running—such as increased injury risks for those who transitioned too quickly—enthusiasm waned. Nonetheless, the movement led to lasting changes in the running shoe industry, with a rise in minimalist designs and a shift away from overly supportive shoes, influencing modern running footwear trends.\n\nUltimately, while the barefoot running craze was short-lived, it initiated important discussions about running health and left a significant mark on how running shoes are designed today.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "9e37172a8989477d",
    "title": {
      "en": "Closing the Chapter on OpenH264",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://bbhtt.space/posts/closing-the-chapter-on-openh264/",
    "score": 5,
    "by": "todsacerdoti",
    "time": 1742896798,
    "content": "People might have noticed me talking about dropping OpenH264 from\nFreedesktop SDK.\nHere, I’ll try to go a bit into the history, the timeline and what led\nto the final decision.\nA bit of an introduction\nIf you are unfamiliar with the Freedesktop SDK project: it was born out\nof the initial 1.6 Flatpak runtime image\ncreated by Alexander Larsson to\nprovide a host independent “runtime” for Flatpaks. Since then, with the\nhelp of Codethink and others, it grew\ninto an independent project maintained by the community that aims to\nprovide a “A minimal Linux runtime”.\nCurrently that includes the org.freedesktop.{Sdk, Platform}\nFlatpak runtimes,\na bunch of other Flatpak extensions\nsuch as the GL (Mesa) extensions, a bootable image stack that includes\nthings like the Linux kernel, firmware and drivers and a collection\nof docker images.\nThe runtimes (and more broadly the “Flatpak” stack) form the base for\nthe GNOME and\nKDE Flatpak\nruntimes which are collectively used by 2000+ Flatpaks, while the\nbootable stack along with other parts forms the base for things like\nGNOME OS.\nSome history\nH.264 is one of the most widely used\ncodecs today but unfortunately it is patented with several patents\nstill active.\nPatents like this are a blocker to shipping software dealing with the\ncodec in the base runtime (since we want it usable by a wide variety of\nvendors and free of any legal grey areas) and unfortunately makes life\ndifficult for everyone involved.\nTo workaround this and ship working software to users, 7 years ago, in\n2019 the precursor to the OpenH264 extension, called the\nhtml5-codecs extension\nwas added to the Freedesktop runtime by Tom Coldrick. The idea was\nsimple - it would contain support for FFMPEG’s internal H.264 decoder\nand would be an optional Flatpak extension since we were unable to ship\nit in the runtime itself.\nFast-forward a few months, in June 2019, Robert McQueen\nopened an issue\nto include Cisco’s OpenH264\n(commonly referred as libopenh264 too) as an extension to the runtime.\nlibopenh264 code is open source but due to the H.264 patents, no\nvendor is legally allowed to distribute their own binaries. The\nsolution to this was to distribute Cisco’s unmodified binaries directly\nto the user which would effectively be free of any royalties but the\ncatch is, the binaries have some license restrictions\non them.\nSo Endless around that time, added extra-data\nsupport to Flatpak. This meant that the Flatpak extension metadata\nwould only contain an URL to Cisco’s binaries, checksums and a “recipe”\nto make a working extension out of it. The binary would be downloaded\non the end user’s machine, inside a sandbox and then the “recipe”\nwould be executed to create a working extension. This avoids various\n“redistribution” restrictions entirely since we aren’t shipping the\nbinary itself.\nHowever, one last catch remained with this approach. Unless the end user\nsystem has downloaded the extension, a part of the ABI will be missing\nfrom the base runtime itself. Additionally, the extension would also\nget mounted to a non-standard location inside the Flatpak sandbox. So to\nbuild something against that we needed a “public” library that is\nincluded in the runtime itself.\nEndless came up with a stub OpenH264 library called noopenhh264,\nas a solution to this. This would be kept ABI/API compatible to the\nactual Cisco’s OpenH264 library and would live inside the runtime at\n$libdir to allow software to link to libopenh264 and provide a\nfallback. At runtime, if the user downloads the actual OpenH264\nextension, the stub library would get overridden by the library in\nextension through Flatpak’s ld.so config and voilà, you have a working\nOpenH264 setup!\nThe start of the openh264 extension\nFollowing the gruelling details above, in August 2019, Tom Coldrick\nagain added\nthis “noopenh264” library to the runtime and set up the extension point\ncalled org.freedesktop.Platform.openh264.\nAround that time, the development of the “stub” noopenh264 library also\nmoved under the Freedesktop SDK umbrella\nwhile the development of this extra-data extension was going on in a\ndedicated repository.\nI talked about a “recipe” of an extra-data Flatpak extension a while\nback, which is nothing but a set of commands in a special file called\napply_extra to stage and install the source. As an example, if it\nwas a .deb it would be a sequence of ar or bsdtar commands to\nextract it, then install it inside Flatpak’s extra-data prefix.\nThe catch here is that using any utility like that makes the extension\ndirectly dependent on a particular branch of the Flatpak runtime and thus\nthe API/ABI provided by it.\nAs every yearly major release of the Freedesktop runtime is a completely\nnew ABI, we would need to continuously spin off new branches of the\nOpenH264 extension i.e. for version 2.1.0 of libopenh264 we would need\norg.freedesktop.Platform.openh264//2.1.0-{18.08, 19.08, 20.08} branches\nfor the extension and so on. This would make it a bit of an chore to\nupdate the extension in Freedesktop runtime itself and also for\nour downstream runtimes.\nAs a solution to this, a custom apply_extra script was written,\nwhich utilised only a few standard headers and would be built statically\nagainst a fixed toolchain.\nThe meant a bit of complexity, since we aren’t allowed to use most\nthings that would make this process easier but it also meant the final\nextension was independent of the runtime API/ABI and was built on top\nof “NoRuntime”.\nThe flaws start to show up\nThis entire setup worked quite well for a long time despite some\nmaintenance issues from Cisco. However there were some flaws.\nA while back I told that every vendor is forced to distribute binaries\nhosted by Cisco at https://ciscobinary.openh264.org. Unfortunately\nciscobinary.openh264.org is missing a valid SSL certificate\nat least since 2014\nand Cisco neither supplies GPG signatures nor strong checksums like\nSHA-256 for the binary releases.\nThis meant that we have no way to verify the authenticity of Cisco’s\nbinaries, opening us to various classes of MiTM and supply-chain\nvulnerabilities. We reached out to Mozilla and via Fedora/RedHat to get\nCisco to fix this, as one can see the comments on the thread but\nnothing ever happened.\nMore recently (around 2024-2025), this SSL issue meant that the domain\nstarted getting DNS blocked\nand would make the extra-data download fail, botching an install.\nAnother critical issue was that if upstream did security fixes and then\nreleased them with an ABI break, we couldn’t fix it for the Flatpak\nruntimes. Distributing binaries directly through extra-data meant there\nis no scope for cherry-picking patches and the ABI break meant the new\nversion would only be able go to a new branch of the runtime.\nWe didn’t really have a choice here. If we dropped this extension\nconsidering the potential flaws, the entirety of the Flatpak user base\nwould loose H.264 decoding and encoding support, so we had to live with\nthe setup.\nThe start of the codecs-extra extension\nAround June 2024 gdk-pixbuf due to a series of security issues,\ndropped support\nfor a lot of “fringe” loaders. We immediately noticed the change as\nwe constantly analyse ABI in stable branches.\nTo compensate for the dropped loaders, we decided\nto add webp, avif, jxl, heif support (with pixbuf loaders) to the\nruntime. Around the same time, Will Thompson\nopened an issue\nto add libheif to the runtime with the problematic HEIC decoder as a\nruntime extension.\nConsequently, we decided that yet another extension just for two libheif\nplugins is not worth the maintenance effort and these would rather be\nin the ffmpeg-full extension (This was the successor to the\nhtml5-codecs extension, created around 2019-2020). So I again\nadded\nlibx265, libde265 and the corresponding libheif plugins to the\nffmpeg-full extension. After a few months I also added\nlibx264 to it too as we were already shipping libx265 and Cisco’s\nOpenH264 had suboptimal encoding performance.\nI was happy with the result and all of this shipped in Freedesktop SDK\n24.08. But some people were unhappy for valid reasons.\nWhen the ffmpeg-full extension was created, due to various concerns\nit was never added to the runtime or SDK manifest; it lived as an\noptional “app” extension - meaning app developers would need to add a\nshort snippet\nin their Flatpak manifest to use it.\nThis was problematic in some ways - sometimes app developers were\nunaware of the extension or that they needed it and the app manifest\nwas missing the snippet that allowed to use the extension. This made\nuser and developer experience a bit poor. Once we started\nshipping libheif in ffmpeg-full, it became slightly more\nproblematic as the actual libheif library was in the runtime but without\nthe extension it cannot decode any HEIF/HEIC image. So the extension\nwas practically mandatory. Albert Astals Cid,\npart of the team who maintains KDE Flatpaks on Flathub complained about\nthis and an issue was opened by Erick555\nto discuss the possibility of making ffmpeg-full a runtime extension.\nI was initially reluctant to do this for various reasons and the\nsituation was again a bit out of our hands (this time due to H.265\npatents) but we asked various parties such as Endless and quite\nsurprisingly it turned out no one really had an objection to making it\na runtime extension or even make it auto-download along with the\nruntime.\nAlong with this change, we decided to rename the ffmpeg-full extension\nagain, this time to codecs-extra to better reflect the fact that it\nwill not only contain FFMPEG with patented parts but other libraries\ntoo that deals with patented codecs. This would sort of be an equivalent\nto the various “meta-packages” that distros provide for patented or\nrestricted codecs.\nThus the seeds were planted and I switched ffmpeg-full to\ncodecs-extra a month ago.\nThis is supposed to ship in Freedesktop SDK 25.08. The major change for\napp developers is that there is no longer any need to have something\nlike\nadd-extensions:\n  org.freedesktop.Platform.ffmpeg-full:\n    version: '24.08'\n    directory: lib/ffmpeg\n    add-ld-path: .\nin the manifest. org.freedesktop.Platform.codecs-extra will be\nautomatically installed by the runtime and will be available to users.\nDropping the openh264 extension\nThe above codecs-extra change meant that we now didn’t really have an\nuse case for org.freedesktop.Platform.openh264 since codecs-extra\nhad FFMPEG’s internal H.264 decoder and the libx264 encoder.\nSo I initially disabled autodownload and considering the various “flaws”\nwith OpenH264 discussed above, I opened an issue\nfor planned retirement in 26.08.\nThis was completely overturned when a high severity flaw\nwas discovered in libopenh264 affecting versions 2.5.0 and earlier.\nThe 23.08 branch of the Freedesktop runtime was locked to 2.2.0 and\nupgrading to a fixed version was not possible due to multiple ABI breaks\nand SONAME bumps upstream. Patching was also not an option as I said\nearlier.\nWe scrambled to mitigate this. For 23.08, we decided to make an\n“ABI break” and drop the extension\nas updating was impossible. So I made a bit of an intimidating announcement.\nBut we still had 24.08 left which was on 2.4.1 and updating to the fixed\nversion i.e. 2.6.0 was again blocked due to an ABI bump.\nHowever, looking through the commits between 2.4.1 and 2.6.0 and\nanalysing the library through libabigail we couldn’t find an actual\nABI break that was made and considering Cisco in the past made\nunnecessary SONAME bumps, we tried patching the 2.6.0 release to\nprovide the older ABI.\nI messed\nwith that venerable apply_extra and wrote a small utility to patch\nSONAMEs. But upstream, in time, pointed us to the ABI break\nand this idea had to be scrapped entirely. (I later found out why\nliabigail was unable to show the ABI break after talking to Dodji\nSeketeli, the libabigail maintainer.)\nWe then asked upstream\nto provide a patch release 2.5.1 instead with the old ABI and to our\nsurprise they did within a few weeks! This shipped in 24.08.15 fixing\nthe 24.08 branch of the runtime.\nAfter all this debacle and the extra “headache”, none of us felt\ncomfortable shipping the openH264 extension anymore. Thus it was\ndropped\nfor the master branch, which means there won’t be any\norg.freedesktop.Platform.openh264 extension or noopenh264 for 25.08+.\nEpilogue\nConsidering all things, I think and hope we made the correct decision\nand hopefully the new org.freedesktop.Platform.codecs-extra works\nout. libx264, libx265 and others are built from source and there\nare no binaries or extra-data involved. So we should theoretically be\nable to patch and fix any issues that come up in the future.\nApart from all this, I’m slightly worried at the prospects of legal\nissues cropping up with this setup and also that the new extension\ncontains “too much”, but we will have to see where things flow.\nAs a fun tidbit, I wasn’t aware that Fedora was also using our\norg.freedesktop.Platform.openh264 extension. I caused a bit of work\nfor them as noopenh264 moved home\nonce again, now to Fedora and they are looking for ways to make their\nown OpenH264 extension similar to how we did.\nI hope the experience here helps anyone in the future wanting to maintain\nsuch an extension and this will also serve as a reminder to how much\nextra work patents like these causes.",
    "summary": {
      "en": "The author of the text discusses the decision to drop the OpenH264 extension from the Freedesktop SDK, providing a detailed history of its development and the challenges faced. Here's a simplified summary of the key points:\n\n1. **Freedesktop SDK Overview**: The Freedesktop SDK project started as a minimal Linux runtime for Flatpaks, evolving into a community-maintained project that includes various runtimes and extensions.\n\n2. **H.264 Codec Challenges**: H.264 is a popular video codec with patent restrictions. This made it difficult to include in the Freedesktop runtime without legal issues.\n\n3. **OpenH264 Development**: In 2019, the OpenH264 extension was created to allow users to download and use Cisco's OpenH264 binaries without distributing them directly, addressing legal concerns.\n\n4. **Technical Complications**: The setup had flaws, including issues with binary verification and the inability to apply security fixes effectively. This led to vulnerabilities in the distribution of the OpenH264 binaries.\n\n5. **Transition to Codecs-Extra**: Due to some security concerns and changing needs, a new extension called \"codecs-extra\" was developed, which includes support for various video codecs, including H.264.\n\n6. **Decision to Drop OpenH264**: As codecs-extra provided sufficient support for H.264, the OpenH264 extension was deemed unnecessary and was planned to be retired. Further complications arose from a critical flaw in OpenH264, leading to its removal from the master branch of the Freedesktop SDK.\n\n7. **Conclusion**: The author believes dropping OpenH264 was the right choice and expresses hope that the new codecs-extra extension will function well. They also acknowledge the ongoing challenges posed by patents in software development.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "7cf3540b3765823d",
    "title": {
      "en": "Gene drive modified mosquitoes offer new tool for malaria elimination efforts",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.imperial.ac.uk/news/261995/gene-drive-modified-mosquitoes-offer-tool/",
    "score": 10,
    "by": "gmays",
    "time": 1742581491,
    "content": "view large\n\n\t\t\t\tGenetic technology co-developed at Imperial could help to eliminate malaria by making mosquitos resistant to the parasite that spreads the disease.\n\n\t\t\t\tTransmission Zero, a research team from Imperial College London in partnership with the Ifakara Health Institute and the National Institute of Medical Research in Tanzania, has developed genetic technology which renders a mosquito unable to transmit the malaria parasite and has a gene drive that ensures that future generations are also resistant to the parasite.\nThe technology could significantly reduce the malaria burden in high-risk countries, saving hundreds of thousands of lives each year, especially among children – who are disproportionately at risk from the disease.\n“Our technology is equitable, it offers hope in the fight against malaria and doesn’t present economic or social barriers to malaria intervention access.” Professor George K. Christophides Professor of Infectious Disease and Immunity at Imperial College London\nThe work of Transmission Zero has been highlighted in a new Global Development Hub STEM forDevelopment Impact Memo authored by Professor George K. Christophides, Professor of Infectious Disease and Immunity at Imperial, Dr Nikolai Windbichler, Reader in Genetics at Imperial, and Dr Dickson Wilson Lwetoijera, Principal Research Scientist at Ifakara Health Institute in Tanzania.\nProfessor Christophides from Imperial’s Department of Life Sciences said: “The solutions we currently have for malaria are not enough – we need something new.\n“Our technology is equitable, it offers hope in the fight against malaria and doesn’t present economic or social barriers to malaria intervention access.”\n\nRead the fullGlobal Development Hub STEM for Development Impact Memo on Transmission Zero here.\n\nProgress on tackling malaria has flatlined in recent years, with cases and deaths further exacerbated by disruption to health services during the COVID-19 pandemic.\nEach year, more than 263 million people contract malaria and the disease is endemic in 83 countries. In 2023, there were almost 600,000 malaria deaths in 83 countries, with African countries accounting for 94 per cent of deaths - 76 per cent of whom were children under five.\nEfforts to prevent the spread of the disease have been complicated by existing malaria interventions becoming less effective due to mosquitoes developing biological and behavioural resistance to insecticides and barrier-based controls. Malaria parasites have also developed resistance to treatment, highlighting the need for new tools to tackle the disease.\n\nDr Nikolai Windbichler and Professor George K. Christophides from the Transmission Zero team at Imperial.\n\nDr Windbichler, also from the Department of Life Sciences, said: “This is a new type of technology and a first-in-class solution to complement our malaria control tools.\n“We want to move at the right speed, not too fast so that everyone is on board and supportive of this new technology, but also with urgency, so that we treat malaria as the emergency that it is.”\n“We want to move at the right speed, not too fast so that everyone is on board and supportive of this new technology, but also with urgency, so that we treat malaria as the emergency that it is.” Dr Nikolai Windbichler Reader in Genetics at Imperial College London\nThe memo was launched at a special event at the Royal Geographical Society which brought together leading scientists, policymakers, and global health advocates to explore the potential of gene drive technology and other innovative solutions to achieve malaria eradication.\nThe event included addresses from the memo authors, alongside a panel discussion and Q&A session which included:\n\nProfessor Faith Osier, Imperial’s Chair in Malaria Immunology and Vaccinology\n\nDr Honorati Masanja, Chief Executive Director at Ifakara Health Institute, Tanzania\n\nMichael Santos, Senior Vice President for Science Partnerships and Chief Population Health Science Officer at the Foundation for the National Institutes of Health (FNIH)\n\nVictoria Fowler, Head of UK Advocacy at Malaria No More\n\nAfrica-owned technology\nTransmission Zero was established to develop new ways to halt the spread of malaria. The project is co-led by Imperial researchers in the UK and scientists at the Ifakara Health Institute and National Institute for Medical Research in Tanzania, in collaboration with the Swiss Tropical and Public Health Institute in Switzerland.\n\"There was strong agreement from the beginning that we had to capacitate local scientists and have buy-in from the government and local communities, which demonstrates the local ownership of the technology.” Dr Dickson Wilson Lwetoijera Principal Research Scientist at Ifakara Health Institute\nIt has been supported by a $15-million grant from the Gates Foundation to substantially expand its operations in the UK and in Tanzania.\nThis collaborative approach has led to the development of a mosquito transgenesis and infection facility at the Ifakara Health Institute with a biosafety level three laboratory and supporting plant room, bolstering genetic research in Tanzania and the wider continent.\nThe project has already achieved the first ever transgenic mosquito made on African soil, made by African scientists – a landmark achievement for research and technology development in Africa.\nDr Lwetoijera said: “There was strong agreement from the beginning that we had to capacitate local scientists and have buy-in from the government and local communities, which demonstrates the local ownership of the technology.”\nThe partnership between Imperial and institutes in Tanzania will help ensure that scientists in the region are trained in the latest genetic technologies and able to develop their own research agenda.\n\nTackling global development challenges\nSTEM for Development Impact Memos offer policymakers and stakeholders insight intoprojects supported by Imperial’s Global Development Hub and their real-world impact. The focus is on the scale-up potential of a science, technology, policy or health solution to development challenges.\nImperial'sGlobal Development Hubis a platform to promote and support Imperial’s sustainable development research, education and innovation. The Hub supports Imperial’s contribution to the United Nations Sustainable Agenda 2030, and our work more broadly with some of the most vulnerable and marginalised in societies where multiple global challenges are acutely concentrated.",
    "summary": {
      "en": "Researchers from Imperial College London have developed a genetic technology that can make mosquitoes resistant to malaria, potentially eliminating the disease. This project, called Transmission Zero, is a collaboration with Tanzanian health institutes and aims to significantly reduce malaria cases and deaths, particularly among children.\n\nThe technology creates mosquitoes that cannot transmit the malaria parasite and ensures that future generations of mosquitoes inherit this resistance. This innovative approach is seen as essential, especially as traditional malaria prevention methods are becoming less effective due to mosquito resistance to insecticides and treatments.\n\nThe project has received a $15 million grant from the Gates Foundation and emphasizes local involvement and training for African scientists. It aims to give communities ownership of the technology and ensure sustainable development in malaria research.\n\nOverall, this initiative highlights the urgent need for new solutions to combat malaria, which remains a major global health challenge, particularly in Africa.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "dfc013cd9b62a96e",
    "title": {
      "en": "Battling the Blaze: Artifacts from the history of firefighting",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://worldhistory.substack.com/p/battling-the-blaze",
    "score": 4,
    "by": "crescit_eundo",
    "time": 1742737584,
    "content": "Share this postLooking Through the PastBattling the BlazeCopy linkFacebookEmailNotesMoreDiscover more from Looking Through the PastIlluminating forgotten corners of history and using them to think about the presentOver 1,000 subscribersSubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.Already have an account? Sign inBattling the BlazeArtifacts from the history of firefightingGeorge DillardMar 23, 202510Share this postLooking Through the PastBattling the BlazeCopy linkFacebookEmailNotesMore2ShareWelcome to Looking Through the Past! At its heart, this is a project in which I find a bunch of cool stuff every week and share it with you. It takes hours of research and writing to produce. If you’re new here, click the button below to subscribe. If you’re already a free subscriber who appreciates getting a bunch of cool stuff in your inbox every week, please consider a paid subscription, for less than $1 a week.SubscribeI thought I knew how I was going to introduce this week’s topic — some fascinating artifacts from the history of firefighting — but it seems that I’ve hit a snag. Let me explain. In our wealth-obsessed society, there’s one thing that reliably attracts attention: the Rich Guy. People pay an immense amount of attention to what we used to call the Lifestyles of the Rich and Famous. If you’re like most Americans, your brain is cluttered with facts or suppositions about Elon’s kids, Bezos’ divorce, Warren Buffett’s car, and all sorts of other stuff about Rich Guys that is much less important for you to remember than that meeting you forgot about earlier today. Our fascination with Rich Guys is sometimes a product of admiration — we want to understand their ways so that we can be more like them — and sometimes a result of envy and resentment — we want to revel in the fact that all the money in the world can’t save these guys from misfortune. The Rich Guy obsession applies to historical figures, too. There’s a steady stream of historical content about the Rich Guys of History, whether robber barons like Carnegie and Rockefeller or ancient plutocrats like Mansa Musa or the Rich Guy I want to discuss today, Marcus Licinius Crassus, a member of Rome’s First Triumvirate. Much of the Rich Guys of History content on the internet, it must be said, is quite low quality — it’s clear that people google them a lot, so there’s a lot of clickbait and content-farmed stuff that comes up toward the top of the search results. Much of it is thinly sourced or exaggerated for effect — search for the “richest person in history,” and you’ll enjoy a lukewarm buffet of AI slop combined with slapdash currency conversion and inflation calculation. It’s difficult if not impossible to convert the wealth of a guy like Crassus into modern terms; even if we take the value of all the gold he owned and convert it into dollars at today’s prices, how can we account for all of the slaves he owned or the fact that, for all his wealth, he didn’t have, like, a refrigerator or a telephone?The value of Crassus’ holdings is not the only thing that the Rich Guy of History websites fail to convey accurately. Take, for example, a commonly-repeated story about the source of Crassus’ wealth — that he operated a “fire brigade” whose purpose was, well, I’ll let the New Yorker’s Hendrik Hertzberg explain:Crassus had his own private fire department, and if your house caught fire his representatives would offer to buy it on the spot, at a one-time-only, fire-sale price that would fall rapidly as the flames climbed. If you said yes, you’d get a few sesterces, after which Crassus’ firefighters would do their thing. If you said no, you’d end up with a pile of ashes. Hertzberg’s anecdote is based on a passage from Plutarch:And besides this, observing how natural and familiar at Rome were such fatalities as the conflagration and collapse of buildings, owing to their being too massive and close together, he proceeded to buy slaves who were architects and builders. Then, when he had over five hundred of these, he would buy houses that were afire, and houses which adjoined those that were afire, and these their owners would let go at a trifling price owing to their fear and uncertainty. In this way the largest part of Rome came into his possession.But, beyond this passage (and Plutarch was known to spice up his narratives if he was trying to make a point), there’s really… nothing else to support this idea that pops up again and again in the popular history of Crassus. As Roman historian Liv Mariah Yarrow found when she dug into the matter, this is something that “everyone KNOWS” but is weakly sourced and quite possibly misunderstood. There seem to have been a lot of private fire brigades. Crassus’ organization may not have been unusual, or it may have had other purposes (protecting his own property, for example). Its importance to his wealth is probably exaggerated. So the tidy story I was going to start with — Crassus’ fire brigades as the beginning of firefighting history — seems a little too tidy. Yes, ancient Rome had firefighters (their most important job was probably spotting fires before they grew). Other ancient societies must have had them, as well, although clear evidence is hard to come by. So, sorry — I don’t have a just-so story about the ancient origins of firefighting. What we do know is this: fire has plagued urban society since its beginning, and these societies have worked to prevent and fight fire for most of that history. Despite the danger of fire, organized, professional municipal firefighting is a pretty recent invention. It wasn’t until 1824 that Edinborough established a taxpayer-supported, full-time fire company (Cincinnati was the first American city to do so, in 1853, although some cities had volunteer firefighters prior to that date). Before the advent of municipal firefighters, homeowners had to buy fire insurance. Insurance entitled the policyholder to the services of a company’s fire brigade in the case of an emergency. But how would the fire brigades know which houses were covered by which companies? Insurance firms marked their territory with fire marks — metal or ceramic plaques that designated a building’s insurance provider. They had to be made of fireproof materials because the insurance companies sometimes wouldn’t pay a claim until the mark was found in the rubble of a burned building.These marks still dot old buildings in British and American cities, and many of them are quite attractive. This one comes from early-19th-century London:Public domainAnd this one from Norwich:Evelyn Simak, CC 2.0I enjoy this dragon-themed one from late-19th-century Singapore:Public domainSome featured firefighting equipment, like this serpentine-looking hose on one from Philadelphia:Public domainOr this early fire engine from Darlington, England:Public domainBut the most excellent fire mark has to be this one from the Commercial Union Assurance Company in London, which shows a dragon amidst the flames. Who says insurance has to be boring?Public domainWhat about the firefighters themselves? In the nineteenth century, they didn’t have the advanced equipment that they do today to protect them from the dangers of their job. But this doesn’t mean they didn’t take great pride in their attire. I love the early fire hats — stovepipe hats, mace of felt or leather, that didn’t offer a ton of protection but did advertise the fire company and often the wearer himself. They’re charmingly hand-decorated:Courtesy of the Missouri Historical Society, St. LouisCourtesy of the Missouri Historical Society, St. LouisBen Franklin, the founder of the United States’ first volunteer fire company, was a frequent subject:Courtesy of the Missouri Historical Society, St. LouisWhile others featured other patriotic figures:Courtesy of the Missouri Historical Society, St. LouisSome simply tried to embody the emotions that firefighters must have desperately wanted to provide to the community:Public domainBy the late nineteenth century, the modern fire helmet — made initially of leather — provided more protection, but they still were charmingly hand-painted. This one, appropriately, features a phoenix rising from the ashes:Courtesy of the Missouri Historical Society, St. LouisThe most excellent fire-protection apparel, however, came from Japan. Japanese firefighters wore reversible coats made of quilted cotton. They would soak the coats in water before they went into the flames. Their task was usually to create a fire break by destroying structures around the burning building; water supplies were insufficient to extinguish large fires. These coats were decorated with remarkable mythological designs; the men would often turn the coats inside out so that the designs would not be harmed when they entered the blaze.  Here’s one with an image of a Toad Spirit that taught humans magic:Art Institute of ChicagoOthers had pictures of legendary warriors, whose bravery would have inspired the firefighters (this one features Rōrihakucho Chō Jun):Public domainSome were quite literal, like this coat showing a warrior fighting the spirit of fire:Public domainWhile others evoked the peace and quiet of nature:Public domainLater, the coats were made of leather, which was more protective. This one has the fire company’s beautifully stylized logo on the back:Brooklyn MuseumOver time, of course, firefighters developed more modern protective technology, especially for the danger that came from smoke inhalation. This is an early design from the Warsaw fire department:Public domainSome of the equipment made them look positively alien, like these gas masks worn by Australian firefighters in 1927:Public domainOr these worn by Norwegians in 1941:Municipal Archives of TrondheimIt takes a special type of person to sign up for a job that involves risking your life, breathing smoke, and battling deadly flames — especially at a time when the technology that might protect them was so primitive. So I’ll leave you with some images of the firefighters themselves. Here’s the Jarrow Ladies Fire Brigade, called into service when most of the men from this town near Newcastle were called up for World War I:Public domainHere we have the Athabasca, Alberta fire brigade, who presumably had to battle wildfire more than urban conflagrations:Public domainAnd here’s a Seattle-area “hose team” posing during some kind of tournament, in 1889, the same year that Seattle’s “Great Fire” occurred. Some of the men in the picture would later fall into Puget Sound when a burning dock collapsed beneath their fire wagon:Public domainOne final image — of firefighters in action, carrying an injured person on a stretcher. I love this one; it shows the calm and competence amidst chaos that is the hallmark of the firefighter:Public domainThough the origins of firefighting may be murky, its purpose is clear. Firefighters are there to assist us during unthinkable tragedies, risking their lives to save people and their possessions. The slogans that were painted on early fire engines say it all:“United We Stand, Divided We Fall.”“Non Sibi Sed Omnibus” (\"Not for self, but for all.\")“When Duty Calls, Tis Ours to Obey.”“Semper Paratus” (\"Always Ready\")And, my favorite, “We'll Try.”This newsletter is free to all, but I count on the kindness of readers to keep it going. If you enjoyed reading this week’s edition, there are two ways to support my work: You can subscribe as a free or paying member:SubscribeOr you can share the newsletter with others:Share Looking Through the PastThanks for reading, and I’ll see you again next week!10 Likes∙2 Restacks10Share this postLooking Through the PastBattling the BlazeCopy linkFacebookEmailNotesMore2Share",
    "summary": {
      "en": "**Summary:**\n\nThis post from \"Looking Through the Past\" explores the history of firefighting and its significant artifacts. The author notes our society's fascination with wealthy individuals, like historical figures such as Marcus Licinius Crassus, and addresses misconceptions surrounding them—specifically, the story of Crassus operating a private fire brigade. This narrative is likely exaggerated and lacks strong evidence. \n\nFirefighting has been a concern since ancient times, but organized municipal fire services only began in the 19th century, with Edinburgh establishing the first taxpayer-funded fire company in 1824. Before this, homeowners relied on fire insurance, which marked their properties with plaques to show coverage.\n\nThe post also highlights the evolution of firefighting gear, from early fire hats to modern protective equipment. It showcases various historical firefighting uniforms and equipment from around the world, emphasizing the bravery of firefighters who risk their lives. The post concludes with the reminder that, despite the unclear origins of firefighting, its vital role in protecting communities is undeniable.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "a75affe91b207eea",
    "title": {
      "en": "CO2 laser enables long-range detection of radioactive material",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://physicsworld.com/a/co2-laser-enables-long-range-detection-of-radioactive-material/",
    "score": 93,
    "by": "EA-3167",
    "time": 1742593278,
    "content": "\"\\u003Cstrong\\u003ERemote detection\\u003C\\/strong\\u003E The ability to locate radioactive material at distances greater than the range of emitted particles could play an important role in nuclear disaster response and nuclear security. (Courtesy: Shutterstock\\/fewerton)\"\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tRemote detection The ability to locate radioactive material at distances greater than the range of emitted particles could play an important role in nuclear disaster response and nuclear security. (Courtesy: Shutterstock/fewerton)\n\n\t\t\t\t\t\t\t\tResearchers have demonstrated that they can remotely detect radioactive material from 10 m away using short-pulse CO2 lasers – a distance over ten times farther than achieved via previous methods.\nConventional radiation detectors, such as Geiger counters, detect particles that are emitted by the radioactive material, typically limiting their operational range to the material’s direct vicinity. The new method, developed by a research team headed up at the University of Maryland, instead leverages the ionization in the surrounding air, enabling detection from much greater distances.\nThe study may one day lead to remote sensing technologies that could be used in nuclear disaster response and nuclear security.\nUsing atmospheric ionization\nRadioactive materials emit particles – such as alpha, beta or gamma particles – that can ionize air molecules, creating free electrons and negative ions. These charged particles are typically present at very low concentrations, making them difficult to detect.\nSenior author Howard Milchberg and colleagues – also from Brookhaven National Laboratory, Los Alamos National Laboratory and Lawrence Livermore National Laboratory – demonstrated that CO2 lasers could accelerate these charged particles, causing them to collide with neutral gas molecules, in turn creating further ionization. These additional free charges would then undergo the same laser-induced accelerations and collisions, leading to a cascade of charged particles.\nThis effect, known as “electron avalanche breakdown”, can create microplasmas that scatter laser light. By measuring the profile of the backscattered light, researchers can detect the presence of radioactive material.\n\nThe team tested their technique using a 3.6-mCi polonium-210 alpha particle source at a standoff distance of 10 m, significantly longer than previous experiments that used different types of lasers and electromagnetic radiation sources.\n“The results are highly impressive,” comments EunMi Choi from the Ulsan National Institute of Science and Technology in South Korea. Choi’s team had used a gyrotron source to detect radioactive materials back in 2017.\n“The researchers successfully demonstrated 10-m standoff detection of radioactive material, significantly surpassing the previous range of approximately 1 m,” she says.\nMilchberg and collaborators had previously used a mid-infrared laser in a similar experiment in 2019. Changing to a long-wavelength (9.2 μm) CO2 laser brought significant advantages, he says.\n“You can’t use any laser to do this cascading breakdown process,” Milchberg explains. The CO2 laser’s wavelength was able to enhance the avalanche process, while being low energy enough to not create its own ionization sources. “CO2 is sort of the limit for long wavelengths on powerful lasers and it turns out CO2 lasers are very, very efficient as well,” he says. “So this is like a sweet spot.”\nImaging microplasmas\nThe team also used a CMOS camera to capture visible-light emissions from the microplasmas. Milchberg says that this fluorescence around radioactive sources resembled balls of plasma, indicating the localized regions where electron avalanche breakdowns had occurred.\nBy counting these “plasma balls” and calibrating them against the backscattered laser signal, the researchers could link fluorescence intensity to the density of ionization in the air, and use that to determine the type of radiation source.\nThe CMOS imagers, however, had to be placed close to the measured radiation source, reducing their applicability to remote sensing.“Although fluorescence imaging is not practical for field deployment due to the need for close-range cameras, it provides a valuable calibration tool,” Milchberg says.\nScaling to longer distances\nThe researchers believe their method can be extended to standoff distances exceeding 100m. The primary limitation is the laser’s focusing geometry, which would affect the regions in which it could trigger an avalanche breakdown. A longer focal length would require a larger laser aperture but could enable kilometre-scale detection.\nChoi points out, however, that deploying a CO2 laser may be difficult in real-world applications. “A CO₂ laser is a bulky system, making it challenging to deploy in a portable manner in the field,” she says, adding that mounting the laser for long-range detection may be a solution.\n\n\t\t\t\t\tRead more\n\n\t\t\t\tTetris-inspired radiation detector uses machine learning\n\nMilchberg says that the next steps will be to continue developing a technique that can differentiate between different types of radioactive sources completely remotely. Choi agrees, noting that accurately quantifying both the amount and type of radioactive material continues to be a significant hurdle to realising remote sensing technologies in the field.\n“There’s also the question of environmental conditions,” says Milchberg, explaining that it is critical to ensure that detection techniques are robust against the noise introduced by aerosols or air turbulence.\nThe research is described in Physical Review Applied.\n\n\t\t\t\t\t\t\t\t\t\t\t\tWant to read more?\n\t\t\t\t\t\t\t\t\t\t\t\tRegistration is free, quick and easy\n\t\t\t\t\t\t\t\t\t\t\t\tNote: The verification e-mail to complete your account registration should arrive immediately. However, in some cases it takes longer. Don't forget to check your spam folder.\n\t\t\t\t\t\t\t\t\t\t\t\tIf you haven't received the e-mail in 24 hours, please contact customerservices@ioppublishing.org.\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tE-mail Address\n\n\t\t\t\t\t\t\t\t\t\t\t\tRegister",
    "summary": {
      "en": "Researchers at the University of Maryland have developed a new method to remotely detect radioactive material from a distance of 10 meters using CO2 lasers, which is more than ten times the range of traditional detectors like Geiger counters. This method works by utilizing ionization in the air caused by the particles emitted from radioactive materials. The CO2 lasers can accelerate charged particles, leading to a chain reaction known as \"electron avalanche breakdown,\" which creates microplasmas that scatter laser light. \n\nBy analyzing the backscattered light, the researchers can identify the presence of radioactive material. They successfully tested this technique with a polonium-210 source, significantly improving detection range compared to previous methods.\n\nWhile the method shows promise for detecting radioactive materials over distances greater than 100 meters, the bulkiness of CO2 lasers poses challenges for practical field deployment. Future work will focus on differentiating between types of radioactive sources and ensuring the detection method is effective in various environmental conditions.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "4b17ba4ef7e411e0",
    "title": {
      "en": "High‑dose Vitamin D reduces disease activity in early multiple sclerosis onset",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://medicalxpress.com/news/2025-03-highdose-vitamin-d-significantly-disease.html",
    "score": 11,
    "by": "pseudolus",
    "time": 1742898037,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "bd9ab8acef055291",
    "title": {
      "en": "Brain Scans of Infants Reveal the Moment We Start Making Memories",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://singularityhub.com/2025/03/20/new-baby-brain-scans-reveal-the-moment-we-start-making-memories/",
    "score": 67,
    "by": "Brajeshwar",
    "time": 1742574750,
    "content": "ScienceBrain Scans of Infants Reveal the Moment We Start Making MemoriesKids form fleeting memories at around 12 months, even as their brains are rapidly rewiring themselves.Shelly FanMar 20, 2025Image CreditJuan Encalada on UnsplashShareA giggling toddler in a pink dress and matching headphones lies down on her back in front of a gigantic whirling machine. A pillowy headrest cushions her head. She seems unfazed as she’s slowly shuttled into the claustrophobic brain scanner. Once settled, a projection showing kaleidoscope-like animations holds her attention as the magnetic resonance imaging (MRI) machine scans her brain.The girl is part of a new study seeking to answer a century-old mystery: Why can’t most us remember the first three years of our lives? Dubbed “infantile amnesia” by Sigmund Freud, the study could provide insight into how the brain develops during our early years. And if we can form memories at a young age, are they fleeting, or are they still buried somewhere in the adult brain?It seems like a simple question, but an answer has eluded scientists.Though infants and toddlers aren’t yet able to give detailed verbal feedback, studying their behavior has begun to shed light on if and when they remember people, things, or places. Still, the approach can’t peek in on what’s happening in the brain in those early years. MRI can.A team from Columbia and Yale University scanned the brains of 26 infants and toddlers aged 4 to 25 months as they completed a memory task. They found that at roughly a year old, a part of the brain crucial to memory formation spun into action and began generating neural signals related to things the kids remembered from the tests.Called the hippocampus, this sea-horse-shaped structure deep inside the brain is crucial to the encoding of our life stories—who, when, where, what. Adults with a damaged hippocampus suffer memory problems. But because wiring inside the hippocampus is still developing during our earliest years, scientists believe it may be too immature to form memories.“It’s not that we don’t have any memories from that period [infancy],” said study author Nicholas Turk-Browne in a press briefing. “In fact, early life is when we learn our language. It’s when we learn how to walk…learn the names of objects and form social relationships.”“What happens during that period when we learn so much, but remember so little?” he added.Stages of MemoryMemory seems like all-or-none: You either remember something, or you don’t.It’s not that simple. Decades of research have identified the hippocampus as the main orchestrator of episodic memories. These allow you to remember an acquaintance at a party, where you parked your car, or what you had for dinner three nights ago.Each everyday experience is encoded in neural connections in the hippocampus. Groups of neurons called engrams capture different memories and keep them separate, so that they don’t bleed into each other.Once encoded, the brain etches important memories into long-term storage during sleep. Studies of slumbering rodents and humans after learning a new task found that the hippocampus replayed brain activity at higher speed during the night, correlating with better performance on a trained memory task the next day.The last step is retrieval. This is when the brain fishes out stored memories and delivers them to our conscious brain—and so, we “remember.”Failure of any of these steps causes amnesia. So, which steps are responsible for the erosion of baby memories?Bundles of JoyBrain scans from 26 infants now offer some intriguing clues.The team behind the new study scanned the children’s brains with functional MRI (fMRI) as they looked at a screen in the scanner and took a memory test. fMRI captures brain oxygen levels (BOLD) as a proxy for local neuron signaling—higher levels mean more brain activity.The head needs to keep very still throughout the scans to avoid blurring. That’s not easily accomplished with babies and toddlers. Previous studies circumvented the problem by imaging their brains while sleeping, but the results couldn’t capture memory processes.Be Part of the FutureSign up to receive top stories about groundbreaking technologies and visionary thinkers from SingularityHub.ruidWeekly NewsletterDaily NewsletterI agree to receive other communications from Singularity.I agree to allow Singularity to store and process my personal data in accordance with the company's Terms of Use and Privacy Policy.*Subscribe100% Free. No Spam. Unsubscribe any time.To keep the infants happy, engaged, and safe, parents brought favorite blankets and pacifiers, and younger infants were wrapped inside a comfortable vacuum pillow to reduce movement. A video system projected images onto the ceiling of the scanner within their line of sight.As the kids looked at a bright kaleidoscope-like video, images of faces, scenes, and objects would flash for a few seconds. These included toys or landscapes of an alpine cabin with mountains in the background. Previous studies found infants like to stare at objects or images they’ve seen before compared to new objects, suggesting they remember previous encounters.Throughout the sessions the team added projections showing a previously seen picture and a new one and monitored the infants’ eye movement using a video camera.“The ingenuity of their experimental approach should not be understated,” wrote Adam Ramsaran and Paul Frankland at the Hospital for Sick Children in Toronto, Canada, who were not involved in the study.BOLD FindingsThe kids often squirmed during the sessions. Some weren’t interested in the pictures; others fell asleep in the scanner.Still, the team managed to capture hippocampal BOLD signals averaging roughly eight minutes per participant and matched them to memory performance. On average, parts of the hippocampus ramped up activity for images that the infants later remembered—that is, they looked at it for longer during the test phases.But not all infants performed the same. The younger cohort, under a year, didn’t show the surge of BOLD signals suggesting memory encoding. They also ignored already seen images compared to new ones.It seems babies start encoding memories around a year of age, even as their hippocampus is still developing.The results are similar to those in baby rodents. The early years are chaotic. The brain undergoes extensive rewiring. This makes it a difficult to form lasting memories. Yet some supposedly lost memories encoded at a young age can be recovered later in life with reminder cues or by directly activating the set of neurons that originally encoded the memory.That’s not to say infants can acquire rich recollections—stories including multiple people, places, and things—at a year. The study only tested brain signatures for individual components.Future studies tracking the hippocampus might shed light on the minimal brain architecture needed to support vivid autobiographical memories. Examining other stages of memory could shine more light on infantile amnesia. For example, do infants also replay neural signals as they sleep to etch new experiences into long-term memory?And maybe—just maybe—our earliest memories could one day be retrieved later in childhood or beyond.NeuroscienceShelly FanShelly FanDr. Shelly Xuelai Fan is a neuroscientist-turned-science-writer. She's fascinated with research about the brain, AI, longevity, biotech, and especially their intersection. As a digital nomad, she enjoys exploring new cultures, local foods, and the great outdoors.Related ArticlesNew Tech Bends Sound Through Space So It Reaches Only Your Ear in a CrowdJiaxin ZhongandYun JingMar 18, 2025A Massive AI Analysis Found Genes Related to Brain Aging—and Drugs to Slow It DownShelly FanMar 17, 2025Scientists Discover Thousands of New Microbial Species Thriving in the Mariana TrenchShelly FanMar 06, 2025ScienceNew Tech Bends Sound Through Space So It Reaches Only Your Ear in a CrowdJiaxin ZhongandYun JingMar 18, 2025ScienceA Massive AI Analysis Found Genes Related to Brain Aging—and Drugs to Slow It DownShelly FanMar 17, 2025ScienceScientists Discover Thousands of New Microbial Species Thriving in the Mariana TrenchShelly FanMar 06, 2025What we’re readingScientists Find Giant Organic Molecules on Mars as Old as Life on EarthGizmodoThe Quantum Apocalypse Is Coming. Be Very AfraidWiredA Mysterious Startup Is Developing a New Form of Solar GeoengineeringWiredInside Google’s Two-Year Frenzy to Catch Up With OpenAIWired1X Will Test Humanoid Robots in ‘a Few Hundred’ Homes in 2025TechCrunchWatch the Atlas Robot Bust a Move in Boston Dynamics’ Latest VideoEngadgetIs Dark Energy Getting Weaker? New Evidence Strengthens the Case.QuantaSatellite Internet Will Enable AI in EverythingWiredAI-Driven Weather Prediction Breakthrough ReportedThe GuardianIn Las Vegas, a Former SpaceX Engineer Is Pulling CO2 From the Air to Make ConcreteFast CompanyThe Unbelievable Scale of AI’s Pirated-Books ProblemThe AtlanticTop Developers Want Nvidia Blackwell Chips. Everyone Else, Not So MuchThe InformationOpenAI’s Deep Research Agent Is Coming for White-Collar WorkWiredNot Everyone Is Convinced by Microsoft’s Topological QubitsIEEE SpectrumJudge Disses Star Trek Icon Data’s Poetry While Ruling AI Can’t Author WorksArs TechnicaSee the First Ever Footage of Sunset on the Moon Captured by Blue GhostDigital TrendsNvidia CEO Says AI Computing Needs to Surge 100-FoldThe Wall Street JournalNvidia Says ‘the Age of Generalist Robotics Is Here’The VergeGoogle Plans to Release New ‘Open’ AI Models for Drug DiscoveryTechCrunchWhen You Might Start Speaking to RobotsMIT Technology Review",
    "summary": {
      "en": "A recent study using brain scans has revealed that infants start forming memories around the age of 12 months. Researchers from Columbia and Yale University conducted scans on 26 infants and toddlers while they completed a memory task. They found that at about one year old, the hippocampus—a brain area essential for memory—begins to activate during memory formation.\n\nThe study aims to understand \"infantile amnesia,\" the phenomenon where most adults cannot recall memories from their early years. Scientists have long puzzled over why this happens despite significant learning during infancy, such as language acquisition and social interactions.\n\nUsing functional MRI (fMRI), the researchers observed how the hippocampus responded to familiar and new images shown to the infants. Results indicated that younger infants (under one year) did not show the same brain activity linked to memory encoding as older infants did. This suggests that while babies can form memories, the chaotic development of their brains during early years makes it challenging to retain them.\n\nThe findings imply that although infants might not have detailed memories, some memories formed during infancy could potentially be retrieved later in life. Future research may further explore how memories are processed during sleep and the minimum brain development needed for vivid autobiographical memories.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "87daf8ed44cf8318",
    "title": {
      "en": "Why we chose LangGraph to build our coding agent",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.qodo.ai/blog/why-we-chose-langgraph-to-build-our-coding-agent/",
    "score": 35,
    "by": "jimminyx",
    "time": 1742881836,
    "content": "Why we chose LangGraph to build our coding agent\n\n\t\t\t\t\t\t\t\tTechnology\n\n\t\t\t\t\t\t\t\tSagi Medina\n\n\t\t\t\t\t\t\t\t\tMarch 21, 2025\n\n\t\t\t\t\t\t\t\t\t4 min\n\n\t\t\t\t\t\tTable of ContentsFlexibility to be opinionatedCoherent interfaceReusable components across workflowsState managementAreas for growth\n\n\t\t\t Join Our Discord\n\n\t\t\t\t\tWe’ve been building AI coding assistants at Qodo since the GPT-3 days. Our initial approach was highly structured with predefined flows for different coding tasks like test generation, code reviews, and improvements. This approach worked well with earlier generations of LLMs and with structured flows we were able to get real-world value from older models, despite all of their limitations.\nSince Claude Sonnet 3.5 was released 9 months ago, LLMs have become significantly more capable at general-purpose coding tasks. The new models opened up the possibility to build something more dynamic and flexible while still maintaining our standards for code quality. We wanted to move away from rigid workflows to an agent that could adapt to any kind of user request, while still reflecting our opinionated views on how AI can be best used for coding.\nInitially, we needed a framework that would let us quickly validate our ideas and from the few options that were available about 4 months ago, we settled on LangGraph for our initial proof of concept. We were pleasantly surprised to see that the framework has proven flexible and mature enough to carry us all the way to production.\nIn this post, I’ll explain why LangGraph was the right choice for us, and how it enabled us to build a coding assistant that balances flexibility with our opinionated approach to coding best practices.\nFlexibility to be opinionated\nOur key consideration was the ability to create opinionated workflows while maintaining adaptability. LangGraph takes a graph-based approach that gives you flexibility to build agents that land anywhere on the spectrum from completely open-ended — where you just give an LLM all available tools and let it run in a loop — to fully structured deterministic flows (like the ones we started with).\nAt its core, LangGraph lets you define a state machine for your agent. You create nodes that represent discrete steps in your workflow and edges that define the possible transitions between them. Each node can perform specific functions—gathering context, planning, generating code, or validating—while the graph structure determines how these functions connect.\nThe density of connections in the graph corresponds to how structured or flexible your agent is. A sparse graph with few connections corresponds with a more rigid, predictable flow where each step leads to exactly one next step. A dense graph with many interconnections gives the agent more freedom to choose its path.\nFuture, more capable models might work best with fully open-ended approaches. But even with the best current LLMs, you still get better results when you guide them through the problem. If you use LLMs directly for coding, you’ve probably already developed your own workflow — like breaking problems down, providing context strategically, guiding the model through complex reasoning, and backtracking or iterating when needed.\nThe nice thing about LangGraph’s flexibility is that we can easily recalibrate how structured our flows are when new, more powerful models are released.\nOur main flow follows a pattern that you might recognize: first, a context collection node gathers relevant information from the codebase (and external resources via MCP integration); next, a planning node breaks down the task into manageable steps; then an execution node generates the actual code; finally, a validation node checks the output against best practices and requirements. When validation fails, the agent loops back to execution with specific feedback rather than starting from scratch.\n\nCoherent interface\nWhen you’re building a complex system, a framework should simplify rather than complicate your work. LangGraph’s API does exactly that.\nHere’s how a simplified version of our main workflow looks when implemented with LangGraph:\nPlain textCopy to clipboardOpen code in new windowEnlighterJS 3 Syntax Highlighterfrom langgraph.graph import StateGraph, ENDworkflow = StateGraph(name=\"coding_assistant\")workflow.add_node(\"context_collector\", collect_relevant_context)workflow.add_node(\"task_planner\", create_execution_plan)workflow.add_node(\"task_executor\", execute_plan)workflow.add_node(\"validator\", validate_output)# Define flow between nodesworkflow.add_edge(\"context_collector\", \"task_planner\")workflow.add_edge(\"task_planner\", \"task_executor\")workflow.add_edge(\"task_executor\", \"validator\")# Conditional routing based on validation resultsworkflow.add_conditional_edges(    \"validator\",    should_revise,    {        True: \"task_executor\",  # Loop back if revision needed        False: END               # Complete if validation passes    })graph = workflow.compile()graph.invoke({\"user_input\": \"build me a game like levelsio\"})from langgraph.graph import StateGraph, END\n\nworkflow = StateGraph(name=\"coding_assistant\")\nworkflow.add_node(\"context_collector\", collect_relevant_context)\nworkflow.add_node(\"task_planner\", create_execution_plan)\nworkflow.add_node(\"task_executor\", execute_plan)\nworkflow.add_node(\"validator\", validate_output)\n\n# Define flow between nodes\nworkflow.add_edge(\"context_collector\", \"task_planner\")\nworkflow.add_edge(\"task_planner\", \"task_executor\")\nworkflow.add_edge(\"task_executor\", \"validator\")\n\n# Conditional routing based on validation results\nworkflow.add_conditional_edges(\n    \"validator\",\n    should_revise,\n    {\n        True: \"task_executor\",  # Loop back if revision needed\n        False: END               # Complete if validation passes\n    }\n)\n\ngraph = workflow.compile()\ngraph.invoke({\"user_input\": \"build me a game like levelsio\"})from langgraph.graph import StateGraph, END\n\nworkflow = StateGraph(name=\"coding_assistant\")\nworkflow.add_node(\"context_collector\", collect_relevant_context)\nworkflow.add_node(\"task_planner\", create_execution_plan)\nworkflow.add_node(\"task_executor\", execute_plan)\nworkflow.add_node(\"validator\", validate_output)\n\n# Define flow between nodes\nworkflow.add_edge(\"context_collector\", \"task_planner\")\nworkflow.add_edge(\"task_planner\", \"task_executor\")\nworkflow.add_edge(\"task_executor\", \"validator\")\n\n# Conditional routing based on validation results\nworkflow.add_conditional_edges(\n    \"validator\",\n    should_revise,\n    {\n        True: \"task_executor\",  # Loop back if revision needed\n        False: END               # Complete if validation passes\n    }\n)\n\ngraph = workflow.compile()\ngraph.invoke({\"user_input\": \"build me a game like levelsio\"})\n\nThis declarative approach makes the code almost self-documenting. The workflow definition directly mirrors our conceptual diagram, which makes it easy to reason about and modify.\nEach node function receives the current state and returns updates to that state. There’s no magic happening behind the scenes, just straightforward state transitions.\nLangChain gets a lot of flack for its overly complicated abstraction, but the team really cooked with the LangGraph interface. It adds just enough structure without getting in your way or forcing you to adopt a complicated mental model and puts your agent logic on full display rather than obscuring it behind abstractions.\nReusable components across workflows\nReusability is what separates valuable frameworks from disposable ones. The node-based architecture that LangGraph uses is great here.\nOur context collection node is a good example. It handles gathering relevant information from the codebase and it’s used in pretty much every flow. The same goes for our validation node, which checks code quality and runs tests. These components can slot into different graphs with minimal configuration.\nAs we build out more flows, the velocity payout is huge. We’re building specialized flows like TDD that have different structures but reuse many of the same nodes, just connected in a different configuration with a few specialized components added in.\n\nState management\nThe most satisfying part of adopting the right framework is when you get useful functionality out of the box. LangGraph’s built-in state management is a perfect example.\nAdding persistence to our agent took just a few lines of code:\nPlain textCopy to clipboardOpen code in new windowEnlighterJS 3 Syntax Highlighterfrom langgraph.graph import StateGraph, ENDfrom langgraph.checkpoint.postgres import PostgresSaverworkflow = StateGraph(name=\"coding_assistant\")......checkpointer = PostgresSaver.from_conn_string(    \"postgresql://user:password@localhost:5432/db\")checkpointer.setup()graph = workflow.compile(checkpointer=checkpointer)from langgraph.graph import StateGraph, END\nfrom langgraph.checkpoint.postgres import PostgresSaver\n\nworkflow = StateGraph(name=\"coding_assistant\")\n...\n...\n\ncheckpointer = PostgresSaver.from_conn_string(\n    \"postgresql://user:password@localhost:5432/db\"\n)\ncheckpointer.setup()\n\ngraph = workflow.compile(checkpointer=checkpointer)from langgraph.graph import StateGraph, END\nfrom langgraph.checkpoint.postgres import PostgresSaver\n\nworkflow = StateGraph(name=\"coding_assistant\")\n...\n...\n\ncheckpointer = PostgresSaver.from_conn_string(\n    \"postgresql://user:password@localhost:5432/db\"\n)\ncheckpointer.setup()\n\ngraph = workflow.compile(checkpointer=checkpointer)\n\nThat’s it. With this simple addition, our entire workflow state—including context collected, plans made, and code generated—persists to our postgres database without us building any custom infrastructure. There are also SQLite and in memory checkpointers that can be added just as easily.\nWhat’s really neat is that this doesn’t just enable basic persistence across sessions. It supports checkpoints and branch points so you can undo and replay changes.\nAreas for growth\nWhile LangGraph has been a great foundation for our agentic flows, it’s not without challenges. One pain point has been documentation. The framework is developing very quickly and the docs are sometimes incomplete or out of date. The maintainers are great and were super responsive on Slack (thanks Harrison and Nuno for all the help :)). Be prepared to potentially need to communicate directly with the project maintainers if you’re using the newer and more niche capabilities.\nTesting and mocking is a huge challenge when developing LLM driven systems that aren’t deterministic. Even relatively simple flows are extremely hard to reproduce. Our agent interacts extensively with the IDE, which is difficult to simulate in automated tests. We built a mock repository that simulates basic IDE operations, but it doesn’t perfectly replicate the real environment. This creates a gap between what we can test automatically and what happens in production.\nFor example, operations like “find all usages of this function” that depend on the IDE’s language server are particularly hard to mock. This forced us to rely more on manual testing than we’d prefer, which slowed down the iteration cycle.\nMature frameworks tend to provide robust infrastructure for mocking and testing. I’m hopeful that LangGraph will develop in these areas over time.\n\n\t\t\t\t\t\t\t\tSagi Medina\n\n\t\t\t\t\t\t\t\t\tMarch 21, 2025\n\n\t\t\t\t\t\t\t\t\t4 min\n\n\t\t\t\t\t\tStart to test, review and generate high quality code\n\n\t\t\tGet Started\n\n\t\t\t\t\t\t\tGet Qodo\n\n\t\t\t\t\t\t\t\t\t\tVS Code\n\n\t\t\t\t\t\t\t\t\t\tJetBrains\n\n\t\t\t\t\t\t\t\t\t\tGithub, Qodo Merge\n\n\t\t\t\t\t\t\tSubscribe to the newsletter\n\n\t\t\t\t\t\t\t\thbspt.forms.create({\n\t\t\t\t\t\t\t\t\tregion: \"eu1\",\n\t\t\t\t\t\t\t\t\tportalId: \"26816375\",\n\t\t\t\t\t\t\t\t\tformId: \"6233f5ed-8ded-40a7-94cf-8fe3631b4b56\",\n\t\t\t\t\t\t\t\t\tonFormReady: function($form) {\n\t\t\t\t\t\t\t\t\t\tdocument.querySelector('.hs-input[type=\"email\"]').value = '';\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t});\n\n\t\t\t\t\t\t\tShare\n\nMore from our blog\n\n\t\t\t\t\t\t\t\t\tCode Reviews\n\n\t\t\t\t\t\t\t\t\tNews & Updates\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t4 min\n\n\t\t\t\tQodo Merge 1.0: solving key challenges in AI-assisted code reviews\n\n\t\t\t\t\t\t\t\tElana Krasner\n\n\t\t\t\t\t\t\t\tJan 29, 2025\n\n\t\t\t\tRead more\n\n\t\t\t\t\t\t\t\t\tLearn\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t14 min\n\n\t\t\t\tCommon JavaScript Errors Explained and How to Fix Them\n\n\t\t\t\t\t\t\t\tDavid Parry\n\n\t\t\t\t\t\t\t\tMar 24, 2025\n\n\t\t\t\tRead more\n\n\t\t\t\t\t\t\t\t\tTesting\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t10 min\n\n\t\t\t\t10 Best Automated Unit Testing Tools for Java\n\n\t\t\t\t\t\t\t\tMarin Levrel\n\n\t\t\t\t\t\t\t\tMar 24, 2025\n\n\t\t\t\tRead more\n\n\t\t\tBrowse the blog",
    "summary": {
      "en": "**Summary: Why We Chose LangGraph for Our Coding Agent**\n\nAt Qodo, we've been creating AI coding assistants since the GPT-3 era. Initially, we used a structured approach with fixed workflows for tasks like test generation and code reviews. However, with the advancement of LLMs (language learning models), particularly since the release of Claude Sonnet 3.5, we wanted a more flexible and adaptable coding agent that still aligns with our coding standards.\n\nWe chose LangGraph for our coding assistant because it allows us to create opinionated workflows while being adaptable. LangGraph uses a graph-based structure, enabling us to define workflows with nodes representing different steps (like gathering context and generating code) and edges showing how these steps connect. This structure allows us to adjust the level of flexibility in our workflows as new models emerge.\n\nLangGraph also offers a coherent interface, making it easy to implement complex systems. Its reusable components allow us to build various workflows efficiently, as many nodes (like context collection and validation) can be used across different tasks.\n\nFurthermore, LangGraph provides built-in state management, which simplifies adding features like data persistence. However, we face challenges, including incomplete documentation and difficulties in testing due to the unpredictable nature of LLMs.\n\nIn conclusion, LangGraph has proven to be a valuable framework for developing our coding agent, balancing flexibility with our best practices, while also allowing for future growth and adaptation.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "462735f2d1f833c4",
    "title": {
      "en": "I won't connect my dishwasher to your cloud",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.jeffgeerling.com/blog/2025/i-wont-connect-my-dishwasher-your-stupid-cloud",
    "score": 732,
    "by": "HieronymusBosch",
    "time": 1742836147,
    "content": "March 24, 2025\n\n            This weekend I had to buy a new dishwasher because our old GE died.\n\nI bought a Bosch 500 series because that's what Consumer Reports recommended, and more importantly, I could find one in stock.\n\nAfter my dad and I got it installed, I went to run a rinse cycle, only to find that that, along with features like delayed start and eco mode, require an app.\n\nNot only that, to use the app, you have to connect your dishwasher to WiFi, set up a cloud account in something called Home Connect, and then, and only then, can you start using all the features on the dishwasher.\n\n  This blog post is a lightly-edited transcript of my latest YouTube video on Level 2 Jeff: I won't connect my dishwasher to your stupid cloud.\n\nSo getting back first to that old GE dishwasher, it was, I don't know, I think that planned obsolescence is something that applies to many consumer products today.\n\nCompanies know if they design something to last only 5 or 10 years, that means in 5 or 10 years someone's going to have to buy a whole new one.\n\nAnd on my GE Amana dishwasher, it started having weird power issues, like the controls would just not light up unless I reset the circuit breaker for a few minutes. That started happening more often, and this past Saturday it just wouldn't come on no matter what, even after I tested and re-wired it all the way from the panel up to the dishwasher's internal power connector.\n\nSo it was dead.\n\nNext up, I looked at what it took to get a control board. Well... $299 for a control board that was 'special order' and might not even fix the problem? That's a non-starter for my $600, 8-year-old dishwasher.\n\nEven if I got it fixed, the front panel was starting to rust out at the hinge points (leaving some metal jaggies that my soon-to-be-crawling 6 month old could slice his fingers on), and other parts of the machine were showing signs of rust/potential future leaks...\n\nIt was Saturday night, and for a family of five, a dishwasher is kinda important. We don't have 1.5 hours every night to spend hand-washing dishes (not to mention the water bill!).\n\nSo I needed to get a new one, and it's really hard for me to schedule a few hours for my Dad and I to get it done in the middle of the week (plus that's multiple days without a dishwasher!).\n\nSo I did some research, and I found Bosch seemed to have the best bet for under $1,000, available locally on a Sunday.\n\nConsumer Reports, random Redditors, etc. seemed to have some praises for Bosch—on Reddit many also praised Miele, but I couldn't find any of those available locally. And Consumer Reports especially praised all the Bosch units, with them topping all their reliability and customer satisfaction charts!\n\nI remembered five or ten years ago, whenever I had bought my old GE, I remembered Bosch topped the charts too, but back then I settled for GE to save a few bucks...\n\nInstallation was (mostly) great!\n\nSo I spent a little more this time, hoping for a better experience. And installation was actually great—it was a lot easier to install the Bosch than it was that the GE.\n\nIt has a plastic base that slides better on the floor, and there's easier routing of the drain hose, inlet hose, and power wire that makes it less risky when you're pushing the thing into the blind cutout under the counter.\n\nThe one weird thing was that whoever like tightened the feet on the bottom at the factory must've used an impact driver or something because they were all practically embedded, and wouldn't budge.\n\nI was turning the little screw on the front that pushes the rear foot down through a little gearing, but the worm gear slipped out and kinda shoved the long rod that connects the front to the back out of place. I had to buy a 10mm hex socket to wiggle the foot loose enough the gearing would actually work.\n\nBut once that was done, the rest of the install was seamless. (Thanks especially to some help from my Dad).\n\nFirst use, encountering the Cloud requirement\n\nSo I turned it on, and immediately hated the new touch sensor stuff on it.\n\nThe old GE had buttons: you press them in, they click and you know that you pressed a button.\n\nThe touch sensor, you kind of touch it and the firmware—like this new dishwasher actually takes time to boot up! I had to reset it like three times and my wife meanwhile was like laughing at me like look at this guy who does tech stuff and he can't even figure out how to change the cycle on it.\n\nThat took about five minutes, sadly.\n\nBut eventually I pulled out the manual book because I was like... \"this is actually confusing.\"\n\nIt should be like: I touch the button and it changes to that mode! But that was not how it was working.\n\nI wanted to run just a rinse cycle to make sure the water would go in, the water would pump out through the sump, and everything worked post-install.\n\nBut I couldn't find a way to do a rinse cycle on the control panel.\n\nSo I looked in the manual and found this note:\n\nIt says options with an asterisk—including Rinse, Machine Care (self-cleaning), HalfLoad, Eco, and Delay start, are \"available through Home Connect app only and depending on your model.\"\n\nThe 500 series model I bought isn't premium enough to feature a 7-segment display like the $400-more-expensive 800 series, so these fancy modes are hidden behind an app and cloud service.\n\nI was like, \"Okay, I'll look up this app and see if I can use it over Bluetooth or locally or whatever.\"\n\nNope! To use the app, you have to connect your dishwasher to your Wi-Fi, which lets the dishwasher reach out on the internet to this Home Connect service.\n\nYou have to set up an account on Home Connect, set up the Home Connect app on your phone, and then you can control your dishwasher through the Internet to run a rinse cycle.\n\nThat doesn't make any sense to me.\n\nAn app? I mean, I can understand maybe adding some neat convenience features for those who want them. Like on my new fridge—which I chose not to connect to WiFI—it has an app that would allow me to monitor the inside temperature or look up service codes more easily. If I wanted those add-on features, which my old fridge didn't have, I could get them.\n\nBut requiring an app to access features that used to be controllable via buttons on the dishwasher itself—or are still if you pay $400 more for the fancy \"800\" model? That's no bueno.\n\nWhat can I do?\n\nWell, first of all, I could just not use those features. That's kind of annoying because I bought it with the assumption that I could run the cleaning cycle, that I could run the rinse cycle without having to have an app and Wi-Fi.\n\nAnother option is I could just connect it to my Wi-Fi, maybe on an IoT VLAN.\n\nBut it's not like a video doorbell, where an Internet connection adds on functionality, like being able to see who rang while you're on vacation, or storing security footage clips on the cloud so you have them available even after someone robs your house...\n\nBut a dishwasher... I'm not going to remote control my dishwasher and like, run an extra rinse cycle while I'm on a beach somewhere.\n\nI don't need Internet on my dishwasher.\n\nAnother third option is somebody has reverse engineered this protocol and built HCPY, a Home Connect Python library.\n\nBut here's the problem: I already spent like four hours getting this dishwasher installed in my kitchen. I don't want to spend another four hours configuring my own web UI for it—which still requires at least a one-time connection through Home Connect!—and maintaining that as a service on my local network, relying on an unauthorized third party library using reverse-engineering to get at the private dishwasher API!\n\nWhat can we do?\n\nI don't think we should let vendors get away with this stuff.\n\nFirst, it lets product designers get lazy.\n\nA feature like a little display, a little seven segment display that can show like two letters and a number of minutes remaining or something like that... How much does that cost?\n\nHow hard is it to integrate that into every model, even the cheap ones?\n\nA lot of cheap dishwashers actually have those things, but not Bosch!\n\nWith Bosch, you have to pay $400 for the privilege of a little display!\n\nSecond, this might be a little bit conspiracy theory or whatever, but it feels like it's part of planned obsolescence. Just like with the GE, where a lot of parts are designed to rust out after 5 or 10 years.\n\nIf you have a cloud app, that means there's a cloud service that has to be running. That costs money to maintain.\n\nAnd if there's no subscription fee right now, that means one of two things:\n\nThey could be selling our data already.\nAt some point, they'll either close the service because it's a cost center (so the rinse cycle and eco mode on all these dishwashers just goes \"POOF!\"), or they're going move to a subscription model.\n\nAll of a sudden, if you want to run the self-cleaning cycle, you better start paying five bucks to Bosch every month. Forever.\n\nThat's insane.\n\nThird, it's a security hole in your local network.\n\nBosch might be a little better than some no-name light bulb company making IoT light bulbs on Amazon, but only a little.\n\nI don't want to have Bosch having full internet access on my local network.\n\nTheir API would be able to talk back to my dishwasher, and the dishwasher—unless I put it on a VLAN, which 99% of consumers have no clue how to do that...\n\nThat's just something that shouldn't have to happen.\n\nThis is a dishwasher!\n\nI don't know.\n\nWhat should be done?\n\nWhen I posted on social media about this, a lot of people told me to return it.\n\nBut I spent four hours installing this thing built into my kitchen.\n\nI hooked it up to the water, it's running through cycles... it is working. I'll give them that. It does the normal stuff, but you know, there are some features that don't work without the app.\n\nAt a minimum, I think what Bosch should do is make it so that the dishwasher can be accessed locally with no requirement for a cloud account. (Really, it'd be even better to have all the functions accessible on the control panel!)\n\nAnyone building an IoT device, here is my consumer-first, e-waste-reduction maxim:\n\n  First local, then cloud.\n\nCloud should be an add-on.\n\nIt should be a convenience for people who don't know how to do things like connect to their dishwasher with an app locally.\n\nAnd it's not that hard.\n\nA little ESP32, a little $1 chip that you can put in there could do all this stuff locally with no cloud requirement at all.\n\nI think that there might be some quants or people who want to make a lot of money building all these cloud services.\n\nI don't think that it's people who are consumer-first and eco-conscious because if they were, they would give us control first and then add on 'nice' quality of life features through a cloud service.\n\nWith my Bosch 500 series dishwasher, I was excited after the easy install (besides those leveling feet), then was let down so hard once I found out I couldn't use all the features it came with.\n\n      Further reading\n\n          Using the Shelly Plug to monitor Starlink's power consumption\n          Raspberry Pi Zero 2 powers the Null 2 RetroPie gaming handheld\n          Build your own private WireGuard VPN with PiVPN\n\n              dishwasher\n              cloud\n              internet of things\n              iot\n              apps\n              home\n              kitchen\n              bosch\n              security\n\n    Add new comment\n\n      Comments\n\n     me – 17 hours ago\n\n            Beyond ridiculous.\nWe have a Whirlpool dryer purchased in (cough) fall 1984.  Nice and simple and old school.  No computers.  No cloud.  JustPlainWorks.\nWe had a guy come out a number of years ago to do minor maintenance on the corresponding 20+ year Whirlpool washer and he said something along the lines of \"yup - love these models from back then. They never break.  They are repairable.  Do not ever replace that dryer. Just run it forever because that's how long it is going to last. So simple.\"\nContrast that with a former coworker who went with a super programmable LG (I think) who had nothing but problems with all the unnecessary electronics.\nAt some point you need to vote with your wallet.  Rip it out and take it back as 'does not work'.  Something that expensive shouldn't require anything at all other than power, water, and you pressing a button.\n\n      Reply\n\n    Billybillybilly – 15 hours ago\n\n            You really appreciate a lot of the ingenuity that goes into engineer consistent design. One example I have that features a useful design that is local first then cloud is ac on cars.\nOn my car, if I hold my keyfob it rolls down all the windows to get the stale air out. It’s a great local feature.\nI was spending time with a friend at a park, and as were walking back the car they whip out their phone to turn on their cars ac remotely. A nice feature that doesn’t work when your car is under 60 ft of rock.\nCut to trying to explain how a car has cellular data and has to connect to a network to be able to remote control it.\nI would like to be able to crank my ac remotely but for now local control best suits my desires.\n\n      Reply\n\n    Jeff Geerling – 15 hours ago\n\n            I found a page on Home Connect on the 'Consumer Action Taskforce' wiki, and it mentions the cloud requirement and 'no cloud' offline mode (which requires a cloud account and WiFi connection to enable, lol), but that page didn't mention the fact some devices hide features behind Home Connect, that you can't use if you don't use Home Connect.\n      Reply\n\n    Bill – 14 hours ago\n\n            Bosch has beaten you.  You paid your money, found a major defect (and it is a defect), and kept it anyway.  They win, you lose.  Zero deterrence.\n\n      Reply\n\n    KaBosch – 11 hours ago\n\n              In reply to Bosch has beaten you.  You… by Bill\n\n            Jeff should return it, it is *only* four hours. The block is mental, not physical. The product is defective. I won't be be buying a Bosch, too bad because I had one in a rental years ago and it was amazing. The MBAs who made this decision should own their mistakes.\n\n      Reply\n\n    Ryan of Drupal – 6 hours ago\n\n              In reply to Jeff should return it, it is… by KaBosch\n\n            Time is money and four hours is a lot of opportunity cost that he could have spent doing multitudes of income generating time. I would be willing to wager that Jeff's time is honestly worth more than 100$ an hour,  so taking it back *is* a loss.\n\n      Reply\n\n    Steve – 11 hours ago\n\n              In reply to Bosch has beaten you.  You… by Bill\n\n            Agreed, they just want money and they got it.\n\n      Reply\n\n    marimba – 12 hours ago\n\n            Please, return it! It‘s the only way they learn.\n\n      Reply\n\n    Some dude from… – 11 hours ago\n\n            Your government ordering strikes on other nations over signal, and you are worried about your dishwasher data stored in eagle data center somewhere in Swiss alps.\n\n      Reply\n\n    another random dude  – 11 hours ago\n\n              In reply to Your government ordering… by Some dude from…\n\n            Hahaha dude you should not be honest online.\n\n      Reply\n\n    Another dude from... – 10 hours ago\n\n              In reply to Your government ordering… by Some dude from…\n\n            Yea, he should be worried about those things of which he has no control over, versus this, which he does and impacts him on a personal level.\n\n      Reply\n\n    Not random – 7 hours ago\n\n              In reply to Your government ordering… by Some dude from…\n\n            It's all billionaire oligarchs doing billionaire oligarch shit. Their power comes from exploiting us and our willingness to let them do it.\nIt starts with forcing us to be renters. It ends with bombs dropping on innocent children held hostage by other oligarchs as human shields so they can launch bombs at innocent merchant marines exploited by still other oligarchs. It's all about control at every level.\nBosch and GE can get bent.\n\n      Reply\n\n    David – 9 hours ago\n\n            There's also an option to use other version of hcpy with HomeAssistant MQTT support which setup should take quite a bit less than a few hours https://github.com/hcpy2-0/hcpy/wiki/HomeAssistant-Addon but fully agreed that this should not be the standard... They've probably went the route they did so not to have anything Bluetooth related as that might look cheap and/or might introduce other vulnerabilities/bugs in the communication stack.\n\n      Reply\n\n    Dmitry Samsonov – 8 hours ago\n\n            Now imagine this scenario: A global war breaks out, and the cloud service controlling your dishwasher activates its hidden military functions... And it turns out there are several of them—backdoors in the chips from China, exploits in the application software from the EU, and vulnerabilities in open-source libraries, inserted by Russia and North Korea... And you could dodge all this nightmare by just washing your dishes the old-fashioned way—supposedly better for your health too!\n\n      Reply\n\n    Seb – 7 hours ago\n\n            Lesson learned from your experience: check out appliance models, and read their manuals before deciding what to get.\n\n      Reply\n\n    pg – 7 hours ago\n\n            Don't get your hopes too high.\nBOSCH's electrical equipment doesn't belong to BOSCH anymore, but is part of the large Siemens group for many years. Siemens in germany is known for its consumer products to be very expensive (cause of their very good reputation they may earned a hundred years ago), full of mostly useless features,  complicated in handling, very low on product quality, high mainting aand repair rates, and outstanding repair cost.\nIn germany they have a joke:\"Does it have to work, or can it be by Siemens?\"\nI had the full parade of Siemens/Bosch-be-part-of-Siemens consumer electrical, and electronical products over the last twentyfive years: all expensive crap. I'm  through with dishwashers, vacuum cleaners, refrigerators, stoves, and several other devices, all Siemens/Bosch, all expensive, and crap!\nBosch and Siemens brands are on my personal black list of never buy again, ever.\nSorry, to tell you that, but you propably be better served with some \"mexican\" dishwasher for half the price - it will at least does its job for maybe 6...8 years without repairs or 'funny behaviour'.\nYou'll see when you Bosch maschine gets broken within the next two and a half years, and face the repair cost half the price of a new machine, and then it will be broken half a year later again. We decided to change your dishwasher to another brand after we paid over 150% of the purchasing price for repairs only within 4 years, not to mention hundreds of situations I needed all my self control not to rip this crap out of the kitchen and throw it out of the window.\n\n      Reply\n\n    MR – 7 hours ago\n\n            We got the Bosch model with the LCD but still have to use the app for Machine Care, Rinse, etc. Some of the IOT features are pretty good honestly, reminders to fill the rinse aid or run a machine care cycle are useful, but I'd much prefer to have no wifi at all! Unfortunately most of the new applicances have wifi or they are low grade base models with no features, and it takes so much work to find a decent used applicance it's difficult to find a proper solution.\n\n      Reply\n\n    Leo – 5 hours ago\n\n            Where I rent, there's a V-Zug dishwasher. Very common over here in Switzerland. It's got exactly what you're asking, local connectivity via Bluetooth and cloud stuff over wifi if you desire. None of it mandatory. However it does come with a different price tag.\nWhat you might be experiencing is the old printer and cartridge business model. Sell a product below cost and make it back later via subscriptions / data brokerage.\n\n      Reply",
    "summary": {
      "en": "This text describes the author's experience buying a new Bosch 500 series dishwasher after their old GE model broke down. They chose Bosch based on positive reviews from Consumer Reports and availability. Installation went well, but they were frustrated to discover that many features, like rinse cycles and eco mode, require an app and a WiFi connection. \n\nThe author criticizes modern appliances for needing internet connectivity to access basic functions, arguing this is a form of planned obsolescence and raises security concerns. They express disappointment that features that used to be accessible through buttons now require an account on a cloud service. \n\nThe author concludes by suggesting that manufacturers should prioritize local control over cloud features in appliances, advocating for consumer-first design to avoid unnecessary complications and maintain functionality. They also mention that returning the dishwasher has been suggested, but they hesitate due to the time invested in installation.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "e5bcbc9609f02ec9",
    "title": {
      "en": "The earliest versions of the first C compiler known to exist",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/mortdeus/legacy-cc",
    "score": 347,
    "by": "diginova",
    "time": 1742545039,
    "content": "legacy-cc\nThe earliest versions of the very first c compiler known to exist in the wild written by the late legend himself dmr.\nThese are not capable of being compiled today with modern c compilers like gcc. I am only posting these here for the enjoyment and reminiscing of the spark that ignited a soon to be trillion dollar industry. Enjoy. :)\nYou guys might be able to play around building this compilier with Aiju's pdp-11/unix emulator. I havent tried building this myself so I cant confirm it works, but I posted the link to her emulator for anybody wanting to hack around with it.\nSource: [https://www.bell-labs.com/usr/dmr/www/primevalC.html] alt-link [http://cm.bell-labs.co/who/dmr/primevalC.html]\nUseful Links\nPDP-11 Emulator: [http://pdp11.aiju.de/] (so i've been playing around with this to see if it still works, and it clearly has problems right now. if somebody wants to try and create like a VM image of unix running on PDP-7 hardware, id happily link to it here.)\nResearch Unix Repository: [https://www.tuhs.org/Archive/Distributions/Research/]",
    "summary": {
      "en": "This text discusses the earliest versions of the first C compiler created by Dennis Ritchie (DMR). These compilers cannot be used with modern tools like GCC. The author shares them for nostalgia and to highlight their historical significance in starting a major industry. \n\nThey mention the possibility of using Aiju's PDP-11/Unix emulator to experiment with building the compiler, although they haven't tested it personally. Links to the emulator and a repository for Research Unix are provided for those interested in exploring further.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "6e96c28d86e44d47",
    "title": {
      "en": "Show HN: I'm a teacher and built an AI presentation tool",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": null,
    "score": 208,
    "by": "slidehero",
    "time": 1742709515,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "76fe87c1edde5391",
    "title": {
      "en": "MRubyD: A new mruby virtual machine implemented in pure C#",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/hadashiA/MRubyD",
    "score": 81,
    "by": "ksec",
    "time": 1742712779,
    "content": "MRubyD\nMRubyD is a new mruby virtual machine implemented in pure C#. The name \"MRubyD\" stands for mruby for dotnet and pays homage to the well-known alternative implementation mruby/c. Designed with seamless integration in mind for C#-based game engines, and emphasize ruby level compatibility. MRubyD leverages the latest C# features for high performance and high extensibility.\nNoteThis library is currently in preview\n\nFeatures\n\nImplemented in C#\nUtilizes the robust capabilities of C# to ensure seamless integration with C#-based game engines.\n\nHigh Performance\nTakes advantage of modern C# language features—such as managed pointers, Span, and the performance benefits of the .NET runtime’s GC and JIT compiler—to deliver superior speed and efficiency.\n\nHigh compatibility with Ruby-level APIs  (Work in progress)\nIt is intended for use in software with a certain amount of resources, such as games. For this reason, we are focusing on Ruby API compatibility.\nAt release, all opcodes are implemented and pass the syntax.rb tests from the mruby repository.\n\nRich Library Integration & Extensibility\nCompared to the original C implementation, calling C#’s extensive libraries from Ruby is straightforward, making the VM highly extensible.\n\nLimitations (Preview Release)\nThis release is a preview version and comes with the following constraints:\n\nBuilt-in types and methods are still being implemented.\n\nPlease refer to ruby test, etc., for currently supported methods.\nWe are working on supporting all methods that are built into mruby by default.\n\nprivate and protected visibitily is not yet implemented. (mruby got support for this in 3.4)\nThis project provides only the VM implementation; it does not include a compiler. To compile mruby scripts, you need the native mruby-compiler.\n\nMost recent roadmap\n\n Implement builtin ruby libs\n Support Fiber\n All ruby code port to C# (for performance reason)\n Unity Integration\n VitalRouter.MRuby for the new version.\n\nInstallation\ndotnet add package MRubyD\n\nBasic Usage\nExecute byte-code\ndef fibonacci(n)\n  return n if n <= 1\n  fibonacci(n - 1) + fibonacci(n - 2)\nend\n\nfibonacci 10\n\n$ mrbc -o fibonaci.mrbc fibonacci.rb\n\nusing MRubyD;\n\n// Read the .mrb byte-code.\nvar bytes = File.ReadAllBytes(\"fibonacci.mrb\");\n\n// initialize state\nvar state = MRubyState.Create();\n\n// execute bytecoe\nvar result = state.Exec(bytes);\n\nresult.IsInteger    //=> true\nresult.IntegerValue //=> 55\n\nThis is a sample of executing bytecode.\nSee the How to compile .mrb  section for information on how to convert Ruby source code to mruby bytecode.\nHandlding MRubyValue\nAbove result is MRubyValue. This represents a Ruby value.\nvalue.IsNil //=> true if nol\nvalue.IsInteger //=> true if integrr\nvalue.IsFloat //=> true if float\nvalue.IsSymbol //=> true if Symbol\nvalue.IsObject //=> true if any allocated object type\n\nvalue.VType //=> get known ruby-type as C# enum.\n\nvalue.IntegerValue //=> get as C# Int64\nvalue.FloatValue //=> get as C# float\nvalue.SymbolValue //=> get as `Symbol`\n\nvalue.As<RString>() //=> get as object value\n\n// pattern matching\nif (vlaue.Object is RString str)\n{\n    // ...\n}\n\nswtich (value)\n{\n    case { IsInteger: true }:\n        // ...\n        break;\n    case { Object: RString str }:\n        // ...\n        break;\n}\n\nvar intValue = MRubyValue.From(100); // create int value\nvar floatValue = MRubyValue.From(1.234f); // create float value\nvar objValue = MRubyValue.From(str); // create allocated ruby object value\n\nDefine ruby class/module/method by C#\n// Create MRubyState object.\nvar state = MRubyState.Create();\n\n// Define class\nvar classA = state.DefineClass(Intern(\"A\"u8), c =>\n{\n    // Method definition that takes a required argument.\n    c.DefineMethod(Intern(\"plus100\"u8), (state, self) =>\n    {\n        var arg0 = state.GetArgAsIntegeger(0); // get first argument (index:0)\n        return MRubyValue.From(arg0 + 100);\n    });\n\n    // Method definition that takes a block argument.\n    c.DefineMethod(Intern(\"method2\"), (state, self) =>\n    {\n        var arg0 = state.GetArg(0);\n        var blockArg = state.GetBlockArg();\n        if (!blockArg.IsNil)\n        {\n            // Execute `Proc#call`\n            state.Send(blockArg, state.Intern(\"call\"u8), arg0);\n        }\n    });\n\n    // Other complex arguments...\n    c.DefineMethod(Intern(\"method3\"), (state, self) =>\n    {\n        var keywordArg = state.GetKeywordArg(state.Intern(\"foo\"))\n        Console.WriteLine($\"foo: {keywordArg}\");\n\n        // argument type checking\n        state.EnsureValueType(keywordArg, MrubyVType.Integer);\n\n        var restArguments = state.GetRestArguments();\n        for (var i = 0; i < restArguments.Length; i++)\n        {\n            Console.WriteLine($\"rest arg({i}: {restArguments[i]})\");\n        }\n    });\n\n    // class method\n    c.DefineClassMethod(Intern(\"classmethod1\"), (state, self) =>\n    {\n        var str = state.NewString($\"hoge fuga\");\n        return MRubyValue.From(str);\n    });\n\n});\n\n// Monkey patching\nclassA.DefineMethod(Intern(\"additional_method1\"u8), (state, self) => { /* ... */ });\n\n// Define module\nvar moduleA = state.DefineModule(Intern(\"ModuleA\");)\nstate.DefineMethod(moduleA, Intern(\"additional_method2\"u8), (state, self) => MRubyValue.From(123));\n\nstate.IncludeModule(classA, moduleA);\n\na = A.new\na.plus100(123) #=> 223\n\na.method2(1) { |a| a } #=> 1\n\na.additionoa_method2 #=> 123\n\nA.classmethod1 #=> \"hoge fuga\"\n\nSymbol/String\nThe string representation within mruby is utf8.\nTherefore, to generate a ruby string from C#, Utf8StringInterpolation is used internally.\n// Create string literal.\nvar str1 = state.NewString(\"HOGE HOGE\"u8);\n\n// Create string via interpolation\nvar x = 123;\nvar str2 = state.NewString($\"x={x}\");\n\n// wrap MRubyValue..\nvar strValue = MRubyValue.From(str1);\n\nThere is a concept in mruby similar to String called Symbol.\nLike String, it is created using utf8 strings, but internally it is a uint integer.\nSymbols are usually used for method IDs and class IDs.\nTo create a symbol from C#, use Intern.\n// symbol literal\nvar sym1 = state.Intern(\"sym\"u8)\n\n// symbol from string\nvar sym2 = state.ToSymbol(state.NewString(\"sym2\"u8));\n\nHow to compile .mrb\nMRubyD only includes the mruby virtual machine. Therefore it is necessary to convert it to .mrb bytecode before executing the .rb source.\nBasically, you need the native compiler provided by the mruby project.\n$ git clone git@github.com:mruby/mruby.git\n$ cd mruby\n$ rake\n$ ./build/host/bin/mrubc\n\nMRubyD.Compiler\nTo simplify compilation from C#, we also provide the MRubyD.Compiler package, which is a thin wrapper for the native compiler.\nNoteThis MRubyD.Compiler package is a thin wrapper for the native binary. Currently, builds for linux (x64/arm64), macOS (x64/arm64), and windows (x64) are provided.\n\ndotnet add package MRubyD.Compiler\n\nusing MRubyD.Compiler;\n\nvar source = \"\"\"\ndef a\n  1\nend\n\na\n\"\"\"u8;\n\nvar state = MRubyState.Create();\nvar compiler = MRubyCompiler.Create(state);\n\nvar irep = compiler.Compile(source);\n\nstate.Exec(irep); // => 1\n\nLICENSE\nMIT\nContact\n@hadahsiA",
    "summary": {
      "en": "**Summary of MRubyD**\n\nMRubyD is a virtual machine for mruby written in C#. It is designed to work well with C#-based game engines, providing high performance and compatibility with Ruby APIs. Currently, it's in a preview stage.\n\n**Key Features:**\n- **C# Implementation:** Built entirely in C#, making it easy to integrate with C# projects.\n- **High Performance:** Utilizes modern C# features for better speed and efficiency.\n- **Ruby Compatibility:** Aims for compatibility with Ruby APIs, with all basic opcodes implemented.\n- **Extensibility:** Easily calls C# libraries from Ruby, enhancing its functionality.\n\n**Limitations:**\n- Some built-in types and methods are still under development.\n- Private and protected visibility features are not yet implemented.\n- Only provides the virtual machine; a separate compiler is needed for mruby scripts.\n\n**Installation:**\nTo install, use the command: `dotnet add package MRubyD`.\n\n**Basic Usage Example:**\nYou can execute Ruby bytecode after compiling it into `.mrb` format. The process involves reading the bytecode, initializing the MRuby state, and executing the code.\n\n**Development Features:**\n- Supports defining Ruby classes and methods in C#.\n- Allows for monkey patching and module definitions.\n\n**Compilation:**\nMRubyD requires an external compiler to convert Ruby scripts to `.mrb` bytecode. A wrapper package (`MRubyD.Compiler`) is available for easier compilation.\n\n**License:** \nMRubyD is licensed under the MIT License. \n\nFor more information, contact @hadahsiA on GitHub.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "69d49cc6504c4fff",
    "title": {
      "en": "Intel: A Bug and a Pro",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.abortretry.fail/p/intel-a-bug-and-a-pro",
    "score": 60,
    "by": "rbanffy",
    "time": 1742851213,
    "content": "Share this postAbort Retry FailIntel: a Bug and a ProCopy linkFacebookEmailNotesMoreDiscover more from Abort Retry FailIn Abort Retry Fail, I am attempting to chronicle the history of the computing industry. This is a goal that will likely not be completed within my lifetime.SubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.Already have an account? Sign inIntel: a Bug and a ProThe FDIV bug and release of the P6Feb 24, 202518Share this postAbort Retry FailIntel: a Bug and a ProCopy linkFacebookEmailNotesMore1ShareBy 1994, Intel had managed to capture about 75% of the microprocessor market for personal computers. The company had also found its way into automobiles, industrial controls, telecommunications equipment, and supercomputers. The Intel x86 architecture had propelled the company to these immense heights, and they’d established a cadence of releases in that CPU line of roughly four years. The investment that both individuals and companies had made in hardware and software for the architecture was measured in billions.While the Pentium had been announced on the 22nd of March in 1993, systems built around the Pentium were announced on the 17th of May in 1993, and volume shipments didn’t occur until later in the year. The Pentium’s 60MHz variant could deliver 100 million instructions per second with the Dhrystone 1.1 benchmark while the 66MHz variant bumped that to 112MIPS. These were the first x86 chips to be capable of this kind of performance, and they nearly doubled the performance of the original i486. This kind of performance nearly put Intel in competition with RISC-based UNIX workstation hardware, and Intel’s biggest software partner, Microsoft, supported those workstations with NT. A key difference, however, was that the Pentium’s price was around $900 while the MIPS R8000 was around $2500 and the Alpha 21064 was over $3000. Naturally, these were all far more reasonably priced when purchased in volume, but Intel was far better in terms of performance per dollar. For less than the cost of a high performance RISC CPU, a customer could purchase an entire Pentium-based PC that, while not as powerful, was capable of the same kinds of work. This is remarkable as PC manufacturers were running into the problems already solved by workstation makers: thermals, volatage variation, RFI, EMI. These kinds of considerations hadn’t been too big a hill to climb with an XT, but they were far more serious issues with a Pentium. They were serious enough that Intel had provided thermal models to system makers before they had provided the silicon.Early in 1994, Intel announced the Pentium P54C for Socket 5. These chips required a 3.3 volt power supply, could run at 75MHz, 90MHz, or 100MHz, and utilized an internal clock multiplier allowing them to operate at a higher frequency than the external buses. These chips bumped the transistor count to 3.3 million, and were built on a half micron BiCMOS process. These Pentiums also added support for use in multiprocessor systems which was the primary source of the increased transistor count. When adding a processor, a machine needs more circuitry for cache, memory, interrupts, and bus utilization.While most news for Intel was great, not all of it was. A few of the first generation Pentium processors were in the hands of Professor Thomas R. Nicely at Lynchburg College in Virginia where they were put to work researching reciprocals of twin prime numbers. On the 13th of June in 1994, Nicely noted that after adding three Pentium-based computers to his lab, he was getting inconsistent results in floating point divisions. He ruled out all causes other than hardware error on the 19th of October. Specifically, Nicely noted that 1 divided by 824633702441 was incorrect on three different Pentium systems while it was correct on multiple i486 and i386 systems. He then sent an email to Intel on the 24th documenting his discovery. Intel didn’t provide much information to him beyond stating that the issue was known. He then sent an email to several other people on the 30th. His email read:It appears that there is a bug in the floating point unit (numeric coprocessor) of many, and perhaps all, Pentium processors.In short, the Pentium FPU is returning erroneous values for certain division operations. For example, 1/824633702441.0 is calculated incorrectly (all digits beyond the eighth significant digit are in error). This can be verified in compiled code, an ordinary spreadsheet such as Quattro Pro or Excel, or even the Windows calculator (use the scientific mode), by computing (824633702441.0)*(1/824633702441.0), which should equal 1 exactly (within some extremely small rounding error; in general, coprocessor results should contain 19 significant decimal digits). However, the Pentiums tested return 0.999999996274709702 for this calculation. A similar erroneous value is obtained for x*(1/x) for most values of x in the interval 824633702418 <= x <= 824633702449, and throughout any interval obtained by multiplying or dividing the above interval by an integer power of 2 (there are yet other intervals which also produce division errors).The bug can also be observed by calculating 1/(1/x) for the above values of x. The Pentium FPU will fail to return the original x (in fact, it will often return a value exactly 3072 = 6*0x200 larger).The bug has been observed on all Pentiums I have tested or had tested to date, including a Dell P90, a Gateway P90, a Micron P60, an Insight P60, and a Packard-Bell P60. It has not been observed on any 486 or earlier system, even those with a PCI bus. If the FPU is locked out (not always possible), the error disappears; but then the Pentium becomes a “586SX”, and floating point must run in emulation, slowing down computations by a factor of roughly ten.I encountered erroneous results which were related to this bug as long ago as June, 1994, but it was not until 19 October 1994 that I felt I had eliminated all other likely sources of error (software logic, compiler, chipset, etc.). I contacted Intel Tech Support regarding this bug on Monday 24 October (call reference number 51270). The contact person later reported that the bug was observed on a 66-MHz system at Intel, but had no further information or explanation, other than the fact that no such bug had been previously reported or observed.Further information can be obtained by contacting me directly, and by downloading files from the [anonymous.nicely.pentium_bug] directory of the acavax.lynchburg.edu machine via anonymous ftp on Internet (password ANONYMOUS, user ID = Internet ID). These files include a documentation file, a DOS executable image demonstrating the bug, and the source code for the demonstration. The zip file uses PKZIP version 2.04g.I would be interested in hearing of test results from other Pentiums, and also from 486-DX4s and (if anybody has one yet) the AMD, Cyrix, and NexGen clones of the Pentium.You may use this information freely as long as you give me attribution by name and employer.This… was exactly the sort of thing folks at Intel would not have wanted to have happen. Andrew Schulman, author of Undocumented DOS and Undocumented Windows, was among the recipients of that email. At this time, Schulman didn’t own a Pentium, so he forwarded the email to Richard Smith of Phar Lap and asked him to test it. Smith and his company confirmed the bug, and Smith posted the email to a CompuServe forum. Within 24 hours, there were 10 more confirmations. This was called the FDIV bug (after the x86 assembly language mnemonic for floating point division), and it made headlines. On the 7th of November in 1994, the Electronic Engineering Times ran an article titled: Intel fixes a Pentium FPU glitch which read:To correct an anomaly that caused inaccurate results on some high-precision calculations, Intel Corp. last week confirmed that it had updated the floating-point unit (FPU) in the Pentium microprocessorThe company said that the glitch was discovered midyear and was fixed with a mask change in recent silicon. \"This was a very rare condition that happened once every 9 to 10 billion operand pairs,\" said Steve Smith, a Pentium engineering manager at Intel.A spot check last week indicated the problem is present in at least one recently made Pentium-based PC. Intel said it could not quantify how many such systems were in the field.Said an Intel spokesman: \"This doesn't even qualify as an errata. We fixed it in a subsequent stepping.\"Erroneous divisionThe issue came to light last week in a message, on Compuserve's \"Canopus\" forum, which was a reposting of a private e-mail communication from Lynchburg College (Lynchburg,Va.) mathematics professor Thomas Nicely. \"The Pentium floating-point unit is returning erroneous values for certain division operations,\" he wrote. \"For example, 1/824633702441 is calculated incorrectly (all digits beyond the eigth significant digit are in error). This can be verified...by computing (824633702441.0) X (1/824633702441.0), which should equal 1 exactly (within some exteremely small rounding error; in general, coprocessor results should contain 19 significant decimal digits). However, the Pentiums tested return 0.999999996274709702 for this calculation.\"\"The bug has been observed on all Pentiums I have tested or had tested to date, including a Dell P90, a Gateway P90, a Micron P60, an Insight P60 and a Packard-Bell P60. It has not been observed on any 486 or earlier system, even those with a PCI bus. If the floating-point unit is locked out (not always possible), the error disappears.\"Intel's Smith emphasized that the anomaly would not affect the average user. Speaking of Nicely, Smith said: \"He's the most exterme user. He spends round-the-clock time calculating reciprocals. What he observed after running this for months is an instance where we have eight decimal points correct, and the ninth not showing up correctly. So you get an error in the ninth decimal digit to the right of the mantissa. I think even if you're an engineer, you're not going to see this.\"Nicely said he pointed out the problem to Intel, because \"it has a major effect in mathematics, because we have to have absolute precision. I suspect that, to the majority of people, it will be irrelevant. But engineers may have a different outlook.\"A spot check conducted at EE Times last week tested out Nicely's expression on an AcerPower Minitower Pentium/60 machine, which was just received from Acer America. The result was 0.999999996247.Intel said it discovered the anomaly through its own random testing. The fix involved a mask change to the Pentium's floating-poing unit. Specifically, according to Intel's Smith, the correction entailed an update to the programmable-logic array (PLA) on the Pentium.\"This is related to the state machine in the floating-point unit. There are certain cases where, way out in the operation, we didn't handle the precision correctly,\" he said. \"It's an iterative calculation, and at each point, you have to go through a lookup process that says what your next approximation for the bits further out in the mantissa, [which equates to the digits] further to the right of the decimal point.\"Better resultsThe fix entailed adding terms, or additional gate-sequences, to the PLA. That corrected the erroneous results returned from the problematic lookup table accesses.Intel said there are no part-number designations or other markings on the updated microprocessors - which became available in the last few months - to differentiate them from the earlier anomalous parts. However, an Intel spokesman said, \"If customers are concerned, they can call and we'll replace\" any of the parts that contained the bug.CNN reported on the bug on the 22nd of November, and then on the 23rd of November in 1994, the New York Times ran an article titled: Flaw Undermines Accuracy of Pentium Chips, which read in part:The flaw appears in all Pentium chips now on the market, in certain types of division problems involving more than five significant digits, a mathematical term that can include numbers before and after a decimal point.Intel declined to say how many Pentium chips it made or sold, but Dataquest, a market research company in San Jose, Calif., estimated that in 1994 Intel would sell 5.5 million to 6 million Pentiums, roughly 10 percent of the number of personal computers sold worldwide.Intel said yesterday that it did not believe the chip needed to be recalled, asserting that the typical user would have but one chance in more than nine billion of encountering an inaccurate result as a consequence of the error, and thus there was no noticeable consequence to users of business or home computers. Indeed, the company said it was continuing to send computer makers Pentium chips built before the problem was detected.This was, most likely, the first widely publicized instance of a hardware error in a microprocessor. The publicity, however, didn’t get Intel to act. They would ship replacements only if a customer could prove to an Intel representative that he/she required accuracy beyond what his/her Pentium could provide. On the 12th of December, IBM announced that it was halting shipments of Pentium-based machines. While Intel had stated that an average user might encounter the bug once in 27,000 years, IBM found that a user would likely encounter this bug every few days. Intel quickly changed their plans announcing on the 19th that they’d replace all flawed Pentiums. This cost them $475 million (or roughly $1.018 billion in 2025 dollars).Ken Sherriff found the bug in silicon and provided an in-depth explanation for what exactly went wrong and how it got corrected if you’d like more detailed coverage of it.The year was far better for Intel in the supercomputer space with the i960 becoming the leading RISC chip in total number of chips sold. This little flake of silicon could be found in networking devices, laser printers, and the world’s most powerful supercomputer at the time built of two Intel Paragon XP/S 140s at Sandia National Laboratories.By the end of the year, Intel’s Tape Carrier Packaging (TCP) had made its way to market. Intel described TCP as offering reduced pitch, thin package profiles, and a small on-board footprint. They also stated: “Key package features include surface mount technology design, lead pitch of 0.25 mm, 48 mm tape format, polyimide-up for pick and place, and slide carrier handling. Shipped flat in slide carriers, the leads are designed to be formed into a ‘gull-wing’ configuration and reflowed onto the PCB by one of several methods.” What this meant was that the Pentium could be used in notebooks. The first of these was (as far as I could find) was the Toshiba T4900CT. This notebook featured an Intel Pentium clocked at 75MHz (3.3 volt, 16K cache), 10.4” SVGA TFT-LCD, 772MB HDD, 8MB of RAM (expandable to 40MB), 2x PCMCIA, VL-bus GPU, 3.5 inch 1.44MB floppy disk drive, and it ran Windows 3.11 for Workgroups. If you are aware of an early Pentium notebook, please let me know in the comments.Despite the FDIV bug and a healthy $3.55 billion spent on capital and R&D, Intel’s income for 1994 was $2.28 billion on revenues of $11.5 billion.With corrected Pentiums and the P54C out in the wild, 1995 was a great year for the company. With the launch of Windows 95 on the 24th of August, sales for the Pentium increased significantly with PC sales increasing by 25% globally to around 60 million. Microsoft’s success was lifting all boats in the PC markets, and it was doing so in a dramatic way. It’s support for the PCI bus didn’t hurt Intel either.Intel Pentium Pro die image, from IntelOn the 1st of November in 1995, Intel launched the Pentium Pro, and had Intel kept its original naming scheme, this would have been the i686. The original Pentium Pro was built of 5.5 million transistors on either a half micron or a 0.35 micron BiCMOS process. It shipped in a 387-pin CPGA (Socket 8), supported a clock speed of 150MHz to 200MHz, and could support single, dual, and quad processor configurations. While a 32bit CPU, the Pentium Pro had a 36bit address width and supported PAE which allowed it to address up to 64GB of RAM. It had an 8K L1 instruction cache, an 8K L1 data cache, and a 256K L2 on the low-end and up to 1MB L2 on the high-end. The L2 was on-package but not on-chip, and it had its own back-side bus. This design allowed the CPU to read memory and cache concurrently. TDP ranged from 29.2W to 44W depending upon the clock, bus speed (60MHz or 66MHz), and cache size.Intel Pentium Pro 200 in socket, image from computerbase.deThe Pentium Pro shared almost nothing with its predecessors beyond its name, manufacturer, and ISA. The lead designer was Fred Pollack who’d previously worked on the iAPX 432. Internally, the Pentium Pro was a superscalar, out of order, RISC chip with support for speculative execution and register renaming. On every cycle, up to 16bytes were fetched from the instruction cache and sent to decoders of which there were three. These decoders took x86 instructions and translated them into RISC micro-operations. Two of these were intended only for simple x86 instructions (simple defined here as instructions that are translated into a single RISC micro-operation in a single cycle) while the third could handle any instruction. The Pentium Pro heavily relied upon microcode and that microcode was upgradeable under either the BIOS or operating system control. Yet another deviation from earlier x86 chips was backward compatibility. Unlike earlier Intel x86 CPUs, the Pentium Pro, while capable of running 16bit code, didn’t emphasize this, and it was, indeed, slower than other x86 chips of that time at executing 16bit code. The cause of the slow down was that a read from a large register, say EAX, would stall until a write to a partial register, say AX, retired. This stall would then result in a pipeline flush. The Pentium Pro was meant to run fully 32bit software, and the ability to run software for earlier chips was meant to be a convenience. The Pentium Pro was specifically targetted at workstations and servers where one wouldn’t usually be running a standard word processor, spreadsheet, or video game. Those applications were the domain of the i386, i486, or original Pentium. Pentium Pro was a monster, image from Microprocessor Report, Vol9, Num15, 13th Nov 1995For those workstation and server users who wanted crazy performance more cheaply than a traditional UNIX on RISC system, the Pentium Pro delivered. When the chip shipped on the 2nd of November, pricing stood at $974 to $1325 (the top end SKU would ship slightly later at a price of $1989), and it was faster than all competitors at 32bit integer performance. While it lagged in floating point, the ability to combine multiple Pentiums Pro in a single system, allowed Intel to compete well despite that disadvantage. Truly, the Pentium Pro was remarkable. The slowest part, at 166MHz, delivered slightly more than double the integer performance of a 200MHz R4400 in SYSmark/NT. Obviously, when dealing with real-world applications, this could vary, but for many applications, the Pentium Pro did punch above its price. To put the pricing of full systems into perspective, Intergraph would launch the first quad-CPU Pentium Pro workstations in November of 1996. All of the machines in this lineup featured a graphics add-in card made by Intergraph (G76, G95, or Intense 3D). The cheapest was the TD-200 ranging from $1500 to $5000 depending upon the graphics card used. In that top of the line single-CPU model, the machine delivered roughly twice the graphical performance of an SGI O2 (according to Intergraph). Completing the line up from Intergraph were the TD-310, TD-410, and TD-610. These came with one, two, or four Pentiums Pro. These could be configured with up to 1GB of RAM, an Ultra SCSI hard disk, an ethernet controller, a Creative Labs Vibra 16c (integrated), up to ten expansion slots (PCI and ISA) in desk-side models, an 8x CD-ROM, a 3.5 inch floppy disk drive, dual-screen support, and Windows NT 4.0.Intergraph TD-310, image from intergraph manualThe Pentium Pro would not have anywhere near the market success of the Pentium, but that wasn’t its purpose. It was meant to break into the workstation and server markets; goals it accomplished. Additionally, the architecture of the Pentium Pro (P6) would server Intel quite well for more than a decade.Intel closed 1995 with $3.5 billion in income on $16.2 billion in revenues. Starting 1996 with the Pentium, Windows 95, and the explosion of the World Wide Web, Intel was benefiting from massive ecosystem growth. While the FDIV bug certainly got quite a bit of press, cost quite a bit of money, and was something Intel would surely rather have avoided, it didn’t hurt the company’s trajectory at all. Intel made it to number 5 on Fortune’s most admired companies list in 1996, and the company’s market value reached $111 billion. Intel closed 1996 with $5.157 billion in income on revenues of $20.847 billion, assets of $13.684 billion, and liabilities of $4.863 billion.I now have readers from many of the companies whose history I cover, and many of you were present for time periods I cover. A few of you are mentioned by name in my articles. All corrections to the record are welcome; feel free to leave a comment.Subscribe to Abort Retry FailBy Bradford Morgan WhiteIn Abort Retry Fail, I am attempting to chronicle the history of the computing industry. This is a goal that will likely not be completed within my lifetime.SubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.18 Likes∙1 Restack18Share this postAbort Retry FailIntel: a Bug and a ProCopy linkFacebookEmailNotesMore1Share",
    "summary": {
      "en": "The text discusses Intel's history, focusing on its significant achievements and challenges in the 1990s, particularly regarding the Pentium microprocessor. By 1994, Intel dominated the microprocessor market with about 75% share, thanks to the x86 architecture. The introduction of the Pentium chip marked a leap in performance, nearly doubling that of its predecessor, the i486, at a competitive price.\n\nHowever, Intel faced a major setback known as the FDIV bug, which affected the accuracy of floating-point division calculations in some Pentium processors. This bug was discovered by a mathematics professor, Thomas Nicely, and led to widespread media coverage and concerns about the chip's reliability. Initially downplaying the issue, Intel eventually agreed to replace faulty chips after significant backlash, costing the company around $475 million.\n\nDespite the bug, Intel's financial performance remained strong. Advances in technology and the launch of Windows 95 contributed to increased sales and the introduction of the Pentium Pro, which targeted workstations and servers. By the end of 1996, Intel's revenues and market value had grown significantly, solidifying its position in the computing industry. Overall, while the FDIV bug was a notable challenge, it did not derail Intel's growth trajectory.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "d68321f58287208e",
    "title": {
      "en": "Can Parrot Brains Teach Us About Human Speech?",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.smithsonianmag.com/smart-news/can-parrot-brains-teach-us-about-human-speech-study-finds-budgies-have-language-producing-regions-that-resemble-our-own-180986282/",
    "score": 27,
    "by": "ulrischa",
    "time": 1742580619,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "f887ef31119c7994",
    "title": {
      "en": "The Prospero Challenge",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.mattkeeter.com/projects/prospero/",
    "score": 89,
    "by": "jstanley",
    "time": 1742807083,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "983d11f977fb37a9",
    "title": {
      "en": "600M years of shared environmental stress response found in algae and plants",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://phys.org/news/2025-03-million-years-environmental-stress-response.html",
    "score": 91,
    "by": "PaulHoule",
    "time": 1742567418,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "ffd247af135b4fee",
    "title": {
      "en": "Teardown, Optimization: Comsol 8Gb USB Flash Stick (2015)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://goughlui.com/2015/04/05/teardown-optimization-comsol-8gb-usb-flash-stick-au6989sn-gt-sdtnrcama-008g/",
    "score": 18,
    "by": "userbinator",
    "time": 1742546665,
    "content": "← Unintentional Teardown: HP iPAQ h5550 Windows Mobile 2003 PDATrip to HK & CN 2014 – Part 6: Being a Tourist – Big Bus Tour (Part 1) →Teardown, Optimization: Comsol 8Gb USB Flash Stick (AU6989SN-GT & SDTNRCAMA-008G)\nPosted on April 5, 2015 by lui_goughA while back, I received a Comsol 8Gb USB Flash Stick for a test. As it turns out, I’ve managed to grab another, so I felt less bad about breaking one apart to work out what’s inside – and as it turns out, it provided me a world of entertainment for the weekend. It was more than I expected, and the optimization process is something engineers (like myself) really get excited about.Teardown and ExperimentsI’m sad to say that the teardown of this product is a relatively destructive process. The main PCB itself is held inside a thin aluminium “tube” by two glued-in translucent plastic end-pieces. Cutting the pieces and twisting them out with pliers was the only method to free the internal PCB, which revealed some interesting results.<img fetchpriority=\"high\" decoding=\"async\" class=\"alignnone size-large wp-image-12067\" src=\"https://goughlui.com/wp-content/uploads/2015/04/DSC_4056-1024x301.jpg\" alt=\"DSC_4056\" width=\"640\" height=\"188\" srcset=\"https://goughlui.com/wp-content/uploads/2015/04/DSC_4056-1024x301.jpg 1024w, https://goughlui.com/wp-content/uploads/2015/04/DSC_4056-300x88.jpg 300w\" sizes=\"(max-width: 640px) 100vw, 640px\" />The PCB is marked C20V-2.0-AU6989-L48-2L-TF-1.5 and is dated 26th June 2014, making this a relatively recent design. A provision is made for a front LED, which is unused, instead in preference of a rear LED. A provision is made for a crystal oscillator, but this is unpopulated because this uses an Alcor Micro AU6989SN-GT controller with integrated crystal.Features, excerpted from usbdev.ru, are:– PCBs are pin compatible with AU69XX USB2.0 series\n– Integrated build-in Regulator\n– Integrated build-in Crystal with Alcor’s patent\n– Supports 72bit/1K BCH ECC engines\n– Supports new generation MLC/TLC flash\n– Supports Toggle/ONFI DDR flash\n– Not support the flash ECC requirement under 24bit/1K\n– Well performance in TLC DDR flash\n– Improved read performance reach 32MB/Sec\n– Integrates hardware DMA engine to tune up the operation performance\n– Works with default driver under the environments of Windows ME, Windows 2000, Windows XP, Vista, Window7, Windows 8, Mac 9.2, Mac OS 10.x. Using Alcor Micro’s vendor driver for the environment under Windows 98SE.\n– Low power operation with SDR/DDR flash\n– Supports software write protection\n– Support Auto Run function\n– Support erasable and read-only mode AP Disk\n– Companion application program with UFD – iStar available for users\n– – To have UFD partition management function\n– – To do password protection for the security in data access\n– – To guard data files with software write protection function\n– – To lock up PC by UFD as the key\n– Available in 48-pin LQFP 7x7mm / TQFP_7x7mm / QFN_6x6mm / QFN_7x7mm package to support 4CE pin flashx2pcs\n– Available in 64-pin LQFP 7x7mm / TQFP_7x7mm package to support 4CE pin flashx4pcsIt’s a very feature rich controller, but that’s what you would expect given the competitors (e.g. Phison) which offer similar features in the value segment.<img decoding=\"async\" class=\"alignnone size-large wp-image-12068\" src=\"https://goughlui.com/wp-content/uploads/2015/04/DSC_4058-1024x301.jpg\" alt=\"DSC_4058\" width=\"640\" height=\"188\" srcset=\"https://goughlui.com/wp-content/uploads/2015/04/DSC_4058-1024x301.jpg 1024w, https://goughlui.com/wp-content/uploads/2015/04/DSC_4058-300x88.jpg 300w\" sizes=\"(max-width: 640px) 100vw, 640px\" />The rear offers a big surprise. The flash is a soldered down single microSDHC card! The card isn’t marked with a manufacturer but does have some etching on it. A space for a surface mount crystal is seen underneath as well.I know what you’re thinking – this is just a card reader and microSDHC combination, minus the connector, but you’d be wrong. The controller is a flash controller, and desoldering reveals an even more interesting outcome. Out comes the hot air gun!<img decoding=\"async\" class=\"alignnone size-large wp-image-12069\" src=\"https://goughlui.com/wp-content/uploads/2015/04/DSC_4060-1024x712.jpg\" alt=\"DSC_4060\" width=\"640\" height=\"445\" srcset=\"https://goughlui.com/wp-content/uploads/2015/04/DSC_4060-1024x712.jpg 1024w, https://goughlui.com/wp-content/uploads/2015/04/DSC_4060-300x209.jpg 300w\" sizes=\"(max-width: 640px) 100vw, 640px\" />Only the power connections seem to be used on the microSDHC’s pin, and instead, pads are used to talk to the card. A total of 21 pin connections are made, but I’m not sure what they are.Maybe playing with the microSDHC card would allow us to read the data from it, offering a simple way to recover the data from the card – turns out this doesn’t work. The card is identified by my reader as a 24/32Mb card and does not read properly. It appears unformatted. The read speed is 10.8Mb/s.<img decoding=\"async\" class=\"alignnone size-full wp-image-12081\" src=\"https://goughlui.com/wp-content/uploads/2015/04/comsol-card.png\" alt=\"comsol-card\" width=\"595\" height=\"520\" srcset=\"https://goughlui.com/wp-content/uploads/2015/04/comsol-card.png 595w, https://goughlui.com/wp-content/uploads/2015/04/comsol-card-300x262.png 300w\" sizes=\"(max-width: 595px) 100vw, 595px\" />I also decided to go one step further and try to extract the CID and CSD information:CID: 035344534430333280ffffffff0062c5\nCSD: 00260032515981e9bef9cfff92404053The CID and CSD provide some very unusual information. The CID gives the manufacturer ID as 0x03, which is Sandisk. The application ID of 0x5344 is also a common Sandisk trait. The Product name is SD032 with revision 8.0, which would suggest 32Mb (or Gb). Serial number seems to be unset (all oxFF’s), with date set as Feburary 2006. This is paired up with a version 1 CSD, with device size of 1958, equaling 32,096,256 bytes – so 32MiB.As a result, it looks like this is a Sandisk part, with an SD controller, but it hasn’t gone through the final manufacturer certification and formatting procedure. Instead, it is integrated in a product using the pads on the underside as a raw NAND package, ignoring the on-board SD controller, and thus takes the format of the controller talking to the raw NAND.How can we prove it’s a Sandisk part? Well, here’s some photos of a similar Sandisk part, as a microSDHC card, sold under the Sandisk brand.<img decoding=\"async\" class=\"alignnone size-medium wp-image-12066\" src=\"https://goughlui.com/wp-content/uploads/2015/04/DSC_4122-300x219.jpg\" alt=\"DSC_4122\" width=\"300\" height=\"219\" srcset=\"https://goughlui.com/wp-content/uploads/2015/04/DSC_4122-300x219.jpg 300w, https://goughlui.com/wp-content/uploads/2015/04/DSC_4122.jpg 846w\" sizes=\"(max-width: 300px) 100vw, 300px\" /> <img decoding=\"async\" class=\"alignnone size-medium wp-image-12070\" src=\"https://goughlui.com/wp-content/uploads/2015/04/DSC_4121-300x217.jpg\" alt=\"DSC_4121\" width=\"300\" height=\"217\" srcset=\"https://goughlui.com/wp-content/uploads/2015/04/DSC_4121-300x217.jpg 300w, https://goughlui.com/wp-content/uploads/2015/04/DSC_4121.jpg 857w\" sizes=\"(max-width: 300px) 100vw, 300px\" />Notice how the etched numbers match the same font-spacing style? Of course the branding hasn’t been printed and an extra etching has been made on the spine, which may be due to special binning (C grade?). The underside also features the pads, but covered by some thick paint of some sort. Now I know what those pads are for!Getting out the hot air gun again, I was able to resolder the package onto the board without damaging it – it still works just fine. So why was this exciting? It’s because, just like the Phison USB keys I had played with earlier prior to blogging, the manufacturer’s tools are available (albeit though shady channels) to the public which allows for some interesting insight into the manufacturing and optimization process.AlcorMP SoftwareThe software for certifying and manufacturing these USB keys is called AlcorMP. An archive of versions of the software is available from usbdev.ru. The software is capable of running in English and Chinese, but the user guide is in Chinese only, so I’ve pretty much done my exploration without much help from it.<img decoding=\"async\" class=\"alignnone size-large wp-image-12078\" src=\"https://goughlui.com/wp-content/uploads/2015/04/NewAlcorMP2-1024x618.gif\" alt=\"NewAlcorMP2\" width=\"640\" height=\"386\" srcset=\"https://goughlui.com/wp-content/uploads/2015/04/NewAlcorMP2-1024x618.gif 1024w, https://goughlui.com/wp-content/uploads/2015/04/NewAlcorMP2-300x181.gif 300w\" sizes=\"(max-width: 640px) 100vw, 640px\" />To work properly with this particular USB key, which uses a very late controller, you must use AlcorMP Version 15.03.05.00 or newer. I tried a Version 14 with no luck, due to the flash not being supported. By default, it will run with administrative rights so it can hook a special driver to talk to the keys and reprogram them. The program has a large set of accompanying .bin files, which seem to contain code to run on the controller itself, which makes this controller a possible safety issue, as noted for Phison controllers in the BadUSB exploit. The fact the controller can be so easily reprogrammed is a boon for those making fake flash keys.These files perform the low-level format, testing and provide the firmware for the controller as well as some special partition tools for those who wish to use the iStar features.This is like an engineer’s playground – a tool made by engineers for engineers. The main screen of the tool looks like this, and at the moment, it is performing a low-level format. Normally, inserting the key will have it show up its flash ID and other information, and would load “sensible” processing settings based on the processing that was done on the unit at the factory.Looking for the configuration details of the Comsol gives us the following data:<img decoding=\"async\" class=\"alignnone size-full wp-image-12073\" src=\"https://goughlui.com/wp-content/uploads/2015/04/ConfigInfo.gif\" alt=\"ConfigInfo\" width=\"343\" height=\"557\" />The drive is formatted with a fixed capacity of 7450Mb and has firmware version 1600. The VID and PID seem to be customized as well, but the vendor/product strings are empty, which explains the no-name nature of the drive in the HDTune Pro tests prior.To begin the certification process, you will need to click on the Setup button. This brings up a password prompt, of which no entry is required – just click OK.<img decoding=\"async\" class=\"alignnone size-full wp-image-12074\" src=\"https://goughlui.com/wp-content/uploads/2015/04/Flash-Config.gif\" alt=\"Flash-Config\" width=\"645\" height=\"439\" />The first screen is a relatively cluttered one but it sets up the flash configuration. The flash is auto-detected from the ID bytes as a Sandisk SDTNRCAMA-008G. Leaving the number, channels, and cycle time at defaults seems to be sufficient for this drive.The manufacturing process can be changed to optimize the drive for Speed, or for Capacity. There is a third option which seems to be for a high level format based on pre-existing bad block marked by the NAND manufacturer. You should really use either Capacity or Speed as the bad block data from the manufacturer is probably damaged.A low level format will test the flash and make sure the flash is usable. This is desirable if you have a fake flash drive to determine its correct capacity. The check mode is in LLF Check, with several levels of thoroughness. I chose Disturb Check because this seems to be more thorough, and checks for adjacent flash cell disturbance, whereas the other tests don’t address this (but can be sufficient). I also selected half-cap check after to ensure a thorough test. The scan level can have an impact on robustness, with Full Scans taking longer but ensuring all the flash is tested – Full Scan 4 is most thorough as far as I know. The ECC level can be set between 0 to 15 – this was one setting which caught my eye and I looked to understand and optimize it further in the next part.The special flash section is there to deal with particular types of flash with quirkiness/compatibility issues, and should be left at Normal unless you have particular errors during manufacturing.The advanced button brings up a new dialogue with more features –<img decoding=\"async\" class=\"alignnone size-full wp-image-12076\" src=\"https://goughlui.com/wp-content/uploads/2015/04/LLF-Enhance.gif\" alt=\"LLF-Enhance\" width=\"434\" height=\"381\" />The ECC enhance level feature is normally off, but can be turned on to improve stability of the result. Low level format revise can increase the scan time by running the low level format loop a few times to improve stability, but is generally unnecessary, as the remaining ECC should be able to handle any marginal blocks that may have passed.Pattern controls the test pattern used in low level formatting, and driving level configures the signal drive for the chips. The MaxL1fCE seems to do with flash chip enables, and should be left at the default setting. I’m not sure what LC Offset is used for. Sync Mode may be indicative of the flash interface being run in synchronous mode – but I’m not entirely sure.Strengthen the stability should be left to default – enabling this brings up a warning that capacity and speed will be sacrificed for stability, which is probably not necessary for normal usage.Use Block Mode controls which blocks are used. This can be changed to odd or even blocks for “salvaging” bad flash chips. Cache program can be enabled or disabled, but I’m not sure what this actually does – so I did try to see if it does anything in a later part.<img decoding=\"async\" class=\"alignnone size-full wp-image-12077\" src=\"https://goughlui.com/wp-content/uploads/2015/04/Mode-Select.gif\" alt=\"Mode-Select\" width=\"645\" height=\"439\" />The next screen across allows you to select the mode which the drive appears as – whether it’s a removable disk, fixed disk, read-only, password protected, or U3-style CD-ROM. You can pre-set some formatting parameters for the drives and the images to be pre-loaded. The LED behaviour can be customized as well. The use of the U3 style CD-ROM can be used to turn the drive into a USB CD-ROM drive for installing OSes which don’t understand USB installation (e.g. Windows XP), or for storing things read-only (without using the other read-only features of the drive)<img decoding=\"async\" class=\"alignnone size-full wp-image-12080\" src=\"https://goughlui.com/wp-content/uploads/2015/04/VID-Setup.gif\" alt=\"VID-Setup\" width=\"645\" height=\"439\" />The information tab allows you to set the VID, PID and strings – so you can customize the “name” of the drive in Device Manager. That can be a pretty good party trick.<img decoding=\"async\" class=\"alignnone size-full wp-image-12071\" src=\"https://goughlui.com/wp-content/uploads/2015/04/Bad-Block-Set.gif\" alt=\"Bad-Block-Set\" width=\"645\" height=\"439\" />The bad block configuration screen gives you the opportunity to set how the flash is configured. Auto check optimizes the drive size based on how much flash is actually workable. This can be potentially dangerous, as it may not leave any spare blocks for replacement should blocks fail during “runtime“. That being said, I’m not sure the Alcor Micro is capable of doing dynamic block replacements.Dynamic Set leaves some blocks for reserve, whereas bin allows the system to automatically optimize with one target or another, and decide which “regular” capacity to allocate a drive to based on the workable flash. This is interesting as it implies there could be very odd-sized flash keys out there – maybe a 4Gb key that has 6.8Gb of workable flash?<img decoding=\"async\" class=\"alignnone size-full wp-image-12072\" src=\"https://goughlui.com/wp-content/uploads/2015/04/Capacity-Bin.gif\" alt=\"Capacity-Bin\" width=\"686\" height=\"490\" />I’d have to say that the majority of manufacturers probably don’t use this mode, and use the fix capacity mode instead – either a drive passes or fails to provide a set capacity, and that’s the end of the day (7450Mb in case of this model). The final mode sets a fixed number of blocks as a percentage as “bad”.Other settings include the format file system (you don’t get a choice really), and options which help in real production usage (up to 32 simultaneous drives qualified using the same machine). There is one interesting ATTO Optimize feature which suggests there are a few tweaks to make the drive benchmark better. The drive can be formatted with MBR or as VFAT (which isn’t reliably bootable, but gives a tiny bit more space). The Enable Reader feature allows other chipsets with integrated SD readers to have the slot “usable”, and MaxMPTime allows for production to be limited to a certain amount of time or fail.<img decoding=\"async\" class=\"alignnone size-full wp-image-12075\" src=\"https://goughlui.com/wp-content/uploads/2015/04/Flash-Type-Power.gif\" alt=\"Flash-Type-Power\" width=\"645\" height=\"439\" />The other page allows you to customize flash, and do multiple loop burn-in tests. The power bits can be adjusted to make it more acceptable to the end user’s requirements. I have no idea what AutoH2 does, but the write log option provides a quick listing in a text file of the results of drive optimization.<img decoding=\"async\" class=\"alignnone size-full wp-image-12079\" src=\"https://goughlui.com/wp-content/uploads/2015/04/UI-Show.gif\" alt=\"UI-Show\" width=\"645\" height=\"439\" />The UI Show features are really only useful for those in production environments if they want to standardize on a particular look or colour coding – the home user can live with the default.Speed/Capacity, and ECC OptimizationThe controller itself advertises support for 72 bit/1K BCH ECC, and no less than 24 bit/1K ECC. The setting for ECC being from 0 to 16 was a little puzzling, so I tried to consult the manual with some digital translation help.The translated sections for ECC using Google Translate reads as follows:ECC set\nFLASH poor quality need to be open for FLASH bad block ECC error correction can improve certain capacity, but\nThere may be some risks. ECC = 0 most stringent low grid FLASH out the most stable; ECC = 15 most relaxed,\nCapacity may be larger, but there may be some risks.\nThe original low-grid setting value refers to a low-level format ECC on the selected use.ECC tuning levels: Level 1-4, may be appropriate to increase the capacity of FLASH, the proposed selection level 1.\nLow grid correction: low grid ECC scan times can make more accurate, but it takes a little more time, check only takes effect.\nScan times: You can manually set the number of low grid scanning, you can make a more accurate scan, but it takes a little more time,\nCheck only takes effect.\nPatten: Patten can choose different scans, mainly for the more special flash.\nUse Block Mode: manually choose to do the entire block or block or even-odd block.\nCache Program: Open or closed manually select cache program command.Using Bing Translator gives me a very similar result:ECC setting\nLow quality FLASH needs to open up FLASH bad block by ECC error correction, guaranteed capacity can be improved, but\nThere may be a certain amount of risk. ECC=0 is the most strict, low FLASH the most stability; ECC=15 is the most relaxed,\nCapacity may be larger, but there may be a certain degree of risk.\nLow setting refers to the use of a low-level format on the chosen ECC values.ECC tuning level: level 1-4, may be appropriate to improve the capacity of the FLASH, choose level 1.\nCorrection: low several times makes the ECC scanning is more accurate, but will spend more time, check the do not take effect.\nScan frequency: low the number of scans that can be manually set, can make the scan is more accurate, but will spend more time,\nCheck the do not take effect.\nPatten: you can select a different scan Patten, mainly for very special Flash.\nUse Block Mode: manually choose to do the full block or even block or odd block.\nCache Program: choose to turn on or turn off the cache manually program command.As a result, it seems that the ECC setting sets the tolerance to block errors during flash low level formatting. To verify this, I decided to run a low level format at every setting (taking an hour each setting) forboth capacity and speed optimize. ECC Tuning was disabled. Random pattern was used during testing (resulting in slightly random variances in formatted size), and Disturb test with Full Scan 4 was used. The results were as follows:<img decoding=\"async\" class=\"alignnone size-full wp-image-12082\" src=\"https://goughlui.com/wp-content/uploads/2015/04/devsize-vs-ecc.png\" alt=\"devsize-vs-ecc\" width=\"932\" height=\"736\" srcset=\"https://goughlui.com/wp-content/uploads/2015/04/devsize-vs-ecc.png 932w, https://goughlui.com/wp-content/uploads/2015/04/devsize-vs-ecc-300x237.png 300w\" sizes=\"(max-width: 932px) 100vw, 932px\" />The results seem to follow the description to some extent with some surprises. For one, the optimize for capacity option did not significantly outperform optimize for speed, and performed worse at low ECC level (strict). The capacity of most ECC levels above 3 were fairly similar, around 7800MB+, which is about 350-450Mb more than the fixed capacity it was shipped with. The number of bad blocks identified varies somewhat from ECC level to ECC level, probably due to random pattern test variation in detecting errors, coupled with some potential wear-out and alignment differences during test.In general, it seems like the ECC level represents the number of bad bits tolerated in a flash block/page during low level formatting before the block/page is marked bad and taken out of use. Hence, lower levels are stricter, and higher levels would allow for more defective bits in the low level format (leaving less margin for wear in the future, making it less stable).Because of the quality of the flash, the capacity is maximised even with fairly strict levels of ~3. Higher levels don’t seem to restore much more capacity, which implies that the bad blocks must contain bursts of bad cells which are uncorrectable even with wider bad-bits tolerance.It’s probably best to certify for a fixed size a bit smaller, if you want to give some room for reallocations (provided the controller actually supports it).So, what’s the cost of Capacity optimization versus Speed? Well, as it turns out, the speed is much better in speed mode – it’s about 84% faster in read and 44% faster in write than the shipped status. Part of the improvement seems to be new firmware – as the drive identifies with firmware version 8E8A using the AlcorMP tool.<img decoding=\"async\" class=\"alignnone wp-image-12085\" src=\"https://goughlui.com/wp-content/uploads/2015/04/Capacity-Optimized-CDM.png\" alt=\"Capacity Optimized CDM\" width=\"317\" height=\"287\" srcset=\"https://goughlui.com/wp-content/uploads/2015/04/Capacity-Optimized-CDM.png 406w, https://goughlui.com/wp-content/uploads/2015/04/Capacity-Optimized-CDM-300x272.png 300w\" sizes=\"(max-width: 317px) 100vw, 317px\" /> <img decoding=\"async\" class=\"alignnone wp-image-12086\" src=\"https://goughlui.com/wp-content/uploads/2015/04/Speed-Optimized-CDM.png\" alt=\"Speed Optimized CDM\" width=\"317\" height=\"287\" srcset=\"https://goughlui.com/wp-content/uploads/2015/04/Speed-Optimized-CDM.png 406w, https://goughlui.com/wp-content/uploads/2015/04/Speed-Optimized-CDM-300x272.png 300w\" sizes=\"(max-width: 317px) 100vw, 317px\" />No more nasty 13.78Mb/s read, and 3.27Mb/s write! It’s still no speed demon, but given the Sandisk card I had with the same sort of shape was a Class 4 card, the performance is probably the best we can expect from the flash. I did change the write-cycle time to the minimum setting, hoping to “push” the flash faster, but it made no difference.ATTO Optimization and Cache Program\nOne other option intrigued me, and that was optimization for ATTO, a commonly used dish benchmark. Was this some sort of cheating, or a deliberate bias towards small block accesses at the expense of sequential access? Was there any tradeoff?The other wonder was what the Cache Program option meant. Did this mean there would be an pSLC cache on the drive or some sort of attempt at optimizing for small transfers? Or was it just an option to cache production firmware on the drives themselves? Did it have any performance impact?<img decoding=\"async\" class=\"alignnone size-full wp-image-12088\" src=\"https://goughlui.com/wp-content/uploads/2015/04/Capacity-Optimized-ATTO-No-Opt.png\" alt=\"Capacity Optimized ATTO No Opt\" width=\"482\" height=\"615\" srcset=\"https://goughlui.com/wp-content/uploads/2015/04/Capacity-Optimized-ATTO-No-Opt.png 482w, https://goughlui.com/wp-content/uploads/2015/04/Capacity-Optimized-ATTO-No-Opt-235x300.png 235w\" sizes=\"(max-width: 482px) 100vw, 482px\" />Capacity Optimized, No ATTO Optimization<img decoding=\"async\" class=\"alignnone size-full wp-image-12089\" src=\"https://goughlui.com/wp-content/uploads/2015/04/Speed-Optimized-ATTO-No-Opt.png\" alt=\"Speed Optimized ATTO No Opt\" width=\"482\" height=\"615\" srcset=\"https://goughlui.com/wp-content/uploads/2015/04/Speed-Optimized-ATTO-No-Opt.png 482w, https://goughlui.com/wp-content/uploads/2015/04/Speed-Optimized-ATTO-No-Opt-235x300.png 235w\" sizes=\"(max-width: 482px) 100vw, 482px\" />Speed Optimized, No ATTO OptimizationOn the whole, with no ATTO optimization, aside from slight variations, it seems that the optimize for performance selection provides better performance across the board.<img decoding=\"async\" class=\"alignnone size-full wp-image-12090\" src=\"https://goughlui.com/wp-content/uploads/2015/04/Speed-ATTO-Optimized-ATTO.png\" alt=\"Speed+ATTO Optimized ATTO\" width=\"482\" height=\"615\" srcset=\"https://goughlui.com/wp-content/uploads/2015/04/Speed-ATTO-Optimized-ATTO.png 482w, https://goughlui.com/wp-content/uploads/2015/04/Speed-ATTO-Optimized-ATTO-235x300.png 235w\" sizes=\"(max-width: 482px) 100vw, 482px\" />Speed Optimized, ATTO Optimization ONTurning on ATTO Optimization doesn’t seem to do much at all. There’s a few slight increases across the board, but I suppose this flash chip isn’t particularly capable and neither is the single channel design.<img decoding=\"async\" class=\"alignnone size-full wp-image-12091\" src=\"https://goughlui.com/wp-content/uploads/2015/04/Speed-ATTO-Cache-ATTO.png\" alt=\"Speed+ATTO+Cache ATTO\" width=\"482\" height=\"615\" srcset=\"https://goughlui.com/wp-content/uploads/2015/04/Speed-ATTO-Cache-ATTO.png 482w, https://goughlui.com/wp-content/uploads/2015/04/Speed-ATTO-Cache-ATTO-235x300.png 235w\" sizes=\"(max-width: 482px) 100vw, 482px\" />Speed Optimized, ATTO Optimization ON, Cache ONTurning on cache didn’t make a lick of difference either, so I might as well leave that at default. It wasn’t the option I hoped it was.I also used H2testw and CrystalDiskMark as benchmarks to find out just how far we’ve improved from the baseline case of “as shipped” and the impact of each of the options. It seems that optimize for speed gives a good boost overall, and ATTO optimization provides a slight advantage. Cache didn’t have much of a notable impact, except for a strange reduction in 512kB writes in CDM and a slight increase in H2testw writes.My finalized settings including ECC Enhance Level 1, with LLF Revise and Dynamic Set (4) was tested as well on the rightmost column, as my “final” optimized result. It doesn’t seem that these options, which affect the LLF process, actually affects the final drive performance beyond that of normal “test to test variation”. Not bad for a “cost free” solution.<img decoding=\"async\" class=\"alignnone size-full wp-image-12095\" src=\"https://goughlui.com/wp-content/uploads/2015/04/Performance-Summary.png\" alt=\"Performance Summary\" width=\"733\" height=\"223\" srcset=\"https://goughlui.com/wp-content/uploads/2015/04/Performance-Summary.png 733w, https://goughlui.com/wp-content/uploads/2015/04/Performance-Summary-300x91.png 300w\" sizes=\"(max-width: 733px) 100vw, 733px\" />Of note was that the drive was stable enough to pass H2testw in the fresh state even at varying format capacities depending on the run. This doesn’t indicate the drive will be stable in the future, as the flash wears, hence my recommendation to use stricter ECC (than the default 8 used by this manufacturer) and dynamic set to maximise the storage and improve the “safety” margin.I even tried being very negligent – doing a manufacture run at ECC 15, with ECC Enhance at 4 (loose), speed optimize, with half-capacity scan, quick-scan 1 selected, and auto size for bad blocks and the resulting unit still passed the H2testw test, however, might be unstable as flash cells wear out. It didn’t yield me any significant capacity gain, and that’s likely due to burst-error accumulation in manufactured flash.ConclusionThe quest for optimization seems to be something in most engineer’s hearts. It’s one reason why I do love overclocking. Playing with a manufacturer’s tool is like overclocking a USB key – performance and capacity was both improved at no cost! It turns a nasty key into one that more closely resembles its advertised “up to 20MB/s” and makes it tolerable for use. Of course, it may not be as stable as otherwise, depending on the options you choose – but now you’re in the driver’s seat and that’s pretty cool.Of course, none of this is without risk, and it’s equally possible to ruin a good drive, brick it or otherwise make the performance or stability worse. You can and will void warranties as well – do all of this at your own risk.In the end, I opted for ECC level 4, plus ECC Enhance Level 1 and LLF Revise ON, cache default and chose to go with Dynamic Set (4) for bad blocks to give some margin for reallocation. That should give me some more capacity, with the stability you would expect.Like it? Share it!FacebookTwitterRedditLinkedInMoreTumblrPocketPinterestWhatsAppTelegramRelatedReview, Teardown: Verbatim 8Gb Store’n’Go PinStripe USB 2.0 Drive (49062)Yet another low-cost USB flash drive gets a review, mainly because I just happened to end up receiving it. Unlike the others though, this one comes from Verbatim which is a brand which has come to be respected due to their reputation built around floppy disks and optical discs. The…December 4, 2016In \"Computing\"Quick Review: Comsol 8Gb USB 2.0 Flash Stick (UF4-8000)USB flash drives are one of the most ubiquitous storage devices, this one was anonymously donated to me for some testing. It's not a particularly special looking drive, but it is one that's available on the market today - it was worth a quick test anyway. The Comsol branding may…February 20, 2015In \"Flash Memory\"Unintentional Teardown & Repair: Kingston 128Gb UHS-I SDXC CardA while back, I posted a gripe article about how I disliked some aspects of the Secure Digital memory card format, one of the issues being the fragility of the plastic shell. Through no fault of my own, it seems it has happened to one of my largest cards. Damn.…May 23, 2015In \"Flash Memory\"Review, Teardown: Corsair Flash Voyager LS 32Gb USB 3.0 Flash DriveCorsair has been a trusted name in DRAM memory for a long time, which resulted in a brand expansion into peripherals, accessories, power supplies and more. It seems like whatever Corsair puts their brand on is a product you can trust. I've always been partial towards Corsair, having owned many…February 28, 2015In \"Computing\"Review, Teardown: Sandisk Cruzer Facet 8Gb USB 2.0 Flash DriveReaders familiar with my penchant for exotic storage would probably stare at me in confusion as to why I would be reviewing such a plain product such as this one? Interestingly enough, as with most things that I do, there is a good reason. The Sandisk Cruzer Facet 8Gb is…January 15, 2016In \"Computing\"Experiment: 8Gb USB Flash Drive Endurance TestFlash memory is, and has been, a commodity item for a while now. Almost everyone has at least a few USB flash drives (sticks) and maybe even a few memory cards. When it comes to rapidly transferring large files between devices, or even storing working documents, USB flash drives are…May 30, 2017In \"Computing\"Review & Teardown: Kogan USB 3.0 Universal Memory Card ReaderLast week, Kogan had a free shipping day, and not being one to let such opportunities slip by, I decided to pick up a Kogan USB 3.0 Universal Memory Card Reader for AU$19. It's a fair price to pay for such a product, similarly priced to my favourite Transcend RDF8.…February 17, 2014In \"Computing\"Teardown, Test: PGH Bricks & Pavers Promotional USB KeyI guess the fact that this post exists is a proof that promotional USB keys \"work\". They serve a useful purpose, and have a decent \"lifetime\", thus helping spread the promotional message. I'm no brick and pavers guy, but I managed to have one of these gifted to me, so…June 30, 2014In \"Flash Memory\"Quick Review, Teardown: “Avoid driving fast card” USB 3.0 Ethernet AdapterIt's always nice to have a few extra Ethernet interfaces on a computer - sometimes you need to run a software firewall, use a computer as a router, or you just need to sniff some network traffic running to a device for some reverse engineering or testing purposes. With the…March 17, 2016In \"Computing\"\n<img alt='' src='https://secure.gravatar.com/avatar/0843025630d91cd79ecc59aa067ac7dd?s=60&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/0843025630d91cd79ecc59aa067ac7dd?s=120&#038;d=mm&#038;r=g 2x' class='avatar avatar-60 photo' height='60' width='60' decoding='async'/>\nAbout lui_gough\nI'm a bit of a nut for electronics, computing, photography, radio, satellite and other technical hobbies. Click for more about me!\n\nView all posts by lui_gough →\nThis entry was posted in Computing, Flash Memory and tagged experiment, flash, flash memory, made-in-china, storage, teardown. Bookmark the permalink.← Unintentional Teardown: HP iPAQ h5550 Windows Mobile 2003 PDATrip to HK & CN 2014 – Part 6: Being a Tourist – Big Bus Tour (Part 1) →\n6 Responses to Teardown, Optimization: Comsol 8Gb USB Flash Stick (AU6989SN-GT & SDTNRCAMA-008G)\n<img alt='' src='https://secure.gravatar.com/avatar/0843025630d91cd79ecc59aa067ac7dd?s=40&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/0843025630d91cd79ecc59aa067ac7dd?s=80&#038;d=mm&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' decoding='async'/>\t\t\t\tlui_gough says:\nApril 11, 2015 at 12:03 am\t\t\t\t\tAfter some playing around, it seems AlcorMP tool is not without some drawbacks. One is that it installs a filter driver into your USB stack named mpfilt and on 64-bit Windows machines after the latest Windows update, you will find that it “breaks” causing all of the USB 2.0 controllers on your machine to fail with an exclamation mark and something about “associated driver failed to start.”To fix this, you will need to remove the mpfilt driver from your LowerFilters. Unless you have a USB 3.0 controller that still works without the driver (where plugging in a keyboard and mouse into those ports lets you take control of the system) or a remote VNC connection, you will have to boot into safe mode first. Get into regedit and do a search for all entries with mpfilt. Whenever you see one that has a name LowerFilters with Data “mpfilt”, delete the LowerFilters entry. DO NOT DELETE ANYTHING ELSE.Reboot your machine and you should be back to normal.\nReply\n<img alt='' src='https://secure.gravatar.com/avatar/f493cc56b8fcaa4eb141bb204b29e0fe?s=40&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/f493cc56b8fcaa4eb141bb204b29e0fe?s=80&#038;d=mm&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' decoding='async'/>\t\t\t\t- dutchman24 - says:\nAugust 1, 2016 at 6:30 pm\t\t\t\t\tThank you I was looking arround the internet to have some detailed information about this program.\nIf you have problems  (like i had) with the USB drivers because of the HUB configuration message you can still operate your pc with a ps2 (purple plug) keyboard. I menaged to restart Windows 10 with shift key pressed down and do a systeme restore and everything went back to normal.\nIts very important to start the AlcorMP program before to insert any USB devices and use USB 2.0 socket and not the 3.0 sockets (blue ones).\nBTW i think that the “Cache” option in the Advanced TAB is to configurate the USB penn to take it out of the PC without deconnecting it. If Cache is disabled it works “stand alone without putting cache into the PC.  If enabled the penn is putting cache in the PC to work faster but you have to deconnect the hardware before you can pull it out of your computer.\nRegards\nReply\n<img alt='' src='https://secure.gravatar.com/avatar/9495d9b367290ba6a36dfacf72276119?s=40&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/9495d9b367290ba6a36dfacf72276119?s=80&#038;d=mm&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' decoding='async'/>\t\t\t\ttc says:\nJuly 18, 2020 at 6:04 pm\t\t\t\t\tOr just use VMware, why would you run this kinda dodgy app on bare metal\nReply\n<img alt='' src='https://secure.gravatar.com/avatar/0843025630d91cd79ecc59aa067ac7dd?s=40&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/0843025630d91cd79ecc59aa067ac7dd?s=80&#038;d=mm&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' decoding='async'/>\t\t\t\tlui_gough says:\nJuly 18, 2020 at 7:48 pm\t\t\t\t\tThere’s a good reason for this. The reconfiguration of the flash controller relies on reprogramming the firmware of the controller itself, which often results in the device resetting and reconnecting to the bus with a new ID to perform various flash verification tasks as a sort of “warm reset”. Many manufacturing tools are very timing sensitive and will throw errors midway after loading the diagnostic code and resetting the controller since it doesn’t “reconnect” to the VM in time or correctly. Sometimes the USB pass-through drivers issue spurious resets to the USB devices which may cause them to lose this diagnostic code (often loaded in RAM temporarily) and reboot as a regular USB stick using the firmware in flash, resulting in no go either.– Gough\nReply\n<img alt='' src='https://graph.facebook.com/v6.0/1950211318481950/picture?type=large' srcset='https://secure.gravatar.com/avatar/d6dc757340a26c503ec3d4bac8c02445?s=80&#038;d=mm&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' decoding='async'/>\t\t\t\tハッピー アニメ マーケット says:\nFebruary 13, 2022 at 3:57 pm\t\t\t\t\tDo you think I can use this to recover my bad sandisk microSD card? the pins seems to match but if not, I can solder from my card to this board.\nReply\n<img alt='' src='https://secure.gravatar.com/avatar/0843025630d91cd79ecc59aa067ac7dd?s=40&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/0843025630d91cd79ecc59aa067ac7dd?s=80&#038;d=mm&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' decoding='async'/>\t\t\t\tlui_gough says:\nFebruary 18, 2022 at 5:52 am\t\t\t\t\tNo. It uses the card as raw flash memory and the controller has its own format which is likely to be incompatible with whatever SanDisk’s own card has as it bypasses it entirely. Without compatible metadata, the USB Flash memory controller would not allow access anyway.– Gough\nReplyError: Comment is Missing!Cancel reply\n\n document.addEventListener('DOMContentLoaded',function(){var commentForms=document.getElementsByClassName('jetpack_remote_comment');for(var i=0;i<commentForms.length;i++){commentForms[i].allowTransparency=!1;commentForms[i].scrolling='no'}})",
    "summary": {
      "en": "The text discusses the teardown and optimization of a Comsol 8Gb USB flash drive. The author, an engineer, describes the process of dismantling the device to explore its components, particularly the main printed circuit board (PCB) and the flash memory. \n\nKey points include:\n\n1. **Teardown Process**: The USB flash drive is encased in a metal tube, which is tricky to open without damaging it. The PCB revealed interesting features, including an unused LED and a built-in controller.\n\n2. **Flash Memory Insights**: The flash memory inside the drive is essentially a microSD card soldered directly to the PCB. Upon inspection, it was found to be a Sandisk part, but not properly formatted or certified for use.\n\n3. **Optimization Tools**: The author used a software called AlcorMP to reprogram the flash drive. This software allows for low-level formatting and optimization of the drive's performance and capacity.\n\n4. **Results of Optimization**: Through various settings adjustments, the author improved the drive’s speed and capacity significantly. It was noted that optimizing for speed yielded better performance compared to capacity.\n\n5. **Risks of Optimization**: There are risks involved in this process, including the potential to ruin the drive or void warranties. The author emphasizes the importance of being cautious and aware of these risks.\n\nOverall, the experience showcases the excitement of engineering and the potential to enhance the performance of everyday technology through hands-on experimentation.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "2159168a750d76f2",
    "title": {
      "en": "Autology: A Lisp with access to its own interpreter",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/Kimbsy/autology",
    "score": 128,
    "by": "simonpure",
    "time": 1742553436,
    "content": "Autology\nA hot take on Lisp metaprogramming, Autology is a Lisp with access\nto its own interpreter.\nWhat is it?\nAutology is a functional interpreted Lisp language, written in Clojure.\nThe Autology interpreter function is defined as a data structure which is available to the Autology program, it is bound to the variable *i*.\nWhenever an expression needs to be evaluated, Autology will retrieve the current value of *i* from the current lexically scoped execution environment and use it to evaluate the expression.\nBy binding new values of *i* we can modify the behaviour of the Autology language while our program is running.\nWhy have you done this?\nMainly for fun and to explore a neat idea.\nAutology is not a particularly useful language as-is, its very slow and resource intensive.\nMacros are the normal tool for Lisp languages to modify their syntax or behaviour, but these are normally only available at compile time. Autology allows dynamic rewriting of the language at runtime.\nEven other non-Lisp languages are able to define some form of Domain Specific Language to allow programmers to express problems more clearly, however these are generally quite restrictive in scope. Autology allows full ad-hoc re-writing of the syntax of the language, as well as defining new features or removing existing ones.\nWhat can you do with it?\nWell to start with you might like to add functions to the language. Autology doesn't have them by default, but we can rebind the *i* symbol to a data structure representing a new interpreter function, one that does have functions.\n(bind (;; grab a copy of the `:atl/eval-list` section of the\n       ;; interpreter which is responsible for evaluating lists.\n       original (get-marker *i* :atl/eval-list)\n\n       ;; define a case test+body for use when the list expression\n       ;; starts with our function special form, in this case `λ`.\n       λ-form (qu (λ (let [[_λ params body] e]\n                       (fn [& values]\n                         (autology.core/evaluate\n                          body\n                          (reduce (fn [acc-env [s v]]\n                                    (assoc acc-env s v))\n                                  env\n                                  (zipmap params values)))))))\n\n       ;; rebind `*i*` to be a new interpreter with the\n       ;; `:atl/eval-list` section replaced with a version that\n       ;; includes our lambda handling special form.\n       *i* (replace-marker *i* :atl/eval-list\n                           (list :atl/eval-list\n                                 (concat (butlast original)\n                                         λ-form\n                                         (list (last original)))))\n\n       ;; We can now immediately define functions since the\n       ;; interpreter will have already been updated to evaluate the\n       ;; remaining bindings like this one.\n       double (λ (n)\n                 (+ n n)))\n\n      ;; Finally we can invoke our new function!\n      (double (double (double (double (double 1.3125))))))\n\nAfter that, pretty much anything you want!\n\nWhy not switch form applicative order evaluation to normal order evaluation?\nMaybe modify the language so it uses a continuation passing style?\nDefine a metacircular Lisp inside your program which has access to it's own interpreter as well at the Autology interpreter.\n\nRun tests\nclojure -X:test",
    "summary": {
      "en": "**Summary of Autology**\n\nAutology is a unique Lisp programming language that allows access to its own interpreter, making it possible to change its behavior while the program runs. It is written in Clojure and is designed as a fun exploration of metaprogramming ideas.\n\n### Key Points:\n- **What is Autology?** It's a functional Lisp language that uses a special interpreter function stored in a variable called *i*. This allows dynamic evaluation of expressions by changing *i*'s value during runtime.\n  \n- **Purpose:** Autology was created mainly for enjoyment and experimentation. While it’s not practical for serious use due to its slow performance, it offers features that typical Lisp macros do not, allowing for flexible, real-time changes to the language’s syntax and behavior.\n\n- **Capabilities:** Users can enhance Autology by adding new functions or modifying existing features. For example, one can define a lambda function to create new functions dynamically.\n\n- **Examples of Modifications:** Users can change how functions are evaluated, switch to different evaluation styles, or even create a self-referential version of Lisp within Autology.\n\n- **Testing:** Autology can be tested with the command `clojure -X:test`. \n\nOverall, Autology is an experimental tool that showcases advanced programming concepts in a playful way.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "1e53cc181f8923ea",
    "title": {
      "en": "Peano's Axioms",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://principlesofcryptography.com/number-theory-primer-an-axiomatic-study-of-natural-numbers-peano-axioms/",
    "score": 86,
    "by": "ulugh",
    "time": 1742844365,
    "content": "Categories\n\n\t\t\t\t\tmathematics Number Theory\n\n\t\t\tNumber Theory Primer : An Axiomatic Study Of Natural Numbers – Peano’s Axioms\n\n\t\t\t\t\t\t\t\tPost author\n\n\t\t\t\t\t\t\tBy premmi\n\n\t\t\t\t\t\t\t\tPost date\n\n\t\t\t\t\t\t\tFebruary 1, 2025\n\nauthored by Premmi andBeguène\n\nPrevious Topic: An Axiomatic Study of Numbers\n\nIntroduction\n\nThinking of numbers intuitively brings to mind the simplest and most fundamental set of numbers, namely the set of natural numbers. These numbers are used to count objects like cars, books, pens, etc. If we associate natural numbers such as 1,2,3,1, 2, 3,1,2,3, etc. with counting, then with what corresponding concepts do we relate numbers like −4,3and227-4, \\sqrt{3} \\text{ and } \\frac{22}{7}−4,3and722?\n\nTo reason about all kinds of numbers encountered during our study of mathematics, we need a precise mathematical framework for defining numbers. We will build this framework by first rigorously defining natural numbers axiomatically without relying on the intuitive notion of counting. Then, using this framework as a foundation, we will construct all the other sets of numbers such as integers, rational numbers, real numbers and complex numbers in terms of the natural numbers.\n\nA good axiomatic system assumes as little as possible, while proving as much as possible. To create an efficient axiomatization for the natural numbers, we must distill these numbers to their essential properties. Intuitively, we understand various aspects of the natural numbers, such as their existence and the basic properties of the binary operations addition, multiplication, and the ‘less than’ relation. How few of these concepts can we take as axioms, from which we can deduce everything else that we need to know about the natural numbers? It turns out that remarkably little is required for the axiomatization of the natural numbers—neither addition, nor multiplication, nor the ‘less than’ relation need to be taken as axioms; these will all be constructed from our fundamental axioms.\n\nThe standard axiomatization of the natural numbers, know as the Peano Axioms, was originally formulated by the Italian mathematician, Giuseppe Peano. In 188918891889, Peano developed the real number system based on his axioms for the natural numbers. He defined natural numbers through nine axioms, four of which established the properties of the equality relation “===” with regard to natural numbers, while the remaining five axioms provided a complete and rigorous definition of natural numbers.\n\nIn order to appreciate the intellectual feat of Peano, it’s worth noting that utilizing only his axioms, we are able to prove all the established properties of the natural numbers. Furthermore, these axioms facilitate the construction of the integer, rational, real, and complex number systems.\n\nBefore we discuss Peano’s Axioms in detail, it is a useful exercise to explore an alternative way to describe natural numbers, distinct from the usual intuitive notion of counting. Such an exploration would help us to independently arrive at Peano’s axioms.\n\nIntuition behind the Axiomatic Definition of Natural Numbers\n\nHow do we model the concept of natural numbers denoted by 0,1,2,30, 1, 2, 3 0,1,2,3 and so on without relying on the notion of counting?\n\nOne way would be to take a set-theoretic approach. That is, we can define a set of natural numbers by systematically enumerating the properties and relationships of the members of this set such that the set results in {0,1,2,3,…}\\{0, 1, 2, 3, \\ldots\\}{0,1,2,3,…}.\n\nLet us denote the set of natural numbers as N\\mathcal{N}N. First, we recognize that 000 should be a part of this set. Next, we want 000 to lead us to 111, 111 to 222 and we should be able to continue this way, naming each successive number as far as we wish. This is illustrated below.\n\n0→1→2→3→…0 \\rightarrow1 \\rightarrow 2 \\rightarrow 3 \\rightarrow \\ldots0→1→2→3→…\n\nFrom the above diagram we can see that to model this relationship we need a “next” operation that given a natural number, produces the next natural number in the sequence.\n\nThe above diagram can also be viewed as shown below.\n\n0→11→22→3⋮    ⋮\\begin{equation*}\n\\begin{split}\n0 &\\rightarrow 1 \\\\\n1 &\\rightarrow 2 \\\\\n2 &\\rightarrow 3 \\\\\n\\vdots &\\quad\\,\\,\\,\\, \\vdots \\\\\n\\end{split}\n\\end{equation*} 012⋮→1→2→3⋮\n\nWe can see from the diagram that an input of 000 yields an output of 111, an input of 111 yields 222 and so on. Therefore, we can model the “next” operation as a function S:N→NS : \\mathcal{N} \\rightarrow \\mathcal{N}S:N→N that takes a natural number as input and produces a natural number as output. Here, the letter SSS stands for ‘successor’ and we have S(0)=1,S(1)=2S(0) = 1, S(1) = 2S(0)=1,S(1)=2 and so forth. We will refer to this function SSS as the successor function since it establishes a succession within the set of natural numbers, N\\mathcal{N}N. This function SSS is illustrated below.\n\nFrom the diagram above, we can observe that the successor function SSS has the following properties:\n\nNot Surjective: There is no natural number in N\\mathcal{N}N that, when given as input to the successor function SSS, results in 000 as output. This implies that not every element of the codomain of SSS is the image of at least one element from its domain. Therefore, we can conclude that SSS is not surjective because S(n)≠0foranyn∈NS(n) \\neq 0 \\text{ for any } n \\in \\mathcal{N}S(n)=0foranyn∈N.\n\nInjective: Different inputs to SSS yield different outputs. This means that every element of the codomain of SSS is the image of at most one element from its domain; that is, S(m)=S(n)impliesthatm=nforanyn,m∈NS(m) = S(n) \\text{ implies that } m = n \\text{ for any } n, m \\in \\mathcal{N}S(m)=S(n)impliesthatm=nforanyn,m∈N. Hence, SSS is injective.\n\nFrom the diagram, we can also see that a natural number is either 000 or can be obtained from 000 by applying the successor function to 000 a finite number of times. This implies that N\\mathcal{N}N is the minimal non-empty set that contains 000 and admits a successor function satisfying conditions (1)and(2)(1) \\text{ and } (2)(1)and(2).\n\nTherefore, for the Peano Axioms to accurately describe the set of natural numbers, they must define a set that contains 000 and admits a successor function as described above.\n\nOriginal Formulation of Peano’s Axioms\n\nBefore we discuss the modern version of Peano’s Axioms, it is interesting to know how these axioms were originally stated by Giuseppe Peano. While perusing Peano’s Axioms it is worth keeping in mind that during Peano’s time, the concept of set was still nascent; it was Peano who introduced the symbol ∈\\in∈ in 188918891889 to denote “is an element of”. Being aware of Peano’s original formulation of these axioms helps us appreciate how far we have come in our mathematical journey and highlights that our journey towards better mathematical notation and abstraction still continues.\n\nThe axioms stated below appear between the pages 387−408387-408387−408 in the book “Historia Mathematica 1\\mathit{1}1” published in the year 197419741974.\n\nIn 189118911891, two years after the publication of his axioms for the natural numbers, Giuseppe Peano published an article titled “Sul concetto di numero” which translates to “On the Concept of Number,” in a journal he founded that same year. In this article, he reduced his list of axioms to five by eliminating the four axioms related to the equality relation “= ⁣ ⁣\"=\\!\\!\"=\".\n\nThe following are the five Peano’s axioms stated in the article :\n\nWe can see how the archaic notation obfuscates these axioms.\n\nThese maybe interpreted as :\n\nIn the Peano Axioms published in 1889and18911889 \\text{ and } 18911889and1891, the sequence of natural numbers began with 111, and the set of natural numbers was denoted by N\\mathit{N}N. However, in 189818981898 these axioms were modified so that the sequence began with 000 and the set was denoted by N ⁣0\\mathit{N_{\\!0}}N0.\n\nThe set of five Peano Axioms was increased to six in 190119011901 with the addition of the axiom – N ⁣0∈Cls\\mathit{N_{\\!0}} \\in \\text{Cls}N0∈Cls, i.e., the natural numbers form a class. With the addition of this last axiom, the axioms have received their final form, which are listed below.\n\nThe Axiomatization of Natural Numbers\n\nWe will first define the notion of equality as it pertains to natural numbers, and then we will formulate the Peano Axioms such that it provides an axiomatic definition of natural numbers.\n\nAlthough Giuseppe Peano omitted the four axioms related to the equality relation from his later formulation of the axioms regarding natural numbers, we will still discuss these axioms for the sake of completeness. The likely reason for this omission is because the concept of equality extends beyond natural numbers to mathematical objects in general and pertains more to the realm of logic, which specifies the conditions under which two mathematical objects can be considered equal.\n\nIn our discussion of Peano’s Axioms, we will adopt some conventions from its final form. Therefore, we will start with 000 instead of 111 in our axiomatic system and denote the set of natural numbers by N0\\mathbb{N_0}N0, where the subscript 000 reminds us that 000 is included.\n\nThe Notion of Equality\n\nBefore we define the set of natural numbers N0\\mathbb{N_0}N0 axiomatically, we will formalize the notion of equality through the four axioms of Peano, which establish the properties that the equality relation, denoted by ===, must satisfy.\n\nSuppose there exists a set N0\\mathbb{N_0}N0 that satisfies Axioms 111 t0 999 listed below.\n\nFirstly, every natural number should be equal to itself; this is called the reflexivity axiom.\n\nAxiom 1\\mathbf{1}1. For every x∈N0,x=xx \\in \\mathbb{N_0}, x = xx∈N0,x=x.\n\nSecondly, if one natural number equals a second one, then the second one should equal the first one. This is known as the symmetry axiom.\n\nAxiom 2\\mathbf{2}2. For every x,y∈N0,ifx=y,theny=xx, y \\in \\mathbb{N_0}, \\text{ if } x = y, \\text{ then } y = xx,y∈N0,ifx=y,theny=x.\n\nThirdly, if one natural number is equal to a second, and that second natural number is equal to a third, then the first and third are equal to each other. This is called the transitivity axiom.\n\nAxiom 3\\mathbf{3}3. For every x,y,z∈N0,ifx=yandy=z,thenx=zx, y, z \\in \\mathbb{N_0}, \\text{ if } x = y \\text{ and } y = z, \\text{ then } x = zx,y,z∈N0,ifx=yandy=z,thenx=z.\n\nThese three properties of reflexivity, symmetry and transitivity are applicable to any two mathematical objects that are related by the equality. As we have already discussed during our study of Set Theory, equality is an example of an equivalence relation, which is a type of homogeneous binary relation that satisfies the above three properties of reflexivity, symmetry and transitivity.\n\nSince the equality relation is defined generically for all mathematical objects and not just for natural numbers, we must make explicit the assumption that if two mathematical objects are equal—i.e., they satisfy the equality relation—and one of them is a natural number, then the other must also be a natural number. The next axiom makes this assumption explicit, namely, a natural number can only be equal to another natural number.\n\nFourthly, if any mathematical object is equal to a natural number, then that mathematical object is itself a natural number. This is called the closure of equality axiom.\n\nAxiom 4\\mathbf{4}4. For all xandy,ifx∈N0andx=y,theny∈N0x \\text{ and } y, \\text{ if } x \\in \\mathbb{N_0} \\text{ and } x = y, \\text{ then } y \\in \\mathbb{N_0}xandy,ifx∈N0andx=y,theny∈N0.\n\nThat is, the set of natural numbers is closed under equality.\n\nThe Peano Axioms\n\nWe will now discuss the five main Peano axioms that define the natural numbers. Peano aimed to formulate these axioms such that the fewest possible axioms could generate all the natural numbers. Therefore, we will construct the Peano axioms by checking, after stating each axiom, whether the axioms stated thus far can unambiguously result in the set of natural numbers that we know of. That is, we will continue constructing the Peano axioms until these axioms, when taken together, incontrovertibly result in N0={0,1,2,…}\\mathbb{N}_0 = \\{0, 1, 2, \\ldots\\}N0={0,1,2,…}.\n\nThis method of constructing the Peano axioms leads to the insight that the entire set of natural numbers can be generated by asserting the existence of at least one natural number and then defining a function called the successor function. This function takes a natural number as input and outputs another natural number, resulting in the construction of all remaining natural numbers.\n\nLet us now proceed with the construction of the Peano Axioms.\n\nSince we start counting from 000, it is unsurprising that 000 is the most obvious element to axiomatically include in the set of natural numbers.\n\nFifthly, 000 is a natural number.\n\nAxiom 5\\mathbf{5}5. 0∈N00 \\in \\mathbb{N_0}0∈N0.\n\nThus far we are only guaranteed the existence of a single natural number, 000.\n\nFrom 000 we should be able to generate the other natural numbers; that is starting with 000 we should be able to reach 111, from 111 reach 222 and so on, akin to counting. We can model this progression from one natural number to the next using a function that takes a natural number as input and produces another natural number as output. This function is called a successor function since it establishes a succession within the set of natural numbers and is written as S:N0→N0S : \\mathbb{N_0} \\rightarrow \\mathbb{N_0}S:N0→N0. The next axiom simply states that there is a function SSS whose domain and codomain are the set of natural numbers, N0\\mathbb{N}_0N0.\n\nSixthly, every natural number has a successor which is also a natural number.\n\nAxiom 6\\mathbf{6}6. If x∈N0x \\in \\mathbb{N}_0x∈N0, then S(x)∈N0S(x) \\in \\mathbb{N}_0S(x)∈N0.\n\nThat is, the set of natural numbers, N0\\mathbb{N}_0N0, is closed under the successor operation, SSS.\n\nAs this axiom implies, we will refer to S(x)S(x)S(x) as the successor of x\\mathit{x}x.\n\nTill now we have only established that the set of natural numbers contains 000 and its successor, S(0)S(0)S(0), where the function SSS takes natural numbers as input and outputs natural numbers. However, we are still quite far from having the set of natural numbers that we know of, since we could have N0={0}\\mathbb{N}_0 = \\{0\\}N0={0} and define S(0)=0S(0) = 0S(0)=0, which would still satisfy all of the above axioms. In this case, N0={0}\\mathbb{N}_0 = \\{0\\}N0={0}, but we want N0={0,1,2,3,…}\\mathbb{N}_0 = \\{0, 1, 2, 3, \\ldots\\}N0={0,1,2,3,…}.\n\nTo achieve this, we need to ensure that the successor function SSS does not output 000. Our next axiom will guarantee this by forbidding 000 from being the successor of any natural number, including itself.\n\nSeventhly, 000 is not  the successor of any natural number.\n\nAxiom 7\\mathbf{7}7. For every natural number x∈N0,S(x)≠0x \\in \\mathbb{N}_0, S(x) \\neq 0x∈N0,S(x)=0.\n\nThat means that there is no natural number whose successor is 000. Consequently, the preimage of 000 under SSS defined on the set of natural numbers is an empty set.\n\nAs a consequence of this axiom, we know that S(0)≠0S(0) \\neq 0S(0)=0. Therefore, S(0)S(0)S(0) must equal some other natural number, which we can denote by 111. Hence, we can define 111 by S(0)=1S(0) = 1S(0)=1.\n\nBased on axioms 5,6and75, 6 \\text{ and } 75,6and7 we are guaranteed the existence of at least two natural numbers, 0and10 \\text{ and } 10and1, but not necessarily others.\n\nFor example, we could define N0={0,1}\\mathbb{N}_0 = \\{0, 1\\}N0={0,1}, where S(0)=1andS(1)=1S(0) = 1 \\text{ and } S(1) = 1S(0)=1andS(1)=1. In this case, both natural numbers 0and10 \\text{ and } 10and1 have the same successor, which is 111.\n\nIf S(0)=1andS(1)=1S(0) = 1 \\text{ and } S(1) = 1S(0)=1andS(1)=1, then the two natural numbers 0and10 \\text{ and } 10and1 have the same successor, 111. This set of natural numbers together with the successor function SSS defined on it would still satisfy all the above axioms. However, if we stop here, the axioms constructed thus far do not guarantee the existence of the rest of the natural numbers that we know of, namely, 2,3,4…2, 3, 4 \\ldots2,3,4….\n\nTherefore, our next axiom should ensure that different natural numbers have different successors. This means that every natural number is the successor of at most one natural number (since 000 is not the successor of any natural number) which implies that SSS must be an injective function.\n\nEighthly, no two natural numbers have the same successor unless they are equal.\n\nAxiom 8\\mathbf{8}8. For all x,y∈N0x, y \\in \\mathbb{N}_0x,y∈N0, if S(x)=S(y)S(x) = S(y)S(x)=S(y), then x=yx = yx=y.\n\nThis axiom leads to some important consequences. It excludes the possibility of defining N0\\mathbb{N}_0N0 to be just {0,1}\\{0, 1\\}{0,1}. We already have S(0)=1S(0) = 1S(0)=1 and since SSS is an injective function, we cannot have S(1)=1S(1) = 1S(1)=1. Axiom 777 excludes the possibility that S(1)=0S(1) = 0S(1)=0. Thus S(1)S(1)S(1) must be some other natural number, which we denote as 222. Therefore, we can define 2=S(1)2 = S(1)2=S(1).\n\nBy a similar argument, S(2)S(2)S(2) cannot be 0,1or20, 1 \\text{ or } 20,1or2. Hence, it must be some other natural number, which we denote as 333. Continuing this way, we see that N0\\mathbb{N}_0N0 must contain all the natural numbers that we know of.\n\nSo far we have established that N0\\mathbb{N}_0N0 must include 000, its successor 1=S(0)1 = S(0)1=S(0), its successor’s successor 2=S(1)2 = S(1)2=S(1) and so on. Thus N0\\mathbb{N}_0N0 must include 0,S(0),S(S(0)),S(S(S(0))),…0, S(0), S(S(0)), S(S(S(0))), \\ldots0,S(0),S(S(0)),S(S(S(0))),…. In order to avoid so many nested applications of SSS we use the numerals 1,2,31, 2, 31,2,3 to denote S(0),S(S(0))andS(S(S(0)))S(0), S(S(0)) \\text{ and } S(S(S(0)))S(0),S(S(0))andS(S(S(0))), respectively.\n\nThese first eight axioms have resulted in the definition of N0\\mathbb{N}_0N0 to include all the natural numbers that we know of.\n\nTherefore, so far we only know that\n\n{0,1,2,…}⊂N0\\{0, 1, 2, \\ldots\\} \\subset \\mathbb{N}_0{0,1,2,…}⊂N0\n\nAt this point, it is interesting to ask whether our axiomatic definition of N0\\mathbb{N}_0N0 precludes the inclusion of additional elements.\n\nIn order to answer this question, let us consider a version of N0\\mathbb{N}_0N0 that satisfies all the above axioms but is not the usual set of natural numbers that we know of. That is,\n\nN0={0,1,2,3,…}∪{!,−}\\mathbb{N}_0 = \\{0, 1, 2, 3, \\ldots\\} \\cup \\{!, -\\}N0={0,1,2,3,…}∪{!,−}\n\nAs can be seen, this version of N0\\mathbb{N}_0N0 contains all the natural numbers and also includes two other symbols, !and−! \\text{ and } -!and−.\n\nWe will next define the successor function defined on this set. For the subset {0,1,2,3,…}\\{0, 1, 2, 3, \\ldots\\}{0,1,2,3,…} of N0\\mathbb{N}_0N0, we define SSS as we have described above i.e., S(0)=1,S(1)=2,S(2)=3S(0) = 1, S(1) = 2, S(2) = 3S(0)=1,S(1)=2,S(2)=3 and so on. For the subset {!,−}\\{!, -\\}{!,−} of N0\\mathbb{N}_0N0, we define S(!)=−andS(−)=!S(!) = - \\text{ and } S(-) = !S(!)=−andS(−)=!.\n\nThis version of N0\\mathbb{N}_0N0 with this successor function satisfies all the axioms, but it has more elements than what we want our set of natural numbers to have. This is shown below.\n\nSimilarly, there could be other versions of N0\\mathbb{N}_0N0 with different successor functions, each of which satisfies all of the above axioms from 5through85 \\text{ through } 85through8 but could also have elements other than natural numbers. This is illustrated below.\n\nBased on axioms 5to85 \\text{ to } 85to8, the set N0\\mathbb{N}_0N0 of natural numbers satisfies the following conditions:\n\n0∈N00 \\in \\mathbb{N}_00∈N0; and\n\nif x∈N0,thenS(x)∈N0x \\in \\mathbb{N}_0,\\text{then } S(x) \\in \\mathbb{N}_0x∈N0,thenS(x)∈N0, where S(x)S(x)S(x) denotes the successor of xxx.\n\nThis way of defining a set, where a base clause specifies the basic element of the set and an inductive clause details how to generate additional elements, is called an inductive definition of the set, and such a set is referred to as an inductive set.\n\nTherefore, the axioms 5to85 \\text{ to } 85to8 only ensure that {0,1,2,…}⊂N0\\{0, 1, 2, \\ldots\\} \\subset \\mathbb{N}_0{0,1,2,…}⊂N0, where N0\\mathbb{N}_0N0 is any set such that 0∈N00 \\in \\mathbb{N}_00∈N0 and if x∈N0,thenS(x)∈N0x \\in \\mathbb{N}_0,\\text{then } S(x) \\in \\mathbb{N}_0x∈N0,thenS(x)∈N0.\n\nHowever, as discussed earlier and shown in the diagram above, this definition of N0\\mathbb{N}_0N0 does not exclude elements other than natural numbers from being contained in the set.\n\nTo ensure that only natural numbers are included in the set N0\\mathbb{N}_0N0, i.e., N0={0,1,2,…}\\mathbb{N}_0 = \\{0, 1, 2, \\ldots\\}N0={0,1,2,…}, our next (and final) axiom should declare N0\\mathbb{N}_0N0 to be the minimal set that satisfies axioms 5through85 \\text{ through } 85through8; specifically, N0\\mathbb{N}_0N0 is the intersection of all sets that satisfy these axioms.\n\nSince axiom 999 uses the inductive definition of a set in its construction, it is referred to as the Axiom of Induction.\n\nWe will now state our ninth and final axiom.\n\nAxiom 9\\mathbf{9}9 (Axiom of Induction). If T⊂N0T \\subset \\mathbb{N}_0T⊂N0 is such that:\n\n0∈T0 \\in T0∈T; and\n\nx∈T  ⟹  S(x)∈Tx \\in T \\implies S(x) \\in Tx∈T⟹S(x)∈T for all x∈N0x \\in \\mathbb{N}_0x∈N0,\n\nthen T=N0T = \\mathbb{N}_0T=N0.\n\nAs we have already discussed, the axioms 5through85 \\text{ through } 85through8 ensure that {0,1,2,… }⊂N0\\{0, 1, 2, \\dots\\} \\subset \\mathbb{N}_0{0,1,2,…}⊂N0. Suppose T={0,1,2,… }T = \\{0, 1, 2, \\dots\\}T={0,1,2,…}. We can see that 0∈Tandx∈T  ⟹  S(x)∈Tforallx∈N00 \\in T \\text{ and } x \\in T \\implies S(x) \\in T \\text{ for all } x \\in \\mathbb{N}_00∈Tandx∈T⟹S(x)∈Tforallx∈N0. Therefore, by Axiom 999, {0,1,2,… }=N0\\{0, 1, 2, \\dots\\} = \\mathbb{N}_0{0,1,2,…}=N0.\n\nThus we finally have the set of natural numbers that we know of, namely, N0={0,1,2,… }\\mathbb{N}_0 = \\{0, 1, 2, \\dots\\}N0={0,1,2,…}.\n\nAlternate Formulations of Axiom of Induction: Set-Based and  Predicate-Based Perspectives\n\nFor the sake of completeness, we will discuss two alternate ways to formulate Peano’s Ninth Axiom, namely the Axiom of Induction.\n\nThe Axiom of Induction is a cornerstone of number theory, providing a powerful tool for proving statements about natural numbers. It can be expressed in two equivalent forms: set-based and predicate-based. Each form offers a different perspective on the same fundamental principle.\n\nSet-Based Axiom of Induction: Focusing on Subsets\n\nAxiom 9\\mathbf{9}9 (Axiom of Induction). If T⊂N0T \\subset \\mathbb{N}_0T⊂N0 is a set such that:\n\n0∈T0 \\in T0∈T; and\n\nx∈T  ⟹  S(x)∈Tx \\in T \\implies S(x) \\in Tx∈T⟹S(x)∈T for all x∈N0x \\in \\mathbb{N}_0x∈N0,\n\nthen N0⊂T\\mathbb{N}_0 \\subset TN0⊂T.\n\nIn simpler terms, if a subset of natural numbers contains 000 and is closed under the successor operation (meaning that if a natural number is in the subset, its successor is also in the subset), then that subset must contain all natural numbers.\n\nAs discussed above, Axioms 5through85 \\text{ through } 85through8 ensure that {0,1,2,…}⊂N0\\{0, 1, 2, \\ldots\\} \\subset \\mathbb{N}_0{0,1,2,…}⊂N0. Additionally, we observe that the set {0,1,2,…}\\{0, 1, 2, \\ldots\\}{0,1,2,…} satisfies the following two conditions of Axiom 999:\n\nIt contains 000; and\n\nWhenever it contains an element xxx, it also contains its successor, namely, S(x)S(x)S(x).\n\nTherefore, by Axiom 999, it follows that N0⊂{0,1,2,…}\\mathbb{N}_0 \\subset \\{0, 1, 2, \\ldots\\}N0⊂{0,1,2,…}.\n\nSince Axioms 5through85 \\text{ through } 85through8 ensure that {0,1,2,…}⊂N0\\{0, 1, 2, \\ldots\\} \\subset \\mathbb{N}_0{0,1,2,…}⊂N0 and by Axiom 999 we have show that N0⊂{0,1,2,…}\\mathbb{N}_0 \\subset \\{0, 1, 2, \\ldots\\}N0⊂{0,1,2,…}, it follows that N0={0,1,2,…}\\mathbb{N}_0 = \\{0, 1, 2, \\ldots\\}N0={0,1,2,…}.\n\nThis perspective emphasizes the structure of the subset itself. We are concerned with the elements that are members of the subset and how they relate to each other through the successor function.\n\nPredicate-Based Axiom of Induction: Focusing on Properties\n\nPeano’s axioms five through eight collectively define a superset of natural numbers, specifically {0,1,2,…}⊂N0\\{0, 1, 2, \\ldots\\} \\subset \\mathbb{N}_0{0,1,2,…}⊂N0. To ensure that this set N0\\mathbb{N}_0N0 includes only natural numbers, Peano’s ninth axiom can also be formulated as the principle of mathematical induction over natural numbers. This formulation is equivalent to the axiom of induction and serves the same purpose of removing unwanted elements from the superset N0\\mathbb{N}_0N0, ensuring that it contains only natural numbers.\n\nThe predicate-based form of the Axiom of Induction shifts the focus from subsets to properties (expressed as predicates) that natural numbers may or may not possess.\n\nThe reformulation of Axiom 999 in terms of predicates results in the Principle of Mathematical Induction which is stated as follows.\n\nAxiom 9\\mathbf{9}9 (Principle of Mathematical Induction). For any predicate P(n)P(n)P(n), where nnn is a natural number, if:\n\nP(0)P(0)P(0) is true, and\n\nfor every natural number nnn, P(n)P(n)P(n) being true implies that P(S(n))P(S(n))P(S(n)) is also true,\n\nthen P(n)P(n)P(n) is true for every natural number nnn.\n\nHere, we are concerned with a property (((represented by the predicate P(n))P(n))P(n)) that natural numbers might possess. If 000 has the property, and if natural number nnn having the property implies that its successor S(n)S(n)S(n) also has the property, then all natural numbers must have the property.\n\nThis perspective emphasizes the properties of individual natural numbers. We are concerned with whether a given natural number has a specific property.\n\nEquivalence and Connection\n\nThe set-based and predicate-based forms are logically equivalent, meaning they express the same fundamental principle. This equivalence is rooted in the relationship between subsets and predicates, as established by the Axiom of Separation.\n\nFrom Subset to Predicate:\n\nGiven T⊂N0T \\subset \\mathbb{N}_0T⊂N0, it implies from the Axiom of Separation that there exists a predicate P(n)P(n)P(n) such that P(n)P(n)P(n) is true if and only if n∈Tn \\in Tn∈T.\n\nFrom Predicate to Subset:\n\nGiven a predicate P(n)P(n)P(n) and a set N0\\mathbb{N}_0N0, we can define a subset T={n∈N0 ∣ P(n)istrue}T = \\{n \\in \\mathbb{N}_0 \\,|\\, P(n) \\text{ is true}\\}T={n∈N0∣P(n)istrue}, by the Axiom of Separation.\n\nThus, the subset and predicate perspectives are simply two ways of expressing the same fundamental idea. The set-based form emphasizes the elements within a collection, while the predicate-based form emphasizes the properties of individual elements. The Axiom of Separation is the bridge that allows us to move seamlessly between these perspectives.\n\nWhy Both Forms Are Useful\n\nBoth forms of the Axiom of Induction are valuable tools in mathematical proofs. The set-based form is often used in set theory and related areas, while the predicate-based form is commonly used in number theory and other branches of mathematics where properties of numbers are the main focus.\n\nEssentially, they are two sides of the same coin, and which one to use depends on the context of the problem and the preference of the mathematician.\n\nMethod of Definition of Natural Numbers using Peano’s Axioms\n\nIt should be noted that Peano’s Axioms only describe how to construct the set of natural numbers and do not define what natural numbers are intrinsically. A particular natural number is given when its generation under the inductive definition is given.\n\nFor example, the natural number 222 is defined as that mathematical object which is obtained by starting with the initial object 000 and applying the successor function once and then again i.e., 222 represents S(S(0))S(S(0))S(S(0)).\n\nExistence of the Set of Natural Numbers satisfying Peano’s Axioms\n\nHow do we establish the existence of a set, an element of that set, and a function from the set to itself, that satisfy Peano’s Axioms? These axioms themselves are insufficient to prove this existence. Consequently, there are two approaches to resolving this matter.\n\nOne common approach in mathematics is to take something as  axiomatic and then use it as the basis upon which we prove all our other results. Hence, such an approach requires us to be satisfied with taking the existence of a set satisfying Peano’s Axioms axiomatically. This axiom is called the existence axiom for natural numbers and guarantees that there exists a set with the properties that Peano’s axioms ascribe to it.\n\nThe statement of this axiom is as follows:\n\nExistence Axiom for Natural Numbers: There exists a set N0\\mathbb{N}_0N0 satisfying Axioms 1through91 \\text{ through } 91through9.\n\nAlternatively, if we use the Zermelo-Fraenkel Axioms as our foundation for set theory, we can prove that something satisfying Peano’s Axioms exists, so we don’t need to assume it separately. We’ll show how this is done later.\n\nProving Properties of Natural Numbers\n\nHaving established the existence of the natural numbers and their fundamental properties as defined by Peano’s Axioms, we will next explore how to rigorously prove that the natural numbers satisfy any specific property using the Axiom of Induction.\n\nNext Topic: Proving Properties of Natural Numbers Using Proof by Induction\n\nShare this:Click to share on Facebook (Opens in new window)Click to share on X (Opens in new window)Click to email a link to a friend (Opens in new window)\n\n\t\t\t\t\t←\n\t\t\t\t\tFormulas and Free Variables\n\n\t\t\t\t\t→\n\t\t\t\t\t\tNumber Theory Primer : Proving Properties of Natural Numbers Using Proof by Induction",
    "summary": {
      "en": "**Summary of Peano's Axioms and Natural Numbers**\n\nThe study of natural numbers starts with Peano's Axioms, which provide a foundational framework for defining these numbers without relying on the intuitive idea of counting. Natural numbers (0, 1, 2, 3, etc.) are essential for counting objects in the real world. \n\n1. **Purpose of Axioms**: The goal of an axiomatic system is to make minimal assumptions while proving as much as possible about natural numbers. Peano's Axioms achieve this by defining natural numbers through a few essential properties.\n\n2. **Peano's Axioms**: Originally formulated by Giuseppe Peano in the late 19th century, the standard version consists of nine axioms that describe the properties of natural numbers. These include:\n   - 0 is a natural number.\n   - Each natural number has a successor, which is also a natural number.\n   - 0 is not the successor of any natural number.\n   - Different natural numbers have different successors.\n   - Axiom of Induction: If a set contains 0 and is closed under the successor operation, then it contains all natural numbers.\n\n3. **Successor Function**: The successor function S takes a natural number and produces the next number in the sequence. For example, S(0) = 1, S(1) = 2, and so on.\n\n4. **Inductive Definition**: The natural numbers can be defined inductively, meaning we start with a base case (0) and generate further numbers using the successor function.\n\n5. **Existence of Natural Numbers**: The existence of a set of natural numbers that satisfies these axioms can be assumed axiomatically or proven using the Zermelo-Fraenkel Axioms of set theory.\n\n6. **Importance of Induction**: The Axiom of Induction is crucial for proving properties about natural numbers, allowing mathematicians to establish truths about all natural numbers based on a few base cases.\n\nIn summary, Peano's Axioms provide a rigorous framework for understanding and proving properties about natural numbers, forming the foundation for much of number theory.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "7dab4e26703d5c25",
    "title": {
      "en": "Mastering Delphi 5 2025 Annotated Edition Is Now Complete",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://blog.marcocantu.com/blog/2025-march-mastering-delphi5-annotated-complete.html",
    "score": 185,
    "by": "mariuz",
    "time": 1742830808,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "0894f185902b17cb",
    "title": {
      "en": "Project Operation Whitecoat (2010)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://scholarworks.lib.csusb.edu/cgi/viewcontent.cgi?article=1201&context=history-in-the-making",
    "score": 102,
    "by": "smegger001",
    "time": 1742834107,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "84af1fd35b76d30e",
    "title": {
      "en": "The Wright brothers invented the airplane, right? Not if you're in Brazil",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.washingtonpost.com/world/2025/03/21/brazil-airplane-wright-brothers-santos-dumont/",
    "score": 232,
    "by": "benbreen",
    "time": 1742608551,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "361dc226f099cf47",
    "title": {
      "en": "Rickover's Lessons",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.chinatalk.media/p/rickovers-lessons-how-to-build-a",
    "score": 113,
    "by": "pepys",
    "time": 1742703526,
    "content": "Share this postChinaTalkRickover’s LessonsCopy linkFacebookEmailNotesMoreDiscover more from ChinaTalkDeep coverage of technology, China, and US policy. We feature original analysis alongside interviews with leading thinkers and policymakers.Over 50,000 subscribersSubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.Already have an account? Sign inRickover’s Lessons\"The status quo has no absolute sanctity\"Lily OttingerMar 20, 202563Share this postChinaTalkRickover’s LessonsCopy linkFacebookEmailNotesMore1212ShareCharles Yang is the executive director for the Center for Industrial Strategy, a bipartisan think tank focused on industrial policy. Previously, he served as an AI and Supply Chain Policy Advisor at the Department of Energy and was an ML Engineer at an AI hardware startup in San Francisco. Today, he’s here to present some excerpts from his research into how Admiral Hyman Rickover built the nuclear navy.Strategic competition demands more than technological innovation — it requires building industrial power. The U.S. is realizing the damage done by decades of underinvestment in the nation’s industrial base, which now jeopardizes its ability to compete on the global stage. Today, the production capacity of Chinese shipyards is over 200 times that of US shipyards, and China has used its chokehold on critical mineral processing as leverage to retaliate against US sanctions.A new bipartisan consensus is emerging around the need for industrial policy — from the passage of the CHIPS and Science Act, to the recent bipartisan introduction of the SHIPS for America Act and the Critical Minerals for the Future Act.As Congress steps into this more active role, policymakers should learn from the successes of our past. Nearly 75 years ago, Admiral Hyman G. Rickover, “Father of the Nuclear Navy”, pioneered a bold program to develop and operationalize nuclear power in the Navy. Under his leadership, the U.S. government harnessed the power of the atom, building the world’s first nuclear-powered submarine and the world’s largest fleet of nuclear reactors for civilian power.Lessons From the PastRickover spent his entire career in the Navy and is still the longest-serving naval officer in US history. He spent the first 20 years of his career as an electrical engineer, where he honed a strong technical foundation and unique management style. In 1946, he was assigned a 1-year tour of duty at the Oak Ridge site of the Manhattan Project. Rickover immediately recognized the transformative potential of nuclear technology — he spent the rest of his career building the “Nuclear Navy,” which ensured US strategic dominance of the high seas for the rest of the 20th century.Within the span of 10 years, Rickover created an entire office dedicated to nuclear propulsion, and successfully launched the world’s first nuclear-powered submarine without cost overruns. He conclusively demonstrated the strategic importance of nuclear propulsion in a timeframe no one thought possible and helped the US beat the Soviets to nuclear propulsion for submarines by 3 years. His institutional legacy is the US Navy’s safe construction and operation of nuclear reactors.As the US gears up for another strategic competition, Rickover’s story can offer helpful lessons for aspiring technocrats. Oftentimes, industrial policy is framed in terms of legislation, but Rickover demonstrates that industrial policy is as much about policy as it is about strong leadership.USS Enterprise, the world’s first nuclear-powered aircraft carrier. Source.Talent, Training, and ManagementRickover spent an inordinate amount of time focused on interviewing personnel — he made the final hiring decision for every naval officer who applied to serve on a nuclear submarine until he retired. And he was an unorthodox interviewer, screening for high agency individuals who could think on their feet — literally! To test their composure, Rickover famously made candidates sit in a chair with the front two legs shortened as he loomed over them during questioning.The infamous “Rickover chair”One interview account:For one interviewee who said he liked hiking, Rickover asked him if he had ever hiked the nearby “Goat Mountain”. When he said he had not, Rickover told him to bring him proof he had climbed it by tomorrow morning and he would be hired. It turns out that Goat Mountain was the peak of a structure for mountain goats in a zoo. He went to the zoo, asked a tourist to take his picture, jumped into the enclosure, and climbed to the top. He’s hired the next day!But it didn’t end at the interview process — Rickover believed in continued technical training for his staff and in building out a talented workforce base for this new technology:While Rickover worked to staff up quickly in the short term, he also set out to build a deep bench and a long-term pipeline of talent. He required each officer and engineer he hired to submit a self-study plan demonstrating mastery of advanced texts in metallurgy, physics, and chemistry, along with field trips to AEC facilities, totaling 854 hours of study or 16 hours per week. He also worked with MIT to develop a survey course on nuclear physics and a master's degree in nuclear engineering, with a curriculum drawn up and agreed to by Rickover, starting in June of 1949. Rickover also worked with Oak Ridge National Lab to develop a 1-year curriculum in nuclear science and technology, a program christened “Oak Ridge School of Reactor Technology (ORSORT) with the first cohort starting in March 1950. Westinghouse, GE, utilities, naval and private shipyards, and Naval Reactors all sent students to ORSORT and the program started turning out ~100 graduates a year, providing another training center to develop a nuclear industry.1 Finally, Rickover had his engineers provide training lectures to a variety of audiences, ranging from senior officials in BuShips to junior technicians, as well as to explain shipboard problems and applications to scientists at Argonne, Oak Ridge, and Westinghouse/GE.He was also known for his unique style of management. Not only did he interview every naval officer in his office, he also maintained direct lines of communication with every nuclear sub commander and project officer on-site with contractors, giving him early awareness of every issue. The demanding oversight he extended over his technical staff under his command pushed them to have greater awareness of their own direct reports:Rickover was also an intensely demanding and scrutinizing manager. As most writing then was done on carbon copy paper, every night Rickover would collect the “pinks” of every piece of writing from his various teams i.e. the carbon copied half, and read over them at home, including drafts. When his officers protested as to how they should be expected to keep track of everything in their purview, including drafts reports from staff below them, Rickovers responded “It’s up to you to see that I don’t know more about what’s going on in your shop than you do”. By enforcing tight lines of supervision over his officers, Rickover ensured that he maintained full visibility into each team, including the project facilities at Knolls, Bettis, and the shipyards, allowing him to catch problems early on. It also enforced a culture of direct accountability and oversight across the organization.Rickover’s focus on hiring, training, and close project management represented his philosophical approach to how to build complex systems managed by humans.Near the end of his career, Rickover testified to Congress after the Three Mile Island Reactor accident. He spent the vast majority of his testimony talking not about regulatory reform, but about the lack of training and inadequate culture of responsibility among the operators.“Human experience shows that people, not organizations or management systems, get things done. For this reason, subordinates must be given authority and responsibility early in their careers…Complex jobs cannot be accomplished effectively with transients. A manager must make the work challenging and rewarding so that his people will remain with the organization for many years. This allows it to benefit fully from their knowledge, experience, and corporate memory.”~ Hyman Rickover, 1982Industrial State CapacityRickover’s scrutinizing style of management extended to the private companies he worked with. He pioneered the practice of project officers, who lived on-site at the projects and who would report directly to him any delays or unforeseen issues, so that Rickover could escalate immediately and ensure the project remained on track.Government contracting was, and still is today, a largely passive and administrative activity. While Rickover acknowledged that the government was the “customer” and the contractor was the one responsible for delivering, Rickover’s unique approach to program management was exercising tight oversight over the contractors. Rickover hired technical experts into his office and then sent them out as project officers to oversee the various contractor sites. There, the project officer was expected to be the active representative of the Naval Reactors Office, reporting directly to Rickover any issues with contractors and ensuring the contractor was on track to deliver the product as expected. In every sense, Rickover’s project officer was to be his eyes and ears on the ground. Rickover took great pains to ensure there was no customer capture, telling one of his project officers, “Don’t go to dinner with them. Your wives must not get friendly with their wives. You’re not even to let your dogs get friendly with their dogs…when you do that, you become one of them…you don’t represent me anymore”.Rickover’s success in scaling industrial technology was demonstrated early on with Zirconium production. In 1949, the world had only produced a shoebox worth of purified Zirconium, but the material showed promise as a fuel cladding material due to its durability under high temperatures without blocking the emitted neutrons needed to enable fission reactions. AEC opened up a simple contract for private companies to bid to produce Zirconium, but none of the companies were able to scale up production. Rickover took over production a year later, applied his practice of close project management with the (now defunct) Bureau of Mines, and only then passed it off to industry:But by 1949, when Rickover was looking to scale up promising fuel cladding material production, the AEC had already decided to run contracts through another AEC division. Unable to exert the centralized control over the contractors, the AEC manufacturers were slow to scale up a high-quality production process. In 1950, after a year of delay, Rickover finally received permission to have the Westinghouse Bettis site directly manufacture Zirconium metal and worked with the Bureau of Mines (BuMines) to purify the Zirconium. Under Rickover’s scrutiny, Bettis scaled a novel purification process to thousands of tons of production capacity. Rickover opened up contract bids for Zirconium only after having derisked this novel technology. When the Secretary of the Navy later asked Westinghouse how they managed to scale up this process, the response he got was “Rickover made us do it”.“The man in charge must concern himself with details. If he does not consider them important, neither will his subordinates.”~ Hyman Rickover, 1982Bureaucratic InnovationBuilding big things requires lots of people. Rickover was not only an exceptional manager of people and deeply technical, but his 20-year naval career before Oak Ridge taught him how to wrangle government bureaucracy — and discern which rules mattered and which didn’t. For example, Rickover was interviewing an officer who thought the monthly reports on the gasoline usage of his base’s motorboats were pointless and wasteful. Rickover told him to simply remove the tickler file that tracked the reports from the boss’s secretary file and to send over a note the next day alerting Rickover that the task had been completed. The interviewee did and was hired.SubscribeRickover’s bureaucratic skill is exemplified by his success in rallying the Navy behind the nuclear-powered submarine. He believed this was a feasible, near-term project, despite widely-held convictions to the contrary — including those of the Atomic Energy Commission (AEC). Even Robert Oppenheimer (who served as one of the first AEC commissioners) doubted nuclear propulsion early on.In light of initial resistance from the civilian AEC, Rickover formulated a unique bureaucratic innovation to position himself within two chains of command — one within the Navy and the other within the AEC.Rickover was also able to realize his bureaucratic innovation to occupy a spot on the org chart both at AEC and in the Navy BuShips, something he first formulated while at Oak Ridge. This way, if the AEC refused something, he could respond that “this is a priority for the Navy” and vice versa. Similar to how the Manhattan Project reduced risk by pursuing parallel technological approaches, Rickover would reduce his bureaucratic risk by pursuing parallel chains of command. This unique structure lives on to this day, with Naval Reactors shared between the semi-autonomous National Nuclear Safety Administration (NNSA) in the Department of Energy (DOE) and the Navy.Joint org chart for AEC and Navy nuclear reactor efforts“The status quo has no absolute sanctity under our form of government. It must constantly justify itself to the people in whom is vested ultimate sovereignty over this nation”~ Hyman RickoverRickover firmly believed that the right team and the right culture could build incredible industrial technologies at scale, even within the government. While discourse in Washington DC often focuses on regulations or money, Rickover’s life brings a uniquely human-centered view of industrial policy: one that recognizes the importance of state capacity, technical personnel, and most importantly, public leaders with the vision and drive to build technology.You can read the full story of Rickover and how he built the world’s first nuclear-powered submarine on Charles’s substack.Rough DraftsHow Hyman Rickover Built the Nuclear NavyThere is a common misconception that the United States stopped building nuclear reactors after the 1970s. But that is not the full story: the US Navy has consistently built at least one small nuclear reactor every year since the 1950s. The fact that the Navy has managed to safely design, build, and operate nuclear reactors for decades, despite the stagn…Read more22 days ago · 15 likes · 1 comment · Charles YangSubscribe to ChinaTalkHundreds of paid subscribersDeep coverage of technology, China, and US policy. We feature original analysis alongside interviews with leading thinkers and policymakers.SubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.63 Likes∙12 Restacks63Share this postChinaTalkRickover’s LessonsCopy linkFacebookEmailNotesMore1212Share",
    "summary": {
      "en": "The article discusses the lessons learned from Admiral Hyman Rickover, known as the \"Father of the Nuclear Navy,\" and emphasizes the importance of strong industrial policy and leadership in the U.S. Rickover’s work in developing the nuclear-powered navy showcased how strategic competition requires not just technological innovation but also a robust industrial base.\n\nKey points include:\n\n1. **Industrial Policy Importance**: The U.S. has suffered from underinvestment in its industrial base, making it difficult to compete globally, especially against China, which has significantly more production capacity.\n\n2. **Rickover's Leadership**: Rickover's success in creating the nuclear navy stemmed from his technical expertise, hands-on management style, and emphasis on hiring and training talented personnel. He personally interviewed every officer for nuclear submarines and stressed continued education.\n\n3. **Management Style**: He maintained strict oversight and accountability, ensuring that his teams were well-informed and prepared to handle complex challenges. He believed in giving authority and responsibility to his subordinates early in their careers.\n\n4. **Bureaucratic Innovation**: Rickover skillfully navigated government bureaucracy, using dual command structures to advocate for nuclear propulsion within both the Navy and the Atomic Energy Commission, which helped him overcome initial resistance to his ideas.\n\n5. **Human-Centered Approach**: Rickover's philosophy was that people, not just systems, drive success in complex projects. He believed in fostering a culture that values talent and innovation.\n\nOverall, Rickover's approach offers valuable insights for current policymakers as they seek to enhance U.S. industrial capacity and compete effectively on the global stage.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "20bb7511fe9936ea",
    "title": {
      "en": "Aircraft detection at planetary scale",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.planet.com/pulse/aircraft-detection-at-planetary-scale/",
    "score": 83,
    "by": "jjwiseman",
    "time": 1742848821,
    "content": "HOMETechAircraft Detection at Planetary ScaleAdam Van EttenMarch 20, 2025•5 min readTechTechIntroductionOn a typical day, there are greater than 10,000 aircraft (commercial, private, and military) in flight at any given time. The precise departure (point A) and destination (point B) locations, as well as the timing of flight activity, provide critically important information for myriad industries. For instance, in the defense and intelligence sector, being able to identify how many aircraft were stationed at one foreign government’s airbase (point A) and how many are gone the next day may reveal insights as to why an allied government’s airbase (point B) just received a new fleet on their tarmac.On the commercial side, understanding how many aircraft landed in a city on a specific day can help predict economic trends and assess the impacts of world events - such as the quantity of aircraft present for large sporting or entertainment events. Satellite imagery is a powerful information source for this type of aircraft detection. Combine automated satellite scans of airfields with a global news reporting large language model, and you can start to get a feed of insights, linking the patterns of politics and economies with global air traffic.ScaleAircraft detection with satellite imagery is nothing new. In fact, aircraft detection on the tarmac is an archetypal use case for remote sensing machine learning. Until now, though, aircraft detection has been limited to a select number of airfields with sporadic coverage and cadence. Planet’s satellites, on the other hand, scan all of Earth’s landmass on a near-daily basis, so we are now able to mitigate those limitations, leveraging novel machine learning techniques with global PlanetScope coverage to drastically scale global aircraft detection.The need for greater scale in aircraft detection is difficult to overstate. More than one hundred thousand commercial and military flights take place each day, as planes transport people and cargo to nearly fifty thousand airports across the globe. Some sites collect and share publicly available data on this complex matrix of movement. But not every flight is recorded, and not every plane is public.Monitoring aircraft activity is vital for economic, defense, and intelligence purposes, but current methods rely on tedious and expensive processes, especially at scale. The rapid pace of geopolitical developments and sheer volume of data means that analyzing airfields manually is no longer feasible, especially in conflict/sensitive zones where aircraft movements aren’t publicly reported.Planet’s new Aircraft Detection Analytic Feed automates daily detection of aircraft by leveraging a machine learning model trained using PlanetScope and SkySat data. It precisely identifies aircraft ≥25 meters in length or wingspan across airfields globally. It represents the first attempt at global detection of large aircraft on a near-daily basis and offers a scalable solution for analysis at scale. This presents a massive and unprecedented opportunity for analysts with or without geospatial expertise. Historical daily counts of aircraft broken down by size and location enables pattern-of-life analyses to be conducted, highlighting when anomalous behavior is detected. This can indicate events taking place or even actions to come.Keep reading for more information and register for our upcoming webinar. Our ApproachOur Aircraft Detection service leverages a supervised object detection model, trained on PlanetScope imagery to detect aircraft at rest on an airfield/airport tarmac. Precise labeling of aircraft can be a challenging task in medium-resolution PlanetScope imagery, so we use high-resolution SkySat imagery collected within 3 minutes of a PlanetScope scene (see Figure 1). This approach enables us to label on high-resolution imagery, then apply those high fidelity labels to medium-resolution PlanetScope data. The vast majority of aircraft are static on a 3-minute timeframe, but around ~2% of aircraft move between the two collections, and we remove those labels in our quality control phase.Figure 1. SkySat aircraft labels are applied to concurrent PlanetScope imagery.With this approach, we collected over 800 concurrent (or “crossover”) image pairs for labeling. After significant experimentation with many different model architectures, we determined that aircraft <25m in length or wingspan are too small to be reliably detected in medium resolution imagery, so we elected to focus on aircraft this size or greater. Splitting the dataset into 85% training / 15% testing, we generated a training dataset consisting of ~15,000 labels, roughly evenly split between civilian and military types. This is by no means an enormous dataset, so expansion of the label corpus would likely yield a performance boost.Figure 2. Airfield locations in the labeled dataset.Figure 3. Observation dates, covering multiple years and every season. A concerted effort was made in 2023 to leverage SkySat capacity to capture data over airfields to ensure we had a healthy amount of data to leverage.We utilize a deep learning object detection model optimized for Planet’s medium resolution imagery, which runs at sufficient speed so that we are free to analyze as many airports and airstrips as desired. The model achieves an F1 score of 0.82 for aircraft detection on our globally distributed test set. Reported errors are at the 1σ level, computed via bootstrap resampling of the test set.High resolution SkySat imagery can be leveraged for more than just training data generation. Occasionally, additional scrutiny will be required to understand exactly what’s going on at a given airfield. An example is provided in Figure 3, which shows a consistent “missed” detection with our PlanetScope model. Yet closer inspection of this airfield with SkySat revealed that this apparent “miss” was in fact merely a painting on the tarmac. The ability to rapidly probe targeted areas much deeper with SkySat is a highly valuable complement to the ubiquitous daily coverage of PlanetScope.Figure 4. PlanetScope (left) detections showing a “missed” aircraft in the upper left. High-resolution SkySat imagery (right) illustrating that only paint exists at this location.Figure 5. Animation showing analysis of a large corpus of global detections.Next StepsOur solution extends significantly beyond the archetypal problem of counting aircraft on the tarmac. We track behavior at an approximately daily cadence by quantifying the fraction of aircraft that moved or remained static. Furthermore, interactive dashboards enable on-the-fly time series analysis, outlier categorization, and trend aggregation. We’ve integrated a global news LLM into this process to further enlighten our users. Detected outliers are subsequently probed for correlation to regional events. We’ll be demonstrating this capability in an upcoming Agile EO Webinar. Register here!Tags:#Aircraft DetectionContentsIntroductionScaleOur ApproachNext StepsShareReady to Get StartedConnect with a member of our Sales team. We'll help you find the right products and pricing for your needsContact SalesOther Articles You Might Like",
    "summary": {
      "en": "**Summary of Aircraft Detection at Planetary Scale**\n\nEvery day, over 10,000 aircraft are in the air, making flight data crucial for various industries, particularly defense and economics. Knowing the number and locations of aircraft can provide insights into military actions and economic trends.\n\nTraditionally, detecting aircraft using satellite imagery has been limited to a few locations and was time-consuming. However, Planet's satellites scan the entire Earth almost daily, allowing for broader and more efficient aircraft detection using advanced machine learning techniques.\n\nPlanet has introduced an Aircraft Detection Analytic Feed that automatically identifies large aircraft (25 meters or longer) on global airfields. This service significantly enhances the ability to monitor aircraft activity, helping analysts detect unusual patterns that may indicate significant events.\n\nThe detection system uses a combination of medium-resolution PlanetScope imagery and high-resolution SkySat imagery to improve accuracy. It has successfully trained a model with around 15,000 aircraft labels, achieving a good detection rate.\n\nBeyond counting aircraft, the service tracks their movements and provides tools for real-time analysis of trends and anomalies. This integration of satellite data with a global news model helps correlate detected changes in aircraft activity with regional events.\n\nFor more details, a webinar will be held to showcase these capabilities.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "059ded92dec3320a",
    "title": {
      "en": "Project Aardvark: reimagining AI weather prediction",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.turing.ac.uk/blog/project-aardvark-reimagining-ai-weather-prediction",
    "score": 297,
    "by": "bentobean",
    "time": 1742772819,
    "content": "Section page\n                            Events from around the Turing Network\n\n                                    Conferences, workshops, and other events from around the Turing Network\n\n    Events from around the Turing Network",
    "summary": {
      "en": "This section provides information about various events related to the Turing Network, including conferences, workshops, and other activities.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "6cc369689d7a0ca5",
    "title": {
      "en": "Researchers search for more precise ways to measure pain",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.washingtonpost.com/science/2025/03/23/pain-measure-precision-research/",
    "score": 88,
    "by": "Petiver",
    "time": 1742788622,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "cf047133cd50e135",
    "title": {
      "en": "Quadlet: Running Podman containers under systemd",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://mo8it.com/blog/quadlet/",
    "score": 283,
    "by": "gjvc",
    "time": 1742776025,
    "content": "Quadlet: Running Podman containers under systemd\n\n        2024-01-02, updated: 2024-02-19\n            Tags:\n            #container,#linux,#selfhosting\n\n        Reading time: ~11min\n\n        Quadlet lets you run your Podman containers as systemd services.\nThis is especially useful for running containers in the background and automatically starting them after a server reboot.\nRunning Podman containers under systemd isn't new.\nIndeed, this was supported by Podman for a long time with the command podman generate systemd.\nBut this command now shows a deprecation warning to migrate to Quadlet.\nFor some months, I was too lazy to do that migration on my home server.\nWhy even touch a working system?\nBut now that I finally found time for it, I really appreciate Quadlet!\nI think that Podman finally has a Docker Compose alternative which is even more flexible and powerful!\nIn this blog post, I will explain how to use Quadlet with rootless Podman and migrate from the old method with podman generate systemd.\n\n        Note\n        If you wonder why systemd: Something has to start containers when there is no daemon (since Podman is daemonless).\nIf you are part of the vocal minority hating against systemd, then please leave instead of trolling.\n\n    Landscape mode recommended on mobile devices\n\n            Contents\n\n                            The deprecated method\n\n                            The problem\n\n                            Quadlet\n\n                            Is it any better?\n\n                            Dependencies\n\n                            Too many files?\n\n                            Updating images\n\n                            What about podman-compose?\n\n                            Further resources\n\nThe deprecated method\nLet's see how the old method worked before comparing it to Quadlet.\nYou can skip to the Quadlet section though.\nFirst, you had to create a container.\nIn an older post about Containerized PostgreSQL with rootless Podman, I created a container with a command similar to the following:\npodman create \\\n  --name test-db \\\n  -p 5432:5432 \\\n  -v ~/volumes/test-db:/var/lib/postgresql/data:Z \\\n  -e POSTGRES_PASSWORD=CHANGE_ME \\\n  --label \"io.containers.autoupdate=registry\" \\\n  docker.io/library/postgres:16\n\nThe command details are explained in the older post.\nThe only option that isn't explained there is --label \"io.containers.autoupdate=registry\".\nThis option enables updating the container image using podman auto-update which will be explained later in this post.\nAfter creating the container, you can run the following command:\npodman generate systemd test-db -fn --new\n\nIt creates the systemd service file container-test-db.service in your current path.\nThe options of the command aren't relevant for now, but they are also explained in the older post.\nTo use this generated service file, you had to place it in the directory ~/.config/systemd/user.\nTo enable and start it, you had to run the following command:\nsystemctl --user enable --now container-test-db\n\nThe problem\nThe problem with the old method is that it required you to run commands to…\n\ncreate a container\ngenerate a service file\nmove the service file if not already in the mentioned directory\nenable the service\n\nEspecially the command for creating the container is often lengthy.\nThis means that you had to create a shell script with these commands if you wanted to be able to rerun them later.\nTo reduce duplication, I created the following fish function to be called in my fish scripts that create the containers:\nfunction podman-default-create\n    set -l container_name $argv[1]\n\n    podman create \\\n        --name $container_name \\\n        --replace \\\n        $argv[2..]; or return 1\n\n    podman generate systemd --no-header --new --name $container_name >~/.config/systemd/user/container-$container_name.service; or return 1\n\n    systemctl --user enable --now container-$container_name\nend\n\nYou don't have to understand the details of the function above.\nWhat I want to demonstrate with it is that the old method was too hacky and involved the usage of redundant commands.\nThere must be an easier way, you might think.\nEspecially if you experienced the convenience that Docker Compose provides.\nBut this is not the only problem.\nThe old method is very inflexible!\nIf you want to cutomize the service file and use all systemd features, you need to manually edit it after each generation!\nQuadlet\nLet's take a look at the new method with Quadlet.\nFirst, you create the directory ~/.config/containers/systemd.\nThen, you place a .container file inside it.\nFor example, here is the test-db.container file:\n[Container]\nImage=docker.io/library/postgres:16\nAutoUpdate=registry\nPublishPort=5432:5432\nVolume=%h/volumes/test-db:/var/lib/postgresql/data:Z\nEnvironment=POSTGRES_PASSWORD=CHANGE_ME\n\n[Service]\nRestart=always\n\n[Install]\nWantedBy=default.target\n\nIt is a normal systemd service file but with the special section [Container].\nThis section has many documented options.\nAlmost all these options map to command line options that can be used to create a container with Podman (podman create).\nThe ones that we are interested in for the example are the following:\n\nImage specifies the image (with tag) to use\nAutoUpdate=registry maps to --label \"io.containers.autoupdate=registry\" (explained later in this post)\nPublishPort maps to -p\nVolume maps to -v\nEnvironment maps to -e\n\nIt is important to use the systemd specifier %h instead of ~ for the user home directory.\nIn the [Service] section, we use the Restart option and set it to always to always restart the container (unless stopped manually).\nTo automatically start the container on boot, we set the WantedBy option in the [Install] section to default.target.\n\n        Note\n        I thought that setting WantedBy to multi-user.target would work because it is the default target on servers.\nBut it doesn't work in the case of rootless containers.\nmulti-user.target is not defined in the user mode in systemd.\nYou can verify this by running the command systemctl --user status multi-user.target.\nIt is only defined in the system mode (systemctl status multi-user.target without --user).\n\nSince we use user services for systemd, we have to enable the linger for our user to start the containers without the user being logged in:\nloginctl enable-linger\n\n        ⚠️ Warning ⚠️\n        Enabling the linger is required for the container to be automatically started after a server reboot!\n\nFor systemd to discover the new service file, run systemctl --user daemon-reload.\nNow, you can start the container with systemctl --user start test-db.\nYou can check the status of the container service by running systemctl --user status test-db.\nYou can also verify that the Podman container is running by running podman ps.\nYou should find the container systemd-test-db.\nThe container has the name of the service file (test-db.container without the .container extension) prefixed by systemd- to avoid collisions with containers not managed by systemd.\nBut you can manually set the name of the container using the ContainerName option in the [Container] section.\nIs it any better?\nMy first impression was: \"Well, now I have to map all the podman create options to their equivalents in the [Container] section. Where is the benefit?\".\nBut after migrating all containers, I found the following benefits:\n\nYou have only one file (the container service file) instead of a script that generates a service file.\nYou can use all options possible in systemd. All options that you know about in the [Unit] and [Service] sections are supported. For example, you can specify a command to run before starting the container by setting the StartExecPre option. No more manual editing of generated files anymore!\nRather subjective: Writing configuration files is easier than writing and debugging shell scripts.\nIt is easier to handle dependencies as we will see in the next section.\n\nDependencies\nLet's assume that we have an app container that depends on the database container that we created.\nYou want the database container to be automatically started when the app container is started.\nYou also want to make sure that the app container is started after the database container.\nOtherwise, the app container might fail to start.\nHow can we express this dependency?\nLet's use OxiTraffic as an example (shameless plug 😅).\nHere is the container service file oxitraffic.container that should be placed in ~/.config/containers/systemd:\n[Container]\nImage=docker.io/mo8it/oxitraffic:0.9.2\nAutoUpdate=registry\nVolume=%h/volumes/oxitraffic/config.toml:/volumes/config.toml:Z,ro\nVolume=%h/volumes/oxitraffic/logs:/var/log/oxitraffic:Z\n\n[Unit]\nRequires=test-db.service\nAfter=test-db.service\n\n[Service]\nRestart=always\n\n[Install]\nWantedBy=default.target\n\nThe new section is [Unit].\nWe set the Requires option to test-db.service to only start the app when the database is started.\nWe also set the After option to make sure that both containers aren't started in parallel.\nNote that we use test-db.service when referencing this container service and not test-db.container.\n\ntest-db.container is the file name.\ntest-db.service is the service name.\nsystemd-test-db is the default container name.\n\nFor the app to communicate with the database, a network should be added to both containers with the Network option in the [Container] section, but networking is out of the scope of this post.\nToo many files?\nIn our example, we created two files, one for the app container and one for the database container.\nDoes this mean that multi-container apps are more complex with Quadlet because you can't just put them in one file like with Docker Compose?\nIt depends on how you define complexity in this context.\nDoes splitting content over multiple files always result in a higher complexity?\nFor me, it is more complex to have everything in the same file.\nI had to maintain Docker Compose files with hundreds of lines and dozens of containers…\nThat wasn't fun!\nHaving each container in its own file has less mental overhead for me because I just have to think about this single container when I am in its file.\nOf course, you need to specify its dependencies on other containers, but you don't need to think about the details of these other containers.\nThe Docker Compose file of Mailcow is a terrifying example of huge Docker Compose files.\n\n        Note\n        Docker Compose supports splitting to multiple files.\n\nSo we need multiple files.\nBut we should still group related ones together!\nQuadlet supports placing unit files inside directories in the ~/.config/containers/systemd directory.\nFor our example, you would create an oxitraffic directory and place both files inside it.\nUpdating images\nNow, we have containers running in the background and automatically started after a server reboot.\nWouldn't it be nice to have an easy method to update the images of these containers without running podman pull for every container and then restarting the updated ones?\nFor example, if a new image is uploaded for PostgreSQL 16 (with the image tag 16 that we used), then the image should be updated and the container should be restarted.\nWith Docker, you would need something like Watchtower.\nBut Podman provides a tool out of the box!\nIf you set AutoUpdate=registry, you can just run podman auto-update and Podman will check if the registry has a newer image which is compatible with the used tag.\nIn that case, the image will be pulled and the container will be restarted.\nIt is that easy 😍\nOf course, this could be dangerous if you use a tag like latest for OxiTraffic instead of a concrete version like 0.9.2.\nBecause the next version pushed to the latest tag might include a breaking change!\nIt is even more dramatic if you use the latest tag for the PostgreSQL image because manual migrations are always required when upgrading PostgreSQL to a new major version.\nTherefore, always use a tag that can't lead to a breaking change!\nTrust me, this is not only a problem with Podman updates.\nI learned this the hard way when trying to deploy Docker containers a while ago that used the latest tag.\nPersonally, I run podman auto-update manually on the server every couple of days to see what has been updated and make sure that the containers are still healthy afterwards.\nWhat about podman-compose?\nThere is the Python script podman-compose which runs Compose files with Podman.\nBut I don't consider it a long-term alternative to Docker Compose for multiple reasons:\n\npodman-compose is just a translation layer between the Compose spec and Podman with systemd. Why yet another layer of abstraction that sacrifices flexibility? It doesn't let you use all features of systemd.\nI consider Python scripts a prototype because of Python's interpreted nature. Official Podman projects are written in a compiled language like Rust or Go.\nIt is not actively maintained. The last commit was 5 months ago.\n\nQuadlet aligns much better with the rootless, daemonless design of Podman.\nIf you are stuck with Compose files and would like to try out Quadlet, check out podlet that can help you during the migration.\nFurther resources\nTake this post as an introduction.\nI highly recommend reading the man page podman-systemd.unit to get a deeper understanding of Quadlet.\nYou don't need to read the documentation of every supported option though.\nQuadlet doesn't only work with containers.\nIt can also manage pods, networks and volumes (see man page).\nIf you are new to writing systemd unit files (like me), I also recommend checking out the man pages systemd.unit and systemd.service.\npodlet is a wonderful Rust tool that can help you during the migration.\nIt can create Quadlet files out of Podman commands or even (Docker) Compose files.\nCheck out the similar blog post on blog.while-true-do for another perspective and a second example.\nIt is my favorite blog related to Linux 🥰\nFinally, if you want to see my migration as an example, then you can compare before and after.",
    "summary": {
      "en": "**Summary of Quadlet: Running Podman Containers under systemd**\n\nQuadlet is a tool that allows you to run Podman containers as systemd services, making it easier to manage containers in the background and ensuring they start automatically after a server reboot. This approach replaces the older method of using the command `podman generate systemd`, which is now deprecated.\n\n### Key Points:\n\n1. **Why Use Quadlet?**\n   - Quadlet simplifies container management by allowing you to define containers in a single configuration file instead of multiple commands.\n   - It supports all systemd features, making it easier to customize and manage dependencies between containers.\n\n2. **Setting Up Quadlet:**\n   - Create a directory at `~/.config/containers/systemd` and add a `.container` file for each container.\n   - A typical `.container` file defines the container image, ports, volumes, environment variables, and service options like restart behavior.\n\n3. **Advantages Over the Old Method:**\n   - Only one configuration file is needed instead of multiple commands, reducing complexity.\n   - Easier to handle service dependencies, ensuring the correct startup order of containers.\n\n4. **Updating Containers:**\n   - Quadlet supports automatic updates through the `AutoUpdate` feature, allowing you to easily refresh images without manual intervention.\n\n5. **Comparison with Docker Compose:**\n   - Unlike Docker Compose, which groups multiple containers in one file, Quadlet uses separate files for each container, which some users find less complex as it focuses on individual container configurations.\n   - A tool named `podlet` can help transition from Docker Compose to Quadlet.\n\n6. **Further Learning:**\n   - For a deeper understanding, refer to the man pages for `podman-systemd.unit` and `systemd` documentation.\n   - `podlet` can assist in creating Quadlet files from existing Podman commands or Docker Compose files.\n\nIn conclusion, Quadlet provides a more flexible and powerful alternative to managing Podman containers, especially for users who prefer using systemd for service management.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "19e0e815410e9b28",
    "title": {
      "en": "A proof checker meant for education",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://jsiek.github.io/deduce/index.html",
    "score": 139,
    "by": "alabhyajindal",
    "time": 1742557657,
    "content": "A proof checker meant for education\n                Teaching correctness proofs of functional programs to students.\n\n                    Get Started\n                    Live Code",
    "summary": {
      "en": "This text is about a proof checker designed for educational purposes. Its main goal is to help students learn how to create correctness proofs for functional programming. The text also encourages users to get started and offers a live coding feature.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "542f250f45f190b5",
    "title": {
      "en": "Langfuse (YC W23) Is Hiring in Berlin, Germany",
      "ko": null,
      "ja": null
    },
    "type": "job",
    "url": "https://langfuse.com/careers",
    "score": 1,
    "by": "clemo_ra",
    "time": 1742810172,
    "content": "Join Us!\nLangfuse is building the leading open-source LLM engineering platform (Why Langfuse?).\nWhile LLMs improve a lot, we don’t see enough applications in production. Building these applications requires a new workflow of continuous monitoring and evaluation that we enable with Langfuse.\nWe are seeing strong traction over recent months (see metrics below), thus it is the right time to grow the team to build out our backend systems, product, and how we communicate with developers.\nWe are hiring across for engineering and technical go-to-market roles. We work in-person in Berlin, Germany & are building an excellent, small team. If you are considering new opportunities — even if you might not be looking immediately, we would love to hear from you & get coffee.\nWe have raised a $4M seed round from Lightspeed, General Catalyst, Y Combinator and angels. We are growing fast (see metrics below) and work with some of the best AI teams such as Samsara, Twilio, KhanAcademy and Rocket Money.\nLearn more about Langfuse\nDocumentation\n10 min product walkthrough\nInteractive demo\nOrigin story\nLangfuse 2.0 scope\nEngineering blog on Langfuse v3\n\nTeam\n\nClemens Rawert, @rawert, Linkedin\nMarc Klingen, @marcklingen, GitHub, Linkedin\nMarlies Mayerhofer, @marliessophie, GitHub, LinkedIn\nMax Deichmann, @maxdeichmann, GitHub, Linkedin\nHassieb Pakzad, @hassiebpakzad, GitHub, LinkedIn\nSteffen Schmitz, GitHub, LinkedIn\nJannik Maierhöfer, @JMaierhoefer, GitHub, LinkedIn\n\nA few tidbits that might help you understand what kind of company we are:\n\nWe have 2 scheduled meetings per week: 15 min planning on Monday, 60 min demo on Friday\nWe are a very high-trust environment, Reed Hastings’ ‘No Rules Rules’ inspires our culture, here’s a summary\nMost team members have led teams before and are excited about being an IC again to ship more\nWe were heavily influenced by our time at Y Combinator and follow its ethos. We’re a Silicon Valley Startup in Berlin - more here\n\nPS: We are based in Berlin (Germany) and work in person. Come join us build one of the most exciting open source devtool companies in Europe!\nRoles\nCan’t find a role that’s a perfect fit? If you are excited about delivering\nexceptional developer experiences and building a high-quality open-source\nproduct alongside an insanely motivated team that ships, feel\nfree to reach out: careers@langfuse.com\n\nDesign Engineer\n\nBerlin, €70K - €130K EUR, 0.1% - 0.35%\nYou are a design-minded frontend engineer who is passionate about building beautiful, functional, and user-friendly interfaces. You own UI/UX in the Langfuse application to help our users achieve their goals and create how thousands of teams build their LLM applications.\n\nBackend Engineer\n\nBerlin, €70K - €130K EUR, 0.1% - 0.35%\nYou will work on our backend systems, powering the core infrastructure of Langfuse. You work on a data-intensive application that is used by hundreds of companies and thousands of developers ingesting hundreds of millions of events.\nThe technical blog post on the v3 release is a great example of what you could have worked on.\n\nProduct Engineer\n\nBerlin, €70K - €130K EUR, 0.1% - 0.35%\nDesign, scope, ship, and launch new full-stack (UI, backend, SDKs, integrations) features on top of our existing data platform. You’ll own what you build truly end-to-end from first conversations with users to operating it in production.\nSee launch weeks (#1, #2) and changelog for a good selection of things you could have worked on.\n\nDeveloper Advocate\n\nBerlin, €70K - €130K EUR, 0.1% - 0.35%\nRun our bottom-up go-to-market via developer-focused content, documentation, partnerships, events, and community-building. Langfuse is a platform of many building blocks and there are lots of common workflows we can educate on to drive awareness and adoption.\nExamples: Hugging Face and Posthog partnerships, LLM Security Monitoring guides/examples, AWS partnership, our YouTube channel\n\nFounding GTM Engineer\n\nBerlin, €70K - €130K EUR, 0.1% - 0.35%\nWork closely with our enterprise customers throughout the inbound sales cycle. Help them resolve complex technical questions, succeed in POCs, and expand after initial adoption.\nBuild ressources and improve the process to further increase scalability of our GTM.\n\nPlease apply via Work At a Startup or send us an email with your CV/LinkedIn to careers@langfuse.com\nYou need to be located in Berlin, Germany or willing to relocate. The first few interviews can be done remotely via Zoom. In the final stage, we will work one day onsite together (usually on an actual high-priority project).\nWhy Join Langfuse?\n\nLangfuse HQ, Berlin (Mitte), Germany\nImpactful work/product for awesome OSS community and users. We get to work with extremely smart users in our OSS community who share a ton of great feedback, and it feels great to help them build exciting products. You’ll also get to work with many large enterprises and YC companies who monitor and improve their LLM products with Langfuse. Community channels: GitHub Discussions, GitHub Issues, Discord, recordings of community town halls\nStrong in-person engineering culture. Almost everyone on the team works on the product; our go-to-market (also Enterprise) is exclusively inbound, and we enjoy building a developer tool together with our open-source community.\nInteresting challenges. Building an open source developer tool with a strong observability component is engineering-wise highly interesting due the various integrations and SDKs, the chance to design the best LLM Engineering workflows for these teams, and the necessity to scale our backend-logic and infrastructure.\nGrowth & Ownership. We believe that a product like ours is best built by a small group of excellent engineers who fully own the components that they build. This includes running them in production, product feedback and roadmap planning, and design. We are a small team that’s moving fast (changelog) with a focus on great developer experience.\nShared curiosity and interest. If you are interested in working with engineering teams who rethink what’s possible when using LLMs to power new use cases, come work with us as we all share this excitement and get to see the leading edge of what great teams build every day. Next to shipping quickly this is the most fun part for sure.\nPublic Metrics\n\nOpen dashboard in new tab ↗Last updated on March 7, 2025Was this page useful?YesCould be betterQuestions? We're here to helpGitHub Q&AGitHubChat EmailTalk to salesSubscribe to updatesGetupdates",
    "summary": {
      "en": "**Join Us at Langfuse!**\n\nLangfuse is creating a top open-source platform for LLM (Large Language Model) engineering. Despite advancements in LLMs, there aren't enough real-world applications. We provide the tools for continuous monitoring and evaluation to help build these applications.\n\nWe're experiencing rapid growth and are expanding our team in Berlin, Germany. We are looking for engineers and technical marketing roles. If you're interested, even casually, we invite you to connect with us over coffee.\n\nWe’ve raised $4M from notable investors, including Lightspeed and Y Combinator, and we work with leading AI teams like Twilio and Khan Academy.\n\n**Why Work with Us?**\n- **Collaborative Environment:** We have two weekly meetings to plan and showcase our work. Our culture is based on high trust, inspired by Reed Hastings’ principles.\n- **Team Experience:** Most team members have leadership experience and are eager to contribute directly to projects.\n- **Exciting Work:** Join us in building an open-source developer tool that addresses complex engineering challenges.\n\n**Available Roles:**\n- **Design Engineer:** Focus on creating user-friendly interfaces.\n- **Backend Engineer:** Develop and maintain our data-intensive backend systems.\n- **Product Engineer:** Design and launch new features from start to finish.\n- **Developer Advocate:** Engage the community and create educational content.\n- **Founding GTM Engineer:** Support enterprise customers and improve our go-to-market strategies.\n\nTo apply, send your CV or LinkedIn profile to careers@langfuse.com. Candidates must be based in Berlin or willing to relocate.\n\n**Why Join Langfuse?**\n- Work on impactful projects within a vibrant open-source community.\n- Enjoy a strong in-person team culture centered on product development.\n- Face interesting engineering challenges with opportunities for growth and ownership.\n- Collaborate with teams exploring innovative LLM applications.\n\nWe look forward to hearing from you!",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "b1a4fad7c98c2484",
    "title": {
      "en": "Mruby/C and picoruby: high level langs in low-level devices?",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/picoruby/picoruby",
    "score": 68,
    "by": "rahil627",
    "time": 1742721642,
    "content": "PicoRuby\nPicoRuby is an alternative mruby implementation which is:\n\nSmall foot print\n\nROM: 256 KB (depending on build config)\nRAM: 128 KB or less (depending on app code)\n(Figures in 32 bit architecture)\n\nPortable\n\nDepends on only standard C library such as glibc, Newlib or Newlib-nano\n\nReference microcontroller boards\n\nRaspberry Pi Pico - Arm Cortex-M0+, 264 KB RAM, 2 MB Flash\n\nAPI documentation with some demo videos\nhttps://picoruby.github.io/\n\nDepends on\n\nmruby/c (mrubyc/mrubyc): Another implementation of mruby virtual machine\n\nUsed by\n\nPRK Firmware: Keyboard firmware for Raspberry Pi Pico\n\npicoruby.wasm: PicoRuby WASI runtime for WebAssembly\n\nBuild\n\nPrerequisites\n\nC toolchain\ngit\nruby (should be CRuby 3.0+)\n\ngit clone --recursive https://github.com/picoruby/picoruby\ncd picoruby/\nrake\n# PICORUBY_DEBUG=1 rake                         # for debug build\n# PICORUBY_DEBUG=1 PICORUBY_NO_LIBC_ALLOC=1 rake  # for debug build using mruby/c's memory allocator\nbin/picoruby -e 'puts \"Hello World!\"'\n\nCross compilation\nSee an example: build_config/r2p2-cortex-m0plus.rb\nBinaries\nrake command will make three kinds of executable binary\n\nbin/picorbc\n\nbin/picorbc path/to/source.rb makes path/to/source.mrb that is VM code runs on an mruby-compatible virtual machine\n\nbin/picoruby\n\nbin/picoruby source.rb executes Ruby just like normal ruby command\nYou can do like bin/picoruby path/to/your_script.rb to run your script\n\nbin/r2p2\n\nPOSIX version of R2P2 (https://github.com/picoruby/R2P2)\n\nRoadmap\nPicoRuby is still developing halfway towards finishing as of 2024.\nSee implementation roadmap on issue/6\nContributing to PicoRuby\nFork, patch, then send a pull request.\nAcknowledgement\nPart of this project was coded by Monstarlab with the support of\nthe Ruby Association Grant Program\n2020\nand\n2021.\nSee also picoruby/picoruby/wiki.\nStargazers over time\n\nLicense\nCopyright © 2020-2024 HASUMI Hitoshi. See MIT-LICENSE for further details.\nCopyright © 2020-2021 Monstarlab. See MIT-LICENSE for further details.",
    "summary": {
      "en": "**PicoRuby Summary**\n\nPicoRuby is a lightweight implementation of mruby designed for small devices. Here are the key points:\n\n- **Size & Resources**: \n  - ROM usage is around 256 KB.\n  - RAM usage is 128 KB or less.\n  \n- **Portability**: \n  - It only requires standard C libraries like glibc or Newlib.\n  \n- **Supported Boards**: \n  - Works on microcontrollers, such as the Raspberry Pi Pico (Arm Cortex-M0+ with 264 KB RAM and 2 MB Flash).\n\n- **Dependencies**: \n  - Uses mruby/c for its virtual machine.\n\n- **Usage**: \n  - Commonly used for projects like keyboard firmware (PRK Firmware) and a WASI runtime for WebAssembly (picoruby.wasm).\n\n- **Building**: \n  - Requires a C toolchain, git, and Ruby (CRuby 3.0 or higher).\n  - You can clone the repository and build it using the `rake` command.\n\n- **Executable Binaries**: \n  - The build process creates three types of executables: \n    - `picorbc` (compiles Ruby code to VM code),\n    - `picoruby` (executes Ruby scripts),\n    - `r2p2` (a POSIX version of R2P2).\n\n- **Development Status**: \n  - PicoRuby is still under development and plans for future enhancements are available.\n\n- **Contribution**: \n  - Developers can contribute by forking the project and submitting pull requests.\n\n- **Acknowledgments**: \n  - The project received support from the Ruby Association and was partly developed by Monstarlab.\n\n- **License**: \n  - It is licensed under the MIT License. \n\nFor more information, visit the [PicoRuby documentation](https://picoruby.github.io/).",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "b8521acaff2ed2e2",
    "title": {
      "en": "Yahoo is selling TechCrunch to private equity firm Regent",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://techcrunch.com/2025/03/21/techcrunch-has-personal-news/",
    "score": 71,
    "by": "Garbage",
    "time": 1742626489,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "ba3512ae44fff864",
    "title": {
      "en": "Arc-AGI-2 and ARC Prize 2025",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://arcprize.org/blog/announcing-arc-agi-2-and-arc-prize-2025",
    "score": 171,
    "by": "gkamradt",
    "time": 1742848530,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "55139fbaabff0af1",
    "title": {
      "en": "Aiter: AI Tensor Engine for ROCm",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://rocm.blogs.amd.com/software-tools-optimization/aiter:-ai-tensor-engine-for-rocm™/README.html",
    "score": 176,
    "by": "hochmartinez",
    "time": 1742724713,
    "content": "AITER: AI Tensor Engine For ROCm#\n\n.author_string {\n    font-size: 14px;\n    font-weight: 300;\n    line-height: 22.5px;\n    text-transform: none;\n    margin-bottom: 8px;\n    font-family: var(--pst-font-family-base);\n    display: inline-block;\n    vertical-align: middle;\n}\n\n.bd-main .bd-content .bd-article-container {\n    max-width: 1750px !important;\n    width: 100% !important;\n    margin: 0 auto !important;\n}\n\n@media (min-width: 1200px) {\n    .bd-main .bd-content .bd-article-container .bd-article {\n        padding-left: 2rem;\n    }\n}\n\n.bd-container {\n    max-width: 1750px !important;\n    width: 100% !important;\n    margin: 0 auto !important;\n}\n\n.author_string {\n\n    font-size: 14px;\n\n    font-weight: 300;\n\n    line-height: 22.5px;\n\n    text-transform: none;\n\n    margin-bottom: 8px;\n\n    font-family: var(--pst-font-family-base);\n\n    display: inline-block;\n\n    vertical-align: middle;\n\n}\n\n.author_string_right {\n    font-size: 14px;\n    font-weight: 300;\n    line-height: 22.5px;\n    text-transform: none;\n    margin-bottom: 8px;\n    font-family: var(--pst-font-family-base);\n    display: inline-block;\n    vertical-align: middle;\n    flex-direction: row-reverse;\n}\n\n.author_string:last-child {\n    margin-bottom: 0px;\n}\n\n.author_string svg {\n    margin-right: 5px;\n}\n\n.bd-sidebar-primary {\n    display: none;\n}\n\n.sidebar-toggle {\n    display: none;\n}\n\nlabel.sidebar-toggle.primary-toggle.btn.btn-sm {\n    display: none;\n}\n\n.author_row {\n    display: flex;\n    justify-content: space-between;\n    align-items: flex-start;\n    width: 100%;\n    margin-bottom: 3px;\n    padding: 2px;\n}\n\n.author_row_right {\n    display: flex;\n    justify-content: space-between;\n    align-items: flex-start;\n    width: 100%;\n    margin-bottom: 3px;\n    padding: 2px;\n    flex-direction: row-reverse;\n}\n\n.author_right {\n    display: flex;\n    flex-direction: column-reverse;\n    max-width: 50%;\n    word-wrap: break-word;\n    align-items: right;\n}\n\n.author_left {\n    display: flex;\n    flex-direction: column-reverse;\n    max-width: 50%;\n    word-wrap: break-word;\n    align-items: left;\n}\n\n.image-container {\n    display: flex;\n    align-items: center;\n    justify-content: center;\n    padding: 5px;\n}\n\n.responsive-image {\n    width: 100%;\n    height: 480px;\n    object-fit: cover;\n    border: 5px solid #333;\n    box-shadow: 0 0 10px rgba(0,0,0,0.5);\n}\n\n             March 21, 2025 by Shekhar Pandey, Liz Li, Carlus Huang, Lingpeng Jin, Anshul Gupta.\n\n            5 min read. | 1331 total words.\n\n            Software tools & optimizations\n\n            AI/ML\n\n.icon-bar.fixed,\n.icon-bar.horizontal {\n    display: none;\n}\n\n@media screen and (min-width: 1520px) {\n    .icon-bar.fixed {\n        display: flex;\n        position: fixed;\n        top: 50%;\n        right: 0;\n        transform: translateY(-50%);\n        flex-direction: column;\n        z-index: 1000;\n    }\n    .icon-bar.fixed a {\n        display: block;\n        text-align: center;\n        padding: 16px;\n        font-size: 20px;\n        color: white;\n        transition: background-color 0.3s, color 0.3s;\n        background-size: 25px 25px;\n    }\n    .icon-bar.fixed a svg {\n        width: 25px;\n        height: 25px;\n    }\n    .icon-bar.fixed a:hover {\n        background-color: #000;\n    }\n}\n\n@media screen and (max-width: 1520px) {\n    .icon-bar.horizontal {\n        display: flex;\n        flex-direction: row;\n        justify-content: flex-start;\n        margin: 20px 0;\n    }\n    .icon-bar.horizontal a {\n        width: 30px;\n        height: 30px;\n        display: flex;\n        align-items: center;\n        justify-content: center;\n        font-size: 14px;\n        color: white;\n        margin-left: 8px;\n        transition: background-color 0.3s, color 0.3s;\n        background-size: 30px 30px;\n        padding: 4px;\n    }\n    .icon-bar.horizontal a svg {\n        width: 20px;\n        height: 20px;\n    }\n    .icon-bar.horizontal a:first-child {\n        margin-left: 0;\n    }\n    .icon-bar.horizontal a:hover {\n        background-color: #000;\n    }\n}\n\n.facebook {\n    background: #3B5998;\n}\n.twitter {\n    background: #55ACEE;\n}\n.reddit {\n    background: #dd4b39;\n}\n.linkedin {\n    background: #007bb5;\n}\n.google {\n    background: #bb0000;\n}\n\nPerformance optimization is critical when working with GPUs, especially for tasks involving artificial intelligence, which can be extremely demanding. To fully leverage the capabilities of advanced hardware, it’s essential to master optimization strategies and ensure every available resource is utilized efficiently. In this blog we will provide an overview ofAMD’s AI Tensor Engine for ROCm (AITER) and show you how easy it is to integrate AITER kernels in basic LLM training and inference workload.AITER helps developers to focus on creating operators while allowing customers to seamlessly integrate this operator collection into their own private, public, or any custom framework.\n\nWhat is AI Tensor Engine for ROCm (AITER)#\nAMD is introducing the AI Tensor Engine for ROCm (AITER), acentralized repository filled with high-performance AI operators [1]designed to accelerate various AI workloads. AITER serves as a unified platform where customers can easily find and integrate optimized operators into their existing frameworks—be it private, public, or custom-built as you can see in Figure 1 below. With AITER, AMD simplifies the complexity of optimization, enabling users to maximize performance while providing flexibility to meet diverse AI requirements.\n\nFigure 1: Block level diagram of AITER#\n\nKey Features#\n\nVersatile and User-Friendly Design: AITER’s architecture is carefully crafted for versatility and ease of use, allowing seamless integration into various workflows and systems.\nDual Programming Interfaces: At the highest abstraction level, AITER supports two primary interfaces—C++ and Python (Torch API). This dual-interface approach makes AITER highly accessible, catering to developers with different programming preferences and skillsets.\nRobust Kernel Infrastructure: Underneath the user-level APIs, AITER employs a powerful and robust kernel infrastructure. This infrastructure is built upon a variety of underlying technologies, including Triton, CK (Compute Kernel), ASM (Assembly), and HIP (Heterogeneous Interface for Portability).\nComprehensive Kernel Support: The AITER kernel ecosystem efficiently supports diverse computational tasks such as inference workloads, training kernels, GEMM (General Matrix Multiplication) operations, and communication kernels. Such comprehensive kernel support ensures that users can confidently handle complex and resource-intensive AI tasks.\nCustomizable and Optimizable Kernel Ecosystem: With its rich kernel environment, AITER allows developers to perform customized optimizations tailored specifically to their applications. This flexibility helps developers to bypass or overcome architectural limitations, resulting in significantly enhanced performance and adaptability.\nSeamless Integration with AMD ROCm: At its core, AITER leverages AMD’s ROCm, ensuring efficient bridging between optimized kernels and AMD GPUs. This integration unlocks the full potential and peak performance of AMD GPUs, delivering optimal efficiency across a wide range of AI workloads.\n\nBy combining user-friendly interfaces, extensive kernel capabilities, and robust GPU integration, AITER empowers developers to achieve maximum efficiency and performance in their AI applications.\n\nPerformance Gains with AITER#\nBy leveraging AITER’s advanced optimizations, users can experience significant performance improvements across various AI operations:\n\nAITER block-scale GEMM: Achieves up to 2x performance boost [2], substantially accelerating general matrix multiplication tasks.\nAITER block-scale fused MoE: Delivers up to 3x performance boost [3], optimizing the efficiency of Mixture of Experts (MoE) operations.\nAITER MLA for decode: Provides an impressive up to 17x performance boost[4], dramatically enhancing decoding efficiency.\nAITER MHA for prefill: Realizes up to 14x performance boost[5], significantly improving Multi-Head Attention (MHA) performance during prefill stages.\n\nNote: As of this blog’s publication, AITER currently provides a performance boost for DeepSeek models on Instinct GPUs via the vLLM and SGLang frameworks. Support for additional       state-of-the-art models is currently under active development and will be added in future vLLM and SGLang Docker releases.\n\nAITER’s Integration in vLLM/SGLang for DeepSeek V3/R1#\nThe integration of AITER into vLLM/SGLang for the DeepSeek v3/r1 model has led to remarkable improvements in total token throughput (tokens per second, tok/s). Before AITER’s integration, the throughput stood at 6484.76 tok/s. After incorporating AITER’s optimizations, throughput dramatically increased to 13704.36 tok/s, marking more than a 2x improvement[6] in processing speed as shown in Figure 2 below.\n\nFigure 2.Throughput Comparison: Before and After Integrating AITER in SGLang on DeepSeek Models on AMD Instinct™ MI300X.#\n\nRunning Deepseek with AITER#\nusing vLLM\nVLLM_SEED=42 VLLM_MLA_DISABLE=0 VLLM_USE_TRITON_FLASH_ATTN=0 \\\nVLLM_USE_ROCM_FP8_FLASH_ATTN=0 VLLM_FP8_PADDING=1 VLLM_USE_AITER_MOE=1 \\\nVLLM_USE_AITER_BLOCK_GEMM=1 VLLM_USE_AITER_MLA=0 vllm serve \\\n\"deepseek-ai/DeepSeek-V3\" \\\n  --host 0.0.0.0 \\\n  --port 8000 \\\n  --api-key abc-123 \\\n  --tensor-parallel-size 8 \\\n  --trust-remote-code \\\n  --seed 42\n\n  Copy to clipboard\n\nusing SGLang\nCK_BLOCK_GEMM=1 SGLANG_ROCM_AITER_BLOCK_MOE=1 RCCL_MSCCL_ENABLE=0 \\\nDEBUG_HIP_BLOCK_SYN=1024 GPU_FORCE_BLIT_COPY_SIZE=64 \\\npython3 -m sglang.launch_server  --model \"deepseek-ai/DeepSeek-V3\" \\\n --tp 8 --trust-remote-code\n\n  Copy to clipboard\n\nGetting Started with AITER#\nTo begin working with AITER, follow these simple installation steps:\n\nClone the repository:\n\ngit clone https://github.com/ROCm/aiter.git\ncd AITER\n\n  Copy to clipboard\n\nUnder the AITER root directory, run the following command to install the library in development mode:\n\npython3 setup.py develop\n\n  Copy to clipboard\n\nImplementing a Simple Linear Layer Using AITER#\nLet’s demonstrate how you can implement a simple replica of PyTorch’s linear layer using AITER’s tgemm function.\nfrom aiter.tuned_gemm import tgemm\nimport torch\n\nclass LinearLayer(torch.nn.Module):\n\tdef __init__(self, in_features, out_features):\n\t\tsuper(LinearLayer, self).__init__()\n\t\tself.weight = torch.nn.Parameter(torch.randn(out_features, in_features).cuda())\n\t\tself.bias = torch.nn.Parameter(torch.randn(out_features).cuda())\n\n\tdef forward(self, input):\n\t\tinput = input.cuda()\n\t\treturn tgemm.mm(input, self.weight, self.bias, None, None)\n\n# Define input size and layer size\nin_features = 128\nout_features = 64\nbatch_size = 32\n\n# Create custom AITER linear layer\nlayer = LinearLayer(in_features, out_features).cuda()\ninput_tensor = torch.randn(batch_size, in_features).cuda()\n\n# Get output from AITER linear layer\noutput_aiter = layer(input_tensor)\n\n# Create PyTorch linear layer with same weights and bias\npytorch_layer = torch.nn.Linear(in_features, out_features).cuda()\npytorch_layer.weight = torch.nn.Parameter(layer.weight.clone())\npytorch_layer.bias = torch.nn.Parameter(layer.bias.clone())\n\n# Get output from PyTorch linear layer\noutput_pytorch = pytorch_layer(input_tensor)\n\n# Compare outputs\nprint(\"Output difference (max absolute error):\", torch.max(torch.abs(output_aiter - output_pytorch)))\nprint(\"Output difference (mean absolute error):\", torch.mean(torch.abs(output_aiter - output_pytorch)))\n\n  Copy to clipboard\n\nIt can be very simple using AITER in daily workload, some of the other low level kernel APIs are mentioned as below which can be used to integrate in your architecture.\n\nKernel\nAPI\n\nMHA (Flash Attention)\naiter.flash_attn_func()\n\nLayerNorm\naiter.layer_norm()\n\nLayerNormFusedResidualAdd\naiter.layernorm2d_with_add_asm()\n\nRoPE forward\naiter.rope_fwd()\n\nRoPE backward\naiter.rope_bwd()\n\nRMSNorm\naiter.rms_norm()\n\nMLA Decode\naiter.ops.triton.mla_decode()\n\nAITER is not just limited to the above mentioned APIs, there are a lot of features available as mentioned in the below table and a lot are coming very soon.\n\nFeature\nType (F=Forward, B=Backward)\nDetails\n\nPrefill Attention\nF/B\nFav3 FWD FP16/BF16  Fav3 BWD FP16/BF16  MLA FP16/BF16  FA FP8 FWD+BWD (Block-Scale)  Chunked-prefill\n\nDecode Attention\nF\nPaged Attention FP16/BF16  Paged Attention FP8 per-tensor quant  Paged Attention FP8/INT8 with KV per-token quant  KVCache Update & Rotary  Batched Decoding  MLA Decoding\n\nFused-Moe\nF\nMoe-Sorting kernel and tiling solution  FP16/BF16 per-token Fused-Moe  FP8/INT8 per-token Fused-Moe  FP8 per-tensor Fused-Moe  FP8/INT4 per-tensor Fused-Moe  Fused-FFN\n\nLow Precision Gemm\nF\nFP8 per-token/channel Gemm  FP8 Block Scale Gemm  INT8 weight-only Gemm\n\nDistributed Gemm\nF/B\nDistributed GEMM\n\nNormalization and Fusion\nF\nLayernorm+quant/shortcut  RMSNorm+quant/shortcut\n\nCustom Comm.\nF\nAR/AG fused with normalization  AR/AG quantized, Optimized hipgraph support\n\nConv2d/2d\nF/B\nFP16/BF16 fwd/bwd/wrw  Fusion with bias/activation, etc.\n\nSummary#\nIn this blog we introduced AMD’s AI Tensor Engine for ROCm (AITER), our centralized high performance AI operators repository, designed to significantly accelerate AI workloads on AMD GPUs. AITER has already demonstrated its value by substantially accelerating AI workloads and significantly improving efficiency and performance. AMD remains committed to continuous innovation, with numerous further enhancements and optimization efforts currently underway. The roadmap includes even greater advancements, which promise to set new standards in AI computation. Stay tuned as AMD continues to push the boundaries of performance, ensuring machine learning engineers can consistently achieve faster, more efficient, and more powerful AI solutions.\n\nUpdated on 24 March 2025\nA note around the current scope of AITER was added under the Performance Gains with AITER section.\n\nAdditional Resources#\nAITER Github: ROCm/aiter\n\nDisclaimers#\nThird-party content is licensed to you directly by the third party that owns the\ncontent and is not licensed to you by AMD. ALL LINKED THIRD-PARTY CONTENT IS\nPROVIDED “AS IS” WITHOUT A WARRANTY OF ANY KIND. USE OF SUCH THIRD-PARTY CONTENT\nIS DONE AT YOUR SOLE DISCRETION AND UNDER NO CIRCUMSTANCES WILL AMD BE LIABLE TO\nYOU FOR ANY THIRD-PARTY CONTENT. YOU ASSUME ALL RISK AND ARE SOLELY RESPONSIBLE\nFOR ANY DAMAGES THAT MAY ARISE FROM YOUR USE OF THIRD-PARTY CONTENT.\n\n[1]\nAI operators :  optimized mathematical functions or computational kernels that perform fundamental AI and machine learning tasks, such as matrix multiplications, convolutions, and activations, which are crucial for accelerating AI workloads.\n\n[2]\nOn average, a system configured with an AMD Instinct™ MI300X GPU shows that AITER block-scale GEMM offers a 2x performance boost substantially accelerating general matrix multiplication tasks. Testing done by AMD on 03/011/2025, results may vary based on configuration, usage, software version, and optimizations.\nSYSTEM CONFIGURATION: AMD Instinct™ MI300X platform\n- System Model: Supermicro GPU A+ Server AS - 8125GS-TNMR2\n- CPU: 2x AMD EPYC 9654 96-core Processor (2 sockets, 96 cores per socket, 2 threads per core)\n- NUMA Config: 2 NUMA node per socket\n- Memory: 2.3 TiB (24 DIMMs, 4800 mts, 96 GiB/DIMM)\n- Disk: Root drive + Data drive combined:\n- 2x 960GB Samsung MZ1L2960HCJR-00A07\n- 4x 3.84TB Samsung MZQL23T8HCLS-00A07\n- GPU: 8x AMD MI300X 192GB HBM3 750W\n- Host OS: Ubuntu 22.04.4 LTS with Linux kernel 5.15.0-116-generic.\n- System BIOS: 3.2\n- System Bios Vendor:   American Megatrends International, LLC.\n- Host GPU Driver (amdgpu version): 6.10.5\n\n[3]\nOn average, a system configured with an AMD Instinct™ MI300X GPU shows AITER block-scale fused MoE offers a 3x performance boost, optimizing the efficiency of Mixture of Experts (MoE) operations.  Testing done by AMD on 03/011/2025, results may vary based on configuration, usage, software version, and optimizations.\nSYSTEM CONFIGURATION: AMD Instinct™ MI300X platform\n- System Model: Supermicro GPU A+ Server AS - 8125GS-TNMR2\n- CPU: 2x AMD EPYC 9654 96-core Processor (2 sockets, 96 cores per socket, 2 threads per core)\n- NUMA Config: 2 NUMA node per socket\n- Memory: 2.3 TiB (24 DIMMs, 4800 mts, 96 GiB/DIMM)\n- Disk: Root drive + Data drive combined:\n- 2x 960GB Samsung MZ1L2960HCJR-00A07\n- 4x 3.84TB Samsung MZQL23T8HCLS-00A07\n- GPU: 8x AMD MI300X 192GB HBM3 750W\n- Host OS: Ubuntu 22.04.4 LTS with Linux kernel 5.15.0-116-generic.\n- System BIOS: 3.2\n- System Bios Vendor:   American Megatrends International, LLC.\n- Host GPU Driver (amdgpu version): 6.10.5\n\n[4]\nOn average, a system configured with an AMD Instinct™ MI300X GPU shows that AITER MLA for decode offers a 17x performance boost enhancing decoding efficiency.  Testing done by AMD on 03/011/2025, results may vary based on configuration, usage, software version, and optimizations.\nSYSTEM CONFIGURATION: AMD Instinct™ MI300X platform\n- System Model: Supermicro GPU A+ Server AS - 8125GS-TNMR2\n- CPU: 2x AMD EPYC 9654 96-core Processor (2 sockets, 96 cores per socket, 2 threads per core)\n- NUMA Config: 2 NUMA node per socket\n- Memory: 2.3 TiB (24 DIMMs, 4800 mts, 96 GiB/DIMM)\n- Disk: Root drive + Data drive combined:\n- 2x 960GB Samsung MZ1L2960HCJR-00A07\n- 4x 3.84TB Samsung MZQL23T8HCLS-00A07\n- GPU: 8x AMD MI300X 192GB HBM3 750W\n- Host OS: Ubuntu 22.04.4 LTS with Linux kernel 5.15.0-116-generic.\n- System BIOS: 3.2\n- System Bios Vendor:   American Megatrends International, LLC.\n- Host GPU Driver (amdgpu version): 6.10.5\n\n[5]\nOn average, a system configured with an AMD Instinct™ MI300X GPU with AITER MHA for prefill shows a14x performance boost, improving Multi-Head Attention (MHA) performance during prefill stages.Testing done by AMD on 03/011/2025, results may vary based configuration, usage, software version, and optimizations.\nSYSTEM CONFIGURATION: AMD Instinct™ MI300X platform\n- System Model: Supermicro GPU A+ Server AS - 8125GS-TNMR2\n- CPU: 2x AMD EPYC 9654 96-core Processor (2 sockets, 96 cores per socket, 2 threads per core)\n- NUMA Config: 2 NUMA node per socket\n- Memory: 2.3 TiB (24 DIMMs, 4800 mts, 96 GiB/DIMM)\n- Disk: Root drive + Data drive combined:\n- 2x 960GB Samsung MZ1L2960HCJR-00A07\n- 4x 3.84TB Samsung MZQL23T8HCLS-00A07\n- GPU: 8x AMD MI300X 192GB HBM3 750W\n- Host OS: Ubuntu 22.04.4 LTS with Linux kernel 5.15.0-116-generic.\n- System BIOS: 3.2\n- System Bios Vendor:   American Megatrends International, LLC.\n- Host GPU Driver (amdgpu version): 6.10.5\n\n[6]\nOn average, a system configured with an AMD Instinct™ MI300X GPU shows that before AITER’s integration, the throughput stood at 6484.76 tok/s. After incorporating AITER’s optimizations, throughput dramatically increased to 13704.36 tok/s, marking more than a 2x improvement in processing speed. Testing done by AMD on 03/11/2025, results may vary based on configuration, usage, software version, and optimizations.\nSYSTEM CONFIGURATION: AMD Instinct™ MI300X platform\n- System Model: Supermicro GPU A+ Server AS - 8125GS-TNMR2\n- CPU: 2x AMD EPYC 9654 96-core Processor (2 sockets, 96 cores per socket, 2 threads per core)\n- NUMA Config: 2 NUMA node per socket\n- Memory: 2.3 TiB (24 DIMMs, 4800 mts, 96 GiB/DIMM)\n- Disk: Root drive + Data drive combined:\n- 2x 960GB Samsung MZ1L2960HCJR-00A07\n- 4x 3.84TB Samsung MZQL23T8HCLS-00A07\n- GPU: 8x AMD MI300X 192GB HBM3 750W\n- Host OS: Ubuntu 22.04.4 LTS with Linux kernel 5.15.0-116-generic.\n- System BIOS: 3.2\n- System Bios Vendor:   American Megatrends International, LLC.\n- Host GPU Driver (amdgpu version): 6.10.5",
    "summary": {
      "en": "**Summary of AMD's AI Tensor Engine for ROCm (AITER)**\n\nAITER is AMD's new AI Tensor Engine designed to enhance performance for artificial intelligence tasks on AMD GPUs. It provides a centralized collection of high-performance AI operators that are easy to integrate into various frameworks—private, public, or custom. \n\n**Key Features:**\n- **User-Friendly Design:** AITER is built for versatility and ease of integration.\n- **Dual Programming Interfaces:** Supports both C++ and Python (Torch API), catering to different developer preferences.\n- **Strong Kernel Infrastructure:** Utilizes advanced technologies like Triton and HIP to support various computational tasks.\n- **Comprehensive Kernel Support:** Handles tasks like inference and training efficiently, ensuring high performance for demanding AI workloads.\n- **Customizable Kernels:** Developers can optimize kernels specifically for their needs, enhancing performance.\n\n**Performance Improvements:**\nAITER offers substantial boosts in performance, such as:\n- Up to **2x** faster general matrix multiplication.\n- Up to **3x** faster efficiency for Mixture of Experts operations.\n- Up to **17x** faster decoding efficiency.\n- Up to **14x** faster Multi-Head Attention performance during prefill.\n\n**Integration and Use:**\nAITER has been integrated into models like DeepSeek, significantly increasing processing speed from 6484.76 tokens per second to 13704.36 tokens per second after optimization.\n\n**Getting Started:**\nDevelopers can start using AITER by cloning the repository, installing the library, and implementing simple functions with its APIs.\n\nIn conclusion, AITER aims to provide powerful tools for developers, facilitating faster and more efficient AI solutions while AMD continues to innovate and improve its offerings in the AI space.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "9c8dde4906b72fde",
    "title": {
      "en": "Gatehouse – a composable, async-friendly authorization policy framework in Rust",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/thepartly/gatehouse",
    "score": 68,
    "by": "hardbyte",
    "time": 1742856647,
    "content": "Gatehouse\n\nA flexible authorization library that combines role-based (RBAC), attribute-based (ABAC), and relationship-based (ReBAC) access control policies.\n\nFeatures\n\nMulti-paradigm Authorization: Support for RBAC, ABAC, and ReBAC patterns\nPolicy Composition: Combine policies with logical operators (AND, OR, NOT)\nDetailed Evaluation Tracing: Complete decision trace for debugging and auditing\nFluent Builder API: Construct custom policies with a PolicyBuilder.\nType Safety: Strongly typed resources/actions/contexts\nAsync Ready: Built with async/await support\n\nCore Components\nPolicy Trait\nThe foundation of the authorization system:\n#[async_trait]\ntrait Policy<Subject, Resource, Action, Context> {\n    async fn evaluate_access(\n        &self,\n        subject: &Subject,\n        action: &Action,\n        resource: &Resource,\n        context: &Context,\n    ) -> PolicyEvalResult;\n}\n\nPermissionChecker\nAggregates multiple policies (e.g. RBAC, ABAC) with OR logic by default: if any policy grants access, permission is granted.\nlet mut checker = PermissionChecker::new();\nchecker.add_policy(rbac_policy);\nchecker.add_policy(owner_policy);\n\n// Check if access is granted\nlet result = checker.evaluate_access(&user, &action, &resource, &context).await;\nif result.is_granted() {\n    // Access allowed\n} else {\n    // Access denied\n}\n\nPolicyBuilder\nThe PolicyBuilder provides a fluent API to construct custom policies by chaining predicate functions for\nsubjects, actions, resources, and context. Once built, the policy can be added to a [PermissionChecker].\nlet custom_policy = PolicyBuilder::<MySubject, MyResource, MyAction, MyContext>::new(\"CustomPolicy\")\n    .subjects(|s| /* ... */)\n    .actions(|a| /* ... */)\n    .resources(|r| /* ... */)\n    .context(|c| /* ... */)\n    .when(|s, a, r, c| /* ... */)\n    .build();\n\nBuilt-in Policies\n\nRbacPolicy: Role-based access control\nAbacPolicy: Attribute-based access control\nRebacPolicy: Relationship-based access control\n\nCombinators\nAndPolicy: Grants access only if all inner policies allow access\nOrPolicy: Grants access if any inner policy allows access\nNotPolicy: Inverts the decision of an inner policy\nExamples\nSee the examples directory for complete demonstration of:\n\nRole-based access control (rbac_policy)\nRelationship-based access control (rebac_policy)\nPolicy combinators (combinator_policy)\n\nRun with:\ncargo run --example rbac_policy",
    "summary": {
      "en": "**Gatehouse Overview**\n\nGatehouse is an authorization library that supports different access control methods: role-based (RBAC), attribute-based (ABAC), and relationship-based (ReBAC).\n\n**Key Features:**\n- **Multi-Paradigm Authorization:** Supports RBAC, ABAC, and ReBAC.\n- **Policy Composition:** Combine policies using logical operators like AND, OR, and NOT.\n- **Evaluation Tracing:** Provides detailed tracking of decisions for debugging and auditing.\n- **Fluent Builder API:** Allows easy construction of custom policies.\n- **Type Safety:** Ensures strong typing for resources, actions, and contexts.\n- **Async Ready:** Compatible with async/await programming.\n\n**Core Components:**\n1. **Policy Trait:** The core of the system that evaluates access based on the subject, action, resource, and context.\n2. **PermissionChecker:** Combines multiple policies to determine access. If any policy grants access, permission is granted.\n3. **PolicyBuilder:** A user-friendly API that helps create custom policies by chaining functions for subjects, actions, resources, and context.\n\n**Built-in Policies:**\n- **RbacPolicy:** For role-based access control.\n- **AbacPolicy:** For attribute-based access control.\n- **RebacPolicy:** For relationship-based access control.\n\n**Combinators:**\n- **AndPolicy:** Grants access only if all policies allow it.\n- **OrPolicy:** Grants access if any policy allows it.\n- **NotPolicy:** Reverses the decision of a policy.\n\n**Examples:** \nYou can find examples of each access control method and policy combinators in the provided directory, with a command to run the RBAC example.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "b3941b721341aca7",
    "title": {
      "en": "David Lynch Presents Interview Project",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.youtube.com/channel/UC4lrTWEywA3JfR-N_dv0d4A",
    "score": 48,
    "by": "toomuchtodo",
    "time": 1742878067,
    "content": "Back\n\n  KR\n\n    Skip navigation\n\n        Search\n\n  Search with your voice\n\nSign in\n\n  KR\n\n  Home\n\n    Home\n\n  Shorts\n\n    Shorts\n\n  Subscriptions\n\n    Subscriptions\n\n  You\n\n    You\n\n  History\n\n    History\n\n          David Lynch Presents Interview Project@davidlynchpresentsIP•15.5K subscribers•158 videosMore about this channel ...more...moreinstagram.com/davidlynchinterviewprojectand 3 more linksSubscribe\n\n              HomeVideosShortsPlaylistsPosts\n\n    Search\n\n  David Lynch Presents Interview Project: TRAILERSearchInfoShoppingTap to unmute2xIf playback doesn't begin shortly, try restarting your device.•David Lynch Presents Interview ProjectSubscribeUnsubscribeYou're signed outVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.CancelConfirmShareInclude playlistAn error occurred while retrieving sharing information. Please try again later.Watch laterShareCopy link0:000:00 / 2:07•Watch full videoLive••\n\n    David Lynch Presents Interview Project: TRAILER\n\n    •\n\n    •\n\n      49,937 views\n\n      9 months ago\n\n  The Interview Project team has decided to re-release all 121 episodes of the original series on October 1, 2024  to commemorate the 15 year anniversary.\n\nEach episode will be shown in high definition, a feature that was previously unavailable to the team.\n\nFor those of you familiar with the series, we hope you enjoy revisiting them. For those new to the series, we hope you enjoy discovering them.\n\nAs David says, \"It's something that's human and you can't stay away from it.\"\n\n  Read more\n\n                Videos\n\n  3:25\n    3:25\n  Now playing\n\nDavid Lynch Presents Interview Project:  Jess\n\n32K views5 months ago\n\n  4:30\n    4:30\n  Now playing\n\nDavid Lynch Presents Interview Project:  Tommie Holliday\n\n13K views5 months ago\n\n  4:19\n    4:19\n  Now playing\n\nDavid Lynch Presents Interview Project:  Kee\n\n10K views5 months ago\n\n  3:33\n    3:33\n  Now playing\n\nDavid Lynch Presents Interview Project:  Palmer Black\n\n4.3K views5 months ago\n\n  4:03\n    4:03\n  Now playing\n\nDavid Lynch Presents Interview Project:  Gordon\n\n5.9K views5 months ago\n\n  4:46\n    4:46\n  Now playing\n\nDavid Lynch Presents Interview Project:  Clara\n\n4K views5 months ago\n\n                Created playlists\n\n    121 videosDavid Lynch Presents Interview Project PlaylistView full playlist\n\n    Shorts\n\n    David Lynch Presents Interview Project:  FRANK793 viewsDavid Lynch Presents Interview Project:  MR. SIEBERT639 viewsDavid Lynch Presents Interview Project:  ROBIN777 viewsDavid Lynch Presents Interview Project. Re-release SUMMER 2024. #davidlynch #twinpeaks #roadtrip2.3K viewsDavid Lynch Presents Interview Project:  BATSO. #davidlynch #twinpeaks #realpeople #interview941 viewsDavid Lynch Presents Interview Project:  BRENDA WRAY. #davidlynch #twinpeaks #realpeople #interviewt814 views\n\n              SearchInfoShoppingTap to unmute2xYou're signed outVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.CancelConfirmIf playback doesn't begin shortly, try restarting your device.•ShareInclude playlistAn error occurred while retrieving sharing information. Please try again later.Watch laterShareCopy link0:00 / •Watch full videoLive••Scroll for details\n\n        …\n\n      NaN / NaN",
    "summary": {
      "en": "David Lynch's Interview Project will re-release all 121 episodes of the original series on October 1, 2024, to celebrate its 15th anniversary. The episodes will be available in high definition for the first time. The creators hope both longtime fans and new viewers will enjoy the series. Lynch emphasizes the project's focus on human experiences, stating, \"It's something that's human and you can't stay away from it.\"",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "2007b33bda208fc1",
    "title": {
      "en": "An Interview with Zen Chief Architect Mike Clark",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.computerenhance.com/p/an-interview-with-zen-chief-architect",
    "score": 32,
    "by": "Smaug123",
    "time": 1742843149,
    "content": "Share this postComputer, Enhance!An Interview with Zen Chief Architect Mike ClarkCopy linkFacebookEmailNotesMoreDiscover more from Computer, Enhance!Programming courses, interviews, commentary.Over 50,000 subscribersSubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.Already have an account? Sign inAn Interview with Zen Chief Architect Mike ClarkCasey MuratoriMar 25, 202596Share this postComputer, Enhance!An Interview with Zen Chief Architect Mike ClarkCopy linkFacebookEmailNotesMore115ShareZen is one of the most important microarchitectures in the history of the x86 ecosystem. Not only is it the reigning champion in many x64 benchmarks, but it is also the architecture that enabled AMD’s dramatic rise in CPU marketshare over the past eight years: from 10% when the first Zen processor was launched, to 25% at the introduction of Zen 5.I recently had the honor of interviewing none other than Zen’s chief architect, Mike Clark. I only had 30 minutes, but I tried to fit in as many of our microarchitecture questions as I could! Subscribers to Computer Enhance will recognize many of them as ones we’ve collectively wondered about during Q&A’s in the Performance-Aware Programming series - and I’m delighted to report that, as you’ll see, Mike gave detailed answers to all of them.Below is the edited transcript of our conversation. I’ve tried to keep it as accurate as possible to the original audio, while reworking the phrasing to be appropriate for reading rather than listening. I have also had AMD approve the transcript to ensure accuracy, and I will be working with them to release an extended video version as well.Now, without further ado, my interview with Mike Clark:CASEY: You will often hear “people on the internet” say that ARM as an ISA is better for low power than x64. People like me who study ISAs tend to be skeptical of this claim. As a hardware designer, are there any specific things about the x64 ISA that you find difficult to deal with for low-power designs?MIKE: Having spent my career working on x86, I might have a bias here! I do think each ISA has its own quirks that influence some of the microarchitecture. But at the base level, we can build low-power x86 designs as well as ARM can, and ARM can build high frequency, high performance designs as well as x86 can. None of the quirks are really limiting you on the microarchitecture. The reality is that the markets we've been targeting have been different, so they've driven the architectures to optimize for different design points. ARM is in much lower power markets where x86 hasn't had the market share to chase.On the x86 side, the higher performance / higher frequency devices are the established market that our devices have to compete in, so that's where our design focus is. We could build the same Zen microarchitecture with an ARM ISA on top instead. We could deliver the same performance per watt. We don't view the ISA as a fundamental input to the design as far as power or performance.CASEY: So the memory model, whether the instructions are variable length, those sorts of things don’t factor in? None of the differences are big enough to matter?MIKE: No. It may take a little bit more microarchitectural work for us to account for stronger memory order on the x86 side, but of course the software has to account for the weaker memory ordering on the ARM side. So there are tradeoffs.Variable length instructions are harder than fixed length, but we've created techniques like the uop cache, and it also gives us better density overall by having smaller instructions. x86 can put more work into each instruction byte, so we can have denser binaries and increase performance that way.So these are all just tradeoffs in the microarchitecture. They’re not fundamental issues for delivering performance per watt at the end of the day.CASEY: Similar question, but moving to the OS side of things: does the 4k page size on x64 create problems for you architecturally by limiting the L1 cache size due to how tagging works? Would architectures like Zen benefit if x64 operating systems moved to 2mb pages as the smallest page size, or perhaps a 16k or 64k page size if you were to introduce that in a future architecture?MIKE: Definitely. We always encourage developers to use larger page sizes if they can, because it gives us a lot more capacity in our TLBs and therefore less TLB pressure overall. But we have the ability to combine 4k pages into larger pages in our TLB if the OS allocates them sequentially. We can turn four 4k pages into a 16k page if they are virtually and physically sequential. That's been a technique we've used even since the original Zen to help software get the benefits of larger page sizes without moving away from 4k pages.However, 4k to 2mb is a big jump. We're always looking for ways to allow our software partners to have larger page sizes, but maybe something in between is more appropriate.CASEY: Just to poke a little further at that, for the L1 cache specifically, you're hitting up against the limit of the address bits. Have you ever wanted to put in bigger L1 caches, but found that you couldn't because the 4k page size means you can't do that without going to a larger-way cache? MIKE: No. In the past we have built L1 caches that don't follow the “ways times 4k page size is the largest index you can have” property. There are ways to do that. We've solved those problems. It is a little bit more logic, but it's a solvable problem. It doesn’t limit us in what we design.  CASEY: Moving on to the sizes of registers and cache lines, I have two questions about how CPUs seem to do things differently than GPUs.First, CPUs seem to be settling into a natural size of 64 bytes. The L1 cache lines are 64 bytes. The registers are 64 bytes. It doesn't look like anyone's trying to go beyond that. But GPUs seem to prefer at least 128 bytes for both. Is this because of the difference in clock rates? Does it have to do with CPU versus GPU workloads? In general, do you see 64 bytes as a natural settling point for CPUs, and if so, why does it seem to be different from GPUs?MIKE: We do look at increasing the line size. We're always going to a clean sheet of paper and making sure we're rethinking things and not missing anything as workloads evolve and things change. We don't want to be locked into a mindset where we think we've proven 64 bytes to be the correct size for everything on a CPU.But the reality is that CPUs are targeted at low latency, smaller datatype, integer workloads as their fundamental value proposition. We've grown that capability with all our out-of-order engines, trying to expose ILP. So far, it’s allowed us to build vector units as wide as 64 bytes.But it's been a journey to even get that wide because if you look at, say, from Zen 4 to Zen 5 - we supported 512-bit vectors on Zen 4 via a 256-bit data path. For Zen 5, we went full bore and supported the full 512-bit data path. That required a fundamental replumbing of the microarchitecture. We had to grow the width of the delivery of data from the L2 to the L1, and we had to double the delivery from the L1 to really take advantage of the wider vector units.The integer workloads that are still primarily reading data out of the cache and branching, they're not getting any benefit from that sort of fundamental change. We have to do it in a very cautious and meticulous manner, so that those highways of delivery can exist while still ensuring that if there's only one car on the highway, we’re not burning power as if all the lanes were full. It’s tricky.When you look at the GPU side, the workloads where they excel are throughput based. Not having to excel at the lowest-latency, small-datatype workloads frees them up to leverage all that extra investment. You need to have workloads that are really focused on using that much data in a wide vector to get the return on that investment.So that's always the trick. If we try to go too big, too wide, we lose our value proposition in performance per watt for the mainstream workloads people buy our new generations for.Does that make sense?CASEY: It makes perfect sense, and it leads right into my next question.Underlying what you said is the implication that, if we as software developers were taking better advantage of wider workloads, it would be worth your while to widen them. One of the problems people often have when trying to widen a data path in software is that CPUs seem to be a lot worse at scatter/gather. It’s an important feature for taking data that isn’t naturally wide and putting it through a wide datapath with some level of efficiency. For example, if I want to widen something that does an array lookup, historically it’s been hard to port that code directly because of poor gather performance.Could you give us some insight on why this is?MIKE: That's a good question. It does tie back to the previous question in the sense that it’s really not the fundamental scatter/gather concept that’s the problem. It's the amount of bandwidth needed to pull all those different elements inside the CPU to put them together to feed to the vector unit.Again, we're focused on latency, not throughput. That has permeated itself out into the interface to what we call our “data fabric”. The memory system isn't wide enough to be able to pull all the data in so it can be assembled into lanes and operated on. If we wanted to attack that, we’d have to widen the interface, and that would come with a large power cost.So again, that's the trick. You're trying to avoid the power cost when you're running workloads that don't require scatter/gather. If you widen these paths, you’ve overbuilt the design for the the baseline workloads that you normally run. We are always trying to grow and pull more applications in, but we have to balance that against the power requirements of widening the bandwidth into the CPU.CASEY: So in other words, it's a chicken and egg problem? If software developers were giving you software that ran fantastically with scatter/gather, you’d do it. But they’re not, so it’s hard to argue for it?MIKE: Right, yes.CASEY: The rest of my questions don’t group together into any particular theme, so I’ll just go through them randomly.Random question number one: previously, on the software side, we thought nontemporal stores were solely there to prevent pollution of caches. That was our mental model. But lately we have noticed that nontemporal stores seem to perform better than regular stores, separate from cache pollution - as if the memory subsystem doesn't have to do as much work, or something similar. Is there more about nontemporal stores that we need to understand, or are we mistaken?MIKE: If you were just doing nontemporal stores to data that is in the caches, obviously that would not be a good thing. So you still have to apply good judgment on when to use nontemporal stores. But tying it back to the ARM-ISA-weakly-ordered discussion, nontemporal stores, while not exactly being weakly ordered, are in some ways easier to deal with in the base case. We can process them efficiently as long as they really are nontemporal. So I think your intuition is right - we can do well with them as long as the software side ensures that the data isn’t finding itself in caches along the way.CASEY: Random question number two: for educational purposes, does anyone publish modern CPU pipeline diagrams that would be reasonably accurate? AMD and Intel, for example, both publish flow charts for new microarchitectures, but not pipeline diagrams.MIKE: It might surprise people, but if you go back to when we did publish pipeline diagrams, those are still fine for learning how a modern CPU works. We do have more complicated pipelines today, and we don't publish them because they reveal proprietary techniques we're using, or give hints that we don't want to give to the competition. But at the end of the day, it's still a fetch block, a decode block, an execute block, a retire block... there's more stages within those blocks, and you can break it down even more than that, but the fundamental pipelining is still similar.CASEY: So, for example, I think the Bulldozer pipeline diagram was the last one I saw from AMD. It’s not woefully out of date? If someone learned that pipeline, they would be able to understand what you actually do now if they were given an updated diagram?MIKE: Roughly speaking, yes.CASEY: Random question number three: if you look at a single-uop instruction like sqrtpd that has a latency longer than the pipeline depth of an execution unit, can you give a cursory explanation of how this works for those of us on the software side who don't understand hardware very well?MIKE: One way to conceptualize it is that you could have taken sqrtpd and split it up into a bunch of different uops that can operate in parallel with dependencies along the way. It can be very expensive to keep all those operations in flight, to build the pipeline to pass the data forward so you can let something new in behind it that's working on an earlier stage. The hardware cost would be too high to create a pipeline to get the execution done in a way that allows another sqrtpd to start on an earlier stage - especially if it's going to be, say, 16 stages of execution until you have achieved your answer.It's really just that cost. Is the amount of hardware worth it to make something like sqrtpd a pipelineable instruction, or can we save a lot of power and hardware by just doing one of them at a time?CASEY: Just to make sure I understand: does that mean inside an execution unit that can do one of these, the uop gets issued and it knows it’s got something special that it has to work on for a while, so it asks not to be given anything else for several cycles while some special control part inside it takes over?MIKE: Correct. The scheduler that feeds it understands that it's not a pipeline execution unit that can take another uop every cycle. But it has a known quantity where, if it has sent one in, after some number of cycles, it knows it can send another one in and it should be safe.CASEY: So the system upstream of the execution unit - the thing that's feeding it - knows not to send more?MIKE: It knows, yes.CASEY: Last question: are there things you wish we as software developers would start - or stop - doing that would help take advantage of the hardware you design, or that would make it easier for you to design new hardware in the future?MIKE: We already hit on one, which is the feedback loop when we add new ISA components - larger vectors, for example. We need software to use them to get the return on investment that we're putting in.Of course we also understand that, as a new feature comes out, it's only on the new hardware. You want your software to run well on our old hardware as well as on our new hardware. We totally understand that problem. But still, if software developers could embrace the new features more aggressively, that would definitely help.It would be great if the software could find ways to leverage wider vectors, AI, and so on - all the areas we've invested a lot of hardware in. And of course we would also like to get feedback from you guys - “if we just had this instruction or this concept, we could really leverage that in our software” and so on. We're constantly open to that, too. We want to know how to make your lives easier.And finally, one other thing I would add is that larger basic blocks are better. Taking branches versus not taking branches can have a big effect on code flow. Try to put conditional operations in the right places. I’m sure you guys probably focus on this already.CASEY: Yes, but it’s always good to hear it from you. We only ever know that something runs faster when we time it - we can't always guess what the designers are thinking on the hardware side.MIKE: Gotcha.CASEY: Well, I think we are out of time. Thank you very much! This has been fantastic. Thank you for answering all of my questions, and please keep in touch. We always have questions like this on the software side, so anytime you want to talk, or if there is anything new you want to tell us about, please let us know.MIKE: Okay, cool. It was a great conversation. And yeah, any time you're wondering what's going on in the hardware, we want to close that gap as best we can!CASEY: We all appreciate it. And we love Zen as well! I’m conducting this interview from a Zen processor as we speak. So thank you for all your hard work over the years.MIKE: Alright, thanks! Talk to you later.If you enjoyed this article, and would like to receive more like it, you can put your email address in the box below to access both our free and paid subscription options:Subscribe96 Likes∙5 Restacks96Share this postComputer, Enhance!An Interview with Zen Chief Architect Mike ClarkCopy linkFacebookEmailNotesMore115SharePrevious",
    "summary": {
      "en": "**Summary: Interview with Mike Clark, Chief Architect of Zen**\n\nIn a recent interview, Mike Clark, the chief architect behind AMD's Zen microarchitecture, discussed the evolution and capabilities of Zen, which has significantly boosted AMD's market share in CPUs. He addressed several technical questions about the differences between x64 and ARM architectures, emphasizing that while both have unique characteristics, they can achieve similar performance and power efficiency. \n\nKey points from the interview include:\n\n1. **Architecture Flexibility**: Clark believes that both x64 and ARM architectures can be optimized for low power or high performance; the choice depends more on market focus than inherent limitations in the ISA (Instruction Set Architecture).\n\n2. **Memory Management**: Larger page sizes in operating systems (like 2MB) could benefit performance by reducing pressure on Translation Lookaside Buffers (TLBs). While 4k pages are standard, Clark encourages developers to utilize larger pages when possible.\n\n3. **Cache and Register Sizes**: CPU designs typically stick to 64 bytes for cache lines and registers due to the focus on low-latency workloads. While there are considerations for wider data paths, it must align with the types of tasks CPUs are optimized for.\n\n4. **Scatter/Gather Performance**: Historically, CPUs have struggled with scatter/gather operations due to bandwidth limitations, which can affect performance when trying to use wider data paths efficiently.\n\n5. **Nontemporal Stores**: These stores can perform better under certain conditions since they reduce cache pollution, but they need to be used judiciously to maintain efficiency.\n\n6. **Software and Hardware Collaboration**: Clark emphasized the importance of developers utilizing new hardware features to maximize performance. He also encouraged feedback from software developers on potential needs for new instructions or features.\n\nIn conclusion, the interview provided insight into how the Zen architecture has evolved and the ongoing relationship between software development and hardware design.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "26514204ccd4c95d",
    "title": {
      "en": "Samsung co-CEO Han Jong-hee has died",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.reuters.com/world/asia-pacific/samsung-electronics-says-co-ceo-han-jong-hee-has-died-cardiac-arrest-2025-03-25/",
    "score": 12,
    "by": "glimshe",
    "time": 1742887108,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "5a6b64c7bd1b498a",
    "title": {
      "en": "Supply Chain Attacks on Linux Distributions – Fedora Pagure",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://fenrisk.com/pagure",
    "score": 200,
    "by": "akyuu",
    "time": 1742414317,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "9e4f0badd03a1ead",
    "title": {
      "en": "Shift-to-Middle Array: A Faster Alternative to Std:Deque?",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/attilatorda/Shift-To-Middle_Array",
    "score": 125,
    "by": "AttilaT",
    "time": 1742772027,
    "content": "Shift-To-Middle Array\nThe Shift-To-Middle Array is a dynamic array designed to optimize insertions and deletions at both ends, offering a high-performance alternative to std::deque, std::vector, and linked lists. It achieves this while maintaining contiguous memory storage, improving cache locality and enabling efficient parallel processing.\n\n🌟 Features\n✅ Amortized O(1) insertions & deletions at both ends\n✅ Fast random access (O(1))\n✅ Better cache locality than linked lists\n✅ Supports SIMD & parallel optimizations\n✅ Efficient memory usage compared to std::deque\n📌 How It Works\nUnlike std::deque, which uses a fragmented block structure, the Shift-To-Middle Array dynamically redistributes space to avoid costly shifts. When resizing, elements are moved toward the middle, ensuring efficient insertions at both ends without excessive copying.\n🚀 Time Complexity Comparison\nThe following table compares the time complexity of Shift-To-Middle Array operations with other common data structures:\n\nOperation\nArrayList (std::vector)\nLinked List\nShift-To-Middle Array\n\nAccess (by index)\nO(1)\nO(n)\nO(1)\n\nInsertion at head\nO(n)\nO(1)\nO(1) amortized\n\nInsertion at tail\nO(1) amortized\nO(1)\nO(1) amortized\n\nInsertion in middle\nO(n)\nO(n)\nO(n)\n\nDeletion at head\nO(n)\nO(1)\nO(1) amortized\n\nDeletion at tail\nO(1)\nO(1)\nO(1) amortized\n\nDeletion in middle\nO(n)\nO(n)\nO(n)\n\nCache Locality\nExcellent\nPoor\nExcellent\n\n🏆 Performance Benchmarks\nBenchmarks comparing Shift-To-Middle Array vs. std::deque vs. ExpandingRingBuffer vs. std::queue demonstrate that performance improvements depend on CPU and GPU capabilities, such as multi-core parallelism, SIMD optimizations, and cache efficiency.\nThe benchmarks were compiled using GCC with the -O3 optimization flag, ensuring high-performance execution. Results vary based on hardware specifications and workload characteristics.\n📂 Installation & Usage\nTo use Shift-To-Middle Array in your project:\n#include \"ShiftToMiddleArray.h\"\nShiftToMiddleArray<int> stmArray;\nstmArray.insert_head(42);\nstmArray.insert_tail(99);\nint value = stmArray.get_head();\nstmArray.remove_head();\n\n🔬 When To Use\n\nHigh-performance queue structures\nGame engines & real-time applications\nNetworking (packet buffering, event queues)\nDynamic sequences in computational geometry & physics\n\n📖 Documentation\nRunning Java Benchmarks\nTo run the Java benchmarks, ensure you have the Trove library installed. Compile and execute using:\njavac -cp trove-3.0.3.jar; ShiftToMiddleArrayBenchmarkTrove.java\njava -cp trove-3.0.3.jar; ShiftToMiddleArrayBenchmarkTrove\n\n📊 Benchmarks & Results\nFor full benchmark details, check out the publication. The provided Python scripts can be used to visualize performance metrics from CSV benchmark results.\n🏛 History\nThe Shift-To-Middle Array was developed as part of an effort to create a more efficient implementation strategy for lists and deques. Traditional data structures, such as std::deque and linked lists, suffer from poor cache locality or fragmented memory allocations, leading to inefficiencies. By leveraging contiguous memory, dynamic mid-shifting, and modern CPU optimizations, Shift-To-Middle Array provides a balanced solution for insertion, deletion, and access performance.\n📜 License\nThis project is licensed under the MIT License.\n🤝 Contributing\nContributions are welcome! Feel free to open an issue or pull request.\n🚀 Try Shift-To-Middle Array today and optimize your data structures!",
    "summary": {
      "en": "### Shift-To-Middle Array Summary\n\nThe Shift-To-Middle Array is a dynamic data structure that enhances performance for inserting and deleting elements at both ends. It is a faster alternative to std::deque, std::vector, and linked lists, while also keeping memory storage contiguous, which improves speed and efficiency.\n\n#### Key Features:\n- **Fast Insertions/Deletions**: O(1) time for adding or removing items at both ends.\n- **Quick Access**: O(1) time for accessing elements by index.\n- **Improved Cache Use**: More efficient than linked lists.\n- **Supports Optimizations**: Works well with SIMD (Single Instruction, Multiple Data) and parallel processing.\n- **Better Memory Efficiency**: Compared to std::deque.\n\n#### How It Works:\nInstead of using fragmented memory like std::deque, the Shift-To-Middle Array moves elements toward the center when resizing. This approach minimizes the need for excessive copying.\n\n#### Performance Comparison:\n- **Access**: O(1) for both Shift-To-Middle Array and ArrayList (std::vector), but O(n) for linked lists.\n- **Insertion**: O(1) amortized for both ends in Shift-To-Middle Array, while linked lists are O(1) at the head but O(n) in the middle.\n- **Deletion**: Similar efficient performance for deletions at both ends in Shift-To-Middle Array compared to linked lists.\n\n#### Use Cases:\n- High-performance queues\n- Game engines and real-time applications\n- Networking tasks like packet buffering\n\n#### Installation:\nTo use, include the header file and create an instance of Shift-To-Middle Array in your project.\n\n#### Additional Information:\n- **Performance Benchmarks**: Performance depends on the hardware and workload.\n- **Documentation and Contributions**: The project is open for contributions and is licensed under MIT.\n\nThe Shift-To-Middle Array is recommended for optimizing data handling in various applications.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "13d4eb798a832725",
    "title": {
      "en": "The Vatican's Latinist (2017)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://newcriterion.com/article/the-vaticans-latinist/",
    "score": 136,
    "by": "bookofjoe",
    "time": 1742779791,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "fd9bcc9f99741e1b",
    "title": {
      "en": "Fuckarounditis (2011)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://leangains.com/fuckarounditis/",
    "score": 16,
    "by": "fzliu",
    "time": 1742856856,
    "content": "Fuckarounditis\n\t\t\t\t\t\tSeptember 27, 2011 — Posted in Training\n\t\t\t\t\t\tGoal Setting Reverse Pyramid Training Weight Training\n\n\t\t\t\t\t\tDear readers, it is with troublesome news I break my three months of silence.\nThe statistics all point towards the same conclusion: we have a global outbreak of fuckarounditis.\n\nFuckarounditis is a behavioral disordercharacterized by a mediocre physique and complete lack of progress, despite significant amounts of time spent in the gym.\nFuckarounditismost commonly manifests itself as an intense preoccupation with crunches, curls, cable movements, belts, gloves, balance boards, Swiss Balls and Tyler Durden.Fear of squats and deadlifts is another distinguishing trait.Physical exertion is either completely lacking or misapplied (towards questionable or unproductive training practices).\nDespite an alarming increase of fuckarounditisin recent years, prevalance may vary greatly depending on location. However, in most commercial gyms, it has been estimated that 90-100% of individuals are afflicted to varying degrees.\nEnvironment andsocial networks are crucial factors for triggering the disease. It has been proposed that the roots of the disease stems from misinformation and counterproductive training advice found in popular media (“fitness magazines”) and information hubs on the Internet.\nHuman nature and the so-called “laziness”, “magic bullet” and “complacency” genes plays a permissive role for allowing the disease to take hold.\nThe disease spreads rapidly, as carriers of the disease communicate with other individuals in locker rooms, Internet discussion forums and other arenas of interaction and information exchange in real life or otherwise.\nThe onset of symptoms typically occurs in young adulthood and may go undiagnosed for a lifetime. Diagnosis is set by a professional and based on observed behaviors and physique progress.\nSymptoms, preventative measures and intervention strategies are reviewed and discussed.\nBerkhan, M. (2011)\n\t“Targeting The Fuckarounditis Epidemic: Preventative Measures and Intervention Strategies.”\nFuckarounditis: A Serious Threat\nI have been trying to cure people of fuckarounditis since the late 90’s, starting from the day I was first cleansed from it myself. Ever since I recognized the disease, I have acknowledged it as a serious threat to physique development and mental well-being. It is therefore with shock and horror I have watched an increasing number of people fall victim to the disease and the dubious training practices it inspires.\nIn educating myself of good strength training practices, I was very lucky. Yes, I wasted some time with the usual bench’n’curl/bodybuilding magazine-inspired crapola all my friends were doing, spending my time in the gym joking around more than doing any actual training. I was 16 or so when I first set my foot at the gym. (See“My Transformation”for pictures from this time period.)\nHowever, I did not spend more than a few months fooling around with my friends. I found that I enjoyed strength training, so I started to take an increasing interest in the topic. I started to explore and I eventually came across an excellent resource that set the tone for my entire approach and attitude. The resource was the book Beyond Brawn by Stuart McRobert, and it taught me the value of working hard at the right things.\nHowever, I may have never been lucky enough to find this tome of wisdom if I had first lost myself in the malicious maze that resides behind…the illusion of complexity.\nThe Illusion of Complexity\nThe Internet provides a rich soil for fuckarounditis to grow and take hold of the unsuspecting observer.Too much information, shit, clutter, woo-woo, noise, bullshit, loony toon theories, too many quacks, morons and people with good intentions giving you bad advice and uninformed answers. Ah yes, the information age.\nSome of it is bullshit wrapped up in a fancy paper with scientific terms, elaborate detail, promising cutting edge strategies based on the latest research. This makes it easier to swallow for intellectuals and those seeking a quick fix; two different groups, both equally susceptible to bullshittery and easy prey for scam artists.\nYes, if anything has surprised me so far in my work, it’s the complete disassociation between IQ and “exercise intelligence” (essentially common sense and knowledge in regards to training fundamentals). I have many clients from academic circles, many clients that are very successful financially and in their each respective field, but some were complete idiots with regards to their training before they came to me.\nThe problem at the core of the fuckarounditis epidemic is the overabundance of information we have available to us.If there are so many theories, articles and opinions on a topic, we perceive it as something complex, something hard to understand. An illusion of complexity is created.\nWe must read everything. Think long and hard about our choices. Only then can we hope to make an informed choice, we reason. And there areso many choices. Finally, that which we perceive as a good and informed choice is often the complete opposite, usually the result of whatever fad routine is trendy at the moment. Sometimes we do a little bit of everything – “can’t be bad trying to be ‘well-rounded’ now, can it?” we foolishly argue.\nWhen it comes to strength training, the right choices are limited and uncomplicated. There are right and wrong ways to do things, not “it depends”, not alternative theories based on new science that we need to investigate or try. Basic do’s and don’t’s that never change. Unfortunately, these fundamental training principles are lost to many, and stumbling over them is like finding a needle in a haystack.\nIsn’t it the same with nutrition? Do we have diet-related fuckarounditis? Sure enough, there’s diet-related fuckarounditis; people who live in the fear of insulin faeries, avoid carbs like the plague for a few days and then binge the hell out of a few boxes of cereal, and never manage to get lean, for example.\nHowever, in contrast to training-related fuckarounditis, rates of diet-related fuckarounditis have remained fairly stable. The lipophobes have merely been replaced by carbophobes. On the whole, I might even venture to say that people have been getting a bit smarter with regards to nutrition. Not so with training practices, unfortunately.\nYes, the global prevalence of fuckarounditis is increasing at an alarming rate. The plethora of bad choices increases at a much higher rate than the good choices. Soon the bad choices will all but drown out the good ones, I fear.\n“I See Weak People”\nIn my dreams? No. In gyms? Yes.Walking around like regular people. They don’t see each other. They only see what they want to see. They don’t know they’re weak.\nThe afflicted are everywhere; the Shape Magazine-reading receptionist who greets you at the gym, the 135-lb skinny-fat PT who tells you that deadlifts are off limits, the bandana wearing bro in the cable-cross machine and the guy with entire day devoted to “abs”. All of them suffer to varying degrees of the debilitating disorder known as fuckarounditis. Yes, even you might be among the afflicted. Today you shall find out if there is cause for alarm.\nKeep in mind that this is a disease that sneaks up on you and then progresses gradually.Some people walk around with a mild case of fuckarounditis that do not completely impair their results in its initial stages. In others, the disease has reached a severe state, which dramatically interferes with their progress and usually stalls it completely.\nFinally, there are those who are all but lost and for whom there is little hope of a cure. Unfortunately, these people will probably never read this. They are too busy emulating the latest bodybuilding pro, doing the Biceps Blaster routine, or rolling around on a Swiss Ball somewhere.\nHow can you tell if you are suffering from the fuckarounditis? Ultimately, it boils down to your results and whether your progress is reasonable relative to the time you put in.\nLet’s be concrete and talk numbers. After all, there needs to be some overarching and objective way of judging whether you are afflicted or not.\nProgress and Goals\nFor someone interested in aesthetics, which I assume most my readers are, relative strength is the single best measure of progress and the quality of your physique. Before seeing a picture of a client, I can easily get a very good idea of his or hers body composition by simply knowing three stats: weight, height and strength. Relative strength is therefore the parameter that will be used to determine reasonable rates of progress, which will then tell you whether you might be suffering of fuckarounditis or not.\nWithin two years of consistent training on a decent routine, the average male should be able to progress to the following levels of strength (1RM):\nStrength Goals: Intermediate\n\nBench press: body weight x 1.2\nChin-ups or pull-ups: body weight x 1.2 or 8 reps with body weight.\nSquat: body weight x 1.6\nDeadlift:body weight x 2\n\nThese numbers are for a raw (no straps, belt or knee wraps) single repetition.\nThe progress towards the intermediate strength goals should be fairly linear, meaning that there should be no plateaus that cannot be solved in an uncomplicated manner. By “consistent” training I do not mean never missing a training day, nor do I consider taking 2-3 months off from training consistent.\nBy “decent training routine”, I mean “not doing blatantly stupid shit” (training 5-6 days/week, 20-25 sets for chest and arms, etc.). I do not mean optimal and flawless.\nStrength Goals: Advanced\nUnder the exact same conditions as the previous example, 3 out of 4 of the following goals should be reached within five years, along with all of the strength goals listed under “intermediate”:\n\nBench press: body weight x 1.5\nChin-ups or pull-ups: body weight x 1.5 or 15 reps with body weight.\nSquat: body weight x 2\nDeadlift:body weight x 2.5\n\nStrength Goals: Highly Advanced\nUnder the exact same conditions, all of the following goals should be reached within ten years. Alternatively, 3 out of 4 should be reached, and one should be “Elite”:\n\nBench press: body weight x 1.5, or x 1.8 (elite)\nChin-ups or pull-ups: body weight x 1.5 or 15 reps with body weight, or x 1.8 / 20 reps(elite)\nSquat: body weight x 2, or x 2.4(elite)\nDeadlift:body weight x 2.5, or x 3(elite)\n\n“Elite” denotes one lift that is often ahead of the others. For example, people who are natural pullers (long arms) may very well hit a 3 x body weight deadlift before a 1.5 x body weight bench, and vice versa for the presser (short arms, stocky and barrel-chested) benching 1.8 x body weight but not being able to pull 2.5 x body weight in the deadlift.\nThe highly advanced strength goals falls in line with what could be considered the pinnacle of physique and strength developmentfor most average and natural trainers. At this point, progress is very slow.\nA 185-lb male that has been training consistently for 5-10 years should therefore be expected to:\n\nBench press 275-280 lbs.\nDo a chin-up with 90-95 lbs hanging from his waist.\nSquat 370 lbs.\nDeadlift 460-465 lbs.\n\nRespective goals for women:\nFor women in the 115-155-lb range, the corresponding advanced strength goals are 0.9 x body weight bench, 1.1 x body weight chin-up, 1.5 x body weight squat and 1.8 x body weight deadlift.Relative to men, women have much less muscle mass around the chest area and shoulder girdle (men have much higher androgen-receptor density in this particular area), but the lower body is comparativly strong to the upper body.\nA 135-lb woman that has been training consistently for 5-10 years should then be expected to:\n\nBench press 120-125 lbs.\nDo 4-5 chin-ups with body weight or do one with an extra 10-15 lbs hanging from her waist.\nSquat 200-205 lbs.\nDeadlift 225-230 lbs.\n\nSo where do you fall in regards to these goals? If you’ve met them, you’ve received a reasonable return on the time you’ve invested in your training and it’s safe to say that you do not suffer from fuckarounditis – even if you have some of the symptoms, remember that it’s not fucking around if what you’re doing is working for you.\nIf you have not met them, then something is amiss. And if you can’t do 8 good chin-ups or struggle with being able to bench press your body weight after a decade of working out….something is horribly amiss.\nThis is no laughing matter. I’ve had clients that spent 10-15 years working out with little or nothing to show for it and some made very basic mistakes that could have been fixed at an early stage. But as fuckarounditis took root, they seemed to only have succumbed deeper, eventually losing that last shred of training sense.\nTaking early preventative measures is key. Where this is not possible, rapid intervention can save the situation. Enter The Fuckarounditis Test.\nThe Fuckarounditis Test\nPlease review these 25 common symptoms and behaviors associated with fuckarounditis. If you recognize yourself in any of these, and have not met the strength standards recently discussed, you must immediately cease the behavior and implement the necessary changes. There is no time to waste.\n1. You don’t keep track.\nHow much can you bench, squat and deadlift? How many chin-ups? You need to be able to answer those questions right now. Don’t let me hear “I THINK I can” or “I’m not sure but…”. You need to know how much weight you can maximally lift in one set when you’re fresh.\nWhether it’s a set of 1, 4 ,6 or 8 reps doesn’t matter. You need to have concrete reference points in order to evaluate your progress. Keep track of them in a training log. Not “in your head”, write it down. The single act of writing it down is more important than you think, whether you keep those data points in a notebook, on your computer or on Post-It notes like me.\nWith tracking comes the motivation to train, the results and everything else. I can’t even imagine where people get their motivation from if they don’t keep track and just choose weights at random based on whatever feels good that day.\nYou should at all times be aware of the best performance in a few key lifts, your body weight and the conditions under which those sets were performed.\nMore on tracking progress and “checkpoints” in“How to Look Awesome Every Day”.\n\nWhen I recently evaluated client and American poker proPhil Galfond‘s progress after his first month on my training routine, I was pleased to see his bench jump from his old personal best of 225 lbs x 4 to 225 lbs x 8 – and he’s dieting. Phil says“The 4 to 8 rep jump is actually bigger than it sounds, since my 4 reps were done with terrible form, bouncing the weight as hard as I could, and the 8 reps were done very controlled.” That’s outstanding progress, but in order to spot it, you need to keep track.\n2. You are not methodical.\nThe only thing that should be changing from week to week is the load on the bar or the reps with the same load you used last time. If you’re doing it right, these should be increasing. Everything else stays the same; the movements and the order you did them in, the sets and the rest periods in between sets. You don’t add in new stuff.\nThis is the only way you can fairly evaluate your progress and see if you’re headed in the right direction.It might sound tedious to keep doing the same movements every week and the appeal of “mixing it up” can seem strong.\nHowever, the tediousness will soon be replaced by the much stronger joy you get from seeing your lifts go up on a weekly basis. Don’t fall for “muscle confusion” bullshit. The only ones confused are the people who keep talking about such nonsense.\nMixing together too much shit and being methodical about the process was one of the fuckarounditis symptoms that plagued social media expert Julien Smith, co-author of Trust Agents, before I set him straight. Here’s what I told him back when he asked me to comment on his “routine” before we started working together:\n\n…Summing it up, I suspect that your laughable progress can be explained by the following:\n1. Your bullshit training routine, which is a haphazard mixture of strength and conditioning. Some people get decent results from CrossFit and the like, but it spells death for hardgainers like yourself. If we work together you will stop flopping around like a fool and start lifting heavy, with plenty of rest in between sets.\n2. Not tracking progress, be that in some key movements like chins, bench press, etc, or benchmark CrossFit-sessions (which I use regularly for CF clients). Training whatever feels good for the moment is one of the worst ways possible to make progress, not to mention serious progress. It’s such a fucking waste of time that I was completely horrified of how you approached it all. That’s going to change if we work together.\n\nHe has since then gained muscle, added more than a hundred pounds to the main lifts, and has not gained an iota of body fat.I cured him with lots of food and a simple regimen of basic movements done consistently on a weekly basis.\n3. You don’t plan for progress.\nNever choose training weights at random. You look at what you used last session and make the choice based solely on that. Not on your ego. Not because you feel like trying higher or lower reps for shits and giggles.\nThere many good progression models but I will recommend two common models that I use depending on the situation.\nBeginners and people who need a “reboot”, i.e. they may have training experience but have been fucking around so much they’re essentially at the beginner stage (e.g., I used a slightly modified SS routine for Julien), Starting Strengthis an excellent model.\nThis is avery simple 3x/week-program with frequent training and practice of the Big Three (bench, squat and deadlift), starting at 1-3 sets of 5. Example:\nSquat: 200 x 5 x 3.\nWhen you can complete 3 sets of 5 reps with 200 lbs, resting 5 minutes between each set, increase the weight next session. The training load starts at your 7-8RM, which means that you do not go to failure initially.\nFor experienced trainers, I recommend the double progression model ofreverse pyramid training. This is what built most of my physique and it’s also by far the most common approach I use with clients. Example:\nSquat: 200 x 6-8 x 2.\nSet 1: 200 x 8\nRest 3-5 mins. Reduce the load by 10% for the second set.\nSet 2: 180 x 8 (however many reps you can get)\nWhen the highest number in the interval (6-8) is reached (“8” in this case), increase the weight by 2.5% or 5 lbs the next session. Thus in the above example, you would use 205 and 185 lbs for your sets the next time. All sets are to be performed with maximal effort and movements are only done once a week (3x/week training frequency).“Double progression” means that you progress both in reps and load; first you hit the reps you need, then you increase the load.\nRPT is very time-efficient. It’s also far superior to anything else I’ve tried for strength/muscle retention/muscle gain during a diet.\n4. You’re doing too much shit.\nBe a minimalist, like me.\n\n\tI snapped this yesterday and stay in this condition all year round.\nI built my physique with these movements primarily: squats, deadlifts, bench presses, chin-ups, triceps extensions and calf raises. These have remained staples throughout the years. I have flirted with other movements, but these were brief periods.\nOn the whole, that’s one movement per muscle group, with the exception of abs and biceps, which I never really trained except for early in my training career, and then very sporadically every once in a while.\nThe point is that most people are doing too much shit. This dilutes the focus and effort that they are able to put into that which really delivers.\n5. You think more about supplements than squats.\nNo single factor in strength training receives so much attention, yet deliver so little in return. Don’t play the fool who chases magic pills, thinking it will compensate for a lack of effort, a fundamentally flawed training routine or a poor diet.\nThere aresupplements that you might actually find useful, but they won’t turn the tide if what you’re doing right now is not working without them.\n6. You’re lifting weights for the calorie burn.\nStrength is strength. Cardio is cardio.Don’t mix, keep them separate, and use cardio sparingly on a diet or if your primary goal is strength and muscle gain.\n\nIf you’re adding 2-3 sessions of HIIT to your 3 sessions of weights, it is almost comparable to adding 2-3 days of weights. Keyword is “almost”, I’m obviously not drawing direct comparisons. That’s all fine and dandy if you think working out 5-6 days/week is a good idea on a diet. But I don’t think anyone – no matter what level of experience – needs more than 3 days a week in the gym when cutting. (Yes, this goes for competitors and beginners alike.)\nIn conclusion, if conditioning is not terribly important for you, if your goal is really about getting shredded while keeping your muscle, I highly suggest limiting moderate to high intensity cardio on a diet – or ditch it completely. Save it for some other time when your recovery is good and not limited by your diet.\nA calorie deficit is a recovery deficit. Avoid deficit spending.\n\nMy full answer here.\n\n\tRickard Nikoley is one of those I saved from fuckarounditis. Before I intervened, Richard was flopping around like a monkey in the gym. Nothing good came from that. There’s nothing primal or Paleo about being weak. I cured him with squats, bench presses, deadlifts and proper rest periods. That way he could put up the effort when it counted.\n7. You’re looking a bit too happy in there.\nIf you’re not grimacing, grunting, or experiencing some sort of discomfort, you’re not training hard enough. One good set where you have to fight for the reps is better than a week of half-ass sets.\n8. You spend more than 5 minutes on abs.\nIf you can’t see your abs, you need to unfatten yourself. Simple as that.You can’t spot reduceand you’ll get plenty of ab work with squats, deadlifts, chin-ups and overhead presses.Let me see someone at 5-6% body fat deadlift 2.5-3 x body weight for reps with with weak abs and underdeveloped abs. It ain’t happening.\nI’ll allow you one set of “abs”. Beyond that don’t waste your time with crunches, ab machines, hanging leg raises or whatever the hell you’re doing. It won’t give you shit and it will only suck time and energy from what you should be doing.\n9.Is this you?\n10. You’re afraid of “bulky muscles” and use terms like “toning.”\nWomen, you need to put down those pink dumbbells, throw that Shape Magazine in the trash can and stop with this nonsense. You seem to believe that the modest amount of strain will cause you to wake up looking like a bodybuilder in the morning. Horseshit. You’re not using enough drugs to ever come close to looking muscle bound. With the training intensity I see most women apply in the gym, they might as well skip weight training altogether and stay on the treadmill.\nhttps://youtube.com/watch?v=Ia-IdX3DIDg%3Fhl%3Den%26fs%3D1\nJenn has actually gotten stronger since she shot that video; she’s now up to 27.5 lbs added weight. That’s more than 99% of the guys at my gym…and I bet it’s more than many male readers of this site as well. Her routine?\nDay 1 (Monday)\n\n5-min walk for warm-up.\nDeadlift 2-3 sets of 4-5 reps reverse pyramid style.\nRest 5-10 mins.\nLeg extensions – Same setup as deadlifts, but reps a bit higher (6-8).\nRest 5-10 mins.\nChins – Same as deads.\n\nDay 3 (Wednesday)\n\n5-min walk for warm-up.\nBench press 2-3sets of 5-7 reps reverse pyramid style.\nRest 5-10 mins\nPull-ups 2-3sets with body weight.\nRest 5-10 mins\n\nDay 5 (Friday)\n\nSquats 2-3sets of 5-7 reps reverse pyramid style.\nRest 5-10 mins.\nWalking lunges 2-3 sets same as squats.\nRest 5-10 mins.\nOverhead Press 2-3 sets of 5-7 reps reverse pyramid style.\nRest 5-10 mins\n\nThat’s the routine Jenn was doing when she sent me the video and the one she gained all that strength on. This is a solid routine, regardless of gender, and it’s vastly superior to what you’re doing if you’re in the fuckarounditis camp. Why not try it and see for yourself? Throw in a set or two of curls, triceps and calves if you want but don’t mess with it beyond that.\n11. You’re “training the core”…and it involves a Swiss Ball, Bosu Ball or something else that makes you look like an idiot.\n“Training the core” is a phrase that is all too often used by people who are afraid to squat. You don’t need special movements for the core, because it comes with the territory if you squat, deadlift, press and chin. No one squats 2 x body weight with a weak core.\n12. You want that Tyler Durden look.\nWhenever I hear a client say this, alarm bells go off. There’s nothing wrong in having ideals. The problem is all the pain I feel after seeing the dumb shit that follows in the questionnaire, under the part where I ask them about their current training routine. I’ll often see some horrifying cardio/pump’n’tone hybrid, some celebrity workout variety or the kind you’d expect to see in those kind of routines. Nothing more faddish than made-up “celebrity workouts”. God damn, how I hate those.\nWant to look like Tyler Durden? Then you need to build up some modest amount of muscle and then you need to lose fat, period. The muscle is best built with heavy weight training, not 15-25-rep set flyes followed by an hour in the pec deck machine. Your gut is best lost with a good diet, not “fat burning workouts”/tons of cardio/pump’n’tone crapola. All those will do is leave you under-muscled, weak and with a severe case of fuckarounditis.\n\n\tThe Tyler Durden look, brought to you by squats, deadlifts, bench, chin-ups and a good diet. When this client told me that he was “closing in on his goal” of achieving a physique similar to that of Brad Pitt, AKA Tyler Durden in “Fight Club”, I told him to gain some fat, start smoking, get into a fight, and stop training his arms and shoulders. (The implication of that being that he had already surpassed his goal.)\n13. You’re using belts, gloves and straps for no good reason.\nWhat is your reason exactly? Don’t use equipment as a crutch.\n14. You avoid squats and deadlift, because you think they’ll give you a wide waist.\nBitch please. Yes, squats and deadlifts will put muscle on your obliques and thicken up your lower back. But weighted chin-ups or pull-ups will give you wider lats, and if you train all these movements, your waist-to-shoulder ratio will develop very favorably. Training all these movements will also help you grow some balls, so you can finally stop making up bullshit excuses for why you shouldn’t train hard (AKA squat and deadlift).\nPetter, the Tyler Durden lookalike, was squatting and deadlifting regularly on my routine. Last time I checked, he was squatting 2 x body weight (300 lbs) for 9 reps. He was also close to being able to complete a one-arm pull-up. Does it look like he has a wide waist? Are my clients notable for their wide waists? Take your “wide waist”-argument/excuse and shove it up your ass right now.\nWhat’s funny is that this argument is usually brought up by guys who want a “Hollywood”/underwear model type physique. They’re often a) dieting, b) not training legs and c) likely doing tons of cardio. That particular combination will strip off whatever little leg muscle they have faster than Gary Busey can do a gram of coke off his dog Chili’s back. It leaves them looking pathetic and weak, and if that sounds good to you then go ahead.\n15.Doing this?\n16. Are you still warming up? I can’t tell.\nA warm-up is a warm-up. Treat it as such. It should be light, just enough to get the blood flowing and prepare the nervous system for the real stuff. It should never cause fatigue or interfere with your performance in the work sets. All the time, I see people doing sets of 10-15 reps before they get to something that remotely resembles their real set. Which is completely retarded, because you will be fatigued and glycogen-depleted, and your performance in the work sets will be limited for metabolic reasons, and not limited by your actual strength.\nThe only standing recommendation I have regarding warm-ups is for compound movements: 1-3 sets of 4-6 reps of 40%-80% of target weight for the first work set. Warm-ups for assistance movements is whatever you feel is needed.Personally, I only warm up for squats, deadlift, bench press and chins. If you do these at the start of your workout, which you should, you’ll find that they usually get your whole body warmed up, which makes warm-ups redundant or entirely unnecessary for the rest of the workout.\n17.Are you still warming up? I can’t tell. What’s that? You’re already done and I can take the bench?\nLet’s see some fucking effort next time. Don’t take up precious space here when you can get a better workout from walking your dog through the park outside.\nYou can be doing everything perfectly, but you will not go anywhere if you’re not willing to experience discomfort on a regular basis. If you’re in the gym 5-6 days a week, lack of effort might actually be the explanation for your lack of progress. If you were actually in there training hard, most people wouldn’t be coming back for more 5-6 days a week.\n18.Was this you? 🖼️\nLearn good form early, so you don’t have to pay for it with injuries and shitty results later down the road. Don’t let your ego screw you and don’t slap on weight indiscriminately. Be methodical and patient.\nNo need to be a form nazi either. People who “practice” form in all eternity are sometimes just afraid to train hard. Striving for picture perfect form on your final reps can be counterproductive when taken to the extreme.\n19. Your workout buddy gets a set of barbell rows on the house when he “spots” you benching.\nTell him to keep his fingers off the damn bar and assist you as little as possible and only when it’s truly needed; meaning only if you are about to fail on the very last rep in the set. Don’t be one of those clowns slapping on twice of what they could lift by themselves to stroke their ego. It looks pathetic and it will make it impossible for you to properly evaluate your progress.\nAnd for me, an unwanted spotter can make it hard to evaluate the progress of the client, like I had to explain to this actor/client:\n\nI am your trainer, the guy who follows you around is only there to make sure your form is not completely retarded. His function right now should be redundant. I assume you know how to perform most movements well by now. If he interferes with your sets, he is counterproductive to the outcome.\nYou’re my only client right now with a personal trainer on top of the help you receive from me. I imagine he is eager to “correct” your form and tinker with your training in any way he can in order to not feel useless. Make sure he stays out of it beyond spotting you on some sets where it’s warranted (i.e. when and if you are about to fail). Some PTs can’t even get that part right and goes way overboard with the helping part.\nThis is another danger of having him around, as it gets impossible for me to measure your progress (strength gains) accurately (i.e. did you gain strength since the last session or did he just help you more this time?). Make it very clear to him when and how he should help (last rep you can’t complete by yourself).\n\nThis actor played a key role in an award winning and Oscar-nominated movie last year. I’m preparing him for a new movie with some very big names that start shooting early next year. I can’t tell you more beyond that, but rest assured that his “celebrity workout” does not include mickey mouse curls supersetted with cable flyes, or anything of the usual variety you read about in magazines.\n20.You obsess about “tempo”.\nForget about tempo. Tempo takes care of itself if you’re working with heavy weights (80% 1RM / 8 reps or less), because your control over the load is very limited. Lift the damn weight, repeat, it’s that simple. Don’t overthink it.\nTempo was hot stuff a few years ago and frequently discusses in articles an on the boards. It seems to have slipped off the radar now. Why? Because it’s yet another distraction from the important stuff.\n21. You’re into “functional” training.\nBut you can’t squat body weightand your “functional” muscle mass is non-existent. Unless you’re an athlete with a very good reason, stay the hell away from anything that involves Swiss Balls, balance boards or pogo sticks.\n22. You’re the guy who sent me these two direct messages 5 minutesafter I tweeted this:\n\ndo you think 8×20 abs is enough ?? i do hanging superstrict..really feel working lower abs!!. also i need bcaa if i situps in fasted state??\nalso how much bcaa u recommend\n\n23.You’re working on your “lagging biceps”, but you can’t even do 8 chin-ups with good form.\nFirst of all, you’re weak and fat.Second of all, body part specialization has its place, but it’s always the ones least qualified that are doing it. Worry about body part specialization after you achieve the major goalposts involving the muscle group that lags behind. Until then, body part specialization is a distraction that will interfere with your development as a whole.\nLet’s see if your chest and your shoulders are lagging once you can bench 1.5 x body weight or overhead press your body weight and if your legs and back are lagging with a 2 x body weight squatand 2.5 x body weight deadlift. Keep doing targeted work for muscle groups like triceps, hams and calves.\nYes, you can keep a few sets of curls in there, but 2-3 sets is enough for now. Do you really think your guns will be lagging once you’re doing chin-ups with 100 lbs hanging from your waist? My clients have told me they saw more biceps growth from the increased focus on weighted chin-ups than any amount of curls they did in the past.I never spent any time on curls and they grew just fine from all those chin-ups.I can say for damn sure you’ll put on a good deal of size on your arms if you add 50 lbs to whatever you can chin-up right now.\nAsk Paul Wilson and his 8 reps with 110 lbs of solid chin-up mastery:\nhttp://www.youtube.com/embed/qhaJWPTJU5A?hl=en&fs=1″\n\n\tPaul flexing in exclusive LG Gear, only available as an award for excellency and strength.\n24.You think ripped abs will get you laid.\nI hate to bust your bubble lil’ buddy, but women don’t give a shit beyond a guy looking reasonably fit. Lower than 10-12% body fat won’t make an ounce of a difference. If women is your main motivation for dieting, don’t bother getting shredded. Women have similar delusions in terms of thinking men like them super-skinny.\nTraining to be liked by others is a very poor motivational factor in itself. You need internal motivation, not external. No one that ends up making something out of his or hers physique, something impressive and head-turning, gets there by training to get liked by others. It might have started out as such, but it changes. I think this quote sums up why and how we make it:\n\nYou have to discipline yourself, and through this discipline will come some level of achievement, and from this achievement will come your pride.\nDavid Lee Roth.\nAnd then there’s the fact that nothing beats the feeling you get from breaking a new personal record. Some of the happiest moments I remember are from walking home after a new 20-rep squat record way back in the days. Walking up stairs was like being repeatedly stabbed with a blunt and rusty butterknife. But it was still pure bliss.\n25. You have all kinds of elaborate excuses for why you don’t need to squat and deadlift.\nYou get your leg training from playing soccer, you’re afraid you might hurt your back, you read somewhere that single-leg training is better, you talk about rear-foot elevated split squats with a 45-degree twist during the concentric being the future of leg training, etc. Horseshit, all of it.\nListen carefully. Squats and deadlifts are undisputably the two best full body movements in the god damn Universe and that’s reason enough to be doing them.\nBut that’s not the whole story. Their true greatness lies in the mental fortitude they foster if you train them hard and consistently. This will benefit your training as a whole and leave echoes in your life outside the gym.\nDid I miss something? Well, I actually left a few signs and symptoms out because this article was getting way too long for my own good and it feels like my eyes are bleeding right now. I thought I’d leave it up to you to finish it.\nThe #Fuckarounditis Contest\nYou need to do three things if you want to be eligible for participation in the contest:\n\nFollow me on Twitter.\nTweet or retweet this article.\nSend me your best/funniest/most original fuckarounditis symptom in 140 signs by using a hashtag followed by fuckarounditis, like this: #fuckarounditis (so it’ll actually be less than 125 signs not counting #fuckarounditis). That way I can click the hashtag and see all contributions.\n\nThe contest will be running from today until the end of October (ends Oct 31st). After the end of the contest, I’ll pick 5-10 winners that each will receive$100 by PayPal. That’s five, ten or any number of winners in between, I’ll just pick whatever great contributions I get. I’ll announce the winners shortly thereafter and they will be added to this article.\nImportant: Don’t PM/email/write your contribution in comments. I already get too many emails and PMs to keep up and I want to keep all contributions in one place: on Twitter. So you make your contribution by: (text goes here) #fuckarounditis. I’ll be retweeting good ones on a daily basis. When I choose the winners I’ll go through my retweets and pick 5-10 winners.\n\n\t\t\t\t\t\t\tFTC: We use income earning affiliate links. Learn More.\n\n\t\t\t\t\t\tShare this post\n\t\t\t\t\t\tFacebookRedditTwitterWhatsAppEmail\n\n\t\t\t\t\t\t\t\tMartin Berkhan\n\n\t\t\t\t\t\t\tMy name is Martin Berkhan and I work as a nutritional consultant, magazine writer and personal trainer.\n\n\t        \t\tYou might also like..\n\n\t        \t\t\t\t\tFuckarounditis\n\n\t        \t\t\t\t\tSupplements You Might Actually Find Useful (Oct 7th, 2018: EAA Update)\n\t        \t\t\t\t\tDecember 12, 2017\n\n\t        \t\t\t\t\tCheat Day Strategies For A Hedonist\n\t        \t\t\t\t\tNovember 25, 2010\n\n\t        \t\t\t\t\tTop Ten Fasting Myths Debunked\n\t        \t\t\t\t\tOctober 21, 2010\n\n\t        \t\t\t\t\tThe Leangains Guide\n\t        \t\t\t\t\tApril 14, 2010\n\n\t        \t\t\t\t\tThe Reverse Pyramid Training Guide\n\t        \t\t\t\t\tJanuary 25, 2018\n\n\t        \t\t\t\t\tSure-Fire Fat Loss\n\t        \t\t\t\t\tJune 27, 2008",
    "summary": {
      "en": "**Summary of \"Fuckarounditis\"**\n\n\"Fuckarounditis\" is a term used to describe a common issue in gym-goers characterized by a lack of progress and a mediocre physique despite spending a lot of time working out. It often involves focusing on ineffective exercises (like crunches and curls) while avoiding essential compound lifts such as squats and deadlifts.\n\nThe prevalence of fuckarounditis is alarmingly high, especially in commercial gyms, where many people are misled by bad advice from fitness magazines and online sources. This results in ineffective training habits and wasted time. Symptoms of this disorder include not tracking progress, being unmethodical in workouts, and prioritizing fancy equipment over fundamental exercises.\n\nTo combat fuckarounditis, individuals should focus on a few key principles:\n1. **Track Progress**: Keep a log of your lifts and improvements.\n2. **Be Methodical**: Stick to a consistent routine that emphasizes strength-building movements.\n3. **Plan for Progress**: Choose weights based on previous sessions rather than random feelings.\n4. **Simplify Your Routine**: Focus on a limited number of effective exercises rather than trying to do too much.\n5. **Train Hard**: Emphasize effort in your workouts and avoid distractions like excessive cardio or supplements.\n\nThe article also outlines specific strength goals for both men and women to assess whether they are making reasonable progress. If individuals are not meeting these goals after years of training, they may need to reconsider their approach.\n\nIn summary, to avoid fuckarounditis, one should adopt a straightforward, disciplined approach to training that prioritizes strength and measurable progress.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "616488f05e0e9d76",
    "title": {
      "en": "Show HN: My iOS app to practice sight reading (10 years in the App Store)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://apps.apple.com/us/app/notes-sight-reading-trainer/id874386416",
    "score": 297,
    "by": "rooster117",
    "time": 1742765108,
    "content": ".ember3, #ember3::before {\n           width: 246px;\n           height: 246px;\n         }\n         .ember3::before {\n           padding-top: 100%;\n         }\n@media (min-width: 735px) {\n           .ember3, #ember3::before {\n           width: 217px;\n           height: 217px;\n         }\n         .ember3::before {\n           padding-top: 100%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember3, #ember3::before {\n           width: 230px;\n           height: 230px;\n         }\n         .ember3::before {\n           padding-top: 100%;\n         }\n         }\n\n          Notes - Sight Reading Trainer\n            4+\n\n            Piano sheet music\n\n            Ryan Newsome\n\n              Designed for iPad\n\n    4.8 • 2.7K Ratings\n\n                Free\n\n                Offers In-App Purchases\n\nScreenshots\n\n                  iPad\n\n                  iPhone\n\n    .ember6, #ember6::before {\n           width: 643px;\n           height: 857px;\n         }\n         .ember6::before {\n           padding-top: 133.28149300155522%;\n         }\n@media (min-width: 735px) {\n           .ember6, #ember6::before {\n           width: 217px;\n           height: 289px;\n         }\n         .ember6::before {\n           padding-top: 133.17972350230414%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember6, #ember6::before {\n           width: 313px;\n           height: 417px;\n         }\n         .ember6::before {\n           padding-top: 133.22683706070288%;\n         }\n         }\n\n    .ember7, #ember7::before {\n           width: 643px;\n           height: 857px;\n         }\n         .ember7::before {\n           padding-top: 133.28149300155522%;\n         }\n@media (min-width: 735px) {\n           .ember7, #ember7::before {\n           width: 217px;\n           height: 289px;\n         }\n         .ember7::before {\n           padding-top: 133.17972350230414%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember7, #ember7::before {\n           width: 313px;\n           height: 417px;\n         }\n         .ember7::before {\n           padding-top: 133.22683706070288%;\n         }\n         }\n\n    .ember8, #ember8::before {\n           width: 643px;\n           height: 857px;\n         }\n         .ember8::before {\n           padding-top: 133.28149300155522%;\n         }\n@media (min-width: 735px) {\n           .ember8, #ember8::before {\n           width: 217px;\n           height: 289px;\n         }\n         .ember8::before {\n           padding-top: 133.17972350230414%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember8, #ember8::before {\n           width: 313px;\n           height: 417px;\n         }\n         .ember8::before {\n           padding-top: 133.22683706070288%;\n         }\n         }\n\n    .ember9, #ember9::before {\n           width: 643px;\n           height: 857px;\n         }\n         .ember9::before {\n           padding-top: 133.28149300155522%;\n         }\n@media (min-width: 735px) {\n           .ember9, #ember9::before {\n           width: 217px;\n           height: 289px;\n         }\n         .ember9::before {\n           padding-top: 133.17972350230414%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember9, #ember9::before {\n           width: 313px;\n           height: 417px;\n         }\n         .ember9::before {\n           padding-top: 133.22683706070288%;\n         }\n         }\n\n    .ember10, #ember10::before {\n           width: 643px;\n           height: 857px;\n         }\n         .ember10::before {\n           padding-top: 133.28149300155522%;\n         }\n@media (min-width: 735px) {\n           .ember10, #ember10::before {\n           width: 217px;\n           height: 289px;\n         }\n         .ember10::before {\n           padding-top: 133.17972350230414%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember10, #ember10::before {\n           width: 313px;\n           height: 417px;\n         }\n         .ember10::before {\n           padding-top: 133.22683706070288%;\n         }\n         }\n\n    .ember11, #ember11::before {\n           width: 643px;\n           height: 857px;\n         }\n         .ember11::before {\n           padding-top: 133.28149300155522%;\n         }\n@media (min-width: 735px) {\n           .ember11, #ember11::before {\n           width: 217px;\n           height: 289px;\n         }\n         .ember11::before {\n           padding-top: 133.17972350230414%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember11, #ember11::before {\n           width: 313px;\n           height: 417px;\n         }\n         .ember11::before {\n           padding-top: 133.22683706070288%;\n         }\n         }\n\nDescription\n\n  Introducing Notes - Sight Reading Trainer, the ultimate iOS app for mastering sight reading in music! Whether you're a beginner or an experienced musician, Notes is your is your tool to become the musician you were meant to be. When you are ready to learn your instrument and music rather than just learning a song this app is for you.\n\nKey Features:\nEngaging Exercises: Explore a variety of interactive exercises designed to strengthen your sight reading abilities with a piano interface.\n\nWays to use the app: Connect with Midi, use the on screen keyboard or your devices microphone for input.\n\nCustomizable Difficulty: Tailor the difficulty level to match your proficiency, allowing you to progress at your own pace and challenge yourself when ready.\n\nComprehensive Music Notation: Learn to read sheet music, decipher key signatures, note durations and augmentations (sharp/flat) with ease.\n\nProgress Tracking: Keep track of your improvements over time with detailed performance analytics and history.\n\nPractice with songs: This app is focused on randomized practice but also has a handful of songs to practice your developing skills with. More to be added soon!\n\nGoal Setting: Set achievable note goals and receive reminders to maintain consistency in your practice.\n\nUnlock the world of sight reading and elevate your musical journey with Notes - Sight Reading Trainer. Download the app now and get serious about learning music!\n\n        more\n\nWhat’s New\n\n    Version History\n\n            Feb 26, 2025\n            Version 7.3\n\n  Fix to allow using midi inputs and microphone for the keyboarding learning lessons\n\nRatings and Reviews\n\n        See All\n\n      4.8 out of 5\n        2.7K Ratings\n\n        2.7K Ratings\n\n    Quality reviewer\n\n    ,\n\n    04/22/2023\n\n    Great app!\n\n  This is one of the best apps I’ve used to study notes. I only sing but I’m trying to learn notes and this is fantastic. You will learn how to use a piano in the process, which is key to making sure you learn to sing in tune. My only objection is that the lessons do NOT include training on sharps & flats. I was hoping the in app purchase for additional lessons (happens after you’ve learned quite a lot, no worries) included this, and there’s no info given on what these lessons contain, only that “more will be added”. The practice mode thankfully does allow you to train flats but not in an organized, piece by piece way like the lessons do. But unfortunately you cannot see flats and sharps labeled on the keys like the other ones.\n\nI have to knock off a star… sharps and flats are a big part of the music that I do and so this app didn’t teach me everything I wanted to know.\n\n        more\n\n    FPPMEDIA\n\n    ,\n\n    01/18/2018\n\n    Thank you for educational app.\n\n  This app and the ease of use has introduced a whole new world to me.\nI never thought I would understand how to play a piano.. My life has been filled with homelessness and bad fortune, so I learned a long time ago that music had a medicinal effect on me.. A personal escape of sorts... I always wanted to understand how to make beautiful sounds with a piano, but I don’t learn so well.. This app made it easy for me to understand the keyboard layout and knowing what each note is from each key.. It seems like this app will help me learn how to write songs and that will make life so happy for me.\nI’m not trying to be sad here.. I just wanted to explain how much this app can help someone like me. I can’t afford the full version but this version, I appreciate none the less.\n“Music may have to save the world someday”.\n\n        more\n\n    Churple\n\n    ,\n\n    03/26/2018\n\n    Absolutely Stunning\n\n  I rarely write reviews, but this app is so beyond amazing I’ll try my best to share my thoughts. First of all, everything is absolutely FREE, NO premium that you have to buy to basically unlock everything, I feel the developers of this app were truly trying to help instead of take all of your money. Secondly, I’ve had this app for one day and I’ve already learned more than I did searching for weeks for a good app that’s free. I’ve started to be able to play simple songs on my keyboard, like Jingle Bells, completely looking at the sheet music. I almost gave up on trying to learn how to read sheet music, but this has given me a great start. Sorry for bad grammar.\n\n        more\n\nApp Privacy\n\n    See Details\n\n    The developer, Ryan Newsome, indicated that the app’s privacy practices may include handling of data as described below. For more information, see the developer’s privacy policy.\n\n        Data Not Linked to You\n        The following data may be collected but it is not linked to your identity:\n\n                  Usage Data\n\n                  Diagnostics\n\n    Privacy practices may vary, for example, based on the features you use or your age. LearnMore\n\nInformation\n\n          Seller\n\n              Ryan Newsome\n\n          Size\n          32 MB\n\n          Category\n\n                Education\n\n        Compatibility\n\n                  iPhone\n\n                Requires iOS 14.0 or later.\n\n                  iPad\n\n                Requires iPadOS 14.0 or later.\n\n                  iPodtouch\n\n                Requires iOS 14.0 or later.\n\n                  Mac\n\n                Requires macOS11.0 or later and a Mac with AppleM1chip or later.\n\n                  Apple Vision\n\n                Requires visionOS 1.0 or later.\n\n        Languages\n\n  English, French, German, Italian, Polish, Simplified Chinese, Spanish, Traditional Chinese\n\n          Age Rating\n\n              4+\n\n        Copyright\n        © Ryan Newsome\n\n          Price\n          Free\n\n          In-App Purchases\n\n                  Premium\n                  $3.99\n\n            App Support\n\n            Privacy Policy\n\nApp Support\n\n          Privacy Policy\n\nMore By This Developer\n\n    .ember21, #ember21::before {\n           width: 320px;\n           height: 320px;\n         }\n         .ember21::before {\n           padding-top: 100%;\n         }\n@media (min-width: 735px) {\n           .ember21, #ember21::before {\n           width: 157px;\n           height: 157px;\n         }\n         .ember21::before {\n           padding-top: 100%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember21, #ember21::before {\n           width: 146px;\n           height: 146px;\n         }\n         .ember21::before {\n           padding-top: 100%;\n         }\n         }\n\n        Violin Sight Reading Trainer\n\n    Music\n\nYou Might Also Like\n\n        See All\n\n    .ember23, #ember23::before {\n           width: 320px;\n           height: 320px;\n         }\n         .ember23::before {\n           padding-top: 100%;\n         }\n@media (min-width: 735px) {\n           .ember23, #ember23::before {\n           width: 157px;\n           height: 157px;\n         }\n         .ember23::before {\n           padding-top: 100%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember23, #ember23::before {\n           width: 146px;\n           height: 146px;\n         }\n         .ember23::before {\n           padding-top: 100%;\n         }\n         }\n\n        Learn Music Notes Sight Read\n\n    Education\n\n    .ember24, #ember24::before {\n           width: 320px;\n           height: 320px;\n         }\n         .ember24::before {\n           padding-top: 100%;\n         }\n@media (min-width: 735px) {\n           .ember24, #ember24::before {\n           width: 157px;\n           height: 157px;\n         }\n         .ember24::before {\n           padding-top: 100%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember24, #ember24::before {\n           width: 146px;\n           height: 146px;\n         }\n         .ember24::before {\n           padding-top: 100%;\n         }\n         }\n\n        MyMusicTheory - Music Theory\n\n    Education\n\n    .ember25, #ember25::before {\n           width: 320px;\n           height: 320px;\n         }\n         .ember25::before {\n           padding-top: 100%;\n         }\n@media (min-width: 735px) {\n           .ember25, #ember25::before {\n           width: 157px;\n           height: 157px;\n         }\n         .ember25::before {\n           padding-top: 100%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember25, #ember25::before {\n           width: 146px;\n           height: 146px;\n         }\n         .ember25::before {\n           padding-top: 100%;\n         }\n         }\n\n        Piano Marvel - Lessons & Music\n\n    Education\n\n    .ember26, #ember26::before {\n           width: 320px;\n           height: 320px;\n         }\n         .ember26::before {\n           padding-top: 100%;\n         }\n@media (min-width: 735px) {\n           .ember26, #ember26::before {\n           width: 157px;\n           height: 157px;\n         }\n         .ember26::before {\n           padding-top: 100%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember26, #ember26::before {\n           width: 146px;\n           height: 146px;\n         }\n         .ember26::before {\n           padding-top: 100%;\n         }\n         }\n\n        Perfect Ear - Ear Trainer\n\n    Education\n\n    .ember27, #ember27::before {\n           width: 320px;\n           height: 320px;\n         }\n         .ember27::before {\n           padding-top: 100%;\n         }\n@media (min-width: 735px) {\n           .ember27, #ember27::before {\n           width: 157px;\n           height: 157px;\n         }\n         .ember27::before {\n           padding-top: 100%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember27, #ember27::before {\n           width: 146px;\n           height: 146px;\n         }\n         .ember27::before {\n           padding-top: 100%;\n         }\n         }\n\n        Sheet Music Reader Halbestunde\n\n    Education\n\n    .ember28, #ember28::before {\n           width: 320px;\n           height: 320px;\n         }\n         .ember28::before {\n           padding-top: 100%;\n         }\n@media (min-width: 735px) {\n           .ember28, #ember28::before {\n           width: 157px;\n           height: 157px;\n         }\n         .ember28::before {\n           padding-top: 100%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember28, #ember28::before {\n           width: 146px;\n           height: 146px;\n         }\n         .ember28::before {\n           padding-top: 100%;\n         }\n         }\n\n        Skoove: Learn to Play Piano\n\n    Education",
    "summary": {
      "en": "**Summary: Notes - Sight Reading Trainer**\n\n- **Overview**: Notes is an iOS app designed to help users improve their sight-reading skills in music, suitable for both beginners and experienced musicians. \n\n- **Key Features**:\n  - **Interactive Exercises**: Offers engaging activities to enhance sight-reading abilities using a piano interface.\n  - **Input Options**: Users can connect via MIDI, use an on-screen keyboard, or their device's microphone.\n  - **Customizable Difficulty**: Adjust the difficulty to match your skill level and progress at your own pace.\n  - **Comprehensive Music Notation**: Learn to read sheet music, including key signatures and note durations.\n  - **Progress Tracking**: Track improvements with detailed analytics.\n  - **Practice Songs**: Includes a selection of songs for practice, with more to be added soon.\n  - **Goal Setting**: Set note goals and receive reminders to stay consistent.\n\n- **User Ratings**: The app has a high rating of 4.8 out of 5 from 2,700 reviews, praised for its educational value and ease of use.\n\n- **Availability**: The app is free with in-app purchases for advanced features.\n\n- **Compatibility**: Requires iOS 14.0 or later for iPhone and iPad.\n\n**Conclusion**: Notes - Sight Reading Trainer is an effective tool for anyone looking to enhance their music reading skills through interactive learning and practice.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "fa5bbe9249635585",
    "title": {
      "en": "The game designer playing through his own psyche",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.newyorker.com/culture/persons-of-interest/the-game-designer-playing-through-his-own-psyche",
    "score": 139,
    "by": "FinnLobsien",
    "time": 1742812772,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "2dfa01bfc8e08067",
    "title": {
      "en": "What is Growth Engineering? A deep dive with MasterClass's former Head of Growth",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://newsletter.pragmaticengineer.com/p/what-is-growth-engineering",
    "score": 3,
    "by": "stosssik",
    "time": 1742895705,
    "content": "Share this postThe Pragmatic EngineerWhat is Growth Engineering?Copy linkFacebookEmailNotesMoreDeepdivesWhat is Growth Engineering?A deep dive into the field of growth engineering, which is often positioned between product engineering and marketing. With former head of growth engineering at MasterClass, Alexey KomissaroukGergely Orosz and Alexey KomissaroukMar 21, 2025∙ Paid165Share this postThe Pragmatic EngineerWhat is Growth Engineering?Copy linkFacebookEmailNotesMore38ShareBefore we start: if you’ve already filled out the What is your tech stack? survey: thank you! If you’ve not done so, your help will be greatly appreciated. It takes 5-15 minutes to complete. Those filling out will receive results before anyone else, and additional analysis from myself and Elin. Fill out this survey here. Fill out the surveyGrowth engineering was barely known a decade ago, but today, most scaleups and many publicly traded tech companies have dedicated growth teams staffed by growth engineers. However, some software engineers are still suspicious of this new area because of its reputation for hacky code with little to no code coverage.For this reason and others, I thought it would be interesting to learn more from an expert who can tell us all about the practicalities of this controversial domain. So I turned to Alexey Komissarouk, who’s been in growth engineering since 2016, and was in charge of it at online education platform, MasterClass. These days, Alexey lives in Tokyo, Japan, where he advises on growth engineering and teaches the Growth Engineering course at Reforge.In today’s deep dive, Alexey covers:What is Growth Engineering? In the simplest terms: writing code to help a company make more money. But there are details to consider: like the company size where it makes sense to have a dedicated team do this.What do Growth Engineers work on? Business-facing work, empowerment and platform work are the main areas.Why Growth Engineers move faster than Product Engineers. Product Engineers ship to build: Growth Engineers ship to learn. Growth Engineers do take shortcuts that would make no sense when building for longevity – doing this on purpose.Tech stack. Common programming languages, monitoring and oncall, feature flags and experimentation, product analytics, review apps, and more.What makes a good Growth Engineer? Curiosity, “build to learn” mindset and a “Jack of all trades” approach.Where do Growth Engineers fit in? Usually part of the engineering department, either operating as with an “owner” or a “hitchiker” model.Becoming a Growth Engineer. A great area if you want to eventually become a founder or product manager – but even if not, it can accelerate your career growth. Working in Growth forces you to learn more about the business.With that, it’s over to Alexey:I’ll never forget the first time I made my employer a million dollars.I was running a push notification A/B test for meal delivery startup Sprig, trying to boost repeat orders.A push notification similar to what we tested to boost repeat ordersInitial results were unpromising; the push notification was not receiving many opens. Still, I wanted to be thorough: before concluding the idea was a failure, I wrote a SQL query to compare order volume for subsequent weeks between customers in test vs control.The SQL used to figure out the push notification’s efficiencyAs it turned out, our test group “beat” the control group by around 10%:‘review_5_push’ was the new type of push notification. Roughly the same amount of users clicked it, but they placed 10% more in ordersI plugged the numbers into a significance calculator, which showed it was statistically significant – or “stat-sig” – and therefore highly unlikely to be a coincidence. This meant we had a winner on our hands! But how meaningful was it, really, and what would adding the push notification mean for revenue, if rolled out to 100% of users?It turned out this experiment created an additional $1.5 million dollars, annually, with just one push notification. Wow!I was hooked. Since that day, I've shipped hundreds of experimental “winners” which generated hundreds of millions of incremental revenue for my employers. But you never forget the first one. Moments like this is what growth engineering is all about.1. What is Growth Engineering?Essentially, growth engineering is the writing of code to make a company money. Of course, all code produced by a business on some level serves this purpose, but while Product Engineers focus on creating a Product worth paying for, Growth Engineers instead focus on making that good product have a good business. To this end, they focus on optimizing and refining key parts of the customer journey, such as:Getting more people to consider the productConverting them into paying customersKeeping them as customers for longer, and spending moreWhat kinds of companies employ Growth Engineers? Places you’ve heard of, like Meta, LinkedIn, DoorDash, Coinbase, and Dropbox, are some of the ones I’ve had students from. There’s also OpenAI, Uber, Tiktok, Tinder, Airbnb, Pinterest… the list of high-profile companies goes on. Most newer public consumer companies you’ve heard have a growth engineering org, too.Typically, growth engineering orgs are started by companies at Series B stage and beyond, so long as they are selling to either consumers or businesses via SaaS. These are often places trying to grow extremely fast, and have enough software engineers that some can focus purely on growth. Before the Series B stage, a team is unlikely to be ready for growth for various reasons; likely that it hasn’t found product-market fit, or has no available headcount, or lacks the visitor traffic required to run A/B tests.Cost is a consideration. A fully-loaded growth team consisting of a handful of engineers, a PM, and a designer costs approximately 1 million dollars annually. To justify this, a rule of thumb is to have at least $5 million dollars in recurring revenue – a milestone often achieved at around the Series B stage.Despite the presence of growth engineering at many public consumer tech companies, the field itself is still quite new, as a discipline and as a proper title.Brief history of growth engineeringWhen I joined Opendoor in 2016, there was a head of growth but no dedicated growth engineers, but there were by the time I left in 2020. At MasterClass soon after, there was a growth org and a dozen dedicated growth engineers. So when did growth engineering originate?The story is that its origins lie at Facebook in 2007. The team was created by then-VP of platform and monetization Chamath Palihapitiya. Reforce founder and CEO Brian Balfour shares:“Growth (the kind found on an org chart) began at Facebook under the direction of Chamath Palihapitiya. In 2007, he joined the early team in a nebulous role that fell somewhere between Product, Marketing, and Operations. According to his retelling of the story on Recode Decode, after struggling to accomplish anything meaningful in his first year on the job, he was on the verge of being fired.Sheryl Sandberg joined soon after him, and in a hail mary move he pitched her the game-changing idea that led to the creation of the first-ever growth team. This idea not only saved his job, but earned him the lion’s share of the credit for Facebook’s unprecedented growth.At the time, Sheryl and Mark asked him, “What do you call this thing where you help change the product, do some SEO and SEM, and algorithmically do this or that?”His response: “I don’t know, I just call that, like, Growth, you know, we’re going to try to grow. I’ll be the head of growing stuff.\"And just like that, Growth became a thing.”Rather than focus on a particular product or feature, the growth team at Facebook focused on moving the needle, and figuring out which features to work on. These days, Meta employs hundreds if not thousands of growth engineers.2. What do Growth Engineers work on?Before we jump into concrete examples, let’s identify three primary focus areas that a growth engineer’s work usually involves.Business-facing work – improving the business directlyEmpowerment work – enabling other teams to improve the businessPlatform work – improving the velocity of the above activitiesLet’s go through all three:Business-facing workThis is the bread and butter of growth engineering, and follows a common pattern:Implement an idea. Try something big or small to try and move a key business metric, which differs by team but is typically related to conversion rate or retention.Quantify impact. Usually via A/B testing.Analyze impact. Await results, analyze impact, ship or roll back – then go back to the first step.Experiments can lead to sweeping or barely noticeable changes. A famous “I can’t believe they needed to test this” was when Google figured out which shade of blue generates the most clicks. At MasterClass, we tested things across the spectrum:Small: should we show the price right on the homepage, was that a winner? Yes, but we framed it in monthly terms of $15/month, not $180/year.Medium: when browsing a course page, should we include related courses, or more details about the course itself? Was it a winner? After lengthy experimentation, it was hard to tell: both are valuable and we needed to strike the right balance.Large: when a potential customer is interested, do we take them straight to checkout, or encourage them to learn more? Counterintuitively, adding steps boosted conversion!EmpowermentOne of the best ways an engineer can move a target metric is by removing themselves as a bottleneck, so colleagues from marketing can iterate and optimize freely. To this end, growth engineers can either build internal tools or integrate self-serve MarTech (Marketing Technology) vendors.With the right tool, there’s a lot that marketers can do without engineering’s involvement:Build and iterate on landing pages (Unbounce, Instapage, etc)Draft and send email, SMS and Push Notifications (Iterable, Braze, Customer.io, etc)Connect new advertising partners (Google Tag Manager, Segment, etc)We go more into detail about benefits and applications in the MarTech section of Tech Stack, below.Platform workAs a business scales, dedicated platform teams help improve stability and velocity for the teams they support. Within growth, this often includes initiatives like:Experiment Platform. Many parts of running an experiment can be standardized, from filtering the audience, to bucketing users properly, to observing statistical methodology. Historically, companies built reusable Experiment Platforms in-house, but more recently, vendors such as Eppo and Statsig have grown in popularity with fancy statistical methodologies like “Controlled Using Pre-Experiment Data” (CUPED) that give more signal with less data.Reusable components. Companies with standard front-end components for things like headlines, buttons, and images, dramatically reduce the time required to spin up a new page. No more \"did you want 5 or 6 pixels here\" with a designer; instead growth engineers rely on tools like Storybook to standardize and share reusable React components.Monitoring. Growth engineering benefits greatly from leveraging monitoring to compensate for reduced code coverage. High-quality business metric monitoring tools can detect bugs before they cause damage.When I worked at MasterClass, having monitoring at the ad layer prevented at least one six-figure incident. One Friday, a marketer accidentally broadened the audience for a particular ad from US-only, to worldwide. In response, the Facebook Ad algorithm went on a spending spree, bringing in plenty of visitors from places like Brazil and India, whom we knew from past experience were unlikely to purchase the product. Fortunately, our monitoring noticed the low-performing campaign within minutes, and an alert was sent to the growth engineer on-call, who immediately reached out to the marketer and confirmed the change was unintentional, and then shut down the campaign.Without this monitoring, a subtle targeting error like this could have gone unnoticed all weekend and would have eaten up $100,000+ of marketing budget. This episode shows that platform investment can benefit everyone; and since growth needs them most, it’s often the growth platform engineering team which implements them.As the day-to-day work of a Growth Engineer shows, A/B tests are a critical tool to both measure success and learn. It’s a numbers game: the more A/B tests a team can run in a given quarter, the more of them will end up winners, making the team successful. It’s no wonder, then, that Growth Engineering will pull out all the stops to improve velocity.3. Why Growth Engineers move faster than Product EngineersOn the surface, growth engineering teams look like product engineering ones; writing code, shipping pull requests, monitoring on-call, etc. So how do they move so much faster? The big reason lies in philosophy and focus, not technology. To quote Elena Verna, head of growth at Dropbox:“Product Engineering teams ship to build; Growth Engineering teams ship to learn.”Real-world case: price changes at MasterclassA few years ago at MasterClass, the growth team wanted to see if changing our pricing model to multiple tiers would improve revenue.Inspired in part by multiple pricing tiers for competitors such as Netflix (above), Disney Plus, and Hulu.The “multiple pricing tier” proposal for MasterClass.From a software engineering perspective, this was a highly complex project because:Backend engineering work: the backend did not yet support multiple pricing options, requiring a decent amount of engineering, and rigorous testing to make sure existing customers weren’t affected.Client app changes: on the device side, multiple platforms (iOS, iPad, Android, Roku, Apple TV, etc) would each need to be updated, including each relevant app store.The software engineering team estimated that becoming a “multi-pricing-tier” company would take months across numerous engineering teams, and engineering leadership was unwilling to greenlight that significant investment.We in growth engineering took this as a challenge. As usual, our goal was not just to add the new pricing model, but to learn how much money it might bring in. The approach we ended up proposing was a Fake Door test, which involves offering a not-yet-available option to customers to gauge interest level. This was risky, as taking a customer who’s ready to pay and telling them to join some kind of waiting list is a colossal waste, and risks making them feel like the target of a “bait and switch” trick.We found a way. The key insight was that people are only offended about a “bait and switch”, if the “switch” is worse than the “bait.” Telling customers they would pay $100 and then switching to $150 would cause a riot, but starting at $150 and then saying “just kidding, it’s only $100” is a pleasant surprise.The good kind of surprise.So long as every test “pricing tier” is less appealing – higher prices, fewer features – than the current offering, we could “upgrade” customers after their initial selection. A customer choosing the cheapest tier gets extra features at no extra cost, while a customer choosing a more expensive tier is offered a discount.We created three new tiers, at different prices. The new “premium” tier would describe the existing, original offering. Regardless of what potential customers selected, they got this “original offering,” during the experiment.The best thing about this was that no backend changes were required. There were no real, new, back-end pricing plans; everybody ended up purchasing the same version of MasterClass for the same price, with the same features. The entirety of the engineering work was on building a new pricing page, and the “congratulations, you’ve been upgraded” popup. This took just a few days.Within a couple of weeks, we had enough data to be confident the financial upside of moving to a multi-pricing-tier model would be significant. With this, we’re able to convince the rest of engineering’s leadership to invest in building the feature properly. In the end, launching multiple pricing tiers turned out to be one of the biggest revenue wins of the year.Building a skyscraper vs building a tentThe MasterClass example demonstrates the spirit of growth engineering; focusing on building to learn, instead of building to last. Consider building skyscrapers versus tents.Building a tent optimizes for speed of set-up and tear-down over longevity. You don’t think of a tent as one that is shoddy or low-quality compared to skyscrapers: it’s not even the same category of buildings! Growth engineers maximize use of lightweight materials. To stick with the tents vs skyscraper metaphor: we prioritize lightweight fabric materials over steel and concrete whenever possible. We only resort to traditional building materials when there’s no other choice, or when a direction is confirmed as correct. Quality is important – after all, a tent must keep out rain and mosquitoes. However, the speed-vs-durability tradeoff decision results in very different approaches and outcomes.4. Tech stackAt first glance, growth and product engineers use the same tooling, and contribute to the same codebases. But growth engineering tends to be high-velocity, experiment-heavy, and with limited test coverage. This means that certain “nice to have” tools for product engineering are mission-critical for growth engineers.This post is for paid subscribersSubscribeAlready a paid subscriber? Sign inA guest post byAlexey KomissaroukGrowth Engineering Advisor: alexeymk.com/growth-engSubscribe to Alexey",
    "summary": {
      "en": "**Summary of Growth Engineering**\n\nGrowth engineering is a relatively new field that sits between product engineering and marketing, focusing on writing code to help companies increase their revenue. While product engineers create products, growth engineers aim to optimize key parts of the customer journey, such as attracting new customers, converting them into paying users, and retaining them longer.\n\nKey points about growth engineering include:\n\n1. **Definition**: Growth engineers write code specifically to drive revenue, often using A/B testing to assess the impact of changes on business metrics.\n\n2. **Work Areas**:\n   - **Business-facing Work**: Directly improving business metrics through experimentation.\n   - **Empowerment Work**: Building tools that allow other teams, like marketing, to operate independently.\n   - **Platform Work**: Creating systems that enhance stability and efficiency across teams.\n\n3. **Speed of Execution**: Growth engineers typically move faster than product engineers because they prioritize learning over building long-term features. They often take shortcuts to validate ideas quickly.\n\n4. **Tech Stack**: Growth engineers use various tools and technologies to facilitate rapid experimentation and monitor results.\n\n5. **Career Path**: Growth engineering can be a great stepping stone for those aspiring to become founders or product managers, as it provides insight into business operations.\n\nIn essence, growth engineering is all about experimenting swiftly to discover what strategies can generate more revenue, often utilizing an agile and iterative approach.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "522a6ccd367a86e3",
    "title": {
      "en": "Goblin.tools: simple, single-task tools to help neurodivergent people with tasks",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://goblin.tools/",
    "score": 318,
    "by": "ValentineC",
    "time": 1742825717,
    "content": "Help\n\n                        Magic Todo acts as a standard todo list, with some special sauce. Try the  button and let it automatically come up with the steps you need to accomplish your task! The 🌶 spiciness level 🌶 gives the tool a hint about how hard or stressful you find the task. The spicier, the more steps it will attempt to break it down into. It's not an exact measure, more a hint to the system.Top level tasks are automatically assigned a category indicated by an emoji. You can filter your list to one or more categories with the  button. The filter button also allows you to hide all completed items.Common task tools (edit, remove, add subtask, and estimator) are under the  button on each item. The  icon on the left can be dragged to reorder items to anywhere.Additional actions for the entire list can be found below it. They include  synchronisation between devices,  export options , undo and redo, and  bulk actions.\n\n        .checkbox-container {\n            flex-shrink: 0;\n            width: 20px;\n        }\n\n        .spinner-border {\n            display: none;\n        }\n\n        .parent-todo {\n            margin-bottom: 8px;\n        }\n\n        .spiciness-indicator {\n            position: absolute;\n            font-size: 0.6rem;\n            transform: translateY(-100%);\n        }\n\n        .edit-input {\n            display: inline;\n            margin-right: 5px;\n        }\n\n        .edit-input-container {\n            width: 100%;\n        }\n\n        .edit-confirm,\n        .edit-cancel {\n            display: inline;\n            margin-left: 2px;\n            padding: 0 4px;\n        }\n\n        .todoText {\n            user-select: text !important;\n            -webkit-user-select: text;\n            -moz-user-select: text;\n            -ms-user-select: text;\n        }\n\n        #categoryFilterMenu {\n            --bs-dropdown-min-width: 10px !important;\n            --bs-dropdown-item-padding-x: 0;\n        }\n\n            #categoryFilterMenu .form-check {\n                padding-left: 2rem;\n                padding-right: 0.4rem;\n                text-align: center;\n                align-content: center;\n            }\n\n        .list-group-item-content {\n            margin: -6px 0 -6px -16px;\n            /*padding: var(--bs-list-group-item-padding-y) var(--bs-list-group-item-padding-x);*/\n            padding: var(--bs-list-group-item-padding-y) 8px  var(--bs-list-group-item-padding-y) 12px !important;\n        }\n\n            .list-group-item-content:hover {\n                background-color: var(--bs-tertiary-bg);\n            }\n\n        #child-frame {\n            display: none;\n        }\n\n        .highlight {\n            background-color: var(--bs-tertiary-bg);\n        }\n\n        .list-group-item {\n            outline: var(--bs-list-group-border-width) solid var(--bs-list-group-border-color) !important;\n            padding: var(--bs-list-group-item-padding-y) 0 var(--bs-list-group-item-padding-y) 18px !important;\n            margin: -1px;\n            border: 0px !important;\n            box-sizing: content-box !important;\n        }\n\n        Magic ToDo\n        Breaking things down so you don't\n\n                    🌶️🌶️🌶️\n\n                Spiciness level:\n\n                    How much breaking down do you need?\n\n                    🌶️\n                    🌶️🌶️🌶️🌶️🌶️\n\n                     Sync\n\n                        EXPERIMENTAL FEATURE\n                        use at your own risk\n\n                                Enable cloud synchronization\n\n                            Username\n\n                            Password\n\n                            Save\n                            Cancel\n\n                            Please provide credentials to synchronize your ToDo list across devices.\n                            There are no accounts, your username must simply be unique.\n                            We recommend taking a backup with  before activating sync on new devices.\n\n                                How does this work?\n\n                                All devices with the same username and password will synchronise lists.\n                                Your list is encrypted on your devices using your username and password. It can not be seen or decrypted by admins or anyone who doesn't have your credentials.\n                                Synchronization is not instant, it can take a few seconds for your changes to come through on other devices.\n\n                        Save to file\n\n                        Load from saved file\n\n                        Copy to clipboard\n\n                        Print\n\n                        Export to Todoist template\n\n                        Export to iCal file\n\n            ☑️\n\n                         Estimate all tasks\n                         Mark all as not completed\n                         Clear all estimates\n                         Clear all completed tasks\n                         Clear entire list",
    "summary": {
      "en": "**Summary of Magic Todo:**\n\nMagic Todo is a task management tool that functions like a regular to-do list but offers unique features. \n\n- **Task Breakdown:** You can use a special button to automatically generate steps for your tasks based on how difficult you find them, indicated by a \"spiciness level\" (🌶). A higher spiciness level means more detailed steps.\n\n- **Task Categories:** Each task is assigned a category represented by an emoji, and you can filter tasks by these categories.\n\n- **Task Management:** Common options like editing, removing, adding subtasks, and estimating tasks are available for each item. You can also reorder tasks by dragging them.\n\n- **Additional Features:** There are options for synchronizing your to-do list across devices, exporting tasks, undoing and redoing actions, and performing bulk actions.\n\n- **Synchronization:** To sync your tasks across devices, you need to create a unique username and password. Your data remains encrypted and secure.\n\nOverall, Magic Todo helps you organize and break down tasks effectively while providing several useful features for task management.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "ed5299a42cd7c383",
    "title": {
      "en": "Btrfs Adding Fast/Realtime ZSTD Compression and Other Performance Optimizations",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.phoronix.com/news/Linux-6.15-Btrfs",
    "score": 8,
    "by": "todsacerdoti",
    "time": 1742888476,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "c32c7963edc80997",
    "title": {
      "en": "Show HN: LinkedIn sucks, so I built a better one",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://heyopenspot.com/",
    "score": 439,
    "by": "fliellerjulian",
    "time": 1742755960,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "c1017d760737a62f",
    "title": {
      "en": "Osgint – OSINT tool to find information about GitHub user",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/hippiiee/osgint",
    "score": 111,
    "by": "CHEF-KOCH",
    "time": 1742795502,
    "content": "OSGINT\n\n  Retrieve informations about a github username/email\n\nFeatures\n\n Find github username from an email\n Find email from github username (not working all the time)\n Find informations about a profile (account creation date, public gists, id, public pgp, public ssh ...)\n\nRequirements\npip3 install -r requirements.txt\n\nUsage\n$ python3 osgint.py -h\n\n .d88888b.                    d8b          888\nd88P\" \"Y88b                   Y8P          888\n888     888                                888\n888     888 .d8888b   .d88b.  888 88888b.  888888\n888     888 88K      d88P\"88b 888 888 \"88b 888\n888     888 \"Y8888b. 888  888 888 888  888 888\nY88b. .d88P      X88 Y88b 888 888 888  888 Y88b.\n \"Y88888P\"   88888P'  \"Y88888 888 888  888  \"Y888\n                          888  v1.0.0\n                     Y8b d88P\n                      \"Y88P\"\nBy Hippie | https://twitter.com/hiippiiie\n\nusage: osgint.py [-h] [-u USERNAME] [-e EMAIL] [--json]\n\noptions:\n  -h, --help            show this help message and exit\n  -u USERNAME, --username USERNAME\n                        Github username of the account to search for (default: None)\n  -e EMAIL, --email EMAIL\n                        Email of the account to search for github username (default: None)\n  --json                Return a json output (default: False)\n\nExample output\nusername\n$ ./osgint.py -u hippiiee\n[+] login : hippiiee\n[+] id : 41185722\n[+] avatar_url : https://avatars.githubusercontent.com/u/41185722?v=4\n[+] name : Hippie\n[+] blog : https://hippie.cat\n[+] bio : Hi !\n[+] public_repos : 10\n[+] public_gists : 0\n[+] followers : 8\n[+] following : 9\n[+] created_at : 2018-07-13T08:28:00Z\n[+] updated_at : 2022-08-21T13:11:36Z\n[+] public_gists : https://gist.github.com/hippiiee\n[+] GPG_keys : https://github.com/hippiiee.gpg\n[+] GPG_key_id : 27cbb171ff857c58\n[+] email : hquere@e3r4p3.42.fr hippolyte.q@gmail.com\n\n$ ./osgint.py -u hippiiee --json\n{\n    \"GPG_key_id\": \"27cbb171ff857c58\",\n    \"GPG_keys\": \"https://github.com/hippiiee.gpg\",\n    \"avatar_url\": \"https://avatars.githubusercontent.com/u/41185722?v=4\",\n    \"bio\": \"Hi !\",\n    \"blog\": \"https://hippie.cat\",\n    \"created_at\": \"2018-07-13T08:28:00Z\",\n    \"email\": [\n        \"hquere@e3r4p3.42.fr\",\n        \"hippolyte.q@gmail.com\"\n    ],\n    \"followers\": 8,\n    \"following\": 9,\n    \"id\": 41185722,\n    \"login\": \"hippiiee\",\n    \"name\": \"Hippie\",\n    \"public_gists\": \"https://gist.github.com/hippiiee\",\n    \"public_repos\": 10,\n    \"updated_at\": \"2022-08-21T13:11:36Z\"\n}\n\nEmail\n$ ./osgint.py -e chrisadr@gentoo.org\n[+] username : ChrisADR\n\n$ ./osgint.py -e chrisadr@gentoo.org --json\n{\n    \"username\": \"ChrisADR\"\n}\n\nHow does it works ?\nTo get a user email, osingt is checking :\n\nall the public commits of the user, if the email is not hidden in one of the commit it will be added to the list\nif the user have a GPG key, if he has one, it's getting the email from the content of the GPG after a base64 decode\ngithub user API\n\nTo get a user email, osgint is checking :\n\ngithub user API\n🚧 spoofing a commit with the email, then checking the name in the commit history (working every time) 🚧 (Work In Progress)\n\nProject inspired from Zen",
    "summary": {
      "en": "**OSGINT Summary**\n\nOSGINT is a tool designed to gather information about GitHub users using their usernames or email addresses. \n\n**Key Features:**\n- Find a GitHub username from an email.\n- Find an email from a GitHub username (may not always work).\n- Retrieve details about a GitHub profile, including account creation date, public repositories, and more.\n\n**Requirements:**\n- Install dependencies with: `pip3 install -r requirements.txt`.\n\n**Usage:**\n- Run the command: `$ python3 osgint.py -h` for help.\n- Options include:\n  - `-u USERNAME`: Search using a GitHub username.\n  - `-e EMAIL`: Search using an email to find the GitHub username.\n  - `--json`: Get the output in JSON format.\n\n**Examples:**\n- To find information by username:\n  ```\n  $ ./osgint.py -u hippiiee\n  ```\n- To find a username by email:\n  ```\n  $ ./osgint.py -e chrisadr@gentoo.org\n  ```\n\n**How It Works:**\n- To find a user's email, OSGINT checks public commits and GPG keys. It also uses the GitHub user API to gather information.\n- Some methods for retrieving emails may involve checking commit history to detect spoofed emails.\n\nThe project is inspired by Zen and is maintained by a user named Hippie.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "ac72d66503620b34",
    "title": {
      "en": "First Known Photographs of Living Specimens",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.inaturalist.org/projects/first-known-photographs-of-living-specimens",
    "score": 120,
    "by": "Morizero",
    "time": 1742501985,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "b0369bf4f1523bda",
    "title": {
      "en": "'Pathetic European free-loading': What was said in Trump officials' group chat",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://news.sky.com/story/pathetic-european-free-loading-what-was-said-in-leaked-group-chat-13335365",
    "score": 6,
    "by": "austinallegro",
    "time": 1742903144,
    "content": "document.currentScript.parentNode.config = {\"id\":\"leaderboard\",\"ad-type\":\"leaderboard\",\"targeting\":{\"platform\":\"live\",\"advert-targeting\":\"'platform': 'live'\",\"artId\":\"13335365\"},\"size\":{\"mobile\":[[320,50],[300,50]],\"tablet\":[[728,90]],\"desktop\":[[728,90],[970,250]]}}\n\n        Explainer\n            Explainer\n    'Pathetic European free-loading': What was said in Trump officials' group chat\n\n                                The editor-in-chief of The Atlantic was an accidental fly on the wall in a group chat where senior US officials discussed airstrike plans and what they thought about \"free-loading\" Europe - with a healthy sprinkling of emojis. Here's what was said.\n\n    Tuesday 25 March 2025 09:56, UK\n\n                Please use Chrome browser for a more accessible video player\n\n                2:39\n\n              Leaked security chat explainedMessages were put onto a group chat that included US vice president JD Vance and defence secretary Pete Hegseth.ShareVideo Player is loading.Play VideoPlaySkip BackwardSkip ForwardMuteCurrent Time0:00/Duration-:-Loaded: 0%0:00Stream TypeLIVESeek to live, currently behind liveLIVERemaining Time--:-1xPlayback RateChaptersChaptersDescriptionsdescriptions off, selectedCaptionsCaptions settings, opens captions settings dialogCaptions off, selectedAudio TrackFullscreenThis is a modal window.\n        Video Unavailable\n        Error Code: MEDIA_ERR_SRC_NOT_SUPPORTED\n        Technical details\n        : This video is currently unavailable\n\n      Session ID: 2025-03-25:6d36ba9a2fd37e94e097bb44 Player Element ID: id_f79d6224-8202-4ee7-be0d-aabaf493f8c5Beginning of dialog window. Escape will cancel and close the window.TextColorWhiteBlackRedGreenBlueYellowMagentaCyanOpacityOpaqueSemi-TransparentText BackgroundColorBlackWhiteRedGreenBlueYellowMagentaCyanOpacityOpaqueSemi-TransparentTransparentCaption Area BackgroundColorBlackWhiteRedGreenBlueYellowMagentaCyanOpacityTransparentSemi-TransparentOpaqueFont Size50%75%100%125%150%175%200%300%400%Text Edge StyleNoneRaisedDepressedUniformDrop shadowFont FamilyProportional Sans-SerifMonospace Sans-SerifProportional SerifMonospace SerifCasualScriptSmall CapsResetDoneClose Modal DialogEnd of dialog window.Close Modal DialogThis is a modal window. This modal can be closed by pressing the Escape key or activating the close button.Close Modal DialogThis is a modal window. This modal can be closed by pressing the Escape key or activating the close button.\n\n              Leaked security chat explained\n\n                Why you can trust Sky News\n\n      A group chat of top US officials - which a journalist was accidentally added to - discussed plans to conduct airstrikes on Yemen's Iran-backed Houthis and fiery critique of \"pathetic\" Europe.Editor-in-chief of The Atlantic, Jeffrey Goldberg, was added to the \"Houthi PC small group\" on Signal on Thursday 13 March.\n\nHe wrote about what was said in the days that followed in his article The Trump Administration Accidentally Texted Me Its War Plans.Here's what we know was said on encrypted messaging service Signal - in a group that included US vice president JD Vance, defence secretary Pete Hegseth, national security adviser Mike Waltz and director of national intelligence Tulsi Gabbard.Thursday 13 March\n\n                    document.currentScript.parentNode.config = {\"id\":\"teads\",\"ad-type\":\"teads\",\"targeting\":{\"platform\":\"live\",\"advert-targeting\":\"'platform': 'live'\",\"artId\":\"13335365\"},\"size\":{\"mobile\":[[4,4]],\"tablet\":[[4,4]],\"desktop\":[[4,4]]}}\n\nThis day was seemingly aimed at establishing who would make up the small group.A message to the group, from Michael Waltz read: \"Team - establishing a principles [sic] group for coordination on Houthis, particularly for over the next 72 hours. My deputy Alex Wong is pulling together a tiger team at deputies/agency Chief of Staff level following up from the meeting in the Sit Room this morning for action items and will be sending that out later this evening.\"The message continued: \"Pls provide the best staff POC from your team for us to coordinate with over the next couple days and over the weekend. Thx.\"One minute later, the secretary of state Marco Antonio Rubio wrote: \"Mike Needham for State,\" apparently designating the current counsellor of the state department as his representative.\n\n                                        document.currentScript.parentNode.config = {\"id\":\"mpu-1\",\"ad-type\":\"mpu-1\",\"showLabel\":true,\"targeting\":{\"platform\":\"live\",\"advert-targeting\":\"'platform': 'live'\",\"artId\":\"13335365\"},\"size\":{\"mobile\":[[300,250],[300,600]],\"tablet\":[[300,250],[300,600]],\"desktop\":[[300,250],[300,600]]}}\n\nJD Vance wrote: \"Andy baker for VP.\" One minute after that, Tulsi Gabbard, the director of national intelligence, wrote: \"Joe Kent for DNI.\"Nine minutes later, treasury secretary Scott Bessent wrote: \"Dan Katz for Treasury.\"At 4.53pm Pete Hegseth wrote: \"Dan Caldwell for DoD.\"And at 6.34pm, a user called \"Brian\" wrote \"Brian McCormack for NSC.\" Someone called \"John Ratcliffe\" then wrote with the name of a CIA official to be included in the group.\n\n        Who's who in the chat?\n\n        Eighteen people were part of the chat - here is who appears to have been added and how they were identified in the chat:\n        JD Vance - US deputy president\n        Michael Waltz - national security adviser\n        Marco Antonio Rubio - the secretary of state, identified as MAR\n        Tulsi Gabbard - the director of national intelligence, identified as TG\n        Scott Bessent - treasury secretary, identified as Scott B\n        Pete Hegseth - US defence secretary\n        Susie Wiles - White House chief of staff\n        Stephen Miller - deputy White House chief of staff, called S M in the chat (identified only by his initials, which the report says Mr Goldberg \"took to stand for Stephen Miller\")\n        Steve Witkoff - US Middle East and Ukraine negotiator\n        Jeffrey Goldberg - editor-in-chief of The Atlantic magazine, identified as JG\n        Other members included various National Security Council officials and a CIA official who was not named in the report because they are an active intelligence officer.\n\nFriday 14 MarchAt 8.05am, Mr Waltz texted the group: \"Team, you should have a statement of conclusions with taskings per the Presidents [sic] guidance this morning in your high side inboxes,\" with \"high side\" referring to classified computer systems.\"State and DOD [Department of Defence], we developed suggested notification lists for regional Allies and partners. Joint Staff is sending this am [morning] a more specific sequence of events in the coming days and we will work w DOD to ensure COS [chief of staff], OVP [office of the vice president] and POTUS [president of the United States] are briefed.\"Mr Goldberg says at this point, a \"fascinating\" policy discussion commenced, during which JD Vance wrote: \"Team, I am out for the day doing an economic event in Michigan. But I think we are making a mistake.\"3 percent of US trade runs through the suez. 40 percent of European trade does. There is a real risk that the public doesn't understand this or why it's necessary. The strongest reason to do this is, as POTUS said, to send a message.\"I am not sure the president is aware how inconsistent this is with his message on Europe right now. There's a further risk that we see a moderate to severe spike in oil prices. I am willing to support the consensus of the team and keep these concerns to myself. But there is a strong argument for delaying this a month, doing the messaging work on why this matters, seeing where the economy is, etc.\"\n\n        Spreaker\n\n                This content is provided by Spreaker, which may be using cookies and other technologies.\n                To show you this content, we need your permission to use cookies.\n                You can use the buttons below to amend your preferences to enable Spreaker cookies or to allow those cookies just once.\n                You can change your settings at any time via the Privacy Options.\n\n                Unfortunately we have been unable to verify if you have consented to Spreaker cookies.\n                To view this content you can use the button below to allow Spreaker cookies for this session only.\n\n                Enable Cookies\n                Allow Cookies Once\n\n👉 Follow Trump 100 on your podcast app 👈At 8.27am, a message arrived from the Pete Hegseth reading: \"VP: I understand your concerns - and fully support you raising w/ POTUS. Important considerations, most of which are tough to know how they play out (economy, Ukraine peace, Gaza, etc). I think messaging is going to be tough no matter what - nobody knows who the Houthis are - which is why we would need to stay focused on: 1) Biden failed & 2) Iran funded.\"Waiting a few weeks or a month does not fundamentally change the calculus. 2 immediate risks on waiting: 1) this leaks, and we look indecisive; 2) Israel takes an action first - or Gaza cease fire falls apart - and we don't get to start this on our own terms. We can manage both.\"We are prepared to execute, and if I had final go or no go vote, I believe we should. This [is] not about the Houthis. I see it as two things: 1) Restoring Freedom of Navigation, a core national interest; and 2) Reestablish deterrence, which Biden cratered. But, we can easily pause. And if we do, I will do all we can to enforce 100% OPSEC - operations security.\"I welcome other thoughts.\"Later in the conversation, Mr Waltz criticised the limited capabilities of European navies, writing: \"Whether it's now or several weeks from now, it will have to be the United States that reopens these shipping lanes. Per the president's request we are working with DOD and State to determine how to compile the cost associated and levy them on the Europeans.\"Mr Vance addressed Mr Hegseth in a message reading: \"If you think we should do it let's go. I just hate bailing Europe out again.\"Mr Hegseth replied: \"VP: I fully share your loathing of European free-loading. It's PATHETIC. But Mike is correct, we are the only ones on the planet (on our side of the ledger) who can do this. Nobody else even close. Question is timing. I feel like now is as good a time as any, given POTUS directive to reopen shipping lanes. I think we should go; but POTUS still retains 24 hours of decision space.\"An account believed to be the deputy White House chief of staff, Stephen Miller, then said: \"As I heard it, the president was clear: green light, but we soon make clear to Egypt and Europe what we expect in return. We also need to figure out how to enforce such a requirement. EG, if Europe doesn't remunerate, then what? If the US successfully restores freedom of navigation at great cost there needs to be some further economic gain extracted in return.\"This was followed by the last text of the day, from Mr Hegseth, who wrote at 9.46am: \"Agree.\"Saturday 15 MarchAt 11.44am, Mr Hegseth posted in Signal a \"TEAM UPDATE\" - but The Atlantic has not published what was said.Mr Goldberg wrote in his article: \"I will not quote from this update, or from certain other subsequent texts. The information contained in them, if they had been read by an adversary of the United States, could conceivably have been used to harm American military and intelligence personnel, particularly in the broader Middle East, Central Command's area of responsibility.\"He did say Mr Hegseth had sent lengthy text about how the first detonations in Yemen would be felt two hours from then, at 1.45pm Eastern Time. He said he waited in his car in a supermarket parking lot, waiting to see if Houthi targets would be bombed.He said he went on X and searched Yemen at about 1.55pm, when he saw reports of explosions being heard across Sanaa, its capital city.He said the only person to respond to Mr Hegseth in the morning was Mr Vance, who wrote: \"I will say a prayer for victory\", along with two prayer emojis.\n\n        Image:\n        Pic: The Atlantic\n\nAt 1.48am, Mr Waltz provided an update in the group which Mr Goldberg did not quote in full, but he did say the national security adviser described the operation as an \"amazing job\".John Ratcliffe then wrote: \"A good start.\"Mr Goldberg said Mr Waltz responded with three emojis: a fist, an American flag and fire.Others soon joined in, including Mr Rubio, who wrote, \"Good Job Pete and your team!!,\" and Susie Wiles, who texted: \"Kudos to all - most particularly those in theater and CENTCOM! Really great. God bless.\"Mr Witkoff responded with five emojis: two hands-praying, a flexed bicep, and two American flags.Tulsi Gabbard responded: \"Great work and effects!\"Mr Goldberg said the after-action discussion included assessments of damage done, including the likely death of a specific individual - he did not say who.",
    "summary": {
      "en": "A recent leak revealed a group chat among senior U.S. officials, including Vice President JD Vance and Defense Secretary Pete Hegseth, discussing military airstrikes on Yemen's Iran-backed Houthis. The editor-in-chief of The Atlantic, Jeffrey Goldberg, was accidentally added to this chat and later wrote about the conversations.\n\nKey points from the discussions included:\n\n1. **Military Coordination**: Officials were organizing a small group to manage the airstrike plans and coordinate efforts over the following days.\n   \n2. **Criticism of Europe**: There were strong sentiments expressed about Europe being \"pathetic\" and \"free-loading,\" with officials discussing how the U.S. was often left to handle military actions alone.\n\n3. **Economic Concerns**: Some officials, including Vance, raised concerns about the potential economic impact of the airstrikes, suggesting that public understanding of the situation could be lacking.\n\n4. **Decision-Making**: The group debated the timing of the airstrikes, with some arguing for immediate action and others suggesting a delay for better messaging and preparation.\n\n5. **Aftermath of the Strikes**: After the strikes, there was a discussion on the effectiveness of the operation, with officials expressing satisfaction and celebrating the military's achievements.\n\nOverall, the chat highlighted internal discussions among U.S. leaders regarding military strategy, international relations, and the role of Europe in global defense efforts.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "449a2c2ef5726266",
    "title": {
      "en": "Japanese scientists use stem cell treatment to restore movement in spinal injury",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://medicalxpress.com/news/2025-03-japanese-scientists-stem-cell-treatment.html",
    "score": 246,
    "by": "pseudolus",
    "time": 1742811919,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "7141c6c92b9df51f",
    "title": {
      "en": "Open Source Mythology",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://lgug2z.com/articles/on-open-source-mythology/",
    "score": 12,
    "by": "yreew",
    "time": 1742864007,
    "content": "On Open Source Mythology\n\n        2025-03-23 ::\n\n      LGUG2Z\n\n      #licensing\n\n        There are two points of popular open source mythology this post will share my\nexperience with:\n\nPeople won’t use your project if you don’t use an Open Source\nInitiative-approved license\nPeople won’t contribute to to your project if you don’t use an Open Source\nInitiative-approved license\n\nMany people have ideas about how society should be like and what must be done\nto change institutions and to work for a revolution. But this is not enough.\nOften these ideas do not conform to reality and if they do conform to reality\nthere is only one way to test them: Try to put them to work and see if they\nsucceed. Testing our ideas in concrete work is the only way we will ever know\nif they are correct.\n– James Forman - Twenty Enemies\n(1971)\n\nI maintain a tiling window manager for Windows called\nkomorebi which is made available under\nthe Komorebi License.\nThe Komorebi License is both an educational source\nlicense which\nensures the availability of source code to individuals for personal use,\nmodification, learning and reference, and a firewall license which preserves an\nindividual’s freedom to refuse by default (… like a firewall!)\nThe preservation of an individual’s freedom to refuse by default means that the\nKomorebi License is not an Open Source Initiative-approved license.\nAs of the publication of this post, komorebi has 126k\ndownloads\nand 10.6k stargazers on\nGitHub, and while I am still by far the most active and primary\ncontributor to the\nproject, there is a healthy stream of users who like to contribute patches for\nthings they have a personal interest in improving.\n\nIt often happens, however, that thinking lags behind reality; this is because\nman’s cognition is limited by numerous social conditions. We are opposed to\ndie-hards in the revolutionary ranks whose thinking fails to advance with\nchanging objective circumstances and has manifested itself historically as\nRight opportunism. These people fail to see that the struggle of opposites\nhas already pushed the objective process forward while their knowledge has\nstopped at the old stage. This is characteristic of the thinking of all\ndie-hards. Their thinking is divorced from social practice, and they cannot\nmarch ahead to guide the chariot of society; they simply trail behind,\ngrumbling that it goes too fast and trying to drag it back or turn it in the\nopposite direction.\n– 毛泽东 - ON PRACTICE\n(1937)\n\nI do not believe I am alone in having the desire to share what I have learned\nso that other people might learn from it, while not buying into the idea that\nin order to do that effectively, I also have to use a software license which\nrequires me to forfeit my freedom to refuse.\nI do not believe that there needs to be a dependency between sharing what I\nhave learned in the form of source code and permitting its use in the\ncommission of evils including but not limited\nto genocide, or participating in my own exploitation by unimaginably wealthy\ncorporations which, incidentally, are also active\nparticipants\nin the commission of evils including but not limited to genocide.\n(It’s probably no surprise that I’m also not a fan of forcing a dependency\nbetween funding for essential government services and funding a genocide)\n\nIn Russia, there was a fundamental difference between the contradiction\nresolved by the February Revolution and the contradiction resolved by the\nOctober Revolution, as well as between the methods used to resolve them.\n[…]\nThe dogmatists do not observe this principle; they do not understand that\nconditions differ in different kinds of revolution and so do not understand\nthat different methods should be used to resolve different contradictions; on\nthe contrary, they invariably adopt what they imagine to be an unalterable\nformula and arbitrarily apply it everywhere, which only causes setbacks to\nthe revolution or makes a sorry mess of what was originally well done.\n– 毛泽东 - ON CONTRADICTION\n(1937)\n\nThe conditions of today differ from the conditions of 1983 as they differ from the\nconditions of 1998. The contradictions of today differ from the contradictions\nof 1983 as they differ from the contradictions of 1998.\nIf you are a software developer who wants to share what you have learned for\nthe educational benefit of others in the world, but are uncomfortable with the\nstrings attached by open source software licensing, I invite you to reject the\ndogmatists, and to renew and share the experiences of your social practice.\n\nWe do ourselves and the revolutionary struggle a great disservice if we do\nnot begin to realize that our concrete work […] must be put into writing\nfor the benefit of all the world, especially ourselves.\nWe must constantly sum up our work, our revolutionary efforts, our social\npractices so that we might learn what is correct and positive and what must be\nfurther developed.\nAt the same time we must discard that which is irrelevant and negative. To\nassume that the Bolsheviks, the Chinese, the Vietnamese, the Cubans, the\nAlgerians and other revolutionaries have not made mistakes from which they\nprofited is not to understand that there are no blueprints for revolutions.\nThe concrete work that we do in the final analysis is the basis for truth.\n– James Forman - Twenty Enemies\n(1971)\n\n        Read other posts\n\n                Educational Source Software\n                →",
    "summary": {
      "en": "This text discusses two common beliefs about open source software: \n\n1. People won't use your project without an Open Source Initiative-approved license.\n2. People won't contribute to your project without such a license.\n\nThe author, who maintains a Windows tiling window manager called komorebi under a unique license, argues that it’s possible to share knowledge and code without giving up personal freedoms. They highlight that their project has been well-received despite not using a traditional open-source license, demonstrating that user engagement and contributions can occur outside established norms.\n\nThe author emphasizes that ideas about societal change must be tested through real-world application. They urge software developers who want to share their knowledge to reject rigid dogmas and adapt their approaches to modern conditions, learning from both successes and mistakes in past revolutionary movements.\n\nIn conclusion, the text encourages sharing experiences and practices in software development, stressing the importance of learning from concrete work rather than adhering strictly to outdated frameworks.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "72c8f64ceeefa7f5",
    "title": {
      "en": "The Mystery of the Oldest Writing System Remained Unsolved Until 1856",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.smithsonianmag.com/history/mystery-worlds-oldest-writing-system-remained-unsolved-until-four-scholars-raced-decipher-it-180985954/",
    "score": 78,
    "by": "wyclif",
    "time": 1742729075,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "a984f46e7f77422e",
    "title": {
      "en": "Ask HN: Should I leave the company I co-founded?",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": null,
    "score": 38,
    "by": "throwaway03902",
    "time": 1742864412,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "2349423c5fce781f",
    "title": {
      "en": "Total lunar eclipse over Teide crater, Tenerife – a project with many obstacles",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://lrtimelapse.com/news/total-lunar-eclipse-over-teide-crater-tenerife/",
    "score": 164,
    "by": "elijahparker",
    "time": 1742569456,
    "content": "← LRTimelapse 7.3 released!Total lunar eclipse over the Teide crater, Tenerife – a project with many obstacles… Posted on 2025-03-21 by Gunther WegnerRarely have I thought so often about abandoning a photography project as during this lunar eclipse on Tenerife. Somehow, everything seemed to conspire against us this time. This is the story of an elaborately planned photo project where almost everything went wrong, but perseverance ultimately paid off. But let’s start from the beginning…Germans: Hier gibt es eine Deutsche Version.We, that is Uli and I, have made it almost a tradition to meet on Tenerife (Canary Islands, Spain) once a year. This time, Uli said, hey, on March 14th there’s a total lunar eclipse, and I’ve calculated a location from which we can photograph the fully eclipsed moon setting directly over the crater of Teide at dawn!It sounded almost too good to be true—because typical lunar eclipses are rather dull photographically, especially after you’ve seen and photographed a few. Without an interesting foreground, pictures of a red moon look the same anywhere in the world. Technically challenging, perhaps, but not very creative.A Unique ProjectNow, several exciting factors came together in Uli’s idea:We would experience a total eclipse on Tenerife, while in Germany, for example, the moon would only be partially eclipsed.We would have a foreground—the impressive crater of Teide, Tenerife’s landmark and Spain’s highest mountain at 3,715 meters.The totality would occur during dawn, meaning the sky would already be somewhat blue, and the moon would be low enough for us to use the crater as a foreground. We expected very aesthetic images from the contrast of the red eclipsed moon against the blue sky.According to my research, this opportunity to photograph a totally eclipsed moon over Teide wouldn’t occur again within the next 10 years—or possibly even much longer; I didn’t look further. These images would be genuinely unique.The PlanningUli used the smartphone app Planit for planning. This app not only displays exact eclipse timings but also shows the moon’s position relative to the observer’s location—in three dimensions. The app has quite a learning curve, but roughly speaking:You define the summit of Teide as your “target,” jump to the app’s day and time of moonset, and primarily search for a camera position reachable by foot.On the left, I’ve aligned the “scene” with the peak of Teide. The blue line on the right marks the spot where we’d need to be standing for the shot, based on where it intersects the trail.This means looking for an intersection on the blue line with a hiking trail or road. On Tenerife, there was actually only one reachable location, on a hiking path in the Canadas, below the caldera rim—about an hour’s hike from the nearest parking lot. This spot becomes your virtual camera position in Planit.Once you’ve found this spot, you can switch Planit to the VR camera mode to see Teide’s peak, the moon, and the Earth’s shadow through a virtual camera. You can even specify the focal length.We need to move further to the right to get the moon lined up over the crater. You can use the arrows to shift the camera position accordingly.Now, you can finely tune both the camera’s time and position—we want the fully shadowed moon precisely descending into the crater.Finally, you save the GPS coordinates for your chosen location.Leaving Nothing to ChanceRather than navigating blindly at night, we decided to scout the location during daylight first.Beautiful Landscapes in the Canadas del TeideOn a sunny morning, we drove an hour and a half up to Teide National Park and hiked to the spot. The route was quite strenuous, but the sun was shining, it was about 10 degrees Celsius—not too cold—and the solitude and landscape were fantastic.We didn’t want to think about the night yet, knowing temperatures would drop below freezing, coupled with the wind.In the background you can see the edge of the Caldera, the giant outer crater that surrounds the Pico del Teide.“A few meters more to the right,” said Uli, and I drew a line in the dust with my foot.Here’s where it would happen.At the moment, the sky was bright blue, and there was no sign of the moon. Hardly imaginable that in a few days, from exactly this spot, it would be precisely above the crater and eclipsed.We felt optimistic. This could really work out. On our way back, a wind began to pick up, funneling down from the direction of the Izaña observatories into our valley, like through a jet.The temperature dropped rapidly, and we already had to push a bit against the wind. A first, small taste of what was still ahead of us…Equipment ShortcomingsI was not entirely happy with the equipment I had available on the island. Usually, I don’t photograph much with telephoto lenses here, so my longest focal length was a 70-200mm f/2.8.Uli—always hardcore—had traveled to the island with 60 kg of gear, prepared for everything. Besides all his Sony lenses (200-600mm) and his Nikon Coolpix P1000 (24-3000mm!), he even had a 300mm and a Nikon 400mm f/2.8 with him. Crazy.“Since you have so many lenses – could you lend me your Nikon?” (The only lens of his that would fit my cameras.) Uli agreed. One less worry, as 200mm was quite short for moon photography.Another issue was my tripod situation. I had only one reliable tripod on the island. The second one didn’t deserve the name—I usually just use it as a holder for my tablet.Since this was a unique opportunity, I wanted to use two cameras. I had my Nikon Z8 and the Nikon Z50 II with me.So I decided to ask a friend—who was planning to visit us a few days later—to bring me a tripod. Unfortunately, he had to cancel the trip last minute, so once again, I was left tripod-less. Ordering stuff here on the island is a bit tricky, and good camera stores are few and far between. So I figured I’d head into the capital to buy one there. The selection I had briefly checked online wasn’t huge, but there was one tripod that actually deserved the name (and to be honest, I’m pretty into it now).After that little half-day trip, I could check off another item from the list. Now everything should finally go smoothly! (… though hindsight is always 20/20… ;-))Bad weather is coming…Weather ChallengesAlright, but the weather was still a big question mark. For days—actually, weeks—there’d been this weird weather pattern hanging over the Atlantic. Weird meaning strong winds from the west (instead of the usual northeast trade winds), rain, even snow up in the mountains, really bad weather in the north of the island, and high clouds—yep, even over the rim of the caldera, which was exactly where we wanted to shoot.And sure enough, the evening of the event was super cloudy. We kept checking the webcams and forecasts over and over.Uli wanted to head up the night before the lunar eclipse to set up a few wide-angle cameras for some time-lapse shots, planning to pick them up again the next morning.I was hoping to catch a bit of sleep beforehand. The alarm was set for 2 a.m.Worst-case scenario kicks inJust as I had packed everything and was about to head to bed, Uli called: “I’ve got good news and bad news. The good—weather’s actually not that bad. Not as windy as we feared.”“Sounds promising—and the bad?”“While I was out setting up my cameras, someone broke into my car and stole my second backpack—with all my telephoto lenses and other cameras…”“What??!!”Uli was surprisingly calm, considering he’d just lost two-thirds of his gear: “It’s insured…” — but still, how were we supposed to shoot the lunar eclipse now, without any long lenses?It really felt like the universe was telling us it wasn’t meant to be. I only had the 70–200 mm, and Uli’s longest lens left was a 135 mm. Not exactly ideal for capturing a lunar eclipse.If it had happened just a day earlier, we might’ve been able to borrow some lenses. But just hours before? No chance.We went back and forth. “So should we just call it off? Weather’s supposed to be unstable anyway…”Thinking about calling it offThat little inner voice—the one that already had zero interest in getting up at 2 a.m. and hiking for hours in the cold—kept chiming in: “This is going to be a bust anyway, it’s way too cold and probably all clouded over. Why don’t you just shoot it from down here in town, at the pier? You could sleep in two extra hours, it’s less windy, and way warmer.”Uli said he’d drive back down first and stop by a couple of electronics stores along the way—maybe, just maybe, he could find a telephoto lens there. Luckily, shops stay open pretty late on Tenerife…“Alright, let’s talk again once you’re back.”I kept refreshing the weather apps and checking webcams. Ran the numbers in Planit to figure out if we could at least capture the event down here on the coast. Sure, it would work… but it just wouldn’t be that once-in-a-lifetime shot.…or maybe not?At some point, Uli called back: “They don’t even carry camera gear at all anymore at places like MediaMarkt—but I just remembered I actually have another 300 mm lens lying around…” Me: “🙈 speechless…” Alright then—300 mm for him, 200 mm for me… not ideal, but worth a shot. Inner slacker, pipe down. “Alright, let’s do this. I’ll pick you up at 2:30,” I said. As you can probably guess, it was a short night. I showed up right on time at Uli’s: “Wow, traveling light, huh?” — honestly, without some dark humor, this whole thing would’ve been unbearable…Let’s go—up the mountain!Already in Vilaflor, the last village before the national park, the car beeped at 4°C. A few curves later, the roads were wet, and a little further up, they even started to sparkle with frost. I crawled up into the national park at 20 km/h. As soon as we entered the Caldera, we could see clouds spilling over the rim from the north—yep, exactly the direction we needed to head. “The observatories are still completely buried in clouds,” Uli said. Yeah… I figured as much. We drove past the massive “Roques de Garcia.” The full moon was so bright, we could actually see the landscape with the naked eye. At one point, I switched off the headlights—crazy how bright that full moon is.Is the weather about to ruin everything?“What if we just shoot the eclipse here, with the silhouette of the Roques in the foreground?”We thought about it for a moment. The downside: the moon would dip behind them later, since the rocks aren’t that tall, and the sky would already be getting brighter with dawn. Plus, there’d be a much higher chance of bumping into other people here—people who might wander straight into our timelapses. And honestly, it just wouldn’t be the shot we’d spent so much time planning for.So, we kept going. From the Teide cable car station onwards, the road got damp again, and I had to slow way down. These curves are no joke—and there are no winter tires on Tenerife.Wisps of clouds started forming above us. Just 4 km to go. The clouds kept thickening. Oh man, this is not looking good…Eventually, we reached the parking lot near El Portillo. The clouds were spilling into the caldera from the left and drifting right over us.Down there, where the clouds are drifting—that’s where we need to go. (Shot on smartphone)“Let’s walk over to the viewpoint and check things out—worst case, we head back to the Roques. Timing-wise, we’d still make it.”But honestly, the viewpoint didn’t really give us any new insights. The clouds were still there, and we couldn’t tell whether they’d get better or worse. We figured we had about a 50:50 chance of getting our shot. At the Roques? Pretty much a guaranteed 100%. But of course… it just wouldn’t be the same. So—what to do?We decided to take the riskIn the end, we went with the more uncertain, but potentially way more rewarding option—and started the hike.Inside the caldera (Smartphone)Thanks to the full moon, we didn’t need a headlamp or any other light. Even the initial bit of scrambling over some rocks worked just fine in moonlight alone. As timelapse and astro photographers, we naturally have a bit of an aversion to those insanely bright LED floodlights people call “headlamps” these days. And speaking of which—we actually saw quite a few of them, despite the unstable weather, making their way up the slopes of Teide. It never ceases to amaze me how many people head up there, no matter the conditions.And if it was already -4°C and pretty windy where we were at 2,200 meters, it must’ve been closer to -10°C up there, more than 1,000 meters higher—with some seriously brutal wind.Teide in the full moonlight (Smartphone)But we had layered up properly, and while we were hiking, we weren’t cold at all. The wind was at our backs, too.Setting upThe moon was still fairly high, so we had plenty of time to set everything up without rushing. Uli planned to use his main camera with the 300 mm on the Polarie star tracker. Getting the mount perfectly aligned with the North Star always takes a little time—but luckily, he managed to catch a clear glimpse of Polaris through a gap in the clouds.I had the 24–70 on the Z50II set to about 50 mm, aiming for a wide shot with the whole Teide in frame. Once the moon moved a bit lower, I was going to switch over to the 70–200 on my new tripod, dialed in at 200 mm, and aim it right at the crater’s summit. But first, I shot a “boring” sequence—just the already-eclipsed moon on its own, no background.A layer of ice formed on our gear in no timeI controlled the exposure of both cameras using qDslrDashboard—but mostly manually, not with the “Auto Holy Grail” mode. The reason? During a lunar eclipse, you need to be able to adjust exposure very quickly, while the “Auto Holy Grail” algorithm is designed to do the opposite: it smooths out short-term brightness changes and only adapts gradually over time—perfect for sunrises and sunsets, but not so much for a fast-changing eclipse.At one point, the moon drifted out of frame on the 200 mm, so I decided it was time to set up the final shot. The idea was to frame the crater at the bottom and start shooting just as the moon was entering the frame—this way, I wouldn’t have to touch the camera again until the moon had completely moved out.Challenge: ExposureA quick word about exposure during a lunar eclipse—because honestly, it’s one of the biggest challenges.While even just a sliver of the full moon is still visible, not even today’s sensors—with their 14 or 15 stops of dynamic range—can capture both the moon and the landscape without compromise. Either you expose for the landscape and the moon is totally blown out, or you expose for the moon and the landscape turns to pure black.The trick is finding that sweet spot: a balance where you can still pull some detail out of the landscape in post without everything being a noisy mess, and where the moon doesn’t just show up as a giant white blob.Test shots and histograms? Not much help either—the moon just shows up as a bright dot on the screen, everything else around it looks pitch black.This is what the image preview looks like on the monitor…At that point, you really have to rely on experience to judge the exposure—trying to nail it so there’s still just enough hidden in the shadows to pull the landscape out later. Even with a flat camera profile, there’s just no way to tell from the JPG or histogram what you’ll actually be able to recover from the RAW file later on.The same image (Nikon Z50 II) after RAW processing – you can now see the moon is already slightly eclipsed, and there’s detail in the landscape too.As totality approaches, exposure gets easier since the contrast drops. The Z50II performed well here, though of course it can’t quite keep up with the dynamic range of the Z8. 30mm, f/2.8, 2 sec, ISO 800.Everything changes suddenly the moment totality begins. In our case, the eclipse was right on the edge of the umbra, so even toward the end of totality, we still had a faint glow on the right side of the moon in the images.To get the best possible timelapse of a lunar eclipse, you have to react fast when totality hits and adjust your exposure accordingly. For the 50 mm wide shot, I decided to let the moon blow out just a little—as little as possible, but enough to still keep some detail in the landscape. For the telephoto shot, I was only capturing totality anyway, which made it way easier to expose properly.Meanwhile, clouds had drifted in front of Teide and we found ourselves standing in freezing rain. Thankfully, the crater and the moon were still more or less visible. Our camera bags were crusted with ice crystals by now, and during my final camera adjustments, I couldn’t feel my fingers anymore. I couldn’t even manage to turn off the display—the button just wouldn’t press. Oh well, the battery would survive. Hopefully.Would the moon actually fall into the crater? Shot with the Nikon Z8 at 200mm, f/2.8, ISO 800, and 2 sec exposure.So, we were all set: my wide shot was rolling, the telephoto sequence was up and running, and both of Uli’s cameras were clicking away too. I had set the interval to 3 seconds, by the way.Unexpected interruptionFinally, we could shove our hands into our warm pockets and just enjoy the show. Just as we were starting to relax, we suddenly saw two of those ultra-bright “searchlight” headlamps making their way down the rocky slope—right between us and Teide. WTF?? There’s no trail there, and definitely no parking spot up top—where the heck did they come from?They just kept weaving their way down toward us, totally unfazed, and their insanely bright beams kept sweeping straight in our direction—almost certainly hitting our cameras, too.You’ve got to be kidding me. “Turn off the lights!” I shouted into the night. “¡Apaguen las luces!” — no reaction. I called again, louder this time. Still nothing. Eventually, they settled on some rocks in front of us—but left their lights on the entire time.Who are these people? I mean, who else would be able to pinpoint this exact location so precisely? Probably only photographers, right? I mean, who else knows how to use Planit? 😉 But then again, photographers—especially astro nerds—usually know better than to blast everything with full-beam headlamps all night. We didn’t get the feeling they were even shooting photos… so what were they doing there?Guess we’ll never know. But seriously, running into someone out here, at this hour, was the last thing we expected.In the end, we just had to roll with it. Whether those lights would show up in our footage… well, we’d find out later. For now, we just wanted to soak in the magic of the total lunar eclipse.The freezing rain had stopped, and the clouds decided to give the summit a break—drifting by just beneath the peak.Totality approachesFrom the moment totality began to the point where the moon disappeared behind the crater, we had just 6 minutes. The sky had already started to take on a faint blue hue with the approaching dawn. And now, more clouds were looming—this time coming in from the right. Just after the moon touched the edge of the crater, one of those clouds drifted right in front of it—because of course it did. Luckily, it was a small one and didn’t mess up the final result too much. And—miraculously—the lights from those ridiculously bright headlamps didn’t show up in the timelapse at all. Sometimes, you just need a bit of luck!We did it!And here it is—the single frame from the timelapse that made all of this effort totally worth it:It worked! 🙂 Nikon Z8, 200mm, f/2.8, 2 sec, ISO 800.Once the moon had disappeared, we briefly considered running off to the right to catch one last glimpse of it beside the mountain—but it didn’t take long to realize that wasn’t going to happen. We still had to de-ice and pack up all the gear, and with how quickly the moon was setting, there was just no way we’d make it in time. So instead, we let the cameras keep rolling a little longer and simply watched as the day slowly began to break.Photo: Uli FehrOne more timelapse with the ice-covered camera, this time facing the edge of the caldera, where clouds were now spilling in from the south as well.We did it! 🙂The way backFrozen to the bone but happy, we packed up everything calmly and started the hike back to the car. Straight into the thick clouds that soon wrapped around us. We battled against the wind, and a thin layer of ice had formed on the ground. I really didn’t want to think about the drive back just yet…Somewhere out there in that fog bank… is our car.Every now and then, Teide lit up in a beautiful orange glow from the rising sun.Everything frozen…🙂When we finally got back to the car, it was completely frozen over too. Ice scrapers in Tenerife? Yeah… not exactly a booming market. So with hands that were finally starting to thaw, we scraped the ice off the windshield. My phone still showed -4°C.And once again, we got lucky: the road wasn’t too icy, so we carefully began the drive back—at a snail’s pace. The higher the sun climbed and the farther south we got, the drier the road became, and before long the thermometer crept back into positive territory.We did it!A little riddle for youTired but seriously happy, we started our descent—and ended up witnessing a super interesting physical-chemical phenomenon. Check out this photo:  In the spots where there was shadow—or just recently had been—the road had already dried. But in the sun? Still wet. How’s that even possible? If you know the answer, drop it in the comments! 🙂Uli picked up the camera he’d set up the night before, and then we headed back down to the warm coast for a well-deserved late breakfast—though not without quickly grabbing our laptops first to offload the sequences. And while we were still waiting for our food and coffee, we couldn’t help ourselves—we already started reviewing the footage. The excitement was just too much!Do you want to learn how to shoot and edit such timelapses? Get my E-Book Timelapse Photography and learn it step by step the easy way!My timelapseAnd to wrap things up, here’s a little compilation of the timelapse sequences I captured. Enjoy the show!\n\n      Total Lunar Eclipse over Crater of Pico del Teide, Tenerife (HDR Timelapse)\n    Video can&#8217;t be loaded because JavaScript is disabled: <a href=\"https://youtu.be/dSuCsWTxZ6U\" title=\"Total Lunar Eclipse over Crater of Pico del Teide, Tenerife (HDR Timelapse)\">Total Lunar Eclipse over Crater of Pico del Teide, Tenerife (HDR Timelapse) (https://youtu.be/dSuCsWTxZ6U)</a>To watch the video in HDR, you’ll need an HDR-capable TV or, on a computer, an HDR-capable monitor and browser (Chromium-based), with HDR enabled in your operating system. Make sure to watch the video directly on YouTube!P.S. Have you ever planned and pulled off a photo project this elaborate? I’d love to hear about it—drop me a comment!P.P.S. And be sure to check out Uli on Instagram and Facebook—he’s always up to some wild photo adventures. 🙂This might also be of interest to you:LA Timelapse Film Festival - the award ceremony now on Youtube! The Los Angeles Timelapse Film Festival (LATFF), first of its kind, took place last Saturday (Jan. 28, 2023) as a live stream and was…Northern Skies 4K (Cello Version, 2024) This is an update of my Film Northern Skies where the Cello in the Music track was played by Georg Roither, a professional cellist…Shorter Basic Tutorial (no Lightroom) This is the simplified, shorter basic tutorial for LRTimelapse without Lightroom Classic. You will learn how to edit your timelapse using LRTimelapse only: keyframe…LRTimelapse 7 released and new Tutorials! Dear Community, I've now released the final version of LRTimelapse 7, the latest \"major\" upgrade of LRTimelapse. I've spent an enormous amount of time…LA Timelapse Film Festival Audience Voting and Livestream Next Saturday, Jan. 28, 2023, the award ceremony for the Los Angeles Timelapse Film Festival will take place in an online ceremony - and…Basic Tutorial This is the complete basic tutorial for LRTimelapse. Additionally to the internal editing and processing in LRTimelapse it also includes editing and exporting via…← LRTimelapse 7.3 released!8 Responses to Total lunar eclipse over the Teide crater, Tenerife – a project with many obstacles… Constantin says: 2025-03-22 at 3:06 pmOn the topic of the road: Parts still in shade have humidity collected from the night in form of very light frost, barely visible. This melts when the sun hits and thereby becomes clearly visible as wet road.Reply Gunther Wegner says: 2025-03-22 at 5:15 pmYes, but here the road is wet in the sun while it is dry in the shade. That’s paradoxical and the other way around, as you would expect. I’ll wait for a few more answers, then I’ll solve it! 🤗Reply Mark Jaeger says: 2025-03-25 at 12:07 amGunther, Here are my thoughts on the wet vs dry pavement: The scene is on the island of Tenerife somewhere near maximum altitude (for the area).  You are 3+ thousand meters above sea level and the air temperature is a little lower than freezing. At altitude the air pressure is reduced, the air temperature is reduced and there is ample wind. All of this is favorable for sublimation but not so much for evaporation as, with below freezing temperatures, the vapor pressure is reduced.Here is how it goes: The entire road begins in shade. The sun is rising from the right side of the photo warming the road from left to right. The wind is blowing and sublimation is taking place.  The road is drying. There is a temperature where in sublimation the molecules will leave the surface and go into the air.  At slightly higher temperatures the molecules can re-attach to the surface (thus not leaving) The left side of the road has passed the critical temperature where sublimated molecules re-attach to the surface because it is “warm enough”. The right side of the road, in shadow, is not warm enough for sublimated molecules to re-attach and the molecules  go into the air (drying). The race to dryness is won by the area in shadow (a sublime story). The photo was taken at a very opportune time as, I think, the race was only won by a hair.Reply Gunther Wegner says: 2025-03-25 at 9:52 amThis is an interesting explanation. I’ve tried to do quite some research by myself, it’s not easy to find an explanation that says “this is true or false”. Sublimation was also what I instantly thought, when I saw it. But upon further research I found out that sublimation takes a very long time – hours or even days to transform thin layers of ice directly into vapor (skipping the melting).An easier, and for me also plausible explanation could be the following: When the sun hits the dark road, it warms the pavement above freezing so the thin layer of ice melts into liquid water—making those spots look wet. In the shaded areas, the surface stays below freezing, so the ice doesn’t turn into water, which would appear shiny and dark on the road. The ice molecules alone in the asphalt would make the road look dry. The moving shadows would then expose the areas at the borders of the shadows to the sun, where after a couple of minutes the ice would also melt, turning the road dark. While it’s not yet melted, however, it still looks dry (iced) at that “borders”. That’s why shaded patches look dry—even though they’re still icy—while sunny patches look wet from melting.Reply Gavin says: 2025-03-22 at 7:27 amI had a think about this phenomenon on the road and I suspect that the road heated by the sun is much closer to the dew point than the road in the shade. The clear night sky has kept the road dry, but with the advent of some warming, the moisture in the air is attracted to objects at or close to the dew point. Best way to find out the answer is to have a stab in the dark, like I have, and then the experts will show up to give us the science.Reply Matthew Wheeler says: 2025-03-22 at 5:07 amThank you for a great finger-nail biting account of this expensive adventure, with such a beautiful, spectacular outcome after all the work and obstacles–it is so well presented and I had to read to the end right away!  About the shadow “photography” on the road: I have seen this fair bit in Canada, where the snow can be etched away by the sun, leaving quite intricate images of the shadows. Frozen mud can become quite light in tone, and darken when it thaws. In this case it is possible the pavement looks dry because drizzle had frozen into it, and the width of the gap outside the current shadow shows the time it took to melt and darken the road surface.Reply Silvana Della Camera says: 2025-03-22 at 12:27 amWonderful! I am glad you didn’t give up. I have found that the battle within is often harder than the difficult task itself. Thank you for sharing your story and the remarkable results.Reply John Edwards says: 2025-03-21 at 7:48 pmLove the planning and perseverance.  Thanks for sharing your story and your excellent timelapse of the lunar eclipse.ReplyLeave a Reply Cancel replyYour email address will not be published. Required fields are marked *Comment *document.getElementById(\"comment\").setAttribute( \"id\", \"a122be334a6d59a99f9fafc8a51fd4ca\" );document.getElementById(\"c60cb81b37\").setAttribute( \"id\", \"comment\" );Name * Email * Website",
    "summary": {
      "en": "The blog post details the challenges faced by Gunther Wegner and his friend Uli while trying to photograph a total lunar eclipse over Teide crater in Tenerife. They had planned this unique project to capture stunning images of the eclipsed moon setting behind the iconic Teide mountain, which wouldn’t occur again for many years. \n\nThe duo meticulously prepared by scouting the location and using a smartphone app to plan their shots. However, they encountered numerous obstacles, including unexpected weather changes, equipment issues, and even a theft of Uli's camera gear the night before the event. Despite considering giving up due to these setbacks, they decided to go ahead with their plans, adapting by borrowing equipment and changing their shooting strategy.\n\nOn the night of the eclipse, they faced freezing temperatures and clouds that threatened to obscure the moon. Nevertheless, they persevered, ultimately capturing the mesmerizing moment as the moon dipped behind the crater. Their efforts paid off, resulting in beautiful timelapse footage of the eclipse.\n\nThe experience highlighted the importance of planning, adaptability, and persistence in photography, reinforcing the idea that even when things go wrong, dedication can lead to rewarding outcomes. The post concludes with a link to their timelapse video, showcasing the fruits of their labor.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "5f918820000a49a2",
    "title": {
      "en": "A Sneaky Phish Just Grabbed My Mailchimp Mailing List",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.troyhunt.com/a-sneaky-phish-just-grabbed-my-mailchimp-mailing-list/",
    "score": 78,
    "by": "gpi",
    "time": 1742890020,
    "content": "A Sneaky Phish Just Grabbed my Mailchimp Mailing List\n\n            25 March 2025",
    "summary": {
      "en": "A person discovered that a phishing scam had successfully stolen their Mailchimp mailing list. The incident highlights the dangers of online scams and the importance of protecting personal information.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "7f1f99c13740279a",
    "title": {
      "en": "Cottagecore Programmers",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://tjmorley.com/blogposts/cottagecoreprogrammers.html",
    "score": 102,
    "by": "morleytj",
    "time": 1742846905,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "e3f4dd00492cfe4a",
    "title": {
      "en": "Play the Virtual Organ from Arp Schnitger",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.orgelstadt-hamburg.de/play-arp/",
    "score": 123,
    "by": "ohjeez",
    "time": 1742754706,
    "content": "© Hauptkirche St. Jacobi Hamburg\n\n          ›play arp‹Virtuelle Orgel St. Jacobi\n          Hier haben Sie die weltweit einmalige Gelegenheit, eine der  international\n            wertvollsten und klangschönsten Barockorgeln selbst zum Klingen zu bringen.\n            Viele Orgelfans und Experten pilgern nach Hamburg, um die berühmte Arp-Schnitger\n            Orgel in der Hauptkirche St. Jacobi zu erleben. Wir bringen eine besondere\n            Auswahl klanglicher Perlen dieses Instrumentes exklusiv zu Ihnen. Spielen\n            Sie selbst!\n            (Für das klanglich beste Erlebnis empfehlen wir einen guten Kopfhörer zu nutzen)",
    "summary": {
      "en": "You can virtually play the unique Arp-Schnitger organ from the St. Jacobi Church in Hamburg, known for its beautiful Baroque sound. Many organ enthusiasts visit to experience this famous instrument. Now, you can enjoy selected pieces from it online. For the best sound quality, it's recommended to use good headphones.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "2577d1b6b5eb6922",
    "title": {
      "en": "LangManus: An Open-Source Manus Agent with LangChain + LangGraph",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/langmanus/langmanus",
    "score": 124,
    "by": "gfortaine",
    "time": 1742554860,
    "content": "🦜🤖 LangManus\n\nEnglish | 简体中文 | 日本語\n\nCome From Open Source, Back to Open Source\n\nLangManus is a community-driven AI automation framework that builds upon the incredible work of the open source community. Our goal is to combine language models with specialized tools for tasks like web search, crawling, and Python code execution, while giving back to the community that made this possible.\nDemo\nTask: Calculate the influence index of DeepSeek R1 on HuggingFace. This index can be designed using a weighted sum of factors such as followers, downloads, and likes.\nLangManus's Fully Automated Plan and Solution:\n\nGather the latest information about \"DeepSeek R1\", \"HuggingFace\", and related topics through online searches.\nInteract with a Chromium instance to visit the HuggingFace official website, search for \"DeepSeek R1\" and retrieve the latest data, including followers, likes, downloads, and other relevant metrics.\nFind formulas for calculating model influence using search engines and web scraping.\nUse Python to compute the influence index of DeepSeek R1 based on the collected data.\nPresent a comprehensive report to the user.\n\nView on YouTube\n\nTable of Contents\n\nQuick Start\nProject Statement\nArchitecture\nFeatures\nWhy LangManus?\nSetup\n\nPrerequisites\nInstallation\nConfiguration\n\nUsage\nDocker\nWeb UI\nDevelopment\nFAQ\nContributing\nLicense\nAcknowledgments\n\nQuick Start\n# Clone the repository\ngit clone https://github.com/langmanus/langmanus.git\ncd langmanus\n\n# Install dependencies, uv will take care of the python interpreter and venv creation\nuv sync\n\n# Playwright install to use Chromium for browser-use by default\nuv run playwright install\n\n# Configure environment\n# Windows: copy .env.example .env\ncp .env.example .env\n# Edit .env with your API keys\n\n# Run the project\nuv run main.py\n\nProject Statement\nThis is an academically driven open-source project, developed by a group of former colleagues in our spare time. It aims to explore and exchange ideas in the fields of Multi-Agent and DeepResearch.\n\nPurpose: The primary purpose of this project is academic research, participation in the GAIA leaderboard, and the future publication of related papers.\nIndependence Statement: This project is entirely independent and unrelated to our primary job responsibilities. It does not represent the views or positions of our employers or any organizations.\nNo Association: This project has no association with Manus (whether it refers to a company, organization, or any other entity).\nClarification Statement: We have not promoted this project on any social media platforms. Any inaccurate reports related to this project are not aligned with its academic spirit.\nContribution Management: Issues and PRs will be addressed during our free time and may experience delays. We appreciate your understanding.\nDisclaimer: This project is open-sourced under the MIT License. Users assume all risks associated with its use. We disclaim any responsibility for any direct or indirect consequences arising from the use of this project.\n\n项目声明\n本项目是一个学术驱动的开源项目，由一群前同事在业余时间开发，旨在探索和交流 Multi-Agent 和 DeepResearch 相关领域的技术。\n\n项目目的：本项目的主要目的是学术研究、参与 GAIA 排行榜，并计划在未来发表相关论文。\n独立性声明：本项目完全独立，与我们的本职工作无关，不代表我们所在公司或任何组织的立场或观点。\n无关联声明：本项目与 Manus（无论是公司、组织还是其他实体）无任何关联。\n澄清声明：我们未在任何社交媒体平台上宣传过本项目，任何与本项目相关的不实报道均与本项目的学术精神无关。\n贡献管理：Issue 和 PR 将在我们空闲时间处理，可能存在延迟，敬请谅解。\n免责声明：本项目基于 MIT 协议开源，使用者需自行承担使用风险。我们对因使用本项目产生的任何直接或间接后果不承担责任。\n\nArchitecture\nLangManus implements a hierarchical multi-agent system where a supervisor coordinates specialized agents to accomplish complex tasks:\n\nThe system consists of the following agents working together:\n\nCoordinator - The entry point that handles initial interactions and routes tasks\nPlanner - Analyzes tasks and creates execution strategies\nSupervisor - Oversees and manages the execution of other agents\nResearcher - Gathers and analyzes information\nCoder - Handles code generation and modifications\nBrowser - Performs web browsing and information retrieval\nReporter - Generates reports and summaries of the workflow results\n\nFeatures\nCore Capabilities\n\n🤖 LLM Integration\n\nIt supports the integration of most models through litellm.\nSupport for open source models like Qwen\nOpenAI-compatible API interface\nMulti-tier LLM system for different task complexities\n\nTools and Integrations\n\n🔍 Search and Retrieval\n\nWeb search via Tavily API\nNeural search with Jina\nAdvanced content extraction\n\nDevelopment Features\n\n🐍 Python Integration\n\nBuilt-in Python REPL\nCode execution environment\nPackage management with uv\n\nWorkflow Management\n\n📊 Visualization and Control\n\nWorkflow graph visualization\nMulti-agent orchestration\nTask delegation and monitoring\n\nWhy LangManus?\nWe believe in the power of open source collaboration. This project wouldn't be possible without the amazing work of projects like:\n\nQwen for their open source LLMs\nTavily for search capabilities\nJina for crawl search technology\nBrowser-use for control browser\nAnd many other open source contributors\n\nWe're committed to giving back to the community and welcome contributions of all kinds - whether it's code, documentation, bug reports, or feature suggestions.\nSetup\nPrerequisites\n\nuv package manager\n\nInstallation\nLangManus leverages uv as its package manager to streamline dependency management.\nFollow the steps below to set up a virtual environment and install the necessary dependencies:\n# Step 1: Create and activate a virtual environment through uv\nuv python install 3.12\nuv venv --python 3.12\n\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Step 2: Install project dependencies\nuv sync\n\nBy completing these steps, you'll ensure your environment is properly configured and ready for development.\nConfiguration\nLangManus uses a three-layer LLM system, which are respectively used for reasoning, basic tasks, and vision-language tasks. Configuration is done using the conf.yaml file in the root directory of the project. You can copy conf.yaml.example to conf.yaml to start the configuration:\ncp conf.yaml.example conf.yaml\n\n# Setting it to true will read the conf.yaml configuration, and setting it to false will use the original .env configuration. The default is false (compatible with existing configurations)\nUSE_CONF: true\n\n# LLM Config\n## Follow the litellm configuration parameters: https://docs.litellm.ai/docs/providers. You can click on the specific provider document to view the completion parameter examples\nREASONING_MODEL:\n  model: \"volcengine/ep-xxxx\"\n  api_key: $REASONING_API_KEY # Supports referencing the environment variable ENV_KEY in the.env file through $ENV_KEY\n  api_base: $REASONING_BASE_URL\n\nBASIC_MODEL:\n  model: \"azure/gpt-4o-2024-08-06\"\n  api_base: $AZURE_API_BASE\n  api_version: $AZURE_API_VERSION\n  api_key: $AZURE_API_KEY\n\nVISION_MODEL:\n  model: \"azure/gpt-4o-2024-08-06\"\n  api_base: $AZURE_API_BASE\n  api_version: $AZURE_API_VERSION\n  api_key: $AZURE_API_KEY\n\nYou can create a .env file in the root directory of the project and configure the following environment variables. You can copy the.env.example file as a template to start:\ncp .env.example .env\n\n# Tool API Key\nTAVILY_API_KEY=your_tavily_api_key\nJINA_API_KEY=your_jina_api_key  # Optional\n\n# Browser Configuration\nCHROME_INSTANCE_PATH=/Applications/Google Chrome.app/Contents/MacOS/Google Chrome  # Optional, the path to the Chrome executable file\nCHROME_HEADLESS=False  # Optional, the default is False\nCHROME_PROXY_SERVER=http://127.0.0.1:10809  # Optional, the default is None\nCHROME_PROXY_USERNAME=  # Optional, the default is None\nCHROME_PROXY_PASSWORD=  # Optional, the default is None\n\nNote:\n\nThe system uses different models for different types of tasks:\n\nThe reasoning LLM is used for complex decision-making and analysis.\nThe basic LLM is used for simple text tasks.\nThe vision-language LLM is used for tasks involving image understanding.\n\nThe configuration of all LLMs can be customized independently.\nThe Jina API key is optional. Providing your own key can obtain a higher rate limit (you can obtain this key at jina.ai).\nThe default configuration for Tavily search is to return up to 5 results (you can obtain this key at app.tavily.com).\n\nConfigure Pre-commit Hook\nLangManus includes a pre-commit hook that runs linting and formatting checks before each commit. To set it up:\n\nMake the pre-commit script executable:\n\nchmod +x pre-commit\n\nInstall the pre-commit hook:\n\nln -s ../../pre-commit .git/hooks/pre-commit\n\nThe pre-commit hook will automatically:\n\nRun linting checks (make lint)\nRun code formatting (make format)\nAdd any reformatted files back to staging\nPrevent commits if there are any linting or formatting errors\n\nUsage\nBasic Execution\nTo run LangManus with default settings:\nuv run main.py\n\nAPI Server\nLangManus provides a FastAPI-based API server with streaming support:\n# Start the API server\nmake serve\n\n# Or run directly\nuv run server.py\n\nThe API server exposes the following endpoints:\n\nPOST /api/chat/stream: Chat endpoint for LangGraph invoke with streaming support\n\nRequest body:\n\n{\n  \"messages\": [{ \"role\": \"user\", \"content\": \"Your query here\" }],\n  \"debug\": false\n}\n\nReturns a Server-Sent Events (SSE) stream with the agent's responses\n\nAdvanced Configuration\nLangManus can be customized through various configuration files in the src/config directory:\n\nenv.py: Configure LLM models, API keys, and base URLs\ntools.py: Adjust tool-specific settings (e.g., Tavily search results limit)\nagents.py: Modify team composition and agent system prompts\n\nAgent Prompts System\nLangManus uses a sophisticated prompting system in the src/prompts directory to define agent behaviors and responsibilities:\nCore Agent Roles\n\nSupervisor (src/prompts/supervisor.md): Coordinates the team and delegates tasks by analyzing requests and determining which specialist should handle them. Makes decisions about task completion and workflow transitions.\n\nResearcher (src/prompts/researcher.md): Specializes in information gathering through web searches and data collection. Uses Tavily search and web crawling capabilities while avoiding mathematical computations or file operations.\n\nCoder (src/prompts/coder.md): Professional software engineer role focused on Python and bash scripting. Handles:\n\nPython code execution and analysis\nShell command execution\nTechnical problem-solving and implementation\n\nFile Manager (src/prompts/file_manager.md): Handles all file system operations with a focus on properly formatting and saving content in markdown format.\n\nBrowser (src/prompts/browser.md): Web interaction specialist that handles:\n\nWebsite navigation\nPage interaction (clicking, typing, scrolling)\nContent extraction from web pages\n\nPrompt System Architecture\nThe prompts system uses a template engine (src/prompts/template.py) that:\n\nLoads role-specific markdown templates\nHandles variable substitution (e.g., current time, team member information)\nFormats system prompts for each agent\n\nEach agent's prompt is defined in a separate markdown file, making it easy to modify behavior and responsibilities without changing the underlying code.\nDocker\nLangManus can be run in a Docker container. default serve api on port 8000.\nBefore run docker, you need to prepare environment variables in .env file.\ndocker build -t langmanus .\ndocker run --name langmanus -d --env-file .env -e CHROME_HEADLESS=True -p 8000:8000 langmanus\n\nYou can also just run the cli with docker.\ndocker build -t langmanus .\ndocker run --rm -it --env-file .env -e CHROME_HEADLESS=True langmanus uv run python main.py\n\nWeb UI\nLangManus provides a default web UI.\nPlease refer to the langmanus/langmanus-web-ui project for more details.\nDocker Compose (include both backend and frontend)\nLangManus provides a docker-compose setup to easily run both the backend and frontend together:\n# Start both backend and frontend\ndocker-compose up -d\n\n# The backend will be available at http://localhost:8000\n# The frontend will be available at http://localhost:3000, which could be accessed through web browser\n\nThis will:\n\nBuild and start the LangManus backend container\nBuild and start the LangManus web UI container\nConnect them using a shared network\n\n** Make sure you have your .env file prepared with the necessary API keys before starting the services. **\nDevelopment\nTesting\nRun the test suite:\n# Run all tests\nmake test\n\n# Run specific test file\npytest tests/integration/test_workflow.py\n\n# Run with coverage\nmake coverage\n\nCode Quality\n# Run linting\nmake lint\n\n# Format code\nmake format\n\nFAQ\nPlease refer to the FAQ.md for more details.\nContributing\nWe welcome contributions of all kinds! Whether you're fixing a typo, improving documentation, or adding a new feature, your help is appreciated. Please see our Contributing Guide for details on how to get started.\nLicense\nThis project is open source and available under the MIT License.\nStar History\n\nAcknowledgments\nSpecial thanks to all the open source projects and contributors that make LangManus possible. We stand on the shoulders of giants.\nIn particular, we want to express our deep appreciation for:\n\nLangChain for their exceptional framework that powers our LLM interactions and chains\nLangGraph for enabling our sophisticated multi-agent orchestration\nBrowser-use for control browser\n\nThese amazing projects form the foundation of LangManus and demonstrate the power of open source collaboration.",
    "summary": {
      "en": "**Summary of LangManus**\n\nLangManus is an open-source AI automation framework created by a community of developers. It aims to combine language models with specialized tools for tasks like web searching and executing Python code, while also giving back to the open-source community.\n\n### Key Features:\n- **Community-Driven**: Built upon contributions from various open-source projects.\n- **Multi-Agent System**: Uses different agents (Coordinator, Planner, Researcher, Coder, Browser, and Reporter) to handle complex tasks.\n- **Integration Capabilities**: Supports various language models and tools for web search and content extraction.\n- **Python Integration**: Includes a Python execution environment for coding tasks.\n- **Visualization**: Offers workflow management tools to visualize and control tasks.\n\n### Setup and Usage:\n- **Installation**: Users can clone the repository, install dependencies, and configure their environment using provided files.\n- **Running the Project**: Can be launched using commands that start the main program or API server.\n- **Docker Support**: LangManus can run in Docker containers, simplifying deployment and management.\n\n### Contributions and Community:\n- The project encourages contributions in multiple forms, such as code, documentation, and bug reports.\n- It operates independently from the developers' primary jobs and does not represent any organizations.\n\n### License:\nLangManus is open source under the MIT License, allowing users to modify and use it freely.\n\n### Acknowledgments:\nThe developers express gratitude to other open-source projects that support LangManus, highlighting the collaborative nature of their work.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "4e1d365d65298255",
    "title": {
      "en": "BeeFormer: CF and CBF hybrid approach for recommendation systems",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/recombee/beeformer",
    "score": 42,
    "by": "klaussilveira",
    "time": 1742732424,
    "content": "beeFormer\nThis is the official implementation provided with our paper beeFormer: Bridging the Gap Between Semantic and Interaction Similarity in Recommender Systems.\nmain idea of beeFormer\n\nCollaborative filtering (CF) methods can capture patterns from interaction data that are not obvious at first sight. For example, when buying a printer, users can also buy toners, papers, or cables to connect the printer, and collaborative filtering can take such patterns into account. However, in the cold-start recommendation setup, where new items do not have any interaction at all, collaborative filtering methods cannot be used, and recommender systems are forced to use other approaches, like content-based filtering (CBF). The problem with content-based filtering is that it relies on item attributes, such as text descriptions. In our printer example, semantic similarity-trained language models will put other printers closer than accessories that users might be searching for. Our method is training language models to learn these user behavior patterns from interaction data to transfer that knowledge to previously unseen items. Our experiments show that performance benefits from this approach are enormous.\nSteps to start training the models:\n\ncreate virtual environment python3.10 -m venv beef and activate it source beef/bin/activate\nclone this repository and navigate to it cd beeformer\ninstall packages pip install -r requirements.txt\ndownload the data for movielens: navigate to the _dataset/ml20m folder and run source download_data\ndownload the data for goodbooks: navigate to the _dataset/goodbooks folder and run source download_data\ndownload the data for amazonbooks: navigate to the _dataset/amazonbooks folder and run source download_data && python preprocess.py\nin the root folder of the project run the train.py, for example like this:\n\npython train.py --seed 42 --scheduler None --lr 1e-5 --epochs 5 --dataset goodbooks --sbert \"sentence-transformers/all-mpnet-base-v2\" --max_seq_length 384 --batch_size 1024 --max_output 10000 --sbert_batch_size 200 --use_cold_start true --save_every_epoch true --model_name my_model\n\nEvaluate the results. To reproduce numbers from the paper using our hugginface repository, run for example:\n\npython evaluate_itemsplit.py --seed 42 --dataset goodbooks --sbert beeformer/Llama-goodbooks-mpnet\n\nor\npython evaluate_timesplit.py --seed 42 --dataset amazon-books --sbert beeformer/Llama-amazbooks-mpnet\n\nDatasets and preprocessing\nPreprocessing information\nWe consider ratings of 4.0 and higher as an interaction. We only keep the users with at least 5 interactions.\nLLM Data augmentations\nSince there are no text descriptions in the original data, we manually connect several datasets with the original data and train our models on it. However, this approach has several limitations: texts from different sources have different styles and different lengths, and this might influence the results. Therefore, we use the Llama-3.1-8b-instruct model to generate item descriptions for us. We use the following conversation template:\nimport pandas as pd\n\nfrom tqdm import tqdm\nfrom vllm import LLM, SamplingParams\n\nitems = pd.read_feather(\"items_with_gathered_side_info.feather\")\n\nllm = LLM(model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",dtype=\"float16\")\n\ntokenizer = llm.get_tokenizer()\nconversation = [ tokenizer.apply_chat_template(\n        [\n            {'role': 'system','content':\"You are ecomerce shop designer. Given a item description create one paragraph long summarization of the product.\"},\n            {'role': 'user', 'content': \"Item description: \"+x},\n            {'role': 'assistant', 'content': \"Sure, here is your one paragraph summary of your product:\"},\n        ],\n        tokenize=False,\n    ) for x in tqdm(items.gathered_features.to_list())]\n\noutput = llm.generate(\n    conversation,\n    SamplingParams(\n        temperature=0.1,\n        top_p=0.9,\n        max_tokens=512,\n        stop_token_ids=[tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")],\n    )\n)\n\nitems_descriptions = [o.outputs[0].text for o in output]\n\nHowever, LLM refused to generate descriptions for some items (For example, because it refuses to generate explicit content). We removed such items from the dataset. We also removed items for which we were not able to connect meaningful descriptions from other datasets, which led to LLM completely hallucinating item descriptions.\nWe share the resulting LLM-generated item descriptions in datasets/ml20m, dataset/goodbooks and datasets/amazonbooks folders.\nStatistics of datasets used for evaluation\n\nGoodBooks-10k\nMovieLens-20M\nAmazon Books\n\n# of items in X\n9975\n16902\n63305\n\n# of users in X\n53365\n136589\n634964\n\n# of interactions in X\n4119623\n9694668\n8290500\n\ndensity of X [%]\n0.7739\n0.4199\n0.0206\n\ndensity of X^TX [%]\n41.22\n26.93\n7.59\n\nPretrained models\nWe share pretrained models at https://huggingface.co/beeformer.\nHyperparameters\nWe used hyperparameters for training our models as follows.\n\nhyperparameter\ndescription\nbeeformer/Llama-goodbooks-mpnet\nbeeformer/Llama-movielens-mpnet\nbeeformer/Llama-goodlens-mpnet\nbeeformer/Llama-amazbooks-mpnet\n\nseed\nrandom seed used during training\n42\n42\n42\n42\n\nscheduler\nlearning rate scheduling strategy\nconstant learning rate\nconstant learning rate\nconstant learning rate\nconstant learning rate\n\nlr\nlearning rate\n1e-5\n1e-5\n1e-5\n1e-5\n\nepochs\nnumber of trained epochs\n5\n5\n10\n5\n\ndevices\ntraining script allow to train on multiple gpus in parallel - we used 4xV100\n[0,1,2,3]\n[0,1,2,3]\n[0,1,2,3]\n[0,1,2,3]\n\ndataset\ndataset used for training\ngoodbooks\nml20m\ngoodlens\namazon-books\n\nsbert\noriginal sentence transformer model used as an initial model for training\nsentence-transformers/all-mpnet-base-v2\nsentence-transformers/all-mpnet-base-v2\nsentence-transformers/all-mpnet-base-v2\nsentence-transformers/all-mpnet-base-v2\n\nmax_seq_length\nlimitation of sequence length; shorter sequences trains faster original mpnet model uses max 512 tokens in. sequence\n384\n384\n384\n384\n\nbatch_size\nnumber of users sampled in random batch from interaction matrix\n1024\n1024\n1024\n1024\n\nmax_output\nnegative sampling hyperparameter (m in the paper). Negatives are sampled uniformly at random.\n10000\n10000\n10000\n12500\n\nsbert_batch_size\nnumber of items processed together during training step (gradient accumulation step size)\n200\n200\n200\n200\n\nuse_cold_start\nsplit the dataset item-wise (some items are hidden to test the genralization towards new items)\ntrue\ntrue\ntrue\nfalse\n\nuse_time_split\nsort interactions by timestamp and use last 20% of interactions as a test set (generalization from the past to the future)\nfalse\nfalse\nfalse\ntrue\n\nRecSys 2024 poster\n\nCitation\nIf you find this repository helpful, feel free to cite our paper:\n@inproceedings{10.1145/3640457.3691707,\n        author = {Van\\v{c}ura, Vojt\\v{e}ch and Kord\\'{\\i}k, Pavel and Straka, Milan},\n        title = {beeFormer: Bridging the Gap Between Semantic and Interaction Similarity in Recommender Systems},\n        year = {2024},\n        isbn = {9798400705052},\n        publisher = {Association for Computing Machinery},\n        address = {New York, NY, USA},\n        url = {https://doi.org/10.1145/3640457.3691707},\n        doi = {10.1145/3640457.3691707},\n        booktitle = {Proceedings of the 18th ACM Conference on Recommender Systems},\n        pages = {1102–1107},\n        numpages = {6},\n        keywords = {Cold-start recommendation, Recommender systems, Sentence embeddings, Text mining, Zero-shot recommendation},\n        location = {Bari, Italy},\n        series = {RecSys '24}\n}",
    "summary": {
      "en": "### Summary of beeFormer\n\n**Overview:**\nbeeFormer is a new method for improving recommendation systems, especially useful when dealing with new items that have no previous user interactions (cold-start situations). Traditional collaborative filtering can identify user behavior patterns, but it struggles with new items. In contrast, content-based filtering focuses on item attributes, which may not effectively connect similar items, such as accessories for a printer.\n\n**Key Idea:**\nbeeFormer trains language models to recognize user behavior patterns from interaction data. This knowledge can then be applied to new items, enhancing recommendation accuracy.\n\n**Getting Started:**\n1. **Set Up Environment:**\n   - Create a virtual environment and activate it.\n   - Clone the beeFormer repository.\n\n2. **Install Required Packages:**\n   - Use `pip` to install necessary packages from a requirements file.\n\n3. **Download Datasets:**\n   - Navigate to specific folders and run scripts to download datasets for MovieLens, GoodBooks, and Amazon Books.\n\n4. **Train Models:**\n   - Execute the training script with various parameters to train the model.\n\n5. **Evaluate Performance:**\n   - Run evaluation scripts to reproduce results from the research paper.\n\n**Data Processing:**\n- Only interactions with ratings of 4.0 or higher are considered.\n- Use a language model to generate item descriptions due to the lack of original text in the datasets. Some items may be excluded if descriptions cannot be generated.\n\n**Datasets:**\n- The method evaluates datasets like GoodBooks, MovieLens, and Amazon Books, each with a specific number of items, users, and interactions.\n\n**Pretrained Models:**\n- Pretrained models are available for use and can be accessed online.\n\n**Hyperparameters:**\n- Specific settings for training include learning rates, batch sizes, and the number of epochs.\n\n**Citation:**\nFor reference or academic use, the paper detailing this method can be cited with the provided citation format.\n\nThis summary simplifies the main points of the beeFormer method and its implementation in recommender systems, making it easier to understand and follow.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "a8de69e131cf93a9",
    "title": {
      "en": "Show HN: I built a website for sharing drum patterns",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "http://drumpatterns.onether.com",
    "score": 476,
    "by": "wesz",
    "time": 1742735121,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "caba56a5a2afb958",
    "title": {
      "en": "The Software Engineering Identity Crisis",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://annievella.com/posts/the-software-engineering-identity-crisis/",
    "score": 129,
    "by": "napolux",
    "time": 1742755045,
    "content": "The Software Engineering Identity Crisis\n\n        March 21, 2025\n\n      Many of us became software engineers because we found our identity in building things. Not managing things. Not overseeing things. Building things. With our own hands, our own minds, our own code.\nBut that identity is being challenged.\nAI coding assistants aren’t just changing how we write software - they’re fundamentally transforming who we are. We’re shifting from creators to orchestrators, from builders to overseers. From engineers to something that looks suspiciously like… managers.\nThe irony cuts deep: for years, we’ve said that software engineering transcends mere coding. Requirements, design, testing, operations - these were all supposedly part of our craft. Yet the industry pushed us in the opposite direction. We handed these responsibilities to specialists - Product Owners, Architects, Quality Engineers, Platform Engineers - while we doubled down on our coding expertise. We became masters of our code, proud wielders of a modern magic.\nAnd now, just as we’ve perfected this craft, AI is threatening to take it away from us.\nThe Joy We’re About to Lose\nLet’s be honest about what’s at stake here. Many of us don’t just write code - we love writing code. Our identity is woven into every elegant solution we craft, every test we make pass, every problem we solve through pure logic and creativity. It’s not just work, not just a craft - it’s who we are.\nThink about those moments of deep satisfaction: when you finally track down that elusive bug that’s been haunting production, when you work out how to optimise that slow algorithm and watch response times drop from seconds to milliseconds, when you transform a maze of legacy code into something clean and maintainable. These aren’t just achievements - they’re expressions of who we are as engineers. They’re the moments that remind us why we chose this path.\nNow imagine AI taking over these moments of craftsmanship. The creators of these tools paint an optimistic picture - they say we’ll spend more time on defining intent, high-level architecture, and systems thinking. But listen carefully to what they’re really saying: we’ll become overseers rather than creators, managers rather than builders.\n\n      Are software engineers evolving into system thinkers and high-level architects?\n\nThis shift raises important questions about our identity as builders: Is oversight what drives us? Is it what makes us leap out of bed in the morning, eager to solve the next puzzle?\nThe Identity Shift: It’s Already Here\nThis isn’t some theoretical future we’re debating - it’s the reality we’re living right now. When Google recently revealed that AI generates more than a quarter of their new code, it was just the beginning. Y Combinator CEO Garry Tan reveals that for about a quarter of their startups, 95% of their code is now written by AI - marking a truly significant shift. My own Master’s research revealed a similar picture: 77% of us are spending less time writing code, and almost half believe our core skill might become secondary to prompt engineering. Think about that transition: from crafting solutions to crafting prompts.\n\n      Will Prompt Engineering Replace Traditional Coding Skills?\n\nWhen asked how to develop prompt engineering skills, software engineers emphasise refining communication skills. Getting an AI to do what you want means being able to articulate things well - providing just the right amount of context along with a clear description of the task. The better you can communicate with Gen AI, the more likely the output will match your expectations. Some even suggest being polite to these tools, treating them like team members - as if you’re leading another human being to do something for you.\nThe changes run so deep that we’re inventing new terms just to describe who we’re becoming. Take vibe coding, a term that Andrej Karpathy recently coined in a tweet. It captures a profound shift in how we write software.\nAt one end of the spectrum lies the traditional way - the craftsperson’s way. We write each line with intent, every function name and architectural decision reflecting our deep understanding of the system.\nAt the other end? We let AI fill in the blanks, “vibing” with its suggestions. We focus on the what, not the how. As Karpathy puts it: “fully give in to the vibes, embrace exponentials, and forget that the code even exists.”\nThat last part gives us pause - are we still engineers if we forget all about the code?\nIn a recent pairing session, engineering thought leaders Gene Kim and Steve Yegge demonstrated what this looks like in practice. Using an AI coding assistant, they ported a 3,500-line legacy Ruby script to Kotlin - a task that would normally take a week - in just one hour. The AI didn’t just translate the code; it improved it, adding the modular architecture and unit tests they’d wanted for years but couldn’t justify the time for.\nEven Patrick Debois, the godfather of DevOps, sees this transformation reshaping our identity. In his recent analysis of AI Native Development patterns, he outlines four fundamental shifts in how we work:\n\n      Patrick Debois: The 4 Patterns of AI Native Dev\n\nThese patterns reveal a profound shift: we’re moving from producers to managers of AI systems, from detailed implementation to expressing intent, from delivery to discovery through rapid experimentation, and from content creation to knowledge curation. Our role is evolving to blend creation with orchestration, building with oversight.\nOverall, I think it’s fair to say that the nature of our professional identity is changing at its core.\nThe Craft That Shaped Our Identity\nTo understand this identity crisis, we need to look at how deeply the craft of coding has shaped who we are. At its core, writing code is about mastery and control - skills we’ve spent years perfecting. Modern programming languages are much higher-level than those of days gone by, but they still require deep technical understanding. Few developers today deal with the nitty-gritty of pointers and memory management, yet we still take pride in knowing how things work under the hood. Even as frameworks do more heavy lifting, we’ve maintained our identity as artisans who understand our tools intimately.\nProgramming today is much more about stitching together APIs, frameworks, and libraries in creative ways to build something meaningful. In fact, recent research at Google suggests that creativity in software engineering centres on the concept of clever reuse over pure novelty. This makes sense to me - I’ve often commented that we’re all just ‘integration’ engineers nowadays, really.\nStill, there’s a strange pride that we take in knowing all the weird syntax required to build something. It’s like a secret language that only we understand. Becoming proficient with a programming language gives you a lot of control to make it do precisely what you want it to do. It’s so detailed - just one wrong character will break the entire thing and it can take a lot of time and patience to make it do what you want.\n First, one must perform perfectly. The computer resembles the magic of legend in this respect, too. If one character, one pause, of the incantation is not strictly in proper form, the magic doesn’t work.\n— Frederick P. Brooks, The Mythical Man-Month, Chapter 1, Addison-Wesley, 1975\nThe other 99% of the population thinks we’re magicians for understanding code and indeed, it can take years of deliberate practice to master it. Those who master more than one programming language have the privilege of being referred to as polyglots. Many of us take great pride in writing clean, elegant code. We passionately argue over different styles and best practices, often taking it all too seriously.\nA Reluctant Manager’s Tale\nLet me share a story about identity evolution that might resonate.\nAfter a decade of being an individual contributor, I hit the infamous ceiling of the technical track. Senior Lead Software Engineer - that was as far as the technical track went. Staff+ Engineering wasn’t a thing yet, and the sole Architecture position at the company I was working for was filled. I faced a choice that would change who I was: stay a builder or become an overseer.\nI chose management. Reluctantly. That’s where the path led me. I told myself it was still engineering, just at a different level. That managing systems wasn’t so different from managing people. That I could still keep my hands in the code in between other tasks.\nSound familiar? The parallels are uncanny. Just as I had to trade direct problem-solving for meetings and documentation, we’re now being asked to trade coding for prompt engineering. The skills that defined us as engineers - mastering syntax, architecting our code elegantly, catching and handling the edge cases, debugging complex issues - are being relegated to AI. Instead, we’re told to focus on skills that sound suspiciously like management: clear communication, systems thinking, problem definition.\nBut here’s what no one’s talking about: the identity crisis. That deep sense of loss when you realise you’re no longer building things with your own hands. When your technical mastery becomes less relevant than your ability to “manage” the tools. When your craft becomes oversight.\nCan orchestrating AI ever give us that same sense of identity? Of being a builder, a creator, a problem solver?\nWhen Machines Challenge Our Identity\nBy now the source of our identity crisis becomes clear. The craft we spent years perfecting - the one that gave us purpose, meaning, and pride - is now being done faster, cheaper, and at scale by a machine. Sure, the quality isn’t as good as your hand-written code (yet). But the speed at which code can now be written is staggering and businesses are tripping over themselves to get in on the action.\nThis is where a glimmer of hope emerges. Remember that irony - how we gave away the broader aspects of our craft to specialists? AI is pushing us to reclaim what we once knew: that software engineering transcends mere coding. That core truth remains - ultimately, software engineering is about solving problems, creating solutions, building things that matter.\nThese broader skills - what Addy Osmani calls “durable engineering skills” in his article on the human 30% of AI-assisted coding - have always separated great engineers from good ones. Communication, big-picture thinking, handling ambiguity - these become even more crucial in an AI-driven world.\nYet this emphasis on broader skills has sparked debate in our community. For some, it sounds suspiciously like management repackaged. And they’re not entirely wrong - a recent CIO article confirms that development teams are already being restructured to focus on oversight rather than creation. The article envisions future teams consisting of a product manager, a UX designer, and a software architect who primarily uses AI to generate prototypes. These architects, or senior developers, must “understand the content… who the customer is and what we’re trying to achieve” - classic management responsibilities repackaged as technical work.\n\n      Management in a Cloak\n\nThis evolution raises fundamental questions about our identity as engineers: As the traditional career ladder transforms, how will the next generation of software engineers develop their skills? How do we preserve the deep technical understanding and craftsmanship that shaped our profession while embracing these new tools? And perhaps most unsettling - as AI capabilities advance exponentially, will our role as craftspeople become as obsolete as the manual weaver’s did during the Industrial Revolution?\nThe Path Forward\nPerhaps the answer lies not in resisting this transformation, but in understanding it through the lens of history. These identity crises - these fundamental shifts in how we define ourselves through our work - aren’t new. They’re part of a pattern that repeats whenever technology reshapes a profession.\nDuring the Industrial Revolution, craftspeople faced a similar crisis. Their traditional skills - honed over generations - were being replaced by machines. But what happened next is fascinating: many adapted, becoming specialist professionals who could fix and improve these same machines that threatened to replace them. Others found ways to apply their deep understanding of materials and processes to improve overall factory operations.\nIf we draw this parallel to our AI era, a similar path emerges. The core of software engineering - solving problems and creating value - remains unchanged. Our tools are evolving, and with them, the skills needed to wield them effectively.\nThe question isn’t whether we’ll become managers of machines - it’s whether we can find the same satisfaction in this evolution of our craft.\nThe Engineer’s Dilemma\nSo where does this leave us? Are we all destined to become overseers of AI agents rather than writers of code? Is this a future to resist or embrace?\nThe truth, as always, is nuanced. Just as some engineers naturally gravitate toward management while others prefer to remain hands-on, we’ll likely see a similar spectrum emerge in how we interact with AI. Some will excel at orchestrating AI systems, focusing on high-level design and making these systems more efficient and reliable - conducting a technical symphony rather than performing solo. Others will find their calling in domains where human expertise remains crucial - perhaps in security-sensitive applications, novel areas where AI lacks training data, or systems where performance and reliability are paramount. The key isn’t to resist this evolution, but to find our place within it.\nWhat’s clear is that the definition of “software engineer” is expanding, not contracting. The skills that make someone valuable are diversifying. And this creates both challenges and opportunities.\nFor those who love the craft of coding, this shift can feel threatening. But remember that AI tools are still just that - tools. They don’t understand the “why” behind the code, the business context, or the human needs being served. They can’t innovate in the true sense of the word, at least not yet. And as far as we know, they can’t feel the satisfaction of solving a complex problem or the joy of creating something new.\nPerhaps the most valuable skill in this new landscape isn’t prompt engineering or systems architecture, but adaptability - the willingness to evolve, to learn new skills, and to find your unique place in a rapidly changing field.\nThe Bright Side\nDespite these challenges, there’s something important we need to acknowledge: these AI tools can be incredibly empowering. With agentic IDEs like Windsurf and Cursor taking software development to a whole new level, it’s like having a supportive pair-programming partner who’s always there, ready to help you tackle problems that might have seemed daunting before.\nFor junior developers or those of us who might feel a bit rusty, AI assistants can be a confidence booster - helping you get started when you’re staring at a blank file, validating your approach when you’re unsure, or explaining complex concepts in a way that makes sense to you. For experienced developers, they’re like having a tireless assistant who can handle the routine tasks while you focus on the more challenging aspects of the problem.\nThe speed at which we can now prototype ideas, explore different approaches, and learn new technologies is truly remarkable. What might have taken weeks of research and trial-and-error can often be accomplished in hours or even minutes. It’s like having a superpower - the ability to amplify our capabilities and turn our ideas into reality faster than ever before.\nThe Reality Check\nBut with great power comes great responsibility. A recent comprehensive GitClear study analysing 211 million lines of code revealed some concerning trends as AI code generation tools became more prevalent:\n\nA 17.1% increase in copy-pasted code, marking the first time AI-assisted code duplication surpassed refactored (moved) code.\nAn 8-fold rise in duplicated code blocks, with 6.66% of commits now containing repeated code sections.\nA 26% increase in code churn, with 5.7% of all code changes being revised or deleted within two weeks.\n\n      GitClear: Trends in Code Changes\n\nWhile we’re producing code faster than ever, we’re also spending more time fixing AI-generated mistakes and dealing with code that’s harder to maintain. This isn’t just about speed - it’s about the craft of writing sustainable, maintainable software.\nThe Hidden Identity Crisis\nYet beneath these surface-level changes lies a deeper challenge - one that strikes at the heart of who we are as engineers. The emerging field of Human-AI teaming is revealing uncomfortable truths about our future. A 2024 study shows that when humans and AI work together, the results often fall short of expectations. Not because AI lacks capability, but because trust works differently with machines than with humans.\nWe don’t build trust with AI the way we do with human teammates.\nWith humans, trust grows gradually, through shared success. Each problem solved together strengthens the bond. Even failures can deepen trust when handled well. With AI, trust often starts high and erodes fast.\nEvery incorrect response, every hallucinated bug fix, every misplaced confidence weakens our faith in the machine. Unlike human relationships where trust typically grows over time, AI trust often peaks early and declines.\nAnd when trust erodes, so does productivity.\nThe research reveals why:\n\nThere’s an inherent unpredictability in how AI interprets our intentions\nIt lacks the contextual awareness that makes human collaboration fluid\nIts decisions often lack transparency, making it hard to rebuild trust once lost\n\nThese challenges mirror what many of us experience when transitioning into technical leadership. Just as new engineering managers must learn to trust their team’s work without doing it themselves, we now face a similar transition with AI - learning to guide and verify rather than write every line ourselves.\nThe reality is stark: despite AI’s raw capabilities, teams often perform worse with AI than without it. Just as a team’s productivity suffers under ineffective leadership, our effectiveness diminishes when we don’t understand how to work with our AI tools.\nReclaiming Your Identity\nDrawing from both my journey as a reluctant manager and my research into this AI transformation, I see three ways we might preserve our identity as builders:\n\nResist - Some will choose to focus on domains where human creativity and deep technical expertise remain essential\nAdapt - Others will embrace AI orchestration, becoming conductors of a new kind of technical symphony\nBalance - And many, like myself, will seek a middle path - using AI for routine tasks while preserving the joy of direct problem-solving\n\nThen I had a realisation that changed my perspective: We don’t have to choose just one path.\nThe Identity Pendulum\nPerhaps the answer to our identity crisis lies in the Engineer/Manager pendulum. My own journey between these roles taught me something crucial about identity:\n\nManagement didn’t replace my engineering identity - it expanded it\nReturns to hands-on work weren’t steps backward - they were identity renewals\nThe pendulum swing itself became part of who I am - adaptable, growing, evolving\n\nAnd that’s when it hit me: This is exactly the model we need for the AI age.\nInstead of being forced into permanent “AI managers,” what if we could swing between:\n\nDeep technical work where we write and refine code directly\nStrategic orchestration where we guide AI systems\nCreative problem-solving that combines both approaches\n\nThis balanced approach resonates deeply with what I’m hearing from fellow engineers. My research shows a clear message: maintaining strong engineering fundamentals is more crucial than ever. We need deep technical knowledge to effectively review, verify, and adjust AI-generated code - because it’s often not quite right. When asked about their concerns with AI coding assistants, software engineers ranked code quality and security well above job security.\n\n      Top Concerns About AI Coding Assistants Among Software Engineers\n\nThat tells me something profound: we see ourselves as guardians of engineering excellence, ensuring that AI-generated solutions adhere to solid software engineering principles. We’re not looking to delegate our expertise to AI - we’re evolving to apply our craft in new ways.\nYour Move\nAs we navigate this transformation, a fundamental truth emerges: our identity crisis isn’t really about AI at all. The research into Human-AI teaming, the parallels with management transitions, the pendulum of roles - they all point to something deeper. Beyond choosing between builder or overseer lies the heart of who we are: creators.\nAnd now we come full circle: AI isn’t taking our jobs; it’s giving us a chance to reclaim those broader aspects of our role that we gave away to specialists. To return to a time when software engineering meant more than just writing code. When it meant understanding the whole problem space, from user needs to business impact, from system design to operational excellence.\nThe pendulum metaphor offers us wisdom here. Just as many of us have swung between engineering and management roles, we can embrace a similar fluidity with AI. Some periods we’ll dive deep into the code, experiencing that thrill of crafting elegant solutions. Other times we’ll step back to guide AI systems - not as overseers, but as master builders who understand every part of their craft. Like the Industrial Revolution’s workers who became experts at optimising the machines that transformed their craft, we can master these AI systems - making them instruments of our creativity, not replacements for it.\nIn the AI era, what matters most is preserving the essence of who we are: that pure joy of building things, of solving hard problems, of making something work exactly right. Our engineering excellence runs deeper than just verifying AI’s work - it flows from knowing systems so intimately that we can shape them, improve them, transform them.\nThe choice isn’t whether AI will change our industry - it already is. The real choice is how we evolve with it. Will we cling to an outdated sense of what it means to be an engineer? Or will we reclaim our craft, not as mere coders, but as master builders of AI-augmented systems?\nThe pendulum is swinging - will you hold your ground, or move with it?",
    "summary": {
      "en": "**Summary: The Software Engineering Identity Crisis**\n\nThe role of software engineers is changing due to the rise of AI coding assistants, shifting their identity from creators to overseers or managers. Many engineers initially entered the field to build and solve problems, finding joy in coding. However, as AI takes over more coding tasks, engineers are spending less time writing code and more time managing AI-generated outputs.\n\nRecent studies show that many engineers feel their core skills are becoming less relevant, with a significant portion of new code being produced by AI. This shift raises questions about the future identity of software engineers: Are they evolving into high-level architects, or are they losing the essence of what it means to be a builder?\n\nThe article discusses the need for engineers to adapt to these changes by embracing new skills, such as prompt engineering and systems thinking, while still preserving their technical expertise. It emphasizes that, despite the challenges, AI tools can enhance the creative process and enable faster problem-solving.\n\nUltimately, the future of software engineering may not be about resisting AI but finding a balance between hands-on coding and overseeing AI systems. The key lies in maintaining a deep understanding of engineering principles while adapting to the evolving landscape, ensuring that the core joy of creating remains intact.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "b5943bb62ae43b9d",
    "title": {
      "en": "Move on to ESM-Only",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://antfu.me/posts/move-on-to-esm-only",
    "score": 112,
    "by": "bpierre",
    "time": 1742776525,
    "content": "The Toolings are Ready Modern Tools Top-Down & Bottom-Up Requiring ESM in Node.js The Troubles with Dual Formats Interop Issues Dependency Resolution Package Size When Should We Move to ESM-only? New Packages Browser-targeted Packages Standalone CLI Node.js Support Know Your Consumers How Far We Are? Moving Forward Three years ago, I wrote a post about shipping ESM & CJS in a single package, advocating for dual CJS/ESM formats to ease user migration and trying to make the best of both worlds. Back then, I didn’t fully agree with aggressively shipping ESM-only, as I considered the ecosystem wasn’t ready, especially since the push was mostly from low-level libraries. Over time, as tools and the ecosystem have evolved, my perspective has gradually shifted towards more and more on adopting ESM-only.As of 2025, a decade has passed since ESM was first introduced in 2015. Modern tools and libraries have increasingly adopted ESM as the primary module format. According to WOOORM’s script, the packages that ships ESM on npm in 2021 was 7.8%, and by the end of 2024, it had reached 25.8%. Although a significant portion of packages still use CJS, the trend clearly shows a good shift towards ESM.ESM adoption over time, generated by the npm-esm-vs-cjs script. Last updated at 2024-11-27Here in this post, I’d like to share my thoughts on the current state of the ecosystem and why I believe it’s time to move on to ESM-only.The Toolings are Ready #Modern Tools #With the rise of Vite as a popular modern frontend build tool, many meta-frameworks like Nuxt, SvelteKit, Astro, SolidStart, Remix, Storybook, Redwood, and many others are all built on top of Vite nowadays, that treating ESM as a first-class citizen.As a complement, we have also testing library Vitest, which was designed for ESM from the day one with powerful module mocking capability and efficient fine-grain caching support.CLI tools like tsx and jiti offer a seamless experience for running TypeScript and ESM code without requiring additional configuration. This simplifies the development process and reduces the overhead associated with setting up a project to use ESM.Other tools, for example, ESLint, in the recent v9.0, introduced a new flat config system that enables native ESM support with eslint.config.mjs, even in CJS projects.Top-Down & Bottom-Up #Back in 2021, when SINDRESORHUS first started migrating all his packages to ESM-only, for example, find-up and execa, it was a bold move. I consider this move as a bottom-up approach, as the packages that rather low-level and many their dependents are not ready for ESM yet. I was worried that this would force those dependents to stay on the old version of the packages, which might result in the ecosystem being fragmented. (As of today, I actually appreciate that move bringing us quite a lot of high-quality ESM packages, regardless that the process wasn’t super smooth).It’s way easier for an ESM or Dual formats package to depend on CJS packages, but not the other way around. In terms of smooth adoption, I believe the top-down approach is more effective in pushing the ecosystem forward. With the support of high-level frameworks and tools from top-down, it’s no longer a significant obstacle to use ESM-only packages. The remaining challenges in terms of ESM adoption primarily lie with package authors needing to migrate and ship their code in ESM format.Requiring ESM in Node.js #The capability to require() ESM modules in Node.js, initiated by JOYEECHEUNG, marks an incredible milestone. This feature allows packages to be published as ESM-only while still being consumable by CJS codebases with minimal modifications. It helps avoid the async infection (also known as Red Functions) introduced by dynamic import() ESM, which can be pretty hard, if not impossible in some cases, to migrate and adapt.This feature was recently unflagged and backported to Node.js v22 (and soon v20), which means it should be available to many developers already. Consider the top-down or bottom-up metaphor, this feature actually makes it possible to start ESM migration also from middle-out, as it allows import chains like ESM → CJS → ESM → CJS to work seamlessly.To solve the interop issue between CJS and ESM in this case, Node.js also introduced a new export { Foo as 'module.exports' } syntax in ESM to export CJS-compatible exports (by this PR). This allows package authors to publish ESM-only packages while still supporting CJS consumers, without even introducing breaking changes (expcet for changing the required Node.js version).For more details on the progress and discussions around this feature, keep track on this issue.The Troubles with Dual Formats #While dual CJS/ESM packages have been a quite helpful transition mechanism, they come with their own set of challenges. Maintaining two separate formats can be cumbersome and error-prone, especially when dealing with complex codebases. Here are some of the issues that arise when maintaining dual formats:Interop Issues #Fundamentally, CJS and ESM are different module systems with distinct design philosophies. Although Node.js has made it possible to import CJS modules in ESM, dynamically import ESM in CJS, and even require() ESM modules, there are still many tricky cases that can lead to interop issues.One key difference is that CJS typically uses a single module.exports object, while ESM supports both default and named exports. When authoring code in ESM and transpiling to CJS, handling exports can be particularly challenging, especially when the exported value is a non-object, such as a function or a class. Additionally, to make the types correct, we also need to introduce further complications with .d.mts and .d.cts declaration files. And so on…As I am trying to explain this problem deeper, I found that I actually wish you didn’t even need to be bothered with this problem at all. It’s frankly too complicated and frustrating. If you are just a user of packages, let alone the package authors to worry about that. This is one of the reasons I advocate for the entire ecosystem to transition to ESM, to leave these problems behind and spare everyone from this unnecessary hassle.Dependency Resolution #When a package has both CJS and ESM formats, the resolution of dependencies can become convoluted. For example, if a package depends on another package that only ships ESM, the consumer must ensure that the ESM version is used. This can lead to version conflicts and dependency resolution issues, especially when dealing with transitive dependencies.Also for packages that are designed to used with singleton pattern, this might introduce multiple copies of the same package and cause unexpected behaviors.Package Size #Shipping dual formats essentially doubles the package size, as both CJS and ESM bundles need to be included. While a few extra kilobytes might not seem significant for a single package, the overhead can quickly add up in projects with hundreds of dependencies, leading to the infamous node_modules bloat. Therefore, package authors should keep an eye on their package size. Moving to ESM-only is a way to optimize it, especially if the package doesn’t have strong requirements on CJS.When Should We Move to ESM-only? #This post does not intend to diminish the value of dual-format publishing. Instead, I want to encourage evaluating the current state of the ecosystem and the potential benefits of transitioning to ESM-only.There are several factors to consider when deciding whether to move to ESM-only:New Packages #I strongly recommend that all new packages be released as ESM-only, as there are no legacy dependencies to consider. New adopters are likely already using a modern, ESM-ready stack, there being ESM-only should not affect the adoption. Additionally, maintaining a single module system simplifies development, reduces maintenance overhead, and ensures that your package benefits from future ecosystem advancements.Browser-targeted Packages #If a package is primarily targeted for the browser, it makes total sense to ship ESM-only. In most cases, browser packages go through a bundler, where ESM provides significant advantages in static analysis and tree-shaking. This leads to smaller and more optimized bundles, which would also improve loading performance and reduce bandwidth consumption for end users.Standalone CLI #For a standalone CLI tool, it’s no difference to end users whether it’s ESM or CJS. However, using ESM would enable your dependencies to also be ESM, facilitating the ecosystem’s transition to ESM from a top-down approach.Node.js Support #If a package is targeting the evergreen Node.js versions, it’s a good time to consider ESM-only, especially with the recent require(ESM) support.Know Your Consumers #If a package already has certain users, it’s essential to understand the dependents’ status and requirements. For example, for an ESLint plugin/utils that requires ESLint v9, while ESLint v9’s new config system supports ESM natively even in CJS projects, there is no blocker for it to be ESM-only.Definitely, there are different factors to consider for different projects. But in general, I believe the ecosystem is ready for more packages to move to ESM-only, and it’s a good time to evaluate the benefits and potential challenges of transitioning.How Far We Are? #The transition to ESM is a gradual process that requires collaboration and effort from the entire ecosystem. Which I believe we are on a good track moving forward.To improve the transparency and visibility of the ESM adoption, I recently built a visualized tool called Node Modules Inspector for analyzing your packages’s dependencies. It provides insights into the ESM adoption status of your dependencies and helps identify potential issues when migrating to ESM.Here are some screenshots of the tool to give you a quick impression:Node Modules Inspector - OverviewNode Modules Inspector - Dependency GraphNode Modules Inspector - Reports like ESM Adoptions and Duplicated PackagesThis tool is still in its early stages, but I hope it will be a valuable resource for package authors and maintainers to track the ESM adoption progress of their dependencies and make informed decisions about transitioning to ESM-only.To learn more about how to use it and inspect your projects, check the repository node-modules-inspector.Moving Forward #I am planning to gradually transition the packages I maintain to ESM-only and take a closer look at the dependencies we rely on. We also have plenty of exciting ideas for the Node Modules Inspector, aiming to provide more useful insights and help find the best path forward.I look forward to a more portable, resilient, and optimized JavaScript/TypeScript ecosystem.I hope this post has shed some light on the benefits of moving to ESM-only and the current state of the ecosystem. If you have any thoughts or questions, feel free to reach out using the links below. Thank you for reading!",
    "summary": {
      "en": "The text discusses the transition from dual CommonJS (CJS) and ECMAScript Modules (ESM) formats to a focus on ESM-only packages in Node.js. Here are the key points simplified:\n\n1. **Current State of ESM Adoption**: The use of ESM in packages is increasing. In 2021, only 7.8% of npm packages used ESM, but by the end of 2024, this rose to 25.8%.\n\n2. **Modern Tools**: New tools and frameworks, like Vite, support ESM as a primary format, making development easier. Tools like ESLint and testing libraries are also adapting to ESM.\n\n3. **Challenges of Dual Formats**: Maintaining both CJS and ESM can lead to issues such as interoperability problems, complicated dependency resolution, and increased package sizes.\n\n4. **When to Transition to ESM-only**:\n   - **New Packages**: Should be ESM-only to simplify development and maintenance.\n   - **Browser-targeted Packages**: ESM improves performance and optimizes bundle sizes.\n   - **Standalone CLI Tools**: Transitioning to ESM can facilitate ecosystem growth.\n   - **Awareness of Consumers**: Understand the needs of current users before making the switch.\n\n5. **Moving Forward**: The transition to ESM is ongoing and requires collaboration. Tools like the Node Modules Inspector help analyze dependencies and ESM adoption, aiding developers in the transition process.\n\nThe author encourages evaluating the benefits of moving to ESM-only and believes the ecosystem is ready for more packages to make this shift.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "bc798ab915ad080e",
    "title": {
      "en": "The case of the critical section that let multiple threads enter a block of code",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://devblogs.microsoft.com/oldnewthing/20250321-00/?p=110984",
    "score": 135,
    "by": "luu",
    "time": 1742717665,
    "content": "March 24, 2025\n      On how different Windows ABIs choose how to pass 32-bit values in 64-bit registers\n\n        Raymond Chen",
    "summary": {
      "en": "On March 24, 2025, Raymond Chen discussed how various Windows Application Binary Interfaces (ABIs) handle the passing of 32-bit values using 64-bit registers.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "8d6f400bc4fd96b8",
    "title": {
      "en": "To Win Trust and Admiration, Fix Your Microphone",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.scientificamerican.com/article/to-win-trust-and-admiration-fix-your-microphone/",
    "score": 10,
    "by": "jimminyx",
    "time": 1742881602,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "59c3d3c0405f7a06",
    "title": {
      "en": "The belay test and the modern American climbing gym",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.climbing.com/people/peter-mayfield-belay-test-modern-american-climbing-gym-history/",
    "score": 151,
    "by": "vasco",
    "time": 1742408381,
    "content": "Meet the Guy Who Invented the Belay Test—and the Modern American Climbing Gym\n\n          The story of California’s City Rock\n\n      Updated\n\n        Mar 18, 2025\n\n          Bruce Hildenbrand\n\n      Comment\n\n    Share\n\n            Share to Outside Feed\n            Create a new post with the article attached\n\n          Copy link\n\n          Email\n\n          Share on X\n\n          Share on Facebook\n\n          Share on Reddit\n\n              Photo: Peter Mayfield Collection\n\n                Peter Mayfield bouldering at City Rock Photo: Peter Mayfield Collection\n\n      Heading out the door? Read this article on the new Outside+ app available now on iOS devices for members!\n      Download the app.\n\n      Forty years ago, there was not a single purpose-built climbing gym in the U.S. In 1987, Seattle’s Vertical Club opened and a year later, the Portland Rock Gym launched. But the concept of a profitable indoor climbing gym was far from proven. This is the story of the person whose vision and determination would revolutionize climbing gyms as we know them today.\nPeter Mayfield was a climbing prodigy. At 16 years old, in 1978, he was repeating the hardest free climbs in Yosemite. He went on to establish some of the hardest aid routes on the Valley’s big walls. By the age of 19, he was guiding for the prestigious Yosemite Mountaineering School, and became chief guide a year later. When sport climbing competitions began in the late 1980s, Mayfield participated in the first ever Snowbird climbing competition as well as Jeff Lowe’s national competitions.\nIn 1988, Mayfield formed a guiding cooperative with legends Kim Schmitz and Jim Bridwell, but by his mid-twenties, he was looking for a job that would keep him closer to home. So when Bridwell returned from a trip to Europe with posters and magazines of climbing walls going up in France, Mayfield knew he’d found his next venture.\n\nPeter Mayfield at 19 years old on his first ascent in 1981 of Zenyatta Mondatta on El Cap with Jim Bridwell. Twenty years after his FA, Mayfield repeated the route with his 19-year-old son Braden. (Photo: Peter Mayfield Collection)\nAt the time, the walls in France were closer to artwork in public spaces than purpose-built climbing structures. It was around that time that the Vertical Club—a space for experienced climbers to train—went up, but Mayfield had something completely different in mind. As chief guide of the Yosemite Mountaineering School, he didn’t just want to create a place for himself and his buddies to get stronger. “I had this serious career of turning people onto this sport,” reflects Mayfield. “I was really thinking [about] kids, corporate programs … The word gym wasn’t even in my mind. It was more like an indoor climbing institute.”\nWith the popularity of indoor climbing gyms these days, it’s hard to imagine what it was like in the late ’80s, when climbing itself was still very much a niche activity. Mayfield knew that to keep the gym lights on, he’d need to attract more than just his hardcore climbing buddies.\nThen there was the problem with California’s legendary great weather. Mayfield was skeptical that climbers would want to climb indoors on days when the sun was shining, because unlike Seattle and Portland, you could climb outdoors virtually year-round. So his marketing plan was also designed to get non-climbers to come to his gym. He wanted to build a place where “10,000 eight-year-old girls could try something they never dreamed of.”\n“That was my pitch,” Mayfield says.\nThe economics of the first commercial climbing gym\nMayfield found his first investor while guiding an ascent of Alaska’s Moose’s Tooth. Tent-bound by storms for days, he discussed his climbing gym dreams with his clients. At the end of the expedition, as they were splitting up at the Oakland Airport, one of the clients asked him what he needed to start his business. He replied that he needed money for a phone line. The client pulled out his wallet and gave him $200. “I literally got handed two C-notes to start my business,” he remembers. He used that money to put in a Pacific Bell phone line in his mother’s basement in Berkeley and got to work.\n“I got no positive reaction from the [climbing] industry at all,” remembers Mayfield. “I was pretty connected because I climbed with Bridwell. I met with the CEO of The North Face. I met with the CEO of Marmot. I met with the president of REI. They all said ‘it will never work. It makes sense in Seattle where it rains. Who would ever climb indoors in California?”\nMayfield even sent his business plan to Yvon Chouinard. “He was very polite and gave me some good advice,” Mayfield recalls. “He did not invest. None of them invested. But he kind of got what I was talking about.” (Chouinard did tell Mayfield that he should try to sell milkshakes: “If you can nail it with the youth, you will kick ass.”)\nSo Mayfield widened his net, contacting ”everybody in Silicon Valley who had ever climbed” and raised half a million dollars with 48 limited partners, essentially a large group of $5,000 investors. He stalled for a while, dealing with the mountain of paperwork associated with creating such a high volume of limited partnerships. But in the midst of his fundraising, he produced the Great Outdoor Adventure Fair in San Francisco. To promote his concept to the general public, he set up a portable climbing wall at the fair. John Gage, high up in management at one of Silicon Valley’s most successful startups, Sun Microsystems, was impressed and tossed in $40,000, re-energizing Mayfield.\nBut at that time, building a climbing gym was a huge, complex undertaking. Mayfield relied on a suite of competent advisers to help him through. He eventually hooked up with Armand Tanny of Gold’s Gym, who explained the new, revolutionary process of selling and managing memberships via an automatic fund transfer. They also impressed upon him the importance of aesthetics. The other two climbing gyms had spent no more than $18,000 building their gyms. Mayfield spent $40,000 on just the bathrooms.\nChristian Griffith (designer of City Rock walls) and Peter Mayfield (Photo: Beth Wald)\nBuilding a gym for climbers—and non-climbers\n“So then I was like, ‘Okay, this is a gym. This is not an indoor sculpture garden that we climb on that’s a climbing institute. I am going to build a business model with monthly memberships,'” adds Mayfield. He decided to call it City Rock Gym. The gym was located in Emeryville, between Oakland and Berkeley.\n“I made the first gym you could take your kid to and trust that they would be well taken care of,” he explains.\nA number of climbing wall companies, such as Entre-Prises, were pressing Mayfield to help him build his gym. Instead, he turned to Christian Griffith, who he had climbed with on the competition circuit, and who felt that they could design an ”incredible space” together. “Christian posted up in south Berkeley in my mom’s yard and carved foam for two weeks and he completely created that gym,” recalls Mayfield. “He took foam core and glued graph paper to it and he did a beautiful model. He was inspired by the gritstone cliffs in England where you are always like ‘what’s around the corner?’ So City Rock had these really protuberant buttresses.”\nOverall, the City Rock Gym design was both intentional and approachable. “When people walked in the door I wanted them to see the friendly slab. I wanted them to think they can do this,” he explains. “I want them to see the six year olds climbing. I really felt that I had to prove the concept. It’s not me and my bros throwing down at the boulders.”\nInstruction served as another key pioneering element of Mayfield’s gym concept. He began teaching the climbing safety techniques he’d learned from the Sierra Club, an organization which had been studying and teaching belay practices since the 1930s. “We really did invent the belay test,” he recalls, citing the need for procedure in such a high-risk business. “I thought somebody would die at our business. I ran the statistics in my head. So many people tying knots. So many people belaying. I really took it seriously.”\nGiven the magnitude of risk, Mayfield says he really “leaned into” establishing a belay test and enforcing safety. “I had some really good, famous, climbers come in and fail the belay test,” he recalls. “These good climbers were just a little too casual with their hands. You don’t vaguely slide your hands around. They had to tighten their act up a little bit.”\nOn City Rock Gym’s grand opening weekend in 1990, 900 people came in to climb. Mayfield watched brand-new climbing ropes wear out in front of his eyes. “We installed these half-pipe sealed bearing things so the ropes wouldn’t wear out, but on Sunday morning: ‘Oh my God! I see cores in that rope. Quick, replace that,’” remembers Mayfield. “I took the climbing school really seriously. I hired the best and they were well trained … It was really high level with really good technique coaching.”\nCity Rock quickly gained a loyal following. With his emphasis on introducing new participants to the sport, Mayfield pioneered programs like a kid’s belay clinic and the first climbing-themed birthday parties. He created an event called Women on Rock and flew in Lynn Hill to teach it. The vertical dance team Bandaloop got its start at City Rock in 1991.\nThe vertical dance company Bandaloop was born at City Rock (Photo: Peter Mayfield Collection)\nBut could a commercial climbing gym keep elite climbers stoked?\nWhile he was intent on attracting non-climbers to the gym, Mayfield didn’t turn his back on his climbing buddies. He created routes for their level of difficulty and also gave some of them jobs. Many experienced climbers including Scott Cosgrove, Steve Schneider, Bird Lew, and Don Welch worked at City Rock.\n“We set some hard routes,” remembers Schneider, a 5.14 climber and one of the routesetters at City Rock. “We did some competitions there which were very successful and those routes would be left up for others to climb.”\nAs the initial buzz of City Rock’s grand opening wore off, two vital questions remained: Were established climbers willing to climb indoors on sunny days? And were the routes difficult enough to keep experienced climbers satisfied? “I was initially a little skeptical but found I really enjoyed it,” recalls Greg Murphy who, together with Chan Harrell, held the fastest known time on the Northwest Face of Half Dome. “There was nothing super steep as I recall, so the harder routes were kind of finger intensive. But it always seemed like there was a good mix of routes and some harder projects to work on.”\nAnne Smith, an excellent climber and the mother of Connor Herson, agrees: “CR [City Rock] was very much geared towards experienced climbers. [It] hosted National Championship events and set at least as many harder climbers as easier ones.”\nAs a competitive sport climber himself, Mayfield eagerly hosted national championship competitions five years in a row. Hans Florine had set the speed record on the Nose with Steve Schnieder the week before he won the 1990 nationals in both difficulty and speed at City Rock. For the next three years, Florine partnered with Mayfield and City Rock to stage the national championships there. The gym played host to top climbers such as Robyn Erbesfield, Bobbi Bensman, Dale Goddard, and Jim Karn.\nHank Levine competing in US Nationals at City Rock (Photo: Peter Mayfield Collection)\nThe inclusive climbing gym concept takes off\nThree years after City Rock’s opening, California got its second climbing gym via Tom Davis’s now-famous Pacific Edge, in Santa Cruz. (Chris Sharma and Natalia Grossman both learned to climb there.) Davis was inspired by what Mayfield had created: “It was visionary to open a gym in California at the time—but that is really hard to imagine now. From a dream to a nationwide industry.”\nBut Davis recalls that perhaps the most important thing City Rock did for the climbing gym industry was establish safety standards to manage risk, including legitimate belay tests. “It could easily have been a dirtbag climbers’ training hangout,” Davis explains. “But they pulled off something that demonstrated the true value of climbing: risk taking with the correct skillset, and on a level that a discerning public could understand.”\nIn 1992, Mayfield and Casey Newman, who ran the Colorado Rock Gym, along with several others, founded the Climbing Gym Association, which has since become the Climbing Wall Association. The purpose of this organization was to regulate the burgeoning climbing gym industry, hopefully keeping the government from stepping in with its own set of regulations. They built a gym accreditation program and traveled around the country accrediting the dozens of climbing gyms that existed by that time in the U.S. But that’s a story for another time.\nWhat ever happened to the original City Rock—and Mayfield?\nSo how did the original design of City Rock hold up? Jim Collins, a notable climber, bestselling author, and business guru who taught MBA students at Stanford University, studied City Rock as a class project. The academic analysis found that City Rock was “going to get blown out of the water by the next wave of climbing gyms,” in Mayfield’s words. Collins and his students had two major reasons to back up their conclusion. The first was that City Rock’s walls weren’t steep enough. And the second reason was that they hypothesized other entrepreneurs would come along and operate gyms more efficiently at a lower cost.\nMayfield explains the limited steepness of his gym’s original walls: “[City Rock] was designed and built before America had super steep climbing. It was before there was a Rifle. It was before there was a Jailhouse Rock. Two or three years later, the next wave of gyms did have arches and bigger, steeper, upside-down climbing structures because the cliffs that people were climbing on were like that. The hard climbing in 1989 was like Smith Rock. Dead vertical or maybe a bit past vertical, like 100 degrees.”\nIn 1997, Mayfield sold City Rock to Mark Melvin, who founded Mission Cliffs, now Touchstone Climbing. He then moved back to Yosemite, where he started an eco-tourism business to educate people about nature and conservation in the Valley.\nWhile the decision to sell the gym he poured his heart and soul into for the better part of 10 years was difficult, he made some money and his investors did well trading up into the Touchstone world. “I couldn’t care less about making money. I am just not wired that way to manipulate the world to make money.” He says that in retrospect, he was “faking it” as a business person and finds himself better adapted to social entrepreneurship.\nAfter the sale, City Rock fell victim to the skyrocketing rents associated with the Internet boom in the Bay Area. When rent tripled in the highly desirable Emeryville area, it was time to look for another location. The new owners of City Rock honored its memberships and opened Berkeley Ironworks a few miles away in 2000. It was three times the size of City Rock and also featured a large fitness area, a yoga studio, and an indoor cycling room. It represented the next generation of climbing gyms.\nNowadays, Mayfield lives in Truckee, CA, where he founded the nonprofit Gateway Mountain Center, which focuses on nature-based therapy for kids. “Even to this day, I stay interested in the effect [of climbing gyms] on youth, especially high-need youth,” says Mayfield. He also works with kids from local drug treatment centers and juvenile detention camps. “I am watching these kids just light up,” he says. “They face incredible challenges in life, but they get to climb. This is transformative for high-need youth.”\nThirty-five years have passed since Mayfield first started City Rock. While his impacts on the climbing gym industry are clear, he also continues to abide by his personal commitment to benefit youth through climbing. Mayfield still climbs, but these days, prefers Berkeley’s iconic Indian Rock to a climbing gym.\nReflecting on his pioneering foray into climbing gyms, he says: “I had kind of a classic business school case study of being the pioneer who spends all the money on R&D to prove the concept, then gets blown out of the water by the next wave, the well-funded MBAs who come after.”\n\n    Similar Reads\n\n          A Climbing Graphic Novel? Competitive Climbing Becomes Art in This New Book.\n\n          The Horrors of the Gym Belay Test\n\n          Want More From Your Climbing Gym? These 7 US Facilities Are Getting Creative.\n\n          Decolonizing Climbing in Malawi and Beyond\n\n      Tags\n\n        climbing gyms\n\n        Peter Mayfield\n\n        Recycle",
    "summary": {
      "en": "**Summary:**\n\nPeter Mayfield, a talented climber, significantly changed the climbing gym landscape in the U.S. by founding City Rock in 1990. Before then, there were no dedicated climbing gyms, and the concept seemed unlikely to succeed, especially in sunny California. Mayfield, driven by a vision to make climbing accessible to everyone, including kids and non-climbers, created a welcoming environment with a focus on safety. He introduced a belay test to ensure climbers were properly trained.\n\nDespite initial skepticism from the climbing industry, Mayfield raised funds and built City Rock in Emeryville, which quickly gained popularity, attracting both novice and experienced climbers. The gym also hosted national climbing competitions and was known for innovative programs like kids' belay clinics and climbing-themed birthday parties.\n\nAfter selling City Rock in 1997, Mayfield moved on to eco-tourism and eventually founded a nonprofit focused on helping at-risk youth through nature and climbing. His pioneering efforts laid the groundwork for the modern climbing gym industry, which has since expanded rapidly across the country. Today, Mayfield continues to influence youth through climbing, reflecting on his journey with a sense of pride and commitment to social entrepreneurship.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "dc82356fa3518944",
    "title": {
      "en": "Most AI value will come from broad automation, not from R & D",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://epoch.ai/gradient-updates/most-ai-value-will-come-from-broad-automation-not-from-r-d",
    "score": 202,
    "by": "ydnyshhh",
    "time": 1742668500,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "b78601fed6f74ae5",
    "title": {
      "en": "A USB Interface to the \"Mother of All Demos\" Keyset",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.righto.com/2025/03/mother-of-all-demos-usb-keyset-interface.html",
    "score": 317,
    "by": "zdw",
    "time": 1742743886,
    "content": "Ken Shirriff's blog\n\nComputer history, restoring vintage computers, IC reverse engineering, and whatever\n\nA USB interface to the \"Mother of All Demos\" keyset\n\nIn the early 1960s, Douglas Engelbart started investigating how computers could augment human intelligence:\n\"If, in your office, you as an intellectual worker\nwere supplied with a computer display backed up by a computer that was alive for you all day and was instantly responsive to every\naction you had, how much value could you derive from that?\"\nEngelbart developed many features of modern computing that we now take for granted: the mouse,1 hypertext, shared documents, windows,\nand a graphical user interface.\nAt the 1968 Joint Computer Conference, Engelbart demonstrated these innovations in a groundbreaking presentation, now known as\n\"The Mother of All Demos.\"\n\nThe keyset with my prototype USB interface.\nEngelbart's demo also featured an input device known as the keyset, but unlike his other innovations, the keyset failed to catch on.\nThe 5-finger keyset lets you type without moving your hand, entering characters by pressing multiple keys simultaneously as a chord.\nChristina Englebart, his daughter, loaned one of Engelbart's keysets to me.\nI constructed an interface to connect the keyset to USB, so that it can be used with a modern computer.\nThe video below shows me typing with the keyset, using the mouse buttons to select upper case and special characters.2\n\nI wrote this blog post to describe my USB keyset interface.\nAlong the way, however, I got sidetracked by the history of The Mother of All Demos and how it obtained that name.\nIt turns out that Engelbart's demo isn't the first demo to be called \"The Mother of All Demos\".\nEngelbart and The Mother of All Demos\n\nEngelbart's work has its roots in\nVannevar Bush's 1945 visionary essay, \"As We May Think.\"\nBush envisioned thinking machines, along with the \"memex\", a compact machine holding a library of collective knowledge with hypertext-style links: \"The Encyclopedia Britannica could be reduced to the volume of a matchbox.\"\nThe memex could search out information based on associative search, building up a hypertext-like trail of connections.\nIn the early 1960s, Engelbart was inspired by Bush's essay and set out\nto develop means to augment human intellect: \"increasing the capability of a man to approach a complex problem situation, to gain comprehension to suit his particular needs, and to derive solutions to problems.\"3\nEngelbart founded the Augmentation Research Center at the Stanford Research Institute (now SRI), where\nhe and his team created a system called NLS (oN-Line System).\nEngelbart editing a hierarchical shopping list.\nIn 1968, Engelbart demonstrated NLS to a crowd of two thousand people\nat the Fall Joint Computer Conference.\nEngelbart gave the demo from the stage, wearing a crisp shirt and tie and a headset microphone.\nEngelbart created hierarchical documents, such as the shopping list above, and moved around them with hyperlinks.\nHe demonstrated how text could be created, moved, and edited with the keyset and mouse.\nOther documents included graphics, crude line drawing by today's standards but cutting-edge for the time.\nThe computer's output was projected onto a giant screen, along with video of Engelbart.\nEngelbart using the keyset to edit text. Note that the display doesn't support lowercase text; instead, uppercase is indicated by a line above the character. Adapted from The Mother of All Demos.\nEngelbart sat at a specially-designed Herman Miller desk6 that held the\nkeyset, keyboard, and mouse, shown above.\nWhile Engelbart was on stage in San Francisco,\nthe SDS 9404 computer that ran the NLS software was 30 miles to the south in Menlo Park.5\nTo the modern eye, the demo resembles a PowerPoint presentation over Zoom, as\nEngelbart collaborated with\nJeff Rulifson and Bill Paxton, miles away in Menlo Park.\n(Just like a modern Zoom call, the remote connection started with \"We're not hearing you. How about now?\")\nJeff Rulifson browsed the NLS code, jumping between code files with hyperlinks and expanding subroutines by clicking on them.\nNLS was written in custom high-level languages, which they developed\nwith a \"compiler compiler\" called TREE-META.\nThe NLS system held interactive documentation as well as tracking bugs and changes.\nBill Paxton interactively drew a diagram and then demonstrated how NLS could be used as a database, retrieving information by searching on keywords.\n(Although Engelbart was stressed by the live demo, Paxton told me that he was \"too young and inexperienced to be concerned.\")\nBill Paxton, in Menlo Park, communicating with the conference in San Francisco.\nBill English, an electrical engineer, not only built the first mouse for Engelbart but was also the hardware mastermind behind the demo.\nIn San Francisco, the screen images were projected on a 20-foot screen by a Volkswagen-sized\nEidophor projector, bouncing light off a modulated oil film.\nNumerous cameras, video switchers and mixers created the video image.\nTwo leased microwave links and half a dozen antennas connected SRI in Menlo Park to the demo in San Francisco.\nHigh-speed modems send the mouse, keyset, and keyboard signals from the demo back to SRI.\nBill English spent months assembling the hardware and network for the demo and then managed the demo behind the scenes, assisted by a team of about 17 people.\nAnother participant was the famed counterculturist Stewart Brand, known for the Whole Earth Catalog\nand the WELL, one of the oldest online virtual communities.\nBrand advised Engelbart on the presentation, as well as running a camera. He'd often point the camera at a monitor to generate swirling psychedelic\nfeedback patterns, reminiscent of the LSD that he and Engelbart had experimented with.\nThe demo received press attention such as\na San Francisco Chronicle article titled \"Fantastic World of Tomorrow's Computer\".\nIt stated, \"The most fantastic glimpse into the computer future was taking place in a windowless room on the third floor of the Civic Auditorium\"\nwhere Engelbart \"made a computer in Menlo Park do secretarial work for him that ten efficient secretaries couldn't do in twice the time.\"\nHis goal: \"We hope to help man do better what he does—perhaps by as much as 50 per cent.\"\nHowever, the demo received little attention in the following decades.7\nEngelbart continued his work at SRI for almost a decade, but as Engelbart commented with frustration,\n“There was a slightly less than universal perception of our value at SRI”.8\nIn 1977, SRI sold the Augmentation Research Center to Tymshare, a time-sharing computing company.\n(Timesharing was the cloud computing of the 1970s and 1980s,\nwhere companies would use time on a centralized computer.)\nAt Tymshare, Engelbart's system was renamed AUGMENT and marketed as an office automation service, but Engelbart himself was sidelined from development,\na situation that he described as\nsitting in a corner and becoming invisible.\nMeanwhile, Bill English and some other SRI researchers9 migrated four miles south to Xerox PARC and worked on the Xerox Alto computer.\nThe Xerox Alto incorporated many ideas from the Augmentation Research Center including the graphical user interface, the mouse, and the keyset.\nThe Alto's keyset\nwas almost identical to the Engelbart keyset, as can be seen in the photo below.\nThe Alto's keyset was most popular for the networked 3D shooter game \"Maze War\", with the clicking of keysets echoing through the hallways of Xerox PARC.\nA Xerox Alto with a keyset on the left.\nXerox famously failed to commercialize the ideas from the Xerox Alto, but Steve Jobs recognized the importance of interactivity, the graphical user interface, and the mouse\nwhen he visited Xerox PARC in 1979.\nSteve Jobs provided the Apple Lisa and Macintosh ended up with a graphical user interface and the mouse (streamlined to one button instead of three), but he left the keyset behind.10\nWhen McDonnell Douglas acquired Tymshare in 1984, Engelbart and his software—now called Augment—had a new home.11\nIn 1987, McDonnell Douglas released a text editor and outline processor for the IBM PC called\nMiniBASE,\none of the few PC applications that supported a keyset.\nThe functionality of MiniBASE was almost identical to Engelbart's 1968 demo, but in 1987, MiniBASE\nwas competing against GUI-based word processors such as MacWrite and Microsoft Word, so MiniBASE had little impact.\nEngelbart left McDonnell Douglas in 1988, forming a research foundation called the Bootstrap Institute to continue his research independently.\nThe name: \"The Mother of All Demos\"\nThe name \"The Mother of All Demos\" has its roots in the Gulf War.\nIn August 1990, Iraq invaded Kuwait, leading to war between Iraq and a coalition of the United States and 41 other countries.\nDuring the months of buildup prior to active conflict, Iraq's leader, Saddam Hussein,\nexhorted the Iraqi people to prepare for \"the mother of all battles\",12 a phrase that caught the attention of the media.\nThe battle didn't proceed as Hussein hoped: during exactly 100 hours of ground combat, the US-led coalition liberated Kuwait, pushed into Iraq, crushed the Iraqi forces,\nand declared a ceasefire.13\nHussein's mother of all battles became the mother of all surrenders.\nThe phrase \"mother of all ...\" became the 1990s equivalent of a meme, used as a slightly-ironic superlative.\nIt was applied to everything\nfrom The Mother of All Traffic Jams to The Mother of All Windows Books, from The Mother of All Butter Cookies to Apple calling mobile devices\nThe Mother of All Markets.14\nIn 1991, this superlative was applied to a computer demo, but it wasn't Engelbart's demo.\nAndy Grove, Intel's president, gave a keynote speech at Comdex 1991 entitled The Second Decade: Computer-Supported Collaboration,\na live demonstration of his vision for PC-based video conferencing and wireless communication in the PC's second decade.\nThis complex hour-long demo required almost six months to prepare, with 15 companies collaborating.\nIntel called this demo \"The Mother of All Demos\", a name repeated in the New York Times, San Francisco Chronicle, Fortune, and PC Week.15\nAndy Grove's demo was a hit, with over 20,000 people requesting a video tape, but the demo was soon forgotten.\nOn the eve of Comdex, the New York Times wrote about Intel's \"Mother of All Demos\". Oct 21, 1991, D1-D2.\nIn 1994, Wired writer Steven Levy wrote Insanely Great: The Life and Times of Macintosh, the Computer that Changed Everything.8\nIn the second chapter of this comprehensive book, Levy explained how Vannevar Bush and Doug Engelbart \"sparked a chain reaction\" that led to the Macintosh.\nThe chapter described Engelbart's 1968 demo in detail including a throwaway line saying, \"It was the mother of all demos.\"16\nBased on my research, I think this is the source of the name \"The Mother of All Demos\" for Engelbart's demo.\nBy the end of the century, multiple publications echoed Levy's catchy phrase.\nIn February 1999, the San Jose Mercury News had a special article on Engelbart, saying that the demonstration was \"still called 'the mother of all demos'\", a description echoed by\nthe industry publication Computerworld.17\nThe book Nerds: A Brief History of the Internet stated that the demo \"has entered legend as 'the mother of all demos'\".\nBy this point, Engelbart's fame for the \"mother of all demos\" was cemented and the phrase became near-obligatory when writing about him.\nThe classic Silicon Valley history Fire in the Valley (1984), for example,\ndidn't even mention Engelbart but in the second edition (2000),\n\"The Mother of All Demos\" had its own chapter.\nInterfacing the keyset to USB\nGetting back to the keyset interface,\nthe keyset consists of five microswitches, triggered by the five levers.\nThe switches are wired to a standard DB-25 connector.\nI used a Teensy 3.6 microcontroller board for the interface, since this board can act both as a USB device\nand as a USB host.\nAs a USB device, the Teensy can emulate a standard USB keyboard.\nAs a USB host, the Teensy can receive input from a standard USB mouse.\nConnecting the keyset to the Teensy is (almost) straightforward, wiring the switches to five data inputs on the Teensy and the common line connected to ground.\nThe Teensy's input lines can be configured with pullup resistors inside the microcontroller. The result is that a data line shows 1 by default and\n0 when the corresponding key is pressed.\nOne complication is that the keyset apparently has a 1.5 kΩ between the leftmost button and ground, maybe to indicate that the device is plugged in.\nThis resistor caused that line to always appear low to the Teensy.\nTo counteract this and allow the Teensy to read the pin, I connected a 1 kΩ pullup resistor to that one line.\nThe interface code\nReading the keyset and sending characters over USB is mostly straightforward, but there are a few complications.\nFirst, it's unlikely that the user will press multiple keyset buttons at exactly the same time. Moreover, the button contacts may bounce.\nTo deal with this, I wait until the buttons have a stable value for 100 ms (a semi-arbitrary delay) before sending a key over USB.\nThe second complication is that with five keys, the keyset only supports 32 characters. To obtain upper case, numbers, special characters, and control\ncharacters, the keyset is designed to be used in conjunction with mouse buttons.\nThus, the interface needs to act as a USB host, so I can plug in a USB mouse to the interface.\nIf I want the mouse to be usable as a mouse, not just buttons in conjunction with the keyset, the interface mus forward mouse events over USB.\nBut it's not that easy, since mouse clicks in conjunction with the keyset shouldn't be forwarded. Otherwise, unwanted clicks will happen while\nusing the keyset.\nTo emulate a keyboard, the code uses the Keyboard library. This library provides\nan API to send characters to the destination computer.\nInconveniently, the simplest method, print(), supports only regular characters, not special characters like ENTER or BACKSPACE. For those, I needed to\nuse the lower-level press() and release() methods.\nTo read the mouse buttons,\nthe code uses the USBHost_t36 library, the Teensy version of the USB Host library.\nFinally, to pass mouse motion through to the destination computer, I use the Mouse library.\nIf you want to make your own keyset, Eric Schlaepfer has a model here.\nConclusions\nEngelbart claimed  that learning a keyset wasn't\ndifficult—a six-year-old kid could learn it in less than a week—but I'm not willing to invest much time into learning it. In my brief use of the keyset, I found it very difficult to use physically.\nPressing four keys at once is difficult, with the worst being all fingers except the ring finger. Combining this with a mouse button or two at the same time\ngave me the feeling that I was sight-reading a difficult piano piece.\nMaybe it becomes easier with use, but I noticed that Alto programs tended to treat the keyset as function keys, rather than a mechanism for typing with chords.18\nDavid Liddle of Xerox PARC said, \"We found that [the keyset] was tending to slow people down, once you got away from really hot [stuff] system programmers.\nIt wasn't quite so good if you were giving it to other engineers, let alone clerical people and so on.\"\nIf anyone else has a keyset that they want to connect via USB (unlikely as it may be), my code is on\ngithub.19  Thanks to Christina Engelbart for loaning me the keyset. Thanks to Bill Paxton for answering my questions.\nFollow me on Bluesky (@righto.com) or RSS for updates.\nFootnotes and references\n\nEngelbart's use of the mouse wasn't arbitrary, but based on research.\nIn 1966, shortly after inventing the mouse, Engelbart carried out a\nNASA-sponsored study\nthat evaluated six input devices: two types of joysticks, a Graphacon positioner, the mouse,\na light pen, and a control operated by the knees (leaving the hands free).\nThe mouse, knee control, and light pen performed best, with users finding the mouse satisfying to use. Although inexperienced subjects had some trouble with the mouse, experienced subjects considered\nit the best device.\nA joystick, Graphacon, mouse, knee control, and light pen were examined as input devices. Photos from the study.\n↩\n\nThe information sheet below from the Augmentation Research Center shows what keyset chords correspond to each character.\nI used this encoding for my interface software.\nEach column corresponds to a different combination of mouse buttons.\nThe information sheet for the keyset specifies how to obtain each character.\nThe special characters above are <CD> (Command Delete, i.e. cancel a partially-entered command), <BC> (Backspace Character), <OK> (confirm command), <BW>(Backspace Word), <RC> (Replace Character), <ESC> (which does filename completion).\nNLS and the Augment software have the concept of a viewspec, a view specification that controls the\nview of a file.\nFor instance, viewspecs can expand or collapse an outline to show more or less detail, filter the content, or show authorship of sections.\nThe keyset can select viewspecs, as shown below.\nBack of the keyset information sheet.\nViewsets are explained in more detail in The Mother of All Demos.\nFor my keyset interface, I ignored viewspecs since I don't have software to use these inputs, but\nit would be easy to modify the code to output the desired viewspec characters.\n↩\n\nSee Augmenting Human Intellect: A Conceptual Framework, Engelbart's 1962 report.↩\n\nEngelbart used an SDS 940 computer running the Berkeley Timesharing System.\nThe computer had 64K words of core memory, with 4.5 MB of drum storage for swapping and 96 MB of disk storage for files.\nFor displays, the computer drove twelve 5\" high-resolution CRTs, but these weren't viewed directly.\nInstead, each CRT had a video camera pointed at it and the video was redisplayed on a larger display in a work station in each office.\nThe SDS 940 was a large 24-bit scientific computer, built by Scientific Data Systems.\nAlthough SDS built the first integrated-circuit-based commercial computer in 1965 (the SDS 92),\nthe SDS 940 was a transistorized system.\nIt consisted of multiple refrigerator-sized cabinets, as shown below. Since each memory cabinet held 16K words and the computer at SRI had 64K,\nSRI's computer had two additional cabinets of memory.\nFront view of an SDS 940 computer. From the Theory of Operation manual.\nIn the late 1960s, Xerox wanted to get into the computer industry, so Xerox\nbought Scientific Data Systems in 1969 for $900 million (about $8 billion in current dollars).\nThe acquisition was a disaster. After steadily losing money, Xerox decided to exit the mainframe computer business in 1975.\nXerox's CEO summed up the purchase: \"With hindsight, we would not have done the same thing.\"↩\n\nThe Mother of All Demos is on YouTube,\nas well as a five-minute summary for the impatient.↩\n\nThe desk for the keyset and mouse was designed by Herman Miller, the office furniture company.\nHerman Miller worked with SRI to design the\ndesks, chairs, and office walls as part of their plans for the office of the future.\nHerman Miller invented the cubicle office in 1964, creating a modern replacement for the commonly used open office arrangement.↩\n\nEngelbart's demo is famous now, but for many years it was ignored.\nFor instance, Electronic Design had a long\narticle\non Engelbart's work in 1969 (putting the system on the cover), but there was no mention of the demo.\nEngelbart's system was featured on the cover of Electronic Design. Feb 1, 1969. (slightly retouched)\nBut by the 1980s, the Engelbart demo started getting attention.\nThe 1986 documentary Silicon Valley Boomtown had a long\nsection on Engelbart's work and the demo. By 1988, the New York Times was referring to the demo as legendary.↩\n\nLevy had written about Engelbart a decade earlier, in the May 1984 issue of the magazine Popular Computing.\nThe article focused on the mouse, recently available to the public through the Apple Lisa and the IBM PC (as an option).\nThe big issue at the time was how many buttons a mouse should have: three like Engelbart's mouse, the one button that Apple used, or two buttons\nas Bill Gates preferred.\nBut Engelbart's larger vision also came through in Levy's interview along with his frustration that most of his research had been ignored,\novershadowed by the mouse.\nNotably, there was no mention of Engelbart's 1968 demo in the article.↩↩\n\nThe SRI researchers who moved to Xerox include Bill English, Charles Irby, Jeff Rulifson, Bill Duval, and Bill Paxton (details).↩\n\nIn 2023, Xerox donated the entire Xerox PARC research center to SRI. The research center remained in Palo Alto but became part of SRI.\nIn a sense, this closed the circle, since many of the people and ideas from SRI had gone to PARC in the 1970s.\nHowever, both PARC and SRI had changed radically since the 1970s, with the cutting edge of computer research moving elsewhere.↩\n\nFor a detailed discussion of the Augment system, see Tymshare's Augment: Heralding a New Era, Oct 1978.\nAugment provided a \"broad range of information handling capability\" that was not available elsewhere.\nUnlike other word processing systems, Augment was targeted at the professional, not clerical workers,\npeople who were \"eager to explore the open-ended possibilities\" of the interactive process.\nThe main complaints about Augment were its price and that it was not easy to use. Accessing Engelbart's NLS system over ARPANET cost an eye-watering $48,000 a year (over $300,000 a year in current dollars).\nTymshare's Augment service was cheaper (about $80 an hour in current dollars), but still much more expensive than a standard word processing\nservice.\nOverall, the article found that Augment users were delighted with the system: \"It is stimulating to belong to the electronic intelligentsia.\"\nUsers found it to be \"a way of life—an absorbing, enriching experience\".↩\n\nWilliam Safire provided background in the New York Times, explaining\nthat \"the mother of all battles\"\noriginally referred to the battle of Qadisiya in A.D. 636, and Saddam Hussein was referencing that ancient battle.\nA translator responded, however,\nthat the Arabic expression would be better translated as \"the great battle\" than \"the mother of all battles.\"↩\n\nThe end of the Gulf War left Saddam Hussein in control of Iraq and left thousands of US troops in Saudi Arabia.\nThese factors would turn out to be catastrophic in the following years.↩\n\nAt the Mobile '92 conference, Apple's CEO, John Sculley, said personal communicators could be \"the mother of all markets,\"\nwhile Andy Grove of Intel said that the idea of a wireless personal communicator in every pocket is \"a pipe dream driven by greed\"\n(link).\nIn hindsight, Sculley was completely right and Grove was completely wrong.↩\n\nSome references to Intel's \"Mother of all demos\" are\nComputer Industry Gathers Amid Chaos, New York Times, Oct 21, 1991\nand \"Intel's High-Tech Vision of the Future: Chipmaker proposes using computers to dramatically improve productivity\", San Francisco Chronicle, Oct 21, 1991, p24.\nThe title of an article in Microprocessor Report, \"Intel Declares Victory in the Mother of All Demos\" (Nov. 20, 1991), alluded to the recently-ended war.\nFortune wrote about Intel's demo in the  Feb 17, 1997 issue.\nA longer description of Intel's demo is in the book Strategy is Destiny.↩\n\nSeveral sources claim that Andy van Dam was the first to call Engelbart's demo \"The Mother of All Demos.\" Although van Dam attended the 1968 demo,\nI couldn't find any evidence that he coined the phrase.\nJohn Markoff, a technology journalist for The New York Times, wrote a book What the Dormouse Said: How the Sixties Counterculture Shaped the Personal Computer Industry.\nIn this book, Markoff wrote about Engelbart's demo, saying \"Years later, his talk remained 'the mother of all demos' in the words of Andries van Dam, a Brown University computer scientist.\"\nAs far as I can tell, van Dam used the phrase but only after it had already been popularized by Levy.↩\n\nIt's curious to write that the demonstration was still called the \"mother of all demos\" when the phrase was just a few years old.↩\n\nThe photo below shows a keyset from the Xerox Alto.\nThe five keys are labeled with separate functions—Copy, Undelete, Move, Draw, and Fine—\nfor use with ALE,\na program for IC design.\nALE supported\nkeyset chording\nin combination with the mouse.\n↩Keyset from a Xerox Alto, courtesy of Digibarn.\n\nAfter I implemented this interface, I came across a project that constructed a 3D-printed chording keyset, also using a Teensy for the USB interface. You can find that project here.↩\n\nEmail ThisBlogThis!Share to XShare to FacebookShare to Pinterest\n\nLabels:\nalto,\nelectronics,\nreverse-engineering\n\n12 comments:\n\nMartin Haeberli\nsaid...\n\nKen - cool project ! I have a (few?) Xerox Optical Mice that I'd like to interface to USB as well - do you know of any off-the shelf solutions? Or maybe this could be adapted? Best, Martin\n\nMarch 23, 2025 at 8:54 AM\n\nKen Shirriff\nsaid...\n\nHi Martin! I don't know of any off-the-shelf solutions, but it would probably be easy to modify my interface. (Famous last words :)\n\nMarch 23, 2025 at 9:29 AM\n\nAnonymous\nsaid...\n\nCharles Moore, creator of the FORTH programming language, famously used a 5-button chording puck for programming in the 1970s.\n\nMarch 23, 2025 at 10:56 AM\n\n<img src=\"//blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglgI5BrNqTAwhKW-P2yd3IezmYF6xIss6PS08E5EYw5JA1g37XT-ua8QrbPkhen-laj2FYOc6-DP4U5H3YyboN9QiKE2XHZuILzMAxl9-3DWkxm3I78bme2aCCItOyU-w/s45-c/Still+of+Teaching+video+headshot.png\" width=\"35\" height=\"35\" class=\"photo\" alt=\"\">\nGeoff Fellows\nsaid...\n\nJust a minor correction 1K ohm is 1024 ohms. Please use S.I. prefixes in this case lowercase k for x1000. See https://www.nist.gov/pml/owm/metric-si-prefixes\n\nMarch 23, 2025 at 1:27 PM\n\nPouya Kary\nsaid...\n\nHi! Your work is amazing! Recently I tried to create the keyset with a midi controller and so since the community of those who care about keyset is so small, I wished to show it to you. https://github.com/pouyakary/nls-keyset-with-midi-keys\n\nMarch 24, 2025 at 12:19 AM\n\nPouya Kary\nsaid...\n\nWow I didn't know herman miller was involved, how cool\n\nMarch 24, 2025 at 12:21 AM\n\nAnonymous\nsaid...\n\nThe existence of Morse code should have told Engelbart that using straight ASCII encoding might not be the optimal solution.Also, why not embrace the fact that you can't press (and release) keys at the same time? You get 24 different codes that way, pressing and releasing 2 of 4 keys in some kind of order. Add single-key presses for 28. Add the thumb key (why only one?) and you've got more possible codes than you'll be able to remember.\n\nMarch 24, 2025 at 2:09 AM\n\nAnonymous\nsaid...\n\nUppercase K would be written with a line on top of it, so it was indeed lowercase k. ;)Also, isn't it really \"Ki\" to indicate 1024, with K representing Kelvin?\n\nMarch 24, 2025 at 2:43 AM\n\nAnonymous\nsaid...\n\nI wonder if you could have gone with mouse scroll wheel for upper/lower case?Have a \"natural gravity\" to middle and when you push up, you get all uppercase, and push back down and you get all lowercase. A quick, short scroll up only uppercases the next letter.Not sure of the best way to provide visual feedback (status tray icon? just look at the letters coming out), but I think it can be tuned to work very nicely.\n\nMarch 24, 2025 at 2:47 AM\n\nAnonymous\nsaid...\n\nI know that the Robot in a Can team has a clean esp and laser cut wood keyset build. I don'tbknow the link to the files and they're currently out of stock, but I'll reach out and see if they can restock and republish the open source design.\n\nMarch 24, 2025 at 5:13 AM\n\nAnonymous\nsaid...\n\nOne place this interface did come into use is on braille devices.\n\nMarch 24, 2025 at 6:43 AM\n\nZack\nsaid...\n\nWhile using key chords to type letters may not be efficient, using key chords to type full words can enable up to 300wpm with stenotype machines [1]. The first commercially available version was sold by the United States Stenograph Corp in 1884, so they'd been around for some time [2].This continues today with the OpenSteno Project, which is compatible with normal keyboards! (and also stenotype machines) [3][1] https://en.wikipedia.org/wiki/Stenotype[2] http://www.stenograph.com/history-writers[3] http://www.openstenoproject.org/\n\nMarch 24, 2025 at 8:20 AM\n\nPost a Comment\n\nOlder Post\n\nHome\n\n      @import url('https://fonts.googleapis.com/css?family=Montserrat:300,400,500,700');\n      .form-preview {\n      display: flex;\n      flex-direction: column;\n      justify-content: center;\n      margin-top: 30px;\n      padding: clamp(17px, 5%, 40px) clamp(17px, 7%, 50px);\n      max-width: 350px;\n      min-height: 200px;\n      border-radius: 6px;\n      box-shadow: 0 5px 25px rgba(34, 60, 47, 0.25);\n      }\n      .form-preview,\n      .form-preview *{\n        box-sizing: border-box;\n      }\n      .form-preview .preview-heading {\n      width: 100%;\n      }\n      .form-preview .preview-heading h5{\n        margin-top: 0;\n        margin-bottom: 0;\n      }\n      .form-preview .preview-input-field {\n      margin-top: 20px;\n      width: 100%;\n      }\n      .form-preview .preview-input-field input {\n      width: 100%;\n      height: 40px;\n      border-radius: 6px;\n      border: 2px solid #e9e8e8;\n      background-color: #fff;\n      outline: none;\n      }\n      .form-preview .preview-input-field input::placeholder, .form-preview .preview-input-field input {\n      opacity: 0.5;\n      color: #000;\n      font-family: \"Montserrat\";\n      font-size: 14px;\n      font-weight: 500;\n      line-height: 20px;\n      text-align: center;\n      }\n      .form-preview .preview-submit-button {\n      margin-top: 10px;\n      width: 100%;\n      }\n      .form-preview .preview-submit-button button {\n      width: 100%;\n      height: 40px;\n      border: 0;\n      border-radius: 6px;\n      line-height: 0px;\n      }\n      .form-preview .preview-submit-button button:hover {\n      cursor: pointer;\n      }\n\n      Get new posts by email:  Subscribe\n\nAbout the site\n\nContact info and site index\n\nPopular Posts\n\nThe Pentium contains a complicated circuit to multiply by three\n\nA USB interface to the \"Mother of All Demos\" keyset\n\nInside a vintage aerospace navigation computer of uncertain purpose\n\nA Multi-Protocol Infrared Remote Library for the Arduino\n\nApple iPhone charger teardown: quality in a tiny expensive package\n\nA dozen USB chargers in the lab: Apple is very good, but not quite the best\n\nMining Bitcoin with pencil and paper: 0.67 hashes per day\n\nTeardown and exploration of Apple's Magsafe connector\n\nSearch This Blog\n\nLabels\n\n386\n\n6502\n\n8008\n\n8085\n\n8086\n\n8087\n\n8088\n\naerospace\n\nalto\n\nanalog\n\nApollo\n\napple\n\narc\n\narduino\n\narm\n\nbeaglebone\n\nbitcoin\n\nc#\n\ncadc\n\ncalculator\n\nchips\n\ncss\n\ndatapoint\n\ndx7\n\nelectronics\n\nf#\n\nfairchild\n\nfpga\n\nfractals\n\ngenome\n\nglobus\n\nhaskell\n\nHP\n\nhtml5\n\nibm\n\nibm1401\n\nibm360\n\nintel\n\nipv6\n\nir\n\njava\n\njavascript\n\nmath\n\nmicrocode\n\noscilloscope\n\nPentium\n\nphoto\n\npower supply\n\nrandom\n\nreverse-engineering\n\nsheevaplug\n\nsnark\n\nspace\n\nspanish\n\nsynth\n\nteardown\n\ntheory\n\nunicode\n\nZ-80\n\nBlog Archive\n\n        ▼\n\n2025\n\n(7)\n\n        ▼\n\nMarch\n\n(2)\n\nA USB interface to the \"Mother of All Demos\" keyset\nThe Pentium contains a complicated circuit to mult...\n\n        ►\n\nFebruary\n\n(1)\n\n        ►\n\nJanuary\n\n(4)\n\n        ►\n\n2024\n\n(21)\n\n        ►\n\nDecember\n\n(1)\n\n        ►\n\nNovember\n\n(1)\n\n        ►\n\nOctober\n\n(1)\n\n        ►\n\nSeptember\n\n(3)\n\n        ►\n\nAugust\n\n(2)\n\n        ►\n\nJuly\n\n(2)\n\n        ►\n\nJune\n\n(1)\n\n        ►\n\nMay\n\n(1)\n\n        ►\n\nApril\n\n(1)\n\n        ►\n\nMarch\n\n(2)\n\n        ►\n\nFebruary\n\n(3)\n\n        ►\n\nJanuary\n\n(3)\n\n        ►\n\n2023\n\n(35)\n\n        ►\n\nDecember\n\n(4)\n\n        ►\n\nNovember\n\n(2)\n\n        ►\n\nOctober\n\n(3)\n\n        ►\n\nSeptember\n\n(1)\n\n        ►\n\nAugust\n\n(2)\n\n        ►\n\nJuly\n\n(3)\n\n        ►\n\nMay\n\n(1)\n\n        ►\n\nApril\n\n(2)\n\n        ►\n\nMarch\n\n(4)\n\n        ►\n\nFebruary\n\n(5)\n\n        ►\n\nJanuary\n\n(8)\n\n        ►\n\n2022\n\n(18)\n\n        ►\n\nNovember\n\n(3)\n\n        ►\n\nAugust\n\n(1)\n\n        ►\n\nJuly\n\n(1)\n\n        ►\n\nJune\n\n(1)\n\n        ►\n\nMay\n\n(1)\n\n        ►\n\nApril\n\n(4)\n\n        ►\n\nMarch\n\n(2)\n\n        ►\n\nFebruary\n\n(3)\n\n        ►\n\nJanuary\n\n(2)\n\n        ►\n\n2021\n\n(26)\n\n        ►\n\nDecember\n\n(4)\n\n        ►\n\nNovember\n\n(2)\n\n        ►\n\nSeptember\n\n(1)\n\n        ►\n\nAugust\n\n(1)\n\n        ►\n\nJuly\n\n(2)\n\n        ►\n\nJune\n\n(2)\n\n        ►\n\nMay\n\n(1)\n\n        ►\n\nApril\n\n(2)\n\n        ►\n\nMarch\n\n(4)\n\n        ►\n\nFebruary\n\n(4)\n\n        ►\n\nJanuary\n\n(3)\n\n        ►\n\n2020\n\n(33)\n\n        ►\n\nDecember\n\n(2)\n\n        ►\n\nNovember\n\n(3)\n\n        ►\n\nOctober\n\n(2)\n\n        ►\n\nSeptember\n\n(4)\n\n        ►\n\nAugust\n\n(5)\n\n        ►\n\nJuly\n\n(2)\n\n        ►\n\nJune\n\n(3)\n\n        ►\n\nMay\n\n(4)\n\n        ►\n\nApril\n\n(2)\n\n        ►\n\nMarch\n\n(5)\n\n        ►\n\nJanuary\n\n(1)\n\n        ►\n\n2019\n\n(18)\n\n        ►\n\nNovember\n\n(3)\n\n        ►\n\nOctober\n\n(2)\n\n        ►\n\nSeptember\n\n(3)\n\n        ►\n\nAugust\n\n(1)\n\n        ►\n\nJuly\n\n(4)\n\n        ►\n\nApril\n\n(2)\n\n        ►\n\nFebruary\n\n(1)\n\n        ►\n\nJanuary\n\n(2)\n\n        ►\n\n2018\n\n(17)\n\n        ►\n\nDecember\n\n(1)\n\n        ►\n\nSeptember\n\n(4)\n\n        ►\n\nAugust\n\n(1)\n\n        ►\n\nJune\n\n(1)\n\n        ►\n\nMay\n\n(1)\n\n        ►\n\nApril\n\n(1)\n\n        ►\n\nMarch\n\n(3)\n\n        ►\n\nFebruary\n\n(1)\n\n        ►\n\nJanuary\n\n(4)\n\n        ►\n\n2017\n\n(21)\n\n        ►\n\nDecember\n\n(5)\n\n        ►\n\nNovember\n\n(2)\n\n        ►\n\nOctober\n\n(3)\n\n        ►\n\nAugust\n\n(1)\n\n        ►\n\nJuly\n\n(2)\n\n        ►\n\nJune\n\n(2)\n\n        ►\n\nApril\n\n(2)\n\n        ►\n\nMarch\n\n(2)\n\n        ►\n\nFebruary\n\n(1)\n\n        ►\n\nJanuary\n\n(1)\n\n        ►\n\n2016\n\n(34)\n\n        ►\n\nDecember\n\n(2)\n\n        ►\n\nOctober\n\n(5)\n\n        ►\n\nSeptember\n\n(8)\n\n        ►\n\nAugust\n\n(2)\n\n        ►\n\nJuly\n\n(3)\n\n        ►\n\nJune\n\n(4)\n\n        ►\n\nMay\n\n(1)\n\n        ►\n\nApril\n\n(1)\n\n        ►\n\nMarch\n\n(1)\n\n        ►\n\nFebruary\n\n(4)\n\n        ►\n\nJanuary\n\n(3)\n\n        ►\n\n2015\n\n(12)\n\n        ►\n\nDecember\n\n(2)\n\n        ►\n\nNovember\n\n(1)\n\n        ►\n\nOctober\n\n(3)\n\n        ►\n\nAugust\n\n(1)\n\n        ►\n\nMay\n\n(2)\n\n        ►\n\nMarch\n\n(2)\n\n        ►\n\nFebruary\n\n(1)\n\n        ►\n\n2014\n\n(13)\n\n        ►\n\nDecember\n\n(1)\n\n        ►\n\nOctober\n\n(1)\n\n        ►\n\nSeptember\n\n(3)\n\n        ►\n\nMay\n\n(2)\n\n        ►\n\nMarch\n\n(1)\n\n        ►\n\nFebruary\n\n(5)\n\n        ►\n\n2013\n\n(24)\n\n        ►\n\nNovember\n\n(2)\n\n        ►\n\nSeptember\n\n(4)\n\n        ►\n\nAugust\n\n(4)\n\n        ►\n\nJuly\n\n(4)\n\n        ►\n\nJune\n\n(2)\n\n        ►\n\nApril\n\n(1)\n\n        ►\n\nMarch\n\n(2)\n\n        ►\n\nFebruary\n\n(2)\n\n        ►\n\nJanuary\n\n(3)\n\n        ►\n\n2012\n\n(10)\n\n        ►\n\nDecember\n\n(1)\n\n        ►\n\nNovember\n\n(5)\n\n        ►\n\nOctober\n\n(1)\n\n        ►\n\nMay\n\n(1)\n\n        ►\n\nMarch\n\n(1)\n\n        ►\n\nFebruary\n\n(1)\n\n        ►\n\n2011\n\n(11)\n\n        ►\n\nDecember\n\n(2)\n\n        ►\n\nJuly\n\n(2)\n\n        ►\n\nMay\n\n(2)\n\n        ►\n\nApril\n\n(1)\n\n        ►\n\nMarch\n\n(1)\n\n        ►\n\nFebruary\n\n(3)\n\n        ►\n\n2010\n\n(22)\n\n        ►\n\nDecember\n\n(2)\n\n        ►\n\nNovember\n\n(4)\n\n        ►\n\nOctober\n\n(3)\n\n        ►\n\nAugust\n\n(1)\n\n        ►\n\nJune\n\n(1)\n\n        ►\n\nMay\n\n(2)\n\n        ►\n\nApril\n\n(3)\n\n        ►\n\nMarch\n\n(4)\n\n        ►\n\nJanuary\n\n(2)\n\n        ►\n\n2009\n\n(22)\n\n        ►\n\nDecember\n\n(2)\n\n        ►\n\nNovember\n\n(5)\n\n        ►\n\nSeptember\n\n(1)\n\n        ►\n\nAugust\n\n(3)\n\n        ►\n\nJuly\n\n(1)\n\n        ►\n\nJune\n\n(3)\n\n        ►\n\nApril\n\n(1)\n\n        ►\n\nMarch\n\n(3)\n\n        ►\n\nFebruary\n\n(2)\n\n        ►\n\nJanuary\n\n(1)\n\n        ►\n\n2008\n\n(27)\n\n        ►\n\nJuly\n\n(3)\n\n        ►\n\nJune\n\n(1)\n\n        ►\n\nMay\n\n(3)\n\n        ►\n\nApril\n\n(4)\n\n        ►\n\nMarch\n\n(10)\n\n        ►\n\nFebruary\n\n(6)\n\nPowered by Blogger.",
    "summary": {
      "en": "**Summary of Ken Shirriff's Blog Post**\n\nKen Shirriff discusses the history and significance of Douglas Engelbart's groundbreaking 1968 demonstration, known as \"The Mother of All Demos,\" where he introduced many features we now use in computing, such as the mouse, hypertext, and graphical user interfaces. Engelbart aimed to enhance human intelligence through technology.\n\nThe blog post focuses on Shirriff's project of creating a USB interface for Engelbart's lesser-known input device, the keyset, which allows typing by pressing multiple keys simultaneously. Engelbart's innovations were heavily influenced by Vannevar Bush's 1945 essay envisioning advanced information systems.\n\nShirriff also explains how the phrase \"The Mother of All Demos\" became associated with Engelbart's presentation. Initially, it was coined by Intel for a different demo in 1991, which was later popularized in relation to Engelbart's work by writers like Steven Levy.\n\nShirriff details the technical aspects of interfacing the keyset with modern computers, including the challenges he faced with the keyset's design and functionality. Despite Engelbart's belief that the keyset was easy to learn, Shirriff found it difficult to use.\n\nOverall, the blog highlights the importance of Engelbart's contributions to computing while showcasing Shirriff's efforts to connect historical technology with today's devices.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "85899ffe934cf308",
    "title": {
      "en": "The SeL4 Microkernel: An Introduction [pdf]",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://sel4.systems/About/seL4-whitepaper.pdf",
    "score": 259,
    "by": "snvzz",
    "time": 1742728168,
    "content": "The seL4 ®   Foundation https://sel4.systems/Foundation  The seL4 Microkernel An Introduction  Gernot Heiser  gernot@sel4.systems Revision 1.4 of 2025-01-08\n\nAbstract  This whitepaper provides an introduction to and overview of seL4. We explain what seL4 is (and is not) and explore its defining features. We explain what makes seL4 uniquely qualified as the operating-system kernel of choice for security- and safety-critical systems, and generally embedded and cyber-physical systems. In particular, we explain seL4’s assurance story, its security- and safety-relevant features, and its benchmark-setting performance. We also discuss typical usage scenarios, including incremental cyber retrofit of legacy systems.  CCS Concepts  •   Software and its engineering   →   Operating Systems  •   Security and privacy   →   Systems security  •   Security and privacy   →   Formal methods and theory of security  •   Computer systems organization   →   Real-time systems   →   Real-time operating systems  •   Computer systems organization   →   Real-time systems   →   Dependable and fault-tolerant systems and networks  Keywords  seL4, microkernel, performance  Reference Format:  Gernot Heiser. The seL4 Microkernel – An Introduction. White paper. The seL4 Foundation, Revision 1.4 of 2025-01-08.  Copyright   ©   2020 seL4 Project a Series of LF Projects, LLC. Distributed under the Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) License. seL4 is a trademark of LF Projects, LLC.\n\nContents  List of Figures   2 1   What Is seL4?   3 2   seL4 Is a Microkernel and a Hypervisor, It Is Not an OS   5  2.1   Monolithic kernels vs microkernels   . . . . . . . . . . . . . . . . . . . . .   5 2.2   seL4 Is a microkernel, not an OS . . . . . . . . . . . . . . . . . . . . . . .   7 2.3   seL4 is also a hypervisor   . . . . . . . . . . . . . . . . . . . . . . . . . . .   8 2.4   seL4 is not seLinux   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   10  3   seL4’s Verification Story   11  3.1   Correctness and security enforcement   . . . . . . . . . . . . . . . . . . .   12 3.2   The seL4 Microkit   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   15  4   About Capabilities   19  4.1   What are capabilities? . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   19 4.2   Why Capabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   20  5   Support for Hard Real-Time Systems   24  5.1   General real-time support . . . . . . . . . . . . . . . . . . . . . . . . . . .   24 5.2   Mixed-criticality systems   . . . . . . . . . . . . . . . . . . . . . . . . . . .   25  6   Security is No Excuse for Poor Performance   29 7   Real-World Deployment and Incremental Cyber Retrofit   30  7.1   General considerations   . . . . . . . . . . . . . . . . . . . . . . . . . . . .   30 7.2   Retrofitting existing systems   . . . . . . . . . . . . . . . . . . . . . . . . .   30  8   Conclusions   32 Bibliography   34  1\n\nList of Figures  2.1   Operating-system structure.   . . . . . . . . . . . . . . . . . . . . . . . . .   5 2.2   L4 microkernel family tree.   . . . . . . . . . . . . . . . . . . . . . . . . . .   7 2.3   VM-provided services   . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   9 2.4   seL4 virtualisation support with usermode VMMs. . . . . . . . . . . . . .   9 3.1   seL4’s proof chain. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   11 3.2   Translation validation proof chain. . . . . . . . . . . . . . . . . . . . . . .   13 3.3   Microkit abstractions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   15 3.4   Verified architecture mapping and system generation.   . . . . . . . . . .   17 4.1   Capabilities are keys to objects . . . . . . . . . . . . . . . . . . . . . . . .   19 4.2   The compiler as a confused deputy. . . . . . . . . . . . . . . . . . . . . .   22 5.1   Simplified example of a mixed-criticality system. . . . . . . . . . . . . . .   26 7.1   Incremental cyber-retrofit on the ULB. . . . . . . . . . . . . . . . . . . . .   31 2\n\nChapter 1 What Is seL4?  seL4 is an operating system microkernel  An operating system (OS) is the low-level system software that controls a computer system’s resources and enforces security. Unlike application software, the OS has exclusive access to a more privileged execution mode of the processor ( kernel mode ) that gives it direct access to hardware. Applications only ever execute in   user mode   and can only access hardware as permitted by the OS. An OS microkernel is a minimal core of an OS, reducing the code executing at higher privilege to a minimum. seL4 is a member of the L4 family of microkernels that goes back to the mid-1990s. (And no,   seL4 has nothing to do with seLinux. )  seL4 is also a hypervisor  seL4 supports virtual machines that can run a fully fledged guest OS such as Linux. Subject to seL4’s enforcement of communication channels, guests and their applications can communicate with each other as well as with native applications. Learn more about what it means that seL4 is a microkernel and its use as a hypervisor in Chapter 2. And learn about real-world deployment scenarios, including approaches for retrofitting security into legacy systems in Chapter 7.  seL4 is proved correct  seL4 comes with a formal, mathematical, machine-checked   proof of implementation correctness , meaning the kernel is in a very strong sense “bug free” with respect to its specification. In fact, seL4 is the world’s first OS kernel with such a proof at the code level [Klein et al., 2009].  seL4 is provably secure  Besides implementation correctness, seL4 comes with further proofs of   security enforcement   [Klein et al., 2014]. They say that in a correctly configured seL4-based system, the kernel guarantees the classical security properties of  confidentiality, integrity and availability . More about these proofs in Chapter 3.  seL4 improves security with fine-grained access control through capabilities  Capabilities are access tokens which support very fine-grained control over which entity can access a particular resource in a system. They support strong 3\n\nsecurity according to the principle of least privilege (also called principle of least authority, POLA). This is a core design principle of highly secure system, and is impossible to achieve with the way access control happens in mainstream systems such as Linux or Windows. seL4 is still the   world’s only OS that is both capability-based and formally verified , and as such has a defensible claim of being the world’s most secure OS. More about capabilities in Chapter 4.  seL4 ensures safety of time-critical systems  seL4 is the world’s only OS kernel (at least in the open literature) that has undergone a complete and sound analysis of its   worst-case execution time  (WCET) [Blackham et al., 2011, Sewell et al., 2017]. This means, if the kernel is configured appropriately, all kernel operations are bounded in time, and the bound is known. This is a prerequisite for building   hard real-time systems , where failure to react to an event within a strictly bounded time period is catastrophic.  seL4 is the world’s most advanced mixed-criticality OS  seL4 provides strong support for mixed criticality real-time systems (MCS), where the timeliness of critical activities must be ensured even if they co-exist with less trusted code executing on the same platform. seL4 achieves this with a flexible model that retains good resource utilisation, unlike the more established MCS OSes that use strict (and inflexible) time and space partitioning [Lyons et al., 2018]. More on seL4’s real-time and MCS support in Chapter 5.  seL4 is the world’s fastest microkernel  Traditionally, systems are either (sort-of) secure, or they are fast. seL4 is unique in that it is both. seL4 is designed to support a wide range of real-world use cases, whether they are security- (or safety-)critical or not, and excellent performance is a requirement. More on seL4’s performance in Chapter 6.  seL4 is pronounced “ess-e-ell-four”  The pronunciation “sell-four” is deprecated.  How to read this document  This document is meant to be approachable by a wide audience. However, for completeness, we will add some deeper technical detail in places.  Such detail will be marked with a chilli, like the one on the left. If you see this then you know you can safely skip the marked passage if you think the technical description is too “spicy” for your taste, or if you are simply not interested in this level of detail. Only other chillied passages will assume you have read it.  Technical section  Where the chilli appears in a section title, such as here, this indicates that the whole section is fairly technical and can be skipped. 4\n\nChapter 2 seL4 Is a Microkernel and a Hypervisor, It Is Not an OS  2.1   Monolithic kernels vs microkernels  To understand the difference between a mainstream OS, such as Linux, and a microkernel, such as seL4, let’s look at Figure 2.1. 10   kSLOC  Hardware  VFS  IPC, File System  Scheduler, Virtual Memory  Device Drivers, Dispatcher  Syscall  Hardware  IPC, Threads, Virtual Memory  Application  NW  Proto -  col  Stack  File  Server  Device  Driver  IPC  Kernel  Mode  User  Mode  20,000  kSLOC  Figure 2.1: Operating-system structure: Monolithic kernel (left) vs microkernel (right). The left side presents a (fairly abstracted) view of the architecture of a system such as Linux. The yellow part is the OS   kernel , it offers services such as file storage and networking to applications. All the code that implements those services executes in the   privileged mode   of the hardware, also called   kernel mode   or   supervisor mode   – the execution mode that has unfettered access and control of all resources in the system. In contrast, applications run in unprivileged, or   user mode , and do not have direct access to many hardware resources, which must be accessed through the OS. The OS is internally structured in a number of layers, where each layer provides abstractions implemented by layers below. The problem with privileged-mode code is that it is dangerous: If anything goes wrong here, there’s nothing to stop the damage. In particular, if this code has a bug that can be exploited by an attacker to run the attacker’s code in privileged mode (called a privilege-escalation or arbitrary code-execution attack) then the attacker can do what they want with the system. Such flaws are the root problem of the many system compromises we experience in mainstream systems. 5\n\nOf course, software bugs are mostly a fact of life, and OSes are not different. For example, the Linux kernel comprises of the order of 20 million lines of source code (20 MSLOC); we can estimate that it contains literally tens of thousands of bugs [Biggs et al., 2018]. This is obviously a huge attack surface! This idea is captured by saying that Linux has a large   trusted computing base   (TCB), which is defined as the subset of the overall system that must be trusted to operate correctly for the system to be secure. The idea behind a microkernel design is to drastically reduce the TCB and thus the attack surface. As schematically shown at the right of Figure 2.1, the kernel, i.e. the part of the system executing in privileged mode, is much smaller. In a well-designed microkernel, such as seL4, it is of the order of ten thousand lines of source code (10 kSLOC). This is literally three orders of magnitude smaller than the Linux kernel, and the attack surface shrinks accordingly (maybe more, as the density of bugs probably grows more than linearly with code size). Obviously, it is not possible to provide the same functionality, in terms of OS services, in such a small code base. In fact, the microkernel provides almost no services: it is just a thin wrapper around hardware, just enough to securely multiplex hardware resources. What the microkernel mostly provides is isolation, sandboxes in which programs can execute without interference from other programs. And, critically, it provides a   protected procedure call   (PPC) mechanism, which is a form of inter-process communication (IPC). For historical reasons the term IPC lives on, but I recommend avoiding it as it leads to misconceptions that result in poor designs.  For a deeper explanation of what seL4 IPC is and is not, I recommend reading my blog How to (and how not to) use seL4 IPC. The PPC mechanism allows one program to securely call a function in a different program, where the microkernel transports function inputs and outputs between the programs and, importantly, enforces interfaces: the “remote” (contained in a different sandbox) function can only be called at an exported entrypoint, and only by explicitly authorised clients (who have been given the appropriate capability, see Chapter 4). The microkernel system uses this approach to provide the services the monolithic OS implements in the kernel. In the microkernel world, these services are just programs, no different from applications, that run in their own sandboxes, and provide a PPC interface for applications to call. Should a server be compromised, that compromise is confined to the server, its sandbox protects the rest of the system. This is in stark contrast to the monolithic case, where a compromise of an OS service compromises the complete system. This effect can be quantified: Our recent study shows that of the known Linux compromises classified as   critical , i.e. most severe, 29% would be fully eliminated by a microkernel design, and another 55% would be mitigated enough to no longer qualify as critical [Biggs et al., 2018]. 6\n\n93   94   95   96   97   98   99   00   01   02   03   04   05   06   07   08   09   10   11   12   13 L3   →   L4   “X”   Hazelnut   Pistachio L4/Alpha L4/MIPS seL4 OKL4 Microkernel OKL4 Microvisor Codezero P4   →   PikeOS Fiasco   Fiasco.OC L4-embedded NOVA API Inheritance Code Inheritance GMD/IBM/TU Karlsruhe   UNSW/NICTA/Data61 TU Dresden   Commercial Clone OK Labs Creators: Figure 2.2: L4 microkernel family tree.  2.2   seL4 Is a microkernel, not an OS  seL4 is a microkernel, and designed for generality while minimising the TCB. It is a member of the L4 microkernel family, which goes back to the mid-’90s; Figure 2.2 shows seL4’s provenance. It was developed by our group at UNSW/NICTA, these days known as Trustworthy Systems (TS). At the time we had 15 years of experience in developing high-performance microkernels, and a track-record of real-world deployments: Our   OKL4 Microkernel   shipped on billions of Qualcomm cellular modem chips, and our   L4-embedded   kernel from the mid-Noughties runs on the secure enclave of all recent iOS devices (iPhones etc). Being a microkernel, seL4 contains none of the usual OS services; such services are provided by programs running in user mode. Besides the great advantages elaborated above, there are downsides to the microkernel design: These components must come from somewhere. Some can be ported from open-source OSes, such as FreeBSD or Linux, or they can be written from scratch. But in any case, this is significant work. To scale up we need the help of the community, and the seL4 Foundation is the key mechanism for enabling the community to cooperate and develop or port such services for seL4-based systems. The most important ones are device drivers, network protocol stacks, and file systems. We have a fair number of these, but much more is needed. Even compared to other microkernels, seL4’s API is very low-level, with only the minimum abstraction as required to securely manage the hardware. As such, building systems on seL4 is particularly difficult. A good way to look at this is to think of seL4 as the “assembly language of operating systems”: very primitive. No-one in their right mind would write a complex system, say a database or web server, in assembly language, we use higher-level programming languages for that. These simplify the task by providing higher-level constructs and abstractions, at the 7\n\nexpense of losing some of the power the hardware provides. In almost all cases, that is a winning trade-off. Similarly, one should not try to build a complex system directly on seL4, but should use a higher-level framework that provides more appropriate abstractions, at the expense of introducing some policy and taming the power of seL4. Specifically, such a framework should allows developers to focus on the code that implements the services, ignore hardware complexities, and automate much of the system integration. There are presently three main component frameworks for seL4, all open source: The Microkit, CAmkES and Genode. The seL4 Microkit reduces the complex seL4 API to literally a handful of simple abstractions, designed around components called   protection domains . It also provides a   software development kit   (SDK) that makes it easy to build and integrate separately compiled modules with a kernel binary to produce a bootable image. The Microkit’s simplicity is achieved by requiring the system architecture to be static, meaning the set of modules and their communications is defined at system configuration time – a model that seems to match the requirements of most (if not all) embedded systems, including complex cyberphysical systems such as cars and aircraft. CAmkES, a predecessor of the Microkit, is also a component framework for a statically-architected system. Its abstractions are higher level yet more complex than those of the Microkit. It also lacks an SDK, resulting in a much more painful build process that forces use of the kernel’s complex build system. CAmkES also introduces significant overheads. Genode is in many ways a more powerful and general framework, that supports multiple microkernels and already comes with a wealth of services and device drivers, especially for x86 platforms, and does not enforce a static system architecture. It is arguably more convenient to work with than CAmkES, and is certainly the way to get a complex system up quickly. However, Genode has drawbacks: 1. As it supports multiple microkernels, not all as powerful as seL4, Genode is based on the least common denominator. In particular, it cannot use all of seL4’s security and safety features. 2. It has no assurance story. More on this in Section 3.2. For these reasons, the Microkit is the recommended framework for building systems on seL4, at least as long as the static architecture works. Section 3.2 presents it in more detail.  2.3   seL4 is also a hypervisor  seL4 is a microkernel, but it is also a hypervisor: It is possible to run virtual machines on seL4, and inside the virtual machine (VM) a mainstream OS, such as Linux. This enables an alternative way of provisioning system services, by having a Linux VM provide them. Such a setup is shown in Figure 2.3, which shows how some services are borrowed from multiple Linux instances running as guest OSes in separate VMs. In this example, we provide two system services: networking and storage. Networking is provided by a native protocol stack running directly on seL4, lwIP or PicoTCP are 8",
    "summary": {
      "en": "The seL4 Foundation has released a whitepaper introducing seL4, a microkernel designed for security- and safety-critical systems. Here are the key points:\n\n1. **What is seL4?**\n   - seL4 is a microkernel, not a traditional operating system. It is part of the L4 family of microkernels and is known for its minimal codebase, which reduces the risk of security vulnerabilities.\n\n2. **Unique Features:**\n   - **Proven Correctness:** seL4 has a formal proof of its correctness, meaning it is highly reliable and \"bug-free\" according to its specifications.\n   - **Security:** It includes proofs for security enforcement, ensuring confidentiality, integrity, and availability when properly configured.\n   - **Capabilities:** seL4 uses a capability-based access control system, allowing precise permissions for accessing resources, which enhances security.\n   - **Real-Time Support:** It has undergone analysis for worst-case execution time, making it suitable for hard real-time systems.\n   - **Mixed-Criticality Systems:** seL4 supports systems where critical tasks run alongside less trusted applications without compromising performance.\n\n3. **Performance:** \n   - seL4 is designed to be both secure and fast, making it suitable for a variety of applications, including embedded systems.\n\n4. **Usage Scenarios:** \n   - The whitepaper discusses typical use cases, including retrofitting security into existing legacy systems.\n\n5. **Development Frameworks:**\n   - While seL4 itself is low-level, several frameworks (like the Microkit, CAmkES, and Genode) exist to help developers build systems on top of it, simplifying the process.\n\nOverall, seL4 stands out for its robustness, security features, and performance, making it an excellent choice for critical applications.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "e3fa46eac06efa4b",
    "title": {
      "en": "Improving recommendation systems and search in the age of LLMs",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://eugeneyan.com/writing/recsys-llm/",
    "score": 401,
    "by": "7d7n",
    "time": 1742701205,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "b69ecdb1b76a6e20",
    "title": {
      "en": "Mathematicians uncover the logic behind how people walk in crowds",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://news.mit.edu/2025/mathematicians-uncover-logic-behind-how-crowds-walk-0324",
    "score": 45,
    "by": "geox",
    "time": 1742849243,
    "content": "The findings could help planners design safer, more efficient pedestrian thoroughfares.\n\n        Jennifer Chu\n  |\n  MIT News\n\n                    Publication Date:\n\n              March 24, 2025\n\nPress Inquiries\n\n                    Press Contact:\n\n            Abby\n\n            Abazorius\n\n            Email:\n              abbya@mit.edu\n\n            Phone:\n              617-253-2709\n\n            MIT News Office\n\n            Media Download\n\n          ↓ Download Image\n\n                    Caption:\n\n              The researchers carried out controlled crowd experiments and studied how real participants walked through a crowd to reach certain locations. They identified a key measure, “angular spread,” that determines whether pedestrian traffic is ordered or disordered.\n\n                    Credits:\n\n              Image: Courtesy of the researchers\n\n  *Terms of Use:\n\n    Images for download on the MIT News office website are made available to non-commercial entities, press and the general public under a\n    Creative Commons Attribution Non-Commercial No Derivatives license.\n    You may not alter the images provided, other than to crop them to size. A credit line must be used when reproducing images; if one is not provided\n    below, credit the images to \"MIT.\"\n\n Close\n\n                    Caption:\n\n              Mathematicians studied the flow of human crowds and developed a way to predict when pedestrian paths will transition from orderly to entangled.\n\n                    Credits:\n\n              Image: iStock\n\n                    Caption:\n\n              The researchers carried out controlled crowd experiments and studied how real participants walked through a crowd to reach certain locations. They identified a key measure, “angular spread,” that determines whether pedestrian traffic is ordered or disordered.\n\n                    Credits:\n\n              Image: Courtesy of the researchers\n\nPrevious image\n          Next image\n\n            Next time you cross a crowded plaza, crosswalk, or airport concourse, take note of the pedestrian flow. Are people walking in orderly lanes, single-file, to their respective destinations? Or is it a haphazard tangle of personal trajectories, as people dodge and weave through the crowd?MIT instructor Karol Bacik and his colleagues studied the flow of human crowds and developed a first-of-its-kind way to predict when pedestrian paths will transition from orderly to entangled. Their findings may help inform the design of public spaces that promote safe and efficient thoroughfares.In a paper appearing this week in the Proceedings of the National Academy of Sciences, the researchers consider a common scenario in which pedestrians navigate a busy crosswalk. The team analyzed the scenario through mathematical analysis and simulations, considering the many angles at which individuals may cross and the dodging maneuvers they may make as they attempt to reach their destinations while avoiding bumping into other pedestrians along the way.The researchers also carried out controlled crowd experiments and studied how real participants walked through a crowd to reach certain locations. Through their mathematical and experimental work, the team identified a key measure that determines whether pedestrian traffic is ordered, such that clear lanes form in the flow, or disordered, in which there are no discernible paths through the crowd. Called “angular spread,” this parameter describes the number of people walking in different directions.If a crowd has a relatively small angular spread, this means that most pedestrians walk in opposite directions and meet the oncoming traffic head-on, such as in a crosswalk. In this case, more orderly, lane-like traffic is likely. If, however, a crowd has a larger angular spread, such as in a concourse, it means there are many more directions that pedestrians can take to cross, with more chance for disorder.In fact, the researchers calculated the point at which a moving crowd can transition from order to disorder. That point, they found, was an angular spread of around 13 degrees, meaning that if pedestrians don’t walk straight across, but instead an average pedestrian veers off at an angle larger than 13 degrees, this can tip a crowd into disordered flow.\n\nImage: Courtesy of the researchers\n\n            “This all is very commonsense,” says Bacik, who is a instructor of applied mathematics at MIT. “The question is whether we can tackle it precisely and mathematically, and where the transition is. Now we have a way to quantify when to expect lanes — this spontaneous, organized, safe flow — versus disordered, less efficient, potentially more dangerous flow.”The study’s co-authors include Grzegorz Sobota and Bogdan Bacik of theAcademy of Physical Education in Katowice, Poland, and Tim Rogers at the University of Bath in the United Kingdom.Right, left, centerBacik, who is trained in fluid dynamics and granular flow, came to study pedestrian flow during 2021, when he and his collaborators looked into the impacts of social distancing, and ways in which people might walk among each other while maintaining safe distances. That work inspired them to look more generally into the dynamics of crowd flow.In 2023, he and his collaborators explored “lane formation,” a phenomenon by which particles, grains, and, yes, people have been observed to spontaneously form lanes, moving in single-file when forced to cross a region from two opposite directions. In that work, the team identified the mechanism by which such lanes form, which Bacik sums up as “an imbalance of turning left versus right.” Essentially, they found that as soon as something in a crowd starts to look like a lane, individuals around that fledgling lane either join up, or are forced to either side of it, walking parallel to the original lane, which others can follow. In this way, a crowd can spontaneously organize into regular, structured lanes.“Now we’re asking, how robust is this mechanism?” Bacik says. “Does it only work in this very idealized situation, or can lane formation tolerate some imperfections, such as some people not going perfectly straight, as they might do in a crowd?”Lane changeFor their new study, the team looked to identify a key transition in crowd flow: When do pedestrians switch from orderly, lane-like traffic, to less organized, messy flow? The researchers first probed the question mathematically, with an equation that is typically used to describe fluid flow, in terms of the average motion of many individual molecules.“If youthink about the whole crowd flowing, rather than individuals, you can use fluid-like descriptions,” Bacik explains. “It’s this art of averaging, where, even if some people may cross more assertively than others, these effects are likely to average out in a sufficiently large crowd. If you only care about the global characteristics like, are there lanes or not, then you can make predictions without detailed knowledge of everyone in the crowd.”Bacik and his colleagues used equations of fluid flow, and applied them to the scenario of pedestrians flowing across a crosswalk. The team tweaked certain parameters in the equation, such as the width of the fluid channel (in this case, the crosswalk), and the angle at which molecules (or people) flowed across, along with various directions that people can “dodge,” or move around each other to avoid colliding.Based on these calculations, the researchers found that pedestrians in a crosswalk are more likely to form lanes, when they walk relatively straight across, from opposite directions. This order largely holds until people start veering across at more extreme angles. Then, the equation predicts that the pedestrian flow is likely to be disordered, with few to no lanes forming.The researchers were curious to see whether the math bears out in reality. For this, they carried out experiments in a gymnasium, where they recorded the movements of pedestrians using an overhead camera. Each volunteer wore a paper hat, depicting a unique barcode that the overhead camera could track.In their experiments, the team assigned volunteers various start and end positions along opposite sides of a simulated crosswalk, and tasked them with simultaneously walking across the crosswalk to their target location without bumping into anyone. They repeated the experiment many times, each time having volunteers assume different start and end positions. In the end, the researchers were able to gather visual data of multiple crowd flows, with pedestrians taking many different crossing angles.When they analyzed the data and noted when lanes spontaneously formed, and when they did not, the team found that, much like the equation predicted, the angular spread mattered. Their experiments confirmed that the transition from ordered to disordered flow occurred somewhere around the theoretically predicted 13 degrees. That is, if an average person veered more than 13 degrees away from straight ahead, the pedestrian flow could tip into disorder, with little lane formation. What’s more, they found that the more disorder there is in a crowd, the less efficiently it moves.The team plans to test their predictions on real-world crowds and pedestrian thoroughfares.“We would like to analyze footage and compare that with our theory,” Bacik says. “And we can imagine that, for anyone designing a public space, if they want to have a safe and efficient pedestrian flow, our work could provide a simpler guideline, or some rules of thumb.”This work is supported, in part, by the Engineering and Physical Sciences Research Council of UK Research and Innovation.\n\n    Share this news article on:\n\n          X\n\n          Facebook\n\n        LinkedIn\n\n        Reddit\n\n        Print\n\n      Paper\n\nPaper: “Order-disorder transition in multidirectional crowds”\n\n      Related Links\n\n      Karol BacikDepartment of MathematicsSchool of Science\n\nRelated Topics\n\n    Cities\n    Fluid dynamics\n    Mathematics\n    Research\n    Urban studies and planning\n    School of Science\n\n  Related Articles\n\n        Street smarts\n\n        Where the sidewalk ends\n\n        How the brain navigates cities\n\n        Tool for predicting pedestrian flow expands its reach\n\nPrevious item\n    Next item",
    "summary": {
      "en": "Researchers at MIT have developed a method to understand and predict pedestrian crowd flow, focusing on how orderly or chaotic movement occurs in busy areas like crosswalks. They identified a key factor called \"angular spread,\" which measures the direction pedestrians are moving. If most people walk straight across, the flow is organized; if they veer off course more than 13 degrees, the crowd becomes disordered.\n\nThe study involved mathematical modeling and experiments where participants walked across a simulated crosswalk. The results confirmed that a smaller angular spread leads to orderly lanes, while a larger spread results in disorder. This research can help urban planners design safer and more efficient public spaces by understanding these movement patterns. The team plans to further test their findings in real-world settings.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "2852f55e961e65fa",
    "title": {
      "en": "Next.js version 15.2.3 has been released to address a security vulnerability",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://nextjs.org/blog/cve-2025-29927",
    "score": 237,
    "by": "makepanic",
    "time": 1742678347,
    "content": "Back to BlogSaturday, March 22nd 2025CVE-2025-29927Posted byLee Robinson@leerobNext.js version 15.2.3 has been released to address a security vulnerability (CVE-2025-29927). Additionally, backported patches are available.\nWe recommend that all self-hosted Next.js deployments using next start and output: 'standalone' should update immediately.\nContinue reading for more details on the CVE.\nTimeline\n\n2025-02-27T06:03Z: Disclosure to Next.js team via GitHub private vulnerability reporting\n2025-03-14T17:13Z: Next.js team started triaging the report\n2025-03-14T19:08Z: Patch pushed for Next.js 15.x\n2025-03-14T19:26Z: Patch pushed for Next.js 14.x\n2025-03-17T22:44Z: Next.js 14.2.25 released\n2025-03-18T00:23Z: Next.js 15.2.3 released\n2025-03-18T18:03Z: CVE-2025-29927 issued by GitHub\n2025-03-21T10:17Z: Security Advisory published\n2025-03-22T21:21Z: Next.js 13.5.9 released\n2025-03-23T06:44Z: Next.js 12.3.5 released\n\nVulnerability details\n\nNext.js uses an internal header x-middleware-subrequest to prevent recursive requests from triggering infinite loops. The security report showed it was possible to skip running Middleware, which could allow requests to skip critical checks—such as authorization cookie validation—before reaching routes.\nImpact scope\n\nAffected\n\nSelf-hosted Next.js applications using Middleware (next start with output: standalone)\nThis affects you if you rely on Middleware for auth or security checks, which are not then validated later in your application.\nApplications using Cloudflare can turn on a Managed WAF rule\n\nNot affected\n\nApplications hosted on Vercel\nApplications hosted on Netlify\nApplications deployed as static exports (Middleware not executed)\n\nPatched versions\n\nFor Next.js 15.x, this issue is fixed in 15.2.3\nFor Next.js 14.x, this issue is fixed in 14.2.25\nFor Next.js 13.x, this issue is fixed in 13.5.9\nFor Next.js 12.x, this issue is fixed in 12.3.5\n\nIf patching to a safe version is infeasible, it is recommended that you prevent external user requests which contain the x-middleware-subrequest header from reaching your Next.js application.\nOur security responsibility\n\nNext.js has published 16 security advisories since 2016. Over time, we've continued to improve how we gather, patch, and disclose vulnerabilities.\nGitHub Security Advisories and CVEs are industry-standard approaches to notifying users, vendors, and companies of vulnerabilities in software. While we have published a CVE, we missed the mark on partner communications.\nTo help us more proactively work with partners depending on Next.js, and other infrastructure providers, we are opening a partner mailing list. Please reach out to partners@nextjs.org to be included.",
    "summary": {
      "en": "Next.js has released version 15.2.3 to fix a security vulnerability (CVE-2025-29927). Users of self-hosted Next.js applications using 'next start' with 'standalone' output should update immediately. \n\n**Timeline of Events:**\n- Vulnerability reported on February 27, 2025.\n- Patches for Next.js 15.x and 14.x were released on March 14.\n- Version 15.2.3 and 14.2.25 were launched on March 18.\n- CVE-2025-29927 was issued on the same day, followed by a security advisory on March 21.\n\n**Vulnerability Details:**\nThe issue involves a header that could allow requests to bypass important checks, like authorization, which could affect security.\n\n**Who is Affected:**\n- Self-hosted Next.js applications using Middleware.\n- Applications relying on Middleware for authorization or security checks.\n\n**Who is Not Affected:**\n- Applications hosted on Vercel or Netlify.\n- Static exports where Middleware isn't executed.\n\n**Patch Information:**\n- Next.js versions 15.2.3, 14.2.25, 13.5.9, and 12.3.5 have patched this issue.\n\nIf updating is not possible, it is advised to block requests containing the specific header from reaching your application.\n\nNext.js has been proactive in addressing security issues and is establishing a mailing list for better communication with partners. Interested parties can contact partners@nextjs.org to join.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "b01e1938a4e4ad6a",
    "title": {
      "en": "Andromeda XXXV: The Faintest Dwarf Satellite of the Andromeda Galaxy",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://iopscience.iop.org/article/10.3847/2041-8213/adb433",
    "score": 42,
    "by": "PaulHoule",
    "time": 1742449604,
    "content": "The American Astronomical Society (AAS), established in 1899 and based in Washington, DC, is the major organization of professional astronomers in North America. Its membership of about 7,000 individuals also includes physicists, mathematicians, geologists, engineers, and others whose research and educational interests lie within the broad spectrum of subjects comprising contemporary astronomy. The mission of the AAS is to enhance and share humanity's scientific understanding of the universe.\n\n                The Institute of Physics (IOP) is a leading scientific society promoting physics and bringing physicists together for the benefit of all. It has a worldwide membership of around 50 000 comprising physicists from all sectors, as well as those with an interest in physics. It works to advance physics research, application and education; and engages with policy makers and the public to develop awareness and understanding of physics. Its publishing company, IOP Publishing, is a world leader in professional scientific communications.\n            A publishing partnershipThe following article is Open accessAndromeda XXXV: The Faintest Dwarf Satellite of the Andromeda GalaxyJose Marco Arias, Eric F. Bell, Katya Gozman, In Sung Jang, Saxon Stockton, Oleg Y. Gnedin, Richard D'Souza, Antonela Monachesi, Jeremy Bailin, David Nidever, and Roelof S. de JongShow full author listPublished 2025 March 11 •\n       \t  © 2025. The Author(s). Published by the American Astronomical Society.\n        The Astrophysical Journal Letters,\n\tVolume 982,\nNumber 1Citation Jose Marco Arias et al 2025 ApJL 982 L3DOI 10.3847/2041-8213/adb433 Download Article PDFDownloadArticle ePub\n            You need an eReader or compatible software to experience the benefits of the ePub3 file format.Authors\n                                    Jose Marco AriasAFFILIATIONSDepartment of Astronomy, University of Michigan, 1085 S. University Ave., Ann Arbor, MI 48109-1107, USA; joarias@umich.eduEMAILjoarias@umich.eduhttps://orcid.org/0009-0002-9085-5928Eric F. BellAFFILIATIONSDepartment of Astronomy, University of Michigan, 1085 S. University Ave., Ann Arbor, MI 48109-1107, USA; joarias@umich.eduhttps://orcid.org/0000-0002-5564-9873Katya GozmanAFFILIATIONSDepartment of Astronomy, University of Michigan, 1085 S. University Ave., Ann Arbor, MI 48109-1107, USA; joarias@umich.eduhttps://orcid.org/0000-0003-2294-4187In Sung JangAFFILIATIONSDepartment of Astronomy and Astrophysics, University of Chicago, Chicago, IL 60637, USAhttps://orcid.org/0000-0002-2502-0070Saxon StocktonAFFILIATIONSDepartment of Physics, Utah Valley University, 800 W. University Pkwy., Orem, UT 84058, USAhttps://orcid.org/0000-0003-1677-0213Oleg Y. GnedinAFFILIATIONSDepartment of Astronomy, University of Michigan, 1085 S. University Ave., Ann Arbor, MI 48109-1107, USA; joarias@umich.eduhttps://orcid.org/0000-0001-9852-9954Richard D'SouzaAFFILIATIONSVatican Observatory, Specola Vaticana, V-00120, Vatican City Statehttps://orcid.org/0000-0001-9269-8167Antonela MonachesiAFFILIATIONSDepartamento de Astronomía, Universidad de La Serena, Avda. Rául Bitrán 1305, La Serena, Chilehttps://orcid.org/0000-0003-2325-9616Jeremy BailinAFFILIATIONSDepartment of Physics and Astronomy, University of Alabama, Box 870324, Tuscaloosa, AL 35487-0324, USAhttps://orcid.org/0000-0001-6380-010XDavid NideverAFFILIATIONSDepartment of Physics, Montana State University, P.O. Box 173840, Bozeman, MT 59717-3840, USAhttps://orcid.org/0000-0002-1793-3689Roelof S. de JongAFFILIATIONSLeibniz-Institut für Astrophysik Potsdam (AIP), An der Sternwarte 16, 14482 Potsdam, Germanyhttps://orcid.org/0000-0001-6982-4081\n                                FiguresSkip to each figure in the articleTablesSkip to each table in the articleReferencesCitationsArticle dataSkip to each data item in the articleWhat\n                                        is article data?Open science Download Article PDFDownloadArticle ePub\n            Authors\n                    Jose Marco AriasAFFILIATIONSDepartment of Astronomy, University of Michigan, 1085 S. University Ave., Ann Arbor, MI 48109-1107, USA; joarias@umich.eduEMAILjoarias@umich.eduhttps://orcid.org/0009-0002-9085-5928Eric F. BellAFFILIATIONSDepartment of Astronomy, University of Michigan, 1085 S. University Ave., Ann Arbor, MI 48109-1107, USA; joarias@umich.eduhttps://orcid.org/0000-0002-5564-9873Katya GozmanAFFILIATIONSDepartment of Astronomy, University of Michigan, 1085 S. University Ave., Ann Arbor, MI 48109-1107, USA; joarias@umich.eduhttps://orcid.org/0000-0003-2294-4187In Sung JangAFFILIATIONSDepartment of Astronomy and Astrophysics, University of Chicago, Chicago, IL 60637, USAhttps://orcid.org/0000-0002-2502-0070Saxon StocktonAFFILIATIONSDepartment of Physics, Utah Valley University, 800 W. University Pkwy., Orem, UT 84058, USAhttps://orcid.org/0000-0003-1677-0213Oleg Y. GnedinAFFILIATIONSDepartment of Astronomy, University of Michigan, 1085 S. University Ave., Ann Arbor, MI 48109-1107, USA; joarias@umich.eduhttps://orcid.org/0000-0001-9852-9954Richard D'SouzaAFFILIATIONSVatican Observatory, Specola Vaticana, V-00120, Vatican City Statehttps://orcid.org/0000-0001-9269-8167Antonela MonachesiAFFILIATIONSDepartamento de Astronomía, Universidad de La Serena, Avda. Rául Bitrán 1305, La Serena, Chilehttps://orcid.org/0000-0003-2325-9616Jeremy BailinAFFILIATIONSDepartment of Physics and Astronomy, University of Alabama, Box 870324, Tuscaloosa, AL 35487-0324, USAhttps://orcid.org/0000-0001-6380-010XDavid NideverAFFILIATIONSDepartment of Physics, Montana State University, P.O. Box 173840, Bozeman, MT 59717-3840, USAhttps://orcid.org/0000-0002-1793-3689Roelof S. de JongAFFILIATIONSLeibniz-Institut für Astrophysik Potsdam (AIP), An der Sternwarte 16, 14482 Potsdam, Germanyhttps://orcid.org/0000-0001-6982-4081\n                Article metrics5375 Total downloads0 Video abstract viewsShare this articleArticle informationDatesReceived 2024 October 30 Revised 2025 February 8 Accepted 2025 February 8 Published 2025 March 11 Unified Astronomy Thesaurus conceptsDwarf galaxies; Andromeda GalaxyJournal RSS\n\n               Create or edit your corridor alerts\n\n                        Corridor alerts\n\n                            Receive alerts on all new research papers in American Astronomical Society\n                            (AAS) journals as soon as they are published.\n                            Select your desired journals and corridors below. You will need to select a minimum of one corridor.\n\n                                Corridors\n\n                                Journals\n\n                                Please note, The Planetary Science Journal (PSJ) does not currently use the corridors.\n\n                            Create alert\n\n                    What are corridors?opens in new tab\n\n    2041-8205/982/1/L3Abstract\n            We present the discovery of Andromeda XXXV, the faintest Andromeda satellite galaxy discovered to date, identified as an overdensity of stars in the Pan-Andromeda Archaeological Survey and confirmed via Hubble Space Telescope imaging. Located at a heliocentric distance of 927−63+76 kpc and 158−45+57 kpc from Andromeda, Andromeda XXXV is an extended (rh=53−11+13 pc), elliptical (=0.4±0.2), metal-poor ([Fe/H]∼−1.9) system, and the least luminous (MV=−5.2±0.3) of Andromeda's dwarf satellites discovered so far. Andromeda XXXV's properties are consistent with the known population of dwarf galaxies around the Local Group, bearing close structural resemblance to the Canes Venatici II and Hydra II Milky Way (MW) dwarf satellite galaxies. Its stellar population, characterized by a red horizontal branch or a red clump feature, mirrors that of other Andromeda satellite galaxies in showing evidence for a spread in age and metallicity, with no signs of younger stellar generations. This age–metallicity spread is not observed in MW satellites of comparable stellar mass, ",
    "summary": {
      "en": "The American Astronomical Society (AAS), founded in 1899 and located in Washington, DC, is the main organization for professional astronomers in North America, with around 7,000 members, including physicists and engineers. Its goal is to improve and share our understanding of the universe.\n\nThe Institute of Physics (IOP) is a global scientific society that supports physics and connects physicists. It has about 50,000 members and focuses on advancing physics research, education, and public awareness. IOP Publishing is a well-known leader in scientific communications.\n\nThe recent article titled \"Andromeda XXXV: The Faintest Dwarf Satellite of the Andromeda Galaxy,\" published on March 11, 2025, reveals the discovery of Andromeda XXXV, the faintest known satellite galaxy of Andromeda. It was found through the Pan-Andromeda Archaeological Survey and confirmed by Hubble Space Telescope images. This galaxy has unique features, including a significant distance from Andromeda and characteristics similar to other dwarf galaxies in the Local Group. Its stellar population shows a range of ages and metallicities, differing from similar galaxies around the Milky Way.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "cd1710a687ab12df",
    "title": {
      "en": "Show HN: XYMake – Turn Your Posts into LLM-Ready Data",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://xymake.com",
    "score": 7,
    "by": "janwilmake",
    "time": 1742842935,
    "content": "Turn Your\n\n                     Into LLM-Ready Data\n\n                    Unlock your X value by letting your MCP agents and other APIs access all your X posts.\n\n                        x.com/...\n\n                        xymake.com/...\n\n                        Just add \"ymake\" to any X URL and instantly get your content in formats that LLMs can use\n\n                        Unlock Your Feed\n\n                        View on GitHub\n\n                    Unlocking your feed will make all your X posts public to anyone. Read the Terms and\n                        Conditions\n\n                    Repurpose & Sell\n\n                        Turn your X threads into sellable content and digital products\n\n                    One-Click Access\n\n                        No complex tools or coding. Just change the URL and get instant markdown format.\n\n                    LLM-Ready Format\n\n                        Perfect for AI tools such as ChatGPT and MCP (Cursor, Claude, etc.)\n\n                See It In Action\n\n                        https://x.com/janwilmake/status/1904080559422288062\n\n                        https://xymake.com/janwilmake/status/1904080559422288062\n\n                        Try It Now\n\n                    Unlock Your X Gold Mine\n\n                        Your X content has untapped value. XYMake gives you the key to unlock it.\n\n                            Free My X\n\n                            View on GitHub\n\n                        By authorizing XYMake you agree to our Terms\n                            and Conditions\n\n                    © 2025 XYMake. All rights reserved. Not affiliated with X Corp.",
    "summary": {
      "en": "### Summary\n\nUnlock the value of your X posts by using XYMake, which allows your content to be accessed in formats suitable for large language models (LLMs). Simply add \"ymake\" to any X URL to convert your posts into a markdown format that AI tools like ChatGPT can use.\n\nKey points:\n- **Easy Access**: No technical skills required; just modify the URL.\n- **Public Content**: Unlocking your feed makes your posts public.\n- **Monetization**: Transform your X threads into sellable digital products.\n- **Untapped Potential**: Your X content has valuable uses that XYMake can help you discover.\n\nFor more details, view it on GitHub and check the terms and conditions.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "01a61a7f5361a371",
    "title": {
      "en": "The Trump administration accidentally texted me its war plans",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.theatlantic.com/politics/archive/2025/03/trump-administration-accidentally-texted-me-its-war-plans/682151/",
    "score": 988,
    "by": "_tk_",
    "time": 1742833435,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "6a7e6bf13f3bfc71",
    "title": {
      "en": "Show HN: Tascli, a simple CLI task and record manager",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://crates.io/crates/tascli",
    "score": 13,
    "by": "Aperocky",
    "time": 1742841176,
    "content": "tascli\nA simple CLI tool for tracking tasks and records from terminal.\nInstallation:\ncargo install tascli\n\nBasic Usage\nTasks and records are stored in ~/.local/share/tascli/tascli.db with rusqlite.\nTasks\nCreate tasks with deadlines:\n# Basic tasks\ntascli task \"Create readme\" today\ntascli task \"Publish package\" tomorrow\ntascli task \"Do taxes\" 4/15\n\n# With category\ntascli task -c work \"Read emails\" week\n\nList tasks:\n# List active tasks\n$ tascli list task\n\noutput:\nTask List:\n----------------------------------------------------------------------------------------------\n| Index  | Category            | Content                               | Deadline            |\n----------------------------------------------------------------------------------------------\n| 1      | life                | Unpack the crib                       | Today               |\n----------------------------------------------------------------------------------------------\n| 2      | tascli              | Add pagination capability for tascli  | Sunday              |\n|        |                     | list actions                          |                     |\n----------------------------------------------------------------------------------------------\n| 3      | tascli              | Add readme section on timestring      | Sunday              |\n|        |                     | format                                |                     |\n----------------------------------------------------------------------------------------------\n| 4      | life                | Do state taxes                        | Sunday              |\n----------------------------------------------------------------------------------------------\n| 5      | tascli              | Sort list output by time instead of   | Sunday              |\n|        |                     | internal id                           |                     |\n----------------------------------------------------------------------------------------------\n| 6      | tascli              | Fix length issue for unicode chars    | Sunday              |\n----------------------------------------------------------------------------------------------\n| 7      | life                | Two month pictures - follow the lead  | 4/23                |\n|        |                     | from the previous one month pictures  |                     |\n----------------------------------------------------------------------------------------------\n\nComplete tasks:\n# Mark index 1 as done\ntascli done 1\n\nList all tasks in tascli category (including completed)\ntascli list task -s all -c tascli\n\noutput:\nTask List:\n----------------------------------------------------------------------------------------------\n| Index  | Category            | Content                               | Deadline            |\n----------------------------------------------------------------------------------------------\n| 1      | tascli              | Add a tascli command to delete a row  | Today (completed)   |\n|        |                     | in the task or record table           |                     |\n----------------------------------------------------------------------------------------------\n| 2      | tascli              | Fix addition and modification commands| Today (completed)   |\n|        |                     | output to have N/A for index          |                     |\n----------------------------------------------------------------------------------------------\n| 3      | tascli              | Insert guardrail against accidental   | Today (completed)   |\n|        |                     | valid syntax like 'task list' that is |                     |\n|        |                     | mistakenly made                       |                     |\n----------------------------------------------------------------------------------------------\n| 4      | tascli              | Create a gif for readme               | Today (completed)   |\n----------------------------------------------------------------------------------------------\n| 5      | tascli              | Add pagination capability for tascli  | Sunday              |\n|        |                     | list actions                          |                     |\n----------------------------------------------------------------------------------------------\n| 6      | tascli              | Add readme section on timestring      | Sunday              |\n|        |                     | format                                |                     |\n----------------------------------------------------------------------------------------------\n\nRecords\nCreate records (for tracking events):\n# With current time\ntascli record -c feeding \"100ML\"\n\n# With specific time\ntascli record -c feeding -t 11:20AM \"100ML\"\n\nList records:\n# -d 1 stand for only get last 1 day of record\ntascli list record -d 1\n\noutput:\nRecords List:\n----------------------------------------------------------------------------------------------\n| Index  | Category            | Content                               | Created At          |\n----------------------------------------------------------------------------------------------\n| 1      | feeding             | 110ML                                 | Today 1:00AM        |\n----------------------------------------------------------------------------------------------\n| 2      | feeding             | breastfeeding                         | Today 4:10AM        |\n----------------------------------------------------------------------------------------------\n| 3      | feeding             | 100ML                                 | Today 7:30AM        |\n----------------------------------------------------------------------------------------------\n| 4      | feeding             | 110ML                                 | Today 11:20AM       |\n----------------------------------------------------------------------------------------------\n\nTime Format\nThis application accepts flexible time strings in various formats:\n\nSimple dates: today, tomorrow, yesterday, friday, eom (end of month), eoy (end of year)\nDate formats: YYYY-MM-DD, MM/DD/YYYY, MM/DD (current year)\nTime formats: HH:MM, 3:00PM, 3PM\nCombined: 2025-03-24 15:30, tomorrow 3PM\n\nWhen only a date is provided, the time defaults to end of day (23:59:59). When only a time is provided, the date defaults to today.\nHelp\ntascli uses clap for argument parsing, use --help to get help on all levels of this cli:\naperocky@~$ tascli -h\nUsage: tascli <COMMAND>\n\nCommands:\n  task    add task with end time\n  record  add record\n  done    Finish task or remove records\n  update  Update tasks or records wording/deadlines\n  list    list tasks or records\n  help    Print this message or the help of the given subcommand(s)\n\nOptions:\n  -h, --help     Print help\n  -V, --version  Print version\naperocky@~$ tascli task -h\nadd task with end time\n\nUsage: tascli task [OPTIONS] <CONTENT> [TIMESTR]\n\nArguments:\n  <CONTENT>  Description of the task\n  [TIMESTR]  Time the task is due, default to EOD\n\nOptions:\n  -c, --category <CATEGORY>  Category of the task\n  -h, --help                 Print help",
    "summary": {
      "en": "**Summary of tascli**\n\n**Overview:**\nTascli is a command-line tool for managing tasks and records from your terminal.\n\n**Installation:**\nYou can install it using the command:\n```\ncargo install tascli\n```\n\n**Basic Usage:**\n\n- **Task Management:**\n  - Create tasks with deadlines:\n    - Example: `tascli task \"Create readme\" today`\n  - List active tasks:\n    ```\n    tascli list task\n    ```\n  - Mark a task as complete:\n    - Example: `tascli done 1`\n\n- **Records Management:**\n  - Create records to track events:\n    - Example: `tascli record -c feeding \"100ML\"`\n  - List records from the last day:\n    ```\n    tascli list record -d 1\n    ```\n\n**Time Formats:**\nTascli accepts various time formats, such as:\n- Simple dates: today, tomorrow\n- Date formats: YYYY-MM-DD, MM/DD/YYYY\n- Time formats: HH:MM, 3:00PM\n- Combined formats: 2025-03-24 15:30\n\n**Help:**\nFor help with commands or options, use `tascli --help`. Key commands include:\n- `task`: Add a task\n- `record`: Add a record\n- `done`: Mark a task as complete or remove records\n- `list`: List tasks or records\n\nThis tool is useful for organizing and tracking tasks and events efficiently from the command line.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "1e51268ed4d16a02",
    "title": {
      "en": "Quitting an Intel x86 Hypervisor",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://halobates.de/blog/p/446",
    "score": 119,
    "by": "todsacerdoti",
    "time": 1742676124,
    "content": "Quitting an Intel x86 hypervisor\nwithout comments\n\nThis is an esoteric topic that might be of interest to people implementing Intel hypervisors. It assumes you know the basics of the Intel virtualization architecture, see Hypervisor from scratch for a tutorial. The actual full VT architecture is described in Volume 3 of the Intel SDMLet’s say we write an x86 hypervisor that starts in the UEFI environment and virtualizes the initialization phase of an OS. But the hypervisor wants to eventually quit itself to not cause extra overhead during OS run time.The way the hypervisor works is that it runs in its own memory and with its own page tables which are switched atomically on every VM exit by the VT-x implementation. This way it is isolated from the main OS.At some vm exit with the hypervisor running in its own context it decides that it is not needed anymore and wants to quit. To disable VT support the VMXOFF instruction can be used. But what we really need is an atomic VMXOFF + switch to the original OS page tables plus a jump, and all that without using any registers which need to be already restored to the original state of the OS. One trick is to use the MOV to CR3 instruction that reloads the page table as a jump. As soon as the page table is reloaded the CPU will fetch the next instruction with the translations from the freshly loaded page table, so we can transfer execution to the guest context. However to do that the MOV CR3 needs to be located just before the page offset of the target instruction. This can be done by copying a trampoline to the right page offset (potentially overlapping into the previous page). The trampoline is located in a special transfer page table mapping that places writable code pages overlapping the target mapping.But there are some complications. The hypervisor also needs to load the segmentation state (like GDT/LDT/IDT) of the guest. In theory they could just be loaded by mapping these guest pages into the transfer mapping and loading them before the transfer.  But what happens if the GDT/LDT/IDT is on the same page as the target address? This is common in real OS’ assembler startup code which is implemented in a small assembler file without any page separation between code and data. One option would be to copy them to the transfer page too and load it there, or the hypervisor first copies them to a temporary buffer and loads it from there. In the second option the base addresses of these structures will be incorrect, but in practice you can often rely on them getting reloaded eventually anyways.Another problem is the register state of the target. MOV to CR3 needs a register as the source of the reload, and it needs to be the last instruction of the trampoline. So it is impossible to restore the register it uses. But remember the hypervisor is doing this as the result of a VM exit. If we chose an exit for a condition that already clobbers a register we can use the same register for the reload and the next instruction executed in the original target (and which caused the exit originally) will just overwrite it again.\n\nA very convenient instruction for this is CPUID. It is executed multiple times in OS startup and overwrites multiple registers. In fact VMX always intercepts CPUID so it has to handle these exits in any case. So the trick to quit an hypervisor is to wait for the next CPUID exit and then use one of the registers clobbered by CPUID for the final CR3 reload.  This will have inconsistent register state for one instruction in the target, but unless the original OS is currently running a debugger it will never notice. In principle any exit as a result of an instruction that clobbers a register can be used for this.There is another potential complication if the target address of the OS conflicts with where the hypervisor is running before entering the transfer mapping. The transfer mapping needs to map the original code so that it can be jumped to. This could be solved with a third auxiliary mapping that is used before jumping to the transfer trampoline. In practice it doesn’t seem to be a problem because x86 OS typically run in a 1:1 mapping for startup, and that cannot conflict with the 1:1 mapping used by  UEFI programs as our hypervisor.\n\nHappy hypervisor hacking!\n\n    Written by therapsid\n    March 18th, 2025 at 9:34 pm\n\n    Posted in curiosities,kernel\n\n    « The browser circle closes again\n\nLeave a Reply\n\nName\n\nMail (will not be published)\n\nWebsite\n\nΔdocument.getElementById( \"ak_js_1\" ).setAttribute( \"value\", ( new Date() ).getTime() );",
    "summary": {
      "en": "This text discusses how to properly quit an Intel x86 hypervisor without causing issues during the operation of an operating system (OS). \n\nKey points include:\n\n1. **Context**: The hypervisor runs in its own memory and uses its own page tables to remain isolated from the main OS.\n\n2. **Quitting the Hypervisor**: To exit the hypervisor, the VMXOFF instruction is used, but it must be done in a way that seamlessly switches back to the OS. This requires an atomic operation that includes changing page tables and jumping to the OS code without interfering with registers.\n\n3. **Using MOV to CR3**: The instruction MOV to CR3 can help with this transition by reloading the page table and allowing the OS to take over. Special care is needed to ensure the instruction is correctly placed in memory.\n\n4. **Complications**: There are complications regarding loading the guest OS's segmentation state (like GDT/LDT/IDT) and managing register states, especially if the target address overlaps with the hypervisor's memory.\n\n5. **Utilizing CPUID**: A practical strategy is to wait for a CPUID instruction exit, which is commonly intercepted by the hypervisor. This allows the hypervisor to use a register already clobbered by CPUID for the transition.\n\n6. **Overall Solution**: While there are potential mapping conflicts, typical x86 OS setups reduce the likelihood of issues, making this approach feasible.\n\nThe article concludes with encouragement for developers working on hypervisors.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "56e5a0c00e5785e6",
    "title": {
      "en": "A Brief History of the Miracle Bacterium",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.asimov.press/p/miracle-bacterium",
    "score": 68,
    "by": "mailyk",
    "time": 1742745552,
    "content": "Share this postAsimov PressA Brief History of the Miracle BacteriumCopy linkFacebookEmailNotesMoreDiscover more from Asimov PressScience and technology for a flourishing future. Website: press.asimov.comSubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.Already have an account? Sign inThe Long ReadA Brief History of the Miracle BacteriumSerratia marcescens, a pathogen with an uncanny resemblance to blood, has had an outsized influence on modern science.Corrado NaiMar 24, 202520Share this postAsimov PressA Brief History of the Miracle BacteriumCopy linkFacebookEmailNotesMore13ShareArticle voiceover1×0:00-24:31Audio playback is not supported on your browser. Please upgrade.At 1:15 p.m. on Monday, August 8th, 1904, a British physician named M. H. Gordon took some soil he had “richly impregnated with a living emulsion” of the virulent bacterium,1 Serratia marcescens, and sprinkled it near a lamp post in front of the U.K. House of Commons. Gordon knew that was the exact spot Members of Parliament had to cross before their 2 p.m. session. His hope was for the politicians to step on the contaminated ground and spread the bacteria inside the Debating Chamber.Gordon wasn’t executing a terrorist attack. Rather, he had been appointed by a committee to study how germs spread inside the House of Commons following an outbreak of influenza among its members. Gordon had chosen Serratia marcescens because the bacterium forms easily recognizable red colonies. For his experiment, he placed numerous open Petri dishes inside the Debating Chamber on which the colonies could grow. Gordon’s idea was simple: politicians’ boots would carry contaminated soil and spread Serratia marcescens into the building; he would go away with his Petri dishes and reveal colonies of the bacterium to point at faults in the House of Commons’ ventilation system.But despite placing hundreds of Petri dishes around the chamber, Gordon wasn’t able to retrieve more than a handful of colonies. Suspecting the microbe might not spread easily by means of boots, he followed this up with a theatrical experiment: Inside the Debating Chamber, he gargled a suspension of Serratia marcescens and recited Shakespeare’s “King Henry V” and “Julius Caesar” for one hour to no one but an audience of open Petri dishes. This time, copious colonies of Serratia marcescens appeared, leading him to conclude that speech can transmit microbes as far as 70 feet (21 meters) away.2Indeed, Serratia marcescens’ vivid blood-red color has prompted its use in a wide range of experiments that have increased our understanding of how germs disperse within human bodies, buildings, and populations. Sightings of the striking microbe outside the lab have awakened both fear and awe in the general population.The awareness that certain strains of Serratia marcescens can cause severe harm to humans — counter-intuitively, the paler varieties are most dangerous — only became evident decades after Gordon’s investigations. Before then, hospitals deliberately sprayed Serratia marcescens inside their facilities to investigate microbial dispersion, and laboratory handbooks demonstrated transmission by handshake by having students coat their fingers in the microbe. While greater awareness of its dangers eventually led to its discontinuation in tracing experiments, Serratia marcescens remains an important subject of biomedical research.Its scientific journey began over 200 years ago with a bloody polenta.Subscribe to Asimov Press.SubscribeBlood MasqueradeWhen growing on solid substances, like foodstuffs or Petri dishes filled with agar, Serratia marcescens forms red colonies reminiscent of blood, earning it the informal moniker “masquerader of blood.” Its appearance on the polenta of a wealthy Paduan farmer during a particularly hot and humid summer in 1819 — ideal conditions for its flourishing — prompted the flurry of investigation that gave it its scientific name.Serratia marcescens looks like blood droplets when grown at room temperature on solid media. Mature colonies are mucilaginous, viscous, and tinged bright red or pink with an \"uncanny resemblance to blood.” Credit: Benutzer:BrudersohnAfter the farmer, Antonio Pittarello, discovered the red-spotted food in his house near Padua, the resultant hubbub of curiosity seekers and the spiritually appalled so disturbed the neighborhood that the local District Commissioner appointed Dr. Vicenze Sette (a physician, surgeon, and district health officer) to investigate the discovery. Also taking an interest in the mysterious “bloody polenta” was Bartolomeo Bizio (1791-1862), a young pharmacy student who joined the investigation on his own. In the meantime, red spots started to appear on food in hundreds of homes throughout Pittarello’s village as well as in villages close by.Through a series of ingenious experiments, Sette and Bizio reproduced the red spots on fresh polenta independently in their own home laboratories. Both concluded (erroneously) that the cause was a microscopic fungus.3 Sette called it Zaogalactina imetrofa (from the Latin, “slime living on food”); Bizio called it Serratia4 to honor an Italian Benedictine monk and physicist, Serafino Serrati (whom he felt had not gotten adequate credit for his contributions to the invention of the steamboat) and marcescens (from the Latin, “to decay”) since, much to his disappointment, the microbe’s red color fades easily.Racing to gain acclaim for identifying the source of the “bloody polenta” before Sette, Bizio made two deft moves. First, by citing Lazzaro Spallanzani, a pioneering biologist known for having worked to disprove the theory of spontaneous generation, he hoped to lend credibility to his findings; indeed, by demonstrating a biological origin for the “bloody polenta,” Bizio again refuted spontaneous generation decades before Louis Pasteur. And second, Bizio quickly published his discoveries in a Venetian newspaper.5 Sette was furious about being scooped by Bizio’s article, although Bizio’s results didn’t contradict his own.Bartolomeo BizioAs Bizio reported in a letter to a priest, Angelo Bellani, three years after the event:For several days succeeding its publication, there was much discussion of my experiments in the columns of this newspaper [The Official Gazette]; and to make them still more well known, an enterprising publisher printed them in a small pamphlet which he sold in the streets, so that the general public as well as educated persons came to know about them.By recreating red spots of Serratia marcescens on fresh polenta, Bizio grew pure colonies of microbes more than half a century before the “culture plate technique” developed by Robert Koch, Walther and Fanny Angelina Hesse, and Julius Petri.6 The significance of Bizio’s experiments has survived over the centuries, showing the falsehood of a dismissive remark made by renowned bacterial systematist S. T. Cowan, who said: “I believe we shall not lose anything by ignoring all work before the pioneer [sic] work of Pasteur.”The bacteriological investigations of another predecessor of Pasteur, the naturalist Christian Gottfried Ehrenberg (1795-1876), have survived as well. In 1848, several decades after the bloody polenta first appeared in Padua, red spots began popping up on boiled potatoes in Germany. Unaware of Bizio’s work, Ehrenberg observed the phenomenon and called the organism responsible Monas prodigiosa, one of the many names that Serratia marcescens has held over the centuries. Ehrenberg studied historical records and concluded that the bacterium was most likely responsible for more than 100 documented cases of so-called “miraculous blood.”!function(){\"use strict\";window.addEventListener(\"message\",(function(e){if(void 0!==e.data[\"datawrapper-height\"]){var t=document.querySelectorAll(\"iframe\");for(var a in e.data[\"datawrapper-height\"])for(var r=0;r<t.length;r++){if(t[r].contentWindow===e.source)t[r].style.height=e.data[\"datawrapper-height\"][a]+\"px\"}}}))}();Specifically, Serratia marcescens holds a large but unintended place in two of the world’s religions due to this shocking resemblance to blood. The 1264 “Miracle of Bolsena” of a Host “bleeding” with Serratia marcescens was believed to have contributed to the establishment of the Holy Communion, a central sacrament of the Greek Orthodox and Roman Catholic church. Additionally, the infamous antisemitic “blood libels” might have been due to this microbe, with thousands of Jews executed as heretics following accusations of having stabbed holy wafers in what was most likely a naturally occurring outbreak of Serratia marcescens on the starchy substratum. It is difficult to verify if Serratia marcescens was the precise biological cause of such “miracles” or “heresies,” however, since many microorganisms, such as mold or yeast, are often red or pink and grow readily on food. But it seems likely, for as researchers have pointed out: “We know of no organism [ … ] looking more like drops of fresh blood than Serratia marcescens.”With its centuries-long history, it’s perhaps no surprise that Serratia marcescens has had so many different names and presents “one of the most confusing taxonomies in the bacterial world.” In 1924, bacteriologists rehabilitated Bizio’s Linnean appellation, retaining Ehrenberg’s “term of miracle (prodigium) bacterium for use as a trivial or common name.” By the end of the 19th century, however, due to its vivid hue, Serratia marcescens had become one of “the cornerstones of modern bacteriology.” Fake Blood, Real ThreatSpecifically, Serratia marcescens’ striking color made it the microbe of choice for several decades amongst bacteriologists studying how pathogens spread through buildings and cities.In 1897, German bacteriologist Carl Flügge used the bacteria to perform experiments which served as a template for Gordon’s (but without the Shakespearean flair).7 Flügge was the first to demonstrate that mouth droplets carry bacteria. His findings spurred surgeon Johann Mikulicz to develop a precursor of today’s face mask, and were a major driver of the six feet (two meters) “social distancing” policy during the COVID-19 pandemic.In 1919, military doctors proved that utensils can indirectly transmit microbes by applying Serratia marcescens on the mouth and lips of “donor soldiers” before their meals. In 1926, bacteriologists traced how handshakes transmit microbes by smearing the bacterium on the hands of test subjects. In 1937, dentists detected bacteremia (entry of bacteria into the blood system) following dental extraction by spreading the pathogen around the gum of teeth. In 1945, military staff correlated air quality with the progression of illness by exposing test subjects to huge quantities of aerosolized Serratia marcescens. In 1957, doctors demonstrated urinary tract infections through catheters by applying Serratia marcescens on genitalia of semi-comatose patients, one of whom died shortly after.8More ominously, Nazi Germany studied the spread of the pathogen in the Paris Métro and the London Underground, as reported in a 1934 article by investigative journalist Henry Wickham Steed, Aerial Warfare: Secret German Plans. The French and U.K. governments took Steed’s article very seriously, as the underground systems in both Paris and London had functioned as shelters during WWI and would again in WWII.London residents take refuge in an Underground station during intense bombing from the Luftwaffe.The U.S. military also deployed Serratia marcescens in a spectacular way in September 1950. In a secret project called “Operation Sea Spray,” the Navy sprayed enormous quantities of the pathogen along the coast of San Francisco to study large-scale, open-air transmission of germs in biological warfare. Navy scientists also set up monitoring stations and traced the microbe up to 50 miles inland, but experts disputed any scientific and epidemiological merit of the experiments. Notably, “Operation Sea Spray” coincided with the first recorded nosocomial outbreak of Serratia marcescens, with eleven inpatients infected at Stanford Hospital between September 1950 and February 1951. Two of them got bacteremia; one died of heart failure.9(In November 1976, the Long Island newspaper Newsday leaked information about “Operation Sea Spray.” Contrary to other biowarfare experiments that flew under the radar, this led to a public Hearing at the Senate in March and May of 1977.10 Surprisingly, however, in August 1977, an investigation by the U.S. Centers for Disease Control concluded that the strain used by the U.S. Military was not related to any infection within the population.)That Serratia marcescens was so widely used in experiments involving human subjects should startle us; especially since the scientific community was accumulating evidence that the bacterium could be deadly. As early as 1903, infectologist E. Bertarelli showed that the bacterium was lethal to mice, rats, and guinea pigs “following inoculation of massive doses of Bacillus prodigiosus” (one of the microbe’s many names). The first report of human infection by Serratia marcescens followed in 1913, when a healthy young man sought medical help after becoming troubled by a foul-smelling red sputum, which he mistakenly took for blood.11 Doctors found no sign of blood cells in the sputum but saw a large number of intestinal bacteria, which grew on agar into distinctive red colonies. The patient later recovered without complications.Among those with weakened immune systems, Serratia marcescens has been involved “in every conceivable kind of infection.” It can affect an individual’s mouth and throat, lungs, gut, urinary tract, blood, heart, wounded skin, eyes, and central nervous system. When susceptible patients are infected by a virulent form of the pathogen, symptoms may include discoloration of extremities, shock, convulsions, deafness, delirium, and coma. Mortality of affected patients (especially in the case of sepsis, or the spread of bacteria in the bloodstream) runs as high as 30-40 percent. Indeed, by the late 1960s, the bacterium was unequivocally declared as a cause of serious infections, including death.Infections with Serratia marcescens haven’t ceased to be a cause for concern,12 and clinicians have wondered if there aren’t many more infections with Serratia marcescens going unrecognized. As recently as 2017, the World Health Organization listed Serratia among bacteria for which new antibiotics are urgently needed.Modern WondersSerratia marcescens striking red color comes from a pigment aptly called “prodigiosin.” Clinicians have observed an inverse correlation between a strain’s high levels of prodigiosin and its ability to cause infections. This might explain why past studies designed to take advantage of Serratia marcescens’ vivid appearance have caused little harm. By using bright red colonies as markers for their experiments, like M. H. Gordon’s in the U.K. Parliament, researchers might have unwittingly selected a less virulent strain of Serratia marcescens.Prodigiosin is an alkaloid with immunosuppressive, anticarcinogenic, and antimicrobial properties. Scientists do not yet fully understand why the bacterium produces prodigiosin, but have noticed that environmental conditions influence red pigmentation (for example, a rise in temperature reduces the bacterium’s vivid color). They speculate that the molecule might be important for cell dormancy, dispersal in the environment, or for gaining advantages over competing microbes.Prodigiosin has many uses as a biomedicine, including the inhibition of microbes such as bacteria, fungi, algae, and viruses. Researchers are still investigating the mechanisms by which prodigiosin acts on a target cell, and have observed adverse effects on the cell membrane as well as in organelles within the cell. Prodigiosin has also contributed to research on microbial “secondary metabolites;” molecules, including antibiotics, which are not involved in primary physiological functions like growth or reproduction, but nonetheless help protect an organism. Researchers showed before the discovery of penicillin, the first antibiotic, that Serratia marcescens inhibits Vibrio bacteria. In 1983, prodigiosin was also used as a molecular marker to clone, for the first time, a gene involved in the biosynthesis of an antibiotic.A formula named Coley’s Toxin containing Serratia marcescens has been used for over a century to stimulate the immune system to fight off cancers. (Its namesake, William Coley, is considered the “Father of Immunotherapy.”) Prodigiosin has shown activity against cancer cells, among others, by inducing DNA cleavage and apoptosis (cell death). Prodigiosin is also being investigated for therapies that require inhibition of the immune system.In nature, Serratia marcescens is part of a complex web of ecological interactions. It can inhibit parasites (like the malaria agent Plasmodium falciparum), kill insects (like the cockroach Blattella germanica by acting synergistically with a fungus), or enhance growth of some plants while killing others. Researchers have also used Serratia marcescens as a model to study how environmental species evolve into pathogens. When confronted with a natural predator (like the protist Tetrahymena thermophila), the bacterium acquires traits that contribute to its virulence. In 2011, researchers also observed that Serratia marcescens had begun to infect corals, the first instance of a “reverse zoonosis” in marine habitats, a situation in which a pathogen moves from humans to animals.As of early 2025, Serratia marcescens stands among the top 30 most studied microbes, topping species such as Agrobacterium tumefaciens (a widely used model microbe to genetically engineer plants), Legionella pneumophila (the cause of Legionnaire’s disease), Clostridioides difficile (a common diarrhea-causing bacterium affecting the intestinal tract), and Mycobacterium smegmatis (a model organism for the study of the pathogen causing tuberculosis, M. tuberculosis). Through surprising sightings and curious experiments, this microbe has left an enormous mark culturally, clinically, and scientifically. Its vivid blood-red color has elicited both wonder and alarm in the eyes of the beholder even as its actual dangers have often been ignored.It is no wonder, then, that Alexander Fleming, that giant of microbial investigation, also paid notice. Fleming used it in a painting method he developed, called microbial art (or “agar art”). When working in this medium, artists take a Petri dish filled with agar and, using a lab tool called a loop, inoculate sections of the plate with various species of microbes that produce different hues. As the microbes grow, their living pigments form an image. For yellow, Fleming used Staphylococcus; for blue, Bacillus violaceus; and for brilliant red, of course, Serratia marcescens.Agar art paintings by Alexander Fleming. Credit: American Society for MicrobiologyThis article is accompanied by an interview with the author. Listen on Spotify or Apple Music.Corrado Nai is a science writer with a Ph.D. in microbiology. He has published articles in Smithsonian Magazine, New Scientist, Reactor (formerly Tor.com), Small Things Considered, and elsewhere. He is writing a graphic novel about the forgotten woman who introduced agar to microbiology, Fanny Angelina Hesse (1850-1934), based on unpublished historical material he helped resurface. Corrado lives in Jakarta with his wife and daughter.Cite: Nai C. “A Brief History of the Miracle Bacterium.” Asimov Press (2025). DOI: 10.62211/48yk-73gfLead image by Ella Watkins-Dulaney, adapted from Benutzer:Brudersohn (CC BY3.0).1I use “virulent bacterium” and “pathogen” interchangeably to refer to a disease-causing microbe. Usually, “pathogen” refers to a species whereas strains (that is, lineages of bacteria) within a species can be more or less “virulent.”2Gordon’s 250+ page report is dramatic. In some experiments he made bacteriological analyses of the Ministerial versus the Opposition side of the Chamber; in others he placed open Petri dishes even on the top of the Clock Tower. In his speech experiment, he retrieved more colonies when he repeated the test with the ventilation system on than with the ventilation system off. Despite inconclusive bacteriology findings, he suggested 14 substantive modifications to the ventilation system.3Christian Gottfried Ehrenberg thought it was an animal, botanist Camille Montagne an algae.4Serratia marcescens is the first bacterium to be ever named after a person. The genus name Serratia is only predated by Vibrio (Otto Müller, 1773) and Polyangium (Heinrich Link, 1809).5The modern counterpart of Bizio’s newspaper article, published in the Gazzetta Privilegiata di Venezia on 24 August 1819, would be a preprint.6Koch was the first to streak bacteria into single colonies in the 1870s; the Hesse couple introduced agar to the laboratory in 1881; and Petri perfected his Petri dishes in 1887. The “culture plate technique” continues to be used unaltered to this day.7The original German article is available here: Carl Flügge (1897), Ueber Luftinfection. Zeitschrift für Hygiene 25:179-224.8It’s uncertain if Serratia marcescens was the cause of death, as doctors were not granted the rights for an autopsy.9The Stanford Hospital case marked the first case of endocarditis (inflammation of the heart) by Serratia marcescens.10The Hearing discussed classified Military biowarfare experiments that took place using Serratia marcescens until 1968, including in the New York City Subway. The Military admitted knowing about the Stanford Hospital outbreak.11This appearance of red sputum without the presence of blood was later named “pseudohaemoptysis.”12A dramatic outbreak happened in the mid-1970s and involved epidemics in four different hospitals in Nashville, Tennessee, caused by rotating staff likely transmitting the pathogen by hand. The virulent strain of Serratia marcescens was resistant to all antibiotics and infected over 200 patients, killing eight. Then, in one of those hospitals, a much more severe epidemic with Klebsiella pneumoniae infected about 400 patients, killing 18. Clinicians attributed this outbreak to co-infection and in vivo transmission of antibiotic-resistant genes from Serratia to Klebsiella. More recently, outbreaks of Serratia marcescens have occurred within prisons due to contaminated disinfectants and amongst intravenous opioid users due to contaminated syringes. In 2004, biotech company Chiron Corporation lost an enormous quantity of flu vaccines due to contamination with Serratia marcescens.Subscribe to Asimov PressScience and technology for a flourishing future. Website: press.asimov.comSubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.20 Likes∙3 Restacks20Share this postAsimov PressA Brief History of the Miracle BacteriumCopy linkFacebookEmailNotesMore13SharePreviousA guest post byCorrado NaiScience writer, PhD fungal ecology. Currently writing a graphic novel about the forgotten woman who introduced agar to the lab, Fanny Angelina Hesse (1850-1934): https://fanny-hesse-graphic-novel.site/",
    "summary": {
      "en": "**Summary: A Brief History of Serratia marcescens**\n\nSerratia marcescens is a bacterium known for its striking red color, resembling blood, which has significantly impacted scientific research and public health. It was first noted in 1819 when red spots appeared on polenta, leading to investigations by scientists, including Bartolomeo Bizio, who named the bacterium. \n\nIn the early 1900s, British physician M. H. Gordon used Serratia marcescens in experiments to study how germs spread, demonstrating that the microbe could be transmitted through speech and contact. Over the years, its unique color made it a popular choice in various scientific experiments related to microbial transmission.\n\nDespite its use in research, Serratia marcescens can be harmful, particularly its paler strains. It has been linked to serious infections in humans, especially those with weakened immune systems, leading to high mortality rates. Historically, this bacterium has been involved in both medical studies and military experiments, raising ethical concerns.\n\nSerratia marcescens also produces a pigment called prodigiosin, which has potential medical uses, including cancer treatment and antimicrobial properties. Today, it remains a widely studied microbe, contributing to our understanding of infections and microbial behavior in various environments.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "b06da4e326eeff0b",
    "title": {
      "en": "The Lost Art of Commit Messages",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.seyhan.me/blog/post/lost-art-of-commit-messages",
    "score": 32,
    "by": "drac89",
    "time": 1742885542,
    "content": "The Lost Art of Commit MessagesHave you ever noticed how commit messages these days are like cryptic haikus from developers who seem to think a single word is all it takes to summarize their masterpiece?\nPeople scribbling \"fix bug\", \"update code\", or the ever descriptive \"refactor\" while pushing changes that could rival a novel in length. It's as if they think we're all mind readers or perhaps they assume their code is so flawless it needs no explanation.\n\nFor the love of clean code history, let's remember we're not monkeys banging on keyboards; we're educated, civilized human beings.\n\nThe Art of the Commit Message\nCrafting a meaningful commit message seems to be a lost art in today's fast-paced development world.\nYes! I said \"lost art\". Because many developers or let me correct that, many keyboard virtuosos apparently don't see the value in leaving a traceable breadcrumb trail of their genius.\nOkay, let's learn how to do it right.\nCommit messages are the narrative of your project's history. They help others (and your future self) understand why changes were made, making collaboration smoother and debugging less of a nightmare.\nGiven that, let's dive into some guidelines to improve your commit messages from vague to valuable.\nCommit Message Guidelines\nEach commit message should follow this structure:\n<type>(optional scope): <short description>\n<BLANK LINE>\n- Optional point 1 in brief\n- Optional point 2 in brief\n<BLANK LINE>\n<footer>\n\nComponents of the Format\n\nType: Specifies the nature of the commit.\nScope (optional): A short identifier for the component, file, or feature affected.\nShort Description: A concise summary of the change, ideally up to 50 characters.\nDetailed Points (optional): Bullet points that provide additional context or details about the changes.\nFooter (optional): Additional metadata like issue references, co-authors, or breaking changes.\n\nGuidelines for Each Component\nType\n\nRequired: Select the appropriate type based on the commit's purpose.\nUse only one of the allowed types.\n\nAllowed Types\nChoose from the following types for the <type> field:\n\nfeat: A new feature\nfix: A bug fix\nchore: Routine tasks that don't impact functionality\nrefactor: Code changes that neither fix bugs nor add features\ndocs: Documentation updates\nstyle: Code style changes that don't affect functionality\ntest: Adding or updating tests\nperf: Performance improvements\nci: Continuous Integration changes\nbuild: Changes related to build processes or dependencies\nrevert: Reverts a previous commit\n\nScope (Optional)\n\nOptional: Specifies the part of the codebase affected.\nKeep it short and descriptive.\nUse lowercase and single-word terms (hyphens are acceptable).\n\nExamples:\n\nauth, api, ui, database, config.\n\nShort Description\n\nRequired: Summarize the change in 50 characters or less.\nStart with a lowercase letter; no punctuation at the end.\nClearly convey the essence of the change.\n\nExamples:\n\nadd user authentication\nfix null pointer exception in payment module\nrefactor data processing pipeline\n\nDetailed Points (Optional)\n\nOptional: Provide additional context in bullet points.\nKeep each point brief and focused.\nUse bullets (-) and avoid lengthy explanations.\n\nExamples:\n\n- implement OAuth2 for third-party login\n- adjust timeout settings to prevent crashes\n- reorganize folder structure for clarity\n\nFooter (Optional)\nYou might wonder, \"Footers? Isn't that a bit much?\" Well, consider this: footers can automate tedious tasks, improve team communication, and make your commit history a beacon of clarity in the chaotic sea of code changes.\nNow, the optional footer section is where you can sprinkle in extra metadata that helps with automation, issue tracking, and keeping your project history as pristine as a freshly refactored codebase.\nFooters are additional lines at the end of your commit message that provide metadata about the commit. They can:\n\nAutomate issue tracking: Automatically close or reference issues and pull requests.\nProvide additional context: Include information like co-authors, sign-offs, or breaking changes.\nEnhance clarity: Make your commit history more informative and navigable.\n\nCommon Footer Tags\nBecause why settle for clarity when you can have extra clarity? Here are some standard footer tags you can use:\n\nBREAKING CHANGE: Signals that the commit introduces a backward-incompatible change.\nFixes: References an issue your commit fixes.\nCloses: Similar to \"Fixes\", but used to close issues or pull requests.\nResolves: Another way to reference and close issues.\nRelated to: Links to issues or tasks related to the commit without closing them.\nCo-authored-by: Credits additional contributors.\nReviewed-by: Indicates who reviewed the changes.\nSigned-off-by: A sign-off acknowledging you have the rights to submit the work.\nSee also: Points to additional resources or relevant information.\n\nExample Commit Messages\nExample 1: Feature Addition\nfeat(auth): add user login functionality\n\n- implement JWT authentication\n- add login endpoint to API\n\nExample 2: Bug Fix that Closes an Issue\nfix(ui): correct alignment on dashboard widgets\n\n- adjust CSS flex properties\n- test on multiple screen sizes\n\nFixes #204\n\nExample 3: Documentation Update\ndocs(readme): update installation instructions\n\n- add steps for setting up the development environment\n- include troubleshooting tips\n\nExample 4: Code Refactor\nrefactor(api): simplify request handlers\n\n- remove redundant code blocks\n- improve error handling mechanisms\n\nExample 5: Documentation Update with See Also\ndocs(README): update setup instructions\n\n- include Docker configuration\n- clarify environment variables\n\nSee also: https://example.com/setup-guide\n\nGeneral Tips\n\nBe concise and clear: Your future self (and teammates) will thank you.\nAvoid redundancy: Don't repeat the same information.\nUse present tense: \"Add feature\", not \"Added feature\".\nBe Consistent: Use standard tags to keep the commit history uniform.\nKeep It Relevant: Only include footers that add value or necessary context.\nStay Professional: This isn't the place for jokes or unnecessary commentary (that's what the code comments are for).\n\nNext time you're tempted to type \"fix stuff\" as a commit message, remember, we're not monkeys. We're educated, civilized human beings capable of conveying complex ideas with clarity. Let's make sure our commit messages reflect that if not for the sake of our teammates, then at least to prevent our future selves from cursing our past selves.\nHappy committing!",
    "summary": {
      "en": "### Summary of The Lost Art of Commit Messages\n\nCommit messages are often vague and unhelpful, resembling cryptic notes rather than clear communication. Good commit messages are essential for understanding project history and aiding collaboration.\n\n**Key Guidelines for Writing Effective Commit Messages:**\n\n1. **Structure:**\n   - Format: `<type>(optional scope): <short description>`\n   - Followed by a blank line, optional bullet points for details, and an optional footer.\n\n2. **Components:**\n   - **Type**: Indicates the nature of the commit (e.g., feat, fix, chore).\n   - **Scope (optional)**: Identifies the affected part of the codebase.\n   - **Short Description**: A concise summary (50 characters or less).\n   - **Detailed Points (optional)**: Brief bullet points for additional context.\n   - **Footer (optional)**: Extra metadata for automation and issue tracking.\n\n3. **Common Types:**\n   - feat: New feature\n   - fix: Bug fix\n   - docs: Documentation updates\n   - refactor: Code changes without new features or bug fixes\n   - Others include style, test, perf, ci, build, and revert.\n\n4. **Example Messages:**\n   - Adding a feature: `feat(auth): add user login functionality`\n   - Fixing a bug: `fix(ui): correct widget alignment` (Fixes #204)\n\n**General Tips:**\n- Be clear, concise, and consistent.\n- Use present tense.\n- Avoid redundancy and irrelevant information.\n- Maintain professionalism.\n\nBy improving commit messages, developers can enhance collaboration and make their project's history more understandable.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "a8531c88fcf194f4",
    "title": {
      "en": "Upcoming changes to how live videos are stored",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.facebook.com/business/help/1005165994166600?_fb_noscript=1",
    "score": 24,
    "by": "EGreg",
    "time": 1742693699,
    "content": "Help chosen for youSome Messaging Metrics UnavailableBusiness Help CenterWhy is My Boost Unavailable?Business Help CenterTroubleshoot a Disabled Ad AccountBusiness Help CenterHow Ad Billing Works on FacebookBusiness Help Center\n\nGet the latest updates from Meta for Business.Provide your email address to receive the latest updates from Meta for Business, including news, events and product updates. By submitting this form, you agree to receive marketing related electronic communications from Meta, including news, events, updates and promotional emails. You may withdraw your consent and unsubscribe from these at any time, for example, by clicking the unsubscribe link included on our emails. For more information about how Meta handles your data please read our Data Policy.form originlead_sourceoptin__cCampaign_IDSubscribe",
    "summary": {
      "en": "This text includes information about resources available for businesses using Meta (formerly Facebook). Key points include:\n\n- Access to help for issues like unavailable messaging metrics, disabled ad accounts, and understanding ad billing on Facebook.\n- An invitation to sign up for updates from Meta for Business, which includes news and events.\n- By signing up, users agree to receive marketing emails from Meta and can unsubscribe at any time. \n\nFor more details on how Meta manages user data, there's a mention of their Data Policy.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "280cf1d44bdbde37",
    "title": {
      "en": "DNS Speed Test",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://dnsspeedtest.online/",
    "score": 28,
    "by": "tosh",
    "time": 1742895275,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "09e9f98019f60115",
    "title": {
      "en": "PyTorch Internals: Ezyang's Blog",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://blog.ezyang.com/2019/05/pytorch-internals/",
    "score": 436,
    "by": "Anon84",
    "time": 1742654344,
    "content": "PyTorch internals\n\nThis post is a long form essay version of a talk about PyTorch internals, that I gave at the PyTorch NYC meetup on May 14, 2019.\n\nHi everyone!  Today I want to talk about the internals of PyTorch.\n\nThis talk is for those of you who have used PyTorch, and thought to yourself, \"It would be great if I could contribute to PyTorch,\" but were scared by PyTorch's behemoth of a C++ codebase.  I'm not going to lie: the PyTorch codebase can be a bit overwhelming at times. The purpose of this talk is to put a map in your hands: to tell you about the basic conceptual structure of a \"tensor library that supports automatic differentiation\", and give you some tools and tricks for finding your way around the codebase.  I'm going to assume that you've written some PyTorch before, but haven't necessarily delved deeper into how a machine learning library is written.\n\nThe talk is in two parts: in the first part, I'm going to first introduce you to the conceptual universe of a tensor library.  I'll start by talking about the tensor data type you know and love, and give a more detailed discussion about what exactly this data type provides, which will lead us to a better understanding of how it is actually implemented under the hood.  If you're an advanced user of PyTorch, you'll be familiar with most of this material.  We'll also talk about the trinity of \"extension points\", layout, device and dtype, which guide how we think about extensions to the tensor class.  In the live talk at PyTorch NYC, I skipped the slides about autograd, but I'll talk a little bit about them in these notes as well.\nThe second part grapples with the actual nitty gritty details involved with actually coding in PyTorch.  I'll tell you how to cut your way through swaths of autograd code, what code actually matters and what is legacy, and also all of the cool tools that PyTorch gives you for writing kernels.\n\nThe tensor is the central data structure in PyTorch.  You probably have a pretty good idea about what a tensor intuitively represents: its an n-dimensional data structure containing some sort of scalar type, e.g., floats, ints, et cetera.  We can think of a tensor as consisting of some data, and then some metadata describing the size of the tensor, the type of the elements in contains (dtype), what device the tensor lives on (CPU memory? CUDA memory?)\n\nThere's also a little piece of metadata you might be less familiar with: the stride.  Strides are actually one of the distinctive features of PyTorch, so it's worth discussing them a little more.\n\nA tensor is a mathematical concept.  But to represent it on our computers, we have to define some sort of physical representation for them.  The most common representation is to lay out each element of the tensor contiguously in memory (that's where the term contiguous comes from), writing out each row to memory, as you see above. In the example above, I've specified that the tensor contains 32-bit integers, so you can see that each integer lies in a physical address, each offset four bytes from each other.  To remember what the actual dimensions of the tensor are, we have to also record what the sizes are as extra metadata.\nSo, what do strides have to do with this picture?\n\nSuppose that I want to access the element at position tensor[1, 0] in my logical representation.  How do I translate this logical position into a location in physical memory?  Strides tell me how to do this: to find out where any element for a tensor lives, I multiply each index with the respective stride for that dimension, and sum them all together.  In the picture above, I've color coded the first dimension blue and the second dimension red, so you can follow the index and stride in the stride calculation.  Doing this sum, I get two (zero-indexed), and indeed, the number three lives two below the beginning of the contiguous array.\n(Later in the talk, I'll talk about TensorAccessor, a convenience class that handles the indexing calculation.  When you use TensorAccessor, rather than raw pointers, this calculation is handled under the covers for you.)\nStrides are the fundamental basis of how we provide views to PyTorch users.  For example, suppose that I want to extract out a tensor that represents the second row of the tensor above:\n\nUsing advanced indexing support, I can just write tensor[1, :] to get this row.  Here's the important thing: when I do this, I don't create a new tensor; instead, I just return a tensor which is a different view on the underlying data.  This means that if I, for example, edit the data in that view, it will be reflected in the original tensor.  In this case, it's not too hard to see how to do this: three and four live in contiguous memory, and all we need to do is record an offset saying that the data of this (logical) tensor lives two down from the top.  (Every tensor records an offset, but most of the time it's zero, and I'll omit it from my diagrams when that's the case.)\n\nQuestion from the talk: If I take a view on a tensor, how do I free the memory of the underlying tensor?\nAnswer: You have to make a copy of the view, thus disconnecting it from the original physical memory.  There's really not much else you can do.  By the way, if you have written Java in the old days, taking substrings of strings has a similar problem, because by default no copy is made, so the substring retains the (possibly very large string). Apparently, they fixed this in Java 7u6.\n\nA more interesting case is if I want to take the first column:\n\nWhen we look at the physical memory, we see that the elements of the column are not contiguous: there's a gap of one element between each one.  Here, strides come to the rescue: instead of specifying a stride of one, we specify a stride of two, saying that between one element and the next, you need to jump two slots.  (By the way, this is why it's called a \"stride\": if we think of an index as walking across the layout, the stride says how many locations we stride forward every time we take a step.)\nThe stride representation can actually let you represent all sorts of interesting views on tensors; if you want to play around with the possibilities, check out the Stride Visualizer.\nLet's step back for a moment, and think about how we would actually implement this functionality (after all, this is an internals talk.)  If we can have views on tensor, this means we have to decouple the notion of the tensor (the user-visible concept that you know and love), and the actual physical data that stores the data of the tensor (called storage):\n\nThere may be multiple tensors which share the same storage.  Storage defines the dtype and physical size of the tensor, while each tensor records the sizes, strides and offset, defining the logical interpretation of the physical memory.\nOne thing to realize is that there is always a pair of Tensor-Storage, even for \"simple\" cases where you don't really need a storage (e.g., you just allocated a contiguous tensor with torch.zeros(2, 2)).\n\nBy the way, we're interested in making this picture not true; instead of having a separate concept of storage, just define a view to be a tensor that is backed by a base tensor.  This is a little more complicated, but it has the benefit that contiguous tensors get a much more direct representation without the Storage indirection.  A change like this would make PyTorch's internal representation a bit more like Numpy's.\n\nWe've talked quite a bit about the data layout of tensor (some might say, if you get the data representation right, everything else falls in place).  But it's also worth briefly talking about how operations on the tensor are implemented.  At the very most abstract level, when you call torch.mm, two dispatches happen:\n\nThe first dispatch is based on the device type and layout of a tensor: e.g., whether or not it is a CPU tensor or a CUDA tensor (and also, e.g., whether or not it is a strided tensor or a sparse one).  This is a dynamic dispatch: it's a virtual function call (exactly where that virtual function call occurs will be the subject of the second half of this talk).  It should make sense that you need to do a dispatch here: the implementation of CPU matrix multiply is quite different from a CUDA implementation.  It is a dynamic dispatch because these kernels may live in separate libraries (e.g., libcaffe2.so versus libcaffe2_gpu.so), and so you have no choice: if you want to get into a library that you don't have a direct dependency on, you have to dynamic dispatch your way there.\nThe second dispatch is a dispatch on the dtype in question.  This dispatch is just a simple switch-statement for whatever dtypes a kernel chooses to support.  Upon reflection, it should also make sense that we need to a dispatch here: the CPU code (or CUDA code, as it may) that implements multiplication on float is different from the code for int.  It stands to reason you need separate kernels for each dtype.\nThis is probably the most important mental picture to have in your head, if you're trying to understand the way operators in PyTorch are invoked.  We'll return to this picture when it's time to look more at code.\n\nSince we have been talking about Tensor, I also want to take a little time to the world of tensor extensions.  After all, there's more to life than dense, CPU float tensors.  There's all sorts of interesting extensions going on, like XLA tensors, or quantized tensors, or MKL-DNN tensors, and one of the things we have to think about, as a tensor library, is how to accommodate these extensions.\n\nOur current model for extensions offers four extension points on tensors.  First, there is the trinity three parameters which uniquely determine what a tensor is:\n\nThe device, the description of where the tensor's physical memory is actually stored, e.g., on a CPU, on an NVIDIA GPU (cuda), or perhaps on an AMD GPU (hip) or a TPU (xla).  The distinguishing characteristic of a device is that it has its own allocator, that doesn't work with any other device.\nThe layout, which describes how we logically interpret this physical memory.  The most common layout is a strided tensor, but sparse tensors have a different layout involving a pair of tensors, one for indices, and one for data; MKL-DNN tensors may have even more exotic layout, like blocked layout, which can't be represented using merely strides.\nThe dtype, which describes what it is that is actually stored in each element of the tensor.  This could be floats or integers, or it could be, for example, quantized integers.\n\nIf you want to add an extension to PyTorch tensors (by the way, if that's what you want to do, please talk to us!  None of these things can be done out-of-tree at the moment), you should think about which of these parameters you would extend.  The Cartesian product of these parameters define all of the possible tensors you can make.  Now, not all of these combinations may actually have kernels (who's got kernels for sparse, quantized tensors on FPGA?) but in principle the combination could make sense, and thus we support expressing it, at the very least.\nThere's one last way you can make an \"extension\" to Tensor functionality, and that's write a wrapper class around PyTorch tensors that implements your object type.  This perhaps sounds obvious, but sometimes people reach for extending one of the three parameters when they should have just made a wrapper class instead.  One notable merit of wrapper classes is they can be developed entirely out of tree.\nWhen should you write a tensor wrapper, versus extending PyTorch itself?  The key test is whether or not you need to pass this tensor along during the autograd backwards pass.  This test, for example, tells us that sparse tensor should be a true tensor extension, and not just a Python object that contains an indices and values tensor: when doing optimization on networks involving embeddings, we want the gradient generated by the embedding to be sparse.\n\nOur philosophy on extensions also has an impact of the data layout of tensor itself.  One thing we really want out of our tensor struct is for it to have a fixed layout: we don't want fundamental (and very frequently called) operations like \"What's the size of a tensor?\" to require virtual dispatches.  So when you look at the actual layout of a Tensor (defined in the TensorImpl struct),  what we see is a common prefix of all fields that we consider all \"tensor\"-like things to universally have, plus a few fields that are only really applicable for strided tensors, but are so important that we've kept them in the main struct, and then a suffix of custom fields that can be done on a per-Tensor basis.  Sparse tensors, for example, store their indices and values in this suffix.\n\nI told you all about tensors, but if that was the only thing PyTorch provided, we'd basically just be a Numpy clone.  The distinguishing characteristic of PyTorch when it was originally released was that it provided automatic differentiation on tensors (these days, we have other cool features like TorchScript; but back then, this was it!)\nWhat does automatic differentiation do?  It's the machinery that's responsible for taking a neural network:\n\n...and fill in the missing code that actually computes the gradients of your network:\n\nTake a moment to study this diagram.  There's a lot to unpack; here's what to look at:\n\nFirst, rest your eyes on the variables in red and blue.  PyTorch implements reverse-mode automatic differentiation, which means that we effectively walk the forward computations \"backward\" to compute the gradients.  You can see this if you look at the variable names: at the bottom of the red, we compute loss; then, the first thing we do in the blue part of the program is compute grad_loss.  loss was computed from next_h2, so we compute grad_next_h2.  Technically, these variables which we call grad_ are not really gradients; they're really Jacobians left-multiplied by a vector, but in PyTorch we just call them grad and mostly everyone knows what we mean.\nIf the structure of the code stays the same, the behavior doesn't: each line from forwards is replaced with a different computation, that represents the derivative of the forward operation.  For example, the tanh operation is translated into a tanh_backward operation (these two lines are connected via a grey line on the left hand side of the diagram).  The inputs and outputs of the forward and backward operations are swapped: if the forward operation produced next_h2, the backward operation takes grad_next_h2 as an input.\n\nThe whole point of autograd is to do the computation that is described by this diagram, but without actually ever generating this source.  PyTorch autograd doesn't do a source-to-source transformation (though PyTorch JIT does know how to do symbolic differentiation).\n\nTo do this, we need to store more metadata when we carry out operations on tensors.  Let's adjust our picture of the tensor data structure: now instead of just a tensor which points to a storage, we now have a variable which wraps this tensor, and also stores more information (AutogradMeta), which is needed for performing autograd when a user calls loss.backward() in their PyTorch script.\n\nThis is yet another slide which will hopefully be out of date in the near future.  Will Feng is working on a Variable-Tensor merge in C++, following a simple merge which happened to PyTorch's frontend interface.\nWe also have to update our picture about dispatch:\n\nBefore we dispatch to CPU or CUDA implementations, there is another dispatch on variables, which is responsible for unwrapping variables, calling the underlying implementation (in green), and then rewrapping the results into variables and recording the necessary autograd metadata for backwards.\nSome implementations don't unwrap; they just call into other variable implementations.  So you might spend a while in the Variable universe.  However, once you unwrap and go into the non-Variable Tensor universe, that's it; you never go back to Variable (except by returning from your function.)\n\nIn my NY meetup talk, I skipped the following seven slides.  I'm also going to delay writeup for them; you'll have to wait for the sequel for some text.\n\nEnough about concepts, let's look at some code.\n\nPyTorch has a lot of folders, and there is a very detailed description of what they are in the CONTRIBUTING document, but really, there are only four directories you really need to know about:\n\nFirst, torch/ contains what you are most familiar with: the actual Python modules that you import and use.  This stuff is Python code and easy to hack on (just make a change and see what happens).  However, lurking not too deep below the surface is...\ntorch/csrc/, the C++ code that implements what you might call the frontend of PyTorch.  In more descriptive terms, it implements the binding code that translates between the Python and C++ universe, and also some pretty important pieces of PyTorch, like the autograd engine and the JIT compiler.  It also contains the C++ frontend code.\naten/, short for \"A Tensor Library\" (coined by Zachary DeVito), is a C++ library that implements the operations of Tensors.  If you're looking for where some kernel code lives, chances are it's in ATen.  ATen itself bifurcates into two neighborhoods of operators: the \"native\" operators, which are modern, C++ implementations of operators, and the \"legacy\" operators (TH, THC, THNN, THCUNN), which are legacy, C implementations.  The legacy operators are the bad part of town; try not to spend too much time there if you can.\nc10/, which is a pun on Caffe2 and A\"Ten\" (get it? Caffe 10) contains the core abstractions of PyTorch, including the actual implementations of the Tensor and Storage data structures.\n\nThat's a lot of places to look for code; we should probably simplify the directory structure, but that's how it is.  If you're trying to work on operators, you'll spend most of your time in aten.\nLet's see how this separation of code breaks down in practice:\n\nWhen you call a function like torch.add, what actually happens?  If you remember the discussion we had about dispatching, you already have the basic picture in your head:\n\nWe have to translate from Python realm to the C++ realm (Python argument parsing)\nWe handle variable dispatch (VariableType--Type, by the way, doesn't really have anything to do programming language types, and is just a gadget for doing dispatch.)\nWe handle device type / layout dispatch (Type)\nWe have the actual kernel, which is either a modern native function, or a legacy TH function.\n\nEach of these steps corresponds concretely to some code.  Let's cut our way through the jungle.\n\nOur initial landing point in the C++ code is the C implementation of a Python function, which we've exposed to the Python side as something like torch._C.VariableFunctions.add.  THPVariable_add is the implementation of one such implementation.\nOne important thing to know about this code is that it is auto-generated.  If you search in the GitHub repository, you won't find it, because you have to actually build PyTorch to see it.  Another important thing is, you don't have to really deeply understand what this code is doing; the idea is to skim over it and get a sense for what it is doing.  Above, I've annotated some of the most important bits in blue: you can see that there is a use of a class PythonArgParser to actually pull out C++ objects out of the Python args and kwargs; we then call a dispatch_add function (which I've inlined in red); this releases the global interpreter lock and then calls a plain old method on the C++ Tensor self.  On its way back, we rewrap the returned Tensor back into a PyObject.\n(At this point, there's an error in the slides: I'm supposed to tell you about the Variable dispatch code.  I haven't fixed it here yet.  Some magic happens, then...)\n\nWhen we call the add method on the Tensor class, no virtual dispatch happens yet.  Instead, we have an inline method which calls a virtual method on a \"Type\" object.  This method is the actual virtual method (this is why I say Type is just a \"gadget\" that gets you dynamic dispatch.)  In the particular case of this example, this virtual call dispatches to an implementation of add on a class named TypeDefault.  This happens to be because we have an implementation of add that is the same for every device type (both CPU and CUDA); if we had happened to have different implementations, we might have instead landed on something like CPUFloatType::add.  It is this implementation of the virtual method that finally gets us to the actual kernel code.\n\nHopefully, this slide will be out-of-date very soon too; Roy Li is working on replacing Type dispatch with another mechanism which will help us better support PyTorch on mobile.\nIt's worth reemphasizing that all of the code, until we got to the kernel, is automatically generated.\n\nIt's a bit twisty and turny, so once you have some basic orientation about what's going on, I recommend just jumping straight to the kernels.\n\nPyTorch offers a lot of useful tools for prospective kernel writers.  In this section, we'll walk through a few of them.  But first of all, what do you need to write a kernel?\n\nWe generally think of a kernel in PyTorch consisting of the following parts:\n\nFirst, there's some metadata which we write about the kernel, which powers the code generation and lets you get all the bindings to Python, without having to write a single line of code.\nOnce you've gotten to the kernel, you're past the device type / layout dispatch. The first thing you need to write is error checking, to make sure the input tensors are the correct dimensions.  (Error checking is really important!  Don't skimp on it!)\nNext, we generally have to allocate the result tensor which we are going to write the output into.\nTime for the kernel proper.  At this point, you now should do the second, dtype dispatch, to jump into a kernel which is specialized per dtype it operates on.  (You don't want to do this too early, because then you will be uselessly duplicating code that looks the same in any case.)\nMost performant kernels need some sort of parallelization, so that you can take advantage of multi-CPU systems.  (CUDA kernels are \"implicitly\" parallelized, since their programming model is built on top of massive parallelization).\nFinally, you need to access the data and do the computation you wanted to do!\n\nIn the subsequent slides, we'll walk through some of the tools PyTorch has for helping you implementing these steps.\n\nTo take advantage of all of the code generation which PyTorch brings, you need to write a schema for your operator.  The schema gives a mypy-esque type of your function, and also controls whether or not we generate bindings for methods or functions on Tensor.  You also tell the schema what implementations of your operator should be called for given device-layout combinations.  Check out the README in native is for more information about this format.\n\nYou also may need to define a derivative for your operation in derivatives.yaml.\n\nError checking can be done by way of either a low level or a high level API.  The low level API is just a macro, TORCH_CHECK, which takes a boolean, and then any number of arguments to make up the error string to render if the boolean is not true.  One nice thing about this macro is that you can intermix strings with non-string data; everything is formatted using their implementation of operator<<, and most important data types in PyTorch have operator<< implementations.\nThe high level API saves you from having to write up repetitive error messages over and over again.  The way it works is you first wrap each Tensor into a TensorArg, which contains information about where the tensor came from (e.g., its argument name).  It then provides a number of pre-canned functions for checking various properties; e.g., checkDim() tests if the tensor's dimensionality is a fixed number.  If it's not, the function provides a user-friendly error message based on the TensorArg metadata.\n\nOne important thing to be aware about when writing operators in PyTorch, is that you are often signing up to write three operators: abs_out, which operates on a preallocated output (this implements the out= keyword argument), abs_, which operates inplace, and abs, which is the plain old functional version of an operator.\nMost of the time, abs_out is the real workhorse, and abs and abs_ are just thin wrappers around abs_out; but sometimes writing specialized implementations for each case are warranted.\n\nTo do dtype dispatch, you should use the AT_DISPATCH_ALL_TYPES macro.  This takes in the dtype of the tensor you want to dispatch over, and a lambda which will be specialized for each dtype that is dispatchable from the macro.  Usually, this lambda just calls a templated helper function.\nThis macro doesn't just \"do dispatch\", it also decides what dtypes your kernel will support.  As such, there are actually quite a few versions of this macro, which let you pick different subsets of dtypes to generate specializations for.  Most of the time, you'll just want AT_DISPATCH_ALL_TYPES, but keep an eye out for situations when you might want to dispatch to some more types.  There's guidance in Dispatch.h for how to select the correct one for your use-case.\n\nOn CPU, you frequently want to parallelize your code.  In the past, this was usually done by directly sprinkling OpenMP pragmas in your code.\n\nAt some point, we have to actually access the data.  PyTorch offers quite a few options for doing this.\n\nIf you just want to get a value at some specific location, you should use TensorAccessor.  A tensor accessor is like a tensor, but it hard codes the dimensionality and dtype of the tensor as template parameters.  When you retrieve an accessor like x.accessor<float, 3>();, we do a runtime test to make sure that the tensor really is this format; but after that, every access is unchecked.  Tensor accessors handle strides correctly, so you should prefer using them over raw pointer access (which, unfortunately, some legacy kernels do.)  There is also a PackedTensorAccessor, which is specifically useful for sending an accessor over a CUDA launch, so that you can get accessors from inside your CUDA kernel.  (One notable gotcha: TensorAccessor defaults to 64-bit indexing, which is much slower than 32-bit indexing in CUDA!)\nIf you're writing some sort of operator with very regular element access, for example, a pointwise operation, you are much better off using a higher level of abstraction, the TensorIterator.   This helper class automatically handles broadcasting and type promotion for you, and is quite handy.\nFor true speed on CPU, you may need to write your kernel using vectorized CPU instructions.  We've got helpers for that too!  The Vec256 class represents a vector of scalars and provides a number of methods which perform vectorized operations on them all at once.  Helpers like binary_kernel_vec then let you easily run vectorized operations, and then finish everything that doesn't round nicely into vector instructions using plain old instructions.  The infrastructure here also manages compiling your kernel multiple times under different instruction sets, and then testing at runtime what instructions your CPU supports, and using the best kernel in those situations.\n\nA lot of kernels in PyTorch are still written in the legacy TH style.  (By the way, TH stands for TorcH.  It's a pretty nice acronym, but unfortunately it is a bit poisoned; if you see TH in the name, assume that it's legacy.)  What do I mean by the legacy TH style?\n\nIt's written in C style, no (or very little) use of C++.\nIt's manually refcounted (with manual calls to THTensor_free to decrease refcounts when you're done using tensors), and\nIt lives in generic/ directory, which means that we are actually going to compile the file multiple times, but with different #define scalar_t.\n\nThis code is pretty crazy, and we hate reviewing it, so please don't add to it.  One of the more useful tasks that you can do, if you like to code but don't know too much about kernel writing, is to port some of these TH functions to ATen.\n\nTo wrap up, I want to talk a little bit about working efficiently on PyTorch.  If the largeness of PyTorch's C++ codebase is the first gatekeeper that stops people from contributing to PyTorch, the efficiency of your workflow is the second gatekeeper.  If you try to work on C++ with Python habits, you will have a bad time: it will take forever to recompile PyTorch, and it will take you forever to tell if your changes worked or not.\nHow to work efficiently could probably be a talk in and of itself, but this slide calls out some of the most common anti-patterns I've seen when someone complains: \"It's hard to work on PyTorch.\"\n\nIf you edit a header, especially one that is included by many source files (and especially if it is included by CUDA files), expect a very long rebuild.  Try to stick to editing cpp files, and edit headers sparingly!\nOur CI is a very wonderful, zero-setup way to test if your changes worked or not. But expect to wait an hour or two before you get back signal.  If you are working on a change that will require lots of experimentation, spend the time setting up a local development environment.  Similarly, if you run into a hard to debug problem on a specific CI configuration, set it up locally.  You can download and run the Docker images locally\nThe CONTRIBUTING guide explains how to setup ccache; this is highly recommended, because sometimes it will help you get lucky and avoid a massive recompile when you edit a header.  It also helps cover up bugs in our build system, when we recompile files when we shouldn't.\nAt the end of the day, we have a lot of C++ code, and you will have a much more pleasant experience if you build on a beefy server with CPUs and RAM.  In particular, I don't recommend doing CUDA builds on a laptop; building CUDA is sloooooow and laptops tend to not have enough juice to turnaround quickly enough.\n\nSo that's it for a whirlwind tour of PyTorch's internals!  Many, many things have been omitted; but hopefully the descriptions and explanations here can help you get a grip on at least a substantial portion of the codebase.\nWhere should you go from here?  What kinds of contributions can you make?  A good place to start is our issue tracker.  Starting earlier this year, we have been triaging issues; issues labeled triaged mean that at least one PyTorch developer has looked at it and made an initial assessment about the issue.  You can use these labels to find out what issues we think are high priority or look up issues specific to some module, e.g., autograd or find issues which we think are small (word of warning: we're sometimes wrong!)\nEven if you don't want to get started with coding right away, there are many other useful activities like improving documentation (I love merging documentation PRs, they are so great), helping us reproduce bug reports from other users, and also just helping us discuss RFCs on the issue tracker. PyTorch would not be where it is today without our open source contributors; we hope you can join us too!\n\n          May 16, 2019\n          PyTorch",
    "summary": {
      "en": "This text summarizes a talk on the internals of PyTorch, aimed at users interested in contributing to the library but feeling intimidated by its complex C++ codebase. \n\nKey points include:\n\n1. **Understanding Tensors**: Tensors are the main data structure in PyTorch, representing n-dimensional data. They consist of data, metadata (like size and type), and strides, which help locate elements in memory.\n\n2. **Tensor Storage and Views**: Tensors can share the same underlying storage, allowing different views without duplicating data. This is managed through strides that indicate how to access non-contiguous elements.\n\n3. **Automatic Differentiation**: PyTorch uses reverse-mode automatic differentiation to compute gradients for neural networks. This requires additional metadata in tensor operations.\n\n4. **Code Structure**: The PyTorch codebase has several key directories: \n   - **torch/** for Python modules,\n   - **torch/csrc/** for C++ code interfacing with Python,\n   - **aten/** for tensor operations,\n   - **c10/** for core abstractions.\n\n5. **Kernel Writing**: Writing kernels involves metadata setup, error checking, and proper data access. PyTorch provides tools for efficient kernel writing, including macros for error handling and dispatching.\n\n6. **Best Practices**: Efficient workflow tips include minimizing header edits, using CI for testing changes, and leveraging local development environments for faster iteration.\n\n7. **Contributions**: The text encourages contributions to PyTorch, including coding, documentation, and bug reporting, and points to the issue tracker for finding areas to help.\n\nOverall, the talk aims to demystify PyTorch's internals and guide users toward effective contributions.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "b6280d3a3041786d",
    "title": {
      "en": "Technicalities of Homeworld 2 Backgrounds",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://simonschreibt.de/gat/homeworld-2-backgrounds/",
    "score": 207,
    "by": "todsacerdoti",
    "time": 1742735655,
    "content": "I made a game! Buy on Steam\n\n\t\t\t\t\t\tA Game Art TrickHomeworld 2 – Backgrounds\n\nRussian Version by Eliiah\n\nKorean Version by Woodorl Kim\n\nWhat you see here\n\nis the stunning background art\n\nof one of the most beautiful sci-fi games.\n\n H o m e w o r l d 2\nThanks for reading.\nJust kidding. Of course i have something to say about this. In the company we look at the art of Homeworld from time to time and bow to the creators of this masterpiece. Once we talked about how great the background look and how interesting this sketched style is. There is something…some details seem…special to us.\n\nI mentioned, that this looks a bit like… a vertex color gradient. But they wouldn’t paint the background on geometry, right? I mean…that would has to be a highly tessellated sphere.\nThe discussion was over but I wasn’t satisfied and wanted at least see the textures. So i used some mod tools to extract the Homeworld 2 Demo data but there were no textures. Only some .HOD files. I used Google and found a thread how to generate these .HOD files from a .TGA. It was said:\n\n“…scans every pixel of the image then based on contrast\nit decides whether or not to add a new vertex and color…”\n\nWhat?\nCould it really be, that this is vertex color? Luckily you can watch at .HOD file with CFHodEdit. And another tool can force a wireframe mode. And now look what this brought to light:\nThis is one\n\nof the most brave\n\nsolutions for game art i ever saw.\n\nAnd here you can see how this influences the sky sphere geometry of the game. Do you see how low the resolution is in the low contrast areas? And how round the sphere is where details were painted?\n\nI never ever had thought, that this can produce such good results. Oh and don’t forget that this technique solves two major problems.\n#1 You don’t have any problems with DDS texture compression artifacts.\n#2 More important from composition perspective: since you can’t get too fine detail (it was said in the tutorial that the base TGA shouldn’t contain too sharp details), the background stays were it should:\nIn the background.\nToo often i see games where the background contains so much noise and details, that you can’t really separate fore-/midground from background.\nThe last time i saw this perfect combination of tech & composition was in Diablo 3. I talk about the 2.5D tree article.\nIf you want know more about how these spheres are generated, read my next article about this topic.\nThanks for reading.\n\nUpdate 1\n\nOskar Stålberg used the Homeworld-Background-Idea in his personal project which looks soooo gorgeous! :,)\nSource: Planet Demo by Oskar Stålberg\n\nUpdate 2\n\nChris Correia works on a space game and asked me about the stars in the Homeworld-Backgrounds because they are super-sharp. I remembered having seen a thread like this a while ago and here it is!\nIn fact, the stars are single textures/billboards:\nSource: Background Star Fields\n\n\t\t\t\t\t23 Replies\n\n\t\t\t\tWant more?\n\t\t\t\tRead more articles like this one by checking out the Game Art Trick Section! By the way: I've a Ko-fi, a Gumroad and a Discord. Have a very nice day! <3\n\n\t\t\tThis entry was posted in Game Art Tricks and tagged Backgrounds, Homeworld 2 on 15. March 2013 by Simon.",
    "summary": {
      "en": "The text discusses the impressive background art of the sci-fi game Homeworld 2. The author expresses admiration for the game's artistic style, particularly the way the backgrounds are created using a technique that generates vertex color gradients from images. This method avoids issues with texture compression artifacts and helps keep background details less distracting, allowing for better separation between foreground and background elements.\n\nThe author shares their experience of exploring the game's art further by extracting game data and analyzing it with tools. They highlight the effectiveness of this technique, comparing it to other games like Diablo 3. The article also mentions updates about other game developers inspired by Homeworld's art style.\n\nIn summary, the text celebrates the innovative art techniques used in Homeworld 2 and how they contribute to the game's beauty and composition.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "99e9a4ff0d6d0812",
    "title": {
      "en": "Landrun: Sandbox any Linux process using Landlock, no root or containers",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/Zouuup/landrun",
    "score": 503,
    "by": "Zoup",
    "time": 1742651819,
    "content": "landrun\nA lightweight, secure sandbox for running Linux processes using Landlock. Think firejail, but with kernel-level security and minimal overhead.\nLinux Landlock is a kernel-native security module that lets unprivileged processes sandbox themselves - but nobody uses it because the API is ... hard!\nLandrun is designed to make it practical to sandbox any command with fine-grained filesystem and network access controls. No root. No containers. No SELinux/AppArmor configs.\nIt's lightweight, auditable, and wraps Landlock v5 features (file access + TCP restrictions).\nFeatures\n\n🔒 Kernel-level security using Landlock\n🚀 Lightweight and fast execution\n🛡️ Fine-grained access control for directories\n🔄 Support for read and write paths\n⚡ Path-specific execution permissions\n🌐 TCP network access control (binding and connecting)\n\nDemo\n\nRequirements\n\nLinux kernel 5.13 or later with Landlock enabled\nLinux kernel 6.7 or later for network restrictions (TCP bind/connect)\nGo 1.18 or later (for building from source)\n\nInstallation\nQuick Install\ngo install github.com/zouuup/landrun/cmd/landrun@latest\n\nFrom Source\ngit clone https://github.com/zouuup/landrun.git\ncd landrun\ngo build -o landrun cmd/landrun/main.go\nsudo cp landrun /usr/local/bin/\n\nUsage\nBasic syntax:\nlandrun [options] <command> [args...]\n\nOptions\n\n--ro <path>: Allow read-only access to specified path (can be specified multiple times or as comma-separated values)\n--rox <path>: Allow read-only access with execution to specified path (can be specified multiple times or as comma-separated values)\n--rw <path>: Allow read-write access to specified path (can be specified multiple times or as comma-separated values)\n--rwx <path>: Allow read-write access with execution to specified path (can be specified multiple times or as comma-separated values)\n--bind-tcp <port>: Allow binding to specified TCP port (can be specified multiple times or as comma-separated values)\n--connect-tcp <port>: Allow connecting to specified TCP port (can be specified multiple times or as comma-separated values)\n--env <var>: Environment variable to pass to the sandboxed command (format: KEY=VALUE or just KEY to pass current value)\n--best-effort: Use best effort mode, falling back to less restrictive sandbox if necessary [default: disabled]\n--log-level <level>: Set logging level (error, info, debug) [default: \"error\"]\n\nImportant Notes\n\nYou must explicitly add the directory to the command you want to run with --rox flag\nFor system commands, you typically need to include /usr/bin, /usr/lib, and other system directories\nUse --rwx for directories where you need both write access and the ability to execute files\nNetwork restrictions require Linux kernel 6.7 or later with Landlock ABI v4\nBy default, no environment variables are passed to the sandboxed command. Use --env to explicitly pass environment variables\nThe --best-effort flag allows graceful degradation on older kernels that don't support all requested restrictions\nPaths can be specified either using multiple flags or as comma-separated values (e.g., --ro /usr,/lib,/home)\n\nEnvironment Variables\n\nLANDRUN_LOG_LEVEL: Set logging level (error, info, debug)\n\nExamples\n\nRun a command with read-only access to a directory:\n\nlandrun --rox /usr/ --ro /path/to/dir ls /path/to/dir\n\nRun a command with write access to a directory:\n\nlandrun --rox /usr/bin --ro /lib --rw /path/to/dir touch /path/to/dir/newfile\n\nRun a command with execution permissions:\n\nlandrun --rox /usr/ --ro /lib,/lib64 /usr/bin/bash\n\nRun with debug logging:\n\nlandrun --log-level debug --rox /usr/ --ro /lib,/lib64,/path/to/dir ls /path/to/dir\n\nRun with network restrictions:\n\nlandrun --rox /usr/ --ro /lib,/lib64 --bind-tcp 8080 --connect-tcp 80 /usr/bin/my-server\n\nThis will allow the program to only bind to TCP port 8080 and connect to TCP port 80.\n\nRun a DNS client with appropriate permissions:\n\nlandrun --log-level debug --ro /etc,/usr --rox /usr/ --connect-tcp 443 nc kernel.org 443\n\nThis allows connections to port 443, requires access to /etc/resolv.conf for resolving DNS.\n\nRun a web server with selective network permissions:\n\nlandrun --rox /usr/bin --ro /lib,/lib64,/var/www --rwx /var/log --bind-tcp 80,443 /usr/bin/nginx\n\nRunning anything without providing parameters is... maximum security jail!\n\nlandrun ls\n\nIf you keep getting permission denied without knowing what exactly going on, best to use strace with it.\n\nlandrun --rox /usr strace -f -e trace=all ls\n\nRun with specific environment variables:\n\nlandrun --rox /usr --ro /etc --env HOME --env PATH --env CUSTOM_VAR=my_value -- env\n\nThis example passes the current HOME and PATH variables, plus a custom variable named CUSTOM_VAR.\nSecurity\nlandrun uses Linux's Landlock to create a secure sandbox environment. It provides:\n\nFile system access control\nDirectory access restrictions\nExecution control\nTCP network restrictions\nProcess isolation\n\nLandlock is an access-control system that enables processes to securely restrict themselves and their future children. As a stackable Linux Security Module (LSM), it creates additional security layers on top of existing system-wide access controls, helping to mitigate security impacts from bugs or malicious behavior in applications.\nLandlock Access Control Rights\nlandrun leverages Landlock's fine-grained access control mechanisms, which include:\nFile-specific rights:\n\nExecute files (LANDLOCK_ACCESS_FS_EXECUTE)\nWrite to files (LANDLOCK_ACCESS_FS_WRITE_FILE)\nRead files (LANDLOCK_ACCESS_FS_READ_FILE)\nTruncate files (LANDLOCK_ACCESS_FS_TRUNCATE) - Available since Landlock ABI v3\n\nDirectory-specific rights:\n\nRead directory contents (LANDLOCK_ACCESS_FS_READ_DIR)\nRemove directories (LANDLOCK_ACCESS_FS_REMOVE_DIR)\nRemove files (LANDLOCK_ACCESS_FS_REMOVE_FILE)\nCreate various filesystem objects (char devices, directories, regular files, sockets, etc.)\nRefer/reparent files across directories (LANDLOCK_ACCESS_FS_REFER) - Available since Landlock ABI v2\n\nNetwork-specific rights (requires Linux 6.7+ with Landlock ABI v4):\n\nBind to specific TCP ports (LANDLOCK_ACCESS_NET_BIND_TCP)\nConnect to specific TCP ports (LANDLOCK_ACCESS_NET_CONNECT_TCP)\n\nLimitations\n\nLandlock must be supported by your kernel\nNetwork restrictions require Linux kernel 6.7 or later with Landlock ABI v4\nSome operations may require additional permissions\nFiles or directories opened before sandboxing are not subject to Landlock restrictions\n\nKernel Compatibility Table\n\nFeature\nMinimum Kernel Version\nLandlock ABI Version\n\nBasic filesystem sandboxing\n5.13\n1\n\nFile referring/reparenting control\n5.19\n2\n\nFile truncation control\n6.2\n3\n\nNetwork TCP restrictions\n6.7\n4\n\nIOCTL on special files\n6.10\n5\n\nTroubleshooting\nIf you receive \"permission denied\" or similar errors:\n\nEnsure you've added all necessary paths with --ro or --rw\nTry running with --log-level debug to see detailed permission information\nCheck that Landlock is supported and enabled on your system:\ngrep -E 'landlock|lsm=' /boot/config-$(uname -r)\n# alternatively, if there are no /boot/config-* files\nzgrep -iE 'landlock|lsm=' /proc/config.gz\n# another alternate method\ngrep -iE 'landlock|lsm=' /lib/modules/$(uname -r)/config\n\nYou should see CONFIG_SECURITY_LANDLOCK=y and lsm=landlock,... in the output\nFor network restrictions, verify your kernel version is 6.7+ with Landlock ABI v4:\nuname -r\n\nTechnical Details\nImplementation\nThis project uses the landlock-lsm/go-landlock package for sandboxing, which provides both filesystem and network restrictions. The current implementation supports:\n\nRead/write/execute restrictions for files and directories\nTCP port binding restrictions\nTCP port connection restrictions\nBest-effort mode for graceful degradation on older kernels\n\nBest-Effort Mode\nWhen using --best-effort (disabled by default), landrun will gracefully degrade to using the best available Landlock version on the current kernel. This means:\n\nOn Linux 6.7+: Full filesystem and network restrictions\nOn Linux 6.2-6.6: Filesystem restrictions including truncation, but no network restrictions\nOn Linux 5.19-6.1: Basic filesystem restrictions including file reparenting, but no truncation control or network restrictions\nOn Linux 5.13-5.18: Basic filesystem restrictions without file reparenting, truncation control, or network restrictions\nOn older Linux: No restrictions (sandbox disabled)\n\nFuture Features\nBased on the Linux Landlock API capabilities, we plan to add:\n\n🔒 Enhanced filesystem controls with more fine-grained permissions\n🌐 Support for UDP and other network protocol restrictions (when supported by Linux kernel)\n🔄 Process scoping and resource controls\n🛡️ Additional security features as they become available in the Landlock API\n\nAcknowledgements\nThis project wouldn't exist without:\n\nLandlock, the kernel security module enabling unprivileged sandboxing - maintained by @l0kod\ngo-landlock, the Go bindings powering this tool - developed by @gnoack\n\nLicense\nThis project is licensed under the GNU General Public License v2\nContributing\nContributions are welcome! Please feel free to submit a Pull Request.",
    "summary": {
      "en": "**Landrun Summary**\n\nLandrun is a lightweight and secure tool for running Linux processes in a sandbox using the Landlock security module. It allows unprivileged processes to create their own secure environments without requiring root access or complex configurations. Here are the key features and details:\n\n### Key Features:\n- **Kernel-level security**: Uses Landlock for enhanced security.\n- **Lightweight**: Minimal performance overhead.\n- **Fine-grained access control**: Manage permissions for file system and network access.\n- **No root or containers needed**: Simple setup with no need for complex security frameworks like SELinux or AppArmor.\n\n### Requirements:\n- Linux kernel 5.13 or later with Landlock enabled.\n- Go 1.18 or later for building from source.\n\n### Installation:\n- Quick installation: `go install github.com/zouuup/landrun/cmd/landrun@latest`\n- From source: Clone the repository, build it, and copy the executable.\n\n### Usage:\nRun commands with specific permissions using options like:\n- `--ro <path>`: Read-only access.\n- `--rw <path>`: Read-write access.\n- `--bind-tcp <port>`: Allow binding to a TCP port.\n\n### Security Features:\n- Controls access to files and directories.\n- Isolates processes to prevent unauthorized access.\n- Supports TCP network restrictions for enhanced security.\n\n### Limitations:\n- Requires a compatible Linux kernel.\n- Some operations may require additional permissions.\n\n### Troubleshooting:\nIf you encounter permission errors, check that you've specified all necessary paths and that your kernel supports Landlock.\n\n### Future Plans:\nFuture updates may include enhanced filesystem controls, support for more network protocols, and additional security features.\n\nThis project is open for contributions and is licensed under the GNU General Public License v2.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "7ebe6c4ba0f90ac6",
    "title": {
      "en": "Derivatives and Logarithms of 3D Transforms",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://nosferalatu.com/./DerivativesLogarithmsTransforms.html",
    "score": 104,
    "by": "nosferalatu123",
    "time": 1742515596,
    "content": "Derivatives, Logarithms, andTransforms\n\n    Date\n\n         Tue 07 February 2023\n\nTags\n\tProgramming\n\n                Given a transform TTT and a point x, we can find the transformed point with T∗xT∗xT * x. But what if we want to smoothly interpolate TTT so it moves xxx along the path from its initial position to its position transformed by TTT?\nWhat we want to find is the point xxx at time ttt:\nx(t)=T(t)∗x(0)x(t)=T(t)∗x(0)x(t) = T(t) *x(0)\nwhere x(0)x(0)x(0) is the point’s initial position, and T(t)T(t)T(t) is the transform at time ttt. Since we have only a single transform TTT, we need to find a way to interpolate it overtime.\nOne way to accomplish this is to raise TTT to the power of ttt, which can be done using the exponential and logarithm of the transform. Interestingly, the logarithm of a transform can also be used to easily find the velocity of a point xxx in space: the velocity vector (also called the tangent vector) is just log(T)∗xlog(T)∗xlog(T) * x. This blog post shows the relationship between the logarithm andvelocity.\nExample\nCheck out this interactive example to see how the vector field changes as you manipulate the gizmo to translate and rotate the transform. The vector field represents the velocity vector at each point in space during thetransformation.\nAs you move the gizmo, you’ll notice a white curve that traces the path from the origin to the gizmo’s transform. Along this curve, you’ll see the interpolated transform as it travels from the origin to the gizmo. As you can see, the interpolation follows the flow of the velocity vector field. The applet’s code is using the exponential and logarithm of the transform to compute the curve, interpolated transform, and vectorfield.\n\n   ControlsGizmoModeRotateTranslateRotateShowVectorFieldShowOriginAxesShowGizmoShowInterpolatedTransform\n\nThe source code for the applet can be found here, which includes an implementation of closed-form log() and exp() for rigid bodytransforms.\nNext, I’ll describe how to compute the interpolated transform and the velocity vector field you see in thisexample.\nWhat’s T(t)T(t)T(t)?\nWe have TTT, but not T(t)T(t)T(t), which changes with time. Assuming that multiplying two transforms represents the composition of those transforms, we can find T(t)T(t)T(t) bysaying\nT(0)=IT(0)=IT(0) = I (the identitytransform)\nT(1)=TT(1)=TT(1) =T\nT(2)=T∗TT(2)=T∗TT(2) = T *T\nT(3)=T∗T∗TT(3)=T∗T∗TT(3) = T * T *T\nMore generally, we can find TTT at any time bysaying\nT(t)=TtT(t)=TtT(t) = T^t.\nThe above trick is from a blog post by Fabian Giesen here but works for any transform that uses multiplication forcomposition.\nNow that we know T(t)=TtT(t)=TtT(t) = T^t, the original equation can be rearrangedto\nx(t)=Tt∗x(0)x(t)=Tt∗x(0)x(t) = T^t * x(0).\nWhat’s TtTtT^t?\nTo compute TtT^t, we need to use the matrix exponential and matrixlogarithm.\nLet’s start with two facts about a matrixX:\nelog(X)=Xe^{log(X)} = Xand\nlog(Xy)=log(X)∗ylog(X^y) = log(X) * y.\nPut together, we can saythat\nTt=elog(Tt)=elog(T)∗tT^t = e^{log(T^t)} =e^{log(T)*t}\nwhich we can plug into the earlier equation, givingus\nx(t)=elog(T)∗t∗x(0)x(t) = e^{log(T) * t} * x(0).\nThis says that to find the point x at time t, find the transform at time t using elog(T)∗te^{log(T) * t}, and use that to transform the point at its initial position (at time0).\nWhat’s thederivative?\nIn calculus, we learnedthat\nddteat=aeat\\dfrac{d}{dt}e^{a t} = a e^{at}\nwhich holds true for matrices aswell:\nddteAt=AeAt\\dfrac{d}{dt}e^{A t} = A e^{At}\nThis relationship is explained in more detail in the section the derivative of the matrix exponential.\nWe can use this property to find the derivative of our earlier equation x(t)=elog(T)tx(0)x(t) = e^{log(T)t} x(0) with respect tot:\nddtx(t)=log(T)elog(T)tx(0)\\dfrac{d}{dt}x(t) = log(T) e^{log(T) t} x(0).\nThis equation states that to find the first derivative (the velocity vector, also called the tangent vector) of the point at time t, you first transform the point’s initial position x(0)x(0) with the interpolated transform elog(T)te^{log(T)t} and then multiply it by the logarithm of the transform log(T)log(T). This expression follows the right-to-left convention of column vectors, so you would start with the initial position x(0)x(0), then apply the interpolated transform elog(T)te^{log(T)t}, and finally multiply by the logarithm log(T)log(T).\nelog(T)te^{log(T) t} acts as an operator that maps points from their initial position to their new position at time t. The matrix exponential can be thought of as like integration. At time 0, elog(T)te^{log(T) t} is the identity matrix (e0=Ie^0=I for matrix exponentials), and at time 1.0, elog(T)te^{log(T) t} is equal to the original transform matrix T (elog(T)=Te^{log(T)}=T).\nWhat’s this allmean?\nIf we take the equation at the end of “What’s TtT^t?”\nx(t)=elog(T)tx(0)x(t) = e^{log(T) t}x(0)\nand substitute that into the equation at the end of “What’s thederivative?”\nddtx(t)=log(T)elog(T)tx(0)\\dfrac{d}{dt}x(t) = log(T) e^{log(T) t} x(0),\nthen wehave:\nddtx(t)=log(T)x(t)\\dfrac{d}{dt}x(t) = log(T) x(t).\nThis relates the derivative of a moving point to the logarithm of the transformation moving thatpoint.\nOne way to think of log(T)log(T) is as a vector field of tangent vectors for the transformation. In other words, it’s the field of first derivatives. This vector field is independent of time and shows the velocity for every point inspace.\nThat equation is saying that if you transform any point in space by the logarithm of the transform, you will get the first derivative at that point. The first derivative is the velocity, so log(T)log(T) defines the velocity field (the field of tangent vectors at every point inspace).\nAs a point moves through space by the transform, it forms a curve. The tangent vector at time t is tangent to the point’s position on the curve at timet.\nYou can think of the logarithm of a matrix as the velocity field of the action performed by that matrix. The velocity field visualized in the interactive example above is thisfield.\nA more informal way of looking at this is tosay\nvelocity=log(transform)∗positionvelocity = log(transform) *position\nmeaning, to understand how a point will move in time, look at the vector field of the log of the transform as a velocity field. As the point flows along that velocity field, it moves intime.\nWhat’s the differentialequation?\nWe can also reformulate all of this as a differential equation. Earlier, wehad\nddtx(t)=log(T)x(t)\\dfrac{d}{dt} x(t) = log(T)x(t)\nwhich is a differential equation. Because log(T)log(T) is a matrix, it is more specifically a matrix differentialequation.\nScalar ordinary differential equations of theform\ny′(t)=ay(t)y'(t)=ay(t)\nhave the generalsolution\ny(t)=eaty(0)y(t)=e^{at}y(0).\nSimilarly, matrix differential equations of theform\nx′(t)=Ax(t)x'(t)=Ax(t)\nhave the generalsolution\nx(t)=eAtx(0)x(t)=e^{At}x(0).\nTherefore, given our equation fromearlier\nddtx(t)=log(T)x(t)\\dfrac{d}{dt} x(t) = log(T)x(t)\nwe have thesolution\nx(t)=elog(T)tx(0)x(t) = e^{log(T) t} x(0).\nThis is the same as our original equation, but we started with a differential equation and found a solution. To prove this solution is correct, just take the derivative of it, which is what we did earlier in the What’s the derivative?section.\nThe exponential map and logarithmmap\nThe exponential map is defined as the infiniteseries\neAt=I+At+12(At)2+13!(At)3+...=∞∑i=0(At)ii!e^{A t} = I + A t + \\frac{1}{2}(A t)^2 + \\frac{1}{3!}(A t)^3 + ... = \\sum_{i=0}^{\\infty} \\frac{(At)^i}{i!}\nand can be used to find the exponential of real numbers, complex numbers, quaternions, matrices, and more. For example, when square matrices are plugged in to the series, the result is called the matrix exponential.\nSimilarly, the logarithm is defined as the infiniteseries\nlog(A)=∞∑i=1(−1)i+1(A−I)iilog(A) = \\sum_{i=1}^{\\infty} (-1)^{i+1} \\frac{(A - I)^i}{i}\nIf you want to know more, search for the exponential map and logarithm map. You’ll find that these are important concepts in Lie group theory. The exponential map and logarithm map are inverses of each other. In Lie theory, the exponential map maps a tangent vector at a point p to a point on the manifold. The logarithm map does the opposite, mapping a point on the manifold back to the tangent vector atp.\nWhen reading about Lie groups, you’ll come across many different kinds of groups. There are only a few groups that are related to transforms, though. SO(3) is a 3D rotation matrix, SU(2) is a quaternion, SE(3) is a 3D rigid body transform (rotation and translation), SIM(3) is rotation, translation, and (positive) uniform scale, and GL(n) is an nxnmatrix.\nThere are several options for how to practically compute the exponential and logarithm map for a matrix or otherobject:\n1) Use a math library like Eigen or Armadillo. These have functions to compute the matrix exponential and matrixlogarithm.\n2) The library Sophus has code for a closed form exp/log for the groups SO(3), SE(3), and SIM(3). Beware that it clamps its quaternions to a 3D rotation angle in -π\\pi … +π\\pithough.\n3) There is an excellent PDF at the web site of Ethan Eade here which contains the closed form equations for the groups SO(3), SE(3), and SIM(3).\n4) Compute the matrix exponential and logarithm by using the infinite series definitions above, and truncating after some number of terms. In my experience, this is not robust when working with floating point numbers, as you quickly start to deal with very small and very large numbers, depending on your inputmatrix.\n5) Compute the exponential with numerical integration. Given a starting point xx, integrating it for time t is the same thing as the exponential. There are many ways to compute numerical integration, from Euler to Runge-Kutta to adaptivemethods.\nPitfalls\nThere are a few issues that you should be awareof.\nPitfall#1\nThe logarithm of a rotation matrix will return a 3D rotation angle in -π\\pi … +π\\pi. More technically, there are an infinite number of logarithms for a matrix, each corresponding to a rotation angle that is  2π\\pi greater than the previous one. Generally matrix logarithm code will return the principal logarithm, which is the logarithm in -π\\pi … +π\\pi. This can lead to discontinuities when interpolating transforms with rotations in them, such rotations from human joints (you can move your head from looking over your left shoulder to over your right shoulder and rotate a little more than 180degrees).\nOn the other hand, the logarithm of a quaternion returns a 3D rotation angle in the larger range of -2π\\pi … +2π\\pi, which makes quaternions nicer to workwith.\nPitfall#2\nWhen working with logarithms, be aware that that theproperty\nlog(AB)=log(A)+log(B)log(AB) = log(A) +log(B)\nis only true when A and B commute, which is not the case for most transforms. Real numbers always commute, though, so the property does apply to them. It’s tempting to apply the property to transforms, but it’s important to remember it only applies when A and Bcommute.\nPitfall#3\nRelated to pitfall #2, you might want to interpolate two transforms A and Bwith\ninterpolate(A,B,t)=e(1−t)∗log(A)+t∗log(B)interpolate(A, B, t) = e^{(1-t)*log(A) +t*log(B)}\nBut be careful: this only works if A and B commute, which is not usually the case for transforms. Otherwise, this interpolation is neither shortest path nor constantspeed.\nInstead, interpolate the relative (also called delta) transform from A to B, likethis:\ninterpolate(A,B,t)=elog(BA−1)tAinterpolate(A, B, t) = e^{log(B A^{-1}) t}A\nHowever, this method only works for interpolating between two transforms and not for blending more than twotransforms.\nVisualizing a matrix as a vectorfield\nIf you are wondering how you can visualize a matrix as a vector field, an eloquent explanation is in 3Blue1Brown’s video about matrix exponentiation. This part about matrices as vector fields explains that verywell:\n\nThe derivative of the matrixexponential\nEarlier we used the property ddteAt=AeAt\\dfrac{d}{dt}e^{A t} = A e^{A t}. It’s not obvious why this property is true, but it’s an important part of unlocking all ofthis.\nA good reference for this derivation is in the textbook Modern Robotics. A free copy of that book can be found here. See equation (3.43) in thatbook.\nThe matrix exponential is definedas\neAt=I+At+12(At)2+13!(At)3+...e^{A t} = I + A t + \\frac{1}{2}(A t)^2 + \\frac{1}{3!}(A t)^3 +...\nWhat then is ddteAt\\dfrac{d}{dt}e^{A t}? If we take the derivative of each term of the matrix exponential’s expanded definition, wehave\nddteAt=0+A+A2t+12A3t2+...\\dfrac{d}{dt}e^{A t} = 0 + A + A^2 t + \\frac{1}{2} A^3 t^2 +...\nPull out A, and then wehave\nddteAt=A∗(I+At+12(At)2+...)=AeAt\\dfrac{d}{dt}e^{A t} = A*(I + A t + \\frac{1}{2} (A t)^2 + ...) = A e^{A t}.\nIt’s worth noting that the matrix AA can go on the left or right, and it always holds truethat\nAeAt=eAtAAe^{A t} = e^{At}A\nfor any square matrix, as stated in equation (3.44) in ModernRobotics.\nComments\nLeave comments on this post with Github Issues here.\nif (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {\n    var align = \"center\",\n        indent = \"0em\",\n        linebreak = \"false\";\n\n    if (false) {\n        align = (screen.width < 768) ? \"left\" : align;\n        indent = (screen.width < 768) ? \"0em\" : indent;\n        linebreak = (screen.width < 768) ? 'true' : linebreak;\n    }\n\n    var mathjaxscript = document.createElement('script');\n    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';\n    mathjaxscript.type = 'text/javascript';\n    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';\n\n    var configscript = document.createElement('script');\n    configscript.type = 'text/x-mathjax-config';\n    configscript[(window.opera ? \"innerHTML\" : \"text\")] =\n        \"MathJax.Hub.Config({\" +\n        \"    config: ['MMLorHTML.js'],\" +\n        \"    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" +\n        \"    jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" +\n        \"    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" +\n        \"    displayAlign: '\"+ align +\"',\" +\n        \"    displayIndent: '\"+ indent +\"',\" +\n        \"    showMathMenu: true,\" +\n        \"    messageStyle: 'normal',\" +\n        \"    tex2jax: { \" +\n        \"        inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" +\n        \"        displayMath: [ ['$$','$$'] ],\" +\n        \"        processEscapes: true,\" +\n        \"        preview: 'TeX',\" +\n        \"    }, \" +\n        \"    'HTML-CSS': { \" +\n        \"        availableFonts: ['STIX', 'TeX'],\" +\n        \"        preferredFont: 'STIX',\" +\n        \"        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" +\n        \"        linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" +\n        \"    }, \" +\n        \"}); \" +\n        \"if ('default' !== 'default') {\" +\n            \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n            \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n        \"}\";\n\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);\n}",
    "summary": {
      "en": "### Summary of \"Derivatives, Logarithms, and Transforms\"\n\nIn this text, the author discusses how to smoothly interpolate a transform (T) over time to move a point (x) from its initial position to its transformed position. The key formula for this is:\n\n\\[ x(t) = T(t) * x(0) \\]\n\nwhere \\( T(t) \\) changes over time and \\( x(0) \\) is the initial position of the point.\n\nTo interpolate the transform over time, the author suggests using the exponential and logarithm of the transform. The logarithm can also help compute the velocity of the point:\n\n\\[ \\text{velocity} = \\log(T) * x \\]\n\nAn interactive example is provided to visualize how the point moves along a path as the transform changes, demonstrating the relationship between the transform and the velocity vector field.\n\n### Key Concepts:\n1. **Matrix Exponential and Logarithm**: These mathematical concepts are used to compute the transform over time and derive the velocity vector.\n2. **Differential Equation**: The relationship between the transform and its derivative can be expressed as a matrix differential equation:\n   \\[ \\frac{d}{dt} x(t) = \\log(T) * x(t) \\]\n   This means the velocity of the point is related to the logarithm of the transform.\n3. **Interpolation**: The author explains how to interpolate between two transforms, emphasizing the need for caution since the logarithm property only holds for commuting transforms.\n4. **Pitfalls**: The author warns of potential issues when working with matrix logarithms and rotations, as well as the limitations of certain interpolation methods.\n\n### Practical Application:\nTo compute the matrix exponential and logarithm, the author suggests using mathematical libraries or numerical integration methods, and provides resources for further exploration.\n\nOverall, this text emphasizes the mathematical foundations of transforms and their applications in visualizing motion and velocity in space.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "edeff629e4f65bc4",
    "title": {
      "en": "Apple Now Selling USB-C to 3.5mm Audio Cable",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.macrumors.com/guide/apple-selling-usb-c-to-3-5mm-audio-cable/",
    "score": 5,
    "by": "walterbell",
    "time": 1742881259,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "edeff629e4f65bc4",
    "title": {
      "en": "Apple Now Selling USB-C to 3.5mm Audio Cable",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.macrumors.com/guide/apple-selling-usb-c-to-3-5mm-audio-cable/",
    "score": 5,
    "by": "walterbell",
    "time": 1742881259,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  }
]