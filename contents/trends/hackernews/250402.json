[
  {
    "id": "8e6c2ef3fc84e92c",
    "title": {
      "en": "Electron Band Structure in Germanium, My Ass",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://pages.cs.wisc.edu/~kovar/hall.html",
    "score": 272,
    "by": "tux3",
    "time": 1743510312,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "a575f22893580738",
    "title": {
      "en": "Bletchley code breaker Betty Webb dies aged 101",
      "ko": "베틀리의 전설, 베티 웨브 101세로 별세",
      "ja": "ベッチリーの英雄、101歳で逝去"
    },
    "type": "story",
    "url": "https://www.bbc.com/news/articles/c78jd30ywv8o",
    "score": 111,
    "by": "danso",
    "time": 1743512128,
    "content": "Bletchley code breaker Betty Webb dies aged 1014 hours agoShareSaveAida FofanaBBC News, West MidlandsShareSaveBBCBletchley Park code breaker Betty Webb has died at the age of 101A decorated World War Two code breaker who spent her youth deciphering enemy messages at Bletchley Park has died at the age of 101.Charlotte \"Betty\" Webb MBE - who was among the last surviving Bletchley code breakers - died on Monday night, the Women's Royal Army Corps Association confirmed.Mrs Webb, from Wythall in Worcestershire, joined operations at the Buckinghamshire base at the age of 18, later going on to help with Japanese codes at The Pentagon in the US. She was awarded France's highest honour - the Légion d'Honneur - in 2021. The Women's Royal Army Corps Association described Mrs Webb as a woman who \"inspired women in the Army for decades\".Bletchley Park Trust CEO Iain Standen said Mrs Webb will not only be remembered for her work but \"also for her efforts to ensure that the story of what she and her colleagues achieved is not forgotten.\"\"Betty's passion for preserving the history and legacy of Bletchley Park has undoubtedly inspired many people to engage with the story and visit the site,\" he said in a statement.Tributes to Mrs Webb have begun to be posted on social media, including one from historian and author Dr Tessa Dunlop who said she was with her in her final hours.Describing Mrs Webb as \"the very best\", she said on X: \"She is one of the most remarkable woman I have ever known.\"Mrs Webb told the BBC in 2020 that she had \"never heard of Bletchley\", Britain's wartime code-breaking centre, before starting work there as a member of the ATS, the Auxiliary Territorial Service.She had been studying at a college near Shrewsbury, Shropshire, when she volunteered as she said she and others on the course felt they \"ought to be serving our country rather than just making sausage rolls\".Her mother had taught her to speak German as a child and ahead of her posting remembered being \"taken into the mansion [at Bletchley] to read the Official Secrets Act\".\"I realised that from then on there was no way that I was going to be able to tell even my parents where I was and what I was doing until 1975 [when restrictions were lifted],\" she recalled.She would tell the family with whom she lodged that she was a secretary.Listen on BBC Sounds: Mrs Webb went to work at Bletchley Park when she was 18When the War ended in Europe in May of 1945, she went to work at the Pentagon after spending four years at Bletchley, which with its analysis of German communications had served as a vital cog in the Allies' war machine.At the Pentagon she would paraphrase and transcribe already-decoded Japanese messages. She said she was the only member of the ATS to be sent to Washington, describing it as a \"tremendous honour\".Mrs Webb, in 2020, recalled she had had no idea the Americans planned to end the conflict by dropping atomic weapon on Japanese cities, describing the weapons' power as \"utterly awful\"After the Allies' final victory, it took Mrs Webb several months to organise return passage to the UK, where she worked as a secretary at a school in Shropshire.The head teacher there had also worked at Bletchley so knew of her professionalism, whereas other would-be employers, she recalled, were left stumped by her being unable to explain - due to secrecy requirements - her previous duties.More than half a century later, in 2021, Mrs Webb was one of 6,000 British citizens to receive the Légion d'Honneur, following a decision by President François Hollande in 2014 to recognise British veterans who helped liberate France.PA MediaBetty Webb, seen in the front row in a red suit, was invited to the CoronationIn 2023, she and her niece were among 2,200 people from 203 countries invited to Westminster Abbey to see King Charles III's coronation.The same year she celebrated her 100th birthday at Bletchley Park with a party. She and her guests were treated to a fly-past by a Lancaster bomber. She said at the time: \"It was for me - it's unbelievable isn't it? Little me.\"More on this storyCodebreaker Betty Webb reveals D-Day confidenceEx-Bletchley Park worker, 98, given French honourWartime code-breaker invited to coronationRelated internet linksWomen’s Royal Army Corps AssociationBletchley Park Bletchley ParkWythall",
    "summary": {
      "en": "Betty Webb, a celebrated code breaker from Bletchley Park during World War II, has passed away at the age of 101. She was one of the last surviving code breakers and contributed significantly to deciphering enemy messages, including Japanese codes at the Pentagon. Born Charlotte \"Betty\" Webb MBE, she started working at Bletchley Park at 18 and was awarded France's highest honor, the Légion d'Honneur, in 2021.\n\nMrs. Webb was known for inspiring women in the Army and dedicated her efforts to preserving the history of Bletchley Park. She initially volunteered to serve her country while studying and kept her work secret until 1975. After the war, she worked as a secretary in Shropshire, where she faced challenges due to her classified background.\n\nIn her later years, she celebrated her 100th birthday at Bletchley Park and attended the coronation of King Charles III in 2023. Tributes have poured in, highlighting her remarkable life and contributions.",
      "ko": "제2차 세계대전 중 블렛클리 파크에서 유명한 암호 해독가인 베티 웹이 101세의 나이로 세상을 떠났습니다. 그녀는 마지막 남은 암호 해독가 중 한 명으로, 적의 메시지, 특히 펜타곤에서 일본 암호를 해독하는 데 큰 기여를 했습니다. 본명은 샬롯 \"베티\" 웹 MBE로, 18세에 블렛클리 파크에서 일하기 시작했으며, 2021년에는 프랑스의 최고 훈장인 레지옹 도뇌르를 수여받았습니다.\n\n웹 여사는 군대 내 여성들에게 영감을 주는 인물로 알려져 있으며, 블렛클리 파크의 역사를 보존하는 데 힘썼습니다. 그녀는 학업을 병행하며 자원봉사로 국가에 봉사하기 시작했고, 1975년까지 자신의 작업을 비밀로 유지했습니다. 전쟁이 끝난 후에는 슈롭셔에서 비서로 일했지만, 기밀 배경으로 인해 어려움을 겪기도 했습니다.\n\n그녀는 말년에는 블렛클리 파크에서 100세 생일을 축하했으며, 2023년에는 찰스 3세의 즉위식에도 참석했습니다. 그녀의 놀라운 삶과 기여를 기리는 추모가 이어지고 있습니다.",
      "ja": "第二次世界大戦中、ブレッチリー・パークで活躍した著名な暗号解読者、ベティ・ウェッブさんが101歳で亡くなりました。彼女は最後の生存者の一人であり、敵のメッセージ、特にペンタゴンでの日本の暗号の解読に大きく貢献しました。シャーロット・「ベティ」・ウェッブさんは、18歳でブレッチリー・パークでの仕事を始め、2021年にはフランスの最高の栄誉であるレジオンドヌール勲章を受賞しました。\n\nウェッブさんは、軍隊の女性たちにインスピレーションを与える存在として知られ、ブレッチリー・パークの歴史を守るために尽力しました。彼女は学生時代に国に奉仕するためにボランティアとして参加し、1975年までその仕事を秘密にしていました。戦後はシュロップシャーで秘書として働きましたが、機密の経歴のために困難に直面しました。\n\n晩年には、ブレッチリー・パークで100歳の誕生日を祝ったり、2023年にはチャールズ3世の戴冠式に出席したりしました。彼女の素晴らしい人生と貢献を称える追悼の言葉が多く寄せられています。"
    }
  },
  {
    "id": "51367c7e5215f98e",
    "title": {
      "en": "Show HN: Terminal dashboard that throttles my PC during peak electricity rates",
      "ko": "전기요금 절약 대시보드",
      "ja": "電力ピーク時のPC制御ダッシュボード"
    },
    "type": "story",
    "url": "https://www.naveen.ing/cli-for-smartplugs/",
    "score": 13,
    "by": "naveen_k",
    "time": 1743520679,
    "content": "WattWise: Terminal-Based Power Monitoring Using Smart Plugs    March 31, 2025\n\n  CLI dashboard showing real-time and historical power draw readings from a Kasa smart plug\n\nThe Challenge: Performance vs. Power Costs\nHigh-performance computing often means high electricity bills, especially in regions with time-of-use pricing. Finding the balance between computational power and energy efficiency becomes critical when running resource-intensive workloads.\nUnderstanding Power Usage\nI have been setting up a workstation for off-loading compute-intensive LLM workflows from my desktop. The build includes dual Epyc CPUs with plans to add 4 GPUs for up to 128GB VRAM to run multiple agents in parallel. A key constraint was ensuring the system could run comfortably on a standard household 120V outlet.\nSince I already use smart plugs throughout my home, I added one to monitor the workstation’s power consumption. However, checking power statistics using the kasa phone app or Home Assistant dashboard proved cumbersome, especially when I already maintain a terminal window with monitoring tools like htop, nvtop, and nload in a 2×2 grid on my secondary display.\n\n  TP-link Kasa EP25 Smart Plug\n\nBuilding a Terminal-Based Solution\nAfter sketching a simple wireframe for what I wanted: a clean, terminal-based UI that would display power consumption data already being collected by my Home Assistant instance through the TP-Link integration. After getting the data access sorted, I spent some time with ‘Claude 3.7 Sonnet Thinking’ iterating on the CLI structure and features.\nThe result: WattWise, a lightweight CLI tool that pulls power usage data from smart plugs (either directly or through Home Assistant) and presents it in a clean, information-dense dashboard right in the terminal.\n\n  WattWise Demo\n\nKey Features\nMonitoring Features\n\nReal-time power monitoring with wattage and current display\nColor-coded power values (green for low usage, yellow for medium, red for high)\nHistorical consumption charts directly in the terminal\nWhile smart plug measurements aren’t lab-grade (typically ±1-3% accuracy), they’re more than sufficient for practical usage monitoring\n\nPower Management Features\n\nAutomatic CPU/GPU throttling based on time-of-use electricity pricing\nConfigurable power thresholds and performance profiles\nSimple configuration through an interactive setup process\n\nDeployment Options\n\nDirect installation from source code\nDocker support for containerized deployment\nConnection options for both direct Kasa smart plug access and Home Assistant integration\n\nDynamic Power Management\nSince my utility provider uses Time of Use (ToU) pricing. Over the years, I’ve built various home automations to minimize electricity usage during peak hours. With my workstation potentially drawing up to 1400W at full load, it made sense to add automatic CPU and GPU throttling during expensive rate periods.\n\n  Power Optimizer: System architecture\n\nMy testing showed that reducing CPU frequency from 3700MHz to 1500MHz saved approximately 225W under load on my specific setup, though results will vary by hardware. The power optimizer service dynamically adjusts clock speeds based on:\n\nSystem load (from os.getloadavg())\nCurrent power consumption (from the smart plug)\nTime of day (to account for ToU periods)\n\n  Power Optimizer: ToU adaption chart\n\nAdaptive Performance Control\nInitially, I explored a full PID (Proportional-Integral-Derivative) controller, but a simpler PI (Proportional-Integral) approach proved more suitable for power management.\nThe PI controller focuses on two key aspects:\n\nProportional (P) term: Provides immediate, proportional response to current errors\nIntegral (I) term: Accumulates past errors to eliminate long-term steady-state deviations\n\nBy removing the Derivative term, we:\n\nSimplify the control logic\nReduce computational overhead\nMaintain smooth performance transitions\nAvoid potential over-optimization from the derivative calculation\n\nFor systems with gradual state changes like power management, the derivative term often introduces unnecessary complexity without meaningful benefits.\nThe controller dynamically adapts system parameters by considering:\n\nCurrent system utilization (CPU/GPU load)\nInstantaneous power consumption\nTime-of-use electricity rate periods\n\nThis approach ensures energy-efficient transitions without the complexity of a full PID implementation.\n\n  Power Optimizer: Control flow (with PID)\n\nImplementation and Limitations\nTerminal Interface\nThe dashboard design is simple with:\n\nA large, easy-to-read current wattage display\nColor-coded elements based on consumption thresholds\nReal-time updates with configurable refresh intervals\nHistorical graph using Unicode block characters for compatibility across terminals\n\nData Sources\nThe tool supports two main data sources:\n\nDirect Kasa Connection: Communicates directly with TP-Link Kasa smart plugs\nHome Assistant Integration: Uses the existing HA setup with authentication tokens\n\nThe code is modular enough that adding support for other smart plug systems shouldn’t be too difficult.\nCurrent Limitations\nThis is very much a personal project with some constraints:\n\nCurrently supports only one smart plug at a time\nWorks only with Kasa smart plugs that have energy monitoring capabilities (Ex: EP25)\nRequires either direct network access to the plug or Home Assistant integration\nPower management features require Linux systems with appropriate CPU/GPU frequency control capabilities\n\nHow TO Use\nBasic setup:\ngit clone https://github.com/naveenkul/WattWise.git\ncd WattWise\npip install -r requirements.txt\npip install .\n\nBasic usage:\n# Quick power view (single reading)\nwattwise\n\n# Continuous monitoring with charts\nwattwise --watch\n\nFuture Improvements\n\nSupport for multiple plugs with aggregated power statistics\nAdditional smart plug brands/models compatibility\nEnhanced visualization options and exports\nIntegration with other power management tools\nBetter predictive algorithms for consumption forecasting\n\nFinal Thoughts\nWattWise started as a simple utility to solve my specific need: monitoring a power-hungry workstation from the terminal I already have open. The addition of automatic power management during peak ToU hours has already saved me from manually adjusting system performance throughout the day.\nThe dashboard part of the project is open-sourced under the MIT license. Documentation and installation instructions are available in the repository. Contributions and feedback are welcome!\n\n        naveenkul/WattWise\n\nFeel free to fork it and adapt it to your specific needs – I’d be curious to see what others build with it.",
    "summary": {
      "en": "**Summary of WattWise: Terminal-Based Power Monitoring Using Smart Plugs**\n\n**Overview:**\nWattWise is a terminal-based tool designed for monitoring power usage from TP-Link Kasa smart plugs. It helps balance high-performance computing demands with electricity costs, especially in areas with variable electricity pricing.\n\n**Key Features:**\n- **Real-time Monitoring:** Displays current power usage, with color-coded indicators (green for low, yellow for medium, red for high).\n- **Historical Data:** Shows consumption trends directly in the terminal.\n- **Power Management:** Automatically adjusts CPU and GPU performance based on electricity pricing to save energy during peak hours.\n- **Deployment Options:** Can be installed from source code or via Docker, and supports both direct smart plug access and Home Assistant integration.\n\n**Technical Aspects:**\n- The tool uses a simplified PI (Proportional-Integral) control method for power management, focusing on immediate and accumulated errors to optimize performance without unnecessary complexity.\n- The interface is user-friendly, featuring a large wattage display and real-time updates.\n\n**Limitations:**\n- Currently supports only one Kasa smart plug with energy monitoring capabilities.\n- Requires Linux systems for power management features.\n\n**Usage:**\nTo set up, users can clone the repository and install the necessary requirements. Basic commands allow for quick power readings or continuous monitoring.\n\n**Future Improvements:**\nPlans include supporting multiple smart plugs, compatibility with more brands, better visualization, and enhanced predictive algorithms.\n\n**Conclusion:**\nWattWise is an open-source project aimed at simplifying power monitoring and management for high-performance workstations. Feedback and contributions are encouraged from users.",
      "ko": "WattWise는 TP-Link Kasa 스마트 플러그의 전력 사용량을 모니터링하기 위해 설계된 터미널 기반 도구입니다. 이 도구는 고성능 컴퓨팅의 요구와 전기 비용을 조화롭게 관리하는 데 도움을 주며, 특히 전기 요금이 변동하는 지역에서 유용합니다.\n\nWattWise의 주요 기능으로는 실시간 모니터링이 있습니다. 현재 전력 사용량을 표시하며, 색상으로 구분된 지표를 통해 저전력(초록), 중간(노랑), 고전력(빨강) 상태를 쉽게 확인할 수 있습니다. 또한, 과거 데이터도 제공하여 소비 추세를 터미널에서 직접 확인할 수 있습니다. 전력 관리 기능은 전기 요금에 따라 CPU와 GPU 성능을 자동으로 조정하여 피크 시간대에 에너지를 절약할 수 있도록 돕습니다. 설치는 소스 코드에서 직접 하거나 Docker를 통해 가능하며, 스마트 플러그에 직접 접근하거나 Home Assistant와 통합하여 사용할 수 있습니다.\n\n기술적으로는 이 도구가 전력 관리를 위해 단순화된 비례-적분 제어 방법을 사용합니다. 이는 즉각적인 오류와 누적 오류에 집중하여 불필요한 복잡성 없이 성능을 최적화합니다. 사용자 인터페이스는 친숙하게 설계되어 있으며, 큰 전력량 표시와 실시간 업데이트 기능을 제공합니다.\n\n현재의 한계로는 에너지 모니터링 기능이 있는 Kasa 스마트 플러그 한 대만 지원하며, 전력 관리 기능을 사용하기 위해서는 리눅스 시스템이 필요합니다. 사용자는 저장소를 복제하고 필요한 요구 사항을 설치하여 설정할 수 있습니다. 기본 명령어를 통해 빠른 전력 측정이나 지속적인 모니터링이 가능합니다.\n\n앞으로의 개선 계획으로는 여러 스마트 플러그 지원, 다양한 브랜드와의 호환성, 더 나은 시각화, 향상된 예측 알고리즘 등이 포함되어 있습니다. WattWise는 고성능 워크스테이션의 전력 모니터링과 관리를 간소화하기 위한 오픈 소스 프로젝트로, 사용자들의 피드백과 기여를 환영합니다.",
      "ja": "WattWiseは、TP-Link Kasaのスマートプラグからの電力使用量を監視するための端末ベースのツールです。このツールは、高性能コンピューティングの需要と電気料金のバランスを取るのに役立ち、特に電気料金が変動する地域での利用に適しています。\n\nWattWiseの主な機能には、リアルタイム監視機能があります。現在の電力使用量を表示し、色分けされたインジケーターで示します。緑は低消費、黄色は中程度、赤は高消費を表します。また、過去のデータも表示され、消費の傾向を端末上で確認できます。電力管理機能により、電気料金に基づいてCPUやGPUのパフォーマンスを自動的に調整し、ピーク時のエネルギーを節約します。インストールはソースコードから行うことも、Dockerを利用することもでき、スマートプラグへの直接アクセスやHome Assistantとの統合もサポートしています。\n\n技術的には、WattWiseは電力管理に簡略化されたPI（比例・積分）制御方式を使用しています。この方法は、即時のエラーと累積エラーに焦点を当て、不要な複雑さを避けながらパフォーマンスを最適化します。インターフェースは使いやすく、大きなワット数表示とリアルタイムの更新が特徴です。\n\n現在の制限としては、エネルギー監視機能を持つKasaスマートプラグが1台のみサポートされています。また、電力管理機能を利用するにはLinuxシステムが必要です。\n\n使用方法としては、ユーザーはリポジトリをクローンし、必要な要件をインストールすることでセットアップできます。基本的なコマンドを使用することで、迅速な電力測定や継続的な監視が可能です。\n\n今後の改善計画には、複数のスマートプラグのサポート、他のブランドとの互換性向上、より良い視覚化、予測アルゴリズムの強化が含まれています。\n\nWattWiseは、高性能ワークステーションの電力監視と管理を簡素化することを目的としたオープンソースプロジェクトです。ユーザーからのフィードバックや貢献が歓迎されています。"
    }
  },
  {
    "id": "f5f9cfdac24aa878",
    "title": {
      "en": "Why F#?",
      "ko": "F#의 매력은?",
      "ja": "F#の魅力"
    },
    "type": "story",
    "url": "https://batsov.com/articles/2025/03/30/why-fsharp/",
    "score": 174,
    "by": "bozhidar",
    "time": 1743510847,
    "content": "Why F#?\n\n          23 minute read\n\n        If someone had told me a few months ago I’d be playing with .NET again after a\n15+ years hiatus I probably would have laughed at this.1 Early on in my\ncareer I played with .NET and Java, and even though .NET had done some things\nbetter than Java (as it had the opportunity to learn from some early Java\nmistakes), I quickly settled on Java as it was a truly portable environment.\n\nI guess everyone who reads my blog knows that in the past few years I’ve been\nplaying on and off with OCaml and I think it’s safe to say that it has become\none of my favorite programming languages - alongside the likes of Ruby and\nClojure. My work with OCaml drew my attention recently to F#, an ML targeting\n.NET, developed by Microsoft. The functional counterpart of the\n(mostly) object-oriented C#. The newest ML language created…\n\nWhat is F#?Permalink\n\n  Unfortunately, no one can be told what the Matrix is. You have to see it for yourself.\n\n  – Morpheus, The Matrix\n\nBefore we start discussing F#, I guess we should answer first the question\n“What is F#?”. I’ll borrow a bit from the official page to answer it.\n\nF# is a universal programming language for writing succinct, robust and performant code.\n\nF# allows you to write uncluttered, self-documenting code, where your focus remains on your problem domain, rather than the details of programming.\n\nIt does this without compromising on speed and compatibility - it is open-source, cross-platform and interoperable.\n\nopen System // Gets access to functionality in System namespace.\n\n// Defines a list of names\nlet names = [ \"Peter\"; \"Julia\"; \"Xi\" ]\n\n// Defines a function that takes a name and produces a greeting.\nlet getGreeting name = $\"Hello, {name}\"\n\n// Prints a greeting for each name!\nnames\n|> List.map getGreeting\n|> List.iter (fun greeting -> printfn $\"{greeting}! Enjoy your F#\")\n\nTrivia: F# is the language that made the pipeline operator (|>) popular.\n\nF# has numerous features, including:\n\n  Lightweight syntax\n  Immutable by default\n  Type inference and automatic generalization\n  First-class functions\n  Powerful data types\n  Pattern matching\n  Async programming\n\nA full set of features are documented in the F# language guide.\n\nLooks pretty promising, right?\n\nF# 1.0 was officially released in May 2005 by Microsoft Research. It was\ninitially developed by Don Syme at Microsoft Research in Cambridge and evolved\nfrom an earlier research project called “Caml.NET,” which aimed to bring OCaml\nto the .NET platform.2 F# was officially moved from Microsoft Research to\nMicrosoft (as part of their developer tooling division) in 2010 (timed\nwith the release of F# 2.0).\n\nF# has been steadily evolving since those early days and the most recent release\nF# 9.0 was\nreleased in November 2024.  It seems only appropriate that F# would come to my\nattention in the year of its 20th birthday!\n\nThere were several reasons why I wanted to try out F#:\n\n  .NET became open-source and portable a few years ago and I wanted to check the progress on that front\n  I was curious if F# offers any advantages over OCaml\n  I’ve heard good things about the F# tooling (e.g. Rider and Ionide)\n  I like playing with new programming languages\n\nBelow you’ll find my initial impressions for several areas.\n\nThe LanguagePermalink\n\nAs a member of the ML family of languages, the syntax won’t surprise\nanyone familiar with OCaml. As there are quite few people familiar with\nOCaml, though, I’ll mention that Haskell programmers will also feel right at\nhome with the syntax. And Lispers.\n\nFor everyone else - it’d be fairly easy to pick up the basics.\n\n// function application\nprintfn \"Hello, World!\"\n\n// function definition\nlet greet name =\n    printfn \"Hello, %s!\" name\n\ngreet \"World\"\n\n// whitespace is significant, like in Python\nlet foo =\n    let i, j, k = (1, 2, 3)\n\n    // Body expression:\n    i + 2 * j + 3 * k\n\n// conditional expressions\nlet test x y =\n  if x = y then \"equals\"\n  elif x < y then \"is less than\"\n  else \"is greater than\"\n\nprintfn \"%d %s %d.\" 10 (test 10 20) 20\n\n// Looping over a list.\nlet list1 = [ 1; 5; 100; 450; 788 ]\nfor i in list1 do\n   printfn \"%d\" i\n\n// Looping over a sequence of tuples\nlet seq1 = seq { for i in 1 .. 10 -> (i, i*i) }\nfor (a, asqr) in seq1 do\n  printfn \"%d squared is %d\" a asqr\n\n// A simple for...to loop.\nlet function1 () =\n  for i = 1 to 10 do\n    printf \"%d \" i\n  printfn \"\"\n\n// A for...to loop that counts in reverse.\nlet function2 () =\n  for i = 10 downto 1 do\n    printf \"%d \" i\n  printfn \"\"\n\n// Records\n\n// Labels are separated by semicolons when defined on the same line.\ntype Point = { X: float; Y: float; Z: float }\n\n// You can define labels on their own line with or without a semicolon.\ntype Customer =\n    { First: string\n      Last: string\n      SSN: uint32\n      AccountNumber: uint32 }\n\nlet mypoint = { X = 1.0; Y = 1.0; Z = -1.0 }\n\n// Discriminated Union\ntype Shape =\n    | Circle of radius: float\n    | Rectangle of width: float * height: float\n\n// Functing using pattern matching\nlet area shape =\n    match shape with\n    | Circle radius -> System.Math.PI * radius * radius\n    | Rectangle (width, height) -> width * height\n\nlet circle = Circle 5.0\nlet rectangle = Rectangle(4.0, 3.0)\n\nprintfn \"Circle area: %f\" (area circle)\nprintfn \"Rectangle area: %f\" (area rectangle)\n\nNothing shocking here, right?\n\nHere’s another slightly more involved example:\n\nopen System\n\n// Sample data - simple sales records\ntype SalesRecord = { Date: DateTime; Product: string; Amount: decimal; Region: string }\n\n// Sample dataset\nlet sales = [\n    { Date = DateTime(2023, 1, 15); Product = \"Laptop\"; Amount = 1200m; Region = \"North\" }\n    { Date = DateTime(2023, 2, 3);  Product = \"Phone\";  Amount = 800m;  Region = \"South\" }\n    { Date = DateTime(2023, 1, 20); Product = \"Tablet\"; Amount = 400m;  Region = \"North\" }\n    { Date = DateTime(2023, 2, 18); Product = \"Laptop\"; Amount = 1250m; Region = \"East\" }\n    { Date = DateTime(2023, 1, 5);  Product = \"Phone\";  Amount = 750m;  Region = \"West\" }\n    { Date = DateTime(2023, 2, 12); Product = \"Tablet\"; Amount = 450m;  Region = \"North\" }\n    { Date = DateTime(2023, 1, 28); Product = \"Laptop\"; Amount = 1150m; Region = \"South\" }\n]\n\n// Quick analysis pipeline\nlet salesSummary =\n    sales\n    |> List.groupBy (fun s -> s.Product)                          // Group by product\n    |> List.map (fun (product, items) ->                          // Transform each group\n        let totalSales = items |> List.sumBy (fun s -> s.Amount)\n        let avgSale = totalSales / decimal (List.length items)\n        let topRegion =\n            items\n            |> List.groupBy (fun s -> s.Region)                   // Nested grouping\n            |> List.maxBy (fun (_, regionItems) ->\n                regionItems |> List.sumBy (fun s -> s.Amount))\n            |> fst\n\n        (product, totalSales, avgSale, topRegion))\n    |> List.sortByDescending (fun (_, total, _, _) -> total)      // Sort by total sales\n\n// Display results\nsalesSummary\n|> List.iter (fun (product, total, avg, region) ->\n    printfn \"%s: $%M total, $%M avg, top region: %s\"\n        product total avg region)\n\nWhy don’t you try saving the snippet above in a file called Sales.fsx and running it like this:\n\ndotnet fsi Sales.fsx\n\nNow you know that F# is a great choice for ad-hoc scripts! Also, running dotnet fsi by itself\nwill pop an F# REPL where you can explore the language at your leisure.\n\nI’m not going to go into great details here, as much of what I wrote about OCaml\nhere applies to F# as well.\nI’d also suggest this quick tour of F#\nto get a better feel for its syntax.\n\nTip: Check out the F# cheatsheet\nif you’d like to see a quick syntax reference.\n\nOne thing that made a good impression to me is the focus of the language designers on\nmaking F# approachable to newcomers, by providing a lot of small quality of life improvements\nfor them. Below are few examples, that probably don’t mean much to you, but would mean something\nto people familiar with OCaml:\n\n// line comments\n(* the classic ML comments are around as well *)\n\n// mutable values\nlet mutable x = 5\nx <- 6\n\n// ranges and slices\nlet l = [1..2..10]\nname[5..]\n\n// C# method calls look pretty natural\nlet name = \"FOO\".ToLower()\n\n// operators can be overloaded for different types\nlet string1 = \"Hello, \" + \"world\"\nlet num1 = 1 + 2\nlet num2 = 1.0 + 2.5\n\n// universal printing\nprintfn \"%A\" [1..2..100]\n\nI guess some of those might be controversial, depending on whether you’re a ML\nlanguage purist or not, but in my book anything that makes ML more popular is a\ngood thing.\n\nDid I also mention it’s easy to work with unicode strings and regular expressions?\n\nOften people say that F# is mostly a staging ground for future C# features, and perhaps that’s true.\nI haven’t observed both languages long enough to have my own opinion on the subject, but I was impressed\nto learn that async/await (of C# and later JavaScript fame) originated in… F# 2.0.\n\n  It all changed in 2012 when C#5 launched with the introduction of what has now\nbecome the popularized async/await keyword pairing. This feature allowed you to\nwrite code with all the benefits of hand-written asynchronous code, such as not\nblocking the UI when a long-running process started, yet read like normal\nsynchronous code. This async/await pattern has now found its way into many\nmodern programming languages such as Python, JS, Swift, Rust, and even C++.\n\n  F#’s approach to asynchronous programming is a little different from async/await\nbut achieves the same goal (in fact, async/await is a cut-down version of F#’s\napproach, which was introduced a few years previously, in F#2).\n\n  – Isaac Abraham, F# in Action\n\nTime will tell what will happen, but I think it’s unlikely that C# will ever be able to fully replace F#.\n\nI’ve also found this encouraging comment from 2022 that Microsoft might be willing to invest more in F#:\n\n  Some good news for you. After 10 years of F# being developed by 2.5 people\ninternally and some random community efforts, Microsoft has finally decided to\nproperly invest in F# and created a full-fledged team in Prague this\nsummer. I’m a dev in this team, just like you I was an F# fan for many years\nso I am happy things got finally moving here.\n\nLooking at the changes in F# 8.0 and F 9.0, it seems the new full-fledged team\nhas done some great work!\n\nEcosystemPermalink\n\nIt’s hard to assess the ecosystem around F# after such a brief period, but overall it seems to\nme that there are fairly few “native” F# libraries and frameworks out there and most people\nrely heavily on the core .NET APIs and many third-party libraries and frameworks geared towards C#.\nThat’s a pretty common setup when it comes to hosted languages in general, so nothing surprising here as well.\n\nIf you’ve ever used another hosted language (e.g. Scala, Clojure, Groovy) then you probably know what\nto expect.\n\nAwesome F# keeps track of popular F# libraries, tools and frameworks. I’ll highlight here the web development and data science libraries:\n\nWeb Development\n\n  Giraffe: A lightweight library for building web applications using ASP.NET Core. It provides a functional approach to web development.\n  Suave: A simple and lightweight web server library with combinators for routing and task composition. (Giraffe was inspired by Suave)\n  Saturn: Built on top of Giraffe and ASP.NET Core, it offers an MVC-style framework inspired by Ruby on Rails and Elixir’s Phoenix.\n  Bolero: A framework for building client-side applications in F# using WebAssembly and Blazor.\n  Fable: A compiler that translates F# code into JavaScript, enabling integration with popular JavaScript ecosystems like React or Node.js.\n  Elmish: A model-view-update (MVU) architecture for building web UIs in F#, often used with Fable.\n  SAFE Stack: An end-to-end, functional-first stack for building cloud-ready web applications. It combines technologies like Saturn, Azure, Fable, and Elmish for a type-safe development experience.\n\nData Science\n\n  Deedle: A library for data manipulation and exploratory analysis, similar to pandas in Python.\n  DiffSharp: A library for automatic differentiation and machine learning.\n  FsLab: A collection of libraries tailored for data science, including visualization and statistical tools.\n\nI haven’t played much with any of them at this point yet, so I’ll reserve any\nfeedback and recommendations for some point in the future.\n\nDocumentationPermalink\n\nThe official documentation is pretty good, although I find it kind of weird that\nsome of it is hosted on Microsoft’s site\nand the rest is on https://fsharp.org/ (the site of the F# Software Foundation).\n\nI really liked the following parts of the documentation:\n\n  F# Style Guide\n  F# Design - a repository of RFCs (every language should have one of those!)\n  F# Standard Library API\n\nhttps://fsharpforfunandprofit.com/ is another good learning resource. (even if it seems a bit dated)\n\nDev ToolingPermalink\n\nF# has a somewhat troubled dev tooling story, as historically\nsupport for F# was great only in Visual Studio, and somewhat subpar\nelsewhere. Fortunately, the tooling story has improved a lot in the past\ndecade:\n\n  In 2014 a technical breakthrough was made with the creation of the\nFSharp.Compiler.Service (FCS) package by Tomas Petricek, Ryan Riley, and Dave\nThomas with many later contributors. This contains the core implementation of\nthe F# compiler, editor tooling and scripting engine in the form of a single\nlibrary and can be used to make F# tooling for a wide range of\nsituations. This has allowed F# to be delivered into many more editors,\nscripting and documentation tools and allowed the development of alternative\nbackends for F#. Key editor community-based tooling includes Ionide, by\nKrzysztof Cieślak and contributors, used for rich editing support in the\ncross-platform VSCode editor, with over 1M downloads at time of writing.\n\n  – Don Syme, The Early History of F#\n\nI’ve played with the F# plugins for several editors:\n\n  Emacs (fsharp-mode)\n  Zed (third-party plugin)\n  Helix (built-in support for F#)\n  VS Code (Ionide)\n  Rider (JetBrains’s .NET IDE)\n\nOverall, Rider and VS Code provide the most (and the most polished) features,\nbut the other options were quite usable as well.  That’s largely due to the fact\nthat the F# LSP server fsautocomplete (naming is hard!) is quite robust and\nany editor with good LSP support gets a lot of functionality for free.\n\nStill, I’ll mention that I found the tooling lacking in some regards:\n\n  fsharp-mode doesn’t use TreeSitter (yet) and doesn’t seem to be very actively developed (looking at the code - it seems it was derived from caml-mode)\n  Zed’s support for F# is quite spartan\n  In VS Code shockingly the expanding and shrinking selection is broken, which is quite odd for what is supposed to be the flagship editor for F#\n\nI’m really struggling with VS Code’s keybindings (too many modifier keys and functions keys for my taste) and editing model, so I’ll likely stick with Emacs going forward. Or I’ll finally spend more quality time with neovim!\n\nIt seems that everyone is using the same code formatter (Fantomas), including the F# team, which is great!\nThe linter story in F# is not as great (seems the only popular linter FSharpLint is abandonware these days), but when your\ncompiler is so good, you don’t really need a linter as much.\n\nOh, well… It seems that Microsoft are not really particularly invested in\nsupporting the tooling for F#, as pretty much all the major projects in this\nspace are community-driven.\n\nUsing AI coding agents (e.g. Copilot) with F# worked pretty well, but I didn’t\nspend much time on this front.\n\nIn the end of the day any editor will likely do, as long as you’re using LSP.\n\nBy the way, I had an interesting observation while programming in F# (and OCaml for that matter) -\nthat when you’re working with a language with a really good type system you don’t really need that much\nfrom your editor. Most the time I’m perfectly happy with just some inline type information (e.g. something like CodeLenses), auto-completion and the ability to easily send code to fsi. Simplicity continues\nto be the ultimate sophistication…\n\nOther tools that should be on your radar are:\n\n  Paket - Paket is a dependency manager for .NET projects. Think of it as something like bundler, npm or pip, but for .NET’s NuGet package ecosystem.\n  FAKE -  A DSL for build tasks and more, where you can use F# to specify the tasks. Somewhat similar to Ruby’s rake. Some people claim that’s the easiest way to sneak F# into an existing .NET project.\n\nUse CasesPermalink\n\nGiven the depth and breath of .NET - I guess that sky is the limit for you!\n\nSeems to me that F# will be a particularly good fit for data analysis and manipulation, because\nof features like type providers.\n\nProbably a good fit for backend services and even full-stack apps, although I haven’t really played\nwith the F# first solutions in this space yet.\n\nFable and Elmish make F# a viable option for client-side programming and might offer\nanother easy way to sneak F# into your day-to-day work.\n\nNote: Historically, Fable has been used to target JavaScript but since Fable\n4, you can also target other languages such as TypeScript, Rust, Python, and\nmore.\n\nHere’s how easy it is to transpile an F# codebase into something else:\n\n# If you want to transpile to JavaScript\ndotnet fable\n\n# If you want to transpile to TypeScript\ndotnet fable --lang typescript\n\n# If you want to transpile to Python\ndotnet fable --lang python\n\nCool stuff!\n\nCommunityPermalink\n\nMy initial impression of the community is that it’s fairly small, perhaps even\nsmaller than that of OCaml.  The F# Reddit and Discord (the one listed on\nReddit) seem like the most active places for F# conversations. There’s supposed\nto be some F# Slack as well, but I couldn’t get an invite for it. (seems the\nautomated process for issuing those invites has been broken for a while)\n\nI’m still not sure what’s the role Microsoft plays in the community, as I\nhaven’t seen much from them overall.\n\nFor a me a small community is not really a problem, as long as the community is\nvibrant and active. Also - I’ve noticed I always feel more connected to smaller\ncommunities. Moving from Java to Ruby back in the day felt like night and day as\nfar as community engagement and sense of belonging go.\n\nI didn’t find many books and community sites/blogs dedicated to F#, but I didn’t\nreally expect to in the first place.\n\nThe most notable community initiatives I discovered were:\n\n  Amplifying F# - an effort to promote F# and to get more businesses involved with it\n  F# for Fun and Profit - a collection of tutorials and essays on F#\n  F# Lab - The community driven toolkit for datascience in F#\n  F# Weekly - a weekly newsletter about the latest developments in the world of F#\n\nSeems to me that more can be done to promote the language and engage new programmers and businesses\nwith it, although that’s never easy 20 years into the existence of some project. I continue to be\nsomewhat puzzled as to why Microsoft doesn’t market F# more, as I think it could be a great\nmarketing vehicle for them.\n\nAll in all - I don’t feel qualified to comment much on the F# community at this point.\n\nThe Popularity ContestPermalink\n\nDepending on the type of person you are you may or may not care about a a programming language’s\n“popularity”. People often ask my why I spent a lot of time with languages that are unlikely to\never result in job opportunities for me, e.g.:\n\n  Emacs Lisp\n  Clojure\n  OCaml\n  F#\n\nProfessional opportunities are important, of course, but so are:\n\n  having fun (and the F in F# stands for “fun”)\n  learning new paradigms and ideas\n  challenging yourself to think and work differently\n\nThat being said, F# is not a popular language by most conventional metrics. It’s not highly ranked\non TIOBE, StackOverflow or most job boards. But it’s also not less popular than most “mainstream”\nfunctional programming languages. The sad reality is that functional programming is still not\nmainstream and perhaps it will never be.\n\nA few more resources on the subject:\n\n  About F#’s popularity\n  How Popular is F# in 2024\n\n      Here’s also a video for the article above\n\nF# vs OCamlPermalink\n\n  The early conception of F# was simple: to bring the benefits of OCaml to .NET and .NET to OCaml: a\nmarriage between strongly typed functional programming and .NET. Here “OCaml” meant both the\ncore of the language itself, and the pragmatic approach to strongly-typed functional programming\nit represented. The initial task was relatively well-defined: I would re-implement the core of the\nOCaml language and a portion of its base library to target the .NET Common Language Runtime.\nThe implementation would be fresh, i.e. not using any of the OCaml codebase, for legal clarity.\n\n  – Don Syme, creator of F#, The Early History of F#\n\nF# was derived from OCaml, so the two languages share a lot of DNA. Early on\nF# made some efforts to support as much of OCaml’s syntax as possible, and it\neven allowed the use of .ml and .mli file extensions for F# code. Over time\nthe languages started to diverge a bit, though.3\n\nCreating a language that’s independent from OCaml, of course, was something\nintended from the very beginning. That’s also reflected in the decision\nto chose the name F#, even if early versions of the language were called “Caml.NET”:\n\n  Although the first version of F# was initially presented as “Caml-for-.NET”,\nin reality it was always a new language, designed for .NET from day 1. F# was\nnever fully compatible with any version of OCaml, though it shared a compatible\nsubset, and it took Caml-Light and OCaml as its principal sources of design\nguidance and inspiration.\n\n  – Don Syme, The Early History of F#\n\nIf you ask most people about the pros and cons of F# over OCaml you’ll probably\nget the following answers.\n\nF# Pros\n\n  Runs on .NET\n\n      Tons of libraries are at disposal\n\n  Backed by Microsoft\n  Arguably it’s a bit easier to learn by newcomers (especially those who have only experience with OO programming)\n\n      The syntax is slightly easier to pick up (I think)\n      The compiler errors and warnings are “friendlier” (easier to understand)\n      It’s easier to debug problems (partially related to the previous item)\n\n  Strong support for async programming\n  Has some cool features, absent in OCaml, like:\n\n      Anonymous Records\n      Active Patterns\n      Computational expressions\n      Sequence comprehensions\n      Type Providers\n      Units of measure\n\nF# Cons\n\n  Runs on .NET\n\n      The interop with .NET influenced a lot of language design decisions (e.g. allowing null)\n\n  Backed by Microsoft\n\n      Not everyone likes Microsoft\n      Seems the resources allocated to F# by Microsoft are modest\n      It’s unclear how committed Microsoft will be to F# in the long run\n\n  Naming conventions: I like snake_case way more than camelCase and PascalCase\n  Misses some cool OCaml features\n\n      First-class modules and functors\n      GADTs\n\n  Doesn’t have a friendly camel logo\n  The name F# sounds cool, but is a search and filename nightmare (and you’ll see FSharp quite often in the wild)\n\nBoth F# and OCaml can also target JavaScript runtimes as well - via Fable on\nthe F# side and Js_of_ocaml and Melange on the OCaml side. Fable seems like a\nmore mature solution at a cursory glance, but I haven’t used any of the three\nenough to be able to offer an informed opinion.\n\nIn the end of the day both remain two fairly similar robust, yet niche,\nlanguages, which are unlikely to become very popular in the future. I’m guessing\nworking professionally with F# is more likely to happen for most people, as .NET\nis super popular and I can imagine it’d be fairly easy to sneak a bit of F# here\nin there in established C# codebases.\n\nOne weird thing I’ve noticed with F# projects is that they still use XML project\nmanifests, where you have to list the source files manually in the order in\nwhich they should be compiled (to account for the dependencies between them). I\nam a bit shocked that the compiler can’t handle the dependencies automatically,\nbut I guess that’s because in F# there’s not direct mapping between source files\nand modules. At any rate - I prefer the OCaml compilation process (and Dune) way\nmore.\n\nAs my interest in MLs is mostly educational I’m personally leaning towards OCaml, but if I had to build\nweb services with an ML language I’d probably pick F#. I also have a weird respect for every language\nwith its own runtime, as this means that it’s unlikely that the runtime will force some compromises\non the language.\n\nClosing thoughtsPermalink\n\n  Question: What can C# do that F# can’t?\nAnswer: NullReferenceException!\n\n  – F# Community joke\n\nAll in all I liked F# way more than I expected to! In a way it reminded me of my\nexperience with Clojure back in the day in the sense that Clojure was the most\npractical Lisp out there when it was released, mostly because of its great\ninterop with Java.\n\nI have a feeling that if .NET was portable (and open-source) since day 1\nprobably ClojureCLR would have become as popular as Clojure, and likely F# would\nhave developed a bigger community and broader usage by now. I’m fairly certain I\nwould have never dabbled in .NET again if it hadn’t been for .NET Core, and I\ndoubt I’m the only one. The fact that F# wasn’t open-sourced until 2010 didn’t help\nwith the early adoption either.\n\nSeems I’m only the only one who thinks this way:\n\n  Mistakes are hard to admit, and best seen in their historical context. From the early history, the\ngreatest mistake related to F# was that neither .NET nor the language were open source or using\nopen engineering. This mistake was well-understood by the core contributors at the time and many\nacross Microsoft were advocating for a shift to open-source. Put simply, an innovative language\ngrew in the research lab of a company that had not yet embraced open source: those involved did\nwhat they could through source drops, and the problem was eventually solved via the shift to open\nsource engineering and design from 2011 to 2014. The rectification of this mistake will likely be the\nmost significant development in the history of the language. Further, the fact that F# was able to\nnavigate 2002-2011 while using closed-engineering is largely due to the recognition of its qualities\nby decision makers at Microsoft.\n\n  – Don Syme, The Early History of F#\n\nLearning OCaml is definitely not hard, but I think that people interested to learn some ML\ndialect might have an easier time with F#. And, as mentioned earlier, you’ll probably have an\neasier path to “production” with it.\n\nI think that everyone who has experience with .NET will benefit from learning F#.\nPerhaps more importantly - everyone looking to do more with an ML family language\nshould definitely consider F#, as it’s a great language in its own right, that gives\nyou access to one of the most powerful programming platforms out there.\n\nLet’s not forget about Fable, which makes it possible for you leverage\nF# in JavaScript, Dart, Rust and Python runtimes!\n\nSo, why F#? In the F# community there’s the saying that the “F” in F# stands for\n“Fun”. In my brief experience with F# I found this to be very true! I’ll go a\nstep further and make the claim that F# is both seriously fun and seriously\npractical!\n\nAlso if your code compiles - it will probably work the way you expect it to. I\nhear that’s generally considered a desirable thing in the world of programming!\n\nThat’s all I have for you today. Please, share in the comments what do you love about F#!\n\nIn sane type systems we trust!\n\nDiscussionsPermalink\n\n  Hacker News\n  Lobsters\n\n      I had some C# courses in the university and I wrote my bachelor’s thesis in C#. It was a rewrite of Arch Linux’s pacman, running on Mono. This was way back in 2007.↩\n\n      See https://fsharp.org/history/hopl-final/hopl-fsharp.pdf↩\n\n      https://github.com/fsharp/fslang-suggestions/issues/985↩\n\n     Tags:\n\n      .NET,\n\n      F#,\n\n      ML,\n\n      OCaml\n\n   Updated: March 30, 2025\n\n  Share on\n\n   Twitter\n\n   Facebook\n\n   LinkedIn\n\n      Previous\n\n      Next\n\n     var HYVOR_TALK_WEBSITE = 2584; // DO NOT CHANGE THIS\n     var HYVOR_TALK_CONFIG = {\n         url: 'https://batsov.com/articles/2025/03/30/why-fsharp/',\n         id: 'https://batsov.com/articles/2025/03/30/why-fsharp/'\n     };",
    "summary": {
      "en": "**Summary of \"Why F#?\"**\n\nThe author reflects on their return to .NET after a long break and shares their positive impressions of the F# programming language, which is part of the ML family. F# is designed to allow developers to write clean, efficient, and robust code while being open-source and cross-platform.\n\n**Key Features of F#:**\n- Lightweight syntax and immutable by default.\n- Type inference and first-class functions.\n- Powerful data types and pattern matching.\n- Asynchronous programming capabilities.\n\nF# was first released in 2005, developed by Don Syme, and has evolved significantly, with the latest version being F# 9.0, released in November 2024. The author is intrigued by F# due to .NET's open-source status, its tooling, and its potential advantages over OCaml, a language they enjoy.\n\n**Language Syntax:**\nF# has a syntax familiar to those who know OCaml or Haskell, making it relatively easy to learn for newcomers. The author provides examples of F# code that highlight its simplicity and expressiveness.\n\n**Ecosystem and Libraries:**\nThe F# ecosystem includes web development libraries like Giraffe and Suave, as well as data science tools like Deedle. However, there is a limited number of native F# libraries compared to C#.\n\n**Development Tools:**\nF# tooling has improved, with strong support in editors like Visual Studio, Rider, and VS Code through community-driven projects. The F# Community has developed tools like Paket for dependency management.\n\n**Community and Popularity:**\nThe F# community is small but active, and the language is not widely popular compared to mainstream languages. The author believes that the \"F\" in F# stands for \"Fun,\" reflecting their enjoyable experience with the language.\n\n**Conclusion:**\nF# is a practical choice for .NET developers and offers a great experience for those looking to explore functional programming. The author encourages others to consider F# for its ease of use, powerful features, and the fun of programming in it.",
      "ko": "저자는 오랜 휴식 후 .NET으로 돌아온 경험을 반영하며, ML 계열에 속하는 F# 프로그래밍 언어에 대한 긍정적인 인상을 공유합니다. F#는 개발자들이 깔끔하고 효율적이며 견고한 코드를 작성할 수 있도록 설계되었으며, 오픈 소스이자 크로스 플랫폼입니다.\n\nF#의 주요 특징으로는 가벼운 문법과 기본적으로 불변성을 제공하는 점, 타입 추론과 일급 함수, 강력한 데이터 타입 및 패턴 매칭, 비동기 프로그래밍 기능이 있습니다. F#은 2005년에 Don Syme에 의해 처음 출시되었으며, 이후 크게 발전하여 2024년 11월에 F# 9.0이 출시되었습니다. 저자는 .NET의 오픈 소스 상태와 도구, 그리고 자신이 좋아하는 언어인 OCaml에 비해 F#이 가질 수 있는 잠재적 장점 때문에 F#에 매료되었습니다.\n\nF#의 문법은 OCaml이나 Haskell을 아는 사람들에게 친숙하여, 초보자들이 배우기 상대적으로 쉽습니다. 저자는 F# 코드의 예시를 제공하며, 그 간결함과 표현력을 강조합니다.\n\nF# 생태계에는 Giraffe와 Suave와 같은 웹 개발 라이브러리, Deedle과 같은 데이터 과학 도구가 포함되어 있습니다. 그러나 C#에 비해 네이티브 F# 라이브러리는 상대적으로 적습니다.\n\nF# 도구는 개선되어 Visual Studio, Rider, VS Code와 같은 편집기에서 강력한 지원을 받고 있으며, 커뮤니티 주도의 프로젝트를 통해 발전하고 있습니다. F# 커뮤니티는 의존성 관리를 위한 Paket과 같은 도구를 개발했습니다.\n\nF# 커뮤니티는 작지만 활발하며, 이 언어는 주류 언어에 비해 널리 사용되지 않습니다. 저자는 F#의 \"F\"가 \"재미(Fun)\"를 의미한다고 믿으며, 이 언어로 프로그래밍하는 즐거운 경험을 반영합니다.\n\nF#은 .NET 개발자에게 실용적인 선택이며, 함수형 프로그래밍을 탐구하고자 하는 이들에게 훌륭한 경험을 제공합니다. 저자는 F#의 사용 용이성, 강력한 기능, 그리고 프로그래밍의 재미를 고려해 다른 이들도 F#을 고려해보기를 권장합니다.",
      "ja": "著者は長いブランクの後に.NETに戻り、F#プログラミング言語についての好印象を共有しています。F#はMLファミリーに属し、開発者がクリーンで効率的、かつ堅牢なコードを書くことを可能にするよう設計されています。また、オープンソースでクロスプラットフォーム対応です。\n\nF#の主な特徴には、軽量な構文とデフォルトで不変であること、型推論とファーストクラス関数、強力なデータ型とパターンマッチング、非同期プログラミング機能があります。F#は2005年にドン・サイムによって初めてリリースされ、その後大きく進化しました。最新バージョンは2024年11月にリリースされたF# 9.0です。著者は、.NETのオープンソースの地位やツール、そして楽しんでいる言語であるOCamlに対する潜在的な利点からF#に興味を持っています。\n\nF#の構文はOCamlやHaskellを知っている人には馴染みやすく、新しい学習者にとっても比較的学びやすいです。著者はF#のコード例を示し、そのシンプルさと表現力を強調しています。\n\nF#のエコシステムには、GiraffeやSuaveといったウェブ開発ライブラリや、Deedleのようなデータサイエンスツールが含まれていますが、C#に比べてネイティブのF#ライブラリは限られています。\n\nF#の開発ツールは改善されており、Visual StudioやRider、VS Codeなどのエディタで強力なサポートがあります。F#コミュニティは、依存関係管理のためのPaketなどのツールを開発しています。\n\nF#コミュニティは小さいですが活発で、言語自体は主流の言語に比べて広くは人気がありません。著者は、F#の「F」は「Fun」を意味すると考えており、言語を使う楽しさを反映しています。\n\nF#は.NET開発者にとって実用的な選択肢であり、関数型プログラミングを探求したい人にとって素晴らしい体験を提供します。著者は、使いやすさ、強力な機能、そしてプログラミングの楽しさからF#を考慮するよう他の人々に勧めています。"
    }
  },
  {
    "id": "5b300b7279ec92db",
    "title": {
      "en": "We Can, Must, and Will Simulate Nematode Brains",
      "ko": "선충 뇌 시뮬레이션!",
      "ja": "線虫脳をシミュレーション！"
    },
    "type": "story",
    "url": "https://asteriskmag.com/issues/09/we-can-must-and-will-simulate-nematode-brains",
    "score": 8,
    "by": "l1n",
    "time": 1743520581,
    "content": "We Can, Must, and Will Simulate Nematode Brains\n\n\t\t\t\t\tMichael Skuhersky\n\nScientists have spent over 25 years trying — and failing — to build computer simulations of the smallest brain we know. Today, we finally have the tools to pull it off.\n\n\t\t\t\t\t\t\t\t\t\t\tA near-perfect simulation of the human brain would have profound implications for humanity. It could offer a pathway for us to transcend the biological limitations that have constrained human potential, and enable unimaginable new forms of intelligence, creativity, and exploration. This represents the next phase in human evolution, freeing our cognition and memory from the limits of our organic structure.Unfortunately, it’s also a long way off. The human brain contains on the order of one hundred billion neurons — interconnected by up to a quadrillion synapses. Reverse-engineering this vast network would require computational resources far exceeding what’s currently available. Scientists seeking a proof of concept for whole brain emulation have had to turn to simpler model organisms. And by far the simplest available brain — at just 300 neurons — belongs to the nematode Caenorhabditis elegans.Scientists have been working on the problem of simulating C. elegans in some form or another for over 25 years. So far, they’ve been met with little success. But with today’s technology, the task is finally possible, and —as I’ll argue —necessary.\n\n    Motion patterns of C. elegans. Credit: Hiroshima University, Osaka University\n\n\t\t\t\t\t\t\t\t\t\t\tA brief history of worm brains\n\n\t\t\t\t\t\t\t\t\t\t\tThe biologist Sydney Brenner became interested in C. elegans as a model organism for developmental biology in the 1970s. Its simplicity and small size made it an ideal lab subject. In 1986, John C. White, a scientist in Brenner’s research group, produced a nearly complete map of the neural connections that make up the C. elegans brain — what scientists now call the connectome. As computers became more accessible, other scientists started building on Brenner’s work. Ernst Neibur and Paul Erdös kicked things off with a biophysical model of nematode locomotion in 1991. Two different teams (one at the University of Oregon and the other in Japan) published plans for building more ambitious models in the late 1990s. Both would have utilized White’s work on neural circuitry. Unfortunately, neither got off the ground.In 2004, the Virtual C. elegans project at Hiroshima University got somewhat farther: they released two papers describing their model, which simulated the nematode’s motor control circuits. The simulated nematode could respond to virtual pokes on its head, but it didn’t do much else. And even this was, arguably, not a true simulation. Although the researchers had a map of the nematode’s neurons, they didn’t know their innate biophysical parameter — that is, the precise electrical characteristics of the connections between them. Instead, the researchers used machine learning to produce a set of values for each neuron that made their simulated nematode respond to a poke like a real one would. As a result, this approach was not entirely grounded in biological reality — a recurring theme that would surface in several future simulation attempts.That is where things stood at the dawn of the 2010s. While work continued on simulating nematode locomotion, there was no progress on simulating a nematode’s brain —let alone a realistic one. Then, on January 1st, 2010, the engineer Giovanni Idili tweeted at the official account of the Whole Brain Catalogue, a project to consolidate data from mouse brains: “new year's resolution: simulate the whole C.Elegans brain (302 neurons)!” U.C. San Diego neuroscience grad student Stephen Larson noticed the tweet and, by August, Larson was pitching the idea at conferences. By early 2011, Larson and Idili had put together a team to start work on what would become the OpenWorm project —the efforts of a decentralized group of academics with the goal of creating a complete, realistic, and open source model of C. elegans.This was a heady time to be interested in simulating extremely tiny brains. Over the next few years, OpenWorm published a series of papers and model updates. In 2013, they hosted their first conference in Paris and landed an optimistic story in The Atlantic (title: “Is This Virtual Worm The First Sign of the Singularity?”). Meanwhile, the researcher David Dalrymple was working on a parallel project at MIT, which he dubbed Nemaload. OpenWorm scientists largely used data from dead nematodes but Dalrymple wanted to use the then-new technique of optogenetics to study living specimens. Optogenetics allows scientists to control neurons and other cells with light. In this case, the technique could be used to collect data on how a nematode’s brain responds to different states by perturbing it thousands-upon-thousands of times. In a 2011 comment on LessWrong, Dalrymple wrote “I would be Extremely Surprised, for whatever that's worth, if this is still an open problem in 2020.”It’s now 2025, and nematode simulation remains an open problem. Dalrymple abandoned Nemaload in 2012. OpenWorm still exists but has not made substantial progress over the past ten years towards creating a truly scientific whole brain simulation, due to a lack of available data. Occasionally, more modern (though still heavily assumption-based) simulations are published, including integrative models that strive to make fewer assumptions. We’re not quite back where we were in the 2010s: we have much better data on the C. elegans nervous system and — as I’ll discuss later — much better tools to study it. But we aren’t much closer to simulating a whole brain.What went wrong? Why has it taken over 25 years to build a working computer simulation of one of the simplest brains known to mankind? And, more importantly, why do I think that this time we can actually pull it off?\n\t\t\t\t\t\t\t\t\t\t\tWhy we got stuck\n\n\t\t\t\t\t\t\t\t\t\t\tBefore explaining what happened, we should ask a more fundamental question: what does it mean to successfully simulate a brain? This is a topic where it’s important to be specific. The term \"simulation\" in academic neuroscience often evokes the notorious failures of the Human Brain Project. In 2013, neuroscientist Henry Markram secured about 1 billion euros from the European Union to \"simulate the human brain\" — a proposal widely deemed unrealistic even at the time. The project faced significant challenges and ultimately did not meet its ambitious yet vague goals. These events cast something of a stigma on brain simulation research, making it especially important for those in the field to set clearer, more realistic goals with concrete milestones along the way.What makes a good simulation is a debate in itself, so I’ll just share my view: a good simulation of a nervous system is one that both accurately replicates its functionality and reliably predicts the future activity of a real system under the same initial conditions. That is, a simulated nematode in a simulated plate of agar should behave the same way as a real nematode in a real plate of agar. If we disturb the simulation —say, by poking or shining a light on it — it should respond the same way the real nematode would. And it should keep acting like a real nematode over time, instead of accumulating more error as time goes on.This definition can help us clarify what is and isn’t simulation. Last October, a consortium of scientists across 127 institutions published the complete connectome of the fruit fly, Drosophila melanogaster. This is a massive accomplishment by any objective standard: it is only the second complete connectome assembled, after that of C. elegans, and contains over 140,000 neurons (as compared to C. elegans’s 300). The success of the project, called FlyWire, has rekindled interest in brain simulation. And, in a sense, the FlyWire connectome can be used to simulate a fruit fly. When Philip Shiu, a researcher on the project, test-‘fired’ the neurons responsible for sensing sugar, the model predicted that other neurons that extend the fly’s proboscis would fire, as they would in a real fly. Other researchers have since used Shiu’s model to accurately predict neural patterns involved in the fly’s sense of taste, grooming, and locomotion.Shiu’s model represents an important advance in our understanding of fruit fly brains, but it isn’t really a simulation. (Nor is it trying to be;Shiu himself has been clear that the model is extremely simplified and makes assumptions about key parameters governing how neurons behave). While the model can successfully predict the behavior of particular groups of neurons, it cannot mimic the exact functionality of an entire fly brain. That’s because the FlyWire model is missing the same thing as OpenWorm (and other attempts to simulate nematodes) did: good data on the relationship between neural structure and neural function.Think of the connectome as a map of the brain. It can tell us how neurons connect to each other through electrical and chemical synapses. But despite revealing which neurons connect to one another, it doesn’t tell us anything about how those connections work. To fully model a brain, we need to understand the biophysical parameters governing each neuron’s behavior. This includes not only the variable strength of synapses (in neuroscience, these are called weights) but also the cells’ membrane properties, such as capacitance and the shapes of dendrites and axons, which affect how electrical signals propagate. We need to know both a neuron’s firing threshold as well as how that threshold changes as the animal learns new things (learning involves shifts in both synaptic weights and the intrinsic properties of neurons themselves). A simulation based only on a static connectome can’t learn —so it won’t behave very much like the real creature it’s trying to simulate.Unfortunately, learning the dynamic biophysical features of a living brain is much harder than understanding its structure (which, as we’ve seen, is hard enough). The primary technique used to map a connectome is electron microscopy. Because electrons have a wavelength up to one hundred thousand times smaller than that of visible light, they can be used to produce images at a much higher resolution than light microscopes. But electron microscopy has a serious disadvantage. It can only be used on sliced brain tissue, so it can’t tell us how a living brain responds to stimuli or changes over time. The technique can give us extremely detailed, high quality images, but can’t tell us a neuron’s electrical characteristics, like the strength of its synapses or how its membranes store electrical charge.For decades, the only way to learn such things was through a technique called patch clamping. The advantage of patch clamping is that it is highly accurate. The disadvantage is that it requires the painstaking placement of electrodes on each individual neuron. With effort, it’s feasible to patch clamp about three neurons at once, making it a less-than-ideal choice for capturing information about neural activity throughout the whole brain. This is where things stood when earlier attempts to simulate C. elegans stalled out. It was a problem of timing: In 2013, the tools that would let us understand what happens inside neurons either didn’t exist, or weren’t ready for practical use.\n\t\t\t\t\t\t\t\t\t\t\tNew ways to see\n\n\t\t\t\t\t\t\t\t\t\t\tAs C. elegans simulation research was losing steam, other researchers pushed forward in advancing the ability to observe cells. First, advances in optical microscopy made it possible to capture fast, relatively sharp images of living cells without destroying them. Since the late 1950s, biologists have relied on confocal microscopes, which use a tiny pinhole to block out-of-focus light. This creates higher resolution images, but the method is also slow, since capturing a whole sample means scanning it point-by-point. This is a serious problem for studying traits that change rapidly (like neuronal activity). This is where modern techniques like light sheet microscopy prove particularly useful. Instead of focusing light through a point, light sheet microscopes use a laser sheet to illuminate an entire 2D cross-section of a sample.The process is dramatically faster and gentler on tissue than traditional confocal methods.Light sheet microscopes have existed since the 1990s, but early versions of the technology struggled to capture fast intracellular processes. That changed with a series of innovations in the early 2010s. First, new techniques were developed to allow optical microscopy below the diffraction limit (the smallest distance between two points at which they can still be distinguished by an optical system). For visible light, this distance is between 200 and 250 nanometers — too big to distinguish most cellular features. That changed with the introduction of super-resolution microscopy which featured resolutions of 100 nanometers and below. Another major advance was DiSPIM,\n\n        1\n\n invented in 2014. In light sheet microscopes, the light illuminating an image has to be perpendicular to the camera picking it up. Originally, this meant that the camera and the light sheet were part of separate assemblies. DiSPIM microscopes use two perpendicular lens assemblies, each equipped with a light source and a camera. This approach doubled the speed with which the microscope could capture images of living samples, and ensured that images could be reconstructed at the same resolution across all three dimensions. In 2015, a group at Columbia University developed a method called SCAPE,\n\n        2\n\n which used an oblique sheet of light to scan and image a sample using a single lens assembly. SCAPE is even faster than earlier light sheet techniques, making it particularly useful for tracking rapid neuronal activity. Another set of innovations has to do with what the microscopes are looking at. All the methods we’ve discussed depend on fluorescent reporters — engineered proteins that fluoresce under certain conditions, such as the presence of a specific protein or the expression of a particular gene. In our case, that trigger is calcium. When a neuron fires, calcium ions flood into the cell, making calcium influx a reliable proxy for neuronal activity. The key breakthrough here was the development of the GCaMP6 family of reporters by a team at the Janelia Research Campus between 2013 and 2015. This new generation of calcium indicators were brighter and more sensitive than earlier versions, quickly becoming the go-to tool for imaging neuronal circuits in living organisms. While GCaMP6 revolutionized calcium-based imaging, even more precise measurements could come from fluorescent reporters that respond to voltage directly. These already exist for larger organisms and are actively being developed for use in C. elegans.Today, the combination of calcium imaging and microscopy techniques like DiSPIM and SCAPE means that we can see how neurons behave throughout the entire C. elegans brain — in real time. The next challenge is to actually do it. And to do it a lot. Our understanding of the C. elegans connectome has improved significantly since White’s groundbreaking work in 1986. White’s connectome was a mosaic of five individual worms. However, the same neuron in different animals might differ in size or capacity for electric charge. To fully understand the C. elegans brain and its operation during a broad range of behaviors, we need to collect data from thousands of individuals.There’s the question of what to do with the data once we have it. This is another area where recent advances –this time, in machine learning — make the process much more feasible. For all its biological complexity, the C. elegans brain still consists of just 300 neurons — tiny compared to state-of-the-art large language models. Using symbolic regression, a machine learning technique for discovering mathematical formulas that explain observed data, we can take our data on neuronal activity and use it to derive key parameters like capacitance and synaptic strength for every single neuron and every single neuronal connection. These equations would likely resemble the biophysical models that scientists have already derived from patch-clamp experiments, but inferred directly from whole-brain data.\n\t\t\t\t\t\t\t\t\t\t\tFish, flies, and beyond\n\n\t\t\t\t\t\t\t\t\t\t\tI don’t mean to suggest that building an accurate C. elegans simulation will be easy. There are many considerations that the technologies I’ve described may not account for, from extra-synaptic signalling to the role of specific neuron morphology (not to mention the fact that neurons and synapses change over the course of a nematode’s life). But with modern techniques, which continue to rapidly improve, I do believe that it is possible.And if we want to one day build simulations of larger animals — including humans — I also believe that it is necessary. The optical microscopy techniques that let us observe the neural activity of living organisms have one key limitation: depth. Light can only penetrate so far into tissue. With current techniques, that limit is roughly 750 microns, a bit less than a millimeter. To build an accurate whole brain simulation, we need activity data from a whole brain — which means that we’re currently limited to brains less than a millimeter deep. In other words, C. elegans, larval zebrafish, and fly brains are our only options. By investigating small organisms, we can develop new methods that allow us to predict neural activity by looking at the brain’s structure and other indirect forms of data. These techniques will make it possible for us to model more complex brains, including those that are too large for us to image their activity directly.My research focuses on creating a scientifically-grounded simulation of C. elegans by integrating these recently developed microscopy, fluorescent reporter, and machine learning methods into a cohesive pipeline and methodological framework. The idea is to create a proven simulation creation blueprint that can then be applied to more complex brains. But achieving a successful simulation of C. elegans would be a remarkable scientific accomplishment on its own. More importantly, it would help us begin to decipher how the structure of a brain relates to the dynamic processes unfolding within it. Over time, this understanding will open the doors to simulating more complex organisms, ultimately including humans. We have a long journey ahead of us, but now is the best time to begin — expeditiously, and with tractable, well-defined milestones along the way.\n\n    Sign up for our newsletter to get Asterisk’s latest interviews, essays, and more.\n\n        Subscribe\n\n    Dual-view Plane Illumination Microscopy\n\n                ↩\n\n    Swept, Confocally Aligned Planar Excitation\n\n                ↩\n\nMichael Skuhersky holds a PhD in neuroscience from MIT and is currently founding a nonprofit research institute focused on brain simulation.\n\nPublished March 2025\n\n\t\tShare with email\n\n\t\tShare on Twitter\n\n\t\tShare on Facebook\n\n\t\tShare on LinkedIn\n\n\t\t\tHave something to say? Email us at letters@asteriskmag.com.\n\nPrevious\n\t\t\t\t\tThe Unbearable Loudness of Chewing\n\n\t\t\t\t\t\t\tNextDeros and the Ur-Abduction\n\nFurther Reading\n\n\t\t\t\tMore:\n\t\t\t\t\t\t\t\t\tscience\n\t\t\t\t\t\t\t\t\ttechnology\n\n\t\t\t\t\t\tA Defense of Weird Research\n\n\t\t\t\t\t\t\t\t\tDeena Mousa\n\n\t\t\t\t\t\t\t\t\tLauren Gilbert\n\n\t\t\t\t\t\tAutomating Math\n\n\t\t\t\t\t\t\t\t\tAdam Marblestone\n\n\t\t\t\t\t\tCan We Build a Five Gigawatt Data Center?\n\n\t\t\t\t\t\t\t\t\tLynette Bye\n\n\t\t\t\t\t\tGreening the Solar System\n\n\t\t\t\t\t\t\t\t\tEdwin Kite\n\n\t\t\t\t\t\t\t\t\tRobin Wordsworth\n\n\t\t\t\t\t\tThe Case for Insect Consciousness\n\n\t\t\t\t\t\t\t\t\tBob Fischer\n\n\t\t\t\t\t\tThe Unbearable Loudness of Chewing\n\n\t\t\t\t\t\t\t\t\tJake Eaton\n\n\t\t\t\t\t\tYes, Shrimp Matter\n\n\t\t\t\t\t\t\t\t\tAndrés Jiménez Zorrilla\n\nManage Consent PreferencesStrictly Necessary CookiesAlways ActiveThese cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.Performance Cookies  Performance Cookies These cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site. All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.Functional Cookies  Functional Cookies These cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages. If you do not allow these cookies then some or all of these services may not function properly.Targeting Cookies  Targeting Cookies These cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\n\nBack ButtonPerformance Cookies  Search IconFilter IconClear checkbox label labelApply CancelConsent Leg.Interest checkbox label label checkbox label label checkbox label label\n\nClear checkbox label labelApply Cancel\n\nConsent Leg.Interest checkbox label label checkbox label label checkbox label label",
    "summary": {
      "en": "**Summary: We Can, Must, and Will Simulate Nematode Brains**\n\nScientists have been trying for over 25 years to create computer simulations of the nematode brain, which has just 300 neurons. Although past attempts have struggled, advancements in technology now make it feasible, and it is seen as essential for future neuroscience.\n\nThe journey began in the 1970s when C. elegans was recognized as a valuable model for studying simple brain structures. Initial mapping of its neural connections occurred in the 1980s, but real progress in simulation has been slow due to technological limitations and a lack of understanding of how neuronal connections function.\n\nRecent improvements in imaging techniques allow researchers to observe live neuronal activity in real time. New methods, like light sheet microscopy and advanced fluorescent indicators, enhance our ability to study these simple brains and gather data from many individuals.\n\nThe key to successful brain simulation lies not only in mapping connections but also in understanding the dynamic behavior of neurons. By leveraging modern imaging and machine learning, scientists are optimistic that they can accurately simulate the C. elegans brain. This could pave the way for more complex brain simulations, including those of larger organisms and potentially humans.\n\nIn conclusion, while challenges remain, the current technological landscape offers unprecedented opportunities for brain simulation research, making this an exciting time for scientists in the field.",
      "ko": "과학자들은 25년 넘게 300개의 뉴런으로 구성된 선형동물의 뇌를 컴퓨터로 시뮬레이션하려고 노력해왔습니다. 이전의 시도들은 어려움을 겪었지만, 최근 기술 발전 덕분에 이제는 가능성이 커졌고, 이는 미래 신경과학에 필수적인 것으로 여겨지고 있습니다.\n\n이 연구는 1970년대에 C. elegans가 간단한 뇌 구조를 연구하는 데 유용한 모델로 인정받으면서 시작되었습니다. 1980년대에는 신경 연결의 초기 맵핑이 이루어졌지만, 기술적 한계와 뉴런 연결 기능에 대한 이해 부족으로 인해 시뮬레이션의 실제 진전은 더디었습니다.\n\n최근 이미징 기술의 발전 덕분에 연구자들은 실시간으로 살아있는 뉴런의 활동을 관찰할 수 있게 되었습니다. 빛 시트 현미경과 고급 형광 지표와 같은 새로운 방법들은 이러한 간단한 뇌를 연구하고 여러 개체로부터 데이터를 수집하는 능력을 향상시킵니다.\n\n뇌 시뮬레이션의 성공 열쇠는 연결을 맵핑하는 것뿐만 아니라 뉴런의 동적 행동을 이해하는 데 있습니다. 현대의 이미징 기술과 기계 학습을 활용함으로써 과학자들은 C. elegans의 뇌를 정확하게 시뮬레이션할 수 있을 것이라는 희망을 가지고 있습니다. 이는 더 복잡한 뇌 시뮬레이션, 예를 들어 더 큰 유기체나 인간의 뇌 시뮬레이션으로 나아가는 길을 열 수 있습니다.\n\n현재의 기술 환경은 뇌 시뮬레이션 연구에 전례 없는 기회를 제공하고 있으며, 이는 이 분야의 과학자들에게 흥미로운 시점이 되고 있습니다.",
      "ja": "科学者たちは、神経細胞がわずか300個しかない線虫の脳のコンピュータシミュレーションを25年以上にわたって試みてきました。過去の試みは苦戦していましたが、技術の進歩により、現在では実現可能と見なされており、将来の神経科学にとって重要なステップとされています。\n\nこの研究は1970年代に始まり、線虫のC. elegansが単純な脳構造を研究するための貴重なモデルとして認識されました。1980年代には神経接続の初期マッピングが行われましたが、技術的な制約や神経接続の機能に対する理解不足のため、シミュレーションの進展は遅れていました。\n\n最近のイメージング技術の改善により、研究者は生きた神経活動をリアルタイムで観察できるようになりました。光シート顕微鏡や高度な蛍光指標などの新しい手法は、これらの単純な脳を研究し、多くの個体からデータを収集する能力を向上させています。\n\n脳のシミュレーションを成功させる鍵は、接続のマッピングだけでなく、神経細胞の動的な挙動を理解することにもあります。現代のイメージング技術と機械学習を活用することで、科学者たちはC. elegansの脳を正確にシミュレーションできると楽観視しています。これにより、より複雑な脳のシミュレーション、さらには大きな生物や人間の脳のシミュレーションへの道が開かれる可能性があります。\n\n課題は残っていますが、現在の技術環境は脳シミュレーション研究に前例のない機会を提供しており、これはこの分野の科学者にとって刺激的な時期となっています。"
    }
  },
  {
    "id": "6b2ece82c228a417",
    "title": {
      "en": "Interactive article about heart arrhythmias",
      "ko": "심장 부정맥 탐구",
      "ja": "心房細動の真実"
    },
    "type": "story",
    "url": "https://jenevoldsen.com/posts/excitable-cells/",
    "score": 173,
    "by": "johannes_ne",
    "time": 1743167152,
    "content": "Pacemaker cells\n\nSinus rhythm. From ECGpedia.\n\nAnother interesting property of cardiomyocytes is their automaticity; cardiomyocytes spontaneously activate if they are not stimulated by a neighboring cells for a period. Cells in the sinoatrial node (sinus node) in the right atrium have the fastest rate of spontaneous activation, and thus activate the remaining heart, making the sinus node the heart’s pacemaker.\nIn the following simulation, a cluster of cells in the top left corner act as pacemaker cells.\n\n👆️ Move the slider below the simulation to change the rate of spontaneous depolarization (heart rate).\n\nWhen the spontaneous depolarization of the sinus node determines the heart rate, the heart is said to be in sinus rhythm, which is the normal (physiological) activation path of the heart. If the sinus node paces the heart at a rate faster than 100 beats per minute, the rhythm is called sinus tachycardia. Sinus tachycardia (together with increased stroke volume) increases the flow of blood from the heart, and is the physiological response to exercise.\n\nReentry tachycardia\n\nVentricular tachycardia. From Ecgpedia.\n\nIn reentry tachycardia, the heart is not paced by the sinus node, but instead from a group of cells that have formed a circuit, where a wave of depolarization can loop around and stimulate itself over and over. This requires a non-responsive area among the cells (an area that cannot be activated), so a circuit can form around it.\nIn the simulation below, the red cells are non-responsive (dead). They could represent scar tissue after a myocardial infarction—a common cause of ventricular tachycardia. Or the simulation could illustrate atrial flutter, where a reentrant loop can form around the tricuspid valve.\n\n👆️ To stop the reentrant tachycardia, shock the cardiac cells by pressing the “⚡️ Defibrillate!” button.\n\n ⚡️ Defibrillate!\n\nAfter defibrillation, the system is in sinus rhythm. The pacemaker cells were always there, but were suppressed by the reentrant loop.\nThe simulation above illustrates how a reentrant loop can sustain itself around a dead area, but how does it start in the first place?\n\n👆️ You can try to click/tap on the simulation above to set of an ectopic beat (a depolarization starting outside the sinus node), but you will not be able to initiate a reentrant loop again.\n\nHow does a reentrant loop start?\nTo initiate a reentrant loop, the wave of depolarization must be traveling only one way around the dead area. Otherwise the two waves traveling in opposite will just eliminate each other when they meet at the other side. However, if one pathway has cells with a longer refractory period, a wave of depolarization can arrive exactly when one pathway is ready to depolarize, while the other is still refractory.\nIn the simulation below, cells in the lower left corner have a longer refractory period.\n\n👆️ Try initiating an ectopic beat (click/tap) next to the area with a longer refractory period, while the area is still refractory.\n\n ⚡️ Defibrillate!\n\n👆️ You can get the system back in sinus rhythm either by ⚡️ defibrillation, or by cleverly timing a new ectopic beat to block the reentrant loop.\n\nHow does a reentrant loop start?\nTo initiate a reentrant loop, the wave of depolarization must be traveling only one way around the dead area. Otherwise the two waves traveling in opposite will just eliminate each other when they meet at the other side. However, if one pathway has cells with a longer refractory period, a wave of depolarization can arrive exactly when one pathway is ready to depolarize, while the other is still refractory.\nIn the simulation below, cells in the lower left corner have a longer refractory period.\n\n👆️ Try initiating an ectopic beat (click/tap) next to the area with a longer refractory period, while the area is still refractory.\n\n ⚡️ Defibrillate!\n\n👆️ You can get the system back in sinus rhythm either by ⚡️ defibrillation, or by cleverly timing a new ectopic beat to block the reentrant loop.\n\nFibrillation\n\nVentricular fibrillation. From Ecgpedia.\n\nAnother common cardiac arrythmia is fibrillation—atrial fibrillation or ventricular fibrillation. Fibrillation is a reentrant arrhythmia, but the wave of depolarization does not propagate around a fixed anatomical area, but rather meanders irregularly through the myocardium.\nIn the simulation below, cells have different refractory times. An ectopic depolarization will activate some cells, while others are still refractory. This can create a quite complex pattern of depolarization, and may create a reentrant loop, without any dead area to loop around.\n\n👆️ Try setting of a few ectopic beats in the simulation below, and see if you can initiate one or more reentrant loops.\nMake the system more unstable by changing the scale (with high scale, nearby cells have similar refractory times) and range (overall range of refractory times in the system). High range and low scale makes the system unstable.\n\nSet variability in refractory time.\nScale:  Range:  ⚡️ Defibrillate!\n\nStimulating the system while some cells are refractory, while others are not, corresponds to an ectopic beat occurring during the T-wave of an ECG—a high-risk situation for ventricular fibrillation. Also, a high range of refractory times, corresponds to a wide T-wave in the ECG.\n\nAcknowledgment\nThis article is inspired by Bartosz Ciechanowski’s amazing interactive articles.",
    "summary": {
      "en": "**Summary of Pacemaker Cells and Heart Rhythm**\n\n- **Pacemaker Cells:** The sinoatrial node (or sinus node) in the right atrium acts as the heart's natural pacemaker, generating the fastest rate of spontaneous activation. This rhythm is known as sinus rhythm, which is the normal heartbeat.\n\n- **Sinus Tachycardia:** If the sinus node sends signals at over 100 beats per minute, it leads to a condition called sinus tachycardia, which increases blood flow, especially during exercise.\n\n- **Reentry Tachycardia:** In this condition, a group of heart cells creates a circuit that allows a wave of depolarization to continuously stimulate the heart, bypassing the sinus node. This often occurs around a non-responsive area, like scar tissue from a heart attack.\n\n- **Initiating a Reentrant Loop:** A reentrant loop starts when a wave of depolarization travels in one direction around a non-responsive area. If one pathway has cells with a longer refractory period, it can allow the wave to propagate while the other pathway is still recovering.\n\n- **Fibrillation:** This is a more chaotic form of arrhythmia where depolarization waves move irregularly through heart tissue, rather than in a structured loop. It can lead to serious conditions like atrial or ventricular fibrillation.\n\n- **Simulation and Interaction:** The text mentions interactive simulations that allow users to manipulate heart rhythms and observe how ectopic beats can influence the heart's normal rhythm.\n\nOverall, the heart relies on specialized cells to maintain a steady rhythm, and disruptions can lead to various arrhythmias, which can be simulated and studied to understand their mechanisms.",
      "ko": "심박조율세포는 심장의 자연적인 박동 조율 역할을 하는 세포로, 우심방에 위치한 동방결절이 가장 빠른 자발적 활성화 속도를 생성합니다. 이 리듬은 정상적인 심장 박동인 동리듬으로 알려져 있습니다.\n\n동리듬이 1분에 100회 이상 신호를 보낼 경우, 이는 동리듬 빈맥이라는 상태로 이어지며, 특히 운동 중에 혈류가 증가합니다.\n\n재진입 빈맥은 심장 세포 그룹이 회로를 형성하여 동방결절을 우회하면서 지속적으로 심장을 자극하는 상태입니다. 이는 종종 심장마비로 인한 흉터 조직과 같은 반응하지 않는 영역 주위에서 발생합니다.\n\n재진입 루프는 탈분극 파동이 반응하지 않는 영역 주위를 한 방향으로 이동할 때 시작됩니다. 한 경로의 세포가 더 긴 불응기를 가지면, 다른 경로가 회복 중일 때도 파동이 전파될 수 있습니다.\n\n세동은 더 혼란스러운 형태의 부정맥으로, 탈분극 파동이 심장 조직을 통해 불규칙하게 이동합니다. 이는 심방세동이나 심실세동과 같은 심각한 상태로 이어질 수 있습니다.\n\n이 텍스트에서는 사용자가 심장 리듬을 조작하고 이소성 박동이 정상 리듬에 미치는 영향을 관찰할 수 있는 상호작용 시뮬레이션에 대해 언급하고 있습니다. 전반적으로 심장은 안정적인 리듬을 유지하기 위해 특수 세포에 의존하며, 이러한 방해는 다양한 부정맥으로 이어질 수 있습니다. 이러한 부정맥은 그 메커니즘을 이해하기 위해 시뮬레이션하고 연구할 수 있습니다.",
      "ja": "心臓のリズムを維持するためには、特別な細胞が重要な役割を果たしています。右心房にある洞房結節は、心臓の自然なペースメーカーとして機能し、最も早い自発的な活動を生成します。このリズムは洞調律と呼ばれ、正常な心拍を示します。\n\n洞調律が100拍以上になると、洞房結節が信号を送ることで洞性頻脈という状態になります。これは特に運動中に血流が増加することを意味します。\n\n再入ループ頻脈では、心臓の細胞の一群が回路を形成し、洞房結節をバイパスして心臓を刺激し続ける脱分極の波を生み出します。これは心筋梗塞による瘢痕組織など、反応しない領域の周りでよく発生します。\n\n再入ループは、脱分極の波が反応しない領域の周りを一方向に進むことで始まります。一方の経路に長い不応期を持つ細胞があると、その波が伝播することができ、もう一方の経路が回復している間に進むことが可能になります。\n\n心房細動や心室細動のようなより混沌とした不整脈は、脱分極の波が心筋を不規則に移動することで発生します。これは深刻な状態を引き起こす可能性があります。\n\nまた、心拍リズムを操作し、異所性拍動が正常なリズムにどのように影響するかを観察できるインタラクティブなシミュレーションも紹介されています。心臓は安定したリズムを維持するために特化した細胞に依存しており、リズムの乱れはさまざまな不整脈を引き起こすことがあります。これらのメカニズムを理解するために、シミュレーションを通じて研究することが重要です。"
    }
  },
  {
    "id": "fe6ab7e585c71ac1",
    "title": {
      "en": "Systems Correctness Practices at AWS: Leveraging Formal and Semi-Formal Methods",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://queue.acm.org/detail.cfm?id=3712057",
    "score": 8,
    "by": "yarapavan",
    "time": 1743519582,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "fc106686a4dcdb96",
    "title": {
      "en": "Show HN: Nue – Apps lighter than a React button",
      "ko": "Nue: 리액트보다 가벼운 앱",
      "ja": "Nue: 軽量アプリ登場"
    },
    "type": "story",
    "url": "https://nuejs.org/blog/large-scale-apps/",
    "score": 494,
    "by": "tipiirai",
    "time": 1743486461,
    "content": "April 1, 2025\n\n  Apps lighter than a React button\n\n    Tero Piirainen\n    @tipiirai\n\n      On this release, we’re showing what happens when you push modern web standards—HTML, CSS, and JS—to their peak:\n\n    <video type=\"video/mp4\" controls src=\"https://video.nuejs.org/39b76cca-e55b-4e9b-8583-b053f9dbd55d/play_720p.mp4\">\n  </video>\n\n  {\"videoid\":\"39b76cca-e55b-4e9b-8583-b053f9dbd55d\",\"poster\":\"thumbnail_70d8de32.jpg\",\"width\":\"704\",\"height\":\"407\"}\n\nThis entire app is lighter than a React/ShadCN button:\n\nSee benchmark and details here ›\nGoing large-scale\nHere’s the same app, now with a Rust computation engine and Event Sourcing for instant search and other operations over 150,000 records — far past where JS-version of the engine crashed with a maximum call stack exception.\n\n    <video type=\"video/mp4\" controls src=\"https://video.nuejs.org/eb65fcdd-5be4-4923-a783-f41efafe58a7/play_720p.mp4\">\n  </video>\n\n  {\"videoid\":\"eb65fcdd-5be4-4923-a783-f41efafe58a7\",\"poster\":\"/img/rust-splash.png\",\"width\":\"704\",\"height\":\"440\"}\n\n    Instant operations across 150.000 records with Rust/WASM\n\nThis demo is here ›\nTooling\nNue crushes HMR and build speed records and sets you up with a millisecond feedback loop for your everyday VSCode/Sublime file-save operations:\n\n    <video type=\"video/mp4\" controls src=\"https://video.nuejs.org/ffbb6d40-5b74-4176-a115-d0ed040edca5/play_720p.mp4\">\n  </video>\n\n  {\"videoid\":\"ffbb6d40-5b74-4176-a115-d0ed040edca5\",\"poster\":\"\",\"width\":\"\",\"height\":\"\"}\n\n    Immediate feedback for design and component updates, preserving app state\n\nHere's what this means:\nFor Rust, Go, and JS engineers\nThis is a game-changer for Rust, Go, and JS engineers stuck wrestling with React idioms instead of leaning on timeless software patterns. Nue emphasizes a model-first approach, delivering modular design with simple, testable functions, true static typing, and minimal dependencies. Nue is a liberating experience for system devs whose skills can finally shine in a separated model layer.\nFor Design Engineers\nThis is an important shift for design engineers bogged down by React patterns and 40,000+ line design systems. Build radically simpler systems with modern CSS (@layers, variables, calc()) and take control of your typography and whitespace.\nFor UX Engineers\nThis is a wake-up call for UX engineers tangled in React hooks and utility class walls instead of owning the user experience. Build apps as light as a React button to push the web—and your skills—forward.\nFAQ: WTH is Nue?\nNue is a web framework focused on web standards, currently in active development. I'm aiming to reveal the hidden complexity that’s become normalized in modern web development. When a single button outweighs an entire application, something’s fundamentally broken.\nNue drives the inevitable shift. We’re rebuilding tools and frameworks from the ground up with a cleaner, more robust architecture. Our goal is to bring back the joy of web development for everyone—whether you’re focused on performance, design, or UX.\n\n  What's next\n  We're improving the developer experience in three distinct phases:\n\n  Join the mailing list to follow our progress and see how our vision unfolds:",
    "summary": {
      "en": "**Summary:**\n\nOn April 1, 2025, Tero Piirainen introduced Nue, a web framework designed to optimize modern web standards (HTML, CSS, JS) for better performance. The framework allows apps to be lighter than a typical React button, showcasing impressive benchmarks.\n\nNue features a Rust computation engine that supports instant operations on over 150,000 records, overcoming limitations faced by JavaScript engines. The framework also significantly accelerates build speeds and offers instant feedback during development, making it easier for engineers who typically struggle with React's complexities.\n\nNue aims to simplify the development process for Rust, Go, and JavaScript engineers by promoting a model-first approach and reducing dependencies. It also helps design engineers create simpler systems using modern CSS and empowers UX engineers to focus on user experience without being bogged down by intricate frameworks.\n\nNue is still in development and seeks to transform web development by addressing the complexities that have become common in the industry, ultimately making it a more enjoyable experience for developers. Interested individuals can join a mailing list to stay updated on its progress.",
      "ko": "2025년 4월 1일, 테로 피이라이넨은 현대 웹 표준(HTML, CSS, JS)의 성능을 최적화하기 위해 설계된 웹 프레임워크인 누(Nue)를 소개했습니다. 이 프레임워크는 일반적인 리액트 버튼보다 가벼운 앱을 만들 수 있도록 하며, 뛰어난 성능 지표를 보여줍니다.\n\n누는 15만 개 이상의 기록에 대해 즉각적인 작업을 지원하는 러스트(Rust) 계산 엔진을 특징으로 하며, 자바스크립트 엔진이 겪는 한계를 극복합니다. 이 프레임워크는 빌드 속도를 크게 향상시키고 개발 중 즉각적인 피드백을 제공하여, 리액트의 복잡성에 어려움을 겪는 엔지니어들에게 더 쉽게 접근할 수 있도록 돕습니다.\n\n누는 러스트, 고(Go), 자바스크립트 엔지니어들이 모델 우선 접근 방식을 채택하고 의존성을 줄이도록 하여 개발 과정을 단순화하는 것을 목표로 하고 있습니다. 또한 디자인 엔지니어들이 현대 CSS를 사용해 더 간단한 시스템을 만들 수 있도록 지원하고, UX 엔지니어들이 복잡한 프레임워크에 얽매이지 않고 사용자 경험에 집중할 수 있게 합니다.\n\n누는 현재 개발 중이며, 업계에서 흔히 발생하는 복잡성을 해결하여 웹 개발을 혁신하고 개발자들에게 더 즐거운 경험을 제공하는 것을 목표로 하고 있습니다. 관심 있는 사람들은 진행 상황을 업데이트 받을 수 있는 메일링 리스트에 가입할 수 있습니다.",
      "ja": "2025年4月1日、テロ・ピイライネン氏は、現代のウェブ標準（HTML、CSS、JS）を最適化し、パフォーマンスを向上させるために設計されたウェブフレームワーク「Nue」を発表しました。このフレームワークは、一般的なReactボタンよりも軽量なアプリを実現し、優れたベンチマークを示しています。\n\nNueは、15万件以上のデータに対して即時操作をサポートするRustの計算エンジンを搭載しており、JavaScriptエンジンが抱える制約を克服しています。また、ビルド速度を大幅に向上させ、開発中に即時フィードバックを提供することで、Reactの複雑さに悩むエンジニアにとって使いやすくなっています。\n\nNueは、Rust、Go、JavaScriptのエンジニアがモデルファーストアプローチを採用し、依存関係を減らすことで開発プロセスを簡素化することを目指しています。さらに、デザインエンジニアが現代のCSSを使ってシンプルなシステムを構築できるよう支援し、UXエンジニアが複雑なフレームワークに煩わされることなくユーザー体験に集中できるようにしています。\n\nNueは現在開発中で、業界で一般的になっている複雑さに対処することでウェブ開発を変革し、開発者にとってより楽しい体験を提供することを目指しています。興味のある方は、進捗を知るためのメーリングリストに参加することができます。"
    }
  },
  {
    "id": "1ba2aeb9d640062e",
    "title": {
      "en": "Launch HN: ASim (YC S21) – Mobile app that generates mobile apps",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": null,
    "score": 8,
    "by": "dli123",
    "time": 1743519971,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "b9a96ce4b7be1628",
    "title": {
      "en": "Show HN: Duolingo-style exercises but with real-world content like the news",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://app.fluentsubs.com/exercises/daily",
    "score": 297,
    "by": "ph4evers",
    "time": 1743486394,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "b80791c80f24c436",
    "title": {
      "en": "The April Fools joke that might have got me fired",
      "ko": "해고 위기 장난!",
      "ja": "クビ寸前のエイプリルフール"
    },
    "type": "story",
    "url": "http://oldvcr.blogspot.com/2025/04/the-april-fools-joke-that-might-have.html",
    "score": 213,
    "by": "goldenskye",
    "time": 1743491490,
    "content": "Old Vintage Computing Research\n\nREWIND and PLAY\n\nTuesday, April 1, 2025\n\nThe April Fools joke that might have got me fired\n\nEveryone should pull one great practical joke in their lifetimes. This one was mine, and I think it's past the statute of limitations. The story is true. Only the names are redacted to protect the guilty.\n\nMy first job out of college was a database programmer, even though my undergraduate degree had nothing to do with computers and my current profession still mostly doesn't. The reason was that the University I worked for couldn't afford competitive wages, but they did offer various fringe benefits, and they were willing to train someone who at least had decent working knowledge. I, as a newly minted graduate of the august University of California system, had decent working knowledge at least of BSD/386 and SunOS, but more importantly also had the glowing recommendation of my predecessor who was being promoted into a new position. I was hired, which was their first mistake.\n\nThe system I was hired to work on was an HP 9000 K250, one of Hewlett-Packard's big PA-RISC servers. I wish I had a photograph of it, but all I have are a couple bad scans of some bad Polaroids of my office and none of the server room. The server room was downstairs from my office back in the days when server rooms were on-premises, complete with a swipe card lock and a halon system that would give you a few seconds of grace before it flooded everything. The K250 hulked in there where it had recently replaced what I think was an Encore mini of some sort (probably a Multimax, since it was a few years old and the 88K Encores would have been too new for the University), along with the AIX RS/6000s that provided student and faculty shell accounts and E-mail, the bonded T1 lines, some of the terminal servers, the massive Cabletron routers and a lot of the telco stuff. One of the tape reels from the Encore hangs on my wall today as a memento.\n\nThe K250 and the Encore it replaced (as well as the L-Class that later replaced the K250 when I was a consultant) ran an all-singing, all-dancing student information system called CARS. CARS is still around, renamed Jenzabar, though I suspect that many of its underpinnings remain if you look under the table. In those days CARS was a massive overlay that was loaded atop the operating system and database, which when I started were, respectively, HP/UX 10.20 and Informix. (I'm old.) It used Informix tables, screens and stored procedures plus its own text UI libraries to run code written variously as Perform screens, SQL, C-shell scripts and plain old C or ESQL/C. Everything was tracked in RCS using overgrown Makefiles. I had the admin side (resource management, financials, attendance trackers, etc.) and my office partner had the academic side (mostly grades and faculty tracking). My job was to write and maintain this code and shortly after to help the University create custom applications in CARS' brand-spanking new web module, which chose the new hotness in scripting languages, i.e., Perl. Fortuitously I had learned Perl in, appropriately enough, a computational linguistics course.\n\nCARS also managed most of the printers on campus except for the few that the RS/6000s controlled directly. Most of the campus admin printers were HP LaserJet 4 units of some derivation equipped with JetDirect cards for networking. These are great warhorse printers, some of the best laser printers HP ever made. I suspect there were line printers other places, but those printers were largely what existed in the University's offices.\n\nIt turns out that the READY message these printers show on their VFD panels is changeable. I don't remember where I read this, probably idly paging through the manual over a lunch break, but initially the only fun things I could think of to do was to have the printer say hi to my boss when she sent jobs to it, stuff like that (whereupon she would tell me to get back to work). Then it dawned on me: because I had access to the printer spools on the K250, and the spool directories were conveniently named the same as their hostnames, I knew where each and every networked LaserJet on campus was. I was young, rash and motivated. This was a hack I just couldn't resist. It would be even better than what had been my favourite joke at my alma mater, where campus services, notable for posting various service suspension notices, posted one April Fools' Day that gravity itself would be suspended to various buildings. I felt sure this hack would eclipse that too.\n\nThe plan on April Fools' Day was to get into work at OMG early o'clock and iterate over every entry in the spool, sending it a sequence that would change the READY message to INSERT 5 CENTS. This would cause every networked LaserJet on campus to appear to ask for a nickel before you printed anything. The script was very simple (this is the actual script, I saved it):\n\n#!/bin/csh -f\n\ncd /opt/carsi/spool\nforeach i (*)\n        echo '^[%-12345X@PJL RDYMSG DISPLAY=\"INSERT 5 CENTS\"' | netto $i 9100\nend\n\nThe ^[ was a literal ASCII 27 ESCape character, and netto was a simple netcat-like script I had written in these days before netcat was widely used. That's it.\n\nNow, let me be clear: the printer was still ready! The effect was merely cosmetic! It would still print if you sent jobs to it! Nevertheless, to complete the effect, this message was sent out on the campus-wide administration mailing list (which I also saved):\n\nTo: xxx@xxx.xxx\nDate: xxx, 1 Apr xxxx 05:41:34 -0800 (PST)\nSubject: IMPORTANT NOTE ON PRINTER POLICY\n\nDue to the increasing costs of service commitments for campus printers,\nall printers on campus will be reprogrammed for pay-per-page service\nto defray these mounting expenses, effective immediately.\n\nMost printers will now require a 5 cent deposit per page for printing. This\nmay be paid on account or through special coin acceptors to be installed\non the unit by technicians through the end of this week. If your office has\nnot yet established an account, your printer will automatically request you to\ninsert 5 cents into the slot per page to be printed. Please check your\nprinter's LCD [sic] display to see if your printer requires the 5 cents per\npage before using your printer.\n\nAdditional printers will be retrofitted as soon as possible. Technicians\nwill be contacting departments with specific details.\n\nAll accounts will be maintained on CARS. Do not call the Helpdesk. To\nestablish or verify your department's printer account, please call me at\nxxxx.\n\nPlease also direct all questions regarding this new policy to me as well.\n\nWe apologise for the inconvenience and hope that the new cost requirement\nwill not adversely affect your department's productivity.\n\nAt the end of the day I would reset everything back to READY, smile smugly, and continue with my menial existence. That was the plan.\n\nHaving sent this out, I fielded a few anxious calls, who laughed uproariously when they realized, and I reset their printers manually afterwards. The people who knew me, knew I was a practical joker, took note of the date, and sent approving replies. One of the best was sent to me later in the day by intercampus mail, printed on their laser printer, with a nickel taped to it.\n\nUnfortunately, not everybody on campus knew me, and those who did not not only did not call me, but instead called university administration directly. By 8:30am it was chaos in the main office and this filtered up to the head of HR, who most definitely did know me, and told me I'd better send a retraction before the CFO got in or I was in big trouble. That went wrong also, because my retraction said that campus administration was not considering charging per-page fees when in fact they actually were, so I had to retract it and send a new retraction that didn't call attention to that fact. I also ran the script to reset everything early. Eventually the hubbub finally settled down around noon. Everybody in the office thought it was very funny. Even my boss, who officially disapproved, thought it was somewhat funny.\n\nThe other thing that went wrong, as if all that weren't enough, was that the director of IT — which is to say, my boss's boss — was away on vacation when all this took place. (Read E-mail remotely? Who does that?) I compounded this situation with the tactical error of going skiing over the coming weekend and part of the next week, most of which I spent snowplowing down the bunny slopes face first, so that he discovered all the angry E-mail in his box without me around to explain myself. (My office partner remembers him coming in wide-eyed asking, \"what did he do??\") When I returned, it was icier in the office than it had been on the mountain. The assistant director, who thought it was funny, was in trouble for not putting a lid on it, and I was in really big trouble for doing it in the first place. I was appropriately contrite and made various apologies and was an uncharacteristically model employee for an unnaturally long period of time.\n\nThe Ice Age eventually thawed and the incident was officially dropped except for a \"poor judgment\" on my next performance review and the satisfaction of what was then considered the best practical joke ever pulled on campus. Indeed, everyone agreed it was much more technically accomplished than the previous award winner, where someone had supposedly gotten it around the grounds that the security guards at the entrance would be charging a nominal admission fee per head. Years later they still said it was legendary.\n\nI like to think they still do.\n\nPosted by\n\nClassicHasClass\n\nat\n\n12:03 AM\n\nEmail ThisBlogThis!Share to XShare to FacebookShare to Pinterest\n\nLabels:\nhp,\nprotip\n\n1 comment:\n\n    (function() {\n      var items = null;\n      var msgs = null;\n      var config = {};\n\n// <![CDATA[\n      var cursor = null;\n      if (items && items.length > 0) {\n        cursor = parseInt(items[items.length - 1].timestamp) + 1;\n      }\n\n      var bodyFromEntry = function(entry) {\n        var text = (entry &&\n                    ((entry.content && entry.content.$t) ||\n                     (entry.summary && entry.summary.$t))) ||\n            '';\n        if (entry && entry.gd$extendedProperty) {\n          for (var k in entry.gd$extendedProperty) {\n            if (entry.gd$extendedProperty[k].name == 'blogger.contentRemoved') {\n              return '<span class=\"deleted-comment\">' + text + '</span>';\n            }\n          }\n        }\n        return text;\n      }\n\n      var parse = function(data) {\n        cursor = null;\n        var comments = [];\n        if (data && data.feed && data.feed.entry) {\n          for (var i = 0, entry; entry = data.feed.entry[i]; i++) {\n            var comment = {};\n            // comment ID, parsed out of the original id format\n            var id = /blog-(\\d+).post-(\\d+)/.exec(entry.id.$t);\n            comment.id = id ? id[2] : null;\n            comment.body = bodyFromEntry(entry);\n            comment.timestamp = Date.parse(entry.published.$t) + '';\n            if (entry.author && entry.author.constructor === Array) {\n              var auth = entry.author[0];\n              if (auth) {\n                comment.author = {\n                  name: (auth.name ? auth.name.$t : undefined),\n                  profileUrl: (auth.uri ? auth.uri.$t : undefined),\n                  avatarUrl: (auth.gd$image ? auth.gd$image.src : undefined)\n                };\n              }\n            }\n            if (entry.link) {\n              if (entry.link[2]) {\n                comment.link = comment.permalink = entry.link[2].href;\n              }\n              if (entry.link[3]) {\n                var pid = /.*comments\\/default\\/(\\d+)\\?.*/.exec(entry.link[3].href);\n                if (pid && pid[1]) {\n                  comment.parentId = pid[1];\n                }\n              }\n            }\n            comment.deleteclass = 'item-control blog-admin';\n            if (entry.gd$extendedProperty) {\n              for (var k in entry.gd$extendedProperty) {\n                if (entry.gd$extendedProperty[k].name == 'blogger.itemClass') {\n                  comment.deleteclass += ' ' + entry.gd$extendedProperty[k].value;\n                } else if (entry.gd$extendedProperty[k].name == 'blogger.displayTime') {\n                  comment.displayTime = entry.gd$extendedProperty[k].value;\n                }\n              }\n            }\n            comments.push(comment);\n          }\n        }\n        return comments;\n      };\n\n      var paginator = function(callback) {\n        if (hasMore()) {\n          var url = config.feed + '?alt=json&v=2&orderby=published&reverse=false&max-results=50';\n          if (cursor) {\n            url += '&published-min=' + new Date(cursor).toISOString();\n          }\n          window.bloggercomments = function(data) {\n            var parsed = parse(data);\n            cursor = parsed.length < 50 ? null\n                : parseInt(parsed[parsed.length - 1].timestamp) + 1\n            callback(parsed);\n            window.bloggercomments = null;\n          }\n          url += '&callback=bloggercomments';\n          var script = document.createElement('script');\n          script.type = 'text/javascript';\n          script.src = url;\n          document.getElementsByTagName('head')[0].appendChild(script);\n        }\n      };\n      var hasMore = function() {\n        return !!cursor;\n      };\n      var getMeta = function(key, comment) {\n        if ('iswriter' == key) {\n          var matches = !!comment.author\n              && comment.author.name == config.authorName\n              && comment.author.profileUrl == config.authorUrl;\n          return matches ? 'true' : '';\n        } else if ('deletelink' == key) {\n          return config.baseUri + '/comment/delete/'\n               + config.blogId + '/' + comment.id;\n        } else if ('deleteclass' == key) {\n          return comment.deleteclass;\n        }\n        return '';\n      };\n\n      var replybox = null;\n      var replyUrlParts = null;\n      var replyParent = undefined;\n\n      var onReply = function(commentId, domId) {\n        if (replybox == null) {\n          // lazily cache replybox, and adjust to suit this style:\n          replybox = document.getElementById('comment-editor');\n          if (replybox != null) {\n            replybox.height = '250px';\n            replybox.style.display = 'block';\n            replyUrlParts = replybox.src.split('#');\n          }\n        }\n        if (replybox && (commentId !== replyParent)) {\n          replybox.src = '';\n          document.getElementById(domId).insertBefore(replybox, null);\n          replybox.src = replyUrlParts[0]\n              + (commentId ? '&parentID=' + commentId : '')\n              + '#' + replyUrlParts[1];\n          replyParent = commentId;\n        }\n      };\n\n      var hash = (window.location.hash || '#').substring(1);\n      var startThread, targetComment;\n      if (/^comment-form_/.test(hash)) {\n        startThread = hash.substring('comment-form_'.length);\n      } else if (/^c[0-9]+$/.test(hash)) {\n        targetComment = hash.substring(1);\n      }\n\n      // Configure commenting API:\n      var configJso = {\n        'maxDepth': config.maxThreadDepth\n      };\n      var provider = {\n        'id': config.postId,\n        'data': items,\n        'loadNext': paginator,\n        'hasMore': hasMore,\n        'getMeta': getMeta,\n        'onReply': onReply,\n        'rendered': true,\n        'initComment': targetComment,\n        'initReplyThread': startThread,\n        'config': configJso,\n        'messages': msgs\n      };\n\n      var render = function() {\n        if (window.goog && window.goog.comments) {\n          var holder = document.getElementById('comment-holder');\n          window.goog.comments.render(holder, provider);\n        }\n      };\n\n      // render now, or queue to render when library loads:\n      if (window.goog && window.goog.comments) {\n        render();\n      } else {\n        window.goog = window.goog || {};\n        window.goog.comments = window.goog.comments || {};\n        window.goog.comments.loadQueue = window.goog.comments.loadQueue || [];\n        window.goog.comments.loadQueue.push(render);\n      }\n    })();\n// ]]>\n\nDanApril 1, 2025 at 5:39 AMMany, many years ago the Computer Science students of a certain UK university used a PostScript flaw to hack into the departmental printers (which were different from the main Uni printers) on Friday evening. They had earlier obtained the fingerprints of the head of department by devious means, and for one whole weekend, printed as faintly as possible were the fingerprints of the head of department on every sheet of output.On Sunday night they re-ran the hack and removed this \"feature\".ReplyDeleteRepliesReplyAdd commentLoad more...\n\nComments are subject to moderation. Be nice.\n\n      BLOG_CMT_createIframe('https://draft.blogger.com/rpc_relay.html');\n\nOlder Post\n\nHome\n\nSubscribe to:\nPost Comments (Atom)\n\nWelcome to Old VCR\n\nMy general vintage computing projects, mostly microcomputers, 6502, PalmOS, 68K/Power Mac and Unix workstations, but that's not all you'll see. While over the decades I've written for publications like COMPUTE, TidBITS and Ars Technica, these articles are all original and just for you. My promise: No AI-generated article text, ever. Be kind, REWIND and PLAY. -- Cameron Kaiser\n\nOld VCR is advertisement- and donation-funded, and what I get goes to maintaining the hardware here at Floodgap. I don't drink coffee, but the Mr Pibb doesn't buy itself. :-) Thanks for reading.\n\n(adsbygoogle = window.adsbygoogle || []).push({});\n\n(adsbygoogle = window.adsbygoogle || []).push({});\n\nGreatest hits\n\nThe MIPS ThinkPad, kind of\n\nMeet your new two-factor authenticator: your Commodore 64\n\nSo thieves broke into your storage unit - again\n\nDusting off Dreamcast Linux\n\nRefurb weekend: Canon Cat\n\nThe Apple Network Server's all-too-secret weapon (featuring PPC Toolbox)\n\nThe April Fools joke that might have got me fired\n\nIf one GUI's not enough for your SPARC workstation, try four\n\nWhen you have too much memory for SheepShaver\n\nSo long, home T1 line; hello, hacking the T1 router\n\nOther stuff I write\n\nOther classic computing posts from TenFourFox Development\nTalospace: OpenPOWER news and experiences from the free computing frontier\nJerk Music Critic: music reviews worth what you paid for them\n\nAbout Me\n\nClassicHasClass\n\nView my complete profile\n\nBlog Archive\n\n        ▼\n\n2025\n\n(4)\n\n        ▼\n\nApril\n\n(1)\n\nThe April Fools joke that might have got me fired\n\n        ►\n\nMarch\n\n(1)\n\n        ►\n\nJanuary\n\n(2)\n\n        ►\n\n2024\n\n(25)\n\n        ►\n\nDecember\n\n(2)\n\n        ►\n\nNovember\n\n(2)\n\n        ►\n\nOctober\n\n(4)\n\n        ►\n\nSeptember\n\n(1)\n\n        ►\n\nJuly\n\n(1)\n\n        ►\n\nJune\n\n(2)\n\n        ►\n\nMay\n\n(3)\n\n        ►\n\nApril\n\n(2)\n\n        ►\n\nMarch\n\n(4)\n\n        ►\n\nFebruary\n\n(2)\n\n        ►\n\nJanuary\n\n(2)\n\n        ►\n\n2023\n\n(39)\n\n        ►\n\nDecember\n\n(1)\n\n        ►\n\nNovember\n\n(2)\n\n        ►\n\nOctober\n\n(3)\n\n        ►\n\nSeptember\n\n(5)\n\n        ►\n\nAugust\n\n(4)\n\n        ►\n\nJuly\n\n(4)\n\n        ►\n\nJune\n\n(4)\n\n        ►\n\nMay\n\n(2)\n\n        ►\n\nApril\n\n(2)\n\n        ►\n\nMarch\n\n(3)\n\n        ►\n\nFebruary\n\n(3)\n\n        ►\n\nJanuary\n\n(6)\n\n        ►\n\n2022\n\n(36)\n\n        ►\n\nDecember\n\n(5)\n\n        ►\n\nNovember\n\n(4)\n\n        ►\n\nOctober\n\n(6)\n\n        ►\n\nSeptember\n\n(4)\n\n        ►\n\nAugust\n\n(3)\n\n        ►\n\nJuly\n\n(3)\n\n        ►\n\nJune\n\n(2)\n\n        ►\n\nMay\n\n(2)\n\n        ►\n\nApril\n\n(1)\n\n        ►\n\nMarch\n\n(2)\n\n        ►\n\nFebruary\n\n(3)\n\n        ►\n\nJanuary\n\n(1)\n\n        ►\n\n2021\n\n(26)\n\n        ►\n\nDecember\n\n(2)\n\n        ►\n\nNovember\n\n(1)\n\n        ►\n\nOctober\n\n(1)\n\n        ►\n\nSeptember\n\n(3)\n\n        ►\n\nAugust\n\n(5)\n\n        ►\n\nJune\n\n(2)\n\n        ►\n\nApril\n\n(3)\n\n        ►\n\nMarch\n\n(4)\n\n        ►\n\nFebruary\n\n(4)\n\n        ►\n\nJanuary\n\n(1)\n\n        ►\n\n2020\n\n(25)\n\n        ►\n\nDecember\n\n(2)\n\n        ►\n\nNovember\n\n(5)\n\n        ►\n\nOctober\n\n(2)\n\n        ►\n\nSeptember\n\n(3)\n\n        ►\n\nAugust\n\n(2)\n\n        ►\n\nJuly\n\n(3)\n\n        ►\n\nJune\n\n(3)\n\n        ►\n\nMay\n\n(2)\n\n        ►\n\nApril\n\n(3)\n\nLabels\n\n3d\n(3)\n\n6502\n(29)\n\n65816\n(9)\n\n6800\n(2)\n\n68000\n(18)\n\n8051\n(2)\n\n9995\n(1)\n\na/ux\n(2)\n\nadministrivia\n(1)\n\naix\n(3)\n\nalpha micro\n(2)\n\namiga\n(1)\n\napple ii\n(4)\n\nappletalk\n(3)\n\narduino\n(1)\n\natari 8-bit\n(1)\n\natari st\n(2)\n\natarilab\n(1)\n\nbebox\n(3)\n\nbeos\n(6)\n\nbrowser\n(7)\n\nbucketlist\n(1)\n\nc128\n(7)\n\nc264\n(1)\n\nc64\n(14)\n\ncanon\n(2)\n\ncap-x comp-x\n(1)\n\ncasio\n(1)\n\nclassilla\n(1)\n\ncobalt\n(1)\n\ncommodore\n(20)\n\nconsole\n(2)\n\ncp/m\n(1)\n\ncp1600\n(1)\n\ncray\n(1)\n\ncrypto ancienne\n(4)\n\ndata general\n(2)\n\ndec\n(2)\n\ndec alpha\n(1)\n\ndecpro\n(1)\n\ndectalk\n(1)\n\ndick smith\n(2)\n\ndos\n(2)\n\ndreamcast\n(2)\n\nemulation\n(1)\n\nfirewire\n(1)\n\nforth\n(2)\n\nfouo\n(1)\n\nfuture\n(1)\n\ngeos\n(1)\n\ngopher\n(7)\n\ngraphics\n(6)\n\nhohoho\n(1)\n\nhp\n(2)\n\nhpux\n(1)\n\nhumour\n(2)\n\niannetta\n(1)\n\nibm\n(1)\n\nintellivision\n(1)\n\ninty\n(1)\n\nitanium\n(1)\n\nivory\n(1)\n\nkim-1\n(8)\n\nlinux\n(1)\n\nlisp\n(1)\n\nlynx\n(3)\n\nmac\n(14)\n\nmagic cap\n(2)\n\nmattel\n(1)\n\nmechanical\n(1)\n\nmemorials\n(12)\n\nmips\n(5)\n\nnetware\n(1)\n\nnetworking\n(13)\n\nnextstep\n(1)\n\nnubus\n(1)\n\npalm\n(7)\n\npanasonic\n(1)\n\nparisc\n(3)\n\npdp-11\n(1)\n\npocket handheld\n(3)\n\npong\n(7)\n\npower mac\n(20)\n\npowerpc\n(8)\n\nprior art\n(3)\n\nprotip\n(3)\n\nrefurb weekend\n(21)\n\nreview\n(6)\n\nscience\n(1)\n\nsega\n(1)\n\nsharp\n(1)\n\nsmartwatch\n(2)\n\nsoftware\n(51)\n\nsolbourne\n(3)\n\nsparc\n(3)\n\nspreadsheet\n(1)\n\nsun\n(2)\n\nsunray\n(2)\n\nsuperh\n(3)\n\nsymbolics\n(1)\n\ntadpole\n(1)\n\ntandy radio shack\n(2)\n\nterminal\n(11)\n\nti\n(4)\n\ntomy tutor\n(4)\n\ntoshiba\n(1)\n\nunboxing\n(1)\n\nunix\n(15)\n\nunscreenshotable\n(5)\n\nusb\n(4)\n\nusenet\n(1)\n\nvenix\n(1)\n\nvtech\n(1)\n\nwince\n(2)\n\nwindows\n(2)\n\nworkslate\n(2)\n\nx86\n(3)\n\nyaddayaddayadda\n(1)\n\nz80\n(3)\n\nCopyright 2020-25 Cameron Kaiser. CC BY-NC-ND 4.0. Powered by Blogger.",
    "summary": {
      "en": "In a humorous anecdote posted on April 1, 2025, a former database programmer recounts a practical joke he pulled at his university job shortly after graduating. He discovered that he could change the message displayed on campus printers to read “INSERT 5 CENTS” as a prank. On April Fools' Day, he implemented a script that changed the display on all networked LaserJet printers, which made it seem like users had to pay to print.\n\nHe also sent out a campus-wide email announcing a new fee policy for printing, which caused confusion and chaos among staff who didn’t know it was a joke. While some colleagues found it amusing, the university administration panicked, leading to a series of retractions and apologies. The prank garnered mixed reactions, with some laughing and others upset, especially since his boss was away on vacation during the incident.\n\nDespite the initial trouble, the prank became legendary, and he received a note with a nickel taped to it as a humorous acknowledgment of his joke. Eventually, the situation calmed down, and he was only lightly reprimanded in his performance review. The story highlights the fun and risks of practical jokes in a workplace setting.",
      "ko": "2025년 4월 1일에 게시된 유머러스한 일화에서 한 전직 데이터베이스 프로그래머가 대학 졸업 후 직장에서 벌인 장난을 회상했다. 그는 캠퍼스 프린터에 표시되는 메시지를 \"5센트를 넣으세요\"로 바꿀 수 있다는 사실을 발견했다. 만우절에 그는 모든 네트워크 연결된 레이저 프린터의 디스플레이를 변경하는 스크립트를 실행하여 사용자가 인쇄를 위해 비용을 지불해야 하는 것처럼 보이게 만들었다.\n\n그는 또한 캠퍼스 전체에 새로운 인쇄 요금 정책을 발표하는 이메일을 보내 혼란과 소동을 일으켰다. 직원들은 이것이 장난이라는 사실을 모르고 당황했다. 일부 동료들은 재미있다고 생각했지만, 대학 행정 측은 패닉에 빠져 일련의 철회와 사과가 이어졌다. 이 장난은 다양한 반응을 불러일으켰고, 특히 사건이 발생하는 동안 그의 상사가 휴가 중이어서 더욱 불만을 샀다.\n\n초기에는 문제가 있었지만, 이 장난은 전설이 되었고, 그는 자신의 장난을 유머러스하게 인정받아 5센트 동전이 붙어 있는 메모를 받았다. 결국 상황은 진정되었고, 그는 성과 평가에서 가벼운 질책만 받았다. 이 이야기는 직장에서의 장난의 재미와 위험성을 잘 보여준다.",
      "ja": "2025年4月1日に投稿されたユーモラスなエピソードでは、元データベースプログラマーが大学の仕事で卒業後すぐに行ったいたずらを語っています。彼は、キャンパスのプリンターに表示されるメッセージを「INSERT 5 CENTS（5セントを挿入）」に変更できることを発見しました。エイプリルフールの日、彼はすべてのネットワーク接続されたレーザープリンターの表示を変更するスクリプトを実行し、ユーザーが印刷するためにお金を支払わなければならないように見せかけました。\n\nさらに、彼はキャンパス全体に新しい印刷料金ポリシーを発表するメールを送信しました。このため、冗談だと知らないスタッフの間に混乱と騒動が起こりました。一部の同僚は面白がっていましたが、大学の管理者はパニックに陥り、一連の撤回と謝罪が行われました。このいたずらには賛否があり、笑う人もいれば、特にその時に上司が休暇中だったために不満を持つ人もいました。\n\n最初のトラブルにもかかわらず、このいたずらは伝説となり、彼は冗談を認めるユーモラスな形で5セント硬貨が貼り付けられたメモを受け取りました。最終的に状況は落ち着き、彼はパフォーマンスレビューで軽い叱責を受けただけでした。この話は、職場でのいたずらの楽しさとリスクを浮き彫りにしています。"
    }
  },
  {
    "id": "bd3730a213f16e97",
    "title": {
      "en": "Globe Gores",
      "ko": "지구의 상처",
      "ja": "地球の傷"
    },
    "type": "story",
    "url": "https://blogs.loc.gov/maps/2025/03/globe-gores/",
    "score": 16,
    "by": "bookofjoe",
    "time": 1743514392,
    "content": "Oterschaden, Johannes, Globus aus der Mitte des XVI Jahrhunderts, 1879, Geography and Map Division.        Globe Gores\n\n                        March 28, 2025\n            Posted by: Abraham Parrish\n\n                Share this post:\n\n        The Geography and Map Division holds a variety of printed globe gores in a variety of sizes and configurations ranging from some of the earliest examples in the 16th century to more modern examples in the 20th century. Globe gores are strips of paper containing printed maps in the sizes and shapes needed for globe construction. The maps can be either terrestrial (showing the earth) or celestial (showing the heavens) and the gores are typically football shaped. The gores can be thought of as pre-assembled building blocks for a globe of a particular size which a globe-maker can quickly utilize to construct a globe without having to spend the time constructing the maps. The printed gores would be cut out of the printed sheet(s) and pasted onto a sphere of appropriate size to construct a globe.\nAs far back as the third century B.C. written records about globe construction by the ancient Greeks show knowledge of a round earth. Greek grammarian and stoic philosopher Cratus of Mallus (ancient city in current day Türkiye) is known for constructing the earliest known globe of the earth around 150 B.C. The 1492 Erdapfel by 15th century German polymath Martin Behaim is the oldest surviving terrestrial globe, which was manually constructed from painted on globe gores and later inscribed with over 2,000 place names by a team of artists and scribes. Another German cartographer by the name of Martin Waldseemüller created what are thought to be the first set of printed globe gores in 1507. Coincidentally, this globe gore map along with his wall map also created in 1507 were also where the name America first appears on a map. Below is an 1879 facsimile of the Waldseemüller globe gores.\nSection from: Waldseemüller, Martin, Erster gedruckter Globus, Martin Hylacomylus (Waltzemüller), 1879, Geography and Map Division.\nThe cleric and printer Johann Schöner was another German who played an important role in the history of globe gores. He was the first to bring printed terrestrial and celestial globes together as a matched pair starting in 1515. Below is an example of celestial globe gores by Schöner from 1517.\nSchöner, Johann, [Celestial globe gores], 1517, Geography and Map Division.The 16th century Flemish cartographer Gerard Mercator’s terrestrial and celestial globe gores created between 1541-1551 improved upon Johann Schöner’s and Martin Waldseemüller woodcut engraving technique with copper engraved plates for his gores and also added equatorial coordinates and an ecliptic (path of the sun in its apparent orbit around the Earth) line that are now common features of modern terrestrial globe gores. The copper engraving technique allowed for greater detail than woodcut and the metal plates could be updated with new geographic information. A new edition of globe gores made from a woodcut would need to be entirely re-sculpted from scratch. The technical additions of features like the ecliptic line made the finished terrestrial globe more useful, especially with its celestial pair which also included this line.\nAnother Flemish cartographer and engraver by the name of Jodocus Hondius was a contemporary of Mercator. He purchased Mercator’s Atlas map plates in 1604 and republished and sold a prolific number of updated copies.\nHondius, Jodocus, [Terrestrial globe gores], 1615, Geography and Map Division.Hondius’ terrestrial 1615 globe gores shown above depict the Mercator-influenced features such as the ecliptic line and equatorial coordinates. The prime meridian on this map appears to be Ferro (Canary Islands), which was common for this time period.\nMy favorite globe gore maker is 17th century Venetian friar, cartographer and cosmographer Vincenzo Maria Coronelli. He was famous for making at the time the largest pair of terrestrial and celestial globes in 1684 for the King of France, Louis XIV which were each 12 feet 7 inches in diameter. After spending two years in Paris building these globes, he returned to Venice and founded the Accademia Cosmografica degli Argonauti where he decided to reproduce the globes at a reduced three-and-a-half-foot diameter scale as printed globe gores. The Geography and Map Division has a copy of the terrestrial three-and-a-half-foot globe gores (24 gores plus 2 polar calottes) cut and mounted flat daisy style in two frames with northern and southern hemisphere sections.\nPhoto by author. Coronelli, Vincenzo, [Three-and-a-half-foot Globe Gores, Northern Hemisphere], 1688, Geography and Map Division.Photo by author. Coronelli, Vincenzo, [Three-and-a-half-foot Globe Gores, Southern Hemisphere], 1688, Geography and Map Division.The southern hemisphere set of gores above contains a cartouche with a dedication to Cardinal César d’Estrées (who sent him to Paris to make the large globes for Louis XIV) and a self-portrait of Coronelli being unveiled by a trio of cherubs. Below is a section of a few of the northern hemisphere gores that shows North America at the time with beautifully done cartography and miniatures depicting watercraft and creatures.\nPhoto by author. Section from: Coronelli, Vincenzo, [Three-and-a-half-foot Globe Gores, Northern Hemisphere], 1688, Geography and Map Division.A graphic diagram called the analemma representing the position and declination (angle) of the sun in the sky seen from a fixed location on Earth at the same mean solar time throughout the year was created by Jean-Paul Grandjean de Fouchy in 1840 and subsequently started appearing on terrestrial globe gores thereafter. Below is an example of an analemma in its typical figure-8 shape on a George F. Cram globe.\nPhoto by author. George F. Cram Company, 16 inch/40 cm physical-political globe, 1994, Geography and Map Division.\nBelow is another example of an analemma on Rand McNally set of globe gores from 1887 in an oval shape not only showing sun declination, but the position of the zodiac for every day of the year.\nRand McNally and Company, Rand McNally & Co.’s new twelve inch terrestrial globe [gores], 1887, Geography and Map Division.Modern globe gores typically have the features accumulated through globe gore making history. As printing became more prevalent, so did the variety of arrangements of the gores on the printed page. This 1942 military globe gore example by the U.S. Office of Strategic Services below shows an arrangement pattern of interleaved gores presumably as an efficiency with the use of the paper to reduce printing costs.\nUnited States. Office of Strategic Services, [50ʺ military globe gores], c. 1942, Geography and Map Division.Printed globe gores provide a fascinating map format, a variety of which can be seen in the Geography and Map Division either in its original printed format on paper sheets, cut out 2D representations, or as part of finished globes. I will leave you with this 1705 example of a pair of constructed 24” terrestrial and celestial globes by George Christoph Eimmart which used globe gores.\nPhoto by author. Eimmart, Georg Christoph, Cum geographica orbis terrarum & Loca stellarum coelesti…, 1705, Geography and Map Division.\n\nFurther Reading\nA Renaissance Globemaker’s Toolbox | Library of Congress\n\n                        Categories\n\n\t16th century cartography\n\t17th century cartography\n\t19th century cartography\n\t20th century cartography",
    "summary": {
      "en": "The Geography and Map Division has a collection of globe gores, which are printed paper strips used to create globes. These gores come in various sizes and date from the 16th to the 20th centuries. Globe gores can be terrestrial (showing the Earth) or celestial (showing the sky) and are shaped like footballs. They allow globe-makers to quickly assemble globes without needing to create maps from scratch.\n\nThe history of globe-making goes back to ancient Greece, with records showing that the Greeks understood the Earth was round. The earliest known globe was made around 150 B.C. by Cratus of Mallus. The oldest surviving terrestrial globe, made in 1492 by Martin Behaim, features over 2,000 place names. Martin Waldseemüller created the first printed globe gores in 1507, which also included the first appearance of the name \"America\" on a map.\n\nJohann Schöner and Gerard Mercator improved globe-making techniques in the 16th century, with Mercator introducing copper engraving for greater detail. Jodocus Hondius, a contemporary of Mercator, published many updated globe gores.\n\nVincenzo Maria Coronelli, a 17th-century cartographer, made large globes for King Louis XIV and later produced smaller printed globe gores. His work included detailed and artistic representations of the globes.\n\nModern globe gores incorporate features from their historical predecessors, including the analemma, which shows the sun's position throughout the year. The collection at the Geography and Map Division showcases various formats of globe gores, from original sheets to assembled globes.",
      "ko": "지리 및 지도 부서는 지구본을 만들기 위해 사용되는 종이 조각인 지구본 고르를 소장하고 있습니다. 이 고르는 다양한 크기로 16세기부터 20세기까지의 것들이 있으며, 지구를 나타내는 지구본 고르와 하늘을 나타내는 천체 고르로 나뉘며, 축구공 모양을 하고 있습니다. 이를 통해 지구본 제작자들은 지도를 새로 만들 필요 없이 빠르게 지구본을 조립할 수 있습니다.\n\n지구본 제작의 역사는 고대 그리스로 거슬러 올라가며, 그리스인들은 지구가 둥글다는 것을 이해하고 있었다는 기록이 있습니다. 알려진 가장 오래된 지구본은 기원전 150년경 크라투스가 만든 것입니다. 1492년에 마르틴 베하임이 만든 가장 오래된 지구본은 2,000개 이상의 지명으로 구성되어 있습니다. 마르틴 발트세뮐러는 1507년에 첫 번째 인쇄된 지구본 고르를 제작했으며, 이 지도에는 \"아메리카\"라는 이름이 처음 등장합니다.\n\n요한 쇤너와 제라르 메르카토르는 16세기에 지구본 제작 기술을 개선했으며, 메르카토르는 더 세밀한 표현을 위해 구리 판화 기법을 도입했습니다. 메르카토르의 동시대인 요도쿠스 혼디우스는 많은 업데이트된 지구본 고르를 출판했습니다.\n\n17세기 지도 제작자인 빈첸조 마리아 코로넬리는 루이 14세를 위해 대형 지구본을 제작하고, 이후에는 소형 인쇄 지구본 고르를 만들었습니다. 그의 작업은 지구본에 대한 세밀하고 예술적인 표현을 포함하고 있습니다.\n\n현대의 지구본 고르는 역사적인 선조들의 특징을 포함하고 있으며, 연중 태양의 위치를 보여주는 아날레마와 같은 요소도 포함되어 있습니다. 지리 및 지도 부서의 소장품은 원본 시트부터 조립된 지구본까지 다양한 형식의 지구본 고르를 전시하고 있습니다.",
      "ja": "地理と地図の部門には、地球儀を作るための印刷された紙のストリップである地球儀用のゴアが収蔵されています。これらのゴアはさまざまなサイズがあり、16世紀から20世紀までのものがあります。地球儀用のゴアは、地球を示す地球儀用と空を示す天球儀用があり、アメリカンフットボールのような形をしています。これにより、地球儀製作者は地図を一から作成することなく、迅速に地球儀を組み立てることができます。\n\n地球儀製作の歴史は古代ギリシャに遡り、ギリシャ人は地球が丸いことを理解していた記録があります。最も古いとされる地球儀は、紀元前150年頃にマルスのクレイタスによって作られました。現存する最古の地球儀は、1492年にマルティン・ベハイムによって作られ、2000以上の地名が記されています。マルティン・ヴァルトゼーミュラーは1507年に初めて印刷された地球儀用ゴアを作成し、地図上で「アメリカ」という名前が初めて登場しました。\n\n16世紀にはヨハン・ショーナーとジェラール・メルカトルが地球儀製作技術を向上させ、メルカトルはより詳細な表現のために銅版画を導入しました。メルカトルの同時代人であるヨドクス・ホンディウスは、多くの更新された地球儀用ゴアを出版しました。\n\n17世紀の地図製作者ヴィンチェンツォ・マリア・コロネッリは、ルイ14世のために大きな地球儀を作り、その後、小型の印刷された地球儀用ゴアも製作しました。彼の作品には、地球儀の詳細で芸術的な表現が含まれています。\n\n現代の地球儀用ゴアは、歴史的な先駆者たちの特徴を取り入れており、年間を通じて太陽の位置を示すアナレマも含まれています。地理と地図の部門のコレクションには、オリジナルのシートから組み立てられた地球儀まで、さまざまな形式の地球儀用ゴアが展示されています。"
    }
  },
  {
    "id": "dd18804c249cc84c",
    "title": {
      "en": "Extend (YC W23) is hiring engineers to build LLM document processing",
      "ko": null,
      "ja": null
    },
    "type": "job",
    "url": "https://jobs.ashbyhq.com/extend/9d4d8974-bd9b-432d-84ec-8268e5a8ed37",
    "score": 1,
    "by": "kbyatnal",
    "time": 1743508900,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "ac5ed5d4df827b90",
    "title": {
      "en": "CERN scientists find evidence of quantum entanglement in sheep",
      "ko": "양의 양자 얽힘 발견!",
      "ja": "羊の量子もつれ発見！"
    },
    "type": "story",
    "url": "https://home.cern/news/news/physics/cern-scientists-find-evidence-quantum-entanglement-sheep",
    "score": 113,
    "by": "mackopes",
    "time": 1743505714,
    "content": "Toggle navigation\n\n                                                                               About\n\n\t\t\t\t\t\tCERN\n\nAt CERN, we probe the fundamental structure of particles that make up everything around us. We do so using the world's largest and most complex scientific instruments.\n\nKnow more\n\n              Who we are\n\n                Our Mission\n\n                Our Governance\n\n                Our Member States\n\n                Our History\n\n                Our People\n\n              What we do\n\n                Fundamental research\n\n                Contribute to society\n\n                Environmentally responsible research\n\n                Bring nations together\n\n                Inspire and educate\n\n                Fast facts and FAQs\n\n                Key Achievements\n\n\t\t\t\t\t\t  Key achievements submenu\n\n\tThe Higgs Boson\n\n\tThe W boson\n\n\tThe Z boson\n\n\tThe Large Hadron Collider\n\n\tThe Birth of the web\n\n\tAntimatter\n\n                                                                               News\n\n      Featured news, updates, stories, opinions, announcements\n\n                CERN releases report on the feasibility of a ...\n\n            Accelerators\n\n            News\n\n            31 March, 2025\n\n                Symmetry between up and down quarks is more b...\n\n            Physics\n\n            News\n\n            28 March, 2025\n\n                A new piece in the matter–antimatter puzzle\n\n            Physics\n\n            Press release\n\n            25 March, 2025\n\n                CERN announces winner of third Collide Copenh...\n\n            At CERN\n\n            News\n\n            18 March, 2025\n\n      Latest news\n\n              News\n\n                Accelerators\n\n                At CERN\n\n                Computing\n\n                Engineering\n\n                Experiments\n\n                Knowledge sharing\n\n                Physics\n\n                Events\n\n              CERN Community\n\n                News and announcements\n\n                Official communications\n\n                Events\n\n              Scientists\n\n                News\n\n                Press Room\n\n\t\t\t\t\t\t  Press Room submenu\n\n\tMedia News\n\n\tResources\n\n\tContact\n\n                                                                               Science\n\n\t\t\t\t\t\tScience\n\nThe research programme at CERN covers topics from kaons to cosmic rays, and from the Standard Model to supersymmetry\n\nKnow more\n\n              Physics\n\n                Antimatter\n\n                Dark matter\n\n                The early universe\n\n                The Higgs boson\n\n                The Standard Model\n\n                + More\n\n              Accelerators\n\n                CERN's accelerators\n\n                The Antiproton Decelerator\n\n                The Large Hadron Collider\n\n                High-Luminosity LHC\n\n                + More\n\n              Engineering\n\n                Accelerating: radiofrequency cavities\n\n                Steering and focusing: magnets and superconductivity\n\n                Circulating: ultra-high vacuum\n\n                Cooling: cryogenic systems\n\n                Powering: energy at CERN\n\n                + More\n\n              Computing\n\n                The CERN Data Centre\n\n                The Worldwide LHC Computing Grid\n\n                CERN openlab\n\n                Open source for open science\n\n                The birth of the web\n\n                + More\n\n              Experiments\n\n                ALICE\n\n                ATLAS\n\n                CMS\n\n                LHCb\n\n                + More\n\n                                                                               Resources\n\n          Featured resources\n\n                CERN Courier Jan/Feb 2025\n\n            Courier\n\n            Physics\n\n            1 January, 2025\n\n                High-Luminosity LHC images\n\n            Image\n\n            Accelerators\n\n            20 June, 2018\n\n                LHC Facts and Figures\n\n            Brochure\n\n            Knowledge sharing\n\n            10 May, 2022\n\n      See all resources\n\n              By Topic\n\n                Accelerators\n\n                At CERN\n\n                Computing\n\n                Engineering\n\n                Experiments\n\n                Knowledge sharing\n\n                Physics\n\n              By format\n\n                360 image\n\n                Annual report\n\n                Brochure\n\n                Bulletin\n\n                Courier\n\n                Image\n\n                Video\n\n                + More\n\n              By audience\n\n                CERN community\n\n                Educators\n\n                General public\n\n                Industry\n\n                Media\n\n                Scientists\n\n                Students\n\n                + More\n\n              search\n\n                  E.G. BIRTH OF WEB, LHC PAGE 1, BULLETIN...\n                  E.G. BIRTH OF WEB, LHC...\n\n      Search\n\n  Search\n\n                |\n                en\n\n      enfr\n\nenfr\n\nAlso On Physics\n\n                Symmetry between up and down quarks is more b...\n\n            Physics\n\n            News\n\n            28 March, 2025\n\n                A new piece in the matter–antimatter puzzle\n\n            Physics\n\n            Press release\n\n            25 March, 2025\n\n                A bestiary of exotic hadrons\n\n            Physics\n\n            News\n\n            20 February, 2025\n\n                Observing triplets of weak bosons\n\n            Physics\n\n            News\n\n            19 February, 2025\n\n                The other 99%\n\n            Physics\n\n            News\n\n            5 February, 2025\n\n                Clocking nature’s heaviest elementary particl...\n\n            Physics\n\n            News\n\n            23 January, 2025\n\n                The measurement of a lifetime\n\n            Physics\n\n            News\n\n            19 December, 2024\n\n                LHCb sheds light on two pieces of the matter–...\n\n            Physics\n\n            News\n\n            16 December, 2024\n\n                A tale of two Higgs: CMS searches for the pro...\n\n            Physics\n\n            News\n\n            11 December, 2024\n\n      View all news\n\nFollow Us\n\nMore Social Media Accounts\n\nCERN\n\tEsplanade des Particules 1\n\tP.O. Box\n\t1211Geneva 23\n\tSwitzerland",
    "summary": {
      "en": "CERN is a research organization that studies the fundamental particles that make up the universe using advanced scientific instruments. Its mission includes conducting fundamental research, contributing to society, promoting environmentally responsible practices, and fostering international collaboration.\n\nKey achievements at CERN include the discovery of particles like the Higgs boson, W boson, and Z boson, as well as the development of the Large Hadron Collider and contributions to the creation of the World Wide Web.\n\nCERN's research covers a wide range of topics in physics, from antimatter to dark matter and the early universe. The organization also operates several accelerators, including the Large Hadron Collider, and has a strong computing infrastructure, including the Worldwide LHC Computing Grid.\n\nCERN provides various resources for the public, educators, and scientists, promoting knowledge sharing and education in the field of particle physics.",
      "ko": "CERN은 우주를 구성하는 기본 입자를 연구하는 연구 기관으로, 첨단 과학 기기를 사용합니다. 이 기관의 사명은 기초 연구를 수행하고, 사회에 기여하며, 환경을 고려한 실천을 촉진하고, 국제 협력을 증진하는 것입니다.\n\nCERN의 주요 성과로는 힉스 보존, W 보존, Z 보존과 같은 입자의 발견이 있으며, 대형 하드론 충돌기 개발과 월드 와이드 웹의 창조에 기여한 것도 포함됩니다.\n\nCERN의 연구는 반물질, 암흑물질, 초기 우주 등 물리학의 다양한 주제를 다룹니다. 이 기관은 대형 하드론 충돌기를 포함한 여러 가속기를 운영하며, 전 세계 LHC 컴퓨팅 그리드를 포함한 강력한 컴퓨팅 인프라를 갖추고 있습니다.\n\nCERN은 대중, 교육자, 과학자들을 위한 다양한 자원을 제공하여 입자 물리학 분야의 지식 공유와 교육을 촉진합니다.",
      "ja": "CERNは、宇宙を構成する基本的な粒子を先進的な科学機器を使って研究する組織です。その使命には、基礎研究の実施、社会への貢献、環境に配慮した実践の推進、国際的な協力の促進が含まれています。\n\nCERNの主な成果には、ヒッグス粒子、Wボソン、Zボソンなどの粒子の発見や、大型ハドロン衝突型加速器の開発、さらにはワールドワイドウェブの創造への貢献があります。\n\nCERNの研究は、反物質や暗黒物質、初期宇宙など、物理学の幅広いトピックをカバーしています。また、大型ハドロン衝突型加速器を含むいくつかの加速器を運営しており、世界規模のLHCコンピューティンググリッドを含む強力なコンピュータインフラも整備しています。\n\nCERNは一般の人々、教育者、科学者向けにさまざまなリソースを提供し、粒子物理学の分野における知識の共有と教育を促進しています。"
    }
  },
  {
    "id": "0bd2a4264c6cb3b8",
    "title": {
      "en": "First Orbital Rocket Launched from European Soil, Rocket Crashes, but It's Cool [video]",
      "ko": "유럽 첫 로켓 발사, 사고에도 괜찮아!",
      "ja": "欧州初のロケット打上げ、墜落も問題なし【動画】"
    },
    "type": "story",
    "url": "https://www.youtube.com/watch?v=eFyMAaeYdvs",
    "score": 27,
    "by": "consumer451",
    "time": 1743367158,
    "content": "Back\n\n        Search",
    "summary": {
      "en": "It seems you haven't provided the text you want summarized. Could you please share it?",
      "ko": "제공하신 번역할 텍스트가 없는 것 같습니다. 번역할 내용을 공유해 주실 수 있나요?",
      "ja": "テキストが提供されていないようです。要約してほしい内容を教えていただけますか？"
    }
  },
  {
    "id": "22e48b2f649d6595",
    "title": {
      "en": "Drawing some ovals (that are not ellipses)",
      "ko": "타원 아닌 타원 그리기",
      "ja": "楕円じゃない卵型"
    },
    "type": "story",
    "url": "https://medium.com/@brunopostle/actually-drawing-some-ovals-that-are-not-ellipses-444ba9fd9cf8",
    "score": 8,
    "by": "todsacerdoti",
    "time": 1743367457,
    "content": "Actually drawing some ovals (that are not ellipses)Bruno Postle·Follow4 min read·Feb 10, 2017--ListenShareIn the last part I hopefully made it clear why you wouldn’t want to use an actual ellipse when making a real object, curves constructed from multiple fixed radius arcs are much more useful and look just the same.So there is a traditional draughting technique for drawing a five centred arch that makes quite a good imitation of an ellipse, here is the diagram again:This is fine, but it is a solution to an old problem of drawing an ellipse on a drawing board with calipers and compass. These days we don’t have this problem, every CAD tool imaginable will let you easily draw ellipses — the problem we have is how to convert that ellipse into fixed radius curves that suit our practical building purposes — for this a bit more control would be useful.Following is my technique, this starts with an ellipse drawn in CAD drawn to the required height and width:The next step is to decide how we want to split the curve, this is done by dividing up a circle to one side, in this case I am splitting a quadrant into a 45 degree segment, a 30 degree segment and a 15 degree segment, the final ellipse will be made up from segments with the same angles. While you are there, connect the ends of the radial lines with straight chords and you should have something like this:The next step is to copy these chords to the ellipse and fit them as best you can without changing the angles, the lengths will be different of course:Note that I say “as best you can”, not all of these intersections can be exactly on the curve of the ellipse.Then similarly, copy the radial lines, without changing any angles, these give you the centres for the circular arcs:That’s it, if you set out a stadium or an arena using this a geometry like this then all the angles between stands will be multiples of 15 degrees and everybody on site will be very very happy (maybe).Of course you don’t need to use these particular angles, here is the circle quadrant divided into nine equal 10 degree segments:This is what they look like mapped onto the ellipse:This is a seventeen centre arch, if you build this probably no one can ever tell that it isn’t actually an ellipse:Just to complete the set, the simplest ellipse like curve is the three centre arch, here I’m dividing the quadrant into a 60 degree segment and a 30 degree segment:Here it is with the chords and radial lines mapped onto the same ellipse:Note that the two chords don’t actually meet on the line of the ellipse, you can’t have everything.The result is a basic three centre arch. If you care to look, this is obviously not an ellipse, but I still think it looks fine — in some ways it may be more useful than the five centre arch as you get a bit more headroom to the sides:Three centre arches really are fine, this one is split with the segments at about 67/23 degrees, if it’s good enough for Christopher Wren then it will be good enough for you:Photo taken by Lonpicman Creative Commons Attribution-Share Alike 3.0 Unported licenseThat’s it, there may be another piece in the series showing how to decompose any curve into arcs, the principle is basically the same for pointed arches, ogees, parabolas etc…",
    "summary": {
      "en": "The text discusses techniques for drawing curves that resemble ellipses using fixed radius arcs instead of actual ellipses. Bruno Postle explains that while traditional methods for drawing ellipses involved calipers and compasses, modern CAD tools make it easier to create ellipses. The challenge now is converting these ellipses into practical curves for construction.\n\nHere’s a simplified summary of the key points:\n\n1. **Using Arcs Instead of Ellipses**: Instead of drawing true ellipses, which can be complex, it's more practical to use fixed radius arcs that look similar.\n\n2. **Drawing Technique**: The process involves starting with an ellipse in CAD, dividing a circle into segments, and using these segments to guide the drawing of arcs.\n\n3. **Angle Segments**: The author suggests dividing the circle into specific angle segments (like 15, 30, or 45 degrees) to create a variety of arch shapes. \n\n4. **Final Result**: The resulting arcs can create arches that appear like ellipses, making them suitable for structures like stadiums and arenas.\n\n5. **Versatility**: This method can be adapted for different designs, including pointed arches and other curves. \n\nOverall, the article emphasizes practical techniques for creating visually appealing curves that are easier to work with in construction.",
      "ko": "이 글에서는 실제 타원이 아닌 고정 반지름 호를 사용하여 타원과 유사한 곡선을 그리는 기술에 대해 설명합니다. 브루노 포슬은 전통적으로 타원을 그릴 때 캘리퍼스와 컴퍼스를 사용했지만, 현대의 CAD 도구 덕분에 타원을 만드는 것이 더 쉬워졌다고 말합니다. 현재의 도전 과제는 이러한 타원을 실제 건설에 적합한 곡선으로 변환하는 것입니다.\n\n첫째, 진짜 타원을 그리는 대신 복잡한 형태를 피하고 고정 반지름 호를 사용하는 것이 더 실용적입니다. 둘째, 이 과정은 CAD에서 타원으로 시작하여 원을 여러 부분으로 나누고, 이 부분을 이용해 호를 그리는 방식입니다. 셋째, 저자는 원을 특정 각도로 나누는 것을 제안합니다. 예를 들어 15도, 30도, 45도와 같은 각도로 나누면 다양한 아치 형태를 만들 수 있습니다.\n\n결과적으로 만들어진 호는 타원처럼 보이는 아치를 형성할 수 있어, 경기장이나 아레나와 같은 구조물에 적합합니다. 마지막으로, 이 방법은 뾰족한 아치나 다른 곡선 등 다양한 디자인에 맞게 조정할 수 있는 유연성을 가지고 있습니다. 전반적으로 이 글은 건설에서 작업하기 쉬운 시각적으로 매력적인 곡선을 만드는 실용적인 기술을 강조합니다.",
      "ja": "曲線を描く技術について、実際の楕円ではなく、固定半径の弧を使って楕円に似た形を作る方法が紹介されています。ブリューノ・ポストルは、従来の楕円を描く方法がキャリパーやコンパスを使っていたのに対し、現代のCADツールを使うことで楕円を簡単に作成できるようになったと説明しています。しかし、今の課題は、これらの楕円を実際の建設に適した曲線に変換することです。\n\nまず、真の楕円を描く代わりに、似た形の固定半径の弧を使用することが提案されています。これは、複雑な楕円を描くよりも実用的です。描画のプロセスは、CADで楕円を作成し、円をセグメントに分割し、そのセグメントを使って弧を描くという流れです。\n\n著者は、円を特定の角度のセグメント（例えば15度、30度、45度など）に分けることを提案し、さまざまなアーチの形を作ることができます。最終的に得られる弧は、楕円のように見えるアーチを作り出し、スタジアムやアリーナなどの構造物に適しています。\n\nこの方法は、尖ったアーチや他の曲線など、さまざまなデザインに適応できる柔軟性があります。全体として、この記事は、建設において扱いやすく、視覚的に魅力的な曲線を作成するための実用的な技術に焦点を当てています。"
    }
  },
  {
    "id": "1d3a19f7e6fb09ac",
    "title": {
      "en": "Experimental Tauri Verso Integration",
      "ko": "타우리 버소 실험 통합",
      "ja": "実験的タウリ統合"
    },
    "type": "story",
    "url": "https://v2.tauri.app/blog/tauri-verso-integration/",
    "score": 88,
    "by": "stareatgoats",
    "time": 1743280249,
    "content": "(() => {\n\t\t\ttry {\n\t\t\t\tif (!matchMedia('(min-width: 50em)').matches) return;\n\t\t\t\t/** @type {HTMLElement | null} */\n\t\t\t\tconst target = document.querySelector('sl-sidebar-state-persist');\n\t\t\t\tconst state = JSON.parse(sessionStorage.getItem('sl-sidebar-state') || '0');\n\t\t\t\tif (!target || !state || target.dataset.hash !== state.hash) return;\n\t\t\t\twindow._starlightScrollRestore = state.scroll;\n\t\t\t\tcustomElements.define(\n\t\t\t\t\t'sl-sidebar-restore',\n\t\t\t\t\tclass SidebarRestore extends HTMLElement {\n\t\t\t\t\t\tconnectedCallback() {\n\t\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t\tconst idx = parseInt(this.dataset.index || '');\n\t\t\t\t\t\t\t\tconst details = this.closest('details');\n\t\t\t\t\t\t\t\tif (details && typeof state.open[idx] === 'boolean') details.open = state.open[idx];\n\t\t\t\t\t\t\t} catch {}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t);\n\t\t\t} catch {}\n\t\t})();\n\t        Quick Start         What is Tauri?     Prerequisites     Create a Project        Frontend Configuration         Overview     Leptos     Next.js     Nuxt     Qwik     SvelteKit     Trunk     Vite            Upgrade & Migrate         Overview     Upgrade from Tauri 1.0     Upgrade from Tauri 2.0 Beta                Core Concepts         Overview     Tauri Architecture     Process Model     App Size        Inter-Process Communication         Overview     Brownfield Pattern     Isolation Pattern                Security         Overview     Permissions     Command Scopes     Capabilities     Content Security Policy (CSP)     HTTP Headers New     Tauri Ecosystem Security     Application Lifecycle Threats     Future Work     Runtime Authority            Develop         Overview     Calling the Frontend from Rust     Calling Rust from the Frontend     Configuration Files     Embedding Additional Files     Embedding External Binaries     State Management     Updating Dependencies        Debug         Overview     CrabNebula DevTools New     Debug in Neovim     Debug in JetBrains IDEs     Debug in VS Code            Plugins         Overview     Mobile Plugin Development            Tests         Overview     Mock Tauri APIs        WebDriver         Overview     Continuous Integration        Example         Setup     Selenium     WebdriverIO                        Distribute         Overview     App Store     AppImage     AUR     CrabNebula Cloud     Debian     DMG     Flathub     Google Play     macOS Application Bundle     Microsoft Store     RPM     Snapcraft     Windows Installer        Sign         macOS     Windows     Linux     iOS     Android            Pipelines         CrabNebula Cloud     GitHub                Learn         Overview     Node.js as a sidecar     Splashscreen     System Tray     Window Customization        Security         Using Plugin Permissions     Capabilities for Different Windows and Platforms     Writing Plugin Permissions         Window Menu            Plugins         Overview     Autostart     Barcode Scanner New     Biometric New     Command Line Interface (CLI)     Clipboard     Deep Linking New     Dialog     File System     Global Shortcut     HTTP Client     Localhost     Logging     NFC New     Notifications     Opener     OS Information     Persisted Scope     Positioner     Process     Shell     Single Instance     SQL     Store     Stronghold     Updater     Upload     Websocket     Window State            About         About Tauri     The Tauri Book     Tauri Governance     Tauri Philosophy     Trademark Guidelines\n\t\t(() => {\n\t\t\tconst scroller = document.getElementById('starlight__sidebar');\n\t\t\tif (!window._starlightScrollRestore || !scroller) return;\n\t\t\tscroller.scrollTop = window._starlightScrollRestore;\n\t\t\tdelete window._starlightScrollRestore;\n\t\t})();\n\t      GitHub Discord Twitter Mastodon RSS            Select language    EnglishFrançaisEspañol简体中文日本語\n\n(() => {\n\t\t\ttry {\n\t\t\t\tif (!matchMedia('(min-width: 50em)').matches) return;\n\t\t\t\t/** @type {HTMLElement | null} */\n\t\t\t\tconst target = document.querySelector('sl-sidebar-state-persist');\n\t\t\t\tconst state = JSON.parse(sessionStorage.getItem('sl-sidebar-state') || '0');\n\t\t\t\tif (!target || !state || target.dataset.hash !== state.hash) return;\n\t\t\t\twindow._starlightScrollRestore = state.scroll;\n\t\t\t\tcustomElements.define(\n\t\t\t\t\t'sl-sidebar-restore',\n\t\t\t\t\tclass SidebarRestore extends HTMLElement {\n\t\t\t\t\t\tconnectedCallback() {\n\t\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t\tconst idx = parseInt(this.dataset.index || '');\n\t\t\t\t\t\t\t\tconst details = this.closest('details');\n\t\t\t\t\t\t\t\tif (details && typeof state.open[idx] === 'boolean') details.open = state.open[idx];\n\t\t\t\t\t\t\t} catch {}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t);\n\t\t\t} catch {}\n\t\t})();\n\t        Security         Capability     Permission     Scope     Core Permissions         Command Line Interface     Configuration     Environment Variables     Webview Versions        Releases         Overview        @tauri-apps            api         2.4.0     2.3.0     2.2.0     2.1.1     2.1.0     2.0.3     2.0.2     2.0.1     2.0.0     2.0.0-rc.6     2.0.0-rc.5     2.0.0-rc.4     2.0.0-rc.3     2.0.0-rc.2     2.0.0-rc.1     2.0.0-rc.0     2.0.0-beta.16     2.0.0-beta.15     2.0.0-beta.14     2.0.0-beta.13     2.0.0-beta.12     2.0.0-beta.11     2.0.0-beta.10     2.0.0-beta.9     2.0.0-beta.8     2.0.0-beta.7     2.0.0-beta.6     2.0.0-beta.5     2.0.0-beta.4     2.0.0-beta.3     2.0.0-beta.2     2.0.0-beta.1     2.0.0-beta.0     2.0.0-alpha.14     2.0.0-alpha.13     2.0.0-alpha.12     2.0.0-alpha.11     2.0.0-alpha.10     2.0.0-alpha.9     2.0.0-alpha.8     2.0.0-alpha.7     2.0.0-alpha.6     2.0.0-alpha.5     2.0.0-alpha.4     2.0.0-alpha.3     2.0.0-alpha.2     2.0.0-alpha.1     2.0.0-alpha.0     1.5.3     1.5.2     1.5.1     1.5.0     1.4.0     1.3.0     1.2.0     1.1.0     1.0.2     1.0.1     1.0.0     1.0.0-rc.7     1.0.0-rc.6     1.0.0-rc.5     1.0.0-rc.4     1.0.0-rc.3     1.0.0-rc.2     1.0.0-rc.1     1.0.0-rc.0     1.0.0-beta-rc.3     1.0.0-beta-rc.2     1.0.0-beta-rc.1     1.0.0-beta-rc.0     1.0.0-beta.8     1.0.0-beta.7     1.0.0-beta.6     1.0.0-beta.5     1.0.0-beta.4     1.0.0-beta.3     1.0.0-beta.2     1.0.0-beta.1     1.0.0-beta.0            cli         2.4.0     2.3.1     2.3.0     2.2.7     2.2.6     2.2.5     2.2.4     2.2.3     2.2.2     2.2.1     2.2.0     2.1.0     2.0.4     2.0.3     2.0.2     2.0.1     2.0.0     2.0.0-rc.18     2.0.0-rc.17     2.0.0-rc.16     2.0.0-rc.15     2.0.0-rc.14     2.0.0-rc.13     2.0.0-rc.12     2.0.0-rc.11     2.0.0-rc.10     2.0.0-rc.9     2.0.0-rc.8     2.0.0-rc.7     2.0.0-rc.6     2.0.0-rc.5     2.0.0-rc.4     2.0.0-rc.3     2.0.0-rc.2     2.0.0-rc.1     2.0.0-rc.0     2.0.0-beta.23     2.0.0-beta.22     2.0.0-beta.21     2.0.0-beta.20     2.0.0-beta.19     2.0.0-beta.18     2.0.0-beta.17     2.0.0-beta.16     2.0.0-beta.15     2.0.0-beta.14     2.0.0-beta.13     2.0.0-beta.12     2.0.0-beta.11     2.0.0-beta.10     2.0.0-beta.9     2.0.0-beta.8     2.0.0-beta.7     2.0.0-beta.6     2.0.0-beta.5     2.0.0-beta.4     2.0.0-beta.3     2.0.0-beta.2     2.0.0-beta.1     2.0.0-beta.0     2.0.0-alpha.21     2.0.0-alpha.20     2.0.0-alpha.19     2.0.0-alpha.18     2.0.0-alpha.17     2.0.0-alpha.16     2.0.0-alpha.15     2.0.0-alpha.14     2.0.0-alpha.13     2.0.0-alpha.12     2.0.0-alpha.11     2.0.0-alpha.10     2.0.0-alpha.9     2.0.0-alpha.8     2.0.0-alpha.7     2.0.0-alpha.6     2.0.0-alpha.5     2.0.0-alpha.4     2.0.0-alpha.3     2.0.0-alpha.2     2.0.0-alpha.1     2.0.0-alpha.0     1.5.11     1.5.10     1.5.9     1.5.8     1.5.7     1.5.6     1.5.5     1.5.4     1.5.3     1.5.2     1.5.1     1.5.0     1.4.0     1.3.1     1.3.0     1.2.3     1.2.2     1.2.1     1.2.0     1.1.1     1.1.0     1.0.5     1.0.4     1.0.3     1.0.2     1.0.1     1.0.0     1.0.0-rc.16     1.0.0-rc.15     1.0.0-rc.14     1.0.0-rc.13     1.0.0-rc.12     1.0.0-rc.11     1.0.0-rc.10     1.0.0-rc.9     1.0.0-rc.8     1.0.0-rc.6     1.0.0-rc.5     1.0.0-rc.4     1.0.0-rc.3     1.0.0-rc.2     1.0.0-rc.1     1.0.0-rc.0                tao         0.32.8     0.32.7     0.32.6     0.32.5     0.32.4     0.32.3     0.32.2     0.32.1     0.32.0     0.31.1     0.31.0     0.30.7     0.30.6     0.30.5     0.30.4     0.30.3     0.30.2     0.30.1     0.30.0     0.29.1     0.29.0     0.28.1     0.28.0     0.27.1     0.27.0     0.26.2     0.26.1     0.26.0     0.25.0     0.24.1     0.24.0     0.23.0     0.22.3     0.22.2     0.22.1     0.22.0     0.21.1     0.21.0     0.20.0     0.19.1     0.19.0     0.18.3     0.18.2     0.18.1     0.18.0     0.17.0     0.16.0     0.15.9     0.15.8     0.15.7     0.15.6     0.15.5     0.15.4     0.15.3     0.15.2     0.15.1     0.15.0     0.14.0     0.13.3     0.13.2     0.13.1     0.13.0     0.12.2     0.12.1     0.12.0     0.11.2     0.11.1     0.11.0     0.10.0     0.9.1     0.9.0     0.8.5     0.8.4     0.8.3     0.8.2     0.8.1     0.8.0     0.7.0     0.6.4     0.6.3     0.6.2     0.6.1     0.6.0     0.5.2     0.5.1     0.5.0     0.4.0     0.3.1     0.3.0     0.2.6     0.2.5     0.2.4     0.2.3     0.2.2     0.2.1     0.2.0            tauri         2.4.0     2.3.1     2.3.0     2.2.5     2.2.4     2.2.3     2.2.2     2.2.1     2.2.0     2.1.1     2.1.0     2.0.6     2.0.5     2.0.4     2.0.3     2.0.2     2.0.1     2.0.0     2.0.0-rc.17     2.0.0-rc.16     2.0.0-rc.15     2.0.0-rc.14     2.0.0-rc.13     2.0.0-rc.12     2.0.0-rc.11     2.0.0-rc.10     2.0.0-rc.9     2.0.0-rc.8     2.0.0-rc.7     2.0.0-rc.6     2.0.0-rc.5     2.0.0-rc.4     2.0.0-rc.3     2.0.0-rc.2     2.0.0-rc.1     2.0.0-rc.0     2.0.0-beta.25     2.0.0-beta.24     2.0.0-beta.23     2.0.0-beta.22     2.0.0-beta.21     2.0.0-beta.20     2.0.0-beta.19     2.0.0-beta.18     2.0.0-beta.17     2.0.0-beta.16     2.0.0-beta.15     2.0.0-beta.14     2.0.0-beta.13     2.0.0-beta.12     2.0.0-beta.11     2.0.0-beta.10     2.0.0-beta.9     2.0.0-beta.8     2.0.0-beta.7     2.0.0-beta.6     2.0.0-beta.5     2.0.0-beta.4     2.0.0-beta.3     2.0.0-beta.2     2.0.0-beta.1     2.0.0-beta.0     2.0.0-alpha.21     2.0.0-alpha.20     2.0.0-alpha.19     2.0.0-alpha.18     2.0.0-alpha.17     2.0.0-alpha.16     2.0.0-alpha.15     2.0.0-alpha.14     2.0.0-alpha.13     2.0.0-alpha.12     2.0.0-alpha.11     2.0.0-alpha.10     2.0.0-alpha.9     2.0.0-alpha.8     2.0.0-alpha.7     2.0.0-alpha.6     2.0.0-alpha.5     2.0.0-alpha.4     2.0.0-alpha.3     2.0.0-alpha.2     2.0.0-alpha.1     2.0.0-alpha.0     1.6.0     1.5.4     1.5.3     1.5.2     1.5.1     1.5.0     1.4.1     1.4.0     1.3.0     1.2.5     1.2.4     1.2.3     1.2.2     1.2.1     1.2.0     1.1.4     1.1.3     1.1.2     1.1.1     1.1.0     1.0.9     1.0.8     1.0.7     1.0.6     1.0.5     1.0.4     1.0.3     1.0.2     1.0.1     1.0.0     1.0.0-rc.17     1.0.0-rc.16     1.0.0-rc.15     1.0.0-rc.14     1.0.0-rc.13     1.0.0-rc.12     1.0.0-rc.11     1.0.0-rc.10     1.0.0-rc.9     1.0.0-rc.8     1.0.0-rc.7     1.0.0-rc.6     1.0.0-rc.5     1.0.0-rc.4     1.0.0-rc.3     1.0.0-rc.2     1.0.0-rc.1     1.0.0-rc.0     1.0.0-beta-rc.4     1.0.0-beta-rc.3     1.0.0-beta-rc.2     1.0.0-beta-rc.1     1.0.0-beta-rc.0     1.0.0-beta.8     1.0.0-beta.7     1.0.0-beta.6     1.0.0-beta.5     1.0.0-beta.4     1.0.0-beta.3     1.0.0-beta.2     1.0.0-beta.1     1.0.0-beta.0     0.11.1     0.11.0     0.10.0     0.9.2     0.9.1     0.9.0     0.8.0     0.7.5     0.7.4     0.7.3     0.7.2     0.7.1     0.7.0     0.6.2     0.6.0            tauri-bundler         2.3.0     2.2.4     2.2.3     2.2.2     2.2.1     2.2.0     2.1.0     2.0.4     2.0.3     2.0.2     2.0.1     2.0.1-rc.15     2.0.1-rc.14     2.0.1-rc.13     2.0.1-rc.12     2.0.1-rc.11     2.0.1-rc.10     2.0.1-rc.9     2.0.1-rc.8     2.0.1-rc.7     2.0.1-rc.6     2.0.1-rc.5     2.0.1-rc.4     2.0.1-rc.3     2.0.1-rc.2     2.0.1-rc.1     2.0.1-rc.0     2.0.1-beta.19     2.0.1-beta.18     2.0.1-beta.17     2.0.1-beta.16     2.0.1-beta.15     2.0.1-beta.14     2.0.1-beta.13     2.0.1-beta.12     2.0.1-beta.11     2.0.1-beta.10     2.0.1-beta.9     2.0.1-beta.8     2.0.1-beta.7     2.0.1-beta.6     2.0.1-beta.5     2.0.1-beta.4     2.0.1-beta.3     2.0.1-beta.2     2.0.1-beta.1     2.0.1-beta.0     2.0.0     2.0.0-rc.0     2.0.0-beta.3     2.0.0-beta.2     2.0.0-beta.1     2.0.0-beta.0     2.0.0-alpha.14     2.0.0-alpha.13     2.0.0-alpha.12     2.0.0-alpha.11     2.0.0-alpha.10     2.0.0-alpha.9     2.0.0-alpha.8     2.0.0-alpha.7     2.0.0-alpha.6     2.0.0-alpha.5     2.0.0-alpha.4     2.0.0-alpha.3     2.0.0-alpha.2     2.0.0-alpha.1     2.0.0-alpha.0     1.5.1     1.5.0     1.4.8     1.4.7     1.4.6     1.4.5     1.4.4     1.4.3     1.4.2     1.4.1     1.4.0     1.3.0     1.2.1     1.2.0     1.1.2     1.1.1     1.1.0     1.0.7     1.0.6     1.0.5     1.0.4     1.0.3     1.0.2     1.0.1     1.0.0     1.0.0-rc.10     1.0.0-rc.9     1.0.0-rc.8     1.0.0-rc.7     1.0.0-rc.6     1.0.0-rc.5     1.0.0-rc.4     1.0.0-rc.3     1.0.0-rc.2     1.0.0-rc.1     1.0.0-rc.0     1.0.0-beta-rc.1     1.0.0-beta-rc.0     1.0.0-beta.4     1.0.0-beta.3     1.0.0-beta.2     1.0.0-beta.1     1.0.0-beta.0     0.9.4     0.9.3     0.9.2     0.9.1     0.9.0     0.8.5     0.8.4     0.8.3     0.8.2     0.8.1     0.8.0     0.7.0            tauri-cli         2.4.0     2.3.1     2.3.0     2.2.7     2.2.6     2.2.5     2.2.4     2.2.3     2.2.2     2.2.1     2.2.0     2.1.0     2.0.4     2.0.3     2.0.2     2.0.1     2.0.0     2.0.0-rc.18     2.0.0-rc.17     2.0.0-rc.16     2.0.0-rc.15     2.0.0-rc.13     2.0.0-rc.12     2.0.0-rc.11     2.0.0-rc.10     2.0.0-rc.9     2.0.0-rc.8     2.0.0-rc.7     2.0.0-rc.6     2.0.0-rc.5     2.0.0-rc.4     2.0.0-rc.3     2.0.0-rc.2     2.0.0-rc.1     2.0.0-rc.0     2.0.0-beta.23     2.0.0-beta.22     2.0.0-beta.21     2.0.0-beta.20     2.0.0-beta.19     2.0.0-beta.18     2.0.0-beta.17     2.0.0-beta.16     2.0.0-beta.15     2.0.0-beta.14     2.0.0-beta.13     2.0.0-beta.12     2.0.0-beta.11     2.0.0-beta.10     2.0.0-beta.9     2.0.0-beta.8     2.0.0-beta.7     2.0.0-beta.6     2.0.0-beta.5     2.0.0-beta.4     2.0.0-beta.3     2.0.0-beta.2     2.0.0-beta.1     2.0.0-beta.0     2.0.0-alpha.21     2.0.0-alpha.20     2.0.0-alpha.19     2.0.0-alpha.18     2.0.0-alpha.17     2.0.0-alpha.16     2.0.0-alpha.15     2.0.0-alpha.14     2.0.0-alpha.13     2.0.0-alpha.12     2.0.0-alpha.11     2.0.0-alpha.10     2.0.0-alpha.9     2.0.0-alpha.8     2.0.0-alpha.7     2.0.0-alpha.6     2.0.0-alpha.5     2.0.0-alpha.4     2.0.0-alpha.3     2.0.0-alpha.2     2.0.0-alpha.1     2.0.0-alpha.0     1.5.11     1.5.10     1.5.9     1.5.8     1.5.7     1.5.6     1.5.5     1.5.4     1.5.3     1.5.2     1.5.1     1.5.0     1.4.0     1.3.1     1.3.0     1.2.3     1.2.2     1.2.1     1.2.0     1.1.1     1.1.0     1.0.5     1.0.4     1.0.3     1.0.2     1.0.1     1.0.0     1.0.0-rc.16     1.0.0-rc.15     1.0.0-rc.14     1.0.0-rc.13     1.0.0-rc.12     1.0.0-rc.11     1.0.0-rc.10     1.0.0-rc.9     1.0.0-rc.8     1.0.0-rc.7     1.0.0-rc.6     1.0.0-rc.5     1.0.0-rc.4     1.0.0-rc.3     1.0.0-rc.2     1.0.0-rc.1     1.0.0-rc.0     1.0.0-beta-rc.4     1.0.0-beta-rc.3     1.0.0-beta-rc.2     1.0.0-beta-rc.1     1.0.0-beta-rc.0     1.0.0-beta.7     1.0.0-beta.6     1.0.0-beta.5     1.0.0-beta.4     1.0.0-beta.3     1.0.0-beta.2     1.0.0-beta.1     1.0.0-beta.0            wry         0.50.5     0.50.4     0.50.3     0.50.2     0.50.1     0.50.0     0.49.0     0.48.1     0.48.0     0.47.2     0.47.1     0.47.0     0.46.3     0.46.2     0.46.1     0.46.0     0.45.0     0.44.1     0.44.0     0.43.1     0.43.0     0.42.0     0.41.0     0.40.1     0.40.0     0.39.5     0.39.4     0.39.3     0.39.2     0.39.1     0.39.0     0.38.2     0.38.1     0.38.0     0.37.0     0.36.0     0.35.2     0.35.1     0.35.0     0.34.2     0.34.1     0.34.0     0.33.1     0.33.0     0.32.0     0.31.0     0.30.0     0.29.0     0.28.3     0.28.2     0.28.1     0.28.0     0.27.3     0.27.2     0.27.1     0.27.0     0.26.0     0.25.0     0.24.1     0.24.0     0.23.4     0.23.3     0.23.2     0.23.1     0.23.0     0.22.6     0.22.5     0.22.4     0.22.3     0.22.2     0.22.1     0.22.0     0.21.1     0.21.0     0.20.2     0.20.1     0.20.0     0.19.0     0.18.3     0.18.2     0.18.1     0.18.0     0.17.0     0.16.2     0.16.1     0.16.0     0.15.1     0.15.0     0.14.0     0.13.3     0.13.2     0.13.1     0.13.0     0.12.2     0.12.1     0.12.0     0.11.0     0.10.3     0.10.2     0.10.1     0.10.0     0.9.4     0.9.3     0.9.2     0.9.1     0.9.0     0.8.0     0.7.0     0.6.2     0.6.1     0.6.0                JavaScript            api         @tauri-apps/api     app     core     dpi     event     image     menu     mocks     path     tray     webview     webviewWindow     window         barcode-scanner     biometric     cli     clipboard-manager     deep-link     dialog     fs     global-shortcut     http     log     nfc     notification     opener     os     positioner     process     shell     sql     store     stronghold     updater     upload     websocket     window-state         Rust (docs.rs)\n\t\t(() => {\n\t\t\tconst scroller = document.getElementById('starlight__sidebar');\n\t\t\tif (!window._starlightScrollRestore || !scroller) return;\n\t\t\tscroller.scrollTop = window._starlightScrollRestore;\n\t\t\tdelete window._starlightScrollRestore;\n\t\t})();\n\t      GitHub Discord Twitter Mastodon RSS            Select language    EnglishFrançaisEspañol简体中文日本語\n\n(() => {\n\t\t\ttry {\n\t\t\t\tif (!matchMedia('(min-width: 50em)').matches) return;\n\t\t\t\t/** @type {HTMLElement | null} */\n\t\t\t\tconst target = document.querySelector('sl-sidebar-state-persist');\n\t\t\t\tconst state = JSON.parse(sessionStorage.getItem('sl-sidebar-state') || '0');\n\t\t\t\tif (!target || !state || target.dataset.hash !== state.hash) return;\n\t\t\t\twindow._starlightScrollRestore = state.scroll;\n\t\t\t\tcustomElements.define(\n\t\t\t\t\t'sl-sidebar-restore',\n\t\t\t\t\tclass SidebarRestore extends HTMLElement {\n\t\t\t\t\t\tconnectedCallback() {\n\t\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t\tconst idx = parseInt(this.dataset.index || '');\n\t\t\t\t\t\t\t\tconst details = this.closest('details');\n\t\t\t\t\t\t\t\tif (details && typeof state.open[idx] === 'boolean') details.open = state.open[idx];\n\t\t\t\t\t\t\t} catch {}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t);\n\t\t\t} catch {}\n\t\t})();\n\t     All posts        Recent posts         Experimental Tauri Verso Integration     Tauri 2.0 Stable Release     Tauri 2.0 Release Candidate     Announcing Tauri 1.7.0     Tauri Board Elections 2024     Rust Security Advisory CVE-2024-24576     Announcing Tauri 1.6.0     Announcing the Tauri v2 Beta Release     Strengthening Tauri: Our Partnership with CrabNebula     Announcing Tauri 1.5.0     Roadmap to Tauri 2.0     Tauri Board Elections & Governance Update     Announcing Tauri 1.4.0     Announcing Tauri 1.3.0     Tauri 2.0.0-alpha.4 Released     create-tauri-app Version 3 Released     Tauri Community Growth & Feedback     Migration to webkit2gtk-4.1 on Linux port     Announcing the Tauri Mobile Alpha Release     Announcing Tauri 1.2.0     Announcing tauri-egui 0.1.0     Announcing Tauri 1.1.0     Tauri Programme Turns 1 and Board Elections     Tauri 1.0 Release\n\t\t(() => {\n\t\t\tconst scroller = document.getElementById('starlight__sidebar');\n\t\t\tif (!window._starlightScrollRestore || !scroller) return;\n\t\t\tscroller.scrollTop = window._starlightScrollRestore;\n\t\t\tdelete window._starlightScrollRestore;\n\t\t})();\n\t      GitHub Discord Twitter Mastodon RSS            Select language    EnglishFrançaisEspañol简体中文日本語",
    "summary": {
      "en": "The text includes a JavaScript code snippet that manages the restoration of a sidebar's scroll position and open state when the page is revisited, but only if the screen width is sufficient. It checks if specific elements and session storage data are available and matches a hash value. If valid, it defines a custom HTML element to restore the sidebar's state based on the stored data.\n\nAdditionally, it lists various sections related to Tauri, a software framework for building desktop applications. These sections cover quick starts, core concepts, security, development tools, debugging, plugins, distribution methods, and more. The document contains version information for Tauri and its components, showing ongoing updates and releases.\n\nIn summary, the text discusses restoring sidebar states in a web application and provides an overview of Tauri's framework structure and updates.",
      "ko": "이 텍스트는 페이지를 다시 방문할 때 사이드바의 스크롤 위치와 열림 상태를 복원하는 자바스크립트 코드 조각을 포함하고 있습니다. 단, 화면 너비가 충분할 경우에만 작동합니다. 특정 요소와 세션 저장소 데이터가 존재하는지 확인하고 해시 값을 비교합니다. 유효한 경우, 저장된 데이터를 기반으로 사이드바의 상태를 복원하는 사용자 정의 HTML 요소를 정의합니다.\n\n또한, Tauri라는 데스크톱 애플리케이션을 만들기 위한 소프트웨어 프레임워크와 관련된 다양한 섹션을 나열합니다. 이 섹션들은 빠른 시작, 핵심 개념, 보안, 개발 도구, 디버깅, 플러그인, 배포 방법 등을 포함합니다. 문서에는 Tauri와 그 구성 요소의 버전 정보가 포함되어 있으며, 지속적인 업데이트와 릴리스를 보여줍니다.\n\n요약하자면, 이 텍스트는 웹 애플리케이션에서 사이드바 상태를 복원하는 방법과 Tauri 프레임워크의 구조 및 업데이트에 대한 개요를 제공합니다.",
      "ja": "このテキストには、ページを再訪した際にサイドバーのスクロール位置と開いている状態を管理するJavaScriptのコードスニペットが含まれています。ただし、これは画面幅が十分な場合に限ります。特定の要素やセッションストレージのデータが利用可能かどうかを確認し、ハッシュ値が一致するかをチェックします。有効な場合、保存されたデータに基づいてサイドバーの状態を復元するためのカスタムHTML要素を定義します。\n\nさらに、デスクトップアプリケーションを構築するためのソフトウェアフレームワークであるTauriに関連するさまざまなセクションがリストされています。これらのセクションには、クイックスタート、コアコンセプト、セキュリティ、開発ツール、デバッグ、プラグイン、配布方法などが含まれています。文書にはTauriとそのコンポーネントのバージョン情報が記載されており、継続的な更新やリリースが示されています。\n\n要するに、このテキストはウェブアプリケーションにおけるサイドバーの状態を復元することについて述べており、Tauriのフレームワーク構造と更新の概要を提供しています。"
    }
  },
  {
    "id": "160a5f2825696717",
    "title": {
      "en": "A deliberate practice app for guitar players who want to level up",
      "ko": "기타 실력 향상 앱",
      "ja": "ギター上達アプリ"
    },
    "type": "story",
    "url": "https://www.captrice.io/",
    "score": 345,
    "by": "adityaathalye",
    "time": 1743218851,
    "content": "Practice using a smart metronome that captures metrics and\n        turns them into actionable insights; paired with an effective\n        practice method focusing on speed, endurance, accuracy, and\n        adaptability.\n\n        No knowledge of music theory required\n        A library of exercises to import from, or create your own\n        Free to use. No ads. No signup\n        Works in any browser on PC & smartphone. No app download needed\n        Data is stored on your device. Export/Delete anytime.\n        Works in \"Offline mode\" (if the browser supports it)",
    "summary": {
      "en": "Use a smart metronome that tracks your practice metrics and provides useful insights. It focuses on improving speed, endurance, accuracy, and adaptability.\n\nKey points:\n- No music theory knowledge is needed.\n- You can use a library of exercises or create your own.\n- It's free, with no ads or sign-up required.\n- Accessible on any browser, on both PC and smartphones, with no app download necessary.\n- Your data is stored on your device, and you can export or delete it whenever you want.\n- It can work offline if your browser supports it.",
      "ko": "스마트 메트로놈을 사용하면 연습 데이터를 추적하고 유용한 통찰력을 제공합니다. 이 도구는 속도, 지구력, 정확성, 적응력을 향상시키는 데 중점을 둡니다.\n\n음악 이론에 대한 지식이 없어도 사용 가능합니다. 제공되는 연습 자료를 활용하거나 자신만의 연습을 만들 수 있습니다. 이 서비스는 무료이며 광고나 회원 가입이 필요하지 않습니다. PC와 스마트폰에서 모든 브라우저를 통해 접근할 수 있으며, 앱을 다운로드할 필요도 없습니다.\n\n사용자의 데이터는 기기에 저장되며, 언제든지 내보내거나 삭제할 수 있습니다. 브라우저가 지원한다면 오프라인에서도 사용할 수 있습니다.",
      "ja": "練習のデータを追跡し、有益な洞察を提供するスマートメトロノームを利用しましょう。このメトロノームは、スピード、持久力、正確性、適応力の向上に重点を置いています。\n\n音楽理論の知識は必要ありません。用意されたエクササイズのライブラリを使うことも、自分でエクササイズを作成することもできます。利用は無料で、広告もなく、サインアップも必要ありません。PCやスマートフォンのブラウザからアクセスでき、アプリのダウンロードは不要です。\n\nデータはデバイスに保存され、必要に応じてエクスポートしたり削除したりできます。また、ブラウザが対応していればオフラインでも使用可能です。"
    }
  },
  {
    "id": "caa90419d1ee2f66",
    "title": {
      "en": "Go Optimization Guide",
      "ko": "최적화 가이드",
      "ja": "最適化ガイド"
    },
    "type": "story",
    "url": "https://goperf.dev/",
    "score": 376,
    "by": "jedeusus",
    "time": 1743452998,
    "content": "Patterns and Techniques for Writing High-Performance Applications with Go¶\nThe Go App Optimization Guide is a collection of technical articles aimed at helping developers write faster, more efficient Go applications. Whether you're building high-throughput APIs, microservices, or distributed systems, this series offers practical patterns, real-world use cases, and low-level performance insights to guide your optimization efforts.\nWhile Go doesn’t expose as many knobs for performance tuning as languages like C++ or Rust, it still provides plenty of opportunities to make your applications significantly faster. From memory reuse and allocation control to efficient networking and concurrency patterns, Go offers a pragmatic set of tools for writing high-performance code.\nWe focus on concrete techniques with measurable impact you can apply immediately—covering everything from core language features to advanced networking strategies.\n What’s Covered So Far¶\nCommon Go Patterns for Performance¶\nIn this first article, we explore a curated set of high-impact performance patterns every Go developer should know:\n\nUsing sync.Pool effectively\nAvoiding unnecessary allocations\nStruct layout and memory alignment\nEfficient error handling\nZero-cost abstractions with interfaces\nIn-place sorting and slices reuse\n\nEach pattern is grounded in practical use cases, with benchmarks and examples you can copy into your own codebase.\n\n What’s Coming Next¶\nHigh-Performance Networking in Go¶\nIn our upcoming deep dive into networking, we'll focus on building high-throughput network services with Go’s standard library and beyond. This includes:\n\nEfficient use of net/http and net.Conn\nManaging large volumes of concurrent connections\nPerformance tuning with epoll/kqueue and GOMAXPROCS\nLoad testing techniques and bottleneck diagnostics\nTBD...\n\nWe'll also explore when to drop down to lower-level libraries like fasthttp, and how to balance performance with maintainability.\n\n Who This Is For¶\nThis series is ideal for:\n\nBackend engineers optimizing Go services in production\nDevelopers working on latency-sensitive systems\nTeams migrating to Go and building performance-critical paths\nAnyone curious about Go’s performance model and trade-offs\n\nStay tuned—more articles, code samples, and tools are on the way. You can bookmark this page to follow the series as it evolves.\n\n    16 hours ago2025-03-31\n\n    1 week ago2025-03-20",
    "summary": {
      "en": "The Go App Optimization Guide helps developers create faster and more efficient applications using the Go programming language. It offers practical patterns, real-world examples, and performance insights for building high-throughput APIs, microservices, and distributed systems.\n\nAlthough Go has fewer performance tuning options compared to languages like C++ or Rust, it provides several ways to enhance application speed. Key techniques include:\n\n- Using sync.Pool effectively\n- Avoiding unnecessary memory allocations\n- Optimizing struct layout and memory alignment\n- Efficient error handling\n- Utilizing zero-cost abstractions with interfaces\n- Reusing slices for in-place sorting\n\nThe guide aims to provide actionable strategies with measurable results, supported by benchmarks and examples.\n\nUpcoming topics include high-performance networking, focusing on building efficient network services and managing many concurrent connections. This will cover techniques for using Go’s standard library and when to consider lower-level libraries for better performance.\n\nThis series is designed for backend engineers, developers of latency-sensitive systems, teams migrating to Go, and anyone interested in Go's performance capabilities. More articles and resources will be added over time.",
      "ko": "Go 앱 최적화 가이드는 개발자들이 Go 프로그래밍 언어를 사용하여 더 빠르고 효율적인 애플리케이션을 만들 수 있도록 돕습니다. 이 가이드는 고성능 API, 마이크로서비스, 분산 시스템을 구축하기 위한 실용적인 패턴, 실제 사례, 성능 통찰력을 제공합니다.\n\nGo는 C++나 Rust와 같은 언어에 비해 성능 조정 옵션이 적지만, 애플리케이션 속도를 향상시킬 수 있는 여러 방법을 제공합니다. 주요 기술로는 효과적인 sync.Pool 사용, 불필요한 메모리 할당 피하기, 구조체 레이아웃 및 메모리 정렬 최적화, 효율적인 오류 처리, 인터페이스를 통한 제로 비용 추상화 활용, 제자리 정렬을 위한 슬라이스 재사용 등이 있습니다.\n\n이 가이드는 측정 가능한 결과를 지원하는 벤치마크와 사례를 통해 실행 가능한 전략을 제공하는 것을 목표로 합니다.\n\n앞으로 다룰 주제는 고성능 네트워킹으로, 효율적인 네트워크 서비스 구축과 많은 동시 연결 관리에 중점을 둡니다. 이 부분에서는 Go의 표준 라이브러리를 사용하는 기술과 더 나은 성능을 위해 저수준 라이브러리를 고려해야 할 때에 대해 설명할 것입니다.\n\n이 시리즈는 백엔드 엔지니어, 지연에 민감한 시스템 개발자, Go로의 이전을 고려하는 팀, 그리고 Go의 성능 능력에 관심 있는 모든 사람을 위해 설계되었습니다. 시간이 지남에 따라 더 많은 기사와 자료가 추가될 예정입니다.",
      "ja": "Goアプリ最適化ガイドは、Goプログラミング言語を使用して、より速く効率的なアプリケーションを開発するための手助けをします。このガイドでは、高スループットのAPI、マイクロサービス、分散システムを構築するための実用的なパターンや実例、パフォーマンスに関する洞察が提供されています。\n\nGoはC++やRustと比べてパフォーマンス調整の選択肢が少ないですが、アプリケーションの速度を向上させるためのいくつかの方法があります。主なテクニックには、sync.Poolの効果的な利用、不要なメモリ割り当ての回避、構造体のレイアウトとメモリアライメントの最適化、効率的なエラーハンドリング、インターフェースを使ったゼロコストの抽象化、スライスの再利用によるインプレースソートが含まれます。\n\nこのガイドは、測定可能な結果を伴う実行可能な戦略を提供することを目的としており、ベンチマークや例によってサポートされています。\n\n今後のトピックには、高性能なネットワーキングが含まれ、効率的なネットワークサービスの構築や多くの同時接続の管理に焦点を当てます。これには、Goの標準ライブラリを使用する技術や、より良いパフォーマンスのために低レベルのライブラリを検討するタイミングが含まれます。\n\nこのシリーズは、バックエンドエンジニアやレイテンシに敏感なシステムの開発者、Goへの移行を考えているチーム、Goのパフォーマンス機能に興味がある人々を対象としています。今後も記事やリソースが追加される予定です。"
    }
  },
  {
    "id": "cfd88f8c59686ba8",
    "title": {
      "en": "Donkey Kong champion wins defamation case against Australian YouTuber Karl Jobst",
      "ko": "돈키콩 챔피언, 명예훼손 승소!",
      "ja": "ドンキーコング王者、名誉毀損訴訟勝利！"
    },
    "type": "story",
    "url": "https://www.theguardian.com/technology/2025/apr/01/donkey-kong-champion-billy-mitchell-wins-defamation-case-australia-youtuber-karl-jobst-ntwnfb",
    "score": 27,
    "by": "throw7",
    "time": 1743517630,
    "content": "Karl Jobst leaves the Brisbane district court on Tuesday. A judge has found Jobst defamed American gamer and Donkey Kong champion Billy Mitchell in a YouTube video. Photograph: Jono Searle/AAPView image in fullscreenKarl Jobst leaves the Brisbane district court on Tuesday. A judge has found Jobst defamed American gamer and Donkey Kong champion Billy Mitchell in a YouTube video. Photograph: Jono Searle/AAPYouTubeDonkey Kong champion wins defamation case against Australian YouTuber Karl JobstQueenslander ordered to pay $350,000 plus interest and costs as judge finds his video about American gamer Billy Mitchell was ‘recklessly’ based on false claim\n\n Get our breaking news email, free app or daily news podcast\nJosh TaylorTue 1 Apr 2025 06.51 BSTLast modified on Tue 1 Apr 2025 06.52 BSTShareA professional YouTuber in Queensland has been ordered to pay $350,000 plus interest and costs to the former world record score holder for Donkey Kong, after the Brisbane district court found the YouTuber had defamed him “recklessly” with false claims of a link between a lawsuit and another YouTuber’s suicide.William “Billy” Mitchell, an American gamer who had held world records in Donkey Kong and Pac-Man going back to 1982, as recognised by the Guinness World Records and the video game database Twin Galaxies, brought the case against Karl Jobst, seeking $400,000 in general damages and $50,000 in aggravated damages.Jobst, who makes videos about “speed running” (finishing games as fast as possible), as well as gaming records and cheating in games, made a number of allegations against Mitchell in a 2021 YouTube video. He accused Mitchell of cheating, and “pursuing unmeritorious litigation” against others who had also accused him of cheating, the court judgment stated.\n\n  Sign up for Guardian Australia’s breaking news email\nThe court heard Mitchell was accused in 2017 of cheating in his Donkey Kong world records by using emulation software instead of original arcade hardware. Twin Galaxies investigated the allegation, and subsequently removed Mitchell’s scores and banned him from participating in its competitions.The Guinness World Records disqualified Mitchell as a holder of all his records – in both Donkey Kong and Pac-Man – after the Twin Galaxies decision.The judgment stated that Jobst’s 2021 video also linked the December 2020 suicide of another YouTuber, Apollo Legend, to “stress arising from [his] settlement” with Mitchell, and wrongly asserted that Apollo Legend had to pay Mitchell “a large sum of money”.‘There’s no stress’: gamers go offline in retro console revivalRead moreThe judgment noted multiple versions of the video were published, with the allegations removed or addressed, before Jobst issued a retraction video in July 2021, two months after the original. The first video was viewed by more than 500,000 people, including 200,000 in Australia, the court heard.Judge Ken Barlow found Jobst had made five defamatory imputations about Mitchell, including that Mitchell had required Apollo Legend pay him a large sum of money and thus implying he “in essence, hounded Apollo Legend to death”.Barlow found this was “based on a fallacy” – and that Jobst, in not making further inquiries before publishing the video, had been “recklessly indifferent” to if it was true.View image in fullscreenJobst (middle) leaves the Brisbane district court on Tuesday. Photograph: Jono Searle/AAPJobst denied the imputations had been made in the video. He argued Mitchell had a pre-existing bad reputation because he had been previously exposed as a cheat, the court heard.Barlow found Mitchell did have an existing reputation as a cheat and for suing people who alleged he was a cheat, and found that Mitchell had expressed joy when he believed – incorrectly – on an earlier occasion that Apollo Legend may have died. But Barlow found Jobst had severely damaged Mitchell’s reputation and caused distress.He described Jobst as having a “self-aggrandising and perhaps self-protective tendency not to admit error and not to back down once he has taken a stance”.Barlow framed Jobst’s actions as a “crusade” against Mitchell, stating that he was trying to “[show] his audience that he is the knight who slew the Mitchell dragon”.The judgment summary noted that the court was not called on to decide, and did not decide, if Mitchell had cheated in his world record scores.The court awarded Mitchell $300,000 in damages for non-economic loss, and an additional $50,000 in aggravated damages due to Jobst’s publishing the video twice, mocking Mitchell’s complaint about it, failing to apologise and withdraw the allegations, and his “clear malice” towards Mitchell.Jobst was ordered to pay more than $40,000 in interest on the damages from the date of publication, as well as Mitchell’s costs.Mitchell had previously sued Twin Galaxies in the US in a case which settled in January 2024. As part of the settlement, Twin Galaxies published a statement that Mitchell had “produced expert opinion that the game play on the tapes of Mr Mitchell’s record game plays could depict play on an original unmodified Donkey Kong arcade hardware if the hardware involved was malfunctioning, likely due to the degradation of components”.The company said it would reinstate all of Mitchell’s previous scores on its website’s official historical database, and remove the dispute thread about Mitchell’s records. Mitchell’s Guinness World Records were also reinstated in June 2020.Explore more on these topicsYouTubeQueenslandBrisbaneGamesnewsShareReuse this content",
    "summary": {
      "en": "Karl Jobst, a YouTuber from Queensland, has been ordered by a Brisbane court to pay $350,000 to American gamer Billy Mitchell after being found guilty of defamation. Jobst made false claims in a 2021 YouTube video, linking Mitchell to a lawsuit and the suicide of another YouTuber, Apollo Legend. \n\nThe court determined that Jobst acted \"recklessly\" and did not verify the truth of his allegations before publishing the video. Judge Ken Barlow noted that Jobst's claims severely harmed Mitchell's reputation, despite Mitchell's existing controversies over cheating accusations in his gaming records. \n\nJobst was also criticized for his refusal to back down and for mocking Mitchell after the complaints were made. The court awarded Mitchell $300,000 for non-economic damages and $50,000 for aggravated damages due to Jobst's persistent and malicious actions. Additionally, Jobst must pay over $40,000 in interest and legal costs.",
      "ko": "퀸즐랜드 출신 유튜버 칼 조브스트가 브리즈번 법원에서 미국 게이머 빌리 미첼에게 35만 달러를 배상하라는 판결을 받았다. 조브스트는 2021년 유튜브 영상에서 미첼을 소송과 다른 유튜버 아폴로 레전드의 자살과 연결짓는 허위 주장을 했다.\n\n법원은 조브스트가 \"무모하게\" 행동했으며, 영상을 게시하기 전에 자신의 주장에 대한 진위를 확인하지 않았다고 판단했다. 켄 바를로 판사는 조브스트의 주장이 미첼의 명성에 심각한 해를 끼쳤다고 언급했으며, 미첼은 게임 기록과 관련된 사기 혐의로 이미 논란이 있었음에도 불구하고 피해를 입었다고 밝혔다.\n\n조브스트는 불만이 제기된 후에도 물러서지 않고 미첼을 조롱한 점에 대해서도 비판을 받았다. 법원은 미첼에게 비경제적 손해에 대해 30만 달러, 조브스트의 지속적이고 악의적인 행동으로 인한 가중 손해에 대해 5만 달러를 배상하라고 판결했다. 또한 조브스트는 4만 달러 이상의 이자와 법적 비용도 지불해야 한다.",
      "ja": "クイーンズランド州のユーチューバー、カール・ジョブストが、ブリスベンの裁判所からアメリカのゲーマー、ビリー・ミッチェルに対して35万ドルを支払うよう命じられました。ジョブストは名誉毀損の罪で有罪判決を受けました。彼は2021年のユーチューブ動画で、ミッチェルを訴訟や別のユーチューバー、アポロ・レジェンドの自殺に関連付ける虚偽の主張をしました。\n\n裁判所は、ジョブストが「無謀に」行動し、動画を公開する前に主張の真実を確認しなかったと判断しました。ケン・バーロー裁判官は、ジョブストの主張がミッチェルの評判に深刻な損害を与えたと指摘しました。ミッチェルはゲーム記録に関する不正行為の疑惑で既に物議を醸していましたが、それにもかかわらずジョブストの行動は問題視されました。\n\nジョブストは、苦情が寄せられた後も引き下がらず、ミッチェルを嘲笑したことでも批判されました。裁判所は、ミッチェルに対して経済的損害がないことに対する30万ドルと、ジョブストの執拗で悪意のある行動に対する加害的損害として5万ドルを認めました。さらに、ジョブストは4万ドル以上の利息と法的費用も支払う必要があります。"
    }
  },
  {
    "id": "ab0513d7954bbd01",
    "title": {
      "en": "Show HN: JavaScript PubSub in 163 Bytes",
      "ko": "163바이트 자바스크립트 PubSub",
      "ja": "163バイトのPubSub"
    },
    "type": "story",
    "url": "https://github.com/hassanshaikley/pico-pubsub",
    "score": 61,
    "by": "hmmokidk",
    "time": 1743385075,
    "content": "pico-pubsub\nThe smallest PubSub library possible. Zero Dependencies. 149 bytes.\nI wrote this article a while back. But I realized...why not just publish the code?\nSmaller than the competition.\n\nnano-pubsub\ntiny-pubusb\n\nBuilt with JS13K games in mind. Such as cred which is unfortunately in need of some weight loss soon, it is almost 25KB now.\nIf you have any ideas that may trim off even one single byte please share it. Create an issue! I don't mind.\nThe Source\nThis is the entire source (index.js).\nlet t = new EventTarget();\n\nsub = (e, c) => (t.addEventListener(e, c), () => t.removeEventListener(e, c));\npub = (n, d) => t.dispatchEvent(new CustomEvent(n, { detail: d }));\n\nUsage\nnpm install pico-pubsub\n\nimport \"pico-pubsub\"\n\nconst unsub = sub('jump', function (anything) {\n  console.log(\"someone jumped - \" + anything.detail)\n});\n\npub('jump', \"a_user_id\")\n>> \"someone jumped - a_user_id\"\n\nunsub()\n\npub('jump', \"another_user_id\")\n>> Nothing happens now\n\nTroubleshoot\n\nMight add TS support in the future. For now you can use the following snippet.\n\ndeclare global {\n  function pub(event: string, data: any): VoidFunction;\n  function sub(event: string, callback: (data: CustomEvent) => void): void;\n}\n\nIf you have export issues just copy paste and change export type.\n\nProve it\nThe following command will produce a 149b file:\nnpx esbuild index.js --bundle --minify --format=esm --outfile=bundle.js\nThe Competition\nComing in at #2 we have nano-pubsub which slims down to an impressive 194b...Not bad at all! Only ~30% larger.\n/**\n * @public\n */\nexport interface Subscriber<Event> {\n  (event: Event): void;\n}\n/**\n * @public\n */\nexport interface PubSub<Message> {\n  publish: (message: Message) => void;\n  subscribe: (subscriber: Subscriber<Message>) => () => void;\n}\n\n/**\n * @public\n */\nexport default function createPubSub<Message = void>(): PubSub<Message> {\n  const subscribers: { [id: string]: Subscriber<Message> } =\n    Object.create(null);\n  let nextId = 0;\n  function subscribe(subscriber: Subscriber<Message>) {\n    const id = nextId++;\n    subscribers[id] = subscriber;\n    return function unsubscribe() {\n      delete subscribers[id];\n    };\n  }\n\n  function publish(event: Message) {\n    for (const id in subscribers) {\n      subscribers[id](event);\n    }\n  }\n\n  return {\n    publish,\n    subscribe,\n  };\n}\n\nAnd at #3 we have tiny-pubsub which brings a non critical function to the table as well as an extra function with the way it handles unsubscribing! The agony! This comes in at a whopping 401b, more than twice nano-pubsub!\nlet subscriptions = Object.create(null);\n\nfunction subscribe(evt, func) {\n  if (typeof func !== \"function\") {\n    throw \"Subscribers must be functions\";\n  }\n  const oldSubscriptions = subscriptions[evt] || [];\n  oldSubscriptions.push(func);\n  subscriptions[evt] = oldSubscriptions;\n}\n\nfunction publish(evt) {\n  let args = Array.prototype.slice.call(arguments, 1);\n  const subFunctions = subscriptions[evt] || [];\n  for (let i = 0; i < subFunctions.length; i++) {\n    subFunctions[i].apply(null, args);\n  }\n}\n\nfunction unsubscribe(evt, func) {\n  const oldSubscriptions = subscriptions[evt] || [];\n  const newSubscriptions = oldSubscriptions.filter((item) => item !== func);\n  subscriptions[evt] = newSubscriptions;\n}\n\nfunction cancel(evt) {\n  delete subscriptions[evt];\n}\n\nmodule.exports = { subscribe, publish, unsubscribe, cancel };\n\nNotes\nIf you don't want to use the window object just do this, just know it'll cost ya 7 bytes:\nlet t = new EventTarget();\n\nexport default {\n  s: (e, c) => (t.addEventListener(e, c), () => t.removeEventListener(e, c)),\n  p: (n, d) => t.dispatchEvent(new CustomEvent(n, { detail: d })),\n};",
    "summary": {
      "en": "**Summary of pico-pubsub**\n\n- **Overview**: pico-pubsub is a minimalistic PubSub library for JavaScript, with no dependencies and a size of only 149 bytes, making it smaller than other similar libraries.\n\n- **Purpose**: It was created with JS13K games in mind and aims to be lightweight. The developer is open to suggestions for further size reductions.\n\n- **Basic Functions**:\n  - `sub(event, callback)`: Subscribes to an event and returns an unsubscribe function.\n  - `pub(eventName, data)`: Publishes an event with associated data.\n\n- **Example Usage**:\n  - Install with `npm install pico-pubsub`.\n  - Import and use the library to subscribe to an event and publish it. Unsubscribing is also straightforward.\n\n- **Troubleshooting**: TypeScript support might be added later, but a snippet is provided for basic global declarations.\n\n- **Competition**: Other libraries like nano-pubsub (194 bytes) and tiny-pubsub (401 bytes) are larger but offer similar functionalities.\n\n- **Conclusion**: pico-pubsub is a compact and efficient solution for event handling in JavaScript, ideal for projects where size is a critical factor.",
      "ko": "pico-pubsub는 JavaScript를 위한 최소한의 PubSub 라이브러리로, 의존성이 없고 크기가 단 149바이트에 불과해 다른 유사 라이브러리보다 작습니다. 이 라이브러리는 JS13K 게임을 염두에 두고 만들어졌으며, 가벼운 성능을 목표로 하고 있습니다. 개발자는 추가적인 크기 축소에 대한 제안도 열려 있습니다.\n\n기본 기능으로는 `sub(event, callback)`가 있습니다. 이 함수는 특정 이벤트에 구독하고, 구독 해제 기능을 반환합니다. 또 다른 기능인 `pub(eventName, data)`는 관련 데이터를 포함한 이벤트를 발행합니다.\n\n사용 예로는 `npm install pico-pubsub`로 설치한 후, 라이브러리를 가져와 이벤트에 구독하고 발행하는 방식이 있습니다. 구독 해제도 간단하게 진행할 수 있습니다.\n\n문제 해결을 위해 TypeScript 지원이 나중에 추가될 수 있지만, 기본적인 전역 선언을 위한 코드 조각이 제공됩니다. 경쟁 라이브러리로는 nano-pubsub(194바이트)와 tiny-pubsub(401바이트)가 있으며, 이들은 크기는 더 크지만 유사한 기능을 제공합니다.\n\npico-pubsub는 JavaScript에서 이벤트 처리를 위한 작고 효율적인 솔루션으로, 크기가 중요한 프로젝트에 적합합니다.",
      "ja": "pico-pubsubは、JavaScript用のミニマリストなPubSubライブラリで、依存関係がなく、サイズはわずか149バイトです。他の類似ライブラリよりも小型です。このライブラリは、JS13Kゲームを念頭に置いて作られ、軽量であることを目指しています。開発者は、さらなるサイズ削減の提案を歓迎しています。\n\n基本機能としては、`sub(event, callback)`があり、これはイベントに登録し、解除するための関数を返します。また、`pub(eventName, data)`を使うことで、関連データを伴ったイベントを発行できます。\n\n使用例としては、まず`npm install pico-pubsub`でインストールし、ライブラリをインポートしてイベントに登録し、発行することができます。解除も簡単です。\n\nトラブルシューティングに関しては、TypeScriptのサポートが後に追加される可能性がありますが、基本的なグローバル宣言のためのスニペットが提供されています。\n\n競合としては、nano-pubsub（194バイト）やtiny-pubsub（401バイト）などがあり、これらはサイズは大きいものの、似たような機能を提供しています。\n\npico-pubsubは、JavaScriptにおけるイベント処理のためのコンパクトで効率的なソリューションであり、サイズが重要なプロジェクトに最適です。"
    }
  },
  {
    "id": "7831ef19f7ed1677",
    "title": {
      "en": "KOReader: Open-Source eBook Reader",
      "ko": "KO리더: 무료 전자책 리더",
      "ja": "KOReader: 無料電子書籍リーダー"
    },
    "type": "story",
    "url": "https://github.com/koreader/koreader",
    "score": 321,
    "by": "charleshan",
    "time": 1743450749,
    "content": "KOReader is a document viewer primarily aimed at e-ink readers.\n\nDownload •\nUser guide •\nWiki •\nDeveloper docs\nMain features\n\nportable: runs on embedded devices (Cervantes, Kindle, Kobo, PocketBook, reMarkable), Android and Linux computers. Developers can run a KOReader emulator in Linux and MacOS.\n\nmulti-format documents: supports fixed page formats (PDF, DjVu, CBT, CBZ) and reflowable e-book formats (EPUB, FB2, Mobi, DOC, RTF, HTML, CHM, TXT). Scanned PDF/DjVu documents can also be reflowed with the built-in K2pdfopt library. ZIP files are also supported for some formats.\n\nfull-featured reading: multi-lingual user interface with a highly customizable reader view and many typesetting options. You can set arbitrary page margins, override line spacing and choose external fonts and styles. It has multi-lingual hyphenation dictionaries bundled into the application.\n\nintegrated with calibre (search metadata, receive ebooks wirelessly, browse library via OPDS), Wallabag, Wikipedia, Google Translate and other content providers.\n\noptimized for e-ink devices: custom UI without animation, with paginated menus, adjustable text contrast, and easy zoom to fit content or page in paged media.\n\nextensible: via plugins\n\nfast: on some older devices, it has been measured to have less than half the page-turn delay as the built in reading software.\n\nand much more: look up words with StarDict dictionaries / Wikipedia, add your own online OPDS catalogs and RSS feeds, over-the-air software updates, an FTP client, an SSH server, …\n\nPlease check the user guide and the wiki to discover more features and to help us document them.\nScreenshots\n\nInstallation\nPlease follow the model specific steps for your device:\nAndroid •\nCervantes •\nKindle •\nKobo •\nLinux •\nPocketbook •\nreMarkable\nDevelopment\nSetting up a build environment •\nCollaborating with Git •\nBuilding targets •\nPorting •\nDeveloper docs\nSupport\nKOReader is developed and supported by volunteers all around the world. There are many ways you can help:\n\nfix bugs and implement new features\ntranslate the program into your language or improve an existing translation\ndocument lesser-known features on the wiki\nhelp others with your knowledge on the forum\n\nRight now we only support liberapay donations.\nContributors",
    "summary": {
      "en": "KOReader is a versatile document viewer designed for e-ink readers and various devices like Kindle, Kobo, and Android. \n\n**Key Features:**\n- **Compatibility**: Works on many devices and can be emulated on Linux and MacOS.\n- **Document Support**: Reads multiple formats including PDFs, EPUBs, and more. It can reflow scanned PDFs and DjVu files.\n- **User Interface**: Offers a customizable, multi-lingual interface with options for margins, spacing, and fonts.\n- **Integration**: Connects with services like calibre, Wikipedia, and Google Translate for enhanced functionality.\n- **E-ink Optimization**: Designed for e-ink displays with simple menus and adjustable text settings.\n- **Extensibility**: Supports plugins and has features like dictionary lookups and online catalog integration.\n- **Speed**: Faster page-turning on some older devices compared to built-in readers.\n\nFor installation, users should follow specific steps for their device type. The project relies on volunteer contributions for development and support. You can help by fixing bugs, translating, or documenting features. Donations are accepted through liberapay.",
      "ko": "KOReader는 전자 잉크 리더와 Kindle, Kobo, Android와 같은 다양한 기기를 위해 설계된 다목적 문서 뷰어입니다. \n\n이 프로그램은 여러 기기에서 호환되며, Linux와 MacOS에서도 에뮬레이션이 가능합니다. 다양한 문서 형식을 지원하여 PDF, EPUB 등 여러 파일을 읽을 수 있으며, 스캔한 PDF와 DjVu 파일도 재구성할 수 있습니다. 사용자 인터페이스는 사용자 맞춤형으로 다국어를 지원하며, 여백, 간격, 글꼴 설정을 조정할 수 있는 옵션이 있습니다. \n\n또한, calibre, 위키백과, 구글 번역과 같은 서비스와 연결되어 기능을 확장할 수 있습니다. 전자 잉크 디스플레이에 최적화되어 있어 간단한 메뉴와 조정 가능한 텍스트 설정을 제공합니다. 플러그인을 지원하며, 사전 검색과 온라인 카탈로그 통합과 같은 기능도 포함되어 있습니다. 일부 구형 기기에서는 내장 리더보다 페이지 전환 속도가 더 빠릅니다. \n\n설치 과정은 사용자의 기기 유형에 따라 특정 단계를 따라야 합니다. 이 프로젝트는 자원봉사자들의 기여로 개발 및 지원이 이루어집니다. 버그 수정, 번역, 기능 문서화 등을 통해 도움을 줄 수 있으며, 기부는 liberapay를 통해 가능합니다.",
      "ja": "KOReaderは、電子インクリーダーやKindle、Kobo、Androidなどのさまざまなデバイス向けに設計された多機能な文書ビューアです。\n\nこのソフトウェアは、多くのデバイスで動作し、LinuxやMacOS上でもエミュレーションが可能です。PDFやEPUBなど、さまざまなフォーマットの文書を読み込むことができ、スキャンしたPDFやDjVuファイルも再フローできます。ユーザーインターフェースはカスタマイズ可能で、多言語対応です。余白や行間、フォントの設定を変更することができます。\n\nまた、calibreやWikipedia、Google翻訳などのサービスと連携し、機能を強化しています。電子インクディスプレイに最適化されており、シンプルなメニューと調整可能なテキスト設定が特徴です。プラグインをサポートしており、辞書検索やオンラインカタログとの統合機能も備えています。さらに、一部の古いデバイスでは、内蔵リーダーよりもページめくりが速くなっています。\n\nインストールの際は、デバイスの種類に応じた特定の手順に従う必要があります。このプロジェクトはボランティアの貢献によって開発とサポートが行われています。バグの修正や翻訳、機能の文書化などで協力することができます。また、liberapayを通じて寄付も受け付けています。"
    }
  },
  {
    "id": "1bcd23019c816f35",
    "title": {
      "en": "US Marines to get high-speed, radar-evading electric seagliders for rescue ops",
      "ko": "전투용 전기 해양 글라이더 도입",
      "ja": "米海兵隊、電動シーグライダー導入！"
    },
    "type": "story",
    "url": "https://interestingengineering.com/military/us-marines-seagliders-for-rescue-ops",
    "score": 61,
    "by": "jdmark",
    "time": 1743199017,
    "content": "ShareMilitaryUS Marines to get high-speed, radar-evading electric seagliders for rescue opsThe Viceroy seaglider can soar up to 180 mph over approximately 180 miles.\nUpdated: Mar 27, 2025 06:03 PM EST1Innovation🚀World’s smallest: Bee-mimicking flying robot uses magnets to aid in search, rescueJijo Malayila day ago2Energy🚀China hits jackpot with massive 110-million-ton offshore oil discoveryMaria Mocerinoa day ago3Science🚀407-million-year-old: Fossil of 1st land-dwelling giant breaks the tree of lifeMrigakshi Dixita day ago4Energy🚀Iron-sodium EV battery challenges Tesla Megapack, offers 7,000 cycles 20-year-lifeBojan Stojkovski2 days ago5Culture🚀British military outpost from 1781 unearthed in rare archaeological find in FloridaMaria Mocerino2 days ago6Science🚀3D simulations reveal disturbances at hypersonic speeds, flow patterns create challengesMrigakshi Dixit2 days ago7Energy🚀UK fusion reactors to get super strong tungsten to beat extreme heat with eMELT deviceAman Tripathi2 days ago8Transportation🚀China's supersonic jet C949 targets 50% range increase over Concorde, 95% less soundBojan Stojkovski2 days ago9Innovation🚀Smartest electronic skin with 'brain' could be magnetic miracle for humans, robotsRupendra Brahambhatt2 days ago10Military🚀Taiwan's 5-ton unmanned attack vessel with explosive warheads poses threat to ChinaBojan Stojkovski3 days ago1Culture🌟1.8 million tons of plastic waste turned into 3D printing threads in GermanyMaria Mocerino38 minutes ago2Innovation🌟US: Quantum computer solves real-world problem for automobile giant FordAmeya Paleja2 hours ago3Energy🌟Panasonic to recycle nickel from used lithium batteries for circular EV economyMrigakshi Dixit2 hours ago4Innovation🌟New humanoid robot enhances hospitality automation with smart, human-like featuresJijo Malayil2 hours ago5Energy🌟Next-gen nuclear reactors in US to get more efficient with Idaho's new test bedSujita Sinha2 hours ago6Science🌟Large Hadron Collider's successor to be 3 times its size, a massive 56.5-mile-longTejasri Gururaj2 hours ago7Energy🌟US firm turns lithium from old Tesla EV batteries into power source for dronesMrigakshi Dixit2 hours ago8Energy🌟US chemical firm eyes 100 MW nuclear reactors to cut 500,000 tons CO2 emissionsKaif Shaikh2 hours ago9Science🌟480-million-year-old sponge discovered in China built Earth's first skeletal reefMaria Mocerino3 hours ago10Innovation🌟Human-robot industrial collaboration to get a boost with advanced simulation techJijo Malayil3 hours agoKapil Kajal5 days ago2ShareViceroy seaglider Regent Craft\nRegent Craft, a developer specializing in all-electric seagliders based in Rhode Island, has completed its initial contract with the U.S. Marine Corps Warfighting Lab (MCWL).\n\nIn a significant progression, the company has secured a follow-up agreement, estimated at $10 million, which offers opportunities for further extensions.\n\nThe initial phase of Regent’s partnership with MCWL, valued at $4.75 million, encompassed twelve key deliverables that validated the technical feasibility of seagliders.\n\nThis phase began with testing a quarter-scale prototype and culminated in the recently successful sea trials of the full-scale Viceroy prototype.\n\nHigh-speed, radar-proof electric seagliders\n\nThe second phase of the agreement aims to deepen the evaluation of the Viceroy’s technical capabilities.\n\nThis will involve demonstrations relevant to specific defense operations, a crucial aspect as military needs evolve.\n\nTom Huntley, Vice President of Government Relations and Defense at Regent, expressed pride in continuing collaboration with the Marine Corps.\n\nHe highlighted the role of Regent’s seagliders in meeting national security requirements, specifically in logistics operations that face challenges in contested maritime environments.\n\nThe advantages of seagliders are particularly noteworthy for defense operations, where speed and efficiency can determine the success of missions.\n\nThe Viceroy seaglider can soar up to 180 mph over approximately 180 miles.\n\nOne of the standout features of sea gliders is their ability to take off and land on water.\n\nThis capability eliminates reliance on traditional runways, which can be vulnerable in conflict situations.\n\nFurthermore, these electric-powered vessels can be recharged from shore and ship resources, ensuring a dependable energy source during operations.\n\nAnother significant benefit is their low radar and sonar signatures. By flying close to the water’s surface, seagliders avoid radar detection while minimizing heat and infrared visibility due to their electric propulsion systems.\n\nFor rescue ops\n\nTheir design simplicity and fewer components translate into lower operational and maintenance costs when compared to traditional aviation and maritime vehicles.\n\nEarlier this year, Regent made strides in its manufacturing capabilities, breaking ground on a facility in the Quonset Business Park in North Kingstown, RI.\n\nSet to be operational by 2026, this facility will focus on component manufacturing, vehicle assembly, and testing for the Viceroy seaglider.\n\nRecent progress has included initiating sea trials for the Viceroy prototype in Narragansett Bay, representing a key milestone in advancing the seaglider’s maritime certification.\n\nFurthermore, Regent has submitted its Viceroy Design Basis Agreement to the U.S. Coast Guard, a necessary step in the certification process.\n\nAs geopolitical dynamics shift focus from land-based conflicts to maritime challenges, the complexities of operations in coastal and contested areas become increasingly pronounced.\n\nThe “tyranny of distance” concept describes the difficulty in efficiently transporting personnel and supplies between combat zones and resupply points.\n\nSituations such as island-hopping strategies in the Indo-Pacific or urgent rescue missions in open waters highlight the necessity for innovative solutions.RECOMMENDED ARTICLES\n\nHuntley emphasized the strategic advantages sea gliders could provide in maritime operations.\n\nDrawing from his experience with humanitarian missions as a former U.S. Coast Guard pilot, he underscored the potential of these vessels to enhance military operations in challenging environments.\n\nSeagliders could play a pivotal role in modern defense strategies, offering new options in increasingly complex operational landscapes.\n2COMMENTSABOUT THE EDITORKapil Kajal Kapil Kajal is an award-winning journalist with a diverse portfolio spanning defense, politics, technology, crime, environment, human rights, and foreign policy. His work has been featured in publications such as Janes, National Geographic, Al Jazeera, Rest of World, Mongabay, and Nikkei. Kapil holds a dual bachelor's degree in Electrical, Electronics, and Communication Engineering and a master’s diploma in journalism from the Institute of Journalism and New Media in Bangalore.NEWSLETTERThe Blueprint DailyStay up-to-date on engineering, tech, space, and science news with The Blueprint.Mail Me  By clicking sign up, you confirm that you accept this site's Terms of Use and Privacy PolicyNewsmilitaryPOPULAR ARTICLES1scienceMysterious magnetic anomaly over Earth puzzles scientists, risks space techNeetika Walter21 hours ago2militaryUS Marines deploy $5K kamikaze drones with 12-mile range for battlefield strikesKapil Kajala day ago3cultureOpenAI’s biggest surge since ChatGPT: AI Ghibli art “melts” GPUs as 1M users flood inAamir Khollama day ago4innovationNorwegian scientists unveil lighter wiring to boost EV range and efficiencyTejasri Gururaja day agoRELATED ARTICLEStransportation715 mph top speed: Bombardier to debut fastest civilian jet since ConcordeenergyScientists can now peer inside working EV batteries to boost its performanceinnovationUS’ new humanoid robot can serve in schools, hotels with minimal trainingculture$300 billion valuation: Sam Altman’s OpenAI to raise $40 billion for advancing AIJOBSLoading opportunities...",
    "summary": {
      "en": "The U.S. Marine Corps is set to receive advanced electric seagliders, known as the Viceroy, from Regent Craft. These seagliders can travel up to 180 mph over a distance of about 180 miles. Regent Craft recently secured a $10 million follow-up contract after successfully completing earlier tests, including sea trials of a full-scale prototype.\n\nThe Viceroy seaglider features the ability to take off and land on water, eliminating the need for runways, which can be vulnerable in military operations. They are also designed to be low-profile, reducing detection by radar and sonar. This makes them particularly useful for rescue missions and logistics in contested maritime areas.\n\nRegent is enhancing its manufacturing capabilities with a new facility in Rhode Island, expected to be operational by 2026. The development of these seagliders comes at a time when military strategies are increasingly focused on maritime challenges. Their design may significantly improve efficiency in transporting personnel and supplies during operations in difficult coastal environments.",
      "ko": "미국 해병대는 레지던트 크래프트에서 제작한 첨단 전기 해상 글라이더인 바이서로이를 도입할 예정입니다. 이 해상 글라이더는 시속 180마일로 약 180마일의 거리를 이동할 수 있습니다. 레지던트 크래프트는 이전에 실시한 해상 시험을 포함한 테스트를 성공적으로 마친 후 1천만 달러 규모의 후속 계약을 체결했습니다.\n\n바이서로이 해상 글라이더는 수면에서 이착륙할 수 있는 기능을 갖추고 있어, 군사 작전에서 취약할 수 있는 활주로가 필요하지 않습니다. 또한 레이더와 소나에 탐지되지 않도록 설계되어 있어, 특히 분쟁이 있는 해양 지역에서 구조 작전과 물류에 유용합니다.\n\n레지던트 크래프트는 2026년 가동 예정인 로드아일랜드의 새로운 시설을 통해 제조 능력을 강화하고 있습니다. 이러한 해상 글라이더의 개발은 군사 전략이 점점 더 해양 문제에 집중되고 있는 시점에서 이루어지고 있습니다. 이들은 어려운 해안 환경에서 인원과 물자를 운송하는 효율성을 크게 향상시킬 수 있을 것으로 기대됩니다.",
      "ja": "アメリカ海兵隊は、レジェントクラフトから先進的な電動シーグライダー「バイセロイ」を受け取る予定です。このシーグライダーは、時速180マイルで約180マイルの距離を移動することができます。レジェントクラフトは、フルスケールのプロトタイプの海上試験を含む以前のテストを成功裏に終えた後、1,000万ドルの追加契約を獲得しました。\n\nバイセロイのシーグライダーは、水上での離着陸が可能で、滑走路が不要です。滑走路は軍事作戦において脆弱な部分となることがあります。また、レーダーやソナーによる探知を減らすために、低いプロファイルで設計されています。これにより、特に競争の激しい海域での救助任務や物流に役立ちます。\n\nレジェントは、2026年に稼働予定の新しい工場をロードアイランド州に設立し、製造能力を強化しています。これらのシーグライダーの開発は、軍事戦略がますます海洋の課題に焦点を当てる中で進められています。彼らの設計は、困難な沿岸環境での人員や物資の輸送効率を大幅に向上させる可能性があります。"
    }
  },
  {
    "id": "e259e78386318098",
    "title": {
      "en": "The demoscene as a UNESCO heritage in Sweden",
      "ko": "스웨덴의 데모씬, 유네스코 유산!",
      "ja": "スウェーデンのデモ遺産"
    },
    "type": "story",
    "url": "https://www.goto80.com/the-demoscene-as-a-unesco-heritage-in-sweden",
    "score": 615,
    "by": "robin_reala",
    "time": 1743417597,
    "content": "The demoscene as a UNESCO heritage in Sweden\n\n\t\t\t\t\t\tMar 31, 2025 | ramblings\n\n\t\t\t\t\tThe demoscene has become a national UNESCO-heritage in Sweden, thanks to an application that Ziphoid and me did last year. This has already happened in several European countries, as part of the international Art of Coding initiative to make the demoscene a global UNESCO heritage. I think this makes plenty of sense, since the demoscene is arguablythe oldest creative digital subculture around. It has largely stuck to its own values and traditions throughout the world’s technological and economical shifts, and that sort of consistency is quite unusual in the digital world.\nThe main idea of the demoscene is to compete with productions that maximize a certain hardware, but that’s not what all demosceners like to do. My demogroup Hack n’ Trade for example, cares more about making weird stuff, and there are plenty of other groups like that. Some demosceners don’t release anything at all, but might do important work to keep the scene alive (BBS-trading, organizing parties, preserving software…).\nI’ve written plenty of papers and blog posts about the demoscene, and I’ve often felt a gap between the stuff I write as a researcher and my personal experience of the demoscene. There is certainly an international demoscene with big events and huge releases that can be described in general terms, but what has mattered more to me is the local scenes, the small parties and the people you hang out with. Meeting up with a bunch of friends and making weird computer stuff “for no reason, really” is a great setting. That’s what I enjoy the most, in the end. For other sceners, it’s different.\nThere is a sort of diversity in the scene that is difficult to capture and generalize. The Swedish coder with a well-paid programming job and a busy family life might consider the demoscene as an escape to his teenage years, while the LSD-munching raver from France who trades illegal warez on BBSs and makes weird pixel art considers the scene as a free culture without corporate or art world bullshit. There’s room for both in the scene, because it is werdly conservative and open at the same time. And perhaps that is one of the reasons why it should be considered an intangible heritage.",
    "summary": {
      "en": "The demoscene has been recognized as a national UNESCO heritage in Sweden, thanks to an application by Ziphoid and the author. This recognition is part of a broader effort across Europe to celebrate the demoscene, which is one of the oldest digital subcultures. The demoscene focuses on creative competitions that showcase digital skills, but not all participants are competitive; some prioritize artistic expression or community involvement.\n\nThe author highlights the difference between their research on the demoscene and their personal experiences, which emphasize local gatherings and friendships over large events. The scene is diverse, attracting people from various backgrounds, each with their own reasons for participating. This mix of traditional values and openness is a key reason for its recognition as intangible heritage.",
      "ko": "스웨덴에서 데모신이 국가 유네스코 유산으로 인정받았다. 이는 Ziphoid와 저자의 신청 덕분이다. 이 인정은 유럽 전역에서 데모신을 기념하려는 노력의 일환으로 이루어졌다. 데모신은 가장 오래된 디지털 서브컬처 중 하나로, 창의적인 경쟁을 통해 디지털 기술을 선보이는 데 초점을 맞추고 있다. 하지만 모든 참가자가 경쟁을 목표로 하는 것은 아니며, 일부는 예술적 표현이나 지역 사회 참여를 더 중요하게 생각한다.\n\n저자는 데모신에 대한 연구와 개인적인 경험의 차이를 강조한다. 그들은 대규모 행사보다는 지역 모임과 우정에 더 중점을 두고 있다. 이 장르는 다양한 배경을 가진 사람들을 끌어모으며, 각자가 참여하는 이유도 다르다. 전통적인 가치와 개방성이 결합된 이 특징이 데모신이 무형 유산으로 인정받는 중요한 이유 중 하나이다.",
      "ja": "スウェーデンでデモシーンが国のユネスコ遺産として認められました。これはZiphoidと著者の申請によるものです。この認識は、デモシーンを祝うためのヨーロッパ全体の取り組みの一環であり、デモシーンは最も古いデジタルサブカルチャーの一つです。デモシーンはデジタルスキルを披露するクリエイティブな競技に焦点を当てていますが、すべての参加者が競争を重視しているわけではなく、アートの表現やコミュニティへの関与を優先する人もいます。\n\n著者は、デモシーンに関する研究と自身の経験の違いを強調しています。著者の経験は、大規模なイベントよりも地域の集まりや友情を重視しています。このシーンは多様性に富んでおり、さまざまな背景を持つ人々が参加しており、それぞれが参加する理由も異なります。この伝統的な価値観とオープンさの組み合わせが、無形文化遺産として認められる大きな理由となっています。"
    }
  },
  {
    "id": "e254abb061cdb248",
    "title": {
      "en": "The Colors of Her Coat",
      "ko": "그녀의 코트 색깔",
      "ja": "彼女のコートの色"
    },
    "type": "story",
    "url": "https://www.astralcodexten.com/p/the-colors-of-her-coat",
    "score": 23,
    "by": "feross",
    "time": 1743515179,
    "content": "Share this postAstral Codex TenThe Colors Of Her CoatCopy linkFacebookEmailNotesMoreThe Colors Of Her Coat...Apr 01, 2025255Share this postAstral Codex TenThe Colors Of Her CoatCopy linkFacebookEmailNotesMore14445ShareI.In Ballad of the White Horse, G.K. Chesterton describes the Virgin Mary:Her face was like an open wordWhen brave men speak and choose,The very colours of her coatWere better than good news.Why the colors of her coat?The medievals took their dyes very seriously. This was before modern chemistry, so you had to try hard if you wanted good colors. Try hard they did; they famously used literal gold, hammered into ultrathin sheets, to make golden highlights.Blue was another tough one. You could do mediocre, half-faded blues with azurite. But if you wanted perfect blue, the color of the heavens on a clear evening, you needed ultramarine.Here is the process for getting ultramarine. First, go to Afghanistan. Keep in mind, you start in England or France or wherever. Afghanistan is four thousand miles away. Your path takes you through tall mountains, burning deserts, and several dozen Muslim countries that are still pissed about the whole Crusades thing. Still alive? After you arrive, climb 7,000 feet in the mountains of Kuran Wa Munjan until you reach the mines of Sar-i-Sang. There, in a freezing desert, the wretched of the earth work themselves to an early grave breaking apart the rocks of Badakhshan to produce a few hundred kilograms per year of blue stone - the only lapis lazuli production in the known world.Buy the stone and retrace your path through the burning deserts and vengeful Muslims until you’re back in England or France or wherever. Still alive? That was the easy part. Now you need to go through a chemical extraction process that makes the Philosopher's Stone look like freshman chem lab. \"The lengthy process of pulverization, sifting, and washing to produce ultramarine makes the natural pigment … roughly ten times more expensive than the stone it came from.\"Finally you have ultramarine! How much? I can’t find good numbers, but Claude estimates that the ultramarine production of all of medieval Europe was around the order of 30 kg per year - not enough to paint a medium-sized wall. Ultramarine had to be saved for ultra-high-value applications. In practice, the medievals converged on a single use case - painting the Virgin Mary’s coat.Madonna and Child, by Filippino LippiTo us moderns, this seems bizarrely specific. But the Catholic Church had united Europe in a single symbolic language, with lots of rules like \"this style is only used for such-and-such a saint”. Within this context, “ultramarine = Virgin Mary’s coat” was a normal piece of symbolic vocabulary. The Catholic Church did this because it worked. Joe Peasant would go to the festival, and his lord and lady would be wearing lovely blue robes, but the blue would always be very slightly faded. He’d go off to war, and the knights would have beautiful blue banners, but still, not quite right. Then he would go to church, and there would be a painting of the Virgin Mary, and there - and only there! - the perfect Platonic blue of Heaven would be translated down to Earth. And he would think, yeah, okay, this is the true religion.In the 19th century, a German man named Christian Gmelin discovered the process of producing synthetic ultramarine. And in the 1960s, French artist Yves Klein came up with a new synthetic ultramarine that he thought was even bluer. This being the 1960s, Klein leveraged his invention into a bunch of entirely blue paintings - literally, he just painted an entire canvas blue and hung it in a gallery -  which caused various scandals and counterscandals and discourse.It’s pretty, but is is art?Klein was a provocateur, and I’m no art historian, so don’t let me tell you what he actually meant by his all-blue paintings. But one thing he could have meant was a callback to all the medieval merchants and monks and miners; everyone who died to get a few drops of ultramarine back to Europe so the Virgin Mary’s robes could be perfectly celestially blue. “Look!” say Klein’s paintings. “Now we’re so rich, so blessed, that I can paint an entire canvas with the perfect blue of the heavens. I can use more blue than the total yearly output of medieval Europe, just so a couple of passers-by can frown and secretly wish that paintings still looked like stuff.”This painting tears me apart. I - confession - am the type of person who, after hearing the story of Afghanistan and Sar-i-Sang and medieval European art economics - would be tempted to buy lapis lazuli and stare at it longingly, trying to recapture the awe that Joe Peasant must have felt staring at the Virgin’s coat. But I’m also the type of person who, if I ran across Klein in a gallery, would frown and secretly wish that paintings still looked like stuff. Should I feel bad about this?A few stanzas later in Ballad Of The White Horse, the Virgin calls out those sophisticates who have lost the ability to innocently enjoy things:The wise men know all evil thingsUnder the twisted trees,Where the perverse in pleasure pineAnd men are weary of green wineAnd sick of crimson seasIf I can frown and walk past a canvas painted in pure ultramarine, would I also grow weary of green wine and crimson seas? If I went to Heaven, surely the first few days would be pretty great. But would I eventually walk past golden mountains and silver trees and crystal ships on crimson seas with the same nonchalance with which I walk past granite mountains / wooden trees / etc today?I know the neurology behind tolerance and accommodation as well as anyone. We’re not designed to be as delighted by the thousandth sunrise as the first. It’s no sin to be able to live a normal life instead of sitting paralyzed by the dazzling variety of beauty all around me. Still, it feels like there ought to be some virtue of innocence, some sense in which we try not to help along our own cynicism, or at the very least we don’t drag the more-naturally-gifted-with-innocence down with us. I sometimes imagine Heaven as a place of green wine, crimson seas, and golden mountains. Everyone goes there, good and bad alike. And if you still have enough innocence in your soul to enjoy things, then great, you’re in Heaven and presumably you have a good time. And if instead you’re one of those people who constitutionally hates everything, then you spend eternity writing thinkpieces with titles like “Can We As A Society Finally Shut Up About Golden Mountains?” or “Do The Wrong Type Of People Like Crimson Seas Too Much?”, and God and the Devil both agree that this counts as sufficient punishment. II.Erik Hoel has a new post, Can We As A Society Finally Shut Up About Golden Mountains. . . no, sorry! It’s actually called Welcome To The Semantic Apocalypse. Hoel is writing about the new “Ghiblification” trend, where people use OpenAI’s new art model to make photos look like Studio Ghibli anime.Source: https://x.com/GrantSlatton/status/1904631016356274286Hoel can’t resist Ghiblifying his own (adorable) children:…but he is also deeply worried:That picture of my kids reading together above, which is from a real photo—I exclaimed in delight when it appeared in the chat window like magic. So I totally get it. It’s a softer world when you have Ghibli glasses on. But by the time I made the third picture, it was less fun. A creeping sadness set in [...]While ChatGPT can’t pull off a perfect Miyazaki copy, it doesn’t really matter. The semantic apocalypse doesn’t require AI art to be exactly as good as the best human art. You just need to flood people with close-enough creations such that the originals feel less meaningful ... Many people are reporting that their mental relationship to art is changing; that as fun as it is to Ghibli-fy at will, something fundamental has been cheapened about the original [...]This is what I fear most about AI, at least in the immediate future. Not some superintelligence that eats the world (it can’t even beat Pokémon yet, a game many of us conquered at ten). Rather, a less noticeable apocalypse. Culture following the same collapse as community on the back of a whirring compute surplus of imitative power provided by Silicon Valley. An oversupply that satiates us at a cultural level, until we become divorced from the semantic meaning and see only the cheap bones of its structure. Once exposed, it’s a thing you have no relation to, really. Just pixels. Just syllables. In some order, yes. But who cares?Every weekend, my son gets to pick out one movie to watch with his little sister. It’s always Totoro. The Studio Ghibli classic. Arguably, the studio’s best movie. It’s also their slowest one, more a collection of individual scenes than anything else. Green growth and cicada whines and the specter of death amid life, haunting the movie in a way children can’t possibly understand, because it never appears. No one dies, or even gets close. For my kids, it’s just about a sibling pair, one so similar to themselves, and their fun adventures. But an adult can see the threat of death as the shadow opposite of the verdant Japanese countryside, in the exact same way that, in the movie, only children can see the forest spirit Totoro. The movie’s execution is an age-reversed mirror of its plot. And for this, I love it too . . . This weekend I will watch with them, and feel more distant from it than I did before. Totoro will just be more Ghibli. As I read Hoel’s post, I thought of ultramarine blue. But also, I thought of the first phonographic records. In 1890, hearing Enrico Caruso sing Pagliacci might be the highlight of your life, the crowning glory of a months-long trip to Italy and back. By 1910, you could hear Enrico Caruso without leaving your house. You could hear him twenty times a day if you wanted. The real thing in Naples would just be more Caruso.And I thought of computer monitors. If you wanted to see Lippi’s Madonna and Child when it was first painted in 1490, you would have to go to Florence and convince Lorenzo de Medici to let you in his house. Now you can see a dozen Lippi paintings in a sitting by typing their names into Wikipedia - something you never do. Why would you? They’re just more Lippi.And what about cameras? A whole industry of portraits, landscapes, cityscapes - totally destroyed. If you wanted to know what Paris looked like, no need to choose between Manet’s interpretation or Beraud’s interpretation or anyone else’s - just glance at a photo. A Frenchman with a camera could generate a hundred pictures of Paris a day, each as cold and perspectiveless as mathematical truth. The artists, defeated, degenerated into Impressionism, or Cubism, or painting a canvas entirely blue and saying it represented Paris in some deeper sense. You could still draw the city true-to-life if you wanted. But it would just be more Paris.Even the places themselves start to feel cheap, unearned. Medieval pilgrims would brave dangerous sea voyages to reach Jerusalem, then go into such fits of rapture that some of them would have seizures on the spot, or speak in tongues, or run off to a monastery and spend the rest of their life in contemplative prayer. I visited Jerusalem once. As holy cities go, I would describe it as cleaner than Benares but not quite as cool as Bodh Gaya. I stayed three days, then took off to Tel Aviv to see the architecture (which sucked).Are these semantic apocalypses? What if they are? It would be facile to say that, just because technology has threatened our sense of meaning before, we shouldn’t worry when technology threatens our sense of meaning today. Some of the past apocalypses were genuinely bad. The semantic satiation of the previous forms gave us modern art and architecture, hardly known for their broad-based appeal. Do we really want Studio Ghibli anime to go the way of paintings that look like stuff?When I contemplate these questions, I encounter a paradox. I acknowledge that my inability to marvel at a live Caruso opera in Naples has cost me something deep and beautiful. But I cannot wish that the phonograph was never invented. Does the increased variety and quantity of music compensate for the decreased profundity of each musical experience? Surely this is part of it, but I would never accept this excuse in other areas that have not yet been cheapened. A thousand moderately pleasant one-night-stands cannot equal one passionate love affair.Maybe Progress repays us with interest for every medium it takes? Without mass-produced, mass-transmissible images, music, and bright colors, we couldn’t have Studio Ghibli. Dare we hope that, if anime becomes too cheap to appreciate, that very cheapness will open the door to new forms of art? But why should this always be true? If AI is better than all human artists, and you can run 100,000 inference copies at 10x serial speed in a data center, then why should anything be non-cheap ever again?None of these sound fully convincing. Instead, maybe we must admit that we are relocating novelty and adventure from individual engagements with art, to the arc of history itself. Our generation will never know the once-in-a-life pleasure of hearing Caruso sing in Naples. But we will get the once-in-a-life pleasure of speaking to a generative AI for the first time. We could protect the magic of the Jerusalem pilgrimage by banning air travel, but it would be a fake and flimsy sort of magic, a sort of enforced perpetual civilizational childhood. What about the magic of seeing the clouds from above? Or the moon landing?III.We have recontextualized the semantic apocalypse from a one-time problem with GPT-4 to a recurrent historical pattern of technology undermining the uniqueness of art. But maybe we should zoom out further. This isn’t just about art. Technology breeds hedonic adaptation, and hedonic adaptation undermines everything.My lack of appreciation for ultramarine dye is of the same kind as my lack of appreciation for not dying of cholera. Or for coffee - an ordinary latte might blend beans from Ethiopia, Ghana, and Suriname with sugar from Brazil and vanilla from a rare orchid found only in Madagascar; by now, it’s so unbearably boring that you can find dozens of Reddit threads asking how to spruce it up, make it feel new again. We gripe about how LLMs are destroying wonder, never thinking about how we’re speaking to an alien intelligence made by etching strange sigils on a tiny glass wafer on a mountainous jungle island off the coast of China, then converting every book ever written into electricity and blasting them through the sigils at near-light-speed. It’s all amazing, and we’re bored to death of all of it.This has hitherto been slow enough to tolerate, but strong AI will make it all worse. You will see wonders beyond your imagination, nod, think “that’s a cool wonder”, and become inured to it. In the process, everything else that matters will wither away. If you get meaning from your job, the AI will take your job. If you get meaning from helping others, the AI will end poverty and cure cancer without your help. If you get meaning from your community, too bad - your friends are hanging out with AI sexbots now. It’ll all be great, of course. The AI taking your job means you never have to write another PowerPoint slide again; you can sit at the beach all day, sipping tropical cocktails. The AI ending poverty will be the best thing that ever happened. The sexbots . . . do you really need me to keep selling you on these? It’ll all be perfect forever, and you’ll spend the whole time writing Substack articles with titles like “Can We As A Society Finally Shut Up About The Wonders Beyond Our Imagination?” and “Do The Wrong Type Of People Like Cancer Cures Too Much?”Is there any hope? Something bothers me about the whole semantic apocalypse framing. It focuses too much on the social level, denies personal agency. Yes, we as a culture are post- some semantic apocalypse where listening to the great symphonies of the past has become so easy that we never do it. But you, as an individual, could do it right now. You could type “Mozart symphony” into YouTube and see what happens.G.K. Chesterton wrote lots of stuff about how if you were really holy and paying attention, then the thousandth sunset would be just as beautiful as the first. I used to interpret this as some kind of meaningless faux-profound slogan. Then I read his biography of William Blake - which made me ask myself, for the first time - what if William Blake was just describing his experience completely accurately? “When the sun rises, do you not see a round disc of fire somewhat like a guinea? O no, no, I see an innumerable company of the heavenly host crying Holy, Holy, Holy is the Lord God Almighty.” I know this sounds crazy, but there’s so much stuff like this, and he’s so consistent; Chesterton sort of suggests that maybe he is actually, literally, seeing the innumerable company of the heavenly host.And the ease with which Chesterton navigates this interpretation - the way he makes it the most natural thing in the world - made me wonder - what if Chesterton is also just describing his experience completely accurately? The thousandth sunset thing is so prominent in his works, and he never expresses any embarrassment about it, never says anything like “a saint would be able to do this, although of course I cannot”. If anything, the mood is one of mild exasperation that nobody listens to him. This sort of thing would make complete neurological sense - it’s just an increase in the precision of sensory evidence relative to top-down priors. Young children do it naturally - as any parent can tell you after having to read their one-year-old the same book for the thousandth time. Any adult can replicate it with five milligrams of psilocybin or a few dozen hours of samatha meditation. Who’s to say you can’t get it through genetics? Or through being very holy?Chesterton’s answer to the semantic apocalypse is to will yourself out of it. If you can’t enjoy My Neighbor Totoro after seeing too many Ghiblified photos, that’s a skill issue. Keep watching sunsets until each one becomes as beautiful as the first (the secret is that the innumerable company of the heavenly host sings in a slightly different key each time).I support Erik Hoel’s crusade to chart some society-level solution to the semantic apocalypse problem. You’re not allowed to say “skill issue” to society-level problems, because some people won’t have the skill; that’s why they invented the word “systemic”. But your personal relationship to the meaning in your life is not a society-level problem. While Erik Hoel works on the systemic issue, you should be thinking of your own individual soul. If you insist that anything too common, anything come by too cheaply, must be boring, then all the wonders of the Singularity cannot save you. You will grow weary of green wine and sick of crimson seas. But if you can bring yourself to really pay attention, to see old things for the first time, then you can combine the limitless variety of modernity with the awe of a peasant seeing an ultramarine mural - or the delight of a 2025er Ghiblifying photos for the first time.My group house’s holiday picture. I don’t really have that many kids, but GPT is an lmpressionist - it depicts how things feel from the inside, not how they really are.People say AI art isn’t art because it doesn’t mean anything. But I think it means the thing as Lippi’s Madonna: unless you become like little children, you will never enter the kingdom of heaven.Subscribe to Astral Codex TenBy Scott AlexanderP(A|B) = [P(A)*P(B|A)]/P(B), all the rest is commentary.SubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.255Share this postAstral Codex TenThe Colors Of Her CoatCopy linkFacebookEmailNotesMore14445SharePrevious",
    "summary": {
      "en": "The text discusses the significance of ultramarine dye in medieval art, particularly in depicting the Virgin Mary, highlighting the labor-intensive process of obtaining and producing this rare color. It emphasizes how the Catholic Church symbolically used ultramarine to represent divine perfection, contrasting it with the more accessible modern art forms, such as those created with synthetic ultramarine. The author reflects on the changing appreciation for art and beauty in a world where technology makes experiences more common and less profound, suggesting that easy access to art can lead to a diminished sense of wonder.\n\nThe discussion extends to the impact of AI and technology on our cultural appreciation, warning against a \"semantic apocalypse\" where the abundance of art and information leads to a loss of meaning. The author advocates for a personal approach to maintaining appreciation for beauty, encouraging individuals to cultivate a renewed sense of wonder rather than succumbing to cynicism about easily accessible art. Ultimately, it suggests that maintaining innocence and curiosity can allow us to experience the richness of art and life, even in a world flooded with imitations.",
      "ko": "중세 미술에서 울트라마린 염료의 중요성에 대해 논의하며, 특히 성모 마리아를 묘사하는 데 이 색상이 어떻게 사용되었는지를 강조합니다. 울트라마린 색상을 얻고 생산하는 과정이 매우 노동 집약적이라는 점도 부각됩니다. 가톨릭 교회는 울트라마린을 신성한 완벽함을 상징적으로 나타내는 데 사용했으며, 이는 현대 미술에서 합성 울트라마린으로 만들어진 작품들과 대조됩니다. 저자는 기술이 경험을 더 흔하고 덜 깊이 있게 만드는 세상에서 예술과 아름다움에 대한 감상이 어떻게 변해왔는지를 반영하며, 예술에 대한 접근이 쉬워질수록 경이로움이 줄어들 수 있다고 제안합니다.\n\n이 논의는 인공지능과 기술이 우리의 문화적 감상에 미치는 영향으로 확장되며, 예술과 정보의 풍부함이 의미의 상실로 이어지는 '의미적 종말'에 대한 경고를 포함합니다. 저자는 아름다움에 대한 감상을 유지하기 위해 개인적인 접근 방식을 지지하며, 쉽게 접근할 수 있는 예술에 대한 냉소에 빠지기보다는 새로운 경이로움을 키우도록 독려합니다. 결국, 순수함과 호기심을 유지하는 것이 모방으로 가득한 세상에서도 예술과 삶의 풍요로움을 경험할 수 있게 해준다고 제안합니다.",
      "ja": "ウルトラマリン染料は中世の芸術において特に重要な役割を果たしており、特に聖母マリアの描写においてその価値が際立っています。この色を得るための過程は非常に手間がかかり、希少性が高いことが強調されています。カトリック教会は、ウルトラマリンを神聖な完璧さを象徴するために使用し、現代のアート、特に合成ウルトラマリンで作られた作品との対比がなされています。著者は、技術の進歩によってアートや美に対する評価が変わってきたことを振り返り、簡単にアクセスできるアートが驚きの感覚を薄れさせる可能性があると指摘しています。\n\nさらに、AIや技術が文化的な評価に与える影響についても言及し、アートや情報の氾濫が意味の喪失を招く「意味の終末」を警告しています。著者は、美への評価を維持するためには個人的なアプローチが重要であると主張し、簡単に手に入るアートに対する皮肉に屈するのではなく、新たな驚きの感覚を育むことを勧めています。最終的には、無邪気さや好奇心を保つことで、模倣にあふれた世界でもアートや人生の豊かさを体験できると示唆しています。"
    }
  },
  {
    "id": "64f593cb399426b9",
    "title": {
      "en": "The Guardian flourishes without a paywall",
      "ko": "가디언, 무료로 성장하다",
      "ja": "ガーディアンの成功"
    },
    "type": "story",
    "url": "https://nymag.com/intelligencer/article/how-the-guardian-us-flourishes-without-a-paywall.html",
    "score": 522,
    "by": "bookofjoe",
    "time": 1743208287,
    "content": "stop the presses\n\n          The Newspaper Flourishing Without a Paywall\n\n            By\n        Charlotte Klein,\n          a features writer and media columnist at New York Magazine\n\n              Mar. 27, 2025\n\n              saved\n\n              Save this article to read it later.\n\n              Find this story in your account’s ‘Saved for Later’ section.\n\n                Comment\n\n                  Photo-Illustration: Intelligencer; Photos: Getty Images\n\n        There was a time in media when having a billionaire owner was an asset. For many outlets, this is still the case, particularly those publishing super-secretive group chats of powerful government officials who seem eager to trample on the rights of the press. But for others, like the Washington Post and the Los Angeles Times, both of which have seen their benefactors bend a knee to Donald Trump and sacrifice the integrity of their newspapers in the process, ownership has become a problem. A problem that the Guardian US, the American arm of the British newspaper, has exploited.\n\n  “All around us, media organizations have begun to capitulate,” editor Betsy Reed writes in the current fundraising appeal at the bottom of each Guardian article. “The Guardian has neither a self-interested billionaire owner nor profit-seeking corporate henchmen pressuring us to appease the rich and powerful. With the new administration boasting about its desire to punish journalists, and Trump and his allies already pursuing lawsuits against newspapers whose stories they don’t like, it has never been more urgent, or more perilous, to pursue fair, accurate reporting. Can you support The Guardian today?”\n\n  The asks themselves are not new.The Guardian for the past eight years has asked readers to donate in support of its journalism, a “reader revenue” program that helps the paper exist without a paywall. But the appeals have gotten more strident — and more Trump-focused — since the Post and the L.A. Times withdrew expected endorsements for Kamala Harris last fall, and to great effect. The last week in February was the second-biggest week in U.S. reader-revenue history for The Guardian; the biggest was the week of the 2024 presidential election.\n\n  The Guardian US expects to hit $44 million in voluntary reader donations in the U.S. and Canada this year, up 33 percent over last year, with some people supporting on a recurring basis and others giving one-time contributions. A first-person account by a Canadian citizen detained by ICE, for example, netted $105,000 in pledged donations. These contributions account for more than 60 percent of the American operation’s total revenue, which last year exceeded costs by $16 million (the rest of its revenue comes from advertising and philanthropic support from foundations). The amount that people are contributing on a pledged basis doubled in October —the month that Jeff Bezos pulled the Post’s endorsement for Harris — and has stayed at that level every month since, according to internal data provided by The Guardian, which just surpassed 350,000 recurring supporters in the U.S.\n\n  “Our messaging does appeal to people to support our work as a kind of cause, and I think that works for a number of reasons,” Reed said in a recent interview in her downtown office, “but one is that there is a real crisis of access to reliable information for people who don’t want or have the means to subscribe to the New York Times. That is a real problem that we have an answer to.”\n\n  Guardian managing director Steve Sachs added that The Guardian’s global perspective — the internal catchphrase of the U.S. operation is that it covers America for the world and the world for America — is also a draw at such a volatile moment in geopolitics. “People are even more interested now than they were six months ago,” he said. The Guardian US’ audience is split 50-50 between American and global readers.\n\n  The Guardian launched its U.S. operation more than a decade ago and seemed to have a promising start, hiring high-profile columnists like Michael Wolff and Glenn Greenwald and winning a Pulitzer for reporting on widespread secret surveillance by the National Security Agency. But as the Obama era gave way to the Trump era, the close association of the Guardian US with surveillance-state reporting made it less relevant to American readers. Annual losses led to restructuring that hit the U.S. operation, once housed in an airy Soho loft, particularly hard.\n\n  But during the pandemic, the Guardian US, like other news outlets, experienced a surge in reader interest, readying it for an expansion in 2022 that included hiring Reed, formerly of The Nation and the Intercept, and Sachs, who’d spent years on the business side of Time Inc. The audience has continued to grow, with Sachs noting that the site typically sees between 40 million to 50 million unique visitors a month. “We’re now at a place where our audience is actually bigger in the U.S. than The Wall Street Journal’s audience in the U.S.,” he said.\n\n  Leaders and organizations across the media industry have been trying to innovate in the way people can pay for news. Sachs says he’s been increasingly asked about the no-paywall model, with others in the industry wondering whether they should adopt the same. “If you’re creative enough, you can figure out some new things,” he tells people. “But the question is, does it scale?” If it’s “another half-million dollars, a million dollars, whatever, it’s not worth the effort,” he said. Other publications that were donations-focused, including Vox.com (which is owned by the same parent company as New York), have moved away to membership models as a result. “The reason I think that it works for us is we cover so much breaking news and it drives a lot of traffic, and we have the scale to make it work,” Reed said. “Even if we only monetize one percent, it’s still a lot.” Still, even scaled-up publications like the Guardian US have to contend with the fact that donations are very boom-and-bust.\n\n  Despite The Guardian’s strident anti-Trump fundraising pushes, its broader audience is less partisan, as is the tone of its news coverage. It’s a weird line to straddle. “The appeals that you see at the bottom of articles are really framed around issues of press freedom and our identity and our structure of ownership,” Reed said. “They are not appeals that say, ‘Trump is bad, you need to support The Guardian, we are against Trump.’” Maybe not explicitly. But they are clearly benefitting from this moment and using the new money to hire, with expectations to continue growing its staff in the U.S. this year.\n\n  The big question is: Will it last? “I mean, unfortunately, there’s no shortage of global crises going on, from what’s happening in the Middle East to Ukraine to what’s happening in this country,” Reed said. “The world is incredibly interested in the shitshow here. God, it’s like they’re rubbernecking at a car crash.”\n\n          Sign Up for the IntelligencerNewsletter\n          Daily news about the politics, business, and technology shaping our world.\n\n            Email\n\n          This site is protected by reCAPTCHA and the Google\n          Privacy Policy and\n          Terms of Service apply.\n\n        Vox Media, LLC Terms and Privacy Notice\n        By submitting your email, you agree to our Terms and Privacy Notice and to receive email correspondence from us.\n\n    More From This Series\n\n              The New Substack Universe\n\n              MSNBC’s Post–Joy Reid, Post-NBC Future Is Kind of Bright?\n\n              How Trump Is Dividing and Conquering the White House Press Corps\n\n        See All\n\n    Tags:\n\n            stop the presses\n\n            media\n\n            the guardian\n\n        Show\n\n    Leave a Comment\n\n      The Newspaper Flourishing Without a Paywall\n\n    const freeLayoutsInstances = [\n      'ecom-article',\n      'ecom-products',\n      'non-monetizable'\n    ];\n    const paywalledLayoutsInstances = ['paywalled-article'];\n    const layoutInstance = document.querySelector('html').getAttribute('data-layout-uri').split('/instances/')[1].replace('@published', '');\n    const siteSlug = 'intelligencer';\n    const keywords = [\"stop the presses\",\"media\",\"the guardian\"]; // This is set by handlebars in the server.\n    const featureTypes = window._nymPermutive?.article?.featureTypes;\n    const freeConditions = {\n      isStrategist: !paywalledLayoutsInstances.includes(layoutInstance) && siteSlug === 'strategist',\n      isFreeLayout: freeLayoutsInstances.includes(layoutInstance),\n      hasExcludePaywallTags: /paywall exclude/i.test(keywords.join(',')),\n      isEcomm: featureTypes && featureTypes.includes('ecomm')\n    };\n    const structuredData = {\n      '@context': 'http://schema.org',\n      '@id': '#articleSchema',\n      hasPart: {\n        '@type': 'WebPageElement',\n        cssSelector: '.article-content',\n        isAccessibleForFree: false\n      },\n      isAccessibleForFree: false\n    };\n    for (const condition of Object.keys(freeConditions)) {\n      if (!freeConditions[condition]) continue;\n      structuredData.isAccessibleForFree = true;\n      structuredData.hasPart.isAccessibleForFree = true\n      break;\n    }\n    const ldJsonScript = document.createElement('script');\n    ldJsonScript.type = \"application/ld+json\";\n    ldJsonScript.innerHTML = JSON.stringify(structuredData);\n    document.head.appendChild(ldJsonScript);",
    "summary": {
      "en": "**Summary: The Guardian's Success Without a Paywall**\n\nThe Guardian US has thrived without a paywall, attracting donations to support its journalism. Editor Betsy Reed emphasizes that the newspaper is free from billionaire ownership and corporate pressures, allowing it to focus on fair reporting, especially during politically sensitive times. Since the Washington Post and Los Angeles Times faced ownership issues, The Guardian's fundraising appeals have become more urgent, leading to a significant increase in reader donations.\n\nIn 2025, The Guardian US expects to receive $44 million in voluntary donations, a 33% increase from the previous year. This funding covers over 60% of its revenue, with the audience growing to more than 350,000 recurring supporters. The Guardian's model appeals to those seeking reliable news without subscription costs, especially amid a crisis of access to information.\n\nThe newspaper's audience is split between American and global readers, and it has seen monthly unique visitors surpass those of The Wall Street Journal. While other media outlets are exploring different payment models, The Guardian's no-paywall approach has proven effective. However, the sustainability of donations remains a concern, especially in a world with ongoing crises that keep audiences engaged.",
      "ko": "가디언 US는 유료 구독 없이도 성공적으로 운영되고 있으며, 저널리즘을 지원하기 위해 독자들의 기부를 받고 있습니다. 편집장인 베치 리드는 이 신문이 억만장자 소유나 기업의 압박에서 자유롭기 때문에 공정한 보도에 집중할 수 있다고 강조합니다. 특히 정치적으로 민감한 시기에 더욱 그러합니다. 워싱턴 포스트와 로스앤젤레스 타임스가 소유 문제로 어려움을 겪으면서 가디언의 기부 요청이 더욱 절실해졌고, 이로 인해 독자들의 기부가 크게 증가했습니다.\n\n2025년에는 가디언 US가 자발적인 기부로 4,400만 달러를 받을 것으로 예상하며, 이는 전년 대비 33% 증가한 수치입니다. 이 기부금은 전체 수익의 60% 이상을 차지하며, 35만 명이 넘는 정기 후원자가 생겼습니다. 가디언의 모델은 구독 비용 없이 신뢰할 수 있는 뉴스를 원하는 사람들에게 매력적이며, 정보 접근에 위기가 있는 상황에서 더욱 그러합니다.\n\n이 신문의 독자층은 미국 독자와 전 세계 독자들로 나뉘며, 월간 고유 방문자 수는 월스트리트 저널을 초과했습니다. 다른 언론사들이 다양한 유료 모델을 탐색하는 동안, 가디언의 무유료 접근 방식은 효과적임을 입증했습니다. 그러나 기부의 지속 가능성은 여전히 우려되는 문제로, 지속적인 위기가 있는 세상에서 독자들의 관심을 유지하는 것이 중요합니다.",
      "ja": "ガーディアンUSは、ペイウォールなしで成功を収め、ジャーナリズムを支えるための寄付を集めています。編集者のベッツィ・リードは、この新聞が億万長者の所有や企業の圧力から自由であるため、公正な報道に集中できると強調しています。特に政治的に敏感な時期には、その重要性が増します。ワシントン・ポストやロサンゼルス・タイムズが所有問題に直面する中で、ガーディアンの資金調達の呼びかけはより緊急性を帯び、読者からの寄付が大幅に増加しました。\n\n2025年には、ガーディアンUSが4400万ドルの自主的な寄付を受け取ることを期待しており、これは前年から33%の増加です。この資金は収益の60%以上をカバーしており、350,000人以上の定期的な支援者がいます。ガーディアンのモデルは、特に情報へのアクセスが危機的な状況にある中で、サブスクリプションコストなしで信頼できるニュースを求める人々に支持されています。\n\n新聞の読者層はアメリカと世界の読者に分かれており、月間のユニークビジター数はウォール・ストリート・ジャーナルを上回っています。他のメディアがさまざまな支払いモデルを模索する中で、ガーディアンのペイウォールなしのアプローチは効果的であることが証明されています。しかし、寄付の持続可能性は依然として懸念されており、継続的な危機が観客の関心を引き続き集める中で、その課題は重要です。"
    }
  },
  {
    "id": "1a9440645f621984",
    "title": {
      "en": "I made a show shuffler that shuffles shows in order",
      "ko": "쇼 셔플러 만들기",
      "ja": "番組シャッフル器"
    },
    "type": "story",
    "url": "http://git.tgwil.net/util/sortashuffle/",
    "score": 16,
    "by": "MountainMan1312",
    "time": 1743377413,
    "content": "BranchCommit messageAuthorAge\nstableImprove instructionsTristan Williams32 hours\nAgeCommit messageAuthorFilesLines\n32 hoursImprove instructionsHEADstableTristan Williams1-2/+26\n41 hoursConform to PEP 8 conventionsTristan Williams1-11/+97\n45 hoursOkay I actually tested it this time and it worksTristan Williams1-1/+1\n45 hoursDid I mention I need to start testing BEFORE commit?Tristan Williams1-1/+1\n45 hoursRefactor for future workTristan Williams1-14/+45\n10 daysStyle consistencyTristan Williams1-5/+2\n12 daysEnsure UTF-8 encoding for _PLAYLIST_INDEX.txtTristan Williams1-1/+1\n13 daysUpdate instructionsTristan Williams1-0/+5\n13 daysI should test things before I commitTristan Williams1-38/+18\n13 daysFix typoTristan Williams1-1/+1\n[...]\nClone\ngit://git.tgwil.net/util/sortashuffle\ngit@git.tgwil.net:util/sortashuffle",
    "summary": {
      "en": "Here’s a simplified summary of the text:\n\n- **Recent Changes by Tristan Williams**:\n  - **32 hours ago**: Improved instructions.\n  - **41 hours ago**: Adjusted code to follow PEP 8 style guidelines.\n  - **45 hours ago**: Confirmed that the code works after testing.\n  - **45 hours ago**: Realized the need to test before making commits.\n  - **45 hours ago**: Refactored code for future development.\n  - **10 days ago**: Ensured consistent style in the code.\n  - **12 days ago**: Made sure a specific file uses UTF-8 encoding.\n  - **13 days ago**: Updated instructions.\n  - **13 days ago**: Acknowledged the need to test before committing.\n  - **13 days ago**: Fixed a typo.\n\n- **Repository Information**: \n  - The project can be cloned from the provided Git links.",
      "ko": "트리스탄 윌리엄스의 최근 변경 사항은 다음과 같습니다. 32시간 전에는 지침을 개선했습니다. 41시간 전에는 코드가 PEP 8 스타일 가이드라인을 따르도록 조정했습니다. 45시간 전에는 테스트 후 코드가 제대로 작동함을 확인했습니다. 같은 시간에 커밋하기 전에 테스트가 필요하다는 것을 깨달았습니다. 또한, 미래 개발을 위해 코드를 리팩토링했습니다. 10일 전에는 코드의 일관된 스타일을 보장했습니다. 12일 전에는 특정 파일이 UTF-8 인코딩을 사용하도록 확인했습니다. 13일 전에는 지침을 업데이트하고, 커밋하기 전에 테스트가 필요하다는 점을 인정했습니다. 같은 날 오타도 수정했습니다.\n\n이 프로젝트는 제공된 Git 링크를 통해 클론할 수 있습니다.",
      "ja": "トリスタン・ウィリアムズによる最近の変更についての要約です。32時間前に、指示が改善されました。41時間前には、コードがPEP 8スタイルガイドラインに従うように調整されました。45時間前には、テスト後にコードが正常に動作することを確認しました。また、同じく45時間前に、コミットする前にテストが必要であることに気づきました。さらに、将来の開発に備えてコードをリファクタリングしました。10日前には、コードのスタイルが一貫していることを確認しました。12日前には、特定のファイルがUTF-8エンコーディングを使用していることを確認しました。13日前には、指示が更新され、コミット前にテストが必要であることが認識されました。また、同じく13日前に誤字が修正されました。\n\nプロジェクトは、提供されたGitリンクからクローンできます。"
    }
  },
  {
    "id": "97b627fc8c34629d",
    "title": {
      "en": "Netflix's Media Production Suite",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://netflixtechblog.com/globalizing-productions-with-netflixs-media-production-suite-fc3c108c0a22",
    "score": 201,
    "by": "MattSayar",
    "time": 1743469353,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "da8bb41deffb1d9e",
    "title": {
      "en": "Sales Compensation Simulator – Tool for Founders",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.exec.com/sales-comp",
    "score": 70,
    "by": "seanlinehan",
    "time": 1743262447,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "3ed3d94d29b20ad5",
    "title": {
      "en": "Interview with the Creator of Deluxe Ski Jump",
      "ko": "델럭스 스키 점프 제작자 인터뷰",
      "ja": "デラックススキージャンプの裏側"
    },
    "type": "story",
    "url": "https://spillhistorie.no/interview-with-the-creator-of-deluxe-ski-jump/",
    "score": 66,
    "by": "Kolorabi",
    "time": 1743169260,
    "content": "Interview with the creator of Deluxe Ski Jump\n\n\t\t\tJussi Koskela tells us the secret behind the game’s unique feeling of flying.\n\nThis is a complicated time for ski jumping in Norway. Interest has been declining for years, with plenty of negative press, and it all peaked recently with the disappointing revelations of widespread suit cheating among Norwegian jumpers. But perhaps we can use this rough patch to reminisce about truly great ski jumping experiences — experiences that, of course, took place in front of a screen.\nDeluxe Ski Jump developer Jussi Koskela’s ski-jumping toy that you can see at the Finnish Museum of Games (Photo: Jussi Koskela)\nThere have been several games that have given us varying degrees of that wonderful feeling of lift. Growing up, I mostly stared into a screen powered by MS-DOS, and I’m guessing that most of us DOS folks have at some point played The Games: Winter Challenge. It offered a variety of winter sports activities, but let’s be honest — ski jumping was the only fun one. It was incredibly frustrating, though, that it was impossible to land jumps where you actually timed the take-off perfectly.\nBack when the New Year’s competition in Garmisch-Partenkirchen felt more important than New Year’s Eve, I was thrilled to discover a little game called Four Hills by Jukka Hakosalo. It let me take part in the whole Four Hills Tournament, competing against legends like Bredesen and Goldberger. The game was very simple, but sometimes it doesn’t take much to be entertained.\nBut everything changed the year we discovered Deluxe Ski Jump. It was the game we’d been waiting for all along. At last, the ultimate ski jumping game had arrived. We had a chat with the man who gave us this gift more than 25 years ago.\nFirst of all—who are you?\nI’m Jussi Koskela, a 45-year-old solo game developer from Finland, and I’ve been working on the Deluxe Ski Jump games since 1999.\nJussi Koskela at the Pixel Heaven 2024 in Warsaw together with DSJ2 players. (Photo: Jussi Koskela)\nHow did you get started with game development?\nI started making little games already as a child. When I was 9, I wrote my first «game», where the goal was to kill a giant spider by swinging a sword at the right time. You just had to press F1 once to win the game.\nLater, my games grew in complexity and size little by little. I sold my first game when I was 12, to a friend who was 7. I think he was mostly interested in the source code…\nCan you tell us about your first commercial game, Fatal Fumes?\nFatal Fumes is a top-down racing game I developed as a hobby project in my teens. My brother helped me a bit with the physics, and a friend contributed the music. We also built the tracks together.\nFor me, Fatal Fumes was mostly a learning project, since it was my first full-scale game. Programming was very different back then because there were no ready-made libraries for sound or graphics, so you had to code everything from scratch. The biggest challenge was getting the game to run smoothly at 70 FPS. I also struggled a bit with implementing CPU opponents who could drive well enough.\nOf course, we dreamed of big sales, but Fatal Fumes never became a commercial success. Still, I was very proud that the game’s demo was included on the cover disks of PC Gamer and PC Format magazines in the UK.\nFatal Fumes. Image: Mobygames.\nMy friends and I played Slicks ’n’ Slide a lot, and it was definitely an inspiration for our racing game. Fatal Fumes aimed to be something similar, but with larger tracks that spanned multiple screens. The charm of Slicks ’n’ Slide was always the multiplayer mode on a shared keyboard, but that didn’t work so well in Fatal Fumes, since I never managed to implement proper split-screen.\nHow did you end up making a new ski jumping simulator?\nI had been experimenting with side-scrolling graphics that gave an effective 3D feel. It was originally meant for a platformer, but at some point, it reminded me of a ski jumping hill. I felt that other ski jumping games at the time had very simplistic physics and didn’t really capture the feeling of flight, so I decided to give it a go myself.\nWhat were the biggest challenges you faced?\nThe biggest challenge was time. I only had three months off after high school before starting military service, and I finished the game just one day before I had to report in!\nTechnically, the hardest part was making the hills and environments look believable using my «fake» 3D graphics. The actual simulation part was mostly basic high school physics. I never took any programming courses in high school, so experience and code snippets from earlier projects were invaluable.\nThe most important factor in the game’s success was making the jumping feel fantastic—and I spent a lot of time on that. I think the biggest innovation was how the mouse controls were mapped to the jumper’s movements. It made players respond with their whole bodies, as if they were ski jumpers themselves.\nOriginal Deluxe Ski Jump. Image: Mobygames.\nYes, what is actually the secret behind creating the great jumping feel in DSJ?\nI give the player 100% control over the jump, without artificial hints about how to do it. The only feedback is the simulated response on the screen, so the player has to actually feel the ski jump physics. This puts the player directly in the jumper’s boots. Other ski jumping games had indicators for take-off and flight position, which caused players to focus on the UI instead of the jumper – which breaks immersion.\nIn DSJ, that sense of being the jumper is reinforced by mapping the player’s fingers to the jumper’s legs. During take-off, you press both mouse buttons—just like a real jumper pushes off with both legs. That connection between fingers and legs continues into the landing. This “two-button” mechanic was so unique that some mouse drivers even had trouble registering both buttons at the same time!\nMy advice to other game developers is to start with the core mechanics. Dare to be unique, fine-tune it until it’s just right – and then build the rest of the game around it.\nDSJ is really fun, but it also increased the risk of mouse strain. How much pain do you think you’ve caused your players?\nI was aware of the potential risks – not to mention neck stiffness from players tilting their heads to the right – but thankfully I haven’t received many complaints. I suppose virtual ski jumping is still safer than jumping from an actual hill…\nLots of options to customize the suit… (Photo: Jon Håvard Gundersen)\nIt’s been a while since we feared Finnish ski jumpers like Janne Ahonen or Toni Nieminen. How’s ski jumping interest in Finland today?\nInterest has declined due to a lack of success on the hill, which in turn makes it harder to find sponsors. It’s a vicious cycle. In my opinion, the decline began when the national broadcaster stopped airing competitions on free TV. Before that, ski jumping was national entertainment.\nBut interest in DSJ remains high?\nAll Deluxe Ski Jump games have held up well. In addition to online tournaments, there are still many events for the original DOS version. Last year, I was invited to two such tournaments in Poland, and the players were still very enthusiastic.\nIn the beginning, most DSJ players came from traditional ski jumping countries like Finland, Norway, Germany, Austria, and Slovenia. But the huge Adam Małysz wave in Poland in the early 2000s changed everything. “Małyszomania” has carried DSJ’s success in Poland right up to today, and currently about 90% of new players are from Poland. Norway’s golden age with DSJ was definitely the early 2000s.\nYou made a lot of changes in version four of Deluxe Ski Jump. Can you tell us a bit about that process?\nMy goal has always been to make each release more realistic than the last. The earlier DSJ games had only fictional hills, so the physics didn’t need to match reality very closely.\nUp through DSJ 3, I wrote the physics engines myself, but for DSJ 4, I decided to use the Open Dynamics Engine.\nFor DSJ 4, I had to study human physiology – things like skeleton structure, limb mass, joint movement, and muscle behavior – to ensure the jumper behaved naturally. Some real ski jumpers also gave me valuable data about how the jumper should move during each phase of the jump.\nJussi Koskela shows the exhibition for Deluxe Ski Jump 2 at the Finnish Museum of Games (Photo: Jussi Koskela)\nIt took months of hard work (and plenty of frustration) to make the simulation both realistic, fun, and highly playable. Hitting two out of three was fairly easy, but achieving all three at once felt nearly impossible. In a simulation like this, everything affects everything else, so even small changes could either fix or ruin the entire experience.\nThe balance between physical realism and playability is tricky. In DSJ4, everything is geared toward realism, but I had to give the jumper super-strong ankles to ensure that flight adjustments felt responsive. Without that, the controls felt too loose, and players didn’t feel in control.\nThe game had a sort of «rebirth» in 2020 when I added the ability for players to create their own hills. Since then, the community has produced hundreds of high-quality hills—most of them far more detailed than any I’ve made myself.\nWhat are you working on these days?\nIn addition to working part-time at a non-game software company, I also work part-time on Deluxe Ski Jump and a new update for version 4.\nThe next update will focus on improving the offline game mode and making it more configurable. It’ll be possible to compete in new formats (e.g., super teams) and run multiple parallel tournaments and cups. Almost everything will be adjustable—even after the season has started.\nFinally, we’re impressed by the Finnish Museum of Games, is your game represented in the museum?\nYes, Deluxe Ski Jump is featured at the Finnish Museum of Games. Visitors can play DSJ2, and they also have some of my original sketches, the DSJ mouse, and a ski jumper toy from my childhood on display.\nWe thank Jussi Koskela for the chat and look forward to seeing what the future holds for Deluxe Ski Jump. Visit the official site to find out more about the game.\nThe images are supplied by Jussi himself, or taken from Mobygames.\nArticle by Jon Håvard Gundersen.\n\nWe have more content in English, including interviews and features. You can find it here. This article is also available in Norwegian.",
    "summary": {
      "en": "**Summary of Interview with Jussi Koskela, Creator of Deluxe Ski Jump**\n\nJussi Koskela, a Finnish game developer, created Deluxe Ski Jump, a popular ski jumping simulator, in 1999. The game stands out for its realistic flying experience, which Koskela achieved by allowing players full control over their jumps without relying on artificial cues. This immersive design connects players physically to the jumper's movements.\n\nKoskela started programming games as a child and released his first commercial game, Fatal Fumes, during his teens. He faced challenges in creating Deluxe Ski Jump, especially with time constraints and developing believable graphics. His innovative two-button control mechanism enhanced the gameplay experience.\n\nDespite declining interest in real-life ski jumping in Finland, Deluxe Ski Jump remains popular, particularly in Poland, where a wave of enthusiasm around ski jumper Adam Małysz brought new players. The latest version, Deluxe Ski Jump 4, incorporates realistic physics and allows players to create their own jumping hills.\n\nCurrently, Koskela is working on updates for the game, focusing on enhancing offline modes and customization options. Deluxe Ski Jump is also showcased at the Finnish Museum of Games, where visitors can experience the game and view related memorabilia.",
      "ko": "핀란드의 게임 개발자 유시 코스켈라가 1999년에 만든 델럭스 스키 점프는 인기 있는 스키 점프 시뮬레이터입니다. 이 게임은 플레이어가 점프를 할 때 인위적인 신호에 의존하지 않고 완전한 제어를 할 수 있도록 하여 현실감 넘치는 비행 경험을 제공합니다. 이러한 몰입감 있는 디자인은 플레이어가 점프하는 선수의 움직임과 신체적으로 연결될 수 있게 합니다.\n\n코스켈라는 어린 시절부터 게임 프로그래밍을 시작했으며, 10대 시절에 첫 상업 게임인 페이탈 퓨메스를 출시했습니다. 델럭스 스키 점프를 만드는 과정에서 그는 시간 제약과 현실감 있는 그래픽 개발 등 여러 도전에 직면했습니다. 그의 혁신적인 두 개의 버튼으로 조작하는 방식은 게임 플레이 경험을 향상시켰습니다.\n\n핀란드에서 실제 스키 점프에 대한 관심이 줄어들고 있음에도 불구하고, 델럭스 스키 점프는 여전히 인기를 끌고 있습니다. 특히 폴란드에서는 스키 점프 선수 아담 말리시의 열풍 덕분에 새로운 플레이어들이 유입되었습니다. 최신 버전인 델럭스 스키 점프 4는 현실적인 물리 엔진을 적용하고 플레이어가 자신만의 점프대를 만들 수 있는 기능을 포함하고 있습니다.\n\n현재 코스켈라는 게임 업데이트 작업을 진행 중이며, 오프라인 모드와 사용자 맞춤 설정 옵션을 강화하는 데 집중하고 있습니다. 델럭스 스키 점프는 핀란드 게임 박물관에서도 전시되고 있어 방문객들이 게임을 체험하고 관련 기념품을 볼 수 있습니다.",
      "ja": "フィンランドのゲーム開発者ユッシ・コスケラは、1999年に人気のスキージャンプシミュレーター「デラックススキージャンプ」を制作しました。このゲームは、プレイヤーがジャンプを完全にコントロールできることでリアルな飛行体験を提供しており、人工的な合図に頼らない設計が特徴です。この没入感のあるデザインは、プレイヤーをジャンパーの動きに物理的に結びつけます。\n\nコスケラは子供の頃からゲームプログラミングを始め、ティーンエイジャーの時に初の商業ゲーム「フェイタルフューム」をリリースしました。「デラックススキージャンプ」の制作では、時間の制約やリアルなグラフィックの開発に苦労しましたが、革新的な二ボタンの操作メカニズムがゲーム体験を向上させました。\n\nフィンランドでは実際のスキージャンプへの関心が低下しているものの、「デラックススキージャンプ」は依然として人気があり、特にポーランドではスキージャンパーのアダム・マウリシュに対する熱狂が新たなプレイヤーを呼び寄せました。最新バージョンの「デラックススキージャンプ4」では、リアルな物理エンジンが組み込まれ、プレイヤーが自分のジャンプ台を作成できるようになっています。\n\n現在、コスケラはゲームのアップデートに取り組んでおり、オフラインモードやカスタマイズオプションの強化に焦点を当てています。また、「デラックススキージャンプ」はフィンランドゲーム博物館でも展示されており、訪問者はゲームを体験したり、関連する記念品を見たりすることができます。"
    }
  },
  {
    "id": "645b8cbb17dc887e",
    "title": {
      "en": "Jargonic: Industry-Tunable ASR Model",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://aiola.ai/blog/introducing-jargonic-asr/",
    "score": 48,
    "by": "agold97",
    "time": 1743492923,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "807a9383c0a645a5",
    "title": {
      "en": "Canoeing on the Danube",
      "ko": "다뉴브 카약 여행",
      "ja": "ダニューブのカヌー"
    },
    "type": "story",
    "url": "http://jameswarnersmith.co.uk/canoeing-the-continent/canoeing-the-danube",
    "score": 59,
    "by": "karagenit",
    "time": 1743260755,
    "content": "CANOEING ON THE DANUBEThe Danube is Europe's second longest river and undergoes dramatic changes throughout its course. Though big, it is paddled fairly regularly each summer, particularly from Germany through to Hungary. There is also the organised Tour International Danubeeach year, a huge regatta where a large group paddle the entire length of the river. Their itinerary is extremely useful, outlining distances per day, the locations of locks and more. The 2017 itinerary can be found here. Below are probably just a few of your questions, so if you want to know more just get in touch through our contact page.\n\n  What maps did you use to canoe the Danube?For the Danube we used the BIKELINE maps for the DANUBE CYCLE PATH (donauradweg) which come in a range of languages and can be ordered online. These maps cover the entire river and are a fantastic scale providing great detail. The maps also act as a concise guide book giving tid-bits of historic information along route. You need nothing more than these maps with the exception of the final 300kms of the Danube where the cycle path leaves the river in Romania. If you plan to canoe this section it is important to get another, up to date, map since the river here is awash with islands and difficult to navigate. To find such maps we suggest you contact Tour International Danube(TID)who apparently provide specific maps for this section.Do you need a river permit to canoe on the Danube?No. The Danube is an international waterway and permits are not required. There are certain rules for individual countries that should be adhered to however. In Serbia national flagsmust be flown (or an equivalent sticker on the canoe). In Austria life jackets must be worn at locks or dams (we strongly suggest you wear lifejackets the entire time, however. We did). In Germany you should have a boat name and address clearly marked on the vessel and should be affiliated with a canoeing body (for us membership of the British Canoe Union (BCU)).Where are there dams on the river?The Danube has more than 100 Dams with most in Germany and Austria, quickly petering out thereafter. All of these dams and locks are well signposted and some have buoys that lead you to the river bank. The Tour International Danubepublishes its annual itinerary which features all there camp points, along with all obstacles and locks including what bank you should pull in for portage (L or R) and what distance marker the Portage should begin. This information is accurate and definitely worth using, it can be found hereon their 2017 PDF file.N.B. On TIDs itinerary for 15.07.17 in location named 'Cunovo' the description is to portage on the right bank to the old Danube river bed. This complicated section of the itinerary is deceiving, since there are really two options on the right hand side:You can go 'far right' or to a large dam that is on the right hand side but does not go all the way to the river-bank. It is for this dam you want to head and, once closer, you will find sign posts to help you with the portage. In short: at 1852 go right but head for the dam not for the river-bank, here there are signs for canoes.How do I get around the dams?There are a two ways around the locks. At most you must carry or wheel your canoe. There will be a ramp, or a series of steps which you can use. Sometimes there will be a trolley waiting there for you and you can wheel your canoe around the dam, and down to a similar dock on the far side of the dam. If there is no trolley, there might be one on the far side, so it could be worth a walk around to see.On some dams there are shoots that you can slide down in your canoe. Sometimes you will see a bell and rope on the bank near one of these banks or a lever to pull. By ringing the bell you alert the control tower who will open the gate for you. Once this has lowered and the green light shows you can plummet down the ramp. If, like us, you find you are not canoeing during tourist season there may be no one there to operate the ramp. For this you can either carry around or (be cheeky) and go down the slide anyway even though theres not much water on it. but be careful because you don't want to get stuck!The two locks in Serbia/Romania marked as Djerdap I and II are the only places you may have problems. The lock keepers do not want to deal with canoes and there is no sign posted set up here. We got shooed away from the lock on either side of the dam but eventually convinced the Romanian side to let us in with a cruise ship and go down in the lock. At the second we chatted and were slowed by countless members of authority but were eventually allowed to walk around and place our canoe back in the river on the other side. With these locks prepare for a long wait and a lot of talking, ultimately you have a right to be allowed passed.Are there other obstacles on the River Danube?River Traffic is around but as the river widens they pose less of a problem. From Germany to Hungary the traffic is far worse then further downstream and is certainly something to be aware of. There are cruise ships, tankers and some smaller speed boats that you can often hear from fairly far away. Generally try and keep to the right and always stay aware of whats going on around and behind you.We joined the Danube at Kelheim but we are informed that in the first 200km or so from the source the river passes many light rapids, which do have the potential to damage canoes and kayaks if paddlers are not careful. From Kelheim onwards, however, there are no rapidsWhat are the border formalities on the River Danube?Up until the Hungary/Serbia border you are travelling in the Shengan Zone and do not need to do anything when you cross borders. Leaving Hungary you need to be aware and look for the port customs at the nearest town to each border. Leaving Hungary there is a large building on the right hand bank with flags flying and here you 'check out' of the country. In the next town there are similar flags flying on the left side of the river where you can stop and 'check in' to Serbia. This procedure is the same for all borders. Before leaving a country you get stamped outand entering each country you get stamped in.As an international waterway you should not have to pay for your passage on the water at any borders. However, in Serbia we were charged a fee since we were deemed a leisure boat and had to pay a government charge. Still not sure whether we were conned on that one or not!We were aware of kayakers in the past who had not needed any paperwork but us, in our canoe, were told we required a crew list for each country. This was a form we were given at each customs office which we would fill out stating our names and information about the canoe (its length, width, make, e.t.c.). It is this form which we stamped in and out at each border.In short: at each border look for a customs office which will be on the riverside in the closest town. Stop and tell them what you are doing, showing your passports. As an international waterway they will let you pass.\n\n  How long  does it take to canoe the Danube?The average distance per day varies greatly and obviously depends a lot on how much you stop and what you want to see on route. That said, at least 60kms a day was pretty standard with the exception of the slow water for the 100kms upstream of the Djerdap dams in Serbia. Our longest day was just over 80kms. From Kelheim the Danube took us 45 days. The Tour International Danubehave an itinerary of 65 days – perhaps more realistic if you want to spend a good amount of time in towns and cities. Bare in mind, on our journey, as we had a lot of ground to cover, we generally did longer days than you might as a regular canoe tourer.Where did you store the canoe when stopping briefly?Take a long bike lock. When leaving the canoe we would always lock it to something using a long bike lock and took our paddles with us. As for leaving bags in the canoe, we learnt this lesson the hard way. Admittedly the city of Tour on the river Loire is no small village, but we had loads of our gear stolen from our dry bags, including tent and sleeping bags, when we visited the city for an afternoon. From then on whenever we stopped anywhere we locked the canoe and took both of our large dry bags with us. That meant the canoe was left alone with only food in it, easily replaced. Other than Tour we never encountered any problems however.Where did you store the canoe at night?Canoe club and rowing clubs are fairly frequent from Germany to Hungary and many offer good camping facilities. The TIDcamping locations, marked on their itinerary, can be useful for this. These are good secure places to stay and meet like minded people.All along the Danube you can find wild camping spots and the further down the river you go the easier this becomes. By Bulgaria and Romania you can camp almost anywhere. We did plenty of wild camping but be aware it is not legal in Germany or Austria so be particularly careful there. Here we simply slept with the canoe rolled over next to our tent. We could also store a few things under it at night time to keep them dry, sometimes using a bike lock to hold everything safely together.Where did you store the canoe in cities?Again, use a trusty bike lock and lock the canoe to something. As in the previous question canoe and rowing clubs are ideal places to store your canoe safely. The people there are like minded and keen to help you out. If you are not staying in a club or campsite our advice is NOT to lock the canoe and leave it by the river while you stay in a hotel. Talk to the hotel and ask them about storage, ask in the tourist info, but for safety in the city be sure to find somewhere. We often used the friendly people on couchsurfing.org or warmshowers.org - online communities set up by travellers to help travellers. If your in need of a friend to help you out abroad, these are the websites to use.What are the options for canoeing the Danube delta?Since we did not follow the delta route our answer is quoted from the following website: danubekayak.moonfruit.com (which appears to no longer be active):\"There are three. At Tulcea the channel splits into three main navigation routes, the Bystroe route in the north, Sulina route in the middle and Sfantul Gheorghe route in the south. In addition to this there are literally hundreds of sub channels crossing all through the delta, so you could pick out a route of your own based on one of the three main channels. Take care coming off the main routes as good navigation or sheer luck is needed – we had more of the second. Still, I would highly recommend getting into these narrow channels if you can, as it will surprise you what you find!From research and local advice we chose the southern channel to Sfantul Gheorghe route. It was a truly magic route, which I would highly recommend. The village of Sfantul Gheorghe is laid back, small…and perfect. There is a superb campsite in town or fancy lodge options if you feel you earned it! Mid-August there is a very alternative film festival in this village, with an excellent party to boot, so if you can time your arrival to coincide with this it will be a fitting end.On the other routes, the northern route is probably the least frequented. Ending in the town of Bystroe takes in some Ukrainian towns, so if you have a visa it could be very interesting place to travel. The central route to Sulina is the main cargo channel, so may be easiest to navigate and holds the true end point of the Danube in a lighthouse, so if you’re a purist, perhaps this is your one.\"What about canoeing the Danube-Black Sea Canal?The Black Sea Canal is a 45km shortcut that took us from the Danube to the Black Sea avoiding the delta. If you are not in a rush our advice would certainly be to take the river route to the sea, taking in the famously spectacular scenery. The canal, however, is accessible to canoes and kayaks with one lock at either end if you choose to take this route.You are not permitted to enter the locks and there is no set up for portaging, for this you just have to clamber up the banks and do it yourself.The canal is also called 'the canal of death' due to the great number of slave labourers and prisoners who died there in poor conditions. It is a grim and eerie place. It is a very unique experience but not an entirely pleasant one. Where it finishes in Agigea we were forbidden access near and beyond the lock since it is a large commercial port. To reach the sea is therefore a particularly long portage.Do I need a water purifier?We took chlorine tablets which we did use fairly often from Serbia onwards. However, you can certainly get by without, simply by having a decent amount of water storage and topping up whenever you get the chance. For safety it is always best to carry something, just in case.Have more questions about canoeing the Danube?Simply get in touch through our contact form.Following this advice?These FAQ pages are intended to answer your common questions but reflect out-dated knowledge from our experiences in 2013. Things on the Danube may be different today. James Warner Smith and Nathan Wilkins do not take responsibility for anybody that follows our advice without seeking further, professional guidance and cannot be held responsible for any loss, damage, injury or death that occurs as a result of following this information. Please be cautious, act responsibly and canoe safely.",
    "summary": {
      "en": "**Summary of Canoeing on the Danube**\n\nThe Danube is Europe’s second longest river, popular for canoeing, especially in the summer from Germany to Hungary. An annual event called Tour International Danube features a large group paddling the river, providing useful itineraries with daily distances and lock locations.\n\n**Maps and Navigation:**\n- The recommended maps for canoeing are BIKELINE maps for the Danube Cycle Path, which offer detailed information.\n- In the last 300km in Romania, you'll need an updated map due to many islands.\n\n**Permits and Regulations:**\n- No river permit is needed to canoe the Danube, but follow specific country rules (e.g., life jackets in Austria, boat identification in Germany).\n\n**Dams and Locks:**\n- There are over 100 dams, mostly in Germany and Austria, with clear signage.\n- Portaging around dams usually involves carrying or wheeling the canoe, with some ramps available.\n- Be aware of delays at locks, especially in Serbia/Romania.\n\n**River Conditions:**\n- Expect river traffic and light rapids in the first 200km from the source. After Kelheim, the river is calmer.\n\n**Border Crossings:**\n- Crossing borders (e.g., Hungary to Serbia) requires checking in and out at customs offices along the river.\n\n**Canoeing Duration:**\n- Average daily distance is about 60km, taking around 45 days from Kelheim, with longer days if you're in a hurry.\n\n**Canoe Storage:**\n- Use a bike lock to secure your canoe when stopping. Canoe clubs offer safe storage options.\n\n**Canoeing the Delta:**\n- The Danube Delta has three main navigation routes. The southern Sfantul Gheorghe route is recommended for its beauty.\n\n**Black Sea Canal:**\n- A 45km shortcut to the Black Sea exists but involves manual portaging at locks.\n\n**Water Purification:**\n- Chlorine tablets can be used for water purification, but it's also fine to refill your water supply when possible.\n\nFor more inquiries, you can reach out via their contact form. Note that this information may be outdated and should be verified for current conditions and regulations.",
      "ko": "다뉴브 강은 유럽에서 두 번째로 긴 강으로, 특히 여름철에 독일에서 헝가리까지 카약을 즐기기에 인기가 많습니다. 매년 열리는 '투어 인터내셔널 다뉴브'라는 행사에서는 많은 사람들이 함께 강을 노 젓고, 매일의 거리와 수문 위치에 대한 유용한 일정이 제공됩니다.\n\n카약을 위한 추천 지도는 다뉴브 자전거 도로를 위한 BIKELINE 지도입니다. 이 지도는 상세한 정보를 제공합니다. 루마니아의 마지막 300킬로미터 구간에서는 많은 섬이 있어 업데이트된 지도가 필요합니다.\n\n다뉴브 강에서 카약을 타기 위해 특별한 허가는 필요하지 않지만, 각 나라의 규정을 따라야 합니다. 예를 들어, 오스트리아에서는 구명조끼를 착용해야 하고, 독일에서는 보트 식별이 필요합니다.\n\n다뉴브에는 100개 이상의 댐이 있으며, 대부분 독일과 오스트리아에 위치해 있습니다. 댐 주변에는 명확한 표지판이 있습니다. 댐을 우회할 때는 카약을 들거나 바퀴로 이동해야 하며, 일부 경사로가 제공됩니다. 세르비아와 루마니아에서는 수문에서 지연이 발생할 수 있으니 주의해야 합니다.\n\n강의 첫 200킬로미터 구간에서는 강의 교통과 가벼운 급류를 예상할 수 있습니다. 켈하임 이후에는 강이 더 잔잔해집니다.\n\n국경을 넘을 때는 헝가리에서 세르비아로 이동하는 경우와 같이 강변의 세관 사무소에서 체크인과 체크아웃을 해야 합니다.\n\n하루 평균 이동 거리는 약 60킬로미터로, 켈하임에서 출발할 경우 약 45일이 소요됩니다. 급하게 이동할 경우 더 긴 거리를 하루에 이동할 수 있습니다.\n\n정차할 때는 자전거 자물쇠를 사용해 카약을 안전하게 잠가두는 것이 좋습니다. 카약 클럽에서는 안전한 보관 옵션을 제공합니다.\n\n다뉴브 삼각주에는 세 가지 주요 항해 경로가 있으며, 남쪽의 스판툴 게오르기에 경로가 아름다움으로 추천됩니다.\n\n흑해로 가는 45킬로미터의 지름길이 있지만, 수문에서 수동으로 카약을 옮겨야 합니다.\n\n물 정화에는 염소 정제약을 사용할 수 있지만, 가능할 때 물을 보충하는 것도 괜찮습니다.\n\n더 궁금한 점이 있으면 연락 양식을 통해 문의할 수 있습니다. 이 정보는 구식일 수 있으니 현재의 조건과 규정을 확인하는 것이 좋습니다.",
      "ja": "ドナウ川はヨーロッパで二番目に長い川で、特に夏にはドイツからハンガリーまでカヌーを楽しむ人々に人気があります。毎年開催される「ツアー・インターナショナル・ドナウ」では、多くの人々が川を漕ぎながら、日ごとの距離やロックの位置を示した便利な旅程を提供しています。\n\nカヌーに適した地図としては、ドナウサイクルパス用のBIKELINE地図が推奨されており、詳細な情報が掲載されています。ルーマニアの最後の300kmでは、多くの島があるため、最新の地図が必要です。\n\nドナウ川をカヌーで漕ぐために特別な許可は必要ありませんが、各国の規則に従うことが求められます。例えば、オーストリアではライフジャケットが必要で、ドイツではボートの識別が求められます。\n\nドナウ川には100以上のダムがあり、主にドイツとオーストリアに集中しています。ダムの周りをポータージングする際は、カヌーを持ち運ぶか、車輪を使って移動することが一般的で、一部にはスロープも用意されています。特にセルビアやルーマニアでは、ロックでの待ち時間に注意が必要です。\n\n川の状況としては、源流から最初の200kmでは川の交通や軽い急流が予想されますが、ケルハイム以降は川が穏やかになります。\n\n国境を越える際（例えばハンガリーからセルビアへ）は、川沿いの税関で出入国手続きを行う必要があります。\n\nカヌーの平均的な漕ぎ距離は約60kmで、ケルハイムからは約45日かかります。急いでいる場合は、より長い距離を漕ぐことも可能です。\n\nカヌーを停める際は、自転車用のロックを使ってカヌーを固定することが推奨されており、カヌークラブでは安全な保管オプションも提供されています。\n\nドナウデルタには主に三つの航路があり、美しさから南のサフントゥル・ゲオルゲ航路が推奨されています。\n\n黒海への45kmのショートカットもありますが、ロックでの手動ポータージングが必要です。\n\n水の浄化には塩素タブレットを使用できますが、可能な限り水を補充することも問題ありません。\n\n詳細な問い合わせは、連絡フォームを通じて行うことができます。この情報は古くなっている可能性があるため、最新の状況や規則を確認することをお勧めします。"
    }
  },
  {
    "id": "a53da00522e0e17a",
    "title": {
      "en": "The <select> element can now be customized with CSS",
      "ko": "CSS로 <select> 맞춤 설정!",
      "ja": "CSSで選択肢を自由自在に！"
    },
    "type": "story",
    "url": "https://developer.chrome.com/blog/a-customizable-select",
    "score": 482,
    "by": "tosh",
    "time": 1743412307,
    "content": "Chrome for Developers\n\n        Blog\n\n      The <select> element can now be customized with CSS\n\n      Stay organized with collections\n\n      Save and categorize content based on your preferences.\n\n.wd-authors {\n  --avatar-size: 65px;\n  display: flex;\n  gap: 2em;\n}\n\n.wd-author {\n  display: flex;\n  flex-wrap: wrap;\n  gap: 1em;\n  line-height: calc(var(--avatar-size) / 2);\n}\n\n.wd-author img {\n  border-radius: 50%;\n  height: var(--avatar-size, 65px);\n  width: var(--avatar-size, 65px);\n}\n\n.dcc-authors {\n  --avatar-size: 65px;\n  display: flex;\n  gap: 2em;\n}\n\n.dcc-author {\n  display: flex;\n  flex-wrap: wrap;\n  gap: 1em;\n  line-height: calc(var(--avatar-size) / 2);\n}\n\n.dcc-author img {\n  border-radius: 50%;\n  height: var(--avatar-size, 65px);\n  width: var(--avatar-size, 65px);\n}\n\n.dcc-author__links {\n  display: flex;\n}\n\n.dcc-author__links a {\n  margin-inline-end: 6px;\n}\n\n.dcc-author__links a:last-of-type {\n  margin-inline-end: 0;\n}\n\n            Adam Argyle\n\n                  X\n\n                  GitHub\n\n                  Glitch\n\n                  Homepage\n\n  Published: March 24, 2025\n\nFrom Chrome 135, web developers and designers can finally unite on an accessible, standardized and CSS styleable <select> element on the web. This has been many years in the making, many hours of engineering and collaborative specification work, and the result is an incredibly rich and powerful component that won't break in older browsers.\n\nHere's a video of customized selects using these new features:\n\nFeaturing demos by Una, Brecht, and Adam.\n\nIf you've been following along closely, you'll notice a few spec names and features have changed since Una's request for community feedback. Luckily, if you worked from that post and are interested in what's changed, Una's also got you covered.\n\nThere's also shiny new documentation on MDN for customizable select, packed with details.\n\nMeet appearance: base-select\n\nA new CSS property appearance: base-select that puts the <select> element into a new, configurable and styleable state to be commonly referred to as \"base\" styles:\n.custom-select {\n  &, &::picker(select) {\n    appearance: base-select;\n  }\n}\n\nUsing base-select unlocks a number of new features and behaviors:\n\nChanges the browsers HTML parser for the contents inside the <select>.\nChanges the rendered internals of the <select>.\nExposes new internal parts and states for the <select>.\nA new minimal look, optimized for customization.\nShown options are in the top-layer, like a popover.\nShown options positioned with anchor().\n\nUsing base-select loses a number of features and behaviors:\n\nThe <select> doesn't render outside the browser pane.\nIt doesn't trigger built-in mobile operating system components.\nThe <select> stops taking the width of the longest <option>.\n\nExpect more \"base\" appearances for other elements to develop over time!\n\nA <select> can now include rich HTML content\n\nBefore you could customize a <select>, if you put things like an image or SVG into the <option> element, the browser would ignore them.\n\nConsider the following HTML, the browser would read it as you authored it:\n<select class=\"custom-select\">\n  <option>\n    <svg aria-hidden>…</svg>\n    <span>HTML</span>\n  </option>\n  <option>\n    <svg aria-hidden>…</svg>\n    <span>CSS</span>\n  </option>\n  <option>\n    <svg aria-hidden>…</svg>\n    <span>JavaScript</span>\n  </option>\n  <option>\n    <svg aria-hidden>…</svg>\n    <span>WASM</span>\n  </option>\n</select>\n\nHowever the used DOM wouldn't include the <svg>:\n<select class=\"custom-select\">\n  <option>\n    <span>HTML</span>\n  </option>\n  <option>\n    <span>CSS</span>\n  </option>\n  <option>\n    <span>JavaScript</span>\n  </option>\n  <option>\n    <span>WASM</span>\n  </option>\n</select>\n\nHere's (from left to right) Chrome, Safari, and Firefox rendering the preceding HTML. If the browser supports appearance: base-select then the SVG will appear in the option, otherwise it won't.\n\n  Try it in this Codepen.\n\nThere's risk in breaking existing websites with customizable select, due to the parser changes. Chrome has the features behind a Finch experiment in case there is an emergency need to turn it off. If things go well, the experiment will end and the code will be shipped permanently into the source.\n\nCompletely customizable\n\nEvery part of a base-select can be swapped out, customized and animated. Here's a demo that uses every new feature to create recognizable and meaningful select experiences.\n\n  Try it in this Codepen.\n\nFind many more examples in the resources section at the end of this post.\n\nUnchanged JavaScript interfaces\n\nThere's no risks to your existing JavaScript interactions with a <select> element.\n\nHowever, if you do begin adding rich HTML into your <option> elements, you should test the selected values, as the browser does still parse and ignore images and SVG. The logic has changed though, for determining the selected content string, and depending on what you have in your options, you may need to make adjustments.\n\nIf you're using the value attribute on an <option> you have nothing to worry about.\n\nResources\n\nChrome is first to implement base-select, but every browser participated in the specifications, and there's more \"base\" elements yet to be completed. This is just a start.\n\nStay tuned as we'll be continuing to add guidance, examples and resources on customizing select elements. Until then, checkout the following links for more information.\n\nWeb Standards\n\nOpen UI-Select\nHTML spec pull request\nIntent to Ship\nMDN\n\nChrome\n\nRequest for Comments and awesome Una explainer\nRequest for Comments Results\nUse <hr> in a <select>\nUna explaining parts and pieces\nUna Codepen collection\n\nCommunity\n\nBrecht De Ruyte Codepen Collection\nCSS Tricks on native versus custom selects\nOpen Props UI-<select>\nCustom select with transition animations example\n\nSpecial thanks to all those who were involved in making this happen!",
    "summary": {
      "en": "### Summary of \"Chrome for Developers\" Blog Post\n\n- **Customizable `<select>` Element**: Starting with Chrome 135, developers can now style the `<select>` element using CSS, making it more accessible and visually appealing. This feature has taken years of development and collaboration.\n\n- **New CSS Property**: The property `appearance: base-select` allows for a configurable `<select>` that includes new features and a minimal design optimized for customization. However, it removes some traditional functionalities, such as rendering outside the browser pane.\n\n- **Rich HTML Content**: Developers can now include rich HTML, like images and SVGs, within `<option>` elements. This change allows for more complex designs, though it may affect existing websites due to parsing changes.\n\n- **JavaScript Compatibility**: Existing JavaScript interactions with `<select>` elements remain unaffected. However, adjustments may be needed when using rich HTML content in options.\n\n- **Future Developments**: More customizable \"base\" elements are expected to be introduced, and resources for further guidance and examples will continue to be added.\n\n- **Community Contributions**: Many developers contributed to this project, and a variety of resources and examples are provided for those interested in using these new features.",
      "ko": "크롬 135 버전부터 개발자들은 CSS를 사용해 `<select>` 요소를 스타일링할 수 있게 되었습니다. 이로 인해 더 접근성이 높고 시각적으로 매력적인 디자인이 가능해졌습니다. 이 기능은 수년간의 개발과 협업을 통해 이루어졌습니다.\n\n새로운 CSS 속성인 `appearance: base-select`는 설정 가능한 `<select>` 요소를 제공하며, 새로운 기능과 최소한의 디자인을 통해 사용자 맞춤화에 최적화되어 있습니다. 하지만 이 속성은 브라우저 창 밖에서 렌더링하는 등 일부 전통적인 기능을 제거합니다.\n\n개발자들은 이제 `<option>` 요소 내에 이미지나 SVG와 같은 풍부한 HTML 콘텐츠를 포함할 수 있습니다. 이 변화는 더 복잡한 디자인을 가능하게 하지만, 기존 웹사이트에는 파싱 변경으로 인해 영향을 미칠 수 있습니다.\n\n기존의 JavaScript와 `<select>` 요소 간의 상호작용은 영향을 받지 않지만, 옵션에 풍부한 HTML 콘텐츠를 사용할 경우 조정이 필요할 수 있습니다.\n\n앞으로 더 많은 사용자 맞춤형 \"기본\" 요소가 도입될 것으로 예상되며, 추가적인 안내와 예제를 위한 자원도 계속 제공될 예정입니다.\n\n많은 개발자들이 이 프로젝트에 기여했으며, 새로운 기능을 사용하고자 하는 이들을 위한 다양한 자원과 예제가 제공됩니다.",
      "ja": "Chrome 135から、開発者はCSSを使って`<select>`要素をスタイリングできるようになりました。これにより、よりアクセスしやすく、視覚的に魅力的なデザインが可能になります。この機能は、数年にわたる開発と協力の成果です。\n\n新しいCSSプロパティ`appearance: base-select`を使用すると、カスタマイズ可能な`<select>`が実現します。このプロパティは新しい機能を含み、カスタマイズに最適化されたシンプルなデザインを提供しますが、従来の機能の一部、例えばブラウザのペインの外での表示ができなくなります。\n\n開発者は、`<option>`要素内に画像やSVGなどのリッチHTMLを含めることができるようになりました。この変更により、より複雑なデザインが可能になりますが、パースの変更により既存のウェブサイトに影響を与える可能性があります。\n\n既存のJavaScriptとの互換性は保たれており、`<select>`要素とのインタラクションには影響がありません。ただし、オプション内でリッチHTMLコンテンツを使用する際には調整が必要になる場合があります。\n\n今後、さらにカスタマイズ可能な「ベース」要素が導入される予定で、追加のガイダンスや例を提供するリソースも引き続き増えていく見込みです。\n\nこのプロジェクトには多くの開発者が貢献しており、新機能を利用したい人のためにさまざまなリソースや例が提供されています。"
    }
  },
  {
    "id": "9f3b9b8fc5d6eba6",
    "title": {
      "en": "It’s not mold, it’s calcium lactate (2018)",
      "ko": "곰팡이가 아닌 칼슘 락테이트",
      "ja": "カルシウムの正体"
    },
    "type": "story",
    "url": "https://www.thephcheese.com/theres-white-stuff-growing-on-your-cheese-that-isnt-mold",
    "score": 387,
    "by": "ilikepi",
    "time": 1743432582,
    "content": "There’s White Stuff Growing on Your Cheese That Isn’t Mold\n\n\t\t\t\t Posted on November 6, 2018November 6, 2018\n\n\t\tTwo weeks ago, one of my best friends and ex-cheese comrades, Chelsea, brought our old mentor/boss-lady, the illustrious Kim Martin, into the shop. Neither of them had visited us before, and it was pretty exciting to show them around our little corner of the co-op.\nAs they were getting ready to leave, Chelsea pulled me aside and silently pointed at a quarter-wheel of aged Gouda on display in the back of the case, tapping the side of it to show me that it was all white.\n“It’s not mold,” I announced without skipping a beat. “It’s calcium lactate.”\nThis is something I actually have to write on the scale label when we wrap wedges of the cheese for sale, because people are inherently put off by a sheet of white on an otherwise butterscotch-orange cheese.\nAfter all, most people are familiar with white, wispy molds growing on the outside of cheese—either as the well-manicured coif of a bloomy-rind cheese or as errant growths on the cut face of half-eaten cheese hunks living in the refrigerator cheese drawer.\nBut there are other white things that can grow on your cheese, and they are actually desirable: crystals!\nYou know what I’m referring to if you have bitten into an aged Gouda, Cheddar, or Parmesan and felt that satisfying crunch. You also know it if you’ve sunk your teeth through the sticky orange exterior of a washed-rind cheese and felt a slight grittiness.\nPeople often come into the shop looking for cheeses that have “salt crystals” in them. As you will learn below, there are two “families” of crystals that form in cheese. Only one of those families has anything to do with salt—and those are not usually the ones people go hunting for in a cheese shop. While a cheese might taste salty and have crystals in it, that doesn’t mean the crunchy bits are salt, per se.\nThe crystals that people really want when they are asking for “salt crystals” are often referred to in the industry as “flavor crystals.” That’s because the sight of these crystals is a sign that you’ve found a flavorful, or fully-developed, cheese.\nIn fact, cheese crystals don’t have any effect on the way a cheese tastes—they are flavorless and scentless. But they do affect other sensory perceptions of a bite of cheese: sound (crunching), touch (bumpiness or rough texture), and sight (white spots, clusters, or patches).\nThere are several different types of crystals that grow in or on cheese at different times in the cheese-making or -aging process. They are either going to be the product of mineral (salt) emulsion during cheesemaking or protein breakdown (proteolysis) as the cheese ages.\n\nThe crystals you may not notice as much are the “inorganic crystals,” or crystals formed by minerals.[i] This “family” of crystals is created when salts emulsify, or disperse throughout the cheese without dissolving, during the cheese-making process.[ii] (These are the ones you could call salt crystals.)\nFor example, calcium phosphate crystals are most commonly found under the rinds of bloomy-rind cheeses, helping them become soft as they ripen.[iii] Two other kinds of inorganic crystals, Ikaite and Struvite, are what you notice when a washed-rind cheese has a gritty rind; Ikaite crystals are formed from calcium carbonate, whereas Struvite crystals come from magnesium ammonium phosphate.[iv]\nThe crystals that are most noticeable in cheese are the “organic crystals” that are formed by the breakdown of amino acids during the cheese-aging process.\nAs a cheese ages, it loses moisture and its protein structure contracts and stretches. As this happens, the amino acid chains running through the cheese that make up that protein structure start to break up; the whole process of protein breakdown is called proteolysis.\nEach type of organic crystal that you will find in a cheese is named after the amino-acid chain that broke up to create it.\nFor example, tyrosine crystals give aged goudas their famous crunchy texture. They can grow inside the paste of cheese, or all around the little holes inside a cheese.\nLeucine crystals have a similar effect, but have a more diffused, smear-like appearance than tyrosine crystals. Both of these types of crystals may be found in goudas, Alpine-style (Swiss) cheeses, and Grana-style cheeses (e.g., Parmigiano Reggiano, Grana Padano, Piave, etc.).\nAnd then there is calcium lactate, which frequently forms on the outside of rindless cheddars as they age. Calcium lactate formations are seen as a sign that the cheese has aged for a long time and should have a more developed flavor profile.\nCalcium lactate originates from an earlier stage of proteolysis, when lactose is still present in the liquid milk that will be fermented into cheese. As the bacterial culture in the cheese eats up all of the lactose, or milk sugar, in the milk, the bacteria create lactic acid. [v] Calcium lactate is a byproduct of that lactic acid interacting with calcium carbonate in the cheese over time.\n\nJust in case you were wondering what calcium lactate has to do with amino acids, lactic acid bacteria convert the proteins in cheese into peptides, and then into amino acids (like tyrosine and leucine).\nYou can find several types of crystals on the same cheese—tyrosine and leucine crystal deposits on aged Parmigiano Reggiano, for example.[vi]\nSo how you do know which is which?\nGenerally speaking, calcium lactate will be found on the outside of a cheese (usually a cheddar), and tyrosine or leucine crystals will be on the inside. Calcium lactate can also form on the inside of cheese, but tyrosine and leucine crystals cannot.\nTyrosine crystals will be hard and crunchy, whereas calcium lactate will be slightly softer, and sometimes almost powdery or flaky, in comparison to tyrosine or leucine crystals.\nCalcium phosphate, Ikaite, and Struvite crystals will be found on any “mold-ripened” cheese: you may notice them in the slight grittiness at the rind of a bloomy rind cheese, like Brie or Camembert, or a washed-rind cheese, like Epoisses, Chimay, or Grayson. (Washed-rind cheeses, also called “smear-ripened cheeses,” fall into the mold-ripened category because their rinds are created by a complex ecosystem of molds and yeasts.)\nWhat all of these crystals have in common—other than the texture they create, of course—is that they signify age in a given cheese. They help mold-ripened cheeses become soft, and they let you know when a hard cheese has been nicely aged.\nSo if you peel open a chunk of Cheddar and find white deposits marbling its outsides, rejoice! You’ve gotten a well-aged cheese that is bound to taste delicious.\nAnd if you crack open a wedge of Gouda, Gruyere, or Parmigiano Reggiano and see little white spots either riddling the paste or clustered around the cheese’s eye holes, also rejoice! You’ve got tyrosine or leucine crystals, and that cheese’s texture is going to be like cheese candy.\n(Cheese candy, good readers! Could you wish for anything better?)\nThe moral of the story? If you see white on your cheese, don’t just throw it away. Touch the white stuff to see if it’s hard or soft. If it’s soft, it’s probably mold (and you can just cut it off of a firm cheese). If it’s hard, it’s a precious little colony of crystals, and you have hit the cheese jackpot.\n_______________________________________________________________\n[i] Tansman, Gil Fils. “Crystal.” The Oxford Companion to Cheese. Ed. Catherine Donnelly. New York: Oxford University Press, 2016. 205-6.\n[ii] Polowsky, Pat. “The Wonderful World of Cheese Crystals.” Cheese Science Toolkit. https://www.cheesescience.org/assets/doc/crystal_handout.pdf. Accessed 4 Nov. 2018.\n[iii] Tansman, 205-6.\n[iv] Polowsky, “The Wonderful World of Cheese Crystals.”\n[v] Polowsky, Pat. “Lactose and Lactic Acid.” Cheese Science Toolkit. https://www.cheesescience.org/lactose.html. Accessed 4 Nov. 2018.\n[vi] Johnson, Mark. “Crystallization in Cheese.” Dairy Pipeline, vol. 26, no. 3, 2014. Wisconsin Center for Dairy Research. https://www.cdr.wisc.edu/sites/default/files/pipelines/2014/pipeline_2014_vol26_03.pdf. Accessed 4 Nov. 2018.\nShare this:Click to share on Facebook (Opens in new window)Click to share on Twitter (Opens in new window)Click to share on Tumblr (Opens in new window)MoreClick to share on Pocket (Opens in new window)Click to email a link to a friend (Opens in new window)Click to print (Opens in new window)\n\n\tRelated\n\n\t\t\tCategoriesCheese EducationTags,aged gouda calcium lactate calcium phosphate cheese cheese blog cheese blogger cheese crystals cheese defects cheese education cheese shop cheesemonger cheeses cheesy Courtney C Johnson courtney johnson phcheese crystals food blog food blogger foodie formaggio fromage hard white stuff on cheese kaas kase lactic acid bacteria lactose in cheese leucine phcheese phcheese blog pnw cheese queso seattle cheese seattle eats that cheese life the phcheese tyrosine white stuff on cheese",
    "summary": {
      "en": "**Summary:**\n\nIf you see white stuff on your cheese, it might not be mold—often, it's calcium lactate or crystals. Calcium lactate appears on aged cheeses like cheddar and indicates they are well-aged. Cheeses can also have \"flavor crystals\" formed from amino acid breakdown during aging, such as tyrosine and leucine, which create a crunchy texture.\n\nThere are two types of crystals in cheese: inorganic (like calcium phosphate) from minerals and organic (like tyrosine) from protein breakdown. Inorganic crystals might be present under the rinds of certain cheeses, while organic crystals are generally found inside and add a pleasant crunch.\n\nIf the white is hard and crunchy, it’s likely a good sign of a flavorful cheese. If it’s soft, it could be mold, which can be cut off. So, don’t throw away cheese with white spots; check it first!",
      "ko": "치즈에서 하얀 물질을 발견했다면, 그것이 곰팡이가 아닐 수도 있습니다. 종종 그것은 칼슘 락테이트나 결정일 수 있습니다. 칼슘 락테이트는 체다와 같은 숙성 치즈에서 나타나며, 이는 치즈가 잘 숙성되었음을 나타냅니다. 치즈는 또한 숙성 과정에서 아미노산이 분해되어 형성된 \"맛 결정\"을 가질 수 있습니다. 이러한 결정은 타이로신과 류신 같은 아미노산에서 나오며, 바삭한 식감을 제공합니다.\n\n치즈에는 두 가지 종류의 결정이 있습니다. 무기 결정은 칼슘 인산염과 같은 광물에서 나오고, 유기 결정은 단백질 분해에서 발생하는 타이로신과 같은 물질에서 나옵니다. 무기 결정은 특정 치즈의 껍질 아래에 있을 수 있으며, 유기 결정은 일반적으로 내부에 위치해 바삭한 식감을 더합니다.\n\n하얀 물질이 단단하고 바삭하다면, 이는 맛있는 치즈의 좋은 징후일 가능성이 높습니다. 만약 부드럽다면 곰팡이일 수 있으며, 이 경우 잘라낼 수 있습니다. 따라서 하얀 반점이 있는 치즈를 버리지 말고, 먼저 확인해 보세요!",
      "ja": "チーズに白いものが見える場合、それはカビではなく、カルシウム乳酸塩や結晶であることが多いです。カルシウム乳酸塩は、チェダーのような熟成されたチーズに現れ、良い熟成を示しています。また、チーズにはアミノ酸の分解から生じる「風味結晶」が含まれることもあり、チロシンやロイシンがその例です。これらはパリッとした食感を生み出します。\n\nチーズには二種類の結晶があります。一つは無機結晶で、カルシウムリン酸塩のような鉱物由来のものです。もう一つは有機結晶で、タンパク質の分解から生じるチロシンのようなものです。無機結晶は特定のチーズの外皮の下に存在することがあり、有機結晶は一般的に内部に見られ、心地よい食感を加えます。\n\n白い部分が硬くてパリッとしている場合、風味豊かなチーズの良い兆候です。一方、柔らかい場合はカビの可能性があり、切り取ることができます。ですので、白い斑点のあるチーズを捨てる前に、まずは確認してみてください。"
    }
  },
  {
    "id": "b65f0e1ccdb51fcf",
    "title": {
      "en": "JEP draft: Prepare to make final mean final",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://openjdk.org/jeps/8349536",
    "score": 198,
    "by": "mfiguiere",
    "time": 1743449751,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "59f729020f7f8a58",
    "title": {
      "en": "Launch HN: Augento (YC W25) – Fine-tune your agents with reinforcement learning",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": null,
    "score": 91,
    "by": "lmeierhoefer",
    "time": 1743442144,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "d5293afbe1a70a60",
    "title": {
      "en": "AI agents: Less capability, more reliability, please",
      "ko": "신뢰성 높은 AI 에이전트",
      "ja": "信頼性重視のAI"
    },
    "type": "story",
    "url": "https://www.sergey.fyi/articles/reliability-vs-capability",
    "score": 396,
    "by": "serjester",
    "time": 1743432335,
    "content": "AI Agents: Less Capability, More Reliability, PleaseMarch 30, 2025",
    "summary": {
      "en": "The article discusses the need for AI agents to prioritize reliability over advanced capabilities. It suggests that while having powerful features is important, ensuring that AI systems are dependable and consistent in their performance is crucial. This shift in focus aims to improve user trust and satisfaction with AI technology.",
      "ko": "이 기사는 AI 에이전트가 고급 기능보다 신뢰성을 우선시해야 한다고 강조합니다. 강력한 기능이 중요하긴 하지만, AI 시스템이 신뢰할 수 있고 일관된 성능을 유지하는 것이 더욱 중요하다는 것입니다. 이러한 초점의 변화는 사용자들이 AI 기술에 대한 신뢰와 만족도를 높이기 위한 것입니다.",
      "ja": "この記事では、AIエージェントが高度な機能よりも信頼性を優先する必要性について述べています。強力な機能を持つことも重要ですが、AIシステムが信頼でき、安定したパフォーマンスを発揮することが非常に重要です。この焦点の移行は、ユーザーの信頼と満足度を向上させることを目的としています。"
    }
  },
  {
    "id": "9d2cbf18cd4e0897",
    "title": {
      "en": "Notes on the Pentium's microcode circuitry",
      "ko": "펜티엄 마이크로코드 비밀",
      "ja": "ペンティアムのマイクロコード解剖"
    },
    "type": "story",
    "url": "https://www.righto.com/2025/03/pentium-microcde-rom-circuitry.html",
    "score": 165,
    "by": "leotravis10",
    "time": 1743446137,
    "content": "Ken Shirriff's blog\n\nComputer history, restoring vintage computers, IC reverse engineering, and whatever\n\nNotes on the Pentium's microcode circuitry\n\nMost people think of machine instructions as the fundamental steps that a computer performs.\nHowever, many processors have another layer of software underneath: microcode.\nWith microcode, instead of building the processor's control circuitry from complex logic gates, the control logic is\nimplemented with code known as microcode, stored in the microcode ROM.\nTo execute a machine instruction, the computer internally executes several simpler micro-instructions, specified by the microcode.\nIn this post, I examine the microcode ROM in the original Pentium, looking at the low-level circuitry.\nThe photo below shows the Pentium's thumbnail-sized silicon die under a microscope.\nI've labeled the main functional blocks.\nThe microcode ROM is highlighted at the right.\nIf you look closely, you can see that the microcode ROM consists of two rectangular banks, one above the other.\nThis die photo of the Pentium shows the location of the microcode ROM. Click this image (or any other) for a larger version.\nThe image below shows a closeup of the two microcode ROM banks.\nEach bank provides 45 bits of output; together they implement a micro-instruction that is 90 bits long.\nEach bank consists of a grid of transistors arranged into 288 rows and 720 columns.\nThe microcode ROM holds 4608 micro-instructions,\n414,720 bits in total.\nAt this magnification, the ROM appears featureless, but it is covered with horizontal wires, each just 1.5 µm\nthick.\nThe 90 output lines from the ROM, with a closeup of six lines exiting the ROM.\nThe ROM's 90 output lines are collected into a bundle of wires between the banks, as shown above.\nThe detail shows how six of the bits exit from the banks and join the bundle.\nThis bundle exits the ROM to the left, travels to various parts of the chip, and controls the chip's circuitry.\nThe output lines are in the chip's top metal layer (M3):\nthe Pentium has three layers of metal wiring with M1 on the bottom, M2 in the middle, and M3 on top.\nThe Pentium has a large number of bits in its micro-instruction, 90 bits compared to 21 bits in the 8086.\nPresumably, the Pentium has a \"horizontal\" microcode architecture, where the microcode bits correspond to low-level control signals,\nas opposed to \"vertical\" microcode, where the bits are encoded into denser micro-instructions.\nI don't have any information on the Pentium's encoding of microcode; unlike the 8086, the Pentium's patents don't provide any clues.\nThe 8086's microcode ROM holds 512 micro-instructions, much less than the Pentium's 4608 micro-instructions.\nThis makes sense, given the much greater complexity of the Pentium's instruction set, including the floating-point unit on the chip.\n\nThe image below shows a closeup of the Pentium's microcode ROM.\nFor this image, I removed the three layers of metal and the polysilicon layer\nto expose the chip's underlying silicon.\nThe pattern of silicon doping is visible, showing the transistors and thus the data stored in the ROM.\nIf you have enough time, you can extract the bits from the ROM by examining the silicon and seeing where transistors are present.\nA closeup of the ROM showing how bits are encoded in the layout of transistors.\nBefore explaining the ROM's circuitry, I'll review how an NMOS transistor is constructed.\nA transistor can be considered a switch between the source and drain, controlled by the gate.\nThe source and drain regions (green) consist of silicon doped with impurities to change its semiconductor properties, forming N+ silicon.\n(These regions are visible in the photo above.)\nThe gate consists of a layer of polysilicon (red), separated from the silicon by a very thin insulating oxide layer. Whenever polysilicon crosses active silicon, a transistor is formed.\nDiagram showing the structure of an NMOS transistor.\nBits are stored in the ROM through the pattern of transistors in the grid.\nThe presence or absence of a transistor stores a 0 or 1 bit.1\nThe closeup below shows eight bits of the microcode ROM. There are four transistors present and four gaps where transistors are\nmissing.\nThus, this part of the ROM holds four 0 bits and four 1 bits.\nFor the diagram below, I removed the three metal layers and the polysilicon to show the underlying silicon.\nI colored doped (active) silicon regions green, and drew in the horizontal polysilicon lines in red.\nAs explained above, a transistor is created if polysilicon crosses doped silicon.\nThus, the contents of the ROM are defined by the pattern of silicon regions, which creates the transistors.\nEight bits of the microcode ROM, with four transistors present.\nThe horizontal silicon lines are used as wiring to provide ground to the transistors, while the horizontal polysilicon lines select one of the\nrows in the ROM.\nThe transistors in that row will turn on, pulling the associated output lines low.\nThat is, the presence of a transistor in a row causes the output to be pulled low, while the absence of a transistor causes\nthe output line to remain high.\nA schematic corresponding to the eight bits above.\nThe diagram below shows the silicon, polysilicon, and bottom metal (M1) layers. I removed the metal from the left to reveal the silicon and polysilicon underneath, but the pattern of vertical metal lines continues there.\nAs shown earlier, the silicon pattern forms transistors. Each horizontal metal line has a connection\nto ground through a metal line (not shown).\nThe horizontal polysilicon lines select a row.\nWhen polysilicon lines cross doped silicon, the gate of a transistor is formed.\nTwo transistors may share the drain, as in the transistor pair on the left.\nDiagram showing the silicon, polysilicon, and M1 layers.\nThe vertical metal wires form the outputs. The circles are contacts between the metal wire and the silicon of a transistor.2\nShort metal jumpers connect the polysilicon lines to the metal layer above, which will be described next.\nThe image below shows the upper left corner of the ROM. The yellowish metal lines are the top metal layer (M3), while the\nreddish metal lines are the middle metal layer (M2).\nThe thick yellowish M3 lines distribute ground to the ROM. Underneath the horizontal M3 line, a horizontal M2 line also\ndistributes ground.\nThe grids of black dots are numerous contacts between the M3 line and the M2 line, providing a low-resistance connection.\nThe M2 line, in turn, connects to vertical M1 ground lines underneath—these wide vertical lines are faintly visible.\nThese M1 lines connect to the silicon, as shown earlier, providing ground to each transistor.\nThis illustrates the complexity of power distribution in the Pentium: the thick top metal (M3) is the primary distribution of\n+5 volts and ground through the chip, but power must be passed down through M2 and M1 to reach the transistors.\nThe upper left corner of the ROM.\nThe other important feature above is the horizontal metal lines, which help distribute the row-select signals.\nAs shown earlier, horizontal polysilicon lines provide the row-select signals to the transistors.\nHowever, polysilicon is not as good a conductor as metal, so long polysilicon lines have too much resistance.\nThe solution is to run metal lines in parallel, periodically connected to the underlying polysilicon lines and\nreducing the overall resistance.\nSince the vertical metal output lines are in the M1 layer, the horizontal row-select lines run in the M2 layer so they don't collide.\nShort \"jumpers\" in the M1 layer connect the M2 lines to the polysilicon lines.\nTo summarize, each ROM bank contains a grid of transistors and transistor vacancies to define the bits of the ROM.\nThe ROM is carefully designed so the different layers—silicon, polysilicon, M1, and M2—work together to maximize the\nROM's performance and density.\nMicrocode Address Register\nAs the Pentium executes an instruction, it provides the address of each micro-instruction to the microcode ROM.\nThe Pentium holds this address—the micro-address—in the Microcode Address Register (MAR).\nThe MAR is a 13-bit register located above the microcode ROM.\nThe diagram below shows the Microcode Address Register above the upper ROM bank.\nIt consists of 13 bits; each bit has multiple latches to hold the value as well as any pushed subroutine micro-addresses.\nBetween bits 7 and 8, some buffer circuitry amplifies the control signals that go to each bit's circuitry.\nAt the right, drivers amplify the outputs from the MAR, sending the signals to the row drivers and column-select circuitry that\nI will discuss below.\nTo the left of the MAR is a 32-bit register that is apparently unrelated to the microcode ROM, although I haven't determined its function.\nThe Microcode Address Register is located above the upper ROM bank.\nThe outputs from the Microcode Address Register select rows and columns in the microcode ROM, as I'll explain\nbelow.\nBits 12 through 7 of the MAR select a block of 8 rows, while bits 6 through 4 select a row in this block.\nBits 3 through 0 select one column out of each group of 16 columns to select an output bit.\nThus, the microcode address controls what word is provided by the ROM.\nSeveral different operations can be performed on the Microcode Address Register.\nWhen executing a machine instruction, the MAR must be loaded with the address of the corresponding\nmicrocode routine.\n(I haven't determined how this address is generated.)\nAs microcode is executed, the MAR is usually incremented to move to the next micro-instruction.\nHowever, the MAR can branch to a new micro-address as required.\nThe MAR also supports microcode subroutine calls; it will push the current micro-address and jump to the new micro-address.\nAt the end of the micro-subroutine, the micro-address is popped so execution returns to the previous location.\nThe MAR supports three levels of subroutine calls, as it contains three registers to hold the stack of pushed micro-addresses.\nThe MAR receives control signals and addresses from standard-cell logic\nlocated above the MAR.\nStrangely, in Intel's published floorplans for the Pentium, this standard-cell logic is\nlabeled as part of the branch prediction logic, which is above it.\nHowever, carefully tracing the signals from the standard-cell logic shows that is connected to the Microcode Address Register, not\nthe branch predictor.\nRow-select drivers\nAs explained above, each ROM bank has 288 rows of transistors, with polysilicon lines to select one of the rows.\nTo the right of the ROM is circuitry that activates one of these row-select lines, based on the micro-address.\nEach row matches a different 9-bit address. A straightforward implementation would use a 9-input AND gate for each\nrow, matching a particular pattern of 9 address bits or their complements.\nHowever, this implementation would require 576 very large AND gates, so it is impractical.\nInstead, the Pentium uses an optimized implementation with one 6-input AND gate for each group of 8 rows.\nThe remaining three address bits are decoded once at the top of the ROM.\nAs a result, each row only needs one gate, detecting if its group of eight rows is selected and if the particular one of eight\nis selected.\nSimplified schematic of the row driver circuitry.\nThe schematic above shows the circuitry for a group of eight rows, slightly simplified.3\nAt the top, three address bits are decoded, generating eight output lines with one active at a time.\nThe remaining six address bits are inverted, providing the bit and its complement to the decoding circuitry.\nThus, the 9 bits are converted into 20 signals that flow through the decoders, a large number of wires, but not unmanageable.\nEach group of eight rows has a 6-input AND gate that matches a particular 6-bit address, determined by which inputs are\ncomplemented and which are not.4\nThe NAND gate and inverter at the left combine the 3-bit decoding and the 6-bit decoding, activating the appropriate row.\nSince there are up to 720 transistors in each row, the row-select lines need to be driven with high current.\nThus, the row-select drivers use large transistors, roughly 25 times the size of a regular transistor.\nTo fit these transistors into the same vertical spacing as the rest of the decoding circuitry, a tricky packing is used.\nThe drivers for each group of 8 rows are packed into a 3×3 grid, except the first column has two drivers (since there\nare 8 drivers in the group, not 9).\nTo avoid a gap, the drivers in the first column are larger vertically and squashed horizontally.\nOutput circuitry\nThe schematic below shows the multiplexer circuit that selects one of 16 columns for a microcode output bit.\nThe first stage has four 4-to-1 multiplexers. Next, another 4-to-1 multiplexer selects one of the outputs.\nFinally, a BiCMOS driver amplifies the output for transmission to the rest of the processor.\nThe 16-to-1 multiplexer/output driver.\nIn more detail, the ROM and the first multiplexer are essentially NMOS circuits, rather than CMOS. Specifically, the ROM's\ngrid of transistors is constructed from NMOS transistors that can pull a column line low, but there are no PMOS transistors in\nthe grid to pull the line high (since that would double the size of the ROM).\nInstead, the multiplexer includes precharge transistors to pull the lines high, presumably in the clock phase before the\nROM is read.\nThe capacitance of the lines will keep the line high unless it is pulled low by a transistor in the grid.\nOne of the four transistors in the multiplexer is activated (by control signal a, b, c, or d) to select the desired line.\nThe output goes to a \"keeper\" circuit, which keeps the output high unless it is pulled low.\nThe keeper uses an inverter with a weak PMOS transistor that can only provide a small pull-up current.\nA stronger low input will overpower this transistor, switching the state of the keeper.\nThe output of this multiplexer, along with the outputs of three other multiplexers, goes to the second-stage multiplexer,5\nwhich selects one of its four inputs, based on control signals e, f, g, and h.\nThe output of this multiplexer is held in a latch built from two inverters. The second latch has weak transistors so the latch\ncan be easily forced into the desired state.\nThe output from the first latch goes through a CMOS switch into a second latch, creating a flip-flop.\nThe output from the second latch goes to a BiCMOS driver, which drives one of the 90 microcode output lines.\nMost processors are built from CMOS circuitry (i.e. NMOS and PMOS transistors), but the Pentium is built from BiCMOS circuitry:\nbipolar transistors as well as CMOS.\nAt the time, bipolar transistors improved performance for high-current drivers; see my\narticle on\nthe Pentium's BiCMOS circuitry.\nThe diagram below shows three bits of the microcode output. This circuitry is for the upper ROM bank; the circuitry is\nmirrored for the lower bank.\nThe circuitry matches the schematic above. Each of the three blocks has 16 input lines from the ROM grid.\nFour 4-to-1 multiplexers reduce this to 4 lines, and the second multiplexer selects a single line. The result is latched\nand amplified by the output driver.\n(Note the large square shape of the bipolar transistors.)\nNext is the shift register that processes the microcode ROM outputs for testing.\nThe shift register uses XOR logic for its feedback; unlike the rest of the circuitry, the XOR logic is irregular since\nonly some bits are fed into XOR gates.\nThree bits of output from the microcode, I removed the three metal layers to show the polysilicon and silicon.\nCircuitry for testing\nWhy does the microcode ROM have shift registers and XOR gates?\nThe reason is that a chip such as the Pentium is very difficult to test: if one out of 3.1 million transistors goes bad, how do you detect it? For a simple processor like the 8086, you can run through the instruction set and be fairly confident that any problem would turn up.\nBut with a complex chip, it is almost impossible to design an instruction sequence that would test every bit of the microcode ROM, every bit of the cache, and so forth.\nStarting with the 386, Intel added circuitry to the processor solely to make testing easier; about 2.7% of the transistors in the 386 were for testing.\nThe Pentium has this testing circuitry for many ROMs and PLAs, including the division PLA that caused the infamous FDIV bug.\nTo test a ROM inside the processor, Intel added circuitry to scan the entire ROM and checksum its contents.\nSpecifically, a pseudo-random number generator runs through each address, while another circuit computes a checksum of the ROM output, forming a \"signature\" word.\nAt the end, if the signature word has the right value, the ROM is almost certainly correct.\nBut if there is even a single bit error, the checksum will be wrong and the chip will be rejected.\nThe pseudo-random numbers and the checksum are both implemented with linear feedback shift registers (LFSR), a shift register along with a few XOR gates to feed the output back to the input.\nFor more information on testing circuitry in the 386, see Design and Test of the 80386, written by Pat Gelsinger, who became Intel's CEO years later.\nConclusions\nYou'd think that implementing a ROM would be straightforward, but the Pentium's microcode ROM is surprisingly complex due to\nits optimized structure and its circuitry for testing.\nI haven't been able to determine much about how the microcode works, except that the micro-instruction is 90 bits wide and\nthe ROM holds 4608 micro-instructions in total.\nBut hopefully you've found this look at the circuitry interesting.\nDisclaimer: this should all be viewed as slightly speculative and there are probably some errors.\nI didn't want to prefix every statement with \"I think that...\" but you should pretend it is there.\nI plan to write more about the implementation of the Pentium, so\nfollow me on Bluesky (@righto.com) or RSS for updates.\nPeter Bosch has done some reverse engineering of the Pentium II microcode; his information is here.\nFootnotes and references\n\nIt is arbitrary if a transistor corresponds to a 0 bit or a 1 bit. A transistor will pull the output line low (i.e. a 0 bit),\nbut the signal could be inverted before it is used.\nMore analysis of the circuitry or ROM contents would clear this up.↩\n\nWhen looking at a ROM like this, the contact pattern seems like it should tell you the contents of the ROM.\nUnfortunately, this doesn't work. Since a contact can be attached to one or two transistors, the contact\npattern doesn't give you enough information.\nYou need to see the silicon to determine the transistor pattern and thus the bits.↩\n\nI simplified the row driver schematic. The most interesting difference is that the NAND gates are optimized to use three\ntransistors each, instead of four transistors. The trick is that one of the NMOS transistors is essentially shared across\nthe group of 8 drivers; an inverter drives the low side of all eight gates.\nThe second simplification is that the 6-input AND gate is implemented with two 3-input NAND gates and a NOR gate for\nelectrical reasons.\nAlso, the decoder that converts 3 bits into 8 select lines is located between the banks, at the right, not at\nthe top of the ROM as I showed in the schematic.\nLikewise, the inverters for the 6 row-select bits are not at the top.\nInstead, there are 6 inverters and 6 buffers arranged in a column to the right of the ROM, which works better for the layout.\nThese are BiCMOS drivers so they can provide the high-current outputs necessary for the long wires and numerous\ntransistor gates that they must drive.↩\n\nThe inputs to the 6-input AND gate are arranged in a binary counting pattern, selecting each row in sequence.\nThis binary arrangment is standard for a ROM's decoder circuitry and is a good way to recognize a ROM on a die.\nThe Pentium has 36 row decoders, rather than the 64 that you'd expect from a 6-bit input. The ROM was made to the size\nnecessary, rather than a full power of two.\nIn most ROMs, it's difficult to determine if the ROM is addressed bottom-to-top or top-to-bottom.\nHowever, because the microcode ROM's counting pattern is truncated, one can see that the top bank starts with 0 at the top\nand counts downward, while the bottom bank is reversed, starting with 0 at the bottom and counting upward.↩\n\nA note to anyone trying to read the ROM contents: it appears that the order of entries in a group of 16 is inconsistent,\nso a straightforward attempt to visually read the ROM will end up with scrambled data.\nThat is, some of the groups are reversed. I don't see any obvious pattern in which groups are reversed.\nA closeup of the first stage output mux. This image shows the M1 metal layer.\nIn the diagram above, look at the contacts from the select lines, connecting the select lines to the mux transistors.\nThe contacts on the left are the mirror image of the contacts on the right, so the columns will be accessed in the opposite\norder.\nThis mirroring pattern isn't consistent, though; sometimes neighboring groups are mirrored and sometimes they aren't.\nI don't know why the circuitry has this layout. Sometimes mirroring adjacent groups makes the layout more efficient, but\nthe inconsistent mirroring argues against this. Maybe an automated layout system decided this was the best way.\nOr maybe Intel did this to provide a bit of obfuscation against reverse engineering.↩\n\nEmail ThisBlogThis!Share to XShare to FacebookShare to Pinterest\n\nLabels:\nintel,\nmicrocode,\nPentium,\nreverse-engineering\n\n1 comment:\n\n<img src=\"//blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEidhui65uLGfWs8Xj2rz9wRCVb0LWNazI4frey6bcRlS8V32KNycnQ0qTGBnvsfxP7yWYIBNOVOaZFECkj0FXu9GF-6k-DmEfGg--kPJ7QjBqjWNNFCZi89lpSI9dtU8g/s45-c/*\" width=\"35\" height=\"35\" class=\"photo\" alt=\"\">\nKen Boak\nsaid...\n\nKen, I know that you like a challenge. Are you familiar with the Ferranti F100-L Cpu from 1977?https://revaldinho.github.io/f100l/doc/F100CPU.html\n\nApril 1, 2025 at 6:19 AM\n\nPost a Comment\n\nOlder Post\n\nHome\n\n      @import url('https://fonts.googleapis.com/css?family=Montserrat:300,400,500,700');\n      .form-preview {\n      display: flex;\n      flex-direction: column;\n      justify-content: center;\n      margin-top: 30px;\n      padding: clamp(17px, 5%, 40px) clamp(17px, 7%, 50px);\n      max-width: 350px;\n      min-height: 200px;\n      border-radius: 6px;\n      box-shadow: 0 5px 25px rgba(34, 60, 47, 0.25);\n      }\n      .form-preview,\n      .form-preview *{\n        box-sizing: border-box;\n      }\n      .form-preview .preview-heading {\n      width: 100%;\n      }\n      .form-preview .preview-heading h5{\n        margin-top: 0;\n        margin-bottom: 0;\n      }\n      .form-preview .preview-input-field {\n      margin-top: 20px;\n      width: 100%;\n      }\n      .form-preview .preview-input-field input {\n      width: 100%;\n      height: 40px;\n      border-radius: 6px;\n      border: 2px solid #e9e8e8;\n      background-color: #fff;\n      outline: none;\n      }\n      .form-preview .preview-input-field input::placeholder, .form-preview .preview-input-field input {\n      opacity: 0.5;\n      color: #000;\n      font-family: \"Montserrat\";\n      font-size: 14px;\n      font-weight: 500;\n      line-height: 20px;\n      text-align: center;\n      }\n      .form-preview .preview-submit-button {\n      margin-top: 10px;\n      width: 100%;\n      }\n      .form-preview .preview-submit-button button {\n      width: 100%;\n      height: 40px;\n      border: 0;\n      border-radius: 6px;\n      line-height: 0px;\n      }\n      .form-preview .preview-submit-button button:hover {\n      cursor: pointer;\n      }\n\n      Get new posts by email:  Subscribe\n\nAbout the site\n\nContact info and site index\n\nPopular Posts\n\nA USB interface to the \"Mother of All Demos\" keyset\n\nThe Pentium contains a complicated circuit to multiply by three\n\nNotes on the Pentium's microcode circuitry\n\nInside a vintage aerospace navigation computer of uncertain purpose\n\nA Multi-Protocol Infrared Remote Library for the Arduino\n\nApple iPhone charger teardown: quality in a tiny expensive package\n\nA dozen USB chargers in the lab: Apple is very good, but not quite the best\n\nMining Bitcoin with pencil and paper: 0.67 hashes per day\n\nSearch This Blog\n\nLabels\n\n386\n\n6502\n\n8008\n\n8085\n\n8086\n\n8087\n\n8088\n\naerospace\n\nalto\n\nanalog\n\nApollo\n\napple\n\narc\n\narduino\n\narm\n\nbeaglebone\n\nbitcoin\n\nc#\n\ncadc\n\ncalculator\n\nchips\n\ncss\n\ndatapoint\n\ndx7\n\nelectronics\n\nf#\n\nfairchild\n\nfpga\n\nfractals\n\ngenome\n\nglobus\n\nhaskell\n\nHP\n\nhtml5\n\nibm\n\nibm1401\n\nibm360\n\nintel\n\nipv6\n\nir\n\njava\n\njavascript\n\nmath\n\nmicrocode\n\noscilloscope\n\nPentium\n\nphoto\n\npower supply\n\nrandom\n\nreverse-engineering\n\nsheevaplug\n\nsnark\n\nspace\n\nspanish\n\nsynth\n\nteardown\n\ntheory\n\nunicode\n\nZ-80\n\nBlog Archive\n\n        ▼\n\n2025\n\n(8)\n\n        ▼\n\nMarch\n\n(3)\n\nNotes on the Pentium's microcode circuitry\nA USB interface to the \"Mother of All Demos\" keyset\nThe Pentium contains a complicated circuit to mult...\n\n        ►\n\nFebruary\n\n(1)\n\n        ►\n\nJanuary\n\n(4)\n\n        ►\n\n2024\n\n(21)\n\n        ►\n\nDecember\n\n(1)\n\n        ►\n\nNovember\n\n(1)\n\n        ►\n\nOctober\n\n(1)\n\n        ►\n\nSeptember\n\n(3)\n\n        ►\n\nAugust\n\n(2)\n\n        ►\n\nJuly\n\n(2)\n\n        ►\n\nJune\n\n(1)\n\n        ►\n\nMay\n\n(1)\n\n        ►\n\nApril\n\n(1)\n\n        ►\n\nMarch\n\n(2)\n\n        ►\n\nFebruary\n\n(3)\n\n        ►\n\nJanuary\n\n(3)\n\n        ►\n\n2023\n\n(35)\n\n        ►\n\nDecember\n\n(4)\n\n        ►\n\nNovember\n\n(2)\n\n        ►\n\nOctober\n\n(3)\n\n        ►\n\nSeptember\n\n(1)\n\n        ►\n\nAugust\n\n(2)\n\n        ►\n\nJuly\n\n(3)\n\n        ►\n\nMay\n\n(1)\n\n        ►\n\nApril\n\n(2)\n\n        ►\n\nMarch\n\n(4)\n\n        ►\n\nFebruary\n\n(5)\n\n        ►\n\nJanuary\n\n(8)\n\n        ►\n\n2022\n\n(18)\n\n        ►\n\nNovember\n\n(3)\n\n        ►\n\nAugust\n\n(1)\n\n        ►\n\nJuly\n\n(1)\n\n        ►\n\nJune\n\n(1)\n\n        ►\n\nMay\n\n(1)\n\n        ►\n\nApril\n\n(4)\n\n        ►\n\nMarch\n\n(2)\n\n        ►\n\nFebruary\n\n(3)\n\n        ►\n\nJanuary\n\n(2)\n\n        ►\n\n2021\n\n(26)\n\n        ►\n\nDecember\n\n(4)\n\n        ►\n\nNovember\n\n(2)\n\n        ►\n\nSeptember\n\n(1)\n\n        ►\n\nAugust\n\n(1)\n\n        ►\n\nJuly\n\n(2)\n\n        ►\n\nJune\n\n(2)\n\n        ►\n\nMay\n\n(1)\n\n        ►\n\nApril\n\n(2)\n\n        ►\n\nMarch\n\n(4)\n\n        ►\n\nFebruary\n\n(4)\n\n        ►\n\nJanuary\n\n(3)\n\n        ►\n\n2020\n\n(33)\n\n        ►\n\nDecember\n\n(2)\n\n        ►\n\nNovember\n\n(3)\n\n        ►\n\nOctober\n\n(2)\n\n        ►\n\nSeptember\n\n(4)\n\n        ►\n\nAugust\n\n(5)\n\n        ►\n\nJuly\n\n(2)\n\n        ►\n\nJune\n\n(3)\n\n        ►\n\nMay\n\n(4)\n\n        ►\n\nApril\n\n(2)\n\n        ►\n\nMarch\n\n(5)\n\n        ►\n\nJanuary\n\n(1)\n\n        ►\n\n2019\n\n(18)\n\n        ►\n\nNovember\n\n(3)\n\n        ►\n\nOctober\n\n(2)\n\n        ►\n\nSeptember\n\n(3)\n\n        ►\n\nAugust\n\n(1)\n\n        ►\n\nJuly\n\n(4)\n\n        ►\n\nApril\n\n(2)\n\n        ►\n\nFebruary\n\n(1)\n\n        ►\n\nJanuary\n\n(2)\n\n        ►\n\n2018\n\n(17)\n\n        ►\n\nDecember\n\n(1)\n\n        ►\n\nSeptember\n\n(4)\n\n        ►\n\nAugust\n\n(1)\n\n        ►\n\nJune\n\n(1)\n\n        ►\n\nMay\n\n(1)\n\n        ►\n\nApril\n\n(1)\n\n        ►\n\nMarch\n\n(3)\n\n        ►\n\nFebruary\n\n(1)\n\n        ►\n\nJanuary\n\n(4)\n\n        ►\n\n2017\n\n(21)\n\n        ►\n\nDecember\n\n(5)\n\n        ►\n\nNovember\n\n(2)\n\n        ►\n\nOctober\n\n(3)\n\n        ►\n\nAugust\n\n(1)\n\n        ►\n\nJuly\n\n(2)\n\n        ►\n\nJune\n\n(2)\n\n        ►\n\nApril\n\n(2)\n\n        ►\n\nMarch\n\n(2)\n\n        ►\n\nFebruary\n\n(1)\n\n        ►\n\nJanuary\n\n(1)\n\n        ►\n\n2016\n\n(34)\n\n        ►\n\nDecember\n\n(2)\n\n        ►\n\nOctober\n\n(5)\n\n        ►\n\nSeptember\n\n(8)\n\n        ►\n\nAugust\n\n(2)\n\n        ►\n\nJuly\n\n(3)\n\n        ►\n\nJune\n\n(4)\n\n        ►\n\nMay\n\n(1)\n\n        ►\n\nApril\n\n(1)\n\n        ►\n\nMarch\n\n(1)\n\n        ►\n\nFebruary\n\n(4)\n\n        ►\n\nJanuary\n\n(3)\n\n        ►\n\n2015\n\n(12)\n\n        ►\n\nDecember\n\n(2)\n\n        ►\n\nNovember\n\n(1)\n\n        ►\n\nOctober\n\n(3)\n\n        ►\n\nAugust\n\n(1)\n\n        ►\n\nMay\n\n(2)\n\n        ►\n\nMarch\n\n(2)\n\n        ►\n\nFebruary\n\n(1)\n\n        ►\n\n2014\n\n(13)\n\n        ►\n\nDecember\n\n(1)\n\n        ►\n\nOctober\n\n(1)\n\n        ►\n\nSeptember\n\n(3)\n\n        ►\n\nMay\n\n(2)\n\n        ►\n\nMarch\n\n(1)\n\n        ►\n\nFebruary\n\n(5)\n\n        ►\n\n2013\n\n(24)\n\n        ►\n\nNovember\n\n(2)\n\n        ►\n\nSeptember\n\n(4)\n\n        ►\n\nAugust\n\n(4)\n\n        ►\n\nJuly\n\n(4)\n\n        ►\n\nJune\n\n(2)\n\n        ►\n\nApril\n\n(1)\n\n        ►\n\nMarch\n\n(2)\n\n        ►\n\nFebruary\n\n(2)\n\n        ►\n\nJanuary\n\n(3)\n\n        ►\n\n2012\n\n(10)\n\n        ►\n\nDecember\n\n(1)\n\n        ►\n\nNovember\n\n(5)\n\n        ►\n\nOctober\n\n(1)\n\n        ►\n\nMay\n\n(1)\n\n        ►\n\nMarch\n\n(1)\n\n        ►\n\nFebruary\n\n(1)\n\n        ►\n\n2011\n\n(11)\n\n        ►\n\nDecember\n\n(2)\n\n        ►\n\nJuly\n\n(2)\n\n        ►\n\nMay\n\n(2)\n\n        ►\n\nApril\n\n(1)\n\n        ►\n\nMarch\n\n(1)\n\n        ►\n\nFebruary\n\n(3)\n\n        ►\n\n2010\n\n(22)\n\n        ►\n\nDecember\n\n(2)\n\n        ►\n\nNovember\n\n(4)\n\n        ►\n\nOctober\n\n(3)\n\n        ►\n\nAugust\n\n(1)\n\n        ►\n\nJune\n\n(1)\n\n        ►\n\nMay\n\n(2)\n\n        ►\n\nApril\n\n(3)\n\n        ►\n\nMarch\n\n(4)\n\n        ►\n\nJanuary\n\n(2)\n\n        ►\n\n2009\n\n(22)\n\n        ►\n\nDecember\n\n(2)\n\n        ►\n\nNovember\n\n(5)\n\n        ►\n\nSeptember\n\n(1)\n\n        ►\n\nAugust\n\n(3)\n\n        ►\n\nJuly\n\n(1)\n\n        ►\n\nJune\n\n(3)\n\n        ►\n\nApril\n\n(1)\n\n        ►\n\nMarch\n\n(3)\n\n        ►\n\nFebruary\n\n(2)\n\n        ►\n\nJanuary\n\n(1)\n\n        ►\n\n2008\n\n(27)\n\n        ►\n\nJuly\n\n(3)\n\n        ►\n\nJune\n\n(1)\n\n        ►\n\nMay\n\n(3)\n\n        ►\n\nApril\n\n(4)\n\n        ►\n\nMarch\n\n(10)\n\n        ►\n\nFebruary\n\n(6)\n\nPowered by Blogger.",
    "summary": {
      "en": "Ken Shirriff's blog discusses the microcode circuitry of the original Pentium processor. Microcode is a layer of software that helps control how a processor executes machine instructions. Instead of using complex logic gates, the Pentium uses microcode stored in a dedicated ROM to translate machine instructions into simpler micro-instructions.\n\nKey highlights include:\n\n- The Pentium's microcode ROM consists of two banks, providing a total of 90-bit micro-instructions and holding 4608 micro-instructions in total.\n- The ROM features a complex layout of transistors that determine the stored bits, with transistors representing binary values (0 or 1).\n- The Microcode Address Register (MAR) holds the address of the current micro-instruction and can perform various operations, like branching to new instructions or handling subroutine calls.\n- To select the appropriate microcode output, the processor uses row-select drivers and multiplexers, which manage the large number of transistors and outputs efficiently.\n- The Pentium includes testing circuitry to ensure the chip functions correctly, utilizing pseudo-random number generators and checksums to verify the integrity of the ROM.\n\nOverall, the Pentium's microcode ROM is a complex but essential part of its processing architecture, showcasing advancements in microprocessor design.",
      "ko": "켄 시리프의 블로그에서는 원래 펜티엄 프로세서의 마이크로코드 회로에 대해 다루고 있습니다. 마이크로코드는 프로세서가 기계 명령을 실행하는 방식을 제어하는 소프트웨어의 한 층입니다. 펜티엄은 복잡한 논리 게이트 대신 전용 ROM에 저장된 마이크로코드를 사용하여 기계 명령을 더 간단한 마이크로 명령으로 변환합니다.\n\n주요 내용으로는 펜티엄의 마이크로코드 ROM이 두 개의 뱅크로 구성되어 있으며, 총 90비트의 마이크로 명령을 제공하고, 총 4608개의 마이크로 명령을 저장할 수 있다는 점이 있습니다. 이 ROM은 저장된 비트를 결정하는 복잡한 트랜지스터 배열을 가지고 있으며, 트랜지스터는 이진 값(0 또는 1)을 나타냅니다. 마이크로코드 주소 레지스터(MAR)는 현재 마이크로 명령의 주소를 보유하고 있으며, 새로운 명령으로 분기하거나 서브루틴 호출을 처리하는 등 다양한 작업을 수행할 수 있습니다.\n\n적절한 마이크로코드 출력을 선택하기 위해 프로세서는 행 선택 드라이버와 멀티플렉서를 사용하여 많은 수의 트랜지스터와 출력을 효율적으로 관리합니다. 또한 펜티엄은 칩이 올바르게 작동하는지 확인하기 위한 테스트 회로를 포함하고 있으며, 의사 난수 생성기와 체크섬을 활용하여 ROM의 무결성을 검증합니다.\n\n전반적으로 펜티엄의 마이크로코드 ROM은 복잡하지만 프로세서 아키텍처에서 필수적인 부분으로, 마이크로프로세서 설계의 발전을 보여줍니다.",
      "ja": "ケン・シリフのブログでは、初代ペンティウムプロセッサのマイクロコード回路について説明しています。マイクロコードは、プロセッサが機械命令を実行する方法を制御するためのソフトウェアの層です。ペンティウムは複雑な論理ゲートを使用する代わりに、専用のROMに保存されたマイクロコードを使って、機械命令をより単純なマイクロ命令に変換します。\n\n主なポイントは以下の通りです。ペンティウムのマイクロコードROMは二つのバンクから構成されており、合計90ビットのマイクロ命令を提供し、4608のマイクロ命令を保持しています。ROMは、保存されたビットを決定するトランジスタの複雑な配置を持ち、トランジスタは二進数の値（0または1）を表します。マイクロコードアドレスレジスタ（MAR）は、現在のマイクロ命令のアドレスを保持し、新しい命令への分岐やサブルーチン呼び出しなど、さまざまな操作を行うことができます。\n\n適切なマイクロコード出力を選択するために、プロセッサは行選択ドライバと多重化器を使用し、多数のトランジスタと出力を効率的に管理します。また、ペンティウムにはチップが正しく機能することを確認するためのテスト回路が含まれており、擬似乱数生成器やチェックサムを利用してROMの整合性を検証します。\n\n全体として、ペンティウムのマイクロコードROMは、その処理アーキテクチャの複雑でありながら重要な部分であり、マイクロプロセッサ設計の進歩を示しています。"
    }
  },
  {
    "id": "7b7e0ae27aef7fdf",
    "title": {
      "en": "The case against conversational interfaces",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://julian.digital/2025/03/27/the-case-against-conversational-interfaces/",
    "score": 248,
    "by": "nnx",
    "time": 1743473687,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "14ff35a71805b6d0",
    "title": {
      "en": "Ask HN: Why hasn't AMD made a viable CUDA alternative?",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": null,
    "score": 77,
    "by": "spacebanana7",
    "time": 1743518274,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "15328fedaa39ac01",
    "title": {
      "en": "Turso SQLite Offline Sync Public Beta",
      "ko": "터소 SQLite 오프라인 동기화 베타",
      "ja": "Turso SQLiteオフライン同期ベータ"
    },
    "type": "story",
    "url": "https://turso.tech/blog/turso-offline-sync-public-beta",
    "score": 210,
    "by": "charlieirish",
    "time": 1743433839,
    "content": "We're excited to announce that Turso Offline Sync is now available in public beta!\nYour applications can continue functioning seamlessly, even when disconnected from the internet. Local database operations can proceed normally, with automatic sync occurring once connectivity is restored.\nHistorically, SQLite has been a database that excels at running local, embedded databases, because the database is just a file. For mobile devices, this means on-device databases.\nTurso takes advantage of this with Embedded Replicas — your local embedded databases, on-device or server can now be kept in sync with your Turso Cloud database, and any changes are propagated to all replicas.\nUntil today, our sync was unidirectional: while you can always read from the database, even if offline, writes happen directly to the Cloud, and are propagated later.\nThis has two consequences:\n\nWrites were always network-slow, since they have to contact the server for every request\nWrites could not work offline\n\nWith today's announcement, the local database will be able to accept writes that are as fast as a file, work offline, and later sync to Turso Cloud.\n#Use Cases That Just Got Easier\nOne of the things Offline Sync unlocks is the ability to create on-device local-first applications (in-browser is planned for the future, with \"the Limbo Project\"). Local-first architectures allow for fast and responsive applications that are resilient to network failures.\nCompared to other local-first applications, Turso's architecture allows for a simpler solution because it always syncs the full database. With Turso's multitenant architecture, you can control which data goes into which database (e.g., per user or per tenant), and then transfer the entire database to the device.\nAs well as on-device local-first applications, Offline Sync also simplifies many other use cases:\n\nMobile Apps — create truly offline-capable mobile experiences, including Expo-based React Native applications.\nPoint-of-Sale Systems — process transactions regardless of internet connectivity\nField Data Collection — gather data in remote locations without worrying about connectivity\nIoT Applications — Maintain local data storage with periodic cloud sync with Turso Cloud\n\n#What's available in the public beta?\nThe following features are part of the public beta:\n\nBi-directional sync (push local changes to remote and pull remote changes)\nRemote write support for embedded replicas\nWAL sync checkpointing\nConflict detection (but resolution is not yet implemented)\n\n#Getting Started\nYou are now invited to try Turso Offline Sync. The beta currently includes support for TypeScript, and Rust.\nMake sure to create a new database in your preferred AWS location:\n# Create a group in your preferred AWS location\nturso group create --location aws-us-east-1 offline\n\n# Create your offline-capable database\nturso db create --group offline offline\n\n#TypeScript\nMake sure to install the latest @libsql/client package:\nnpm install @libsql/client\n\nThen inside your project, make sure to set offline: true to enable offline mode:\nimport { createClient } from '@libsql/client';\n\nconst client = createClient({\n  url: 'file:local.db',\n  syncUrl: process.env.TURSO_SYNC_URL,\n  authToken: process.env.TURSO_AUTH_TOKEN,\n});\n\n// Function to save a note that works offline\nasync function saveNote(content) {\n  await client.execute(`\n    CREATE TABLE IF NOT EXISTS notes (\n      id INTEGER PRIMARY KEY,\n      content TEXT,\n      created_at TEXT\n    )\n  `);\n\n  // Add the note - works even without internet\n  await client.execute({\n    sql: `INSERT INTO notes (content, created_at) VALUES (?, datetime('now'))`,\n    args: [content],\n  });\n\n  // Try to sync changes with the remote database\n  try {\n    await client.sync();\n    console.log('Note synced to cloud');\n  } catch (error) {\n    console.log('Note saved locally, will sync later');\n  }\n}\n\n// Read notes function that works offline\nasync function getNotes() {\n  const result = await client.execute(\n    'SELECT * FROM notes ORDER BY created_at DESC',\n  );\n  return result.rows;\n}\n\n#Rust\nMake sure to install the latest libsql crate using Cargo:\ncargo add libsql\n\nHere's how you can use it in your Rust applications\nuse libsql::{Builder, Database, ResultSet, params};\nuse std::env;\n\nasync fn setup_db() -> Result<Database, Box<dyn std::error::Error>> {\n    let db_path = env::var(\"LOCAL_DB_PATH\").unwrap_or(\"local.db\".to_string());\n    let sync_url = env::var(\"TURSO_SYNC_URL\").expect(\"TURSO_SYNC_URL must be set\");\n    let auth_token = env::var(\"TURSO_AUTH_TOKEN\").expect(\"TURSO_AUTH_TOKEN must be set\");\n\n    // Create a synced database with offline capabilities\n    let db = Builder::new_synced_database(db_path, sync_url, auth_token)\n        .build()\n        .await?;\n\n    let conn = db.connect()?;\n    conn.execute(\n        \"CREATE TABLE IF NOT EXISTS field_data (\n            id INTEGER PRIMARY KEY,\n            location TEXT,\n            reading REAL,\n            notes TEXT,\n            timestamp TEXT\n        )\",\n        ()\n    ).await?;\n\n    Ok(db)\n}\n\nasync fn record_data(db: &Database, location: &str, reading: f64, notes: &str) -> Result<(), Box<dyn std::error::Error>> {\n    let conn = db.connect()?;\n\n    // Insert data locally - works offline\n    conn.execute(\n        \"INSERT INTO field_data (location, reading, notes, timestamp)\n         VALUES (?, ?, ?, datetime('now'))\",\n        params![location, reading, notes]\n    ).await?;\n\n    println!(\"Data recorded locally\");\n\n    // Try to sync with server if possible\n    match db.sync().await {\n        Ok(_) => println!(\"Data synchronized with central database\"),\n        Err(e) => println!(\"Working offline - data will sync later: {}\", e),\n    }\n\n    Ok(())\n}\n\n#Mobile Apps with Expo\nFor those building local-first offline apps, Expo works great with Turso. You can get started with expo-sqlite:\nbunx create-expo-app -e with-libsql my-libsql-app\n\nimport { SQLiteProvider, useSQLiteContext } from 'expo-sqlite';\n\nexport default function App() {\n  return (\n    <SQLiteProvider\n      databaseName=\"local.db\"\n      onInit={\n        // Run some optional migration\n      }\n      options={{\n        libSQLOptions: {\n          url: '...',\n          authToken: '...',\n        },\n      }}\n    >\n      <Main />\n    </SQLiteProvider>\n  );\n}\n\nfunction Main() {\n  const db = useSQLiteContext();\n\n  return <View>{/* Your offline-first Expo app */}</View>;\n}\n\n#What's next?\nWith your support, we're working hard on the following features:\n\nAutomatic and manual conflict resolution\nSync protocol bandwidth optimization\nEncryption at rest\n\nYou can follow the progress over on GitHub.\n#Thank you\nWe want to extend our heartfelt gratitude to the Turso community members who participated in the private beta. Your feedback, bug reports, and feature suggestions have been invaluable in shaping this release.\nTurso Offline Sync is currently in beta quality, and not yet recommended for production use. During the beta period, there are no durability guarantees, which means data loss is possible.\nMake sure to join us on Discord to stay updated on what's new with the beta, and to provide feedback.",
    "summary": {
      "en": "Turso has launched the public beta of its Offline Sync feature, allowing applications to function without internet connectivity. This means local database operations can continue, and changes will automatically sync to the Turso Cloud once the connection is restored. \n\nKey features include:\n- **Bi-directional sync**: Local changes can be pushed to the cloud and vice versa.\n- **Remote write support**: Embedded databases can now accept local writes.\n- **Conflict detection**: The system can identify conflicts, though resolution is not yet implemented.\n\nThis feature is beneficial for various applications, including mobile apps, point-of-sale systems, field data collection, and IoT applications. It enables developers to create responsive, offline-capable applications easily.\n\nTo get started with Turso Offline Sync, developers can create a new database and utilize it in applications built with TypeScript or Rust. While the beta is available, it is not recommended for production use due to potential data loss.\n\nTurso is actively seeking user feedback and will continue to improve the feature, with plans for future updates like conflict resolution and encryption. Users can join the Turso Discord for updates and support.",
      "ko": "Turso는 오프라인 동기화 기능의 공개 베타 버전을 출시했습니다. 이 기능을 통해 애플리케이션은 인터넷 연결 없이도 작동할 수 있습니다. 즉, 로컬 데이터베이스 작업이 계속 진행되며, 연결이 복원되면 변경 사항이 자동으로 Turso Cloud에 동기화됩니다.\n\n주요 기능으로는 양방향 동기화가 있습니다. 로컬에서 변경된 내용은 클라우드로 전송할 수 있고, 반대로 클라우드의 내용도 로컬로 가져올 수 있습니다. 또한, 원격 쓰기 지원이 추가되어 내장된 데이터베이스가 로컬에서의 쓰기를 수용할 수 있게 되었습니다. 시스템은 충돌을 감지할 수 있지만, 충돌 해결 기능은 아직 구현되지 않았습니다.\n\n이 기능은 모바일 앱, 판매 시점 시스템, 현장 데이터 수집, 사물인터넷(IoT) 애플리케이션 등 다양한 용도로 유용합니다. 개발자들은 이 기능을 통해 오프라인에서도 반응성이 뛰어난 애플리케이션을 쉽게 만들 수 있습니다.\n\nTurso 오프라인 동기화를 시작하려면 개발자는 새로운 데이터베이스를 생성하고 TypeScript 또는 Rust로 구축된 애플리케이션에서 이를 활용할 수 있습니다. 현재 베타 버전이 제공되지만, 데이터 손실 가능성 때문에 생산 환경에서는 사용하지 않는 것이 좋습니다.\n\nTurso는 사용자 피드백을 적극적으로 수집하고 있으며, 충돌 해결 및 암호화와 같은 향후 업데이트 계획을 가지고 기능을 지속적으로 개선할 예정입니다. 사용자들은 Turso Discord에 참여하여 업데이트와 지원을 받을 수 있습니다.",
      "ja": "Tursoは、インターネット接続がなくてもアプリケーションが機能するオフライン同期機能のパブリックベータ版を開始しました。これにより、ローカルデータベースの操作を続けることができ、接続が復旧すると自動的に変更がTurso Cloudに同期されます。\n\n主な機能には、ローカルの変更をクラウドに送信したり、その逆も可能な双方向同期、埋め込みデータベースがローカルの書き込みを受け入れるリモート書き込みサポート、そしてシステムが競合を検出できる機能がありますが、競合の解決機能はまだ実装されていません。\n\nこの機能は、モバイルアプリ、販売時点管理システム、現場データ収集、IoTアプリケーションなど、さまざまなアプリケーションに役立ちます。開発者は、オフラインでも応答性の高いアプリケーションを簡単に作成できるようになります。\n\nTursoのオフライン同期を始めるには、開発者は新しいデータベースを作成し、TypeScriptやRustで構築されたアプリケーションで利用できます。ベータ版は利用可能ですが、データ損失の可能性があるため、商用利用は推奨されていません。\n\nTursoはユーザーからのフィードバックを積極的に求めており、競合解決や暗号化などの将来のアップデートを計画しています。ユーザーはTursoのDiscordに参加して、最新情報やサポートを受けることができます。"
    }
  },
  {
    "id": "89661daa19698695",
    "title": {
      "en": "Why America now eats a crazy number of avocados",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.wsj.com/business/avocado-prices-tariffs-mexico-imports-3a951021",
    "score": 77,
    "by": "fortran77",
    "time": 1743284138,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "64fec81d4aa198b2",
    "title": {
      "en": "Why Kagi launched \"no use, no pay\"",
      "ko": "카기, \"사용 안 하면 요금 없다\" 출시!",
      "ja": "「使わなければ無料」"
    },
    "type": "story",
    "url": "https://getlago.substack.com/p/why-kagi-launched-no-use-no-pay",
    "score": 9,
    "by": "AnhTho_FR",
    "time": 1743505532,
    "content": "Share this postThe Bill, PleaseWhy Kagi launched “no use, no pay”Copy linkFacebookEmailNotesMoreDiscover more from The Bill, PleaseThe Bill, Please tells the stories and learnings from building Lago, a YC-backed open-source billing & metering product.Over 1,000 subscribersSubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.Already have an account? Sign inWhy Kagi launched “no use, no pay”FinnApr 01, 2025Share this postThe Bill, PleaseWhy Kagi launched “no use, no pay”Copy linkFacebookEmailNotesMoreSharePaying for a SaaS subscription you don’t use sucks. Sure, I don’t use that shameful cupboard with the ice cream maker, sous-vide device and electric ham-cutter either, but at least those things don’t charge me again!In SaaS, billing is an underrated aspect of the customer experience. Money brings the customer-vendor relationship down to brass tacks. Even the friendliest customer support reps lose their charm if you feel like you’re getting ripped off.The ad-free subscription search engine Kagi recently did the opposite: Buried in their changelog, they invested in their customers with a simple billing change that unexpectedly went viral. The change was simple: Kagi wouldn’t bill you if you didn’t use the product in a given month. No use, no pay.The change didn’t even get its own blog article:But the decision rippled outward from Kagi’s own Discord to social media and then to the top of Hacker news, from which the word spread to media sites including the Verge.It caught our attention since we’re in the billing/pricing/monetization space and it was the first time we heard about something like this. It’s also a great example of how billing practices can make a real impact on your brand and positioning.I sat down with Brandon Saltalamacchia who leads marketing at Kagi to chat about this update and its reception. Here’s what I learned from him:Why Kagi decided to not charge usersAs Brandon told me, the idea first came about from an idea session: “Our Founder Vlad is friends with an incredible marketer in the UK known as Rory Sutherland. They were on a call together brainstorming ideas and talking about their businesses when Rory suggested refunding users who don’t use our product in a month. A no use, no pay scheme.”Generous schemes like this often look like marketing ploys. Any “no questions asked 100% money back guarantee” to me just reads like “refunding anyone who asks is cheaper than doing customer support”.But I don’t have that cynicism with Kagi’s fair pricing. Had this been for marketing, it would have come with a launch, not buried in a changelog along with minor improvements.No read, no pay. Subscribe to our newsletter and don’t pay, even if you do read.SubscribeI asked Brandon for the motivation behind this. He told me: “This billing strategy aligns with treating our customers like neighbors and there was no intention except for making customers happy.We had zero conversations about how this would impact revenue, if it could reactivate dormant users or if it could increase new subscriber growth.”There’s an interesting insight here: Business teams often start with the goal of increasing activation or mitigating churn or attract more users. Fair pricing surely did all three for Kagi, yet it wasn’t the intention. Sometimes, just doing something people will obviously like is the right thing to do—whether or not there’s a business justification for it.This would be harder to sell in a VC-funded startup (the only outside investors in Kagi are members of its own community). Kagi doesn’t need to grow at breakneck speed and show increasing numbers every quarter.A caveat is that this is uniquely suited for search. It’s a product category that creates value through active usage, so this policy would be pointless for things that run in the background. You’d never expect to be refunded if your cybersecurity threat monitoring doesn’t detect anything in a month. You pay for that peace of mind!Is this all a marketing scheme?Then we were a little overwhelmed with the responses from around the world. We did not expect to see this reaction. In my mind this was a “small” feature, even though it took a few weeks to plan and implement, but looking back at it, it’s a huge feature and I wish we did it earlier.It wasn’t a small project by any means, lots had to be done to make it a fluid experience for our users, down to the production of email copy, to making sure there are no bugs on our end with crediting back the cost.”That’s what Brandon told me when I asked him about the marketing aspect of all this.This last part is my favorite part of the interview. The idea sounds easy to build. But when building this, these things often run into issues with the billing system:Do you want to refund the money or issue a credit for a free month?If you want to issue credits, do you have a credit system?With 3 paid plans, how do we account for credits for users on different plans?What if someone upgrades their plan while still having a credit?If a customer is on a yearly plan, do they pay less the next year, do they get a free month after their plan or can they withdraw the money?These are just the first 5 things that come to mind—and I’m sure the team had to accommodate more edge cases. Building all of this is why billing is important, but underrated. Flexibility to ship pricing/monetization updates requires a billing system that can accommodate all the cases without breaking anything. Not having a flexible system means these kinds of updates take months to ship and require multiple engineers.(Lago is that system and makes it easy to ship billing changes like that)How Kagi benefitted from its billing changeWhen I asked Brandon about the results of this, he told me that “Most importantly, our current customers are very happy that this has been implemented. A very tiny % of our users forget to use our product in a single month, so it affects very little of our community, but it’s there as a safety net for our customers so that they can relax, knowing that should in the future they forget to use Kagi Search, we will look after them and credit their accounts.”That’s in addition to the virality of this “little” update also brought in a lot of new customers who heard about Kagi for the first time because of the buzz. It also brought in customers who were previously on the fence who now felt safe that they weren’t taking a risk.The day of the announcement marked their biggest single day of growth in 6 months.Will this catch on?I wondered whether this kind of practice could become more popular.“A part of me wants to be optimistic about this, but unfortunately for a typical SAAS company it goes against their very nature to implement something like this, as it risks losing reliable recurring revenue, so I can’t see it being implemented by many anytime soon.”I agree here. Most startups are struggling because they’re burning VC money or are   barely kept afloat by ramen profitability. They won’t reject money a user is contractually obligated to give them.That’s short-term thinking, of course. A user who pays for a product they don’t use will be frustrated and churn anyway, while someone whose account is more paused (they’re not paying) might come back. If you’re focused on hypergrowth and report in 3-month intervals, the long-term trust gains don’t really matter.Kagi can afford this because they go further than being bootstrapped and profitable:  As a Public Benefit Corporation, not beholden to maximizing shareholder value.That doesn’t mean that no use, no pay is a bad business decision. Ultimately, it sacrifices short-term revenue for long-term trust. And that matters most when you’re not looking for a quick acquisition or eyeing an IPO.You made it all the way here? Looks like you enjoyed this newsletter. You know what that means. Time to subscribe!SubscribeShare this postThe Bill, PleaseWhy Kagi launched “no use, no pay”Copy linkFacebookEmailNotesMoreShare",
    "summary": {
      "en": "Kagi, an ad-free subscription search engine, introduced a new billing policy called \"no use, no pay,\" which means customers won't be charged if they don't use the service in a month. This change, suggested during a brainstorming session, aimed to enhance customer satisfaction without focusing on immediate revenue impacts. \n\nThe decision quickly gained attention online, highlighting how billing practices can significantly affect a company's reputation. Although the update required careful implementation due to potential complexities in billing systems, it ultimately made existing customers happier and attracted new users who appreciated the reduced risk.\n\nHowever, the article notes that such a policy might not be feasible for most SaaS companies, as it could threaten their recurring revenue. Kagi, being a Public Benefit Corporation, can prioritize long-term customer trust over short-term profits, which is a luxury many startups cannot afford.",
      "ko": "광고 없는 구독형 검색 엔진인 카기는 \"사용하지 않으면 요금도 없다\"는 새로운 요금 정책을 도입했습니다. 이 정책은 고객이 한 달 동안 서비스를 사용하지 않으면 요금이 부과되지 않는 내용을 담고 있습니다. 이 변화는 브레인스토밍 세션에서 제안되었으며, 즉각적인 수익에 집중하기보다는 고객 만족도를 높이기 위한 목적이었습니다.\n\n이 결정은 온라인에서 빠르게 주목받았으며, 요금 청구 방식이 회사의 명성에 얼마나 큰 영향을 미칠 수 있는지를 보여주었습니다. 이 업데이트는 청구 시스템의 복잡성으로 인해 신중한 실행이 필요했지만, 결국 기존 고객들의 만족도를 높이고 위험이 줄어든 점을 높이 평가한 새로운 사용자들을 끌어들이는 결과를 가져왔습니다.\n\n하지만 이 기사에서는 이러한 정책이 대부분의 SaaS(서비스형 소프트웨어) 기업에는 실현 가능성이 낮을 수 있다고 언급하고 있습니다. 이는 반복적인 수익에 위협이 될 수 있기 때문입니다. 카기는 공익 법인으로서 단기적인 이익보다 장기적인 고객 신뢰를 우선시할 수 있는 여유가 있지만, 많은 스타트업들은 그러한 여유가 없습니다.",
      "ja": "Kagiは、広告のないサブスクリプション型検索エンジンで、「使わなければ支払わない」という新しい料金ポリシーを導入しました。このポリシーでは、顧客がその月にサービスを利用しなかった場合、料金が発生しません。この変更は、ブレインストーミングセッションで提案され、即時の収益にこだわらず、顧客満足度を向上させることを目的としています。\n\nこの決定はすぐにオンラインで注目を集め、料金の取り決めが企業の評判に大きな影響を与えることを示しました。更新には請求システムの複雑さから慎重な実施が求められましたが、結果的に既存の顧客はより満足し、リスクが低減されたことを評価する新しいユーザーも引き寄せました。\n\nしかし、この記事では、このようなポリシーがほとんどのSaaS企業にとって実現可能ではないかもしれないと指摘しています。なぜなら、定期的な収益が脅かされる可能性があるからです。Kagiは公共利益法人であり、短期的な利益よりも長期的な顧客の信頼を優先することができますが、これは多くのスタートアップには難しい選択肢です。"
    }
  },
  {
    "id": "421a7d60c70ace8e",
    "title": {
      "en": "MCP: The new \"USB-C for AI\" that's bringing fierce rivals together",
      "ko": "MCP: AI의 USB-C 혁명",
      "ja": "MCP: AIのUSB-C革命"
    },
    "type": "story",
    "url": "https://arstechnica.com/information-technology/2025/04/mcp-the-new-usb-c-for-ai-thats-bringing-fierce-rivals-together/",
    "score": 15,
    "by": "CharlesW",
    "time": 1743510072,
    "content": "surfing the infoscape\n\n        MCP: The new “USB-C for AI” that’s bringing fierce rivals together\n\n        Model context protocol standardizes how AI uses data sources, supported by OpenAI and Anthropic.\n\n    Benj Edwards\n\n  –\n\n    2025년 4월 1일 오후 8:30\n    |\n\n    26\n\n          Credit:\n\n          NanoStockk\n\n          Credit:\n\n          NanoStockk\n\n      Text\n        settings\n\n            Story text\n\n          Size\n\n  Small\n  Standard\n  Large\n\nWidth\n      *\n\n  Standard\n  Wide\n\nLinks\n\n  Standard\n  Orange\n\n    * Subscribers only\n    Learn more\n\n            Minimize to nav\n\nWhat does it take to get OpenAI and Anthropic—two competitors in the AI assistant market—to get along? Despite a fundamental difference in direction that led Anthropic's founders to quit OpenAI in 2020 and later create the Claude AI assistant, a shared technical hurdle has now brought them together: How to easily connect their AI models to external data sources.\nThe solution comes from Anthropic, which developed and released an open specification called Model Context Protocol (MCP) in November 2024. MCP establishes a royalty-free protocol that allows AI models to connect with outside data sources and services without requiring unique integrations for each service.\n\"Think of MCP as a USB-C port for AI applications,\" wrote Anthropic in MCP's documentation. The analogy is imperfect, but it represents the idea that, similar to how USB-C unified various cables and ports (with admittedly a debatable level of success), MCP aims to standardize how AI models connect to the infoscape around them.\nSo far, MCP has also garnered interest from multiple tech companies in a rare show of cross-platform collaboration. For example, Microsoft has integrated MCP into its Azure OpenAI service, and as we mentioned above, Anthropic competitor OpenAI is on board. Last week, OpenAI acknowledged MCP in its Agents API documentation, with vocal support from the boss upstairs.\n\"People love MCP and we are excited to add support across our products,\" wrote OpenAI CEO Sam Altman on X last Wednesday.\n\n            MCP has also rapidly begun to gain community support in recent months. For example, just browsing this list of over 300 open source servers shared on GitHub reveals growing interest in standardizing AI-to-tool connections. The collection spans diverse domains, including database connectors like PostgreSQL, MySQL, and vector databases; development tools that integrate with Git repositories and code editors; file system access for various storage platforms; knowledge retrieval systems for documents and websites; and specialized tools for finance, health care, and creative applications.\n\nOther notable examples include servers that connect AI models to home automation systems, real-time weather data, e-commerce platforms, and music streaming services. Some implementations allow AI assistants to interact with gaming engines, 3D modeling software, and IoT devices.\nWhat is “context” anyway?\nTo fully appreciate why a universal AI standard for external data sources is useful, you'll need to understand what \"context\" means in the AI field.\nWith current AI model architecture, what an AI model \"knows\" about the world is baked into its neural network in a largely unchangeable form, placed there by an initial procedure called \"pre-training,\" which calculates statistical relationships between vast quantities of input data (\"training data\"—like books, articles, and images) and feeds it into the network as numerical values called \"weights.\" Later, a process called \"fine-tuning\" might adjust those weights to alter behavior (such as through reinforcement learning like RLHF) or provide examples of new concepts.\nTypically, the training phase is very expensive computationally and happens either only once in the case of a base model, or infrequently with periodic model updates and fine-tunings. That means AI models only have internal neural network representations of events prior to a \"cutoff date\" when the training dataset was finalized.\nAfter that, the AI model is run in a kind of read-only mode called \"inference,\" where users feed inputs into the neural network to produce outputs, which are called \"predictions.\" They're called predictions because the systems are tuned to predict the most likely next token (a chunk of data, such as portions of a word) in a user-provided sequence.\nIn the AI field, context is the user-provided sequence—all the data fed into an AI model that guides the model to produce a response output. This context includes the user's input (the \"prompt\"), the running conversation history (in the case of chatbots), and any external information sources pulled into the conversation, including a \"system prompt\" that defines model behavior and \"memory\" systems that recall portions of past conversations. The limit on the amount of context a model can ingest at once is often called a \"context window,\" \"context length, \" or \"context limit,\" depending on personal preference.\n\nWhile the prompt provides important information for the model to operate upon, accessing external information sources has traditionally been cumbersome. Before MCP, AI assistants like ChatGPT and Claude could access external data (a process often called retrieval augmented generation, or RAG), but doing so required custom integrations for each service—plugins, APIs, and proprietary connectors that didn't work across different AI models. Each new data source demanded unique code, creating maintenance challenges and compatibility issues.\nMCP addresses these problems by providing a standardized method or set of rules (a \"protocol\") that allows any supporting AI model framework to connect with external tools and information sources.\n\nHow does MCP work?\nTo make the connections behind the scenes between AI models and data sources, MCP uses a client-server model. An AI model (or its host application) acts as an MCP client that connects to one or more MCP servers. Each server provides access to a specific resource or capability, such as a database, search engine, or file system. When the AI needs information beyond its training data, it sends a request to the appropriate server, which performs the action and returns the result.\nTo illustrate how the client-server model works in practice, consider a customer support chatbot using MCP that could check shipping details in real time from a company database. \"What's the status of order #12345?\" would trigger the AI to query an order database MCP server, which would look up the information and pass it back to the model. The model could then incorporate that data into its response: \"Your order shipped on March 30 and should arrive April 2.\"\nBeyond specific use cases like customer support, the potential scope is very broad. Early developers have already built MCP servers for services likeGoogle Drive, Slack, GitHub, and Postgres databases. This means AI assistants could potentially search documents in a company Drive, review recent Slack messages, examine code in a repository, or analyze data in a database—all through a standard interface.\n\nFrom a technical implementation perspective, Anthropic designed the standard for flexibility by running in two main modes: Some MCP servers operate locally on the same machine as the client (communicating via standard input-output streams), while others run remotely and stream responses over HTTP. In both cases, the model works with a list of available tools and calls them as needed.\nA work in progress\nDespite the growing ecosystem around MCP, the protocol remains an early-stage project. The limited announcements of support from major companies are promising first steps, but MCP's future as an industry standard may depend on broader acceptance, although the number of MCP servers seems to be growing at a rapid pace.\nRegardless of its ultimate adoption rate, MCP may have some interesting second-order effects. For example, MCP also has the potential to reduce vendor lock-in. Because the protocol is model-agnostic, a company could switch from one AI provider to another while keeping the same tools and data connections intact.\nMCP may also allow a shift toward smaller and more efficient AI systems that can interact more fluidly with external resources without the need for customized fine-tuning. Also, rather than building increasingly massive models with all knowledge baked in, companies may instead be able to use smaller models with large context windows.\nFor now, the future of MCP is wide open. Anthropic maintains MCP as an open source initiative on GitHub, where interested developers can either contribute to the code or find specifications about how it works. Anthropic has also provided extensive documentation about how to connect Claude to various services. OpenAI maintains its own API documentation for MCP on its website.\n\n      Benj Edwards\n      Senior AI Reporter\n\n      Benj Edwards\n      Senior AI Reporter\n\n      Benj Edwards is Ars Technica's Senior AI Reporter and founder of the site's dedicated AI beat in 2022. He's also a tech historian with almost two decades of experience. In his free time, he writes and records music, collects vintage computers, and enjoys nature. He lives in Raleigh, NC.\n\n    26 Comments",
    "summary": {
      "en": "**Summary:**\n\nThe Model Context Protocol (MCP) is a new standard for connecting AI models to external data sources, developed by Anthropic in late 2024. It aims to simplify how AI systems like OpenAI's ChatGPT and Anthropic's Claude access information, much like a USB-C port standardizes connections for devices. This protocol allows various AI models to easily integrate with different services without needing custom setups for each one.\n\nMCP has gained support from major tech companies, including Microsoft and OpenAI, and has led to over 300 open-source projects that demonstrate its versatility. It allows AI models to retrieve real-time information from databases, messaging platforms, and more, enhancing their capabilities.\n\nThe protocol operates on a client-server model, where the AI acts as a client that requests data from servers. This setup can be local or remote, making it flexible for developers.\n\nWhile still in its early stages, MCP has the potential to reduce dependency on specific AI providers and facilitate the use of smaller, more efficient AI models. Anthropic has made MCP an open-source initiative, inviting contributions and collaboration from the tech community.",
      "ko": "모델 컨텍스트 프로토콜(MCP)은 2024년 말 앤트로픽이 개발한 새로운 표준으로, AI 모델과 외부 데이터 소스를 연결하는 방식을 단순화하는 것을 목표로 합니다. 이는 USB-C 포트가 다양한 장치의 연결을 표준화하는 것과 유사하게, OpenAI의 ChatGPT와 앤트로픽의 클로드와 같은 AI 시스템이 정보를 쉽게 접근할 수 있도록 돕습니다. 이 프로토콜은 다양한 AI 모델이 각기 다른 서비스와 쉽게 통합될 수 있게 하여, 매번 맞춤형 설정이 필요하지 않도록 합니다.\n\nMCP는 마이크로소프트와 OpenAI를 포함한 주요 기술 기업들의 지지를 받고 있으며, 그 활용성을 보여주는 300개 이상의 오픈 소스 프로젝트가 등장했습니다. 이 프로토콜은 AI 모델이 데이터베이스, 메시징 플랫폼 등에서 실시간 정보를 검색할 수 있게 하여, AI의 기능을 향상시킵니다.\n\nMCP는 클라이언트-서버 모델로 작동하며, AI는 데이터를 요청하는 클라이언트 역할을 합니다. 이 설정은 로컬 또는 원격으로 구성할 수 있어 개발자들에게 유연성을 제공합니다.\n\n아직 초기 단계에 있지만, MCP는 특정 AI 제공업체에 대한 의존도를 줄이고, 더 작고 효율적인 AI 모델의 사용을 촉진할 잠재력을 가지고 있습니다. 앤트로픽은 MCP를 오픈 소스 프로젝트로 만들고, 기술 커뮤니티의 기여와 협업을 초대하고 있습니다.",
      "ja": "モデルコンテキストプロトコル（MCP）は、2024年末にAnthropicによって開発された、AIモデルと外部データソースを接続するための新しい標準です。このプロトコルは、OpenAIのChatGPTやAnthropicのClaudeのようなAIシステムが情報にアクセスする方法を簡素化することを目的としています。USB-Cポートがデバイスの接続を標準化するのと同様に、MCPはさまざまなAIモデルが異なるサービスと簡単に統合できるようにします。これにより、各サービスごとに特別な設定を必要とせずに済みます。\n\nMCPは、MicrosoftやOpenAIを含む主要なテクノロジー企業から支持を受けており、その柔軟性を示す300以上のオープンソースプロジェクトが生まれています。このプロトコルを使うことで、AIモデルはデータベースやメッセージングプラットフォームなどからリアルタイムの情報を取得でき、機能が向上します。\n\nMCPはクライアント-サーバーモデルで動作し、AIがデータをサーバーからリクエストするクライアントとして機能します。この設定はローカルまたはリモートで行うことができ、開発者にとって柔軟性があります。\n\nまだ初期段階にあるMCPですが、特定のAIプロバイダーへの依存を減らし、より小型で効率的なAIモデルの利用を促進する可能性があります。AnthropicはMCPをオープンソースの取り組みとして位置づけ、テクノロジーコミュニティからの貢献や協力を呼びかけています。"
    }
  },
  {
    "id": "251919415e41694b",
    "title": {
      "en": "Honey has now lost 4M Chrome users after shady tactics were revealed",
      "ko": "허니, 크롬 사용자 400만명 감소!",
      "ja": "ハニー、400万ユーザー喪失！"
    },
    "type": "story",
    "url": "https://9to5google.com/2025/03/31/honey-extension-users-dropped-chrome-march-2025/",
    "score": 601,
    "by": "tantalor",
    "time": 1743445683,
    "content": "Google is still rolling out Astra camera and screen sharing to Gemini Live\n\n\tAbner Li\n\tMar 31 2025",
    "summary": {
      "en": "Google is currently continuing to introduce the Astra camera and screen sharing features to Gemini Live.",
      "ko": "구글은 현재 제미니 라이브에 아스트라 카메라와 화면 공유 기능을 계속해서 도입하고 있습니다.",
      "ja": "Googleは現在、Astraカメラと画面共有機能をGemini Liveに導入し続けています。"
    }
  },
  {
    "id": "eea0d557038fc197",
    "title": {
      "en": "James Webb Space Telescope reveals that most galaxies rotate clockwise",
      "ko": "제임스 웹 망원경, 은하 회전 방향 밝혀!",
      "ja": "銀河は右回り！"
    },
    "type": "story",
    "url": "https://www.smithsonianmag.com/smart-news/james-webb-space-telescope-reveals-that-most-galaxies-rotate-clockwise-180986224/",
    "score": 343,
    "by": "instagraham",
    "time": 1743417099,
    "content": "SMART NEWS\n\n      James Webb Space Telescope Reveals That Most Galaxies Rotate Clockwise\n      This preferred direction of spin might be due to one of two reasons: either our entire universe exists in a black hole, or astronomers have been measuring the universe’s expansion incorrectly\n\n          Margherita Bassi\n\n        Daily Correspondent\n\n      March 17, 2025\n\n                Spiral galaxies imaged by JWST that rotate in the same direction relative to the Milky Way (red) and in the opposite direction relative to the Milky Way (blue). The number of galaxies rotating in the opposite direction relative to the Milky Way as observed from Earth is far higher.\n              Shamir, Lior, Monthly Notices of the Royal Astronomical Society, 2025 under CC BY 4.0\n\n        NASA’s James Webb Space Telescope (JWST) launched into orbit around the sun in December 2021. Since then, it has been studying the history of our universe. Now, images of deep space from JWST’s Advanced Deep Extragalactic Survey (JADES) have revealed something puzzling: most galaxies rotate in the same direction.\n\nAbout two-thirds of the 263 galaxies studied in a paper published February 17 in the Monthly Notices of the Royal Astronomical Society rotate clockwise, while the other one-third rotate counterclockwise.\n\nfreestar.queue.push(function() {freestar.newVideo(\"FreeStarVideoAdContainer\");});“The analysis of the galaxies was done by quantitative analysis of their shapes, but the difference is so obvious that any person looking at the image can see it,\" Lior Shamir, a computer scientist from Kansas State University and sole author of the study, says in a statement. \"There is no need for special skills or knowledge to see that the numbers are different. With the power of the James Webb Space Telescope, anyone can see it.”\n\n      The galaxies identified as rotating in the opposite direction relative to the Milky Way.\n\n      Shamir, Lior, Monthly Notices of the Royal Astronomical Society, 2025 under CC BY 4.0\n\nThe problem is that astronomers have long posited that galaxies should be evenly split between rotating in one direction or the other, astronomer Dan Weisz from the University of California, Berkeley, who was not involved with the study, wrote for Astronomy back in 2017. “This stems from the idea that we live in an ‘isotropic’ universe, which means that the universe looks roughly the same in every direction. By extension, galaxies shouldn’t have a preferred direction of spin from our perspective,” he added. According to Shamir, there are two strong potential explanations for this discrepancy.\n\nOne explanation is that the universe came into existence while in rotation. This theory would support what’s known as black hole cosmology: the hypothesis that our universe exists within a black hole that exists within another parent universe. In other words, black holes create universes within themselves, meaning that the black holes in our own universe also lead to other baby universes.\n\n\"A preferred axis in our universe, inherited by the axis of rotation of its parent black hole, might have influenced the rotation dynamics of galaxies, creating the observed clockwise-counterclockwise asymmetry,” Nikodem Poplawski, a theoretical physicist at the University of New Haven who was not involved in the study, tells Space.com’s Robert Lea. \"The discovery by the JWST that galaxies rotate in a preferred direction would support the theory of black holes creating new universes, and I would be extremely excited if these findings are confirmed.”\n\n      The galaxies identified as rotating in the same direction relative to the Milky Way (counterclockwise).\n\n      Shamir, Lior, Monthly Notices of the Royal Astronomical Society, 2025 under CC BY 4.0\n\nAnother possible explanation involves the Milky Way’s rotation. Due to an effect called the Doppler shift, astronomers expect galaxies rotating opposite to the Milky Way’s motion to appear brighter, which could explain their overrepresentation in telescopic surveys.\n\n“If that is indeed the case, we will need to re-calibrate our distance measurements for the deep universe,” Shamir explains in the statement. \"The re-calibration of distance measurements can also explain several other unsolved questions in cosmology such as the differences in the expansion rates of the universe and the large galaxies that according to the existing distance measurements are expected to be older than the universe itself.”\n\nIt remains to be seen whether further research will confirm black hole cosmology or that astronomers have been measuring the universe’s expansion incorrectly—or perhaps something else entirely.\n\n      Get the latest stories in your inbox every weekday.\n\n        Email Powered by Salesforce Marketing Cloud (Privacy Notice / Terms & Conditions)\n\n              Margherita Bassi\n\n              | READ MORE\n\n            Margherita Bassi is afreelancejournalistand trilingual storyteller. Her work has appeared in publications including BBC Travel,Discover magazine,Live Science,Atlas ObscuraandHidden Compass.\n\n              Filed Under:\n\n                    Astronomy,\n\n                    Black Holes,\n\n                    James Webb Space Telescope,\n\n                    New Research,\n\n                    Outer Space\n\n    freestar.config.enabled_slots.push({ placementName: \"smithsonianmag_rail_right_1\", slotId: \"smithsonianmag_rail_right_1\" });\n\n  Most Popular\n\n            This Painting of Lounging Lions Was Hanging in a Family's Living Room. It Turned Out to Be an Original Delacroix\n\n            Getting Annoyed at Your Noisy Neighbor? Spiders Are, Too. New Research Finds They'll Build Webs Differently in Loud Conditions\n\n            The Real Story Behind 'Wolf Hall' and the Fall of Thomas Cromwell, Henry VIII's Most Controversial Adviser\n\n            See 15 Captivating Images From the British Wildlife Photography Awards, From a Majestic Shark to Hungry Pigeons\n\n            Grizzly Bears Are Emerging From Their Dens in Yellowstone, Grand Teton National Parks",
    "summary": {
      "en": "The James Webb Space Telescope (JWST) has discovered that most galaxies rotate in a clockwise direction. In a study of 263 galaxies, about two-thirds were found to spin clockwise, while one-third rotated counterclockwise. This finding contradicts the common belief that galaxies should rotate equally in both directions, as our universe is thought to be isotropic, meaning it looks the same from all angles.\n\nThere are two main theories to explain this unexpected rotation pattern. One theory suggests that the universe may have started out rotating, possibly due to existing within a black hole, which could influence the spin of galaxies. The second theory posits that the Milky Way's rotation might affect how we observe other galaxies, potentially making those rotating in the opposite direction appear brighter and more numerous.\n\nFurther research is needed to confirm these theories and resolve questions about the universe's expansion and galaxy formation.",
      "ko": "제임스 웹 우주 망원경(JWST)은 대부분의 은하가 시계 방향으로 회전한다는 사실을 발견했습니다. 263개의 은하를 연구한 결과, 약 3분의 2는 시계 방향으로 회전하고, 나머지 3분의 1은 반시계 방향으로 회전하는 것으로 나타났습니다. 이 발견은 은하가 양 방향으로 균등하게 회전해야 한다는 일반적인 믿음과는 상반됩니다. 우주는 모든 방향에서 동일하게 보인다고 여겨지기 때문입니다.\n\n이러한 예상치 못한 회전 패턴을 설명하기 위한 두 가지 주요 이론이 있습니다. 첫 번째 이론은 우주가 처음에 회전하고 있었을 가능성이 있다는 것입니다. 이는 블랙홀의 영향을 받을 수 있으며, 이로 인해 은하의 회전이 영향을 받을 수 있습니다. 두 번째 이론은 우리 은하인 밀키웨이의 회전이 다른 은하를 관찰하는 방식에 영향을 미칠 수 있다는 것입니다. 이 경우 반대 방향으로 회전하는 은하가 더 밝고 더 많이 보일 수 있습니다.\n\n이 이론들을 확인하고 우주의 팽창 및 은하 형성에 대한 질문을 해결하기 위해서는 추가 연구가 필요합니다.",
      "ja": "ジェームズ・ウェッブ宇宙望遠鏡（JWST）は、ほとんどの銀河が時計回りに回転していることを発見しました。263の銀河を調査した結果、約3分の2が時計回りに回転している一方で、3分の1は反時計回りでした。この発見は、銀河が均等に両方向に回転するべきだという一般的な考え方と矛盾しています。私たちの宇宙は等方的であり、どの角度から見ても同じように見えると考えられています。\n\nこの予想外の回転パターンを説明するための主な理論は二つあります。一つ目の理論は、宇宙が回転しながら始まった可能性があるというもので、これは銀河の回転に影響を与えるブラックホールの存在によるものかもしれません。二つ目の理論は、私たちの銀河である天の川の回転が他の銀河の観測に影響を与え、反対方向に回転している銀河がより明るく、数が多く見える可能性があるというものです。\n\nこれらの理論を確認し、宇宙の膨張や銀河の形成に関する疑問を解決するためには、さらなる研究が必要です。"
    }
  },
  {
    "id": "f8f53137243edea7",
    "title": {
      "en": "Everything is Ghibli",
      "ko": "모두가 지브리",
      "ja": "すべてはジブリ"
    },
    "type": "story",
    "url": "https://carly.substack.com/p/everything-is-ghibli",
    "score": 146,
    "by": "ghuntley",
    "time": 1743457494,
    "content": "Share this postGood Graf!Everything is GhibliCopy linkFacebookEmailNotesMoreDiscover more from Good Graf!Design, tech, and writing with substance and style.Over 4,000 subscribersSubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.Already have an account? Sign inEverything is GhibliWhen we got “create anything” and all made the same thingCarly AyresMar 28, 2025174Share this postGood Graf!Everything is GhibliCopy linkFacebookEmailNotesMore1532ShareOpenAI unleashed its native image generation in ChatGPT on Tuesday. By Wednesday morning, every social feed was drowning in Studio Ghibli-style portraits. (Linkedin, check back next week.) What happened—and why—is another signal of where AI, art, and our attention are headed.SubscribeHow it started: Just some bros and a selfieOn March 25, OpenAI livestreamed GPT-4o’s image generation features—blending text and visuals, rendering flawless typography, and handling complex prompts with ease. At one point, the team turned a group selfie into an anime-style portrait.Sure, the technical updates were significant: improved text rendering (a longtime challenge), better attribute binding (complex prompts, handled), and context carried across conversation. But what really matters is what we did with it.How it’s going: Full-on GhiblificationIt started with one tweet: “Tremendous alpha right now in sending your wife photos of yall converted to studio ghibli anime,” wrote Grant Slatton. Soon everyone—even Mike Tyson—was stylizing themselves, their kids, 9/11, and memes into the soft, pastel aesthetic of Japan’s beloved animation house.“only a few times in my life i got to see days like today when technological marvels bring together the timeline in rapturous joy,” tweeted roon. The vibes got so high, OpenAI had to delay rollout to free users.When Grant tweeted about the trend, CEO Sam Altman replied: “Believe it or not we put a lot of thought into the initial examples we show.” (Note the Ghibli-fied avatar.)@sama claps backMiyazaki vs. the machineAs billions of pixels were being Ghibli-fied, a 2016 clip resurfaced: Hayao Miyazaki, Studio Ghibli’s legendary founder, watching an AI demo. His take? “I strongly feel that this is an insult to life itself. I am utterly disgusted.”Trung Phan called out the math: each Ghibli film contains 60–70,000 hand-drawn, watercolored frames. A four-second scene in The Wind Rises took one animator 15 months. Now, anyone summon the style in seconds.“Imagine being Miyazaki,” tweeted Nabeel S. Qureshi, “pouring decades of heart and soul into this transcendent, tender style… and seeing it sloppified by linear algebra.”Slop, aesthetics, and valueThis tension—between delight and devaluation—sparked a broader conversation. Reggie James dubbed the trend “Ghibli slop,” not as dismissal, but as critique of what happens when distinctive artistic vision becomes infinitely reproducible at scale. “What was once valuable in the awareness of painstaking labor, beautiful stories, and coherent aesthetic,” he writes, “is now valuable PURELY in our reception to, and reproduction of, the aesthetic.”When anyone can generate a passable Ghibli homage in seconds, the scarcity shifts—from execution to conception, from craft to taste. As signüll put it: “we’re exiting the scarcity economy of visuals & entering something weirder—where aesthetics become ambient infrastructure, like wifi.”Chris Paik also captured this paradox: “This commodification turns meaningful, handcrafted narratives—filled with emotional depth and human struggle—into hollow memes, diluting their artistic significance.”The everything generatorWhat’s most telling isn’t just what people made, but what they didn’t. “They gave us an everything generator but everyone is obsessed with making it do the same thing,” noted Luke Miles. Given infinite possibility, we chose repetition. Ad nauseum.Still, some interesting (dare I say useful?) applications are emerging: Sherwin Wu visualized a home renovation. Sam Dape spun up a sticker pack. Ken Wheeler personalized a kid’s book. EP made a YouTube title card.@sherwinwu on XStealing Google’s thunder: Vibes > benchmarksAnother subplot: the hullabaloo completely eclipsed Google’s announcement of Gemini 2.5 Pro—their most advanced AI model to date—released the same day. It also came on the heels of image generation upstart Reve. As signüll observed: “Google dropped their biggest upgrade ever & ghibli core completely hijacked the zeitgeist. Peak example of how vibes now override tech merit in the attention economy.” signüll vs. noisehow to lose the internet in few hoursgemini 2.5 was supposed to be google’s chatgpt moment. smarter reasoning. faster context. tighter multimodal fusion. years of infrastructure bets finally lining up…Read more6 days ago · 37 likes · 1 comment · signüllThe takeaway? If you’re launching a consumer product, technical superiority doesn’t automatically translate to cultural impact. In the battle for attention, a memorable vibe often beats a better benchmark.What stays valuableAs Ghibli clones flooded the feed, “we’re cooked” tweets flew. But the flood revealed something else: when execution becomes trivial, direction becomes essential.“This is a huge signal that design won’t disappear,” argued Joseph Alessio. “Everyone is doing ghiblicore because that’s exactly what OpenAI demonstrated. People still need someone to shape concepts—lower the floor, raise the ceiling.”Runway’s co-founder Cristóbal Valenzuela made a bet: “If you are thinking about a new career, I would strongly suggest Art Direction. Being a good art director might end up being the most important creative role of the next decade.”Will Manidis predicted: “The most valuable items in the future will be the old and beautiful. all of text will become ai slop, all of art will become ai slop or a reaction to ai slop. The vast majority of genuine human beauty that will exist has already been created.” As Willem Van Lancker put it in January: “Our children will romanticize and fetishize the last era of pure human creation in ways we can’t imagine.”The policy responseBy Wednesday night, OpenAI appeared to be adjusting its approach. Some users reported being blocked from generating Ghibli-style images. An spokesperson clarified that while the company prevents “generations in the style of individual living artists,” it permits “broader studio styles.” Still, they added, “We’re always learning from real-world use and feedback.”Where do we go from here?The Ghibli-fication of the internet made the abstract tangible. It showed us AI’s capacity to delight and its power to flatten. What comes next will depend on how we respond, not just to the tools, but to the values they surface. We’re not just generating images. We’re generating norms.—CarlyShare174Share this postGood Graf!Everything is GhibliCopy linkFacebookEmailNotesMore1532SharePreviousNext",
    "summary": {
      "en": "OpenAI launched its image generation feature in ChatGPT, leading to a rapid trend where users created Studio Ghibli-style portraits of themselves and others. This phenomenon highlighted the blend of AI, art, and user engagement, with many expressing joy over the ability to generate these images quickly. However, it also raised concerns from artists, particularly Hayao Miyazaki, about the devaluation of handcrafted art in the face of mass production by AI. \n\nCritics pointed out that while the technology allows for creative expression, it also risks diluting the emotional depth and uniqueness of art, turning meaningful narratives into mere reproductions. Despite the potential for a wide range of creative outputs, many users focused on replicating the same Ghibli aesthetic. \n\nThe overwhelming interest in Ghibli-style creations overshadowed other significant AI advancements, showcasing how cultural impact can sometimes outweigh technical superiority. As the trend continues, the importance of art direction and unique concepts is expected to increase, highlighting the need for human creativity in an era of AI-generated content. OpenAI is also adjusting its policies in response to user behavior, indicating ongoing discussions about the implications of AI in art.",
      "ko": "OpenAI는 ChatGPT에서 이미지 생성 기능을 출시하면서 사용자들이 스튜디오 지브리 스타일의 초상화를 빠르게 만들어내는 트렌드가 생겼습니다. 이 현상은 인공지능, 예술, 사용자 참여가 결합된 모습을 보여주었고, 많은 사람들이 이러한 이미지를 신속하게 생성할 수 있다는 점에 기뻐했습니다. 그러나 이는 특히 미야자키 하야오와 같은 예술가들로부터 수공예 예술의 가치가 AI의 대량 생산으로 인해 저하될 것이라는 우려를 불러일으켰습니다.\n\n비평가들은 이 기술이 창의적인 표현을 가능하게 하지만, 예술의 감정적 깊이와 독창성을 희석시킬 위험이 있다고 지적했습니다. 의미 있는 이야기들이 단순한 복제로 전락할 수 있다는 것입니다. 다양한 창의적 결과물이 나올 가능성이 있음에도 불구하고, 많은 사용자들은 동일한 지브리 미학을 재현하는 데 집중했습니다.\n\n지브리 스타일의 창작물에 대한 압도적인 관심은 다른 중요한 AI 발전을 가릴 정도로 문화적 영향력이 기술적 우수성을 초월할 수 있음을 보여주었습니다. 이 트렌드가 계속됨에 따라 예술적 방향성과 독창적인 개념의 중요성이 더욱 커질 것으로 예상되며, AI가 생성한 콘텐츠 시대에 인간의 창의성이 필요하다는 점이 강조되고 있습니다. OpenAI는 사용자 행동에 따라 정책을 조정하고 있으며, 예술에서 AI의 의미에 대한 논의가 계속되고 있음을 나타냅니다.",
      "ja": "OpenAIはChatGPTに画像生成機能を導入し、ユーザーがスタジオジブリ風のポートレートを自分や他の人のために作成するという急速なトレンドが生まれました。この現象は、AI、アート、ユーザーの関与が融合する様子を浮き彫りにし、多くの人々がこれらの画像を迅速に生成できることに喜びを表現しました。しかし、これは特に宮崎駿をはじめとするアーティストたちから、AIによる大量生産が手作りのアートの価値を下げることへの懸念も引き起こしました。\n\n批評家たちは、この技術が創造的な表現を可能にする一方で、アートの感情的な深みや独自性を薄めるリスクがあると指摘しています。意味のある物語が単なる再生産に変わってしまう恐れがあるのです。多様な創造的な出力の可能性があるにもかかわらず、多くのユーザーは同じジブリの美学を再現することに集中しています。\n\nジブリ風の作品への圧倒的な関心は、他の重要なAIの進展を overshadow し、文化的な影響が時には技術的な優位性を上回ることを示しています。このトレンドが続く中で、アートディレクションや独自のコンセプトの重要性が増すと予想されており、AI生成コンテンツの時代において人間の創造性の必要性が強調されています。OpenAIもユーザーの行動に応じて方針を調整しており、アートにおけるAIの影響についての議論が続いていることを示しています。"
    }
  },
  {
    "id": "9139735915353582",
    "title": {
      "en": "The Pragmatic Open Source Contributor",
      "ko": "실용적인 오픈소스 기여자",
      "ja": "実践的オープンソース貢献者"
    },
    "type": "story",
    "url": "https://diurnal.st/2025/03/02/the-pragmatic-open-source-contributor.html",
    "score": 52,
    "by": "diurnalist",
    "time": 1743258760,
    "content": "The Pragmatic Open Source Contributor\n\nThe Pragmatic Contributor\nThe Pragmatic Patch Playbook\n\nStep 0: Talk to legal\nStep 1: Get the lay of the land\nStep 2: Get maintainer buy-in\nStep 3: Do the work\nStep 4: Do the other work\nStep 5: Finish line\nStep 6: Tie it off\n\nConclusion\nI sometimes have the feeling that salaried programmers shy away from fixing or extending open source code due to some combination of the following:\n\n  They don’t think it’s part of their job.\n  Their employer doesn’t think it’s part of their job, and/or does not have the necessary legal infrastructure and will.\n  They’ve been frustrated in the past with rejected or ignored patch requests.\n  They are worried about the time committment for the entire process.\n\nThese are understandable barriers that I would like to break down. More broadly speaking, I have two aims for this guide.\n\nFirst, I hope to encourage fostering the shared knowledge that open source software represents. Especially if you work in larger-scale environments or are early adopters of promising new technologies, you have a great opportunity to spot issues and areas of improvement that can benefit everyone.\n\nSecond, I aim to provide a playbook for how to successfully do this kind of work and set expectations for what you can expect and what will be expected of you. I have made contributions to many projects over the years, generally following a pattern of identifying some code that solves 95% of my problem and contributing the 5% delta back. I’ve also worked more extensively in a single open source community, OpenStack, where public contributions and distributed coordination are the status quo.\n\nThe Pragmatic Contributor\nThe community guide to open source succinctly summarizes the benefits of contributing back to open source projects. Still, I find that these guides frame the process as motivated by individual incentives, e.g., personal development or a sense of fulfillment. For the pragmatic contributor, one motivation trumps all: the drive to improve software the business relies on. You want to fix a tool that isn’t enabling you to do the best job you can.\n\nThis mindset influences all aspects of the contribution process. If you’re mostly interested in learning something new or making connections, you can pick and choose what projects you want to contribute to, and what problems you’d like to pick off the bug tracker. A pragmatic contributor never tries to find problems; they find you. Maybe it’s a newer project and hasn’t fleshed out all the little details, e.g., support for multiple authentication methods. Or, perhaps you simply found what seems like a bug in the behavior, or an inefficiency when used with high throughput.\n\nA pragmatic contributor also pressure-tests the solution. It’s likely you ran in to some problem nobody else has seen. Do you really need to implement a fix in the open source layer, or is it fair to say your application is just behaving weirdly? What is the wider benefit, really, of contributing this feature back to the community? Software naturally wants to expand in surface area and complexity over time. Some maintainers rule with an iron fist to keep the scope of their code low and steady, others are more willing to give you the benefit of the doubt that expanding scope is going to make things better. Over time I’ve come to appreciate the wisdom of the first approach, though it introduces challenges for you as an outsider. In either case, I have found that a respect for the maintainer’s view (it is their code you’ve been happily using, after all) and a willingness to find the most elegant solution goes a long way.\n\nThe Pragmatic Patch Playbook\nOnce you’ve identified a potential contribution, here are some general steps I like to follow.\n\nStep 0: Talk to legal\nBefore you do anything else, get approval from your legal department. Even if you’re contributing to a project that does not have a Contributor License Agreement (CLA), you have a duty to ensure you are not adding risk to your company or yourself. Ideally, this conversation results in approval to sign any reviewed CLAs; Company CLAs (CCLA), if supported by the project, can enable self-servicing of licenses to new members of your team, and are much more future-proof than individual CLAs tied to one employee.\n\nThis can be a bit of an uphill battle, but in my experience it is mostly a battle of time. You might need some patience to walk all the stakeholders through the business case for contributing back to open source, and any possible risks. I usually lean on the following argument:\n\n  We currently use open source system X, and it provides business value through capabilities and cost-efficiency, i.e., it’s usually free—“as in beer.”\n\n  Yet, it can’t handle some new business use-case without modification.\n  Modification effort is small relative to working around the constraint.\n  We do not need to and will not expose proprietary code.\n  Privately adapting the code (forking) introduces long-term maintenance burden and adds risk. It’s likely X will be changed in the future in a way that requires significant rework of our adaptations and thus blocks us from performing security upgrades.\n  Publicly adapting the code reduces this risk and (especially if X is well-known in the industry) can even serve to attract new talent by increasing the company’s visibility.\n  Contributing to X is thus in the business’s best interest.\n\nI have never seen this argument fail given enough motivation, but I’m sure there are exceptions. One thing to remember is you’re not arguing for actually doing the work or estimating the time commitment or return on investment; you’re making a case for the option. You should however be prepared to give concrete examples of the type of contributions you might make.\n\nStep 1: Get the lay of the land\nI see developers often skip this step and go straight to submitting a patch. This can lead to frustration for both you and the maintainers. Doing a quick check of any defined contribution process and putting yourself in the maintainers’ shoes often prevents these issues.\n\nWhat is the contribution process? Do you need to sign a CLA first? Are pull requests welcome on the repo? Can you find examples of contributions from the outside? How did the maintainers respond to the contribution request? Anything you can learn from this?\n\n  See also How to Contribute to Open Source: Orienting Yourself to a New Project\n\nHow active are the maintainers? Are they reviewing patches on a daily basis, or does it seem more sporadic? Can you notice any patterns in how and when they respond to queries? Is there a shared maintainer model or is there a single owner? If shared, who seems to be the most active recently?\n\nHow long will this realistically take? What is the latency between the time a pull request is open and it is merged? How much of that is waiting for the patch author versus feedback from maintainers? How many patches do you think you’ll need to do, and do they need to be done serially? From this, you can usually get a ballpark estimate, but I also have a heuristic: expect two weeks to one month for a bugfix to land, and three months to a year for major feature work. Much of that depends on how much of your attention you give to tending to the process.\n\nStep 2: Get maintainer buy-in\nFor small changes, you can usually skip this step, but if you’re thinking of making any significant changes to the codebase, investing time here will make the entire process much smoother. Your objective is to identify at least one maintainer who will help champion your change.\n\nMeet the maintainers where they are. Do they have a Slack channel they use to coordinate changes and share info? A bi-weekly special interest group (SIG) meeting? A mailing list? Figure out what their preferred communication method is and then introduce yourself. Give a bit of background on who you are and why you’re interested in contributing to the project, and what problem you’re trying to solve.\n\nFollow the formal proposal process. If the project uses a proposal system (e.g., the Kubernetes Enhancement Proposal [KEP]), learn about how to submit a proposal. You can either talk to maintainers before making a proposal, or notify them after you have submitted the proposal. I still think a “warm handoff” is important here, to actually reach out in person to the maintainers to let them know that you’re open to discussion and are serious about embarking on the process of making a significant contribution.\n\nAgree on scope and keep a paper trail. If in your conversations with the maintainers you arrive at agreement on what the scope of your contribution will be, and, importantly, what can be considered out of scope, make sure that you write that down somewhere publicly. This can be helpful to give context to other maintainers who might be reviewing the work down the line. Generally, this information should also be in a proposal document, if that process exists, but it’s still useful to have other records.\n\nFor this reason, I also like to have conversations about proposals in, e.g., public Slack channels as opposed to private messages. You can hash out details in private, but then post a summary of the conversation in Slack to preserve a record.\n\nStep 3: Do the work\nYou may have noticed that we haven’t written any code yet. Guess what? This is the only step where we’re going to talk about code! The lion’s share of open source work is communication. That said, there are some general rules that in my experience improve the outcome of patch requests.\n\nDon’t be afraid to fork. Some people recoil at the word “fork.” Personally, when I’m working on open source contributions, I always fork the project, make the patches there, and use the fork internally for a while. You’re making a trade-off between speed of delivery (leveraging your patch immediately) and maintenance burden (carrying your fork through upgrades); in my workplaces the former usually trumps the latter, depending on how often you expect to pull in upstream changes. Forking internally also lets you battle-test your changes in a real environment before you submit the patches upstream. I often find bugs this way, or can correct mistaken assumptions about how something works in practice.\n\nWork backwards from your specific desired outcome to a generic mechanism that helps achieve that outcome (and perhaps others.) For example, in this old webpack patch, what I wanted was a way to put a Git commit SHA in the name of files built by webpack. Rather than code this case explicitly, I proposed a way to enable plugins to provide support for new filename pattern placeholders. This enabled me to handle my needs in a separate plugin, and appears to have been useful to others over the years.\n\nAdd tests! If you’ve found a bug in some code, it probably means there wasn’t a good-enough test for that behavior. Add a test that fails without your patch and succeeds with your patch. If you’re adding new functionality, make sure you have good coverage. The maintainers will ultimately be on the hook for bugs in your code, and your job is to reduce that burden as much as you can. It sometimes happens that there is not appropriate test infrastructure to express the tests you need. In that case, reach out to the maintainers to ask their opinion on how to proceed; often, they will be okay with less test coverage as they are working on figuring out a generic solution for testing that aspect of the code.\n\nKeep every patch to one atomic change. The definition of “change” here is open to interpretation. If you are working extensively in an open source project and have a large feature you’re implementing that touches many areas of the code, you should probably break up each piece of the implementation into a single patch that targets a subset of the codebase. This works best if the broader context of your work is known and formally tracked; not all projects have this infrastructure. I like to keep my patches scoped to minimize context overhead for the reviewer. For example, when working on a larger feature, I first identified one (rather large) refactor I could do that would make implementing the feature easier. I submitted one patch\nfor that change, and then one patch for the minimal feature implementation.\n\nIf your atomic change is still large, break it into iterative commits. In the latter example patch, I broke it into several commits to make it easier to review and see the thought process. I could have broken those commits into separate pull requests, but it seemed to me to reduce cognitive overhead (for the reviewers) when everything was in a single pull request that could be referred to and iterated upon. You can always break it into separate requests later if you have the commits structured this way.\n\nKeep refactoring to a minimum. You may be tempted to “clean up” other areas of the code not specifically related to your code. It may be difficult, but you should try not to give in to these impulses. Any unnecessary refactoring, especially when it concerns readability or styling of code, adds to the work the reviewer must do and disrupts the message of what you’re actually trying to achieve with your patch. Reduce refactoring to only what is necessary to make your change possible. Later, you can come back and do the refactor if you want. Patch requests are also a good opportunity to ask the maintainers how they would feel about such a refactor in the future, enabling you to get some early buy-in.\n\nPreserve backwards-compatibility. Open source projects are widely consumed and you cannot know all of the ways in which it’s being used today. You have probably been burned at some point by a library changing default behavior or its API surface without a major version bump. Major versions are big steps for an open source codebase and if you are tying your change to a breaking-change release, it will increase the latency of your change being available drastically. As such, work to ensure that whatever you do does not break existing behavior.\n\nStep 4: Do the other work\nThis is probably the part of the process that developers like the least, but you should plan to spend some time here to have a high-quality contribution.\n\nWrite good documentation. You should write docs for any new capabilities you are adding to the project. Sometimes you need to add an entirely new section of documentation! I don’t think I need to argue for the benefits of documentation, and how frustrating it can be when documentation is sorely lacking. If this is a task that is particularly difficult for you, I’m guessing that LLMs are probably pretty good at writing docs these days, and could be a good tool for summarizing how a feature works in a more consumable format. I haven’t tried this myself yet and still write all documentation by hand.\n\n  See also these useful resources from WriteTheDocs.\n\nExamples are also documentation. Usually, technical documentation for open source code is of the “technical reference” variety. However, that is only one type of documentation. Examples add more of a “how to” flavor, and sometimes showing really is better than telling. If you’re adding another feature to the code, you should provide some examples of how to use it.\n\nStep 5: Finish line\nOnce you have all the code and non-code pieces assembled, and have checked all the other required boxes in the process, you’re ready to submit your patch! This can feel a bit like “hurry up and wait,” especially if you’re been working on a patch to deliver something else as part of your job. In my experience, patience is important here. Just because it’s a good time for you to iterate on the patch (because you’ve built up all the context) doesn’t mean it’s the same story for the maintainers. Here are some tips for how to navigate this time.\n\nProactively reach out to the maintainers. Use those channels and connections you established in Step 2 if it’s a bigger change. Maintainers may want to discuss your change formally as part of their own processes, and giving them a heads-up that your patch is ready for review can help them figure out how to prioritize it. For smaller changes, I don’t usually do this, as I think it’s a bit annoying to just ping maintainers for something they essentially were already notified about via the patch request itself.\n\nPolitely check in periodically to raise visibility. If I’m having trouble getting any eyeballs on a patch request, I usually wait a week or two and then nicely ping some maintainers and request they please have a look-see at the patch, or if there is anything else that I should do. If that doesn’t work the first time, I’ll do it again in another week or so. If a month or so goes by, I will start to get more creative and see if I can reach them on another appropriate public social channel. Always be polite, and definitely never be pushy; even though you think you’re helping the project, you’re also taking the time and attention of maintainers.\n\nAs soon as you get attention from the maintainers on your patch, leap into action. Especially for larger requests, if you get a first-pass review of your code, try to respond to feedback within a day or so. This keeps the patch conversation pretty fresh for everybody. The maintainer just spent time building up context on your work by reviewing your code and it’s my experience that they appreciate quick follow-through on comments. This can be difficult to balance depending on your own job commitments. I try to set expectations that when a patch is in the final stages, I should have some work time set aside as a buffer.\n\nStep 6: Tie it off\nIf you made it here, your patch was accepted upstream! There’s always a great feeling (of relief?) when you see that merge complete successfully. This doesn’t always happen—I have some pull requests that have been sitting for years collecting a trickle of sad “+1” comments. As a pragmatic contributor, this isn’t such a big deal, as it usually indicates the code has a low rate of evolution and therefore it’s not too much work to maintain your own fork.\n\nAnyways, there are a few final things I usually do once crossing the finish line.\n\nThank the maintainers who reviewed your code. Being a maintainer is often a thankless task, so I find a sincere thank-you goes a long way. I often learn a lot from code reviews with open source maintainers, and view that as a gift. You can just leave a thank-you on your patch conversation, or reach out directly, whichever seems more appropriate given your past communication with them.\n\nReduce your bus factor. Did you start work on a longer set of related features? If so, make sure you clearly document what the next steps are for the work somewhere. Lots of things could happen. You could get involved in other work commitments that take up all your attention, you could be laid off, your company could stop using the software you patched altogether. Still, somebody might want to come along later to finish what you started. It should be possible to do that without your involvement.\n\nBring your patches back internally. If you’ve been working on a private fork of the code, work to bring your changes back in to your fork. Maybe you’ve even made it possible to stop using your fork entirely!\n\nConclusion\nLet’s revisit some of the reasons I suggested prevent folks from contributing to open source:\n\nThey don’t think it’s part of their job. Hopefully I’ve made a brief but decent case for why this is important, both for the wider community, and for your own growth. Familiarity and confidence in this process empowers you to blast through technical barriers, as you might no longer be “blocked” from achieving your goals due to some underlying third-party code not supporting XYZ.\n\nTheir employer think it’s part of their job, and/or does not have the necessary legal infrastructure and will. Step 0 describes ways you can try to make the case for contributions. Ultimately this may still be a barrier in practice, but I think it’s worth poking at assumptions here. Sometimes you can be surprised by how open-minded your employer is about work like this.\n\nThey’ve been frustrated in the past with rejected or ignored patch requests. Steps 1 and 2 should put you in a better position to set your own expectations and improve the timelines by having better relationships with folks you’re dependent on.\n\nThey are worried about the time committment for the entire process. This is the wisest objection to the whole endeavor, in my opinion. It’s hopefully clear that writing the code is a very small part of this entire process. Still, I do find that this gets easier the more experience you have doing it, because you know more about how to keep the ball rolling, and if you’re working within the same ecosystem, over time you should gain more trust from the maintainers, which helps greatly with future contributions.\n\n  Write the change you want to see in the world!\n\n      Originally published 2025-03-02.",
    "summary": {
      "en": "### Summary of \"The Pragmatic Open Source Contributor\"\n\nThis guide provides a step-by-step approach for developers looking to contribute to open source projects, aiming to encourage participation and ease common barriers.\n\n#### Key Steps in the Contribution Process:\n1. **Talk to Legal**: Get approval from your legal team to avoid risks to yourself or your company.\n2. **Get the Lay of the Land**: Understand the project's contribution process and maintainer dynamics before submitting changes.\n3. **Get Maintainer Buy-In**: Engage with maintainers to gain support for significant changes, ensuring clear communication and agreement on the scope of your contribution.\n4. **Do the Work**: Focus on coding while following best practices like forking, adding tests, and keeping changes atomic and backward-compatible.\n5. **Do the Other Work**: Write good documentation and provide examples to clarify your contributions.\n6. **Finish Line**: Submit your patch and actively engage with maintainers for feedback.\n7. **Tie It Off**: After acceptance, thank maintainers, document future steps, and integrate your changes back into your internal systems.\n\n#### Conclusion:\nMany developers hesitate to contribute due to concerns about their role, legal issues, past frustrations, and time commitments. This guide aims to empower them by outlining a practical process for contributions, emphasizing the importance of open source for both personal growth and community benefit.",
      "ko": "이 가이드는 오픈 소스 프로젝트에 기여하고자 하는 개발자들을 위해 단계별 접근 방식을 제공합니다. 참여를 장려하고 일반적인 장벽을 완화하는 데 중점을 두고 있습니다.\n\n기여 과정의 주요 단계는 다음과 같습니다. 첫째, 법무팀과 상담하여 법적 승인을 받는 것이 중요합니다. 이를 통해 개인이나 회사에 대한 위험을 피할 수 있습니다. 둘째, 프로젝트의 기여 과정과 유지 관리자의 동향을 이해해야 합니다. 변경 사항을 제출하기 전에 이 과정을 충분히 파악하는 것이 필요합니다. 셋째, 유지 관리자와의 소통을 통해 중요한 변경 사항에 대한 지지를 얻어야 합니다. 기여의 범위에 대해 명확하게 소통하고 합의하는 것이 중요합니다. 넷째, 코딩에 집중하면서 포크, 테스트 추가, 변경 사항을 작고 호환 가능하게 유지하는 등의 모범 사례를 따릅니다. 다섯째, 기여 내용을 명확히 하기 위해 좋은 문서를 작성하고 예제를 제공해야 합니다. 여섯째, 패치를 제출한 후 유지 관리자와 적극적으로 소통하여 피드백을 받습니다. 마지막으로, 기여가 수용된 후에는 유지 관리자에게 감사하고, 향후 단계에 대해 문서화하며, 내부 시스템에 변경 사항을 통합하는 과정을 마무리합니다.\n\n많은 개발자들이 자신의 역할, 법적 문제, 과거의 불만, 시간적 제약 때문에 기여를 주저합니다. 이 가이드는 기여를 위한 실용적인 과정을 제시함으로써 그들을 지원하고, 개인의 성장과 커뮤니티의 이익을 위해 오픈 소스의 중요성을 강조하고자 합니다.",
      "ja": "このガイドは、オープンソースプロジェクトに貢献したい開発者のために、段階的なアプローチを提供します。参加を促し、一般的な障壁を軽減することを目的としています。\n\n貢献プロセスの重要なステップには、まず法務部門と相談し、リスクを避けるための承認を得ることが含まれます。次に、プロジェクトの貢献プロセスやメンテイナーの関係を理解してから変更を提出することが重要です。大きな変更を行う際には、メンテイナーと連携し、貢献の範囲について明確なコミュニケーションと合意を得ることが求められます。\n\n作業に取り掛かる際は、コーディングに集中し、フォークやテストの追加、変更を小さく保ちつつ後方互換性を持たせるなどのベストプラクティスに従うことが大切です。また、良いドキュメントを作成し、貢献内容を明確にするための例を提供することも必要です。最後に、パッチを提出し、メンテイナーからのフィードバックを積極的に受け入れます。\n\n受け入れられた後は、メンテイナーに感謝し、今後のステップを文書化し、自社のシステムに変更を統合することが求められます。\n\n多くの開発者は、自分の役割や法的な問題、過去のフラストレーション、時間の制約から貢献をためらっています。このガイドは、貢献のための実践的なプロセスを示すことで、彼らを力づけ、オープンソースが個人の成長やコミュニティの利益にとって重要であることを強調しています。"
    }
  },
  {
    "id": "2a0b7640c5e50af7",
    "title": {
      "en": "Things I Won't Work With: Dioxygen Difluoride (2010)",
      "ko": "내가 다루지 않을 것: 디옥시젠 디플루오라이드",
      "ja": "扱いたくない物: 二フッ化酸素"
    },
    "type": "story",
    "url": "https://www.science.org/content/blog-post/things-i-won-t-work-dioxygen-difluoride",
    "score": 259,
    "by": "PebblesRox",
    "time": 1743418733,
    "content": "The latest addition to the long list of chemicals that I never hope to encounter takes us back to the wonderful world of fluorine chemistry. I'm always struck by how much work has taken place in that field, how long ago some of it was first done, and how many violently hideous compounds have been carefully studied. Here's how the experimental prep of today's fragrant breath of spring starts:The heater was warmed to approximately 700C. The heater block glowed a dull red color, observable with room lights turned off. The ballast tank was filled to 300 torr with oxygen, and fluorine was added until the total pressure was 901 torr. . .And yes, what happens next is just what you think happens: you run a mixture of oxygen and fluorine through a 700-degree-heating block. \"Oh, no you don't,\" is the common reaction of most chemists to that proposal, \". . .not unless I'm at least a mile away, two miles if I'm downwind.\" This, folks, is the bracingly direct route to preparing dioxygen difluoride, often referred to in the literature by its evocative formula of FOOF.Well, \"often\" is sort of a relative term. Most of the references to this stuff are clearly from groups who've just been thinking about it, not making it. Rarely does an abstract that mentions density function theory ever lead to a paper featuring machine-shop diagrams, and so it is here. Once you strip away all the \"calculated geometry of. . .\" underbrush from the reference list, you're left with a much smaller core of experimental papers.And a hard core it is! This stuff was first prepared in Germany in 1932 by Ruff and Menzel, who must have been likely lads indeed, because it's not like people didn't respect fluorine back then. No, elemental fluorine has commanded respect since well before anyone managed to isolate it, a process that took a good fifty years to work out in the 1800s. (The list of people who were blown up or poisoned while trying to do so is impressive). And that's at room temperature. At seven hundred freaking degrees, fluorine starts to dissociate into monoatomic radicals, thereby losing its gentle and forgiving nature. But that's how you get it to react with oxygen to make a product that's worse in pretty much every way.FOOF is only stable at low temperatures; you'll never get close to RT with the stuff without it tearing itself to pieces. I've seen one reference to storing it as a solid at 90 Kelvin for later use, but that paper, a 1962 effort from A. G. Streng of Temple University, is deeply alarming in several ways. Not only did Streng prepare multiple batches of dioxygen difluoride and keep it around, he was apparently charged with finding out what it did to things. All sorts of things. One damn thing after another, actually:\"Being a high energy oxidizer, dioxygen difluoride reacted vigorously with organic compounds, even at temperatures close to its melting point. It reacted instantaneously with solid ethyl alcohol, producing a blue flame and an explosion. When a drop of liquid 02F2 was added to liquid methane, cooled at 90°K., a white flame was produced instantaneously, which turned green upon further burning. When 0.2 (mL) of liquid 02F2 was added to 0.5 (mL) of liquid CH4 at 90°K., a violent explosion occurred.\"And he's just getting warmed up, if that's the right phrase to use for something that detonates things at -180C (that's -300 Fahrenheit, if you only have a kitchen thermometer). The great majority of Streng's reactions have surely never been run again. The paper goes on to react FOOF with everything else you wouldn't react it with: ammonia (\"vigorous\", this at 100K), water ice (explosion, natch), chlorine (\"violent explosion\", so he added it more slowly the second time), red phosphorus (not good), bromine fluoride, chlorine trifluoride (say what?), perchloryl fluoride (!), tetrafluorohydrazine (how on Earth. . .), and on, and on. If the paper weren't laid out in complete grammatical sentences and published in JACS, you'd swear it was the work of a violent lunatic. I ran out of vulgar expletives after the second page. A. G. Streng, folks, absolutely takes the corrosive exploding cake, and I have to tip my asbestos-lined titanium hat to him.Even Streng had to give up on some of the planned experiments, though (bonus dormitat Strengus?). Sulfur compounds defeated him, because the thermodynamics were just too titanic. Hydrogen sulfide, for example, reacts with four molecules of FOOF to give sulfur hexafluoride, 2 molecules of HF and four oxygens. . .and 433 kcal, which is the kind of every-man-for-himself exotherm that you want to avoid at all cost. The sulfur chemistry of FOOF remains unexplored, so if you feel like whipping up a batch of Satan's kimchi, go right ahead.Update: note that this is 433 kcal per mole, not per molecule (which would be impossible for even nuclear fission and fusion reaction (see here for the figures). Chemists almost always think in energetics in terms of moles, thus the confusion. It's still a ridiculous amount of energy to shed, and you don't want to be around when it happens.So does anyone use dioxygen difluoride for anything? Not as far as I can see. Most of the recent work with the stuff has come from groups at Los Alamos, where it's been used to prepare national-security substances such as plutonium and neptunium hexafluoride. But I do note that if you run the structure through SciFinder, it comes out with a most unexpected icon that indicates a commercial supplier. That would be the Hangzhou Sage Chemical Company. They offer it in 100g, 500g, and 1 kilo amounts, which is interesting, because I don't think a kilo of dioxygen difluoride has ever existed. Someone should call them on this - ask for the free shipping, and if they object, tell them Amazon offers it on this item. Serves 'em right. Morons.",
    "summary": {
      "en": "The text discusses the dangerous chemical dioxygen difluoride (FOOF), a compound derived from fluorine and oxygen. The preparation of FOOF involves heating a mixture of fluorine and oxygen to around 700°C, which is extremely hazardous and typically avoided by chemists. \n\nFirst created in 1932, FOOF is highly unstable and only safe to store at very low temperatures. Historical experiments, especially by A.G. Streng in the 1960s, revealed that FOOF is a powerful oxidizer that reacts violently with various substances, causing explosions even at very low temperatures. Streng's work included many reactions that are too dangerous to repeat today. \n\nCurrently, FOOF is not widely used, except in specific applications at Los Alamos National Laboratory related to national security. Interestingly, a chemical supplier in China claims to sell FOOF in large quantities, which raises skepticism about their actual ability to provide such a hazardous material. Overall, the text highlights the extreme risks and limited practical use of dioxygen difluoride in chemistry.",
      "ko": "이 글에서는 위험한 화학 물질인 디옥시젠 디플루오라이드(FOOF)에 대해 다룹니다. FOOF는 플루오르와 산소에서 유래한 화합물로, 이를 제조하기 위해서는 플루오르와 산소의 혼합물을 약 700도에서 가열해야 합니다. 이 과정은 매우 위험하여 일반적으로 화학자들은 피하는 방법입니다.\n\nFOOF는 1932년에 처음 만들어졌으며, 매우 불안정하여 극저온에서만 안전하게 보관할 수 있습니다. 1960년대 A.G. 스트렝의 실험에서는 FOOF가 강력한 산화제로 작용하며, 다양한 물질과 격렬하게 반응하여 심지어 낮은 온도에서도 폭발을 일으킬 수 있다는 사실이 밝혀졌습니다. 스트렝의 연구에는 오늘날 반복하기에는 너무 위험한 반응들이 포함되어 있었습니다.\n\n현재 FOOF는 특정 응용 분야를 제외하고는 널리 사용되지 않으며, 로스앨러모스 국립연구소에서 국가 안보와 관련된 용도로만 사용됩니다. 흥미롭게도, 중국의 한 화학 공급업체는 FOOF를 대량으로 판매한다고 주장하고 있어, 실제로 이렇게 위험한 물질을 제공할 수 있는지에 대한 의구심이 제기되고 있습니다. 전반적으로 이 글은 디옥시젠 디플루오라이드의 극단적인 위험성과 제한된 실용성을 강조하고 있습니다.",
      "ja": "ジオキシジェンジフルオリド（FOOF）は、フッ素と酸素から派生した危険な化学物質です。FOOFの調製には、フッ素と酸素の混合物を約700℃に加熱する必要があり、これは非常に危険で、通常は化学者によって避けられています。\n\nFOOFは1932年に初めて合成されましたが、非常に不安定で、極低温でしか安全に保管できません。特に1960年代のA.G.ストレンによる実験では、FOOFが強力な酸化剤であり、さまざまな物質と激しく反応し、低温でも爆発を引き起こすことが明らかになりました。ストレンの研究には、今日では再現が危険すぎる反応が多く含まれていました。\n\n現在、FOOFは広く使用されておらず、国家安全保障に関連するロスアラモス国立研究所での特定の用途に限られています。興味深いことに、中国のある化学供給業者はFOOFを大量に販売していると主張していますが、そのような危険な物質を実際に提供できるかどうかには疑問が残ります。全体として、FOOFの化学における極端なリスクと限られた実用性が強調されています。"
    }
  },
  {
    "id": "cf04b6c5f941abb4",
    "title": {
      "en": "LLM Workflows then Agents: Getting Started with Apache Airflow",
      "ko": "에어플로우로 시작하는 LLM 워크플로우",
      "ja": "エアフロー入門：LLMとエージェント"
    },
    "type": "story",
    "url": "https://github.com/astronomer/airflow-ai-sdk",
    "score": 121,
    "by": "alittletooraph2",
    "time": 1743445948,
    "content": "airflow-ai-sdk\nThis repository contains an SDK for working with LLMs from Apache Airflow, based on Pydantic AI. It allows users to call LLMs and orchestrate agent calls directly within their Airflow pipelines using decorator-based tasks. The SDK leverages the familiar Airflow @task syntax with extensions like @task.llm, @task.llm_branch, and @task.agent.\nTo get started, check out the examples repository here, which offers a full local Airflow instance with the AI SDK installed and 5 example pipelines. To run this locally, run:\ngit clone https://github.com/astronomer/ai-sdk-examples.git\ncd ai-sdk-examples\nastro dev start\n\nIf you don't have the Astro CLI installed, run brew install astro (or see other options here).\nIf you already have Airflow running, you can also install the package with any optional dependencies you need:\npip install airflow-ai-sdk[openai,duckduckgo]\n\nNote that installing the package with no optional dependencies will install the slim version of the package, which does not include any LLM models or tools. The available optional packages are listed here. While this SDK offers the optional dependencies for convenience sake, you can also install the optional dependencies from Pydantic AI directly.\nTable of Contents:\n\nFeatures\nMotivation\nExamples\n\nLLM calls from a DAG (summarize Airflow's commits)\nLLM calls with structured outputs using @task.llm (user feedback -> sentiment and feature requests)\nAgent calls with @task.agent (deep research agent)\nChanging dag control flow with @task.llm_branch (support ticket routing)\n\nFuture Work\n\nFeatures\n\nLLM tasks with @task.llm: Define tasks that call language models (e.g. GPT-3.5-turbo) to process text.\nAgent tasks with @task.agent: Orchestrate multi-step AI reasoning by leveraging custom tools.\nAutomatic output parsing: Use function type hints (including Pydantic models) to automatically parse and validate LLM outputs.\nBranching with @task.llm_branch: Change the control flow of a DAG based on the output of an LLM.\nModel support: Support for all models in the Pydantic AI library (OpenAI, Anthropic, Gemini, Ollama, Groq, Mistral, Cohere, Bedrock)\n\nDesign Principles\nWe follow the taskflow pattern of Airflow with three decorators:\n\n@task.llm: Define a task that calls an LLM. Under the hood, this creates a Pydantic AI Agent with no tools.\n@task.agent: Define a task that calls an agent. You can pass in a Pydantic AI Agent directly.\n@task.llm_branch: Define a task that branches the control flow of a DAG based on the output of an LLM. Enforces that the LLM output is one of the downstream task_ids.\n\nThe function supplied to each decorator is a translation function that converts the Airflow task's input into the LLM's input. If you don't want to do any translation, you\ncan just return the input unchanged.\nMotivation\nAI workflows are becoming increasingly common as organizations look for pragmatic ways to get value out of LLMs. As with\nany workflow, it's important to have a flexible and scalable way to orchestrate them.\nAirflow is a popular choice for orchestrating data pipelines. It's a powerful tool for managing the dependencies\nbetween tasks and for scheduling and monitoring them, and has been trusted by data teams everywhere for 10+ years. It comes \"batteries included\" with a rich set of capabilities:\n\nFlexible scheduling: run tasks on a fixed schedule, on-demand, or based on external events\nDynamic task mapping: easily process multiple inputs in parallel with full error handling and observability\nBranching and conditional logic: change the control flow of a DAG based on the output of certain tasks\nError handling: built-in support for retries, exponential backoff, and timeouts\nResource management: limit the concurrency of tasks with Airflow Pools\nMonitoring: detailed logs and monitoring capabilities\nScalability: designed for production workflows\n\nThis SDK is designed to make it easy to integrate LLM workflows into your Airflow pipelines. It allows you to do anything from simple LLM calls to complex agentic workflows.\nExamples\nSee the full set of examples in the examples/dags directory.\nLLM calls from a DAG (summarize Airflow's commits)\nThis example shows how to use the @task.llm decorator as part of an Airflow DAG. In the @task.llm decorator, we can\nspecify a model and system prompt. The decorator allows you to transform the Airflow task's input into the LLM's input.\nSee full example: github_changelog.py\nimport os\n\nimport pendulum\n\nfrom airflow.decorators import dag, task\n\nfrom github import Github\n\n@task\ndef get_recent_commits(data_interval_start: pendulum.DateTime, data_interval_end: pendulum.DateTime) -> list[str]:\n    \"\"\"\n    This task returns a mocked list of recent commits. In a real workflow, this\n    task would get the recent commits from a database or API.\n    \"\"\"\n    print(f\"Getting commits for {data_interval_start} to {data_interval_end}\")\n    gh = Github(os.getenv(\"GITHUB_TOKEN\"))\n    repo = gh.get_repo(\"apache/airflow\")\n    commits = repo.get_commits(since=data_interval_start, until=data_interval_end)\n    return [f\"{commit.commit.sha}: {commit.commit.message}\" for commit in commits]\n\n@task.llm(\n    model=\"gpt-4o-mini\",\n    result_type=str,\n    system_prompt=\"\"\"\n    Your job is to summarize the commits to the Airflow project given a week's worth\n    of commits. Pay particular attention to large changes and new features as opposed\n    to bug fixes and minor changes.\n\n    You don't need to include every commit, just the most important ones. Add a one line\n    overall summary of the changes at the top, followed by bullet points of the most\n    important changes.\n\n    Example output:\n\n    This week, we made architectural changes to the core scheduler to make it more\n    maintainable and easier to understand.\n\n    - Made the scheduler 20% faster (commit 1234567)\n    - Added a new task type: `example_task` (commit 1234568)\n    - Added a new operator: `example_operator` (commit 1234569)\n    - Added a new sensor: `example_sensor` (commit 1234570)\n    \"\"\"\n)\ndef summarize_commits(commits: list[str] | None = None) -> str:\n    \"\"\"\n    This task summarizes the commits. You can add logic here to transform the input\n    before it gets passed to the LLM.\n    \"\"\"\n    # don't need to do any translation\n    return \"\\n\".join(commits)\n\n@task\ndef send_summaries(summaries: str):\n    ...\n\n@dag(\n    schedule=\"@weekly\",\n    start_date=pendulum.datetime(2025, 3, 1, tz=\"UTC\"),\n    catchup=False,\n)\ndef github_changelog():\n    commits = get_recent_commits()\n    summaries = summarize_commits(commits=commits)\n    send_summaries(summaries)\n\ngithub_changelog()\n\nLLM calls with structured outputs using @task.llm (user feedback -> sentiment and feature requests)\nThis example demonstrates how to use the @task.llm decorator to call an LLM and return a structured output. In this\ncase, we're using a Pydantic model to validate the output of the LLM. We recommend using the airflow_ai_sdk.BaseModel\nclass to define your Pydantic models in case we add more functionality in the future.\nSee full example: product_feedback_summarization.py\nimport pendulum\n\nfrom typing import Literal, Any\n\nfrom airflow.decorators import dag, task\nfrom airflow.exceptions import AirflowSkipException\n\nimport airflow_ai_sdk as ai_sdk\n\nfrom include.pii import mask_pii\n\n@task\ndef get_product_feedback() -> list[str]:\n    \"\"\"\n    This task returns a mocked list of product feedback. In a real workflow, this\n    task would get the product feedback from a database or API.\n    \"\"\"\n    ...\n\nclass ProductFeedbackSummary(ai_sdk.BaseModel):\n    summary: str\n    sentiment: Literal[\"positive\", \"negative\", \"neutral\"]\n    feature_requests: list[str]\n\n@task.llm(\n    model=\"gpt-4o-mini\",\n    result_type=ProductFeedbackSummary,\n    system_prompt=\"\"\"\n    You are a helpful assistant that summarizes product feedback.\n    \"\"\"\n)\ndef summarize_product_feedback(feedback: str | None = None) -> ProductFeedbackSummary:\n    \"\"\"\n    This task summarizes the product feedback. You can add logic here to transform the input\n    before summarizing it.\n    \"\"\"\n    # if the feedback doesn't mention Airflow, skip it\n    if \"Airflow\" not in feedback:\n        raise AirflowSkipException(\"Feedback does not mention Airflow\")\n\n    # mask PII in the feedback\n    feedback = mask_pii(feedback)\n\n    return feedback\n\n@task\ndef upload_summaries(summaries: list[dict[str, Any]]):\n    ...\n\n@dag(\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n)\ndef product_feedback_summarization():\n    feedback = get_product_feedback()\n    summaries = summarize_product_feedback.expand(feedback=feedback)\n    upload_summaries(summaries)\n\nproduct_feedback_summarization()\n\nAgent calls with @task.agent (deep research agent)\nThis example shows how to build an AI agent that can autonomously invoke external tools (e.g., a knowledge base search) when answering a user question.\nSee full example: deep_research.py\nimport pendulum\nimport requests\n\nfrom airflow.decorators import dag, task\nfrom airflow.models.dagrun import DagRun\nfrom airflow.models.param import Param\n\nfrom bs4 import BeautifulSoup\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.common_tools.duckduckgo import duckduckgo_search_tool\n\n# custom tool to get the content of a page\ndef get_page_content(url: str) -> str:\n    \"\"\"\n    Get the content of a page.\n    \"\"\"\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, \"html.parser\")\n\n    distillation_agent = Agent(\n        \"gpt-4o-mini\",\n        system_prompt=\"\"\"\n        You are responsible for distilling information from a text. The summary will be used by a research agent to generate a research report.\n\n        Keep the summary concise and to the point, focusing on only key information.\n        \"\"\",\n    )\n\n    return distillation_agent.run_sync(soup.get_text())\n\ndeep_research_agent = Agent(\n    \"o3-mini\",\n    system_prompt=\"\"\"\n    You are a deep research agent who is very skilled at distilling information from the web. You are given a query and your job is to generate a research report.\n\n    You can search the web by using the `duckduckgo_search_tool`. You can also use the `get_page_content` tool to get the contents of a page.\n\n    Keep going until you have enough information to generate a research report. Assume you know nothing about the query or contents, so you need to search the web for relevant information.\n\n    Do not generate new information, only distill information from the web.\n    \"\"\",\n    tools=[duckduckgo_search_tool(), get_page_content],\n)\n\n@task.agent(agent=deep_research_agent)\ndef deep_research_task(dag_run: DagRun) -> str:\n    \"\"\"\n    This task performs a deep research on the given query.\n    \"\"\"\n    query = dag_run.conf.get(\"query\")\n\n    if not query:\n        raise ValueError(\"Query is required\")\n\n    print(f\"Performing deep research on {query}\")\n\n    return query\n\n@task\ndef upload_results(results: str):\n    ...\n\n@dag(\n    schedule=None,\n    start_date=pendulum.datetime(2025, 3, 1, tz=\"UTC\"),\n    catchup=False,\n    params={\n        \"query\": Param(\n            type=\"string\",\n            default=\"How has the field of data engineering evolved in the last 5 years?\",\n        ),\n    },\n)\ndef deep_research():\n    results = deep_research_task()\n    upload_results(results)\n\ndeep_research()\n\nChanging dag control flow with @task.llm_branch (support ticket routing)\nThis example demonstrates how to use the @task.llm_branch decorator to change the control flow of a DAG based on the output of an LLM. In this case, we're routing support tickets based on the severity of the ticket.\nSee full example: support_ticket_routing.py\nimport pendulum\nfrom airflow.decorators import dag, task\nfrom airflow.models.dagrun import DagRun\n\n@task.llm_branch(\n    model=\"gpt-4o-mini\",\n    system_prompt=\"\"\"\n    You are a support agent that routes support tickets based on the priority of the ticket.\n\n    Here are the priority definitions:\n    - P0: Critical issues that impact the user's ability to use the product, specifically for a production deployment.\n    - P1: Issues that impact the user's ability to use the product, but not as severely (or not for their production deployment).\n    - P2: Issues that are low priority and can wait until the next business day\n    - P3: Issues that are not important or time sensitive\n\n    Here are some examples of tickets and their priorities:\n    - \"Our production deployment just went down because it ran out of memory. Please help.\": P0\n    - \"Our staging / dev / QA deployment just went down because it ran out of memory. Please help.\": P1\n    - \"I'm having trouble logging in to my account.\": P1\n    - \"The UI is not loading.\": P1\n    - \"I need help setting up my account.\": P2\n    - \"I have a question about the product.\": P3\n    \"\"\",\n    allow_multiple_branches=True,\n)\ndef route_ticket(dag_run: DagRun) -> str:\n    return dag_run.conf.get(\"ticket\")\n\n@task\ndef handle_p0_ticket(ticket: str):\n    print(f\"Handling P0 ticket: {ticket}\")\n\n@task\ndef handle_p1_ticket(ticket: str):\n    print(f\"Handling P1 ticket: {ticket}\")\n\n@task\ndef handle_p2_ticket(ticket: str):\n    print(f\"Handling P2 ticket: {ticket}\")\n\n@task\ndef handle_p3_ticket(ticket: str):\n    print(f\"Handling P3 ticket: {ticket}\")\n\n@dag(\n    start_date=pendulum.datetime(2025, 1, 1, tz=\"UTC\"),\n    schedule=None,\n    catchup=False,\n    params={\"ticket\": \"Hi, our production deployment just went down because it ran out of memory. Please help.\"}\n)\ndef support_ticket_routing():\n    ticket = route_ticket()\n\n    handle_p0_ticket(ticket)\n    handle_p1_ticket(ticket)\n    handle_p2_ticket(ticket)\n    handle_p3_ticket(ticket)\n\nsupport_ticket_routing()",
    "summary": {
      "en": "### Summary of airflow-ai-sdk\n\nThe **airflow-ai-sdk** is a software development kit (SDK) for integrating large language models (LLMs) into Apache Airflow pipelines. Here are the key points:\n\n- **Functionality**: The SDK allows users to use decorators to call LLMs and manage AI workflows within Airflow tasks. Key decorators include:\n  - `@task.llm`: For calling LLMs.\n  - `@task.agent`: For orchestrating complex reasoning tasks.\n  - `@task.llm_branch`: For modifying the flow of tasks based on LLM outputs.\n\n- **Getting Started**: \n  - Clone the examples repository and start Airflow locally with:\n    ```\n    git clone https://github.com/astronomer/ai-sdk-examples.git\n    cd ai-sdk-examples\n    astro dev start\n    ```\n  - Install the SDK with optional dependencies using:\n    ```\n    pip install airflow-ai-sdk[openai,duckduckgo]\n    ```\n\n- **Features**:\n  - Supports various LLMs and allows for structured outputs using Pydantic models.\n  - Automatic parsing and validation of LLM outputs.\n  - Flexible control flow management based on LLM outputs.\n\n- **Examples**: The SDK includes multiple examples demonstrating its use, such as:\n  - Summarizing commits from a GitHub repository.\n  - Analyzing product feedback with structured outputs.\n  - Performing deep research using web searching and content distillation.\n  - Routing support tickets based on priority using LLMs.\n\n- **Design Principles**: The SDK follows Airflow's taskflow pattern and is designed for flexibility, scalability, and ease of integration into existing workflows.\n\nOverall, the airflow-ai-sdk facilitates the incorporation of AI capabilities into data workflows managed by Apache Airflow, enhancing their functionality and efficiency.",
      "ko": "airflow-ai-sdk는 대형 언어 모델(LLM)을 아파치 에어플로우 파이프라인에 통합하기 위한 소프트웨어 개발 키트(SDK)입니다. 이 SDK의 주요 기능은 사용자가 데코레이터를 사용하여 LLM을 호출하고 에어플로우 작업 내에서 AI 워크플로를 관리할 수 있도록 하는 것입니다. 주요 데코레이터로는 LLM을 호출하는 `@task.llm`, 복잡한 추론 작업을 조정하는 `@task.agent`, LLM 출력에 따라 작업 흐름을 수정하는 `@task.llm_branch`가 있습니다.\n\n시작하려면 예제 저장소를 복제하고 로컬에서 에어플로우를 실행해야 합니다. 이를 위해 다음 명령어를 입력합니다. git clone https://github.com/astronomer/ai-sdk-examples.git, cd ai-sdk-examples, astro dev start. SDK를 설치할 때는 선택적 종속성을 포함하여 pip install airflow-ai-sdk[openai,duckduckgo] 명령어를 사용합니다.\n\n이 SDK는 다양한 LLM을 지원하며, Pydantic 모델을 사용하여 구조화된 출력을 제공합니다. LLM 출력의 자동 파싱 및 검증 기능도 포함되어 있으며, LLM 출력에 따라 유연한 제어 흐름 관리가 가능합니다.\n\nSDK에는 여러 사용 예제가 포함되어 있습니다. 예를 들어, GitHub 저장소의 커밋 요약, 구조화된 출력을 통한 제품 피드백 분석, 웹 검색과 콘텐츠 증류를 통한 심층 연구 수행, LLM을 사용한 우선순위 기반 지원 티켓 라우팅 등이 있습니다.\n\n이 SDK는 에어플로우의 작업 흐름 패턴을 따르며, 유연성, 확장성, 기존 워크플로에의 통합 용이성을 염두에 두고 설계되었습니다. 전반적으로 airflow-ai-sdk는 아파치 에어플로우로 관리되는 데이터 워크플로에 AI 기능을 통합하여 기능성과 효율성을 향상시킵니다.",
      "ja": "airflow-ai-sdkは、Apache Airflowのパイプラインに大規模言語モデル（LLM）を統合するためのソフトウェア開発キット（SDK）です。このSDKの主な機能は、デコレーターを使用してLLMを呼び出し、Airflowのタスク内でAIワークフローを管理できることです。重要なデコレーターには、LLMを呼び出すための`@task.llm`、複雑な推論タスクを調整するための`@task.agent`、LLMの出力に基づいてタスクの流れを変更するための`@task.llm_branch`があります。\n\n始めるには、まず例のリポジトリをクローンし、ローカルでAirflowを起動します。具体的には、次のコマンドを実行します。git clone https://github.com/astronomer/ai-sdk-examples.git、cd ai-sdk-examples、astro dev startです。また、オプションの依存関係を含めてSDKをインストールするには、pip install airflow-ai-sdk[openai,duckduckgo]を使用します。\n\nこのSDKは、さまざまなLLMをサポートし、Pydanticモデルを使って構造化された出力を可能にします。LLMの出力は自動的に解析され、検証されます。また、LLMの出力に基づいて柔軟な制御フロー管理が行えます。\n\nSDKには、GitHubリポジトリからのコミットの要約、構造化された出力を用いた製品フィードバックの分析、ウェブ検索とコンテンツの抽出を用いた深い研究の実施、LLMを使用した優先度に基づくサポートチケットのルーティングなど、さまざまな使用例が含まれています。\n\nデザイン原則として、SDKはAirflowのタスクフローのパターンに従い、柔軟性、スケーラビリティ、既存のワークフローへの統合の容易さを考慮して設計されています。全体として、airflow-ai-sdkはApache Airflowによって管理されるデータワークフローにAI機能を組み込むことを容易にし、その機能性と効率を向上させます。"
    }
  },
  {
    "id": "543ca928e3a6efbc",
    "title": {
      "en": "The Egg (2009)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.galactanet.com/oneoff/theegg.html",
    "score": 208,
    "by": "jxmorris12",
    "time": 1743421532,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "68ee7d52befa0da8",
    "title": {
      "en": "Show HN: I made a little puzzle game about a rogue chess knight",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://knightride.rakhim.org/",
    "score": 138,
    "by": "freetonik",
    "time": 1743256973,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "ca3efc5178c08598",
    "title": {
      "en": "Automating Interactive Fiction Logic Generation with LLMs in Emacs",
      "ko": "이맥스의 자동화된 이야기 생성",
      "ja": "エマacsで自動化する物語生成"
    },
    "type": "story",
    "url": "https://blog.tendollaradventure.com/automating-story-logic-with-llms/",
    "score": 90,
    "by": "dskhatri",
    "time": 1743436635,
    "content": "Automating Interactive Fiction Logic Generation with LLMs in Emacs",
    "summary": {
      "en": "The text discusses using Large Language Models (LLMs) to automate the creation of logic for interactive fiction within Emacs, a text editor. It highlights how LLMs can help streamline the process of generating storylines and decisions in interactive narratives, making it easier for writers to develop complex plots and character interactions. Overall, the focus is on enhancing the writing experience by integrating advanced AI tools into the creative process.",
      "ko": "이 글에서는 Emacs라는 텍스트 편집기에서 인터랙티브 픽션의 논리를 자동으로 생성하는 데 대형 언어 모델(LLM)을 사용하는 방법에 대해 다룹니다. LLM은 인터랙티브 내러티브에서 스토리라인과 결정 과정을 생성하는 과정을 간소화하는 데 도움을 줄 수 있습니다. 이를 통해 작가들은 복잡한 줄거리와 캐릭터 상호작용을 더 쉽게 개발할 수 있습니다. 전반적으로, 이 글은 창작 과정에 첨단 인공지능 도구를 통합하여 글쓰기 경험을 향상시키는 데 중점을 두고 있습니다.",
      "ja": "この文章では、テキストエディタであるEmacsを使って、インタラクティブフィクションの論理を自動生成するために大規模言語モデル（LLM）を活用する方法について説明しています。LLMは、インタラクティブな物語のストーリーラインや選択肢を生成するプロセスを効率化するのに役立ちます。これにより、作家は複雑なプロットやキャラクターの相互作用をより簡単に発展させることができます。全体として、先進的なAIツールを創作プロセスに統合することで、執筆体験を向上させることに重点が置かれています。"
    }
  },
  {
    "id": "85860e5ac32c1d89",
    "title": {
      "en": "Compiler Options Hardening Guide for C and C++",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://best.openssf.org/Compiler-Hardening-Guides/Compiler-Options-Hardening-Guide-for-C-and-C++.html",
    "score": 216,
    "by": "pjmlp",
    "time": 1743418910,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "f842b434b5b74d0d",
    "title": {
      "en": "Why I run FreeBSD for my home servers (2024)",
      "ko": "홈 서버의 선택, FreeBSD!",
      "ja": "自宅サーバーにFreeBSDを選ぶ理由"
    },
    "type": "story",
    "url": "https://aumont.fr/posts/FreeBSD-Home-Server/",
    "score": 166,
    "by": "psxuaw",
    "time": 1743425963,
    "content": "Page date: \"2024-11-19T00:00:00.000Z\" Page dateModified: Is dateModified defined: false8 min read.19 Nov 2024Linux is pretty nice as Desktop : #In 2024 GNU/Linux is now a pretty nice OS as Desktop environnement you can absolutely runs everything without any issue.Event if you're a gamer, huge progress has been made, thanks in particular to Valve/Steam.On daily basic I use EndeavourOS on my laptop and it is great distro.Everything works fine.And much like a Windows user, I don't care how a particular service is launched, how the Ram is managed, etc. I just want to install/remove programs. Use Wifi, bluetooth devices, web applications, various remote control tools and some markdown / txt editors and have access to a shell.As laptop user, the rest doesn't matter to me.So yeah Linux is definitively a great OS Desktop.And a big mess as Server OS : #Like every other sys/{ops,admin,store}, my company/customers use RHEL or Centos when it is necessary to use a LinuxAnd to be honest after dozens of year of administration; troubleshooting, workarround. I still didn’t understand HOW this factory works.And I've read tons of RedHat documentation / KB.warning SystemD.TLDR : the main problem is SYSTEMD, the secondary problem is the stacking of abstraction layers docker / kubersomething most of the time used of unknown reasons “just because it's modern”.Don’t keep it Simple let’s do something complicated : #Looks like this is the new mantra ! The new “MicroService” way of life that’s generated gigatons of logs impossible to debug when you have issue on production.Another point : everything that systemD manages (i.e. the entire system) makes things complicated.Simple things like device mounting, which you'd think would be handled entirely by /etc/fstab, can be bypassed by systemd options.Complicated stuff = high probably of failure = Complicated issue to troubleshoot in productionIf you 2024 backend/system Product/Owner/Developpeur ... what ever please mark theses words !warning Reminder :In other words : Overall system reliability is therefore the product of the individual reliability of each component.Thus, the more components there are in the system (excepted redundancy), the greater the probability that at least one component will fail, reducing overall system reliability.Example with a simple system the GNU/Linux log management : #As you can see log management is pretty simple on SystemD/Linux.When one of your service doesn’t trace anymore more the easiest way is to restart everything (journalct , syslog, rsyslog, syslog.socket ... who knows)Also it’s pretty common on overloaded systemd to see journalctl consumming 100% of one core.Could luck for trying to mitigate this.I can ask you the same question what is the righ tool to use for Debian when you want deal with packages ? apt , aptitude, apt-get ?We can also talk about the network configuration which can be done with nmcli or by changing config file ...It doesn't really matter in a desktop environment. But on a server configuration, it does matter when 3 years later you have to reinstall the machine.Yeah Debian ... 😁 #Don’t talk me about SystemD/Debian please ...Long time ago like a lot of people I was an sysadmin home user of Debian. I was running my services with “almost” RIP OpenVZ a paravirtualisation solution. At that time, Docker was just beginning to appear.It was perfectly supported in 2.8 Kernel (Debian 8 or something even older) but for some reasons the Debian project choosen to not include the OpenVZ into the next Debian release as upstream solution. The only way to continue using OpenVZ was to ... downgrade its kernel to continue taking advantage of portability ?!!And if you want to keep using OpenVZ you have to use DevianWhat a joke.So I have to redo all my “architecture” redeploy all my service into a new container solution ?! Rewrite/test the backup Script and so on ... Ho thank you and good bye Debian !Professional distributions with which you can't even do a rolling upgrade (Rhel / Rocky) : #That’s probably the biggest joke for a profesionnal distribution : if you want to jump from 1 main branch to another one you have to reinstall everything (or use tools like leap)Yeah I know , you can do it with Debian/Fedora/Arch (which is rolling).And with FreeBSD 😎 #Everything is simple , everything is documented, everything make sense.Ok things are probably a bit “old-school” and use the old Unix-way to do thing.But this is what we like/needs in production : simplicity and stability !I ran FreeBSD on my personnals server since 9.x released and so far upgrade after upgrade no breaking changes !Everything works since the beginning.Of course, I've changed physical servers several times and had to deal with disaster recovery situations.Each time, the operation of restarting the services proved to be simple and effective.This site for instance is hosted (with dozens of other services) with a small box powered with a N100 with 16G of RAM. I do not host several XEON Core that host K8S on several servers.I'm european and electriciy is expensive, I want to run low power machines. I don't like thing that brings too much unnecessarily overhead.When I have ISP or Electricity outage, very rare, but it happens, there's not much point in switching the service into another machine in the same room, which also has no power ...Yes, I could have a machine hosted elsewhere with a cloud provider ... but I don't like unnecessary subscriptions.Restoring from disaster (macro plan) : #Basically you have to :Restore you /home or /rootRestore /etc/rc.confRestore your firewall configuration /etc/pf.confRestore your service that are all running into a jailEt voila.Pros : #This is problably the main point : All is simple with FreeBSD.All the programme will have there configuration files into /usr/local/etcMemory / CPU footprint is minimal.By default logs are managed by newsyslog not a labyrinthine system.Jails are so powerfull to manage tons of services and still there since FreeBSD 6.x ... This is a nativ tool of FreeBSD. There are severals wrapper iocage/bastille/ezjail ... but basically this is still the same thing : jail and you can easily migrate from one wrapper to another. Build easy network setting or complicated secured stuff. Mount a NFS share within the jail ... You don’t have to choose between, lxd,lxc, docker,containerd ...Virtualisation is nice and powerfull with Bhyve : Post to follow on this topic.Whole system configuration is done in /etc/rc.confThe firewall solution availables are awesome (Packet Filter).pkg to manage packets, that’s all.You don’t have to cross your finger when you run an OS major upgrade.The network stack is fast and efficient.ZFS is more efficient on FreeBSD (Insert Source)Cons : #Probably a few sleepless nights to get things like bluethooth working ...You'll have to be more adventurous to run a full desktop environment (even if it works!).Still a bit Unix old-fashion way to manage things ? But is it a problem finally ?The biggest issue with FreeBSD : The very bad habbit of developper to deploy OpenSource software only with Docker 😈 #There is a new fashion on the OpenSource world. Developpers think Docker is the new de-facto standard and only propose to install the tools with Docker.Most of the time they do no provide any DOCUMENTATION to install their software in a baremetal way.At best, they offer an .rpm or .deb package for installation on a non-docker OS.But most of the time it's a docker file.Even worse, modern applications often deploy the code and database needed to run the code directly in the docker compose file.At what point did these people think it was relevant to run 10 different databases when I'm hosting 10 applications ?By chance the FreeBSD communauty is determined and competent and will alway try a way to create a packet for freeBSD 😇Identified software that suck hard with there docker deploy mode : #note NoteMaybe change this chapter into dedicated page if the list grows.Immich : At least there is a doc to deploy on TrueNas but this is just Screeshot and nothing about how to deploy by hand (Also this app also deploy PostgreSQL ... Dude I have allready a mutualised MariaDB I just want to deploy your app into my database, not bootstrap tons of other components) :Complexity level to deploy on FreeBSD : Medium / hard (Probably doable by spending some week end on this).BunkerWeb : This is supposed to be a Nginx hardened on steroïds, On the Github there is any documentation to deploy / build the app from SOURCE, use docker / kub / rpm / .deb or NOTHING.Complexity level to deploy on FreeBSD : Hard , I’ve tried to read the files on the .deb and this is tons of python scripts that installs dozen of lua/rpm module .. Why this is so complicated to deploy a Nginx with some Lua scripts ?Silverbullet : Ho my god I love this app 😍 I use it on daily-basis as my note garden. I will probably create a page about this app later. I recently create a init scrip to start it and it works fine.Complexity level to deploy on FreeBSD : EasyList to be continued ...{\"@context\":\"https://schema.org\",\"@type\":\"Article\",\"headline\":\"Why do I run FreeBSD for my home servers.\",\"image\":[\"https://aumont.fr/img/remote/ZdW06x-1920w.png\"],\"author\":\"Mathieu Aumont\",\"genre\":\"Insert a schema.org genre\",\"publisher\":{\"@type\":\"Organization\",\"name\":\"Maumont corporation\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"/img/favicon/favicon-192x192.png?hash=2089033c93\"}},\"url\":\"https://aumont.fr/posts/FreeBSD-Home-Server/\",\"mainEntityOfPage\":\"https://aumont.fr/posts/FreeBSD-Home-Server/\",\"datePublished\":\"2024-11-19\",\"dateModified\":\"\",\"description\":\"Linux is pretty nice as Desktop : # In 2024 GNU/Linux is now a pretty nice OS as Desktop environnement you can absolutely runs everything...\"}",
    "summary": {
      "en": "In 2024, GNU/Linux is a solid choice for desktop use, with improvements making it user-friendly for tasks like gaming and general applications. The author uses EndeavourOS on their laptop and finds it effective for everyday tasks like installing programs and connecting devices.\n\nHowever, the author criticizes Linux as a server operating system, particularly due to SystemD, which complicates system management and troubleshooting. The growing complexity from modern technologies, like Docker and microservices, leads to reliability issues and difficult log management.\n\nThe author shares frustrations with distributions like Debian and RHEL, noting their cumbersome upgrade processes and lack of simplicity. In contrast, they advocate for FreeBSD, praising its stability, minimal resource usage, and straightforward management of services. Although FreeBSD may require more effort to set up certain desktop features, it excels in server environments with efficient configuration and reliable performance.\n\nThe author also highlights the challenges of modern developers favoring Docker for deployment, which complicates software installation on FreeBSD. Overall, they value simplicity and reliability in server management, making FreeBSD their preferred choice.",
      "ko": "2024년에는 GNU/Linux가 데스크탑 사용에 적합한 선택으로, 게임이나 일반적인 애플리케이션 작업을 위한 사용자 친화적인 개선이 이루어졌습니다. 저자는 자신의 노트북에서 EndeavourOS를 사용하고 있으며, 프로그램 설치와 장치 연결 같은 일상적인 작업에 효과적이라고 느끼고 있습니다.\n\n하지만 저자는 Linux를 서버 운영 체제로 비판하며, 특히 SystemD로 인해 시스템 관리와 문제 해결이 복잡해진 점을 지적합니다. Docker와 마이크로서비스와 같은 현대 기술의 복잡성은 신뢰성 문제와 로그 관리의 어려움을 초래합니다.\n\n저자는 Debian과 RHEL과 같은 배포판에 대한 불만을 공유하며, 이들의 번거로운 업그레이드 과정과 단순함의 부족을 언급합니다. 반면, FreeBSD를 지지하며 그 안정성, 최소한의 자원 사용, 서비스 관리의 간편함을 칭찬합니다. FreeBSD는 특정 데스크탑 기능을 설정하는 데 더 많은 노력이 필요할 수 있지만, 서버 환경에서는 효율적인 구성과 신뢰할 수 있는 성능으로 뛰어납니다.\n\n또한 저자는 현대 개발자들이 배포를 위해 Docker를 선호하는 문제를 강조하며, 이로 인해 FreeBSD에서 소프트웨어 설치가 복잡해진다고 말합니다. 전반적으로 저자는 서버 관리에서 단순함과 신뢰성을 중요시하며, FreeBSD를 선호하는 선택으로 삼고 있습니다.",
      "ja": "2024年、GNU/Linuxはデスクトップ用途において非常に良い選択肢となっています。ゲームや一般的なアプリケーションに対する使いやすさが向上しています。著者は自分のノートパソコンでEndeavourOSを使用しており、プログラムのインストールやデバイスの接続といった日常的な作業に効果的だと感じています。\n\nしかし、著者はLinuxをサーバー用オペレーティングシステムとして批判しています。特にSystemDがシステム管理やトラブルシューティングを複雑にしている点が問題です。Dockerやマイクロサービスなどの現代技術の進展により、信頼性の問題やログ管理の難しさが増しています。\n\n著者はDebianやRHELといったディストリビューションに対する不満も述べており、これらのアップグレードプロセスが煩雑で、シンプルさに欠けると指摘しています。それに対して、FreeBSDを支持し、その安定性やリソースの少ない使用、サービス管理の簡潔さを称賛しています。FreeBSDは特定のデスクトップ機能の設定に手間がかかることもありますが、サーバー環境では効率的な設定と信頼性の高いパフォーマンスを発揮します。\n\nまた、著者は現代の開発者がデプロイメントにDockerを好むことが、FreeBSD上でのソフトウェアインストールを複雑にしているという課題も指摘しています。全体として、著者はサーバー管理におけるシンプルさと信頼性を重視しており、FreeBSDを好んで選んでいます。"
    }
  },
  {
    "id": "b4a0c5592cfd9da8",
    "title": {
      "en": "Oracle attempt to hide cybersecurity incident from customers?",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://doublepulsar.com/oracle-attempt-to-hide-serious-cybersecurity-incident-from-customers-in-oracle-saas-service-9231c8daff4a",
    "score": 605,
    "by": "2bluesc",
    "time": 1743433893,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "0b93898b758048b8",
    "title": {
      "en": "MLB says Yankees’ new “torpedo bats” are legal and likely coming",
      "ko": "양키스의 신무기, 합법 판정!",
      "ja": "ヤンキースの新バット合法化！"
    },
    "type": "story",
    "url": "https://thelibertyline.com/2025/03/30/yankees-new-torpedo-bat/",
    "score": 290,
    "by": "cf100clunk",
    "time": 1743434823,
    "content": "MLB says Yankees’ new Torpedo Bats are legal and likely coming to a dugout near you but are we sure this is what we want?\n\n\tDrew Smith\n    March 30, 2025\nTrending\n2 Comments\n\nYou score 20 runs, hit 9 bombs, and people start asking questions. That’s exactly what happened to the Yankees on Saturday after they revealed the “torpedo bat” during their demolition of the Milwaukee Brewers. Yankees have an MIT Physicist that built them the Torpedo Bat…Yes, the Yankees have a literal genius MIT Physicist, Lenny (who is the man), on payroll. He invented the “Torpedo” barrel. It brings more wood – and mass – to where you most often make contact as a hitter. The idea is to increase the number of “barrels” and decrease misses. pic.twitter.com/CsC1wkAM9G— Kevin Smith (@KJS_4) March 29, 2025 Anthony Volpe and Jazz Chisholm Jr. used it. Both went deep. The bat is thicker in the area where they make the most contact, basically moving the “sweet spot” lower than the typical barrel. Think more mass where it matters. Less guessing, more barreled-up baseballs.I’m not sure if I like where this is going.Remember when kids started coming to the local baseball field with those big-barreled whiffle ball bats instead of your classic yellow ones?I’m not sure making a custom bat with an MIT physicist is the solution to turn Anthony Volpe and Jazz Chisholm Jr. into competent hitters at the plate.Unfortunately, the MLB reviewed the torpedo bats after the game and somehow had zero issues with them? How’s that possible?Welcome to the age of the torpedo bat.The Yankees have new bats today :\"where they moved a lot of the wood into the label so the harder part of the bat is going to strike the ball.\"They tied their franchise record for HRs in a game in only 4 innings pic.twitter.com/nte8YpuH9V— Barstool Sports (@barstoolsports) March 29, 2025 MLB Rule is Cut and Dry: One piece of solid wood, no more than 2.61 inches at the thickest point, max 42 inches long. That’s it. As long as it fits in that box, it’s fair game. The Yankees are just playing smart.We should probably ban the Yankees’ Torpedo Bat, right? The Yankees have apparently been working with Aaron Leanhardt (aka “Lenny”), a former MIT physicist turned baseball brain, who helped re-engineer the traditional wooden bat to better match where individual hitters make the most consistent contact.According to former Yankees minor leaguer Kevin Smith, the physics checks out. You might lose a tick or two of exit velocity, but the improved “barrel percentage” makes up for it. In other words, more solid contact in the spot that matters most.The result? A bat that looks like it was forged in a lab—and kind of was.The Yankees used new bats today that moved the wood to where the ball get hits with the hardest part of the bat 👀They broke a franchise record for most home runs in a game (9) 🤯 pic.twitter.com/9f3CiI810q— DraftKings Sportsbook (@DKSportsbook) March 29, 2025 Instead of the typical bat tapering evenly toward the end, the “torpedo” design fattens up closer to the handle, where players like Volpe and Jazz Chisholm Jr. naturally tend to make contact.Basically, if you stink at baseball then you should probably get a torpedo app from this MIT geek and save your career because the sweet spot just got a whole lot bigger. Outlaw the Torpedo Bat..Baseball traditionalists like myself are having a stroke watching Vlope and Jazz actually make contact with this new bat and rightfully so. Again, you can’t just make a new bat and ruin over 100 years of baseball to allow guys to actually hit and not be a complete mess at the plate.The result is shitty players like Volpe turning around and hitting home runs with what looked like a Louisville Slugger mixed with a bowling pin. MLB Needs to Change This: One piece of solid wood\n\nNo more than 2.61 inches in diameter at the thickest part\n\nNo longer than 42 inchesSure, that’s fair game…I guess. But when the Yankees launched nine home runs—a franchise record, mind you—amid a 20-9 beatdown of Milwaukee, the league should probably take some extra time and better understand what’s going on with this baseball bat and the future impact on the game itself moving forward. The Future of MLB Bats?Lenny, the brains behind the design, once predicted this bat shape would take over the league in 5 to 10 years. After watching Saturday’s offensive explosion? He might want to revise that timeline to 5 to 10 days. Now the real question is: how long until the rest of the league catches on?Because if you can legally swing a bat that turns your weakest hits into loud outs — or better — why wouldn’t you? The Yankees just brought a bat to a knife fight. And unless MLB changes the rules, the “torpedo” era might be just getting started.Join The ChaseLike this:Like Loading...\n\n\t\t\tShare this:TwitterFacebook\n\n\t\t\t\t\tDrew Smith\n\n\t\t\tunfiltered, opinionated, and certainly do not care if you like it or not.\n\n\t\tRead More\n\n\tSad: Giants fans are ready to run through a brick wall after backup quarterback Jameis Winston promises more ‘giggles’ next yearTrendingLook, I love Jameis Winston as much as the next…\n\n\tWATCH: Chicago Cubs Catcher Carson Kelly hits for the most unexpected cycle in Major League Baseball historyTrendingThere are baseball moments you see coming from a mile…\n\n\tWATCH: Mavericks fan screams “fire Nico” directly into the face of the GM who traded Luka DoncicTrendingIf you thought Mavericks fans were going to forget about…\n\nComments (2)\n\n\t\t\t\t\t\t\t\t\t\t\t\tAnonymous says:\n\n\t\t\t\t\t\tMarch 31, 2025 at 4:18 pm\n\n\t\t\t\t\tI don’t care either way, I love rugby and football. That help develop brains and brawn, rather than only one at a time, at a much faster rate. And are not soul crushing to watch unless one do the team is really too far apart than the other one in a few terms.\nM, I also don’t care if you like or dislike my fact based opinion. Because I know what kinda of sports you like.\nCool website, loads fast in 3G so I might add it to my white list so next time. That way you’ll have more information about site visitors.\nAnd consistent and not mentally deranged opinion, which is good 👍\nThe guardian must copy this style for a few crazy people on their team writing absolute rahe bait garbage. Or it’s probably for the best that I’ll send many congratulations to the same writers for being underrated hidden gems to ensure they never improve.\n\nLoading...\n\n\t\t\t\tReply\n\n\t\t\t\t\t\t\t\t\t\t\t\tAnonymous says:\n\n\t\t\t\t\t\tMarch 31, 2025 at 7:46 pm\n\n\t\t\t\t\tMaybe this will make baseball interesting again. It’s such a boring sport. More scoring = maybe I’ll tune in. Otherwise it’s a snooze fest that belongs in 1800 with peanuts and cracker jacks.\n\nLoading...\n\n\t\t\t\tReply\n\n\t\t\tLeave a ReplyCancel reply\n\n\t\t\t\t\t\tdocument.addEventListener('DOMContentLoaded', function () {\n\t\t\t\t\t\t\tvar commentForms = document.getElementsByClassName('jetpack_remote_comment');\n\t\t\t\t\t\t\tfor (var i = 0; i < commentForms.length; i++) {\n\t\t\t\t\t\t\t\tcommentForms[i].allowTransparency = false;\n\t\t\t\t\t\t\t\tcommentForms[i].scrolling = 'no';\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t});",
    "summary": {
      "en": "The New York Yankees have introduced a new baseball bat called the \"torpedo bat,\" which is designed by an MIT physicist named Lenny. This bat has a thicker barrel where players typically make contact, aiming to improve hitting performance by increasing the number of solid hits. During a recent game, players using the torpedo bat scored 20 runs and hit nine home runs, tying a franchise record.\n\nWhile the bat complies with MLB regulations, some are concerned that it alters the traditional nature of the game. Critics argue that it could give an unfair advantage to players and lead to a shift in how baseball is played. There are calls to ban the torpedo bat or at least rethink its impact on the sport's future, especially after witnessing its immediate success with the Yankees. The discussion raises questions about the balance between innovation and preserving the essence of baseball.",
      "ko": "뉴욕 양키스가 MIT 물리학자 레니가 디자인한 새로운 야구 방망이인 \"토르피도 배트\"를 소개했습니다. 이 방망이는 선수들이 공을 치는 부분이 두꺼워져 있어, 더 많은 강한 타구를 만들어내는 데 도움을 주도록 설계되었습니다. 최근 경기에서 토르피도 배트를 사용한 선수들은 20점을 기록하고 아홉 개의 홈런을 쳐서 구단 기록을 타이했습니다.\n\n이 방망이는 MLB 규정을 준수하지만, 일부에서는 전통적인 게임의 성격을 변화시킬 수 있다는 우려를 표하고 있습니다. 비판자들은 이 방망이가 선수들에게 불공정한 이점을 줄 수 있으며, 야구의 플레이 방식에 변화를 초래할 수 있다고 주장합니다. 특히 양키스에서의 즉각적인 성공을 목격한 후, 토르피도 배트를 금지하거나 그 영향에 대해 재고해야 한다는 목소리가 커지고 있습니다. 이러한 논의는 혁신과 야구의 본질을 보존하는 것 사이의 균형에 대한 질문을 제기합니다.",
      "ja": "ニューヨーク・ヤンキースは「トーピードバット」と呼ばれる新しい野球バットを導入しました。このバットはMITの物理学者レニーによって設計されており、選手が通常ボールに当てる部分が太くなっています。これにより、ヒットの数を増やし、打撃パフォーマンスを向上させることを目指しています。最近の試合では、トーピードバットを使用した選手たちが20得点を挙げ、9本のホームランを打ち、フランチャイズ記録に並びました。\n\nこのバットはメジャーリーグベースボール（MLB）の規則に適合していますが、伝統的なゲームの性質を変えてしまうのではないかと懸念する声もあります。批評家たちは、このバットが選手に不公平なアドバンテージを与え、野球のプレースタイルに変化をもたらす可能性があると主張しています。特にヤンキースでの即効性のある成功を目の当たりにした後、トーピードバットの禁止やそのスポーツへの影響を再考する必要があるとの声が上がっています。この議論は、革新と野球の本質を守ることとのバランスについての疑問を提起しています。"
    }
  },
  {
    "id": "96e2576285ebe896",
    "title": {
      "en": "How IMAP works under the hood",
      "ko": "IMAP의 비밀",
      "ja": "IMAPの仕組み解剖"
    },
    "type": "story",
    "url": "https://blog.lohr.dev/imap-introduction",
    "score": 219,
    "by": "michidk",
    "time": 1743239133,
    "content": "[data-rmiz-ghost] {\n      position: absolute;\n      pointer-events: none;\n    }\n    [data-rmiz-btn-zoom],\n    [data-rmiz-btn-unzoom] {\n      background-color: rgba(0, 0, 0, 0.7);\n      border-radius: 50%;\n      border: none;\n      box-shadow: 0 0 1px rgba(255, 255, 255, 0.5);\n      color: #fff;\n      height: 40px;\n      margin: 0;\n      outline-offset: 2px;\n      padding: 9px;\n      touch-action: manipulation;\n      width: 40px;\n      -webkit-appearance: none;\n      -moz-appearance: none;\n      appearance: none;\n    }\n    [data-rmiz-btn-zoom]:not(:focus):not(:active) {\n      position: absolute;\n      clip: rect(0 0 0 0);\n      clip-path: inset(50%);\n      height: 1px;\n      overflow: hidden;\n      pointer-events: none;\n      white-space: nowrap;\n      width: 1px;\n    }\n    [data-rmiz-btn-zoom] {\n      position: absolute;\n      inset: 10px 10px auto auto;\n      cursor: zoom-in;\n    }\n    [data-rmiz-btn-unzoom] {\n      position: absolute;\n      inset: 20px 20px auto auto;\n      cursor: zoom-out;\n      z-index: 1;\n      display: none;\n    }\n    [data-rmiz-content=\"found\"] img,\n    [data-rmiz-content=\"found\"] svg,\n    [data-rmiz-content=\"found\"] [role=\"img\"],\n    [data-rmiz-content=\"found\"] [data-zoom] {\n      cursor: zoom-in;\n    }\n    [data-rmiz-modal]::backdrop {\n      display: none;\n    }\n    [data-rmiz-modal][open] {\n      position: fixed;\n      width: 100vw;\n      width: 100dvw;\n      height: 100vh;\n      height: 100dvh;\n      max-width: none;\n      max-height: none;\n      margin: 0;\n      padding: 0;\n      border: 0;\n      background: transparent;\n      overflow: hidden;\n    }\n    [data-rmiz-modal-overlay] {\n      position: absolute;\n      inset: 0;\n      transition: background-color 0.3s;\n    }\n    [data-rmiz-modal-overlay=\"hidden\"] {\n      background-color: rgba(255, 255, 255, 0);\n    }\n    [data-rmiz-modal-overlay=\"visible\"] {\n      /* This bg color is different from default */\n      background-color: rgba(0, 0, 0, 0.5);\n    }\n    [data-rmiz-modal-content] {\n      position: relative;\n      width: 100%;\n      height: 100%;\n    }\n    [data-rmiz-modal-img] {\n      position: absolute;\n      cursor: zoom-out;\n      image-rendering: high-quality;\n      transform-origin: top left;\n      transition: transform 0.3s;\n      /* This is added additionally to override prose styles of image*/\n      margin: 0 !important;\n    }\n    @media (prefers-reduced-motion: reduce) {\n      [data-rmiz-modal-overlay],\n      [data-rmiz-modal-img] {\n        transition-duration: 0.01ms !important;\n      }\n    }\n<img alt=\"Talking To Your Mailserver Is Not as Hard as You Think!\" decoding=\"async\" data-nimg=\"responsive\" style=\"position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%\" class=\"mb-0 block w-full\" src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1735043663893/3ab04669-58f8-4a59-8bda-2f3f29e5745d.webp?w=1600&amp;h=840&amp;fit=crop&amp;crop=entropy&amp;auto=compress,format&amp;format=webp\"/>Talking To Your Mailserver Is Not as Hard as You Think!Learning how the IMAP protocol actually works<img alt=\"Michael Lohr&#x27;s photo\" loading=\"lazy\" decoding=\"async\" data-nimg=\"intrinsic\" style=\"position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%\" class=\"relative z-20 block w-full rounded-full\" src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1654931409870/hbzPxZhm8.jpg?w=200&amp;h=200&amp;fit=crop&amp;crop=faces&amp;auto=compress,format&amp;format=webp\"/>Michael Lohr·Dec 22, 2024·6 min readThe technology behind E-Mails (or ‘emails‘) always interested me. This is because in some sense it was way ahead of its time by being a decentralized communication system. Unlike WhatsApp, Telegram or other messenger services, where the service provider has a centralized server (or nowadays a network of servers) processing all the messages; with emails, everybody can host their own mailserver to send and receive messages to everybody else. If Meta or Telegram decides to shut down their server, all communication is lost and the service cannot be used anymore. But with email, you can just move to another server (as long as you own your own domain at least).\nOne of the major components that power our modern email systems is the IMAP4 protocol. It is used to connect your email client to your mailserver. It’s the language used by the client to ask the server for the newest unread emails and the server responds with its contents. POP3 is (or was) also very popular and still widely supported but not used so much because it comes with a few drawbacks (a topic for another day). You probably have heard about SMTP as well, and this is the protocol that is used to send emails.\nBut those protocols alone don’t allow us to send emails. Emails heavily rely on other technologies like Domains, DNS, IPv4, SPF, DKIM, DMARC, SSL/TLS, S/MIME, PGP, NTP. In general, Emails are not too different from websites as they share a lot of the same technology (and nowadays you can even send HTML emails).\nHowever, in this blog post, I want to focus on IMAP, which stands for Internet Message Access Protocol. IMAP4 was released in 1994 as RFC 1730. The most recent version is IMAP4rev2 (released in 2021), as defined in RFC 9051 but most mail servers (as far as I can tell) still use IMAP4rev1 as defined in RFC 3501. It can be used to retrieve, manage, and manipulate email messages stored on a remote mail server, allowing users to perform actions such as searching, flagging, deleting, and organizing messages into folders while maintaining synchronization across multiple clients. Similarly, HTTP is a text-based protocol that relies on human-readable text commands and responses for communication between clients and servers.\nSo now, enough introduction - let’s get right into business. How do we connect to a mailserver?\nLike with HTML, we can use the telnet utility to build up a simple TCP connection to a mailserver:\n\nAfter connecting we are greeted with OK followed by a list of ‘capabilities‘, which is a list of features supported by the mailserver in the current state. It also shows us which IMAP version is supported. Sometimes (but not in the screenshot above) the server will also tell you what kind of software it is running. Most of the time you will see “Docevot“ being mentioned. Dovecot is a secure open-source IMAP & POP3 server running on Linux.\nIf we are fast enough and are not getting kicked out of the session because we idled too long, we can try to log in to authenticate ourselves.\nIn order to do that, we have to understand how the IMAP commands are structured.\nCopy<tag> <command> [arguments]\n\nThe tag prefix is supposed to be a unique tag within the session used to match the request and response. It can be anything but has to start with a letter and cannot contain spaces. While it’s supposed to be unique and typically look like A001, I’m lazy so I’ll always use just a. The server will then respond with:\nCopy<tag> <status> <response>\n\nThere are multiple ways to authenticate. The LOGIN command is the most straightforward one but is also considered insecure because it transmits the credentials in plaintext (especially when using unencrypted connections). I will use it anyway 🙃. There is also the AUTHENTICATE command, which uses SASL (Simple Authentication and Security Layer). The most basic way to authenticate using SASL involves encoding the credentials in base64 but can go as far as sending hashes or using token-based authentication.\nNow if I try to log in, the following happens:\nCopya login michael@myemail.com MySecretPassword\na BAD [CLIENTBUG] Login is disabled\n\nOn most other servers, this will actually just work. But on this specific server, unencrypted connections like this are disabled.\nSo instead, let’s use OpenSSL to build up a secure connection and log in. Notice that we now have to use the 993 port instead of 143:\nCopy❯ openssl s_client -connect imap.myemail.com:993 -crlf -quiet\nConnecting to 123.123.123.123\ndepth=2 C=US, O=DigiCert Inc, OU=www.digicert.com, CN=DigiCert Global Root G2\nverify return:1\ndepth=1 C=US, O=DigiCert Inc, OU=www.digicert.com, CN=RapidSSL TLS RSA CA G1\nverify return:1\ndepth=0 CN=imap.myemail.com\nverify return:1\n* OK [CAPABILITY IMAP4rev1 SASL-IR LOGIN-REFERRALS ID ENABLE IDLE LITERAL+ AUTH=PLAIN AUTH=LOGIN AUTH=DIGEST-MD5 AUTH=CRAM-MD5] Dovecot ready.\na login michael@myemail.com MySecretPassword\na OK [CAPABILITY IMAP4rev1 SASL-IR LOGIN-REFERRALS ID ENABLE IDLE SORT SORT=DISPLAY THREAD=REFERENCES THREAD=REFS THREAD=ORDEREDSUBJECT MULTIAPPEND URL-PARTIAL CATENATE UNSELECT CHILDREN NAMESPACE UIDPLUS LIST-EXTENDED I18NLEVEL=1 CONDSTORE QRESYNC ESEARCH ESORT SEARCHRES WITHIN CONTEXT=SEARCH LIST-STATUS BINARY MOVE SNIPPET=FUZZY PREVIEW=FUZZY PREVIEW STATUS=SIZE SAVEDATE LITERAL+ NOTIFY SPECIAL-USE QUOTA] Logged in\n\nFinally, we get an ‘OK‘ and a new list of the now unlocked capabilities (not every server sends us the new capabilities automatically; sometimes you have to issue the CAPABILITY command manually). This should also increase the time we have until we get thrown out of the session for inactivity.\nWe can now list all our folders:\nCopya LIST \"\" \"*\"\n* LIST (\\HasNoChildren \\UnMarked) \"/\" CustomFolder\n* LIST (\\HasNoChildren \\UnMarked \\Archive) \"/\" Archives\n* LIST (\\HasChildren \\Marked \\Trash) \"/\" Trash\n* LIST (\\HasNoChildren \\UnMarked) \"/\" Trash/OldStuff\n* LIST (\\HasNoChildren \\UnMarked \\Drafts) \"/\" Drafts\n* LIST (\\HasNoChildren \\UnMarked \\Sent) \"/\" Sent\n* LIST (\\HasNoChildren \\UnMarked \\Junk) \"/\" Spam\n* LIST (\\HasNoChildren) \"/\" INBOX\na OK List completed (0.018 + 0.000 + 0.017 secs).\n\nEven though the main inbox folder is named INBOX by convention, this helps us to identify the structure of our mailbox and where to look for new mail! We can also see folders marked with flags like \\Sent which IMAP uses for special functionality. Next, we need to select one of those folders:\nCopya SELECT INBOX\n* FLAGS (\\Answered \\Flagged \\Deleted \\Seen \\Draft NonJunk $Forwarded Junk)\n* OK [PERMANENTFLAGS (\\Answered \\Flagged \\Deleted \\Seen \\Draft NonJunk $Forwarded Junk \\*)] Flags permitted.\n* 4183 EXISTS\n* 0 RECENT\n* OK [UIDVALIDITY <secret>] UIDs valid\n* OK [UIDNEXT <secret>] Predicted next UID\n* OK [HIGHESTMODSEQ <secret>] Highest\na OK [READ-WRITE] Select completed (0.007 + 0.000 + 0.006 secs).\n\nAnd this tells us that I have 4183 emails in the inbox!\nNow that we have selected a folder, we can search in it! If our capabilities contain SEARCH=FUZZY we can add the FUZZY keyword to enable a fuzzy search (otherwise just leave it out):\nCopya SEARCH FUZZY SUBJECT \"buy\"\n* SEARCH 1082 1236 1830 3141 3263 4149 7401 10113 10118 10240 18106 18107 18113 18139 18140 18142 18270 18966 19226 20675 20684 20695 20709 26406\na OK SEARCH completed (12.310 s)\n\nAmongst others, you can use BODY to search in the body, SUBJECT to search in the subject or TEXT to search in both. I can then use the FETCH command to display one of those IDs (or any others):\nCopya FETCH 26046 (BODY[])\n* 26046 FETCH (BODY[] {130059}\nX-Envelope-From: <****@email.epicgames.com>\nX-Envelope-To: <michael@myemail.com>\nX-Delivery-Time: 1725393397\nX-UID: <secret>\nReturn-Path: ....\n\nIt starts with the headers and will eventually show the contents further down. This format is called MIME (Multipurpose Internet Mail Extensions) as defined in RFC 2045-2049.\nNow we can search and read emails via the command line without the help of any email client. Of course, we can do a lot more with IMAP but this will get you started. Remember for sending emails we need to use SMTP instead - I might cover this in future as well!\nemailIMAPdevelopmentsoftware developmentShare this",
    "summary": {
      "en": "The text discusses the basics of email technology, focusing on the IMAP (Internet Message Access Protocol). Here are the key points:\n\n1. **Email vs. Messaging Apps**: Email is decentralized, allowing anyone to host their own mail server, unlike messaging apps that rely on central servers.\n\n2. **IMAP Protocol**: IMAP connects email clients to mail servers, enabling users to retrieve and manage emails. It was first introduced in 1994, with the latest version, IMAP4rev2, released in 2021.\n\n3. **Email Communication**: IMAP allows actions such as searching, deleting, and organizing emails while keeping everything synced across devices. Other protocols like SMTP are used for sending emails.\n\n4. **Connecting to a Mail Server**: The text explains how to connect to a mail server using the command line with tools like telnet and OpenSSL for secured connections.\n\n5. **Logging In**: It discusses different authentication methods, highlighting the importance of secure connections to protect user credentials.\n\n6. **Folder Management**: Once logged in, users can list and select email folders, check for the number of emails, and perform searches within their inbox.\n\n7. **Email Retrieval**: Users can retrieve email content using commands that fetch specific emails based on search criteria.\n\nThe document concludes by noting that while this guide covers the basics of IMAP, sending emails involves using SMTP, which may be discussed in future content.",
      "ko": "이 글은 이메일 기술의 기본 개념을 다루며, IMAP(인터넷 메시지 접근 프로토콜)에 초점을 맞추고 있습니다. 이메일은 분산형 시스템으로, 누구나 자신의 메일 서버를 운영할 수 있는 반면, 메시징 앱은 중앙 서버에 의존합니다. \n\nIMAP 프로토콜은 이메일 클라이언트와 메일 서버를 연결하여 사용자가 이메일을 검색하고 관리할 수 있도록 합니다. 이 프로토콜은 1994년에 처음 도입되었으며, 최신 버전인 IMAP4rev2는 2021년에 출시되었습니다. IMAP을 사용하면 이메일을 검색하고 삭제하며 정리하는 등의 작업을 할 수 있으며, 모든 기기에서 동기화가 유지됩니다. 이메일 전송에는 SMTP와 같은 다른 프로토콜이 사용됩니다.\n\n메일 서버에 연결하는 방법은 명령줄을 이용해 telnet과 OpenSSL 같은 도구를 사용하는 것으로 설명됩니다. 이 과정에서 안전한 연결을 통해 사용자 인증 정보를 보호하는 것이 중요하다고 강조합니다. 로그인 후에는 이메일 폴더를 나열하고 선택할 수 있으며, 받은 편지함 내에서 이메일 수를 확인하고 검색할 수 있습니다.\n\n사용자는 특정 검색 기준에 따라 이메일 내용을 가져오는 명령을 사용하여 이메일을 검색할 수 있습니다. 이 가이드는 IMAP의 기본 사항을 다루고 있지만, 이메일 전송에 관한 SMTP는 향후 내용에서 다룰 예정임을 언급하며 마무리됩니다.",
      "ja": "メール技術の基本について、特にIMAP（インターネットメッセージアクセスプロトコル）に焦点を当てた内容です。\n\nメールは分散型であり、誰でも自分のメールサーバーをホストできるのに対し、メッセージアプリは中央サーバーに依存しています。IMAPはメールクライアントとメールサーバーを接続し、ユーザーがメールを取得し管理できるようにします。このプロトコルは1994年に初めて導入され、最新のバージョンであるIMAP4rev2は2021年にリリースされました。\n\nIMAPを使用すると、メールの検索、削除、整理などの操作が可能で、すべてのデバイス間で同期が保たれます。メールを送信するためには、SMTPという別のプロトコルが使用されます。\n\nメールサーバーへの接続方法については、コマンドラインを使用し、telnetやOpenSSLなどのツールを使って安全な接続を行う方法が説明されています。ログイン時には、さまざまな認証方法があり、ユーザーの資格情報を保護するために安全な接続の重要性が強調されています。\n\nログイン後は、ユーザーはメールフォルダを一覧表示し、選択したり、メールの数を確認したり、受信トレイ内で検索を行ったりできます。特定の検索条件に基づいてメールを取得するためのコマンドを使用して、メールの内容を取り出すことも可能です。\n\nこのガイドではIMAPの基本を扱っていますが、メールを送信する際にはSMTPを使用する必要があり、今後の内容でその点についても触れる予定です。"
    }
  },
  {
    "id": "72c1575b544ceb5a",
    "title": {
      "en": "Symmetry between up and down quarks is more broken than expected",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://phys.org/news/2025-03-symmetry-quarks-broken.html",
    "score": 84,
    "by": "terminalbraid",
    "time": 1743243123,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "9051b48d3fecbde8",
    "title": {
      "en": "Discover European alternatives to popular SaaS",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://xwiki.com/en/Blog/European-alternatives-to-SaaS/",
    "score": 3,
    "by": "lorinab",
    "time": 1743521636,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "46195240b650cf62",
    "title": {
      "en": "Ask HN: What are you working on? (March 2025)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": null,
    "score": 359,
    "by": "david927",
    "time": 1743367395,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "dfeb07f46543fd71",
    "title": {
      "en": "Debugging Lotus 1-2-3 by fax",
      "ko": "팩스로 디버깅하는 로터스 1-2-3",
      "ja": "ファックスでデバッグ！"
    },
    "type": "story",
    "url": "https://blog.jgc.org/2025/03/debugging-lotus-1-2-3-by-fax.html",
    "score": 88,
    "by": "jgrahamc",
    "time": 1743434630,
    "content": "John Graham-Cumming's blog\n\n2025-03-31\n\nDebugging Lotus 1-2-3 by fax\n\nThere isn't a lot to this story beyond the fact that in around 1990 I helped debug someone's Lotus 1-2-3 set up via fax. But it's a good reminder of how important the Zeroth Law of Debugging is (see below).Without some sort of online connection with these folks, and with transatlantic phone calls being very, very expensive (I was in the UK, they were in the US) fax was the obvious answer.I was reminded of this when I came across the actual fax itself:What I asked them to do was give me the output of /ppomr. Hitting / in 1-2-3 started the menu system and each letter took you down a submenu until you hit an actual menu item. /ppomr is /Print Printer Options Margins Right (read all about it in the Quick Reference) and so I was asking for the size of the right margin when printing.Here's what that looks like in action:And then I instructed the person on the other end to alter the left margin with /ppoml(/Print Printer Options Margins Left). I also told them if they wanted a change to the page headers I'd need to \"send them a disc\" (i.e. mail a floppy disc to the US!)If you are going to debug anything you need to tighten the loop between trying something and observing the output of your change. This should be the Zeroth Law of Debugging: get the smallest/fastest test case so you can iterate.Worst case send faxes across the Atlantic.\n\nat\n\nMarch 31, 2025\n\nEmail ThisBlogThis!Share to XShare to FacebookShare to Pinterest\n\nLabels:\nretro\n\n3 comments:\n\nAdam\nsaid...\n\nIsn't a fax a transatlantic phone call? :⁠,⁠-⁠)\n\n6:35 PM\n\nJohn Graham-Cumming\nsaid...\n\nYes, but considerably cheaper than calling someone and have to talk to them with all the associated pleasantries and misunderstandings.\n\n6:36 PM\n\nMajid Sadr\nsaid...\n\nNicely described. :)\n\n9:27 PM\n\nPost a Comment\n\nOlder Post\n\nHome\n\nSubscribe to:\nPost Comments (Atom)\n\nSearch This Blog\n\nLabels\n\npseudo-randomness\n\nhardware\n\nbabbage\n\nanti-spam\n\ngnu make\n\nretro\n\nsecurity\n\ncodes and ciphers\n\nthe geek atlas\n\nmathematics\n\nminitel\n\nbehind the screens\n\npopfile\n\nprivacy\n\nradio\n\nPopular Posts\n\nDebugging Lotus 1-2-3 by fax\nThere isn't a lot to this story beyond the fact that in around 1990 I helped debug someone's Lotus 1-2-3  set up via fax. But it...\n\nPimping my Casio with Oddly Specific Objects' alternate motherboard and firmware\nSome time ago I bought a replacement motherboard  for my classic Casio F-91W from Crowd Supply. The project keeps the original Casio LCD but...\n\nYour last name contains invalid characters\nMy last name is \"Graham-Cumming\".  But here's a typical form response when I enter it: Does the web site have any idea how rud...\n\nThe original WWW proposal is a Word for Macintosh 4.0 file from 1990, can we open it?\nThe W3C has a page with the original WWW proposal  from Tim Berners-Lee. One of the downloads says The original document file (I think - I ...\n\nProgrammer\nRecently, I joined a new company called CloudFlare  where I was asked to pick a job title.  I decided to go with Programmer.  It was the fir...\n\nCracking an old ZIP file to help open source the ANC's \"Operation Vula\" secret crypto code\nIt's not often that you find yourself staring at code that few people have ever seen, code that was an important part in bringing down t...\n\nControlling the Taylor Swift Eras Tour wristbands with Flipper Zero\nMany large concerts feature wristbands that light up on command. They are used to produce varied visual effects across a stadium. One compan...\n\nI have come to loathe acronyms\nUnless they are widely, widely known they make communication worse. And they have a tendency to make the writer seem pompous, or a member of...\n\nGetting the KIM-1 to talk to my Mac\nI've written before about my 1976 KIM-1  and code I wrote  for a similar one long ago. But I hadn't done much with the KIM-1 and str...\n\nArchiving hardware projects\nFrom time to time I do some project involving old hardware that requires connecting it to a modern computer. For example,   Getting the K...\n\nBlog Archive\n\n        ▼\n\n2025\n\n(5)\n\n        ▼\n\nMarch\n\n(1)\n\nDebugging Lotus 1-2-3 by fax\n\n        ►\n\nFebruary\n\n(4)\n\n        ►\n\n2024\n\n(19)\n\n        ►\n\nDecember\n\n(3)\n\n        ►\n\nNovember\n\n(1)\n\n        ►\n\nOctober\n\n(1)\n\n        ►\n\nSeptember\n\n(3)\n\n        ►\n\nJune\n\n(3)\n\n        ►\n\nMay\n\n(1)\n\n        ►\n\nApril\n\n(1)\n\n        ►\n\nMarch\n\n(3)\n\n        ►\n\nFebruary\n\n(2)\n\n        ►\n\nJanuary\n\n(1)\n\n        ►\n\n2023\n\n(30)\n\n        ►\n\nDecember\n\n(2)\n\n        ►\n\nNovember\n\n(5)\n\n        ►\n\nOctober\n\n(3)\n\n        ►\n\nSeptember\n\n(4)\n\n        ►\n\nAugust\n\n(1)\n\n        ►\n\nJuly\n\n(5)\n\n        ►\n\nJune\n\n(1)\n\n        ►\n\nMay\n\n(3)\n\n        ►\n\nApril\n\n(3)\n\n        ►\n\nMarch\n\n(3)\n\n        ►\n\n2022\n\n(24)\n\n        ►\n\nDecember\n\n(2)\n\n        ►\n\nNovember\n\n(3)\n\n        ►\n\nOctober\n\n(8)\n\n        ►\n\nSeptember\n\n(1)\n\n        ►\n\nJuly\n\n(2)\n\n        ►\n\nJune\n\n(2)\n\n        ►\n\nApril\n\n(2)\n\n        ►\n\nMarch\n\n(4)\n\n        ►\n\n2021\n\n(6)\n\n        ►\n\nNovember\n\n(1)\n\n        ►\n\nJuly\n\n(1)\n\n        ►\n\nMay\n\n(2)\n\n        ►\n\nApril\n\n(1)\n\n        ►\n\nJanuary\n\n(1)\n\n        ►\n\n2020\n\n(2)\n\n        ►\n\nJune\n\n(2)\n\n        ►\n\n2018\n\n(2)\n\n        ►\n\nDecember\n\n(2)\n\n        ►\n\n2017\n\n(4)\n\n        ►\n\nMay\n\n(1)\n\n        ►\n\nApril\n\n(3)\n\n        ►\n\n2016\n\n(11)\n\n        ►\n\nNovember\n\n(1)\n\n        ►\n\nSeptember\n\n(1)\n\n        ►\n\nJuly\n\n(2)\n\n        ►\n\nJune\n\n(1)\n\n        ►\n\nMay\n\n(3)\n\n        ►\n\nApril\n\n(1)\n\n        ►\n\nMarch\n\n(2)\n\n        ►\n\n2015\n\n(11)\n\n        ►\n\nNovember\n\n(1)\n\n        ►\n\nJuly\n\n(2)\n\n        ►\n\nMay\n\n(1)\n\n        ►\n\nApril\n\n(5)\n\n        ►\n\nMarch\n\n(1)\n\n        ►\n\nJanuary\n\n(1)\n\n        ►\n\n2014\n\n(4)\n\n        ►\n\nNovember\n\n(2)\n\n        ►\n\nJuly\n\n(1)\n\n        ►\n\nJune\n\n(1)\n\n        ►\n\n2013\n\n(39)\n\n        ►\n\nSeptember\n\n(4)\n\n        ►\n\nAugust\n\n(1)\n\n        ►\n\nJuly\n\n(4)\n\n        ►\n\nJune\n\n(2)\n\n        ►\n\nMay\n\n(5)\n\n        ►\n\nApril\n\n(12)\n\n        ►\n\nMarch\n\n(3)\n\n        ►\n\nFebruary\n\n(2)\n\n        ►\n\nJanuary\n\n(6)\n\n        ►\n\n2012\n\n(77)\n\n        ►\n\nDecember\n\n(3)\n\n        ►\n\nNovember\n\n(6)\n\n        ►\n\nOctober\n\n(6)\n\n        ►\n\nSeptember\n\n(7)\n\n        ►\n\nAugust\n\n(6)\n\n        ►\n\nJuly\n\n(6)\n\n        ►\n\nJune\n\n(8)\n\n        ►\n\nMay\n\n(4)\n\n        ►\n\nApril\n\n(9)\n\n        ►\n\nMarch\n\n(7)\n\n        ►\n\nFebruary\n\n(9)\n\n        ►\n\nJanuary\n\n(6)\n\n        ►\n\n2011\n\n(79)\n\n        ►\n\nDecember\n\n(5)\n\n        ►\n\nNovember\n\n(7)\n\n        ►\n\nOctober\n\n(2)\n\n        ►\n\nSeptember\n\n(5)\n\n        ►\n\nAugust\n\n(2)\n\n        ►\n\nJuly\n\n(8)\n\n        ►\n\nJune\n\n(10)\n\n        ►\n\nMay\n\n(6)\n\n        ►\n\nApril\n\n(9)\n\n        ►\n\nMarch\n\n(6)\n\n        ►\n\nFebruary\n\n(12)\n\n        ►\n\nJanuary\n\n(7)\n\n        ►\n\n2010\n\n(95)\n\n        ►\n\nDecember\n\n(13)\n\n        ►\n\nNovember\n\n(9)\n\n        ►\n\nOctober\n\n(16)\n\n        ►\n\nSeptember\n\n(19)\n\n        ►\n\nAugust\n\n(2)\n\n        ►\n\nJuly\n\n(9)\n\n        ►\n\nJune\n\n(12)\n\n        ►\n\nMay\n\n(1)\n\n        ►\n\nApril\n\n(1)\n\n        ►\n\nMarch\n\n(4)\n\n        ►\n\nFebruary\n\n(2)\n\n        ►\n\nJanuary\n\n(7)\n\n        ►\n\n2009\n\n(31)\n\n        ►\n\nDecember\n\n(1)\n\n        ►\n\nNovember\n\n(8)\n\n        ►\n\nOctober\n\n(3)\n\n        ►\n\nSeptember\n\n(6)\n\n        ►\n\nAugust\n\n(9)\n\n        ►\n\nJune\n\n(1)\n\n        ►\n\nMay\n\n(1)\n\n        ►\n\nJanuary\n\n(2)\n\n        ►\n\n2008\n\n(19)\n\n        ►\n\nDecember\n\n(2)\n\n        ►\n\nSeptember\n\n(1)\n\n        ►\n\nJune\n\n(3)\n\n        ►\n\nMay\n\n(3)\n\n        ►\n\nMarch\n\n(2)\n\n        ►\n\nFebruary\n\n(6)\n\n        ►\n\nJanuary\n\n(2)\n\n        ►\n\n2007\n\n(34)\n\n        ►\n\nDecember\n\n(2)\n\n        ►\n\nNovember\n\n(3)\n\n        ►\n\nOctober\n\n(5)\n\n        ►\n\nAugust\n\n(2)\n\n        ►\n\nJuly\n\n(2)\n\n        ►\n\nJune\n\n(4)\n\n        ►\n\nMay\n\n(3)\n\n        ►\n\nApril\n\n(1)\n\n        ►\n\nMarch\n\n(6)\n\n        ►\n\nFebruary\n\n(4)\n\n        ►\n\nJanuary\n\n(2)\n\n        ►\n\n2006\n\n(25)\n\n        ►\n\nDecember\n\n(2)\n\n        ►\n\nNovember\n\n(2)\n\n        ►\n\nOctober\n\n(3)\n\n        ►\n\nSeptember\n\n(5)\n\n        ►\n\nJuly\n\n(1)\n\n        ►\n\nJune\n\n(1)\n\n        ►\n\nApril\n\n(6)\n\n        ►\n\nFebruary\n\n(1)\n\n        ►\n\nJanuary\n\n(4)\n\n        ►\n\n2005\n\n(3)\n\n        ►\n\nDecember\n\n(1)\n\n        ►\n\nNovember\n\n(2)\n\nCopyright (c) 1999-2025 John Graham-Cumming. Awesome Inc. theme. Powered by Blogger.",
    "summary": {
      "en": "In a blog post dated March 31, 2025, John Graham-Cumming shares a story from around 1990 when he helped debug a Lotus 1-2-3 setup via fax. Due to expensive long-distance calls between the UK and the US, fax became the practical choice for communication. He recalls asking the user to provide printer settings using specific commands. He emphasizes the importance of the \"Zeroth Law of Debugging,\" which suggests that to debug effectively, you should minimize the time between making changes and observing results. In this case, sending faxes was the best option for quick feedback across the Atlantic.",
      "ko": "2025년 3월 31일에 작성된 블로그 포스트에서 존 그레이엄-커밍은 1990년경의 이야기를 공유합니다. 그는 당시 팩스를 통해 로터스 1-2-3 설정을 디버깅하는 데 도움을 주었습니다. 영국과 미국 간의 비싼 장거리 통화 때문에 팩스가 실용적인 소통 수단이 되었습니다. 그는 사용자에게 특정 명령어를 사용해 프린터 설정을 제공해 달라고 요청했던 기억을 떠올립니다. 그는 \"디버깅의 제로 법칙\"의 중요성을 강조하는데, 이 법칙은 효과적으로 디버깅을 하려면 변경 사항을 적용한 후 결과를 관찰하는 시간 간격을 최소화해야 한다고 말합니다. 이 경우, 대서양을 가로지르는 빠른 피드백을 위해 팩스를 보내는 것이 최선의 선택이었습니다.",
      "ja": "2025年3月31日付のブログ記事で、ジョン・グラハム・カミングは1990年頃のエピソードを紹介しています。当時、彼はファックスを使ってLotus 1-2-3の設定をデバッグする手助けをしました。イギリスとアメリカの間の長距離電話が高額だったため、ファックスが実用的なコミュニケーション手段となりました。彼は、ユーザーに特定のコマンドを使ってプリンターの設定を提供するように頼んだことを思い出しています。彼は「デバッグのゼロス法則」の重要性を強調しています。この法則は、効果的にデバッグするためには、変更を加えてから結果を観察するまでの時間を最小限にするべきだと示唆しています。このケースでは、大西洋を越えて迅速なフィードバックを得るためにファックスを送ることが最良の選択でした。"
    }
  },
  {
    "id": "265270cc18175dc3",
    "title": {
      "en": "An e-bike that charges off USB-C",
      "ko": "USB-C로 충전하는 전기 자전거",
      "ja": "USB-C充電のeバイク"
    },
    "type": "story",
    "url": "https://www.theverge.com/news/639681/usb-c-charging-e-bike-ampler-nova-specs-price",
    "score": 16,
    "by": "thunderbong",
    "time": 1743505963,
    "content": "NewsFinally, an e-bike that charges off USB-CA fast three hours means the Ampler Nova is no April fool.A fast three hours means the Ampler Nova is no April fool.by  Thomas RickerApr 1, 2025, 3:00 PM GMT+9LinkFacebookThreadsUp to 140W off a USB-C PD 3.1 laptop charger you might already own. Image: AmplerThomas Ricker is a deputy editor and Verge co-founder with a passion for human-centric cities, e-bikes, and life as a digital nomad. He’s been a tech journalist for almost 20 years.Ampler made one of the first modern e-bikes I ever tested, and now it’s selling the first commercial electric bikes — the Nova and Nova Pro — I’m aware of that can be charged over a USB-C port integrated right into the frame. Hell, that same jack will even charge your gadgets in a pinch, but not in the US because this e-bike is for Europe only.Estonia-based Ampler is best known for making reliable direct-to-consumer e-bikes from its home in Tallinn using decent, mostly off-the-shelf parts that can be serviced at any local bike shop. Adding USB-C charging from a laptop charger you might already own makes a ton of sense as Europe standardizes on the port to eliminate redundancy and reduce waste. It also makes it easier to find a compatible bike charger if you leave yours at home.You can charge the relatively small 48V 336Wh integrated batteries (removable for service only) on the Nova series e-bikes from zero to full in as little as three hours from a USB-C PD 3.1 charger operating at up to 140W. That’s very respectable. Some e-bikes fitted with a battery this same size can take five hours or longer to refill from chargers with proprietary connectors, and others fitted with larger batteries and 300-plus-watt charging bricks can bring that down to around two hours. Fortunately, 240W USB-C chargers are now starting to ship, so USB-C is well in position to compete.PreviousNext1/10Ampler has also tested its bikes to work with less capable Ikea and MacBook chargers that support the older USB-C PD 3.0 spec. If you don’t already have such a charger, Ampler will sell you its own 140W USB-C PD 3.1 charger for €80 (about $87). And while the USB-C port is bidirectional on the Nova series, it’s only capable of charging your USB-C gadgets at a modest 15W. Both the €2,990 (about $3,230) Nova and €3,490 (about $3,770) Nova Pro are available in step-through and step-over frames in three sizes to accommodate most riders, with EU standard 250W motors maxing out at a basic 25km/h. They’re essentially improved Stout and Stella models designed to be more compatible with accessories like U-locks, third-party saddles, and child carriers. The base Nova model is fitted with wide 27.5-inch tires and depends on an oily chain and derailleur for its nine-speed transmission. The Nova Pro with single-speed Gates Carbon belt drive is the bike I’d opt for in flat Amsterdam, instead of the 10-speed Shimano Deore chain-driven system.Both the Nova and Nova Pro are available to preorder starting today in the UK, EU, and Switzerland, with shipping set to begin in June. Ampler is backed by official service centers in Germany and a network of “Ampler Friendly” workshops scattered around Europe. Each bike comes with a two-year warranty and a 14-day return policy.Correction, April 1st: An earlier version of this article said the Nova Pro had the option for a belt-driven 10-speed hub, but the choice is between a single-speed belt drive or traditional 10-speed chain and derailleur. See More: Electric BikesNewsRideablesTranspoMost PopularMost PopularNintendo has moved beyond specsiOS 18.4 is out now with Apple Intelligence-powered priority notificationsFinally, an e-bike that charges off USB-CThe 50 best things Microsoft has ever madeChatGPT’s improved image generation is now available for freeInstallerA weekly newsletter by David Pierce designed to tell you everything you need to download, watch, read, listen to, and explore that fits in The Verge’s universe.Email (required)Sign UpBy submitting your email, you agree to our Terms and Privacy Notice. This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply.Advertiser Content FromThis is the title for the native ad",
    "summary": {
      "en": "The Ampler Nova is a new e-bike that can be charged using a USB-C port, allowing it to recharge in just three hours with a compatible laptop charger. This bike is designed for the European market and is made by Ampler, a company known for producing reliable e-bikes from Estonia. \n\nThe Nova features a 48V 336Wh battery that is removable for servicing, and it can also charge other devices at a lower power. There are two models available: the standard Nova and the upgraded Nova Pro, which come in different frame styles and sizes to fit various riders. The bikes have a maximum speed of 25 km/h and are equipped with either a nine-speed chain or a single-speed belt drive.\n\nPre-orders for the Nova and Nova Pro are now open in the UK, EU, and Switzerland, with shipping expected to start in June. Each bike includes a two-year warranty. Ampler also offers support through service centers and workshops across Europe.",
      "ko": "암플러 노바는 USB-C 포트를 통해 충전할 수 있는 새로운 전기 자전거로, 호환 가능한 노트북 충전기를 사용하면 단 3시간 만에 충전이 가능합니다. 이 자전거는 유럽 시장을 위해 설계되었으며, 에스토니아에서 신뢰할 수 있는 전기 자전거를 생산하는 암플러라는 회사에서 제작했습니다.\n\n노바는 48V 336Wh의 배터리를 갖추고 있으며, 이 배터리는 서비스 목적으로 분리할 수 있습니다. 또한, 낮은 전력으로 다른 기기를 충전할 수도 있습니다. 노바는 기본 모델과 업그레이드된 노바 프로 모델 두 가지가 있으며, 다양한 라이더에 맞춰 서로 다른 프레임 스타일과 크기로 제공됩니다. 이 자전거의 최대 속도는 시속 25km이며, 9단 체인 또는 단일 속도 벨트 드라이브로 장착되어 있습니다.\n\n노바와 노바 프로의 사전 주문이 현재 영국, 유럽연합, 스위스에서 가능하며, 배송은 6월부터 시작될 예정입니다. 각 자전거에는 2년 보증이 포함되어 있습니다. 암플러는 유럽 전역에 서비스 센터와 워크숍을 통해 지원을 제공합니다.",
      "ja": "Ampler Novaは新しい電動自転車で、USB-Cポートを使って充電できるため、対応するノートパソコンの充電器を使えばわずか3時間で充電が完了します。この自転車はヨーロッパ市場向けに設計されており、エストニアの信頼性の高い電動自転車を製造する会社、Amplerによって作られています。\n\nNovaは48V 336Whのバッテリーを搭載しており、取り外し可能なのでメンテナンスが簡単です。また、他のデバイスを低出力で充電することもできます。モデルは標準のNovaとアップグレード版のNova Proの2種類があり、さまざまなライダーに合わせた異なるフレームスタイルとサイズがあります。最高速度は時速25キロメートルで、9段変速のチェーンまたは単速のベルトドライブが装備されています。\n\nNovaとNova Proの予約注文は現在、イギリス、EU、スイスで受け付けており、発送は6月から始まる予定です。各自転車には2年間の保証が付いています。Amplerはヨーロッパ各地にサービスセンターやワークショップを通じてサポートも提供しています。"
    }
  },
  {
    "id": "43d9b81710a37e92",
    "title": {
      "en": "Kagi for Kids",
      "ko": "어린이를 위한 카기",
      "ja": "子ども向けカギ"
    },
    "type": "story",
    "url": "https://help.kagi.com/kagi/plans/family-plan.html#kidslogin",
    "score": 188,
    "by": "ryanjamurphy",
    "time": 1743446850,
    "content": "Main Navigation KagiOrionAppearance",
    "summary": {
      "en": "It seems like you provided a fragment that doesn't contain any detailed information to summarize. If you can share more context or a longer text, I'd be happy to help summarize it for you!",
      "ko": "제공하신 내용은 요약할 수 있는 구체적인 정보가 포함되어 있지 않은 것 같습니다. 더 많은 맥락이나 긴 텍스트를 공유해 주시면 기꺼이 요약해 드리겠습니다!",
      "ja": "詳細な情報が含まれていない断片を提供されたようです。もっと具体的な文脈や長めのテキストを共有していただければ、喜んで要約のお手伝いをいたします。"
    }
  },
  {
    "id": "29a7e4b4531b80f8",
    "title": {
      "en": "Don’t let an LLM make decisions or execute business logic",
      "ko": "LLM에 맡기지 마세요!",
      "ja": "LLMに任せるな！"
    },
    "type": "story",
    "url": "https://sgnt.ai/p/hell-out-of-llms/",
    "score": 297,
    "by": "petesergeant",
    "time": 1743474880,
    "content": "Get the hell out of the LLM as soon as possible   April 1, 2025\n\nDon’t let an LLM make decisions or execute business logic: they suck at that. I build NPCs for an online game, and I get asked a lot “How did you get ChatGPT to do that?” The answer is invariably: “I didn’t, and also you shouldn’t”.\nIn most applications, the LLM should be the user-interface only between the user and an API into your application logic. The LLM shouldn’t be executing any logic. Get the hell out of the LLM as soon as possible, and stay out as long as you can.\nY Tho?\nThis is best illustrated by a contrived example: you want to write a chess-playing bot you access over WhatsApp. The user sends a description of what they want to do (“use my bishop to take the knight”), and the bot plays against them.\nCould you get the LLM to be in charge of maintaining the state of the chess board and playing convincingly? Possibly, maybe. Would you? Hell no, for some intuitive reasons:\n\nPerformance: It’s impressive that LLMs might be able to play chess at all, but they suck at it (as of 2025-04-01). A specialized chess engine is always going to be a faster, better, cheaper chess player. Even modern chess engines like Stockfish that incorporate neural networks are still purpose-built specialized systems with well-defined inputs and evaluation functions - not general-purpose language models trying to maintain game state through text.\nDebugging and adjusting: It’s impossible to reason about and debug why the LLM made a given decision, which means it’s very hard to change how it makes those decisions if you need to tweak them. You don’t understand the journey it took through the high-dimensional semantic space to get to your answer, and it’s really poor at explaining it too. Even purpose-built neural networks like those in chess engines can be challenging for observability, and a general LLM is a nightmare, despite Anthropic’s great strides in this area\nAnd the rest…: testing LLM outputs is much harder than unit-testing known code-paths; LLMs are much worse at math than your CPU; LLMs are insufficiently good at picking random numbers; version-control and auditing becomes much harder; monitoring and observability gets painful; state management through natural language is fragile; you’re at the mercy of API rate limits and costs; and security boundaries become fuzzy when everything flows through prompts.\n\nExamples\nThe chess example illustrates the fundamental problem with using LLMs for core application logic, but this principle extends far beyond games. In any domain where precision, reliability, and efficiency matter, you should follow the same approach:\n\nThe user says they want to attack player X with their vorpal sword? The LLM shouldn’t be the system figuring out is the user has a vorpal sword, or what the results of that would be: the LLM is responsible for translating the free-text the user gave you into an API call only and translating the result into text for the user\nYou’re building a negotiation agent that should respond to user offers? The LLM isn’t in charge of the negotiation, just in charge of packaging it up, passing it off to the negotiating engine, and telling the user about the result\nYou need to make a random choice about how to respond to the user? The LLM doesn’t get to choose\n\nReminder of what LLMs are good at\nWhile I’ve focused on what LLMs shouldn’t do, it’s equally important to understand their strengths so you can leverage them appropriately:\nLLMs excel at transformation and at categorization, and have a pretty good grounding in “how the world works”, and this is where you in your process you should be deploying them.\nThe LLM is good at taking “hit the orc with my sword” and turning it into attack(target=\"orc\", weapon=\"sword\"). Or taking {\"error\": \"insufficient_funds\"} and turning it into “You don’t have enough gold for that.”\nThe LLM is good at figuring out what the hell the user is trying to do and routing it to the right part of your system. Is this a combat command? An inventory check? A request for help?\nFinally, the LLM is good at knowing about human concepts, and knowing that a “blade” is probably a sword and “smash” probably means attack.\nNotice that all these strengths involve transformation, interpretation, or communication—not complex decision-making or maintaining critical application state. By restricting LLMs to these roles, you get their benefits without the pitfalls described earlier.\nThe future\nWhat LLMs can and can’t do is ever-shifting and reminds me of the “God of the gaps”. a term from theology where each mysterious phenomenon was once explained by divine intervention—until science filled that gap. Likewise, people constantly identify new “human-only” tasks to claim that LLMs aren’t truly intelligent or capable. Then, just a few months later, a new model emerges that handles those tasks just fine, forcing everyone to move the goalposts again, examples passim. It’s a constantly evolving target, and what seems out of reach today may be solved sooner than we expect.\nAnd so like in our chess example, we will probably soon end up with LLMs that can handle all of our above examples reasonably well. I suspect however that most of the drawbacks won’t go away: your non-LLM logic that you pass off to is going to be easier to reason about, easier to maintain, cheaper to run, and more easily version-controlled.\nEven as LLMs continue to improve, the fundamental architectural principle remains: use LLMs for what they’re best at—the interface layer—and rely on purpose-built systems for your core logic.    Share",
    "summary": {
      "en": "**Summary of \"Get the hell out of the LLM as soon as possible\"**\n\nThe author advises against using Large Language Models (LLMs) for decision-making or executing business logic because they are not reliable for those tasks. Instead, LLMs should be used primarily as a user interface to translate user requests into API calls, while the actual logic should be handled by specialized systems.\n\nKey points include:\n\n1. **Performance Issues**: LLMs are generally slower and less effective than specialized systems (e.g., chess engines) for tasks requiring precision.\n2. **Debugging Challenges**: It's difficult to understand or modify the decisions made by LLMs, making it hard to troubleshoot issues.\n3. **Complexity in Testing**: Testing LLM outputs is more complicated than traditional code, and they struggle with tasks like math and randomness.\n4. **Application Examples**: In various scenarios, LLMs should only translate user input into structured commands rather than determine game states or manage negotiations.\n5. **Strengths of LLMs**: They are good at interpreting user requests, transforming commands, and understanding concepts, but not at complex decision-making or logic management.\n\nThe author concludes that while LLMs may improve over time, it’s best to keep them as interfaces and rely on dedicated systems for core logic to ensure reliability and maintainability.",
      "ko": "저자는 대규모 언어 모델(LLM)을 의사결정이나 비즈니스 로직 실행에 사용하는 것을 권장하지 않습니다. LLM은 이러한 작업에 신뢰할 수 없기 때문입니다. 대신 LLM은 사용자 요청을 API 호출로 변환하는 사용자 인터페이스로 주로 사용해야 하며, 실제 로직은 전문 시스템이 처리해야 합니다.\n\n주요 내용은 다음과 같습니다. 첫째, 성능 문제입니다. LLM은 정밀도가 요구되는 작업에서 전문 시스템(예: 체스 엔진)보다 일반적으로 느리고 효과적이지 않습니다. 둘째, 디버깅의 어려움입니다. LLM이 내린 결정을 이해하거나 수정하기 어려워 문제를 해결하기 힘듭니다. 셋째, 테스트의 복잡성입니다. LLM의 출력을 테스트하는 것은 전통적인 코드보다 더 복잡하며, 수학 문제나 무작위성 같은 작업에서 어려움을 겪습니다. 넷째, 적용 사례입니다. 다양한 상황에서 LLM은 사용자 입력을 구조화된 명령으로 변환하는 데만 사용해야 하며, 게임 상태를 결정하거나 협상을 관리하는 데는 사용하지 않아야 합니다. 마지막으로 LLM의 강점은 사용자 요청을 해석하고 명령을 변환하며 개념을 이해하는 데 뛰어나지만, 복잡한 의사결정이나 로직 관리에는 적합하지 않습니다.\n\n저자는 LLM이 시간이 지남에 따라 개선될 수 있지만, LLM을 인터페이스로 유지하고 핵심 로직은 전담 시스템에 의존하는 것이 신뢰성과 유지 관리 측면에서 최선이라고 결론짓습니다.",
      "ja": "著者は、大規模言語モデル（LLM）を意思決定やビジネスロジックの実行に使用することに反対しています。これらのモデルは、そのようなタスクに対して信頼性が低いためです。LLMは主にユーザーインターフェースとして利用し、ユーザーのリクエストをAPI呼び出しに変換する役割を果たすべきであり、実際のロジックは専門のシステムが担当するべきだと述べています。\n\n主なポイントは以下の通りです。まず、LLMは一般的に、精度が求められるタスクにおいて専門のシステム（例えば、チェスエンジン）よりも遅く、効果が薄いというパフォーマンスの問題があります。次に、LLMが下した決定を理解したり修正したりするのが難しく、トラブルシューティングが困難になるというデバッグの課題があります。また、LLMの出力をテストするのは従来のコードよりも複雑で、数学やランダム性に関するタスクに苦労することがあります。\n\n具体的なアプリケーションの例としては、さまざまなシナリオにおいて、LLMはユーザーの入力を構造化されたコマンドに変換することにとどまり、ゲームの状態を判断したり交渉を管理したりするべきではないとされています。LLMの強みは、ユーザーのリクエストを解釈し、コマンドを変換し、概念を理解することにありますが、複雑な意思決定やロジックの管理には向いていません。\n\n著者は、LLMが将来的に改善される可能性があるものの、インターフェースとしての役割に留め、コアロジックには専用のシステムを利用することが信頼性と保守性を確保するために最良であると結論づけています。"
    }
  },
  {
    "id": "08f128bb131677c8",
    "title": {
      "en": "Gemini 2.5 Pro vs. Claude 3.7 Sonnet: Coding Comparison",
      "ko": "제미니 2.5 vs 클로드 3.7: 코딩 대결",
      "ja": "双子2.5 Pro vs. クロード3.7 コード対決"
    },
    "type": "story",
    "url": "https://composio.dev/blog/gemini-2-5-pro-vs-claude-3-7-sonnet-coding-comparison/",
    "score": 451,
    "by": "mraniki",
    "time": 1743422989,
    "content": "Gemini 2.5 Pro vs. Claude 3.7 Sonnet: Coding ComparisonShrijalMarch 30, 2025 Claude, Gemini, LLM8 min read\n\nGoogle just launched Gemini 2.5 Pro on March 26th, claiming to be the best in coding, reasoning and overall everything. But I mostly care about how the model compares against the best available coding model, Claude 3.7 Sonnet (thinking), released at the end of February, which I have been using, and it has been a great experience.\n\nLet’s compare these two coding models and see if I need to change my favourite coding model or if Claude 3.7 still holds.\n\nTL;DR\n\nIf you want to jump straight to the conclusion, I’d say go for Gemini 2.5 Pro, it’s better at coding, has one million in context window as compared to Claude’s 200k, and you can get it for free (a big plus). However, Claude’s 3.7 Sonnet is not that far behind. Though at this point there’s no point using it over Gemini 2.5 Pro.\n\nJust an article ago, Claude 3.7 Sonnet was the default answer to every model comparison, and this remained the same for quite some time. But here you go, Gemini 2.5 Pro takes the lead.\n\nBrief on Gemini 2.5 Pro\n\nGemini 2.5 Pro, an experimental thinking model, became the talk of the town within a week of its release. Everyone’s talking about this model on Twitter (X) and YouTube. It’s trending everywhere, like seriously. The first model from Google to receive such fanfare.\n\nAnd it is #1 in the LMArena just like that. But what does this mean? It means that this model is killing all the other models in coding, math, Science, Image understanding, and other areas.\n\nGemini 2.5 pro comes with a 1 million token context window, with a 2 million context window coming soon. 🤯\n\nYou can check out other folks like Theo-t3 talking about this model to get a bit more insight into it:\n\nIt is the best coding model to date, with an accuracy of about 63.8% on the SWE bench. This is definitely higher than our previous top coding model, Claude 3.7 Sonnet, which had an accuracy of about 62.3%.\n\nThis is a quick demo Google shared on this model of building a dinosaur game.\n\nHere’s a quick benchmark of this model on Reasoning, Mathematics, and Science. This confirms that the model is not just suitable for coding but also for all your other needs. They claim it’s an all-rounder. 🤷‍♂️\n\nThis is all cool, and I’ll confirm the claim, but in this article, I will mainly be comparing the model on coding, and let’s see how well it performs compared to Claude 3.7 Sonnet.\n\nCoding Problems\n\nLet’s compare these two models in coding. We’ll do a total of 4 tests, mainly on WebDev, animation and a tricky LeetCode question.\n\n1. Flight Simulator\n\nPrompt: Create a simple flight simulator using JavaScript. The simulator should feature a basic plane that can take off from a flat runway. The plane’s movement should be controlled with simple keyboard inputs (e.g., arrow keys or WASD). Additionally, it generates a basic cityscape using blocky structures, similar to Minecraft.\n\nResponse from Gemini 2.5 Pro\n\nYou can find the code it generated here: Link\n\nHere’s the output of the program:\n\nI definitely got exactly what I asked for, with everything functioning, from plane movements to the basic Minecraft-styled block buildings. I can’t really complain about anything here. 10/10 for this one.\n\nResponse from Claude 3.7 Sonnet\n\nYou can find the code it generated here: Link\n\nHere’s the output of the program:\n\nI can see some issues with this one. The plane clearly faces sideways, and I don’t know why. Again, it was out of control once it took off and went clearly outside the city. Basically, I’d say we didn’t really get a completely working flight simulator here.\n\nSummary:\n\nIt’s fair to say that Gemini 2.5 really got this correct in one shot. But the issues with the Claude 3.7 Sonnet code aren’t really that big to resolve. Yeah, we didn’t really get the output as expected, and it’s definitely not close to what Gemini 2.5 Pro got us.\n\n2. Rubik’s Cube Solver\n\nThis is one of the toughest questions for LLMs. I’ve tried it with many other LLMs, but none could correct it. Let’s see how these two models do this one.\n\nPrompt: Build a simple 3D Rubik’s Cube visualizer and solver in JavaScript using Three.js. The cube should be a 3×3 Rubik’s Cube with standard colours. Have a scramble button that randomly scrambles the cube. Include a solve function that animates the solution step by step. Allow basic mouse controls to rotate the view.\n\nResponse from Gemini 2.5 Pro\n\nYou can find the code it generated here: Link\n\nHere’s the output of the program:\n\nIt’s impressive that it could do something this hard in one shot. With the 1 million token context window, I can truly see how powerful this model seems to be.\n\nResponse from Claude 3.7 Sonnet\n\nYou can find the code it generated here: Link\n\nHere’s the output of the program:\n\nAgain, I was kind of disappointed that it had the same issue as some other LLMs: failing with the colours and completely failing to solve the cube. I did try to help it come up with the answer, but it didn’t really help.\n\nSummary:\n\nHere again, Gemini 2.5 Pro takes the lead. And the best part is that all of it was done in one shot. Claude 3.7 was really disappointing, as it could not get this one correct, despite being one of the finest coding models out there.\n\n3. Ball Bouncing Inside a Spinning 4D Tesseract\n\nPrompt: Create a simple JavaScript script that visualizes a ball bouncing inside a rotating 4D tesseract. When the ball collides with a side, highlight that side to indicate the impact.\n\nResponse from Gemini 2.5 Pro\n\nYou can find the code it generated here: Link\n\nHere’s the output of the program:\n\nI cannot notice a single issue in the output. The ball and the collision physics all work perfectly, even the part where I asked it to highlight the collision side works. This free model seems to be insane for coding. 🔥\n\nResponse from Claude 3.7 Sonnet\n\nYou can find the code it generated here: Link\n\nHere’s the output of the program:\n\nWow, finally, Claude 3.7 Sonnet got an answer correct. It also added colors to each side, but who asked for it? 🤷‍♂️ Nevertheless, I can’t really complain much here, as the main functionality seems to work just fine.\n\nSummary:\n\nThe answer is evident this time. Both models got the answer correct, implementing everything I asked for. I won’t really say that I like the output of Claude 3.7 Sonnet more, but it definitely put in quite some work compared to Gemini 2.5 Pro.\n\n4. LeetCode Problem\n\nFor this one, let’s do a quick LeetCode check with to see how these models handle solving a tricky LeetCode question with an acceptance rate of just 14.9%: Maximum Value Sum by Placing 3 Rooks.\n\nClaude 3.7 Sonnet is known to be super good at solving LC questions. If you want to see how Claude 3.7 compares to some top models like Grok 3 and o3-mini-high, check out this blog post:\n\nClaude 3.7 Sonnet vs. Grok 3 vs. o3-mini-high: Coding comparison\n\nPrompt:\n\nYou are given a m x n 2D array board representing a chessboard, where board[i][j] represents the value of the cell (i, j).\n\nRooks in the same row or column attack each other. You need to place three rooks on the chessboard such that the rooks do not attack each other.\n\nReturn the maximum sum of the cell values on which the rooks are placed.\n\nExample 1:\n\nInput: board = [[-3,1,1,1],[-3,1,-3,1],[-3,2,1,1]]\nOutput: 4\nExplanation:\nWe can place the rooks in the cells (0, 2), (1, 3), and (2, 1) for a sum of 1 + 1 + 2 = 4.\n\nExample 2:\n\nInput: board = [[1,2,3],[4,5,6],[7,8,9]]\nOutput: 15\nExplanation:\nWe can place the rooks in the cells (0, 0), (1, 1), and (2, 2) for a sum of 1 + 5 + 9 = 15.\n\nExample 3:\n\nInput: board = [[1,1,1],[1,1,1],[1,1,1]]\nOutput: 3\nExplanation:\nWe can place the rooks in the cells (0, 2), (1, 1), and (2, 0) for a sum of 1 + 1 + 1 = 3.\n\nConstraints:\n\n3 <= m == board.length <= 100\n3 <= n == board[i].length <= 100\n-109 <= board[i][j] <= 109\n\nResponse from Gemini 2.5 Pro\n\nGiven how easily it answered all three of the coding questions we tested, I have quite high hopes for this model.\n\nYou can find the code it generated here: Link\n\nIt did take quite some time to answer this one, though, and the code it wrote is kind of super complex to make sense of. I think it answered it more complicated than required. But still, the main thing we’re looking for is to see if it can answer it correctly.\n\nAs expected, it also answered this tough LeetCode question in one shot. This is one of the questions I got stuck on when learning DSA. I’m not sure if I’m happy it did.\n\nResponse from Claude 3.7 Sonnet\n\nI hope this model will crush this one, as in all the other coding tests I’ve done, Claude 3.7 Sonnet has answered all of the LeetCode questions correctly.\n\nYou can find the code it generated here: Link\n\nIt did write correct code but got TLE, but if I have to compare the code’s simplicity, I’d say this model made the code simpler and easier to understand.\n\nSummary:\n\nGemini 2.5 got the answer correct and also wrote the code in the expected time complexity, but Claude 3.7 Sonnet fell into TLE. If I have to compare the code simplicity, Claude 3.7’s generated code seems to be better.\n\nConclusion\n\nFor me, Gemini 2.5 Pro is the winner. We’ve compared two models that are said to be the best at coding. The big difference I see in the model stats is just that Gemini 2.5 Pro has a slightly higher context window, but let’s not forget that this is an experimental model, and improvements are still on the way.\n\nImagine this model’s performance after a 2M token context window.\n\nGoogle’s been killing it recently with such solid models, previously with the Gemma 3 27B model, a super lightweight model with unbelievable results, and now with this beast of a model, Gemini 2.5 Pro.\n\nBy the way, if you are here, Composio is building the skill repository for agents. You can connect LLMs to any application from Gmail to Asana and get things done quickly. You can use MCP servers, or directly add the tools to LLMs in the traditional agentic way.",
    "summary": {
      "en": "**Summary: Gemini 2.5 Pro vs. Claude 3.7 Sonnet: Coding Comparison**\n\nGoogle recently released Gemini 2.5 Pro, which is touted as the best coding model. This comparison evaluates it against Claude 3.7 Sonnet, a well-regarded coding model.\n\n**Key Points:**\n- **Recommendation:** Gemini 2.5 Pro is recommended for coding due to its superior performance, larger context window (1 million tokens vs. Claude's 200k), and it is free.\n- **Performance Overview:** \n  - Gemini 2.5 Pro achieved an accuracy of 63.8% on the SWE benchmark, surpassing Claude 3.7's 62.3%.\n  - It is recognized for excelling in coding, mathematics, science, and image understanding.\n  \n**Coding Tests Results:**\n1. **Flight Simulator:** \n   - **Gemini 2.5 Pro:** Successfully created a functional flight simulator.\n   - **Claude 3.7 Sonnet:** Had issues with plane orientation and control.\n  \n2. **Rubik’s Cube Solver:** \n   - **Gemini 2.5 Pro:** Impressed with a correct solution in one attempt.\n   - **Claude 3.7 Sonnet:** Failed to solve correctly, missing key functionalities.\n\n3. **Bouncing Ball in 4D Tesseract:** \n   - Both models produced correct outputs, but Gemini's response was more aligned with the prompt.\n\n4. **LeetCode Problem:** \n   - **Gemini 2.5 Pro:** Answered correctly but with complex code.\n   - **Claude 3.7 Sonnet:** Provided a simpler solution but was slower and faced performance issues.\n\n**Conclusion:** \nGemini 2.5 Pro is the clear winner in this comparison, showcasing significant improvements over Claude 3.7 Sonnet. Its better coding capabilities and larger context window make it the preferred choice for coding tasks. Future developments, including an even larger context window, may enhance its performance further.",
      "ko": "구글이 최근에 출시한 Gemini 2.5 Pro는 최고의 코딩 모델로 평가받고 있습니다. 이번 비교에서는 잘 알려진 코딩 모델인 Claude 3.7 Sonnet과의 성능을 평가합니다.\n\nGemini 2.5 Pro는 뛰어난 성능과 더 큰 컨텍스트 창(100만 토큰, Claude는 20만 토큰) 덕분에 코딩에 추천됩니다. 또한 무료로 제공됩니다. 성능 측면에서 Gemini 2.5 Pro는 SWE 벤치마크에서 63.8%의 정확도를 기록하며, Claude 3.7의 62.3%를 초과했습니다. 이 모델은 코딩, 수학, 과학, 이미지 이해에서 두각을 나타내고 있습니다.\n\n코딩 테스트 결과를 살펴보면, 첫 번째로 비행 시뮬레이터를 만들었을 때 Gemini 2.5 Pro는 기능적인 비행 시뮬레이터를 성공적으로 제작했습니다. 반면 Claude 3.7 Sonnet은 비행기 방향과 제어에 문제가 있었습니다. 두 번째로 루빅스 큐브 해결에서는 Gemini 2.5 Pro가 한 번의 시도로 정확한 해결책을 제시했지만, Claude 3.7 Sonnet은 주요 기능을 놓치며 올바르게 해결하지 못했습니다.\n\n4D 테서랙트에서의 튕기는 공 문제에서는 두 모델 모두 올바른 출력을 생성했지만, Gemini의 응답이 더 프롬프트에 잘 맞았습니다. LeetCode 문제에서는 Gemini 2.5 Pro가 정확한 답변을 했지만 복잡한 코드로 작성되었습니다. 반면 Claude 3.7 Sonnet은 더 간단한 해결책을 제시했지만 속도가 느리고 성능 문제를 겪었습니다.\n\n이 비교에서 Gemini 2.5 Pro는 명확한 승자로, Claude 3.7 Sonnet에 비해 상당한 개선을 보여주었습니다. 더 나은 코딩 능력과 더 큰 컨텍스트 창 덕분에 코딩 작업에 더 적합한 선택이 됩니다. 향후 더 큰 컨텍스트 창을 포함한 발전이 이루어진다면 성능이 더욱 향상될 가능성이 있습니다.",
      "ja": "Googleは最近、最優秀なコーディングモデルとして評価されているGemini 2.5 Proを発表しました。この比較では、評判の良いコーディングモデルであるClaude 3.7 Sonnetと対比しています。\n\nGemini 2.5 Proは、その優れた性能と1百万トークンという大きなコンテキストウィンドウを持ち、Claudeの200,000トークンに対して優位性を示しています。また、無料で利用できる点も大きな魅力です。性能面では、Gemini 2.5 ProはSWEベンチマークで63.8%の精度を達成し、Claude 3.7の62.3%を上回りました。特にコーディング、数学、科学、画像理解において優れた能力を発揮しています。\n\nコーディングテストの結果では、まずフライトシミュレーターの作成において、Gemini 2.5 Proは機能的なフライトシミュレーターを成功裏に作成しましたが、Claude 3.7 Sonnetは飛行機の向きや制御に問題がありました。次に、ルービックキューブの解法では、Gemini 2.5 Proが一度の試行で正しい解を示し、Claude 3.7 Sonnetは重要な機能を欠いたため正しく解けませんでした。\n\n4Dテッセラクト内のバウンシングボールに関しては、両モデルとも正しい出力を生成しましたが、Geminiの応答はプロンプトにより適合していました。LeetCodeの問題では、Gemini 2.5 Proが正しい回答を出しましたが、コードが複雑でした。一方、Claude 3.7 Sonnetはよりシンプルな解法を提供しましたが、速度が遅く、性能面での問題がありました。\n\nこの比較では、Gemini 2.5 Proが明確な勝者であり、Claude 3.7 Sonnetに対して大きな改善を示しています。優れたコーディング能力と大きなコンテキストウィンドウにより、コーディングタスクにおいて好ましい選択肢となっています。将来的には、さらに大きなコンテキストウィンドウを持つ開発が進むことで、性能がさらに向上する可能性があります。"
    }
  },
  {
    "id": "ab84a76e75dfc07f",
    "title": {
      "en": "Charlie Javice convicted of defrauding JPMorgan in $175M startup sale",
      "ko": "찰리 자비스, JPMorgan 사기 혐의 유죄",
      "ja": "チャーリー・ジャビス、JPMorgan詐欺で有罪"
    },
    "type": "story",
    "url": "https://apnews.com/article/charlie-javice-convicted-fraud-jp-morgan-783cb7b089f6ab5d814c4c0984f0302b",
    "score": 144,
    "by": "ilamont",
    "time": 1743195350,
    "content": "Charlie Javice leaves Federal Court, Wednesday, Aug. 23, 2023, in New York. (AP Photo/John Minchillo, File)\n\n            By\n            MICHAEL R. SISAK and LARRY NEUMEISTER\n\n                    Updated 6:38 AM GMT+9, March 29, 2025\n\n        Share\n\n                    Share\n\n    Facebook\n\n    Copy\n    Link copied\n\n        Print\n\n        Email\n\n    X\n\n    LinkedIn\n\nBluesky\n\n    Flipboard\n\n    Pinterest\n\n    Reddit\n\n                                        NEW YORK (AP) — Charlie Javice, the charismatic founder of a startup company that claimed to be revolutionizing the way college students apply for financial aid, was convicted Friday of defrauding one of the world’s largest banks, JPMorgan Chase, out of $175 million by exaggerating her customer base tenfold.A jury returned the verdict after a five-week trial in federal court in Manhattan. Javice, 32, and her co-defendant, Olivier Amar, face the possibility of decades in prison in a case that has drawn comparisons to Theranos founder Elizabeth Holmes.Javice appeared sullen at the defense table as the verdict was read. A lawyer placed her hand on Javice’s back. She brushed past reporters and didn’t speak as she left court.Javice was in her mid-20s when she founded Frank, a company with software that promised to simplify the arduous process of filling out the Free Application for Federal Student Aid, a complex government form used by students to apply for aid for college or graduate school.\n\nThe company promoted itself as a way for financially needy students to obtain more aid faster, in return for a few hundred dollars in fees. Javice appeared regularly on cable news programs to boost Frank’s profile, once appearing on Forbes’ “30 Under 30” list before JPMorgan bought the startup in 2021.\n\n                            RELATED COVERAGE\n\n                Trump trade policy: 4 years of high drama. Limited results.\n\n                Here’s what tariffs are and how they work\n\n                A timeline of US-China retaliatory tariffs since Trump’s first term\n\nJPMorgan executives testified that Javice told them she had more than four million clients and would have about 10 million by year’s end, but it turned out there were only about 300,000 customers and a list verifying her outsized claim was largely bogus.\n\nJavice’s lawyer, Jose Baez, told the jury that JPMorgan knew what it was getting in the deal, accusing the bank of making up the fraud allegations because of buyer’s remorse after regulatory changes made the data it received in the deal useless to its hopes of gaining new young customers.Defense lawyers asked the judge to set aside the verdict, arguing the evidence was not sufficient to sustain the conviction.\n\nJudge Alvin K. Hellerstein said he would hear arguments on that next week and resolve a dispute over whether Javice and Amar must wear ankle monitors while awaiting sentencing on July 23. Javice’s lawyers argued the device will interfere with her new career: teaching Pilates classes for three or four hours a day.Javice, who lived in Florida, has been free on $2 million bail since her 2023 arrest.Javice and Amar, Frank’s chief growth and acquisition officer and effectively its No. 2, were convicted on all four counts in their indictments, including conspiracy, bank fraud and wire fraud charges that are each punishable by up to 30 years in prison.“While Javice and Amar may have thought that they could lie and cheat their way to a huge payday, their lies caught up with them, and they now stand convicted by a jury of their peers,” Acting Manhattan U.S. Attorney Matthew Podolsky said in a statement.Javice was among a number of young tech executives who vaulted to fame with supposedly disruptive or transformative companies, only to see them collapse amid questions about whether they had engaged in puffery and fraud while dealing with investors.\n\nJavice founded Frank soon after graduating from the University of Pennsylvania’s Wharton School of Business, saying she was motivated by her own frustrations navigating the financial aid process. Frank’s backers included venture capitalist Michael Eisenberg. The company said its offering, akin to online tax preparation software, could help students maximize financial aid while making the application process less painful.JPMorgan became interested partly because of the potential it saw in Frank’s supposedly huge list of satisfied clients. The bank believed those future college graduates could become lifelong bank customers. But after buying the company, JPMorgan said it found evidence Javice had lied about Frank’s success.Frank’s chief software engineer, Patrick Vovor, testified that Javice had asked him to generate synthetic data to support her claim that the company had more than 4 million users.\n\nWhen Vovor asked if that was legal, prosecutors said, Javice and Amar assured him that it was — and told him they didn’t want to end up in orange prison jumpsuits. Vovor testified that he refused to help.“I told them I would not do anything illegal,” Vovor told jurors.Seeking to dent Vovor’s credibility, defense lawyers suggested he was resentful that Javice didn’t want to date him. He denied that.Prosecutors said Javice ended up paying a college friend $18,000 to create millions of fake names with pedigree information. The results were sent to JPMorgan’s third-party data provider, but testimony showed that firm never checked to ensure the people were real.“JPMorgan is not telling the truth,” Baez argued. “They knew the numbers.”\n\n                MICHAEL R. SISAK\n\n                Sisak is an Associated Press reporter covering law enforcement and courts in New York City, including former President Donald Trump’s criminal and civil cases and problems plaguing the federal prison system.\n\n        twitter\n\n        mailto\n\n(function (d, s, b) {\nvar mElmt, primisElmt = d.createElement('script');\nprimisElmt.setAttribute('type', 'text/javascript');\nprimisElmt.setAttribute('async','async');\nprimisElmt.setAttribute('src',  s);\nvar elmtInterval = setInterval(function() {\nmElmt = d.getElementById(b);\nif (mElmt) {\nmElmt.parentNode.insertBefore(primisElmt, mElmt.nextSibling);\nmElmt.parentNode.removeChild(mElmt);\nreturn clearInterval(elmtInterval);\n}\n}, 200);\n})(document, 'https://live.primis.tech/live/liveView.php?s=119023', 'widgetLoaded');\n\n        window._taboola = window._taboola || [];\n        _taboola.push({container: 'taboola-below-article-thumbnails', mode: 'thumbnails-feed-h', placement: 'Below Article Thumbnails', target_type: 'mix'\n        });",
    "summary": {
      "en": "Charlie Javice, the founder of a startup called Frank that aimed to simplify college financial aid applications, was found guilty of defrauding JPMorgan Chase out of $175 million by falsely inflating her customer base. A jury convicted her and her co-defendant, Olivier Amar, after a five-week trial in New York. They face potential decades in prison for charges including bank fraud and conspiracy.\n\nJavice had claimed that Frank had over four million clients, but evidence revealed there were only about 300,000. Testimony indicated that she asked her team to create fake data to support her claims. Despite her defense arguing that JPMorgan was aware of the real numbers and made up the fraud allegations, the jury disagreed.\n\nThe judge will discuss whether Javice and Amar need to wear ankle monitors while awaiting sentencing on July 23. Javice has been free on bail since her arrest in 2023. This case has drawn parallels to other tech fraud cases, highlighting issues of dishonesty among some young entrepreneurs.",
      "ko": "프랭크라는 스타트업의 창립자인 찰리 자비스가 대학 재정 지원 신청을 간소화하려는 목표로 설립한 회사에서 고객 수를 허위로 부풀려 JPMorgan Chase를 1억 7천 5백만 달러 사기 혐의로 유죄 판결을 받았다. 뉴욕에서 진행된 5주간의 재판 끝에 배심원단은 그녀와 공동 피고인인 올리비에 아마르에게 유죄를 선고했다. 이들은 은행 사기와 공모 등의 혐의로 수십 년의 징역형에 처해질 수 있다.\n\n자비스는 프랭크의 고객 수가 400만 명이 넘는다고 주장했지만, 실제로는 약 30만 명에 불과하다는 증거가 드러났다. 증언에 따르면 그녀는 자신의 주장을 뒷받침하기 위해 팀에게 가짜 데이터를 만들도록 요청한 것으로 나타났다. 자비스의 변호인은 JPMorgan이 실제 숫자를 알고 있었고 사기 혐의를 조작했다고 주장했지만, 배심원단은 이에 동의하지 않았다.\n\n판사는 자비스와 아마르가 7월 23일 선고를 기다리는 동안 발목 모니터를 착용해야 하는지에 대해 논의할 예정이다. 자비스는 2023년 체포 이후 보석으로 풀려난 상태다. 이 사건은 일부 젊은 기업가들 사이의 부정직 문제를 부각시키며 다른 기술 사기 사건들과 유사한 점이 있다.",
      "ja": "フランクというスタートアップの創設者チャーリー・ジャビスが、顧客数を不正に膨らませてJPMorgan Chaseから1億7500万ドルを詐取したとして有罪判決を受けました。ニューヨークでの5週間にわたる裁判の結果、陪審員は彼女と共犯のオリビエ・アマールに有罪を言い渡しました。彼らは銀行詐欺や共謀などの罪で数十年の懲役刑を受ける可能性があります。\n\nジャビスはフランクが400万人以上の顧客を持っていると主張していましたが、証拠によると実際には約30万人しかいなかったことが明らかになりました。証言によれば、彼女はチームに対して自分の主張を裏付けるための偽データを作成するよう指示していたとのことです。ジャビスの弁護側は、JPMorganが実際の数字を把握しており、詐欺の主張は虚偽だと主張しましたが、陪審員はこれに反対しました。\n\n裁判官は、ジャビスとアマールが7月23日の判決を待つ間、足首にモニターを装着する必要があるかどうかを議論します。ジャビスは2023年の逮捕以来、保釈中です。この事件は、若い起業家の中に見られる不正直さの問題を浮き彫りにし、他のテクノロジー詐欺事件と類似点があると指摘されています。"
    }
  },
  {
    "id": "17b14cab49d4368f",
    "title": {
      "en": "LLM providers on the cusp of an 'extinction' phase as capex realities bite",
      "ko": "LLM 공급자, '멸종' 위기!",
      "ja": "LLM消滅危機！"
    },
    "type": "story",
    "url": "https://www.theregister.com/2025/03/31/llm_providers_extinction/",
    "score": 74,
    "by": "abawany",
    "time": 1743488530,
    "content": "AI + ML\n\n            48\n\nLLM providers on the cusp of an 'extinction' phase as capex realities bite\n\n            48\n\nOnly the strong will survive, but analyst says cull will not be as rapid as during dotcom era\n\nLindsay Clark\n\nMon31Mar2025                                     //\n14:32UTC\n\nGartner says the market for large language model (LLM) providers is on the cusp of an extinction phase as it grapples with the capital-intensive costs of building products in a competitive market.\n\nAdoption right now is the most important thing: speed, adoption, being in the market – there is extinction coming. The market will not be able to support this number of model providers that we currently have\n\nAs the global tech research company forecasts worldwide generative AI (GenAI) spending will reach $644 billion in 2025, up around 76 percent from 2024, John-David Lovelock, Distinguished VP Analyst at Gartner, said the market for model providers would see a similar consolidation to that of the cloud market, which is currently dominated by AWS, Microsoft Azure, and Google Cloud.\n\"Cloud was more transformational in one sense, [it] changed how we did and what we did and where we did it,\" Lovelock said. \"But it was a replacement. I had to change from using my on-premises CRM system to move to a cloud-based system like Salesforce. GenAI comes in and adds to whatever it is you're running, and it will be part of every mobile phone, every PC, every laptop, every server, every piece of software, it will be in your car, your TV, and your watch.\"\nMeanwhile, CIOs are set to opt for commercial-off-the-shelf solutions for more predictable implementation and business value, rather than building their own software around AI, he said.\n\"Despite model improvements, CIOs will reduce [proof of concept] and self-development efforts, focusing instead on GenAI features from existing software providers,\" said Lovelock.\n\n            <a href=\"https://pubads.g.doubleclick.net/gampad/jump?co=1&amp;iu=/6978/reg_software/aiml&amp;sz=300x50%7C300x100%7C300x250%7C300x251%7C300x252%7C300x600%7C300x601&amp;tile=2&amp;c=2Z-wJlLfI38bYU8-dRPK2MAAAA44&amp;t=ct%3Dns%26unitnum%3D2%26raptor%3Dcondor%26pos%3Dtop%26test%3D0\" target=\"_blank\">\n                <img src=\"https://pubads.g.doubleclick.net/gampad/ad?co=1&amp;iu=/6978/reg_software/aiml&amp;sz=300x50%7C300x100%7C300x250%7C300x251%7C300x252%7C300x600%7C300x601&amp;tile=2&amp;c=2Z-wJlLfI38bYU8-dRPK2MAAAA44&amp;t=ct%3Dns%26unitnum%3D2%26raptor%3Dcondor%26pos%3Dtop%26test%3D0\" alt=\"\">\n            </a>\n\nAs a result, LLM developers are currently chasing adoption among users, businesses, and software vendors as a higher priority than revenue or costs.\n\nYou know that generative AI browser assistant extension is probably beaming everything to the cloud, right?\n\nAI agents swarm Microsoft Security Copilot\n\nA closer look at Dynamo, Nvidia's 'operating system' for AI inference\n\nCloudflare builds an AI to lead AI scraper bots into a horrible maze of junk content\n\n\"Adoption right now is the most important thing: speed, adoption, being in the market – there is extinction coming. The market will not be able to support this number of model providers that we currently have. From the capital expenditure requirements through to revenue, the cloud market only bears three and you expect the same with GenAI model developers,\" he said.\nHowever, the market was unlikely to see a rapid collapse along the lines of the dotcom crash, partly because that also came on the back of Y2K spending. \"We're not going to wake up one morning and find 20 companies out of business. It will be a slower pruning.\"\n\n                <a href=\"https://pubads.g.doubleclick.net/gampad/jump?co=1&amp;iu=/6978/reg_software/aiml&amp;sz=300x50%7C300x100%7C300x250%7C300x251%7C300x252%7C300x600%7C300x601&amp;tile=4&amp;c=44Z-wJlLfI38bYU8-dRPK2MAAAA44&amp;t=ct%3Dns%26unitnum%3D4%26raptor%3Dfalcon%26pos%3Dmid%26test%3D0\" target=\"_blank\">\n                    <img src=\"https://pubads.g.doubleclick.net/gampad/ad?co=1&amp;iu=/6978/reg_software/aiml&amp;sz=300x50%7C300x100%7C300x250%7C300x251%7C300x252%7C300x600%7C300x601&amp;tile=4&amp;c=44Z-wJlLfI38bYU8-dRPK2MAAAA44&amp;t=ct%3Dns%26unitnum%3D426raptor%3Dfalcon%26pos%3Dmid%26test%3D0\" alt=\"\">\n                </a>\n\nAI services are set for the most rapid growth in the market, Gartner said, climbing from $10.6 billion in 2024 to $27.8 billion in 2025, a 163 percent increase. While devices and software-based GenAI were both set to nearly double, the server-based market for GenAI would see the slowest growth at 33.1 percent to reach $180 billion. ®\n                                    <strong>Get our</strong> <a href=\"https://whitepapers.theregister.com/\" style=\"text-transform:uppercase\">Tech Resources</a>\n\n        Share\n\n            More about\n\n                Large Language Model\n\n        More like these\n\n            ×\n\n                    More about\n\n                Large Language Model\n\n                    Narrower topics\n\n                Amazon Bedrock\n\n                Anthropic\n\n                ChatGPT\n\n                Gemini\n\n                GPT-3\n\n                GPT-4\n\n                    Broader topics\n\n                AI\n\n                    More about\n\n        Share\n\n                48\n\n                    COMMENTS\n\n            More about\n\n                Large Language Model\n\n        More like these\n\n            ×\n\n                    More about\n\n                Large Language Model\n\n                    Narrower topics\n\n                Amazon Bedrock\n\n                Anthropic\n\n                ChatGPT\n\n                Gemini\n\n                GPT-3\n\n                GPT-4\n\n                    Broader topics\n\n                AI\n\n        TIP US OFF\n        Send us news",
    "summary": {
      "en": "The market for large language model (LLM) providers is facing significant challenges due to high costs and competition, leading analysts to predict an \"extinction phase\" for many providers. Gartner forecasts that generative AI spending will grow to $644 billion by 2025, but the number of successful LLM providers will likely shrink, similar to the cloud market's consolidation.\n\nCIOs are expected to prefer off-the-shelf AI solutions for easier implementation and value, rather than developing their own. Consequently, LLM developers are prioritizing user adoption over immediate revenue. Although a decline in the number of providers is anticipated, it will happen gradually rather than abruptly, unlike the dotcom crash.\n\nGartner also predicts that AI services will see substantial growth, with spending increasing from $10.6 billion in 2024 to $27.8 billion in 2025, while the server-based GenAI market will grow more slowly at 33.1%.",
      "ko": "대형 언어 모델(LLM) 제공업체 시장은 높은 비용과 경쟁으로 인해 큰 어려움에 직면하고 있으며, 이로 인해 많은 분석가들은 여러 제공업체가 \"멸종 단계\"에 들어설 것이라고 예측하고 있습니다. 가트너는 생성형 인공지능에 대한 지출이 2025년까지 6440억 달러에 이를 것으로 전망하고 있지만, 성공적인 LLM 제공업체의 수는 줄어들 것으로 보입니다. 이는 클라우드 시장의 통합과 유사한 양상입니다.\n\nCIO들은 자체 개발보다는 구현이 용이하고 가치를 제공하는 기성 AI 솔루션을 선호할 것으로 예상됩니다. 따라서 LLM 개발자들은 즉각적인 수익보다 사용자 채택을 우선시하고 있습니다. 제공업체 수의 감소가 예상되지만, 이는 닷컴 붕괴와는 달리 갑작스럽지 않고 점진적으로 이루어질 것입니다.\n\n가트너는 AI 서비스가 상당한 성장을 이룰 것으로 예측하며, 2024년 106억 달러에서 2025년 278억 달러로 지출이 증가할 것으로 보입니다. 반면 서버 기반 생성형 AI 시장은 33.1%의 느린 성장세를 보일 것으로 예상됩니다.",
      "ja": "大規模言語モデル（LLM）を提供する市場は、高コストや競争の激化により大きな課題に直面しています。このため、多くのプロバイダーが「絶滅期」に入るとアナリストたちは予測しています。ガートナーは、生成AIへの支出が2025年までに6440億ドルに達すると予測していますが、成功するLLMプロバイダーの数は減少する見込みです。これは、クラウド市場の統合と似た状況です。\n\nCIO（最高情報責任者）は、自社でAIソリューションを開発するのではなく、導入が容易で価値のある既製品のAIソリューションを好むと考えられています。そのため、LLM開発者は即時の収益よりもユーザーの採用を優先しています。プロバイダーの数が減少することは予想されていますが、ドットコムバブルの崩壊のように急激ではなく、徐々に進行するでしょう。\n\nガートナーはまた、AIサービスが大きな成長を遂げると予測しており、2024年の支出は106億ドルから2025年には278億ドルに増加するとしています。一方、サーバーベースの生成AI市場は33.1%の成長にとどまる見込みです。"
    }
  },
  {
    "id": "3842c63788f7a417",
    "title": {
      "en": "Show HN: I made a C program to create a vanity SHA-1 hash for a text file",
      "ko": "자랑스러운 SHA-1 해시 생성기",
      "ja": "SHA-1ハッシュ生成ツール"
    },
    "type": "story",
    "url": "https://gist.github.com/o0101/77eb378b5076fe47c3336583330ac615",
    "score": 55,
    "by": "keepamovin",
    "time": 1743159304,
    "content": "Instantly share code, notes, and snippets.\n\n            o0101/vanity.c\n\n          Created\n          March 28, 2025 10:53\n\nShow Gist options\n\n          Download ZIP\n\n    Star\n\n          17\n          (17)\n\nYou must be signed in to star a gist\n\n    Fork\n\n          2\n          (2)\n\nYou must be signed in to fork a gist\n\n           Embed\n\n           Embed\n      Embed this gist in your website.\n\n           Share\n      Copy sharable link for this gist.\n\n          Clone via HTTPS\n      Clone using the web URL.\n\n                Learn more about clone URLs\n\n        Clone this repository at &lt;script src=&quot;https://gist.github.com/o0101/77eb378b5076fe47c3336583330ac615.js&quot;&gt;&lt;/script&gt;\n\n        Save o0101/77eb378b5076fe47c3336583330ac615 to your computer and use it in GitHub Desktop.\n\n      Code\n\n        Revisions\n        1\n\n        Stars\n        17\n\n        Forks\n        2\n\n           Embed\n\n           Embed\n      Embed this gist in your website.\n\n           Share\n      Copy sharable link for this gist.\n\n          Clone via HTTPS\n      Clone using the web URL.\n\n                Learn more about clone URLs\n\n        Clone this repository at &lt;script src=&quot;https://gist.github.com/o0101/77eb378b5076fe47c3336583330ac615.js&quot;&gt;&lt;/script&gt;\n\n        Save o0101/77eb378b5076fe47c3336583330ac615 to your computer and use it in GitHub Desktop.\n\n      Download ZIP\n\n    gcc -O2 -o vanity vanity.c -lssl -lcrypto\n\n    Raw\n\n              vanity.c\n\n          #include <stdio.h>\n\n          #include <stdlib.h>\n\n          #include <string.h>\n\n          #include <ctype.h>\n\n          #include <openssl/sha.h>\n\n          #define TARGET_PREFIX \"20250327\"\n\n          #define MAX_WORDS 256\n\n          #define MAX_TEXT 2048\n\n          #define MAX_ATTEMPTS (1ULL << 32) // 2^32 attempts (~4.3B, enough for 8-char prefix)\n\n          const char *ORIGINAL_TEXT = \"They're not particularly rare, but I think they're more pleasing to see.\\n\\n\"\n\n                                      \"For instance this \\\"2822302\\\" makes me especially happy: https://github.com/BrowserBox/BrowserBox/commit/2822302387c4cb7ff71c4239da3dc5fa4c07e165\\n\\n\"\n\n                                      \"It even extends up to 10 digits! That's not particularly rare - roughly 1% chance (10^10/16^10 i think) - but I just think they look nice.\\n\\n\"\n\n                                      \"Are there any other people out there who are particularly pleased when they hit that? Sorta like hitting 7777 on the odometer, or whatever.\\n\\n\"\n\n                                      \"I'm also a fan of the purely numeric identifiers Twitter/X uses (and has for ages).\\n\\n\"\n\n                                      \"This all reminds me of that \\\"commit fuzzing tool\\\" that can make your log have whatever commit you want. Asked AI, turns out it's: https://github.com/not-an-aardvark/lucky-commit\\n\\n\"\n\n                                      \"Lucky Commit! What a perfect name. I guess this Ask HN should be: Does anyone else like lucky commits?\\n\\n\"\n\n                                      \"BTW the SHA has of this message starts with: 20250327\";\n\n          typedef struct {\n\n              char *lower;\n\n              char *upper;\n\n              int start;\n\n              int len;\n\n          } Word;\n\n          void normalize_text(char *dest, const char *src, int len) {\n\n              int j = 0, last_space = 0;\n\n              for (int i = 0; i < len && src[i]; i++) {\n\n                  if (32 == src[i]) {\n\n                      if (!last_space && j > 0) dest[j++] = ' ';\n\n                      last_space = 1;\n\n                  } else {\n\n                      dest[j++] = src[i];\n\n                      last_space = 0;\n\n                  }\n\n              }\n\n              dest[j] = '\\0';\n\n          }\n\n          void compute_hash(const char *text, char *hash_str) {\n\n              unsigned char hash[SHA_DIGEST_LENGTH];\n\n              SHA1((unsigned char *)text, strlen(text), hash);\n\n              for (int i = 0; i < SHA_DIGEST_LENGTH; i++) {\n\n                  sprintf(hash_str + (i * 2), \"%02x\", hash[i]);\n\n              }\n\n              hash_str[SHA_DIGEST_LENGTH * 2] = '\\0';\n\n          }\n\n          int is_in_url(int pos, const char *text) {\n\n              const char *ptr = text;\n\n              while (ptr < text + pos) {\n\n                  if (strncmp(ptr, \"http\", 4) == 0) {\n\n                      const char *start = ptr;\n\n                      while (*ptr && !isspace(*ptr)) ptr++;\n\n                      if (pos >= start - text && pos < ptr - text) return 1;\n\n                  } else {\n\n                      ptr++;\n\n                  }\n\n              }\n\n              return 0;\n\n          }\n\n          int parse_words(const char *text, Word *words, int *skip_indices, int *mutable_count) {\n\n              int word_count = 0, i = 0, start = 0;\n\n              int text_len = strlen(text);\n\n              char *buf = malloc(text_len + 1);\n\n              while (i <= text_len) {\n\n                  if (i == text_len || text[i] == 32) {\n\n                      if (i > start) {\n\n                          int len = i - start;\n\n                          strncpy(buf, text + start, len);\n\n                          buf[len] = '\\0';\n\n                          if (strncmp(buf, \"http\", 4) == 0 || !isalpha(buf[0])) {\n\n                              skip_indices[word_count] = 1;\n\n                          } else {\n\n                              words[*mutable_count].lower = strdup(buf);\n\n                              words[*mutable_count].upper = strdup(buf);\n\n                              words[*mutable_count].upper[0] = toupper(buf[0]);\n\n                              words[*mutable_count].lower[0] = tolower(buf[0]);\n\n                              words[*mutable_count].start = start;\n\n                              words[*mutable_count].len = len;\n\n                              (*mutable_count)++;\n\n                          }\n\n                          word_count++;\n\n                      }\n\n                      start = i + 1;\n\n                  }\n\n                  i++;\n\n              }\n\n              free(buf);\n\n              return word_count;\n\n          }\n\n          int main(int argc, char *argv[]) {\n\n              const char *file_path = (argc > 1) ? argv[1] : \"input.txt\";\n\n              printf(\"Processing file: %s\\n\", file_path);\n\n              printf(\"Target prefix: %s\\n\", TARGET_PREFIX);\n\n              Word words[MAX_WORDS];\n\n              int skip_indices[MAX_WORDS] = {0};\n\n              int mutable_count = 0;\n\n              int total_words = parse_words(ORIGINAL_TEXT, words, skip_indices, &mutable_count);\n\n              printf(\"Total mutable words: %d\\n\", mutable_count);\n\n              unsigned long long total_combinations = (mutable_count > 64) ? MAX_ATTEMPTS : (1ULL << mutable_count);\n\n              printf(\"Total combinations: %llu\\n\", total_combinations);\n\n              char *current_text = malloc(MAX_TEXT);\n\n              char hash_str[SHA_DIGEST_LENGTH * 2 + 1];\n\n              unsigned long long counter = 0;\n\n              const unsigned long long report_interval = 1000000;\n\n              FILE *fp = fopen(file_path, \"w\");\n\n              if (!fp) { perror(\"File open failed\"); return 1; }\n\n              fprintf(fp, \"%s\", ORIGINAL_TEXT);\n\n              fclose(fp);\n\n              while (counter < total_combinations) {\n\n                  strcpy(current_text, ORIGINAL_TEXT);\n\n                  int word_idx = 0;\n\n                  for (int i = 0; i < total_words && word_idx < mutable_count; i++) {\n\n                      if (!skip_indices[i]) {\n\n                          int bit = (counter >> word_idx) & 1;\n\n                          char *variant = bit ? words[word_idx].upper : words[word_idx].lower;\n\n                          memcpy(current_text + words[word_idx].start, variant, words[word_idx].len);\n\n                          word_idx++;\n\n                      }\n\n                  }\n\n                  normalize_text(current_text, current_text, MAX_TEXT);\n\n                  compute_hash(current_text, hash_str);\n\n                  if (strncmp(hash_str, TARGET_PREFIX, strlen(TARGET_PREFIX)) == 0) {\n\n                      printf(\"Success! Hash: %s\\n\", hash_str);\n\n                      printf(\"Counter: %llu (binary: \", counter);\n\n                      for (int i = mutable_count - 1; i >= 0; i--) printf(\"%d\", (int)((counter >> i) & 1));\n\n                      printf(\")\\n\");\n\n                      char *output_file = malloc(strlen(file_path) + 8);\n\n                      strcpy(output_file, file_path);\n\n                      strcat(output_file, \".vanity\");\n\n                      fp = fopen(output_file, \"w\");\n\n                      fprintf(fp, \"%s\", current_text);\n\n                      fclose(fp);\n\n                      free(output_file);\n\n                      break;\n\n                  }\n\n                  if (counter % report_interval == 0) {\n\n                      printf(\"Progress: %llu/%llu - %.8s\\n\", counter, total_combinations, hash_str);\n\n                  }\n\n                  counter++;\n\n              }\n\n              if (counter == total_combinations) {\n\n                  printf(\"Failed after trying all %llu combinations\\n\", total_combinations);\n\n              }\n\n              for (int i = 0; i < mutable_count; i++) {\n\n                  free(words[i].lower);\n\n                  free(words[i].upper);\n\n              }\n\n              free(current_text);\n\n              return 0;\n\n          }\n\n    Sign up for free\n    to join this conversation on GitHub.\n    Already have an account?\n    Sign in to comment",
    "summary": {
      "en": "This text describes a code snippet for a program named `vanity.c`, created on March 28, 2025. The program's purpose is to generate a specific hash value that starts with a defined prefix, \"20250327\", by modifying words in a given text. Here are the key points:\n\n- **Functionality**: The program processes an input text, searching for words that can be altered. It generates different combinations of these words to try to create a hash that matches the target prefix.\n\n- **Hashing**: It uses the SHA-1 hashing algorithm to compute the hash of the modified text.\n\n- **Progress Reporting**: The program reports its progress every million attempts, displaying the current hash being checked.\n\n- **Output**: If a matching hash is found, the modified text is saved to a new file with a `.vanity` extension. If all combinations are tried without a match, it indicates failure.\n\n- **Memory Management**: The program includes mechanisms to allocate and free memory for strings and words used in the process.\n\nOverall, this code aims to create a \"vanity\" hash by adjusting parts of a text in a systematic way.",
      "ko": "이 텍스트는 2025년 3월 28일에 작성된 `vanity.c`라는 프로그램의 코드 조각을 설명합니다. 이 프로그램의 목적은 주어진 텍스트에서 단어를 수정하여 특정 해시 값을 생성하는 것입니다. 이 해시 값은 \"20250327\"이라는 정의된 접두사로 시작해야 합니다.\n\n프로그램은 입력된 텍스트를 처리하며, 수정할 수 있는 단어를 찾아냅니다. 그런 다음 이 단어들의 다양한 조합을 생성하여 목표 접두사와 일치하는 해시를 만들기 위해 시도합니다.\n\n해시는 SHA-1 해싱 알고리즘을 사용하여 수정된 텍스트의 해시를 계산합니다. 프로그램은 백만 번의 시도마다 진행 상황을 보고하며, 현재 확인 중인 해시 값을 표시합니다.\n\n일치하는 해시가 발견되면 수정된 텍스트는 `.vanity` 확장자를 가진 새로운 파일에 저장됩니다. 모든 조합을 시도했지만 일치하는 해시를 찾지 못하면 실패를 나타냅니다.\n\n프로그램은 과정에서 사용되는 문자열과 단어에 대한 메모리를 할당하고 해제하는 메커니즘도 포함되어 있습니다. 전반적으로 이 코드는 텍스트의 일부를 체계적으로 조정하여 \"바니티\" 해시를 생성하는 것을 목표로 하고 있습니다.",
      "ja": "このテキストは、2025年3月28日に作成されたプログラム「vanity.c」のコードスニペットについて説明しています。このプログラムの目的は、特定の接頭辞「20250327」で始まるハッシュ値を生成することです。そのために、与えられたテキスト内の単語を変更します。\n\nプログラムは、入力されたテキストを処理し、変更可能な単語を探します。これらの単語の異なる組み合わせを生成し、目標の接頭辞に一致するハッシュを作成しようとします。\n\nハッシュの計算にはSHA-1というハッシュアルゴリズムを使用しています。このアルゴリズムにより、変更されたテキストのハッシュが計算されます。\n\nプログラムは、毎百万回の試行ごとに進捗を報告し、現在チェック中のハッシュを表示します。\n\nもし一致するハッシュが見つかれば、変更されたテキストは新しいファイルに保存され、そのファイルには「.vanity」という拡張子が付けられます。すべての組み合わせを試しても一致しなかった場合は、失敗を示します。\n\nプログラムには、使用する文字列や単語のメモリを確保し、解放するための仕組みも含まれています。\n\n全体として、このコードはテキストの一部を体系的に調整することで「バニティ」ハッシュを作成することを目指しています。"
    }
  },
  {
    "id": "4188330b6dc3b945",
    "title": {
      "en": "One Last Ride for Antarctica's 'Ivan the Terra Bus'",
      "ko": "안타르카티카의 마지막 여정",
      "ja": "南極の「アイバン」最後の旅"
    },
    "type": "story",
    "url": "https://www.atlasobscura.com/articles/antarctica-ivan-the-terra-bus-retired",
    "score": 85,
    "by": "Thevet",
    "time": 1743230104,
    "content": "One Last Ride for Antarctica’s ‘Ivan the Terra Bus’\n\n          After more than 30 years as an iconic fixture of the polar research station, this beloved bus is bidding farewell to the southern continent.\n\n            by Allegra Rosenberg\n\n            March 27, 2025\n\n              One Last Ride for Antarctica’s 'Ivan the Terra Bus'\n\n  function copyURLWithConfirm () {\n    copyCurrentUrlToClipboard();\n    $('.DDPNavbarItem[data-toggle=\"tooltip\"]').tooltip('show');\n    window.setTimeout(function () {\n      $('.DDPNavbarItem[data-toggle=\"tooltip\"]').tooltip('destroy');\n    }, 1500)\n  }\n\n  $(function (){\n    $('.js-BTFNav__popover-trigger').popover({\n      trigger: 'click',\n      html: true,\n      placement: 'bottom',\n      template: '<div class=\"BTFNav__share-popover popover\" role=\"tooltip\"><div class=\"arrow\"></div><h3 class=\"popover-title\"></h3><div class=\"popover-content\"></div></div>',\n      content: function () {\n        return $($('.js-BTFNav__popover-content').html());\n      },\n      delay: {\n        hide: 100\n      }\n    });\n  })\n\n              For the last 30 years, Ivan the Terra Bus has dutifully transported people between Antarctica’s airfields and research stations. The 2024–25 summer season marked Ivan’s final days on the ice. Eli Duke / CC BY-SA 2.0\n\n            In This Story\n\n                                                                                                                                                                            Add McMurdo Station to a New List\n\n                Place\n\n                  McMurdo Station\n\n                  Antarctica's bustling metropolis, originally established by Richard E. Byrd.\n\n Destination Guide\n\n                  Antarctica\n\n                63 Articles\n\n                37 Places\n\n            Everyone in Antarctica knows Ivan. Even those that haven’t had the pleasure of riding inside of him—in comfortable seats, surrounded by wood paneling and the pleasant sounds of jazz warbling from his internal speakers as he rumbles slowly along the ice—have heard of him and probably walked right past him, sitting pretty near McMurdo Station’s cafeteria in his iconic orange-and-white livery.\nRecently, rumors grew that Ivan’s time on the ice was coming to an end, and it was time for the old bus to be retired. This caused an outcry among his longtime fans, who feared that he would end up unceremoniously scrapped for parts—a potentially sad end for such an iconic vehicle.\nIvan the Terra Bus arrived in McMurdo Sound in 1994, a shiny new supplement to the existing Delta passenger transport vehicles that had been brought over in the 1970s by the U.S. Navy. This fleet was responsible for bringing people from the runways out on the ice, where passenger planes would land after taking off from New Zealand, to the two national bases on Ross Island—America’s McMurdo and New Zealand’s smaller Scott Base. The hefty vehicle was about 46 feet long and 12 and a half feet wide, with a turning radius of 160 feet—the equivalent of the width of a football field. It had enormous tires with nearly six-foot diameters, and a ladder was required to climb up into its interior, which could accommodate up to 56 passengers. “It was warm and big and impressive,” remembers scientist David Theil, who rode in Ivan in 1995 when he was still almost brand new.\nA young Ivan the Terra Bus in 1994, shortly after his arrival in Antarctica. Courtsey of Chris ‘Xenon’ Hanson\n\n        ATLAS OBSCURA BOOKS\n\n        A Visual Odyssey Through the Marvels of Life\n        Venture into Nature's Unseen Realms with Our New Book Atlas Obscura: Wild Life\n\n        Order Now\n\nAntarctic veterans remember the contest that was held among McMurdo residents to name the bus when it arrived. Roy Harrison, a mechanic, remembers being disappointed that his own suggestion, “Magic Bus” (in honor of the song by The Who) wasn’t chosen. The winning name “Ivan the Terra Bus,” was, of course, a reference to Ivan the Terrible, legendary medieval tsar of Russia; there’s also the happy coincidence that “Terra Bus,” the name that the Canadian manufacturer Foremost gave the model in 1981, sounds very much like a pun on the two Ross Island mountains that rise above McMurdo Station—Mount Erebus and Mount Terror. Those mountains were named after the two ships that first explored the regions, HMS Terror and Erebus (more famous, perhaps, for later being lost in the Arctic with John Franklin’s doomed expedition).\n“Ivan the Terra Bus” was suggested by the engineer John Wright, whose main accomplishment was masterminding the thousand-mile snow road between McMurdo Station, on the edge of the continent, and Amundsen-Scott South Pole station at its center. That wasn’t a route Ivan ever took, though. His job was solely to rumble along between the airfields and the stations, and pretty soon after arriving he became a staple of life at McMurdo, the subject of songs, jokes, and fond memories. Bill Jirsa and Allison “Sandwich” Barden wrote this ditty during the 2006-7 summer season to celebrate Ivan:\nHe’s Ivan the Terra Bus / He’s bringing our friends to us / He took some friends away / Took them down to the Ice Runway / Someday he’ll come for us …/ He’s Ivan the Terra Bus!\nThere were certain quirks about Ivan that his drivers and passengers grew to love. He was rather slow compared to the smaller Deltas, taking over an hour to reach McMurdo from the airfields, but the ride was always memorable. The day the vehicle was unloaded from the cargo ship at the ice pier, an overenthusiastic driver backed Ivan into a bollard, resulting in a large dent on his rear bumper. This dent, which went unfixed, was forever known as “Charlie’s Folly” after the driver, and was marked thusly with permanent marker. Stickers were plastered all over Ivan’s charming wood-paneled interior, mementoes of Antarctic projects, excursions, and memes—such as a picture of a penguin holding a knife with the slogan “BECOME UNGOVERNABLE.”\nStickers plastered on the wood-paneled interior of Ivan the Terra Bus. Courtesy of Eric Chevreuil\nBex Henderson first arrived on the ice in 2018, long after Ivan had become part of everyday life at the station, but she was still honored to get to drive him every day. It wasn’t an easy job. “He had a whole set of instructions just to even get him turned on,” she remembers, involving a 30-to-60 minute engine warm-up period, and easily fogged windows that often meant having to drive with poor visibility for the beginning of a ride. But, she says, “Ivan could just make it through anything. I mean, he just floated across when the roads went bad,” as opposed to the Deltas and the 65-passenger Kress trailer, which often got bogged down in slushy snow and transitional terrain, and led to passengers being stranded out on the ice.\nSome of the newer vans that arrived after Ivan were preferred for their speed and comfort, when it came to getting to and from the base quickly, but Ivan was always the slow but steady old reliable of the airfield fleet. “Have you really been to Antarctica if you didn’t get picked up in Ivan?” Henderson says.\nIvan’s impending retirement was a cause for alarm among his passengers and fans. As replacement parts stopped being manufactured, repairs became difficult and fixes required increasingly expensive manufacturing and engineering. That’s a lot of money and effort for a 30-year-old bus, no matter how beloved, especially at a time when the National Science Foundation’s Antarctic efforts are facing increasing budget cuts and threats from political upheaval.\nOnce Ivan was given a name, it was painted onto the side of the bus, just above one of the massive wheel wells. Eli Duke / CC BY-SA 2.0\nEric Chevreuil, a longtime USAP worker in various departments including supply and IT, was distressed to find out during the 2024–25 summer season that Ivan’s fate had been sealed. The beloved Terra Bus had apparently been slated for auction at Port Hueneme, California, due to be inevitably scrapped for metal value or, Chevreuil imagined, bought cheaply to decorate someone’s roadside farm equipment store and then left to rust. Chevreuil was determined to prevent that from happening.\n“Ivan was a safe haven, a whole experience by itself, especially on our first deployment, [our] first trip from the icefield to the station,” he remembers. “Warm, wooden panels, some jazz music in the background, muffled sounds of various conversations (56 seats), record breaking 15 to 20 mph at 2200 rpm max, the ice outside, sometimes fog or blizzard…”\nChevreuil contacted museums across America and various NSF personnel, making the case that Ivan was an important artifact of Antarctic history that deserved to live on. Soon he was relieved to hear the news that Ivan had been given an eleventh-hour pardon, and would be shipped not to the scrapyard but instead to Christchurch, New Zealand.\nIt seems that the groundswell of protective nostalgia for Ivan and his legacy at McMurdo helped save him from the scrapper. Especially for people like Chevreuil and Henderson, who caught the tail end of Ivan’s legendary lifespan, Ivan represents an earlier era of Antarctic living, one which is rapidly fading into memory.\n“[Ivan] was a mark of an older time, which I think a lot of people—especially people who had been there for years—had that special connection to, as things at McMurdo changed so much,” Henderson says.\nA community effort saved Ivan from being put up for auction. Instead, the bus will be preserved and shipped to New Zealand. Eli Duke / CC BY-SA 2.0\nWhen Ivan first arrived, McMurdo had a bowling alley, a greenhouse, multiple bars, and an old-school boy’s club culture still very much influenced by the Navy, which only turned over McMurdo operations fully to the NSF in 1993, shortly before Ivan’s arrival.\nThe base has evolved over the years, with new facilities replacing beloved old ones, and certain more casual aspects of life on the ice (such as drunken partying) restricted, due at least in part to the rising awareness, on the ice and off, of McMurdo’s persistent culture of sexual harassment. That kind of change is very much welcome, but material remnants of an older McMurdo are still mourned as they are deprecated, demolished, or consigned to the scrapyard.\nWith Ivan arriving safely in New Zealand last week, at the close of the Antarctic summer season, his devoted riders and drivers from over the years can breathe a sigh of relief. According to a statement by an NSF spokesperson, “Ivan is currently parked in Christchurch while NSF works with partners to determine its long-term home. USAP will continue to use other all-terrain vehicles on the continent to transport personnel.”\nThe rumor among Antarctic workers is that Ivan will likely be displayed at the International Antarctic Centre in Christchurch, but it’s possible he might end up at a museum somewhere else in the world. Either way, Ivan will live on as a beloved part of Antarctic history.\n\n            var bodyViewedPercentageLogger = debounce( function() {\n                  logPercentBelowViewport( $('#article-body'), 'contentViewed' );\n\n                  // Chase campaign: Simple Reach reporting %>\n                  // END Chase campaign\n\n                }, 16);\n\n            $(window).on('scroll resize', bodyViewedPercentageLogger);\n\n            Read next\n\n                Demand for Tiny Plants Is Driving a Poaching Crisis in South Africa\n\n                The Succulent Karoo is home to desert plant species found nowhere else on the planet—criminal networks have been digging them up by the millions.\n\n              busantarcticcarstransportationscience\n\n                    Want to see fewer ads?\n                    Become a Member.\n\n                        Want to see fewer ads?\n                        Become a Member.",
    "summary": {
      "en": "**Summary: One Last Ride for Antarctica’s ‘Ivan the Terra Bus’**\n\nAfter over 30 years of service at McMurdo Station in Antarctica, the iconic Ivan the Terra Bus is retiring. Known for its comfort and charm, Ivan has been a vital link between airfields and research stations. The bus, which arrived in 1994, became a beloved part of life in Antarctica, often celebrated in songs and memories.\n\nAs Ivan's retirement approached, concerns grew that he might be scrapped. However, a dedicated community effort helped save him. Instead of being auctioned off, Ivan will be preserved and sent to Christchurch, New Zealand, where he may become part of a museum exhibit. His legacy represents a nostalgic era of Antarctic exploration and life at McMurdo Station, which has changed significantly over the years.",
      "ko": "30년 넘게 남극 맥머도 기지에서 활동해온 아이반 테라 버스가 은퇴합니다. 편안함과 매력으로 유명한 아이반은 공항과 연구 기지 간의 중요한 연결 고리 역할을 해왔습니다. 1994년에 도착한 이 버스는 남극 생활의 사랑받는 일부분이 되었으며, 종종 노래와 추억 속에서 기념되었습니다.\n\n아이반의 은퇴가 다가오면서 그가 폐기될까 우려하는 목소리가 커졌습니다. 그러나 헌신적인 지역 사회의 노력 덕분에 그는 구해졌습니다. 경매에 부쳐지는 대신 아이반은 보존되어 뉴질랜드 크라이스트처치로 보내질 예정이며, 그곳에서 박물관 전시의 일부가 될 수 있습니다. 그의 유산은 남극 탐험과 맥머도 기지에서의 삶의 향수를 불러일으키며, 시간이 흐르면서 크게 변화한 남극의 역사를 상징합니다.",
      "ja": "南極のマクマード基地で30年以上にわたり活躍してきたアイバン・ザ・テラバスが引退します。快適さと魅力で知られるアイバンは、空港と研究基地を結ぶ重要な役割を果たしてきました。1994年に到着したこのバスは、南極での生活の一部として愛され、歌や思い出の中でしばしば称えられてきました。\n\n引退が近づくにつれ、アイバンが廃棄されるのではないかという懸念が高まりました。しかし、熱心なコミュニティの努力により、彼は救われました。オークションに出されるのではなく、アイバンは保存され、ニュージーランドのクライストチャーチに送られることになりました。そこで彼は博物館の展示の一部になる可能性があります。アイバンの存在は、南極探検とマクマード基地での生活の懐かしい時代を象徴しており、年月とともに大きく変わったその歴史を物語っています。"
    }
  },
  {
    "id": "a1ea51213cb88506",
    "title": {
      "en": "In the 1980s we downloaded games from the radio",
      "ko": "라디오 게임 다운로드 시대",
      "ja": "ラジオでゲームダウンロード"
    },
    "type": "story",
    "url": "https://newslttrs.com/yes-in-the-1980s-we-downloaded-games-from-the-radio/",
    "score": 279,
    "by": "spzb",
    "time": 1743200073,
    "content": "Nostalgia\n            Yes, in the 1980s we downloaded games from the radio\n\n                    Simon\n\n                        28 Mar 2025\n                            — 5 min read\n\n            Photo by William Warby / Unsplash\n\n            So there I was, minding my own business, doom-scrolling my way through Facebook posts when I happened upon one that hit me straight in the nostalgia. A photo of a 1980s home computer, a cassette player and some tapes. The text underneath proclaimed \"In the 1980s, people could download video games from radio broadcasts by recording the audio onto cassette tapes. These tapes could then be played on computers to load the games\". I nodded sagely to myself as I remembered doing just that.Then I started to read the comments underneath and people were flat-out denying that this had ever happened. The reply guys broadly fell into two camps: the \"I have never heard of this, therefore it never happened\" and the over confident \"expert\" saying things like \"this would be technically impossible due to some fancy sounding words I've heard like 'hertz', 'compression' and 'frequency shift keying', therefore it never happened\".Just to make sure I was in a spluttering rage the page itself was titled \"Unbelievable facts\" as if my own childhood had become unbelievable. Although now I think about it it was an unbelievably long time ago, so maybe they have a point.My youth is now an unbelievable fact, apparentlyAnyway, come back with me to the UK in the early 1980s. Recession, strikes, unemployment and the first female Prime Minister, Margaret Thatcher, dominated the news. The home video cassette recorder was only just becoming common, the compact disc wouldn't be launched until the middle of the decade and mobile phone networks didn't even exist. Dexy's Midnight Runners, Irene Cara and Culture Club soundtracked the era and, across the land, the home computer boom was booming.Computers were new, barely making their way even into the workplace. Most people in office jobs were using typewriters, carbon papers and the postal system. But the microprocessor revolution promised to make computer skills essential to the economy and so the British Broadcasting Corporation began a public education exercise : The BBC Computer Literacy Project.The BBC's project is best remembered for the TV programmes fronted by Ian McNaught Davis and Chris Serle and, of course, the eponymous BBC Micro specially developed by Acorn to accompany the programmes. Less well known was a Radio 4 series called The Chip Shop. According to the ever reliable internet, it was presented by Barry Norman (much better known as a film critic than a technology expert) although I have no recollection of that.Home computers at the time were a marvel of cost efficient engineering. Usually consisting of a chunky wedge-shaped keyboard with all the gadgetry inside, it used your normal home TV as a display and a normal portable cassette recorded as a data storage device. Software (which for most of us meant games) would be supplied on an audio cassette on which a series of piercing screeching noises were recorded. You'd hook up the cassette player to your computer, play the screeching noises into your computer through a cable of some description and after a few minutes your game would be loaded up and ready to play. Or, more often, you'd hear several minutes of screeching before the process died with a cryptic message like \"R Tape Error\" and you'd have to start again.There were many different companies making these computers all competing for the nascent home market. And, with a few notable exceptions, they were all incompatible with each other and the screeching noises on cassette for, say, your ZX Spectrum would be of no use to the kid next door who had a Commodore 64. This presented a problem for Barry and his Chip Shop. The BBC wanted to broadcast software as part of the radio programme but they'd have to play a different set of screeching noises for each type of computer and their regular listeners would be subjected to twenty minutes of screeching noises at a time.The solution lay over the water in The Netherlands. The Dutch public broadcaster NOS had encountered the same problem and had developed a system called BASICODE. Often described as a kind of \"Computer Esperanto\", it allowed the same software to run on different types of computer. You would order a cassette that had BASICODE interpreters for different machines, load up the one that matched your device and then that interpreter would load up the BASICODE program you'd recorded off your radio.The BBC extended this system as BASICODE 2 (and later 2+) to include more functionality and support more brands of computer. And so was born The Chip Shop Takeaway. Late at night when anyone with any sense was asleep and not listening to their radio, the BBC would broadcast BASICODE programs for home computer enthusiasts to record and use on their machines. To call these \"video games\" would be a bit of a stretch as BASICODE didn't really support any kind of graphics but I certainly remember some very basic text-based games amongst a load of academic software which meant absolutely nothing to me as an eight year old boy.Nothing lasts forever though. The mass of competing computer systems became an unsustainable boom market. Manufacturers went broke, the range declined, technology moved on and the boom became a bust. Newer 16 bit machines eschewed cassette storage for new-fangled disk drives and the screeching of a BASICODE takeaway became a forgotten sound on Britain's radio waves. According to Wikipedia BASICODE 3 was also developed and continued to be popular in the old East Germany up until the early 1990s but for those of us in the UK it had already moved into the realm of \"unbelievable facts\".\n\n                    Sign up for Newslttrs\n                    Computers, Geekiness, Occasional Nonsense\n\n                    Subscribe\n\n                .nc-loop-dots-4-24-icon-o{--animation-duration:0.8s}\n                .nc-loop-dots-4-24-icon-o *{opacity:.4;transform:scale(.75);animation:nc-loop-dots-4-anim var(--animation-duration) infinite}\n                .nc-loop-dots-4-24-icon-o :nth-child(1){transform-origin:4px 12px;animation-delay:-.3s;animation-delay:calc(var(--animation-duration)/-2.666)}\n                .nc-loop-dots-4-24-icon-o :nth-child(2){transform-origin:12px 12px;animation-delay:-.15s;animation-delay:calc(var(--animation-duration)/-5.333)}\n                .nc-loop-dots-4-24-icon-o :nth-child(3){transform-origin:20px 12px}\n                @keyframes nc-loop-dots-4-anim{0%,100%{opacity:.4;transform:scale(.75)}50%{opacity:1;transform:scale(1)}}\n\n                Email sent! Check your inbox to complete your signup.\n\n                    No spam. Unsubscribe anytime.",
    "summary": {
      "en": "The article discusses the nostalgic experience of downloading video games from radio broadcasts in the 1980s. The author recalls how people recorded audio from the radio onto cassette tapes, which they then used to load games onto home computers. Despite the author's memories, many commenters on social media doubted this ever happened, claiming it was technically impossible.\n\nIn the early 1980s, the UK faced economic challenges while home computers were just becoming popular. The BBC launched the Computer Literacy Project to educate the public about computers, which included a radio program called \"The Chip Shop.\" This program used a system called BASICODE, developed in the Netherlands, to allow the same software to work on different types of computers. The BBC adapted this system to broadcast programs that enthusiasts could record and play on their machines.\n\nHowever, as technology advanced and the market changed, the use of cassette storage faded, and the practice of downloading games from the radio became a forgotten memory. The article reflects on how this once-common experience now seems unbelievable to younger generations.",
      "ko": "이 기사는 1980년대 라디오 방송을 통해 비디오 게임을 다운로드하던 향수를 불러일으키는 경험에 대해 이야기합니다. 저자는 사람들이 라디오에서 오디오를 녹음해 카세트 테이프에 담고, 이를 이용해 가정용 컴퓨터에서 게임을 로드하던 시절을 회상합니다. 그러나 저자의 기억에도 불구하고, 소셜 미디어의 많은 댓글 작성자들은 이런 일이 실제로 일어났는지 의심하며 기술적으로 불가능하다고 주장했습니다.\n\n1980년대 초 영국은 경제적 어려움에 직면했으며, 가정용 컴퓨터가 막 인기를 끌기 시작했습니다. BBC는 대중에게 컴퓨터에 대한 교육을 제공하기 위해 컴퓨터 문해력 프로젝트를 시작했으며, 이 프로젝트의 일환으로 \"더 칩 샵\"이라는 라디오 프로그램이 방영되었습니다. 이 프로그램은 네덜란드에서 개발된 BASICODE라는 시스템을 사용하여 다양한 종류의 컴퓨터에서 동일한 소프트웨어가 작동할 수 있도록 했습니다. BBC는 이 시스템을 활용해 열성 팬들이 녹음하고 자신의 기계에서 재생할 수 있는 프로그램을 방송했습니다.\n\n하지만 기술이 발전하고 시장이 변화함에 따라 카세트 저장 방식의 사용은 줄어들었고, 라디오에서 게임을 다운로드하는 관행은 잊혀진 기억이 되었습니다. 이 기사는 한때 흔했던 이 경험이 이제는 젊은 세대에게는 믿기 어려운 일로 여겨진다는 점을 반영하고 있습니다.",
      "ja": "この記事では、1980年代にラジオ放送からビデオゲームをダウンロードする懐かしい体験について語られています。著者は、人々がラジオの音声をカセットテープに録音し、それを使って家庭用コンピュータにゲームをロードしていたことを思い出しています。しかし、著者の記憶にもかかわらず、ソーシャルメディアの多くのコメント者は、これが実際に起こったことに疑問を呈し、技術的に不可能だったと主張しています。\n\n1980年代初頭、イギリスは経済的な課題に直面していましたが、家庭用コンピュータが普及し始めていました。BBCは「コンピュータリテラシープロジェクト」を立ち上げ、一般の人々にコンピュータについて教育するための取り組みを行いました。その一環として「ザ・チップショップ」というラジオ番組が放送されました。この番組では、オランダで開発されたBASICODEというシステムを使用し、異なるタイプのコンピュータで同じソフトウェアが動作するようにしていました。BBCはこのシステムを適応させ、愛好者が録音して自分の機械で再生できるプログラムを放送しました。\n\nしかし、技術が進歩し市場が変化するにつれて、カセットテープによるデータ保存の利用は減少し、ラジオからゲームをダウンロードするという行為は忘れられた記憶となりました。この記事は、かつて一般的だったこの体験が、今の若い世代には信じられないものに思えることを反映しています。"
    }
  },
  {
    "id": "796519ee07800ea9",
    "title": {
      "en": "Show HN: GuMCP – Open-source MCP servers, hosted for free",
      "ko": "무료 오픈소스 MCP 서버",
      "ja": "無料のオープンMCPサーバー"
    },
    "type": "story",
    "url": "https://github.com/gumloop/guMCP",
    "score": 67,
    "by": "murb",
    "time": 1743433583,
    "content": "Gumloop Unified Model Context Protocol (guMCP)\nguMCP is an open-source collection of Model Context Protocol (MCP) servers that can be run both remotely and locally. The project aims to create the largest collection of MCP servers with a unified backend, fostering a community around AI integrations and the future of AGI.\nOverview\nWhile many MCP server providers are closed source, and open-source alternatives typically only support local hosting through stdio, guMCP provides:\n\nA comprehensive collection of MCP servers that work both locally and remotely\nSupport for both stdio and SSE (Server-Sent Events) transports\nA unified backend architecture for consistent implementation\nFull open-source access to encourage community contributions\n\nGetting Started\n\n🎬 RECOMMENDED: Watch our quick setup video before getting started!\nThis under 5-minute walkthrough will save you time and help you understand the key concepts.\n\nPrerequisites\n\nPython 3.11\nGit\nFor Windows users: A bash-compatible shell is recommended (Git Bash, WSL, etc.)\n\nInstallation\n\nClone the repository:\ngit clone https://github.com/gumloop/guMCP.git\ncd guMCP\n\nSet up a virtual environment:\nThis isolates the project dependencies from your system Python installation.\n# Create the virtual environment\npython -m venv venv\n\n# Activate it (choose the appropriate command for your OS)\n# On Unix/macOS:\nsource venv/bin/activate\n\n# On Windows (Command Prompt):\nvenv\\Scripts\\activate\n\n# On Windows (PowerShell):\n.\\venv\\Scripts\\Activate.ps1\n\n# On Windows (Git Bash):\nsource venv/Scripts/activate\n\nYou'll know your virtual environment is active when you see (venv) at the beginning of your command prompt.\n\nInstall dependencies:\n# Install core dependencies\npip install -r requirements.txt\n\n# Install development dependencies (for contributing)\npip install -r requirements-dev.txt\n\nConfigure environment variables:\n# Create a local environment file from the template\ncp .env.example .env\n\n# Open the .env file in your preferred text editor and update values as needed\n# If you're using VS Code:\ncode .env\n\nThe .env file contains configuration for:\n\nAPI keys for service integrations\nAuthentication settings\nServer configuration options\nDevelopment environment settings\n\nUsage\nRunning SSE Servers\nAn SSE server can be run locally. This will provide a single URL and host every server available.\nTo start the SSE development server, run:\n./start_sse_dev_server.sh\n\nFor convenience, we also provide a lightweight MCP Client to connect to SSE servers. Usage:\npython tests/clients/RemoteMCPTestClient.py --endpoint=http://localhost:8000/simple-tools-server/local\n\nRunning Stdio Servers\npython src/servers/local.py --server=simple-tools-server\n\nFor convenience, we also provide a lightweight MCP Client to start and connect to stdio servers. Usage:\n python tests/clients/LocalMCPTestClient.py --server=simple-tools-server\n\nSupported Servers and Authentication Methods\nThe following table provides an overview of the current servers implemented in guMCP, their authentication requirements, and relative ease of use with different authentication methods:\n\nServer\nAuth Type\nRemote Auth(e.g., Cursor)\nLocal Auth(e.g., Claude Desktop)\nDocumentation\n\nGoogle Services\n\nGoogle Sheets\nOAuth 2.0\n✅ Seamless with Gumloop auth\n⚠️ Requires GCP project & OAuth setup\nGSheets Docs\n\nGmail\nOAuth 2.0\n✅ Seamless with Gumloop auth\n⚠️ Requires GCP project & OAuth setup\nGmail Docs\n\nGoogle Docs\nOAuth 2.0\n✅ Seamless with Gumloop auth\n⚠️ Requires GCP project & OAuth setup\nGDocs Docs\n\nGoogle Drive\nOAuth 2.0\n✅ Seamless with Gumloop auth\n⚠️ Requires GCP project & OAuth setup\nGDrive Docs\n\nGoogle Calendar\nOAuth 2.0\n✅ Seamless with Gumloop auth\n⚠️ Requires GCP project & OAuth setup\nGCalendar Docs\n\nCommunication Tools\n\nSlack\nOAuth 2.0\n✅ Seamless with Gumloop auth\n⚠️ Requires OAuth setup with HTTPS redirect\nSlack Docs\n\nOutlook\nOAuth 2.0\n✅ Seamless with Gumloop auth\n⚠️ Requires Azure app registration\nOutlook Docs\n\nProductivity Tools\n\nAirtable\nOAuth 2.0\n✅ Seamless with Gumloop auth\n⚠️ Requires Airtable OAuth application\nAirtable Docs\n\nLinear\nOAuth 2.0\n✅ Seamless with Gumloop auth\n⚠️ Requires Linear OAuth application\nLinear Docs\n\nAttio\nMixed\n✅ Seamless with Gumloop auth\n⚠️ Requires Attio OAuth application\nAttio Docs\n\nSearch Tools\n\nPerplexity\nAPI Key\n✅ Seamless with Gumloop auth\n✅ Easy - Simple API key setup\nPerplexity Docs\n\nContributing\nWe welcome contributions! Please see our CONTRIBUTING.md for guidelines.\nKey areas for contribution:\n\nNew server implementations\nTransport improvements\nDocumentation\nTest coverage\nBug fixes\n\nLicense\nThis project is licensed under the GNU General Public License v3.0 (GPL-3.0) - see the LICENSE file for details.\nSecurity\nSecurity is the single highest priority for us.\nThat's why Gumloop is SOC 2 Type II, GDPR, and HIPAA compliant, and why for this OSS repo, we're actively rewarding those who responsibly disclose serious vulnerabilities and exploits to security@gumloop.com.\nCommunity\n\nGitHub Issues: Bug reports and feature requests\nGitHub Discussions: General questions and discussions\nForum: Community discussions and support\n\nAcknowledgments\nThis project builds on the Model Context Protocol (MCP) specification and is inspired by the work of various MCP implementations in the community, especially those at Anthropic.\nThank you to all the contributors that have provided feedback, advice, and early servers for the repository.",
    "summary": {
      "en": "**Gumloop Unified Model Context Protocol (guMCP) Summary**\n\nguMCP is an open-source project that provides a collection of Model Context Protocol (MCP) servers, which can be run either locally or remotely. The goal is to build a large community around AI integrations and the future of Artificial General Intelligence (AGI).\n\n**Key Features:**\n- Offers both local and remote MCP server options.\n- Supports multiple communication methods, including standard input/output (stdio) and Server-Sent Events (SSE).\n- Provides a unified backend for consistent use across all servers.\n- Fully open-source to encourage community contributions.\n\n**Getting Started:**\n1. **Prerequisites:** You need Python 3.11, Git, and a bash-compatible shell for Windows users.\n2. **Installation Steps:**\n   - Clone the repository and navigate to the folder.\n   - Set up a virtual environment to manage dependencies.\n   - Install necessary dependencies listed in the project files.\n   - Configure environment variables using a provided template.\n\n**Usage:**\n- To run an SSE server locally, use a specific command to start it.\n- A lightweight client is available to connect to both SSE and stdio servers for testing.\n\n**Supported Servers:**\nguMCP supports various servers (like Google Services, Slack, Airtable) with different authentication methods, primarily OAuth 2.0.\n\n**Contributing:**\nThe project welcomes contributions in areas like new server implementations, documentation, and bug fixes.\n\n**License and Security:**\nguMCP is licensed under the GNU General Public License v3.0. The project prioritizes security, being compliant with SOC 2 Type II, GDPR, and HIPAA.\n\n**Community Support:**\nUsers can report issues, request features, or participate in discussions through GitHub and community forums.\n\nOverall, guMCP aims to enhance the accessibility and integration of AI services through a collaborative open-source environment.",
      "ko": "guMCP는 오픈 소스 프로젝트로, 로컬 또는 원격에서 실행할 수 있는 모델 컨텍스트 프로토콜(MCP) 서버 모음을 제공합니다. 이 프로젝트의 목표는 인공지능 통합과 인공지능 일반(AGI)의 미래에 대한 대규모 커뮤니티를 구축하는 것입니다.\n\n주요 기능으로는 로컬 및 원격 MCP 서버 옵션을 제공하며, 표준 입력/출력(stdio)과 서버 전송 이벤트(SSE) 등 다양한 통신 방법을 지원합니다. 모든 서버에서 일관된 사용을 위해 통합된 백엔드를 제공하며, 커뮤니티 기여를 장려하기 위해 완전한 오픈 소스입니다.\n\n시작하려면 Python 3.11, Git, 그리고 Windows 사용자를 위한 bash 호환 셸이 필요합니다. 설치 과정은 저장소를 복제하고 폴더로 이동한 후, 의존성을 관리하기 위해 가상 환경을 설정하고, 프로젝트 파일에 나열된 필수 의존성을 설치하며, 제공된 템플릿을 사용해 환경 변수를 구성하는 단계로 이루어집니다.\n\nSSE 서버를 로컬에서 실행하려면 특정 명령어를 사용해야 하며, 경량 클라이언트를 통해 SSE 및 stdio 서버에 연결하여 테스트할 수 있습니다. guMCP는 Google 서비스, Slack, Airtable 등 다양한 서버를 지원하며, 주로 OAuth 2.0을 통한 다양한 인증 방법을 제공합니다.\n\n이 프로젝트는 새로운 서버 구현, 문서화, 버그 수정 등 다양한 분야에서의 기여를 환영합니다. guMCP는 GNU 일반 공공 라이선스 v3.0에 따라 라이선스가 부여되며, SOC 2 Type II, GDPR, HIPAA와 같은 보안 기준을 준수하여 보안을 최우선으로 합니다.\n\n사용자는 GitHub와 커뮤니티 포럼을 통해 문제를 보고하거나 기능 요청 및 토론에 참여할 수 있습니다. 전반적으로 guMCP는 협업적인 오픈 소스 환경을 통해 인공지능 서비스의 접근성과 통합을 향상시키는 것을 목표로 하고 있습니다.",
      "ja": "guMCPは、モデルコンテキストプロトコル（MCP）サーバーのコレクションを提供するオープンソースプロジェクトです。これらのサーバーは、ローカルまたはリモートで実行できます。このプロジェクトの目標は、AI統合と人工一般知能（AGI）の未来に関する大きなコミュニティを築くことです。\n\n主な特徴として、ローカルとリモートのMCPサーバーオプションを提供し、標準入力/出力（stdio）やサーバー送信イベント（SSE）など、複数の通信方法をサポートしています。また、すべてのサーバーで一貫して使用できる統一されたバックエンドを提供し、コミュニティの貢献を促進するために完全にオープンソースです。\n\n始めるためには、まずPython 3.11、Git、およびWindowsユーザー向けのbash互換シェルが必要です。インストール手順としては、リポジトリをクローンしてフォルダに移動し、依存関係を管理するための仮想環境を設定します。その後、プロジェクトファイルに記載された必要な依存関係をインストールし、提供されたテンプレートを使用して環境変数を設定します。\n\n使用方法としては、ローカルでSSEサーバーを実行するための特定のコマンドを使用します。また、SSEおよびstdioサーバーに接続するための軽量クライアントも利用可能です。\n\nguMCPは、Googleサービス、Slack、Airtableなど、さまざまなサーバーをサポートしており、主にOAuth 2.0を使用した異なる認証方法があります。プロジェクトは、新しいサーバーの実装、ドキュメント、バグ修正などの分野での貢献を歓迎しています。\n\nguMCPはGNU一般公衆ライセンスv3.0の下でライセンスされています。プロジェクトはセキュリティを重視しており、SOC 2 Type II、GDPR、HIPAAに準拠しています。ユーザーは、GitHubやコミュニティフォーラムを通じて問題を報告したり、機能をリクエストしたり、ディスカッションに参加したりできます。\n\n全体として、guMCPは協力的なオープンソース環境を通じてAIサービスのアクセス性と統合を向上させることを目指しています。"
    }
  },
  {
    "id": "d757cd2a7f90103d",
    "title": {
      "en": "Show HN: Wasp – the first full-stack framework powered by an LLM",
      "ko": "Wasp: LLM의 첫 풀스택 프레임워크",
      "ja": "Wasp: LLM全開の新フレームワーク"
    },
    "type": "story",
    "url": "https://wasp.sh/blog/2025/04/01/wasp-first-full-stack-framework-powered-by-llm",
    "score": 13,
    "by": "matijash",
    "time": 1743514435,
    "content": "Wasp: The first full-stack framework powered by an LLM - running on vibes, not a compilerApril 1, 2025 · 4 min readMatija SosicCo-founder & CEO @ WaspFor those of you who are new here, we have been building a full-stack, batteries-included web framework for the last four years. You can think of Wasp as a modern, JS-based incarnation of Laravel, Django or Ruby on Rails. We based it on a custom compiler so it doesn’t depend on the specific stack or the architecture in the long run (currently it supports React, Node.js and Prisma).Wasp just crossed 16,000 stars on GitHub, with thousands of devs using it both in their startups and enterprises.Run on vibes, not the compilerStill, with the recent developments in the LLM-powered code generation, we realised our current approach simply isn’t feasible anymore. We decided that instead of implementing and maintaning our own compiler, we can simply outsource that to an LLM.Let's get these vibes a'goin'.Here's why it makes much more sense and completely changes the game:Nobody looks at the generated code anymoreLet's face it - this is what it's gonna be.Gone are the days when we as developers argue about which is the best UI or state management library. No more asking our colleagues or on Reddit \"what is everyone using nowadays,\" just to figure out there's a cool new thing you now have to learn.All of that is now delegated to the AI — it has been trained on millions of codebases, and it can bring a much more sound decision than we can with our limited experience. If it turns out that its weapon of choice is jQuery or Redux v3.7.0 from 2017, so be it. It's not like anybody is ever going to be looking at the actual code and try to refactor it manually.Full-stack goes “fluid” - never get the same app twiceWe’re now entering the era of a generative and so-called “fluid” UI. Instead of the traditional hard-coding of UIs for the specific pages (e.g. User profile, Settings, Billing, …), the AI will generate a personalized UI for each user. We decided to take that one step further - not only UI should be customized, but rather all the parts of the stack, including backend logic and the database models.That means each time you change or re-deploy your app, it will be slightly different. Sometimes it might be completely unnoticeable change (e.g. a different hashing algorithm or lodash using another version), and sometimes a feature will work a bit differently than the last time a user tried it.There are two obvious benefits to this:Your UI will always feel “fresh” and keep your users engaged, making sure they don’t get caught up in a rut.It’s a great security enhancement - good luck hacking an app that constantly changes it’s implementationPrompts are the new code, but with less hassleCode is so 2024. Embrace the prompt.With these fundamental changes, Wasp will move from the classical notion of the framework to the novel AI-first, prompt-based system for building full-stack experiences. Wasp will preserve a certain degree of backward compatibility with the previous versions and let developers define their own components, and even the backend logic, but we recommend everyone to start transitioning towards the new paradigm.We’ve discovered that prompts are vastly superior to the code - they don’t have to be versioned nor shared among the team members. Each new team member can simply browse the app and discover its features, instead of reading through all the tedious code and tests.Lastly, there is no specialized knowledge required - anybody can start contributing right away.Thoughts and ideas? We’d like to hear from you!As always, we’re grateful for your support and would love to hear from you. We realize this is quite a big change but are confident in the new direction we’re taking with Wasp. AI and LLM-powered code generation offers so many new possibilities and it is our responsibility to find the best way to make use of it.Let us know what you think on our Discord - we’re looking forward to seeing you there!Tags:updateEdit this page",
    "summary": {
      "en": "**Summary of Wasp's New Approach:**\n\nWasp is a web framework similar to Laravel and Django, built over four years. It currently supports React, Node.js, and Prisma and has gained popularity with over 16,000 stars on GitHub.\n\nRecently, Wasp has shifted to using AI for code generation instead of maintaining its own compiler. This change allows developers to stop worrying about which libraries to use, as the AI will make these decisions based on its extensive training on code.\n\nWasp is also moving towards a “fluid” user interface (UI), where each app deployment might create a slightly different version. This means users will always experience a fresh interface, enhancing engagement and security.\n\nThe framework will transition to a prompt-based system for building applications, making it easier for new team members to understand the app without needing to read complex code. This approach requires less specialized knowledge, enabling anyone to contribute.\n\nWasp is excited about these changes and encourages feedback from the community.",
      "ko": "Wasp는 Laravel과 Django와 유사한 웹 프레임워크로, 4년 동안 개발되었습니다. 현재 React, Node.js, Prisma를 지원하며, GitHub에서 16,000개 이상의 별을 받아 인기를 끌고 있습니다.\n\n최근 Wasp는 자체 컴파일러를 유지하는 대신 AI를 활용해 코드 생성을 진행하기로 방향을 전환했습니다. 이 변화로 개발자들은 어떤 라이브러리를 사용할지 고민할 필요가 없어졌습니다. AI가 방대한 코드 학습을 바탕으로 이러한 결정을 내리기 때문입니다.\n\nWasp는 또한 “유동적인” 사용자 인터페이스(UI)로 나아가고 있습니다. 각 애플리케이션 배포가 약간씩 다른 버전을 생성할 수 있다는 의미입니다. 이를 통해 사용자들은 항상 새로운 인터페이스를 경험하게 되어 참여도와 보안이 향상됩니다.\n\n프레임워크는 애플리케이션 구축을 위한 프롬프트 기반 시스템으로 전환할 예정입니다. 이 방식은 새로운 팀원이 복잡한 코드를 읽지 않고도 애플리케이션을 이해할 수 있도록 도와줍니다. 따라서 전문 지식이 덜 필요해 누구나 기여할 수 있는 환경이 조성됩니다.\n\nWasp는 이러한 변화에 대해 기대하고 있으며, 커뮤니티의 피드백을 적극적으로 환영하고 있습니다.",
      "ja": "Waspは、LaravelやDjangoに似たウェブフレームワークで、4年間の開発を経て完成しました。現在、React、Node.js、Prismaをサポートしており、GitHubでは16,000以上のスターを獲得するなど人気を集めています。\n\n最近、Waspは独自のコンパイラを維持する代わりに、AIを使ったコード生成にシフトしました。この変更により、開発者はどのライブラリを使用するかを心配する必要がなくなります。AIは、膨大なコードのトレーニングに基づいてこれらの決定を行います。\n\nさらに、Waspは「流動的」なユーザーインターフェース（UI）に向かっています。これにより、各アプリのデプロイメントはわずかに異なるバージョンを生成する可能性があります。ユーザーは常に新鮮なインターフェースを体験でき、エンゲージメントやセキュリティが向上します。\n\nフレームワークは、アプリケーションを構築するためのプロンプトベースのシステムに移行します。これにより、新しいチームメンバーは複雑なコードを読むことなくアプリを理解しやすくなります。このアプローチは、専門的な知識を必要とせず、誰でも貢献できるようになります。\n\nWaspはこれらの変化に期待を寄せており、コミュニティからのフィードバックを歓迎しています。"
    }
  },
  {
    "id": "002d9de6024d7719",
    "title": {
      "en": "Technical Analysis – Improper Use of Private iOS APIs in Vietnamese Banking Apps",
      "ko": "베트남 은행 앱의 iOS API 문제",
      "ja": "ベトナム銀行アプリの危険なAPI使用"
    },
    "type": "story",
    "url": "https://blog.verichains.io/p/technical-analysis-improper-use-of",
    "score": 84,
    "by": "quyleanh",
    "time": 1743145441,
    "content": "Share this postVerichainsTechnical Analysis - Improper Use of Private iOS APIs in some Vietnamese Banking AppsCopy linkFacebookEmailNotesMoreDiscover more from VerichainsLeading finance security firm in APAC. Trusted by top blockchain customers such as Binance, Bullish, Bybit, Galaxy Digital, Polygon, BNB Chain, Aptos, Sui, Kakao, Line Corp, Abu Dhabi Blockchain Center (ADBC), as well as many banks and mobile wallets.SubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.Already have an account? Sign inTechnical Analysis - Improper Use of Private iOS APIs in some Vietnamese Banking AppsTechnical analysis reveals that BIDV and Agribank mobile banking apps exploit private iOS API side channel issue to detect other apps installed on users’ iPhone/iPad devices.Thanh NguyenMar 28, 202520Share this postVerichainsTechnical Analysis - Improper Use of Private iOS APIs in some Vietnamese Banking AppsCopy linkFacebookEmailNotesMoreShareBài viết tiếng Việt có thể xem tại đâyPhân tích: Lạm dụng API riêng tư trên iOS ở một số app ngân hàng ViệtThanh Nguyen·3월 29일Read full storyIntroductionYesterday, reports emerged in the Vietnamese infosec community that two popular banking apps – BIDV SmartBanking and Agribank – were using hidden / private iOS API to detect other apps installed on users’ iPhones. This behavior was initially highlighted by @opa334, developer of TrollStore, on infosec.exchange two days ago and later in a Facebook post on Vietnamse J2TEAM forum. The controversy quickly gained traction because such behavior suggests a violation of Apple’s policies and an invasion of user privacy.The initial information in the Facebook post mentioned suspicions regarding our BShield Mobile Security solution, leading to a misunderstanding that inaccurately affected the reputation of BShield and Verichains. Therefore, we conducted a thorough technical analysis of the mentioned bank apps BIDV SmartBanking (v5.2.62, updated on Mar 14, 2025) and Agribank Plus (v5.1.8, updated on Mar 25, 2025), detailed in this write-up, which also examines implications for users and the banking apps.Disclaimer:\nThis analysis was released strictly for security research, transparency, and customer protection purposes. This document also aims to clarify false claims regarding BShield's involvement, promote technical transparency.\n\nWhile we strive for accuracy, we make no representations or warranties of any kind, express or implied, about the completeness, accuracy, reliability, suitability, or availability with respect to the information, products, services, or related graphics contained in this blog for any purpose. The information provided in this article is not intended to encourage, guide, or support any actions that violate the policies of Apple, Google, or current legal regulations. We shall not be held liable for any damages or consequences arising from the misuse of the information presented in this analysis.Technical AnalysisTLDR;The apps (BIDV SmartBanking and Agribank Plus) were reported of using the private iOS API SBSLaunchApplicationWithIdentifierAndURLAndLaunchOptions to detect TrollStore and other applications installed on users’ devices. Technical analysis reveals that BIDV SmartBanking (v5.2.62, updated on Mar 14, 2025) and Agribank Plus (v5.1.8, updated on Mar 25, 2025), developed by VNPay, use commercial Mobile App Shield DexProtector/Licel and Dexguard/iXGuard, together with a self-developed code likely named “VNPay Runtime Protection” or “VNPShield”. VNPay Runtime Protection employs a mechanism exploiting side-channel issue of iOS private API SBSLaunchApplicationWithIdentifierAndURLAndLaunchOptions to detect app presence on user iOS devices, combining with a very weak XOR encryption trying to hide the private API strings from scanning. This side-channel issue works on latest iOS versions on non-jailbroken device. According to Apple Security Bounty Program,  this issue likely falls under the category \"Device attack via user-installed app – Unauthorized access to sensitive data\", qualifying for a $5,000 bug bounty. Exploiting private APIs to enumerate installed apps violates Apple App Store policies and pose a significant risk of App removal by Apple, potentially impacting millions of bank customers. This policy violation has NO relation to BShield of Verichains. The primary purpose of application shielding is to proactively protect banking applications against malicious apps on user devices, preventing account takeover, theft of money and personal data. From time to time, we are aware of various new detection methods, including certain tricks utilizing N-day exploits or private APIs. However, as an bank-grade Mobile App Protection solution protecting more than 100 millions users, BShield strictly uses only stable and legitimate detection techniques.  We do NOT use risky or dangerous methods, ensuring full compliance with store policies, prioritizing user and business security amid rising threats targeting banking apps.Mechanism of ObfuscationTo hide their use of the private API, the apps employ a very weak XOR encryption to obfuscate sensitive strings, such as API names and application bundle identifiers. This technique likely aims to conceal API strings from static analysis tools, making it harder to detect the misuse of APIs. At runtime, the encrypted strings are decrypted by XORing them with the key to reveal their original values.The key is “94826663\" on BIDV app (“41818020” on Agribank app) and encrypted data is stored in the data section. The decryption function, named RE_Decrypt_10233F108, can be represented in Python as:Snippet of  decrypted strings from encrypted dataFrom the decrypted strings, we noted that this code likely a part of a module named VNPay Runtime Protection. Returning to the original code, we observed string usage patterns as below:The code block works as below:Creating metadata for constants stored in the data section.Call swift_initStaticObject to initialize the static object and the value stored at another location in the data section (in this case, the result will be a Swift String object).Calling RE_Decrypt_10233F108 to decrypt strings to process.This could be simplified as v40 = dlopen(XOR_DECRYPT(X), 1), where X is the encrypted string in the data section. Sample code snippet invoking the an API call from decrypted stringsThe above code snippet equivalent to invoke the csops function from /usr/lib/libSystem.B.dylib\"/usr/lib/libSystem.B.dylib\".csops(a1, a2, a3, a4);Similarly, the code using the same method to invoke other private APIs, for examples:Private API to detect Apps installed on users’ iOS devicesCode snippet detects TrollStore and other appsThe code check for installed application by calling private iOS API SBSLaunchApplicationWithIdentifier with the bundle ID of the app. Typically, developers launch apps through schemes if supported. However, this API directly launches any app if the calling app has Apple-issued entitlement. Hence, this private API could be used as a side channel to verify app existence through returned error codes. Note that this API from SpringBoard is a private API and not officially provided by Apple for developers so using it violates Apple Store policy.List of some targeted apps checks:org.coolstar.SileoStorecom.opa334.Dopamine.roothidecom.roothide.managercom.cokepokes.AppStorePlusxyz.willy.Zebracom.opa334.Dopaminecom.kahsooa.piqwkk.dummyLLDB screenshot below at breakpoint in SBSLaunchApplicationWithIdentifierAndURLAndLaunchOptions showing the check for ”com.opa334.Dopamine.roothide”.SBSLaunchApplicationWithIdentifierAndURLAndLaunchOptions is called from SBSLaunchApplicationWithIdentifier.The private API is used as a side channel to verify app existence through returned error codes:If installed: returns security policy error (error number 9) due to lack of Apple entitlement).If not installed: returns application not found (error number 7) .This demonstrates a side-channel exploit via private iOS verifying if an app is installed, executed in class call VNPShieldBridgingManager. We confirmed that SBSLaunchApplicationWithIdentifier private API could be used as a side channel to verify app existence through returned error codes, works on latest iOS versions on non-jailbroken devices.When running the banking application that exploit this private API to detect software installed on a user's device, the app's console log will show error messages indicating attempts to launch applications without permissions.Apple Store Policy ViolationAccording to Apple’s App Store Review Guidelines (Sections 2.5.1 and Legal 5), using non-public (private) APIs or hidden system calls without explicit user consent breaches data transparency, user control, and security standards, undermining user trust.Specifically, Guideline 2.5.1 mandates that apps \"may only use public APIs\" and must utilize these APIs strictly for their intended purposes. Any use of internal, undisclosed system frameworks or functions will result in app rejection or removal. Apple enforces these guidelines to maintain app stability, protect user privacy, and uphold platform security.Beyond just compliance, abusing private APIs can be a security and privacy issue. Apple’s rules exist partly to prevent apps from accessing data or capabilities users didn’t consent to. For example, using hidden system calls to probe a device’s state can violate user privacy and platform security. Scanning a user’s device for other installed apps without permission is explicitly disallowed and undermines user trust.Attempting to bypass iOS sandbox restrictions or gather unauthorized data (e.g., installed app lists) is a major violation, raising serious red flags for Apple and security-conscious users. Such practices risk app bans or removal from the App Store, potentially impacting millions of bank customers.In this analysis, we confirm that SBSLaunchApplicationWithIdentifier private API could be used as a side channel to verify app existence through returned error codes, works on latest iOS versions on non-jailbroken devices. From Apple Security Bounty Program,  this shall likely fall into “Device attack via user-installed app - Unauthorized access to sensitive data” for the $5,000 bug bounty. ConclusionTechnical analysis reveals that BIDV SmartBanking (v5.2.62, updated on Mar 14, 2025) and Agribank Plus (v5.1.8, updated on Mar 25, 2025), developed by VNPay, use commercial Mobile App Shield DexProtector/Licel and Dexguard/iXGuard, together with a self-developed code likely named “VNPay Runtime Protection” or “VNPShield”. VNPay Runtime Protection employs a mechanism exploiting side-channel issue of iOS private API SBSLaunchApplicationWithIdentifierAndURLAndLaunchOptions to detect app presence on user iOS devices, combining with a very weak XOR encryption trying to hide the private API strings from scanning. This side-channel issue works on latest iOS versions on non-jailbroken device. According to Apple Security Bounty Program,  this issue likely falls under the category \"Device attack via user-installed app – Unauthorized access to sensitive data\", qualifying for a $5,000 bug bounty. Exploiting private APIs to enumerate installed apps violates Apple App Store policies and pose a significant risk of App removal by Apple, potentially impacting millions of bank customers. This policy violation has NO relation to BShield of Verichains. The primary purpose of application shielding is to proactively protect banking applications against malicious apps on user devices, preventing account takeover, theft of money and personal data. From time to time, we are aware of various new detection methods, including certain tricks utilizing N-day exploits or private APIs. However, as an bank-grade Mobile App Protection solution protecting more than 100 millions users, BShield strictly uses only stable and legitimate detection techniques.  We do NOT use risky or dangerous methods, ensuring full compliance with store policies, prioritizing user and business security amid rising threats targeting banking apps.Subscribe to VerichainsLaunched 3 years agoLeading finance security firm in APAC. Trusted by top blockchain customers such as Binance, Bullish, Bybit, Galaxy Digital, Polygon, BNB Chain, Aptos, Sui, Kakao, Line Corp, Abu Dhabi Blockchain Center (ADBC), as well as many banks and mobile wallets.SubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.20Share this postVerichainsTechnical Analysis - Improper Use of Private iOS APIs in some Vietnamese Banking AppsCopy linkFacebookEmailNotesMoreSharePreviousNext",
    "summary": {
      "en": "A recent analysis by Verichains revealed that two Vietnamese banking apps, BIDV SmartBanking and Agribank Plus, improperly use a private iOS API to detect other apps installed on users’ devices. This practice violates Apple’s policies and raises privacy concerns. \n\nThe apps exploit a specific API called SBSLaunchApplicationWithIdentifierAndURLAndLaunchOptions, which can identify other installed applications without user consent. The analysis highlighted that these apps use weak encryption to hide their API usage, making it challenging to detect. \n\nThis behavior can lead to severe consequences, including the potential removal of the apps from the App Store, affecting millions of customers. Verichains clarified that their own security solution, BShield, is not involved in this misuse and prioritizes user safety by adhering to legitimate protection techniques. \n\nOverall, the findings emphasize the importance of respecting user privacy and complying with app store regulations to maintain trust and security.",
      "ko": "최근 Verichains의 분석에 따르면, 두 개의 베트남 은행 앱인 BIDV SmartBanking과 Agribank Plus가 사용자의 기기에 설치된 다른 앱을 감지하기 위해 사적인 iOS API를 부적절하게 사용하고 있는 것으로 나타났습니다. 이러한 행위는 애플의 정책을 위반하며, 개인 정보 보호에 대한 우려를 불러일으킵니다.\n\n이 앱들은 SBSLaunchApplicationWithIdentifierAndURLAndLaunchOptions라는 특정 API를 이용해 사용자의 동의 없이 다른 설치된 애플리케이션을 식별할 수 있습니다. 분석 결과, 이 앱들은 API 사용을 숨기기 위해 약한 암호화를 사용하고 있어 이를 탐지하기가 어렵습니다.\n\n이러한 행동은 앱이 앱 스토어에서 삭제될 가능성을 포함해 심각한 결과를 초래할 수 있으며, 이는 수백만 고객에게 영향을 미칠 수 있습니다. Verichains는 자사의 보안 솔루션인 BShield는 이러한 부정 사용과 관련이 없으며, 합법적인 보호 기술을 준수하여 사용자 안전을 최우선으로 한다고 밝혔습니다.\n\n전반적으로 이번 발견은 사용자 개인 정보 보호를 존중하고 앱 스토어 규정을 준수하는 것이 신뢰와 보안을 유지하는 데 얼마나 중요한지를 강조합니다.",
      "ja": "最近のVerichainsの分析によると、ベトナムの銀行アプリであるBIDV SmartBankingとAgribank Plusが、ユーザーのデバイスにインストールされている他のアプリを不正に検出するために、プライベートなiOS APIを使用していることが明らかになりました。この行為はAppleのポリシーに違反しており、プライバシーに関する懸念を引き起こしています。\n\nこれらのアプリは、SBSLaunchApplicationWithIdentifierAndURLAndLaunchOptionsという特定のAPIを利用しており、ユーザーの同意なしに他のインストールされたアプリを特定することができます。分析では、これらのアプリがAPIの使用を隠すために弱い暗号化を用いているため、検出が難しいことが指摘されています。\n\nこのような行動は深刻な結果を招く可能性があり、アプリがApp Storeから削除されることも考えられ、何百万もの顧客に影響を及ぼす恐れがあります。Verichainsは、自社のセキュリティソリューションであるBShieldはこの不正使用には関与しておらず、正当な保護技術を遵守することでユーザーの安全を優先していると明言しました。\n\n全体として、これらの調査結果は、ユーザーのプライバシーを尊重し、アプリストアの規則を守ることの重要性を強調しています。信頼と安全を維持するためには、これらの原則を守ることが必要です。"
    }
  },
  {
    "id": "6edd397dfee718eb",
    "title": {
      "en": "You Can Still Read NASA's Deleted \"First Woman\" Graphic Novels",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://nasawatch.com/education/you-can-still-read-nasas-deleted-first-woman-graphic-novels/",
    "score": 29,
    "by": "rbanffy",
    "time": 1743501306,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "5c8f972af5850570",
    "title": {
      "en": "AWS S3 sync does not sync all the files",
      "ko": "S3 동기화 문제",
      "ja": "S3同期の落とし穴"
    },
    "type": "story",
    "url": "https://github.com/aws/aws-cli/issues/3273",
    "score": 10,
    "by": "pera",
    "time": 1743504306,
    "content": "JordonPhillipsadded closing-soonThis issue will automatically close in 4 days unless further comments are made.This issue will automatically close in 4 days unless further comments are made. on Apr 28, 2018\n\nkyleknapremoved closing-soonThis issue will automatically close in 4 days unless further comments are made.This issue will automatically close in 4 days unless further comments are made. on Jun 15, 2018\n\njustnancementioned this on Feb 21, 2019Sync missing files #3957\n\nSync missing files #3957",
    "summary": {
      "en": "The text discusses an issue that was set to close automatically in 4 days unless more comments were added. Jordon Phillips noted this on April 28, 2018, and Kyle Knap later removed the closing notice on June 15, 2018. Additionally, Justin mentioned this issue again on February 21, 2019, referring to \"Sync missing files #3957.\"",
      "ko": "이 내용은 4일 후 자동으로 종료될 예정인 문제에 대해 다루고 있습니다. 추가 댓글이 없으면 종료된다는 점이 강조되었습니다. 조던 필립스는 2018년 4월 28일에 이 사실을 언급했으며, 카일 크납은 2018년 6월 15일에 종료 알림을 삭제했습니다. 또한 저스틴은 2019년 2월 21일에 \"동기화 누락 파일 #3957\"을 언급하며 이 문제를 다시 언급했습니다.",
      "ja": "この文章では、4日後に自動的に閉じる予定だった問題について述べています。ジョーダン・フィリップスは2018年4月28日にこのことに気づき、カイル・クナップはその後、2018年6月15日に閉じる通知を削除しました。また、ジャスティンは2019年2月21日に「同期が欠けているファイル #3957」に言及し、この問題を再度取り上げました。"
    }
  },
  {
    "id": "0a15d4a411ede364",
    "title": {
      "en": "Exptv.org is a 24/7 curated, montage of vintage and obscure media",
      "ko": "레트로 미디어 24/7",
      "ja": "ビンテージメディアの宝庫"
    },
    "type": "story",
    "url": "https://exptv.org/",
    "score": 12,
    "by": "mraniki",
    "time": 1743472972,
    "content": "WITCHES BREW MIXTAPE\n\n              EXP TV GUIDE\n\n                Monday\n\n                Tuesday\n\n                Wednesday\n\n                Thursday\n\n                Friday\n\n                Saturday\n\n                Sunday\n\n              5am‑6pm\n\n              VIDEO BREAKSClassic MTV style video collage series featuring never-ending and ever-changing archival clips on every subject imaginable\n\n              VIDEO BREAKSClassic MTV style video collage series featuring never-ending and ever-changing archival clips on every subject imaginable\n\n              VIDEO BREAKSClassic MTV style video collage series featuring never-ending and ever-changing archival clips on every subject imaginable\n\n              VIDEO BREAKSClassic MTV style video collage series featuring never-ending and ever-changing archival clips on every subject imaginable\n\n              VIDEO BREAKSClassic MTV style video collage series featuring never-ending and ever-changing archival clips on every subject imaginable\n\n              VIDEO BREAKSClassic MTV style video collage series featuring never-ending and ever-changing archival clips on every subject imaginable\n\n              VIDEO BREAKSClassic MTV style video collage series featuring never-ending and ever-changing archival clips on every subject imaginable\n\n              7pm\n\n              VIDEO BREAKSClassic MTV style video collage series featuring never-ending and ever-changing archival clips on every subject imaginable\n\n              UNDERGROUND USA #7Ongoing series archiving relics of 80s music subculture.  Tonight: Punks & Poseurs\n\n              VIDEO BREAKSClassic MTV style video collage series featuring never-ending and ever-changing archival clips on every subject imaginable\n\n              VIDEO BREAKSClassic MTV style video collage series featuring never-ending and ever-changing archival clips on every subject imaginable\n\n              UNDERGROUND USA #9Ongoing series archiving relics of 80s music subculture.  Tonight: Fringe\n\n              VIDEO BREAKSClassic MTV style video collage series featuring never-ending and ever-changing archival clips on every subject imaginable\n\n              VIDEO BREAKSClassic MTV style video collage series featuring never-ending and ever-changing archival clips on every subject imaginable\n\n              8pm\n\n              UNDERGROUND USA #1Ongoing series archiving relics of 80s music subculture.  Tonight: Fashion\n\n              LOVE IS...Spend Valentine's Day with the one you love...EXP TV\n\n              UNDERGROUND USA #3Ongoing series archiving relics of 80s music subculture.  Tonight: Lifestyles\n\n              UNDERGROUND USA #6Ongoing series archiving relics of 80s music subculture.  Tonight: Brainwash\n\n              TOTAL NEWSThe totally not fake news according to EXP TV\n\n              VIDEO BREAKSClassic MTV style video collage series featuring never-ending and ever-changing archival clips on every subject imaginable\n\n              VIDEO BREAKSClassic MTV style video collage series featuring never-ending and ever-changing archival clips on every subject imaginable\n\n              9pm\n\n              TOTAL NEWSThe totally not fake news according to EXP TV\n\n              FONE FRIENDSThe 1-900 MIX. $2 a Call\n\n              EVERYBODY IS A STARA public access mixtape\n\n              SONGS YOU CAN SEE #2Second installment of this Audio/Video collaboration between Edan and Tom\n\n              MOSAIC #2Rare films from the world of fine art\n\n              KUNG FU WIZARDSIt’s a non-stop supernatural showdown when martial arts Merlins face off and utilize every lo-fi special fx trick in their Buddah-blessed arsenals, zapping each other with lasers, fire, skulls, ghost blood and haunted peaches!\n\n              PIXEL POWERAn archival history of computer graphix.  Now 30min longer!\n\n              ↓\n\n              ECHO PARTYEdan's classic megamix audio adventure collab with EXP TV\n\n              ↓\n\n              ↓\n\n              PLANTS ARE PEOPLE TOOA revealing glimpse into the often hidden and amazing world of plants\n\n              ↓\n\n              ↓\n\n              10pm\n\n              KUNG FU WIZARDSIt’s a non-stop supernatural showdown when martial arts Merlins face off and utilize every lo-fi special fx trick in their Buddah-blessed arsenals, zapping each other with lasers, fire, skulls, ghost blood and haunted peaches!\n\n              PIXEL POWERAn archival history of computer graphix.  Now 30min longer!\n\n              JAMAICA NO PROBLEMVideo crash course in Jamaican music\n\n              WITCHES BREW MIXTAPEDeep dive history of witches onscreen\n\n              SONGS YOU CAN SEE #1Audio/Video collaboration between Edan and Tom\n\n              BOLLYWEIRDWildest Bollywood musical segments\n\n              DAVID BOWIE MIXTAPEBowie’s glory years captured on rare film and video\n\n              11pm\n\n              CATS!Cats, cats, cats in cinema and beyond\n\n              INCREDIBLY STRANGE METALExactly what the title says\n\n              NOTHING BUT STAR WARSStar Wars gone wild, weird, and wrong--Rare curios\n\n              MOSAIC #1Rare films from the world of fine art\n\n              FONE FRIENDSThe 1-900 MIX. $2 a Call\n\n              DISCO ODYSSEY #2Italo disco video mixtape\n\n              ↓Bowie continued...\n\n              ↓\n\n              WOW #1Psychedelic and experimental animation from around the world\n\n              ↓\n\n              HALLOWEEN FREAKER'S BALLHalloween video mixtape\n\n              INCREDIBLY STRANGE METALExactly what the title says\n\n              DISCO ODYSSEY #1Space disco video mixtape\n\n              BIGFOOTSearching for sasquatch in cinema\n\n              12am\n\n              BIZARRE ENCOUNTERSGhosts, Aliens, Witches, Pyramids, ESP and EXP\n\n              WITCHES BREW MIXTAPEDeep dive history of witches onscreen\n\n              ↓continued\n\n              MIDNIGHT MOVIESHistory of the midnite movie phenomenon thru trailers\n\n              JAMAICA NO PROBLEMVideo crash course in Jamaican music\n\n              EVERYBODY IS A STARA public access mixtape\n\n              LOVE IS...Spend Valentine's Day with the one you love...EXP TV\n\n              ↓\n\n              ↓\n\n              COSMONAUTUltimate Russian sci-fi megamix with whacked out synths\n\n              ↓\n\n              ↓\n\n              ↓\n\n              ↓\n\n              1am\n\n              ↓Bizarre Encounters continues...\n\n              FUNK OFF feat. Cut ChemistCut Chemist’s definitive comp of 80s French tape culture\n\n              DISCO ODYSSEY #1Space disco video mixtape\n\n              POMEGRANATESPre-revolutionary Iranian cinema and music\n\n              NOTHING BUT STAR WARSStar Wars gone wild, weird, and wrong--Rare curios\n\n              SONGS YOU CAN SEE #2Second installment of this Audio/Video collaboration between Edan and Tom\n\n              BIZARRE ENCOUNTERSGhosts, Aliens, Witches, Pyramids, ESP and EXP\n\n              LA VIDEOTHEQUERare 60s and 70s French pop promo videos\n\n              CZECH YOUR HEAD60s Czech pop music and video\n\n              DISCO ODYSSEY #2Italo disco video mixtape\n\n              WOW #1Psychedelic and experimental animation from around the world\n\n              ↓\n\n              ↓\n\n              ↓\n\n              2am\n\n              OUTLAW COUNTRYA crash course in the genre\n\n              BOLLYWEIRDWildest Bollywood musical segments\n\n              CATS!Cats, cats, cats in cinema and beyond\n\n              THE ZANZIBAR FILMSSurvey of a forgotten period of acid soaked French film history born from the revolutionary May 1968\n\n              ↓continued\n\n              HALLOWEEN FREAKER'S BALLHalloween video mixtape\n\n              ↓Bizarre Encounters continues...\n\n              PLANTS ARE PEOPLE TOOA revealing glimpse into the often hidden and amazing world of plants\n\n              ↓\n\n              ↓\n\n              ↓\n\n              LA VIDEOTHEQUERare 60s and 70s French pop promo videos\n\n              MOSAIC #2Rare films from the world of fine art\n\n              FUNK OFF feat. Cut ChemistCut Chemist’s definitive comp of 80s French tape culture\n\n              3am\n\n              NIGHT COMFORTUnwind with calming found footage\n\n              NIGHT COMFORTUnwind with calming found footage\n\n              NIGHT COMFORTUnwind with calming found footage\n\n              NIGHT COMFORTUnwind with calming found footage\n\n              NIGHT COMFORTUnwind with calming found footage\n\n              CZECH YOUR HEAD60s Czech pop music and video\n\n              MOSAIC #1Rare films from the world of fine art\n\n              ↓\n\n              ↓\n\n              ↓\n\n              ↓\n\n              ↓\n\n              MELT #1Pure zonk culture jam\n\n              MELT #1Pure zonk culture jam\n\n              4am\n\n              VIDEO BREAKSClassic MTV style video collage series featuring never-ending and ever-changing archival clips on every subject imaginable\n\n              VIDEO BREAKSClassic MTV style video collage series featuring never-ending and ever-changing archival clips on every subject imaginable\n\n              VIDEO BREAKSClassic MTV style video collage series featuring never-ending and ever-changing archival clips on every subject imaginable\n\n              VIDEO BREAKSClassic MTV style video collage series featuring never-ending and ever-changing archival clips on every subject imaginable\n\n              VIDEO BREAKSClassic MTV style video collage series featuring never-ending and ever-changing archival clips on every subject imaginable\n\n              NIGHT COMFORTUnwind with calming found footage\n\n              NIGHT COMFORTUnwind with calming found footage\n\n                  TIME ZONE IS UNIVERSAL\n\n        EXP TV\n        EXP TV is a live tv channel broadcasting an endless stream of obscure media and video ephemera.EXP TV’s daytime block is \"Video Breaks\"--a video collage series featuring wild, rare, unpredictable, and ever-changing archival clips touching on every subject imaginable. The nighttime block starts at 10pm and features specialty themed video mixes and deep dives.In an age where we all waste so much time figuring out what to watch online, EXP TV airs 24/7 and there’s always something cool on.Stay Tuned. Read more about us here.\n\n      ×\n\n                    SEND US A MESSAGE\n\n                        Submit\n\n                    Message sent!\n                    FOLLOW EXP TV ON INSTAGRAM & PATREON\n\n        ×",
    "summary": {
      "en": "**EXP TV Overview**\n\nEXP TV is a live TV channel that broadcasts a continuous stream of unique and obscure video content. \n\n**Daytime Programming:**\n- The main feature is \"Video Breaks,\" a series of classic MTV-style video collages showcasing a variety of archival clips on numerous topics.\n\n**Nighttime Programming (Starting at 10 PM):**\n- The schedule includes themed video mixes and deep dives into specific subjects. Examples of shows include:\n  - \"Underground USA\": A series about 80s music subculture.\n  - \"Witches Brew Mixtape\": A look at the history of witches on screen.\n  - \"Kung Fu Wizards\": A supernatural martial arts showdown.\n  - \"Pixel Power\": An exploration of computer graphics history.\n  - \"Bizarre Encounters\": A mix of ghosts, aliens, and other strange phenomena.\n\n**Additional Highlights:**\n- Other shows include music-themed segments, experimental animations, and cultural explorations, such as Jamaican music and Bollywood musicals.\n\nEXP TV offers a diverse range of content 24/7, making it easy for viewers to find something interesting to watch at any time.",
      "ko": "EXP TV는 독특하고 생소한 비디오 콘텐츠를 지속적으로 방송하는 라이브 TV 채널입니다. \n\n낮 시간대 프로그램의 주요 특징은 \"비디오 브레이크\"로, 다양한 주제에 대한 고전 MTV 스타일의 비디오 콜라주를 보여주는 시리즈입니다. \n\n밤 10시부터 시작되는 야간 프로그램은 주제별 비디오 믹스와 특정 주제에 대한 심층 탐구로 구성됩니다. 예를 들어, 80년대 음악 서브컬처를 다룬 \"언더그라운드 USA\", 스크린에서의 마녀 역사에 대한 \"위치스 브루 믹스테이프\", 초자연적인 무술 대결을 그린 \"쿵푸 위자드\", 컴퓨터 그래픽 역사에 대한 탐구인 \"픽셀 파워\", 그리고 유령, 외계인 등 이상한 현상을 다룬 \"기이한 만남\" 같은 프로그램이 있습니다. \n\n기타 프로그램으로는 음악을 주제로 한 세그먼트, 실험 애니메이션, 자메이카 음악과 볼리우드 뮤지컬 같은 문화 탐구도 포함됩니다. EXP TV는 24시간 다양한 콘텐츠를 제공하여 언제든지 흥미로운 프로그램을 쉽게 찾아볼 수 있도록 합니다.",
      "ja": "EXP TVは、独特で珍しい映像コンテンツを継続的に放送するライブテレビチャンネルです。\n\n昼間のプログラムでは、「ビデオブレイク」というコーナーが主な特徴で、さまざまなテーマに関するアーカイブ映像を集めたMTVスタイルのビデオコラージュが紹介されます。\n\n夜間のプログラムは午後10時から始まり、テーマに沿ったビデオミックスや特定のテーマに深く掘り下げた内容が放送されます。例えば、「アンダーグラウンドUSA」という80年代の音楽サブカルチャーに関するシリーズや、「ウィッチズブリュー・ミックステープ」という画面上の魔女の歴史を探る番組、「カンフーウィザード」という超自然的な武道の対決を描いたもの、「ピクセルパワー」というコンピュータグラフィックスの歴史を探る番組、そして「ビザールエンカウンター」という幽霊や宇宙人、その他の奇妙な現象を取り上げたミックス番組があります。\n\nその他のハイライトには、音楽に関連したセグメントや実験的なアニメーション、ジャマイカ音楽やボリウッドミュージカルなどの文化的探求が含まれています。\n\nEXP TVは、24時間365日多様なコンテンツを提供しており、視聴者がいつでも興味深いものを見つけやすい環境を整えています。"
    }
  },
  {
    "id": "ab4516cfbe6a1723",
    "title": {
      "en": "Blue95: a desktop for your childhood home's computer room",
      "ko": "추억의 컴퓨터 방, 블루95",
      "ja": "青い95の思い出"
    },
    "type": "story",
    "url": "https://github.com/winblues/blue95",
    "score": 555,
    "by": "elvis70",
    "time": 1743348622,
    "content": "blue95\n\nA desktop for your childhood home's computer room.\n\nBlue95 is a modern and lightweight desktop experience that is reminiscent of a bygone era of computing.\nBased on Fedora Atomic Xfce with the Chicago95 theme.\nFor more screenshots, see screenshots.md.\nTry It Out\nNoteLive CD is a new feature and is still in testing.\n\nWe are now creating a Live ISO that can be used to boot into a Blue95 live environment. Test it out without needing to install anything.\nNote that the included installer is an alpha version and it is recommended to instead install Blue95 via the other methods listed below.\nInstallation\nFrom ISO\nWe are currently having issues with our installer ISOs. The current recommended installation path is though rebasing from a different Fedora Atomic desktop, preferably from an Xfce-based image such as winblues/vauxite.\nAfter installing vauixite, you can rebase directly to this image with:\nrpm-ostree rebase ostree-image-signed:docker://ghcr.io/winblues/blue95:latest\n\nFrom Other Atomic Desktops\nIf you are currently using an atomic desktop, you can rebase to the latest blue95 image.\n\nFirst rebase to the unsigned image, to get the proper signing keys and policies installed:\nrpm-ostree rebase ostree-unverified-registry:ghcr.io/winblues/blue95:latest\n\nReboot and then rebase to the signed image, like so:\nrpm-ostree rebase ostree-image-signed:docker://ghcr.io/winblues/blue95:latest\n\nIt is recommended to create a new user after rebasing.\nProject Goals\n\nMatch upstream Fedora Xfce in terms of core system components and update schedule.\nPull in tweaks from Universal Blue (e.g. codecs, automatic updates, etc) to produce a more usable out-of-the box experience.\nProvide an aesthetic rooted in a bygone era of computing.\n\nNon goals:\n\nFaithful reproduction of design elements from decades old operating systems. Whenever usability and exact replication are at odds, usability and accessibility will generally be preferred.\n\nShoutouts\n\n@grassmunk/@dominichayesferen for Chicago95 and Chicagofier respectively\nBlueBuild, Universal Blue and Fedora\nThe Xfce team",
    "summary": {
      "en": "**Summary of Blue95**\n\nBlue95 is a modern desktop experience designed to resemble the classic computing era. It is based on Fedora Atomic Xfce and features the Chicago95 theme. \n\n**Key Features:**\n- **Live Testing:** Users can try Blue95 through a Live ISO without installation. The installer is still in early testing.\n- **Installation Options:** \n  - Recommended to install by rebasing from another Xfce-based Fedora Atomic desktop rather than using the installer ISO due to issues.\n  - Instructions are provided for rebasing from other atomic desktops to Blue95.\n\n**Project Goals:**\n- Align with Fedora Xfce in terms of updates and core components.\n- Enhance usability with features from Universal Blue, like codecs and automatic updates.\n- Create a nostalgic aesthetic without strictly replicating old designs.\n\n**Notable Contributors:**\nThanks to @grassmunk, @dominichayesferen, BlueBuild, Universal Blue, Fedora, and the Xfce team for their contributions.",
      "ko": "Blue95는 클래식 컴퓨팅 시대를 연상시키는 현대적인 데스크탑 환경입니다. Fedora Atomic Xfce를 기반으로 하며, Chicago95 테마를 특징으로 합니다.\n\n사용자는 설치 없이 Live ISO를 통해 Blue95를 체험할 수 있습니다. 설치 프로그램은 아직 초기 테스트 단계에 있습니다. 설치는 설치 프로그램 ISO를 사용하는 것보다 다른 Xfce 기반의 Fedora Atomic 데스크탑에서 재베이스하는 것이 권장됩니다. 다른 아토믹 데스크탑에서 Blue95로 재베이스하는 방법에 대한 안내도 제공됩니다.\n\n이 프로젝트의 목표는 Fedora Xfce와 업데이트 및 핵심 구성 요소를 일치시키는 것입니다. Universal Blue의 기능을 활용하여 사용성을 향상시키고, 코덱 및 자동 업데이트와 같은 기능을 추가할 계획입니다. 또한, 옛 디자인을 엄격히 재현하기보다는 향수를 불러일으키는 미적 감각을 창출하는 것을 목표로 하고 있습니다.\n\n이 프로젝트에 기여한 @grassmunk, @dominichayesferen, BlueBuild, Universal Blue, Fedora, 그리고 Xfce 팀에게 감사의 말씀을 전합니다.",
      "ja": "Blue95は、クラシックなコンピュータ時代を彷彿とさせる現代的なデスクトップ体験を提供します。Fedora Atomic Xfceを基盤にしており、Chicago95テーマを採用しています。\n\nユーザーは、インストールなしでLive ISOを通じてBlue95を試すことができます。インストーラーはまだ初期段階のテスト中です。インストールに関しては、問題があるため、インストーラーISOを使用するのではなく、他のXfceベースのFedora Atomicデスクトップからのリベースを推奨しています。他のアトミックデスクトップからBlue95へのリベース方法についての指示も提供されています。\n\nプロジェクトの目標は、Fedora Xfceと更新やコアコンポーネントの整合性を保つことです。また、Universal Blueからの機能を取り入れ、使いやすさを向上させることを目指しています。古いデザインを厳密に再現するのではなく、懐かしい美しさを創出することも重要なポイントです。\n\nこのプロジェクトには、@grassmunk、@dominichayesferen、BlueBuild、Universal Blue、Fedora、Xfceチームなど、多くの貢献者が関わっています。"
    }
  },
  {
    "id": "c0f0c7317fae684f",
    "title": {
      "en": "Unmasking a slow and steady password spray attack",
      "ko": "비밀을 벗긴 패스워드 공격",
      "ja": "パスワード攻撃の真実"
    },
    "type": "story",
    "url": "https://petrasecurity.substack.com/p/unmasking-a-slow-and-steady-password",
    "score": 61,
    "by": "noleary",
    "time": 1743225194,
    "content": "Share this postMicrosoft Detection Deep DivesUnmasking A Slow and Steady Password Spray AttackCopy linkFacebookEmailNotesMoreDiscover more from Microsoft Detection Deep DivesDeep dives on how to detect Microsoft account compromises, from the team at Petra SecuritySubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.Already have an account? Sign inUnmasking A Slow and Steady Password Spray AttackCatching an attacker hiding in plain sight with some creative log slicingAdithya VellalMar 29, 20251Share this postMicrosoft Detection Deep DivesUnmasking A Slow and Steady Password Spray AttackCopy linkFacebookEmailNotesMore2ShareWe caught an attack where the first successful login we saw for a user was malicious. How we could be sure this was an attack? After all, we had no baseline for this user …Let’s dive in. The User Timeline: 1 Failed Login —> 1 Successful LoginThis is all the data we had for the user at the time of the attack:1-26-25: Failed login to Microsoft Azure CLI from Hurricane Electric LLC, a data center in Mexico1-29-25: Successful login to Microsoft Azure CLI from different IP in the same data centerFrom just these two logins, it’s pretty tricky to tell whether this is expected behavior or an attack. On one hand, command line logins usually come from people running cloud computing workloads in data centers. Programmatic workloads also often rotate IPs using tools like FireProx. On the other hand, this could be an attacker who compromised this account with a brute force script. Perhaps they’re using the data center to run a large scale attack campaign targeting users all over the world. The key to solving this case is to look beyond this single user and examine all the recent activity in this tenant. The Tenant Timeline: 24 Users Targeted in 1 WeekWhen we look at all the logins for this tenant from Mexico, we see an attack campaign hiding in plain sight: this attacker targeted 24 distinct users using Microsoft Azure CLI. They tried every user no more than 2 times to avoid setting off any brute force detections. They also used a slew of IPs from the range 2001:0470:c8e0::/48 to evade any IOC-based detections. Takeaway: Looking at User Activity Timelines Isn’t EnoughThe repeated failed logins across multiple users, combined with clear attempts to avoid triggering alarms, make it clear that this is an attack. We would have missed this if we just analyzed the activity timelines for each individual user.Slicing Microsoft’s logs in creative ways allows us to unmask attackers hiding in plain sight. In this particular case, shifting our perspective from user activity to tenant-wide activity was the key. Subscribe to Microsoft Detection Deep DivesBy Adithya Vellal · Launched 4 months agoDeep dives on how to detect Microsoft account compromises, from the team at Petra SecuritySubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.1Share this postMicrosoft Detection Deep DivesUnmasking A Slow and Steady Password Spray AttackCopy linkFacebookEmailNotesMore2Share",
    "summary": {
      "en": "This text discusses a method for detecting a password spray attack on Microsoft accounts. The situation involved monitoring login attempts for a specific user, where one failed login was followed by a successful one. Initially, it was unclear if this was legitimate activity or an attack.\n\nTo investigate, the team looked at the broader context of logins within the same tenant. They discovered that 24 different users were targeted in one week, with the attacker carefully managing their attempts to avoid detection. They used various IP addresses and limited their tries per user, which indicated a coordinated attack rather than normal user behavior.\n\nThe key takeaway is that focusing solely on individual user activity can miss broader attack patterns. By examining activity across the entire tenant, the team could identify the attack and take action. This highlights the importance of analyzing data creatively to detect hidden threats.",
      "ko": "이 텍스트는 마이크로소프트 계정에서 비밀번호 스프레이 공격을 탐지하는 방법에 대해 설명합니다. 특정 사용자의 로그인 시도를 모니터링하는 과정에서 한 번의 실패한 로그인 뒤에 성공적인 로그인이 발생했습니다. 처음에는 이것이 정상적인 활동인지 공격인지 불분명했습니다.\n\n조사를 위해 팀은 같은 테넌트 내의 로그인 활동을 더 넓은 맥락에서 살펴보았습니다. 그들은 일주일 동안 24명의 다른 사용자가 공격의 대상이 되었음을 발견했습니다. 공격자는 탐지를 피하기 위해 로그인 시도를 신중하게 관리했습니다. 다양한 IP 주소를 사용하고 사용자당 시도를 제한한 점에서 이는 정상적인 사용자 행동이 아닌 조직적인 공격임을 나타냅니다.\n\n주요 요점은 개별 사용자 활동에만 집중하면 더 넓은 공격 패턴을 놓칠 수 있다는 것입니다. 전체 테넌트의 활동을 분석함으로써 팀은 공격을 식별하고 대응할 수 있었습니다. 이는 숨겨진 위협을 탐지하기 위해 데이터를 창의적으로 분석하는 것이 얼마나 중요한지를 강조합니다.",
      "ja": "このテキストでは、Microsoftアカウントに対するパスワードスプレー攻撃を検出する方法について説明しています。特定のユーザーのログイン試行を監視している中で、一度の失敗したログインの後に成功したログインがあったため、最初はこれが正当な活動なのか攻撃なのか不明でした。\n\n調査のために、チームは同じテナント内のログインの全体的な状況を確認しました。その結果、1週間の間に24人の異なるユーザーが標的にされていることがわかりました。攻撃者は検出を避けるために、試行回数をユーザーごとに制限し、さまざまなIPアドレスを使用していました。これは、通常のユーザー行動ではなく、計画的な攻撃を示しています。\n\n重要なポイントは、個々のユーザーの活動だけに注目すると、広範な攻撃パターンを見逃す可能性があるということです。テナント全体の活動を調べることで、チームは攻撃を特定し、対策を講じることができました。これは、隠れた脅威を検出するためにデータを創造的に分析する重要性を強調しています。"
    }
  },
  {
    "id": "a0b53628d2adfa65",
    "title": {
      "en": "Fragments of a rare Merlin manuscript from c. 1300",
      "ko": "희귀 멀린 원고 조각",
      "ja": "1300年のマーレン断片"
    },
    "type": "story",
    "url": "https://www.cam.ac.uk/stories/merlin-manuscript-discovered-cambridge",
    "score": 132,
    "by": "derbOac",
    "time": 1743384429,
    "content": "MODERN MAGIC UNLOCKS MERLIN’S MEDIEVAL SECRETS\n              Fragments of a rare Merlin manuscript from c. 1300 have been discovered and digitised in a ground-breaking three-year project at Cambridge University Library\n\n              By Jessica Keating\n\n            Amélie Deblauwe and Błażej Mikuła undertaking 3D scanning of the manuscript.\n\n            Amélie Deblauwe and Błażej Mikuła undertaking 3D scanning of the manuscript.\n\n        (function(w) {\n          var el = (window.__shadowRoot || document).getElementById('IkTO8r6BjR-background-viewport');\n          w.Shorthand && w.Shorthand.initInstantImage ? w.Shorthand.initInstantImage(el) : w.SHPreloadInstantImages ? w.SHPreloadInstantImages.push(el) : w.SHPreloadInstantImages = [el];\n          w.Shorthand.initFocalPointPictures();\n        })(window)\n\n                  A fragile 13th century manuscript fragment, hidden in plain sight as the binding of a 16th-century archival register, has been discovered in Cambridge and revealed to contain rare medieval stories of Merlin and King Arthur.\n                  The manuscript, first discovered at Cambridge University Library in 2019, has now been identified as part of the Suite Vulgate du Merlin, a French-language sequel to the legend of King Arthur. The story was part of the Lancelot-Grail cycle, a medieval best seller but few now remain.\n                  There are less than 40 surviving manuscripts of the Suite Vulgate du Merlin, with each one unique since they were individually handwritten by medieval scribes.This latest discovery has been identified as having been written between 1275 and 1315.\n\n                        The inner front cover of the manuscript as it was discovered. Seen here are some of the folds, including flaps and turn-ins, making reading and accessing the text hidden beneath particularly difficult without damaging the material.\n\n                        The inner front cover of the manuscript as it was discovered. Seen here are some of the folds, including flaps and turn-ins, making reading and accessing the text hidden beneath particularly difficult without damaging the material.\n\n                  The manuscript had survived the centuries after being recycled and repurposed in the 1500s as the cover for a property record from Huntingfield Manor in Suffolk, owned by the Vanneck family of Heveningham.\n                  It meant the remarkable discovery was folded, torn, and even stitched into the binding of the book - making it almost impossible for Cambridge experts to access it, read it, or confirm its origins.\n                  What followed the discovery has been a ground-breaking collaborative project, showcasing the work of the University Library’s Cultural Heritage Imaging Laboratory (CHIL) and combining historical scholarship with cutting-edge digital techniques, to unlock the manuscript's long-held secrets - without damaging the unique document.\n\n                A delicate discovery\n\n                  Dr Irène Fabry-Tehranchi, French Specialist in Collections and Academic Liaison at Cambridge University Library, was among those who first recognized the importance of the find.\n\n                    \"It was first thought to be a 14th century story about Sir Gawain but further examination revealed it to be part of the Old French Vulgate Merlin sequel, a different and extremely significant Arthurian text.\"\n\n                  As every manuscript of the period was copied by hand, it means each one is distinctive and reflects the variations introduced by medieval scribes.\n                  This one is believed to belong to the short version of the Vulgate Merlin, and small errors—such as the mistaken use of the name \"Dorilas\" instead of \"Dodalis\"— will help Dr Fabry-Tehranchi and her colleague Nathalie Koble (ENS Paris), to trace its lineage among surviving manuscripts.\n\n                        The outer right cover, photographed in standard conditions as seen by the naked eye, shows the wear and tear sustained over the centuries, including a band of faded text around the middle, crumples and tears.\n\n                        The outer right cover, photographed in standard conditions as seen by the naked eye, shows the wear and tear sustained over the centuries, including a band of faded text around the middle, crumples and tears.\n\n                  The way the manuscript has been carefully executed, with decorated initials in red and blue, gave further clues to its origins and helped indicate that it was produced between the end of the 13th and the beginning of the 14th century.\n                  The text is written in Old French, the language of the court and aristocracy in medieval England following the Norman Conquest and this particular fragment belongs to the genre of Arthurian romances which were intended for a noble audience, including women.\n\n                        The inner cover\n\n                        The inner cover\n\n                  The fragment tells two key episodes from the end of the Suite Vulgate du Merlin. The first part recounts the victory of the Christians against the Saxons at the Battle of Cambénic. It tells of the fight of Gauvain (with his sword Excalibur, his horse Gringalet and his supernatural powers), his brothers, and his father King Loth, against the Saxon Kings Dodalis, Moydas, Oriancés, and Brandalus.\n\n                        The outer left cover and extension\n\n                        The outer left cover and extension\n\n                  The second passage presents a more courtly scene, set on the Feast of the Assumption of the Virgin Mary, with Merlin appearing at Arthur’s court disguised as a harpist—a moment that highlights his magical abilities and his importance as an advisor to the king.\n\n                      Dr Irène Fabry-Tehranchi holding the rare manuscript before inserting it into the Micro CT scanner in the Department of Zoology.\n\n                      Dr Irène Fabry-Tehranchi holding the rare manuscript before inserting it into the Micro CT scanner in the Department of Zoology.\n\n                      The medieval fragment was discovered in 2019 in this box of court rolls during the re-cataloguing of the manorial and estate records relating to the Vannecks of Heveningham (Suffolk).\n\n                      The medieval fragment was discovered in 2019 in this box of court rolls during the re-cataloguing of the manorial and estate records relating to the Vannecks of Heveningham (Suffolk).\n\n                “While they were rejoicing in the feast, and Kay the seneschal brought the first dish to King Arthur and Queen Guinevere, there arrived the most handsome man ever seen in Christian lands. He was wearing a silk tunic girded by a silk harness woven with gold and precious stones which glittered with such brightness that it illuminated the whole room.\"\n                A translation from the Suite Vulgate du Merlin manuscript found at Cambridge University Library\n\n                Innovative digitisation techniques\n\n                Multispectral Imaging: Principle Component Analysis (PCA) of the back inner cover\n\n                Multispectral Imaging: Principle Component Analysis (PCA) of the back inner cover\n\n                      Sally Kilby (Conservation Department) and Błażej Mikuła (CHIL) carefully photograph inside the folds of the manuscript.\n\n                      Sally Kilby (Conservation Department) and Błażej Mikuła (CHIL) carefully photograph inside the folds of the manuscript.\n\n                      This image, made with Multispectral Imaging and processed using the Minimal Noise Fraction method, brings out annotations on the left-hand side which were invisible to the naked eye, including the stamp ‘Huntingfield’ believed to have been added in the 16th century when the manuscript was repurposed as a binding.\n\n                      This image, made with Multispectral Imaging and processed using the Minimal Noise Fraction method, brings out annotations on the left-hand side which were invisible to the naked eye, including the stamp ‘Huntingfield’ believed to have been added in the 16th century when the manuscript was repurposed as a binding.\n\n                      This image, taken with a Multispectral Imaging camera and processed using the Minimal Noise Fraction method, brought out the band of faded and erased text across the cover, making it readable again. One theory by researchers is that at some point over the centuries, there was a band (possibly made of an oily material such as leather) around the book holding it in place which rubbed off the text. Through this digitisation process, they were able to bring out the missing text digitally, allowing researchers to read the story clearly and help place the manuscript amongst the Merlin literary canon.\n\n                      This image, taken with a Multispectral Imaging camera and processed using the Minimal Noise Fraction method, brought out the band of faded and erased text across the cover, making it readable again. One theory by researchers is that at some point over the centuries, there was a band (possibly made of an oily material such as leather) around the book holding it in place which rubbed off the text. Through this digitisation process, they were able to bring out the missing text digitally, allowing researchers to read the story clearly and help place the manuscript amongst the Merlin literary canon.\n\n                      3D model produced using Artec Space Spider\n\n                      3D model produced using Artec Space Spider\n\n                  The discovery set off an extensive conservation and research project, supported by the University Library with funding from Cambridge Digital Humanities.\n                  The fragment’s condition posed a significant challenge. It was fragile, with tears and folds that made it difficult to handle. Traditional methods of conservation might have involved physically removing the binding to unfold the fragment, but this risked causing irreparable damage.\n                  Instead, the team decided to preserve the fragment in situ, keeping it as an example of 16th-century archival binding practices while using cutting-edge technology to virtually unfold and digitise it.\n\n                    “It’s not just about the text itself, but also about the material artefact. The way it was reused tells us about archival practices in 16th-century England. It’s a piece of history in its own right.”\n                      Dr. Fabry-Tehranchi explaining the importance of preserving the fragment in its original state\n\n                  A multidisciplinary team comprising curators, conservators, and imaging specialists from across the University of Cambridge, including departments such as Archives and Modern Manuscripts, Conservation & Heritage, and Collections and Academic Liaison, all worked together with CHIL to analyse and digitise the fragment.\n                  The methods undertaken by Amélie Deblauwe, Błażej Mikuła and Maciej Pawlikowski from CHIL, with the support of Jennifer Murray from the Library’s Conservation Department, allowed them to unfold the fragment virtually and access hidden parts of the text.\n                  To achieve this, the team undertook:\n                  Multispectral Imaging (MSI) This technique used in CHIL involved capturing the fragment in various wavelengths of light, from ultraviolet to infrared. The high-resolution images produced by MSI allowed the team to enhance the readability of the text. Images processed using geospatial software revealed details that were invisible to the naked eye. The fragment had been heavily rubbed and worn from its use as a cover, but MSI helped to bring out the text and highlighted annotations in the margins.\n                  Computed Tomography (CT) scanningConducted with equipment and expertise from the University’s Zoology department, the team used a powerful X-ray scanner—typically used for scanning fossils or skeletons—to virtually penetrate the layers of parchment and uncover hidden structures in the binding. It provided a 3D model of the fragment and its binding and allows researchers to examine the structure of the binding without physically dismantling it. The scan revealed how the fragment had been stitched into the cover, providing insights into 16th-century archival binding techniques.\n\n                      A micro CT scan of the binding structure utilises advanced segmenting structures to separate different materials within the manuscript, such as threads used in the binding. By digitally removing the book and leaving just the thread, the model provides a clearer view, enabling precise viewing and facilitating more accurate research. Use your mouse to rotate, zoom, and examine the binding in detail.\n\n                  A CT scanner relies on the difference in density of the material. In this case both materials were the same density. The pages were stitched using thin strips of the same or similar parchment. The team spotted enough gap between the pages and the stitching to record that separation.\n                  3D modelling Industrial scanning techniques created highly detailed virtual models of the fragment, allowing researchers to study its creases, stitching, and folds in remarkable detail.\n\n                      A 3D model of the manuscript. Use your mouse to rotate, zoom, and examine the text in detail.\n\n                  Virtual unfoldingThe fragment’s text was not in a straightforward sequence; parts of it were hidden under folds or stitched into the binding. Using mirrors, prisms, magnets, and other tools, the team at CHIL carefully photographed each section of the fragment. The hundreds of resulting images were then painstakingly reassembled digitally, much like a jigsaw, to create a coherent image of the text. By manipulating the digital images, the team could simulate what the document might look like if it were physically opened.Scroll down to watch the unfolding.\n\n            Virtual opening of the Merlin fragment. This video showcases a 3D animation that simulates the parchment unfolding. It uses composite images to uncover the previously concealed text.\n\n            Virtual opening of the Merlin fragment. This video showcases a 3D animation that simulates the parchment unfolding. It uses composite images to uncover the previously concealed text.\n\n                  Maciej M Pawlikowsk, Head of The Cultural Heritage Imaging Laboratory (CHIL) at Cambridge University Library said:\n\n                    “This project was a fabulous opportunity to employ all possible advanced imaging techniques from our photographic arsenal. And each of them brought something very important to light. This resulted in the creation of a set of unique digital objects which placed the original fragment in a whole new context and has transformed our understanding of it.”\n\n                  Dr. Fabry-Tehranchi described the process as “like solving a puzzle.” \"If this had been done 30 years ago, the fragment might have been cut, unfolded, and flattened. But today, preserving it in situ gives us a crucial insight into 16th-century archival practices, as well as access to the medieval story itself.\"\n                  A model for the future\n                  Beyond revealing the fragment’s story of Merlin, this project has established a new benchmark for the conservation and digitisation of medieval fragments.\n                  \"This project was not just about unlocking one text—it was about developing a methodology that can be used for other manuscripts. Libraries and archives around the world face similar challenges with fragile fragments embedded in bindings, and our approach provides a model for non-invasive access and study.\"\n                  The discovery has already sparked interest among researchers and conservators, including those at the UK’s National Archives, who are keen to explore how these techniques might be applied to their own collections.\n\n                        Amélie Deblauwe, Chief Photographic Technician, CHIL, using a probe macro lens. A digitsation technique she used on the Merlin manuscript.\n\n                        Amélie Deblauwe, Chief Photographic Technician, CHIL, using a probe macro lens. A digitsation technique she used on the Merlin manuscript.\n\n                  The team hopes the project will inspire further research into medieval manuscripts hidden in unexpected places. The story of King Arthur and Merlin has been told and retold for centuries, but thanks to modern technology, we are still uncovering new chapters.\n\n                The digital results of the project are now available for everyone to explore online via theCambridge Digital Library. This means that for the first time, scholars and public alike can explore the fragment in unprecedented detail.  The digital edition includes high-resolution images, multispectral scans, and 3D models, allowing viewers to rotate, zoom, and examine the text as if handling the manuscript itself.\n\n                Join Dr Irene Fabry-Tehranchi, Amélie Deblauwe and Błażej Mikuła at this year’s Cambridge Festival on Wednesday 26 March. They will be showing and explaining to audiences in-person how these advanced digitisation techniques allowed the virtual unfolding of Merlin’s fragment for the first time. Book your spot HERE\n\n                  The text in this work is licensed under a Creative Commons Attribution 4.0 International License.\n\n                  25 March 2025Words and design: Jessica KeatingManuscript photography: Cambridge University Library / CHILAll other photography: Cambridge University Library / Błażej Mikuła / Amélie Deblauwe\n\n    TopBuilt withShorthand",
    "summary": {
      "en": "**Summary:**\n\nA rare manuscript fragment from around 1300, containing medieval stories about Merlin and King Arthur, has been found and digitised at Cambridge University Library. Initially discovered in 2019, this fragment is part of the *Suite Vulgate du Merlin*, a sequel to the Arthurian legend, with fewer than 40 unique copies still existing. \n\nThe manuscript was hidden as the binding of a 16th-century property record, making it difficult to access without damage. A collaborative project using advanced digital techniques allowed researchers to uncover its secrets without harming it.\n\nThe fragment includes two stories: one about a battle between Christians and Saxons, and another featuring Merlin at Arthur's court. The manuscript’s text is in Old French and decorated with colorful initials, indicating it was created for a noble audience.\n\nResearchers used innovative methods like multispectral imaging and CT scans to reveal hidden text and understand the manuscript's binding. These techniques provided insights into 16th-century archival practices and allowed for a virtual unfolding of the text.\n\nThe project has set a new standard for conserving fragile manuscripts and has generated interest among researchers worldwide. The digitised version of the manuscript is now available online for public exploration, showcasing high-resolution images and 3D models.",
      "ko": "1300년경의 희귀한 원고 조각이 발견되어 케임브리지 대학교 도서관에서 디지털화되었습니다. 이 원고는 중세의 멀린과 아서 왕에 관한 이야기를 담고 있으며, 2019년에 처음 발견되었습니다. 이 조각은 아서 전설의 후속작인 *Suite Vulgate du Merlin*의 일부로, 현재 40개 미만의 독특한 사본만이 남아 있습니다.\n\n이 원고는 16세기 재산 기록의 제본 안에 숨겨져 있어 손상 없이 접근하기 어려웠습니다. 그러나 첨단 디지털 기술을 활용한 협력 프로젝트 덕분에 연구자들은 원고를 해치지 않고 그 비밀을 밝혀낼 수 있었습니다.\n\n조각에는 기독교인과 색슨족 간의 전투에 관한 이야기와 아서 왕의 궁정에서 멀린이 등장하는 이야기가 포함되어 있습니다. 원고의 텍스트는 고대 프랑스어로 작성되었으며, 화려한 머리글 장식이 있어 귀족을 위한 작품임을 나타냅니다.\n\n연구자들은 다중 스펙트럼 이미징과 CT 스캔과 같은 혁신적인 방법을 사용하여 숨겨진 텍스트를 드러내고 원고의 제본을 이해했습니다. 이러한 기술은 16세기 기록 보관 관행에 대한 통찰을 제공하고, 텍스트를 가상으로 펼칠 수 있게 해주었습니다.\n\n이 프로젝트는 섬세한 원고 보존에 대한 새로운 기준을 세웠으며, 전 세계 연구자들의 관심을 끌고 있습니다. 디지털화된 원고는 이제 온라인에서 공개되어 고해상도 이미지와 3D 모델을 통해 탐색할 수 있습니다.",
      "ja": "1300年頃の珍しい写本の断片が、ケンブリッジ大学図書館で発見され、デジタル化されました。この断片は、アーサー王伝説の続編である『スイート・ヴルガート・デュ・メルラン』の一部で、現存するユニークなコピーは40未満です。2019年に最初に発見されたこの写本は、16世紀の不動産記録の製本の中に隠されており、損傷を避けるためにアクセスが難しい状態でした。\n\n高度なデジタル技術を用いた共同プロジェクトにより、研究者たちはこの写本の秘密を傷めることなく明らかにしました。断片には、キリスト教徒とサクソン人の戦いに関する物語と、アーサー王の宮廷でのメルリンに関する物語の二つが含まれています。写本のテキストは古フランス語で書かれており、色鮮やかな頭文字で飾られていることから、貴族向けに作られたことがわかります。\n\n研究者たちは、多スペクトルイメージングやCTスキャンといった革新的な手法を用いて、隠れたテキストを明らかにし、写本の製本について理解を深めました。これらの技術は、16世紀のアーカイブの実践についての洞察を提供し、テキストを仮想的に展開することを可能にしました。\n\nこのプロジェクトは、脆弱な写本の保存に新たな基準を設け、世界中の研究者の関心を集めています。デジタル化された写本は現在、オンラインで一般に公開されており、高解像度の画像や3Dモデルを通じて探索することができます。"
    }
  },
  {
    "id": "681f8f9457fe9b4e",
    "title": {
      "en": "France fines Apple €150M for “excessive” pop-ups that let users reject tracking",
      "ko": "프랑스, 애플에 150억 유로 벌금!",
      "ja": "アップル、ポップアップで1.5億ユーロの罰金"
    },
    "type": "story",
    "url": "https://arstechnica.com/tech-policy/2025/03/france-fines-apple-e150m-for-excessive-pop-ups-that-let-users-reject-tracking/",
    "score": 244,
    "by": "sebastian_z",
    "time": 1743442696,
    "content": "Apple's App Tracking Transparency\n\n        France fines Apple €150M for “excessive” pop-ups that let users reject tracking\n\n        Requiring \"double consent\" for user tracking is too much, French agency says.\n\n    Jon Brodkin\n\n  –\n\n    2025년 4월 1일 오전 2:25\n    |\n\n    125\n\n      An iPhone 15 Pro and a MacBook Pro in 2023.\n\n          Credit:\n\n          Getty Images | dontree_m\n\n      An iPhone 15 Pro and a MacBook Pro in 2023.\n\n          Credit:\n\n          Getty Images | dontree_m\n\n      Text\n        settings\n\n            Story text\n\n          Size\n\n  Small\n  Standard\n  Large\n\nWidth\n      *\n\n  Standard\n  Wide\n\nLinks\n\n  Standard\n  Orange\n\n    * Subscribers only\n    Learn more\n\n            Minimize to nav\n\n          France's competition regulator fined Apple €150 million, saying the iPhone maker went overboard in its implementation of pop-up messages that let users consent to or reject tracking that third-party applications use for targeted advertising.\nThe App Tracking Transparency (ATT) framework used by Apple on iPhones and iPads since 2021 makes the use of third-party applications too complex and hurts small companies that rely on advertising revenue, said a press release today by the Autorité de la concurrence (Competition Authority). The system harms \"smaller publishers in particular since, unlike the main vertically integrated platforms, they depend to a large extent on third-party data collection to finance their business,\" the agency said.\nUser consent obtained via the ATA framework \"authorizes the application in question to collect user data for targeted advertising purposes,\" the agency said. \"If consent is given, the application can access the Identifier for Advertisers ('IDFA'), the identifier by which each device can be tracked through its use of third-party applications and sites.\" The French investigation was triggered by a complaint lodged by advertising industry associations.\nThe intent of ATT \"is not problematic in terms of the likely benefits for users as regards privacy protection,\" but \"how the framework is implemented is abusive within the meaning of competition law,\" the agency said. Apple's \"implementation methods artificially complicate the use of third-party applications and distort the neutrality of the framework to the detriment of small publishers financed by advertising,\" it said.\n\n            Third-party publishers \"cannot rely on the ATT framework to comply with their legal obligations,\" so they \"must continue to use their own consent collection solution,\" the French agency said. \"The result is that multiple consent pop-ups are displayed, making the use of third-party applications in the iOS environment excessively complex.\"\n\nA typical ATT pop-up asks a user whether to allow an app \"to track your activity across other companies' apps and websites,\" and says that \"your data will be used to deliver personalized ads to you.\"\nAgency: “Double consent” too cumbersome\nThe agency said there is an \"asymmetry\" in which user consent for Apple's own data collection is obtained with a single pop-up, but other publishers are \"required to obtain double consent from users for tracking on third-party sites and applications.\" The press release notes that \"while advertising tracking only needs to be refused once, the user must always confirm their consent a second time.\"\nThe system was said to be less harmful for big companies like Meta and Google and \"particularly harmful for smaller publishers that do not enjoy alternative targeting possibilities, in particular in the absence of sufficient proprietary data.\" Although France's focus is on how ATT affects smaller companies, Apple's privacy system has also been criticized by Facebook.\nThe €150 million fine won't make much of a dent in Apple's revenue, but Apple will apparently have to make some changes to comply with the French order. The agency's press release said the problem \"could be avoided by marginal modifications to the ATT framework.\"\nBenoit Coeure, the head of France's competition authority, \"told reporters the regulator had not spelled out how Apple should change its app, but that it was up to the company to make sure it now complied with the ruling,\" according to Reuters. \"The compliance process could take some time, he added, because Apple was waiting for rulings on regulators in Germany, Italy, Poland and Romania who are also investigating the ATT tool.\"\nApple said in a statement that the ATT \"prompt is consistent for all developers, including Apple, and we have received strong support for this feature from consumers, privacy advocates, and data protection authorities around the world. While we are disappointed with today's decision, the French Competition Authority (FCA) has not required any specific changes to ATT.\"\n\n      Jon Brodkin\n      Senior IT Reporter\n\n      Jon Brodkin\n      Senior IT Reporter\n\n      Jon is a Senior IT Reporter for Ars Technica. He covers the telecom industry, Federal Communications Commission rulemakings, broadband consumer affairs, court cases, and government regulation of the tech industry.\n\n    125 Comments",
    "summary": {
      "en": "France's competition authority has fined Apple €150 million for its overly complicated App Tracking Transparency (ATT) system. The authority criticized Apple's requirement for \"double consent\" from users to allow third-party tracking, stating that this approach harms smaller companies that rely on advertising revenue. While the intent of ATT is to protect user privacy, its implementation creates unnecessary complexity for users and disadvantages smaller publishers compared to larger companies like Meta and Google. The fine is not expected to significantly impact Apple's finances, but the company may need to modify the ATT framework to comply with the ruling. Apple responded by stating that its ATT practices are consistent and supported by consumers and privacy advocates.",
      "ko": "프랑스의 경쟁 당국이 애플에 1억 5천만 유로의 벌금을 부과했습니다. 이는 애플의 복잡한 앱 추적 투명성(ATT) 시스템 때문입니다. 당국은 애플이 제3자의 추적을 허용하기 위해 사용자에게 '이중 동의'를 요구하는 방식이 광고 수익에 의존하는 소규모 기업에 피해를 준다고 비판했습니다. ATT의 목적은 사용자 개인정보를 보호하는 것이지만, 그 실행 방식은 사용자에게 불필요한 복잡성을 초래하고 메타나 구글과 같은 대기업에 비해 소규모 출판사에 불리하게 작용합니다. 이번 벌금이 애플의 재정에 큰 영향을 미치지는 않을 것으로 보이지만, 애플은 판결에 따라 ATT 시스템을 수정해야 할 필요성이 있을 수 있습니다. 애플은 자사의 ATT 관행이 일관되며 소비자와 개인정보 보호 옹호자들로부터 지지를 받고 있다고 응답했습니다.",
      "ja": "フランスの競争当局は、Appleに対して1億5000万ユーロの罰金を科しました。この罰金は、Appleのアプリ追跡透明性（ATT）システムがあまりにも複雑であることに対するものです。当局は、ユーザーからの「二重同意」を求めるAppleの方針を批判し、このアプローチが広告収入に依存する小規模企業に悪影響を与えていると指摘しました。ATTの目的はユーザーのプライバシーを守ることですが、その実施はユーザーにとって不必要な複雑さを生み出し、MetaやGoogleのような大企業に比べて小規模な出版社を不利にしています。この罰金がAppleの財務に大きな影響を与えることはないと見込まれていますが、Appleは判決に従うためにATTの枠組みを修正する必要があるかもしれません。Appleは、自社のATTの実施が一貫しており、消費者やプライバシー擁護者から支持されていると述べています。"
    }
  },
  {
    "id": "442ed77225849d34",
    "title": {
      "en": "Over 200M Records Allegedly Belonging to X Leaked Online",
      "ko": "X의 2억 건 데이터 유출!",
      "ja": "Xの2億件超のデータ流出"
    },
    "type": "story",
    "url": "https://www.safetydetectives.com/news/x200m-leak-report/",
    "score": 11,
    "by": "RankingMember",
    "time": 1743515509,
    "content": "SafetyDetectives Cybersecurity Team\n\n                                        Updated on: March 30, 2025\n                                                                                                                SafetyDetectives Cybersecurity Team\n\n    This article contains\n\n                    What Is X?\n\n                    Where Was The Data Found?\n\n                    What Was Leaked?\n\n                    What Risks Does This Data Exposure Pose?\n\n                    What to Do If You Believe Your Data Was Exposed\n\n                    What Are Clearweb Leaks and Why Should You Care?\n\n                                                        SafetyDetectives’ Cybersecurity Team stumbled upon a clear web forum post where a threat actor published a .CSV file allegedly containing over 200 million records from X users.\nWhat Is X?\nFormerly known as Twitter, X is one of the world’s largest social media platforms where users can share messages, images, and videos in short posts. They can also like and repost other users’ content among other features.\nIn October 2022 the company was acquired by Elon Musk, who renamed it X.\nWhere Was The Data Found?\nThe data was found in a forum post available on the clear surface web. This well-known forum operates message boards dedicated to database downloads, leaks, cracks, and more.\nWhat Was Leaked?\nAccording to the author of the post, in January 2025, 400 GB of data on 2.8+ billion X’s users was leaked. The author claims that they decided to post the data after seeing “no sign that X or the general public is aware of the largest social media breach ever.” They also claim that they “tried contacting X via several methods with no response.”\nAlthough the author of the post did not release all the data, they claim to have accessed all data entries from the January 2023 leak —which was believed to be a public data scrape— as well as cross-referenced them with the new data. The author claims to only have included records of X users present in both datasets. They then appended the new entries from the latest data to the old data, resulting ina 34 GB .CSV file containing 201,186,753 total entries of data allegedly belonging to X’s users.\nThe headers on the .CSV file are the following:\n\nID,\nscreen_name,\nname,\nlocation,\ndescription,\nurl,\nEmail,\ntime zone,\nlanguage,\nfollowers_count,\nfriends_count,\nlisted_count,\nfavourites_count,\nstatuses_count,\nprotected,\nverified,\ndefault_profile,\ndefault_profile_image,\nlast_status_created_at,\nlast_status_source,\ncreated_at\n\nSafety Detectives’ Cybersecurity Team reviewed a sample of the data to assess its authenticity. We reviewed the information corresponding to 100 users in the list, and we found that it matched what was shown on Twitter. We also verified a considerable amount of emails, which turned out to be valid email addresses, though we cannot confirm that the emails belong to the accounts listed.\nThe entire file consists of 1,048,576 rows, each one presumably containing multiple data points on one X user. The data was not behind a paywall meaning that it was free to anyone with an account in the forum to download.\nThis is a screenshot of the original post.\nThis is a screenshot of the response the author gives to a comment of another user, where he claims that the data is legitimate and that this could be “the largest social media breach”.\nWhat Risks Does This Data Exposure Pose?\nThe purportedly leaked data presents a risk to the security and privacy of all users impacted by this breach. Each of them may be vulnerable to:\n\nPhishing attacks: Cybercriminals may use the leaked information to create convincing emails or messages that appear to be from X or other legitimate sources. These messages aim to trick individuals into providing more sensitive information or clicking on malicious links.\nTargeted scams: Armed with knowledge of the individual’s activity on X, scammers could potentially tailor their fraudulent schemes to appear more legitimate and increase their likelihood of success.\nSocial engineering attacks: A social engineering attack occurs when a cybercriminal uses manipulation to deceive a target into revealing confidential information or performing actions that jeopardize security.\n\nWhat to Do If You Believe Your Data Was Exposed\nIf you suspect that your personal information was compromised in this data leak, you can take these steps to protect yourself:\n\nBeware of Phishing Attempts: Be cautious of unsolicited emails, messages, or phone calls asking for personal information or payment details. Do not click on links or download attachments from unknown sources.\nUpdate Privacy Settings: Review and update the privacy settings on your social media accounts and other online platforms to limit the amount of personal information visible to the public.\nBeware of social engineering attacks: Understand social engineering risks, including phishing and scam attempts. Be cautious and verify the authenticity of any unexpected communication, particularly if it requests personal or financial data.\nReport any unusual events: Notify X of any fraudulent activity or suspicious communications related to this incident. Be wary of sharing information with unknown contacts or unverified sources.\n\nWhat Are Clearweb Leaks and Why Should You Care?\nHackers utilize various parts of the internet to coordinate attacks, share information, and discuss data breaches. One of the most popular channels hackers use for these purposes are clearweb forums, which are online networks —available to anyone with an internet connection— that allow users to share information about breaches and leaks. These forums provide a sense of anonymity to their members and features like paywalling for those users who require payment to access the information they are sharing.\nBy reporting on these incidents, we aim to proactively inform potentially affected parties earlier so that they can act quickly to protect their data. Our disclosures are rooted in meticulous research and are intended solely for informational and preventive purposes. In no way should these reports be construed as allegations, insinuations, or indicators of fault or negligence by any individual or organization.\nIn a recent discovery, SafetyDetectives’ Cybersecurity Team stumbled upon a clear web forum post where a threat actor publicized a database allegedly belonging to 5 Miles Lab. The breach supposedly exposed 8.3 million lines of their corporate inbox information.",
    "summary": {
      "en": "**Summary of SafetyDetectives Cybersecurity Team Article:**\n\nOn March 30, 2025, SafetyDetectives Cybersecurity Team discovered a forum post revealing a CSV file that allegedly contains over 200 million records of users from X (formerly Twitter). This data was reportedly leaked in January 2025 and is believed to be part of the largest social media breach to date. The forum where the data was posted is accessible on the clear web, which allows users to share information about data leaks.\n\nThe leaked data includes various user details such as IDs, screen names, email addresses, and account statistics. SafetyDetectives checked a sample of the data and found it to be legitimate, although they could not confirm the ownership of the email addresses.\n\nThe breach poses several risks including phishing attacks, targeted scams, and social engineering attacks aimed at exploiting the leaked information. Users who suspect their data may have been compromised are advised to be cautious about unsolicited communications, update their privacy settings, and report any unusual activity.\n\nThe article also explains the concept of clearweb leaks, emphasizing the importance of awareness and proactive measures to protect personal data in light of such breaches.",
      "ko": "2025년 3월 30일, SafetyDetectives 사이버 보안 팀은 X(구 트위터) 사용자 2억 건 이상의 기록이 포함된 CSV 파일을 공개한 포럼 게시물을 발견했습니다. 이 데이터는 2025년 1월에 유출된 것으로 알려져 있으며, 지금까지 발생한 가장 큰 소셜 미디어 데이터 유출 사건의 일부로 여겨집니다. 데이터가 게시된 포럼은 일반 웹에서 접근 가능하여 사용자들이 데이터 유출에 대한 정보를 공유할 수 있도록 하고 있습니다.\n\n유출된 데이터에는 사용자 ID, 화면 이름, 이메일 주소, 계정 통계 등 다양한 정보가 포함되어 있습니다. SafetyDetectives는 데이터 샘플을 확인했으며, 그 데이터가 진짜임을 확인했지만 이메일 주소의 소유자는 확인할 수 없었습니다.\n\n이번 유출 사건은 피싱 공격, 표적 사기, 사회 공학적 공격 등 여러 가지 위험을 초래합니다. 자신의 데이터가 유출되었을 가능성이 있는 사용자들은 원치 않는 연락에 주의하고, 개인 정보 설정을 업데이트하며, 이상 활동을 보고할 것을 권장합니다.\n\n이 기사는 또한 일반 웹에서의 데이터 유출 개념을 설명하며, 이러한 유출 사건에 대비해 개인 정보를 보호하기 위한 인식과 적극적인 조치의 중요성을 강조하고 있습니다.",
      "ja": "2025年3月30日、SafetyDetectivesのサイバーセキュリティチームは、X（旧Twitter）のユーザーに関する2億件以上の記録が含まれているとされるCSVファイルを示すフォーラムの投稿を発見しました。このデータは2025年1月に流出したとされ、これまでで最大のソーシャルメディアのデータ侵害の一部と考えられています。データが投稿されたフォーラムは、一般のウェブ上でアクセス可能で、ユーザーがデータ流出に関する情報を共有する場となっています。\n\n流出したデータには、ユーザーのID、スクリーンネーム、メールアドレス、アカウントの統計情報など、さまざまな詳細が含まれています。SafetyDetectivesはデータのサンプルを確認し、正当なものであることを確認しましたが、メールアドレスの所有者を特定することはできませんでした。\n\nこのデータ侵害は、フィッシング攻撃やターゲットを絞った詐欺、流出した情報を悪用するソーシャルエンジニアリング攻撃など、いくつかのリスクを引き起こします。自分のデータが危険にさらされている可能性があるユーザーは、不要な連絡に注意し、プライバシー設定を更新し、異常な活動を報告することが推奨されます。\n\n記事では、クリアウェブの流出についても説明しており、このような侵害を受けた際に個人データを守るための意識と積極的な対策の重要性が強調されています。"
    }
  },
  {
    "id": "09958d1881ee1e7f",
    "title": {
      "en": "Why is this site built with C",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://marcelofern.com/posts/c/why-is-this-site-built-with-c/index.html",
    "score": 149,
    "by": "todsacerdoti",
    "time": 1743357102,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "7dd49c870a418a46",
    "title": {
      "en": "Win98-quickinstall: A framework and installer to quickly install Windows 98",
      "ko": "윈98 빠른 설치",
      "ja": "ウィン98簡単インストール"
    },
    "type": "story",
    "url": "https://github.com/oerg866/win98-quickinstall",
    "score": 201,
    "by": "userbinator",
    "time": 1743394245,
    "content": "Windows 9x QuickInstall ISO Creator Package\n\n                   (C) 2012-2024 Eric Voirin (oerg866)\n\nDisclaimer\nWindows is a trademark that belongs to Microsoft Corporation.\nThis project has NO ENDORSEMENT FROM ANY INVOLVED PARTIES, SUCH AS MICROSOFT.\nPlease don't sue. I just like old computers :(\nWindows 9x QuickInstall\n\nDescription\nThis is a framework that is intended for creating and preparing Windows 98 installation ISO images in a way that is optimized for extremely quick installation, similar to nLite, but with a completely different method and context.\nIt takes the root file system of an already installed Windows 98 system and packages it, whilst allowing drivers and tools to be slipstreamed at will.\nFor the installer, it uses Linux as a base, paired with some tools to allow hard disk partitioning and formatting, as well as  a custom installer with a custom data packing method that is optimized for streaming directly from CD to the hard disk without any seeking.\nBottom line, this means that the effort for the user to build an ISO is higher than for example nLite, but the degree of customizability is also massively increased.\nHow fast is it really?\nOn a Pentium III class machine with ATA / ATAPI Ultra DMA available for all storage devices, Windows 98 -- using an ISO built with this framework -- can be installed from CD in roughly 60-90 seconds.\nBuilding the Framework, Bugs, License, etc.\nSee BUILDING.md.\nI don't want to read and/or do all of this.\nOkay. Go to the releases tab :-)\nSupported Target Operating Systems\n\nMicrosoft Windows 98 (Build 4.10.1998)\nMicrosoft Windows 98 Second Edition (Build 4.10.2222)\nMicrosoft Windows Millenium Edition (Build 4.90.3000)\n\nSupport for international versions is not properly tested. It should work and in my testing it does, but YMMV. Please report bugs!\nNO versions of Windows 95 are supported due to non-PNP device detection being part of the DOS-based installer stage.\nSystem requirements to use QuickInstall\n\ni486-class CPU, at least a 486SX (but it will be very slow)\n24 MiB of memory\nAn IDE / SATA / SCSI controller supported by Linux\n\nHow to boot a QuickInstall image\nThere are several provided methods to boot into Windows 98 QuickInstall:\n\nCD/DVD-ROM boot\nThe Windows 98 QuickInstall ISO image can be booted on any computer that supports floppy-emulation CD-ROM boot.\nRecommended if you have a PC that supports CD-ROM boot.\n\n1.44M floppy boot with DOS (dosflop.img)\nThis is a 1.44M floppy disk image that contains FreeDOS and LOADLIN to boot the kernel directly off the CD-ROM.\nRecommended if you have a computer that does not support CD-ROM boot or wish to install from a hard disk.\nAlso recommended if you have the QuickInstall files on an ATA/ATAPI media or other device that is exposed using Int 13h\nIf you have a SCSI CD-ROM, the image must be modified accordingly.\n\n1.44M floppy boot with tiny kernel (floppy.img)\nThis is a 1.44M floppy disk image that contains a proper kernel.\nRecommended only if you have QuickInstall on a non-ATA/ATAPI media and your BIOS does not support CD-ROM boot.\nThe kernel on this image is limited in functionality, driver support and does not enable kernel logs (no printk, no dmesg)\nDue to the strong compression, it takes a very long time to boot on slow systems. It is only meant as a last resort.\n\nRemovable media such as USB Flash Drives\nUsing the system preparation script it is possible to create bootable USB images. The steps to do so are described below.\n\nSystem requirements to build QuickInstall Images\n\nWindows 7, 8.1, 10 or 11\nOR\nModern Linux variant or WSL\nTested with:\n\nWindows 10 21H2, Build 19044.2846\nWindows Subsystem For Linux (Ubuntu 20.04.5 LTS)\nUbuntu 20.04.4 LTS (native)\n\npython (3.5 or newer)\n\nOn Windows 7 and 8.1:\nDownload a python installation package from https://www.python.org/downloads/\n\nOn Windows 10 and 11:\nUse the Microsoft Store to download an appropriate variant.\n\nOn Linux (Debian, Ubuntu):\nsudo apt install python3.8\n\nScript requirements\nRun the following command in the framework directory:\npip3 install -r requirements.txt\n\nmkisofs from cdrtools\n\nOn Windows:\nTool is bundled with the framework, no action required.\n\nOn Linux (Debian, Ubuntu):\nsudo apt install genisofs\n\nwine (Linux Only)\nsudo apt install wine\nMany parts of the ISO building process are Windows specific and not able to be cleanly implemented natively on Linux, such as modifying system registry, parsing driver INF files, etc.\n\n86Box (recommended) or another virtual machine capable of installing Windows 9x\n\nSoftware to extract files from a hard disk image\ne.g. 7zip (on Linux: sudo apt install p7zip-full)\n\nThe system preparation script (sysprep.py)\nThis script serves the purpose of preparing an installation for packaging into an ISO and/or USB image file.\nIt takes the following parameters:\n\n--iso <ISO>\nInstructs the script to create an ISO image with the given file name\n\n--usb <USB>\nInstructs the script to create an USB key image with the given file name\n\n--osroot <OSROOT>\nSpecifies a Windows 98 / ME system root directory (\"OS Root\") to be used.\nThis can be specified multiple times, in which case the installation wizard will show a selection menu.\n\n--extra <EXTRA>\nInstructs the script to add the files in this directory to the final ISO/USB output.\nDefault: _EXTRA_CD_FILES_ in the framework directory\nThis can be specified multiple times, all the files in all the directories will be added in this case\n\n--drivers <DRIVERS>\nInstructs the script to process slipstream all drivers in this directory.\nSlipstream means that these drivers will be installed automatically when the hardware for them is detected at any point of the installation's lifetime, even when the hardware is not yet present at the time of installation.\nDefault: _DRIVER_ in the framework directory. It already contains a curated selection of drivers.\nThis parameter can only be specified once.\n\n--extradrivers <EXTRADRIVERS>\nInstructs the script to process all drivers in this directory and add them into the extra drivers directory.\nThese drivers are NOT slipstreamed and thus not automatically installed. They are however made available on the resulting installation media and can be installed by pointing the Windows 98 / ME Add New Hardware wizard to the DRIVER.EX directory on the media.\nThe reason for this folder's existence is the vast selection of hardware available for the operating systems and the varying compatibility / size of them.\nVery large drivers are recommended to go in here, as well as drivers for which it cannot be assumed that different versions have different compatibility and speed.\nFor example, it is better to choose an older driver for an older nVidia GeForce card even though a newer one would also support this hardware for speed reasons, whilst the newer driver should also be available, in case newer hardware is present.\nDefault: _EXTRA_DRIVER_ in the framework directory. It already contains a curated selection of drivers.\nThis parameter can only be specified once.\n\n--verbose VERBOSE\n\nThis parameter controls console output verbosity of the script.\nWhere VERBOSE is either True or False.\nThe default is False.\nThis parameter is currently broken, sorry. It's always quiet.\nPreparing a Windows 98 / ME installation for packaging\n\nInstall Windows 98 / ME in a virtual machine or emulator, just as you want it.\nI recommend using 86Box using the following configuration:\n\nMachine:\n\nMachine Type: Slot 1\nAchine: [i440BX] ABIT BF6\nCPU type: Intel Pentium II (Deschutes)\nMemory: 64 MB\n\nDisplay:\n\nVideo: [ISA] VGA (or any video card that does not have an integrated driver in Windows 98 / ME)\n\nNetwork:\n\nNone, VERY IMPORTANT TO MAKE SURE NETWORK DRIVER SETUP STAYS INTACT!!\n\nHard disks:\n\nIDE (0:0), raw image, big enough to install the operating system.\n\nIt is recommended that you install Windows in APM mode, because ACPI is a buggy mess (setup /p i).\nWARNING: The operating system must be installed from the HARD DISK and it must contain a folder containing the Windows 98 CAB files from the CDROM. Otherwise, drivers can not be slipstreamed.\nNOTE: It is recommended that NO extra drivers are installed in this VM.\n\nConfigure the Windows 98 / ME installation as you wish. Examples:\n\nMachine name\nUser name\nSystem options\nThemes\nPatches\nUtilities\nSoftware\n\nShut down the virtual machine and DO NOT TURN IT BACK ON\n\nUse 7zip or an imaging software and extract the entire root of the\npartition you installed Windows 98 to.\nExtract all files into a directory. We will call this the OS Root. The default for this is _OS_ROOT_ in the framework directory.\nOn Windows, you can open the image file using the 7zip File Manager or the 7zip context menu. Or WinImage, et cetera.\nOn Linux, you can do this with by using '7z' from the p7zip-full package.\n7z x -o_OS_ROOT_/ /path/to/image/file\nWARNING: Only one Windows directory is allowed, and only one Windows setup files CAB directory. If you install using 98Lite, make sure that the 98Lite setup directory is the sole carrier of these CABs.\nINFO: The script detects the Windows directory by finding WIN.COM\nINFO: The CAB file directory is detected by finding PRECOPY2.CAB\nThis location must be specified when running the script by using the --osroot parameter.\n\nPreparing & Packaging\n\nCopy drivers that you want slipstreamed into a directory of your choice. By default this is _DRIVER_ in the framework directory directory.\nNOTE: _DRIVER_ is already filled with a curated selection of drivers. Ycan remove these, if you wish.\nIf you choose a non-default directory for this, you must specify it using the --drivers parameter.\n\nCopy extra drivers that will not be slipstreamed but added to a directory of your choice. By default this is _EXTRA_DRIVER_ in the framework directory.\nNOTE: These drivers will be processed in the same way as the slipstreamed ones but will not be copied to the hard drive during installation.\nINFO: This folder will be named DRIVER.EX on the ISO. You can point the Windows 98 hardware wizard to this folder and the drivers will be found and installed correctly.\nNOTE: _EXTRA_DRIVER_ is already filled with a curated selection of drivers. Ycan remove these, if you wish.\nIf you choose a non-default directory for this, you must specify it using the --extradrivers parameter.\n\nAdd any extra files you wish added to the ISO to a directory of your choice. By default this is _EXTRA_CD_FILES in the framework directory.\nThis can include drivers that you do not wish to be processed with the QuickInstall tools, e.g. drivers that contain bundled software.\nIf you choose a non-default directory for this, you must specify it using the --extra parameter.\n\nRun the following command to build the package:\nsysprep.py --osroot <OS Root Folder>\nNOTE: You must add the other parameters if you deviate from the defaults.\nThis will build the installation package in the _OUTPUT_ directory in the Framework directory.\nIf you want to create an ISO image or a bootable USB key image, see below.\nWARNING: THE OS DIRECTORIES WILL BE MODIFIED IN PLACE!\n\nCreating a bootable ISO image\n\nUse the --iso parameter for the sysprep.py script.\ne.g. adding --iso output.iso to the command line will yield a file named output.iso that can be burned to a CD/DVD/Blu-Ray or used in a virtual machine.\nRefer to the parameter descriptions above for more information.\n\nCreating a bootable USB key image\n\nUse the --usb parameter for the sysprep.py script.\ne.g. adding --usb output.img to the command line will yield a file named output.img that can be written to a USB key, SD or CF card, hard disk or other media.\nRefer to the parameter descriptions above for more information.\n\nHow to write the bootable USB image to a USB flash drive\n\nOn Linux, you can use dd\n\ndd if=<USB image file> of=/dev/sdX bs=1024k status=progress\nReplace /dev/sdX with the USB flash drive's device path.\n\nOn Windows, you can use the following tools:\n\ndd For Windows: http://www.chrysocome.net/dd\nWin32 DiskImager: https://sourceforge.net/projects/win32diskimager/\n\nPackaging multiple operating systems in one image\nThis is an advanced feature.\nBy specifying the --osroot parameter multiple times, you can create a multi-variant installation image. In this case a selection menu will appear during installation prompting the user which variant should be installed.\nExample:\npython3 sysprep.py --osroot D:\\quickinstall\\Windows98SE --osroot D:\\quickinstall\\WindowsME --iso multi.iso\nwin98qi.inf\nThis file should be present in every OS Root directory. It contains the display name of the installation in the variant selection menu shown in the installer.\nIt should fit on the screen.\nIt can be ASCII or UTF8 encoded.\nFAQ\nQ: Windows 98 / ME complains about system file integrity when I create an image after a Daylight Savings Time swap-over\nA: This is a weird glitch that happens on Windows hosts where files created after DST are suddenly are offset by one hour.\nIn the future, the sysprep script will work with Hard Disk image files, which will make this a non-issue.\nWorkaround 1: Re-extract the files from the hard disk image you used as the base to create your image\nWorkaround 2: Create images on Linux\nWorkaround 3: Wait until summer :-)\nQ: Windows 98 / ME complains about missing CAT files when installing a driver from the extra drivers\nA: 98Lite deletes the catalog root directory to save installation space. Catalog files can safely be skipped, but if the error annoys you, you can unpack them from the Win98 CAB files to prevent it.\nQ: I'm getting a python error about non-zero return code in msdos.exe right after Using SHELL32.xxx to reboot!\nExample:\nsubprocess.CalledProcessError: Command '['L:\\\\win98-installer\\\\__BIN__\\\\tools\\\\msdos.exe', 'L:\\\\win98-installer\\\\__BIN__\\\\registry\\\\regedit.exe', '/L:SYSTEM.DAT', '/R:USER.DAT', 'tmp.reg']' returned non-zero exit status 1.\n\nA: This problem happens when running the script on Windows whilst the script directory is in a share hosted by a WSL session (Windows Subsystem for Linux). This causes some incompatibilities. Run the script from the WSL Linux shell instead.\nQ: All the operating system files are un-hidden after installation! Why?\nA: You've probably used Linux to do the image creation. Linux has no concept of hidden files, therefore this file property cannot be replicated in the images.\nQ: I'm getting I/O and read errors, segmentation faults and other weird behavior when installing from CD on an Intel i430 / i440-based system with an Intel 82371SB south bridge (e.g. i440FX)\nA: This problem has been verified by Deksor, Rigo and myself, and is a deeply rooted problem that has existed since at least version 2.4.xx. Operating the drives in PIO mode can help.\nA BIOS update may help, the issue is currently under investigation as we found some BIOS versions where this problem does not occur.\nFor now, you can work around this problem by using a PCI SCSI or IDE adapter card that supports CD-ROM boot or has DOS drivers with the DOS boot floppy option.\nQ: I'm trying to install on a VIA MVP3-based motherboard and I'm getting a \"General Protection Fault\" on the first boot. (Repoted by Rigo)\nA: To work around this issue, select the \"slow\" hardware detection variant in the installation wizard. The problem is currently under investigation.\nQ: I'm trying to install on my 486 and I'm getting Disk I/O errors!\nA: Your BIOS might have an incomplete/buggy LBA implementation. Partition the drive to use a FAT32 non-LBA partition and try again.\nQ: I'm getting a While initializing device VCACHE: Windows protection error when running on my modern PC (Ryzen, Intel 13th gen, etc.)\nInstall the CREGFIX patch (the reference ISOs contain it in the extras folder):\nhttps://github.com/mintsuki/cregfix\nFor technical reasons this patch cannot be pre-installed with the ISO and must be applied manually.",
    "summary": {
      "en": "### Summary of Windows 9x QuickInstall ISO Creator Package\n\n**Overview:**\nThe Windows 9x QuickInstall ISO Creator is a tool for creating optimized installation ISO images for Windows 98 and Windows ME. It allows users to quickly install these operating systems by packaging the root file system from an existing installation. \n\n**Key Features:**\n- **Fast Installation:** An ISO built with this tool can install Windows 98 in about 60-90 seconds on compatible hardware.\n- **Customizability:** Users can easily add drivers and tools to the installation, providing more options than similar tools like nLite.\n- **Supported OS Versions:** It supports Windows 98, Windows 98 Second Edition, and Windows Millennium Edition, but not Windows 95.\n\n**System Requirements:**\n- **To Use QuickInstall:** At least an i486 CPU, 24MB RAM, and a compatible hard disk controller.\n- **To Build Images:** Windows 7 or newer, or a modern Linux variant, with Python 3.5 or newer.\n\n**Booting Options:**\nUsers can boot the QuickInstall image via:\n1. CD/DVD-ROM\n2. Floppy disk with FreeDOS\n3. USB flash drives (requires additional setup)\n\n**Building an ISO:**\nTo create an ISO or USB image, users need to prepare their Windows installation, specify directories for drivers and extra files, and run a script with the appropriate parameters.\n\n**Advanced Features:**\n- Users can create multi-variant installation images by specifying multiple OS root directories, allowing a selection menu during installation.\n\n**Common Issues:**\nThe text includes a FAQ section addressing common problems, such as file integrity errors or installation issues on specific hardware setups, along with suggested workarounds.\n\n**Conclusion:**\nThis package is designed for enthusiasts who enjoy working with older operating systems and want a customizable and efficient way to install Windows 98 and ME.",
      "ko": "Windows 9x QuickInstall ISO Creator는 Windows 98과 Windows ME의 최적화된 설치 ISO 이미지를 만드는 도구입니다. 이 도구를 사용하면 기존 설치에서 루트 파일 시스템을 패키징하여 운영 체제를 빠르게 설치할 수 있습니다.\n\n이 도구의 주요 특징 중 하나는 빠른 설치 속도입니다. 이 도구로 생성된 ISO는 호환되는 하드웨어에서 약 60-90초 만에 Windows 98을 설치할 수 있습니다. 또한, 사용자는 드라이버와 도구를 쉽게 추가할 수 있어 nLite와 같은 유사 도구보다 더 많은 옵션을 제공합니다. 지원되는 운영 체제는 Windows 98, Windows 98 Second Edition, Windows Millennium Edition이며, Windows 95는 지원하지 않습니다.\n\nQuickInstall을 사용하기 위해서는 최소한 i486 CPU와 24MB RAM, 호환되는 하드 디스크 컨트롤러가 필요합니다. 이미지를 만들기 위해서는 Windows 7 이상 또는 최신 리눅스 버전과 Python 3.5 이상이 필요합니다.\n\n사용자는 QuickInstall 이미지를 CD/DVD-ROM, FreeDOS가 설치된 플로피 디스크, 또는 추가 설정이 필요한 USB 플래시 드라이브를 통해 부팅할 수 있습니다. ISO 또는 USB 이미지를 만들기 위해서는 Windows 설치를 준비하고, 드라이버 및 추가 파일을 위한 디렉토리를 지정한 후, 적절한 매개변수로 스크립트를 실행해야 합니다.\n\n고급 기능으로는 여러 운영 체제의 루트 디렉토리를 지정하여 다중 변형 설치 이미지를 생성할 수 있는 기능이 있어, 설치 중 선택 메뉴를 제공할 수 있습니다. \n\n이 패키지는 파일 무결성 오류나 특정 하드웨어 설정에서의 설치 문제와 같은 일반적인 문제를 다루는 FAQ 섹션도 포함되어 있으며, 해결 방법도 제시하고 있습니다. 이 도구는 오래된 운영 체제를 다루는 것을 좋아하는 사용자들에게 맞춤형이고 효율적인 Windows 98 및 ME 설치 방법을 제공합니다.",
      "ja": "Windows 9x QuickInstall ISO Creatorは、Windows 98およびWindows ME用の最適化されたインストールISOイメージを作成するためのツールです。このツールを使用することで、既存のインストールからルートファイルシステムをパッケージ化し、迅速にこれらのオペレーティングシステムをインストールできます。\n\nこのツールの主な特徴は、まずインストールが非常に速いことです。このツールで作成されたISOを使用すると、対応するハードウェア上で約60〜90秒でWindows 98をインストールできます。また、ユーザーはドライバーやツールを簡単に追加できるため、nLiteのような他のツールよりも多くの選択肢があります。対応しているOSのバージョンは、Windows 98、Windows 98 Second Edition、Windows Millennium Editionですが、Windows 95には対応していません。\n\nQuickInstallを使用するためには、最低でもi486 CPU、24MBのRAM、および互換性のあるハードディスクコントローラーが必要です。ISOイメージを作成するには、Windows 7以降のバージョン、または最新のLinuxバリアントが必要で、Python 3.5以降がインストールされている必要があります。\n\nQuickInstallイメージは、CD/DVD-ROM、FreeDOSを搭載したフロッピーディスク、または追加の設定が必要なUSBフラッシュドライブから起動できます。ISOまたはUSBイメージを作成するには、Windowsのインストールを準備し、ドライバーや追加ファイルのディレクトリを指定し、適切なパラメータでスクリプトを実行する必要があります。\n\nさらに、ユーザーは複数のOSルートディレクトリを指定することで、多様なインストールイメージを作成でき、インストール中に選択メニューを表示することが可能です。一般的な問題に関しては、ファイルの整合性エラーや特定のハードウェアセットアップでのインストール問題などを扱ったFAQセクションがあり、推奨される対処法も示されています。\n\nこのパッケージは、古いオペレーティングシステムを扱うのが好きな愛好者向けに設計されており、Windows 98およびMEをカスタマイズ可能かつ効率的にインストールする方法を提供します。"
    }
  },
  {
    "id": "c1d8b1135456d65f",
    "title": {
      "en": "Browsercraft: Java Minecraft in the browser",
      "ko": "브라우저 마인크래프트",
      "ja": "ブラウザクラフト"
    },
    "type": "story",
    "url": "https://browsercraft.cheerpj.com/",
    "score": 155,
    "by": "John7878781",
    "time": 1743432995,
    "content": "What is this\n\n\t\t\t\tBrowsercraft makes unmodified Minecraft run in the browser using CheerpJ, a Java runtime for modern browsers.\n\n\t\t\tWhat this is not\n\n\t\t\t\tMinecraft Classic, an alpha version of Minecraft playable in the browser\n\t\t\t\tMinecraft Bedrock Edition\n\t\t\t\tThe latest version of Minecraft. Newer releases of Minecraft use a newer version of Java and OpenGL which we currently do not support.\n\t\t\t\tA modified version of Minecraft. We do not modify the game in any way, we just run the original JARs.\n\t\t\t\tA reimplementation of Minecraft in another programming language\n\n\t\t\tHow it works\n\n\t\t\t\tCheerpJ is a Java Virtual Machine written in WebAssembly and it runs entirely in your browser. CheerpJ can run any Java application, without modification and without requiring the source code. This demo demonstrate these capabilities by running an older version (1.2.5) of Minecraft and LWJGL entirely in the browser.\n\n\t\t\t\tThis project is a work-in-progress and not everything works yet. In particular:\n\n\t\t\t\tAudio is not supported\n\t\t\t\tProbably other subtle problems\n\n\t\t\t\tNone of these issues are fundamental limitations, they just haven't been implemented yet.\n\n\t\t\t\tIf you're a programmer, we'd love your help in fixing these issues! Join the Discord server and contribute on GitHub.",
    "summary": {
      "en": "**Summary:**\n\nBrowsercraft allows you to play unmodified Minecraft in your web browser using CheerpJ, which is a Java runtime for modern browsers. \n\n**What it is not:**\n- It is not Minecraft Classic or Minecraft Bedrock Edition.\n- It does not represent the latest version of Minecraft, as newer versions use different technology that isn’t supported yet.\n- It does not alter the original Minecraft; it runs the original game files as they are.\n- It is not a version of Minecraft written in a different programming language.\n\n**How it works:**\nCheerpJ is a Java Virtual Machine that operates in the browser and can run Java applications without changes. Currently, it runs Minecraft version 1.2.5 and some other applications, but it is still being developed. \n\n**Current limitations:**\n- Audio is not working.\n- There may be other minor issues that need fixing.\n\nThese problems are not permanent, and if you are a programmer, you can help by joining their Discord server or contributing on GitHub.",
      "ko": "Browsercraft는 CheerpJ를 사용하여 웹 브라우저에서 수정되지 않은 마인크래프트를 플레이할 수 있게 해줍니다. CheerpJ는 최신 브라우저에서 작동하는 자바 런타임입니다.\n\nBrowsercraft는 마인크래프트 클래식이나 마인크래프트 베드락 에디션이 아닙니다. 최신 버전의 마인크래프트를 반영하지 않으며, 최신 버전은 지원되지 않는 다른 기술을 사용합니다. 또한, 원래의 마인크래프트를 변경하지 않고, 원본 게임 파일을 그대로 실행합니다. 다른 프로그래밍 언어로 작성된 마인크래프트 버전도 아닙니다.\n\nCheerpJ는 브라우저에서 작동하는 자바 가상 머신으로, 변경 없이 자바 애플리케이션을 실행할 수 있습니다. 현재 마인크래프트 1.2.5 버전과 몇 가지 다른 애플리케이션을 실행할 수 있지만, 아직 개발 중입니다.\n\n현재 몇 가지 제한 사항이 있습니다. 오디오 기능이 작동하지 않으며, 수정이 필요한 다른 작은 문제들이 있을 수 있습니다. 이러한 문제는 영구적이지 않으며, 프로그래머라면 그들의 디스코드 서버에 참여하거나 GitHub에서 기여함으로써 도움을 줄 수 있습니다.",
      "ja": "Browsercraftは、CheerpJを使用して、ウェブブラウザ上で改造されていないMinecraftをプレイできるサービスです。CheerpJは、現代のブラウザ向けのJavaランタイムです。\n\nこのサービスは、Minecraft ClassicやMinecraft Bedrock Editionではありません。また、最新のMinecraftのバージョンを反映しているわけでもなく、新しいバージョンは異なる技術を使用しているため、まだサポートされていません。さらに、オリジナルのMinecraftを変更することなく、元のゲームファイルをそのまま実行します。異なるプログラミング言語で書かれたMinecraftのバージョンでもありません。\n\nCheerpJは、ブラウザ内で動作するJava仮想マシンで、変更なしにJavaアプリケーションを実行できます。現在、Minecraftのバージョン1.2.5やその他のアプリケーションを実行していますが、まだ開発中です。\n\n現在の制限として、音声が機能していないことや、他にも修正が必要な小さな問題があるかもしれません。これらの問題は永久的なものではなく、プログラマーであれば、彼らのDiscordサーバーに参加したり、GitHubで貢献することで手助けできます。"
    }
  },
  {
    "id": "edd344cb4fcc7615",
    "title": {
      "en": "ToS;DR",
      "ko": "약관 요약",
      "ja": "利用規約の真実"
    },
    "type": "story",
    "url": "https://tosdr.org/en",
    "score": 287,
    "by": "ColinWright",
    "time": 1743414876,
    "content": "\"I have read and agree to the Terms\" is the biggest lie on the web. Together, we can fix that.As featured onTIME\nThe Verge\nLe Monde\nZeit Online\nWIRED\nstrategy+business\nDating News\nWakaTIME\nThe Verge\nLe Monde\nZeit Online\nWIRED\nstrategy+business\nDating News\nWaka\nSearchFacebookGrade E\nFacebook stores your data whether you have an account or not.\nThe service can read your private messages\nThis service can view your browser history\nDeleted content is not really deleted\nThis service keeps user logs for an undefined period of time\nView Details\n\nAnalyze DocumentsAmazonGrade E\nThird-party cookies are used for advertising\nTerms may be changed any time at their discretion, without notice to the user\nThis service tracks you on other websites\nThe service can delete your account without prior notice and without a reason\nVoice data is collected and shared with third-parties\nView Details\n\nAnalyze DocumentsRedditGrade E\nThe service can read your private messages\nYou sign away moral rights\nThe service can delete specific content without prior notice and without a reason\nThis service may keep personal data after a request for erasure for business interests or legal obligations\nTracking via third-party cookies for other purposes without your consent.\nView Details\n\nAnalyze DocumentsWikipediaGrade B\nThe service can delete your account without prior notice and without a reason\nUsers have a reduced time period to take legal action against the service\nThe service may use tracking pixels, web beacons, browser fingerprinting, and/or device fingerprinting on users.\nYour data may be processed and stored anywhere in the world\nYou publish your contributions under free licenses\nView Details\n\nAnalyze DocumentsDuckDuckGoGrade B\nInstead of asking directly, this Service will assume your consent merely from your usage.\nYou can delete your account and Duck Addresses\nNo need to register\nThis service provides an onion site accessible over Tor\nThe service makes critical changes to its terms without user involvement\nView Details\n\nAnalyze DocumentsYouTubeGrade E\nThis service can view your browser history\nDeleted videos are not really deleted\nThird-party cookies are used for advertising\nThis service gathers information about you through third parties\nReduction of legal period for cause of action\nView Details\n\nAnalyze DocumentsKhan AcademyGrade D\nThe service can delete specific content without prior notice and without a reason\nTracking via third-party cookies for other purposes without your consent.\nThe service collects many different types of personal data\nThis service may keep personal data after a request for erasure for business interests or legal obligations\nThe service can delete your account without prior notice and without a reason\nView Details\n\nAnalyze DocumentsQuoraGrade D\nThey store data on you even if you did not interact with the service\nTracking via third-party cookies for other purposes including advertising\nThis service may keep personal data after a request for erasure for business interests or legal obligations\nThis service tracks you on other websites\nThis service gathers information about you through third parties\nView Details\n\nAnalyze DocumentsBlizzardGrade E\nYou waive your moral rights\nThe service can delete specific content without prior notice and without a reason\nSome personal data may be kept for business interests or legal obligations\nThis service tracks you on other websites\nThe copyright license maintained by the service over user data and/or content is broader than necessary.\nView Details\n\nAnalyze DocumentsTor BrowserGrade C\nYou can access most of the pages on the service's website without revealing any personal information\nYour personal data is not sold\nAn anonymous payment method is offered\nView Details\n\nAnalyze DocumentsPayPalGrade E\nYou waive your moral rights\nThis service still tracks you even if you opted out from tracking\nThis service holds onto content that you've deleted\nYou must provide your identifiable information\nThe service collects many different types of personal data\nView Details\n\nAnalyze DocumentsStartpageGrade A\nThis service does not track you\nThe service will resist legal requests for user information where reasonably possible\nIP addresses of website visitors are not tracked\nThe cookies used by this service do not contain information that would personally identify you\nThe cookies used by this service do not contain information that would personally identify you\nView Details\n\nAnalyze DocumentsWikiHowGrade E\nThe service can read your private messages\nThis service can share your personal information to third parties\nThis service tracks you on other websites\nThe service can delete your account without prior notice and without a reason\nThis service forces users into binding arbitration in the case of disputes\nView Details\n\nAnalyze DocumentsCNNGrade E\nsign away moral rights\nThe service can delete specific content without prior notice and without a reason\nTerms may be changed any time at their discretion, without notice to the user\nThe service collects many different types of personal data\nThis service shares your personal data with third parties that are not involved in its operation\nView Details\n\nAnalyze DocumentsApple ServicesGrade D\nContent you post may be edited by the service for any reason\nThe service can delete specific content without reason and may do it without prior notice\nTerms may be changed any time at their discretion, without notice to the user\nMany different types of personal data are collected\nThis service keeps a license on user-generated content even after users close their accounts.\nView Details\n\nAnalyze DocumentsData last fetched: Tuesday, 01-Apr-25 11:51:31 UTCHow it worksToS;DR aims to provide easy-to-understand summaries of Privacy Policies and Terms of Service through a transparent and peer-reviewed process.Terms of service are reviewed by volunteer contributors, who highlight small points that we can discuss, compare and ultimately assign a score: \"good\", \"neutral\", \"bad\", and scariest of all, \"blocker\".Once a service has enough points to assess the fairness of their terms, we use a formula to provide ratings from Grade A to Grade E:Grade A — The terms of service treat you fairly, respect your rights, and will not abuse your data.Grade B — The terms of service are fair towards the user but they could be improved.Grade C — The terms of service are okay but some issues need your consideration.Grade D — The terms of service are very uneven, or there are some important issues that need your attention.Grade E — The terms of service raise very serious concerns.No Grade Yet — Not enough information exists to accurately grade this service yet.Right now you will notice that many services do not yet have a grade assigned. This is where you come in! Help us analyse more documents so that we may increase our coverage.\n    document.addEventListener('DOMContentLoaded', function() {\n        const searchInput = document.getElementById('searchInput');\n        const searchButton = document.getElementById('searchButton');\n\n        searchInput.focus();\n\n        function performSearch() {\n            const searchTerm = searchInput.value.trim();\n\n            if (searchTerm === 'x' || searchTerm.length > 3) {\n                window.location.href = `/en/search/${encodeURIComponent(searchTerm)}`;\n            }\n        }\n\n        searchButton.addEventListener('click', performSearch);\n\n        searchInput.addEventListener('keypress', function(event) {\n            if (event.key === 'Enter') {\n                performSearch();\n            }\n        });\n    });",
    "summary": {
      "en": "The text discusses how many online services have unfair and concerning Terms of Service (ToS) that users often agree to without reading. It highlights the importance of understanding these terms, as many services collect personal data, can delete accounts or content without warning, and often change their policies without notifying users.\n\nKey points include:\n\n- Many services, such as Facebook, Amazon, and Reddit, have poor data privacy practices, often tracking users without consent and retaining data even after deletion requests.\n- Terms of Service are graded from A (fair) to E (very concerning), with many popular platforms receiving low grades due to their practices.\n- ToS;DR (Terms of Service; Didn't Read) aims to summarize and analyze these agreements to help users better understand their rights and the risks involved.\n\nOverall, the goal is to raise awareness about the implications of agreeing to these terms and to encourage users to be more informed.",
      "ko": "많은 온라인 서비스가 사용자들이 읽지 않고 동의하는 불공정하고 우려스러운 서비스 약관을 가지고 있다는 내용이 다뤄지고 있습니다. 이러한 약관을 이해하는 것이 중요하다는 점이 강조되며, 많은 서비스가 개인 정보를 수집하고, 경고 없이 계정이나 콘텐츠를 삭제할 수 있으며, 정책을 사용자에게 알리지 않고 자주 변경한다는 사실이 언급됩니다.\n\n주요 내용으로는, 페이스북, 아마존, 레딧과 같은 많은 서비스가 데이터 프라이버시 관행이 좋지 않아 사용자의 동의 없이 추적하고, 삭제 요청이 있더라도 데이터를 보관하는 경우가 많다는 점이 있습니다. 서비스 약관은 A(공정)부터 E(매우 우려됨)까지 등급이 매겨지며, 많은 인기 플랫폼이 그들의 관행 때문에 낮은 등급을 받고 있습니다. \n\nToS;DR(서비스 약관; 읽지 않음)은 이러한 약관을 요약하고 분석하여 사용자가 자신의 권리와 관련된 위험을 더 잘 이해할 수 있도록 돕는 것을 목표로 하고 있습니다. 전반적으로 이러한 약관에 동의하는 것의 의미에 대한 인식을 높이고, 사용자들이 더 많은 정보를 갖도록 장려하는 것이 목적입니다.",
      "ja": "多くのオンラインサービスには、不公平で懸念すべき利用規約が存在し、ユーザーはそれを読むことなく同意してしまうことが多いです。これらの規約を理解することの重要性が強調されています。多くのサービスは個人データを収集し、警告なしにアカウントやコンテンツを削除することがあり、また、ユーザーに通知することなくポリシーを変更することもあります。\n\n具体的には、FacebookやAmazon、Redditなどの多くのサービスは、データプライバシーの実践が不十分で、ユーザーの同意なしに追跡を行い、削除リクエストがあってもデータを保持し続けることがあります。利用規約は、A（公正）からE（非常に懸念される）までの評価がされており、多くの人気プラットフォームはその実践により低い評価を受けています。\n\nToS;DR（利用規約を読まなかった）は、これらの合意を要約し分析することを目的としており、ユーザーが自分の権利やリスクをよりよく理解できるように手助けしています。全体として、これらの規約に同意することの影響についての認識を高め、ユーザーがより情報を持つことを促すことが目指されています。"
    }
  },
  {
    "id": "4ba75e729c4fc4d5",
    "title": {
      "en": "Everyone knows all the apps on your phone",
      "ko": "모두 아는 앱들",
      "ja": "みんなのアプリ事情"
    },
    "type": "story",
    "url": "https://peabee.substack.com/p/everyone-knows-what-apps-you-use",
    "score": 1158,
    "by": "gniting",
    "time": 1743283592,
    "content": "Share this postPea BeeEveryone knows all the apps on your phoneCopy linkFacebookEmailNotesMoreDiscover more from Pea Beetales from indian web rabbit holes.Over 3,000 subscribersSubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.Already have an account? Sign inEveryone knows all the apps on your phoneMar 28, 2025147Share this postPea BeeEveryone knows all the apps on your phoneCopy linkFacebookEmailNotesMore1421ShareUntil a few years ago, any app you installed on an Android device could see all other apps on your phone without your permission. Since 2022, with Android 11, Google removed this access from app developers. Under their new package visibility policy, apps should only see other installed apps if it’s essential to their core functionality. Developers must also explicitly declare these apps in the AndroidManifest.xml file - a required configuration file for all Android apps.For extremely specific use cases such as file managers, browsers or antivirus apps, Google grants an exception by allowing QUERY_ALL_PACKAGES permission, which provides full visibility into installed apps. I don’t use Android as my primary phone, but I have a spare one and I was really curious to find out which apps from Indian companies had checks to see what other apps I had installed.So I downloaded a few dozen Indian apps I could think of on top of my head and started reading their manifest files. Surely they will be respectful of my privacy and will only query apps essential to their app's core functionality? 🙃SubscribeIt's worth acknowledging that there are some legitimate reasons for an app to check which other apps are installed on your phone. For example, an app might check which UPI apps are installed to show relevant payment options. Most of the manifest files I examined included checks for these apps. Some also looked for app cloning or multi-account apps, likely for security and fraud detection. All acceptable use cases.But a few Indian companies went above and beyond with these checks. Let’s start with Swiggy. It has a staggering 154 package names listed in its manifest file, allowing it to query those apps on my phone. Here’s the full list:I don’t even know where to begin unpacking this madness. How is knowing whether I have the Xbox or the Playstation app installed on my phone essential to their Swiggy's core functionality? How will knowing if I have the Naukri or Upstox app help them deliver groceries to my doorstep?The wide range of categories of apps in this list strongly suggests Swiggy is collecting installed apps data for user profiling and to build a behavioural profile of their customers. This seems to be against Play Store's policies which considers the list of installed apps to be personal and sensitive user data.This reminded me of that ppt from Blume Ventures - the one that blue tick twitter accounts living in certain pin codes of Bengaluru passionately discuss amongst themselves for a week every year. It had this interesting slide on apps used by different Indias:Swiggy queries most of these apps and more on your phone. It not only knows which India you belong to, but it can pinpoint exactly where you fall within it.Let's talk about another app now, and it's the usual suspect, the undisputed champion of asshole design - Zepto. They have listed 165 apps to check for on your device.￼From Netflix to Bumble to Binance, the list includes nearly every popular app across all categories. There were recent reports of Zepto displaying different prices for iOS and Android users. With the help of this data, they can also show different pricing for different Android phones, which some customers are already seeing.Even though Swiggy and Zepto have to declare these apps to query in the manifest file, as a user, you have no visibility into this list when you download their apps from the Play Store.I also analyzed Swiggy and Zepto's apps for their delivery riders. The app query list is different from their consumer apps. Both include checks to see which other companies their riders work for. Here’s Zepto's list:But Swiggy takes it a step further - it also checks for personal loan apps, personal finance apps, and even keeps tabs on apps like like Ludo King or Carrom Pool on their delivery riders' phones.Can't we even play Ludo in peace without being spied on by our employers? Does even downtime need to be tracked by Swiggy? It’s embarrassing that Swiggy feels the need to include these ridiculous app queries on their delivery riders' phones.Speaking of personal loan apps in India, their predatory practices are well documented. A couple of years ago, there was a major crackdown that led to the removal of thousands of such apps from the Play Store. I took a look at some that still exist. Kreditbee is listed as one of the top apps in the personal loans space on the play store with over 50 million downloads. And can you believe their app checks for 860 apps installed on your phone? 860!!! I am sorry you may have to squint or zoom in a little to view this list. ￼I only skimmed through this list - there are just too many apps. I hope someone reading this can do a thorough analysis. It's probably because of the bubble I live in, but I hadn’t even heard of most of these apps. Even though most of them have tens of millions of downloads.Beyond the usual categories, I see there are checks for apps like Tamil Calendar, Odia Calendar, Qibla Direction Finder, mandir apps, astrology apps. They know what they’re doing.There is \"Jodii for Diploma, +2,10 below\", a matrimony app for those who haven’t graduated high school. It has 10M+ downloads.Then there is also \"गाय भैंस खरीदें बेचें Animall\" (cow buy/sell marketplace?) which also has more than 10M downloads.This list of apps is a window into how a large part of India uses their phones - their daily lives, habits, and priorities. Another leading personal loan app, Moneyview, with over 50 million downloads, has included checks for a staggering 944 apps in its manifest file - the highest among all the apps I examined. I am not including it in this post, you can read the full list here. I'm surprised KreditBee and Moneyview apps passed the Play Store's review. Play Store policy explicitly restricts personal loan apps from using the QUERY_ALL_PACKAGES permission.  But these apps are bypassing this restriction by individually listing every app they want to detect in their manifest file instead.I found only one manifest file which had the high-risk and sensitive QUERY_ALL_PACKAGES permission - it was Cred’s. Play Store grants a \"temporary exception\" to include this permission if apps have “a verifiable core purpose facilitating financial-transactions involving financially regulated instruments”.  But none of the other apps in the same segment as Cred I analyzed like PhonePe or PayTM had this permission in their manifest files. In fact, Cred offers personal loans too which as per Play Store’s Personal loans policy, is not eligible for this exception. Not sure how Cred is still allowed to keep this permission, which lets it see all the apps on your phone without any disclosures.I read the manifest files of around 50 popular apps from Indian companies. Apart from Swiggy, Zepto, Cred, and a couple of personal loan apps, most had fairly reasonable and respectful app query lists. Guess I expected worse. Maybe I am too cynical about these apps - could they actually be the good guys? 🙃As I was about to conclude this exercise, I noticed a couple of interesting lines when I was skimming through the manifest file of one of the apps:<queries>\n  [...]\n  <intent>\n    <action android:name=\"android.intent.action.MAIN\" />\n  </intent>\n  [...]\n</queries>I am no expert in Android development, but from what I understand, the \"ACTION_MAIN\" filter in the configuration above allows visibility to all installed apps that, simply put, have a screen.Since most installed apps run in the foreground and have a user interface, this filter grants developers access to see all the apps on your phone - without needing the QUERY_ALL_PACKAGES permission!To be sure, I vibe co -- I can't say it without wincing -- I vibe coded a basic android app and added the same \"ACTION_MAIN\" filter in my manifest file. And when I queried for installed packages, just as expected, this little hack returned a list of all the apps on my phone!!!This seems like a massive privacy loophole in Android. Surely Play Store would reject apps that use this hack as this is a blatant violation of their store's user data policy? Out of 47 Indian apps I randomly analyzed, 31 of them used the \"ACTION_MAIN\" filter - giving them access to see all the apps on your phone without any disclosure. That's 2 out of 3 apps.Apps using this hack: Astrotalk, Axis Mobile, Bajaj Finserv, BookMyShow, Cars24, Cure.fit, Fibe, Groww, Housing, Instamart, Ixigo, JioHotstar, KreditBee, KukuTV, LazyPay, Ludo King, Meesho, MoneyTap, Moneyview, Navi, NoBroker, Nykaa, Ola, PhonePe, PhysicsWallah, Slice, Spinny, Swiggy, Swiggy Delivery, Tata Neu, and Zomato.Apps that don't use this hack: Airtel Thanks, Blinkit, Byju’s, MyGate, Dream11, Flipkart, HDFC Mobile, Healthify, INDmoney, MyJio, Paytm, PaisaBazaar, ShareChat, Unacademy, Vedantu, ZeptoEven fucking Ludo King has this in its manifest file. So most Indian companies can actually see all the apps on your phone - they're just sneakier about it than the likes of Swiggy and Zepto. So much for being the good guys.In fact, Swiggy has got this filter config too, yet it still chooses to explicitly lists the apps it queries when it could just as easily do this discreetly behind closed doors like others. But I’m not complaining. This oversight from them gives a glimpse into Swiggy’s data collection practices. If Google had enforced this policy properly, we might have had similar visibility into other companies as well.All the manifest files I read are in my Github. The majority were downloaded on March 18 or 19.This hack isn’t exclusively used by apps from Indian companies. I checked the manifest files of some other popular apps. Facebook, Instagram, Snapchat, Subway Surfers, and Truecaller all have this config. Meanwhile, Amazon, Spotify, X, Discord, and WhatsApp didn’t. I didn’t investigate further beyond these.This makes me wonder, what was the whole purpose of Google's package visibility policy? It was supposed to protect users, yet most apps seem to have found ways around it anyway.And installed app data is very sensitive and personal. In 2022, Vice reported that a data marketplace called Narrative was selling data on users who had downloaded period-tracking apps right after news emerged that Roe v. Wade (which had federally protected abortion rights in the U.S.) could be overturned. This is frightening to even think about. Installed apps data is one data point. The extensive set of permissions each and every one of these apps have included in their manifest files, often far beyond what’s necessary is another can of worm for someone else to open. I’ll conclude this post with a tiny example from Zepto. They ask for READ_SMS permission. You can deny it, but it’s mandatory if you sign up for Zepto Postpaid. When you grant the permission, this is the list of sender IDs they check for in your inbox:Most of them are TRAI sender IDs of banks. They're likely reading these for their Postpaid plan eligibility check. They can still read this even if you never opt for it. And look how they've sneaked in SMSes from Blinkit, Swiggy, Bigbasket, Flipkart too.Their competitors are probably doing the same, they just didn’t leave behind such an obvious trail of evidence in the app itself. The point is when any app gets permissions like READ_SMS, as users, we have no visibility over when or what it’s accessing.Please remember the next time you casually install an app on your Android device, this information is being broadcast to the whole world. Data brokers will use it to profile you, cross-reference it with data about you from other ad networks and eventually it will be used to decide how much you’ll be asked to pay the next time you order a samosa.Thank you for reading. In case you subscribed to this newsletter after reading the \"What's inside this QR code menu at this cafe?\" post and can't find it anymore. Here's my tweet about it.I am also on Bluesky.Thanks for reading Pea Bee! Subscribe for free to receive new posts and support my work.Subscribe147Share this postPea BeeEveryone knows all the apps on your phoneCopy linkFacebookEmailNotesMore1421SharePrevious",
    "summary": {
      "en": "The article discusses changes in Android's privacy policies regarding app visibility, particularly how apps can see other apps installed on a user's device. Previously, apps could access this information without permission, but since 2022, developers must limit visibility to only essential apps and declare them in their configuration files. \n\nThe author examines several Indian apps, notably Swiggy and Zepto, which have extensive lists of other apps they check for, raising concerns about user privacy and data collection. Swiggy checks 154 apps, while Zepto checks 165, including many unrelated to their services. This suggests they may be profiling users based on their installed apps, which could violate privacy policies.\n\nAdditionally, some apps employ a loophole by using an \"ACTION_MAIN\" filter, allowing them to see all installed apps without needing special permissions. This method was found in many popular Indian apps, indicating widespread privacy concerns.\n\nThe article concludes that while some apps attempt to respect user privacy, many exploit the system, and users should be aware of the data they are sharing when installing apps on their devices.",
      "ko": "이 기사는 안드로이드의 앱 가시성에 관한 개인정보 보호 정책 변화에 대해 다루고 있습니다. 특히, 앱이 사용자의 기기에 설치된 다른 앱을 어떻게 확인할 수 있는지에 대한 내용입니다. 이전에는 앱이 이 정보를 허가 없이 접근할 수 있었지만, 2022년부터 개발자들은 가시성을 필수 앱으로 제한하고 이를 설정 파일에 명시해야 합니다.\n\n저자는 여러 인도 앱, 특히 스위기(Swiggy)와 제프토(Zepto)를 살펴보았습니다. 이들 앱은 확인하는 다른 앱의 목록이 방대하여 사용자 개인정보와 데이터 수집에 대한 우려를 불러일으킵니다. 스위기는 154개의 앱을 확인하고, 제프토는 165개의 앱을 확인하는데, 이들 중 많은 앱은 그들의 서비스와 관련이 없습니다. 이는 이들이 사용자의 설치된 앱을 기반으로 프로파일링을 할 가능성이 있음을 시사하며, 이는 개인정보 보호 정책을 위반할 수 있습니다.\n\n또한 일부 앱은 \"ACTION_MAIN\" 필터를 사용하여 특별한 허가 없이도 모든 설치된 앱을 확인할 수 있는 허점을 이용하고 있습니다. 이 방법은 많은 인기 있는 인도 앱에서 발견되어 광범위한 개인정보 보호 문제를 나타냅니다.\n\n기사는 일부 앱이 사용자 개인정보를 존중하려고 시도하는 반면, 많은 앱이 시스템을 악용하고 있으며, 사용자는 자신의 기기에 앱을 설치할 때 어떤 데이터를 공유하고 있는지 인식해야 한다고 결론짓고 있습니다.",
      "ja": "この記事では、Androidのプライバシーポリシーの変更について、特にアプリがユーザーのデバイスにインストールされている他のアプリをどのように確認できるかに焦点を当てています。以前は、アプリがこの情報にアクセスする際に許可が必要ありませんでしたが、2022年以降、開発者は必要なアプリのみに視認性を制限し、それを設定ファイルに明記する必要があります。\n\n著者は、インドのいくつかのアプリ、特にSwiggyとZeptoを調査しました。これらのアプリは、確認する他のアプリのリストが非常に多く、ユーザーのプライバシーやデータ収集に対する懸念が高まっています。Swiggyは154のアプリをチェックし、Zeptoは165のアプリを確認していますが、これらの多くは彼らのサービスとは無関係です。これは、インストールされているアプリに基づいてユーザーをプロファイリングしている可能性があり、プライバシーポリシーに違反する恐れがあります。\n\nさらに、一部のアプリは「ACTION_MAIN」フィルターを利用することで、特別な許可なしにすべてのインストール済みアプリを確認できる抜け道を使っています。この手法は、多くの人気のあるインドのアプリで見られ、広範なプライバシーの懸念を示しています。\n\nこの記事は、一部のアプリがユーザーのプライバシーを尊重しようとする一方で、多くのアプリがシステムを悪用していることを指摘し、ユーザーはアプリをインストールする際に自分が共有しているデータについて認識しておくべきだと結論づけています。"
    }
  },
  {
    "id": "44711a311a9593b2",
    "title": {
      "en": "Is BIND9 suitable as a recursive resolver in 2025?",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://szafka.net/blog/bind9-as-resolver.html",
    "score": 21,
    "by": "plagiat0r",
    "time": 1743443126,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "5cca68ac6b92240a",
    "title": {
      "en": "Talkin’ about a Revolution",
      "ko": "혁명 이야기",
      "ja": "革命の話"
    },
    "type": "story",
    "url": "https://drb.ie/articles/talkin-about-a-revolution/",
    "score": 63,
    "by": "pepys",
    "time": 1743201727,
    "content": "PHILOSOPHY\n\n                        Talkin’ about a Revolution\n\n                                                                    Johnny Lyons\n                                \t\t\t\t\t\t\t\t\t\t\t\t\t\tFebruary 2025\n\n               Donate\n\nHegel’s World Revolutions, by Richard Bourke, Princeton University Press, 344 pp, £25, ISBN: 978-0691250182\nIs human history ‘a tale told by an idiot, full of sound and fury, signifying nothing’ or rather a heroic story of the inevitable unfolding of human progress? Apart from professional optimists like Steven Pinker, most of us might feel on safer ground with Macbeth’s verdict. The less sanguine view of our past as one damned thing after another is more readily compatible with the currently lamentable state of the world. At the time of writing this piece (early January 2025) it is yet to be announced by the Bulletin of the Atomic Scientists how close to midnight we currently are on the Doomsday clock. In January of last year the board members of the Bulletin, which includes no less than eight Nobel scientists, judged that we are as close as 90 seconds to self-destruction. The following trends were cited to justify their unnervingly bleak diagnosis of our situation:\nThe war in Ukraine and the widespread and growing reliance on nuclear weapons increase the risk of nuclear escalation. China, Russia and the United States are all spending huge sums to expand or modernise their nuclear arsenals, adding to the ever-present danger of nuclear war through mistake or miscalculation. In 2023, Earth experienced its hottest year on record, and massive floods, wildfires and other climate-related disasters affected millions of people around the world. Meanwhile, rapid and worrisome developments in the life sciences and other disruptive technologies accelerated, while governments made only feeble efforts to control them. (https://thebulletin.org/doomsday-clock/current-time/)\nThe Bulletin assures us that ‘the world can be made safer. The Clock can move away from midnight.’ Fair enough, but any honest reckoning with global events in the last twelve months must conclude that the only credible direction the Clock can move is forward and ever closer to our annihilation.\nHow quickly humanity’s fortunes can change. It seems like only the day before yesterday that we convinced ourselves of having reached history’s glorious culmination following the collapse of the Soviet Union and with it communism, and the triumph of Western liberal democracy and its inseparable ally, capitalism. Back then it seemed all that was left to do was iron out the remaining creases in our own infallible political economy and export it to those parts of the planet which had yet to see the neoliberal light.\nIn those halcyon days the Doomsday Clock was set a full seventeen minutes shy of midnight – the members of the Bulletin board joyously declared in 1991 that the revised setting ‘reflects our optimism that we are entering a new era’. Such shallow sunniness proved short-lived however. By the noughties the clock had to be drastically reset following the outbreak of ‘the second nuclear age’ and the belated realisation that our planet is heating up due to our own activities. While things fluctuated a little in the late noughties the clock has drawn consistently nearer to midnight since 2010. One is left asking how close must we get to Armageddon before the Doomsday Bulletin fulfils the aim of one of its founding editors, Eugene Rabinowitch, to finally ‘frighten men into rationality’.\nThe likelihood that we might be frightened into rationality is looking slimmer by the day. Moreover, there is increasing evidence that we may well have passed the point of no return and are already baked into a nightmare of our own making in which the planet will become uninhabitable, and soon. And that’s assuming we don’t blow ourselves up in thermonuclear smithereens before the ‘natural’ end of our days.\nA sobering prospect and one which suggests human history will end in agony rather than neo-liberal ecstasy. But let’s imagine for a moment we have not yet passed the point of no return, and that we convince ourselves of the need to radically change our ways for the good of humanity and the rest of Mother Earth. What does humanity look like when it is frightened into rationality? How would we think and behave in such changed circumstances? All the late Eugene Rabinowitch had to say on the matter was that the Bulletin’s clock ‘is intended to reflect basic changes in the level of continuous danger in which mankind lives in the nuclear age, and will continue living, until society adjusts its basic attitudes and institutions’. That may well be true, but would human society need to undergo a revolution before being prepared to accept and implement these requisite adjustments? And what form would such a revolution have to take? Would it require a clean and violent departure from the status quo or, alternatively, an approach that builds a necessarily radical solution based on the more progressive aspects of our current regime(s)? Is such a global transformation even imaginatively possible, let alone politically and economically feasible? These are hardly idle questions in our currently drastic predicament.\nThere was a time when philosophers produced answers to such large and important questions. Today the vast majority of them don’t take any professional interest in politics while those that do pursue the subject in a way that would appear bizarre to their more capacious and engaged predecessors. Most contemporary academic philosophers tend to focus on a specific branch of the subject, such as the philosophy of mind or the philosophy of language, or on a certain philosopher or school of philosophy, as for example Wittgenstein or existentialism. The idea that philosophy is one integral whole has fallen victim to the professionalisation of the subject as an academic discipline, which means that its practitioners spend their careers studying more and more about less and less. This pattern may have proved pragmatically successful for the business of the university but its effect on philosophy can hardly be viewed as a happy one, even among those of us who don’t entirely buy the idea of some lost golden age when philosophy enjoyed the power to change the world.\nOne of the great long-dead philosophers who emphatically did see philosophy as one and sought to address the defining questions of his day was the German thinker Georg Wilhelm Friedrich Hegel. Unlike his philosophical predecessors, Hegel argued that history provides the key to answering the riddle of our existence. Before Hegel, philosophers tended to regard humanity as fundamentally unchanging even if they disagreed about what makes human beings tick. Hegel came up with the revolutionary view that not only has human nature transformed radically and irreversibly over time but that this change happens in a particular way and, most strikingly of all, forms part of an underlying and profound purpose. The idea that history is ‘a tale told by an idiot’ was, therefore, anathema to Hegel. He felt there is a discernible and coherent pattern to human history and that it is leading inevitably and progressively via a dialectical process to a final and ultimately happy destination. ‘The history of the world’, he famously declared, ‘is none other than the progress of the consciousness of freedom.’\nToday, there is a tendency to adopt a Jekyll and Hyde attitude to Hegel’s view of human history. His idea that we are historically conditioned all the way down is quite widely regarded as insightful but his notion that we have been (and presumably still are) travelling ineluctably towards full and free consciousness or Spirit (Geist) is generally dismissed as ludicrous – though politicians are not averse to invoking the Hegelian-like notion of the arc of history when it suits their agenda. The main problem with this prevailing impression of Hegel is that it stops us being curious about his ideas, which are immeasurably more nuanced and instructive than the caricature implies. And that brings us to the work under review, Hegel’s World Revolutions by Richard Bourke.\nBourke enjoys an international reputation as an intellectual historian. Since graduating from University College Dublin and the University of Cambridge, he has held teaching appointments and fellowships at several prestigious academic institutions in Europe and the US and is the current holder of the chair in the history of political thought at the University of Cambridge. During this time he has produced a steady stream of publications, including his own monographs Romantic Discourse and Political Modernity (1993), Peace in Ireland (2013), his seminal Empire and Revolution: The Political Life of Edmund Burke (2015) as well as a series of books co-edited with other eminent scholars on an a broad range of topics and themes from popular sovereignty and political judgement to modern Irish history and the political thought of the Irish revolution. At this stage in his career a new book by Bourke must count as something of an intellectual event, and particularly when the subject of his latest offering is a thinker as ambitious and influential as Hegel.\nAnyone who takes on Hegel knows they have their work cut out. He is among the most complex, capacious and ambitious philosophers in the history of Western thought. And unlike other comparable figures in that tradition, such as Plato, Descartes or Hume, his style of writing can be notoriously difficult – indeed one of the more striking aspects of the book is the contrast between Bourke’s consistently clear, exact and clipped style and the frequently obscure, vague and stentorian prose of his subject. As the title of Bourke’s book signals, it is not a synoptic or comprehensive study of Hegel’s thought. Rather it is primarily concerned with the historicism and political philosophy of the nineteenth century thinker and even then in a quite defined way.\nBourke indicates in the book’s preface that his goal is threefold: to explain Hegel’s notion of world revolutions, especially the French Revolution, within the context of his time; secondly, to provide a more discriminating picture of Hegel’s understanding of the historical backdrop of modern European history from which the 1789 world revolution arose; and, finally, to consider the sharply mixed reception of Hegel’s thought in the twentieth century, before confronting the knotty question of the applicability of past political ideas to present concerns.\nOne of the more intriguing aspects of this preface is its opening reference to an alleged malaise afflicting contemporary culture, and especially the academy. Describing this sickness as ‘a posture of suspicion’ that dismisses reason as controlling hubris, freedom as domination, and liberal democracy as an ally of imperialism, Bourke regards the phenomenon as culturally noxious and intellectually unfounded, since it denies the genuine and hard-won accomplishments of human history. More interestingly, he asserts that Hegel is the first thinker to help us to see through the spuriousness of this kind of wholesale rejection of Enlightenment values. This is a topic I shall return to in greater detail below as I think these early remarks prefigure something important about Bourke’s evolving conception of the role and authority an intellectual historian can and should assume.\nThe central focus of the opening two-thirds of the book is Hegel’s philosophy of history, or perhaps more accurately his attempt to provide an authentically historical history of philosophy. This is hardly virgin territory, but what makes Bourke’s treatment original and instructive is that he exposes the superficiality of conventional accounts of the topic. Rather than seeing the onward march of human history as fundamentally seamless, he highlights the fact that Hegel understood history as a far more complex and stumbling affair, in which the path to progress often has to endure going one step forward and four steps back before it eventually accomplishes a net gain. A truer picture of the Hegelian historical dialectic from thesis via antithesis through to synthesis is, therefore, more in the spirit of ‘try again, fail again, fail better’.\nBourke finds this pattern noteworthy for a number of reasons but mostly because it offers a vital corrective to the tendency which indulges the illusion that genuine epochal transformation requires an immediate and complete departure from the one which preceded it. This may seem a distinctly elementary point but it’s quite remarkable how often we allow ourselves to become subject to the myth of starting afresh, which has not infrequently caused untold human suffering. Bourke shows that a close reading of Hegel’s Philosophy of History reveals a far more ambivalent and balanced historical sense which offers a potent antidote not just to the myth of the fresh start but to those who, like Nietzsche and Foucault, adopt an unduly dismissive (or, in Pinker’s case, triumphant) view of our current norms and ideals.\nWhat’s especially striking about Bourke’s treatment is how he widens the familiar frame by examining Hegel’s account of ‘world revolutions’ that preceded the paradigmatic one of the French Revolution. What emerges is a highly erudite and persuasive elucidation and clarification of Hegel’s view of human history that progresses along the lines of a game of snakes and ladders rather than like an elevator that only goes up. One of the more vital lessons from Bourke’s detailed work of exegesis is a recognition of the indispensability of continuity as much as change in the historical process. Indeed the first third of his book brings this important and enduring insight to vivid life by showing how Hegel himself could never have accomplished the intellectual breakthroughs he did without standing on the shoulders of his great predecessor Kant. The detailed and cogent case that Bourke puts forward to show how Hegel had to wrestle with Kant’s epistemological and ethical outlook before he could proceed with formulating his own distinctive and revolutionary world view is one of the most arresting sections in what is a truly impressive work.\nReturning to the matter of Bourke’s view of the purpose of his chosen field of study, he addresses this question in the closing section of the book by asking whether intellectual history should adhere to ‘revivalism’ (seeking to resuscitate past ideas) or ‘historicism’ (accepting the pastness of the past). His account of the recent history of the academic study of intellectual history and its major players is characteristically assured even if it can occasionally give the false impression that the only place where worthwhile intellectual history happens is Cambridge (and its outposts).\nWhat are the key takeaways from Bourke’s assessment of the current state of the discipline? There are roughly two major points that he is keen to make. The first is that the tendency of the founders of the so-called Cambridge school of the history of ideas to assume the role of the moralist by reviving outmoded ideas for present purposes is problematic since it betrays their original and valid historicist impulses. Bourke judges that John Pocock, Quentin Skinner and John Dunn exhibited more wisdom when they focused exclusively on reconstructing the historical identity of past thinkers and their thoughts and left ‘the historical past to the past’. His second and related point is that one of the chief reasons why intellectual historians (and to an extent political theorists) are better off resisting revivalism is because their truer vocation is as diagnosticians rather than moralists. Bourke’s firm preference for adopting a ‘diagnostic’ (as distinct from a ‘prescriptive’) approach is that:\nIt helps us understand the character of political structures as products of earlier constellations of forces. It spurs us to pick apart distinct formations as well as to identify continuities across time. Its first duty is to avoid confusion between these two dimensions. Viewed from this angle, the most important task of contextualization is to highlight the diversity of contexts, not least their lack of homogenous synchronicity. We do not study Hegel to confound his circumstances with our own, but precisely to evaluate discrepancies between past and present. The process might reveal correlations and affinities, or equally it might bring out disparities. As Hegel argued at the beginning of the Science of Logic, there is no merit in cleaving ‘to forms of an earlier culture.’\nStrong stuff! But is he right? I’m not so sure. At least one major problem with Bourke’s bold assertion about the purpose of intellectual history is its historicist purism, maybe even fundamentalism. It’s far from clear why we should think that past ideas that are appropriately updated and adjusted cannot be used for present purposes. Of course, one can appreciate why Hegel believed it was unwise to try to recuperate or adhere to outmoded ideas since crucially he believed that history was on a progressive march forward leading us inexorably to perfect freedom. But given that Hegel’s teleological view of history has long ceased to be (or never was) credible, surely this affects how we might regard the relationship between the past and the present. Indeed one wonders if Hegel would have been prepared to accept Bourke’s austere view of the uses of history if he himself had lost faith in his own historical inevitabilism.\nThere is no need to believe in the myth that we can turn the clock back to insist that there are aspects of our past, even the very distant past of the ancient world, that may be worth reviving. In our current existential condition we need all the help we can muster to stop or at least delay that other clock getting any closer to midnight.\n1/2/2025\nJohnny Lyons lives in Dublin.\n\n    X\n\nDonation amount\n\n                                €1\n\n                                €2\n\n                                €5\n\n                                €10\n\n                                €20\n\n                                €50\n\n                                €100\n\n                                €200\n\n                                €500\n\n                                Other\n\n\t\t\tAmount\n\n\t\t\t\t\t\t€\n\nDonation frequency\n\n                    One-time\n\n                    Monthly\n\n                    Annual\n\n            E-mail address\n\n            Cardholder’s name\n\n\t\t\tCard info\n\n\t\t\t\tProve you are a human\n\n\t\tDonate €1\n\n        If you donate and also register with the drb you will enjoy permanent free access to our archive of several thousand essays on reaching €100 in donations. You can also keep track of your donations on your drb dashboard.\n\n            DONATE With Registration\n\n                DONATE\n\n    jQuery(\".donate-pop1\").click(function(){\n        jQuery(\".donate-content\").show();\n        jQuery(\".donate-but-content\").hide();\n        jQuery(\"html, body\").animate({\n            scrollTop: jQuery(\"#wpfs-card-holder-email--MTY1NjB\").offset().top\n        }, 2000);\n    })\n     jQuery(\".donate-close1\").click(function(){\n        jQuery(\".donate-content\").hide();\n        jQuery(\".donate-but-content\").show();\n    })\n\n       .pf-content .printfriendly{display:none}\n\n     jQuery(document).ready(function() {\n         jQuery('img[alt=\"Download PDF\"]:odd').hide();\n        jQuery(\".opencat\").click(function() {\n            jQuery(this).hide();\n            jQuery(\".closecat\").show();\n            jQuery(this).parent(\"li\").find(\"ul\").show();\n        })\n        jQuery(\".closecat\").click(function() {\n            jQuery(this).hide();\n            jQuery(\".opencat\").show();\n            jQuery(this).parent(\"li\").find(\"ul\").hide();\n        })\n\t\tjQuery(\".donate-pop\").click(function(){\n\t\t\tjQuery(\".donate-content\").show();\n\t\t\tjQuery(\".donate-but-content\").hide();\n\t\t\tjQuery(\".btn-without-div\").hide();\n\t\t\tjQuery(\".no-donate-content,.not-logged-donate,.btn-without\").show();\n\t\t})\n\t\t jQuery(\".btn-without\").click(function(){\n\t\t\t jQuery(\".btn-without-div\").show();\n\t\t\t jQuery(\".btn-without,.no-donate-content,.not-logged-donate\").hide();\n\t\t })\n\t\t jQuery(\".donate-close\").click(function(){\n\t\t\tjQuery(\".donate-content\").hide();\n\t\t\tjQuery(\".donate-but-content\").show();\n\t\t\t jQuery(\".no-donate-content,.not-logged-donate,.btn-without\").show();\n\t\t})\n     });\n\n                                        Previous article\n                                        Hardened Skin in the Game",
    "summary": {
      "en": "**Summary of \"Talkin’ about a Revolution\" by Johnny Lyons**\n\nThe text discusses the current state of humanity and the perilous condition of the world, reflecting on the Doomsday Clock, which indicates how close we are to global disaster. As of early January 2025, the clock is set at just 90 seconds to midnight, highlighting concerns such as nuclear threats from major powers and severe climate change impacts.\n\nLyons contrasts this grim reality with the past optimism following the Cold War, when the collapse of communism led many to believe in continuous progress through capitalism and democracy. However, recent global developments suggest that humanity may be moving toward destruction instead of progress.\n\nThe author poses important questions about whether we can change our course and what that would entail—whether it would require a complete revolution or a radical adjustment of our current systems. He critiques contemporary philosophy for its detachment from political engagement and reflects on Hegel’s views, which see history as a purposeful journey toward greater freedom.\n\nRichard Bourke’s book, \"Hegel’s World Revolutions,\" is introduced as a significant examination of Hegel’s philosophy, especially his concept of historical progress through revolutions. Bourke argues that true historical change is complex and often involves setbacks rather than a straightforward progression.\n\nBourke also critiques a modern cultural malaise that rejects Enlightenment values. He advocates for a historical understanding that acknowledges both continuity and change, suggesting that intellectual historians should focus on understanding past ideas without necessarily reviving them for present use.\n\nIn conclusion, Lyons emphasizes the need for critical engagement with both history and philosophy to address today's existential challenges, suggesting that while we cannot revert to the past, we can learn from it to navigate our future.",
      "ko": "현재 인류의 상태와 세계의 위험한 상황에 대해 논의하는 이 글은 종말 시계에 주목합니다. 이 시계는 우리가 글로벌 재앙에 얼마나 가까운지를 나타내며, 2025년 1월 초 현재 자정까지 단 90초 남았다는 경고를 전합니다. 이는 주요 강국의 핵 위협과 심각한 기후 변화의 영향 등 여러 우려를 반영합니다.\n\n저자는 이러한 암울한 현실을 냉전 이후의 과거 낙관주의와 대조합니다. 공산주의의 붕괴 이후 많은 사람들이 자본주의와 민주주의를 통해 지속적인 발전이 가능하다고 믿었지만, 최근의 세계적 발전은 인류가 진보가 아닌 파멸로 나아가고 있음을 시사합니다.\n\n저자는 우리가 방향을 바꿀 수 있는지, 그리고 그것이 무엇을 의미하는지를 질문합니다. 이는 완전한 혁명이 필요할 것인지, 아니면 현재 시스템의 급진적인 조정이 필요한지를 고민하게 만듭니다. 그는 현대 철학이 정치적 참여와의 단절을 비판하며, 헤겔의 역사관을 반영합니다. 헤겔은 역사를 더 큰 자유를 향한 목적 있는 여정으로 보았습니다.\n\n리처드 부르크의 저서 \"헤겔의 세계 혁명\"은 헤겔 철학, 특히 혁명을 통한 역사적 발전 개념을 심도 있게 다루고 있습니다. 부르크는 진정한 역사적 변화가 복잡하며 종종 직선적인 발전이 아닌 후퇴를 포함한다고 주장합니다.\n\n부르크는 또한 계몽주의 가치를 거부하는 현대 문화의 불만을 비판합니다. 그는 연속성과 변화를 모두 인정하는 역사적 이해를 지지하며, 지식 역사학자들이 과거의 아이디어를 현재에 되살리는 것이 아니라 이해하는 데 집중해야 한다고 제안합니다.\n\n저자는 역사와 철학에 대한 비판적 참여가 오늘날의 존재론적 도전에 대응하는 데 필요하다고 강조합니다. 과거로 돌아갈 수는 없지만, 과거로부터 배워 미래를 헤쳐 나갈 수 있다는 메시지를 전합니다.",
      "ja": "「トーキン・アバウト・ア・レボリューション」の要約では、人類の現状と世界の危機的な状態について述べられています。特に、世界的な災害までの距離を示す「終末時計」に焦点を当てており、2025年1月初めには、時計が真夜中まで90秒という危険な状態に設定されています。このことは、主要国からの核の脅威や深刻な気候変動の影響などの懸念を浮き彫りにしています。\n\nライオンズは、この厳しい現実を冷戦後の楽観主義と対比させています。共産主義の崩壊により、多くの人々が資本主義と民主主義を通じての継続的な進歩を信じていました。しかし、最近の世界の動きは、人類が進歩ではなく破壊に向かっている可能性を示唆しています。\n\n著者は、私たちが進むべき道を変えることができるのか、そしてそれには何が必要なのかという重要な問いを投げかけています。それは完全な革命を必要とするのか、現在のシステムの根本的な調整で済むのかということです。ライオンズは、現代の哲学が政治的な関与から乖離していることを批判し、ヘーゲルの見解を反映させています。ヘーゲルは歴史をより大きな自由に向かう目的のある旅と見なしています。\n\nリチャード・バークの著書「ヘーゲルの世界革命」が紹介されており、ヘーゲルの哲学、特に革命を通じた歴史的進歩の概念についての重要な考察がなされています。バークは、真の歴史的変化は複雑であり、単純な進行ではなく、しばしば後退を伴うと主張しています。\n\nまた、バークは啓蒙思想の価値を拒否する現代の文化的な病理についても批判しています。彼は、連続性と変化の両方を認識する歴史的理解を提唱し、知的歴史家は過去のアイデアを現在のために復活させるのではなく、理解することに焦点を当てるべきだと述べています。\n\nライオンズは、今日の存在的な課題に対処するためには、歴史と哲学に対する批判的な関与が必要であると強調しています。過去に戻ることはできませんが、過去から学ぶことで未来を切り開くことができると示唆しています。"
    }
  },
  {
    "id": "bd76ed7f1710b8d2",
    "title": {
      "en": "AI was enemy No. 1 during Hollywood strikes. Now it's in Oscar-winning films",
      "ko": "할리우드의 AI 반전",
      "ja": "ハリウッドの敵、AIの逆転"
    },
    "type": "story",
    "url": "https://www.bbc.co.uk/news/articles/ce303x19dwgo",
    "score": 18,
    "by": "jeffwass",
    "time": 1743490411,
    "content": "AI was enemy No. 1 during Hollywood strikes. Now it's in Oscar-winning filmsImage source, Getty ImagesImage caption, Countless signs during the historic Hollywood strikes decried using AI. It's now creeping further into the entertainment industry Regan Morris BBC News, Los Angeles Published31 March 2025Inside a soundstage once used by silent film stars Charlie Chaplin and Mabel Normand, Hollywood executives, actors and filmmakers sipped cocktails as they marvelled at what some say is the biggest breakthrough since the talkies: AI-generated video.But whether AI will be the future, or the end, of cinema is still up for debate.It was only two years ago that actors and writers shut down Hollywood with strikes demanding protections from AI. Now the technology is controversially creeping into TV, movies and video games. Two films honoured at the Oscars even used the technology. As a DJ played '90s hip hop, computer developers rubbed shoulders with actors and executives, in a sign of the changing power players in the industry.AI in Hollywood is \"inevitable\", says Bryn Mooser, the party's host and the co-founder of Moonvalley, which created the AI generator tool Marey by paying for footage from filmmakers with their consent. Mooser says that while AI may still be a dirty word, their product is \"clean\" because it pays for its content.Image source, Courtesy of Joelle Grace Taylor for AsteriaImage caption, Filmmakers, executives and actors rubbed shoulders at the new AI production company\"Artists should be at the table,\" he says, adding that it's better to build the tool for filmmakers rather than get \"rolled over by big tech companies\".Artificial Intelligence has long been depicted as a villain in Hollywood. In The Terminator, AI used by the US military decides it must destroy everyone on earth.But it's AI's creators, and not the technology itself, that has received the brunt of real-life criticism. Companies use publicly available data to build their AI models - which includes copyrighted material shared online - and creators say they're being ripped off.OpenAI, Google and other tech companies are facing multiple lawsuits from writers, actors and news organisations, alleging their work was stolen to train AI without their consent. Studios like Paramount, Disney and Universal who own the copyright on movies and TV shows have been urged by writers to do the same, though none have taken legal action.\"We've all fought very hard for copyright laws, and nobody wants to see their work stolen to have somebody else profit from it,\" Mooser says. Hollywood has begun toying with the new technology. The Oscar-nominated films Emilia Perez and The Brutalist used AI to alter voices. Adrian Brody won the Academy Award for best actor, even with the help of AI to fine tune his accent when he spoke Hungarian in his starring role in The Brutalist. AI has even been used to de-age actors like Tom Hanks and Harrison Ford.Image source, Getty ImagesImage caption, Generative AI was used to fine tune Adrian Brody's accent when he spoke Hungarian in The BrutalistThe technology is seemingly everywhere. OpenAI hosted an AI film festival in Los Angeles earlier this month. Marvel directors Joe and Anthony Russo told the Wall Street Journal they plan to invest $400 million to craft AI tools for filmmakers.But the impacts on how it will alter the future of the entertainment industry remain unclear. Generative AI, for instance, allows computers to learn and solve problems in ways that can seem human – albeit much faster. And many worry about the technology replacing their jobs as AI is used to generate scripts, animation, locations, voices and human actors.If you ask OpenAI's popular chatbot, ChatGPT, which Hollywood jobs are most easily replaced by AI, background actors are top of the list as \"most vulnerable\" with \"A-List actors & directors\" considered safest because \"star power and brand recognition keep top talent irreplaceable\".At the Moonvalley party, everyone was talking about new AI technology though few wanted to speak with a reporter on record about it. But dozens of powerful people made the trek east to the hip Silver Lake neighbourhood from west Los Angeles, even though it was raining. In LA, that's remarkable.\"We're here to learn,\" said one executive who spent an hour in traffic getting to the party. \"We're not signing anything or buying anything, but we're interested.\"Mooser and his co-founder Naeem Talukdar speak passionately about how AI will transform Hollywood and allow filmmakers to create blockbuster style epics on much lower budgets. It could lead to many bad films - but it could also help discover the next Quentin Tarantino or Martin Scorsese, even if they don't have the backing of a big studio.\"This technology is utterly meaningless without the artist at the centre of it; the technology needs to ultimately be subservient to the artist,\" says Talukdar.Image source, Getty ImagesImage caption, Arnold Schwarzenneger played a murderous AI in the TerminatorHollywood's foray into using AI comes as the Trump administration prepares a new AI plan for the United States. Tech companies say they can't compete with China under existing US copyright laws and that they need unfettered access to art - from Mickey Mouse to Moana to The Matrix - to train their AI models as a matter of national security.Google and OpenAI want the US government to designate copyrighted art, movies and TV shows as \"fair use\" for them to train AI, arguing that without the exceptions, they will lose the race for dominance to China.Hollywood filmmakers say tech companies are attempting to undermine the entertainment industry, which they point out supports more than 2.3 million US jobs.\"We firmly believe that America's global AI leadership must not come at the expense of our essential creative industries,\" a group of more than 400 Hollywood A-listers - led by actress/writer Natasha Lyonne who helped develop Moonvalley - wrote in an open letter to the Trump administration, which has been soliciting public comment for its AI Action Plan.The letter's signatories included A-List stars like Ben Stiller, Sir Paul McCartney, Cate Blanchett and Lilly Wachowski, who co-created The Matrix, which depicts a dystopian simulated reality where humans are enslaved by intelligent machines.Many in Hollywood remain terrified of what AI means for their futures.Outside a Disney Character Voices office earlier this month, dozens of actors picketed against video game companies for refusing to come to an agreement on using AI in video games.\"Using actual actors is the key to a lot of the drama and enjoyment that people get from video games,\" actor DW McCann said. \"People have lived experiences that AI just can't understand.\"The actors want a contract that guarantees their voices and likeness will not be used without their consent to train AI models that replace them in the future. Mooser says AI will allow filmmakers to create amazing art – if it's done right.  With humans calling the shots, he says, AI could help them create sets and worlds they couldn't easily access or invent – and to do so much faster than what they could traditionally do with computer graphics and visual effects.\"We're trying to say, look, technology is going to be in everything. Let's make sure that we try to fight as hard as we can to make sure that it's done in the right way, and that artists aren't run over by big companies.\"Related topicsLos AngelesFilmUnited StatesHollywoodCaliforniaMore on this storyHollywood writers fear losing work to AIPublished27 July 2023Actors go on strike over video games AI threatPublished26 July 2024'A tech firm stole our voices - then cloned and sold them'Published1 September 2024Susan Sarandon on the dangers of AI in film industry. Video, 00:00:30Susan Sarandon on the dangers of AI in film industryPublished14 July 20230:30Hollywood writers in deal to end US studio strikePublished25 September 2023",
    "summary": {
      "en": "AI has shifted from being seen as a threat during recent Hollywood strikes to being embraced in Oscar-winning films. Just two years ago, actors and writers protested against AI, demanding protections as the technology began to infiltrate the entertainment industry. Now, AI is being used in films like \"Emilia Perez\" and \"The Brutalist,\" where it has altered voices and helped actors fine-tune performances. \n\nAt a recent event in Hollywood, industry leaders expressed mixed feelings about AI's role, with some arguing it can enhance filmmaking while others worry it might replace jobs. Companies like OpenAI and Google are facing lawsuits for allegedly using copyrighted material to train their AI models without consent. \n\nMany Hollywood professionals are concerned about the implications of AI, particularly regarding job security. They advocate for strong copyright protections to ensure that artists' rights are respected. A group of over 400 industry figures has urged the U.S. government to protect creative jobs while navigating AI's growing presence.\n\nDespite fears, some believe AI could help democratize filmmaking by enabling lower-budget projects. The overall impact of AI on the entertainment industry remains uncertain, but there is a clear push for artists to retain control over how the technology is used.",
      "ko": "최근 할리우드 파업을 겪으면서 AI는 위협으로 여겨지던 시기를 지나, 이제는 오스카 수상 영화에서도 활용되고 있다. 불과 2년 전, 배우들과 작가들은 AI의 침투에 반대하며 보호를 요구하는 시위를 벌였다. 그러나 현재 \"에밀리아 페레즈\"와 \"브루탈리스트\"와 같은 영화에서는 AI가 목소리를 변형하고 배우들의 연기를 조정하는 데 도움을 주고 있다.\n\n최근 할리우드에서 열린 행사에서 업계 리더들은 AI의 역할에 대해 엇갈린 의견을 보였다. 일부는 AI가 영화 제작을 향상시킬 수 있다고 주장하는 반면, 다른 이들은 AI가 일자리를 대체할까 우려하고 있다. OpenAI와 구글과 같은 기업들은 저작권이 있는 자료를 동의 없이 사용해 AI 모델을 훈련시켰다는 이유로 소송에 직면해 있다.\n\n많은 할리우드 전문가들은 AI의 영향, 특히 일자리 안정성에 대해 걱정하고 있다. 그들은 예술가의 권리가 존중받을 수 있도록 강력한 저작권 보호를 요구하고 있다. 400명 이상의 업계 인사들은 미국 정부에 창의적인 일자리를 보호해 줄 것을 촉구하며 AI의 확산에 대응하고 있다.\n\n두려움에도 불구하고, 일부는 AI가 저예산 프로젝트를 가능하게 하여 영화 제작의 민주화를 도울 수 있다고 믿고 있다. AI가 엔터테인먼트 산업에 미치는 전반적인 영향은 아직 불확실하지만, 기술 사용에 대한 예술가들의 통제권을 유지하려는 명확한 요구가 있다.",
      "ja": "最近のハリウッドのストライキを経て、AIは脅威と見なされることから、オスカー受賞作品に取り入れられる存在へと変わりました。わずか2年前、俳優や脚本家たちはAIに対して抗議し、この技術がエンターテインメント業界に浸透し始めたことに対する保護を求めていました。しかし今では、「エミリア・ペレス」や「ザ・ブルータリスト」といった映画でAIが活用され、声の変換や演技の微調整に役立っています。\n\n最近のハリウッドのイベントでは、業界のリーダーたちがAIの役割についてさまざまな意見を述べました。AIが映画制作を向上させる可能性があると主張する人もいれば、仕事を奪うのではないかと懸念する人もいます。OpenAIやGoogleなどの企業は、著作権のある素材を無断で使用してAIモデルを訓練したとして訴訟に直面しています。\n\n多くのハリウッドの専門家は、AIの影響、特に雇用の安定性について懸念を抱いています。彼らはアーティストの権利が尊重されるよう、強力な著作権保護を求めています。400人以上の業界関係者が、AIの存在が増す中で創造的な仕事を守るために米国政府に対策を求めています。\n\n恐れがある一方で、AIが低予算のプロジェクトを可能にすることで映画制作の民主化を助ける可能性があると考える人もいます。AIがエンターテインメント業界に与える全体的な影響はまだ不透明ですが、アーティストが技術の使用方法をコントロールできるようにするための明確な動きがあります。"
    }
  },
  {
    "id": "89453cd5a70e4320",
    "title": {
      "en": "RPCEmu is an emulator of classic Acorn computer systems",
      "ko": "고전 아콘 컴퓨터 에뮬레이터",
      "ja": "アコーン復活！"
    },
    "type": "story",
    "url": "https://www.marutan.net/rpcemu/index.php",
    "score": 61,
    "by": "andsoitis",
    "time": 1743398883,
    "content": "RPCEmu is an emulator of classic Acorn computer systems, such as the Risc PC and A7000. It runs on\nmultiple platforms including Windows, Linux and MacOSX\n\nRPCEmu requires a RISC OS ROM image to work, find details of\nwhere to find one or try an RPCEmu Easy-Start bundle.\n\nRPCEmu should be considered Alpha Quality code. It\nhas many known and unknown bugs, and all files used with it should be\nwell backed up before using them with RPCEmu.\n\nDocumentation\n\nThe following documents are available.\n\n User Manual - All platforms.\n Networking Guide - Windows/Linux.\n Using ROOL RISC OS 5 - All platforms.\n Tutorial - Installing RPCEmu and RISC OS 3.71 on Windows - on 4corn.\n How to compile from source - Windows\n How to compile from source - Linux\n How to compile from source - FreeBSD\n How to compile from source - OpenBSD OpenBSD compilation requires a newer compiler than supplied in the default distribution, suggestions welcomed\n\n (Advanced) Tutorial - Running RPCEmu as the Phoebe Risc PC 2 - on 4corn.\n\nIf you have any questions or need help, there is a RPCEmu Mailing\nList, however please specify the version number\nto avoid confusion. Your patches, suggestions and even bug reports are\ngratefully received.\n\nDownloads\n\n Want to say thank you for RPCEmu?\n\n .bmc-btn svg {\n    height: 32px !important;\n    margin-bottom: 0px !important;\n    box-shadow: none !important;\n    border: none !important;\n    vertical-align: middle !important;\n    transform: scale(0.9);\n    flex-shrink: 0;\n}\n\n.bmc-btn {\n    min-width: 210px;\n    color: #000000;\n    background-color: #FFDD00 !important;\n    height: 60px;\n    border-radius: 12px;\n    font-size: 24px;\n    font-weight: Bold;\n    border: none;\n    padding: 0px 24px;\n    line-height: 27px;\n    text-decoration: none !important;\n    display: inline-flex !important;\n    align-items: center;\n    font-family: 'Inter', cursive !important;\n    -webkit-box-sizing: border-box !important;\n    box-sizing: border-box !important;\n}\n\n.bmc-btn:hover, .bmc-btn:active, .bmc-btn:focus {\n    text-decoration: none !important;\n    cursor: pointer;\n}\n\n.bmc-btn-text {\n   text-align: left;\n   margin-left: 8px;\n   display: inline-block;\n   line-height: 0;\n   width: 100%;\n   flex-shrink: 0;\n   font-family: [FONT] !important;\n}\n\n.logo-outline {\n    fill: #000000;\n}\n\n.logo-coffee {\n    fill: #ffffff;\n}\n\nBuy me a tea or coffee\n\n <a href=\"https://www.buymeacoffee.com//rpcemu\">Buy me a tea or coffee</a>\n\nBinary Packages (Version 0.9.5)\n\n   Format Notes\n\n  Windows781011(32/64 bit)\n\n  Zip Archive Install\n      to the location of your choice.\n\nSource Code (Version 0.9.5)\n\n   Format Notes\n\n  All Platforms\n  Source Code\n  Use this to compile up a version for Linux or OpenBSD, also for Windows.\n\nEasy-Start bundles\n\nDownload combination RPCEmu, RISC OS and harddisc Easy-Start bundles, featuring full RISC OS 3.71 and RISC OS Direct (5.27) builds.\n\nContributed Builds\n\nhttps://github.com/Septercius/rpcemu-dev/releases\n\n   Format Notes\n\n  MacOSX\n  Binary\n  0.9.3a. A 0.9.3 binary (Timothy Coltman<)\n\nPrevious Releases\n\nAdditional Downloads\n\n  Format\n\n  Blank Pre-Formatted Hard Discs\n  ADFSE\n  256MB\n  1GB\n\n  ADFSE+ (long filenames,RISCOS 3.80 or later only)\n  256MB\n  1GB\n\n  Blank Pre-Formatted Floppy Discs\n  ADFSE\n  800KB\n\n  All downloads in Zip format, decompress before use. No download is larger that 1MB.\n\nRelease Notes\n\nVersion 0.9.5 - 26/10/2024\n\nChanges in this build\n\n Mouse\n\n  Implement mouse-wheel support to scroll window contents. Based\n        upon work by Daniel Clarke for Arcem.\n\n Floppy Disc\n\n  Add the ability to create blank, pre formatted, floppy disc images\n  from the UI.\n  Restore the ability to format blank files as disc images (a regression\n  in 0.9.4)\n\n IDE Hard Discs\n\n  Support LBA (Logical Block Addressing) hard disc image files, based\n  upon work by Phil Morris.\n  Correctly detect and work around hard drive images that have the\n  off by 512 byte (1 sector) bug and no longer create new hard drive\n  images that have that issue when formatting new hard discs.\n\n Networking\n\n  Refactor and refine the network driver\n  Use up to date header files for RISC OS libraries\n  Correct the usage of podule interrupts, allowing other podules\n  to also handle them.\n\n Timing\n\n  Improve timer accuracy of 'intermediate' values. Based upon work by\n  Jeffrey Lee\n\n HostFS\n\n  Add support for high-resolution timestamps for HostFS on 64-bit Linux\n   (this was already available on Windows)\n\nOlder Release Notes\nFull Changelog\n\nDeveloper Information\n\n We're very happy for RPCEmu to accept code and contributions from third\n parties, already many people have contributed code and we hope to\n continue down this route.\n\n The RPCEmu source is stored inside the Mercurial version control system,\n Mercurial clients are available for many platforms, including (but not\n limited to) Windows, Linux, Mac OS X, Solaris, BSDs. If you are used to\n a different version control system, such as CVS or SVN, Mercurial (as a\n distributed version control system) may seem a little odd, but there are\n plenty of tutorials and advice for users coming from other systems on the\n web.\n\n The Mercurial repository is based at\n\n http://www.home.marutan.net/hg/rpcemu\n\n and checking out the code is as simple as\n\n hg clone http://www.home.marutan.net/hg/rpcemu rpcemu\n\n How to get code committed into the repository\n Prepare a patch file of\n the differences between your new code and the current 'HEAD' of the\n project.\n Mercurial provides the 'hg diff' command, which generates diff files\n suitable for this.\n\nThen post this patch file to the Stardot forum (or to us directly if you\n so wish), with an explanation of what it's for, and in the case of bug\n fixes, the bug it's meant to fix (it's not always obvious from the code)\n\nGenerally the smaller the patch, with the most specific function or\n reason, the easier the patch is to merge in, whereas a \"I fixed\n everything I thought was wrong\" would take a very long time to verify and\n test.\n\nPatches may get edited before being committed, or if the changes required\n are particularly large an updated patch may be requested of the\n developer. In some rare cases it might not be possible to commit a patch,\n and at that point there should be a stated reason (e.g. patch overlaps\n with another patch that deprecates it, patch is too widespread to verify\n (suggest breaking into smaller patches), etc).\n\nIf you have any questions, once again the Stardot forum is the best place\n to ask, also, if you're considering a particulaly large change, that\n would affect a large functional area or many files, it might well be\n worth getting some advice from the developers on the forum first,\n incase it overlaps with other development work, or even just a suggestion\n of how to accomplish it in smaller changes.",
    "summary": {
      "en": "**RPCEmu Overview:**\n\nRPCEmu is an emulator for classic Acorn computers like the Risc PC and A7000. It works on Windows, Linux, and MacOSX, but requires a RISC OS ROM image to function. Users should back up their files, as the software is still in Alpha quality and may have many bugs.\n\n**Documentation Available:**\n- User Manual for all platforms\n- Networking Guide for Windows/Linux\n- Tutorial for installing RPCEmu and RISC OS\n- Compilation guides for different operating systems\n- Advanced tutorial for running RPCEmu as Phoebe Risc PC 2\n\nFor questions or support, there's a mailing list available. It's helpful to mention the version number when seeking assistance.\n\n**Downloads:**\n- **Binary Packages:** Version 0.9.5 available for Windows (32/64 bit).\n- **Source Code:** For compiling on various platforms.\n- **Easy-Start Bundles:** Includes RPCEmu, RISC OS, and hard disk images.\n- **Additional Downloads:** Pre-formatted hard disks and floppy disk images.\n\n**Latest Release Notes (Version 0.9.5):**\n- Added mouse-wheel support.\n- Improved floppy disk image creation.\n- Enhanced IDE hard disk support.\n- Refined networking capabilities.\n- Improved timer accuracy.\n\n**Developer Information:**\nRPCEmu welcomes contributions from developers. The source code is managed in Mercurial, and patches can be submitted for review. For assistance, the Stardot forum is recommended, especially for larger changes.",
      "ko": "RPCEmu는 Risc PC와 A7000과 같은 고전 Acorn 컴퓨터를 위한 에뮬레이터입니다. 이 소프트웨어는 Windows, Linux, MacOSX에서 작동하지만, 기능을 위해 RISC OS ROM 이미지가 필요합니다. 사용자는 소프트웨어가 아직 알파 버전이므로 많은 버그가 있을 수 있으니 파일을 백업하는 것이 좋습니다.\n\n사용자 매뉴얼, Windows/Linux용 네트워킹 가이드, RPCEmu와 RISC OS 설치 튜토리얼, 다양한 운영 체제에 대한 컴파일 가이드, Phoebe Risc PC 2로 RPCEmu를 실행하는 고급 튜토리얼 등 다양한 문서가 제공됩니다. 질문이나 지원이 필요할 경우 메일링 리스트를 이용할 수 있으며, 도움을 요청할 때는 버전 번호를 언급하는 것이 유용합니다.\n\n다운로드 항목으로는 Windows(32/64비트)용 버전 0.9.5의 바이너리 패키지, 다양한 플랫폼에서 컴파일할 수 있는 소스 코드, RPCEmu, RISC OS 및 하드 디스크 이미지를 포함한 간편 시작 번들이 있습니다. 추가 다운로드 항목으로는 미리 포맷된 하드 디스크와 플로피 디스크 이미지가 있습니다.\n\n최신 릴리스 노트(버전 0.9.5)에서는 마우스 휠 지원 추가, 플로피 디스크 이미지 생성 개선, IDE 하드 디스크 지원 강화, 네트워킹 기능 향상, 타이머 정확도 개선이 포함되었습니다.\n\nRPCEmu는 개발자들의 기여를 환영합니다. 소스 코드는 Mercurial에서 관리되며, 패치는 검토를 위해 제출할 수 있습니다. 더 큰 변경 사항에 대한 지원이 필요할 경우 Stardot 포럼을 추천합니다.",
      "ja": "RPCEmuは、Risc PCやA7000などの古いAcornコンピュータ用のエミュレーターです。このソフトウェアはWindows、Linux、MacOSXで動作しますが、機能するためにはRISC OSのROMイメージが必要です。ユーザーは、ソフトウェアがまだアルファ版であり、多くのバグが存在する可能性があるため、ファイルのバックアップを取ることをお勧めします。\n\n利用可能なドキュメントには、すべてのプラットフォーム向けのユーザーマニュアル、Windows/Linux用のネットワーキングガイド、RPCEmuとRISC OSのインストールに関するチュートリアル、さまざまなオペレーティングシステム向けのコンパイルガイド、Phoebe Risc PC 2としてRPCEmuを実行するための高度なチュートリアルがあります。質問やサポートが必要な場合は、メーリングリストが利用可能で、助けを求める際にはバージョン番号を記載することが役立ちます。\n\nダウンロードには、Windows（32/64ビット）用のバイナリパッケージ（バージョン0.9.5）、さまざまなプラットフォーム向けのソースコード、RPCEmu、RISC OS、ハードディスクイメージを含むイージースタートバンドル、事前フォーマットされたハードディスクやフロッピーディスクイメージなどの追加ダウンロードがあります。\n\n最新のリリースノート（バージョン0.9.5）では、マウスホイールのサポートが追加され、フロッピーディスクイメージの作成が改善され、IDEハードディスクのサポートが強化され、ネットワーキング機能が洗練され、タイマーの精度が向上しました。\n\nRPCEmuは、開発者からの貢献を歓迎しています。ソースコードはMercurialで管理されており、パッチを提出してレビューを受けることができます。大きな変更については、Stardotフォーラムでのサポートをお勧めします。"
    }
  },
  {
    "id": "ca8e6e4e22452519",
    "title": {
      "en": "The Surprising History of Scientific Ballooning in 11 Missions",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://nautil.us/the-surprising-history-of-scientific-ballooning-in-11-missions-1200217/",
    "score": 18,
    "by": "dnetesn",
    "time": 1743263242,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "1415dc1cdc5155ad",
    "title": {
      "en": "How to Secure Existing C and C++ Software Without Memory Safety [pdf]",
      "ko": "C/C++ 소프트웨어 안전하게 지키기",
      "ja": "メモリ安全性なしでC/C++を守る方法"
    },
    "type": "story",
    "url": "https://arxiv.org/abs/2503.21145",
    "score": 128,
    "by": "aw1621107",
    "time": 1743406616,
    "content": "The most important security benefit of software memory safety is easy to state: for C and C++ software, attackers can exploit most bugs and vulnerabilities to gain full, unfettered control of software behavior, whereas this is not true for most bugs in memory-safe software. Fortunately, this security benefit -- most bugs don't give attackers full control -- can be had for unmodified C/C++ software, without per-application effort. This doesn't require trying to establish memory safety; instead, it is sufficient to eliminate most of the combinatorial ways in which software with corrupted memory can execute. To eliminate these interleavings, there already exist practical compiler and runtime mechanisms that incur little overhead and need no special hardware or platform support. Each of the mechanisms described here is already in production use, at scale, on one or more platforms. By supporting their combined use in development toolchains, the security of all C and C++ software against remote code execution attacks can be rapidly, and dramatically, improved.",
    "summary": {
      "en": "Software memory safety greatly enhances security, especially for C and C++ programs. In these languages, many bugs can allow attackers to take complete control of the software. However, most bugs in memory-safe software do not have this risk. \n\nGood news: we can improve the security of existing C/C++ software without needing to change the code or use special hardware. Instead of trying to make the software completely memory-safe, we can reduce the ways that corrupted memory can lead to security issues. There are practical tools and techniques available that do this with minimal impact on performance, and they are already in use in various systems. By integrating these tools into development processes, we can significantly enhance the security of C and C++ software against attacks.",
      "ko": "소프트웨어 메모리 안전성은 보안을 크게 향상시키며, 특히 C와 C++ 프로그램에서 그 효과가 두드러집니다. 이 언어들에서는 많은 버그가 공격자에게 소프트웨어를 완전히 제어할 수 있는 기회를 줄 수 있습니다. 그러나 메모리 안전성이 보장된 소프트웨어에서는 대부분의 버그가 이러한 위험을 동반하지 않습니다.\n\n기쁜 소식은 기존의 C/C++ 소프트웨어의 보안을 코드 변경이나 특별한 하드웨어 없이도 개선할 수 있다는 점입니다. 소프트웨어를 완전히 메모리 안전하게 만들려는 대신, 손상된 메모리가 보안 문제로 이어질 수 있는 경로를 줄이는 방법을 사용할 수 있습니다. 성능에 미치는 영향이 최소화된 실용적인 도구와 기술이 이미 다양한 시스템에서 사용되고 있습니다. 이러한 도구를 개발 과정에 통합함으로써 C와 C++ 소프트웨어의 공격에 대한 보안을 크게 강화할 수 있습니다.",
      "ja": "ソフトウェアのメモリ安全性は、特にCやC++プログラムのセキュリティを大幅に向上させます。これらの言語では、多くのバグが攻撃者にソフトウェアの完全な制御を許す可能性があります。しかし、メモリ安全なソフトウェアのバグの多くは、このリスクを伴いません。\n\n朗報です。既存のC/C++ソフトウェアのセキュリティを、コードを変更したり特別なハードウェアを使用したりすることなく向上させることができます。ソフトウェアを完全にメモリ安全にするのではなく、破損したメモリがセキュリティ問題につながる方法を減らすことが可能です。パフォーマンスに最小限の影響を与える実用的なツールや技術があり、すでにさまざまなシステムで使用されています。これらのツールを開発プロセスに統合することで、CやC++ソフトウェアの攻撃に対するセキュリティを大幅に強化できます。"
    }
  },
  {
    "id": "b6e98c41244e3e3f",
    "title": {
      "en": "White House says it's 'case closed' on the Signal group chat review",
      "ko": "백악관, 시그널 그룹채팅 조사 종료 선언",
      "ja": "ホワイトハウス「信号グループ調査終了」"
    },
    "type": "story",
    "url": "https://www.npr.org/2025/03/31/nx-s1-5345865/white-house-signal-group-chat-review",
    "score": 55,
    "by": "doener",
    "time": 1743499735,
    "content": "Politics\n\n      White House says it's 'case closed' on the Signal group chat review\n\n            Updated March 31, 202510:56 PM ET\n\n                    Originally published March 31, 20253:04 PM ET\n\n      Franco Ordoñez\n\n                White House press secretary Karoline Leavitt speaks to reporters on the White House driveway on March 31, 2025.\n\n                    Andrew Harnik/Getty Images\n\n                hide caption\n\n            toggle caption\n\n        Andrew Harnik/Getty Images\n\n   The White House has concluded its review of how Atlantic editor Jeffrey Goldberg was inadvertently included on a Signal message group chat of high-ranking officials discussing impending strikes in Yemen.\n\n                        National Security\n            'Heads are exploding': How security experts see the Signal war-plan breach\n\n   The Atlantic story, published one week ago, stunned Washington because of the sensitive nature of the information disclosed on the app. The White House has said none of the information was classified.   \"This case has been closed here at the White House as far as we are concerned,\" press secretary Karoline Leavitt told reporters on Monday. \"There have been steps made to ensure that something like that can obviously never happen again, and we're moving forward,\" she said.   Leavitt did not offer details about what steps the White House is taking after its review. Last week, she had told reporters that the National Security Council, the White House counsel's office and Trump adviser Elon Musk were all looking into how the mishap happened.\n    Sponsor Message\n\n   Leavitt said Trump's national security adviser Mike Waltz — who created the group chat and added Goldberg to it — \"continues to be an important part of (Trump's) national security team.\"   The Republican chairman and ranking Democrat on the Senate Armed Services Committee have asked the Pentagon's acting inspector general to investigate the use of the app for sharing the information.   A nonprofit watchdog, American Oversight, has sued to ensure the records of the Signal group chat are kept in accordance with the Federal Records Act.   NPR disclosure: Katherine Maher, the CEO of NPR, chairs the board of the Signal Foundation.\n\n            Donald Trump\n      Mike Waltz\n      Yemen\n\n            Facebook\n      Flipboard\n      Email",
    "summary": {
      "en": "The White House has finished its review of a situation where Atlantic editor Jeffrey Goldberg was mistakenly added to a Signal group chat discussing sensitive military operations in Yemen. Press Secretary Karoline Leavitt stated that the matter is considered \"case closed\" and emphasized that no classified information was shared. She mentioned that measures are being taken to prevent similar incidents in the future, but did not provide specific details. National Security Adviser Mike Waltz, who created the chat, remains an important member of Trump's team. There are ongoing investigations by the Pentagon and a lawsuit from a watchdog group to ensure proper record-keeping of the chat.",
      "ko": "백악관은 아틀란틱 편집자인 제프리 골드버그가 예멘에서의 민감한 군사 작전과 관련된 시그널 그룹 채팅에 잘못 추가된 사건에 대한 검토를 마쳤습니다. 카롤라인 레빗 대변인은 이 사건이 \"종결\"된 것으로 간주되며, 기밀 정보는 공유되지 않았다고 강조했습니다. 그녀는 향후 유사한 사건을 방지하기 위한 조치가 취해지고 있다고 언급했지만, 구체적인 내용은 밝히지 않았습니다. 이 채팅을 만든 마이크 월츠 국가안보보좌관은 트럼프 팀의 중요한 구성원으로 남아 있습니다. 현재 펜타곤의 조사와 감시 단체의 소송이 진행 중이며, 채팅의 적절한 기록 유지를 보장하기 위한 노력이 이루어지고 있습니다.",
      "ja": "ホワイトハウスは、アトランティックの編集者ジェフリー・ゴールドバーグが、イエメンでの敏感な軍事作戦について話し合うシグナルのグループチャットに誤って追加された件についての調査を終えたと発表しました。報道官のカロライン・レヴィットは、この問題は「終了」と見なされており、機密情報は共有されなかったと強調しました。今後同様の事態を防ぐための対策が講じられていると述べましたが、具体的な詳細は明らかにしませんでした。チャットを作成した国家安全保障担当アドバイザーのマイク・ウォルツは、トランプ政権の重要なメンバーとして残っています。ペンタゴンによる調査や、チャットの適切な記録保持を確保するための監視団体からの訴訟も進行中です。"
    }
  },
  {
    "id": "c2a616047da9caa0",
    "title": {
      "en": "The average college student today",
      "ko": "오늘의 대학생",
      "ja": "今どきの大学生"
    },
    "type": "story",
    "url": "https://hilariusbookbinder.substack.com/p/the-average-college-student-today",
    "score": 716,
    "by": "Jyaif",
    "time": 1743330011,
    "content": "Share this postScriptorium PhilosophiaThe average college student todayCopy linkFacebookEmailNotesMoreDiscover more from Scriptorium PhilosophiaThoughts on philosophy, reasoning, knowledge, academia, and occasionally bookbinding.Over 3,000 subscribersSubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.Already have an account? Sign inThe average college student todayHow things have changedHilarius BookbinderMar 26, 20253,537Share this postScriptorium PhilosophiaThe average college student todayCopy linkFacebookEmailNotesMore786653ShareI’m Gen X. I was pretty young when I earned my PhD, so I’ve been a professor for a long time—over 30 years. If you’re not in academia, or it’s been awhile since you were in college, you might not know this: the students are not what they used to be. The problem with even talking about this topic at all is the knee-jerk response of, “yeah, just another old man complaining about the kids today, the same way everyone has since Gilgamesh. Shake your fist at the clouds, dude.”1 So yes, I’m ready to hear that. Go right ahead. Because people need to know. First, some context. I teach at a regional public university in the US. Our students are  average on just about any dimension you care to name—aspirations, intellect, socio-economic status, physical fitness. They wear hoodies and yoga pants and like Buffalo wings. They listen to Zach Bryan and Taylor Swift. That’s in no way a put-down: I firmly believe that the average citizen deserves a shot at a good education and even more importantly a shot at a good life. All I mean is that our students are representative; they’re neither the bottom of the academic barrel nor the cream off the top. As with every college we get a range of students, and our best philosophy majors have gone on to earn PhDs or go to law school. We’re also an NCAA Division 2 school and I watched one of our graduates become an All-Pro lineman for the Saints. These are exceptions, and what I say here does not apply to every single student. But what I’m about to describe are the average students at Average State U.Scriptorium Philosophia is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.SubscribeReadingMost of our students are functionally illiterate. This is not a joke. By “functionally illiterate” I mean “unable to read and comprehend adult novels by people like Barbara Kingsolver, Colson Whitehead, and Richard Powers.” I picked those three authors because they are all recent Pulitzer Prize winners, an objective standard of “serious adult novel.” Furthermore, I’ve read them all and can testify that they are brilliant, captivating writers; we’re not talking about Finnegans Wake here. But at the same time they aren’t YA, romantasy, or Harry Potter either. I’m not saying our students just prefer genre books or graphic novels or whatever. No, our average graduate literally could not read a serious adult novel cover-to-cover and understand what they read. They just couldn’t do it. They don’t have the desire to try, the vocabulary to grasp what they read,2 and most certainly not the attention span to finish. For them to sit down and try to read a book like The Overstory might as well be me attempting an Iron Man triathlon: much suffering with zero chance of success. Students are not absolutely illiterate in the sense of being unable to sound out any words whatsoever. Reading bores them, though. They are impatient to get through whatever burden of reading they have to, and move their eyes over the words just to get it done. They’re like me clicking through a mandatory online HR training. Students get exam questions wrong simply because they didn't even take the time to read the question properly. Reading anything more than a menu is a chore and to be avoided.The Buffalo wings look goodThey also lie about it. I wrote the textbook for a course I regularly teach. It’s a fairly popular textbook, so I’m assuming it is not terribly written. I did everything I could to make the writing lively and packed with my most engaging examples. The majority of students don’t read it. Oh, they will come to my office hours (occasionally) because they are bombing the course, and tell me that they have been doing the reading, but it’s obvious they are lying. The most charitable interpretation is that they looked at some of the words, didn’t understand anything, pretended that counted as reading, and returned to looking at TikTok. This study says that 65% of college students reported that they skipped buying or renting a textbook because of cost. I believe they didn’t buy the books, but I’m skeptical that cost is the true reason, as opposed to just the excuse they offer. Yes, I know some texts, especially in the sciences, are expensive. However, the books I assign are low-priced. All texts combined for one of my courses is between $35-$100 and they still don’t buy them. Why buy what you aren’t going to read anyway? Just google it. Even in upper-division courses that students supposedly take out of genuine interest they won’t read. I’m teaching Existentialism this semester. It is entirely primary texts—Dostoevsky, Kierkegaard, Nietzsche, Camus, Sartre. The reading ranges from accessible but challenging to extremely difficult but we’re making a go of it anyway (looking at you, Being and Nothingness). This is a close textual analysis course. My students come to class without the books, which they probably do not own and definitely did not read. WritingTheir writing skills are at the 8th-grade level. Spelling is atrocious, grammar is random, and the correct use of apostrophes is cause for celebration. Worse is the resistance to original thought. What I mean is the reflexive submission of the cheapest cliché as novel insight. Exam question: Describe the attitude of Dostoevsky’s Underground Man towards acting in one’s own self-interest, and how this is connected to his concerns about free will. Are his views self-contradictory?Student: With the UGM its all about our journey in life, not the destination. He beleives we need to take time to enjoy the little things becuase life is short and you never gonna know what happens. Sometimes he contradicts himself cause sometimes you say one thing but then you think something else later. It’s all relative.You probably think that’s satire. Either that, or it looks like this:Exam question: Describe the attitude of Dostoevsky’s Underground Man towards acting in one’s own self-interest, and how this is connected to his concerns about free will. Are his views self-contradictory?Student: Dostoevsky’s Underground Man paradoxically rejects the idea that people always act in their own self-interest, arguing instead that humans often behave irrationally to assert their free will. He criticizes rationalist philosophies like utilitarianism, which he sees as reducing individuals to predictable mechanisms, and insists that people may choose suffering just to prove their autonomy. However, his stance is self-contradictory—while he champions free will, he is paralyzed by inaction and self-loathing, trapped in a cycle of bitterness. Through this, Dostoevsky explores the tension between reason, free will, and self-interest, exposing the complexities of human motivation.That’s right, ChatGPT. The students cheat. I’ve written about cheating in “Why AI is Destroying Academic Integrity,” so I won’t repeat it here, but the cheating tsunami has definitely changed what assignments I give. I can’t assign papers any more because I’ll just get AI back, and there’s nothing I can do to make it stop. Sadly, not writing exacerbates their illiteracy; writing is a muscle and dedicated writing is a workout for the mind as well as the pen.ArithmeticI’m less informed to speak out on this one, but my math prof friends tell me that their students are increasingly less capable and less willing to put in the effort. As a result they have had to make their tests easier with fewer hard problems. When I was a first semester freshman (at a private SLAC, yes, but it wasn’t CalTech) I took Calculus 1. Second semester I took Calculus 2. I don’t think pre-calculus was even a thing back then. Now apparently pre-calc counts as an advanced content course. My psych prof friends who teach statistics have similarly lamented having to water down the content over time.Symbolic Logic was a requirement when I was a grad student. The course was a cross-listed upper-division undergrad/grad class. Jaegwon Kim taught the course, and our sole textbook was W. V. Quine’s Methods of Logic, which we worked through in its entirety. I think we spent two weeks on propositional logic before moving on to the predicate calculus. We proved compactness, soundness, and completeness, and probably some other theorems I forget. There is no possible way our students, unless they were math or computer science majors, would survive that class.What’s changed?The average student has seen college as basically transactional for as long as I’ve been doing this. They go through the motions and maybe learn something along the way, but it is all in service to the only conception of the good life they can imagine: a job with middle-class wages. I’ve mostly made my peace with that, do my best to give them a taste of the life of the mind, and celebrate the successes.Things have changed. Ted Gioia describes modern students as checked-out, phone-addicted zombies. Troy Jollimore writes, “I once believed my students and I were in this together, engaged in a shared intellectual pursuit. That faith has been obliterated over the past few semesters.” Faculty have seen a stunning level of disconnection. SubscribeWhat has changed exactly? Chronic absenteeism. As a friend in Sociology put it, “Attendance is a HUGE problem—many just treat class as optional.” Last semester across all sections, my average student missed two weeks of class. Actually it was more than that, since I’m not counting excused absences or students who eventually withdrew. A friend in Mathematics told me, “Students are less respectful of the university experience —attendance, lateness, e-mails to me about nonsense, less sense of responsibility.” Disappearing students. Students routinely just vanish at some point during the semester. They don’t officially drop or withdraw from the course, they simply quit coming. No email, no notification to anyone in authority about some problem. They just pull an Amelia Earhart. It’s gotten to the point that on the first day of class, especially in lower-division, I tell the students, “look to your right. Now look to your left. One of you will be gone by the end of the semester. Don’t let it be you.”They can’t sit in a seat for 50 minutes. Students routinely get up during a 50 minute class, sometimes just 15 minutes in, and leave the classroom. I’m supposed to believe that they suddenly, urgently need the toilet, but the reality is that they are going to look at their phones. They know I’ll call them out on it in class, so instead they walk out. I’ve even told them to plan ahead and pee before class, like you tell a small child before a road trip, but it has no effect. They can’t make it an hour without getting their phone fix.They want me to do their work for them. During the Covid lockdown, faculty bent over backwards in every way we knew how to accommodate students during an unprecedented (in our lifetimes) health crisis. Now students expect that as a matter of routine. I am frequently asked for my PowerPoint slides, which basically function for me as lecture notes. It is unimaginable to me that I would have ever asked one of my professors for their own lecture notes. No, you can’t have my slides. Get the notes from a classmate. Read the book. Come to office hours for a conversation if you are still confused after the preceding steps. Last week I had an email from a student who essentially asked me to recap an entire week’s worth of lecture material for him prior to yesterday’s midterm. No, I’m not doing that. I’m not writing you a 3000-word email. Try coming to class.Pretending to type notes in their laptops. I hate laptops in class, but if I try to ban them the students will just run to Accommodative Services and get them to tell me that the student must use a laptop or they will explode into tiny pieces. But I know for a fact that note-taking is at best a small part of what they are doing. Last semester I had a good student tell me, “hey you know that kid who sits in front of me with the laptop? Yeah, I thought you should know that all he does in class is gamble on his computer.” Gambling, looking at the socials, whatever, they are not listening to me or participating in discussion. They are staring at a screen.Indifference. Like everyone else, I allow students to make up missed work if they have an excused absence. No, you can’t make up the midterm because you were  hungover and slept through your alarm, but you can if you had Covid. Then they just don’t show up. A missed quiz from a month ago might as well have happened in the Stone Age; students can’t be bothered to make it up or even talk to me about it because they just don’t care.It’s the phones, stupid. They are absolutely addicted to their phones. When I go work out at the Campus Rec Center, easily half of the students there are just sitting on the machines scrolling on their phones. I was talking with a retired faculty member at the Rec this morning who works out all the time. He said he has done six sets waiting for a student to put down their phone and get off the machine he wanted. The students can’t get off their phones for an hour to do a voluntary activity they chose for fun. Sometimes I’m amazed they ever leave their goon caves at all.I don’t blame K-12 teachers. This is not an educational system problem, this is a societal problem. What am I supposed to do? Keep standards high and fail them all? That’s not an option for untenured faculty who would like to keep their jobs. I’m a tenured full professor. I could probably get away with that for a while, but sooner or later the Dean’s going to bring me in for a sit-down. Plus, if we flunk out half the student body and drive the university into bankruptcy, all we’re doing is depriving the good students of an education. We’re told to meet the students where they are, flip the classroom, use multimedia, just be more entertaining, get better. As if rearranging the deck chairs just the right way will stop the Titanic from going down. As if it is somehow the fault of the faculty. It’s not our fault. We’re doing the best we can with what we’ve been given.All this might sound like an angry rant. I’m not sure. I’m not angry, though, not at all. I’m just sad. One thing all faculty have to learn is that the students are not us. We can’t expect them all to burn with the sacred fire we have for our disciplines, to see philosophy, psychology, math, physics, sociology or economics as the divine light of reason in a world of shadow. Our job is to kindle that flame, and we’re trying to get that spark to catch, but it is getting harder and harder and we don’t know what to do.Thanks for reading Scriptorium Philosophia! This post is public so feel free to share it.Share1Careful about bogus “ancient” quotations on this topic, though. 2Students often ask me the meaning of common words on exams, words like “caricature.”Subscribe to Scriptorium PhilosophiaBy Hilarius Bookbinder · Launched 5 months agoThoughts on philosophy, reasoning, knowledge, academia, and occasionally bookbinding.SubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.3,537Share this postScriptorium PhilosophiaThe average college student todayCopy linkFacebookEmailNotesMore786653Share",
    "summary": {
      "en": "The author, a long-time professor, discusses the significant changes in college students over the years. He teaches at a regional public university and reflects on the average student’s abilities and attitudes today. Key points include:\n\n1. **Functionally Illiterate**: Many students struggle to read and understand serious literature, often resorting to superficial reading just to get through assignments.\n\n2. **Poor Writing Skills**: The writing of students often resembles that of an 8th grader, with frequent spelling and grammar mistakes, and a lack of original thought.\n\n3. **Disengagement**: Students show a concerning level of disconnection from their studies. They frequently miss classes, leave early, and often expect professors to do the work for them, such as providing detailed summaries of lectures.\n\n4. **Phone Addiction**: Many students are addicted to their phones, which distracts them during classes and affects their engagement with learning.\n\n5. **Cheating and Academic Integrity**: The rise of AI and other means of cheating has made it difficult for professors to assess students’ true understanding and abilities.\n\n6. **Societal Problem**: The author emphasizes that these issues are not solely an educational failure but reflect broader societal changes in attention, responsibility, and engagement.\n\nOverall, the author expresses sadness about the current state of student engagement and learning, feeling a disconnect between faculty expectations and student realities.",
      "ko": "저자는 오랜 교수 경력을 바탕으로 대학생들의 변화에 대해 이야기합니다. 그는 지역 공립대학교에서 강의를 하며 현재 학생들의 능력과 태도에 대해 반성합니다. 주요 내용은 다음과 같습니다.\n\n많은 학생들이 심도 있는 문학 작품을 읽고 이해하는 데 어려움을 겪고 있으며, 과제를 수행하기 위해 피상적인 독서에 의존하는 경우가 많습니다. 학생들의 글쓰기 수준은 8학년 수준에 불과하며, 철자와 문법 오류가 잦고 독창적인 생각이 부족합니다. \n\n학생들은 학업에 대한 관심이 떨어져 있으며, 수업에 자주 결석하거나 일찍 떠나는 경향이 있습니다. 그들은 교수들이 강의 내용을 자세히 요약해 주기를 기대하는 경우가 많습니다. 또한, 많은 학생들이 스마트폰에 중독되어 있어 수업 중 집중력을 잃고 학습에 대한 참여도가 낮아지고 있습니다.\n\nAI와 같은 새로운 부정행위 수단의 등장은 교수들이 학생들의 진정한 이해도와 능력을 평가하는 데 어려움을 겪게 하고 있습니다. 저자는 이러한 문제들이 단순한 교육적 실패가 아니라, 주의력, 책임감, 참여도와 같은 더 넓은 사회적 변화의 반영이라고 강조합니다.\n\n전반적으로 저자는 현재 학생들의 참여와 학습 상태에 대해 안타까움을 느끼며, 교수들의 기대와 학생들의 현실 사이의 괴리를 느끼고 있습니다.",
      "ja": "著者は長年の教授として、大学生の変化について考察しています。彼は地方の公立大学で教えており、現在の学生の能力や態度について振り返っています。主なポイントは以下の通りです。\n\n多くの学生が、真剣な文学を読むことや理解することに苦労しており、課題をこなすために表面的な読み方に頼っています。学生の文章力はしばしば中学2年生程度で、スペルや文法の間違いが多く、独自の考えが欠けています。\n\n学生は学業に対して無関心な傾向が見られ、授業を欠席したり早退したりすることが多く、教授に講義の詳細な要約を求めることが一般的になっています。多くの学生はスマートフォンに依存しており、授業中に気が散り、学びへの関与に影響を与えています。\n\nAIなどの不正行為が増加しているため、教授は学生の真の理解度や能力を評価することが難しくなっています。著者は、これらの問題は単なる教育の失敗ではなく、注意力や責任感、関与のあり方など、社会全体の変化を反映していると強調しています。\n\n全体として、著者は学生の関与や学びの現状に悲しみを感じており、教員の期待と学生の現実との間にギャップがあることを実感しています。"
    }
  },
  {
    "id": "c97a6adf06e8d684",
    "title": {
      "en": "'Please leave feedback': how constant online reviews are changing our brains",
      "ko": "리뷰가 뇌를 바꾼다",
      "ja": "「レビューが脳を変える」"
    },
    "type": "story",
    "url": "https://www.theguardian.com/business/2025/apr/01/please-leave-feedback-how-constant-online-reviews-are-changing-our-brains-and-our-lives",
    "score": 10,
    "by": "sandebert",
    "time": 1743503080,
    "content": "‘If I don’t rate them, will they rate me badly?’ Composite: Guardian Design; posed by model; Maskot/Getty ImagesView image in fullscreen‘If I don’t rate them, will they rate me badly?’ Composite: Guardian Design; posed by model; Maskot/Getty ImagesRetail industry‘Please leave feedback’: how constant online reviews are changing our brains – and our livesWe live under mutual surveillance, asked to leave public ratings for every purchase, meal, taxi ride or hair appointment. What is it doing to us?Chloë HamiltonTue 1 Apr 2025 10.00 BSTLast modified on Tue 1 Apr 2025 10.37 BSTShare‘Alexlilly1999* has left you feedback!” pings the Vinted notification. My stomach flips as the app loads and I open my review: “Quite good.” A gut punch. I sit in shock, scrutinising the words in front of me. “Good” is a bit uninspired but “quite” feels both passive-aggressive and viciously spiteful: quite good. Alexlilly1999 has also given me just four stars. It’s a lukewarm write-up considering the dress I sold them was good quality, a brilliant price and shipped quickly. I glare at the review. And then another notification pops up: do I want to leave feedback for the buyer? Well, yes, actually, I do.It’s likely we’ve all, at some stage, been asked to leave feedback online. Called your electricity provider with a query? Please answer a few quick questions about the service you received. Had something delivered by a courier? Please rate your experience. Often, the promise of prizes – from £200 worth of high street vouchers, to spa trips and luxury hampers – is dangled in exchange for our appraisals. We’re asked to judge the people who serve us coffee; drive us in taxis; cut our hair; extract our teeth. A friend of a friend was recently asked to leave feedback for an interview process just moments after the company had rejected them for the role.The odd thing is, lots of us do leave feedback. Perhaps it has become normalised to mark, out of five, the toilet plunger we bought at 2am – or perhaps we’ve been influenced by “influencer culture”, in which everyone can (and does) have an opinion. Lots of us also, of course, like getting feedback, with positive ratings on platforms like Uber, Airbnb and Vinted becoming badges of honour.View image in fullscreenDoes your new purchase cut the mustard? Photograph: Posed by model; filadendron/Getty ImagesAccording to a study by Power Reviews, 98% of consumers see reviews as an essential part of the decision-making process and 45% of users won’t buy a product if there aren’t any reviews. Ratings, it seems, have become so ubiquitous that an absence of them is an automatic red flag.But while, on the face of it, authentic, real-world information about goods and services may seem helpful – necessary, even – one can’t help but wonder what this culture of mutual surveillance is doing to our brains – and our lives. If every decision is judged, often publicly, how long before it starts to change the way we interact?For Helen Plyconic, it already has. The 45-year-old has a 4.12 (out of 5) rating on Uber – a ranking so low it can, reportedly, mean some Uber drivers refuse to pick you up (although the company itself says destination and fare are more likely to drive a driver’s decision to turn down a booking). The Walthamstow resident, who says she has never been rude or drunk in an Uber, believes her rating is low because she once missed a taxi when her phone ran out of battery after booking, and she jumped in a friend’s cab instead. While she concedes this was her fault, she has, since then, received consistently low ratings for reasons, she thinks, such as needing help with her bags (she had three children with her) and a driver having to change a pickup point due to closed roads. As a result, she has begun to alter her behaviour, tipping extra-generously and even explaining her predicament to drivers, all to no avail.The Vinted phenomenon: how one woman sold her clothes – and created a billion-dollar companyRead moreA self-confessed people pleaser, Plyconic now feels anxious each time she takes an Uber. “Do I chat to show I’m friendly or be silent so as not to upset them?” she says. She describes, too, how drivers even tell her how lucky she is they picked her up, given her poor rating. “I then question why they did, and wonder if they are dodgy themselves.” (Riders, incidentally, can check their drivers’ ratings, meaning Plyconic could, if she wanted, refuse a driver if she deemed their rating too low. She would, however, incur a £5 cancellation fee.)Anxiety specialist Vic Paterson – who, when we speak, is considering how best to review her dog walker – says she has noticed her clients struggling with this constant feedback culture. “While specific research on ubiquitous rating systems is still developing, therapists and psychologists – including me – are noticing recurring patterns,” she says. “Many clients [experience] increased self-consciousness during everyday interactions that may be rated.” Paterson suggests that, although the rise of online reviews and ratings “gives opportunities to those who might otherwise not speak up”, the endless and sometimes seemingly needless requests for feedback are changing how people interact, on both sides of the transaction.“Now, I’m greeted cheerily by name when I collect a coffee – not always what I want – because the server is concerned about how I might rate them, which forces me into an interaction,” she says. “The delivery driver could be having a horrible day and just want to get stuff done, but if I want to have a quick chat, they’re forced to spend some time in case I rate them poorly. I have no way to tell whether their cheery chat is authentic or based around their need for a five-star review,” she says. “When everyday encounters become potential evaluations, it may change how freely we express ourselves.” From a psychological perspective, Paterson says, there is reason for “thoughtful concern about constantly outsourcing evaluation of our performance to others” – especially if those “others” may have their own agenda.Members of the public threaten to leave over-embellished reviews, fake reviews, if they don’t get their way Last year, 42-year-old Sam Morgan, who owns and runs eight restaurants across the north-west, the Midlands and north Wales, was blackmailed by fraudsters threatening to post fake one-star reviews across the restaurants’ online platforms unless the company sent them £2,000. When Morgan and his business partner, chef Andy Sheridan, didn’t comply, the extortionists began to post negative reviews on Google and Tripadvisor, claiming the wine list was “overpriced” and even that the restaurant itself was a “scam”.Although Morgan, who used to work in law enforcement before moving into the hospitality industry, describes the experience as “a challenging time” that took its toll, he says their restaurants are still, in a way, being held to ransom – but by customers. “The fraudster, that was at the most extreme end,” he says. “But this is an everyday occurrence in respect of members of the public leaving what we would call over-embellished reviews, fake reviews, threatening to leave these reviews if they don’t get their way.” This, he says, is the bigger problem.Morgan has short shrift, too, for the review platforms themselves, saying it was only once his team had gone to the press that they acted. “They [Google and Tripadvisor] weren’t interested in dealing with it,” he says. “They don’t do any checks.” Neither Tripadvisor nor Google responded to a request for comment but, previously, a spokesperson for Google told the BBC: “Our policies clearly state that reviews must be based on real experiences and information. We’ve looked into this case and are removing the policy-violating content.”View image in fullscreenEven as you rate your lunch, the cafe owner may be rating you. Photograph: Posed by models; Tom Werner/Getty ImagesPerhaps Morgan should take a leaf out of restaurateur Chris D’Sylva’s book. The owner of Dorian in Notting Hill – supposedly David Beckham’s favourite restaurant – reviews diners using a behind-the-scenes system to record their behaviour and rank them accordingly. But, although Morgan reveals they have banned guests before, he says he wouldn’t introduce a similar system. “Ranking or scoring is a little extreme and, to be honest, would be hypocritical of me when I don’t like the purpose of review sites,” he says. “I do, however, wonder how guests would feel if we created a ‘GuestAdvisor’ and [reviewed] their behaviour online.”Quite apart from the looming sense that we’re under constant observation, the very act of leaving feedback also takes time – something many of us feel we don’t have. “I’ve just stopped giving feedback now,” says 38-year-old Daisy Ferns, a business owner from Derby. In the last few days, she tells me, she has been asked to leave a review for a hotel stay, a vitamin purchase, something she bought from Boots, an Uber trip, a courier delivery and a returns process. “I don’t mind a tick box thing but it’s a lot of brain power to think of comments.” She worries, though, that if she doesn’t engage, it will reflect badly on her. “If I don’t rate them, will they not rate me or rate me badly?”‘The customer isn’t always right’: top chef loses appetite for difficult dinersRead moreAs fed up with feedback as we may be, it is pertinent to remember that some people – small businesses, especially – live and die by their reviews. And reviews can still have their uses. I devour online reviews of clothing, for example, before making a purchase, picking apart customers’ thoughts on style, fabric, and – especially useful given how different sizing is across brands – fit. Paterson agrees this is one of the benefits: public feedback is a “form of social proof” that reduces anxiety around unknown experiences, whether you’re worried about the cleanliness of an Airbnb or the sizing of a River Island skirt.However, it’s worth questioning the authenticity of some reviews. The rise of fraudulent feedback has become so prolific that new laws under the Digital Markets and Competition and Consumer Act, coming in on 6 April, include the banning of submitting and commissioning fake reviews. Helen Dewdney, the Complaining Cow consumer expert, says AI reviews, in particular, are often easy to spot: “They are bland, lacking in emotion or feeling and appear to have no strong opinion about the subject of the review.”Constructive feedback can also drive improvements. Certainly, the buyer on Vinted who flagged, politely, that my packaging hadn’t been sufficiently sturdy made me rethink how I was sending out my items. And there will always be people, as well, who go the extra mile in search of praise, such as the seller who sent extra treats – some wax melts – for me in my parcel (along with a note reminding me to confirm delivery and leave a review). This particular seller had five-star reviews across the board, glowing write-ups and – I noted with interest – repeat custom, so her approach is clearly working.The more we are canvassed for online feedback, the less value it may come to hold. Paterson believes that “unsolicited feedback is generally seen as more genuine because it wasn’t prompted”, adding that readers tend to give this feedback more weight, too.As I finish my drink in a cafe, I notice a card on the table in front of me: a QR code requesting feedback on my experience. Instead, I approach the till and, taking a deep breath, thank the barista for my coffee, for the pleasant space and for their hard work. He is taken aback. “You’re welcome,” he beams. “We put a lot of effort in, so it’s nice when it’s noticed and people say.”* This name has been changedExplore more on these topicsRetail industryOnline shoppingAirbnbfeaturesShareReuse this content",
    "summary": {
      "en": "The text discusses how the culture of online reviews and ratings is affecting our lives and mental health. People are constantly asked to rate services and products after every interaction, which can create anxiety and change how we behave in social situations. Many individuals feel pressured to leave feedback, fearing that if they don't, they will receive negative ratings in return. \n\nResearch shows that most consumers rely on reviews when making purchases, and the lack of reviews can be a red flag. However, this constant demand for ratings can lead to increased self-consciousness and awkward interactions. \n\nSome businesses face challenges with fake reviews and extortion attempts, highlighting the darker side of the review culture. While feedback can be useful for improvement and building trust, the overwhelming number of requests can feel burdensome. \n\nOverall, the article emphasizes the need to rethink our relationship with online feedback and consider its impact on our daily interactions and mental well-being.",
      "ko": "온라인 리뷰와 평점 문화가 우리의 삶과 정신 건강에 미치는 영향에 대해 논의하고 있다. 사람들은 매번 서비스나 제품을 이용한 후 평가를 요구받으며, 이는 불안을 유발하고 사회적 상황에서의 행동 방식에 변화를 가져올 수 있다. 많은 사람들은 피드백을 남기지 않으면 부정적인 평가를 받을까 두려워서 압박감을 느낀다.\n\n연구에 따르면, 대부분의 소비자들은 구매를 할 때 리뷰에 의존하며, 리뷰가 없는 경우 신뢰할 수 없는 신호로 간주된다. 그러나 이러한 지속적인 평점 요구는 자아 의식의 증가와 어색한 상호작용을 초래할 수 있다.\n\n일부 기업은 가짜 리뷰와 갈취 시도와 같은 문제에 직면해 있으며, 이는 리뷰 문화의 어두운 면을 드러낸다. 피드백은 개선과 신뢰 구축에 유용할 수 있지만, 지나치게 많은 요청은 부담으로 느껴질 수 있다.\n\n전반적으로 이 글은 온라인 피드백과의 관계를 재고하고, 그것이 우리의 일상적인 상호작용과 정신적 웰빙에 미치는 영향을 고려할 필요성을 강조하고 있다.",
      "ja": "オンラインレビューや評価の文化が私たちの生活やメンタルヘルスに与える影響について述べています。人々はサービスや商品を利用するたびに評価を求められ、これが不安を生み出し、社会的な場面での行動にも影響を与えています。多くの人がフィードバックを残すことにプレッシャーを感じており、もし評価をしなければ、逆に低い評価を受けるのではないかと心配しています。\n\n研究によると、ほとんどの消費者は購入時にレビューを参考にしており、レビューがないことは警戒すべきサインとされています。しかし、この評価を求める圧力は自己意識を高め、ぎこちないやり取りを引き起こすことがあります。\n\nまた、一部の企業は偽のレビューや恐喝の試みに直面しており、レビュー文化の暗い側面が浮き彫りになっています。フィードバックは改善や信頼構築に役立つ一方で、評価を求める要求が多すぎると負担に感じることもあります。\n\n全体として、この記事はオンラインフィードバックとの関係を見直し、それが日常のやり取りやメンタルヘルスに与える影響を考える必要性を強調しています。"
    }
  },
  {
    "id": "41d3838128d8a7ec",
    "title": {
      "en": "Public secrets exposure leads to supply chain attack on GitHub CodeQL",
      "ko": "공공 비밀 폭로, GitHub 공급망 공격",
      "ja": "公然の秘密がGitHub CodeQLを襲う"
    },
    "type": "story",
    "url": "https://www.praetorian.com/blog/codeqleaked-public-secrets-exposure-leads-to-supply-chain-attack-on-github-codeql/",
    "score": 294,
    "by": "cyberbender",
    "time": 1743364486,
    "content": "A potential supply chain attack on GitHub CodeQL started simply: a publicly exposed secret, valid for 1.022 seconds at a time.\n\nIn that second, an attacker could take a series of steps that would allow them to execute code within a GitHub Actions workflow in most repositories using CodeQL, GitHub’s code analysis engine trusted by hundreds of thousands of repositories. The impact would reach both public GitHub (GitHub Cloud) and GitHub Enterprise.\n\nIf backdooring GitHub Actions sounds familiar, that’s because it’s exactly what threat actors did in the recent tj-actions/changed-files supply chain attack. Imagine that very same supply chain attack, but instead of backdooring actions in tj-actions, they backdoored actions in GitHub CodeQL.\n\nAn attacker could use this to:\n\nCompromise intellectual property by exfiltrating the source code of private repositories using CodeQL.\n\nSteal credentials within GitHub Actions secrets of workflow jobs using CodeQL and leverage those secrets to execute further supply chain attacks.\n\nExecute code on internal infrastructure running CodeQL workflows.\n\nCompromise GitHub Actions secrets of workflows using the GitHub Actions Cache within a repo that uses CodeQL.\n\nThis is the story of how we uncovered an exposed secret leading to a race condition, a potential supply chain attack, and CVE-2025-24362.\n\nNote: Per GitHub’s advisory, they have found no evidence of compromise to its platform or systems.\n\nHow Did We Get Here?\n\nIn January 2025, I took a break from Praetorian’s Red Team and began three months of research. I aimed to push the limits of public GitHub Actions exploitation, building on presentations we’ve given at Black Hat, DEF CON, Schmoocon, and Black Hat Arsenal. Tools and takeaways from this research will be implemented in our CI/CD Professional Services Engagements, and into Chariot, our Continuous Threat Exposure Management platform.\n\nI began my research rotation by scanning GitHub Actions workflow artifacts for secrets.\n\nSecret Scanning\n\nIn August 2024, Palo Alto researcher Yaron Avital published an article about identifying secrets in workflow artifacts. I had a hunch that there were still secrets to be found, especially since there hadn’t been much public follow-up work since the article.\n\nI built a simple Actions Artifacts Secret Scanner to get started. It downloads artifacts from GitHub Actions workflows, recursively extracts their contents, and scans their contents for secrets with Nosey Parker, Praetorian’s open-sourced secrets scanning tool.\n\nThe Actions Artifacts Secret Scanner has been integrated into Chariot and open-sourced as a Gato capability.\n\nAfter running this scanner for one day, it found a secret that could lead to a supply chain attack on GitHub CodeQL.\n\nBut first, I needed to see if the key was usable.\n\nBackground\n\nCI/CD vulnerabilities sound complicated until you understand the terminology. Let’s catch you up.\n\nGitHub Actions is a continuous integration and continuous delivery (CI/CD) platform that allows the execution of code specified within workflows as part of the CI/CD process. When you push code to a GitHub repository or create a pull request, GitHub Actions can automatically build, test, and deploy your code using workflows defined in YAML files.\n\nFor example, let’s say you are building a web application that is hosted in AWS. You can configure a GitHub Actions workflow so that whenever you push code to your repository, it is automatically tested and then deployed to AWS.\n\nIf you are new to GitHub Actions, we’d recommend reading through some examples.\n\nEvery workflow run generates a GITHUB_TOKEN — a special, automatically generated GitHub App installation token that allows the workflow to interact with the repository. This token’s permissions can be configured in the workflow file, at the repository level, or at the org level, determining what actions it can perform within the repository.\n\nPut simply:\n\nGitHub workflows execute on GitHub runners (typically a VM or Docker containers).\n\nGitHub runners need a way to authenticate to GitHub to do stuff the workflows tell them to do.\n\nFor that purpose, they use the GITHUB_TOKEN.\n\nIf the token has high privileges, then token compromise == bad.\n\nWhat are Workflow Artifacts?\n\nWe found the publicly exposed secret in a GitHub Actions workflow artifact.\n\nGitHub Actions workflows can upload workflow “artifacts” to GitHub Actions. Workflow artifacts can be any file and are saved by that workflow for later use. By default, artifacts are publicly accessible to anyone with read access to the repository and are stored for up to 90 days.\n\nAnd Finally, What is CodeQL?\n\nCodeQL is GitHub’s Code Analysis Engine. The CodeQL actions perform static code analysis on GitHub repositories to try and identify vulnerabilities. They have found several hundred CVEs over it’s lifetime, protecting organizations from breaches.\n\nSecurity tools, like CodeQL, often need access to sensitive systems and data, making them an attractive target to an attacker.\n\nIf CodeQL was compromised, one of the most widely used security tools now becomes a backdoor.\n\nFinding the Token\n\nAfter running the Actions Artifact Secrets Scanner for a day, it picked up a token in a github/codeql-action repository artifact published by this run. The Actions Artifact Secrets Scanner downloaded the “my-debug-artifacts” zip uploaded by the “PR Check – Debug artifacts after failure” workflow, recursively extracted the “my-db-java-partial.zip” file stored inside, and ran Nosey Parker. Within seconds, Nosey Parker flagged a GitHub Token starting with “ghs_” in a crash report.\n\n<img fetchpriority=\"high\" decoding=\"async\" width=\"1024\" height=\"443\" src=\"https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-04-17.37.56-1024x443.png\" alt=\"Finding the Token\" srcset=\"https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-04-17.37.56-1024x443.png 1024w, https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-04-17.37.56-300x130.png 300w, https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-04-17.37.56-768x332.png 768w, https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-04-17.37.56.png 1072w\" sizes=\"(max-width: 1024px) 100vw, 1024px\"/>\n\nInvestigating manually, I confirmed this was a GitHub App token installation token stored in a file containing the environment variables of the GitHub Runner executing the workflow.\n\nInvestigating Impact\n\nSecrets compromise is cool, but what can we do with this token? The impact of a compromisedGITHUB_TOKEN is minimal if it only has read permissions.\n\nThe easiest way to determine the privileges of a GITHUB_TOKEN is to look at workflow logs. To investigate this, I navigated to the “Setup Job” step of the workflow that uploaded the token.\n\n<img decoding=\"async\" width=\"1024\" height=\"857\" src=\"https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-04-17.42.13-1024x857.png\" alt=\"Investigating Impact\" srcset=\"https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-04-17.42.13-1024x857.png 1024w, https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-04-17.42.13-300x251.png 300w, https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-04-17.42.13-768x643.png 768w, https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-04-17.42.13.png 1364w\" sizes=\"(max-width: 1024px) 100vw, 1024px\"/>\n\nThe GitHub token had full write privileges.\n\nWe could spend a lot of time talking about each privilege, but let’s focus on the ones that are particularly interesting.\n\nContents: write – Allows the token to create branches, create tags, and upload release artifacts.Actions: write – Allows you to work with Actions, including trigger workflow_dispatch events.Packages: write – Allows the token to upload packages.\n\nWith these privileges, an attacker has a lot of potential for repository tampering, but there is still one issue. These tokens are only valid for the duration of their specific workflow job. That means that once the job is over, the token is useless. Three things needed to happen for an attacker to be able to abuse this token:\n\nThe token needs to have some sort of write privileges (already confirmed).\n\nThe token needs to use V4 of the upload artifact API, as that is the only version that allows you to retrieve an artifact before the job is complete (and after the job is complete, the token is invalid.)\n\nThe time between uploading the artifact and completing the job needs to be great enough for us to download, extract, and use the token.\n\nIf all of these conditions are met, this publicly exposed token could be used to launch a full scale supply chain attack on CodeQL. This was like finding out that the security guard was accidentally leaving their master key in plain sight for a brief moment, over and over again.\n\nWe had to determine if the guard left us enough time to steal the key and use it before they returned to their post.\n\nLet’s investigate further. Tick, tock.\n\nDetermining the Artifact Upload Version\n\nIdentifying the artifact upload version is typically straightforward. If a workflow usesactions/upload-artifact@v4***, we can retrieve the artifact before job completion. If it uses an earlier version, we cannot do so.\n\nIn this case, CodeQL wasn’t using the actions/upload-artifact action; they were manually using the upload artifact client in the source code. Code comments indicated it used version 4. That was enough for me to continue.\n\nNow we needed to determine if the job lasted long enough for us to retrieve and use the token.\n\nCalculating our Execution Time\n\nLooking at the raw GitHub logs for this workflow, we can see two key timestamps:\n\n<img decoding=\"async\" width=\"1024\" height=\"221\" src=\"https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-04-17.43.08-1024x221.png\" alt=\"Calculating our Execution Time\" srcset=\"https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-04-17.43.08-1024x221.png 1024w, https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-04-17.43.08-300x65.png 300w, https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-04-17.43.08-768x166.png 768w, https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-04-17.43.08.png 1350w\" sizes=\"(max-width: 1024px) 100vw, 1024px\"/>\n\n“Finalizing artifact upload” occurred at 17:22:09:888.\n\nThe final step in the job, “Cleaning up orphan processes”, happened at 17:22:10:911.\n\nThat means we had approximately 1.022 seconds to download the artifact, extract the GitHub token, and use it. I noticed the token stayed valid for about a second after the “Cleaning up orphan processes” step, so we’ll call it two seconds.\n\nThe guard was giving us two seconds to steal the key and use it before they returned.Is that enough time for an attacker to use this token? Or is this another theoretical vulnerability?\n\nStart Your Engines\n\nTo test this, I made a Python script artifact_racer.py. Artifact racer performs the following actions.\n\nContinuously queries the github/codeql-action GitHub repository until it sees a “PR Check – Debug artifacts after failure” workflow begin.\n\nMonitors the running workflow for artifacts.\n\nOnce it sees a “PR Check – Debug artifacts after failure” workflow run, it downloads the artifact and extracts the GITHUB_TOKEN.\n\nShelling out for file operations and downloads was key to increasing the speed, although there are probably ways to make it even faster.\n\nUses the GITHUB_TOKEN to make a new branch.\n\nUse the GITHUB_TOKEN to push an empty file named poc.txt to that branch.\n\nMakes a new tag for that commit.\n\nIf I could make a new branch, add a file, and create a tag for that commit, that would prove an attacker could use the token for nefarious purposes before it expired.\n\nGiven that the workflow artifact was only ~21MBs, I thought we had a chance. After successfully executing against a test repository, I moved on to the github/codeql-action repository.\n\n<img loading=\"lazy\" loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"429\" src=\"https://www.praetorian.com/wp-content/uploads/2025/03/image-11-1024x429.png\" alt=\"\" class=\"wp-image-3815\" srcset=\"https://www.praetorian.com/wp-content/uploads/2025/03/image-11-1024x429.png 1024w, https://www.praetorian.com/wp-content/uploads/2025/03/image-11-300x126.png 300w, https://www.praetorian.com/wp-content/uploads/2025/03/image-11-768x322.png 768w, https://www.praetorian.com/wp-content/uploads/2025/03/image-11-1536x644.png 1536w, https://www.praetorian.com/wp-content/uploads/2025/03/image-11.png 1728w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" />\n\nExecuting the Proof of Concept\n\nI ran the racer.\n\nAnd then I waited.\n\nAbout two hours later, a “PR Check – Debug artifacts after failure” workflow executed. The racer successfully retrieved the GITHUB_TOKEN, created the branch, pushed the file, and added the tag.\n\n<img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"586\" src=\"https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-04-17.45.00-1024x586.png\" alt=\"Executing the Proof of Concept\" srcset=\"https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-04-17.45.00-1024x586.png 1024w, https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-04-17.45.00-300x172.png 300w, https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-04-17.45.00-768x440.png 768w, https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-04-17.45.00.png 1300w\" sizes=\"(max-width: 1024px) 100vw, 1024px\"/>\n\nBranch URL: https://github.com/github/codeql-action/tree/testpocCommit URL: https://github.com/github/codeql-action/commit/26fcd8e2368067be04a705a229590749a426fefeTag URL: https://github.com/github/codeql-action/releases/tag/testpoctag\n\n<img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"516\" src=\"https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-04-17.45.20-1024x516.png\" alt=\"New tag created\" srcset=\"https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-04-17.45.20-1024x516.png 1024w, https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-04-17.45.20-300x151.png 300w, https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-04-17.45.20-768x387.png 768w, https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-04-17.45.20.png 1414w\" sizes=\"(max-width: 1024px) 100vw, 1024px\"/>\n\nThe ability to create a tag becomes very important in this attack. Keep that in mind as we go.\n\nAfter confirming the GITHUB_TOKEN could be used within the short time window, we responsibly disclosed this vulnerability to GitHub.\n\nWhat if You’re Still Not Impressed?\n\nUsing the GITHUB_TOKEN, an attacker could add malicious code to any unprotected branch. A covert tactic would be to target feature branches pre-merge, smuggle in a small malicious code change, and wait for it to get merged. This would be especially effective due to how frequently the GitHub Actions bot commits to the CodeQL Actions repository.\n\nThey could also add tags that point to specific commits. For example, if they had malicious code on a branch and then added a v3 tag, anyone who manually used codeql-action…@v3 would execute the malicious code. More on this later.\n\nThrough code execution, you’d be able to compromise any GitHub Actions secret used within that job, as well as exfiltrate the source code of that repository. If their actions were executing on internal infrastructure, which is common with self-hosted GitHub runners, you’d also have code execution on their internal network or cloud environment.\n\nThe impact from this attack would have been very similar to the recent tj-actions/changed-files supply chain attack.\n\nThis impact is impressive, but it doesn’t quite live up to the claims I made in the beginning. Yes, through these paths, they could launch a supply chain attack against repos manually using the CodeQL actions. However, most organizations don’t include these actions manually. They just go into their repository settings, click “Enable CodeQL”, and go from there.\n\nAt first, I assumed that enabling CodeQL in your repository didn’t interact with the github/codeql-action repository at all.\n\nI was wrong.\n\nExponential Impact\n\nAfter discussing this issue with some colleagues, I decided to investigate further. What actually happens when you enable CodeQL?\n\nThis section is key to understanding the full impact of this vulnerability. Stick with me.\n\nTo investigate, I created my own public repository, “John’s Top Secret Repo”, and enabled CodeQL.\n\nAfter you enable CodeQL with the default settings, a special GitHub Actions workflow runs in your repository. This CodeQL action won’t show up in your repository workflows, but you can navigate to the workflow logs to see what it is doing.\n\n<img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"422\" src=\"https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-04-17.46.57-1024x422.png\" alt=\"\" srcset=\"https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-04-17.46.57-1024x422.png 1024w, https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-04-17.46.57-300x124.png 300w, https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-04-17.46.57-768x317.png 768w, https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-04-17.46.57.png 1174w\" sizes=\"(max-width: 1024px) 100vw, 1024px\"/>\n\nEnabling CodeQL in your repository settings.\n\n<img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"522\" src=\"https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-04-17.47.12-1024x522.png\" alt=\"CodeQL Setup\" srcset=\"https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-04-17.47.12-1024x522.png 1024w, https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-04-17.47.12-300x153.png 300w, https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-04-17.47.12-768x391.png 768w, https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-04-17.47.12.png 1378w\" sizes=\"(max-width: 1024px) 100vw, 1024px\"/>\n\nObserving the CodeQL workflow.\n\nBased on my observations, CodeQL:\n\nChecks out your repository to the filesystem\n\nInitializes CodeQL\n\nRuns CodeQL scans\n\nUploads the scan results\n\n<img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"979\" src=\"https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-04-17.47.31-1024x979.png\" alt=\"CodeQL\" srcset=\"https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-04-17.47.31-1024x979.png 1024w, https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-04-17.47.31-300x287.png 300w, https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-04-17.47.31-768x735.png 768w, https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-04-17.47.31.png 1240w\" sizes=\"(max-width: 1024px) 100vw, 1024px\"/>\n\nLet’s take a closer look at step 3.\n\n<img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"230\" src=\"https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-04-17.47.39-1024x230.png\" alt=\"step 3.\" srcset=\"https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-04-17.47.39-1024x230.png 1024w, https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-04-17.47.39-300x67.png 300w, https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-04-17.47.39-768x173.png 768w, https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-04-17.47.39.png 1210w\" sizes=\"(max-width: 1024px) 100vw, 1024px\"/>\n\nIf this doesn’t shock you, look again. Remember that we have the ability to push tags to the github/codeql-action repository.\n\nCodeQL, under the hood, is executing the actions in the github/codeql-action repository, using the commit referenced by the v3 tag. This tag was not immutable, and they were not using workflow pinning (which GitHub recommends), which meant that an attacker could overwrite the v3 tag using the compromised GITHUB_TOKEN. Now, if an attacker removed and then added a v3 tag to their malicious commit, every single repository using the default CodeQL workflow would execute their malicious code.\n\n<img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"237\" src=\"https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-04-17.48.37-1024x237.png\" alt=\"CodeQL\" srcset=\"https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-04-17.48.37-1024x237.png 1024w, https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-04-17.48.37-300x69.png 300w, https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-04-17.48.37-768x178.png 768w, https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-04-17.48.37.png 1194w\" sizes=\"(max-width: 1024px) 100vw, 1024px\"/>\n\nThe Action created when selecting “Advanced CodeQL” also used the reusable github/codeql-action with the v3 tag.\n\nThe CodeQL actions check out the source code of every repository they run on, which means that a malicious CodeQL action could exfiltrate the source code of any repository using default CodeQL configurations.\n\nThis would result in significant disclosure of intellectual property. And if you’ve ever operated on a Red Team, you know how many hardcoded secrets are lying around in private source code repositories.\n\nBut Wait, There’s More\n\nWe’re almost done. But remember, I promised one more thing:\n\n4. Compromise GitHub Actions secrets of any workflow using the GitHub Actions Cache within a repo that uses CodeQL\n\nWhen assessing the impact of CI/CD attack paths, I look for ways to compromise GitHub Actions secrets. Usually, those secrets are where the crown jewels live.\n\nIf the CodeQL action is executing with write privileges or alongside GitHub Actions secrets, then it’s trivial to use the code execution to exfiltrate those secrets. But the default CodeQL action uses a GITHUB_TOKEN that only has read privileges, so you can’t perform repository write operations, backdoor releases, or use fancy workflow dispatch events to steal secrets, like what happened with PyTorch.\n\nWhat the default CodeQL action does do is execute in the main branch of the repository. The main branch of any GitHub repository can write cache entries that will be used by the entire repo. This opens up an opportunity to conduct GitHub Actions cache poisoning.\n\nGitHub Actions Cache Poisoning is thoroughly explained in this article by Adnan Khan, which documents the discovery and exploitation of cache poisoning. The easiest way to conduct GitHub Actions cache poisoning is by deploying Cacheract, malware that persists in a build pipeline through cache poisoning.\n\nIf an attacker deployed Cacheract in the CodeQL workflow, it would:\n\nPredict cache entries\n\nOverwrite these entries with a malicious action\n\nGain code execution within any workflow that uses action-cache (the Actions Cache is used by most repositories)\n\nLeverage code execution to compromise GitHub Actions secrets used by those workflows, capture privileged GITHUB_TOKENs, and more\n\nEven if someone noticed the malicious CodeQL action and remediated the vulnerability, Cacheract would continue poisoning caches.\n\nI spent ten minutes looking for prominent repos that use CodeQL & actions/cache and identifiedHomebrew, Angular, and Grafana.\n\nCache poisoning would allow an attacker to leverage this CodeQL supply chain attack to gain write access to repositories and repository secrets.\n\nCongratulations, You Made It\n\nWe’ve now hit all the impact highlights I mentioned at the beginning:\n\nCompromise intellectual property by exfiltrating the source code of all private repositories using CodeQL.\n\nSteal credentials within GitHub Actions secrets of any workflow job using CodeQL, and leverage those secrets to execute further supply chain attacks.\n\nExecute code on internal infrastructure running CodeQL workflows.\n\nCompromise GitHub Actions secrets of any workflow using the GitHub Actions Cache within a repo that uses CodeQL.\n\n<img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"351\" src=\"https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-15-11.23.53-1024x351.png\" alt=\"Congratulations, You Made It\" srcset=\"https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-15-11.23.53-1024x351.png 1024w, https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-15-11.23.53-300x103.png 300w, https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-15-11.23.53-768x263.png 768w, https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-15-11.23.53-1536x526.png 1536w, https://www.praetorian.com/wp-content/uploads/2025/03/Box-Notes-Image-2025-02-15-11.23.53-2048x701.png 2048w\" sizes=\"(max-width: 1024px) 100vw, 1024px\"/>\n\nSupply chain attacks like these are scary, especially when they start with something as simple as a publicly exposed credential. If this is your first time hearing about abusing GitHub Actions to launch supply chain attacks, I’ll let you in on a secret: these vulnerabilities occur all the time.\n\nGitHub Actions abuse has been around for several years, but it is still one of the highest-impact, least-understood vulnerability classes. That is slowly starting to change (emphasis on ~slowly~). The DevOps and security communities need to commit to learning about these vulnerabilities to protect their organizations from risk. Vulnerabilities like this, and the recent tj-actions/changed-files supply chain attack, are starting to bring these issues to the public spotlight. That is why we invest in research to uncover these vulnerabilities and design solutions to prevent them.\n\nCVE-2025-24362\n\nA side-effect of this disclosure was CVE-2025-24362. The publicly exposed GITHUB_TOKEN was within a debug artifact uploaded by the CodeQL Action after a failed code scanning workflow. The CodeQL Actions repository was intentionally triggering this failure, but other users of CodeQL Actions could have exposed their own secrets as environment variables to the workflow, had their workflows experienced a similar failure.\n\nThis issue was fixed in CodeQL Action version 3.28.3.\n\nEven though this disclosure resulted in a CVE, the highest potential impact still lies in exploiting that vulnerability against the CodeQL Actions repository and launching a supply chain attack against CodeQL users.\n\nRemediation\n\nGitHub had one of the most rapid and impressive remediation responses we have ever seen.\n\nJan 22, 2025, 3:13 PM UTC: Report Submitted to GitHubJan 22, 2025, 5:48 PM UTC: GitHub acknowledged receipt of the submissionJan 22, 2025, 6:28 PM UTC: GitHub confirmed the vulnerability, temporarily disabled the “PR Check – Debug artifacts after failure” workflow, and submitted this PR to disable debug artifacts upload. This occurred just three hours after submitting the report, which is a very rapid resolution time.Jan 24, 2025: GitHub assigned CVE-2025-24362 and published this security advisory, which notes they found no evidence of compromise to its platform or systems.\n\nIf you are concerned about you’re own GitHub Actions workflow artifacts, you can take the following steps to limit the risk of secrets exposure:\n\nideally, only upload specific files or directories as workflow artifacts\n\navoid uploading artifacts containing environment variables, the .git/config file, or any files in the runner’s <path_to_runner_dir>/_work/_temp/ directory\n\nlimit GITHUB_TOKEN permissions to read-only\n\nscan artifacts for secrets prior to uploading\n\nTo learn more about this vulnerability, how I discovered it, and how you can detect similar vulnerabilities in your own environment, please join me for a webinar on April 10 at 1pm ET.\n\nHow Can Praetorian Help\n\nPraetorian has been leading the charge in offensive CI/CD security for several years, inventing novel tooling and giving presentations at Black Hat, DEF CON, Schmoocon, and Black Hat Arsenal.\n\nOur Continuous Threat Exposure Management (CTEM) platform, Chariot, can identify vulnerabilities like this in your attack surface before the attackers do.\n\nOur CI/CD Security Assessments can take an in-depth look at your internal CI/CD security posture, enumerating attack paths that an attacker would exploit to go from low-privileged access to complete organization compromise.\n\nThe next major supply chain attack could start with something as simple as a publicly exposed secret. Help make sure that doesn’t happen by learning about CI/CD vulnerabilities and implementing continuous controls to protect your organization from compromise.\n\nYou can create a free Chariot account anytime. Alternatively, if you’re interested in our managed Chariot offering, reach out to speak with our team.\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<img loading=\"lazy\" width=\"27\" height=\"30\" src=\"https://www.praetorian.com/wp-content/uploads/2024/06/icon-praetorian-white.svg\" class=\"attachment-thumbnail size-thumbnail wp-image-3232\" alt=\"icon-praetorian-\" />\n\n\t\t\t\t\tSee Praetorian in Action\n\n\t\t\t\t\t\t\t\t\tRequest a 30-day free trial of our Managed Continuous Threat Exposure Management solution.\n\n\t\t\t\t\t\t\t\t\tLet's Get Started\n\nAbout the Authors\n\n\t\t\t\t<img src=\"https://www.praetorian.com/wp-content/uploads/2024/07/john-stawinski-hs-v2-150x150.jpg\" alt=\"John Stawinski\" />John StawinskiJohn is a Red Team Operator at Praetorian, focused on covert operations, CICD + supply chain security, corporate engagements, and public vulnerability research.\n\nCatch the Latest\n\n\t\t\t\t\t\t\t\t\tCatch our latest exploits, news, articles, and events.\n\n\t\t.e-loop-item-3740 .elementor-element.elementor-element-03ccb64::before, .e-loop-item-3740 .elementor-element.elementor-element-03ccb64 > .elementor-background-video-container::before, .e-loop-item-3740 .elementor-element.elementor-element-03ccb64 > .e-con-inner > .elementor-background-video-container::before, .e-loop-item-3740 .elementor-element.elementor-element-03ccb64 > .elementor-background-slideshow::before, .e-loop-item-3740 .elementor-element.elementor-element-03ccb64 > .e-con-inner > .elementor-background-slideshow::before, .e-loop-item-3740 .elementor-element.elementor-element-03ccb64 > .elementor-motion-effects-container > .elementor-motion-effects-layer::before{background-image:url(\"https://www.praetorian.com/wp-content/uploads/2025/03/codeQL-blog-hero.jpg\");}.elementor-1992 .elementor-element.elementor-element-03ccb64{--display:flex;--min-height:400px;--justify-content:flex-end;--flex-wrap:wrap;--overlay-opacity:0.15;--overlay-mix-blend-mode:luminosity;border-style:solid;--border-style:solid;border-width:1px 1px 1px 1px;--border-top-width:1px;--border-right-width:1px;--border-bottom-width:1px;--border-left-width:1px;border-color:var( --e-global-color-cfa35cc );--border-color:var( --e-global-color-cfa35cc );--border-radius:6px 6px 6px 6px;--margin-top:0px;--margin-bottom:32px;--margin-left:0px;--margin-right:0px;--padding-top:24px;--padding-bottom:24px;--padding-left:24px;--padding-right:24px;}.elementor-1992 .elementor-element.elementor-element-03ccb64:not(.elementor-motion-effects-element-type-background), .elementor-1992 .elementor-element.elementor-element-03ccb64 > .elementor-motion-effects-container > .elementor-motion-effects-layer{background-color:var( --e-global-color-primary );}.elementor-1992 .elementor-element.elementor-element-03ccb64::before, .elementor-1992 .elementor-element.elementor-element-03ccb64 > .elementor-background-video-container::before, .elementor-1992 .elementor-element.elementor-element-03ccb64 > .e-con-inner > .elementor-background-video-container::before, .elementor-1992 .elementor-element.elementor-element-03ccb64 > .elementor-background-slideshow::before, .elementor-1992 .elementor-element.elementor-element-03ccb64 > .e-con-inner > .elementor-background-slideshow::before, .elementor-1992 .elementor-element.elementor-element-03ccb64 > .elementor-motion-effects-container > .elementor-motion-effects-layer::before{background-color:var( --e-global-color-primary );--background-overlay:'';}.elementor-1992 .elementor-element.elementor-element-03ccb64:hover::before, .elementor-1992 .elementor-element.elementor-element-03ccb64:hover > .elementor-background-video-container::before, .elementor-1992 .elementor-element.elementor-element-03ccb64:hover > .e-con-inner > .elementor-background-video-container::before, .elementor-1992 .elementor-element.elementor-element-03ccb64 > .elementor-background-slideshow:hover::before, .elementor-1992 .elementor-element.elementor-element-03ccb64 > .e-con-inner > .elementor-background-slideshow:hover::before{--background-overlay:'';}.elementor-1992 .elementor-element.elementor-element-03ccb64:hover{--overlay-opacity:0;}.elementor-1992 .elementor-element.elementor-element-03ccb64:hover::before{filter:brightness( 100% ) contrast( 100% ) saturate( 100% ) blur( 0px ) hue-rotate( 0deg );}.elementor-1992 .elementor-element.elementor-element-03ccb64.e-con{--flex-grow:1;--flex-shrink:0;}.elementor-1992 .elementor-element.elementor-element-03832b4 > .elementor-widget-container{background-color:#D4D5D6;padding:2px 15px 2px 15px;border-radius:50px 50px 50px 50px;}.elementor-1992 .elementor-element.elementor-element-03832b4.elementor-element{--align-self:flex-start;}body:not(.rtl) .elementor-1992 .elementor-element.elementor-element-03832b4{left:24px;}body.rtl .elementor-1992 .elementor-element.elementor-element-03832b4{right:24px;}.elementor-1992 .elementor-element.elementor-element-03832b4{top:24px;}.elementor-1992 .elementor-element.elementor-element-03832b4 .elementor-icon-list-icon{width:14px;}.elementor-1992 .elementor-element.elementor-element-03832b4 .elementor-icon-list-icon i{font-size:14px;}.elementor-1992 .elementor-element.elementor-element-03832b4 .elementor-icon-list-icon svg{--e-icon-list-icon-size:14px;}body:not(.rtl) .elementor-1992 .elementor-element.elementor-element-03832b4 .elementor-icon-list-text{padding-left:0px;}body.rtl .elementor-1992 .elementor-element.elementor-element-03832b4 .elementor-icon-list-text{padding-right:0px;}.elementor-1992 .elementor-element.elementor-element-03832b4 .elementor-icon-list-text, .elementor-1992 .elementor-element.elementor-element-03832b4 .elementor-icon-list-text a{color:var( --e-global-color-primary );}.elementor-1992 .elementor-element.elementor-element-03832b4 .elementor-icon-list-item{font-family:var( --e-global-typography-de81f5c-font-family ), Sans-serif;font-size:var( --e-global-typography-de81f5c-font-size );font-weight:var( --e-global-typography-de81f5c-font-weight );line-height:var( --e-global-typography-de81f5c-line-height );}.elementor-1992 .elementor-element.elementor-element-4ba8c0d .elementor-icon-list-icon{width:14px;}.elementor-1992 .elementor-element.elementor-element-4ba8c0d .elementor-icon-list-icon i{font-size:14px;}.elementor-1992 .elementor-element.elementor-element-4ba8c0d .elementor-icon-list-icon svg{--e-icon-list-icon-size:14px;}.elementor-1992 .elementor-element.elementor-element-4ba8c0d .elementor-icon-list-text, .elementor-1992 .elementor-element.elementor-element-4ba8c0d .elementor-icon-list-text a{color:var( --e-global-color-1e0bfa1 );}.elementor-1992 .elementor-element.elementor-element-4ba8c0d .elementor-icon-list-item{font-family:var( --e-global-typography-de81f5c-font-family ), Sans-serif;font-size:var( --e-global-typography-de81f5c-font-size );font-weight:var( --e-global-typography-de81f5c-font-weight );line-height:var( --e-global-typography-de81f5c-line-height );}.elementor-1992 .elementor-element.elementor-element-25de712 .elementor-heading-title{font-family:var( --e-global-typography-accent-font-family ), Sans-serif;font-size:var( --e-global-typography-accent-font-size );font-weight:var( --e-global-typography-accent-font-weight );line-height:var( --e-global-typography-accent-line-height );color:var( --e-global-color-1e0bfa1 );}.elementor-1992 .elementor-element.elementor-element-d8430d4 .elementor-button{background-color:#F4F5F400;font-family:var( --e-global-typography-9689da3-font-family ), Sans-serif;font-size:var( --e-global-typography-9689da3-font-size );line-height:var( --e-global-typography-9689da3-line-height );fill:var( --e-global-color-1e0bfa1 );color:var( --e-global-color-1e0bfa1 );border-style:none;border-radius:6px 6px 6px 6px;padding:0px 0px 0px 0px;}.elementor-1992 .elementor-element.elementor-element-d8430d4 .elementor-button-content-wrapper{flex-direction:row-reverse;}.elementor-1992 .elementor-element.elementor-element-d8430d4 .elementor-button:hover, .elementor-1992 .elementor-element.elementor-element-d8430d4 .elementor-button:focus{color:var( --e-global-color-accent );}.elementor-1992 .elementor-element.elementor-element-d8430d4 .elementor-button:hover svg, .elementor-1992 .elementor-element.elementor-element-d8430d4 .elementor-button:focus svg{fill:var( --e-global-color-accent );}@media(max-width:1200px){.elementor-1992 .elementor-element.elementor-element-03832b4 .elementor-icon-list-item{font-size:var( --e-global-typography-de81f5c-font-size );line-height:var( --e-global-typography-de81f5c-line-height );}.elementor-1992 .elementor-element.elementor-element-4ba8c0d .elementor-icon-list-item{font-size:var( --e-global-typography-de81f5c-font-size );line-height:var( --e-global-typography-de81f5c-line-height );}.elementor-1992 .elementor-element.elementor-element-25de712 .elementor-heading-title{font-size:var( --e-global-typography-accent-font-size );line-height:var( --e-global-typography-accent-line-height );}.elementor-1992 .elementor-element.elementor-element-d8430d4 .elementor-button{font-size:var( --e-global-typography-9689da3-font-size );line-height:var( --e-global-typography-9689da3-line-height );}}@media(max-width:767px){.elementor-1992 .elementor-element.elementor-element-03832b4 .elementor-icon-list-item{font-size:var( --e-global-typography-de81f5c-font-size );line-height:var( --e-global-typography-de81f5c-line-height );}.elementor-1992 .elementor-element.elementor-element-4ba8c0d .elementor-icon-list-item{font-size:var( --e-global-typography-de81f5c-font-size );line-height:var( --e-global-typography-de81f5c-line-height );}.elementor-1992 .elementor-element.elementor-element-25de712 .elementor-heading-title{font-size:var( --e-global-typography-accent-font-size );line-height:var( --e-global-typography-accent-line-height );}.elementor-1992 .elementor-element.elementor-element-d8430d4 .elementor-button{font-size:var( --e-global-typography-9689da3-font-size );line-height:var( --e-global-typography-9689da3-line-height );}}\n\n\t\t\t\tVulnerability Research\n\n\t\t\t\t\t\t\t\t\t\tMarch 26, 2025\n\n\t\t\t\t\tCodeQLEAKED – Public Secrets Exposure Leads to Supply Chain Attack on GitHub CodeQL\n\n\t\t\t\t\t\t\t\t\tRead More\n\n\t\t.e-loop-item-3552 .elementor-element.elementor-element-03ccb64::before, .e-loop-item-3552 .elementor-element.elementor-element-03ccb64 > .elementor-background-video-container::before, .e-loop-item-3552 .elementor-element.elementor-element-03ccb64 > .e-con-inner > .elementor-background-video-container::before, .e-loop-item-3552 .elementor-element.elementor-element-03ccb64 > .elementor-background-slideshow::before, .e-loop-item-3552 .elementor-element.elementor-element-03ccb64 > .e-con-inner > .elementor-background-slideshow::before, .e-loop-item-3552 .elementor-element.elementor-element-03ccb64 > .elementor-motion-effects-container > .elementor-motion-effects-layer::before{background-image:url(\"https://www.praetorian.com/wp-content/uploads/2025/02/Azure_Hero-Image.png\");}\n\n\t\t\t\tLabs, Uncategorized\n\n\t\t\t\t\t\t\t\t\t\tFebruary 13, 2025\n\n\t\t\t\t\tAzure RBAC Privilege Escalations: Azure VM\n\n\t\t\t\t\t\t\t\t\tRead More\n\n\t\t.e-loop-item-3491 .elementor-element.elementor-element-03ccb64::before, .e-loop-item-3491 .elementor-element.elementor-element-03ccb64 > .elementor-background-video-container::before, .e-loop-item-3491 .elementor-element.elementor-element-03ccb64 > .e-con-inner > .elementor-background-video-container::before, .e-loop-item-3491 .elementor-element.elementor-element-03ccb64 > .elementor-background-slideshow::before, .e-loop-item-3491 .elementor-element.elementor-element-03ccb64 > .e-con-inner > .elementor-background-slideshow::before, .e-loop-item-3491 .elementor-element.elementor-element-03ccb64 > .elementor-motion-effects-container > .elementor-motion-effects-layer::before{background-image:url(\"https://www.praetorian.com/wp-content/uploads/2025/02/Option-2.png\");}\n\n\t\t\t\tOffensive Security\n\n\t\t\t\t\t\t\t\t\t\tFebruary 10, 2025\n\n\t\t\t\t\tLeveraging Microsoft Text Services Framework (TSF) for Red Team Operations\n\n\t\t\t\t\t\t\t\t\tRead More\n\nReady to Discuss Your Next Continuous Threat Exposure Management Initiative?\n\n\t\t\t\t\t\t\t\t\tPraetorian’s Offense Security Experts are Ready to Answer Your Questions\n\n\t\t\t\t\t\t\t\t\tGet Started",
    "summary": {
      "en": "A potential supply chain attack on GitHub's CodeQL was discovered due to a publicly exposed secret that was valid for just over a second. An attacker could exploit this vulnerability to run malicious code in GitHub Actions workflows, potentially affecting both public and enterprise repositories.\n\nThe attacker could achieve various goals, including:\n\n1. **Stealing Source Code**: Accessing the source code of private repositories.\n2. **Compromising Credentials**: Extracting sensitive information from GitHub Actions secrets.\n3. **Executing Malicious Code**: Running harmful code on internal systems.\n4. **Tampering with Actions**: Modifying GitHub Actions secrets and workflows.\n\nThe vulnerability was uncovered during research that involved scanning GitHub Actions for secret exposure. A tool was developed to automate this scanning process, leading to the discovery of a GitHub token in a public artifact.\n\nThe token had full write privileges, allowing an attacker to create branches and tags. However, it was only valid for a short time, creating a challenge for its use. Testing showed that it was feasible to retrieve and utilize the token within that timeframe.\n\nThe findings revealed that enabling CodeQL in repositories executed workflows that could inadvertently expose GitHub Actions secrets and intellectual property. If an attacker compromised the CodeQL actions, they could overwrite tags and execute malicious code across many repositories that relied on CodeQL.\n\nGitHub responded rapidly to the discovery, addressing the vulnerability and issuing a security advisory. For organizations using GitHub Actions, it is crucial to limit permissions, avoid uploading sensitive information, and regularly scan for secrets.\n\nIn summary, this incident highlighted significant risks associated with supply chain vulnerabilities in CI/CD processes, emphasizing the need for ongoing vigilance and security measures.",
      "ko": "GitHub의 CodeQL에 대한 잠재적인 공급망 공격이 발견되었습니다. 이는 공개적으로 노출된 비밀이 단 1초 이상 유효하지 않았기 때문입니다. 공격자는 이 취약점을 이용해 GitHub Actions 워크플로우에서 악성 코드를 실행할 수 있으며, 이는 공용 및 기업 저장소 모두에 영향을 미칠 수 있습니다.\n\n공격자는 여러 가지 목표를 달성할 수 있습니다. 첫째, 비공개 저장소의 소스 코드를 훔칠 수 있습니다. 둘째, GitHub Actions 비밀에서 민감한 정보를 추출할 수 있습니다. 셋째, 내부 시스템에서 해로운 코드를 실행할 수 있습니다. 넷째, GitHub Actions의 비밀과 워크플로우를 수정할 수 있습니다.\n\n이 취약점은 GitHub Actions에서 비밀 노출을 스캔하는 연구 과정에서 발견되었습니다. 이 스캔 과정을 자동화하기 위한 도구가 개발되었고, 그 결과 공개 아티팩트에서 GitHub 토큰이 발견되었습니다.\n\n이 토큰은 전체 쓰기 권한을 가지고 있어 공격자가 브랜치와 태그를 생성할 수 있었습니다. 그러나 유효 기간이 짧아 사용에 어려움이 있었습니다. 테스트 결과, 그 짧은 시간 안에 토큰을 회수하고 활용하는 것이 가능하다는 것이 확인되었습니다.\n\n조사 결과, 저장소에서 CodeQL을 활성화하면 워크플로우가 실행되어 GitHub Actions 비밀과 지적 재산이 우연히 노출될 수 있다는 사실이 드러났습니다. 만약 공격자가 CodeQL 액션을 손상시킨다면, 태그를 덮어쓰고 CodeQL에 의존하는 여러 저장소에서 악성 코드를 실행할 수 있습니다.\n\nGitHub은 이 발견에 신속하게 대응하여 취약점을 수정하고 보안 권고를 발표했습니다. GitHub Actions를 사용하는 조직은 권한을 제한하고, 민감한 정보를 업로드하지 않으며, 정기적으로 비밀을 스캔하는 것이 중요합니다.\n\n이번 사건은 CI/CD 프로세스에서 공급망 취약성과 관련된 심각한 위험을 강조하며, 지속적인 경계와 보안 조치의 필요성을 일깨워 주었습니다.",
      "ja": "GitHubのCodeQLに対する潜在的なサプライチェーン攻撃が発見されました。この攻撃は、公開された秘密情報が1秒以上有効であったことに起因しています。攻撃者はこの脆弱性を利用して、GitHub Actionsのワークフロー内で悪意のあるコードを実行し、公共および企業のリポジトリに影響を与える可能性があります。\n\n攻撃者はさまざまな目的を達成できる可能性があります。まず、プライベートリポジトリのソースコードにアクセスすることで、ソースコードを盗むことができます。また、GitHub Actionsの秘密情報から機密情報を抽出し、認証情報を危険にさらすことも可能です。さらに、内部システムで有害なコードを実行したり、GitHub Actionsの秘密やワークフローを改ざんすることも考えられます。\n\nこの脆弱性は、GitHub Actionsの秘密情報の露出をスキャンする研究の中で明らかになりました。このスキャンプロセスを自動化するツールが開発され、その結果、公開されたアーティファクト内にGitHubトークンが見つかりました。\n\nこのトークンは完全な書き込み権限を持っており、攻撃者はブランチやタグを作成することができました。しかし、トークンの有効期限が短いため、その利用には課題がありました。テストの結果、その時間内にトークンを取得し利用することが可能であることが示されました。\n\n調査の結果、リポジトリでCodeQLを有効にすると、GitHub Actionsの秘密や知的財産が意図せず露出する可能性があることが明らかになりました。もし攻撃者がCodeQLのアクションを侵害した場合、タグを上書きしたり、CodeQLに依存する多くのリポジトリで悪意のあるコードを実行することができるでしょう。\n\nGitHubはこの発見に迅速に対応し、脆弱性を修正し、セキュリティアドバイザリーを発行しました。GitHub Actionsを使用している組織にとっては、権限を制限し、機密情報のアップロードを避け、定期的に秘密情報をスキャンすることが重要です。\n\nこの事件は、CI/CDプロセスにおけるサプライチェーンの脆弱性に関連する重大なリスクを浮き彫りにし、継続的な警戒とセキュリティ対策の必要性を強調しました。"
    }
  },
  {
    "id": "4deca5b7c1f644f7",
    "title": {
      "en": "C and C++ prioritize performance over correctness (2023)",
      "ko": "성능 우선 C/C++",
      "ja": "CとC++の性能重視"
    },
    "type": "story",
    "url": "https://research.swtch.com/ub",
    "score": 91,
    "by": "endorphine",
    "time": 1743193573,
    "content": "C and C++ Prioritize Performance over Correctness\n\n            Posted on Friday, August 18, 2023.\n\n          PDF\n\nThe original ANSI C standard, C89, introduced the concept of “undefined behavior,”\nwhich was used both to describe the effect of outright bugs like\naccessing memory in a freed object\nand also to capture the fact that existing implementations differed about\nhandling certain aspects of the language,\nincluding use of uninitialized values,\nsigned integer overflow, and null pointer handling.\n\nThe C89 spec defined undefined behavior (in section 1.6) as:\n\nUndefined behavior—behavior, upon use of a nonportable or\nerroneous program construct, of erroneous data, or of\nindeterminately-valued objects, for which the Standard imposes no\nrequirements.  Permissible undefined behavior ranges from ignoring the\nsituation completely with unpredictable results, to behaving during\ntranslation or program execution in a documented manner characteristic\nof the environment (with or without the issuance of a diagnostic\nmessage), to terminating a translation or execution (with the issuance\nof a diagnostic message).\n\nLumping both non-portable and buggy code into the same category was a mistake.\nAs time has gone on, the way compilers treat undefined behavior\nhas led to more and more unexpectedly broken programs,\nto the point where it is becoming difficult to tell whether any program\nwill compile to the meaning in the original source.\nThis post looks at a few examples and then tries to make some general observations.\nIn particular, today’s C and C++ prioritize\nperformance to the clear detriment of correctness.\nUninitialized variables\n\nC and C++ do not require variables to be initialized\non declaration (explicitly or implicitly) like Go and Java.\nReading from an uninitialized variable is undefined behavior.\n\nIn a blog post,\nChris Lattner (creator of LLVM and Clang) explains the rationale:\n\nUse of an uninitialized variable:\nThis is commonly known as source of problems in C programs\nand there are many tools to catch these:\nfrom compiler warnings to static and dynamic analyzers.\nThis improves performance by not requiring that all variables\nbe zero initialized when they come into scope (as Java does).\nFor most scalar variables, this would cause little overhead,\nbut stack arrays and malloc’d memory would incur\na memset of the storage, which could be quite costly,\nparticularly since the storage is usually completely overwritten.\n\nEarly C compilers were too crude to detect\nuse of uninitialized basic variables like integers and pointers,\nbut modern compilers are dramatically more sophisticated.\nThey could absolutely react in these cases by\n“terminating a translation or execution (with the issuance\nof a diagnostic message),”\nwhich is to say reporting a compile error.\nOr, if they were worried about not rejecting old programs,\nthey could insert a zero initialization with, as Lattner admits, little overhead.\nBut they don’t do either of these.\nInstead, they just do whatever they feel like during code generation.\n\nFor example, here’s a simple C++ program with an uninitialized variable (a bug):\n#include <stdio.h>\n\nint main() {\n    for(int i; i < 10; i++) {\n        printf(\"%d\\n\", i);\n    }\n    return 0;\n}\n\nIf you compile this with clang++ -O1, it deletes the loop entirely:\nmain contains only the return 0.\nIn effect, Clang has noticed the uninitialized variable and chosen\nnot to report the error to the user but instead\nto pretend i is always initialized above 10, making the loop disappear.\n\nIt is true that if you compile with -Wall, then Clang does report the\nuse of the uninitialized variable as a warning.\nThis is why you should always build with and fix warnings in C and C++ programs.\nBut not all compiler-optimized undefined behaviors\nare reliably reported as warnings.\nArithmetic overflow\n\nAt the time C89 was standardized, there were still legacy\nones’-complement computers,\nso ANSI C could not assume the now-standard two’s-complement representation\nfor negative numbers.\nIn two’s complement, an int8 −1 is 0b11111111;\nin ones’ complement that’s −0, while −1 is 0b11111110.\nThis meant that operations like signed integer overflow could not be defined,\nbecause\n\nint8 127+1 = 0b01111111+1 = 0b10000000\n\nis −127 in ones’ complement but −128 in two’s complement.\nThat is, signed integer overflow was non-portable.\nDeclaring it undefined behavior let compilers escalate the behavior\nfrom “non-portable”, with one of two clear meanings,\nto whatever they feel like doing.\nFor example, a common thing programmers expect is that you can test\nfor signed integer overflow by checking whether the result is\nless than one of the operands, as in this program:\n#include <stdio.h>\n\nint f(int x) {\n    if(x+100 < x)\n        printf(\"overflow\\n\");\n    return x+100;\n}\n\nClang optimizes away the if statement.\nThe justification is that since signed integer overflow is undefined behavior,\nthe compiler can assume it never happens, so x+100 must never be less than x.\nIronically, this program would correctly detect overflow\non both ones’-complement and two’s-complement machines\nif the compiler would actually emit the check.\n\nIn this case, clang++ -O1 -Wall prints no warning while it deletes the if statement,\nand neither does g++,\nalthough I seem to remember it used to, perhaps in subtly different situations\nor with different flags.\n\nFor C++20, the first version of proposal P0907\nsuggested standardizing that signed integer overflow\nwraps in two’s complement. The original draft gave a very clear statement of the history\nof the undefined behavior and the motivation for making a change:\n\n[C11] Integer types allows three representations for signed integral types:\n\nSigned magnitude\n\nOnes’ complement\n\nTwo’s complement\n\nSee §4 C Signed Integer Wording for full wording.\n\nC++ inherits these three signed integer representations from C. To the author’s knowledge no modern machine uses both C++ and a signed integer representation other than two’s complement (see §5 Survey of Signed Integer Representations). None of [MSVC], [GCC], and [LLVM] support other representations. This means that the C++ that is taught is effectively two’s complement, and the C++ that is written is two’s complement. It is extremely unlikely that there exist any significant code base developed for two’s complement machines that would actually work when run on a non-two’s complement machine.\n\nThe C++ that is spec’d, however, is not two’s complement. Signed integers currently allow for trap representations, extra padding bits, integral negative zero, and introduce undefined behavior and implementation-defined behavior for the sake of this extremely abstract machine.\n\nSpecifically, the current wording has the following effects:\n\nAssociativity and commutativity of integers is needlessly obtuse.\n\nNaïve overflow checks, which are often security-critical, often get eliminated by compilers. This leads to exploitable code when the intent was clearly not to and the code, while naïve, was correctly performing security checks for two’s complement integers. Correct overflow checks are difficult to write and equally difficult to read, exponentially so in generic code.\n\nConversion between signed and unsigned are implementation-defined.\n\nThere is no portable way to generate an arithmetic right-shift, or to sign-extend an integer, which every modern CPU supports.\n\nconstexpr is further restrained by this extraneous undefined behavior.\n\nAtomic integral are already two’s complement and have no undefined results, therefore even freestanding implementations already support two’s complement in C++.\n\nLet’s stop pretending that the C++ abstract machine should represent integers as signed magnitude or ones’ complement. These theoretical implementations are a different programming language, not our real-world C++. Users of C++ who require signed magnitude or ones’ complement integers would be better served by a pure-library solution, and so would the rest of us.\n\nIn the end, the C++ standards committee put up “strong resistance against” the idea of defining\nsigned integer overflow the way every programmer expects; the undefined behavior remains.\nInfinite loops\n\nA programmer would never accidentally cause a program to execute an infinite loop, would they?\nConsider this program:\n#include <stdio.h>\n\nint stop = 1;\n\nvoid maybeStop() {\n    if(stop)\n        for(;;);\n}\n\nint main() {\n    printf(\"hello, \");\n    maybeStop();\n    printf(\"world\\n\");\n}\n\nThis seems like a completely reasonable program to write. Perhaps you are debugging and want the program to stop so you can attach a debugger. Changing the initializer for stop to 0 lets the program run to completion.\nBut it turns out that, at least with the latest Clang, the program runs to completion anyway:\nthe call to maybeStop is optimized away entirely, even when stop is 1.\n\nThe problem is that C++ defines that every side-effect-free loop may be assumed by the compiler to terminate.\nThat is, a loop that does not terminate is therefore undefined behavior.\nThis is purely for compiler optimizations, once again treated as more important than correctness.\nThe rationale for this decision played out in the C standard and was more or less adopted in the C++ standard as well.\n\nJohn Regehr pointed out this problem in his post\n“C Compilers Disprove Fermat’s Last Theorem,”\nwhich included this entry in a FAQ:\n\nQ: Does the C standard permit/forbid the compiler to terminate infinite loops?\n\nA: The compiler is given considerable freedom in how it implements the C program,\nbut its output must have the same externally visible behavior that the program would have when interpreted by the “C abstract machine” that is described in the standard.  Many knowledgeable people (including me) read this as saying that the termination behavior of a program must not be changed.  Obviously some compiler writers disagree, or else don’t believe that it matters.  The fact that reasonable people disagree on the interpretation would seem to indicate that the C standard is flawed.\n\nA few months later, Douglas Walls wrote WG14/N1509: Optimizing away infinite loops,\nmaking the case that the standard should not allow this optimization.\nIn response, Hans-J. Boehm wrote\nWG14/N1528: Why undefined behavior for infinite loops?,\narguing for allowing the optimization.\n\nConsider the potential optimization of this code:\nfor (p = q; p != 0; p = p->next)\n    ++count;\nfor (p = q; p != 0; p = p->next)\n    ++count2;\n\nA sufficiently smart compiler might reduce it to this code:\nfor (p = q; p != 0; p = p->next) {\n        ++count;\n        ++count2;\n}\n\nIs that safe? Not if the first loop is an infinite loop. If the list at p is cyclic and another thread is modifying count2,\nthen the first program has no race, while the second program does.\nCompilers clearly can’t turn correct, race-free programs into racy programs.\nBut what if we declare that infinite loops are not correct programs?\nThat is, what if infinite loops were undefined behavior?\nThen the compiler could optimize to its robotic heart’s content.\nThis is exactly what the C standards committee decided to do.\n\nThe rationale, paraphrased, was:\n\nIt is very difficult to tell if a given loop is infinite.\n\nInfinite loops are rare and typically unintentional.\n\nThere are many loop optimizations that are only valid for non-infinite loops.\n\nThe performance wins of these optimizations are deemed important.\n\nSome compilers already apply these optimizations, making infinite loops non-portable too.\n\nTherefore, we should declare programs with infinite loops undefined behavior, enabling the optimizations.\nNull pointer usage\n\nWe’ve all seen how dereferencing a null pointer causes a crash on modern operating systems:\nthey leave page zero unmapped by default precisely for this purpose.\nBut not all systems where C and C++ run have hardware memory protection.\nFor example, I wrote my first C and C++ programs using Turbo C on an MS-DOS system.\nReading or writing a null pointer did not cause any kind of fault:\nthe program just touched the memory at location zero and kept running.\nThe correctness of my code improved dramatically when I moved to\na Unix system that made those programs crash at the moment of the mistake.\nBecause the behavior is non-portable, though, dereferencing a null pointer is undefined behavior.\n\nAt some point, the justification for keeping the undefined behavior became performance.\nChris Lattner explains:\n\nIn C-based languages, NULL being undefined enables a large number of simple scalar optimizations that are exposed as a result of macro expansion and inlining.\n\nIn an earlier post, I showed this example, lifted from Twitter in 2017:\n#include <cstdlib>\n\ntypedef int (*Function)();\n\nstatic Function Do;\n\nstatic int EraseAll() {\n    return system(\"rm -rf slash\");\n}\n\nvoid NeverCalled() {\n    Do = EraseAll;\n}\n\nint main() {\n    return Do();\n}\n\nBecause calling Do() is undefined behavior when Do is null, a modern C++ compiler like Clang\nsimply assumes that can’t possibly be what’s happening in main.\nSince Do must be either null or EraseAll and since null is undefined behavior,\nwe might as well assume Do is EraseAll unconditionally,\neven though NeverCalled is never called.\nSo this program can be (and is) optimized to:\nint main() {\n    return system(\"rm -rf slash\");\n}\n\nLattner gives an equivalent example (search for FP())\nand then this advice:\n\nThe upshot is that it is a fixable issue: if you suspect something weird is going on like this, try building at -O0, where the compiler is much less likely to be doing any optimizations at all.\n\nThis advice is not uncommon: if you cannot debug the correctness problems in your C++ program, disable optimizations.\nCrashes out of sorts\n\nC++’s std::sort sorts a collection of values\n(abstracted as a random access iterator, but almost always an array)\naccording to a user-specified comparison function.\nThe default function is operator<, but you can write any function.\nFor example if you were sorting instances of class Person your\ncomparison function might sort by the LastName field, breaking\nties with the FirstName field.\nThese comparison functions end up being subtle yet boring to write,\nand it’s easy to make a mistake.\nIf you do make a mistake and pass in a comparison function that\nreturns inconsistent results or accidentally reports that any value\nis less than itself, that’s undefined behavior:\nstd::sort is now allowed to do whatever it likes,\nincluding walking off either end of the array\nand corrupting other memory.\nIf you’re lucky, it will pass some of this memory to your comparison\nfunction, and since it won’t have pointers in the right places,\nyour comparison function will crash.\nThen at least you have a chance of guessing the comparison function is at fault.\nIn the worst case, memory is silently corrupted and the crash happens much later,\nwith std::sort is nowhere to be found.\n\nProgrammers make mistakes, and when they do, std::sort corupts memory.\nThis is not hypothetical. It happens enough in practice to be a\npopular question on StackOverflow.\n\nAs a final note, it turns out that operator< is not a valid comparison function\non floating-point numbers if NaNs are involved, because:\n\n1 < NaN and NaN < 1 are both false, implying NaN == 1.\n\n2 < NaN and NaN < 2 are both false, implying NaN == 2.\n\nSince NaN == 1 and NaN == 2, 1 == 2, yet 1 < 2 is true.\n\nProgramming with NaNs is never pleasant, but it seems particularly extreme\nto allow std::sort to crash when handed one.\nReflections and revealed preferences\n\nLooking over these examples,\nit could not be more obvious that in modern C and C++,\nperformance is job one and correctness is job two.\nTo a C/C++ compiler, a programmer making a mistake and (gasp!)\ncompiling a program containing a bug is just not a concern.\nRather than have the compiler point out the bug or at least\ncompile the code in a clear, understandable, debuggable manner,\nthe approach over and over again is\nto let the compiler do whatever it likes,\nin the name of performance.\n\nThis may not be the wrong decision for these languages.\nThere are undeniably power users for whom every last bit of performance\ntranslates to very large sums of money, and I don’t claim\nto know how to satisfy them otherwise.\nOn the other hand, this performance comes at a significant\ndevelopment cost, and there are probably plenty of people and companies\nwho spend more than their performance savings\non unnecessarily difficult debugging sessions\nand additional testing and sanitizing.\nIt also seems like there must be a middle ground where\nprogrammers retain most of the control they have in C and C++\nbut the program doesn’t crash when sorting NaNs or\nbehave arbitrarily badly if you accidentally dereference a null pointer.\nWhatever the merits, it is important to see clearly the choice that C and C++ are making.\n\nIn the case of arithmetic overflow, later drafts of the\nproposal removed the defined behavior for wrapping, explaining:\n\nThe main change between [P0907r0] and the subsequent revision is to maintain undefined behavior when signed integer overflow occurs, instead of defining wrapping behavior. This direction was motivated by:\n\nPerformance concerns, whereby defining the behavior prevents optimizers from assuming that overflow never occurs;\n\nImplementation leeway for tools such as sanitizers;\n\nData from Google suggesting that over 90% of all overflow is a bug, and defining wrapping behavior would not have solved the bug.\n\nAgain, performance concerns rank first.\nI find the third item in the list particularly telling.\nI’ve known C/C++ compiler authors who got excited about a 0.1% performance improvement,\nand incredibly excited about 1%.\nYet here we have an idea that would change 10% of affected programs from incorrect to correct,\nand it is rejected, because performance is more important.\n\nThe argument about sanitizers is more nuanced.\nLeaving a behavior undefined allows any implementation at all, including reporting the\nbehavior at runtime and stopping the program.\nTrue, the widespread use of undefined behavior enables sanitizers like ThreadSanitizer, MemorySanitizer, and UBSan,\nbut so would defining the behavior as “either this specific behavior, or a sanitizer report.”\nIf you believed correctness was job one, you could\ndefine overflow to wrap, fixing the 10% of programs outright\nand making the 90% behave at least more predictably,\nand then at the same time define that overflow is still\na bug that can be reported by sanitizers.\nYou might object that requiring wrapping in the absence of a sanitizer\nwould hurt performance, and that’s fine: it’s just more evidence that\nperformance trumps correctness.\n\nOne thing I find surprising, though, is that correctness gets ignored even\nwhen it clearly doesn’t hurt performance.\nIt would certainly not hurt performance to emit a compiler warning\nabout deleting the if statement testing for signed overflow,\nor about optimizing away the possible null pointer dereference in Do().\nYet I could find no way to make compilers report either one; certainly not -Wall.\n\nThe explanatory shift from non-portable to optimizable also seems revealing.\nAs far as I can tell, C89 did not use performance as a justification for any of\nits undefined behaviors.\nThey were non-portabilities, like signed overflow and null pointer dereferences,\nor they were outright bugs, like use-after-free.\nBut now experts like Chris Lattner and Hans Boehm point to optimization potential,\nnot portability, as justification for undefined behaviors.\nI conclude that the rationales really have shifted from the mid-1980s to today:\nan idea that meant to capture non-portability has been preserved for performance,\ntrumping concerns like correctness and debuggability.\n\nOccasionally in Go we have changed library functions to remove surprising behavior,\nIt’s always a difficult decision, but we are willing\nto break existing programs depending on a mistake\nif correcting the mistake fixes a much larger number of programs.\nI find it striking that the C and C++ standards committees are\nwilling in some cases to break existing programs if doing so\nmerely speeds up a large number of programs.\nThis is exactly what happened with the infinite loops.\n\nI find the infinite loop example telling for a second reason:\nit shows clearly the escalation from non-portable to optimizable.\nIn fact, it would appear that if you want to break C++ programs in\nservice of optimization, one possible approach is to just do that in a\ncompiler and wait for the standards committee to notice.\nThe de facto non-portability of whatever programs you have broken\ncan then serve as justification for undefining their behavior,\nleading to a future version of the standard in which your optimization is legal.\nIn the process, programmers have been handed yet another footgun\nto try to avoid setting off.\n\n(A common counterargument is that the standards committee cannot\nforce existing implementations to change their compilers.\nThis doesn’t hold up to scrutiny: every new feature that gets added\nis the standards committee forcing existing implementations\nto change their compilers.)\n\nI am not claiming that anything should change about C and C++.\nI just want people to recognize that the current versions of these\nsacrifice correctness for performance.\nTo some extent, all languages do this: there is almost always a tradeoff\nbetween performance and slower, safer implementations.\nGo has data races in part for performance reasons:\nwe could have done everything by message copying\nor with a single global lock instead, but the performance wins of\nshared memory were too large to pass up.\nFor C and C++, though, it seems no performance win is too small\nto trade against correctness.\n\nAs a programmer, you have a tradeoff to make too,\nand the language standards make it clear which side they are on.\nIn some contexts, performance is the dominant priority and\nnothing else matters quite as much.\nIf so, C or C++ may be the right tool for you.\nBut in most contexts, the balance flips the other way.\nIf programmer productivity, debuggability, reproducible bugs,\nand overall correctness and understandability\nare more important than squeezing every last little bit of performance,\nthen C and C++ are not the right tools for you.\nI say this with some regret, as I spent many years happily writing C programs.\n\nI have tried to avoid exaggerated, hyperbolic language in this post,\ninstead laying out the tradeoff and the preferences revealed\nby the decisions being made.\nJohn Regehr wrote a less restrained series of posts about undefined behavior\na decade ago, and in one of them he concluded:\n\nIt is basically evil to make certain program actions wrong, but to not give developers any way to tell whether or not their code performs these actions and, if so, where. One of C’s design points was “trust the programmer.” This is fine, but there’s trust and then there’s trust. I mean, I trust my 5 year old but I still don’t let him cross a busy street by himself. Creating a large piece of safety-critical or security-critical code in C or C++ is the programming equivalent of crossing an 8-lane freeway blindfolded.\n\nTo be fair to C and C++,\nif you set yourself the goal of crossing an 8-lane freeway blindfolded,\nit does make sense to focus on doing it as fast as you possibly can.",
    "summary": {
      "en": "The article discusses how C and C++ prioritize performance over correctness, particularly through the concept of \"undefined behavior,\" which can lead to unpredictable results in programs. \n\nKey points include:\n\n1. **Undefined Behavior**: Introduced in the C89 standard, it describes various situations where the standard does not dictate how a program should behave, such as using uninitialized variables or encountering signed integer overflow. This can lead to programs compiling or running in unexpected ways.\n\n2. **Uninitialized Variables**: Unlike languages like Java, C and C++ do not require variables to be initialized before use, which can result in undefined behavior if a variable is read without being set.\n\n3. **Arithmetic Overflow**: The handling of signed integer overflow is left as undefined behavior, allowing compilers to optimize code aggressively, even if it means eliminating checks that would protect against overflow errors.\n\n4. **Infinite Loops**: C and C++ compilers may optimize away what they determine to be infinite loops, which can lead to incorrect program behavior during execution.\n\n5. **Null Pointer Dereferencing**: Accessing a null pointer is also undefined behavior, enabling optimizations at the cost of potentially crashing the program.\n\n6. **Sorting Functions**: Mistakes in user-defined comparison functions for sorting can lead to memory corruption, as the behavior of sorting functions becomes undefined with incorrect implementations.\n\nThe article concludes that while performance is crucial for certain applications, the trade-offs can lead to significant correctness issues, making C and C++ less suitable for projects where debugging and safety are priorities. It emphasizes the importance of understanding these trade-offs when choosing a programming language.",
      "ko": "이 기사는 C와 C++ 언어가 정확성보다 성능을 우선시하는 방식, 특히 \"정의되지 않은 동작\"이라는 개념을 통해 프로그램에서 예측할 수 없는 결과를 초래할 수 있는 방법에 대해 설명합니다.\n\n정의되지 않은 동작은 C89 표준에서 도입된 개념으로, 프로그램이 어떻게 동작해야 하는지에 대한 규정이 없는 여러 상황을 설명합니다. 예를 들어, 초기화되지 않은 변수를 사용하거나 부호 있는 정수 오버플로우가 발생하는 경우가 이에 해당합니다. 이러한 상황은 프로그램이 예상치 못한 방식으로 컴파일되거나 실행될 수 있게 만듭니다.\n\nC와 C++는 자바와 달리 변수를 사용하기 전에 반드시 초기화할 필요가 없습니다. 이로 인해 변수가 설정되지 않은 상태에서 읽히면 정의되지 않은 동작이 발생할 수 있습니다. 또한, 부호 있는 정수 오버플로우의 처리는 정의되지 않은 동작으로 남겨져 있어, 컴파일러가 오버플로우 오류를 방지하기 위한 검사를 제거하더라도 코드를 적극적으로 최적화할 수 있습니다.\n\nC와 C++ 컴파일러는 무한 루프라고 판단되는 부분을 최적화하여 제거할 수 있습니다. 이로 인해 실행 중 프로그램의 동작이 잘못될 수 있습니다. 널 포인터를 접근하는 것도 정의되지 않은 동작으로, 이로 인해 프로그램이 충돌할 위험이 있지만 최적화를 가능하게 합니다.\n\n사용자가 정의한 정렬 함수에서의 실수는 메모리 손상을 초래할 수 있습니다. 잘못된 구현으로 인해 정렬 함수의 동작이 정의되지 않게 되기 때문입니다.\n\n이 기사는 성능이 특정 애플리케이션에 중요하지만, 이러한 성능 우선의 선택이 심각한 정확성 문제로 이어질 수 있음을 강조합니다. 따라서 디버깅과 안전성이 중요한 프로젝트에는 C와 C++가 덜 적합할 수 있음을 알리고, 프로그래밍 언어를 선택할 때 이러한 트레이드오프를 이해하는 것이 중요하다고 강조합니다.",
      "ja": "この記事では、C言語とC++が正確性よりもパフォーマンスを重視していることについて説明しています。特に「未定義動作」という概念が、プログラムに予測できない結果をもたらす可能性があることが強調されています。\n\n未定義動作はC89標準で導入され、プログラムがどのように動作すべきかが標準で定義されていないさまざまな状況を指します。例えば、初期化されていない変数を使用したり、符号付き整数のオーバーフローが発生した場合などです。これにより、プログラムが予期しない方法でコンパイルされたり実行されたりすることがあります。\n\nC言語とC++では、Javaのように変数を使用する前に初期化する必要がありません。このため、設定されていない変数を読み取ると未定義動作が発生する可能性があります。\n\n符号付き整数のオーバーフローについては、未定義動作として扱われており、コンパイラはオーバーフローエラーを防ぐためのチェックを省略してでもコードを積極的に最適化することができます。\n\nC言語とC++のコンパイラは、無限ループと判断したものを最適化して削除することがありますが、これが実行時にプログラムの不正な動作を引き起こすことがあります。\n\nヌルポインタの逆参照も未定義動作であり、これによりプログラムがクラッシュする可能性がある一方で、最適化が可能になります。\n\nユーザー定義の比較関数に誤りがあると、ソート関数の動作が未定義となり、メモリの破損を引き起こすことがあります。\n\nこの記事は、パフォーマンスが特定のアプリケーションにとって重要である一方で、そのトレードオフが重大な正確性の問題を引き起こす可能性があることを指摘しています。そのため、デバッグや安全性が優先されるプロジェクトにはC言語やC++が適さない場合があると結論づけています。プログラミング言語を選ぶ際には、これらのトレードオフを理解することが重要であると強調されています。"
    }
  },
  {
    "id": "8d1ca1704cea29e7",
    "title": {
      "en": "Oka.wiki",
      "ko": "오카위키",
      "ja": "オカウィキ"
    },
    "type": "story",
    "url": "https://oka.wiki/",
    "score": 105,
    "by": "jjmarr",
    "time": 1743379163,
    "content": "Open Knowledge Association (OKA)\n\nDisseminating free content on Wikipedia and open platforms through targeted funding\n\nLearn more\n\nDonate\n\nWe are a non-profit organization dedicated to improving Wikipedia and other open platforms. We do so by providing monthly stipends to full-time contributors and translators.\n\nWe pragmatically prioritize work that generates the most impact. To maximize returns on our donors’ funds and decrease systematic bias, we hire content writers from countries  underrepresented among Wikipedia editors.\n\nOur processes are public. Our work is transparent. Our freelancers have a high-level of autonomy. We are based in Switzerland and recognized as a tax-exempt organization with a public utility purpose.\n\nWikipedia was mostly written by volunteers. But sometimes they need a bit of help.\n\nSo far, we have created 2,200+ new articles on Wikipedia, which generate 8m views per year.\n\nWhat makes us different\n\nWikipedia relies entirely on volunteers to create and maintain its content. That often works well, but there are tasks that volunteers don’t enjoy doing, and topics where volunteers are missing.\n\nFor example, articles in topics such as Science, technology, engineering, and Finance are lacking compared to topics such as History, Geography, and Humanities. Content from non-anglophone countries is underrepresented. Hundreds of thousands of high-quality articles are not translated.\n\nWith targeted funding, these opportunities can be addressed for high impact with very limited costs.\n\nBut even when paid, Wikipedia editors need to enjoy what they do. Otherwise quality suffers. That’s why we strive to provide as much autonomy and ownership to our freelancers as possible, just like regular volunteers. They use their own account for all edits, and are responsible to maintain the articles they create.\n\nWhere we focus\n\nWe currently prioritize translations over new content, which requires significantly less time to achieve similar results. We leverage Machine Learning tools for initial drafts to minimize manual work.\n\nWe concentrate on the English, Spanish and Portuguese Wikipedia to reach larger audiences. We prioritize high-quality articles in topics for which content ages well, so that they remain up-to-date.\n\nCurrently, our team consists of over 15 full-time translators covers English, Spanish, Portuguese, French, Italian, Russian and German.\n\nHow we get funding\n\nCurrently, the majority of our funding comes from our individual donations, but we also accept donations from other charities, government entities, and companies.\n\nIf they wish to, our donors can specify the area in which they want their money to be spent. For example, a local government may want its donation to be dedicated to the translation of articles related to their region. A company may wish us to focus on articles related to their industry.\n\nBut they are not allowed to influence the work of our editors. Our grant recipients retain complete editorial freedom and are asked to ensure objectivity.\n\nLearn more\n\nMore details about who we are and how we work.\n\nOur process\n\nSupport us\n\nHelp finance one of our grants. We accept all major currencies.\n\nDonate\n\nReach out\n\nWant to learn more or explore partnership opportunities?\n\nContact us",
    "summary": {
      "en": "**Open Knowledge Association (OKA) Summary**\n\nThe Open Knowledge Association (OKA) is a non-profit organization based in Switzerland that aims to improve Wikipedia and other open platforms by providing financial support to full-time contributors and translators. They focus on creating impactful content while addressing underrepresentation on Wikipedia, particularly in science and technology topics and from non-English-speaking countries.\n\nKey points include:\n- OKA offers monthly stipends to content writers and translators, prioritizing high-impact work.\n- They have created over 2,200 new Wikipedia articles, generating 8 million views annually.\n- Their approach is transparent, allowing freelancers autonomy and ownership of their edits.\n- Currently, they focus on translating existing content rather than creating new articles, using Machine Learning tools to streamline the process.\n- Their funding primarily comes from individual donations, with options for donors to specify areas of focus, while maintaining editorial independence.\n\nOKA is committed to enhancing the quality and diversity of Wikipedia content while ensuring that contributors enjoy their work.",
      "ko": "오픈 지식 협회(OKA)는 스위스에 본사를 둔 비영리 단체로, 위키백과와 다른 오픈 플랫폼을 개선하기 위해 전업 기여자와 번역가에게 재정 지원을 제공합니다. 이들은 과학 및 기술 주제와 비영어권 국가의 저조한 대표성을 해결하면서 영향력 있는 콘텐츠를 만드는 데 집중하고 있습니다.\n\nOKA는 콘텐츠 작성자와 번역가에게 매달 지원금을 제공하며, 특히 영향력이 큰 작업을 우선시합니다. 이들은 2,200개 이상의 새로운 위키백과 기사를 작성하여 매년 800만 회의 조회수를 기록했습니다. 그들의 접근 방식은 투명하며, 프리랜서들이 자신의 편집에 대한 자율성과 소유권을 가질 수 있도록 하고 있습니다. 현재는 새로운 기사를 만드는 대신 기존 콘텐츠를 번역하는 데 집중하고 있으며, 머신러닝 도구를 사용하여 이 과정을 효율화하고 있습니다. \n\nOKA의 자금은 주로 개인 기부에서 나오며, 기부자들이 특정 분야에 집중할 수 있는 옵션을 제공하면서도 편집 독립성을 유지하고 있습니다. OKA는 위키백과 콘텐츠의 질과 다양성을 향상시키는 데 전념하며, 기여자들이 자신의 작업을 즐길 수 있도록 하고 있습니다.",
      "ja": "オープンナレッジ協会（OKA）は、スイスに拠点を置く非営利団体で、ウィキペディアやその他のオープンプラットフォームの改善を目指しています。彼らは、フルタイムの寄稿者や翻訳者に財政的支援を提供することで、影響力のあるコンテンツを作成し、特に科学技術や非英語圏の国々におけるウィキペディアの表現不足に取り組んでいます。\n\nOKAの主な活動として、コンテンツライターや翻訳者に月額の手当を支給し、高い影響力を持つ仕事を優先しています。これまでに2,200以上の新しいウィキペディアの記事を作成し、年間800万回の閲覧を達成しています。彼らのアプローチは透明性があり、フリーランスの寄稿者は自分の編集に対して自主性と所有権を持つことができます。\n\n現在、OKAは新しい記事の作成よりも既存のコンテンツの翻訳に重点を置いており、機械学習ツールを活用してプロセスを効率化しています。資金は主に個人からの寄付によって賄われており、寄付者は特定の焦点を指定することができますが、編集の独立性は保たれています。\n\nOKAは、ウィキペディアのコンテンツの質と多様性を向上させることに尽力し、寄稿者が自分の仕事を楽しめるようにしています。"
    }
  },
  {
    "id": "26a13aaa5d7dbf11",
    "title": {
      "en": "Claim for a missing tooth",
      "ko": "잃어버린 치아 보상",
      "ja": "歯の欠損請求"
    },
    "type": "story",
    "url": "https://tf230.matteason.co.uk/",
    "score": 351,
    "by": "lukecarr",
    "time": 1743164700,
    "content": "Claim for a missing tooth Use this service to claim financial reimbursement for a tooth which has been lost and cannot be collected by the Tooth Fairy - for example, teeth which have been: accidentally swallowedstolen by a ferretdestroyed by an asteroidStart nowOther ways to claim You can also print and fill in Form TF-230 and leave it under your pillow.",
    "summary": {
      "en": "You can claim money for a lost tooth that the Tooth Fairy can't collect. This includes teeth that were accidentally swallowed, stolen by a ferret, or destroyed by an asteroid. To start your claim, use the online service or print Form TF-230 and place it under your pillow.",
      "ko": "이빨 요정이 수집할 수 없는 잃어버린 이빨에 대해 보상을 청구할 수 있습니다. 여기에는 실수로 삼킨 이빨, 족제비에게 도난당한 이빨, 또는 소행성에 의해 파괴된 이빨이 포함됩니다. 청구를 시작하려면 온라인 서비스를 이용하거나 TF-230 양식을 인쇄하여 베개 아래에 두면 됩니다.",
      "ja": "歯がなくなった場合、歯の妖精が回収できないお金を請求することができます。これには、うっかり飲み込んでしまった歯や、フェレットに盗まれた歯、さらには小惑星によって壊された歯も含まれます。請求を始めるには、オンラインサービスを利用するか、TF-230フォームを印刷して枕の下に置いてください。"
    }
  }
]