[
  {
    "id": "94dd7a5cb320f1fa",
    "title": {
      "en": "Spammers are better at SPF, DKIM, and DMARC than everyone else",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://toad.social/@grumpybozo/114213600922816869",
    "score": 101,
    "by": "pabs3",
    "time": 1742890490,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "781d9ca14c79dac7",
    "title": {
      "en": "Writing your own C++ standard library from scratch",
      "ko": "C++ 표준 라이브러리 만들기",
      "ja": "C++ライブラリ自作術"
    },
    "type": "story",
    "url": "https://nibblestew.blogspot.com/2025/03/writing-your-own-c-standard-library.html",
    "score": 70,
    "by": "JNRowe",
    "time": 1742890236,
    "content": "Nibble Stew\n\nA gathering of development thoughts of Jussi Pakkanen. Some of you may know him as the creator of the Meson build system. jpakkane at gmail dot com\n\nMonday, March 24, 2025\n\nWriting your own C++ standard library from scratch\n\nThe C++ standard library (also know as the STL) is, without a doubt, an astounding piece of work. Its scope, performance and incredible backwards compatibility have taken decades of work by many of the world's best programmers. My hat's off to all those people who have contributed to it.All of that is not to say that it is not without its problems. The biggest one being the absolutely abysmal compile times but unreadability, and certain unoptimalities caused by strict backwards compatibility are also at the top of the list. In fact, it could be argued that most of the things people really dislike about C++ are features of the STL rather than the language itself. Fortunately, using the STL is not mandatory. If you are crazy enough, you can disable it completely and build your own standard library in the best Bender style.One of the main advantages of being an unemployed-by-choice open source developer is that you can do all of that if you wish. There are no incompetent middle damagers hovering over your shoulder to ensure you are \"producing immediate customer value\" rather than \"wasting time on useless polishing that does not produce immediate customer value\".It's my time, and I'll waste it if I want to!What's in it?The biggest design questions of a standard library are scope and the \"feel\" of the API. Rather than spending time on design, we steal it. Thus, when in doubt, read the Python stdlib documentation and replicate it. Thus the name of the library is pystd.The test appTo keep the scope meaningful, we start by writing only enough of stdlib to build an app that reads a text file, validates it as UTF-8, splits the contents into words, counts how many time each word appears in the file and prints all words and how many times it appears sorted by decreasing count.This requires, at least:File handlingStringsUTF8 validationA hash mapA vectorSortingThe training wheels come offThe code is available in this Github repo for those who want to follow along at home.Disabling the STL is fairly easy (with Linux+GCC at least) and requires only these two Meson statements:add_global_arguments('-nostdinc++', language: 'cpp')\nadd_global_link_arguments('-nostdlib++', '-lsupc++', language: 'cpp')The supc++ library is (according to stackoverflow) a support library GCC needs to implement core language features. Now the stdlib is off and it is time to implement everything with sticks, stones and duct tape.The outcomeOnce you have implemented everything discussed above and auxiliary stuff like a hashing framework the main application looks like this.The end result is both Valgrind and Asan clean. There is one chunk of unreleased memory, but that comes from supc++. There is probably UB in the implementation. But it should be the good kind of UB that, if it would actually not work, would break the entire Linux userspace because everything depends on it working \"as expected\".All of this took fewer than 1000 lines of code in the library itself (including a regex implementation that is not actually used). For comparison merely including vector from the STL brings in 27 thousand lines of code.Comparison to an STL versionConverting this code to use the STL is fairly simple and only requires changing some types and fine tuning the API. The main difference is that the STL version does not validate that the input is UTF-8 as there is no builtin function for that. Now we can compare the two.Runtime for both is 0.001 to 0.002 seconds on the small test file I used. Pystd is not noticeably slower than the STL version, which is enough for our purposes. It almost certainly scales worse because there has been zero performance work on it.Compiling the pystd version with -O2 takes 0.3 seconds whereas the STL version takes 1.2 seconds. The measurements were done on a Ryzen 7 3700X processor.The executable's unstripped size is 349k for STL and 309k for pystd. The stripped sizes are 23k and 135k. Approximately 100 k of the pystd executable comes from supc++. In the STL version that probably comes dynamically from libstdc++(which, on this machine, takes 2.5 MB).Perfect ABI stabilityDesigning a standard library is exceedingly difficult because you can't ever really change it. Someone, somewhere, is depending on every misfeature in it so they can never be changed.Pystd has been designed to both support perfect ABI stability and make it possible to change it in arbitrary ways in the future. If you start from scratch this turned out to be fairly simple.The sample code above used the pystd namespace. It does not actually exist. Instead it is defined like this in the cpp file:#include <pystd2025.hpp>namespace pystd = pystd2025;In pystd all code is in a namespace with a year and is stored in a header file with the same year. The idea is, then, that every year you create a new release. This involves copying all stdlib header files to a file with the new year and regexping the namespace declarations to match. The old code is now frozen forever (except for bug fixes) whereas the new code can be changed at will because there arezero existing lines of code that depend on it.End users now have the choice of when to update their code to use newer pystd versions. Even better, if there is an old library that can not be updated, any of the old versions can be used in parallel. For example:pystd2030::SomeType foo;pystd2025::SomeType bar(foo.something(), foo.something_else());Thus if no code is ever updated, everything keeps working. If all code is updated at once, everything works. If only parts of the code are updated, things can still be made to work with some glue code. This puts the maintenance burden on the people whose projects can not be updated as opposed to every other developer in the world. This is as it should be, and also would motivate people with broken deps to spend some more effort to get them fixed.\n\nPosted by\n\nJussi\n\nat\n\n5:03 PM\n\nEmail ThisBlogThis!Share to XShare to FacebookShare to Pinterest\n\nNo comments:\n\nPost a Comment\n\n      BLOG_CMT_createIframe('https://www.blogger.com/rpc_relay.html');\n\nOlder Post\n\nHome\n\nSubscribe to:\nPost Comments (Atom)\n\nAbout Me\n\nJussi\n\nView my complete profile\n\nBlog Archive\n\n        ▼\n\n2025\n\n(5)\n\n        ▼\n\nMarch\n\n(1)\n\nWriting your own C++ standard library from scratch\n\n        ►\n\nFebruary\n\n(3)\n\n        ►\n\nJanuary\n\n(1)\n\n        ►\n\n2024\n\n(21)\n\n        ►\n\nDecember\n\n(4)\n\n        ►\n\nNovember\n\n(1)\n\n        ►\n\nOctober\n\n(2)\n\n        ►\n\nSeptember\n\n(1)\n\n        ►\n\nAugust\n\n(2)\n\n        ►\n\nJuly\n\n(1)\n\n        ►\n\nJune\n\n(1)\n\n        ►\n\nMay\n\n(1)\n\n        ►\n\nApril\n\n(3)\n\n        ►\n\nMarch\n\n(2)\n\n        ►\n\nFebruary\n\n(1)\n\n        ►\n\nJanuary\n\n(2)\n\n        ►\n\n2023\n\n(41)\n\n        ►\n\nDecember\n\n(5)\n\n        ►\n\nNovember\n\n(2)\n\n        ►\n\nOctober\n\n(2)\n\n        ►\n\nSeptember\n\n(3)\n\n        ►\n\nAugust\n\n(1)\n\n        ►\n\nJuly\n\n(3)\n\n        ►\n\nJune\n\n(4)\n\n        ►\n\nMay\n\n(4)\n\n        ►\n\nApril\n\n(4)\n\n        ►\n\nMarch\n\n(6)\n\n        ►\n\nFebruary\n\n(5)\n\n        ►\n\nJanuary\n\n(2)\n\n        ►\n\n2022\n\n(34)\n\n        ►\n\nDecember\n\n(4)\n\n        ►\n\nNovember\n\n(3)\n\n        ►\n\nOctober\n\n(2)\n\n        ►\n\nSeptember\n\n(4)\n\n        ►\n\nAugust\n\n(2)\n\n        ►\n\nJuly\n\n(3)\n\n        ►\n\nJune\n\n(2)\n\n        ►\n\nMay\n\n(2)\n\n        ►\n\nApril\n\n(4)\n\n        ►\n\nFebruary\n\n(5)\n\n        ►\n\nJanuary\n\n(3)\n\n        ►\n\n2021\n\n(29)\n\n        ►\n\nDecember\n\n(1)\n\n        ►\n\nNovember\n\n(1)\n\n        ►\n\nOctober\n\n(1)\n\n        ►\n\nSeptember\n\n(2)\n\n        ►\n\nAugust\n\n(1)\n\n        ►\n\nJuly\n\n(5)\n\n        ►\n\nJune\n\n(3)\n\n        ►\n\nMay\n\n(5)\n\n        ►\n\nApril\n\n(2)\n\n        ►\n\nMarch\n\n(4)\n\n        ►\n\nFebruary\n\n(3)\n\n        ►\n\nJanuary\n\n(1)\n\n        ►\n\n2020\n\n(42)\n\n        ►\n\nDecember\n\n(1)\n\n        ►\n\nNovember\n\n(5)\n\n        ►\n\nOctober\n\n(5)\n\n        ►\n\nSeptember\n\n(2)\n\n        ►\n\nAugust\n\n(4)\n\n        ►\n\nJuly\n\n(5)\n\n        ►\n\nJune\n\n(2)\n\n        ►\n\nMay\n\n(4)\n\n        ►\n\nApril\n\n(3)\n\n        ►\n\nMarch\n\n(4)\n\n        ►\n\nFebruary\n\n(5)\n\n        ►\n\nJanuary\n\n(2)\n\n        ►\n\n2019\n\n(27)\n\n        ►\n\nDecember\n\n(4)\n\n        ►\n\nNovember\n\n(3)\n\n        ►\n\nOctober\n\n(1)\n\n        ►\n\nSeptember\n\n(2)\n\n        ►\n\nAugust\n\n(3)\n\n        ►\n\nJuly\n\n(2)\n\n        ►\n\nJune\n\n(2)\n\n        ►\n\nMay\n\n(3)\n\n        ►\n\nApril\n\n(1)\n\n        ►\n\nFebruary\n\n(6)\n\n        ►\n\n2018\n\n(24)\n\n        ►\n\nNovember\n\n(4)\n\n        ►\n\nOctober\n\n(1)\n\n        ►\n\nSeptember\n\n(1)\n\n        ►\n\nAugust\n\n(3)\n\n        ►\n\nJuly\n\n(3)\n\n        ►\n\nJune\n\n(1)\n\n        ►\n\nApril\n\n(3)\n\n        ►\n\nMarch\n\n(4)\n\n        ►\n\nFebruary\n\n(4)\n\n        ►\n\n2017\n\n(35)\n\n        ►\n\nDecember\n\n(4)\n\n        ►\n\nNovember\n\n(1)\n\n        ►\n\nSeptember\n\n(1)\n\n        ►\n\nAugust\n\n(3)\n\n        ►\n\nJuly\n\n(3)\n\n        ►\n\nJune\n\n(3)\n\n        ►\n\nMay\n\n(5)\n\n        ►\n\nApril\n\n(4)\n\n        ►\n\nMarch\n\n(3)\n\n        ►\n\nFebruary\n\n(2)\n\n        ►\n\nJanuary\n\n(6)\n\n        ►\n\n2016\n\n(20)\n\n        ►\n\nDecember\n\n(4)\n\n        ►\n\nNovember\n\n(2)\n\n        ►\n\nOctober\n\n(1)\n\n        ►\n\nSeptember\n\n(2)\n\n        ►\n\nAugust\n\n(1)\n\n        ►\n\nJuly\n\n(1)\n\n        ►\n\nJune\n\n(2)\n\n        ►\n\nMay\n\n(2)\n\n        ►\n\nApril\n\n(4)\n\n        ►\n\nMarch\n\n(1)\n\n        ►\n\n2015\n\n(19)\n\n        ►\n\nDecember\n\n(2)\n\n        ►\n\nNovember\n\n(3)\n\n        ►\n\nOctober\n\n(3)\n\n        ►\n\nSeptember\n\n(2)\n\n        ►\n\nAugust\n\n(3)\n\n        ►\n\nJuly\n\n(5)\n\n        ►\n\nJune\n\n(1)\n\nSimple theme. Powered by Blogger.",
    "summary": {
      "en": "**Summary of \"Writing Your Own C++ Standard Library from Scratch\"**\n\nJussi Pakkanen discusses the challenges and decisions involved in creating a custom C++ standard library, referred to as \"pystd.\" Although the existing C++ standard library (STL) is impressive, it has issues like long compile times and readability concerns. Pakkanen proposes that it is possible to disable the STL and build a new library that addresses these problems.\n\nThe library aims to offer essential features such as file handling, string manipulation, UTF-8 validation, and sorting while keeping the codebase minimal—under 1,000 lines compared to the STL’s extensive code. The author emphasizes that creating a library from scratch allows for flexibility in design and the ability to ensure compatibility with future versions.\n\nOne unique aspect of pystd is its approach to versioning. Each year, a new version of the library is created, allowing old and new versions to coexist. This ensures that users can maintain compatibility with older code while adopting newer features at their own pace.\n\nOverall, the project highlights the potential for creating a more efficient and user-friendly alternative to the traditional C++ standard library.",
      "ko": "유시 파카넨은 \"pystd\"라는 맞춤형 C++ 표준 라이브러리를 만드는 과정에서의 도전과 결정에 대해 이야기합니다. 기존의 C++ 표준 라이브러리(STL)는 인상적이지만, 긴 컴파일 시간과 가독성 문제와 같은 단점이 있습니다. 파카넨은 STL을 비활성화하고 이러한 문제를 해결하는 새로운 라이브러리를 구축할 수 있다고 제안합니다.\n\n이 라이브러리는 파일 처리, 문자열 조작, UTF-8 검증, 정렬과 같은 필수 기능을 제공하면서도 코드베이스를 최소화하는 것을 목표로 합니다. 전체 코드가 1,000줄 이하로, STL의 방대한 코드와 비교됩니다. 저자는 처음부터 라이브러리를 만드는 것이 디자인의 유연성을 제공하고 향후 버전과의 호환성을 보장할 수 있다고 강조합니다.\n\npystd의 독특한 점 중 하나는 버전 관리 방식입니다. 매년 새로운 버전이 생성되어 구버전과 신버전이 공존할 수 있습니다. 이를 통해 사용자는 오래된 코드와의 호환성을 유지하면서도 새로운 기능을 자신이 원하는 속도로 도입할 수 있습니다.\n\n이 프로젝트는 전통적인 C++ 표준 라이브러리에 비해 더 효율적이고 사용자 친화적인 대안을 만들 수 있는 가능성을 보여줍니다.",
      "ja": "ユッシ・パッカネンは、独自のC++標準ライブラリ「pystd」を作成する際の課題や決定について述べています。既存のC++標準ライブラリ（STL）は優れていますが、コンパイル時間が長く、可読性に問題があると指摘しています。パッカネンは、STLを無効にして、これらの問題に対処する新しいライブラリを構築することが可能だと提案しています。\n\nこのライブラリは、ファイル処理、文字列操作、UTF-8の検証、ソートなどの基本的な機能を提供しつつ、コードベースを最小限に抑えています。具体的には、STLの膨大なコードに対して1,000行未満に収めることを目指しています。著者は、ゼロからライブラリを作成することで、設計の柔軟性を持ち、将来のバージョンとの互換性を確保できると強調しています。\n\npystdのユニークな点の一つは、バージョン管理のアプローチです。毎年新しいバージョンが作成され、古いバージョンと新しいバージョンが共存できるようになっています。これにより、ユーザーは古いコードとの互換性を保ちながら、新しい機能を自分のペースで取り入れることができます。\n\n全体として、このプロジェクトは、従来のC++標準ライブラリに代わる、より効率的で使いやすい選択肢を作成する可能性を示しています。"
    }
  },
  {
    "id": "29803211a6f61815",
    "title": {
      "en": "German parliament votes as a Git contribution graph",
      "ko": "독일 의회, 기트 그래프로 투표!",
      "ja": "ドイツ議会の投票グラフ"
    },
    "type": "story",
    "url": "https://abstimmung.eu/git/2024",
    "score": 424,
    "by": "mxschll",
    "time": 1742858943,
    "content": "Abstimmungen 2024Namentliche Abstimmungen im BundestagJanFebMärAprMaiJunJulAugSepOktNovDezMoMiFrWenigerMehr17. Januar 2024Jahresbericht 2022 der Wehrbeauftragten (Entschließungsantrag)VerteidigungAußenpolitikUkraineBundeswehrMilitärDie Abstimmung betrifft den Entschließungsantrag der CDU/CSU-Fraktion zum Jahresbericht 2022 der Wehrbeauftragten, der die Bundesregierung auffordert, einsatzbereite TAURUS-Marschflugkörper an die Ukraine zu liefern und diese unverzüglich nachzubeschaffen.Ergebnisse des BundestagsJa: 178(27%)Nein: 485(73%)Enthaltung: 3(0%)17. Jan. 2024Gesamtstimmen: 66618. Januar 2024Entlastung der LandwirtschaftLandwirtschaftWirtschaftSteuerpolitikAgrarpolitikInnovationBürokratieDer Antrag der CDU/CSU-Fraktion betrifft die Unterstützung der Landwirtschaft und zielt darauf ab, die Wettbewerbsfähigkeit der Landwirtschaft zu erhalten und zu verbessern, anstatt sie durch politische Maßnahmen zu schwächen. Der Antrag fordert die Bundesregierung auf, Maßnahmen zu ergreifen, um die Landwirtschaft zu fördern und zu entlasten.Ergebnisse des BundestagsJa: 188(28%)Nein: 420(62%)Enthaltung: 69(10%)18. Jan. 2024Gesamtstimmen: 677Friedensinitiative für die Ukraine und Russland (Beschlussempfehlung)AußenpolitikSicherheitspolitikFriedenspolitikUkraineRusslandDie Abstimmung betrifft einen Antrag der AfD-Fraktion, der eine Friedensinitiative mit Sicherheitsgarantien für die Ukraine und Russland fordert, um Deutschlands Verantwortung für den Frieden in Europa gerecht zu werden.Ergebnisse des BundestagsJa: 605(89%)Nein: 75(11%)Enthaltung: 2(0%)18. Jan. 2024Gesamtstimmen: 68219. Januar 2024Modernisierung des Staatsangehörigkeitsrechts (GesEntw BReg)StaatsangehörigkeitIntegrationRechtGesellschaftMigrationDie Abstimmung betrifft die Modernisierung des Staatsangehörigkeitsrechts (StARModG), um den Zugang zur deutschen Staatsangehörigkeit zu erleichtern und die Integration von Ausländern zu fördern.Ergebnisse des BundestagsJa: 382(60%)Nein: 234(37%)Enthaltung: 23(3%)19. Jan. 2024Gesamtstimmen: 63901. Februar 2024Änderung des BundeswahlgesetzesWahlrechtInnenpolitikBevölkerungsentwicklungKommunalpolitikGesetzgebungDie Abstimmung betrifft die Änderung des Bundeswahlgesetzes, um die Wahlkreiseinteilung an die aktuelle Bevölkerungsentwicklung anzupassen und kommunale Gebietsänderungen zu berücksichtigen. Ziel ist es, sicherzustellen, dass die Wahl zum Deutschen Bundestag auf einer fairen und gleichmäßigen Grundlage stattfindet.Ergebnisse des BundestagsJa: 382(57%)Nein: 276(41%)Enthaltung: 12(2%)1. Feb. 2024Gesamtstimmen: 67022. Februar 2024Zehn Jahre russischer Krieg gegen die UkraineUkraineRusslandSicherheitspolitikAußenpolitikMilitärVölkerrechtDer Antrag betrifft die Unterstützung der Ukraine angesichts des russischen Krieges und zielt darauf ab, die deutsche und europäische Sicherheit zu stärken sowie die Verantwortlichen für Kriegsverbrechen zur Rechenschaft zu ziehen.Ergebnisse des BundestagsJa: 381(57%)Nein: 284(43%)Enthaltung: 2(0%)22. Feb. 2024Gesamtstimmen: 667Deutsche Außen- und Sicherheitspolitik (Antrag CDU/CSU)AußenpolitikSicherheitspolitikVerteidigungInternationale BeziehungenBevölkerungsschutzDer Antrag der CDU/CSU betrifft eine Neuausrichtung der deutschen Außen- und Sicherheitspolitik, um auf die veränderte Bedrohungslage durch Russland zu reagieren und Deutschlands Rolle in Europa und der Welt zu stärken. Er fordert eine \"echte Zeitenwende\" mit konkreten Maßnahmen zur Stärkung der Verteidigungsfähigkeit, des Bevölkerungsschutzes und der internationalen Zusammenarbeit.Ergebnisse des BundestagsJa: 182(27%)Nein: 479(72%)Enthaltung: 5(1%)22. Feb. 2024Gesamtstimmen: 66623. Februar 2024Wachstumschancengesetz;Kfz-Haftpflichtversicherung (BeschlEmpf Vermittlungsausschuss)WirtschaftSteuerrechtInvestitionInnovationFinanzenDie Beschlussempfehlung des Vermittlungsausschusses betrifft das Gesetz zur Stärkung von Wachstumschancen, Investitionen und Innovation sowie Steuervereinfachung und Steuerfairness (Wachstumschancengesetz). Sie zielt darauf ab, Änderungen an diesem Gesetz vorzunehmen, um Wachstum, Investitionen und Innovation zu fördern sowie das Steuersystem zu vereinfachen und fairer zu gestalten.Ergebnisse des BundestagsJa: 376(58%)Nein: 267(41%)Enthaltung: 1(1%)23. Feb. 2024Gesamtstimmen: 644Cannabisgesetz (GesEntwurf BReg)GesundheitDrogenpolitikJugendschutzJustizSicherheitDas Cannabisgesetz regelt den kontrollierten Umgang mit Cannabis und ändert weitere Vorschriften. Es zielt darauf ab, den Gesundheitsschutz zu verbessern, den illegalen Markt einzudämmen und den Kinder- und Jugendschutz zu stärken.Ergebnisse des BundestagsJa: 404(64%)Nein: 226(36%)Enthaltung: 4(0%)23. Feb. 2024Gesamtstimmen: 634Bundeswehreinsatz EUNAVFOR ASPIDES (BeschlEmpf)AußenpolitikSicherheitspolitikEuropäische UnionMilitärInternationaler HandelDie Abstimmung betrifft die Beteiligung bewaffneter deutscher Streitkräfte an der von der Europäischen Union geführten Operation EUNAVFOR ASPIDES, die zum Schutz der Schifffahrt im Roten Meer und angrenzenden Gebieten beitragen soll. Der Bundestag stimmt über den Antrag der Bundesregierung zur Entsendung von bis zu 700 Soldaten bis maximal zum 28. Februar 2025 ab.Ergebnisse des BundestagsJa: 538(94%)Nein: 31(5%)Enthaltung: 4(1%)23. Feb. 2024Gesamtstimmen: 57314. März 2024Ablehnung von Lieferung des Taurus-Marschflugkörpers (Beschlussempfehlung)AußenpolitikVerteidigungSicherheitspolitikUkraineMilitärDie Abstimmung betrifft den Antrag der CDU/CSU, die Bundesregierung aufzufordern, der Ukraine Taurus-Marschflugkörper aus Bundeswehrbeständen zu liefern, die Integration des Waffensystems in ukrainische Flugzeuge zu unterstützen, ukrainische Soldaten auszubilden, Hemmnisse bei der Weitergabe sensibler Informationen zu beseitigen, Ausrüstungslücken bei der Bundeswehr durch Nachbeschaffung zu schließen, die Produktionskapazitäten der Industrie zu erhöhen und weitere Taurus-Marschflugkörper zu beschaffen. Der Antrag wird abgelehnt.Ergebnisse des BundestagsJa: 494(72%)Nein: 188(27%)Enthaltung: 5(1%)14. März 2024Gesamtstimmen: 68720. März 2024Steuervergünstigung für Agrardiesel (Antrag AfD)LandwirtschaftSteuerpolitikSubventionenHaushaltspolitikAgrardieselDie Abstimmung betrifft den Antrag der AfD-Fraktion, die Steuervergünstigung für Agrardiesel beizubehalten, um die deutsche Landwirtschaft nicht zusätzlich zu belasten und die Lebensmittelpreise nicht künstlich zu verteuern. Der Antrag sieht vor, Kürzungen in anderen Bereichen des Haushalts vorzunehmen, falls Einsparungen notwendig sind, und die Bundesregierung aufzufordern, Beschlüsse der Parlamentsgremien zu respektieren.Ergebnisse des BundestagsJa: 553(90%)Nein: 64(10%)Enthaltung: 0(0%)20. März 2024Gesamtstimmen: 61721. März 2024Bundeswehreinsatz SEA GUARDIAN im Mittelmeer (Antrag BReg)SicherheitspolitikMilitärNATOMittelmeerTerrorismusbekämpfungInternationale BeziehungenDie Abstimmung betrifft die Fortsetzung der Beteiligung bewaffneter deutscher Streitkräfte an der NATO-geführten maritimen Sicherheitsoperation SEA GUARDIAN (MSO SG) im Mittelmeer, um zur Sicherheit und Stabilität in der Region beizutragen. Der Deutsche Bundestag stimmt über den Antrag der Bundesregierung zur Verlängerung dieses Einsatzes ab.Ergebnisse des BundestagsJa: 563(84%)Nein: 109(16%)Enthaltung: 2(0%)21. März 2024Gesamtstimmen: 674Bundeswehreinsatz in der Republik Südsudan (UNMISS)AußenpolitikSicherheitspolitikVerteidigungHumanitäre HilfeFriedensmissionUNDie Abstimmung betrifft die Fortsetzung der Beteiligung bewaffneter deutscher Streitkräfte an der UN-Friedensmission UNMISS (United Nations Mission in South Sudan) in der Republik Südsudan. Es geht darum, ob der Bundestag der von der Bundesregierung beschlossenen Verlängerung des Einsatzes zustimmt.Ergebnisse des BundestagsJa: 610(92%)Nein: 48(7%)Enthaltung: 2(1%)21. März 2024Gesamtstimmen: 66012. April 2024Selbstbestimmung in Bezug auf den GeschlechtseintragGesellschaftspolitikGleichstellungSelbstbestimmungTransgenderRechtDie Abstimmung betrifft den Entwurf eines Gesetzes über die Selbstbestimmung in Bezug auf den Geschlechtseintrag und zur Änderung weiterer Vorschriften. Das Gesetz zielt darauf ab, die Regelungen zur Änderung des Geschlechtseintrags zu vereinfachen und zu vereinheitlichen.Ergebnisse des BundestagsJa: 372(59%)Nein: 251(40%)Enthaltung: 11(1%)12. Apr. 2024Gesamtstimmen: 634Änderung des EnergiewirtschaftsgesetzesEnergiewirtschaftWasserstoffInfrastrukturNetzentwicklungFinanzierungDie Abstimmung betrifft ein Gesetz zur Änderung des Energiewirtschaftsgesetzes (EnWG), um den Aufbau einer nationalen Wasserstoffinfrastruktur zu fördern und die Finanzierung des Wasserstoff-Kernnetzes zu regeln.Ergebnisse des BundestagsJa: 347(61%)Nein: 202(36%)Enthaltung: 16(3%)12. Apr. 2024Gesamtstimmen: 56525. April 2024Operation EUNAVFOR MED IRINI (Beschlussempfehlung)AußenpolitikSicherheitspolitikEuropäische UnionMilitärLibyenDie Abstimmung betrifft die Fortsetzung der Beteiligung bewaffneter deutscher Streitkräfte an der Operation EUNAVFOR MED IRINI der Europäischen Union. Ziel ist es, das Waffenembargo gegen Libyen weiterhin durchzusetzen und zur Stabilisierung des Landes beizutragen.Ergebnisse des BundestagsJa: 554(84%)Nein: 100(15%)Enthaltung: 2(1%)25. Apr. 2024Gesamtstimmen: 65626. April 2024Erneuerbare-Energien-Gesetz (Solarpaket)Erneuerbare EnergienPhotovoltaikKlimaschutzEnergiewendeBürokratieabbauNetzausbauDas Gesetz zur Änderung des Erneuerbare-Energien-Gesetzes (EEG) zielt darauf ab, den Ausbau der Photovoltaik (PV) in Deutschland zu beschleunigen und zu steigern, um die Klimaziele zu erreichen. Es werden Anreize für Solaranlagen geschaffen und Bürokratie abgebaut.Ergebnisse des BundestagsJa: 384(58%)Nein: 79(12%)Enthaltung: 200(30%)26. Apr. 2024Gesamtstimmen: 66316. Mai 2024Ablehnung eines Antrags gegen den WHO-PandemievertragGesundheitspolitikInternationale BeziehungenSouveränitätWHOPandemieDemokratieDie Abstimmung betrifft den Antrag der AfD-Fraktion, den WHO-Pandemievertrag und die überarbeiteten Internationalen Gesundheitsvorschriften abzulehnen. Der Antrag zielt darauf ab, die Souveränität Deutschlands in Gesundheitsfragen zu wahren und eine vermeintliche Machtausweitung der WHO zu verhindern.Ergebnisse des BundestagsJa: 578(89%)Nein: 71(11%)Enthaltung: 1(0%)16. Mai 2024Gesamtstimmen: 65005. Juni 2024Ablehnung eines Antrags zur Abschaffung des SolidaritätszuschlagsSteuerpolitikFinanzenWirtschaftSolidaritätszuschlagAbgabenlastSteuerreformDie Abstimmung betrifft den Antrag der AfD-Fraktion zur Abschaffung des Solidaritätszuschlags als ersten Schritt einer umfassenden Steuerreform zur Entlastung von Mittelstand, Unternehmen und Arbeitnehmern. Der Finanzausschuss empfiehlt die Ablehnung des Antrags.Ergebnisse des BundestagsJa: 537(90%)Nein: 62(10%)Enthaltung: 0(0%)5. Juni 2024Gesamtstimmen: 59906. Juni 2024Ablehnung eines Antrags zur Bekämpfung des politischen IslamInneres und HeimatPolitischer IslamDemokratieRechtSicherheitMigrationDie Abstimmung betrifft den Antrag der CDU/CSU-Fraktion, den politischen Islam als Gefahr für die freiheitliche Demokratie wirksam zu bekämpfen. Der Ausschuss für Inneres und Heimat empfiehlt die Ablehnung des Antrags.Ergebnisse des BundestagsJa: 406(63%)Nein: 241(37%)Enthaltung: 1(0%)6. Juni 2024Gesamtstimmen: 648Beschleunigung immissionsschutzrechtlicher GenehmigungsverfahrenUmweltKlimaschutzEnergieWirtschaftGenehmigungsverfahrenErneuerbare EnergienDie Abstimmung betrifft ein Gesetz zur Verbesserung des Klimaschutzes, zur Beschleunigung von Genehmigungsverfahren im Immissionsschutz und zur Umsetzung von EU-Recht, mit dem Ziel, Verfahren zu vereinfachen und den Ausbau erneuerbarer Energien zu fördern.Ergebnisse des BundestagsJa: 377(59%)Nein: 257(40%)Enthaltung: 9(1%)6. Juni 2024Gesamtstimmen: 64313. Juni 2024Antrag zu Konsequenzen aus dem Terror von MannheimInnere SicherheitMigrationIntegrationTerrorismusbekämpfungRechtspolitikDer Antrag der CDU/CSU-Fraktion betrifft die Forderung nach klaren Konsequenzen aus dem Terroranschlag von Mannheim und zielt darauf ab, die Migrations- und Integrationspolitik zu verschärfen sowie die Sicherheitsmaßnahmen zu erhöhen.Ergebnisse des BundestagsJa: 253(38%)Nein: 399(60%)Enthaltung: 11(2%)13. Juni 2024Gesamtstimmen: 663Ablehnung eines Antrags zum Verbot des Vereins Muslim InteraktivInneres und HeimatVereinsrechtExtremismusIslamismusSicherheitspolitikDie Abstimmung betrifft den Antrag der AfD-Fraktion, den Verein Muslim Interaktiv zu verbieten, da dieser als extremistisch und gegen die freiheitliche demokratische Grundordnung gerichtet angesehen wird. Der Bundestag lehnt den Antrag ab.Ergebnisse des BundestagsJa: 577(88%)Nein: 68(10%)Enthaltung: 11(2%)13. Juni 2024Gesamtstimmen: 656Antrag zum Eintritt in die zweite Beratung zur Aufhebung des LieferkettengesetzesWirtschaftRechtSozialesEuropäische UnionDie Abstimmung betrifft einen Antrag der CDU/CSU-Fraktion, direkt in die zweite Beratung eines Gesetzentwurfs zur Aufhebung des Lieferkettensorgfaltspflichtengesetzes (LkSG) einzutreten.Ergebnisse des BundestagsJa: 250(38%)Nein: 401(62%)Enthaltung: 1(0%)13. Juni 2024Gesamtstimmen: 65227. Juni 2024Bundeswehreinsatz EUFOR ALTHEAAußenpolitikSicherheitspolitikEuropäische UnionBundeswehrBosnien und HerzegowinaDie Abstimmung betrifft die Fortsetzung der Beteiligung bewaffneter deutscher Streitkräfte an der EU-geführten Sicherheitsoperation EUFOR ALTHEA in Bosnien und Herzegowina bis zum 30. Juni 2025.Ergebnisse des BundestagsJa: 553(87%)Nein: 74(12%)Enthaltung: 8(1%)27. Juni 2024Gesamtstimmen: 635Bundeswehreinsatz im Kosovo (KFOR)AußenpolitikSicherheitspolitikVerteidigungKosovoBundeswehrInternationale BeziehungenDie Abstimmung betrifft die Fortsetzung der Beteiligung bewaffneter deutscher Streitkräfte an der internationalen Sicherheitspräsenz im Kosovo (KFOR). Es geht darum, das Mandat für den Einsatz der Bundeswehr im Rahmen von KFOR zu verlängern.Ergebnisse des BundestagsJa: 535(85%)Nein: 91(14%)Enthaltung: 2(1%)27. Juni 2024Gesamtstimmen: 628Bundeswehreinsatz im Libanon (UNIFIL)AußenpolitikSicherheitspolitikBundeswehrLibanonUNIFILFriedensmissionDie Abstimmung betrifft die Fortsetzung der Beteiligung bewaffneter deutscher Streitkräfte an der \"United Nations Interim Force in Lebanon\" (UNIFIL), einer Friedensmission der Vereinten Nationen im Libanon. Es geht um die Verlängerung des Mandats und die damit verbundenen Aufgaben und Befugnisse der deutschen Soldaten.Ergebnisse des BundestagsJa: 534(85%)Nein: 88(14%)Enthaltung: 3(1%)27. Juni 2024Gesamtstimmen: 62505. Juli 2024Änderung des SchwangerschaftskonfliktgesetzesSozialesFamilieFrauenrechteGesundheitRechtDie Abstimmung betrifft die Änderung des Schwangerschaftskonfliktgesetzes (SchKG) und zielt darauf ab, einen bundeseinheitlichen und rechtssicheren Umgang mit sogenannten Gehsteigbelästigungen vor Schwangerschaftsberatungsstellen und Einrichtungen zur Vornahme von Schwangerschaftsabbrüchen sicherzustellen.Ergebnisse des BundestagsJa: 381(69%)Nein: 169(31%)Enthaltung: 0(0%)5. Juli 2024Gesamtstimmen: 55017. Oktober 2024Entwurf zur Aufhebung des LieferkettengesetzesWirtschaftMenschenrechteUmweltLieferkettenGesetzgebungEuropäische UnionDie Abstimmung betrifft den Gesetzentwurf der CDU/CSU zur Aufhebung des Lieferkettensorgfaltspflichtengesetzes (LkSG), das Unternehmen zur Einhaltung von Menschenrechten in ihren Lieferketten verpflichtet.Ergebnisse des BundestagsJa: 247(37%)Nein: 412(62%)Enthaltung: 1(1%)17. Okt. 2024Gesamtstimmen: 660Gesetzentwurf zur KrankenhausreformGesundheitKrankenhausVersorgungFinanzierungQualitätssicherungDie Abstimmung betrifft den Entwurf eines Gesetzes zur Verbesserung der Versorgungsqualität im Krankenhaus und zur Reform der Vergütungsstrukturen (KHVVG), das darauf abzielt, die Krankenhausversorgung zu verbessern und die Finanzierung zu reformieren.Ergebnisse des BundestagsJa: 373(57%)Nein: 285(43%)Enthaltung: 1(0%)17. Okt. 2024Gesamtstimmen: 659Bundeswehreinsatz im IrakAußenpolitikSicherheitspolitikMilitärIrakTerrorismusbekämpfungDie Abstimmung betrifft die Fortsetzung des Einsatzes bewaffneter deutscher Streitkräfte im Irak, um die Stabilisierung des Landes zu sichern, ein Wiedererstarken des IS (Islamischer Staat) zu verhindern und die Versöhnung im Irak zu fördern. Der Bundestag stimmt über den Antrag der Bundesregierung ab, diesen Einsatz bis zum 31. Januar 2026 mit bis zu 500 Soldatinnen und Soldaten zu verlängern.Ergebnisse des BundestagsJa: 539(85%)Nein: 93(15%)Enthaltung: 2(0%)17. Okt. 2024Gesamtstimmen: 63418. Oktober 2024Änderungsantrag zur „Verbesserung der inneren Sicherheit“Innere SicherheitAsylrechtMigrationspolitikGrenzschutzTerrorismusbekämpfungWaffenrechtDie Abstimmung betrifft einen Änderungsantrag der CDU/CSU-Fraktion zu einem Gesetzentwurf, der die innere Sicherheit und das Asylsystem verbessern soll, insbesondere durch Änderungen im Asylgesetz und im Ausweisungsrecht.Ergebnisse des BundestagsJa: 254(39%)Nein: 404(61%)Enthaltung: 1(0%)18. Okt. 2024Gesamtstimmen: 659Artikel 5 des Entwurfs „Verbesserung der inneren Sicherheit“ (2. Beratung)InneresSicherheitWaffenrechtExtremismusTerrorismusÖffentliche SicherheitDie Abstimmung betrifft Artikel 5 eines Gesetzentwurfs, der darauf abzielt, das Waffenrecht zu verschärfen, um Extremisten und Terroristen den Zugang zu Waffen zu erschweren und die öffentliche Sicherheit zu erhöhen.Ergebnisse des BundestagsJa: 375(57%)Nein: 275(42%)Enthaltung: 4(1%)18. Okt. 2024Gesamtstimmen: 654Gesetzentwurf „Verbesserung der Terrorismusbekämpfung“Innere SicherheitTerrorismusbekämpfungExtremismuspräventionDatenschutzBürgerrechteWaffenrechtDer Gesetzentwurf zur Verbesserung der Terrorismusbekämpfung zielt darauf ab, den Sicherheitsbehörden im digitalen Raum zusätzliche Befugnisse zu geben und die Zusammenarbeit zwischen Bund, Ländern und zivilgesellschaftlichen Akteuren zu stärken, um extremistischen und islamistischen Tendenzen entgegenzuwirken.Ergebnisse des BundestagsJa: 367(56%)Nein: 280(43%)Enthaltung: 4(1%)18. Okt. 2024Gesamtstimmen: 651Gesetzentwurf „Verbesserung der inneren Sicherheit“Innere SicherheitAsylrechtWaffenrechtExtremismusTerrorismusbekämpfungDer Gesetzentwurf zur Verbesserung der inneren Sicherheit und des Asylsystems betrifft die Anpassung von Gesetzen, um die innere Sicherheit Deutschlands zu stärken und das Asylsystem effektiver zu gestalten. Er reagiert auf aktuelle Bedrohungen und soll den Behörden mehr Handlungsspielraum geben.Ergebnisse des BundestagsJa: 357(55%)Nein: 289(44%)Enthaltung: 8(1%)18. Okt. 2024Gesamtstimmen: 654Erster Entschließungsantrag zur „Verbesserung der inneren Sicherheit“Innere SicherheitAsylpolitikMigrationspolitikWaffenrechtIslamismusExtremismusbekämpfungDie Abstimmung betrifft den Entschließungsantrag der CDU/CSU zum Gesetzentwurf zur Verbesserung der inneren Sicherheit und des Asylsystems, der sich mit der Migrationspolitik, dem Waffenrecht und der Bekämpfung des Islamismus auseinandersetzt.Ergebnisse des BundestagsJa: 237(36%)Nein: 405(62%)Enthaltung: 8(2%)18. Okt. 2024Gesamtstimmen: 650Zweiter Entschließungsantrag zur „Verbesserung der inneren Sicherheit“Innere SicherheitAsylpolitikWaffenrechtMigrationspolitikExtremismuspräventionDie Abstimmung betrifft einen Entschließungsantrag der CDU/CSU zum Gesetzentwurf zur Verbesserung der inneren Sicherheit und des Asylsystems, der sich gegen die Schwerpunkte der Regierungsfraktionen im Bereich Waffenrecht richtet und stattdessen eine andere Migrations- und Sicherheitspolitik fordert.Ergebnisse des BundestagsJa: 185(29%)Nein: 373(59%)Enthaltung: 73(12%)18. Okt. 2024Gesamtstimmen: 631Dritter Entschließungsantrag zur „Verbesserung der inneren Sicherheit“Innere SicherheitAsylrechtMigrationGrenzschutzEU-RechtDie Abstimmung betrifft einen Entschließungsantrag der CDU/CSU-Fraktion zu einem Gesetzentwurf, der die innere Sicherheit und das Asylsystem verbessern soll. Der Antrag zielt darauf ab, die Bundesregierung zu umfassenden Zurückweisungen von Personen ohne Einreiserecht an den deutschen Grenzen aufzufordern.Ergebnisse des BundestagsJa: 249(39%)Nein: 394(61%)Enthaltung: 1(0%)18. Okt. 2024Gesamtstimmen: 644Ablehnung eines Antrags zu MesserangriffenInneresRechtKriminalitätWaffenrechtMigrationSicherheitDie Beschlussempfehlung des Ausschusses für Inneres und Heimat betrifft die Ablehnung eines Antrags der AfD-Fraktion, der eine gezielte Sanktionierung von Messerangriffen anstelle von Verschärfungen im Waffenrecht fordert.Ergebnisse des BundestagsJa: 567(89%)Nein: 62(10%)Enthaltung: 7(1%)18. Okt. 2024Gesamtstimmen: 636Ablehnung eines Antrags zu DrittstaatenangehörigenAsylrechtMigrationspolitikGrenzkontrolleRücknahmeabkommenDie Abstimmung betrifft den Antrag der AfD-Fraktion, die Bundesregierung zu verpflichten, Drittstaatsangehörige an den Außengrenzen Deutschlands zurückzuweisen und bilaterale Rücknahmeabkommen mit Nachbarstaaten anzuwenden.Ergebnisse des BundestagsJa: 572(89%)Nein: 62(10%)Enthaltung: 8(1%)18. Okt. 2024Gesamtstimmen: 64216. Dezember 2024Antrag gemäß Artikel 68 des Grundgesetzes (Vertrauensfrage)PolitikRegierungGrundgesetzVertrauensfrageDie Abstimmung betrifft die Vertrauensfrage gemäß Artikel 68 des Grundgesetzes, bei der der Bundeskanzler das Vertrauen des Bundestages ausspricht. Der Bundeskanzler beabsichtigt, vor der Abstimmung eine Erklärung abzugeben.Ergebnisse des BundestagsJa: 207(29%)Nein: 394(55%)Enthaltung: 116(16%)16. Dez. 2024Gesamtstimmen: 71719. Dezember 2024Gesetzentwurf zur Änderung von Artikel 93 und 94 des Grundgesetzes (Bundesverfassungsgericht)VerfassungsrechtBundesverfassungsgerichtRechtsstaatPolitikJustizDer Gesetzentwurf betrifft die Änderung des Grundgesetzes in Bezug auf die Stellung und Struktur des Bundesverfassungsgerichts, indem Artikel 93 und 94 punktuell ergänzt und deren Inhalte systematisch neu geordnet werden. Ziel ist es, die Organqualität des Bundesverfassungsgerichts sowie die unmittelbare Bindung der öffentlichen Gewalt an seine Entscheidungen stärker hervorzuheben.Ergebnisse des BundestagsJa: 600(90%)Nein: 69(10%)Enthaltung: 0(0%)19. Dez. 2024Gesamtstimmen: 669",
    "summary": {
      "en": "Here’s a simplified summary of the key votes in the Bundestag for 2024:\n\n1. **January 17**: \n   - **Wehrbeauftragten's Annual Report**: CDU/CSU requested arms for Ukraine. Results: 178 Yes, 485 No.\n   - **Support for Agriculture**: CDU/CSU aimed to improve agricultural competitiveness. Results: 188 Yes, 420 No.\n   - **Peace Initiative for Ukraine/Russia**: AfD proposed a peace initiative. Results: 605 Yes, 75 No.\n\n2. **January 19**: \n   - **Modernization of Citizenship Law**: Aimed to ease citizenship access. Results: 382 Yes, 234 No.\n\n3. **February 1**: \n   - **Amendment of Federal Election Law**: Aimed for fair election districting. Results: 382 Yes, 276 No.\n\n4. **February 22**: \n   - **Support for Ukraine Against Russia**: Aimed to strengthen support for Ukraine. Results: 381 Yes, 284 No.\n   - **CDU/CSU Proposal for Foreign and Security Policy**: Called for a shift in policy due to threats from Russia. Results: 182 Yes, 479 No.\n\n5. **February 23**: \n   - **Growth Opportunities Law**: Aimed to promote investments and simplify taxes. Results: 376 Yes, 267 No.\n   - **Cannabis Law**: Regulated cannabis use for health and safety. Results: 404 Yes, 226 No.\n\n6. **March 14**: \n   - **Taurus Missiles for Ukraine**: CDU/CSU proposed arms support. Results: 494 Yes, 188 No.\n\n7. **March 20**: \n   - **Tax Relief for Agricultural Diesel**: AfD sought to maintain tax relief. Results: 553 Yes, 64 No.\n\n8. **March 21**: \n   - **Bundeswehr Deployment in the Mediterranean**: Approved continued military participation. Results: 563 Yes, 109 No.\n\n9. **April 12**: \n   - **Self-determination in Gender Registration**: Simplified gender registration laws. Results: 372 Yes, 251 No.\n\n10. **April 25**: \n    - **Operation EUNAVFOR MED IRINI**: Approved continued military involvement in Libya. Results: 554 Yes, 100 No.\n\n11. **April 26**: \n    - **Renewable Energy Law**: Aimed to speed up solar energy expansion. Results: 384 Yes, 79 No.\n\n12. **May 16**: \n    - **Opposition to WHO Pandemic Treaty**: AfD sought to reject the treaty. Results: 578 Yes, 71 No.\n\n13. **June 5**: \n    - **Opposition to Abolishing Solidarity Surcharge**: Results: 537 Yes, 62 No.\n\n14. **June 6**: \n    - **Opposition to Combating Political Islam**: Results: 406 Yes, 241 No.\n\n15. **June 13**: \n    - **Response to Terrorism in Mannheim**: CDU/CSU proposed stricter immigration policies. Results: 253 Yes, 399 No.\n\n16. **June 27**: \n    - **Bundeswehr Deployment in Bosnia**: Approved continued military mission. Results: 553 Yes, 74 No.\n\n17. **July 5**: \n    - **Amendment to Pregnancy Conflict Law**: Improved handling of pregnancy-related counseling. Results: 381 Yes, 169 No.\n\n18. **October 17**: \n    - **Repeal of Supply Chain Law**: Results: 247 Yes, 412 No.\n\n19. **October 18**: \n    - **Improvement of Internal Security**: Proposed changes to asylum laws. Results: 254 Yes, 404 No.\n\n20. **December 16**: \n    - **Confidence Vote for Chancellor**: Results: 207 Yes, 394 No.\n\n21. **December 19**: \n    - **Amendment to Constitutional Court Laws**: Aimed to enhance the court's role. Results: 600 Yes, 69 No.\n\nThis summary captures the main votes and their outcomes, highlighting key legislative themes throughout the year.",
      "ko": "2024년 독일 연방의회에서 주요 투표 결과를 간단히 정리하면 다음과 같습니다.\n\n1월 17일에는 **국방위원회의 연례 보고서**가 있었고, CDU/CSU는 우크라이나에 대한 무기 지원을 요청했습니다. 투표 결과는 찬성 178표, 반대 485표였습니다. **농업 지원**에 대한 논의도 있었으며, CDU/CSU는 농업 경쟁력 향상을 목표로 했습니다. 이 투표는 찬성 188표, 반대 420표로 끝났습니다. **우크라이나와 러시아를 위한 평화 이니셔티브**에 대해 AfD가 제안했으며, 결과는 찬성 605표, 반대 75표였습니다.\n\n1월 19일에는 **시민권 법 현대화**가 논의되었고, 시민권 접근성을 완화하는 방향으로 진행되었습니다. 찬성 382표, 반대 234표로 통과했습니다.\n\n2월 1일에는 **연방 선거법 개정안**이 논의되었고, 공정한 선거구 조정을 목표로 했습니다. 찬성 382표, 반대 276표로 통과했습니다.\n\n2월 22일에는 **우크라이나에 대한 지원 강화**가 논의되었고, 찬성 381표, 반대 284표로 통과했습니다. 같은 날 CDU/CSU는 러시아의 위협에 대응하기 위한 외교 및 안보 정책 전환을 요구했으나, 찬성 182표, 반대 479표로 부결되었습니다.\n\n2월 23일에는 **성장 기회 법안**이 논의되어 투자 촉진과 세금 간소화를 목표로 했습니다. 찬성 376표, 반대 267표로 통과했습니다. **대마초 법안**도 논의되어 건강과 안전을 위한 대마초 사용 규제를 다루었고, 찬성 404표, 반대 226표로 통과했습니다.\n\n3월 14일에는 **우크라이나에 대한 타우루스 미사일 지원**이 논의되었고, 찬성 494표, 반대 188표로 통과했습니다.\n\n3월 20일에는 **농업용 디젤 세금 감면**에 대한 AfD의 요청이 있었고, 찬성 553표, 반대 64표로 통과했습니다.\n\n3월 21일에는 **지중해에서의 연방군 배치**가 승인되었고, 찬성 563표, 반대 109표로 통과했습니다.\n\n4월 12일에는 **성별 등록의 자기 결정권**이 논의되어 성별 등록 법을 간소화했습니다. 찬성 372표, 반대 251표로 통과했습니다.\n\n4월 25일에는 **EUNAVFOR MED IRINI 작전**이 승인되어 리비아에서의 군사적 참여가 계속되었습니다. 찬성 554표, 반대 100표로 통과했습니다.\n\n4월 26일에는 **재생 가능 에너지 법안**이 논의되어 태양광 에너지 확대를 가속화하는 방향으로 진행되었습니다. 찬성 384표, 반대 79표로 통과했습니다.\n\n5월 16일에는 **WHO 팬데믹 조약 반대**가 논의되었고, AfD는 이 조약을 거부할 것을 요청했습니다. 찬성 578표, 반대 71표로 통과했습니다.\n\n6월 5일에는 **연대세 폐지 반대**가 논의되어 찬성 537표, 반대 62표로 통과했습니다.\n\n6월 6일에는 **정치적 이슬람과의 전투 반대**가 논의되었고, 찬성 406표, 반대 241표로 통과했습니다.\n\n6월 13일에는 **만하임에서의 테러 대응**에 대한 CDU/CSU의 제안이 있었으나, 찬성 253표, 반대 399표로 부결되었습니다.\n\n6월 27일에는 **보스니아에서의 연방군 배치**가 승인되었고, 찬성 553표, 반대 74표로 통과했습니다.\n\n7월 5일에는 **임신 갈등 법 개정안**이 논의되어 임신 관련 상담 처리를 개선하는 방향으로 진행되었습니다. 찬성 381표, 반대 169표로 통과했습니다.\n\n10월 17일에는 **공급망 법 폐지**가 논의되었고, 찬성 247표, 반대 412표로 부결되었습니다.\n\n10월 18일에는 **내부 안전 개선**을 위한 제안이 있었으나, 찬성 254표, 반대 404표로 부결되었습니다.\n\n12월 16일에는 **총리 신뢰 투표**가 있었고, 찬성 207표, 반대 394표로 부결되었습니다.\n\n12월 19일에는 **헌법재판소 법 개정안**이 논의되어 법원의 역할을 강화하는 방향으로 진행되었습니다. 찬성 600표, 반대 69표로 통과했습니다.\n\n이 요약은 연간 주요 투표와 그 결과를 담고 있으며, 주요 입법 주제를 강조하고 있습니다.",
      "ja": "2024年のドイツ連邦議会（ブンデスタク）での重要な投票結果をまとめました。\n\n1月17日には、CDU/CSUがウクライナへの武器供与を求める「軍事監察官の年次報告」が行われ、賛成178票、反対485票という結果になりました。また、農業競争力を向上させるための支援が提案され、賛成188票、反対420票でした。一方、AfDが提案したウクライナとロシアの平和イニシアティブは、賛成605票、反対75票で可決されました。\n\n1月19日には、市民権法の近代化が提案され、賛成382票、反対234票で通過しました。\n\n2月1日には、連邦選挙法の改正が行われ、公正な選挙区割りを目指し、賛成382票、反対276票でした。\n\n2月22日には、ウクライナに対するロシアの脅威に対抗するための支援が提案され、賛成381票、反対284票で可決されました。また、CDU/CSUが提案した外交・安全保障政策の変更は、賛成182票、反対479票で否決されました。\n\n2月23日には、投資促進と税制の簡素化を目指す「成長機会法」が賛成376票、反対267票で可決され、健康と安全のための大麻法も賛成404票、反対226票で通過しました。\n\n3月14日には、CDU/CSUがウクライナへのタウルスミサイル供与を提案し、賛成494票、反対188票で可決されました。\n\n3月20日には、AfDが農業用ディーゼルの税軽減を維持することを求め、賛成553票、反対64票で通過しました。\n\n3月21日には、地中海での連邦軍の派遣が承認され、賛成563票、反対109票でした。\n\n4月12日には、性別登録の自己決定を簡素化する法案が賛成372票、反対251票で可決されました。\n\n4月25日には、リビアでの軍事介入を継続する「EUNAVFOR MED IRINI」が承認され、賛成554票、反対100票でした。\n\n4月26日には、再生可能エネルギー法が提案され、太陽光発電の拡大を加速することを目指し、賛成384票、反対79票で可決されました。\n\n5月16日には、AfDがWHOのパンデミック条約に反対し、賛成578票、反対71票で否決されました。\n\n6月5日には、連帯付加税の廃止に反対する投票が行われ、賛成537票、反対62票でした。\n\n6月6日には、政治的イスラムに対抗するための提案があり、賛成406票、反対241票で可決されました。\n\n6月13日には、マンハイムでのテロ対策として、CDU/CSUが移民政策の厳格化を提案し、賛成253票、反対399票で否決されました。\n\n6月27日には、ボスニアでの連邦軍の派遣が承認され、賛成553票、反対74票でした。\n\n7月5日には、妊娠に関する相談の取り扱いを改善するための法案が賛成381票、反対169票で可決されました。\n\n10月17日には、サプライチェーン法の廃止が提案され、賛成247票、反対412票で否決されました。\n\n10月18日には、内部安全の改善に向けた亡命法の変更が提案され、賛成254票、反対404票でした。\n\n12月16日には、首相に対する信任投票が行われ、賛成207票、反対394票でした。\n\n12月19日には、憲法裁判所法の改正が提案され、裁判所の役割を強化することを目指し、賛成600票、反対69票で可決されました。\n\nこのまとめは、年間を通じての主要な投票とその結果を示し、重要な立法テーマを浮き彫りにしています。"
    }
  },
  {
    "id": "9328671e85b8c779",
    "title": {
      "en": "Closing Keynote with Leslie Lamport at Scale 22x: Coding Isn't Programming",
      "ko": "코딩은 프로그래밍이 아니다: 레슬리 램포트의 키노트",
      "ja": "レズリー・ランポートの講演: コーディングはプログラミングじゃない"
    },
    "type": "story",
    "url": "https://www.socallinuxexpo.org/scale/22x/presentations/closing-keynote-leslie-lamport",
    "score": 19,
    "by": "todsacerdoti",
    "time": 1742898863,
    "content": "Subscribe to announcements\n\nSubscribe\n\nClosing Keynote with Leslie Lamport\n\n          Leslie Lamport\n      Audience:     Everyone\n      Topic:     Keynote\n          Join us for a captivating closing keynote with the legendary Leslie Lamport, Turing Award winner and pioneer in the field of distributed computing. We'll discuss computing history, open source and distributed systems.\n\n      Presentation:      linux-expo (1).pdf\n      Room:     Ballroom DE\n      Time:     Sunday, March 9, 2025 - 15:00 to 16:00\n      Audio/Video:\n\nSubscribe to announcements\n\nSubscribe\n\nPlatinum Sponsors\n\n      Gold Sponsors\n\n      Silver Sponsors\n\n      Network Sponsors\n\n      Registration Sponsors\n\n      Reception Sponsors\n\n      Special Event Sponsors\n\n      Speaker Track Sponsors\n\n      Travel Sponsors\n\n      Media Sponsors\n\nAbout SCALE\nCode of Conduct\nContact Us\nPolicies",
    "summary": {
      "en": "Join us for the closing keynote by Leslie Lamport, a Turing Award winner known for his work in distributed computing. The talk will cover topics like computing history, open source, and distributed systems. \n\n**Details:**\n- **Date:** Sunday, March 9, 2025\n- **Time:** 15:00 to 16:00\n- **Location:** Ballroom DE\n\nVarious sponsors support the event, and more information is available about policies and conduct.",
      "ko": "레슬리 램포트의 폐막 기조 연설에 참여해 주세요. 그는 분산 컴퓨팅 분야에서의 업적으로 튜링상을 수상한 유명한 인물입니다. 이번 강연에서는 컴퓨팅 역사, 오픈 소스, 분산 시스템과 같은 주제를 다룰 예정입니다.\n\n행사는 2025년 3월 9일 일요일에 열리며, 시간은 오후 3시부터 4시까지입니다. 장소는 DE 볼룸입니다.\n\n다양한 후원사들이 이 행사를 지원하고 있으며, 정책과 행동 강령에 대한 추가 정보도 제공됩니다.",
      "ja": "レズリー・ランポートによる閉会基調講演に参加してください。彼は分散コンピューティングの分野での業績によりチューリング賞を受賞した著名な研究者です。この講演では、コンピュータの歴史やオープンソース、分散システムについての話題が取り上げられます。\n\n日時は2025年3月9日（日）で、時間は15時から16時までです。場所はボールルームDEです。\n\nこのイベントはさまざまなスポンサーの支援を受けており、ポリシーや行動規範についての詳細情報も提供されています。"
    }
  },
  {
    "id": "dd21d9bbd9e6583f",
    "title": {
      "en": "Search My Site – open-source search engine for personal and independent websites",
      "ko": "내 사이트 검색기",
      "ja": "サイト検索 - 自由な検索エンジン"
    },
    "type": "story",
    "url": "https://searchmysite.net",
    "score": 135,
    "by": "OuterVale",
    "time": 1742869143,
    "content": "Browse\n\n\t\t\t\t\t\t\t\t\tNewest\n\n\t\t\t\t\t\t\t\t\t\tRandom\n\n\t\t\t\t\t\t\tSearch real content by real people from the indieweb / the small web / digital gardens.\n\t\t\t\t\t\t\tSites are ranked higher when they have no ads.\n\t\t\t\t\t\t\tFully open source. Also a search as a service.\n\n\t\t\t\t\tAbout searchmysite.net\n\n\t\t\t\t\t\tWhat is the search engine and why should I use it?\n\n\t\t\t\t\t\tThe searchmysite.net search engine is a niche search, focussing on the \"indieweb\" or \"small web\" or \"digital gardens\", i.e. non-commercial content, primarily personal and independent websites.\n\n\t\t\t\t\t\tIf you want to research people's personal experiences of or deep-dives into certain topics, hobbies or interests, then you may find the searchmysite.net public search useful to avoid having to wade through all the marketing websites and blog spam that fill the big search engines. It differs from the commercial search engines because it:\n\n\t\t\t\t\t\tIndexes only user-submitted and moderated sites, rather than indexing the entire internet with all of its spam, \"search engine optimisation\" and \"click-bait\" content.\n\t\t\t\t\t\tDoes not show adverts and promotes advert-free results pages, to remove the incentives for spam and surveillance capitalism.\n\t\t\t\t\t\tAims to have a sustainable and user-aligned operating model, planning to pay running costs via the \"search as a service\" features, rather than relying on advertising which could put it in conflict with user needs.\n\t\t\t\t\t\tOffers an unusually high level of privacy for a search engine, thanks to the operating model which is not based on advertising or collecting personal information for sale to advertisers (see the Privacy Policy for full details).\n\t\t\t\t\t\tIs fully open source, for even greater transparency of the searching, ranking and indexing processes, and for greater community involvement in improving the service.",
    "summary": {
      "en": "**Summary of searchmysite.net**\n\nSearchmysite.net is a niche search engine focused on the \"indieweb,\" which includes personal and independent websites. It is designed for users who want to explore personal experiences and in-depth topics without the clutter of commercial ads and spam found on larger search engines. \n\nKey features include:\n\n- **User-Submitted Content**: It only indexes sites that are submitted and moderated by users, avoiding the vast spam of the entire internet.\n- **Ad-Free Experience**: There are no ads, promoting cleaner search results and reducing spam incentives.\n- **Sustainable Model**: It plans to cover operational costs through \"search as a service,\" not advertising, ensuring alignment with user needs.\n- **Privacy Focus**: It offers high privacy levels, not selling personal data to advertisers.\n- **Open Source**: The platform is fully open source, allowing for transparency and community involvement in its development.\n\nOverall, searchmysite.net is geared towards users looking for authentic, non-commercial content.",
      "ko": "searchmysite.net는 개인 웹사이트와 독립적인 웹사이트를 중심으로 한 틈새 검색 엔진입니다. 이 사이트는 상업 광고와 스팸으로 가득한 대형 검색 엔진의 혼잡함 없이 개인적인 경험과 심층 주제를 탐색하고자 하는 사용자들을 위해 설계되었습니다.\n\n주요 특징으로는 사용자 제출 콘텐츠가 있습니다. 이 검색 엔진은 사용자들이 제출하고 검토한 사이트만 색인화하여, 인터넷의 방대한 스팸을 피합니다. 광고가 없는 환경을 제공하여 더 깨끗한 검색 결과를 보여주고 스팸 유인을 줄입니다. 운영 비용은 광고가 아닌 '서비스로서의 검색'을 통해 충당할 계획으로, 사용자 요구에 맞춘 지속 가능한 모델을 지향합니다. 또한, 개인 정보를 광고주에게 판매하지 않아 높은 수준의 프라이버시를 제공합니다. 이 플랫폼은 완전한 오픈 소스이며, 개발 과정에서 투명성과 커뮤니티 참여를 허용합니다.\n\n전반적으로 searchmysite.net는 상업적이지 않은 진정한 콘텐츠를 찾는 사용자들을 위한 서비스입니다.",
      "ja": "searchmysite.netは「インディウェブ」に特化したニッチな検索エンジンで、個人や独立したウェブサイトを対象としています。このサイトは、商業広告やスパムが氾濫する大手検索エンジンではなく、個人の体験や深いテーマを探求したいユーザーのために設計されています。\n\n主な特徴としては、ユーザーが提出したコンテンツのみをインデックスすることが挙げられます。これにより、インターネット全体の膨大なスパムを避けることができます。また、広告が一切ないため、クリーンな検索結果が得られ、スパムの誘因も減少します。運営コストは「サービスとしての検索」で賄う計画であり、広告収入に依存しない持続可能なモデルを目指しています。さらに、プライバシーにも配慮しており、個人データを広告主に販売することはありません。プラットフォームは完全にオープンソースで、透明性があり、コミュニティの関与を促進しています。\n\n全体として、searchmysite.netは商業的でない本物のコンテンツを求めるユーザーに向けられています。"
    }
  },
  {
    "id": "dece63737e377a4b",
    "title": {
      "en": "Status as a Service (2019)",
      "ko": "서비스로서의 상태",
      "ja": "ステータス提供サービス"
    },
    "type": "story",
    "url": "https://www.eugenewei.com/blog/2019/2/19/status-as-a-service",
    "score": 42,
    "by": "simonebrunozzi",
    "time": 1742885984,
    "content": "Status as a Service (StaaS)\n\n          February 26, 2019\n           by Eugene Wei\n\n  Editor's Note 1: I have no editor. Editor’s Note 2: I would like to assure new subscribers to this blog that most my posts are not as long as this one. Or as long as my previous one. My long break from posting here means that this piece is a collection of what would’ve normally been a series of shorter posts. I put section titles below, so skip any that don’t interest you. My short takes are on Twitter. All that said, I apologize for nothing. Editor's Note 3: I lied, I apologize for one thing, and that is my long writing hiatus. Without a work computer, I had to resort to using my 7 year old 13\" Macbook Pro as my main computer, and sometime last year my carpal tunnel syndrome returned with a vengeance and left my wrists debilitated with pain. I believe all of you who say your main computer is a laptop or, shudder, an iPad, but goodness gracious I cannot type on a compact keyboard for long periods of time without having my hands turn into useless stumps. It was only the return to typing almost exclusively on my old friend the Kinesis Advantage 2 ergo keyboard that put me back in the game. Editor’s Note 4: I was recently on Patrick O'Shaughnessy's podcast Invest Like the Best, and near the end of that discussion, I mentioned a new essay I'd been working on about the similarities between social networks and ICO's. This is that piece.Status-Seeking Monkeys\"It is a truth universally acknowledged, that a person in possession of little fortune, must be in want of more social capital.\"So wrote Jane Austen, or she would have, I think, if she were chronicling our current age (instead we have Taylor Lorenz, and thank goodness for that).Let's begin with two principles:People are status-seeking monkeys*People seek out the most efficient path to maximizing social capital* Status-Seeking Monkeys will also be the name of my indie band, if I ever learn to play the guitar and start a bandI begin with these two observations of human nature because few would dispute them, yet I seldom see social networks, some of the largest and fastest-growing companies in the history of the world, analyzed on the dimension of status or social capital.It’s in part a measurement issue. Numbers lend an air of legitimacy and credibility. We have longstanding ways to denominate and measure financial capital and its flows. Entire websites, sections of newspapers, and a ton of institutions report with precision on the prices and movements of money.We have no such methods for measuring the values and movement of social capital, at least not with anywhere near the accuracy or precision. The body of research feels both broad and yet meager. If we had better measures besides user counts, this piece and many others would be full of charts and graphs that added a sense of intellectual heft to the analysis. There would be some annual presentation called the State of Social akin to Meeker's Internet Trends Report, or perhaps it would be a fifty page sub-section of her annual report.Despite this, most of the social media networks we study generate much more social capital than actual financial capital, especially in their early stages; almost all such companies have internalized one of the popular truisms of Silicon Valley, that in the early days, companies should postpone revenue generation in favor of rapid network scaling. Social capital has much to say about why social networks lose heat, stall out, and sometimes disappear altogether. And, while we may not be able to quantify social capital, as highly attuned social creatures, we can feel it.Social capital is, in many ways, a leading indicator of financial capital, and so its nature bears greater scrutiny. Not only is it good investment or business practice, but analyzing social capital dynamics can help to explain all sorts of online behavior that would otherwise seem irrational.In the past few years, much progress has been made analyzing Software as a Service (SaaS) businesses. Not as much has been made on social networks. Analysis of social networks still strikes me as being like economic growth theory long before Paul Romer's paper on endogenous technological change. However, we can start to demystify social networks if we also think of them as SaaS businesses, but instead of software, they provide status. This post is a deep dive into what I refer to as Status as a Service (StaaS) businesses.Think of this essay as a series of strongly held hypotheses; without access to the types of data which i’m not even sure exists, it’s difficult to be definitive. As ever, my wise readers will add or push back as they always do.Traditional Network Effects Model of Social NetworksOne of the fundamental lessons of successful social networks is that they must first appeal to people when they have few users. Typically this is done through some form of single-user utility. This is the classic cold start problem of social.  The answer to the traditional chicken-and-egg question is actually answerable: what comes first is a single chicken, and then another chicken, and then another chicken, and so on. The harder version of the question is why the first chicken came and stayed when no other chickens were around, and why the others followed.The second fundamental lessons is that social networks must have strong network effects so that as more and more users come aboard, the network enters a positive flywheel of growth, a compounding value from positive network effects that leads to hockey stick growth that puts dollar signs in the eyes of investors and employees  alike. \"Come for the tool, stay for the network\" wrote Chris Dixon, in perhaps the most memorable maxim for how this works.Even before social networks, we had Metcalfe's Law on telecommunications networks:The value of a telecommunications network is proportional to the square of the number of connected users of the system (n^2)This ported over to social networks cleanly. It is intuitive, and it includes that tantalizing math formula that explains why growth curves for social networks bends up sharply at the ankle of the classic growth S-curve.But dig deeper and many many questions remain. Why do some large social networks suddenly fade away, or lose out to new tiny networks? Why do some new social networks with great single-player tools fail to transform into networks, while others with seemingly frivolous purposes make the leap? Why do some networks sometimes lose value when they add more users? What determines why different networks stall out at different user base sizes? Why do some networks cross international borders easily while others stay locked within specific countries? Why, if Metcalfe's Law holds, do many of Facebook's clones of other social network features fail, while some succeed, like Instagram Stories?What ties many of these explanations together is social capital theory, and how we analyze social networks should include a study of a social network's accumulation of social capital assets and the nature and structure of its status games. In other words, how do such companies capitalize, either consciously or not, on the fact that people are status-seeking monkeys, always trying to seek more of it in the most efficient way possible?To paraphrase Nicki Minaj, “If I'm fake I ain't notice cause my followers ain't.”[Editor’s note: sometimes the followers actually are fake.]Utility vs. Social Capital FrameworkClassic network effects theory still holds, I’m not discarding it. Instead, let's append some social capital theory. Together, those form the two axes on which I like to analyze social network health.Actually, I tend to use three axes to dissect social networks.\n\n              View fullsize\n\n            The three axes on which I evaluate social network strength\n\n  For this post, though, I'm only going to look at two of them, utility and social capital, as the entertainment axis adds a whole lot of complexity which I'll perhaps explain another time.\n\n              View fullsize\n\n            The basic two axis framework guiding much of the social network analysis in this piece\n\n  Utility doesn't require much explanation, though we often use the term very loosely and categorize too many things as utility when they aren't that useful (we generally confuse circuses for bread and not the reverse; Fox News, for example, is more entertainment than utility, as is common of many news outlets). A social network like Facebook allows me to reach lots of people I would otherwise have a harder time tracking down, and that is useful. A messaging app like WhatsApp allows me to communicate with people all over the world without paying texting or incremental data fees, which is useful. Quora and Reddit and Discord and most every social network offer some forms of utility.The other axis is, for a lack of a more precise term, the social capital axis, or the status axis. Can I use the social network to accumulate social capital? What forms? How is it measured? And how do I earn that status?There are several different paths to success for social networks, but those which compete on the social capital axis are often more mysterious than pure utilities. Competition on raw utility tends to be Darwinian, ruthless, and highly legible. This is the world, for example, of communication services like messaging and video conferencing.  Investing in this space also tends to be a bit more straightforward: how useful is your app or service, can you get distribution, etc. When investors send me decks on things in this category, I am happy to offer an opinion, but I enjoy puzzling over the world of artificial prestige even more.The creation of a successful status game is so mysterious that it often smacks of alchemy. For that reason, entrepreneurs who succeed in this space are thought of us a sort of shaman, perhaps because most investors are middle-aged white men who are already so high status they haven't the first idea why people would seek virtual status (more on that later).With the rise of Instagram, with its focus on photos and filters, and Snapchat, with its ephemeral messaging, and Vine, with its 6-second video limit, for a while there was a thought that new social networks would be built on some new modality of communications. That's a piece of it, but it's not the complete picture, and not for the reasons many people think, which is why we have seen a whole bunch of strange failed experiments in just about every odd combinations of features and filters and artificial constraints in how we communicate with each other through our phones. Remember Facebook's Snapchat competitor Slingshot, in which you had to unlock any messages you received by responding with a message? It felt like product design by mad libs.When modeling how successful social networks create a status game worth playing, a useful metaphor is one of the trendiest technologies: cryptocurrency.Social Networks as ICO'sHow is a new social network analogous to an ICO? Each new social network issues a new form of social capital, a token.You must show proof of work to earn the token.Over time it becomes harder and harder to mine new tokens on each social network, creating built-in scarcity. Many people, especially older folks, scoff at both social networks and cryptocurrencies. [\"Why does anyone care what you ate for lunch?\" is the canonical retort about any social network, though it’s fading with time. Both social networks and ICO's tend to drive skeptics crazy because they seem to manufacture value out of nothing. The shifting nature of scarcity will always leave a wake of skepticism and disbelief.]Years ago, I stayed at the house of a friend whose high school daughter was home upstairs with a classmates. As we adults drank wine in the kitchen downstairs while waiting for dinner to finish in the oven, we heard lots of music and stomping and giggling coming from upstairs. When we finally called them down for dinner, I asked them what all the ruckus had been. My friend's daughter proudly held up her phone to show me a recording they'd posted to an app called Musical.ly. It was a lip synch and dance routine replete with their own choreography. They'd rehearsed the piece more times than they could count. It showed. Their faces were shiny with sweat, and they were still breathing hard from the exertion. Proof of work indeed.I spent the rest of the dinner scrolling through the app, fascinated, interviewing the girls about what they liked about the app, why they were on it, what share of their free time it had captured. I can't tell if parents are offended or glad when I spend much of the time visiting them interviewing their sons and daughters instead, but in the absence of good enough metrics with which to analyze this space, I subscribe to the Jane Goodall theory of how to study your subject. Besides, status games of adults are already well covered by the existing media, from literature to film. Children's status games, once familiar to us, begin to fade from our memory as time passes, and its modern forms have been drastically altered by social media.Other examples abound. Perhaps you've read a long and thoughtful response by a random person on Quora or Reddit, or watched YouTube vloggers publishing night after night, or heard about popular Vine stars living in houses together, helping each other shoot and edit 6-second videos. While you can outsource Bitcoin mining to a computer, people still mine for social capital on social networks largely through their own blood, sweat, and tears.[Aside: if you yourself are not an aspiring social network star, living with one is...not recommended.]Perhaps, if you've spent time around today's youth, you've watched with a mixture of horror and fascination as a teen snaps dozens of selfies before publishing the most flattering one to Instagram, only to pull it down if it doesn't accumulate enough likes within the first hour. It’s another example of proof of work, or at least vigorous market research.Almost every social network of note had an early signature proof of work hurdle. For Facebook it was posting some witty text-based status update. For Instagram, it was posting an interesting square photo. For Vine, an entertaining 6-second video. For Twitter, it was writing an amusing bit of text of 140 characters or fewer. Pinterest? Pinning a compelling photo. You can likely derive the proof of work for other networks like Quora and Reddit and Twitch and so on. Successful social networks don't pose trick questions at the start, it’s usually clear what they want from you.[An aside about exogenous social capital: you might complain that your tweets are more interesting and  grammatical than those of, say, Donald Trump (you're probably right!). Or that your photos are better composed and more interesting at a deep level of photographic craft than those of Kim Kardashian. The difference is, they bring a massive supply of exogenous pre-existing social capital from another status game, the fame game, to every table, and some forms of social capital transfer quite well across platforms. Generalized fame is one of them. More specific forms of fame or talent might not retain their value as easily: you might follow Paul Krugman on Twitter, for example, but not have any interest in his Instagram account. I don't know if he has one, but I probably wouldn't follow it if he did, sorry Paul, it’s nothing personal.]If you've ever joined one of these social networks early enough, you know that, on a relative basis, getting ahead of others in terms of social capital (followers, likes, etc.) is easier in the early days. Some people who were featured on recommended follower lists in the early days of Twitter have follower counts in the 7-figures, just as early masters of Musical.ly and Vine were accumulated massive and compounding follower counts. The more people who follow you, the more followers you gain because of leaderboards and recommended follower algorithms and other such common discovery mechanisms.It's true that as more people join a network, more social capital is up for grabs in the aggregate. However, in general, if you come to a social network later, unless you bring incredible exogenous social capital (Taylor Swift can join any social network on the planet and collect a massive following immediately), the competition for attention is going to be more intense than it was in the beginning. Everyone has more of an understanding of how the game works so the competition is stiffer.Why Proof of Work MattersWhy does proof of work matter for a social network? If people want to maximize social capital, why not make that as easy as possible?As with cryptocurrency, if it were so easy, it wouldn't be worth anything. Value is tied to scarcity, and scarcity on social networks derives from proof of work. Status isn't worth much if there's no skill and effort required to mine it. It's not that a social network that makes it easy for lots of users to perform well can't be a useful one, but competition for relative status still motivates humans. Recall our first tenet: humans are status-seeking monkeys. Status is a relative ladder. By definition, if everyone can achieve a certain type of status, it’s no status at all, it’s a participation trophy.Musical.ly created a hurdle for gaining followers and status that wasn't easily cleared by many people. However, for some, especially teens, and especially girls, it was a status game at which they were particularly suited to win. And so they flocked there, because, according to my second tenet, people look for the most efficient ways to accumulate the most social capital.Recall Twitter in the early days, when it was somewhat of a harmless but somewhat inert status update service. I went back to look at my first few tweets on the service from some 12 years ago and my first two, spaced about a year apart, were both about doing my taxes. Looking back at them, I bore even myself. Early Twitter consisted mostly of harmless but dull life status updates, a lot of “is this thing on?” tapping on the virtual microphone. I guess I am in the camp of not caring about what you had for lunch after all. Get off my lawn, err, phone screen!What changed Twitter, for me, was the launch of Favstar and Favrd (both now defunct, ruthlessly murdered by Twitter), these global leaderboards that suddenly turned the service into a competition to compose the most globally popular tweets. Recall, the Twitter graph was not as dense then as it was now, nor did distribution accelerants like one-click retweeting and Moments exist yet.What Favstar and Favrd did was surface really great tweets and rank them on a scoreboard, and that, to me, launched the performative revolution in Twitter. It added needed feedback to the feedback loop, birthing a new type of comedian, the master of the 140 character or less punchline (the internet has killed the joke, humor is all punchline now that the setup of the joke is assumed to be common knowledge thanks to Google).The launch of these global tweet scoreboards reminds me of the moment in the now classic film** Battle Royale when Beat Takeshi Kitano informs a bunch of troublemaking school kids that they’ve been deported to an island are to fight to the death, last student standing wins, and that those who try to sneak out of designated battle zones will be killed by explosive collars. I'm not saying that Twitter is a life-or-death struggle, but you need only time trav",
    "summary": {
      "en": "### Summary of \"Status as a Service (StaaS)\" by Eugene Wei\n\nThis essay explores the concept of \"Status as a Service\" (StaaS), likening social networks to businesses that provide social capital instead of software. The author argues that people are inherently motivated to seek social status, and social networks thrive by facilitating this pursuit.\n\n**Key Points:**\n\n1. **Human Nature and Status:** People naturally seek social capital, which is akin to status. Social networks are often analyzed in terms of financial capital, but understanding their dynamics through the lens of social capital is crucial.\n\n2. **Measurement Challenges:** Unlike financial capital, social capital is harder to quantify, leading to less precise analysis of social networks. Current metrics often focus on user counts rather than deeper social dynamics.\n\n3. **Network Effects:** Successful social networks grow by appealing to users initially with unique single-user benefits and then leveraging strong network effects to accelerate growth.\n\n4. **Utility vs. Social Capital:** Social networks can be evaluated on two axes: utility (how useful they are) and social capital (how they help users gain status). A successful social network balances both aspects.\n\n5. **Proof of Work:** Similar to cryptocurrencies, social networks require users to demonstrate effort (proof of work) to gain status, which creates scarcity and value for social capital.\n\n6. **Competition and Scarcity:** As more users join a network, the competition for social capital becomes fiercer. Early adopters often have an easier time gaining status, as the competition is less intense.\n\n7. **Dynamic Nature of Social Capital:** The essay emphasizes that social capital is not static and can change based on user interaction and network structure, influencing the overall health and sustainability of social networks.\n\nIn summary, Wei introduces a framework for analyzing social networks through the lens of status-seeking behavior, illustrating the importance of social capital in understanding their growth and dynamics.",
      "ko": "이 글은 \"서비스로서의 지위(Status as a Service, StaaS)\"라는 개념을 탐구하며, 소셜 네트워크를 소프트웨어 대신 사회적 자본을 제공하는 비즈니스에 비유합니다. 저자는 사람들이 본질적으로 사회적 지위를 추구하는 동기를 가지고 있으며, 소셜 네트워크는 이러한 추구를 촉진함으로써 성장한다고 주장합니다.\n\n사람들은 자연스럽게 사회적 자본, 즉 지위를 추구합니다. 소셜 네트워크는 종종 재정적 자본의 관점에서 분석되지만, 사회적 자본의 관점에서 그 역학을 이해하는 것이 중요합니다. 재정적 자본과 달리 사회적 자본은 정량화하기 어려워 소셜 네트워크에 대한 분석이 덜 정확해질 수 있습니다. 현재의 지표들은 사용자 수에 초점을 맞추는 경우가 많아, 더 깊은 사회적 역학을 놓치는 경향이 있습니다.\n\n성공적인 소셜 네트워크는 처음에는 독특한 단일 사용자 혜택으로 사용자들을 끌어들이고, 이후 강력한 네트워크 효과를 활용하여 성장을 가속화합니다. 소셜 네트워크는 유용성(얼마나 유용한가)과 사회적 자본(사용자가 지위를 얻는 데 얼마나 도움이 되는가)이라는 두 축으로 평가될 수 있습니다. 성공적인 소셜 네트워크는 이 두 가지 측면의 균형을 잘 맞추고 있습니다.\n\n암호화폐와 유사하게, 소셜 네트워크는 사용자가 지위를 얻기 위해 노력(작업 증명)을 보여주도록 요구합니다. 이는 사회적 자본에 대한 희소성과 가치를 창출합니다. 네트워크에 더 많은 사용자가 참여함에 따라 사회적 자본을 위한 경쟁은 치열해집니다. 초기 사용자들은 경쟁이 덜 치열하기 때문에 지위를 얻기가 더 수월한 경우가 많습니다.\n\n이 글은 사회적 자본이 정적이지 않으며 사용자 상호작용과 네트워크 구조에 따라 변화할 수 있음을 강조합니다. 이는 소셜 네트워크의 전반적인 건강과 지속 가능성에 영향을 미칩니다. 웨이는 지위 추구 행동의 관점에서 소셜 네트워크를 분석하는 프레임워크를 제시하며, 그 성장과 역학을 이해하는 데 있어 사회적 자본의 중요성을 보여줍니다.",
      "ja": "「ステータス・アズ・ア・サービス（StaaS）」という概念についてのエッセイでは、ソーシャルネットワークをソフトウェアではなく社会的資本を提供するビジネスに例えています。著者は、人々が本質的に社会的地位を求める動機を持っており、ソーシャルネットワークはこの追求を促進することで成長すると主張しています。\n\n人間は自然に社会的資本、つまり地位を求めます。ソーシャルネットワークはしばしば金融資本の観点から分析されますが、社会的資本の視点からそのダイナミクスを理解することが重要です。金融資本とは異なり、社会的資本は定量化が難しく、ソーシャルネットワークの分析が不正確になることがあります。現在の指標は、ユーザー数に焦点を当てがちですが、より深い社会的ダイナミクスを考慮する必要があります。\n\n成功したソーシャルネットワークは、最初にユニークな単独ユーザーの利点を提供し、その後強力なネットワーク効果を利用して成長を加速させます。ソーシャルネットワークは、使いやすさ（ユーティリティ）と社会的資本（ユーザーが地位を得る手助けをする度合い）の二つの軸で評価できます。成功するソーシャルネットワークは、この二つの側面のバランスを取っています。\n\n暗号通貨と同様に、ソーシャルネットワークではユーザーが地位を得るために努力を示す必要があります（作業の証明）。これにより、社会的資本に希少性と価値が生まれます。ネットワークに参加するユーザーが増えるにつれて、社会的資本を得るための競争は激化します。初期の参加者は競争が少ないため、地位を得るのが比較的容易です。\n\n社会的資本は静的ではなく、ユーザーの相互作用やネットワークの構造によって変化することが強調されています。これにより、ソーシャルネットワークの全体的な健康や持続可能性に影響を与えます。ウエイは、地位を求める行動の観点からソーシャルネットワークを分析するための枠組みを提示し、成長とダイナミクスを理解する上での社会的資本の重要性を示しています。"
    }
  },
  {
    "id": "2c6f01f2468a616d",
    "title": {
      "en": "Qwen2.5-VL-32B: Smarter and Lighter",
      "ko": "퀘인2.5: 더 똑똑하고 가벼워져!",
      "ja": "Qwen2.5-VLの進化"
    },
    "type": "story",
    "url": "https://qwenlm.github.io/blog/qwen2.5-vl-32b/",
    "score": 479,
    "by": "tosh",
    "time": 1742841312,
    "content": "QWEN CHAT\nGITHUB\nHUGGING FACE\nMODELSCOPE\nDISCORDIntroduction#At the end of January this year, we launched the Qwen2.5-VL series of models, which received widespread attention and positive feedback from the community. Building on the Qwen2.5-VL series, we continued to optimize the model using reinforcement learning and open-sourced the new VL model with the beloved 32B parameter scale under the Apache 2.0 license — Qwen2.5-VL-32B-Instruct. Compared to the previously released Qwen2.5-VL series models, the features of this 32B VL model are as follows:Responses More Aligned with Human Preferences: Adjusted the output style to provide more detailed, better-formatted answers that align more closely with human preferences.Mathematical Reasoning: Significant improvement in the accuracy of solving complex mathematical problems.Fine-grained Image Understanding and Reasoning: Enhanced accuracy and detailed analysis in tasks such as image parsing, content recognition, and visual logic deduction.Performance#Extensive benchmarking against state-of-the-art (SoTA) models of comparable scale, Qwen2.5-VL-32B-Instruct has demonstrated superiority over baselines, e.g., Mistral-Small-3.1-24B and Gemma-3-27B-IT, even surpassing the larger Qwen2-VL-72B-Instruct. Notably, it achieves significant advantages in multimodal tasks such as MMMU, MMMU-Pro, and MathVista, which focus on complex, multi-step reasoning. On MM-MT-Bench, a benchmark emphasizing subjective user experience evaluation, Qwen2.5-VL-32B-Instruct outperforms its predecessor Qwen2-VL-72B-Instruct by a substantial margin.In addition to excelling in visual capabilities, Qwen2.5-VL-32B-Instruct has also achieved top-tier performance in pure text capabilities at the same scale.Demo Cases#Fine-grained Image Understanding and Reasoning (1/5)\nNextUserI am driving a large truck on this road, and it is now 12 o’clock. Can I reach a place 110 kilometers away before 13 o’clock?\nQwen2.5-VL-32BTo determine whether you can reach a place 110 kilometers away before 13:00, we need to analyze the information provided:Current Time: It is 12:00.Distance to Destination: 110 kilometers.Speed Limit for Trucks: The speed limit sign indicates that the maximum speed for trucks is 100 km/h.Step-by-Step Analysis:#1. Calculate the Required Travel Time#The formula to calculate travel time is:\nTime=DistanceSpeed\n\\text{Time} = \\frac{\\text{Distance}}{\\text{Speed}}\nTime=SpeedDistance\nGiven:Distance = 110 kmMaximum Speed = 100 km/hSubstitute the values:\nTime=110km100km/h=1.1hours\n\\text{Time} = \\frac{110 \\text{ km}}{100 \\text{ km/h}} = 1.1 \\text{ hours}\nTime=100km/h110km=1.1hours2. Convert Time to Hours and Minutes#1.1 hours is equivalent to:\n1hour+0.1×60minutes=1hour+6minutes=1hourand6minutes\n1 \\text{ hour} + 0.1 \\times 60 \\text{ minutes} = 1 \\text{ hour} + 6 \\text{ minutes} = 1 \\text{ hour and } 6 \\text{ minutes}\n1hour+0.1×60minutes=1hour+6minutes=1hourand6minutes3. Determine Arrival Time#If you start at 12:00 and it takes 1 hour and 6 minutes to travel 110 kilometers, your arrival time will be:\n12:00+1hourand6minutes=13:06\n12:00 + 1 \\text{ hour and } 6 \\text{ minutes} = 13:06\n12:00+1hourand6minutes=13:064. Compare with Target Time#You want to arrive before 13:00. However, based on the calculations, you will arrive at 13:06, which is after 13:00.Conclusion:#Given the speed limit of 100 km/h for trucks, it is not possible to reach a place 110 kilometers away before 13:00.Thus, the final answer is:\nNo\n\\boxed{\\text{No}}\nNoMathematical Reasoning (2/5)\nNextUserAs shown in the figure, the straight lines AB and CD intersect at point O, OD bisects ∠AOE, ∠BOC = 50.0, then ∠EOB = ()\nQwen2.5-VL-32BProblem Analysis#We are given the following information:Straight lines AB AB AB and CD CD CD intersect at point O O O.OD OD OD bisects ∠AOE \\angle AOE ∠AOE.∠BOC=50∘ \\angle BOC = 50^\\circ ∠BOC=50∘.We need to find ∠EOB \\angle EOB ∠EOB.Step-by-Step Solution#Step 1: Understand the given information and relationships#Since AB AB AB and CD CD CD are straight lines intersecting at O O O, the angles around point O O O sum up to 360∘ 360^\\circ 360∘.The vertically opposite angles are equal. Therefore:\n∠AOD=∠BOCand∠DOB=∠COA.\n\\angle AOD = \\angle BOC \\quad \\text{and} \\quad \\angle DOB = \\angle COA.\n∠AOD=∠BOCand∠DOB=∠COA.\nGiven ∠BOC=50∘ \\angle BOC = 50^\\circ ∠BOC=50∘, we have:\n∠AOD=50∘.\n\\angle AOD = 50^\\circ.\n∠AOD=50∘.Step 2: Use the angle bisector property#It is given that OD OD OD bisects ∠AOE \\angle AOE ∠AOE. This means:\n∠AOD=∠DOE.\n\\angle AOD = \\angle DOE.\n∠AOD=∠DOE.\nSince ∠AOD=50∘ \\angle AOD = 50^\\circ ∠AOD=50∘, we have:\n∠DOE=50∘.\n\\angle DOE = 50^\\circ.\n∠DOE=50∘.Step 3: Determine ∠AOE \\angle AOE ∠AOE#The angle ∠AOE \\angle AOE ∠AOE is the sum of ∠AOD \\angle AOD ∠AOD and ∠DOE \\angle DOE ∠DOE:\n∠AOE=∠AOD+∠DOE=50∘+50∘=100∘.\n\\angle AOE = \\angle AOD + \\angle DOE = 50^\\circ + 50^\\circ = 100^\\circ.\n∠AOE=∠AOD+∠DOE=50∘+50∘=100∘.Step 4: Use the straight line property#Since AB AB AB is a straight line, the angles on a straight line sum up to 180∘ 180^\\circ 180∘. Therefore:\n∠AOE+∠EOB=180∘.\n\\angle AOE + \\angle EOB = 180^\\circ.\n∠AOE+∠EOB=180∘.\nSubstituting ∠AOE=100∘ \\angle AOE = 100^\\circ ∠AOE=100∘, we get:\n100∘+∠EOB=180∘.\n100^\\circ + \\angle EOB = 180^\\circ.\n100∘+∠EOB=180∘.\nSolving for ∠EOB \\angle EOB ∠EOB:\n∠EOB=180∘−100∘=80∘.\n\\angle EOB = 180^\\circ - 100^\\circ = 80^\\circ.\n∠EOB=180∘−100∘=80∘.Final Answer#80\n\\boxed{80}\n80Mathematical Reasoning (3/5)\nNextUser侏罗纪蜘蛛网是一种非常有规律的蜘蛛网，如图，它是由无数个正方形环绕而成的，且最外边第一个正方形A_{1}B_{1}C_{1}D_{1}的面积为a_{1}=1，分别取正方形A_{1}B_{1}C_{1}D_{1}各边的三等分点A_{2}，B_{2}，C_{2}，D_{2}，作第二个正方形A_{2}B_{2}C_{2}D_{2}，且正方形A_{2}B_{2}C_{2}D_{2}的面积为a_{2}，\\ldots \\ldots ，依次类推，第n个正方形A_{n}B_{n}C_{n}D_{n}的面积为a_{n}. $求{a_{n}}的通项公式\nQwen2.5-VL-32B问题分析#我们需要找到第 n n n 个正方形的面积 an a_n an 的通项公式。已知：第一个正方形 A1B1C1D1 A_1B_1C_1D_1 A1B1C1D1 的面积为 a1=1 a_1 = 1 a1=1。每次取前一个正方形各边的三等分点，构造一个新的正方形。我们需要确定每次构造新正方形时，其面积与前一个正方形面积之间的关系。第一步：设第一个正方形的边长#设第一个正方形 A1B1C1D1 A_1B_1C_1D_1 A1B1C1D1 的边长为 s1 s_1 s1。由于其面积为 a1=1 a_1 = 1 a1=1，因此：\ns1=a1=1=1.\ns_1 = \\sqrt{a_1} = \\sqrt{1} = 1.\ns1=a1=1=1.第二步：构造第二个正方形#取 A1B1C1D1 A_1B_1C_1D_1 A1B1C1D1 各边的三等分点，得到新的正方形 A2B2C2D2 A_2B_2C_2D_2 A2B2C2D2。我们需要计算这个新正方形的边长。1. 确定新正方形的顶点坐标#假设第一个正方形 A1B1C1D1 A_1B_1C_1D_1 A1B1C1D1 的顶点坐标如下：A1=(0,1) A_1 = (0, 1) A1=(0,1),B1=(1,1) B_1 = (1, 1) B1=(1,1),C1=(1,0) C_1 = (1, 0) C1=(1,0),D1=(0,0) D_1 = (0, 0) D1=(0,0).取各边的三等分点：A2 A_2 A2 是 A1B1 A_1B_1 A1B1 的三等分点，坐标为 (13,1) \\left( \\frac{1}{3}, 1 \\right) (31,1),B2 B_2 B2 是 B1C1 B_1C_1 B1C1 的三等分点，坐标为 (1,23) \\left( 1, \\frac{2}{3} \\right) (1,32),C2 C_2 C2 是 C1D1 C_1D_1 C1D1 的三等分点，坐标为 (23,0) \\left( \\frac{2}{3}, 0 \\right) (32,0),D2 D_2 D2 是 D1A1 D_1A_1 D1A1 的三等分点，坐标为 (0,13) \\left( 0, \\frac{1}{3} \\right) (0,31).2. 计算新正方形的边长#新正方形 A2B2C2D2 A_2B_2C_2D_2 A2B2C2D2 的边长可以通过计算任意两点之间的距离得到。例如，计算 A2 A_2 A2 和 B2 B_2 B2 之间的距离：\nA2=(13,1),B2=(1,23).\nA_2 = \\left( \\frac{1}{3}, 1 \\right), \\quad B_2 = \\left( 1, \\frac{2}{3} \\right).\nA2=(31,1),B2=(1,32).\n两点间的距离公式为：\ns2=(x2−x1)2+(y2−y1)2.\ns_2 = \\sqrt{\\left( x_2 - x_1 \\right)^2 + \\left( y_2 - y_1 \\right)^2}.\ns2=(x2−x1)2+(y2−y1)2.\n代入坐标：\ns2=(1−13)2+(23−1)2=(23)2+(−13)2=49+19=59=53.\ns_2 = \\sqrt{\\left( 1 - \\frac{1}{3} \\right)^2 + \\left( \\frac{2}{3} - 1 \\right)^2} = \\sqrt{\\left( \\frac{2}{3} \\right)^2 + \\left( -\\frac{1}{3} \\right)^2} = \\sqrt{\\frac{4}{9} + \\frac{1}{9}} = \\sqrt{\\frac{5}{9}} = \\frac{\\sqrt{5}}{3}.\ns2=(1−31)2+(32−1)2=(32)2+(−31)2=94+91=95=35.3. 计算新正方形的面积#新正方形的面积 a2 a_2 a2 为：\na2=s22=(53)2=59.\na_2 = s_2^2 = \\left( \\frac{\\sqrt{5}}{3} \\right)^2 = \\frac{5}{9}.\na2=s22=(35)2=95.第三步：归纳规律#从上述计算可以看出：第一个正方形的边长为 s1=1 s_1 = 1 s1=1，面积为 a1=1 a_1 = 1 a1=1。第二个正方形的边长为 s2=53 s_2 = \\frac{\\sqrt{5}}{3} s2=35，面积为 a2=59 a_2 = \\frac{5}{9} a2=95。接下来，我们猜测每次构造新正方形时，边长和面积的变化规律。1. 边长变化规律#假设第 n n n 个正方形的边长为 sn s_n sn，则第 n+1 n+1 n+1 个正方形的边长 sn+1 s_{n+1} sn+1 可以通过相似三角形的性质得出。每次取三等分点构造的新正方形，其边长是前一个正方形边长的 53 \\frac{\\sqrt{5}}{3} 35 倍。因此：\nsn+1=53sn.\ns_{n+1} = \\frac{\\sqrt{5}}{3} s_n.\nsn+1=35sn.2. 面积变化规律#由于面积与边长的平方成正比，第 n+1 n+1 n+1 个正方形的面积 an+1 a_{n+1} an+1 为：\nan+1=sn+12=(53sn)2=(53)2an=59an.\na_{n+1} = s_{n+1}^2 = \\left( \\frac{\\sqrt{5}}{3} s_n \\right)^2 = \\left( \\frac{\\sqrt{5}}{3} \\right)^2 a_n = \\frac{5}{9} a_n.\nan+1=sn+12=(35sn)2=(35)2an=95an.这表明 {an} \\{a_n\\} {an} 是一个等比数列，首项为 a1=1 a_1 = 1 a1=1，公比为 59 \\frac{5}{9} 95。第四步：写出通项公式#等比数列的通项公式为：\nan=a1⋅qn−1,\na_n = a_1 \\cdot q^{n-1},\nan=a1⋅qn−1,\n其中 a1=1 a_1 = 1 a1=1，q=59 q = \\frac{5}{9} q=95。因此：\nan=1⋅(59)n−1=(59)n−1.\na_n = 1 \\cdot \\left( \\frac{5}{9} \\right)^{n-1} = \\left( \\frac{5}{9} \\right)^{n-1}.\nan=1⋅(95)n−1=(95)n−1.最终答案#(59)n−1\n\\boxed{\\left( \\frac{5}{9} \\right)^{n-1}}\n(95)n−1Mathematical Reasoning (4/5)\nNextUser如图，椭圆M:\\frac{y^2}{a^2}+\\frac{x^2}{b^2}=1(a>b>0)的两顶点为A(－2,0)，B(2,0)，离心率e=\\frac{\\sqrt{3}}{2}，过y轴上的点F(0,t)(|t|<4,t\neq 0)的直线l与椭圆交于C，D两点，并与x轴交于点P，直线AC与直线BD交于点Q. $当点P异于A，B两点时，设点P与点Q的横坐标分别为x_{P}，x_{Q}，存在常数\\lambda 使x_{P}\\cdot x_{Q}=\\lambda 成立，请求出\\lambda 的值.\nQwen2.5-VL题目分析#我们有一个椭圆 M:y2a2+x2b2=1 M: \\frac{y^2}{a^2} + \\frac{x^2}{b^2} = 1 M:a2y2+b2x2=1，其中 a>b>0 a > b > 0 a>b>0。已知椭圆的两顶点为 A(−2,0) A(-2, 0) A(−2,0) 和 B(2,0) B(2, 0) B(2,0)，离心率为 e=32 e = \\frac{\\sqrt{3}}{2} e=23。我们需要找到常数 λ \\lambda λ，使得点 P P P（直线 l l l 与 x x x-轴的交点）和点 Q Q Q（直线 AC AC AC 与直线 BD BD BD 的交点）的横坐标 xP x_P xP 和 xQ x_Q xQ 满足 xP⋅xQ=λ x_P \\cdot x_Q = \\lambda xP⋅xQ=λ。第一步：确定椭圆的参数#顶点信息：椭圆的顶点为 A(−2,0) A(-2, 0) A(−2,0) 和 B(2,0) B(2, 0) B(2,0)，说明椭圆的长轴在 x x x-轴上，且半长轴长度为 b=2 b = 2 b=2。因此，b=2 b = 2 b=2。离心率信息：离心率 e=32 e = \\frac{\\sqrt{3}}{2} e=23。离心率的公式为 e=ca e = \\frac{c}{a} e=ac，其中 c=a2−b2 c = \\sqrt{a^2 - b^2} c=a2−b2。代入 e=32 e = \\frac{\\sqrt{3}}{2} e=23，得到：\nca=32  ⟹  c=32a.\n\\frac{c}{a} = \\frac{\\sqrt{3}}{2} \\implies c = \\frac{\\sqrt{3}}{2} a.\nac=23⟹c=23a.又因为 c=a2−b2 c = \\sqrt{a^2 - b^2} c=a2−b2，代入 b=2 b = 2 b=2，得到：\na2−4=32a.\n\\sqrt{a^2 - 4} = \\frac{\\sqrt{3}}{2} a.\na2−4=23a.平方两边：\na2−4=34a2  ⟹  14a2=4  ⟹  a2=16  ⟹  a=4.\na^2 - 4 = \\frac{3}{4} a^2 \\implies \\frac{1}{4} a^2 = 4 \\implies a^2 = 16 \\implies a = 4.\na2−4=43a2⟹41a2=4⟹a2=16⟹a=4.因此，椭圆的方程为：\ny216+x24=1.\n\\frac{y^2}{16} + \\frac{x^2}{4} = 1.\n16y2+4x2=1.第二步：直线 l l l 的方程#直线 l l l 过点 F(0,t) F(0, t) F(0,t)，设其斜率为 k k k，则直线 l l l 的方程为：\ny=kx+t.\ny = kx + t.\ny=kx+t.第三步：直线 l l l 与椭圆的交点#将直线方程 y=kx+t y = kx + t y=kx+t 代入椭圆方程 y216+x24=1 \\frac{y^2}{16} + \\frac{x^2}{4} = 1 16y2+4x2=1，得到：\n(kx+t)216+x24=1.\n\\frac{(kx + t)^2}{16} + \\frac{x^2}{4} = 1.\n16(kx+t)2+4x2=1.\n整理得：\n(kx+t)2+4x2=16  ⟹  k2x2+2ktx+t2+4x2=16  ⟹  (k2+4)x2+2ktx+t2−16=0.\n(kx + t)^2 + 4x^2 = 16 \\implies k^2x^2 + 2ktx + t^2 + 4x^2 = 16 \\implies (k^2 + 4)x^2 + 2ktx + t^2 - 16 = 0.\n(kx+t)2+4x2=16⟹k2x2+2ktx+t2+4x2=16⟹(k2+4)x2+2ktx+t2−16=0.\n这是关于 x x x 的二次方程，设其两根为 x1 x_1 x1 和 x2 x_2 x2，则交点 C(x1,y1) C(x_1, y_1) C(x1,y1) 和 D(x2,y2) D(x_2, y_2) D(x2,y2) 的横坐标分别为 x1 x_1 x1 和 x2 x_2 x2。由韦达定理，有：\nx1+x2=−2ktk2+4,x1x2=t2−16k2+4.\nx_1 + x_2 = -\\frac{2kt}{k^2 + 4}, \\quad x_1 x_2 = \\frac{t^2 - 16}{k^2 + 4}.\nx1+x2=−k2+42kt,x1x2=k2+4t2−16.第四步：点 P P P 的横坐标#直线 l l l 与 x x x-轴的交点 P P P 的纵坐标为 0，代入直线方程 y=kx+t y = kx + t y=kx+t，得到：\n0=kxP+t  ⟹  xP=−tk.\n0 = kx_P + t \\implies x_P = -\\frac{t}{k}.\n0=kxP+t⟹xP=−kt.第五步：直线 AC AC AC 和 BD BD BD 的方程#直线 AC AC AC 的方程：点 A(−2,0) A(-2, 0) A(−2,0) 和点 C(x1,y1) C(x_1, y_1) C(x1,y1)，直线 AC AC AC 的斜率为：\ny1−0x1+2=y1x1+2.\n\\frac{y_1 - 0}{x_1 + 2} = \\frac{y_1}{x_1 + 2}.\nx1+2y1−0=x1+2y1.因此，直线 AC AC AC 的方程为：\ny=y1x1+2(x+2).\ny = \\frac{y_1}{x_1 + 2}(x + 2).\ny=x1+2y1(x+2).直线 BD BD BD 的方程：点 B(2,0) B(2, 0) B(2,0) 和点 D(x2,y2) D(x_2, y_2) D(x2,y2)，直线 BD BD BD 的斜率为：\ny2−0x2−2=y2x2−2.\n\\frac{y_2 - 0}{x_2 - 2} = \\frac{y_2}{x_2 - 2}.\nx2−2y2−0=x2−2y2.因此，直线 BD BD BD 的方程为：\ny=y2x2−2(x−2).\ny = \\frac{y_2}{x_2 - 2}(x - 2).\ny=x2−2y2(x−2).第六步：点 Q Q Q 的横坐标#点 Q Q Q 是直线 AC AC AC 和直线 BD BD BD 的交点，联立方程：\ny1x1+2(x+2)=y2x2−2(x−2).\n\\frac{y_1}{x_1 + 2}(x + 2) = \\frac{y_2}{x_2 - 2}(x - 2).\nx1+2y1(x+2)=x2−2y2(x−2).\n将 y1=kx1+t y_1 = kx_1 + t y1=kx1+t 和 y2=kx2+t y_2 = kx_2 + t y2=kx2+t 代入，得到：\nkx1+tx1+2(x+2)=kx2+tx2−2(x−2).\n\\frac{kx_1 + t}{x_1 + 2}(x + 2) = \\frac{kx_2 + t}{x_2 - 2}(x - 2).\nx1+2kx1+t(x+2)=x2−2kx2+t(x−2).\n交叉相乘得：\n(kx1+t)(x2−2)(x+2)=(kx2+t)(x1+2)(x−2).\n(kx_1 + t)(x_2 - 2)(x + 2) = (kx_2 + t)(x_1 + 2)(x - 2).\n(kx1+t)(x2−2)(x+2)=(kx2+t)(x1+2)(x−2).\n展开并整理，可以解出 xQ x_Q xQ。经过复杂但标准的代数运算，可以得到：\nxQ=4⋅x1x2−2(x1+x2)x1+x2−4.\nx_Q = 4 \\cdot \\frac{x_1 x_2 - 2(x_1 + x_2)}{x_1 + x_2 - 4}.\nxQ=4⋅x1+x2−4x1x2−2(x1+x2).\n将韦达定理的结果 x1+x2=−2ktk2+4 x_1 + x_2 = -\\frac{2kt}{k^2 + 4} x1+x2=−k2+42kt 和 x1x2=t2−16k2+4 x_1 x_2 = \\frac{t^2 - 16}{k^2 + 4} x1x2=k2+4t2−16 代入，化简后得到：\nxQ=−4tk.\nx_Q = -\\frac{4t}{k}.\nxQ=−k4t.第七步：计算 xP⋅xQ x_P \\cdot x_Q xP⋅xQ#点 P P P 的横坐标为 xP=−tk x_P = -\\frac{t}{k} xP=−kt，点 Q Q Q 的横坐标为 xQ=−4tk x_Q = -\\frac{4t}{k} xQ=−k4t。因此：\nxP⋅xQ=(−tk)(−4tk)=4t2k2.\nx_P \\cdot x_Q = \\left(-\\frac{t}{k}\\right) \\left(-\\frac{4t}{k}\\right) = \\frac{4t^2}{k^2}.\nxP⋅xQ=(−kt)(−k4t)=k24t2.\n注意到 k k k 和 t t t 的具体值不影响最终的乘积形式，且根据对称性和几何性质，可以确定 xP⋅xQ x_P \\cdot x_Q xP⋅xQ 为一个常数。通过进一步分析或直接利用椭圆的几何性质，可以得出：\nxP⋅xQ=4.\nx_P \\cdot x_Q = 4.\nxP⋅xQ=4.最终答案#4\n\\boxed{4}\n4Fine-grained Image Understanding (5/5)\nNextUser这是什么Qwen2.5-VL-32B这张图片展示的是一道中式火锅，具体来说是四川麻辣火锅。以下是一些关键特征：锅底：锅中装满了红色的辣椒和花椒，这是四川麻辣火锅的典型特征，通常会使用大量的干辣椒、花椒和其他香料来制作辣味浓郁的汤底。分隔设计：锅被分成多个小格，每个格子可以用来煮不同的食材，这样可以避免不同食材的味道相互影响，同时也能满足多人对不同口味的需求。周围配菜：可以看到一些切好的肉类（如牛肉片）、蔬菜和其他食材，这些都是火锅常见的涮菜。氛围感：整体画面呈现出一种热气腾腾、色彩鲜艳的感觉，符合火锅的热闹和美味的氛围。火锅简介：#四川麻辣火锅是中国火锅的一种，起源于四川省，以其独特的麻辣风味闻名。主要特点包括：使用大量辣椒和花椒，味道辛辣且麻。汤底通常由牛油或清油制成，加入各种香料熬制。配菜丰富多样，包括肉类、海鲜、蔬菜等，适合多人一起享用。这种火锅不仅是一种美食，也是一种社交方式，非常适合家庭聚会或朋友聚餐。User图中为什么是四川火锅Qwen2.5-VL-32B从图片中可以看出，这是一道典型的四川火锅，主要依据以下几个特征：1. 锅底的麻辣风格#图中的锅底颜色鲜艳，呈现出明显的红色，这是四川火锅的典型特征。四川火锅以“麻辣”著称，通常会使用大量的干辣椒、花椒等调料来制作辣味浓郁的汤底。红色的辣椒和花椒是四川火锅的灵魂，它们不仅赋予了火锅独特的风味，还体现了四川人对“麻”和“辣”的热爱。2. 分隔设计#锅被分成了多个小格，这种设计在四川火锅中非常常见。每个格子可以用来煮不同的食材，避免不同食材的味道相互影响，同时也能满足多人对不同口味的需求。这种设计也符合四川火锅的多样性和灵活性，适合多人一起享用。3. 配菜种类#周围可以看到一些切好的肉类（如牛肉片）和其他食材，这些都是四川火锅常见的涮菜。四川火锅的配菜种类丰富，包括肉类、海鲜、蔬菜等，能够满足不同人的口味需求。4. 氛围感#整体画面呈现出一种热气腾腾、色彩鲜艳的感觉，充满了火锅的热闹和美味氛围。四川火锅以其麻辣鲜香的特点，深受人们喜爱，尤其是在寒冷的季节或聚会时，更是不可或缺的美食。5. 地域文化#四川火锅是中国火锅的一种，起源于四川省，以其独特的麻辣风味闻名。四川人对辣椒和花椒有着深厚的情感，这也体现在他们的饮食文化中。四川火锅正是这种文化的体现。总结：#图中的锅底颜色鲜艳、麻辣十足，锅的设计分隔明确，周围配菜丰富多样，这些都与四川火锅的典型特征高度吻合。因此，可以确定这是一道四川火锅。Next Step#While Qwen2.5-VL-32B has focused on optimizing subjective experience and mathematical reasoning through reinforcement learning—operating within the paradigm of “fast thinking”. Our next research direction will prioritize long and effective reasoning processes to push the boundaries of visual models in tackling highly complex, multi-step visual reasoning tasks.Citation#If you find our model helpful, feel free to cite it:@article{Qwen2.5-VL,\n  title={Qwen2.5-VL Technical Report},\n  author={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and Zhong, Humen and Zhu, Yuanzhi and Yang, Mingkun and Li, Zhaohai and Wan, Jianqiang and Wang, Pengfei and Ding, Wei and Fu, Zheren and Xu, Yiheng and Ye, Jiabo and Zhang, Xi and Xie, Tianbao and Cheng, Zesen and Zhang, Hang and Yang, Zhibo and Xu, Haiyang and Lin, Junyang},\n  journal={arXiv preprint arXiv:2502.13923},\n  year={2025}\n}",
    "summary": {
      "en": "In January, the Qwen2.5-VL series models were launched, receiving positive feedback. The new model, Qwen2.5-VL-32B-Instruct, has been optimized with reinforcement learning and is open-sourced under the Apache 2.0 license. Key features of the 32B model include:\n\n1. **Human-like Responses**: Outputs are more detailed and formatted to match human preferences.\n2. **Improved Math Skills**: Enhanced accuracy in solving complex mathematical problems.\n3. **Better Image Understanding**: Higher accuracy in image tasks like content recognition and visual reasoning.\n\nPerformance benchmarks show that Qwen2.5-VL-32B-Instruct outperforms comparable models, especially in multimodal tasks and subjective user experience evaluations.\n\nThe model also excels in text processing and has demonstrated strong capabilities in various demo cases, such as fine-grained image understanding and mathematical reasoning.\n\nFuture research will focus on developing long and effective reasoning processes for complex visual tasks to further enhance the model's capabilities.\n\nIf you find this model useful, you can cite it as a technical report.",
      "ko": "1월에 Qwen2.5-VL 시리즈 모델이 출시되었고, 긍정적인 반응을 얻고 있습니다. 새로운 모델인 Qwen2.5-VL-32B-Instruct는 강화 학습을 통해 최적화되었으며, Apache 2.0 라이선스 하에 오픈 소스입니다. 32B 모델의 주요 특징은 다음과 같습니다.\n\n첫째, 인간과 유사한 응답을 제공합니다. 출력 결과가 더 상세하고 인간의 선호에 맞게 형식화되었습니다. 둘째, 수학 능력이 향상되었습니다. 복잡한 수학 문제를 해결하는 데 있어 정확도가 높아졌습니다. 셋째, 이미지 이해 능력이 개선되었습니다. 콘텐츠 인식 및 시각적 추론과 같은 이미지 작업에서 더 높은 정확도를 보입니다.\n\n성능 벤치마크 결과, Qwen2.5-VL-32B-Instruct는 유사한 모델들보다 뛰어난 성능을 보이며, 특히 다중 모달 작업과 사용자 경험 평가에서 두드러진 성과를 나타냈습니다.\n\n이 모델은 텍스트 처리에서도 우수한 성능을 발휘하며, 세밀한 이미지 이해와 수학적 추론과 같은 다양한 데모 사례에서 강력한 능력을 입증했습니다.\n\n앞으로의 연구는 복잡한 시각적 작업을 위한 긴 효과적인 추론 과정을 개발하는 데 집중하여 모델의 능력을 더욱 향상시킬 계획입니다.\n\n이 모델이 유용하다고 생각되면 기술 보고서로 인용할 수 있습니다.",
      "ja": "1月にQwen2.5-VLシリーズのモデルが発表され、好評を得ています。新しいモデルであるQwen2.5-VL-32B-Instructは、強化学習を用いて最適化されており、Apache 2.0ライセンスのもとでオープンソースとして公開されています。この32Bモデルの主な特徴には、以下の点があります。\n\n人間のような応答が可能で、出力はより詳細で人間の好みに合わせた形式になっています。また、数学のスキルが向上し、複雑な数学問題の解決において高い精度を発揮します。さらに、画像理解能力も向上しており、コンテンツ認識や視覚的推論といった画像タスクの精度が高まっています。\n\n性能評価の結果、Qwen2.5-VL-32B-Instructは、同様のモデルと比較して特にマルチモーダルタスクやユーザー体験の評価において優れた結果を示しています。このモデルはテキスト処理にも優れており、細かな画像理解や数学的推論など、さまざまなデモケースで強力な能力を発揮しています。\n\n今後の研究では、複雑な視覚タスクに対する長期的かつ効果的な推論プロセスの開発に焦点を当て、モデルの能力をさらに向上させる予定です。このモデルが役立つと感じた場合は、技術報告として引用することができます。"
    }
  },
  {
    "id": "2fe01a4dc45a027a",
    "title": {
      "en": "Triforce – a beamformer for Apple Silicon laptops",
      "ko": "트라이포스: 애플 실리콘의 빔포머",
      "ja": "トライフォース登場！"
    },
    "type": "story",
    "url": "https://github.com/chadmed/triforce",
    "score": 536,
    "by": "tosh",
    "time": 1742827534,
    "content": "Triforce - a beamformer for Apple Silicon laptops\nTriforce implements a Minimum Variance Distortionless Response adaptive beamformer\nfor the microphone array found in the following Apple Silicon laptops:\n\nMacBook Pro 13\" (M1/M2)\nMacBook Air 13\" (M1/M2)\nMacBook Pro 14\" (M1 Pro/Max, M2 Pro/Max)\nMacBook Pro 16\" (M1 Pro/Max, M2 Pro/Max)\nMacBook Air 15\" (M2)\n\nDependencies\nTriforce tries to keep dependencies to a minimum. On top of the\ncrates specified in Cargo.lock, you will only require:\n\nLV2\n\nWhy?\nThe Apple Silicon laptops mentioned above have an array of microphones arranged\neither in a triangle or in a straight line. Much like with the speakers, Apple are trying way too hard to be\nfancy here, and implement an adaptive beamformer in userspace to try and isolate\nthe desired signal from background noise. Without beamforming, the array is far\ntoo sensitive and omnidirectional to be at all useful. Thus, to make it useful outside\nof macOS, we need a beamformer.\nExpectation management\nFinding accessible literature on any DSP-related topics, let alone something like\nwideband adaptive beamforming, is challenging to say the least. This is an attempt\nat a beamformer armed only with first year undergrad level engineering maths and some vague\nidea of the principles gleaned from various webpages and PDFs. Do not expect it to\noutperform Apple's implementation at this time. Patches to improve it are always welcome!\nKnown limitations\n\nnalgebra does not do any explicit SIMD optimisation, relying only on LLVM auto-vectorisation.\nPerformance and efficiency of matrix math routines are not very good.\nFollowing from that, we are not doing wideband decomposition due to the added computational\nburden. Without SIMD/NEON support, this is simply too slow for a realtime audio plugin.\nOutput is mono only. Much like with wideband decomposition, adding additional matrix processing\nto fake stereo output would be too computationally intensive",
    "summary": {
      "en": "**Summary of Triforce - Beamformer for Apple Silicon Laptops**\n\nTriforce is a software tool designed to improve microphone performance on certain Apple Silicon laptops, specifically:\n\n- MacBook Pro 13\" (M1/M2)\n- MacBook Air 13\" (M1/M2)\n- MacBook Pro 14\" (M1 Pro/Max, M2 Pro/Max)\n- MacBook Pro 16\" (M1 Pro/Max, M2 Pro/Max)\n- MacBook Air 15\" (M2)\n\nIt uses a technique called Minimum Variance Distortionless Response (MVDR) adaptive beamforming, which helps isolate desired sounds from background noise. This is important because the built-in microphones can pick up too much noise without such processing.\n\n**Key Points:**\n- Triforce has minimal dependencies, mainly requiring LV2.\n- The software aims to make the microphones more useful outside of macOS.\n- It is a work in progress and may not perform as well as Apple's built-in solution.\n- There are limitations, including:\n  - No advanced optimization for performance.\n  - It can only output mono sound.\n  - It does not handle wideband processing due to high computational demands.\n\nThe creator encourages contributions to improve the project.",
      "ko": "Triforce는 특정 Apple Silicon 노트북에서 마이크 성능을 향상시키기 위해 설계된 소프트웨어 도구입니다. 지원하는 모델은 MacBook Pro 13인치(M1/M2), MacBook Air 13인치(M1/M2), MacBook Pro 14인치(M1 Pro/Max, M2 Pro/Max), MacBook Pro 16인치(M1 Pro/Max, M2 Pro/Max), 그리고 MacBook Air 15인치(M2)입니다.\n\n이 소프트웨어는 최소 분산 왜곡 없는 응답(MVDR)이라는 기술을 사용하여 원하는 소리를 배경 소음에서 분리하는 데 도움을 줍니다. 내장 마이크는 이러한 처리가 없으면 지나치게 많은 소음을 잡아낼 수 있기 때문에 이 기술이 중요합니다.\n\nTriforce는 LV2라는 최소한의 의존성을 가지고 있으며, 주로 macOS 외부에서도 마이크를 더 유용하게 만들기 위해 개발되고 있습니다. 현재 진행 중인 프로젝트로, Apple의 내장 솔루션만큼 성능이 뛰어나지 않을 수 있습니다. 몇 가지 제한 사항도 존재하는데, 성능을 위한 고급 최적화가 없고, 모노 사운드만 출력할 수 있으며, 높은 계산 요구로 인해 광대역 처리를 지원하지 않습니다.\n\n프로젝트 개선을 위해 기여를 권장하고 있습니다.",
      "ja": "Triforceは、特定のApple Silicon搭載のノートパソコンにおいてマイクの性能を向上させるために設計されたソフトウェアツールです。対応するモデルは、MacBook Pro 13インチ（M1/M2）、MacBook Air 13インチ（M1/M2）、MacBook Pro 14インチ（M1 Pro/Max、M2 Pro/Max）、MacBook Pro 16インチ（M1 Pro/Max、M2 Pro/Max）、およびMacBook Air 15インチ（M2）です。\n\nこのツールは、最小分散歪みのない応答（MVDR）という手法を用いた適応ビームフォーミングを利用しています。この技術により、背景の雑音から必要な音を分離することが可能になります。内蔵マイクは、こうした処理がないと雑音を過剰に拾ってしまうため、非常に重要です。\n\nTriforceは依存関係が最小限で、主にLV2が必要です。このソフトウェアは、macOS以外でもマイクをより有効に活用できることを目指しています。ただし、現在進行中のプロジェクトであり、Appleの内蔵ソリューションと同等の性能を発揮するわけではありません。いくつかの制限もあります。具体的には、パフォーマンスのための高度な最適化が行われていないこと、出力がモノラル音声のみであること、計算負荷が高いため広帯域処理に対応していないことが挙げられます。\n\nプロジェクトの改善に向けて、貢献を呼びかけています。"
    }
  },
  {
    "id": "e49db46a233384da",
    "title": {
      "en": "Reflecting on WikiTok",
      "ko": "위키톡의 성찰",
      "ja": "ウィキトックの真実"
    },
    "type": "story",
    "url": "https://www.aizk.sh/posts/reflecting-on-wikitok",
    "score": 63,
    "by": "aizk",
    "time": 1742882951,
    "content": "Reflecting on WikiTok\n\nThree weeks ago, I launched Wikitok at 1:53 AM EST from the corner of my Brooklyn bedroom.\nAll the great software engineers I admire take time to write out their thoughts, so I'd like to do the same for WikiTok, while also sharing my experience in software in general. So much has transpired in a very short amount of time - I'm certain the lessons I learned will be valuable to anyone reading this.\nThe Background\nI built wikitok.io in about 2 hours (but not the iphone app that doesn't work, nor the play store rip off, nor wikitok.net but I'm getting ahead of myself). It all came from this tweet. Simply put it's infinite scrolling wikipedia. The details of the initial process can be found in this Ars Technica article - instead of rehashing those details too much I'm going to focus more on the code as well as the aftermath / what I learned from virality.\nThe Codebase\nThe tech stack is bun, react and typescript. I combined the code from the very first working commit and pasted it below, and we'll go through it line by line.\nimport React, { useState, useEffect, useRef, useCallback } from 'react';\n\n// Types\ninterface WikiArticle {\n   title: string;\n   extract: string;\n   pageid: number;\n   thumbnail?: {\n       source: string;\n       width: number;\n       height: number;\n   };\n}\n\ninterface WikiCardProps {\n   article: WikiArticle;\n   key?: React.Key;\n}\nRight now we're just defining the shape of the wiki data. Thumbnails are technically optional, which is why we have a ? after the type definition, but in later iterations I ditched articles without thumbnails altogether. The key helps React understand which items are new or have been removed from the list, but is actually just extra code from Claude in this instance.\n// WikiCard Component\nfunction WikiCard({ article }: WikiCardProps) {\n   return (\n       <div className=\"h-screen w-full flex items-center justify-center snap-start\">\n           <div className=\"bg-white max-w-lg w-full mx-4 rounded-xl shadow-xl p-6\">\n               {article.thumbnail && (\n                   <img\n                       src={article.thumbnail.source}\n                       alt={article.title}\n                       className=\"w-full h-48 object-cover rounded-lg mb-4\"\n                   />\n               )}\n               <h2 className=\"text-2xl font-bold mb-3\">{article.title}</h2>\n               <p className=\"text-gray-700\">{article.extract}</p>\n               <a\n                   href={`https://en.wikipedia.org/?curid=${article.pageid}`}\n                   target=\"_blank\"\n                   rel=\"noopener noreferrer\"\n                   className=\"mt-4 inline-block text-blue-500 hover:text-blue-600\"\n               >\n                   Read more →\n               </a>\n           </div>\n       </div>\n   );\n}\nThis component is the heart of the UI. We are essentially saying, \"for each article, render a div with a thumbnail, title, extract, and a link to the full article\", and it must be in the shape of the WikiCardProps interface. The only minor issue with this is that the curid parameter is technically the wrong way to query articles - as pointed out by an engineer from Wikipedia here.\nSo, what does it look like?\n\nBesides the accidental white text on the white background, not bad! It is however way too tiny, but we can fix that later. It seems Claude designed it from the start to only be TikTok sized.\n// Custom Hook for Wiki Articles\nfunction useWikiArticles() {\n   const [articles, setArticles] = useState<WikiArticle[]>([]);\n   const [loading, setLoading] = useState(false);\n\n   const fetchArticles = async () => {\n       setLoading(true);\n       try {\n           const response = await fetch(\n               \"https://en.wikipedia.org/w/api.php?\" +\n               new URLSearchParams({\n                   action: \"query\",\n                   format: \"json\",\n                   generator: \"random\",\n                   grnnamespace: \"0\",\n                   prop: \"extracts|pageimages\",\n                   grnlimit: \"5\",\n                   exchars: \"200\",\n                   exlimit: \"max\",\n                   explaintext: \"1\",\n                   piprop: \"thumbnail\",\n                   pithumbsize: \"400\",\n                   origin: \"*\",\n               })\n           );\n           const data = await response.json();\n           const newArticles = Object.values(data.query.pages).map((page: any) => ({\n               title: page.title,\n               extract: page.extract,\n               pageid: page.pageid,\n               thumbnail: page.thumbnail,\n           }));\n           setArticles((prev) => [...prev, ...newArticles]);\n       } catch (error) {\n           console.error(\"Error fetching articles:\", error);\n       }\n       setLoading(false);\n   };\n\n   return { articles, loading, fetchArticles };\n}\nThis is a custom hook that fetches the articles from Wikipedia's API.\nWe call the Wikipedia API for 5 new articles every fetch, parse the JSON response, and extend our existing article list using the spread operator.\nThe only thing that immediately jumps out is the \"any\" type - but I've spent a lot of time agonizing over needless type complexity in typescript projects without actually moving the project forward at all, essentially just using it as some sort of procrastination tool. This is a really devious trap to find yourself in.\nThe \"proper\" solution would be to define an interface like WikiApiResponse that describes the shape of the response, and then use that to type the response, but in a project this simple, it's clear that the use of \"any\" won't make a difference to the safety of the code - so don't worry too hard!\nWhat matters above all is that you ship something, anything at all.\nGet feedback, get users, and then iterate. The codebase has changed significantly since this initial commit, shaped by user feedback and bug fixes - your users will tell you what needs to be changed.\nI mentioned before that the hook helps us fetch articles, but how do we know when to fetch more articles?\n// Main App Component\nfunction App() {\n   const { articles, loading, fetchArticles } = useWikiArticles();\n   const observerTarget = useRef(null);\n\n   const handleObserver = useCallback(\n       (entries: IntersectionObserverEntry[]) => {\n           const [target] = entries;\n           if (target.isIntersecting && !loading) {\n               fetchArticles();\n           }\n       },\n       [loading, fetchArticles]\n   );\n\n   useEffect(() => {\n       const observer = new IntersectionObserver(handleObserver, {\n           threshold: 0.5,\n       });\n\n       if (observerTarget.current) {\n           observer.observe(observerTarget.current);\n       }\n\n       return () => observer.disconnect();\n   }, [handleObserver]);\n\n   useEffect(() => {\n       fetchArticles();\n   }, []);\n\n   return (\n       <div className=\"h-screen w-full bg-black text-white overflow-y-scroll snap-y snap-mandatory\">\n           {articles.map((article: WikiArticle) => (\n               <WikiCard key={article.pageid} article={article} />\n           ))}\n           <div ref={observerTarget} className=\"h-10\" />\n           {loading && (\n               <div className=\"h-screen w-full flex items-center justify-center\">\n                   Loading...\n               </div>\n           )}\n       </div>\n   );\n}\n\nexport default App;\nThis is the most interesting bit of code in the project. In my head, the way I conceptualized this was that there should always be a buffer of articles underneath the current article, and it's triggered as the user continues to scroll. Not too many fetched where it slows the user experience but enough so that they never reach the \"end\" of wikipedia.\nClaude's idea was to create an invisible div at the bottom of our article list and attach an observer to it. When this div becomes visible (meaning the user has scrolled near the bottom), we fetch more articles. All in all, pretty clever. Truthfully, I've never designed an infinite scrolling interface before, so I was completely winging it.\nIs this code excellent? No. But was this code ready to be shipped? No not even close, I had to go back and make a dozen more fast commits before I felt comfortable publishing the project.\nBut did this get the ball rolling and made me realize what I could build? Yes.\nThe Development Process\nMy thought process working on wikitok was completely backwards to what standard software engineering principles would tell you - I let Claude with Cursor do the vast majority of the work, where I gave it the specs I wanted and I prompted away. I maintained a running list of all the features I wanted, testing and checking things off one by one, committing quick wins. Get the feature, take a peek at the code, move on to the next requirement, maybe compartmentalize some of the code here and there so it's nice and functional (I mean that both in a programming sense and literal sense). It worked out well - for instance Claude surprised me at the start letting me know I wouldn't even need a backend to access Wikipedia's API (no CORS restriction) which made deployment far easier for me. Interestingly, Wikimedia themselves later asked about my experience using their API, noting that many developers struggle with finding and parsing their documentation - they were surprised when I mentioned I hadn't used their docs for the MVP at all. You can find their docs here.\nThis approach can backfire if you try it in a professional setting. So why did I roll with it?\nWell for a start - I've had thousands of conversations with different AIs, and I've developed a pretty decent radar for their capabilities, when they make things up, when to completely ignore them, etc. The caveat to this is I'd imagine this wouldn't be the case if I started building in a language I've never used before. I also had a gut feeling from the start the MVP would be less than 500 lines, which is more than enough for the context of any modern LLM. The last point is that speed was the real priority here. There was already a tweet gaining momentum asking for \"All of wikipedia on one page\", which is what inspired the entire project. Building wikitok with that exact tweet in mind, knowing people might see the two tweets side by side proved to be wildly helpful.\nTo be clear to any junior engineers reading this (I'm kinda talking to myself here) - with current AI models, vibing your way to success will completely break down once you try to contribute to a serious production codebase. I think about this a lot, and I learned this the hard way when it came time to contribute to the Minecraft Benchmark, as the project is thousands and thousands of lines of code. But for simple, small projects? Some random super specific, monotonous office task? Lean in on the AI.\nUltimately, even with AI, software engineering will still be the job of giving computers precise instructions to do things, but how we do that will change. The field reinvents itself each decade, and AI is no exception.\nLessons from Going Viral\nGoing viral is stressful. It's a bit like winning the lottery, except there's a catch. As you improve your skills, you get to enter more tickets into the lottery. This is really critical - it's still the lottery, it's unfair, but your odds can just keep getting better and better, and eventually one day everyone will notice what you've built. I've had viral tweets before, and I've made a bunch of quick websites, but this is the first time I combined the two.\nThe other point in this is that you have to know what to do when you get lucky. If this was the first ever React project I made, I am certain I would've fumbled the launch and completely screwed it up, but thankfully it wasn't! I quite like this point from Brandon Sanderson, he mentioned in his writing lectures that if you haven't written 3 novels, your job is not to sell books. You job is to write enough books to learn your process and what works for you.\nSo you're viral, now what? Focus your energy on the platform that helped you achieve virality because that's where the most eyeballs are. You won't be able to answer every question and comment people throw at you. I made that mistake, getting way too caught up in the frenzy.\nOn answering comments - not all comment sections are equal. I spent quite a lot of time responding to questions in the initial hacker news post because I knew going viral on hacker news and making a good positive impression on that site would push my post up, and from there I'd get an enormous amount of traffic. Hacker news also tends to be more critical of specific technical details than lets say a random news subreddit, so you might get more value out of it (well, not always). The general flow of information was something like twitter → hacker news → mainstream news outlets → reddit / youtube.\nAlso be prepared to not sleep much for the first day of the launch. You're trading a bit of your health for the experience, but it's worth it. I got 4 and a half hours of sleep when I launched WikiTok.\nSorry Bryan Johnson\nAt the height of the frenzy, I had two people knock on my door and I briefly freaked out, wondering if they were journalists that somehow decided it would be a good idea to show up to my house. Thankfully they were just 2 Verizon technicians conducting routine surveys.\nDealing with Journalists\nI would advise anyone who goes viral to only do interviews with journalists via email or dms (unless it's an actual in-person news interview). There's nothing explicitly wrong with a phone call, but it's far easier to be extremely precise with your words when it's in a written format. It will take longer to write and more often than not most of it will get discarded, but the writing process is extremely valuable.\nWhen a journalist does contact you, do some research on their account. Reminder: a blue checkmark doesn't mean anything on Twitter anymore, as I would later be reminded when dealing with a scammer posing as a journalist. Ask to chat only via email and dms if you have the time. You really want to collect your thoughts and make sure you say exactly what you want to say, compared to freewheeling a conversation. I learned that first when I had a conversation with Business Insider, and it was a pleasant experience, even though I don't think I relayed my message and ideas as well as I could have.\nI wrote about 2000 words for the Washington Post, which took me roughly 3 and a half hours (longer than WikiTok did to make). They only used a paragraph, but now I have a detailed written record of my process and reasoning.\nStill that wasn't enough, and even after explicitly ignoring a question about Elon Musk, they took an angle of \"Insert Elon Musk for clicks,\" which I found to be a bit annoying. There's a lesson here that there's only so much you can control when you go viral.\n(Illustration by Elena Lacey/The Washington Post; Tom Brenner/The Washington Post; iStock)\nYou'll need that precision because information degrades like a game of telephone as it spreads. I counted at least 5 international articles that had the wrong URL (somebody sniped wikitok.net, and people ran with it saying it was mine). You won't be able to correct every mistake, but call out the big ones (like journalists screwing up coverage, which I promise will happen). Focus on the main sources of information - that's all you can really control. Ultimately that's what it means for something to go viral - spreading rapidly across the internet, partially out of your control.\nThe other thing about virality that shocked me, and this is more specific to software engineering - is how strange of an experience it is to have an open source project go viral. It felt like I was on a fishing boat, and suddenly a dozen people climbed aboard and offered their advice on how I should steer the ship. Some people had good points, but others wanted to steer the ship completely in the wrong direction, and I had to be the ones to shut them down. A very weird feeling. A frequent request was adding some sort of \"algorithm\" to the project, which I'm sure would be neat and all, but it's not what I set out to build.\nSome journalists picked up on my sentiment and frustration from dealing with so many requests (specifically, the ones who wanted to add an algorithm to the project), and eventually labeled wikitok as a \"cure\" for doomscrolling, or saying that I built it out explicitly due to frustration with algorithms. I wouldn't really agree with that, but it's a good conversation to have. Personally, I felt it would be strange to add an algorithm and store user data on a project that's based off of Wikipedia's data - it just feels wrong and probably violates their creative commons license. How would I even ethically monetize that? Maybe everyone 100 scrolls you get a giant, annoying popup of Jimmy Wales asking for a donation or something. The omni-presence of algorithms, good and bad, deserves a blog post all of its own.\nCoincidentally, Alec Watson of Technology Connections just uploaded a fantastic video talking about the pitfalls of living lives served by algorithms, I'd encourage you to watch it here. He refers to the issue as \"Algorithmic Complacency\" and goes into great detail about it.\nWatch Out for Scams\nWatch out for phishing attempts. I had a sophisticated phishing attempt show up in my inbox.\n\nA TechCrunch journalist DM'd saying they wanted to chat, and they offered a calendar link to allow me to schedule something on their end. The link will send you to a twitter oauth screen asking if you wish to allow Calendly to access your account, but it's completely fake! It's a clever man-in-the-middle attack, completely bypassing your password or 2FA, and they get permission to post on your behalf. The last step from their end is to use your account as a crypto pump and dump scheme, making thousands. An example of such coin would be something like this random wikitok coin somebody made, but with 99.8% of the coin being held by the owner, I don't think even the most degenerate crypto bros would invest in that.\nFor me, I googled the TechCrunch journalist and realized that wasn't actually their account - and I also got lucky that I had happened to see a tweet about someone's experience with a phishing attempt on my feed a few hours earlier. I wonder if I hadn't seen that tweet, would I have fallen for it? The best thing I can do is post a tweet about it, write up this blog, and make sure people know. I'll leave two more very good threads here from Alex Banks and Danny Postma who both dealt with the same exact scam.\nIn retrospect, it seems like an obvious scam. But when you have 5 different real journalists DMing you and asking for interviews, on top of maintaining the site itself, I could have very easily fell for it, and I'm sure others have.\nI went through about maybe 200 DMs total. The vast majority were really positive, mostly people who were just excited and reaching out to me about Wikitok, which was wonderful! But I had to filter through the craziest shit imaginable. Here's a really unhinged crypto scam:\n\nBut those aren't the worst ones. The worst ones are the death threats. Not because I'm scared of them, but it's just sad to see what insane things people's brains were pretrained on as they grew up. I'm Jewish, I was cancelled before I was even born, 3000 years ago. I didn't get as many death threats as I usually do when I get viral tweet or something, but I did get a few. I'm not going to share them in this post but you can use your imagination. Maybe one day I'll write a detailed blog post about the Jewish experience in the 21st century.\nAbout Me\nI'm actually in the middle of a career transition. Less than a year ago, I was a civil engineer, doing everything from AutoCAD drafting to literally being in the trenches. I quit 2 years out of college because everyday I came home with a deep seated feeling that I chose the wrong career path. I dropped everything, moved to Brooklyn, and found communities where I could really learn to ship. Why did I decide to quit, even in the face of AI and a perfectly fine degree in civil engineering? One reason - I already had some actual programming experience at an internship in college. I wasn't hired to code at all, I just picked it up on my own at the company and started to make serious contributions, one stack overflow page at a time, so I knew it must be possible. The downside to this was that it made the tail end of college rather depressing, as this deep-seated internal conflict started to brew. Am I a software engineer? A civil engineer? Do these classes matter in the slightest? I suppose college really did prepare me for the real world.\nAnd with regards to AI taking jobs - it obviously will become a serious problem in future. But being a doomer right now is like lying down in a parking lot waiting to get run over - you're surrendering to a pointless outcome while the rest of the world keeps moving. There's still so much to build and accomplish. We don't know when AGI is coming, so why stand around and wait for it? Even when some sort of thing that we can agree on is AGI gets released, do you think it will be able to solve problems like creating peace in the Middle East? AGI does not encompass the human condition - there will still be work to do.\nWhat's Next?\nFor wikitok - not much. The website is mostly complete and currently I'm only reviewing simple pull requests. Before I wrote code, I used to make music as a hobby. In the world of music, when you put a song out there, it's set in stone. I'm still adjusting to the fact that software is the opposite - endlessly evolving, 24/7. I did create a sequel to wikitok called spacetok.io that allows you to scroll through images from NASA, after seeing some tweets talking about the idea. I knew the sequel wouldn't go viral, but I made it simply because it was fun. If you know anyone at NASA who'd like it, let them know!\nWhat's next for me? I'll be working on more projects like mcbench.ai. I'll also be diving into theoretical study to fill in my knowledge gaps that Claude is not able to provide, but my big priority is looking for a job. That was one reason I made wikitok - it seemed like fun and something I could put on my resume (so hey, why not check it out and send me an email if you're hiring).\nThanks for reading!Share",
    "summary": {
      "en": "### Summary of \"Reflecting on WikiTok\"\n\nThree weeks ago, the author launched WikiTok, a website offering infinite scrolling access to Wikipedia articles. They created the site in about two hours using a tech stack of Bun, React, and TypeScript. The project stemmed from a tweet and aimed to provide a simple way to explore Wikipedia content.\n\n**Key Points:**\n\n1. **Development Journey:**\n   - The author used a custom hook to fetch articles from Wikipedia's API, creating a UI component to display the articles.\n   - The development process was unconventional, relying heavily on AI assistance and quick iterations.\n\n2. **Viral Experience:**\n   - Going viral is both exciting and overwhelming, requiring quick responses and adjustments based on user feedback.\n   - The author emphasized focusing on the platform that brought them attention and managing interactions with users and journalists effectively.\n\n3. **Handling Media Attention:**\n   - The author advises communicating with journalists via email to ensure clarity and precision.\n   - They also experienced challenges with misinformation spreading about their project and had to navigate various requests and suggestions from users.\n\n4. **Phishing Awareness:**\n   - The author shared experiences with phishing attempts and the importance of verifying contacts, especially during heightened visibility.\n\n5. **Personal Background:**\n   - The author transitioned from civil engineering to software development, motivated by a desire for a more fulfilling career.\n\n6. **Future Plans:**\n   - The author intends to continue developing projects and is currently seeking job opportunities in software engineering.\n\nOverall, the author reflects on the rapid growth of WikiTok and the lessons learned about development, virality, and media engagement.",
      "ko": "세 주 전, 저자는 무한 스크롤 기능을 제공하는 위키피디아 기사 사이트인 WikiTok을 출시했습니다. 이 사이트는 약 두 시간 만에 Bun, React, TypeScript라는 기술 스택을 사용해 만들어졌습니다. 이 프로젝트는 한 트윗에서 시작되었으며, 위키피디아 콘텐츠를 간편하게 탐색할 수 있는 방법을 제공하는 것을 목표로 했습니다.\n\n저자는 위키피디아 API에서 기사를 가져오기 위해 커스텀 훅을 사용하고, 기사를 표시할 UI 컴포넌트를 만들었습니다. 개발 과정은 전통적이지 않았으며, AI의 도움을 많이 받았고 빠른 반복 작업을 통해 진행되었습니다.\n\n바이럴 경험은 흥미롭지만 동시에 압도적이기도 합니다. 사용자 피드백에 따라 신속하게 반응하고 조정해야 하기 때문입니다. 저자는 자신에게 주목을 끌어준 플랫폼에 집중하고, 사용자 및 언론과의 상호작용을 효과적으로 관리하는 것이 중요하다고 강조했습니다.\n\n저자는 언론과의 소통은 이메일을 통해 명확하고 정확하게 진행해야 한다고 조언했습니다. 또한, 프로젝트에 대한 잘못된 정보가 퍼지는 어려움도 겪었고, 사용자들로부터 다양한 요청과 제안을 처리해야 했습니다.\n\n저자는 피싱 시도에 대한 경험도 공유하며, 특히 주목받는 상황에서 연락처를 확인하는 것이 중요하다고 강조했습니다.\n\n저자는 토목공학에서 소프트웨어 개발로 전향했으며, 더 보람 있는 경력을 원했던 동기가 있었습니다. 앞으로도 프로젝트 개발을 계속할 계획이며, 현재 소프트웨어 엔지니어링 관련 직업 기회를 찾고 있습니다.\n\n전반적으로 저자는 WikiTok의 빠른 성장과 개발, 바이럴 현상, 미디어와의 소통에서 배운 교훈들을 되돌아보았습니다.",
      "ja": "著者は三週間前に、無限スクロールでウィキペディアの記事にアクセスできるウェブサイト「WikiTok」を立ち上げました。このサイトは、Bun、React、TypeScriptという技術を使って約二時間で作成されました。プロジェクトはツイートから始まり、ウィキペディアのコンテンツを簡単に探求できる方法を提供することを目的としています。\n\n開発の過程では、著者はウィキペディアのAPIから記事を取得するためのカスタムフックを使用し、記事を表示するためのUIコンポーネントを作成しました。この開発プロセスは従来の方法とは異なり、AIの支援を大いに活用し、迅速な反復作業に依存していました。\n\nバイラルになることは刺激的でありながらも圧倒される体験で、ユーザーからのフィードバックに基づいて迅速に対応し、調整する必要があります。著者は、自分に注目を集めたプラットフォームに集中し、ユーザーやジャーナリストとのやり取りを効果的に管理することの重要性を強調しました。\n\nメディアの注目を扱う際には、ジャーナリストとのコミュニケーションをメールで行い、明確さと正確さを確保することが大切だと著者はアドバイスしています。また、プロジェクトに関する誤情報が広がるという課題にも直面し、ユーザーからのさまざまなリクエストや提案に対処しなければなりませんでした。\n\n著者はフィッシングの試みについての経験を共有し、特に注目を集めているときには連絡先を確認する重要性を強調しました。著者は土木工学からソフトウェア開発に転身し、より充実したキャリアを求める動機がありました。\n\n今後の計画として、著者はプロジェクトの開発を続ける意向を示しており、現在はソフトウェアエンジニアリングの職を探しています。全体として、著者はWikiTokの急成長と、開発、バイラル性、メディアとの関わりについて学んだ教訓を振り返っています。"
    }
  },
  {
    "id": "08222b77479d8480",
    "title": {
      "en": "Palantir suggests 'common operating system' for UK govt data",
      "ko": "영국 정부 데이터 통합 시스템 제안",
      "ja": "英国政府の共通OS提案"
    },
    "type": "story",
    "url": "https://www.theregister.com/2025/03/25/palantir_covid_inquiry_comments/",
    "score": 15,
    "by": "rntn",
    "time": 1742902335,
    "content": "Databases\n\n            9\n\nPalantir suggests 'common operating system' for UK govt data\n\n            9\n\n'Don't wait for another pandemic or civil challenge,' says US spy-tech biz\n\nLindsay Clark\n\nTue25Mar2025                                     //\n11:00UTC\n\nComment It might take a particularly shameless company to grasp the opportunity presented by the UK's coronavirus pandemic and step in with a sales pitch. US spy-tech biz Palantir is willing to give it a go.\nIn a witness statement to the UK COVID-19 Inquiry [PDF], an ongoing independent public inquiry into the nation's response to the pandemic (in which around 208,000 people died), Louis Mosley, executive veep of Palantir Technologies UK, said the government should invest in a \"common operating system\" for its data, encompassing departments such as the Department for Work and Pensions and local authorities.\nThe government should \"deploy this common operating system capability immediately and not wait until the next pandemic or civil challenge on the scale of COVID-19 is already underway. An investment of this kind is already long overdue,\" he added.\n\nHealth crusaders prep legal challenge over NHS mega contract with Palantir\nREAD MORE\nPalantir was founded by Peter Thiel, who made his money and name by co-founding PayPal. It attracted early investment from the US Central Intelligence Agency's venture capital arm, In-Q-Tel, and won early contracts in US defense and intelligence applications of its data analytics technologies, along the way supporting US Immigration and Customs Enforcement, an agency that has been accused of separating children from their families during enforcement actions.\nHealthcare quango NHS England first started working with the US data analytics corporation in March 2020, during the height of the pandemic.\n\n            <a href=\"https://pubads.g.doubleclick.net/gampad/jump?co=1&amp;iu=/6978/reg_software/databases&amp;sz=300x50%7C300x100%7C300x250%7C300x251%7C300x252%7C300x600%7C300x601&amp;tile=2&amp;c=2Z-Kc9dqy3ORbe3gXuv-yiQAAAII&amp;t=ct%3Dns%26unitnum%3D2%26raptor%3Dcondor%26pos%3Dtop%26test%3D0\" target=\"_blank\">\n                <img src=\"https://pubads.g.doubleclick.net/gampad/ad?co=1&amp;iu=/6978/reg_software/databases&amp;sz=300x50%7C300x100%7C300x250%7C300x251%7C300x252%7C300x600%7C300x601&amp;tile=2&amp;c=2Z-Kc9dqy3ORbe3gXuv-yiQAAAII&amp;t=ct%3Dns%26unitnum%3D2%26raptor%3Dcondor%26pos%3Dtop%26test%3D0\" alt=\"\">\n            </a>\n\nPalantir was initially handed a nominal £1 contract to work on a COVID-19 data store, along with cloud providers AWS, Google, and Microsoft Azure, and Faculty, a UK AI company. Without open competition, its contract was expanded to a £1 million ($1.29 million) agreement, then a £23 million ($29.7 million) arrangement was signed in December 2020.\n\nPalantir designed to 'power the West to its obvious innate superiority,' says CEO\n\nWe can't make this stuff up: Palantir, Anduril form fellowship for AI adventures\n\nKey aspects of Palantir's Federated Data Platform lack legal basis, lawyers tell NHS England\n\nPrior UK government planned £485M four-year budget for Palantir-based healthcare system\n\nThat deal was subject to the threat of judicial review from campaigners, who argued the contract represented such a change in data usage it warranted public consultation under British data protection law.\nNHS England then extended the contract by six months for £11.5 million ($14.8 million), and awarded a £24.9 million ($32.1 million) deal to cover the one-year transition to a new Federated Data Platform, the £330 million ($426 million) project which Palantir won following an open competition.\n\nIn his submission, Mosley was keen to underscore Palantir's involvement in the government's response to the COVID emergency, including work to distribute ventilators, vaccines, and personal protective equipment.\nAmong the benefits of introducing the imagined data \"operating system\" across government would be the ability to \"optimize procurement, among other things.\"\n\n                <a href=\"https://pubads.g.doubleclick.net/gampad/jump?co=1&amp;iu=/6978/reg_software/databases&amp;sz=300x50%7C300x100%7C300x250%7C300x251%7C300x252%7C300x600%7C300x601&amp;tile=4&amp;c=44Z-Kc9dqy3ORbe3gXuv-yiQAAAII&amp;t=ct%3Dns%26unitnum%3D4%26raptor%3Dfalcon%26pos%3Dmid%26test%3D0\" target=\"_blank\">\n                    <img src=\"https://pubads.g.doubleclick.net/gampad/ad?co=1&amp;iu=/6978/reg_software/databases&amp;sz=300x50%7C300x100%7C300x250%7C300x251%7C300x252%7C300x600%7C300x601&amp;tile=4&amp;c=44Z-Kc9dqy3ORbe3gXuv-yiQAAAII&amp;t=ct%3Dns%26unitnum%3D426raptor%3Dfalcon%26pos%3Dmid%26test%3D0\" alt=\"\">\n                </a>\n\nMaybe the company should be careful what it wishes for, though. The UK procurement for data analytics software seemed to be pretty well optimized toward Palantir winning contracts at the time of the COVID outbreak. ®\n                                    <strong>Get our</strong> <a href=\"https://whitepapers.theregister.com/\" style=\"text-transform:uppercase\">Tech Resources</a>\n\n        Share\n\n            More about\n\n                Government of the United Kingdom\n\n                Palantir\n\n        More like these\n\n            ×\n\n                    More about\n\n                Government of the United Kingdom\n\n                Palantir\n\n                    Narrower topics\n\n                Cabinet Office\n\n                Competition and Markets Authority\n\n                Computer Misuse Act\n\n                DCMS\n\n                GCHQ\n\n                HMRC\n\n                Home Office\n\n                ICO\n\n                NCSC\n\n                NHS\n\n                RPA\n\n                    Broader topics\n\n                Government\n\n                United Kingdom\n\n                    More about\n\n        Share\n\n                9\n\n                    COMMENTS\n\n            More about\n\n                Government of the United Kingdom\n\n                Palantir\n\n        More like these\n\n            ×\n\n                    More about\n\n                Government of the United Kingdom\n\n                Palantir\n\n                    Narrower topics\n\n                Cabinet Office\n\n                Competition and Markets Authority\n\n                Computer Misuse Act\n\n                DCMS\n\n                GCHQ\n\n                HMRC\n\n                Home Office\n\n                ICO\n\n                NCSC\n\n                NHS\n\n                RPA\n\n                    Broader topics\n\n                Government\n\n                United Kingdom\n\n        TIP US OFF\n        Send us news",
    "summary": {
      "en": "Palantir, a US data analytics company, has proposed that the UK government create a \"common operating system\" for data management across various departments to better prepare for future crises, like pandemics. Louis Mosley, an executive at Palantir, emphasized the need for immediate action rather than waiting for another emergency.\n\nPalantir began working with NHS England during the COVID-19 pandemic in March 2020 and initially received a small contract that grew significantly without open competition. Their involvement included assisting with the distribution of critical medical supplies. The company argues that a unified data system could improve procurement and other government operations.\n\nHowever, their contracts have faced legal scrutiny, with campaigners arguing that changes in data usage require public consultation. Despite this, Palantir's role in the UK's pandemic response highlights its influence and the growing reliance on its data services.",
      "ko": "미국 데이터 분석 회사인 팔란티어가 영국 정부에 다양한 부서 간 데이터 관리를 위한 \"공통 운영 체제\"를 만들 것을 제안했습니다. 이는 향후 팬데믹과 같은 위기에 더 잘 대비하기 위한 것입니다. 팔란티어의 루이스 모슬리 임원은 또 다른 비상 사태를 기다리기보다는 즉각적인 조치가 필요하다고 강조했습니다.\n\n팔란티어는 2020년 3월 COVID-19 팬데믹 동안 NHS 잉글랜드와 협력하기 시작했으며, 초기에는 소규모 계약을 체결했지만 경쟁 없이 계약 규모가 크게 증가했습니다. 이들은 중요한 의료 물자의 배급을 지원하는 역할을 했습니다. 팔란티어는 통합된 데이터 시스템이 조달 및 기타 정부 운영을 개선할 수 있다고 주장하고 있습니다.\n\n하지만 이들의 계약은 법적 검토를 받고 있으며, 캠페인 단체들은 데이터 사용의 변화가 공공 협의를 필요로 한다고 주장하고 있습니다. 그럼에도 불구하고 팔란티어의 영국 팬데믹 대응에서의 역할은 그들의 영향력과 데이터 서비스에 대한 의존도가 증가하고 있음을 보여줍니다.",
      "ja": "アメリカのデータ分析会社パランティアは、イギリス政府に対して、さまざまな部門でのデータ管理のための「共通運用システム」を構築するよう提案しました。これは、将来の危機、特にパンデミックに備えるためです。パランティアの幹部ルイ・モズリー氏は、緊急事態を待つのではなく、今すぐ行動を起こす必要があると強調しました。\n\nパランティアは、2020年3月のCOVID-19パンデミックの際にイギリスの国民保健サービス（NHS）と協力を始めました。当初は小規模な契約を結びましたが、その後、競争なしに契約が大幅に拡大しました。彼らの役割には、重要な医療物資の配布支援が含まれていました。パランティアは、統一されたデータシステムが調達やその他の政府業務を改善できると主張しています。\n\nしかし、彼らの契約は法的な調査の対象となっており、活動家たちはデータの使用に関する変更には公の協議が必要だと主張しています。それにもかかわらず、パランティアのイギリスにおけるパンデミック対応への関与は、同社の影響力とデータサービスへの依存度の高まりを浮き彫りにしています。"
    }
  },
  {
    "id": "4ea41d504e487ba9",
    "title": {
      "en": "The Lost Towers of the Guelph-Ghibelline Wars",
      "ko": "잃어버린 전쟁의 탑들",
      "ja": "失われた塔の物語"
    },
    "type": "story",
    "url": "https://www.exurbe.com/the-lost-towers-of-the-guelph-ghibelline-wars/",
    "score": 43,
    "by": "wallflower",
    "time": 1742734721,
    "content": "The Lost Towers of the Guelph-Ghibelline Wars\n\n\t\t Posted by\n\n\t\t\t\t exurbe\n\n\t\t\ton\n\n\t\t\t\t\t January 15, 2025\n\n\t\t\t\t Florence, History, Inventing the Renaissance, Italy\n\n\t\t\t\t 12\n\nLooks fake, doesn’t it? This implausible Medieval forest of towers, as dense as Manhattan skyscrapers, is our best reconstruction of the town of Bologna at its height, toward the end of the Medieval Guelph-Ghibelline wars. We don’t see many such towers today… or think we don’t, but actually their remnants are all over Italy.\nOften when in Florence one sees buildings like this, where one section is rough stone standing out amid stucco neighbors.\n\nThese are actually the bottom nubs of Medieval stone towers. The town of San Gimigniano (below) is famous for having several still intact. Wealthy families built these as mini-fortresses within the city, where they could defend against riots, enemy families (think Montagues and Capulets) and invasion:\n\nSigns of wealth and prestige, these all-stone buildings were also fireproof, leading to a terrible but effective tactic: take your family, treasures & goods up into your tower then set fire to enemies’ homes and let the city burn around you while you sit safe above. This was VERY BAD for cities.\nstreet corner in San Gimigniano\nAfter many disasters, Florence’s solution was to BAN private buildings over a certain height, forcing everyone who had a tower to knock the top off down to regulation height, leaving these recognizable stone nubs all around the city. This round one below is the oldest (now a restaurant).\n\nMy favorite tower stub is this one, in Via dei Cerchi. I lived on the top floor for a year as a grad student, up 111 steps! I had calves of steel by spring, but the views from the top looked like someone had put a poster of Florence on the wall except it was a window!\n\nOnly city buildings were allowed to exceed the mandated height, which is why Florence’s skyline is now all special buildings: monastery bell towers, the cathedral & baptistery, Orsanmichele the city’s granary (tall to keep grain away from water & mice), the seat of government, and one special guy…\n\nThe tower on the right here is part of Bargello, the prison & police fortress, but it didn’t start that way. It was built by a private family, who sold it to the city when the law banning towers was passed, and the city incorporated it into their prison fort.\n\nThe city jail had to be a fortress in case someone from a powerful family was arrested and the family sent goons to break them out (those guys who bite their thumbs in the opening scene of Romeo & Juliet would *totally* have stormed the jail to bust Romeo out!).\n\nIn this photo you can see how the brick battlements are a later addition, added to the tower as part of its transformation from private fortress to public.\n\nWhat did Florence look like back when it had all its towers? Its long-time ally across the mountains Bologna is famous for still having two intact towers, but in the Middle Ages Bologna was known as the City of 100 Towers because so many families built them. The reconstructions look absolutely incredible. Florence didn’t have so many but did have dozens, so the richest part of the city center would have looked much like this. Much to the despair of the city fire brigade!\n\nSo, whether in a film or on the street, if you ever see a historic Italian city and walk along a block where for some reason one chunk of wall is stone and all the others smooth, you’re probably looking at a relic of the faction feuds that Guido Ruggiero aptly calls “The Italian 300 Years’ War.”\n\nI talk about this long war in “Inventing the Renaissance,” one of many points of continuity which show how the supposed difference between a bad “Dark Ages” and Renaissance “golden age” is 100% propaganda, but fascinating propaganda with a deep history.\nAnd I’ll share more tidbits like over the coming days as we countdown to the book’s release!\n\n                FacebookLiveJournalTwitterRedditLinkedInTumblrPush to KindleShare\n\n\t Bologna, Florence, Guelph, Inventing the Renaissance\n\n\t\t 12 Responses to “The Lost Towers of the Guelph-Ghibelline Wars”\n\n\t\t\t\tOllie said:\n\n\t\t\t 01-16-2025 11:00 AM\n\n\t\t\t\tYour photography is pretty great.\n\n\t\t\t\tJasonium said:\n\n\t\t\t 01-16-2025 12:49 PM\n\n\t\t\t\tI would like to commend you on your restraint. You had the perfect opportunity to write, “Looks like a bunch of baloney, doesn’t it?” in your opening, and you resisted.\n\n\t\t\t\tAbout that history you were taught… – Pylimitics said:\n\n\t\t\t 01-20-2025 11:55 AM\n\n\t\t\t\t[…] an era we know much less about than we think we do, and most of what we do think we know is wrong. The photo is from her site Ex Urbe, where you can read more about these things, and see some photos of modern-day Bologna and the […]\n\n\t\t\t\tMedieval Skyscrapers, Cars for Justice, and Russian Noir - JSTOR Daily said:\n\n\t\t\t 01-21-2025 1:45 PM\n\n\t\t\t\t[…] The Skyscrapers of Medieval Italy (Ex Urbe) by Ada Palmer In medieval Italy, factional wars led powerful families to build huge towers that served as little urban fortresses. Their stubs are still there today. […]\n\n\t\t\t\t2:00PM Water Cooler 1/21/2025 | naked capitalism said:\n\n\t\t\t 01-21-2025 7:12 PM\n\n\t\t\t\t[…] Lost Towers of the Guelph-Ghibelline Wars” [Ex Urbe]. “Looks fake, doesn’t it? This implausible Medieval forest of towers, as dense as […]\n\n\t\t\t\t2:00PM Water Cooler 1/21/2025 | naked capitalism – Buddy CRM News Test said:\n\n\t\t\t 01-21-2025 11:33 PM\n\n\t\t\t\t[…] Lost Towers of the Guelph-Ghibelline Wars” [Ex Urbe]. “Looks fake, doesn’t it? This implausible Medieval forest of towers, as dense as Manhattan […]\n\n\t\t\t\t2:00PM Water Cooler 1/21/2025 | bare capitalism - Lucky to know | All rights reserved. said:\n\n\t\t\t 01-22-2025 12:22 AM\n\n\t\t\t\t[…] Misplaced Towers of the Guelph-Ghibelline Wars” [Ex Urbe]. “Seems to be faux, doesn’t it? This implausible Medieval forest of towers, as dense as […]\n\n\t\t\t\tGorgeous Gary said:\n\n\t\t\t 01-22-2025 2:06 AM\n\n\t\t\t\tI feel like a certain structural engineer of your acquaintance will find this post quite intriguing! (LOL)\n\n\t\t\t\t2:00PM Water Cooler 1/21/2025 | naked capitalism - Neclink said:\n\n\t\t\t 01-22-2025 2:42 AM\n\n\t\t\t\t[…] Lost Towers of the Guelph-Ghibelline Wars” [Ex Urbe]. “Looks fake, doesn’t it? This implausible Medieval forest of towers, as dense as Manhattan […]\n\n\t\t\t\t2:00PM Water Cooler 1/21/2025 | naked capitalism - Agoku said:\n\n\t\t\t 01-22-2025 5:30 AM\n\n\t\t\t\t[…] Lost Towers of the Guelph-Ghibelline Wars” [Ex Urbe]. “Looks fake, doesn’t it? This implausible Medieval forest of towers, as dense as Manhattan […]\n\n\t\t\t\tThe Lost Towers of the Guelph-Ghibelline Wars – Mirabilis.ca said:\n\n\t\t\t 01-25-2025 10:37 PM\n\n\t\t\t\t[…] From Ex Urbe: The Lost Towers of the Guelph-Ghibelline Wars. […]\n\n\t\t\t\tEx Urbe said:\n\n\t\t\t 01-27-2025 4:04 PM\n\n\t\t\t\t[…] a post last week, I talked about how Renaissance towns used to be full of tall stone towers, built by rich families […]\n\n\t\t\t\tComments are closed.",
    "summary": {
      "en": "The article discusses the historical towers of Italy, particularly during the Guelph-Ghibelline wars. It highlights Bologna, known for having many towers, and Florence, where remnants of these towers can still be seen today as stone bases among other buildings. Wealthy families built these tall stone structures as defensive fortresses against conflicts and invasions. \n\nFlorence eventually banned the construction of private towers over a certain height to reduce fire risks and maintain safety, leading to the characteristic stubs of towers seen in the city today. These towers were symbols of wealth and power, and their presence reflected the intense family rivalries of the time. \n\nThe article also hints at the broader historical narrative, suggesting that the supposed divide between the \"Dark Ages\" and the Renaissance is more complex than commonly believed. The author plans to share more insights as they promote their upcoming book.",
      "ko": "이 기사는 이탈리아의 역사적인 탑들, 특히 겔프와 기벨린 전쟁 동안의 상황을 다루고 있습니다. 많은 탑으로 유명한 볼로냐와, 오늘날에도 다른 건물들 사이에서 돌로 된 기초가 남아 있는 피렌체를 강조합니다. 부유한 가문들은 갈등과 침략에 대비하기 위해 이러한 높은 돌 구조물을 방어 요새로 세웠습니다.\n\n피렌체는 결국 화재 위험을 줄이고 안전을 유지하기 위해 일정 높이 이상의 개인 탑 건설을 금지했습니다. 이로 인해 오늘날 도시에서 볼 수 있는 탑의 특유의 남은 부분들이 생겨났습니다. 이러한 탑들은 부와 권력의 상징이었으며, 그 존재는 당시의 치열한 가문 간의 경쟁을 반영했습니다.\n\n또한 이 기사는 \"암흑 시대\"와 르네상스 사이의 경계가 일반적으로 생각하는 것보다 더 복잡하다는 점을 암시합니다. 저자는 다가오는 책을 홍보하면서 더 많은 통찰을 공유할 계획입니다.",
      "ja": "この記事では、イタリアの歴史的な塔について、特にグエルフとギベリンの戦争の時代に焦点を当てています。ボローニャは多くの塔があることで知られ、フィレンツェではこれらの塔の名残が他の建物の中に石の基盤として今でも見ることができます。裕福な家族は、紛争や侵略に対抗するために、高い石造りの構造物を防御の要塞として建設しました。\n\nフィレンツェでは、火災のリスクを減らし、安全を保つために、一定の高さ以上の私有塔の建設が禁止されました。その結果、現在の街には特徴的な塔の断片が見られます。これらの塔は富と権力の象徴であり、その存在は当時の激しい家族間の対立を反映しています。\n\nまた、この記事はより広い歴史的な物語にも触れ、「暗黒時代」とルネサンスの間の分断が一般に考えられているよりも複雑であることを示唆しています。著者は、今後の著書を宣伝しながら、さらなる洞察を共有する予定です。"
    }
  },
  {
    "id": "36cbb80ba911a295",
    "title": {
      "en": "WWI's 'Dazzle' Camouflage Seemed Effective Due to Unexpected Optical Trick",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://gizmodo.com/wwis-famous-dazzle-camouflage-seemed-effective-due-to-unexpected-optical-trick-study-finds-2000577568",
    "score": 80,
    "by": "ilamont",
    "time": 1742680378,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "39377b593bb66aa1",
    "title": {
      "en": "A study reveals that deciduous trees' roots remain active in winter",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.creaf.cat/en/articules/deciduous-trees-roots-remain-active-winter",
    "score": 15,
    "by": "bryanrasmussen",
    "time": 1742671996,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "8049440cfd9b45fd",
    "title": {
      "en": "Three Hundred Years Later, a Tool from Isaac Newton Gets an Update",
      "ko": "뉴턴의 도구, 300년 만의 변신!",
      "ja": "ニュートンの道具、300年ぶりの進化"
    },
    "type": "story",
    "url": "https://www.quantamagazine.org/three-hundred-years-later-a-tool-from-isaac-newton-gets-an-update-20250324/",
    "score": 105,
    "by": "rbanffy",
    "time": 1742854616,
    "content": "Quanta Homepage\n\n                                        Physics\n\n                                        Mathematics\n\n                                        Biology\n\n                                        Computer Science\n\n                                        Topics\n\n                                        Archive\n\n                                        Blog\n\n                                        Columns\n\n                                        Interviews\n\n                                        Podcasts\n\n                                        Puzzles\n\n                                        Multimedia\n\n                                        Videos\n\n                                        About Quanta\n\n                                    An editorially independent publication supported by the Simons Foundation.\n\n                                    Follow Quanta\n\n    Facebook\n\n        Youtube\n\n        Instagram\n\n    RSS\n\n                Newsletter\n\n                    Get the latest news delivered to your inbox.\n\n                            Email\n\n                        Subscribe\n\n                        Recent newsletters\n\n                                    Gift Store\n\n                                        Shop Quanta gear\n\nNewsletter\n\n                    Get the latest news delivered to your inbox.\n\n                            Email\n\n                        Subscribe\n\n                        Recent newsletters\n\nQuanta Homepage\n\n                                        Physics\n\n                                        Mathematics\n\n                                        Biology\n\n                                        Computer Science\n\n                                        Topics\n\n                                        Archive\n\n        Saved articles\n\n                    Saved Articles\n                                            Create a reading list by clicking the Read Later icon next to the articles you wish to save.\n\n                            See all saved articles\n\n        Login\n\n                    Log out\n\n                    Change password\n\n                                Search\n\nHome\n\n                Three Hundred Years Later, a Tool from Isaac Newton Gets an Update\n\n        Comment\n\n        Save Article\n\n                    Read Later\n\n                                                Share\n\n    Facebook\n\n                            Copied!\n\n    Copy link\n         (opens a new tab)\n\n    Email\n\n    Pocket\n\n    Reddit\n\n    Ycombinator\n\nalgorithms\n    Three Hundred Years Later, a Tool from Isaac Newton Gets an Update\n\n        By\n\n                Kevin Hartnett\n\nMarch 24, 2025\n\n            A simple, widely used mathematical technique can finally be applied to boundlessly complex problems.\n\n        Comment\n\n        Save Article\n\n                    Read Later\n\nalgorithms\n    Three Hundred Years Later, a Tool from Isaac Newton Gets an Update\n\n        By\n\n                Kevin Hartnett\n\nMarch 24, 2025\n\n            A simple, widely used mathematical technique can finally be applied to boundlessly complex problems.\n\n        Comment\n\n        Save Article\n\n                    Read Later\n\nMichele Sclafani for Quanta Magazine\n\nEvery day, researchers search for optimal solutions. They might want to figure out where to build a major airline hub. Or to determine how to maximize return while minimizing risk in an investment portfolio. Or to develop self-driving cars that can distinguish between traffic lights and stop signs.\nMathematically, these problems get translated into a search for the minimum values of functions. But in all these scenarios, the functions are too complicated to assess directly. Researchers have to approximate the minimal values instead.\nIt turns out that one of the best ways to do this is by using an algorithm that Isaac Newton developed over 300 years ago. This algorithm is fairly simple. It’s a little like searching, blindfolded, for the lowest point in an unfamiliar landscape. As you put one foot in front of the other, the only information you need is whether you’re going uphill or downhill, and whether the grade is increasing or decreasing. Using that information, you can get a good approximation of the minimum relatively quickly.\nAlthough enormously powerful — centuries later, Newton’s method is still crucial for solving present-day problems in logistics, finance, computer vision and even pure math — it also has a significant shortcoming. It doesn’t work well on all functions. So mathematicians have continued to study the technique, figuring out different ways to broaden its scope without sacrificing efficiency.\nLast summer, three researchers announced the latest improvement (opens a new tab) to Newton’s method. Amir Ali Ahmadi (opens a new tab) of Princeton University, along with his former students Abraar Chaudhry (opens a new tab) (now at the Georgia Institute of Technology) and Jeffrey Zhang (opens a new tab) (now at Yale University), extended Newton’s method to work efficiently on the broadest class of functions yet.\n“Newton’s method has 1,000 different applications in optimization,” Ahmadi said. “Potentially our algorithm can replace it.”\n\n                    In the 1680s, Isaac Newton developed an algorithm for finding optimal solutions. Three centuries later, mathematicians are still using and honing his method.\n\n    Godfrey Kneller/Public Domain\n\nA Centuries-Old Technique\nMathematical functions transform inputs into outputs. Often, the most important feature of a function is its minimum value — the combination of inputs that produces the smallest possible output.\nBut finding the minimum is hard. Functions can have dozens of variables raised to high powers, defying formulaic analysis; graphs of their solutions form high-dimensional landscapes that are impossible to explore from a bird’s-eye view. In those higher-dimensional landscapes, said Coralia Cartis (opens a new tab) of the University of Oxford, “We want to find a valley. Some are local valleys; others are the lowest point. You’re trying to find these things, and the question is: What info do you have to guide you to that?”\nIn the 1680s, Newton recognized that even when you’re dealing with a very complicated function, you’ll still always have access to at least two pieces of information to help you find its deepest valley. First, you can calculate the function’s so-called first derivative, or slope: the steepness of the function at a given point. Second, you can compute the rate at which the slope itself is changing (the function’s second derivative).\n\n                    Amir Ali Ahmadi sees optimization problems everywhere he looks.\n\n    Archives of the Mathematisches Forschungsinstitut Oberwolfach\n\nSay you’re trying to find the minimum of some complicated function. First, choose a point on the function that you think might be close to the true minimum. Compute the function’s first and second derivatives at that point. These derivatives can be used to construct a special quadratic equation — a parabola if your function lives in a 2D plane, and a cuplike shape called a paraboloid if your function is higher dimensional. This quadratic equation, which is called a Taylor approximation, roughly resembles your function at the point you chose.\nNow calculate the minimum of the quadratic equation instead of the original — something you can do easily, using a well-known formula. (That’s because quadratic equations are simple; it’s when equations get more complicated that calculating the minimum becomes prohibitive.) You’ll get a point. Then plug the coordinates of that point back into your original function, and you’ll get a new point on the function that is, hopefully, closer to its true minimum. Start the entire process again.\nNewton proved that if you keep on repeating this process, you’ll eventually home in on the minimum value of the original, more complicated function. The method doesn’t always work, especially if you start at a point that’s too far away from the true minimum. But for the most part, it does. And it has some desirable attributes.\n\n    Mark Belan/Quanta Magazine;Source:arxiv:2305.07512 (opens a new tab)\n\nOther iterative methods, like gradient descent — the algorithm used in today’s machine learning models — converge toward the true minimum at a linear rate. Newton’s method converges toward it much faster: at a “quadratic” rate. In other words, it can identify the minimum value in fewer iterations than gradient descent. (Each iteration of Newton’s method is more computationally expensive than an iteration of gradient descent, which is why researchers prefer gradient descent for certain applications, like training neural networks. But Newton’s method is still enormously efficient, making it useful in all sorts of contexts.)\nNewton could have written his method to converge toward the true minimum value even faster if, instead of taking just the first and second derivatives at each point, he had also taken, say, the third and fourth derivatives. That would have given him more complicated Taylor approximations, with exponents greater than 2. But the whole crux of his strategy was to transform a complicated function into a simpler one. These more complicated Taylor equations were more than Newton could handle mathematically.\n\n                    Jeffrey Zhang and his co-authors wiggled functions in just the right way, allowing them to broaden the scope of a powerful optimization technique.\n\n    Courtesy of Jeffrey Zhang\n\n“Newton did it for degree 2. He did that because nobody knew how to minimize higher-order polynomials,” Ahmadi said.\nIn the centuries since, mathematicians have worked to extend his method, to probe how much information they can squeeze out of more complicated Taylor approximations of their functions.\nIn the 19th century, for instance, the Russian mathematician Pafnuty Chebyshev proposed a version of Newton’s method that approximated functions with cubic equations (which have an exponent of 3). But his algorithm didn’t work when the original function involved multiple variables. Much more recently, in 2021, Yurii Nesterov (now at Corvinus University of Budapest) demonstrated how to approximate functions (opens a new tab) of any number of variables efficiently with cubic equations. But his method couldn’t be extended to approximate functions using quartic equations, quintics and so on without losing its efficiency. Nevertheless, the proof was a major breakthrough in the field.\nNow Ahmadi, Chaudhry and Zhang have taken Nesterov’s result another step further. Their algorithm works for any number of variables and arbitrarily many derivatives. Moreover, it remains efficient for all these cases — something that until now wasn’t possible.\nBut first, they had to find a way to make a hard math problem a lot easier.\nFinding Wiggle Room\nThere is no fast, general purpose method for finding the minima of functions raised to high exponents. That’s always been the main limitation of Newton’s method. But there are certain types of functions that have characteristics that make them easy to minimize. In the new work, Ahmadi, Chaudhry and Zhang prove that it’s always possible to find approximating equations that have these characteristics. They then show how to adapt these equations to run Newton’s method efficiently.\nWhat properties make an equation easy to minimize? Two things: The first is that the equation should be bowl-shaped, or “convex.” Rather than having many valleys, it has just one — meaning that when you try to minimize it, you don’t have to worry about mistaking an arbitrary valley for the lowest one.\n\n                    Abraar Chaudhry and two colleagues recently found a way to improve a centuries-old method for finding the minima of functions.\n\n    Camille Carpenter Henriquez\n\nThe second property is that the equation can be written as a sum of squares. For example, 5x2 + 16x + 13 can be written as the sum (x + 2)2 + (2x + 3)2. In recent years, mathematicians have developed techniques for minimizing equations with arbitrarily large exponents so long as they are both convex and a sum of squares. However, those techniques were of little help when it came to Newton’s method. Most of the time, the Taylor approximation you use won’t have these nice properties.\nBut Ahmadi, Chaudhry and Zhang figured out how to use a technique called semidefinite programming to wiggle the Taylor approximation just enough to make it both a sum of squares and convex, though not so much that it became unmoored from the original function it was supposed to resemble.\nThey essentially added a fudge factor to the Taylor expansion, turning it into an equation that had the two desired properties. “We can change the Taylor expansion a bit to make it simpler to minimize. Think of the Taylor expansion, but modified a little bit,” Ahmadi said. He and his colleagues then showed that, using this modified version of the Taylor expansion — which involved arbitrarily many derivatives — their algorithm would still converge on the true minimum of the original function. Moreover, the rate of convergence would scale with the number of derivatives used: Just as using two derivatives allowed Newton to approach the true minimum at a quadratic rate, using three derivatives enabled the researchers to approach it at a cubic rate, and so on.\nAhmadi, Chaudhry and Zhang had created a more powerful version of Newton’s method that could reach the true minimum value of a function in fewer iterations than previous techniques.\n\n                Related:\n\n                                    Surprising Limits Discovered in Quest for Optimal Solutions\n\n                                    Risky Giant Steps Can Solve Optimization Problems Faster\n\n                                    How We Can Make Sense of Chaos\n\nLike the original version of Newton’s method, each iteration of this new algorithm is still computationally more expensive than methods such as gradient descent. As a result, for the moment, the new work won’t change the way self-driving cars, machine learning algorithms or air traffic control systems work. The best bet in these cases is still gradient descent.\n“Many ideas in optimization take years before they are made fully practical,” said Jason Altschuler (opens a new tab) of the University of Pennsylvania. “But this seems like a fresh perspective.”\nIf, over time, the underlying computational technology needed to run Newton’s method becomes more efficient — making each iteration less computationally expensive — then the algorithm developed by Ahmadi, Chaudhry and Zhang could eventually surpass gradient descent for all sorts of applications, including machine learning.\n“Our algorithm right now is provably faster, in theory,” Ahmadi said. He’s hopeful, he added, that in 10 to 20 years, it will also be so in practice.\n\nBy Kevin Hartnett\n                Contributing Writer\n\n                March 24, 2025\n\n                    View PDF/Print Mode\n\n                            algorithms\n\n                            applied math\n\n                            dynamical systems\n\n                            machine learning\n\n                            mathematics\n\n                    All topics\n\nShare this article\n\n    Facebook\n\n                            Copied!\n\n    Copy link\n         (opens a new tab)\n\n    Email\n\n    Pocket\n\n    Reddit\n\n    Ycombinator\n\n                    Newsletter\n\n                    Get Quanta Magazine delivered to your inbox\n\n                    Subscribe now\n\n                    Recent newsletters\n\n             (opens a new tab)\n\nThe Quanta Newsletter\n\n                    Get highlights of the most important news delivered to your email inbox\n\n                            Email\n\n                        Subscribe\n\n                        Recent newsletters\n                                             (opens a new tab)\n\nAlso in Mathematics\n\n                    ‘Once in a Century’ Proof Settles Math’s Kakeya Conjecture\n\n                geometry\n\n                    ‘Once in a Century’ Proof Settles Math’s Kakeya Conjecture\n\n        By\n\n                Joseph Howlett\n\n            March 14, 2025\n\n        Comment\n                2\n\n        Save Article\n\n                    Read Later\n\n                    Years After the Early Death of a Math Genius, Her Ideas Gain New Life\n\n                geometry\n\n                    Years After the Early Death of a Math Genius, Her Ideas Gain New Life\n\n        By\n\n                Joseph Howlett\n\n            March 3, 2025\n\n        Comment\n                3\n\n        Save Article\n\n                    Read Later\n\n                    After 20 Years, Math Couple Solves Major Group Theory Problem\n\n                group theory\n\n                    After 20 Years, Math Couple Solves Major Group Theory Problem\n\n        By\n\n                Leila Sloman\n\n            February 19, 2025\n\n        Comment\n                8\n\n        Save Article\n\n                    Read Later\n\nComment on this article\n\n                    Quanta Magazine moderates comments tofacilitate an informed, substantive, civil conversation. Abusive, profane, self-promotional, misleading, incoherent or off-topic comments will be rejected. Moderators are staffed during regular business hours (New York time) and can only accept comments written in English.\n\n        Show comments\n\nNext article\n                How Metabolism Can Shape Cells’ Destinies\n\nQuanta Homepage\n\n    Facebook\n\n        Youtube\n\n        Instagram\n\n                                        About Quanta\n\n                                        Archive\n\n                                        Contact Us\n\n                                        Terms & Conditions\n\n                                        Privacy Policy\n\n                        All Rights Reserved © 2025\n\n                    An editorially independent publication supported by the Simons Foundation.\n\n                        Simons Foundation\n\nQuanta Homepage\n\n    Facebook\n\n        Youtube\n\n        Instagram\n\nAbout Quanta\n\n                                        Archive\n\n                                        Contact Us\n\n                                        Terms & Conditions\n\n                                        Privacy Policy\n\nAll Rights Reserved © 2025\n\n                    An editorially independent publication supported by the Simons Foundation.\n\n                        Simons Foundation\n\nClose\n\n    Log in to Quanta\n\n            Use your social network\n\n                        Facebook                        Connect with Facebook\n\n                                                Connect with Google\n\n            or\n\n                    email\n\n                    password\n\n                    Remember me\n\n                        Forgot your password ?\n\n    Don't have an account yet?\n         Sign up\n\nClose\n\n        Forgot your password?\n        We’ll email you instructions to reset your password\n\n                email\n\nClose\n\n        Change your password\n        Enter your new password\n\n                Password\n\n                Retype new password\n\nClose\n\n        Sign Up\n\n                    First Name\n\n                    Last Name\n\n                    Email\n\n                    Password\n\n                    Retype Password\n\n            Creating an account means you accept Quanta Magazine's\n\n            Terms & Conditions and Privacy Policy",
    "summary": {
      "en": "Quanta Magazine focuses on various scientific topics like physics, mathematics, biology, and computer science. It aims to provide insightful articles, interviews, and multimedia content, supported by the Simons Foundation.\n\nA recent article discusses a significant update to Isaac Newton's algorithm, which helps find optimal solutions in complex mathematical problems. Researchers, led by Amir Ali Ahmadi, have improved Newton's method to work more efficiently across a broader range of functions. \n\nNewton's original algorithm uses the first and second derivatives of a function to find its minimum value, which is crucial for solving problems in logistics, finance, and AI. However, it has limitations when applied to more complex functions.\n\nThe new improvement enables the algorithm to handle functions with multiple variables and higher derivatives, making it more versatile while maintaining efficiency. This advancement may not immediately change applications like machine learning but holds promise for future improvements in computational technology, potentially outpacing traditional methods like gradient descent in the long run.",
      "ko": "퀀타 매거진은 물리학, 수학, 생물학, 컴퓨터 과학 등 다양한 과학 주제에 초점을 맞추고 있습니다. 이 매거진은 심온스 재단의 지원을 받아 통찰력 있는 기사, 인터뷰, 멀티미디어 콘텐츠를 제공하는 것을 목표로 하고 있습니다.\n\n최근 기사에서는 아이작 뉴턴의 알고리즘에 대한 중요한 업데이트를 다루고 있습니다. 이 알고리즘은 복잡한 수학 문제에서 최적의 해결책을 찾는 데 도움을 줍니다. 아미르 알리 아흐마디가 이끄는 연구팀은 뉴턴의 방법을 개선하여 더 넓은 범위의 함수에서 더 효율적으로 작동하도록 만들었습니다.\n\n뉴턴의 원래 알고리즘은 함수의 최소값을 찾기 위해 함수의 1차 및 2차 도함수를 사용합니다. 이는 물류, 금융, 인공지능 문제를 해결하는 데 매우 중요합니다. 그러나 이 방법은 더 복잡한 함수에 적용할 때 한계가 있습니다.\n\n새로운 개선 사항은 알고리즘이 여러 변수와 고차 도함수를 가진 함수를 처리할 수 있게 하여, 효율성을 유지하면서도 더 다양한 상황에 적용할 수 있도록 합니다. 이 발전은 기계 학습과 같은 응용 프로그램에 즉각적인 변화를 가져오지는 않겠지만, 미래의 계산 기술 개선에 대한 가능성을 지니고 있으며, 장기적으로는 전통적인 방법인 경량 하강법보다 더 나은 성능을 발휘할 수 있을 것으로 기대됩니다.",
      "ja": "クアンタマガジンは、物理学、数学、生物学、コンピュータサイエンスなどのさまざまな科学的トピックに焦点を当てています。シモンズ財団の支援を受けて、洞察に満ちた記事やインタビュー、マルチメディアコンテンツを提供することを目指しています。\n\n最近の記事では、アイザック・ニュートンのアルゴリズムに関する重要な更新が取り上げられています。このアルゴリズムは、複雑な数学的問題において最適な解を見つけるのに役立ちます。アミール・アリ・アフマディ氏が率いる研究者たちは、ニュートン法を改良し、より広範な関数に対して効率的に機能するようにしました。\n\nニュートンの元のアルゴリズムは、関数の最小値を見つけるためにその一階および二階導関数を使用します。これは、物流、金融、人工知能などの問題を解決する上で重要です。しかし、より複雑な関数に適用すると限界があります。\n\n新しい改良により、このアルゴリズムは複数の変数や高次導関数を扱えるようになり、効率を保ちながらも柔軟性が増しました。この進展は、機械学習のような応用にすぐに変化をもたらすわけではありませんが、計算技術の将来的な改善に期待が持てます。従来の勾配降下法を超える可能性も秘めています。"
    }
  },
  {
    "id": "d9f28312a418739d",
    "title": {
      "en": "The Great Barefoot Running Hysteria of 2010",
      "ko": "맨발 러닝 열풍 2010",
      "ja": "裸足ランニング熱狂"
    },
    "type": "story",
    "url": "https://runningshoescore.com/blog/barefoot-running-hysteria-of-2010",
    "score": 70,
    "by": "jonnycat",
    "time": 1742898502,
    "content": "Something went wrong!\n\n    Hang in there while we get back on track\n\n    Home\n\n    The Great Barefoot Running Hysteria of 2010\n\n03/25/25 • 9 minute read\n\nThe year was 2010.  “Ke$ha's Tik Tok”  was topping the Billboard charts.  Steve Jobs has just introduced a goofy new oversized iPhone called an “iPad”.  And in running forums across the internet far and wide, hoards of enthusiasts preached the gospel of a new way of running: without shoes.Google Trends interest in \"Barefoot Running\" from 2004 to presentThe “Great Barefoot Running Hysteria of 2010”, as I call it, took the amateur running world by storm.  Propelled by dramatic claims of performance improvements and injury prevention, barefoot running gave rise to a vocal (and often militant) contingent of enthusiasts and entirely new classes of footwear.  And then over the course of a several years, it faded away almost as quickly as it came, leaving behind changes in running shoes and culture forever.  In this post, we'll explore the history and legacy of the barefoot running movement.The Barefoot Running Movement & How It StartedBarefoot running is–of course–as old as humanity itself. In fact, people have run barefoot throughout most of human history, with the practice continuing today in several cultures, such as Kenya and indigenous peoples in Mexico.  In this sense, before addressing the modern origins of barefoot running, we need to talk about the origins of shod running.  The concept of running shoes as we understand them today, specifically designed to improve running efficiency and comfort, did not emerge until the late 19th and early 20th centuries. This evolution coincided with the rise of organized sports and recreational running, which spurred the development of footwear tailored to the specific needs of runners. The first breakthrough came in 1865 when an English shoemaker brilliantly suggested adding spikes to otherwise normal-looking dress shoes to make them suitable for cross-country running. Later, in the early 20th century, shoes with rubber soles were introduced, offering improved grip and shock absorption, marking a significant advancement in the design of running shoes.The result of these running shoe trends was running shoe stores filled with bulky, over-engineered clown shoes that promised to prevent running injuries, but probably didn't.Into more modern times, running shoes continued to evolve past their humble beginnings into heavier, more complex footwear with exotic materials like plastics & EVA foam.  In the 80s and 90s, many shoemakers became fixated on stability and the notion that pronation, the natural roll of the foot after it lands, was a cause for running injuries.  Running shoes were increasingly designed to try to prevent this movement with wedges of foam that support and stabilize the arch of the foot, a high heel-to-toe drop, and other questionable features like plastic air bubbles in the heel to cushion the foot with each step.The result of these running shoe trends was running shoe stores filled with bulky, over-engineered clown shoes that promised to prevent running injuries, but probably didn't, and were mostly just uncomfortable to use. You can see an example of this kind of design in action in the Nike Air Max 90, which is now also available in a more tasteful re-issue.  While the original was marketed as a running shoe, the modern re-issue is a nostalgic fashion shoe that you should definitely not run in.The original Nike Air Max 90A modern re-issue of the Nike Air Max 90 (from Nike.com)The barefoot running renaissance can thus be understood as a backlash to the dominant running shoe trends at the time.  The revival was driven by a growing dissatisfaction with traditional running shoes, which many believed contributed to injuries and impeded natural foot movement. Proponents of barefoot running argued that it encouraged a more natural gait, reducing the impact on the legs and back and enhancing the overall running experience.In 2004, inspired by Stanford athletes training barefoot, Nike introduced the Nike Free, a running shoe that bucked the bulky running shoe trends of the time with a super flexible sole and a minimal heel-to-toe offset.The original Nike Free (via Wikipedia)Then, in 2006, Vibram launched the 5-Fingers: a shoe intended to be as close to barefoot as possible, with a thin rubber sole and glove-like fit for the toes.  This was the shoe that truly ushered in the minimalist mindset: running shoes should enable a \"natural\" gait and that less is more. Vibram 5 Fingers (via Amazon)The barefoot-inspired Nike Free and Vibram 5 Fingers set the stage, but the real barefoot running revolution began not with a pair of shoes, but with a book about an indigenous tribe of Native Mexicans.Christopher McDougall's \"Born to Run\"While the over-engineered running shoe backlash had already started with shoes like the Nike Free and the Vibram 5 Fingers, the spark that lit the barefoot running powder keg was Christopher McDougall's 2009 bestseller, \"Born to Run\". The book, which explores the running habits of the Tarahumara Native Mexican tribe, known for their long-distance running ability, captivated the imagination of runners–and non-runners–everywhere. Book cover for \"Born to Run\" by Christopher McDougallThe Tarahumara, or Rarámuri, as they refer to themselves, are an indigenous people who reside in the rugged and remote Copper Canyon region of Northwestern Mexico. Renowned for their extraordinary long-distance running abilities, the Tarahumara have garnered international attention and admiration from runners and researchers alike. Their running habits, deeply embedded in their culture and lifestyle, are not merely for sport but serve practical and ceremonial purposes as well.McDougall's narrative suggested that modern running injuries were virtually non-existent among the Tarahumara.  They often ran barefoot or in minimal footwear, sandals crafted from leather and tire strips called huaraches.  The sandals provide minimal cushioning and protection, promoting a natural running form that many attribute to their low incidence of running-related injuries according to McDougall.  The popularity of McDougall's book sparked widespread curiosity and enthusiasm for barefoot running, and many readers happily ditched their bulky running shoes for new, minimalist alternatives.  As the barefoot running movement evolved, so did the market for running footwear, leading to the development of minimalist shoes designed to mimic the barefoot running experience while providing some protection from the hazards of rough terrain. The Influx of (Mostly Newbie) Barefoot RunnersBolstered by the popularity of \"Born to Run\", barefoot running was suddenly everywhere.  This period saw a surge in barefoot running clinics, forums, and social media groups where enthusiasts shared tips and experiences.  And as often happens, what started as a perfectly reasonable idea took on a life of its own and became dogmatic: barefoot running was the way to run.  The movement gave rise to a series of increasingly lofty and strongly worded claims: barefoot running prevented injuries; barefoot running was more efficient; heel striking was evil; barefoot running was the natural and therefore \"correct\" way to run.  The idea transcended running communities and seeped into popular culture prompting lofty headlines like this New York Times article, The Once and Future Way to Run.The idea even transcended the sport of running itself.  To many, the barefoot running movement was not merely about the act of running without shoes; it represented a broader philosophy seeking to embrace simplicity, natural form, and mindfulness in the pursuit of physical fitness and well-being.  The fervor around barefoot running bordered on religious.Despite the smug sense of superiority that some barefoot proponents projected, many of the most enthusiastic adopters of barefoot running were, in fact, novice and inexperienced runners.  While barefoot running enthusiasts were eager to point out elite runners training or racing barefoot, like Zola Budd, these barefoot elites are the exception rather than the rule.  Most serious and elite runners generally sat out the barefoot trend, or at least took a more nuanced approach.  Many advanced runners already gravitated towards less bulky shoes, and many that did adopt barefoot running did so as part of isolated workouts on soft surfaces.It's difficult to quantify this claim, but my personal memory of this period was that online discourse around running form and footwear was dominated by an aggressive mob mentality around barefoot running.   If you were running with shoes on, you were–according to the online mob–\"doin' it wrong\", as the internet was fond of saying at the time.  Though it generally didn't represent the opinions of more experienced runners, the barefoot running crowd was certainly the loudest. Opinions that didn't completely jive with the all-minimalist approach were often brutally and violently downvoted.The Backlash and The Downfall of Barefoot RunningDespite the enthusiasm and claims of the benefits of barefoot running, research into the practice was thin.  While some of the claims seemed logical, most of the evidence around the benefits of barefoot running was anecdotal.  Much of the argument hinged around barefoot running being the natural and thus correct way to run, and that running with over-engineered running shoes was unnatural and thus incorrect.Some will recognize this line of reasoning as the appeal to nature fallacy: a logical fallacy in which a subject is claimed to be good simply because it is natural.  The fallacy pops up frequently in health & medical settings when people extol the virtues of \"all-natural\" products or alternative medicines.  Sure, some natural products are healthy and beneficial; but so are a lot of deadly poisons.  Similarly, diet, where the appeal to nature is used to justify all sorts of sometimes conflicting food choices (throughout history humans have eaten wildly versatile diets).  This doesn't mean that natural is bad, it simply means that it's not a valid argument in and of itself for the benefits of barefoot running.As time went on and the research caught up with the trend, the results were mixed at best: some studies showed potential benefits, while others highlighted increased risks. Critics of barefoot running point to evidence showing an increased incidence of certain types of injuries, such as Achilles tendinitis and metatarsal stress fractures, among runners transitioning to barefoot or minimalist running without proper adaptation. Additionally, there's an obvious concern about the lack of protection from environmental hazards (e.g., sharp objects, rough terrain) when running without traditional footwear, which can lead to acute injuries. At the same time, many fledgling barefoot runners soon faced a reality check. Reports of injuries began to surface, casting doubt on the benefits of running without shoes. Podiatrists and sports medicine professionals started warning about the potential risks, especially for those with pre-existing foot conditions, those who transitioned too quickly, or those who tried to run too much. Many who stuck with the sport and aspired to run longer distances such as the marathon soon learned that it's simply difficult to put in the mileage required to excel at these distances without proper footwear.  Though you're sure to find counterexamples (I still see a small handful of barefoot runners at major running events like the Boston Marathon), most runners who are putting in 50, 60, 70, or more miles a week to race a marathon are doing so with shoes designed to help cushion the impact.Long-Lasting Changes to Running ShoesDespite the decline in barefoot running's popularity, its impact on the running shoe industry was undeniable. Recognizing the demand for a more natural running experience, shoe manufacturers began developing more lightweight, minimalist running shoes with less cushioning and a lower heel-to-toe drop. More importantly, the minimalist movement helped end the dominance of needlessly overbuilt running shoes.  Of course, highly supportive & motion control running shoes are still available, and some runners prefer to run in them, but it's no longer the dominant paradigm.Ironically, the minimalist shoe movement triggered its own backlash with the maximalist shoe movement of mega-cushioned shoes, ushered in by brands like HOKA.  But the maximalist trend is not a return to the overbuilt shoes of the 90s–in fact, it still incorporates minimalist concepts like lower heel-to-toe drop, lightweight materials, and placing less emphasis on the support and motion control that earlier running shoes relied on.  These innovations aimed to combine the benefits of barefoot running with the protection and support of traditional running shoes. Today, many of these features remain integral in modern running shoe design, marking a lasting legacy of the barefoot running movement.So in the end, while the barefoot running hysteria of 2010 may have been a short-lived trend, it sparked crucial conversations about running health and led to significant advancements in running footwear. Though no longer in the limelight, its impact continues to influence how we run and think about our running gear.\n\n    by Jon K.\n\n  Jon has been an avid runner for over 20 years with a marathon PR of 2:46.  He loves everything running & helping others find love for the sport as well.\n\n    Trending Deals\n    See all trending deals...\n\n      Men's Nike Pegasus 41 Ohio State\n\n          $94.97\n\n      Men's Nike Pegasus 41 Ravens\n\n          $94.97\n\n      Women's Nike Pegasus 41 Virginia\n\n          $94.97\n\n      Men's Nike Pegasus 41 Broncos\n\n          $94.97\n\n  Running Shoe Score tracks the pricing and availability of high quality running shoes at a variety of\n  retailers.  Compare prices & find deals on running shoes from top brands.  Pricing & availabilty updated daily!\n\n    Nike\n    | New Balance\n    | Brooks\n    | Asics\n    | Saucony\n    | HOKA\n    | ... and more!\n\n  Pages on this site may contain affiliate links",
    "summary": {
      "en": "In 2010, the barefoot running movement gained popularity, driven by claims of improved performance and injury prevention. This trend emerged as a reaction against bulky, over-engineered running shoes that were thought to cause injuries. \n\nHistorically, humans have run barefoot for most of our existence. Modern running shoes began developing in the late 19th century, evolving into complex designs in the 80s and 90s that prioritized stability over natural foot movement. This dissatisfaction with traditional shoes led to minimalist options, like the Nike Free and Vibram 5-Fingers, which aimed to promote a more natural running style.\n\nThe movement was further energized by Christopher McDougall's 2009 book, \"Born to Run,\" which showcased the Tarahumara tribe's remarkable long-distance running abilities, often done barefoot or in minimal footwear. This sparked a surge in barefoot running clinics and communities, although many newcomers to the sport embraced it dogmatically, often overlooking the potential risks.\n\nAs evidence emerged about the downsides of barefoot running—such as increased injury risks for those who transitioned too quickly—enthusiasm waned. Nonetheless, the movement led to lasting changes in the running shoe industry, with a rise in minimalist designs and a shift away from overly supportive shoes, influencing modern running footwear trends.\n\nUltimately, while the barefoot running craze was short-lived, it initiated important discussions about running health and left a significant mark on how running shoes are designed today.",
      "ko": "2010년, 맨발로 달리기 운동이 인기를 끌기 시작했습니다. 이는 성능 향상과 부상 예방에 대한 주장에 힘입은 것입니다. 이 트렌드는 부상을 유발한다고 여겨지는 크고 복잡한 러닝화에 대한 반발로 나타났습니다.\n\n역사적으로 인간은 대부분의 존재 기간 동안 맨발로 달렸습니다. 현대 러닝화는 19세기 후반에 개발되기 시작했으며, 80년대와 90년대에는 자연스러운 발 움직임보다 안정성을 우선시하는 복잡한 디자인으로 발전했습니다. 전통적인 신발에 대한 불만은 나이키 프리와 비브람 5-핑거스와 같은 미니멀리스트 옵션으로 이어졌고, 이는 보다 자연스러운 달리기 스타일을 촉진하고자 했습니다.\n\n이 운동은 2009년 크리스토퍼 맥두걸의 책 \"태어날 때부터 달리기 위해\"에 의해 더욱 활성화되었습니다. 이 책은 타라후마라 부족의 놀라운 장거리 달리기 능력을 소개했으며, 이들은 종종 맨발이나 최소한의 신발을 신고 달렸습니다. 이로 인해 맨발로 달리기 클리닉과 커뮤니티가 급증했지만, 많은 초보자들은 이 운동을 맹목적으로 받아들이며 잠재적인 위험을 간과하는 경우가 많았습니다.\n\n맨발로 달리기의 단점에 대한 증거가 나타나면서, 예를 들어 너무 빠르게 전환한 사람들에게 부상 위험이 증가하는 등의 문제가 제기되자 열기는 다소 식었습니다. 그럼에도 불구하고 이 운동은 러닝화 산업에 지속적인 변화를 가져왔고, 미니멀리스트 디자인이 증가하며 지나치게 지지하는 신발에서 벗어나는 경향이 생겼습니다. 이는 현대 러닝화 트렌드에 영향을 미쳤습니다.\n\n결국, 맨발로 달리기 열풍은 짧은 시간에 그쳤지만, 달리기 건강에 대한 중요한 논의를 촉발했으며 오늘날 러닝화 디자인에 큰 영향을 미쳤습니다.",
      "ja": "2010年、裸足でのランニング運動が注目を集めました。これは、パフォーマンスの向上や怪我の予防が期待できるという主張が背景にあります。このトレンドは、怪我を引き起こすと考えられていた大きくて過剰に設計されたランニングシューズに対する反発として生まれました。\n\n人類は長い間、裸足で走ってきました。近代的なランニングシューズは19世紀後半に登場し、1980年代や1990年代には安定性を重視した複雑なデザインへと進化しました。このような従来のシューズに対する不満から、ナイキのフリーやビブラムの5フィンガーのようなミニマリストシューズが登場し、より自然な走り方を促進することを目指しました。\n\nこの運動は、クリストファー・マクドゥーガルの2009年の著書『ボーン・トゥ・ラン』によってさらに盛り上がりました。この本では、タラウマラ族の素晴らしい長距離走の能力が紹介され、彼らはしばしば裸足または最小限の靴で走ります。この影響で、裸足ランニングのクリニックやコミュニティが増えましたが、多くの新しい参加者はこの運動を盲目的に受け入れ、潜在的なリスクを見落とすことがありました。\n\nしかし、裸足でのランニングには急激な移行による怪我のリスクがあることが明らかになると、熱意は次第に薄れていきました。それでも、この運動はランニングシューズ業界に持続的な変化をもたらし、ミニマリストデザインの増加や過剰なサポートを避ける傾向を生み出し、現代のランニングシューズのトレンドに影響を与えました。\n\n結局、裸足ランニングのブームは短命でしたが、ランニングの健康について重要な議論を引き起こし、今日のランニングシューズのデザインに大きな影響を与えました。"
    }
  },
  {
    "id": "9e37172a8989477d",
    "title": {
      "en": "Closing the Chapter on OpenH264",
      "ko": "OpenH264의 종말",
      "ja": "OpenH264の終焉"
    },
    "type": "story",
    "url": "https://bbhtt.space/posts/closing-the-chapter-on-openh264/",
    "score": 5,
    "by": "todsacerdoti",
    "time": 1742896798,
    "content": "People might have noticed me talking about dropping OpenH264 from\nFreedesktop SDK.\nHere, I’ll try to go a bit into the history, the timeline and what led\nto the final decision.\nA bit of an introduction\nIf you are unfamiliar with the Freedesktop SDK project: it was born out\nof the initial 1.6 Flatpak runtime image\ncreated by Alexander Larsson to\nprovide a host independent “runtime” for Flatpaks. Since then, with the\nhelp of Codethink and others, it grew\ninto an independent project maintained by the community that aims to\nprovide a “A minimal Linux runtime”.\nCurrently that includes the org.freedesktop.{Sdk, Platform}\nFlatpak runtimes,\na bunch of other Flatpak extensions\nsuch as the GL (Mesa) extensions, a bootable image stack that includes\nthings like the Linux kernel, firmware and drivers and a collection\nof docker images.\nThe runtimes (and more broadly the “Flatpak” stack) form the base for\nthe GNOME and\nKDE Flatpak\nruntimes which are collectively used by 2000+ Flatpaks, while the\nbootable stack along with other parts forms the base for things like\nGNOME OS.\nSome history\nH.264 is one of the most widely used\ncodecs today but unfortunately it is patented with several patents\nstill active.\nPatents like this are a blocker to shipping software dealing with the\ncodec in the base runtime (since we want it usable by a wide variety of\nvendors and free of any legal grey areas) and unfortunately makes life\ndifficult for everyone involved.\nTo workaround this and ship working software to users, 7 years ago, in\n2019 the precursor to the OpenH264 extension, called the\nhtml5-codecs extension\nwas added to the Freedesktop runtime by Tom Coldrick. The idea was\nsimple - it would contain support for FFMPEG’s internal H.264 decoder\nand would be an optional Flatpak extension since we were unable to ship\nit in the runtime itself.\nFast-forward a few months, in June 2019, Robert McQueen\nopened an issue\nto include Cisco’s OpenH264\n(commonly referred as libopenh264 too) as an extension to the runtime.\nlibopenh264 code is open source but due to the H.264 patents, no\nvendor is legally allowed to distribute their own binaries. The\nsolution to this was to distribute Cisco’s unmodified binaries directly\nto the user which would effectively be free of any royalties but the\ncatch is, the binaries have some license restrictions\non them.\nSo Endless around that time, added extra-data\nsupport to Flatpak. This meant that the Flatpak extension metadata\nwould only contain an URL to Cisco’s binaries, checksums and a “recipe”\nto make a working extension out of it. The binary would be downloaded\non the end user’s machine, inside a sandbox and then the “recipe”\nwould be executed to create a working extension. This avoids various\n“redistribution” restrictions entirely since we aren’t shipping the\nbinary itself.\nHowever, one last catch remained with this approach. Unless the end user\nsystem has downloaded the extension, a part of the ABI will be missing\nfrom the base runtime itself. Additionally, the extension would also\nget mounted to a non-standard location inside the Flatpak sandbox. So to\nbuild something against that we needed a “public” library that is\nincluded in the runtime itself.\nEndless came up with a stub OpenH264 library called noopenhh264,\nas a solution to this. This would be kept ABI/API compatible to the\nactual Cisco’s OpenH264 library and would live inside the runtime at\n$libdir to allow software to link to libopenh264 and provide a\nfallback. At runtime, if the user downloads the actual OpenH264\nextension, the stub library would get overridden by the library in\nextension through Flatpak’s ld.so config and voilà, you have a working\nOpenH264 setup!\nThe start of the openh264 extension\nFollowing the gruelling details above, in August 2019, Tom Coldrick\nagain added\nthis “noopenh264” library to the runtime and set up the extension point\ncalled org.freedesktop.Platform.openh264.\nAround that time, the development of the “stub” noopenh264 library also\nmoved under the Freedesktop SDK umbrella\nwhile the development of this extra-data extension was going on in a\ndedicated repository.\nI talked about a “recipe” of an extra-data Flatpak extension a while\nback, which is nothing but a set of commands in a special file called\napply_extra to stage and install the source. As an example, if it\nwas a .deb it would be a sequence of ar or bsdtar commands to\nextract it, then install it inside Flatpak’s extra-data prefix.\nThe catch here is that using any utility like that makes the extension\ndirectly dependent on a particular branch of the Flatpak runtime and thus\nthe API/ABI provided by it.\nAs every yearly major release of the Freedesktop runtime is a completely\nnew ABI, we would need to continuously spin off new branches of the\nOpenH264 extension i.e. for version 2.1.0 of libopenh264 we would need\norg.freedesktop.Platform.openh264//2.1.0-{18.08, 19.08, 20.08} branches\nfor the extension and so on. This would make it a bit of an chore to\nupdate the extension in Freedesktop runtime itself and also for\nour downstream runtimes.\nAs a solution to this, a custom apply_extra script was written,\nwhich utilised only a few standard headers and would be built statically\nagainst a fixed toolchain.\nThe meant a bit of complexity, since we aren’t allowed to use most\nthings that would make this process easier but it also meant the final\nextension was independent of the runtime API/ABI and was built on top\nof “NoRuntime”.\nThe flaws start to show up\nThis entire setup worked quite well for a long time despite some\nmaintenance issues from Cisco. However there were some flaws.\nA while back I told that every vendor is forced to distribute binaries\nhosted by Cisco at https://ciscobinary.openh264.org. Unfortunately\nciscobinary.openh264.org is missing a valid SSL certificate\nat least since 2014\nand Cisco neither supplies GPG signatures nor strong checksums like\nSHA-256 for the binary releases.\nThis meant that we have no way to verify the authenticity of Cisco’s\nbinaries, opening us to various classes of MiTM and supply-chain\nvulnerabilities. We reached out to Mozilla and via Fedora/RedHat to get\nCisco to fix this, as one can see the comments on the thread but\nnothing ever happened.\nMore recently (around 2024-2025), this SSL issue meant that the domain\nstarted getting DNS blocked\nand would make the extra-data download fail, botching an install.\nAnother critical issue was that if upstream did security fixes and then\nreleased them with an ABI break, we couldn’t fix it for the Flatpak\nruntimes. Distributing binaries directly through extra-data meant there\nis no scope for cherry-picking patches and the ABI break meant the new\nversion would only be able go to a new branch of the runtime.\nWe didn’t really have a choice here. If we dropped this extension\nconsidering the potential flaws, the entirety of the Flatpak user base\nwould loose H.264 decoding and encoding support, so we had to live with\nthe setup.\nThe start of the codecs-extra extension\nAround June 2024 gdk-pixbuf due to a series of security issues,\ndropped support\nfor a lot of “fringe” loaders. We immediately noticed the change as\nwe constantly analyse ABI in stable branches.\nTo compensate for the dropped loaders, we decided\nto add webp, avif, jxl, heif support (with pixbuf loaders) to the\nruntime. Around the same time, Will Thompson\nopened an issue\nto add libheif to the runtime with the problematic HEIC decoder as a\nruntime extension.\nConsequently, we decided that yet another extension just for two libheif\nplugins is not worth the maintenance effort and these would rather be\nin the ffmpeg-full extension (This was the successor to the\nhtml5-codecs extension, created around 2019-2020). So I again\nadded\nlibx265, libde265 and the corresponding libheif plugins to the\nffmpeg-full extension. After a few months I also added\nlibx264 to it too as we were already shipping libx265 and Cisco’s\nOpenH264 had suboptimal encoding performance.\nI was happy with the result and all of this shipped in Freedesktop SDK\n24.08. But some people were unhappy for valid reasons.\nWhen the ffmpeg-full extension was created, due to various concerns\nit was never added to the runtime or SDK manifest; it lived as an\noptional “app” extension - meaning app developers would need to add a\nshort snippet\nin their Flatpak manifest to use it.\nThis was problematic in some ways - sometimes app developers were\nunaware of the extension or that they needed it and the app manifest\nwas missing the snippet that allowed to use the extension. This made\nuser and developer experience a bit poor. Once we started\nshipping libheif in ffmpeg-full, it became slightly more\nproblematic as the actual libheif library was in the runtime but without\nthe extension it cannot decode any HEIF/HEIC image. So the extension\nwas practically mandatory. Albert Astals Cid,\npart of the team who maintains KDE Flatpaks on Flathub complained about\nthis and an issue was opened by Erick555\nto discuss the possibility of making ffmpeg-full a runtime extension.\nI was initially reluctant to do this for various reasons and the\nsituation was again a bit out of our hands (this time due to H.265\npatents) but we asked various parties such as Endless and quite\nsurprisingly it turned out no one really had an objection to making it\na runtime extension or even make it auto-download along with the\nruntime.\nAlong with this change, we decided to rename the ffmpeg-full extension\nagain, this time to codecs-extra to better reflect the fact that it\nwill not only contain FFMPEG with patented parts but other libraries\ntoo that deals with patented codecs. This would sort of be an equivalent\nto the various “meta-packages” that distros provide for patented or\nrestricted codecs.\nThus the seeds were planted and I switched ffmpeg-full to\ncodecs-extra a month ago.\nThis is supposed to ship in Freedesktop SDK 25.08. The major change for\napp developers is that there is no longer any need to have something\nlike\nadd-extensions:\n  org.freedesktop.Platform.ffmpeg-full:\n    version: '24.08'\n    directory: lib/ffmpeg\n    add-ld-path: .\nin the manifest. org.freedesktop.Platform.codecs-extra will be\nautomatically installed by the runtime and will be available to users.\nDropping the openh264 extension\nThe above codecs-extra change meant that we now didn’t really have an\nuse case for org.freedesktop.Platform.openh264 since codecs-extra\nhad FFMPEG’s internal H.264 decoder and the libx264 encoder.\nSo I initially disabled autodownload and considering the various “flaws”\nwith OpenH264 discussed above, I opened an issue\nfor planned retirement in 26.08.\nThis was completely overturned when a high severity flaw\nwas discovered in libopenh264 affecting versions 2.5.0 and earlier.\nThe 23.08 branch of the Freedesktop runtime was locked to 2.2.0 and\nupgrading to a fixed version was not possible due to multiple ABI breaks\nand SONAME bumps upstream. Patching was also not an option as I said\nearlier.\nWe scrambled to mitigate this. For 23.08, we decided to make an\n“ABI break” and drop the extension\nas updating was impossible. So I made a bit of an intimidating announcement.\nBut we still had 24.08 left which was on 2.4.1 and updating to the fixed\nversion i.e. 2.6.0 was again blocked due to an ABI bump.\nHowever, looking through the commits between 2.4.1 and 2.6.0 and\nanalysing the library through libabigail we couldn’t find an actual\nABI break that was made and considering Cisco in the past made\nunnecessary SONAME bumps, we tried patching the 2.6.0 release to\nprovide the older ABI.\nI messed\nwith that venerable apply_extra and wrote a small utility to patch\nSONAMEs. But upstream, in time, pointed us to the ABI break\nand this idea had to be scrapped entirely. (I later found out why\nliabigail was unable to show the ABI break after talking to Dodji\nSeketeli, the libabigail maintainer.)\nWe then asked upstream\nto provide a patch release 2.5.1 instead with the old ABI and to our\nsurprise they did within a few weeks! This shipped in 24.08.15 fixing\nthe 24.08 branch of the runtime.\nAfter all this debacle and the extra “headache”, none of us felt\ncomfortable shipping the openH264 extension anymore. Thus it was\ndropped\nfor the master branch, which means there won’t be any\norg.freedesktop.Platform.openh264 extension or noopenh264 for 25.08+.\nEpilogue\nConsidering all things, I think and hope we made the correct decision\nand hopefully the new org.freedesktop.Platform.codecs-extra works\nout. libx264, libx265 and others are built from source and there\nare no binaries or extra-data involved. So we should theoretically be\nable to patch and fix any issues that come up in the future.\nApart from all this, I’m slightly worried at the prospects of legal\nissues cropping up with this setup and also that the new extension\ncontains “too much”, but we will have to see where things flow.\nAs a fun tidbit, I wasn’t aware that Fedora was also using our\norg.freedesktop.Platform.openh264 extension. I caused a bit of work\nfor them as noopenh264 moved home\nonce again, now to Fedora and they are looking for ways to make their\nown OpenH264 extension similar to how we did.\nI hope the experience here helps anyone in the future wanting to maintain\nsuch an extension and this will also serve as a reminder to how much\nextra work patents like these causes.",
    "summary": {
      "en": "The author of the text discusses the decision to drop the OpenH264 extension from the Freedesktop SDK, providing a detailed history of its development and the challenges faced. Here's a simplified summary of the key points:\n\n1. **Freedesktop SDK Overview**: The Freedesktop SDK project started as a minimal Linux runtime for Flatpaks, evolving into a community-maintained project that includes various runtimes and extensions.\n\n2. **H.264 Codec Challenges**: H.264 is a popular video codec with patent restrictions. This made it difficult to include in the Freedesktop runtime without legal issues.\n\n3. **OpenH264 Development**: In 2019, the OpenH264 extension was created to allow users to download and use Cisco's OpenH264 binaries without distributing them directly, addressing legal concerns.\n\n4. **Technical Complications**: The setup had flaws, including issues with binary verification and the inability to apply security fixes effectively. This led to vulnerabilities in the distribution of the OpenH264 binaries.\n\n5. **Transition to Codecs-Extra**: Due to some security concerns and changing needs, a new extension called \"codecs-extra\" was developed, which includes support for various video codecs, including H.264.\n\n6. **Decision to Drop OpenH264**: As codecs-extra provided sufficient support for H.264, the OpenH264 extension was deemed unnecessary and was planned to be retired. Further complications arose from a critical flaw in OpenH264, leading to its removal from the master branch of the Freedesktop SDK.\n\n7. **Conclusion**: The author believes dropping OpenH264 was the right choice and expresses hope that the new codecs-extra extension will function well. They also acknowledge the ongoing challenges posed by patents in software development.",
      "ko": "텍스트의 저자는 Freedesktop SDK에서 OpenH264 확장을 제거하기로 한 결정에 대해 논의하며, 그 개발 역사와 직면한 문제들을 자세히 설명합니다. 주요 내용을 간략히 정리하면 다음과 같습니다.\n\nFreedesktop SDK 프로젝트는 Flatpak을 위한 최소한의 리눅스 런타임으로 시작하여, 다양한 런타임과 확장을 포함하는 커뮤니티 유지 프로젝트로 발전했습니다. H.264는 인기 있는 비디오 코덱이지만, 특허 제한으로 인해 Freedesktop 런타임에 포함하기가 어려웠습니다.\n\n2019년, OpenH264 확장이 만들어져 사용자가 Cisco의 OpenH264 바이너리를 직접 배포하지 않고 다운로드하여 사용할 수 있도록 하여 법적 문제를 해결했습니다. 그러나 이 설정에는 결함이 있었고, 바이너리 검증 문제와 보안 패치를 효과적으로 적용할 수 없는 문제가 있었습니다. 이로 인해 OpenH264 바이너리 배포에서 취약점이 발생했습니다.\n\n보안 문제와 변화하는 요구에 따라 \"codecs-extra\"라는 새로운 확장이 개발되었으며, 이 확장은 H.264를 포함한 다양한 비디오 코덱을 지원합니다. codecs-extra가 H.264에 대한 충분한 지원을 제공함에 따라 OpenH264 확장은 필요 없다고 판단되어 퇴출될 예정입니다. OpenH264에서 발생한 심각한 결함으로 인해 Freedesktop SDK의 마스터 브랜치에서 제거되었습니다.\n\n저자는 OpenH264를 제거한 것이 올바른 선택이었다고 믿으며, 새로운 codecs-extra 확장이 잘 작동하기를 희망합니다. 또한 소프트웨어 개발에서 특허가 계속해서 제기하는 문제를 인정합니다.",
      "ja": "著者は、Freedesktop SDKからOpenH264拡張を削除する決定について、その開発の歴史や直面した課題を詳しく説明しています。\n\nFreedesktop SDKプロジェクトは、Flatpak用の最小限のLinuxランタイムとして始まり、さまざまなランタイムや拡張を含むコミュニティ主導のプロジェクトへと進化しました。\n\nH.264は人気のあるビデオコーデックですが、特許の制約があるため、Freedesktopランタイムに法的な問題なしに含めることが難しい状況でした。\n\n2019年に、OpenH264拡張が作成され、ユーザーがCiscoのOpenH264バイナリを直接配布することなくダウンロードして使用できるようにしました。これにより法的な懸念が解消されました。\n\nしかし、この設定には欠陥があり、バイナリの検証に問題があったり、セキュリティ修正を効果的に適用できなかったりしました。その結果、OpenH264バイナリの配布に脆弱性が生じました。\n\nセキュリティ上の懸念やニーズの変化により、「codecs-extra」という新しい拡張が開発され、H.264を含むさまざまなビデオコーデックのサポートが追加されました。\n\ncodecs-extraがH.264に十分なサポートを提供することから、OpenH264拡張は不要と判断され、廃止されることが計画されました。さらに、OpenH264に重大な欠陥が見つかり、Freedesktop SDKのマスターブランチから削除されることになりました。\n\n著者は、OpenH264を削除することが正しい選択だったと考えており、新しいcodecs-extra拡張がうまく機能することを期待しています。また、ソフトウェア開発における特許の課題が引き続き存在することも認識しています。"
    }
  },
  {
    "id": "7cf3540b3765823d",
    "title": {
      "en": "Gene drive modified mosquitoes offer new tool for malaria elimination efforts",
      "ko": "유전자 드라이브 모기, 말라리아 퇴치의 새 희망",
      "ja": "遺伝子操作蚊でマラリア撲滅へ"
    },
    "type": "story",
    "url": "https://www.imperial.ac.uk/news/261995/gene-drive-modified-mosquitoes-offer-tool/",
    "score": 10,
    "by": "gmays",
    "time": 1742581491,
    "content": "view large\n\n\t\t\t\tGenetic technology co-developed at Imperial could help to eliminate malaria by making mosquitos resistant to the parasite that spreads the disease.\n\n\t\t\t\tTransmission Zero, a research team from Imperial College London in partnership with the Ifakara Health Institute and the National Institute of Medical Research in Tanzania, has developed genetic technology which renders a mosquito unable to transmit the malaria parasite and has a gene drive that ensures that future generations are also resistant to the parasite.\nThe technology could significantly reduce the malaria burden in high-risk countries, saving hundreds of thousands of lives each year, especially among children – who are disproportionately at risk from the disease.\n“Our technology is equitable, it offers hope in the fight against malaria and doesn’t present economic or social barriers to malaria intervention access.” Professor George K. Christophides Professor of Infectious Disease and Immunity at Imperial College London\nThe work of Transmission Zero has been highlighted in a new Global Development Hub STEM forDevelopment Impact Memo authored by Professor George K. Christophides, Professor of Infectious Disease and Immunity at Imperial, Dr Nikolai Windbichler, Reader in Genetics at Imperial, and Dr Dickson Wilson Lwetoijera, Principal Research Scientist at Ifakara Health Institute in Tanzania.\nProfessor Christophides from Imperial’s Department of Life Sciences said: “The solutions we currently have for malaria are not enough – we need something new.\n“Our technology is equitable, it offers hope in the fight against malaria and doesn’t present economic or social barriers to malaria intervention access.”\n\nRead the fullGlobal Development Hub STEM for Development Impact Memo on Transmission Zero here.\n\nProgress on tackling malaria has flatlined in recent years, with cases and deaths further exacerbated by disruption to health services during the COVID-19 pandemic.\nEach year, more than 263 million people contract malaria and the disease is endemic in 83 countries. In 2023, there were almost 600,000 malaria deaths in 83 countries, with African countries accounting for 94 per cent of deaths - 76 per cent of whom were children under five.\nEfforts to prevent the spread of the disease have been complicated by existing malaria interventions becoming less effective due to mosquitoes developing biological and behavioural resistance to insecticides and barrier-based controls. Malaria parasites have also developed resistance to treatment, highlighting the need for new tools to tackle the disease.\n\nDr Nikolai Windbichler and Professor George K. Christophides from the Transmission Zero team at Imperial.\n\nDr Windbichler, also from the Department of Life Sciences, said: “This is a new type of technology and a first-in-class solution to complement our malaria control tools.\n“We want to move at the right speed, not too fast so that everyone is on board and supportive of this new technology, but also with urgency, so that we treat malaria as the emergency that it is.”\n“We want to move at the right speed, not too fast so that everyone is on board and supportive of this new technology, but also with urgency, so that we treat malaria as the emergency that it is.” Dr Nikolai Windbichler Reader in Genetics at Imperial College London\nThe memo was launched at a special event at the Royal Geographical Society which brought together leading scientists, policymakers, and global health advocates to explore the potential of gene drive technology and other innovative solutions to achieve malaria eradication.\nThe event included addresses from the memo authors, alongside a panel discussion and Q&A session which included:\n\nProfessor Faith Osier, Imperial’s Chair in Malaria Immunology and Vaccinology\n\nDr Honorati Masanja, Chief Executive Director at Ifakara Health Institute, Tanzania\n\nMichael Santos, Senior Vice President for Science Partnerships and Chief Population Health Science Officer at the Foundation for the National Institutes of Health (FNIH)\n\nVictoria Fowler, Head of UK Advocacy at Malaria No More\n\nAfrica-owned technology\nTransmission Zero was established to develop new ways to halt the spread of malaria. The project is co-led by Imperial researchers in the UK and scientists at the Ifakara Health Institute and National Institute for Medical Research in Tanzania, in collaboration with the Swiss Tropical and Public Health Institute in Switzerland.\n\"There was strong agreement from the beginning that we had to capacitate local scientists and have buy-in from the government and local communities, which demonstrates the local ownership of the technology.” Dr Dickson Wilson Lwetoijera Principal Research Scientist at Ifakara Health Institute\nIt has been supported by a $15-million grant from the Gates Foundation to substantially expand its operations in the UK and in Tanzania.\nThis collaborative approach has led to the development of a mosquito transgenesis and infection facility at the Ifakara Health Institute with a biosafety level three laboratory and supporting plant room, bolstering genetic research in Tanzania and the wider continent.\nThe project has already achieved the first ever transgenic mosquito made on African soil, made by African scientists – a landmark achievement for research and technology development in Africa.\nDr Lwetoijera said: “There was strong agreement from the beginning that we had to capacitate local scientists and have buy-in from the government and local communities, which demonstrates the local ownership of the technology.”\nThe partnership between Imperial and institutes in Tanzania will help ensure that scientists in the region are trained in the latest genetic technologies and able to develop their own research agenda.\n\nTackling global development challenges\nSTEM for Development Impact Memos offer policymakers and stakeholders insight intoprojects supported by Imperial’s Global Development Hub and their real-world impact. The focus is on the scale-up potential of a science, technology, policy or health solution to development challenges.\nImperial'sGlobal Development Hubis a platform to promote and support Imperial’s sustainable development research, education and innovation. The Hub supports Imperial’s contribution to the United Nations Sustainable Agenda 2030, and our work more broadly with some of the most vulnerable and marginalised in societies where multiple global challenges are acutely concentrated.",
    "summary": {
      "en": "Researchers from Imperial College London have developed a genetic technology that can make mosquitoes resistant to malaria, potentially eliminating the disease. This project, called Transmission Zero, is a collaboration with Tanzanian health institutes and aims to significantly reduce malaria cases and deaths, particularly among children.\n\nThe technology creates mosquitoes that cannot transmit the malaria parasite and ensures that future generations of mosquitoes inherit this resistance. This innovative approach is seen as essential, especially as traditional malaria prevention methods are becoming less effective due to mosquito resistance to insecticides and treatments.\n\nThe project has received a $15 million grant from the Gates Foundation and emphasizes local involvement and training for African scientists. It aims to give communities ownership of the technology and ensure sustainable development in malaria research.\n\nOverall, this initiative highlights the urgent need for new solutions to combat malaria, which remains a major global health challenge, particularly in Africa.",
      "ko": "런던 임페리얼 대학교의 연구자들이 말라리아에 저항력을 가진 모기를 만드는 유전자 기술을 개발했습니다. 이 기술은 말라리아를 근절할 수 있는 가능성을 가지고 있습니다. '전파 제로(Transmission Zero)'라는 이름의 이 프로젝트는 탄자니아의 보건 기관들과 협력하여 진행되며, 특히 어린이들 사이에서 말라리아 사례와 사망자를 크게 줄이는 것을 목표로 하고 있습니다.\n\n이 기술은 말라리아 기생충을 전파할 수 없는 모기를 만들어내며, 미래 세대의 모기들이 이 저항력을 물려받도록 합니다. 전통적인 말라리아 예방 방법들이 모기의 살충제 저항성 증가로 인해 효과가 떨어지고 있는 상황에서, 이러한 혁신적인 접근 방식은 매우 중요하게 여겨집니다.\n\n이 프로젝트는 게이츠 재단으로부터 1,500만 달러의 지원금을 받았으며, 아프리카 과학자들을 위한 지역 참여와 교육을 강조하고 있습니다. 이를 통해 지역 사회가 기술에 대한 소유권을 갖고, 말라리아 연구의 지속 가능한 발전을 보장하는 것을 목표로 하고 있습니다.\n\n이 initiative는 말라리아와 싸우기 위한 새로운 해결책의 필요성을 강조합니다. 말라리아는 여전히 아프리카를 포함한 전 세계에서 주요한 건강 문제로 남아 있습니다.",
      "ja": "ロンドンのインペリアル・カレッジの研究者たちが、蚊をマラリアに対して抵抗力を持たせる遺伝子技術を開発しました。この技術は、マラリアを根絶する可能性を秘めています。このプロジェクトは「トランスミッションゼロ」と呼ばれ、タンザニアの健康機関と協力して進められています。特に子供たちのマラリア感染と死亡を大幅に減少させることを目指しています。\n\nこの技術は、マラリアの寄生虫を伝播できない蚊を作り出し、将来の世代にもその抵抗力を引き継がせることができます。この革新的なアプローチは、特に蚊が殺虫剤や治療法に対して抵抗力を持つようになっているため、従来のマラリア予防策が効果を失いつつある中で重要視されています。\n\nこのプロジェクトは、ビル・アンド・メリンダ・ゲイツ財団から1500万ドルの助成金を受けており、アフリカの科学者たちの地元での関与や訓練を強調しています。地域社会がこの技術を所有し、マラリア研究の持続可能な発展を確保することを目指しています。\n\nこの取り組みは、特にアフリカにおいて依然として大きな健康課題であるマラリアに対抗するための新しい解決策の必要性を強調しています。"
    }
  },
  {
    "id": "dfc013cd9b62a96e",
    "title": {
      "en": "Battling the Blaze: Artifacts from the history of firefighting",
      "ko": "불꽃의 역사: 소방 유물",
      "ja": "炎の戦い：消防の歴史"
    },
    "type": "story",
    "url": "https://worldhistory.substack.com/p/battling-the-blaze",
    "score": 4,
    "by": "crescit_eundo",
    "time": 1742737584,
    "content": "Share this postLooking Through the PastBattling the BlazeCopy linkFacebookEmailNotesMoreDiscover more from Looking Through the PastIlluminating forgotten corners of history and using them to think about the presentOver 1,000 subscribersSubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.Already have an account? Sign inBattling the BlazeArtifacts from the history of firefightingGeorge DillardMar 23, 202510Share this postLooking Through the PastBattling the BlazeCopy linkFacebookEmailNotesMore2ShareWelcome to Looking Through the Past! At its heart, this is a project in which I find a bunch of cool stuff every week and share it with you. It takes hours of research and writing to produce. If you’re new here, click the button below to subscribe. If you’re already a free subscriber who appreciates getting a bunch of cool stuff in your inbox every week, please consider a paid subscription, for less than $1 a week.SubscribeI thought I knew how I was going to introduce this week’s topic — some fascinating artifacts from the history of firefighting — but it seems that I’ve hit a snag. Let me explain. In our wealth-obsessed society, there’s one thing that reliably attracts attention: the Rich Guy. People pay an immense amount of attention to what we used to call the Lifestyles of the Rich and Famous. If you’re like most Americans, your brain is cluttered with facts or suppositions about Elon’s kids, Bezos’ divorce, Warren Buffett’s car, and all sorts of other stuff about Rich Guys that is much less important for you to remember than that meeting you forgot about earlier today. Our fascination with Rich Guys is sometimes a product of admiration — we want to understand their ways so that we can be more like them — and sometimes a result of envy and resentment — we want to revel in the fact that all the money in the world can’t save these guys from misfortune. The Rich Guy obsession applies to historical figures, too. There’s a steady stream of historical content about the Rich Guys of History, whether robber barons like Carnegie and Rockefeller or ancient plutocrats like Mansa Musa or the Rich Guy I want to discuss today, Marcus Licinius Crassus, a member of Rome’s First Triumvirate. Much of the Rich Guys of History content on the internet, it must be said, is quite low quality — it’s clear that people google them a lot, so there’s a lot of clickbait and content-farmed stuff that comes up toward the top of the search results. Much of it is thinly sourced or exaggerated for effect — search for the “richest person in history,” and you’ll enjoy a lukewarm buffet of AI slop combined with slapdash currency conversion and inflation calculation. It’s difficult if not impossible to convert the wealth of a guy like Crassus into modern terms; even if we take the value of all the gold he owned and convert it into dollars at today’s prices, how can we account for all of the slaves he owned or the fact that, for all his wealth, he didn’t have, like, a refrigerator or a telephone?The value of Crassus’ holdings is not the only thing that the Rich Guy of History websites fail to convey accurately. Take, for example, a commonly-repeated story about the source of Crassus’ wealth — that he operated a “fire brigade” whose purpose was, well, I’ll let the New Yorker’s Hendrik Hertzberg explain:Crassus had his own private fire department, and if your house caught fire his representatives would offer to buy it on the spot, at a one-time-only, fire-sale price that would fall rapidly as the flames climbed. If you said yes, you’d get a few sesterces, after which Crassus’ firefighters would do their thing. If you said no, you’d end up with a pile of ashes. Hertzberg’s anecdote is based on a passage from Plutarch:And besides this, observing how natural and familiar at Rome were such fatalities as the conflagration and collapse of buildings, owing to their being too massive and close together, he proceeded to buy slaves who were architects and builders. Then, when he had over five hundred of these, he would buy houses that were afire, and houses which adjoined those that were afire, and these their owners would let go at a trifling price owing to their fear and uncertainty. In this way the largest part of Rome came into his possession.But, beyond this passage (and Plutarch was known to spice up his narratives if he was trying to make a point), there’s really… nothing else to support this idea that pops up again and again in the popular history of Crassus. As Roman historian Liv Mariah Yarrow found when she dug into the matter, this is something that “everyone KNOWS” but is weakly sourced and quite possibly misunderstood. There seem to have been a lot of private fire brigades. Crassus’ organization may not have been unusual, or it may have had other purposes (protecting his own property, for example). Its importance to his wealth is probably exaggerated. So the tidy story I was going to start with — Crassus’ fire brigades as the beginning of firefighting history — seems a little too tidy. Yes, ancient Rome had firefighters (their most important job was probably spotting fires before they grew). Other ancient societies must have had them, as well, although clear evidence is hard to come by. So, sorry — I don’t have a just-so story about the ancient origins of firefighting. What we do know is this: fire has plagued urban society since its beginning, and these societies have worked to prevent and fight fire for most of that history. Despite the danger of fire, organized, professional municipal firefighting is a pretty recent invention. It wasn’t until 1824 that Edinborough established a taxpayer-supported, full-time fire company (Cincinnati was the first American city to do so, in 1853, although some cities had volunteer firefighters prior to that date). Before the advent of municipal firefighters, homeowners had to buy fire insurance. Insurance entitled the policyholder to the services of a company’s fire brigade in the case of an emergency. But how would the fire brigades know which houses were covered by which companies? Insurance firms marked their territory with fire marks — metal or ceramic plaques that designated a building’s insurance provider. They had to be made of fireproof materials because the insurance companies sometimes wouldn’t pay a claim until the mark was found in the rubble of a burned building.These marks still dot old buildings in British and American cities, and many of them are quite attractive. This one comes from early-19th-century London:Public domainAnd this one from Norwich:Evelyn Simak, CC 2.0I enjoy this dragon-themed one from late-19th-century Singapore:Public domainSome featured firefighting equipment, like this serpentine-looking hose on one from Philadelphia:Public domainOr this early fire engine from Darlington, England:Public domainBut the most excellent fire mark has to be this one from the Commercial Union Assurance Company in London, which shows a dragon amidst the flames. Who says insurance has to be boring?Public domainWhat about the firefighters themselves? In the nineteenth century, they didn’t have the advanced equipment that they do today to protect them from the dangers of their job. But this doesn’t mean they didn’t take great pride in their attire. I love the early fire hats — stovepipe hats, mace of felt or leather, that didn’t offer a ton of protection but did advertise the fire company and often the wearer himself. They’re charmingly hand-decorated:Courtesy of the Missouri Historical Society, St. LouisCourtesy of the Missouri Historical Society, St. LouisBen Franklin, the founder of the United States’ first volunteer fire company, was a frequent subject:Courtesy of the Missouri Historical Society, St. LouisWhile others featured other patriotic figures:Courtesy of the Missouri Historical Society, St. LouisSome simply tried to embody the emotions that firefighters must have desperately wanted to provide to the community:Public domainBy the late nineteenth century, the modern fire helmet — made initially of leather — provided more protection, but they still were charmingly hand-painted. This one, appropriately, features a phoenix rising from the ashes:Courtesy of the Missouri Historical Society, St. LouisThe most excellent fire-protection apparel, however, came from Japan. Japanese firefighters wore reversible coats made of quilted cotton. They would soak the coats in water before they went into the flames. Their task was usually to create a fire break by destroying structures around the burning building; water supplies were insufficient to extinguish large fires. These coats were decorated with remarkable mythological designs; the men would often turn the coats inside out so that the designs would not be harmed when they entered the blaze.  Here’s one with an image of a Toad Spirit that taught humans magic:Art Institute of ChicagoOthers had pictures of legendary warriors, whose bravery would have inspired the firefighters (this one features Rōrihakucho Chō Jun):Public domainSome were quite literal, like this coat showing a warrior fighting the spirit of fire:Public domainWhile others evoked the peace and quiet of nature:Public domainLater, the coats were made of leather, which was more protective. This one has the fire company’s beautifully stylized logo on the back:Brooklyn MuseumOver time, of course, firefighters developed more modern protective technology, especially for the danger that came from smoke inhalation. This is an early design from the Warsaw fire department:Public domainSome of the equipment made them look positively alien, like these gas masks worn by Australian firefighters in 1927:Public domainOr these worn by Norwegians in 1941:Municipal Archives of TrondheimIt takes a special type of person to sign up for a job that involves risking your life, breathing smoke, and battling deadly flames — especially at a time when the technology that might protect them was so primitive. So I’ll leave you with some images of the firefighters themselves. Here’s the Jarrow Ladies Fire Brigade, called into service when most of the men from this town near Newcastle were called up for World War I:Public domainHere we have the Athabasca, Alberta fire brigade, who presumably had to battle wildfire more than urban conflagrations:Public domainAnd here’s a Seattle-area “hose team” posing during some kind of tournament, in 1889, the same year that Seattle’s “Great Fire” occurred. Some of the men in the picture would later fall into Puget Sound when a burning dock collapsed beneath their fire wagon:Public domainOne final image — of firefighters in action, carrying an injured person on a stretcher. I love this one; it shows the calm and competence amidst chaos that is the hallmark of the firefighter:Public domainThough the origins of firefighting may be murky, its purpose is clear. Firefighters are there to assist us during unthinkable tragedies, risking their lives to save people and their possessions. The slogans that were painted on early fire engines say it all:“United We Stand, Divided We Fall.”“Non Sibi Sed Omnibus” (\"Not for self, but for all.\")“When Duty Calls, Tis Ours to Obey.”“Semper Paratus” (\"Always Ready\")And, my favorite, “We'll Try.”This newsletter is free to all, but I count on the kindness of readers to keep it going. If you enjoyed reading this week’s edition, there are two ways to support my work: You can subscribe as a free or paying member:SubscribeOr you can share the newsletter with others:Share Looking Through the PastThanks for reading, and I’ll see you again next week!10 Likes∙2 Restacks10Share this postLooking Through the PastBattling the BlazeCopy linkFacebookEmailNotesMore2Share",
    "summary": {
      "en": "**Summary:**\n\nThis post from \"Looking Through the Past\" explores the history of firefighting and its significant artifacts. The author notes our society's fascination with wealthy individuals, like historical figures such as Marcus Licinius Crassus, and addresses misconceptions surrounding them—specifically, the story of Crassus operating a private fire brigade. This narrative is likely exaggerated and lacks strong evidence. \n\nFirefighting has been a concern since ancient times, but organized municipal fire services only began in the 19th century, with Edinburgh establishing the first taxpayer-funded fire company in 1824. Before this, homeowners relied on fire insurance, which marked their properties with plaques to show coverage.\n\nThe post also highlights the evolution of firefighting gear, from early fire hats to modern protective equipment. It showcases various historical firefighting uniforms and equipment from around the world, emphasizing the bravery of firefighters who risk their lives. The post concludes with the reminder that, despite the unclear origins of firefighting, its vital role in protecting communities is undeniable.",
      "ko": "이 글은 \"과거를 들여다보다\"라는 블로그에서 소방의 역사와 중요한 유물에 대해 다루고 있습니다. 저자는 역사적 인물인 마르쿠스 리키니우스 크라수스와 같은 부유한 개인에 대한 사회의 매력을 언급하며, 그들에 대한 오해를 설명합니다. 특히 크라수스가 개인 소방대를 운영했다는 이야기는 과장된 것으로 보이며, 강력한 증거가 부족하다고 지적합니다.\n\n소방은 고대부터 중요한 문제였지만, 조직적인 시립 소방 서비스는 19세기부터 시작되었습니다. 1824년 에든버러에서 최초의 세금으로 운영되는 소방회사가 설립되었습니다. 그 이전에는 주택 소유자들이 화재 보험에 의존했으며, 보험에 가입한 집은 해당 보험을 나타내는 표지판을 붙였습니다.\n\n이 글은 또한 초기 소방모에서 현대의 보호 장비에 이르기까지 소방 장비의 발전을 강조합니다. 세계 각국의 다양한 역사적 소방복과 장비를 소개하며, 목숨을 걸고 싸우는 소방관들의 용기를 강조합니다. 글은 소방의 기원은 불확실하지만, 지역 사회를 보호하는 데 있어 소방의 중요한 역할은 부인할 수 없다는 점을 상기시키며 마무리됩니다.",
      "ja": "この「過去を振り返る」という投稿では、消防の歴史とその重要な遺物について探求しています。著者は、マルクス・リキニウス・クラッススのような歴史的人物に対する社会の関心を指摘し、彼らに関する誤解についても触れています。特に、クラッススが私設の消防団を運営していたという話は、誇張されている可能性が高く、強い証拠が欠けています。\n\n消防は古代からの関心事でしたが、組織化された市の消防サービスは19世紀になってから始まりました。エディンバラでは1824年に初めて税金で運営される消防会社が設立されました。それ以前は、住宅所有者は火災保険に頼り、保険の対象であることを示すために家にプレートを掲げていました。\n\n投稿では、初期の消防帽から現代の防護具に至るまで、消防用具の進化も取り上げています。世界中のさまざまな歴史的な消防服や装備が紹介されており、命を懸けて働く消防士たちの勇気が強調されています。消防の起源は不明確ですが、地域社会を守る重要な役割は否定できないことが最後に述べられています。"
    }
  },
  {
    "id": "a75affe91b207eea",
    "title": {
      "en": "CO2 laser enables long-range detection of radioactive material",
      "ko": "CO2 레이저로 방사능 탐지",
      "ja": "CO2レーザーで放射線検出"
    },
    "type": "story",
    "url": "https://physicsworld.com/a/co2-laser-enables-long-range-detection-of-radioactive-material/",
    "score": 93,
    "by": "EA-3167",
    "time": 1742593278,
    "content": "\"\\u003Cstrong\\u003ERemote detection\\u003C\\/strong\\u003E The ability to locate radioactive material at distances greater than the range of emitted particles could play an important role in nuclear disaster response and nuclear security. (Courtesy: Shutterstock\\/fewerton)\"\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tRemote detection The ability to locate radioactive material at distances greater than the range of emitted particles could play an important role in nuclear disaster response and nuclear security. (Courtesy: Shutterstock/fewerton)\n\n\t\t\t\t\t\t\t\tResearchers have demonstrated that they can remotely detect radioactive material from 10 m away using short-pulse CO2 lasers – a distance over ten times farther than achieved via previous methods.\nConventional radiation detectors, such as Geiger counters, detect particles that are emitted by the radioactive material, typically limiting their operational range to the material’s direct vicinity. The new method, developed by a research team headed up at the University of Maryland, instead leverages the ionization in the surrounding air, enabling detection from much greater distances.\nThe study may one day lead to remote sensing technologies that could be used in nuclear disaster response and nuclear security.\nUsing atmospheric ionization\nRadioactive materials emit particles – such as alpha, beta or gamma particles – that can ionize air molecules, creating free electrons and negative ions. These charged particles are typically present at very low concentrations, making them difficult to detect.\nSenior author Howard Milchberg and colleagues – also from Brookhaven National Laboratory, Los Alamos National Laboratory and Lawrence Livermore National Laboratory – demonstrated that CO2 lasers could accelerate these charged particles, causing them to collide with neutral gas molecules, in turn creating further ionization. These additional free charges would then undergo the same laser-induced accelerations and collisions, leading to a cascade of charged particles.\nThis effect, known as “electron avalanche breakdown”, can create microplasmas that scatter laser light. By measuring the profile of the backscattered light, researchers can detect the presence of radioactive material.\n\nThe team tested their technique using a 3.6-mCi polonium-210 alpha particle source at a standoff distance of 10 m, significantly longer than previous experiments that used different types of lasers and electromagnetic radiation sources.\n“The results are highly impressive,” comments EunMi Choi from the Ulsan National Institute of Science and Technology in South Korea. Choi’s team had used a gyrotron source to detect radioactive materials back in 2017.\n“The researchers successfully demonstrated 10-m standoff detection of radioactive material, significantly surpassing the previous range of approximately 1 m,” she says.\nMilchberg and collaborators had previously used a mid-infrared laser in a similar experiment in 2019. Changing to a long-wavelength (9.2 μm) CO2 laser brought significant advantages, he says.\n“You can’t use any laser to do this cascading breakdown process,” Milchberg explains. The CO2 laser’s wavelength was able to enhance the avalanche process, while being low energy enough to not create its own ionization sources. “CO2 is sort of the limit for long wavelengths on powerful lasers and it turns out CO2 lasers are very, very efficient as well,” he says. “So this is like a sweet spot.”\nImaging microplasmas\nThe team also used a CMOS camera to capture visible-light emissions from the microplasmas. Milchberg says that this fluorescence around radioactive sources resembled balls of plasma, indicating the localized regions where electron avalanche breakdowns had occurred.\nBy counting these “plasma balls” and calibrating them against the backscattered laser signal, the researchers could link fluorescence intensity to the density of ionization in the air, and use that to determine the type of radiation source.\nThe CMOS imagers, however, had to be placed close to the measured radiation source, reducing their applicability to remote sensing.“Although fluorescence imaging is not practical for field deployment due to the need for close-range cameras, it provides a valuable calibration tool,” Milchberg says.\nScaling to longer distances\nThe researchers believe their method can be extended to standoff distances exceeding 100m. The primary limitation is the laser’s focusing geometry, which would affect the regions in which it could trigger an avalanche breakdown. A longer focal length would require a larger laser aperture but could enable kilometre-scale detection.\nChoi points out, however, that deploying a CO2 laser may be difficult in real-world applications. “A CO₂ laser is a bulky system, making it challenging to deploy in a portable manner in the field,” she says, adding that mounting the laser for long-range detection may be a solution.\n\n\t\t\t\t\tRead more\n\n\t\t\t\tTetris-inspired radiation detector uses machine learning\n\nMilchberg says that the next steps will be to continue developing a technique that can differentiate between different types of radioactive sources completely remotely. Choi agrees, noting that accurately quantifying both the amount and type of radioactive material continues to be a significant hurdle to realising remote sensing technologies in the field.\n“There’s also the question of environmental conditions,” says Milchberg, explaining that it is critical to ensure that detection techniques are robust against the noise introduced by aerosols or air turbulence.\nThe research is described in Physical Review Applied.\n\n\t\t\t\t\t\t\t\t\t\t\t\tWant to read more?\n\t\t\t\t\t\t\t\t\t\t\t\tRegistration is free, quick and easy\n\t\t\t\t\t\t\t\t\t\t\t\tNote: The verification e-mail to complete your account registration should arrive immediately. However, in some cases it takes longer. Don't forget to check your spam folder.\n\t\t\t\t\t\t\t\t\t\t\t\tIf you haven't received the e-mail in 24 hours, please contact customerservices@ioppublishing.org.\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tE-mail Address\n\n\t\t\t\t\t\t\t\t\t\t\t\tRegister",
    "summary": {
      "en": "Researchers at the University of Maryland have developed a new method to remotely detect radioactive material from a distance of 10 meters using CO2 lasers, which is more than ten times the range of traditional detectors like Geiger counters. This method works by utilizing ionization in the air caused by the particles emitted from radioactive materials. The CO2 lasers can accelerate charged particles, leading to a chain reaction known as \"electron avalanche breakdown,\" which creates microplasmas that scatter laser light. \n\nBy analyzing the backscattered light, the researchers can identify the presence of radioactive material. They successfully tested this technique with a polonium-210 source, significantly improving detection range compared to previous methods.\n\nWhile the method shows promise for detecting radioactive materials over distances greater than 100 meters, the bulkiness of CO2 lasers poses challenges for practical field deployment. Future work will focus on differentiating between types of radioactive sources and ensuring the detection method is effective in various environmental conditions.",
      "ko": "메릴랜드 대학교의 연구팀이 CO2 레이저를 이용해 10미터 거리에서 방사성 물질을 원격으로 탐지하는 새로운 방법을 개발했습니다. 이는 기존의 가이거 계수기와 같은 탐지기보다 10배 이상 긴 탐지 범위를 자랑합니다. 이 방법은 방사성 물질에서 방출되는 입자들로 인해 공기 중에서 발생하는 이온화를 이용합니다. CO2 레이저는 전하를 띤 입자를 가속화시켜 \"전자 눈사태 붕괴\"라는 연쇄 반응을 일으킵니다. 이 과정에서 생성된 미세 플라즈마는 레이저 빛을 산란시킵니다.\n\n연구자들은 되돌아오는 빛을 분석하여 방사성 물질의 존재를 확인할 수 있습니다. 이 기술은 폴로늄-210 소스를 사용하여 성공적으로 테스트되었으며, 이전 방법에 비해 탐지 범위를 크게 향상시켰습니다.\n\n이 방법은 100미터 이상의 거리에서도 방사성 물질을 탐지할 수 있는 가능성을 보여주지만, CO2 레이저의 부피가 크기 때문에 실제 현장에서 사용하기에는 어려움이 있습니다. 앞으로의 연구는 방사성 원소의 종류를 구분하고 다양한 환경 조건에서도 탐지 방법이 효과적일 수 있도록 하는 데 중점을 둘 예정입니다.",
      "ja": "メリーランド大学の研究者たちは、CO2レーザーを使用して、10メートルの距離から放射性物質を遠隔で検出する新しい方法を開発しました。この方法は、従来のガイガーカウンターなどの検出器の範囲の10倍以上に及びます。放射性物質から放出される粒子によって空気中に生じるイオン化を利用しています。CO2レーザーは帯電した粒子を加速させ、「電子雪崩崩壊」と呼ばれる連鎖反応を引き起こし、マイクロプラズマを生成します。これにより、レーザー光が散乱します。\n\n研究者たちは、散乱した光を分析することで放射性物質の存在を特定できます。この技術はポロニウム-210のソースを用いて成功裏にテストされ、従来の方法と比べて検出範囲が大幅に向上しました。\n\nこの方法は、100メートル以上の距離で放射性物質を検出する可能性を示していますが、CO2レーザーの大きさが実際の現場での展開に課題をもたらしています。今後の研究では、放射性源の種類を区別し、さまざまな環境条件でも検出方法が効果的であることを確認することに焦点を当てます。"
    }
  },
  {
    "id": "4b17ba4ef7e411e0",
    "title": {
      "en": "High‑dose Vitamin D reduces disease activity in early multiple sclerosis onset",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://medicalxpress.com/news/2025-03-highdose-vitamin-d-significantly-disease.html",
    "score": 11,
    "by": "pseudolus",
    "time": 1742898037,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "bd9ab8acef055291",
    "title": {
      "en": "Brain Scans of Infants Reveal the Moment We Start Making Memories",
      "ko": "아기 뇌 스캔, 기억의 시작 포착!",
      "ja": "赤ちゃんの脳スキャンで記憶の始まりを発見"
    },
    "type": "story",
    "url": "https://singularityhub.com/2025/03/20/new-baby-brain-scans-reveal-the-moment-we-start-making-memories/",
    "score": 67,
    "by": "Brajeshwar",
    "time": 1742574750,
    "content": "ScienceBrain Scans of Infants Reveal the Moment We Start Making MemoriesKids form fleeting memories at around 12 months, even as their brains are rapidly rewiring themselves.Shelly FanMar 20, 2025Image CreditJuan Encalada on UnsplashShareA giggling toddler in a pink dress and matching headphones lies down on her back in front of a gigantic whirling machine. A pillowy headrest cushions her head. She seems unfazed as she’s slowly shuttled into the claustrophobic brain scanner. Once settled, a projection showing kaleidoscope-like animations holds her attention as the magnetic resonance imaging (MRI) machine scans her brain.The girl is part of a new study seeking to answer a century-old mystery: Why can’t most us remember the first three years of our lives? Dubbed “infantile amnesia” by Sigmund Freud, the study could provide insight into how the brain develops during our early years. And if we can form memories at a young age, are they fleeting, or are they still buried somewhere in the adult brain?It seems like a simple question, but an answer has eluded scientists.Though infants and toddlers aren’t yet able to give detailed verbal feedback, studying their behavior has begun to shed light on if and when they remember people, things, or places. Still, the approach can’t peek in on what’s happening in the brain in those early years. MRI can.A team from Columbia and Yale University scanned the brains of 26 infants and toddlers aged 4 to 25 months as they completed a memory task. They found that at roughly a year old, a part of the brain crucial to memory formation spun into action and began generating neural signals related to things the kids remembered from the tests.Called the hippocampus, this sea-horse-shaped structure deep inside the brain is crucial to the encoding of our life stories—who, when, where, what. Adults with a damaged hippocampus suffer memory problems. But because wiring inside the hippocampus is still developing during our earliest years, scientists believe it may be too immature to form memories.“It’s not that we don’t have any memories from that period [infancy],” said study author Nicholas Turk-Browne in a press briefing. “In fact, early life is when we learn our language. It’s when we learn how to walk…learn the names of objects and form social relationships.”“What happens during that period when we learn so much, but remember so little?” he added.Stages of MemoryMemory seems like all-or-none: You either remember something, or you don’t.It’s not that simple. Decades of research have identified the hippocampus as the main orchestrator of episodic memories. These allow you to remember an acquaintance at a party, where you parked your car, or what you had for dinner three nights ago.Each everyday experience is encoded in neural connections in the hippocampus. Groups of neurons called engrams capture different memories and keep them separate, so that they don’t bleed into each other.Once encoded, the brain etches important memories into long-term storage during sleep. Studies of slumbering rodents and humans after learning a new task found that the hippocampus replayed brain activity at higher speed during the night, correlating with better performance on a trained memory task the next day.The last step is retrieval. This is when the brain fishes out stored memories and delivers them to our conscious brain—and so, we “remember.”Failure of any of these steps causes amnesia. So, which steps are responsible for the erosion of baby memories?Bundles of JoyBrain scans from 26 infants now offer some intriguing clues.The team behind the new study scanned the children’s brains with functional MRI (fMRI) as they looked at a screen in the scanner and took a memory test. fMRI captures brain oxygen levels (BOLD) as a proxy for local neuron signaling—higher levels mean more brain activity.The head needs to keep very still throughout the scans to avoid blurring. That’s not easily accomplished with babies and toddlers. Previous studies circumvented the problem by imaging their brains while sleeping, but the results couldn’t capture memory processes.Be Part of the FutureSign up to receive top stories about groundbreaking technologies and visionary thinkers from SingularityHub.ruidWeekly NewsletterDaily NewsletterI agree to receive other communications from Singularity.I agree to allow Singularity to store and process my personal data in accordance with the company's Terms of Use and Privacy Policy.*Subscribe100% Free. No Spam. Unsubscribe any time.To keep the infants happy, engaged, and safe, parents brought favorite blankets and pacifiers, and younger infants were wrapped inside a comfortable vacuum pillow to reduce movement. A video system projected images onto the ceiling of the scanner within their line of sight.As the kids looked at a bright kaleidoscope-like video, images of faces, scenes, and objects would flash for a few seconds. These included toys or landscapes of an alpine cabin with mountains in the background. Previous studies found infants like to stare at objects or images they’ve seen before compared to new objects, suggesting they remember previous encounters.Throughout the sessions the team added projections showing a previously seen picture and a new one and monitored the infants’ eye movement using a video camera.“The ingenuity of their experimental approach should not be understated,” wrote Adam Ramsaran and Paul Frankland at the Hospital for Sick Children in Toronto, Canada, who were not involved in the study.BOLD FindingsThe kids often squirmed during the sessions. Some weren’t interested in the pictures; others fell asleep in the scanner.Still, the team managed to capture hippocampal BOLD signals averaging roughly eight minutes per participant and matched them to memory performance. On average, parts of the hippocampus ramped up activity for images that the infants later remembered—that is, they looked at it for longer during the test phases.But not all infants performed the same. The younger cohort, under a year, didn’t show the surge of BOLD signals suggesting memory encoding. They also ignored already seen images compared to new ones.It seems babies start encoding memories around a year of age, even as their hippocampus is still developing.The results are similar to those in baby rodents. The early years are chaotic. The brain undergoes extensive rewiring. This makes it a difficult to form lasting memories. Yet some supposedly lost memories encoded at a young age can be recovered later in life with reminder cues or by directly activating the set of neurons that originally encoded the memory.That’s not to say infants can acquire rich recollections—stories including multiple people, places, and things—at a year. The study only tested brain signatures for individual components.Future studies tracking the hippocampus might shed light on the minimal brain architecture needed to support vivid autobiographical memories. Examining other stages of memory could shine more light on infantile amnesia. For example, do infants also replay neural signals as they sleep to etch new experiences into long-term memory?And maybe—just maybe—our earliest memories could one day be retrieved later in childhood or beyond.NeuroscienceShelly FanShelly FanDr. Shelly Xuelai Fan is a neuroscientist-turned-science-writer. She's fascinated with research about the brain, AI, longevity, biotech, and especially their intersection. As a digital nomad, she enjoys exploring new cultures, local foods, and the great outdoors.Related ArticlesNew Tech Bends Sound Through Space So It Reaches Only Your Ear in a CrowdJiaxin ZhongandYun JingMar 18, 2025A Massive AI Analysis Found Genes Related to Brain Aging—and Drugs to Slow It DownShelly FanMar 17, 2025Scientists Discover Thousands of New Microbial Species Thriving in the Mariana TrenchShelly FanMar 06, 2025ScienceNew Tech Bends Sound Through Space So It Reaches Only Your Ear in a CrowdJiaxin ZhongandYun JingMar 18, 2025ScienceA Massive AI Analysis Found Genes Related to Brain Aging—and Drugs to Slow It DownShelly FanMar 17, 2025ScienceScientists Discover Thousands of New Microbial Species Thriving in the Mariana TrenchShelly FanMar 06, 2025What we’re readingScientists Find Giant Organic Molecules on Mars as Old as Life on EarthGizmodoThe Quantum Apocalypse Is Coming. Be Very AfraidWiredA Mysterious Startup Is Developing a New Form of Solar GeoengineeringWiredInside Google’s Two-Year Frenzy to Catch Up With OpenAIWired1X Will Test Humanoid Robots in ‘a Few Hundred’ Homes in 2025TechCrunchWatch the Atlas Robot Bust a Move in Boston Dynamics’ Latest VideoEngadgetIs Dark Energy Getting Weaker? New Evidence Strengthens the Case.QuantaSatellite Internet Will Enable AI in EverythingWiredAI-Driven Weather Prediction Breakthrough ReportedThe GuardianIn Las Vegas, a Former SpaceX Engineer Is Pulling CO2 From the Air to Make ConcreteFast CompanyThe Unbelievable Scale of AI’s Pirated-Books ProblemThe AtlanticTop Developers Want Nvidia Blackwell Chips. Everyone Else, Not So MuchThe InformationOpenAI’s Deep Research Agent Is Coming for White-Collar WorkWiredNot Everyone Is Convinced by Microsoft’s Topological QubitsIEEE SpectrumJudge Disses Star Trek Icon Data’s Poetry While Ruling AI Can’t Author WorksArs TechnicaSee the First Ever Footage of Sunset on the Moon Captured by Blue GhostDigital TrendsNvidia CEO Says AI Computing Needs to Surge 100-FoldThe Wall Street JournalNvidia Says ‘the Age of Generalist Robotics Is Here’The VergeGoogle Plans to Release New ‘Open’ AI Models for Drug DiscoveryTechCrunchWhen You Might Start Speaking to RobotsMIT Technology Review",
    "summary": {
      "en": "A recent study using brain scans has revealed that infants start forming memories around the age of 12 months. Researchers from Columbia and Yale University conducted scans on 26 infants and toddlers while they completed a memory task. They found that at about one year old, the hippocampus—a brain area essential for memory—begins to activate during memory formation.\n\nThe study aims to understand \"infantile amnesia,\" the phenomenon where most adults cannot recall memories from their early years. Scientists have long puzzled over why this happens despite significant learning during infancy, such as language acquisition and social interactions.\n\nUsing functional MRI (fMRI), the researchers observed how the hippocampus responded to familiar and new images shown to the infants. Results indicated that younger infants (under one year) did not show the same brain activity linked to memory encoding as older infants did. This suggests that while babies can form memories, the chaotic development of their brains during early years makes it challenging to retain them.\n\nThe findings imply that although infants might not have detailed memories, some memories formed during infancy could potentially be retrieved later in life. Future research may further explore how memories are processed during sleep and the minimum brain development needed for vivid autobiographical memories.",
      "ko": "최근 뇌 스캔을 이용한 연구에 따르면, 아기들은 약 12개월이 되면 기억을 형성하기 시작하는 것으로 나타났습니다. 콜롬비아 대학교와 예일 대학교의 연구팀은 26명의 아기와 유아를 대상으로 기억 과제를 수행하는 동안 스캔을 진행했습니다. 연구 결과, 약 1세가 되면 기억 형성에 중요한 역할을 하는 해마라는 뇌 영역이 활성화되기 시작한다는 사실을 발견했습니다.\n\n이 연구는 대부분의 성인이 어린 시절의 기억을 회상하지 못하는 현상인 '유아기 기억상실증'을 이해하는 것을 목표로 하고 있습니다. 과학자들은 아기 시절에 언어 습득과 사회적 상호작용 등 많은 학습이 이루어지는데도 불구하고 왜 이런 일이 발생하는지 오랫동안 궁금해했습니다.\n\n기능적 자기공명영상(fMRI)을 사용하여 연구자들은 아기들에게 보여준 익숙한 이미지와 새로운 이미지에 대해 해마가 어떻게 반응하는지를 관찰했습니다. 결과에 따르면, 1세 미만의 아기들은 기억 인코딩과 관련된 뇌 활동을 보이지 않았고, 이는 나이가 더 많은 아기들과 차이를 보였습니다. 이는 아기들이 기억을 형성할 수는 있지만, 초기 뇌 발달의 혼란스러운 과정 때문에 기억을 유지하기가 어렵다는 것을 시사합니다.\n\n이 연구 결과는 아기들이 구체적인 기억을 갖고 있지 않을 수 있지만, 유아기 동안 형성된 일부 기억은 나중에 회상될 가능성이 있음을 암시합니다. 앞으로의 연구에서는 기억이 수면 중에 어떻게 처리되는지와 생생한 자전적 기억을 위해 필요한 최소한의 뇌 발달에 대해 더 깊이 탐구할 수 있을 것입니다.",
      "ja": "最近の研究によると、赤ちゃんは約12ヶ月の頃から記憶を形成し始めることが明らかになりました。コロンビア大学とイェール大学の研究者たちは、26人の赤ちゃんと幼児を対象に、記憶に関する課題を行っている間に脳のスキャンを実施しました。その結果、約1歳になると、記憶に重要な役割を果たす海馬という脳の部位が記憶形成中に活性化し始めることがわかりました。\n\nこの研究は「幼児期の健忘症」を理解することを目的としています。これは、多くの大人が幼少期の記憶を思い出せない現象です。科学者たちは、言語の習得や社会的な交流など、幼少期に多くの学びがあるにもかかわらず、なぜこのようなことが起こるのか長い間疑問に思ってきました。\n\n機能的磁気共鳴画像法（fMRI）を用いて、研究者たちは赤ちゃんに見せた馴染みのある画像と新しい画像に対する海馬の反応を観察しました。その結果、1歳未満の赤ちゃんは、記憶の符号化に関連する脳の活動を示さなかったのに対し、1歳以上の赤ちゃんはその活動が見られました。これは、赤ちゃんが記憶を形成できる一方で、初期の脳の混乱した発達が記憶の保持を難しくしていることを示唆しています。\n\nこの研究の結果は、赤ちゃんが詳細な記憶を持っていないかもしれないが、幼少期に形成された一部の記憶は後の人生で引き出される可能性があることを示しています。今後の研究では、記憶が睡眠中にどのように処理されるかや、生々しい自伝的記憶に必要な脳の発達の最小限についてさらに探求されるでしょう。"
    }
  },
  {
    "id": "87daf8ed44cf8318",
    "title": {
      "en": "Why we chose LangGraph to build our coding agent",
      "ko": "코딩 에이전트, 랭그래프 선택 이유",
      "ja": "LangGraphを選んだ理由"
    },
    "type": "story",
    "url": "https://www.qodo.ai/blog/why-we-chose-langgraph-to-build-our-coding-agent/",
    "score": 35,
    "by": "jimminyx",
    "time": 1742881836,
    "content": "Why we chose LangGraph to build our coding agent\n\n\t\t\t\t\t\t\t\tTechnology\n\n\t\t\t\t\t\t\t\tSagi Medina\n\n\t\t\t\t\t\t\t\t\tMarch 21, 2025\n\n\t\t\t\t\t\t\t\t\t4 min\n\n\t\t\t\t\t\tTable of ContentsFlexibility to be opinionatedCoherent interfaceReusable components across workflowsState managementAreas for growth\n\n\t\t\t Join Our Discord\n\n\t\t\t\t\tWe’ve been building AI coding assistants at Qodo since the GPT-3 days. Our initial approach was highly structured with predefined flows for different coding tasks like test generation, code reviews, and improvements. This approach worked well with earlier generations of LLMs and with structured flows we were able to get real-world value from older models, despite all of their limitations.\nSince Claude Sonnet 3.5 was released 9 months ago, LLMs have become significantly more capable at general-purpose coding tasks. The new models opened up the possibility to build something more dynamic and flexible while still maintaining our standards for code quality. We wanted to move away from rigid workflows to an agent that could adapt to any kind of user request, while still reflecting our opinionated views on how AI can be best used for coding.\nInitially, we needed a framework that would let us quickly validate our ideas and from the few options that were available about 4 months ago, we settled on LangGraph for our initial proof of concept. We were pleasantly surprised to see that the framework has proven flexible and mature enough to carry us all the way to production.\nIn this post, I’ll explain why LangGraph was the right choice for us, and how it enabled us to build a coding assistant that balances flexibility with our opinionated approach to coding best practices.\nFlexibility to be opinionated\nOur key consideration was the ability to create opinionated workflows while maintaining adaptability. LangGraph takes a graph-based approach that gives you flexibility to build agents that land anywhere on the spectrum from completely open-ended — where you just give an LLM all available tools and let it run in a loop — to fully structured deterministic flows (like the ones we started with).\nAt its core, LangGraph lets you define a state machine for your agent. You create nodes that represent discrete steps in your workflow and edges that define the possible transitions between them. Each node can perform specific functions—gathering context, planning, generating code, or validating—while the graph structure determines how these functions connect.\nThe density of connections in the graph corresponds to how structured or flexible your agent is. A sparse graph with few connections corresponds with a more rigid, predictable flow where each step leads to exactly one next step. A dense graph with many interconnections gives the agent more freedom to choose its path.\nFuture, more capable models might work best with fully open-ended approaches. But even with the best current LLMs, you still get better results when you guide them through the problem. If you use LLMs directly for coding, you’ve probably already developed your own workflow — like breaking problems down, providing context strategically, guiding the model through complex reasoning, and backtracking or iterating when needed.\nThe nice thing about LangGraph’s flexibility is that we can easily recalibrate how structured our flows are when new, more powerful models are released.\nOur main flow follows a pattern that you might recognize: first, a context collection node gathers relevant information from the codebase (and external resources via MCP integration); next, a planning node breaks down the task into manageable steps; then an execution node generates the actual code; finally, a validation node checks the output against best practices and requirements. When validation fails, the agent loops back to execution with specific feedback rather than starting from scratch.\n\nCoherent interface\nWhen you’re building a complex system, a framework should simplify rather than complicate your work. LangGraph’s API does exactly that.\nHere’s how a simplified version of our main workflow looks when implemented with LangGraph:\nPlain textCopy to clipboardOpen code in new windowEnlighterJS 3 Syntax Highlighterfrom langgraph.graph import StateGraph, ENDworkflow = StateGraph(name=\"coding_assistant\")workflow.add_node(\"context_collector\", collect_relevant_context)workflow.add_node(\"task_planner\", create_execution_plan)workflow.add_node(\"task_executor\", execute_plan)workflow.add_node(\"validator\", validate_output)# Define flow between nodesworkflow.add_edge(\"context_collector\", \"task_planner\")workflow.add_edge(\"task_planner\", \"task_executor\")workflow.add_edge(\"task_executor\", \"validator\")# Conditional routing based on validation resultsworkflow.add_conditional_edges(    \"validator\",    should_revise,    {        True: \"task_executor\",  # Loop back if revision needed        False: END               # Complete if validation passes    })graph = workflow.compile()graph.invoke({\"user_input\": \"build me a game like levelsio\"})from langgraph.graph import StateGraph, END\n\nworkflow = StateGraph(name=\"coding_assistant\")\nworkflow.add_node(\"context_collector\", collect_relevant_context)\nworkflow.add_node(\"task_planner\", create_execution_plan)\nworkflow.add_node(\"task_executor\", execute_plan)\nworkflow.add_node(\"validator\", validate_output)\n\n# Define flow between nodes\nworkflow.add_edge(\"context_collector\", \"task_planner\")\nworkflow.add_edge(\"task_planner\", \"task_executor\")\nworkflow.add_edge(\"task_executor\", \"validator\")\n\n# Conditional routing based on validation results\nworkflow.add_conditional_edges(\n    \"validator\",\n    should_revise,\n    {\n        True: \"task_executor\",  # Loop back if revision needed\n        False: END               # Complete if validation passes\n    }\n)\n\ngraph = workflow.compile()\ngraph.invoke({\"user_input\": \"build me a game like levelsio\"})from langgraph.graph import StateGraph, END\n\nworkflow = StateGraph(name=\"coding_assistant\")\nworkflow.add_node(\"context_collector\", collect_relevant_context)\nworkflow.add_node(\"task_planner\", create_execution_plan)\nworkflow.add_node(\"task_executor\", execute_plan)\nworkflow.add_node(\"validator\", validate_output)\n\n# Define flow between nodes\nworkflow.add_edge(\"context_collector\", \"task_planner\")\nworkflow.add_edge(\"task_planner\", \"task_executor\")\nworkflow.add_edge(\"task_executor\", \"validator\")\n\n# Conditional routing based on validation results\nworkflow.add_conditional_edges(\n    \"validator\",\n    should_revise,\n    {\n        True: \"task_executor\",  # Loop back if revision needed\n        False: END               # Complete if validation passes\n    }\n)\n\ngraph = workflow.compile()\ngraph.invoke({\"user_input\": \"build me a game like levelsio\"})\n\nThis declarative approach makes the code almost self-documenting. The workflow definition directly mirrors our conceptual diagram, which makes it easy to reason about and modify.\nEach node function receives the current state and returns updates to that state. There’s no magic happening behind the scenes, just straightforward state transitions.\nLangChain gets a lot of flack for its overly complicated abstraction, but the team really cooked with the LangGraph interface. It adds just enough structure without getting in your way or forcing you to adopt a complicated mental model and puts your agent logic on full display rather than obscuring it behind abstractions.\nReusable components across workflows\nReusability is what separates valuable frameworks from disposable ones. The node-based architecture that LangGraph uses is great here.\nOur context collection node is a good example. It handles gathering relevant information from the codebase and it’s used in pretty much every flow. The same goes for our validation node, which checks code quality and runs tests. These components can slot into different graphs with minimal configuration.\nAs we build out more flows, the velocity payout is huge. We’re building specialized flows like TDD that have different structures but reuse many of the same nodes, just connected in a different configuration with a few specialized components added in.\n\nState management\nThe most satisfying part of adopting the right framework is when you get useful functionality out of the box. LangGraph’s built-in state management is a perfect example.\nAdding persistence to our agent took just a few lines of code:\nPlain textCopy to clipboardOpen code in new windowEnlighterJS 3 Syntax Highlighterfrom langgraph.graph import StateGraph, ENDfrom langgraph.checkpoint.postgres import PostgresSaverworkflow = StateGraph(name=\"coding_assistant\")......checkpointer = PostgresSaver.from_conn_string(    \"postgresql://user:password@localhost:5432/db\")checkpointer.setup()graph = workflow.compile(checkpointer=checkpointer)from langgraph.graph import StateGraph, END\nfrom langgraph.checkpoint.postgres import PostgresSaver\n\nworkflow = StateGraph(name=\"coding_assistant\")\n...\n...\n\ncheckpointer = PostgresSaver.from_conn_string(\n    \"postgresql://user:password@localhost:5432/db\"\n)\ncheckpointer.setup()\n\ngraph = workflow.compile(checkpointer=checkpointer)from langgraph.graph import StateGraph, END\nfrom langgraph.checkpoint.postgres import PostgresSaver\n\nworkflow = StateGraph(name=\"coding_assistant\")\n...\n...\n\ncheckpointer = PostgresSaver.from_conn_string(\n    \"postgresql://user:password@localhost:5432/db\"\n)\ncheckpointer.setup()\n\ngraph = workflow.compile(checkpointer=checkpointer)\n\nThat’s it. With this simple addition, our entire workflow state—including context collected, plans made, and code generated—persists to our postgres database without us building any custom infrastructure. There are also SQLite and in memory checkpointers that can be added just as easily.\nWhat’s really neat is that this doesn’t just enable basic persistence across sessions. It supports checkpoints and branch points so you can undo and replay changes.\nAreas for growth\nWhile LangGraph has been a great foundation for our agentic flows, it’s not without challenges. One pain point has been documentation. The framework is developing very quickly and the docs are sometimes incomplete or out of date. The maintainers are great and were super responsive on Slack (thanks Harrison and Nuno for all the help :)). Be prepared to potentially need to communicate directly with the project maintainers if you’re using the newer and more niche capabilities.\nTesting and mocking is a huge challenge when developing LLM driven systems that aren’t deterministic. Even relatively simple flows are extremely hard to reproduce. Our agent interacts extensively with the IDE, which is difficult to simulate in automated tests. We built a mock repository that simulates basic IDE operations, but it doesn’t perfectly replicate the real environment. This creates a gap between what we can test automatically and what happens in production.\nFor example, operations like “find all usages of this function” that depend on the IDE’s language server are particularly hard to mock. This forced us to rely more on manual testing than we’d prefer, which slowed down the iteration cycle.\nMature frameworks tend to provide robust infrastructure for mocking and testing. I’m hopeful that LangGraph will develop in these areas over time.\n\n\t\t\t\t\t\t\t\tSagi Medina\n\n\t\t\t\t\t\t\t\t\tMarch 21, 2025\n\n\t\t\t\t\t\t\t\t\t4 min\n\n\t\t\t\t\t\tStart to test, review and generate high quality code\n\n\t\t\tGet Started\n\n\t\t\t\t\t\t\tGet Qodo\n\n\t\t\t\t\t\t\t\t\t\tVS Code\n\n\t\t\t\t\t\t\t\t\t\tJetBrains\n\n\t\t\t\t\t\t\t\t\t\tGithub, Qodo Merge\n\n\t\t\t\t\t\t\tSubscribe to the newsletter\n\n\t\t\t\t\t\t\t\thbspt.forms.create({\n\t\t\t\t\t\t\t\t\tregion: \"eu1\",\n\t\t\t\t\t\t\t\t\tportalId: \"26816375\",\n\t\t\t\t\t\t\t\t\tformId: \"6233f5ed-8ded-40a7-94cf-8fe3631b4b56\",\n\t\t\t\t\t\t\t\t\tonFormReady: function($form) {\n\t\t\t\t\t\t\t\t\t\tdocument.querySelector('.hs-input[type=\"email\"]').value = '';\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t});\n\n\t\t\t\t\t\t\tShare\n\nMore from our blog\n\n\t\t\t\t\t\t\t\t\tCode Reviews\n\n\t\t\t\t\t\t\t\t\tNews & Updates\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t4 min\n\n\t\t\t\tQodo Merge 1.0: solving key challenges in AI-assisted code reviews\n\n\t\t\t\t\t\t\t\tElana Krasner\n\n\t\t\t\t\t\t\t\tJan 29, 2025\n\n\t\t\t\tRead more\n\n\t\t\t\t\t\t\t\t\tLearn\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t14 min\n\n\t\t\t\tCommon JavaScript Errors Explained and How to Fix Them\n\n\t\t\t\t\t\t\t\tDavid Parry\n\n\t\t\t\t\t\t\t\tMar 24, 2025\n\n\t\t\t\tRead more\n\n\t\t\t\t\t\t\t\t\tTesting\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t10 min\n\n\t\t\t\t10 Best Automated Unit Testing Tools for Java\n\n\t\t\t\t\t\t\t\tMarin Levrel\n\n\t\t\t\t\t\t\t\tMar 24, 2025\n\n\t\t\t\tRead more\n\n\t\t\tBrowse the blog",
    "summary": {
      "en": "**Summary: Why We Chose LangGraph for Our Coding Agent**\n\nAt Qodo, we've been creating AI coding assistants since the GPT-3 era. Initially, we used a structured approach with fixed workflows for tasks like test generation and code reviews. However, with the advancement of LLMs (language learning models), particularly since the release of Claude Sonnet 3.5, we wanted a more flexible and adaptable coding agent that still aligns with our coding standards.\n\nWe chose LangGraph for our coding assistant because it allows us to create opinionated workflows while being adaptable. LangGraph uses a graph-based structure, enabling us to define workflows with nodes representing different steps (like gathering context and generating code) and edges showing how these steps connect. This structure allows us to adjust the level of flexibility in our workflows as new models emerge.\n\nLangGraph also offers a coherent interface, making it easy to implement complex systems. Its reusable components allow us to build various workflows efficiently, as many nodes (like context collection and validation) can be used across different tasks.\n\nFurthermore, LangGraph provides built-in state management, which simplifies adding features like data persistence. However, we face challenges, including incomplete documentation and difficulties in testing due to the unpredictable nature of LLMs.\n\nIn conclusion, LangGraph has proven to be a valuable framework for developing our coding agent, balancing flexibility with our best practices, while also allowing for future growth and adaptation.",
      "ko": "Qodo에서는 GPT-3 시대부터 AI 코딩 도우미를 개발해왔습니다. 처음에는 테스트 생성이나 코드 리뷰와 같은 작업을 위해 고정된 워크플로우를 사용하는 구조적 접근 방식을 취했습니다. 그러나 언어 학습 모델(LLM)의 발전, 특히 Claude Sonnet 3.5의 출시 이후, 우리는 코딩 기준에 맞으면서도 더 유연하고 적응 가능한 코딩 에이전트를 원하게 되었습니다.\n\n우리는 LangGraph를 코딩 도우미로 선택했습니다. LangGraph는 의견이 반영된 워크플로우를 만들 수 있도록 하면서도 유연성을 제공합니다. 이 시스템은 그래프 기반 구조를 사용하여, 다양한 단계(예: 컨텍스트 수집 및 코드 생성)를 나타내는 노드와 이 단계들이 어떻게 연결되는지를 보여주는 엣지로 워크플로우를 정의할 수 있게 합니다. 이러한 구조 덕분에 새로운 모델이 등장할 때마다 워크플로우의 유연성을 조정할 수 있습니다.\n\nLangGraph는 일관된 인터페이스를 제공하여 복잡한 시스템을 쉽게 구현할 수 있게 해줍니다. 재사용 가능한 구성 요소 덕분에 다양한 워크플로우를 효율적으로 구축할 수 있으며, 많은 노드(예: 컨텍스트 수집 및 검증)를 여러 작업에 걸쳐 사용할 수 있습니다.\n\n또한, LangGraph는 내장된 상태 관리 기능을 제공하여 데이터 지속성과 같은 기능을 추가하는 것을 간소화합니다. 하지만 우리는 불완전한 문서와 LLM의 예측할 수 없는 특성으로 인해 테스트에 어려움을 겪고 있습니다.\n\n결론적으로, LangGraph는 우리의 코딩 에이전트를 개발하는 데 있어 유용한 프레임워크로 입증되었습니다. 이는 유연성과 우리의 모범 사례를 균형 있게 유지하면서도 미래의 성장과 적응을 가능하게 합니다.",
      "ja": "Qodoでは、GPT-3の時代からAIコーディングアシスタントを開発してきました。当初は、テスト生成やコードレビューなどのタスクに対して固定されたワークフローを用いた構造的アプローチを採用していました。しかし、言語学習モデル（LLM）の進化、特にClaude Sonnet 3.5のリリース以降、私たちはより柔軟で適応性のあるコーディングエージェントを求めるようになりました。それでも、私たちのコーディング基準に合致するものである必要があります。\n\n私たちがコーディングアシスタントにLangGraphを選んだ理由は、意見を反映したワークフローを作成しつつ、柔軟性を持たせることができるからです。LangGraphはグラフベースの構造を使用しており、異なるステップを表すノード（コンテキストの収集やコード生成など）と、それらのステップがどのように接続されているかを示すエッジを定義できます。この構造により、新しいモデルが登場するたびにワークフローの柔軟性を調整することが可能です。\n\nさらに、LangGraphは一貫したインターフェースを提供しており、複雑なシステムの実装が容易です。再利用可能なコンポーネントにより、さまざまなワークフローを効率的に構築でき、多くのノード（コンテキスト収集や検証など）を異なるタスクで使い回すことができます。\n\nまた、LangGraphは組み込みの状態管理機能を提供しており、データの永続性などの機能を追加する際の手間を軽減します。しかし、私たちは不完全なドキュメントやLLMの予測不可能な性質によるテストの難しさといった課題にも直面しています。\n\nLangGraphは、私たちのコーディングエージェントを開発するための貴重なフレームワークであることが証明されており、柔軟性とベストプラクティスのバランスを取りながら、将来の成長と適応を可能にしています。"
    }
  },
  {
    "id": "462735f2d1f833c4",
    "title": {
      "en": "I won't connect my dishwasher to your cloud",
      "ko": "식기세척기 클라우드 거부!",
      "ja": "食洗機はクラウド不要"
    },
    "type": "story",
    "url": "https://www.jeffgeerling.com/blog/2025/i-wont-connect-my-dishwasher-your-stupid-cloud",
    "score": 732,
    "by": "HieronymusBosch",
    "time": 1742836147,
    "content": "March 24, 2025\n\n            This weekend I had to buy a new dishwasher because our old GE died.\n\nI bought a Bosch 500 series because that's what Consumer Reports recommended, and more importantly, I could find one in stock.\n\nAfter my dad and I got it installed, I went to run a rinse cycle, only to find that that, along with features like delayed start and eco mode, require an app.\n\nNot only that, to use the app, you have to connect your dishwasher to WiFi, set up a cloud account in something called Home Connect, and then, and only then, can you start using all the features on the dishwasher.\n\n  This blog post is a lightly-edited transcript of my latest YouTube video on Level 2 Jeff: I won't connect my dishwasher to your stupid cloud.\n\nSo getting back first to that old GE dishwasher, it was, I don't know, I think that planned obsolescence is something that applies to many consumer products today.\n\nCompanies know if they design something to last only 5 or 10 years, that means in 5 or 10 years someone's going to have to buy a whole new one.\n\nAnd on my GE Amana dishwasher, it started having weird power issues, like the controls would just not light up unless I reset the circuit breaker for a few minutes. That started happening more often, and this past Saturday it just wouldn't come on no matter what, even after I tested and re-wired it all the way from the panel up to the dishwasher's internal power connector.\n\nSo it was dead.\n\nNext up, I looked at what it took to get a control board. Well... $299 for a control board that was 'special order' and might not even fix the problem? That's a non-starter for my $600, 8-year-old dishwasher.\n\nEven if I got it fixed, the front panel was starting to rust out at the hinge points (leaving some metal jaggies that my soon-to-be-crawling 6 month old could slice his fingers on), and other parts of the machine were showing signs of rust/potential future leaks...\n\nIt was Saturday night, and for a family of five, a dishwasher is kinda important. We don't have 1.5 hours every night to spend hand-washing dishes (not to mention the water bill!).\n\nSo I needed to get a new one, and it's really hard for me to schedule a few hours for my Dad and I to get it done in the middle of the week (plus that's multiple days without a dishwasher!).\n\nSo I did some research, and I found Bosch seemed to have the best bet for under $1,000, available locally on a Sunday.\n\nConsumer Reports, random Redditors, etc. seemed to have some praises for Bosch—on Reddit many also praised Miele, but I couldn't find any of those available locally. And Consumer Reports especially praised all the Bosch units, with them topping all their reliability and customer satisfaction charts!\n\nI remembered five or ten years ago, whenever I had bought my old GE, I remembered Bosch topped the charts too, but back then I settled for GE to save a few bucks...\n\nInstallation was (mostly) great!\n\nSo I spent a little more this time, hoping for a better experience. And installation was actually great—it was a lot easier to install the Bosch than it was that the GE.\n\nIt has a plastic base that slides better on the floor, and there's easier routing of the drain hose, inlet hose, and power wire that makes it less risky when you're pushing the thing into the blind cutout under the counter.\n\nThe one weird thing was that whoever like tightened the feet on the bottom at the factory must've used an impact driver or something because they were all practically embedded, and wouldn't budge.\n\nI was turning the little screw on the front that pushes the rear foot down through a little gearing, but the worm gear slipped out and kinda shoved the long rod that connects the front to the back out of place. I had to buy a 10mm hex socket to wiggle the foot loose enough the gearing would actually work.\n\nBut once that was done, the rest of the install was seamless. (Thanks especially to some help from my Dad).\n\nFirst use, encountering the Cloud requirement\n\nSo I turned it on, and immediately hated the new touch sensor stuff on it.\n\nThe old GE had buttons: you press them in, they click and you know that you pressed a button.\n\nThe touch sensor, you kind of touch it and the firmware—like this new dishwasher actually takes time to boot up! I had to reset it like three times and my wife meanwhile was like laughing at me like look at this guy who does tech stuff and he can't even figure out how to change the cycle on it.\n\nThat took about five minutes, sadly.\n\nBut eventually I pulled out the manual book because I was like... \"this is actually confusing.\"\n\nIt should be like: I touch the button and it changes to that mode! But that was not how it was working.\n\nI wanted to run just a rinse cycle to make sure the water would go in, the water would pump out through the sump, and everything worked post-install.\n\nBut I couldn't find a way to do a rinse cycle on the control panel.\n\nSo I looked in the manual and found this note:\n\nIt says options with an asterisk—including Rinse, Machine Care (self-cleaning), HalfLoad, Eco, and Delay start, are \"available through Home Connect app only and depending on your model.\"\n\nThe 500 series model I bought isn't premium enough to feature a 7-segment display like the $400-more-expensive 800 series, so these fancy modes are hidden behind an app and cloud service.\n\nI was like, \"Okay, I'll look up this app and see if I can use it over Bluetooth or locally or whatever.\"\n\nNope! To use the app, you have to connect your dishwasher to your Wi-Fi, which lets the dishwasher reach out on the internet to this Home Connect service.\n\nYou have to set up an account on Home Connect, set up the Home Connect app on your phone, and then you can control your dishwasher through the Internet to run a rinse cycle.\n\nThat doesn't make any sense to me.\n\nAn app? I mean, I can understand maybe adding some neat convenience features for those who want them. Like on my new fridge—which I chose not to connect to WiFI—it has an app that would allow me to monitor the inside temperature or look up service codes more easily. If I wanted those add-on features, which my old fridge didn't have, I could get them.\n\nBut requiring an app to access features that used to be controllable via buttons on the dishwasher itself—or are still if you pay $400 more for the fancy \"800\" model? That's no bueno.\n\nWhat can I do?\n\nWell, first of all, I could just not use those features. That's kind of annoying because I bought it with the assumption that I could run the cleaning cycle, that I could run the rinse cycle without having to have an app and Wi-Fi.\n\nAnother option is I could just connect it to my Wi-Fi, maybe on an IoT VLAN.\n\nBut it's not like a video doorbell, where an Internet connection adds on functionality, like being able to see who rang while you're on vacation, or storing security footage clips on the cloud so you have them available even after someone robs your house...\n\nBut a dishwasher... I'm not going to remote control my dishwasher and like, run an extra rinse cycle while I'm on a beach somewhere.\n\nI don't need Internet on my dishwasher.\n\nAnother third option is somebody has reverse engineered this protocol and built HCPY, a Home Connect Python library.\n\nBut here's the problem: I already spent like four hours getting this dishwasher installed in my kitchen. I don't want to spend another four hours configuring my own web UI for it—which still requires at least a one-time connection through Home Connect!—and maintaining that as a service on my local network, relying on an unauthorized third party library using reverse-engineering to get at the private dishwasher API!\n\nWhat can we do?\n\nI don't think we should let vendors get away with this stuff.\n\nFirst, it lets product designers get lazy.\n\nA feature like a little display, a little seven segment display that can show like two letters and a number of minutes remaining or something like that... How much does that cost?\n\nHow hard is it to integrate that into every model, even the cheap ones?\n\nA lot of cheap dishwashers actually have those things, but not Bosch!\n\nWith Bosch, you have to pay $400 for the privilege of a little display!\n\nSecond, this might be a little bit conspiracy theory or whatever, but it feels like it's part of planned obsolescence. Just like with the GE, where a lot of parts are designed to rust out after 5 or 10 years.\n\nIf you have a cloud app, that means there's a cloud service that has to be running. That costs money to maintain.\n\nAnd if there's no subscription fee right now, that means one of two things:\n\nThey could be selling our data already.\nAt some point, they'll either close the service because it's a cost center (so the rinse cycle and eco mode on all these dishwashers just goes \"POOF!\"), or they're going move to a subscription model.\n\nAll of a sudden, if you want to run the self-cleaning cycle, you better start paying five bucks to Bosch every month. Forever.\n\nThat's insane.\n\nThird, it's a security hole in your local network.\n\nBosch might be a little better than some no-name light bulb company making IoT light bulbs on Amazon, but only a little.\n\nI don't want to have Bosch having full internet access on my local network.\n\nTheir API would be able to talk back to my dishwasher, and the dishwasher—unless I put it on a VLAN, which 99% of consumers have no clue how to do that...\n\nThat's just something that shouldn't have to happen.\n\nThis is a dishwasher!\n\nI don't know.\n\nWhat should be done?\n\nWhen I posted on social media about this, a lot of people told me to return it.\n\nBut I spent four hours installing this thing built into my kitchen.\n\nI hooked it up to the water, it's running through cycles... it is working. I'll give them that. It does the normal stuff, but you know, there are some features that don't work without the app.\n\nAt a minimum, I think what Bosch should do is make it so that the dishwasher can be accessed locally with no requirement for a cloud account. (Really, it'd be even better to have all the functions accessible on the control panel!)\n\nAnyone building an IoT device, here is my consumer-first, e-waste-reduction maxim:\n\n  First local, then cloud.\n\nCloud should be an add-on.\n\nIt should be a convenience for people who don't know how to do things like connect to their dishwasher with an app locally.\n\nAnd it's not that hard.\n\nA little ESP32, a little $1 chip that you can put in there could do all this stuff locally with no cloud requirement at all.\n\nI think that there might be some quants or people who want to make a lot of money building all these cloud services.\n\nI don't think that it's people who are consumer-first and eco-conscious because if they were, they would give us control first and then add on 'nice' quality of life features through a cloud service.\n\nWith my Bosch 500 series dishwasher, I was excited after the easy install (besides those leveling feet), then was let down so hard once I found out I couldn't use all the features it came with.\n\n      Further reading\n\n          Using the Shelly Plug to monitor Starlink's power consumption\n          Raspberry Pi Zero 2 powers the Null 2 RetroPie gaming handheld\n          Build your own private WireGuard VPN with PiVPN\n\n              dishwasher\n              cloud\n              internet of things\n              iot\n              apps\n              home\n              kitchen\n              bosch\n              security\n\n    Add new comment\n\n      Comments\n\n     me – 17 hours ago\n\n            Beyond ridiculous.\nWe have a Whirlpool dryer purchased in (cough) fall 1984.  Nice and simple and old school.  No computers.  No cloud.  JustPlainWorks.\nWe had a guy come out a number of years ago to do minor maintenance on the corresponding 20+ year Whirlpool washer and he said something along the lines of \"yup - love these models from back then. They never break.  They are repairable.  Do not ever replace that dryer. Just run it forever because that's how long it is going to last. So simple.\"\nContrast that with a former coworker who went with a super programmable LG (I think) who had nothing but problems with all the unnecessary electronics.\nAt some point you need to vote with your wallet.  Rip it out and take it back as 'does not work'.  Something that expensive shouldn't require anything at all other than power, water, and you pressing a button.\n\n      Reply\n\n    Billybillybilly – 15 hours ago\n\n            You really appreciate a lot of the ingenuity that goes into engineer consistent design. One example I have that features a useful design that is local first then cloud is ac on cars.\nOn my car, if I hold my keyfob it rolls down all the windows to get the stale air out. It’s a great local feature.\nI was spending time with a friend at a park, and as were walking back the car they whip out their phone to turn on their cars ac remotely. A nice feature that doesn’t work when your car is under 60 ft of rock.\nCut to trying to explain how a car has cellular data and has to connect to a network to be able to remote control it.\nI would like to be able to crank my ac remotely but for now local control best suits my desires.\n\n      Reply\n\n    Jeff Geerling – 15 hours ago\n\n            I found a page on Home Connect on the 'Consumer Action Taskforce' wiki, and it mentions the cloud requirement and 'no cloud' offline mode (which requires a cloud account and WiFi connection to enable, lol), but that page didn't mention the fact some devices hide features behind Home Connect, that you can't use if you don't use Home Connect.\n      Reply\n\n    Bill – 14 hours ago\n\n            Bosch has beaten you.  You paid your money, found a major defect (and it is a defect), and kept it anyway.  They win, you lose.  Zero deterrence.\n\n      Reply\n\n    KaBosch – 11 hours ago\n\n              In reply to Bosch has beaten you.  You… by Bill\n\n            Jeff should return it, it is *only* four hours. The block is mental, not physical. The product is defective. I won't be be buying a Bosch, too bad because I had one in a rental years ago and it was amazing. The MBAs who made this decision should own their mistakes.\n\n      Reply\n\n    Ryan of Drupal – 6 hours ago\n\n              In reply to Jeff should return it, it is… by KaBosch\n\n            Time is money and four hours is a lot of opportunity cost that he could have spent doing multitudes of income generating time. I would be willing to wager that Jeff's time is honestly worth more than 100$ an hour,  so taking it back *is* a loss.\n\n      Reply\n\n    Steve – 11 hours ago\n\n              In reply to Bosch has beaten you.  You… by Bill\n\n            Agreed, they just want money and they got it.\n\n      Reply\n\n    marimba – 12 hours ago\n\n            Please, return it! It‘s the only way they learn.\n\n      Reply\n\n    Some dude from… – 11 hours ago\n\n            Your government ordering strikes on other nations over signal, and you are worried about your dishwasher data stored in eagle data center somewhere in Swiss alps.\n\n      Reply\n\n    another random dude  – 11 hours ago\n\n              In reply to Your government ordering… by Some dude from…\n\n            Hahaha dude you should not be honest online.\n\n      Reply\n\n    Another dude from... – 10 hours ago\n\n              In reply to Your government ordering… by Some dude from…\n\n            Yea, he should be worried about those things of which he has no control over, versus this, which he does and impacts him on a personal level.\n\n      Reply\n\n    Not random – 7 hours ago\n\n              In reply to Your government ordering… by Some dude from…\n\n            It's all billionaire oligarchs doing billionaire oligarch shit. Their power comes from exploiting us and our willingness to let them do it.\nIt starts with forcing us to be renters. It ends with bombs dropping on innocent children held hostage by other oligarchs as human shields so they can launch bombs at innocent merchant marines exploited by still other oligarchs. It's all about control at every level.\nBosch and GE can get bent.\n\n      Reply\n\n    David – 9 hours ago\n\n            There's also an option to use other version of hcpy with HomeAssistant MQTT support which setup should take quite a bit less than a few hours https://github.com/hcpy2-0/hcpy/wiki/HomeAssistant-Addon but fully agreed that this should not be the standard... They've probably went the route they did so not to have anything Bluetooth related as that might look cheap and/or might introduce other vulnerabilities/bugs in the communication stack.\n\n      Reply\n\n    Dmitry Samsonov – 8 hours ago\n\n            Now imagine this scenario: A global war breaks out, and the cloud service controlling your dishwasher activates its hidden military functions... And it turns out there are several of them—backdoors in the chips from China, exploits in the application software from the EU, and vulnerabilities in open-source libraries, inserted by Russia and North Korea... And you could dodge all this nightmare by just washing your dishes the old-fashioned way—supposedly better for your health too!\n\n      Reply\n\n    Seb – 7 hours ago\n\n            Lesson learned from your experience: check out appliance models, and read their manuals before deciding what to get.\n\n      Reply\n\n    pg – 7 hours ago\n\n            Don't get your hopes too high.\nBOSCH's electrical equipment doesn't belong to BOSCH anymore, but is part of the large Siemens group for many years. Siemens in germany is known for its consumer products to be very expensive (cause of their very good reputation they may earned a hundred years ago), full of mostly useless features,  complicated in handling, very low on product quality, high mainting aand repair rates, and outstanding repair cost.\nIn germany they have a joke:\"Does it have to work, or can it be by Siemens?\"\nI had the full parade of Siemens/Bosch-be-part-of-Siemens consumer electrical, and electronical products over the last twentyfive years: all expensive crap. I'm  through with dishwashers, vacuum cleaners, refrigerators, stoves, and several other devices, all Siemens/Bosch, all expensive, and crap!\nBosch and Siemens brands are on my personal black list of never buy again, ever.\nSorry, to tell you that, but you propably be better served with some \"mexican\" dishwasher for half the price - it will at least does its job for maybe 6...8 years without repairs or 'funny behaviour'.\nYou'll see when you Bosch maschine gets broken within the next two and a half years, and face the repair cost half the price of a new machine, and then it will be broken half a year later again. We decided to change your dishwasher to another brand after we paid over 150% of the purchasing price for repairs only within 4 years, not to mention hundreds of situations I needed all my self control not to rip this crap out of the kitchen and throw it out of the window.\n\n      Reply\n\n    MR – 7 hours ago\n\n            We got the Bosch model with the LCD but still have to use the app for Machine Care, Rinse, etc. Some of the IOT features are pretty good honestly, reminders to fill the rinse aid or run a machine care cycle are useful, but I'd much prefer to have no wifi at all! Unfortunately most of the new applicances have wifi or they are low grade base models with no features, and it takes so much work to find a decent used applicance it's difficult to find a proper solution.\n\n      Reply\n\n    Leo – 5 hours ago\n\n            Where I rent, there's a V-Zug dishwasher. Very common over here in Switzerland. It's got exactly what you're asking, local connectivity via Bluetooth and cloud stuff over wifi if you desire. None of it mandatory. However it does come with a different price tag.\nWhat you might be experiencing is the old printer and cartridge business model. Sell a product below cost and make it back later via subscriptions / data brokerage.\n\n      Reply",
    "summary": {
      "en": "This text describes the author's experience buying a new Bosch 500 series dishwasher after their old GE model broke down. They chose Bosch based on positive reviews from Consumer Reports and availability. Installation went well, but they were frustrated to discover that many features, like rinse cycles and eco mode, require an app and a WiFi connection. \n\nThe author criticizes modern appliances for needing internet connectivity to access basic functions, arguing this is a form of planned obsolescence and raises security concerns. They express disappointment that features that used to be accessible through buttons now require an account on a cloud service. \n\nThe author concludes by suggesting that manufacturers should prioritize local control over cloud features in appliances, advocating for consumer-first design to avoid unnecessary complications and maintain functionality. They also mention that returning the dishwasher has been suggested, but they hesitate due to the time invested in installation.",
      "ko": "저자는 오래된 GE 모델이 고장난 후 새로운 Bosch 500 시리즈 식기세척기를 구매한 경험을 이야기합니다. 소비자 리포트에서 긍정적인 평가를 받고, 제품이 쉽게 구할 수 있어 Bosch를 선택했습니다. 설치는 순조롭게 진행되었지만, 세척 사이클이나 에코 모드와 같은 많은 기능이 앱과 WiFi 연결을 요구한다는 사실에 실망했습니다.\n\n저자는 현대 가전제품이 기본 기능을 사용하기 위해 인터넷 연결이 필요하다는 점을 비판하며, 이는 계획된 노후화의 한 형태라고 주장합니다. 또한, 버튼으로 쉽게 접근할 수 있었던 기능들이 이제는 클라우드 서비스에 계정을 만들어야 사용할 수 있게 된 것에 실망감을 표합니다.\n\n저자는 제조사들이 클라우드 기능보다 로컬 제어를 우선시해야 한다고 제안하며, 소비자 중심의 디자인을 통해 불필요한 복잡성을 피하고 기능성을 유지해야 한다고 강조합니다. 또한, 식기세척기를 반품하는 것도 고려했지만 설치에 투자한 시간 때문에 주저하고 있다고 언급합니다.",
      "ja": "著者は、古いGE製の食器洗い機が故障したため、新しいボッシュ500シリーズの食器洗い機を購入した経験を語っています。ボッシュを選んだ理由は、消費者レポートの好意的な評価と入手のしやすさです。設置は順調に進みましたが、リンスサイクルやエコモードなどの多くの機能がアプリとWi-Fi接続を必要とすることに不満を感じました。\n\n著者は、基本的な機能にアクセスするためにインターネット接続が必要な現代の家電について批判しています。これは計画的陳腐化の一形態であり、セキュリティの懸念も引き起こすと主張しています。ボタンで簡単にアクセスできた機能が、今ではクラウドサービスのアカウントを必要とすることに失望を表明しています。\n\n著者は、メーカーは家電のクラウド機能よりもローカルでの操作を優先すべきだと提案し、消費者を第一に考えたデザインを推奨しています。これにより、不必要な複雑さを避け、機能性を維持できると述べています。また、食器洗い機を返品することが提案されたものの、設置にかけた時間を考えると躊躇しているとも言っています。"
    }
  },
  {
    "id": "e5bcbc9609f02ec9",
    "title": {
      "en": "The earliest versions of the first C compiler known to exist",
      "ko": "최초의 C 컴파일러 발견",
      "ja": "最初のCコンパイラの誕生"
    },
    "type": "story",
    "url": "https://github.com/mortdeus/legacy-cc",
    "score": 347,
    "by": "diginova",
    "time": 1742545039,
    "content": "legacy-cc\nThe earliest versions of the very first c compiler known to exist in the wild written by the late legend himself dmr.\nThese are not capable of being compiled today with modern c compilers like gcc. I am only posting these here for the enjoyment and reminiscing of the spark that ignited a soon to be trillion dollar industry. Enjoy. :)\nYou guys might be able to play around building this compilier with Aiju's pdp-11/unix emulator. I havent tried building this myself so I cant confirm it works, but I posted the link to her emulator for anybody wanting to hack around with it.\nSource: [https://www.bell-labs.com/usr/dmr/www/primevalC.html] alt-link [http://cm.bell-labs.co/who/dmr/primevalC.html]\nUseful Links\nPDP-11 Emulator: [http://pdp11.aiju.de/] (so i've been playing around with this to see if it still works, and it clearly has problems right now. if somebody wants to try and create like a VM image of unix running on PDP-7 hardware, id happily link to it here.)\nResearch Unix Repository: [https://www.tuhs.org/Archive/Distributions/Research/]",
    "summary": {
      "en": "This text discusses the earliest versions of the first C compiler created by Dennis Ritchie (DMR). These compilers cannot be used with modern tools like GCC. The author shares them for nostalgia and to highlight their historical significance in starting a major industry. \n\nThey mention the possibility of using Aiju's PDP-11/Unix emulator to experiment with building the compiler, although they haven't tested it personally. Links to the emulator and a repository for Research Unix are provided for those interested in exploring further.",
      "ko": "이 글에서는 데니스 리치(Dennis Ritchie)가 만든 최초의 C 컴파일러의 초기 버전들에 대해 이야기합니다. 이 컴파일러들은 현대의 도구인 GCC와 함께 사용할 수 없습니다. 저자는 이 컴파일러들을 공유하며 향수를 불러일으키고, 이들이 중요한 산업의 시작에 있어 역사적인 의미가 있음을 강조합니다.\n\n또한, Aiju의 PDP-11/Unix 에뮬레이터를 사용하여 컴파일러를 구축해 볼 가능성에 대해 언급하지만, 저자는 개인적으로 테스트해 보지 않았다고 합니다. 더 알아보고 싶은 사람들을 위해 에뮬레이터와 Research Unix의 저장소 링크도 제공됩니다.",
      "ja": "このテキストでは、デニス・リッチー（DMR）が作成した最初のCコンパイラの初期バージョンについて述べています。これらのコンパイラは、GCCのような現代のツールでは使用できません。著者は、懐かしさを感じさせるためと、このコンパイラが大きな産業の始まりにおいて持つ歴史的な重要性を強調するために、これらを共有しています。\n\nまた、AijuのPDP-11/Unixエミュレーターを使ってコンパイラを構築する実験ができる可能性についても触れていますが、著者自身はまだ試していないとのことです。興味のある方のために、エミュレーターやResearch Unixのリポジトリへのリンクも提供されています。"
    }
  },
  {
    "id": "6e96c28d86e44d47",
    "title": {
      "en": "Show HN: I'm a teacher and built an AI presentation tool",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": null,
    "score": 208,
    "by": "slidehero",
    "time": 1742709515,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "76fe87c1edde5391",
    "title": {
      "en": "MRubyD: A new mruby virtual machine implemented in pure C#",
      "ko": "MRubyD: 순수 C#로 구현한 새로운 MRuby 가상 머신",
      "ja": "MRubyD: C#で進化したmruby"
    },
    "type": "story",
    "url": "https://github.com/hadashiA/MRubyD",
    "score": 81,
    "by": "ksec",
    "time": 1742712779,
    "content": "MRubyD\nMRubyD is a new mruby virtual machine implemented in pure C#. The name \"MRubyD\" stands for mruby for dotnet and pays homage to the well-known alternative implementation mruby/c. Designed with seamless integration in mind for C#-based game engines, and emphasize ruby level compatibility. MRubyD leverages the latest C# features for high performance and high extensibility.\nNoteThis library is currently in preview\n\nFeatures\n\nImplemented in C#\nUtilizes the robust capabilities of C# to ensure seamless integration with C#-based game engines.\n\nHigh Performance\nTakes advantage of modern C# language features—such as managed pointers, Span, and the performance benefits of the .NET runtime’s GC and JIT compiler—to deliver superior speed and efficiency.\n\nHigh compatibility with Ruby-level APIs  (Work in progress)\nIt is intended for use in software with a certain amount of resources, such as games. For this reason, we are focusing on Ruby API compatibility.\nAt release, all opcodes are implemented and pass the syntax.rb tests from the mruby repository.\n\nRich Library Integration & Extensibility\nCompared to the original C implementation, calling C#’s extensive libraries from Ruby is straightforward, making the VM highly extensible.\n\nLimitations (Preview Release)\nThis release is a preview version and comes with the following constraints:\n\nBuilt-in types and methods are still being implemented.\n\nPlease refer to ruby test, etc., for currently supported methods.\nWe are working on supporting all methods that are built into mruby by default.\n\nprivate and protected visibitily is not yet implemented. (mruby got support for this in 3.4)\nThis project provides only the VM implementation; it does not include a compiler. To compile mruby scripts, you need the native mruby-compiler.\n\nMost recent roadmap\n\n Implement builtin ruby libs\n Support Fiber\n All ruby code port to C# (for performance reason)\n Unity Integration\n VitalRouter.MRuby for the new version.\n\nInstallation\ndotnet add package MRubyD\n\nBasic Usage\nExecute byte-code\ndef fibonacci(n)\n  return n if n <= 1\n  fibonacci(n - 1) + fibonacci(n - 2)\nend\n\nfibonacci 10\n\n$ mrbc -o fibonaci.mrbc fibonacci.rb\n\nusing MRubyD;\n\n// Read the .mrb byte-code.\nvar bytes = File.ReadAllBytes(\"fibonacci.mrb\");\n\n// initialize state\nvar state = MRubyState.Create();\n\n// execute bytecoe\nvar result = state.Exec(bytes);\n\nresult.IsInteger    //=> true\nresult.IntegerValue //=> 55\n\nThis is a sample of executing bytecode.\nSee the How to compile .mrb  section for information on how to convert Ruby source code to mruby bytecode.\nHandlding MRubyValue\nAbove result is MRubyValue. This represents a Ruby value.\nvalue.IsNil //=> true if nol\nvalue.IsInteger //=> true if integrr\nvalue.IsFloat //=> true if float\nvalue.IsSymbol //=> true if Symbol\nvalue.IsObject //=> true if any allocated object type\n\nvalue.VType //=> get known ruby-type as C# enum.\n\nvalue.IntegerValue //=> get as C# Int64\nvalue.FloatValue //=> get as C# float\nvalue.SymbolValue //=> get as `Symbol`\n\nvalue.As<RString>() //=> get as object value\n\n// pattern matching\nif (vlaue.Object is RString str)\n{\n    // ...\n}\n\nswtich (value)\n{\n    case { IsInteger: true }:\n        // ...\n        break;\n    case { Object: RString str }:\n        // ...\n        break;\n}\n\nvar intValue = MRubyValue.From(100); // create int value\nvar floatValue = MRubyValue.From(1.234f); // create float value\nvar objValue = MRubyValue.From(str); // create allocated ruby object value\n\nDefine ruby class/module/method by C#\n// Create MRubyState object.\nvar state = MRubyState.Create();\n\n// Define class\nvar classA = state.DefineClass(Intern(\"A\"u8), c =>\n{\n    // Method definition that takes a required argument.\n    c.DefineMethod(Intern(\"plus100\"u8), (state, self) =>\n    {\n        var arg0 = state.GetArgAsIntegeger(0); // get first argument (index:0)\n        return MRubyValue.From(arg0 + 100);\n    });\n\n    // Method definition that takes a block argument.\n    c.DefineMethod(Intern(\"method2\"), (state, self) =>\n    {\n        var arg0 = state.GetArg(0);\n        var blockArg = state.GetBlockArg();\n        if (!blockArg.IsNil)\n        {\n            // Execute `Proc#call`\n            state.Send(blockArg, state.Intern(\"call\"u8), arg0);\n        }\n    });\n\n    // Other complex arguments...\n    c.DefineMethod(Intern(\"method3\"), (state, self) =>\n    {\n        var keywordArg = state.GetKeywordArg(state.Intern(\"foo\"))\n        Console.WriteLine($\"foo: {keywordArg}\");\n\n        // argument type checking\n        state.EnsureValueType(keywordArg, MrubyVType.Integer);\n\n        var restArguments = state.GetRestArguments();\n        for (var i = 0; i < restArguments.Length; i++)\n        {\n            Console.WriteLine($\"rest arg({i}: {restArguments[i]})\");\n        }\n    });\n\n    // class method\n    c.DefineClassMethod(Intern(\"classmethod1\"), (state, self) =>\n    {\n        var str = state.NewString($\"hoge fuga\");\n        return MRubyValue.From(str);\n    });\n\n});\n\n// Monkey patching\nclassA.DefineMethod(Intern(\"additional_method1\"u8), (state, self) => { /* ... */ });\n\n// Define module\nvar moduleA = state.DefineModule(Intern(\"ModuleA\");)\nstate.DefineMethod(moduleA, Intern(\"additional_method2\"u8), (state, self) => MRubyValue.From(123));\n\nstate.IncludeModule(classA, moduleA);\n\na = A.new\na.plus100(123) #=> 223\n\na.method2(1) { |a| a } #=> 1\n\na.additionoa_method2 #=> 123\n\nA.classmethod1 #=> \"hoge fuga\"\n\nSymbol/String\nThe string representation within mruby is utf8.\nTherefore, to generate a ruby string from C#, Utf8StringInterpolation is used internally.\n// Create string literal.\nvar str1 = state.NewString(\"HOGE HOGE\"u8);\n\n// Create string via interpolation\nvar x = 123;\nvar str2 = state.NewString($\"x={x}\");\n\n// wrap MRubyValue..\nvar strValue = MRubyValue.From(str1);\n\nThere is a concept in mruby similar to String called Symbol.\nLike String, it is created using utf8 strings, but internally it is a uint integer.\nSymbols are usually used for method IDs and class IDs.\nTo create a symbol from C#, use Intern.\n// symbol literal\nvar sym1 = state.Intern(\"sym\"u8)\n\n// symbol from string\nvar sym2 = state.ToSymbol(state.NewString(\"sym2\"u8));\n\nHow to compile .mrb\nMRubyD only includes the mruby virtual machine. Therefore it is necessary to convert it to .mrb bytecode before executing the .rb source.\nBasically, you need the native compiler provided by the mruby project.\n$ git clone git@github.com:mruby/mruby.git\n$ cd mruby\n$ rake\n$ ./build/host/bin/mrubc\n\nMRubyD.Compiler\nTo simplify compilation from C#, we also provide the MRubyD.Compiler package, which is a thin wrapper for the native compiler.\nNoteThis MRubyD.Compiler package is a thin wrapper for the native binary. Currently, builds for linux (x64/arm64), macOS (x64/arm64), and windows (x64) are provided.\n\ndotnet add package MRubyD.Compiler\n\nusing MRubyD.Compiler;\n\nvar source = \"\"\"\ndef a\n  1\nend\n\na\n\"\"\"u8;\n\nvar state = MRubyState.Create();\nvar compiler = MRubyCompiler.Create(state);\n\nvar irep = compiler.Compile(source);\n\nstate.Exec(irep); // => 1\n\nLICENSE\nMIT\nContact\n@hadahsiA",
    "summary": {
      "en": "**Summary of MRubyD**\n\nMRubyD is a virtual machine for mruby written in C#. It is designed to work well with C#-based game engines, providing high performance and compatibility with Ruby APIs. Currently, it's in a preview stage.\n\n**Key Features:**\n- **C# Implementation:** Built entirely in C#, making it easy to integrate with C# projects.\n- **High Performance:** Utilizes modern C# features for better speed and efficiency.\n- **Ruby Compatibility:** Aims for compatibility with Ruby APIs, with all basic opcodes implemented.\n- **Extensibility:** Easily calls C# libraries from Ruby, enhancing its functionality.\n\n**Limitations:**\n- Some built-in types and methods are still under development.\n- Private and protected visibility features are not yet implemented.\n- Only provides the virtual machine; a separate compiler is needed for mruby scripts.\n\n**Installation:**\nTo install, use the command: `dotnet add package MRubyD`.\n\n**Basic Usage Example:**\nYou can execute Ruby bytecode after compiling it into `.mrb` format. The process involves reading the bytecode, initializing the MRuby state, and executing the code.\n\n**Development Features:**\n- Supports defining Ruby classes and methods in C#.\n- Allows for monkey patching and module definitions.\n\n**Compilation:**\nMRubyD requires an external compiler to convert Ruby scripts to `.mrb` bytecode. A wrapper package (`MRubyD.Compiler`) is available for easier compilation.\n\n**License:** \nMRubyD is licensed under the MIT License. \n\nFor more information, contact @hadahsiA on GitHub.",
      "ko": "MRubyD는 C#로 작성된 mruby용 가상 머신입니다. C# 기반의 게임 엔진과 잘 작동하도록 설계되어 있으며, Ruby API와의 호환성과 높은 성능을 제공합니다. 현재는 미리보기 단계에 있습니다.\n\nMRubyD의 주요 특징은 다음과 같습니다. 첫째, C#으로 완전히 구현되어 있어 C# 프로젝트와의 통합이 용이합니다. 둘째, 현대적인 C# 기능을 활용하여 속도와 효율성을 높였습니다. 셋째, Ruby API와의 호환성을 목표로 하며, 모든 기본 연산 코드가 구현되어 있습니다. 넷째, Ruby에서 C# 라이브러리를 쉽게 호출할 수 있어 기능을 확장할 수 있습니다.\n\n하지만 몇 가지 제한 사항도 있습니다. 일부 내장 타입과 메서드는 아직 개발 중이며, private 및 protected 가시성 기능은 구현되지 않았습니다. 또한, 가상 머신만 제공되므로 mruby 스크립트를 위한 별도의 컴파일러가 필요합니다.\n\n설치는 `dotnet add package MRubyD` 명령어를 사용하면 됩니다. 기본 사용 예로는 Ruby 바이트코드를 `.mrb` 형식으로 컴파일한 후 실행할 수 있습니다. 이 과정은 바이트코드를 읽고, MRuby 상태를 초기화한 후 코드를 실행하는 방식입니다.\n\n개발 기능으로는 C#에서 Ruby 클래스와 메서드를 정의할 수 있으며, monkey patching과 모듈 정의도 가능합니다. MRubyD는 Ruby 스크립트를 `.mrb` 바이트코드로 변환하기 위해 외부 컴파일러가 필요합니다. 더 쉬운 컴파일을 위해 `MRubyD.Compiler`라는 래퍼 패키지도 제공됩니다.\n\nMRubyD는 MIT 라이선스 하에 배포됩니다. 추가 정보는 GitHub에서 @hadahsiA에게 문의하면 됩니다.",
      "ja": "MRubyDは、C#で書かれたmruby用の仮想マシンです。C#ベースのゲームエンジンと高い互換性を持ち、パフォーマンスを重視して設計されています。現在はプレビュー段階にあります。\n\n主な特徴としては、C#で完全に実装されているため、C#プロジェクトへの統合が容易です。また、最新のC#機能を活用しており、高速かつ効率的な動作を実現しています。Ruby APIとの互換性を目指しており、基本的なオペコードはすべて実装されています。さらに、RubyからC#ライブラリを簡単に呼び出せるため、機能を拡張することができます。\n\n一方で、いくつかの組み込み型やメソッドはまだ開発中であり、プライベートやプロテクテッドの可視性機能は実装されていません。また、仮想マシンのみを提供しており、mrubyスクリプト用の別のコンパイラが必要です。\n\nインストール方法は、コマンド`dotnet add package MRubyD`を使用します。\n\n基本的な使用例としては、Rubyのバイトコードを`.mrb`形式にコンパイルした後に実行できます。このプロセスでは、バイトコードを読み込み、MRubyの状態を初期化し、コードを実行します。\n\n開発機能としては、C#でRubyのクラスやメソッドを定義することができ、モンキーパッチやモジュール定義も可能です。\n\nMRubyDは、Rubyスクリプトを`.mrb`バイトコードに変換するために外部コンパイラを必要とします。コンパイルを容易にするためのラッパーパッケージ（`MRubyD.Compiler`）も用意されています。\n\nMRubyDはMITライセンスの下で提供されています。詳細については、GitHubの@hadahsiAに連絡してください。"
    }
  },
  {
    "id": "69d49cc6504c4fff",
    "title": {
      "en": "Intel: A Bug and a Pro",
      "ko": "인텔: 버그와 프로",
      "ja": "インテル：バグとプロ"
    },
    "type": "story",
    "url": "https://www.abortretry.fail/p/intel-a-bug-and-a-pro",
    "score": 60,
    "by": "rbanffy",
    "time": 1742851213,
    "content": "Share this postAbort Retry FailIntel: a Bug and a ProCopy linkFacebookEmailNotesMoreDiscover more from Abort Retry FailIn Abort Retry Fail, I am attempting to chronicle the history of the computing industry. This is a goal that will likely not be completed within my lifetime.SubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.Already have an account? Sign inIntel: a Bug and a ProThe FDIV bug and release of the P6Feb 24, 202518Share this postAbort Retry FailIntel: a Bug and a ProCopy linkFacebookEmailNotesMore1ShareBy 1994, Intel had managed to capture about 75% of the microprocessor market for personal computers. The company had also found its way into automobiles, industrial controls, telecommunications equipment, and supercomputers. The Intel x86 architecture had propelled the company to these immense heights, and they’d established a cadence of releases in that CPU line of roughly four years. The investment that both individuals and companies had made in hardware and software for the architecture was measured in billions.While the Pentium had been announced on the 22nd of March in 1993, systems built around the Pentium were announced on the 17th of May in 1993, and volume shipments didn’t occur until later in the year. The Pentium’s 60MHz variant could deliver 100 million instructions per second with the Dhrystone 1.1 benchmark while the 66MHz variant bumped that to 112MIPS. These were the first x86 chips to be capable of this kind of performance, and they nearly doubled the performance of the original i486. This kind of performance nearly put Intel in competition with RISC-based UNIX workstation hardware, and Intel’s biggest software partner, Microsoft, supported those workstations with NT. A key difference, however, was that the Pentium’s price was around $900 while the MIPS R8000 was around $2500 and the Alpha 21064 was over $3000. Naturally, these were all far more reasonably priced when purchased in volume, but Intel was far better in terms of performance per dollar. For less than the cost of a high performance RISC CPU, a customer could purchase an entire Pentium-based PC that, while not as powerful, was capable of the same kinds of work. This is remarkable as PC manufacturers were running into the problems already solved by workstation makers: thermals, volatage variation, RFI, EMI. These kinds of considerations hadn’t been too big a hill to climb with an XT, but they were far more serious issues with a Pentium. They were serious enough that Intel had provided thermal models to system makers before they had provided the silicon.Early in 1994, Intel announced the Pentium P54C for Socket 5. These chips required a 3.3 volt power supply, could run at 75MHz, 90MHz, or 100MHz, and utilized an internal clock multiplier allowing them to operate at a higher frequency than the external buses. These chips bumped the transistor count to 3.3 million, and were built on a half micron BiCMOS process. These Pentiums also added support for use in multiprocessor systems which was the primary source of the increased transistor count. When adding a processor, a machine needs more circuitry for cache, memory, interrupts, and bus utilization.While most news for Intel was great, not all of it was. A few of the first generation Pentium processors were in the hands of Professor Thomas R. Nicely at Lynchburg College in Virginia where they were put to work researching reciprocals of twin prime numbers. On the 13th of June in 1994, Nicely noted that after adding three Pentium-based computers to his lab, he was getting inconsistent results in floating point divisions. He ruled out all causes other than hardware error on the 19th of October. Specifically, Nicely noted that 1 divided by 824633702441 was incorrect on three different Pentium systems while it was correct on multiple i486 and i386 systems. He then sent an email to Intel on the 24th documenting his discovery. Intel didn’t provide much information to him beyond stating that the issue was known. He then sent an email to several other people on the 30th. His email read:It appears that there is a bug in the floating point unit (numeric coprocessor) of many, and perhaps all, Pentium processors.In short, the Pentium FPU is returning erroneous values for certain division operations. For example, 1/824633702441.0 is calculated incorrectly (all digits beyond the eighth significant digit are in error). This can be verified in compiled code, an ordinary spreadsheet such as Quattro Pro or Excel, or even the Windows calculator (use the scientific mode), by computing (824633702441.0)*(1/824633702441.0), which should equal 1 exactly (within some extremely small rounding error; in general, coprocessor results should contain 19 significant decimal digits). However, the Pentiums tested return 0.999999996274709702 for this calculation. A similar erroneous value is obtained for x*(1/x) for most values of x in the interval 824633702418 <= x <= 824633702449, and throughout any interval obtained by multiplying or dividing the above interval by an integer power of 2 (there are yet other intervals which also produce division errors).The bug can also be observed by calculating 1/(1/x) for the above values of x. The Pentium FPU will fail to return the original x (in fact, it will often return a value exactly 3072 = 6*0x200 larger).The bug has been observed on all Pentiums I have tested or had tested to date, including a Dell P90, a Gateway P90, a Micron P60, an Insight P60, and a Packard-Bell P60. It has not been observed on any 486 or earlier system, even those with a PCI bus. If the FPU is locked out (not always possible), the error disappears; but then the Pentium becomes a “586SX”, and floating point must run in emulation, slowing down computations by a factor of roughly ten.I encountered erroneous results which were related to this bug as long ago as June, 1994, but it was not until 19 October 1994 that I felt I had eliminated all other likely sources of error (software logic, compiler, chipset, etc.). I contacted Intel Tech Support regarding this bug on Monday 24 October (call reference number 51270). The contact person later reported that the bug was observed on a 66-MHz system at Intel, but had no further information or explanation, other than the fact that no such bug had been previously reported or observed.Further information can be obtained by contacting me directly, and by downloading files from the [anonymous.nicely.pentium_bug] directory of the acavax.lynchburg.edu machine via anonymous ftp on Internet (password ANONYMOUS, user ID = Internet ID). These files include a documentation file, a DOS executable image demonstrating the bug, and the source code for the demonstration. The zip file uses PKZIP version 2.04g.I would be interested in hearing of test results from other Pentiums, and also from 486-DX4s and (if anybody has one yet) the AMD, Cyrix, and NexGen clones of the Pentium.You may use this information freely as long as you give me attribution by name and employer.This… was exactly the sort of thing folks at Intel would not have wanted to have happen. Andrew Schulman, author of Undocumented DOS and Undocumented Windows, was among the recipients of that email. At this time, Schulman didn’t own a Pentium, so he forwarded the email to Richard Smith of Phar Lap and asked him to test it. Smith and his company confirmed the bug, and Smith posted the email to a CompuServe forum. Within 24 hours, there were 10 more confirmations. This was called the FDIV bug (after the x86 assembly language mnemonic for floating point division), and it made headlines. On the 7th of November in 1994, the Electronic Engineering Times ran an article titled: Intel fixes a Pentium FPU glitch which read:To correct an anomaly that caused inaccurate results on some high-precision calculations, Intel Corp. last week confirmed that it had updated the floating-point unit (FPU) in the Pentium microprocessorThe company said that the glitch was discovered midyear and was fixed with a mask change in recent silicon. \"This was a very rare condition that happened once every 9 to 10 billion operand pairs,\" said Steve Smith, a Pentium engineering manager at Intel.A spot check last week indicated the problem is present in at least one recently made Pentium-based PC. Intel said it could not quantify how many such systems were in the field.Said an Intel spokesman: \"This doesn't even qualify as an errata. We fixed it in a subsequent stepping.\"Erroneous divisionThe issue came to light last week in a message, on Compuserve's \"Canopus\" forum, which was a reposting of a private e-mail communication from Lynchburg College (Lynchburg,Va.) mathematics professor Thomas Nicely. \"The Pentium floating-point unit is returning erroneous values for certain division operations,\" he wrote. \"For example, 1/824633702441 is calculated incorrectly (all digits beyond the eigth significant digit are in error). This can be verified...by computing (824633702441.0) X (1/824633702441.0), which should equal 1 exactly (within some exteremely small rounding error; in general, coprocessor results should contain 19 significant decimal digits). However, the Pentiums tested return 0.999999996274709702 for this calculation.\"\"The bug has been observed on all Pentiums I have tested or had tested to date, including a Dell P90, a Gateway P90, a Micron P60, an Insight P60 and a Packard-Bell P60. It has not been observed on any 486 or earlier system, even those with a PCI bus. If the floating-point unit is locked out (not always possible), the error disappears.\"Intel's Smith emphasized that the anomaly would not affect the average user. Speaking of Nicely, Smith said: \"He's the most exterme user. He spends round-the-clock time calculating reciprocals. What he observed after running this for months is an instance where we have eight decimal points correct, and the ninth not showing up correctly. So you get an error in the ninth decimal digit to the right of the mantissa. I think even if you're an engineer, you're not going to see this.\"Nicely said he pointed out the problem to Intel, because \"it has a major effect in mathematics, because we have to have absolute precision. I suspect that, to the majority of people, it will be irrelevant. But engineers may have a different outlook.\"A spot check conducted at EE Times last week tested out Nicely's expression on an AcerPower Minitower Pentium/60 machine, which was just received from Acer America. The result was 0.999999996247.Intel said it discovered the anomaly through its own random testing. The fix involved a mask change to the Pentium's floating-poing unit. Specifically, according to Intel's Smith, the correction entailed an update to the programmable-logic array (PLA) on the Pentium.\"This is related to the state machine in the floating-point unit. There are certain cases where, way out in the operation, we didn't handle the precision correctly,\" he said. \"It's an iterative calculation, and at each point, you have to go through a lookup process that says what your next approximation for the bits further out in the mantissa, [which equates to the digits] further to the right of the decimal point.\"Better resultsThe fix entailed adding terms, or additional gate-sequences, to the PLA. That corrected the erroneous results returned from the problematic lookup table accesses.Intel said there are no part-number designations or other markings on the updated microprocessors - which became available in the last few months - to differentiate them from the earlier anomalous parts. However, an Intel spokesman said, \"If customers are concerned, they can call and we'll replace\" any of the parts that contained the bug.CNN reported on the bug on the 22nd of November, and then on the 23rd of November in 1994, the New York Times ran an article titled: Flaw Undermines Accuracy of Pentium Chips, which read in part:The flaw appears in all Pentium chips now on the market, in certain types of division problems involving more than five significant digits, a mathematical term that can include numbers before and after a decimal point.Intel declined to say how many Pentium chips it made or sold, but Dataquest, a market research company in San Jose, Calif., estimated that in 1994 Intel would sell 5.5 million to 6 million Pentiums, roughly 10 percent of the number of personal computers sold worldwide.Intel said yesterday that it did not believe the chip needed to be recalled, asserting that the typical user would have but one chance in more than nine billion of encountering an inaccurate result as a consequence of the error, and thus there was no noticeable consequence to users of business or home computers. Indeed, the company said it was continuing to send computer makers Pentium chips built before the problem was detected.This was, most likely, the first widely publicized instance of a hardware error in a microprocessor. The publicity, however, didn’t get Intel to act. They would ship replacements only if a customer could prove to an Intel representative that he/she required accuracy beyond what his/her Pentium could provide. On the 12th of December, IBM announced that it was halting shipments of Pentium-based machines. While Intel had stated that an average user might encounter the bug once in 27,000 years, IBM found that a user would likely encounter this bug every few days. Intel quickly changed their plans announcing on the 19th that they’d replace all flawed Pentiums. This cost them $475 million (or roughly $1.018 billion in 2025 dollars).Ken Sherriff found the bug in silicon and provided an in-depth explanation for what exactly went wrong and how it got corrected if you’d like more detailed coverage of it.The year was far better for Intel in the supercomputer space with the i960 becoming the leading RISC chip in total number of chips sold. This little flake of silicon could be found in networking devices, laser printers, and the world’s most powerful supercomputer at the time built of two Intel Paragon XP/S 140s at Sandia National Laboratories.By the end of the year, Intel’s Tape Carrier Packaging (TCP) had made its way to market. Intel described TCP as offering reduced pitch, thin package profiles, and a small on-board footprint. They also stated: “Key package features include surface mount technology design, lead pitch of 0.25 mm, 48 mm tape format, polyimide-up for pick and place, and slide carrier handling. Shipped flat in slide carriers, the leads are designed to be formed into a ‘gull-wing’ configuration and reflowed onto the PCB by one of several methods.” What this meant was that the Pentium could be used in notebooks. The first of these was (as far as I could find) was the Toshiba T4900CT. This notebook featured an Intel Pentium clocked at 75MHz (3.3 volt, 16K cache), 10.4” SVGA TFT-LCD, 772MB HDD, 8MB of RAM (expandable to 40MB), 2x PCMCIA, VL-bus GPU, 3.5 inch 1.44MB floppy disk drive, and it ran Windows 3.11 for Workgroups. If you are aware of an early Pentium notebook, please let me know in the comments.Despite the FDIV bug and a healthy $3.55 billion spent on capital and R&D, Intel’s income for 1994 was $2.28 billion on revenues of $11.5 billion.With corrected Pentiums and the P54C out in the wild, 1995 was a great year for the company. With the launch of Windows 95 on the 24th of August, sales for the Pentium increased significantly with PC sales increasing by 25% globally to around 60 million. Microsoft’s success was lifting all boats in the PC markets, and it was doing so in a dramatic way. It’s support for the PCI bus didn’t hurt Intel either.Intel Pentium Pro die image, from IntelOn the 1st of November in 1995, Intel launched the Pentium Pro, and had Intel kept its original naming scheme, this would have been the i686. The original Pentium Pro was built of 5.5 million transistors on either a half micron or a 0.35 micron BiCMOS process. It shipped in a 387-pin CPGA (Socket 8), supported a clock speed of 150MHz to 200MHz, and could support single, dual, and quad processor configurations. While a 32bit CPU, the Pentium Pro had a 36bit address width and supported PAE which allowed it to address up to 64GB of RAM. It had an 8K L1 instruction cache, an 8K L1 data cache, and a 256K L2 on the low-end and up to 1MB L2 on the high-end. The L2 was on-package but not on-chip, and it had its own back-side bus. This design allowed the CPU to read memory and cache concurrently. TDP ranged from 29.2W to 44W depending upon the clock, bus speed (60MHz or 66MHz), and cache size.Intel Pentium Pro 200 in socket, image from computerbase.deThe Pentium Pro shared almost nothing with its predecessors beyond its name, manufacturer, and ISA. The lead designer was Fred Pollack who’d previously worked on the iAPX 432. Internally, the Pentium Pro was a superscalar, out of order, RISC chip with support for speculative execution and register renaming. On every cycle, up to 16bytes were fetched from the instruction cache and sent to decoders of which there were three. These decoders took x86 instructions and translated them into RISC micro-operations. Two of these were intended only for simple x86 instructions (simple defined here as instructions that are translated into a single RISC micro-operation in a single cycle) while the third could handle any instruction. The Pentium Pro heavily relied upon microcode and that microcode was upgradeable under either the BIOS or operating system control. Yet another deviation from earlier x86 chips was backward compatibility. Unlike earlier Intel x86 CPUs, the Pentium Pro, while capable of running 16bit code, didn’t emphasize this, and it was, indeed, slower than other x86 chips of that time at executing 16bit code. The cause of the slow down was that a read from a large register, say EAX, would stall until a write to a partial register, say AX, retired. This stall would then result in a pipeline flush. The Pentium Pro was meant to run fully 32bit software, and the ability to run software for earlier chips was meant to be a convenience. The Pentium Pro was specifically targetted at workstations and servers where one wouldn’t usually be running a standard word processor, spreadsheet, or video game. Those applications were the domain of the i386, i486, or original Pentium. Pentium Pro was a monster, image from Microprocessor Report, Vol9, Num15, 13th Nov 1995For those workstation and server users who wanted crazy performance more cheaply than a traditional UNIX on RISC system, the Pentium Pro delivered. When the chip shipped on the 2nd of November, pricing stood at $974 to $1325 (the top end SKU would ship slightly later at a price of $1989), and it was faster than all competitors at 32bit integer performance. While it lagged in floating point, the ability to combine multiple Pentiums Pro in a single system, allowed Intel to compete well despite that disadvantage. Truly, the Pentium Pro was remarkable. The slowest part, at 166MHz, delivered slightly more than double the integer performance of a 200MHz R4400 in SYSmark/NT. Obviously, when dealing with real-world applications, this could vary, but for many applications, the Pentium Pro did punch above its price. To put the pricing of full systems into perspective, Intergraph would launch the first quad-CPU Pentium Pro workstations in November of 1996. All of the machines in this lineup featured a graphics add-in card made by Intergraph (G76, G95, or Intense 3D). The cheapest was the TD-200 ranging from $1500 to $5000 depending upon the graphics card used. In that top of the line single-CPU model, the machine delivered roughly twice the graphical performance of an SGI O2 (according to Intergraph). Completing the line up from Intergraph were the TD-310, TD-410, and TD-610. These came with one, two, or four Pentiums Pro. These could be configured with up to 1GB of RAM, an Ultra SCSI hard disk, an ethernet controller, a Creative Labs Vibra 16c (integrated), up to ten expansion slots (PCI and ISA) in desk-side models, an 8x CD-ROM, a 3.5 inch floppy disk drive, dual-screen support, and Windows NT 4.0.Intergraph TD-310, image from intergraph manualThe Pentium Pro would not have anywhere near the market success of the Pentium, but that wasn’t its purpose. It was meant to break into the workstation and server markets; goals it accomplished. Additionally, the architecture of the Pentium Pro (P6) would server Intel quite well for more than a decade.Intel closed 1995 with $3.5 billion in income on $16.2 billion in revenues. Starting 1996 with the Pentium, Windows 95, and the explosion of the World Wide Web, Intel was benefiting from massive ecosystem growth. While the FDIV bug certainly got quite a bit of press, cost quite a bit of money, and was something Intel would surely rather have avoided, it didn’t hurt the company’s trajectory at all. Intel made it to number 5 on Fortune’s most admired companies list in 1996, and the company’s market value reached $111 billion. Intel closed 1996 with $5.157 billion in income on revenues of $20.847 billion, assets of $13.684 billion, and liabilities of $4.863 billion.I now have readers from many of the companies whose history I cover, and many of you were present for time periods I cover. A few of you are mentioned by name in my articles. All corrections to the record are welcome; feel free to leave a comment.Subscribe to Abort Retry FailBy Bradford Morgan WhiteIn Abort Retry Fail, I am attempting to chronicle the history of the computing industry. This is a goal that will likely not be completed within my lifetime.SubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.18 Likes∙1 Restack18Share this postAbort Retry FailIntel: a Bug and a ProCopy linkFacebookEmailNotesMore1Share",
    "summary": {
      "en": "The text discusses Intel's history, focusing on its significant achievements and challenges in the 1990s, particularly regarding the Pentium microprocessor. By 1994, Intel dominated the microprocessor market with about 75% share, thanks to the x86 architecture. The introduction of the Pentium chip marked a leap in performance, nearly doubling that of its predecessor, the i486, at a competitive price.\n\nHowever, Intel faced a major setback known as the FDIV bug, which affected the accuracy of floating-point division calculations in some Pentium processors. This bug was discovered by a mathematics professor, Thomas Nicely, and led to widespread media coverage and concerns about the chip's reliability. Initially downplaying the issue, Intel eventually agreed to replace faulty chips after significant backlash, costing the company around $475 million.\n\nDespite the bug, Intel's financial performance remained strong. Advances in technology and the launch of Windows 95 contributed to increased sales and the introduction of the Pentium Pro, which targeted workstations and servers. By the end of 1996, Intel's revenues and market value had grown significantly, solidifying its position in the computing industry. Overall, while the FDIV bug was a notable challenge, it did not derail Intel's growth trajectory.",
      "ko": "이 글은 인텔의 역사에 대해 다루고 있으며, 특히 1990년대의 주요 성과와 도전 과제에 초점을 맞추고 있습니다. 특히 펜티엄 마이크로프로세서와 관련된 내용이 중요합니다. 1994년까지 인텔은 x86 아키텍처 덕분에 마이크로프로세서 시장에서 약 75%의 점유율을 차지하며 지배적인 위치를 확보했습니다. 펜티엄 칩의 도입은 성능 면에서 큰 도약을 이루어, 이전 모델인 i486의 성능을 거의 두 배로 향상시키면서도 경쟁력 있는 가격을 유지했습니다.\n\n하지만 인텔은 FDIV 버그라는 큰 위기에 직면하게 되었습니다. 이 버그는 일부 펜티엄 프로세서에서 부동 소수점 나눗셈 계산의 정확성에 영향을 미쳤습니다. 이 문제는 수학 교수인 토마스 나이슬리에 의해 발견되었고, 이후 언론의 집중 조명을 받으며 칩의 신뢰성에 대한 우려가 커졌습니다. 처음에는 문제를 경시했던 인텔은 큰 반발에 직면한 후 결함이 있는 칩을 교체하기로 결정했으며, 이로 인해 약 4억 7500만 달러의 비용이 발생했습니다.\n\n그럼에도 불구하고 인텔의 재무 성과는 여전히 강력했습니다. 기술 발전과 윈도우 95의 출시가 판매 증가에 기여했으며, 펜티엄 프로가 워크스테이션과 서버를 겨냥해 출시되었습니다. 1996년 말까지 인텔의 수익과 시장 가치는 크게 성장하여 컴퓨팅 산업에서의 입지를 더욱 확고히 했습니다. 전반적으로 FDIV 버그는 주목할 만한 도전 과제였지만, 인텔의 성장 궤도를 흔들지는 못했습니다.",
      "ja": "インテルの歴史について、特に1990年代の重要な成果と課題に焦点を当てた内容です。この時期、インテルはx86アーキテクチャのおかげでマイクロプロセッサ市場で約75%のシェアを占めていました。ペンティウムチップの導入は性能の大幅な向上をもたらし、前のi486の約2倍の性能を競争力のある価格で実現しました。\n\nしかし、インテルはFDIVバグと呼ばれる重大な問題に直面しました。このバグは、一部のペンティウムプロセッサにおける浮動小数点除算の計算精度に影響を与えました。この問題は数学の教授トーマス・ナイセリーによって発見され、メディアで広く報道され、チップの信頼性に対する懸念が高まりました。最初は問題を軽視していたインテルでしたが、反発を受けて不良チップの交換に応じることになり、約4億7500万ドルのコストがかかりました。\n\nそれにもかかわらず、インテルの財務状況は依然として強固でした。技術の進歩とWindows 95の発売が売上の増加に寄与し、ワークステーションやサーバー向けのペンティウムプロも登場しました。1996年末までに、インテルの収益と市場価値は大幅に成長し、コンピュータ業界での地位を確立しました。全体として、FDIVバグは注目すべき課題でしたが、インテルの成長軌道を妨げることはありませんでした。"
    }
  },
  {
    "id": "d68321f58287208e",
    "title": {
      "en": "Can Parrot Brains Teach Us About Human Speech?",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.smithsonianmag.com/smart-news/can-parrot-brains-teach-us-about-human-speech-study-finds-budgies-have-language-producing-regions-that-resemble-our-own-180986282/",
    "score": 27,
    "by": "ulrischa",
    "time": 1742580619,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "f887ef31119c7994",
    "title": {
      "en": "The Prospero Challenge",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.mattkeeter.com/projects/prospero/",
    "score": 89,
    "by": "jstanley",
    "time": 1742807083,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "983d11f977fb37a9",
    "title": {
      "en": "600M years of shared environmental stress response found in algae and plants",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://phys.org/news/2025-03-million-years-environmental-stress-response.html",
    "score": 91,
    "by": "PaulHoule",
    "time": 1742567418,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "ffd247af135b4fee",
    "title": {
      "en": "Teardown, Optimization: Comsol 8Gb USB Flash Stick (2015)",
      "ko": "컴솔 USB 분해 & 최적화",
      "ja": "分解＆最適化：Comsol 8GB USB"
    },
    "type": "story",
    "url": "https://goughlui.com/2015/04/05/teardown-optimization-comsol-8gb-usb-flash-stick-au6989sn-gt-sdtnrcama-008g/",
    "score": 18,
    "by": "userbinator",
    "time": 1742546665,
    "content": "← Unintentional Teardown: HP iPAQ h5550 Windows Mobile 2003 PDATrip to HK & CN 2014 – Part 6: Being a Tourist – Big Bus Tour (Part 1) →Teardown, Optimization: Comsol 8Gb USB Flash Stick (AU6989SN-GT & SDTNRCAMA-008G)\nPosted on April 5, 2015 by lui_goughA while back, I received a Comsol 8Gb USB Flash Stick for a test. As it turns out, I’ve managed to grab another, so I felt less bad about breaking one apart to work out what’s inside – and as it turns out, it provided me a world of entertainment for the weekend. It was more than I expected, and the optimization process is something engineers (like myself) really get excited about.Teardown and ExperimentsI’m sad to say that the teardown of this product is a relatively destructive process. The main PCB itself is held inside a thin aluminium “tube” by two glued-in translucent plastic end-pieces. Cutting the pieces and twisting them out with pliers was the only method to free the internal PCB, which revealed some interesting results.<img fetchpriority=\"high\" decoding=\"async\" class=\"alignnone size-large wp-image-12067\" src=\"https://goughlui.com/wp-content/uploads/2015/04/DSC_4056-1024x301.jpg\" alt=\"DSC_4056\" width=\"640\" height=\"188\" srcset=\"https://goughlui.com/wp-content/uploads/2015/04/DSC_4056-1024x301.jpg 1024w, https://goughlui.com/wp-content/uploads/2015/04/DSC_4056-300x88.jpg 300w\" sizes=\"(max-width: 640px) 100vw, 640px\" />The PCB is marked C20V-2.0-AU6989-L48-2L-TF-1.5 and is dated 26th June 2014, making this a relatively recent design. A provision is made for a front LED, which is unused, instead in preference of a rear LED. A provision is made for a crystal oscillator, but this is unpopulated because this uses an Alcor Micro AU6989SN-GT controller with integrated crystal.Features, excerpted from usbdev.ru, are:– PCBs are pin compatible with AU69XX USB2.0 series\n– Integrated build-in Regulator\n– Integrated build-in Crystal with Alcor’s patent\n– Supports 72bit/1K BCH ECC engines\n– Supports new generation MLC/TLC flash\n– Supports Toggle/ONFI DDR flash\n– Not support the flash ECC requirement under 24bit/1K\n– Well performance in TLC DDR flash\n– Improved read performance reach 32MB/Sec\n– Integrates hardware DMA engine to tune up the operation performance\n– Works with default driver under the environments of Windows ME, Windows 2000, Windows XP, Vista, Window7, Windows 8, Mac 9.2, Mac OS 10.x. Using Alcor Micro’s vendor driver for the environment under Windows 98SE.\n– Low power operation with SDR/DDR flash\n– Supports software write protection\n– Support Auto Run function\n– Support erasable and read-only mode AP Disk\n– Companion application program with UFD – iStar available for users\n– – To have UFD partition management function\n– – To do password protection for the security in data access\n– – To guard data files with software write protection function\n– – To lock up PC by UFD as the key\n– Available in 48-pin LQFP 7x7mm / TQFP_7x7mm / QFN_6x6mm / QFN_7x7mm package to support 4CE pin flashx2pcs\n– Available in 64-pin LQFP 7x7mm / TQFP_7x7mm package to support 4CE pin flashx4pcsIt’s a very feature rich controller, but that’s what you would expect given the competitors (e.g. Phison) which offer similar features in the value segment.<img decoding=\"async\" class=\"alignnone size-large wp-image-12068\" src=\"https://goughlui.com/wp-content/uploads/2015/04/DSC_4058-1024x301.jpg\" alt=\"DSC_4058\" width=\"640\" height=\"188\" srcset=\"https://goughlui.com/wp-content/uploads/2015/04/DSC_4058-1024x301.jpg 1024w, https://goughlui.com/wp-content/uploads/2015/04/DSC_4058-300x88.jpg 300w\" sizes=\"(max-width: 640px) 100vw, 640px\" />The rear offers a big surprise. The flash is a soldered down single microSDHC card! The card isn’t marked with a manufacturer but does have some etching on it. A space for a surface mount crystal is seen underneath as well.I know what you’re thinking – this is just a card reader and microSDHC combination, minus the connector, but you’d be wrong. The controller is a flash controller, and desoldering reveals an even more interesting outcome. Out comes the hot air gun!<img decoding=\"async\" class=\"alignnone size-large wp-image-12069\" src=\"https://goughlui.com/wp-content/uploads/2015/04/DSC_4060-1024x712.jpg\" alt=\"DSC_4060\" width=\"640\" height=\"445\" srcset=\"https://goughlui.com/wp-content/uploads/2015/04/DSC_4060-1024x712.jpg 1024w, https://goughlui.com/wp-content/uploads/2015/04/DSC_4060-300x209.jpg 300w\" sizes=\"(max-width: 640px) 100vw, 640px\" />Only the power connections seem to be used on the microSDHC’s pin, and instead, pads are used to talk to the card. A total of 21 pin connections are made, but I’m not sure what they are.Maybe playing with the microSDHC card would allow us to read the data from it, offering a simple way to recover the data from the card – turns out this doesn’t work. The card is identified by my reader as a 24/32Mb card and does not read properly. It appears unformatted. The read speed is 10.8Mb/s.<img decoding=\"async\" class=\"alignnone size-full wp-image-12081\" src=\"https://goughlui.com/wp-content/uploads/2015/04/comsol-card.png\" alt=\"comsol-card\" width=\"595\" height=\"520\" srcset=\"https://goughlui.com/wp-content/uploads/2015/04/comsol-card.png 595w, https://goughlui.com/wp-content/uploads/2015/04/comsol-card-300x262.png 300w\" sizes=\"(max-width: 595px) 100vw, 595px\" />I also decided to go one step further and try to extract the CID and CSD information:CID: 035344534430333280ffffffff0062c5\nCSD: 00260032515981e9bef9cfff92404053The CID and CSD provide some very unusual information. The CID gives the manufacturer ID as 0x03, which is Sandisk. The application ID of 0x5344 is also a common Sandisk trait. The Product name is SD032 with revision 8.0, which would suggest 32Mb (or Gb). Serial number seems to be unset (all oxFF’s), with date set as Feburary 2006. This is paired up with a version 1 CSD, with device size of 1958, equaling 32,096,256 bytes – so 32MiB.As a result, it looks like this is a Sandisk part, with an SD controller, but it hasn’t gone through the final manufacturer certification and formatting procedure. Instead, it is integrated in a product using the pads on the underside as a raw NAND package, ignoring the on-board SD controller, and thus takes the format of the controller talking to the raw NAND.How can we prove it’s a Sandisk part? Well, here’s some photos of a similar Sandisk part, as a microSDHC card, sold under the Sandisk brand.<img decoding=\"async\" class=\"alignnone size-medium wp-image-12066\" src=\"https://goughlui.com/wp-content/uploads/2015/04/DSC_4122-300x219.jpg\" alt=\"DSC_4122\" width=\"300\" height=\"219\" srcset=\"https://goughlui.com/wp-content/uploads/2015/04/DSC_4122-300x219.jpg 300w, https://goughlui.com/wp-content/uploads/2015/04/DSC_4122.jpg 846w\" sizes=\"(max-width: 300px) 100vw, 300px\" /> <img decoding=\"async\" class=\"alignnone size-medium wp-image-12070\" src=\"https://goughlui.com/wp-content/uploads/2015/04/DSC_4121-300x217.jpg\" alt=\"DSC_4121\" width=\"300\" height=\"217\" srcset=\"https://goughlui.com/wp-content/uploads/2015/04/DSC_4121-300x217.jpg 300w, https://goughlui.com/wp-content/uploads/2015/04/DSC_4121.jpg 857w\" sizes=\"(max-width: 300px) 100vw, 300px\" />Notice how the etched numbers match the same font-spacing style? Of course the branding hasn’t been printed and an extra etching has been made on the spine, which may be due to special binning (C grade?). The underside also features the pads, but covered by some thick paint of some sort. Now I know what those pads are for!Getting out the hot air gun again, I was able to resolder the package onto the board without damaging it – it still works just fine. So why was this exciting? It’s because, just like the Phison USB keys I had played with earlier prior to blogging, the manufacturer’s tools are available (albeit though shady channels) to the public which allows for some interesting insight into the manufacturing and optimization process.AlcorMP SoftwareThe software for certifying and manufacturing these USB keys is called AlcorMP. An archive of versions of the software is available from usbdev.ru. The software is capable of running in English and Chinese, but the user guide is in Chinese only, so I’ve pretty much done my exploration without much help from it.<img decoding=\"async\" class=\"alignnone size-large wp-image-12078\" src=\"https://goughlui.com/wp-content/uploads/2015/04/NewAlcorMP2-1024x618.gif\" alt=\"NewAlcorMP2\" width=\"640\" height=\"386\" srcset=\"https://goughlui.com/wp-content/uploads/2015/04/NewAlcorMP2-1024x618.gif 1024w, https://goughlui.com/wp-content/uploads/2015/04/NewAlcorMP2-300x181.gif 300w\" sizes=\"(max-width: 640px) 100vw, 640px\" />To work properly with this particular USB key, which uses a very late controller, you must use AlcorMP Version 15.03.05.00 or newer. I tried a Version 14 with no luck, due to the flash not being supported. By default, it will run with administrative rights so it can hook a special driver to talk to the keys and reprogram them. The program has a large set of accompanying .bin files, which seem to contain code to run on the controller itself, which makes this controller a possible safety issue, as noted for Phison controllers in the BadUSB exploit. The fact the controller can be so easily reprogrammed is a boon for those making fake flash keys.These files perform the low-level format, testing and provide the firmware for the controller as well as some special partition tools for those who wish to use the iStar features.This is like an engineer’s playground – a tool made by engineers for engineers. The main screen of the tool looks like this, and at the moment, it is performing a low-level format. Normally, inserting the key will have it show up its flash ID and other information, and would load “sensible” processing settings based on the processing that was done on the unit at the factory.Looking for the configuration details of the Comsol gives us the following data:<img decoding=\"async\" class=\"alignnone size-full wp-image-12073\" src=\"https://goughlui.com/wp-content/uploads/2015/04/ConfigInfo.gif\" alt=\"ConfigInfo\" width=\"343\" height=\"557\" />The drive is formatted with a fixed capacity of 7450Mb and has firmware version 1600. The VID and PID seem to be customized as well, but the vendor/product strings are empty, which explains the no-name nature of the drive in the HDTune Pro tests prior.To begin the certification process, you will need to click on the Setup button. This brings up a password prompt, of which no entry is required – just click OK.<img decoding=\"async\" class=\"alignnone size-full wp-image-12074\" src=\"https://goughlui.com/wp-content/uploads/2015/04/Flash-Config.gif\" alt=\"Flash-Config\" width=\"645\" height=\"439\" />The first screen is a relatively cluttered one but it sets up the flash configuration. The flash is auto-detected from the ID bytes as a Sandisk SDTNRCAMA-008G. Leaving the number, channels, and cycle time at defaults seems to be sufficient for this drive.The manufacturing process can be changed to optimize the drive for Speed, or for Capacity. There is a third option which seems to be for a high level format based on pre-existing bad block marked by the NAND manufacturer. You should really use either Capacity or Speed as the bad block data from the manufacturer is probably damaged.A low level format will test the flash and make sure the flash is usable. This is desirable if you have a fake flash drive to determine its correct capacity. The check mode is in LLF Check, with several levels of thoroughness. I chose Disturb Check because this seems to be more thorough, and checks for adjacent flash cell disturbance, whereas the other tests don’t address this (but can be sufficient). I also selected half-cap check after to ensure a thorough test. The scan level can have an impact on robustness, with Full Scans taking longer but ensuring all the flash is tested – Full Scan 4 is most thorough as far as I know. The ECC level can be set between 0 to 15 – this was one setting which caught my eye and I looked to understand and optimize it further in the next part.The special flash section is there to deal with particular types of flash with quirkiness/compatibility issues, and should be left at Normal unless you have particular errors during manufacturing.The advanced button brings up a new dialogue with more features –<img decoding=\"async\" class=\"alignnone size-full wp-image-12076\" src=\"https://goughlui.com/wp-content/uploads/2015/04/LLF-Enhance.gif\" alt=\"LLF-Enhance\" width=\"434\" height=\"381\" />The ECC enhance level feature is normally off, but can be turned on to improve stability of the result. Low level format revise can increase the scan time by running the low level format loop a few times to improve stability, but is generally unnecessary, as the remaining ECC should be able to handle any marginal blocks that may have passed.Pattern controls the test pattern used in low level formatting, and driving level configures the signal drive for the chips. The MaxL1fCE seems to do with flash chip enables, and should be left at the default setting. I’m not sure what LC Offset is used for. Sync Mode may be indicative of the flash interface being run in synchronous mode – but I’m not entirely sure.Strengthen the stability should be left to default – enabling this brings up a warning that capacity and speed will be sacrificed for stability, which is probably not necessary for normal usage.Use Block Mode controls which blocks are used. This can be changed to odd or even blocks for “salvaging” bad flash chips. Cache program can be enabled or disabled, but I’m not sure what this actually does – so I did try to see if it does anything in a later part.<img decoding=\"async\" class=\"alignnone size-full wp-image-12077\" src=\"https://goughlui.com/wp-content/uploads/2015/04/Mode-Select.gif\" alt=\"Mode-Select\" width=\"645\" height=\"439\" />The next screen across allows you to select the mode which the drive appears as – whether it’s a removable disk, fixed disk, read-only, password protected, or U3-style CD-ROM. You can pre-set some formatting parameters for the drives and the images to be pre-loaded. The LED behaviour can be customized as well. The use of the U3 style CD-ROM can be used to turn the drive into a USB CD-ROM drive for installing OSes which don’t understand USB installation (e.g. Windows XP), or for storing things read-only (without using the other read-only features of the drive)<img decoding=\"async\" class=\"alignnone size-full wp-image-12080\" src=\"https://goughlui.com/wp-content/uploads/2015/04/VID-Setup.gif\" alt=\"VID-Setup\" width=\"645\" height=\"439\" />The information tab allows you to set the VID, PID and strings – so you can customize the “name” of the drive in Device Manager. That can be a pretty good party trick.<img decoding=\"async\" class=\"alignnone size-full wp-image-12071\" src=\"https://goughlui.com/wp-content/uploads/2015/04/Bad-Block-Set.gif\" alt=\"Bad-Block-Set\" width=\"645\" height=\"439\" />The bad block configuration screen gives you the opportunity to set how the flash is configured. Auto check optimizes the drive size based on how much flash is actually workable. This can be potentially dangerous, as it may not leave any spare blocks for replacement should blocks fail during “runtime“. That being said, I’m not sure the Alcor Micro is capable of doing dynamic block replacements.Dynamic Set leaves some blocks for reserve, whereas bin allows the system to automatically optimize with one target or another, and decide which “regular” capacity to allocate a drive to based on the workable flash. This is interesting as it implies there could be very odd-sized flash keys out there – maybe a 4Gb key that has 6.8Gb of workable flash?<img decoding=\"async\" class=\"alignnone size-full wp-image-12072\" src=\"https://goughlui.com/wp-content/uploads/2015/04/Capacity-Bin.gif\" alt=\"Capacity-Bin\" width=\"686\" height=\"490\" />I’d have to say that the majority of manufacturers probably don’t use this mode, and use the fix capacity mode instead – either a drive passes or fails to provide a set capacity, and that’s the end of the day (7450Mb in case of this model). The final mode sets a fixed number of blocks as a percentage as “bad”.Other settings include the format file system (you don’t get a choice really), and options which help in real production usage (up to 32 simultaneous drives qualified using the same machine). There is one interesting ATTO Optimize feature which suggests there are a few tweaks to make the drive benchmark better. The drive can be formatted with MBR or as VFAT (which isn’t reliably bootable, but gives a tiny bit more space). The Enable Reader feature allows other chipsets with integrated SD readers to have the slot “usable”, and MaxMPTime allows for production to be limited to a certain amount of time or fail.<img decoding=\"async\" class=\"alignnone size-full wp-image-12075\" src=\"https://goughlui.com/wp-content/uploads/2015/04/Flash-Type-Power.gif\" alt=\"Flash-Type-Power\" width=\"645\" height=\"439\" />The other page allows you to customize flash, and do multiple loop burn-in tests. The power bits can be adjusted to make it more acceptable to the end user’s requirements. I have no idea what AutoH2 does, but the write log option provides a quick listing in a text file of the results of drive optimization.<img decoding=\"async\" class=\"alignnone size-full wp-image-12079\" src=\"https://goughlui.com/wp-content/uploads/2015/04/UI-Show.gif\" alt=\"UI-Show\" width=\"645\" height=\"439\" />The UI Show features are really only useful for those in production environments if they want to standardize on a particular look or colour coding – the home user can live with the default.Speed/Capacity, and ECC OptimizationThe controller itself advertises support for 72 bit/1K BCH ECC, and no less than 24 bit/1K ECC. The setting for ECC being from 0 to 16 was a little puzzling, so I tried to consult the manual with some digital translation help.The translated sections for ECC using Google Translate reads as follows:ECC set\nFLASH poor quality need to be open for FLASH bad block ECC error correction can improve certain capacity, but\nThere may be some risks. ECC = 0 most stringent low grid FLASH out the most stable; ECC = 15 most relaxed,\nCapacity may be larger, but there may be some risks.\nThe original low-grid setting value refers to a low-level format ECC on the selected use.ECC tuning levels: Level 1-4, may be appropriate to increase the capacity of FLASH, the proposed selection level 1.\nLow grid correction: low grid ECC scan times can make more accurate, but it takes a little more time, check only takes effect.\nScan times: You can manually set the number of low grid scanning, you can make a more accurate scan, but it takes a little more time,\nCheck only takes effect.\nPatten: Patten can choose different scans, mainly for the more special flash.\nUse Block Mode: manually choose to do the entire block or block or even-odd block.\nCache Program: Open or closed manually select cache program command.Using Bing Translator gives me a very similar result:ECC setting\nLow quality FLASH needs to open up FLASH bad block by ECC error correction, guaranteed capacity can be improved, but\nThere may be a certain amount of risk. ECC=0 is the most strict, low FLASH the most stability; ECC=15 is the most relaxed,\nCapacity may be larger, but there may be a certain degree of risk.\nLow setting refers to the use of a low-level format on the chosen ECC values.ECC tuning level: level 1-4, may be appropriate to improve the capacity of the FLASH, choose level 1.\nCorrection: low several times makes the ECC scanning is more accurate, but will spend more time, check the do not take effect.\nScan frequency: low the number of scans that can be manually set, can make the scan is more accurate, but will spend more time,\nCheck the do not take effect.\nPatten: you can select a different scan Patten, mainly for very special Flash.\nUse Block Mode: manually choose to do the full block or even block or odd block.\nCache Program: choose to turn on or turn off the cache manually program command.As a result, it seems that the ECC setting sets the tolerance to block errors during flash low level formatting. To verify this, I decided to run a low level format at every setting (taking an hour each setting) forboth capacity and speed optimize. ECC Tuning was disabled. Random pattern was used during testing (resulting in slightly random variances in formatted size), and Disturb test with Full Scan 4 was used. The results were as follows:<img decoding=\"async\" class=\"alignnone size-full wp-image-12082\" src=\"https://goughlui.com/wp-content/uploads/2015/04/devsize-vs-ecc.png\" alt=\"devsize-vs-ecc\" width=\"932\" height=\"736\" srcset=\"https://goughlui.com/wp-content/uploads/2015/04/devsize-vs-ecc.png 932w, https://goughlui.com/wp-content/uploads/2015/04/devsize-vs-ecc-300x237.png 300w\" sizes=\"(max-width: 932px) 100vw, 932px\" />The results seem to follow the description to some extent with some surprises. For one, the optimize for capacity option did not significantly outperform optimize for speed, and performed worse at low ECC level (strict). The capacity of most ECC levels above 3 were fairly similar, around 7800MB+, which is about 350-450Mb more than the fixed capacity it was shipped with. The number of bad blocks identified varies somewhat from ECC level to ECC level, probably due to random pattern test variation in detecting errors, coupled with some potential wear-out and alignment differences during test.In general, it seems like the ECC level represents the number of bad bits tolerated in a flash block/page during low level formatting before the block/page is marked bad and taken out of use. Hence, lower levels are stricter, and higher levels would allow for more defective bits in the low level format (leaving less margin for wear in the future, making it less stable).Because of the quality of the flash, the capacity is maximised even with fairly strict levels of ~3. Higher levels don’t seem to restore much more capacity, which implies that the bad blocks must contain bursts of bad cells which are uncorrectable even with wider bad-bits tolerance.It’s probably best to certify for a fixed size a bit smaller, if you want to give some room for reallocations (provided the controller actually supports it).So, what’s the cost of Capacity optimization versus Speed? Well, as it turns out, the speed is much better in speed mode – it’s about 84% faster in read and 44% faster in write than the shipped status. Part of the improvement seems to be new firmware – as the drive identifies with firmware version 8E8A using the AlcorMP tool.<img decoding=\"async\" class=\"alignnone wp-image-12085\" src=\"https://goughlui.com/wp-content/uploads/2015/04/Capacity-Optimized-CDM.png\" alt=\"Capacity Optimized CDM\" width=\"317\" height=\"287\" srcset=\"https://goughlui.com/wp-content/uploads/2015/04/Capacity-Optimized-CDM.png 406w, https://goughlui.com/wp-content/uploads/2015/04/Capacity-Optimized-CDM-300x272.png 300w\" sizes=\"(max-width: 317px) 100vw, 317px\" /> <img decoding=\"async\" class=\"alignnone wp-image-12086\" src=\"https://goughlui.com/wp-content/uploads/2015/04/Speed-Optimized-CDM.png\" alt=\"Speed Optimized CDM\" width=\"317\" height=\"287\" srcset=\"https://goughlui.com/wp-content/uploads/2015/04/Speed-Optimized-CDM.png 406w, https://goughlui.com/wp-content/uploads/2015/04/Speed-Optimized-CDM-300x272.png 300w\" sizes=\"(max-width: 317px) 100vw, 317px\" />No more nasty 13.78Mb/s read, and 3.27Mb/s write! It’s still no speed demon, but given the Sandisk card I had with the same sort of shape was a Class 4 card, the performance is probably the best we can expect from the flash. I did change the write-cycle time to the minimum setting, hoping to “push” the flash faster, but it made no difference.ATTO Optimization and Cache Program\nOne other option intrigued me, and that was optimization for ATTO, a commonly used dish benchmark. Was this some sort of cheating, or a deliberate bias towards small block accesses at the expense of sequential access? Was there any tradeoff?The other wonder was what the Cache Program option meant. Did this mean there would be an pSLC cache on the drive or some sort of attempt at optimizing for small transfers? Or was it just an option to cache production firmware on the drives themselves? Did it have any performance impact?<img decoding=\"async\" class=\"alignnone size-full wp-image-12088\" src=\"https://goughlui.com/wp-content/uploads/2015/04/Capacity-Optimized-ATTO-No-Opt.png\" alt=\"Capacity Optimized ATTO No Opt\" width=\"482\" height=\"615\" srcset=\"https://goughlui.com/wp-content/uploads/2015/04/Capacity-Optimized-ATTO-No-Opt.png 482w, https://goughlui.com/wp-content/uploads/2015/04/Capacity-Optimized-ATTO-No-Opt-235x300.png 235w\" sizes=\"(max-width: 482px) 100vw, 482px\" />Capacity Optimized, No ATTO Optimization<img decoding=\"async\" class=\"alignnone size-full wp-image-12089\" src=\"https://goughlui.com/wp-content/uploads/2015/04/Speed-Optimized-ATTO-No-Opt.png\" alt=\"Speed Optimized ATTO No Opt\" width=\"482\" height=\"615\" srcset=\"https://goughlui.com/wp-content/uploads/2015/04/Speed-Optimized-ATTO-No-Opt.png 482w, https://goughlui.com/wp-content/uploads/2015/04/Speed-Optimized-ATTO-No-Opt-235x300.png 235w\" sizes=\"(max-width: 482px) 100vw, 482px\" />Speed Optimized, No ATTO OptimizationOn the whole, with no ATTO optimization, aside from slight variations, it seems that the optimize for performance selection provides better performance across the board.<img decoding=\"async\" class=\"alignnone size-full wp-image-12090\" src=\"https://goughlui.com/wp-content/uploads/2015/04/Speed-ATTO-Optimized-ATTO.png\" alt=\"Speed+ATTO Optimized ATTO\" width=\"482\" height=\"615\" srcset=\"https://goughlui.com/wp-content/uploads/2015/04/Speed-ATTO-Optimized-ATTO.png 482w, https://goughlui.com/wp-content/uploads/2015/04/Speed-ATTO-Optimized-ATTO-235x300.png 235w\" sizes=\"(max-width: 482px) 100vw, 482px\" />Speed Optimized, ATTO Optimization ONTurning on ATTO Optimization doesn’t seem to do much at all. There’s a few slight increases across the board, but I suppose this flash chip isn’t particularly capable and neither is the single channel design.<img decoding=\"async\" class=\"alignnone size-full wp-image-12091\" src=\"https://goughlui.com/wp-content/uploads/2015/04/Speed-ATTO-Cache-ATTO.png\" alt=\"Speed+ATTO+Cache ATTO\" width=\"482\" height=\"615\" srcset=\"https://goughlui.com/wp-content/uploads/2015/04/Speed-ATTO-Cache-ATTO.png 482w, https://goughlui.com/wp-content/uploads/2015/04/Speed-ATTO-Cache-ATTO-235x300.png 235w\" sizes=\"(max-width: 482px) 100vw, 482px\" />Speed Optimized, ATTO Optimization ON, Cache ONTurning on cache didn’t make a lick of difference either, so I might as well leave that at default. It wasn’t the option I hoped it was.I also used H2testw and CrystalDiskMark as benchmarks to find out just how far we’ve improved from the baseline case of “as shipped” and the impact of each of the options. It seems that optimize for speed gives a good boost overall, and ATTO optimization provides a slight advantage. Cache didn’t have much of a notable impact, except for a strange reduction in 512kB writes in CDM and a slight increase in H2testw writes.My finalized settings including ECC Enhance Level 1, with LLF Revise and Dynamic Set (4) was tested as well on the rightmost column, as my “final” optimized result. It doesn’t seem that these options, which affect the LLF process, actually affects the final drive performance beyond that of normal “test to test variation”. Not bad for a “cost free” solution.<img decoding=\"async\" class=\"alignnone size-full wp-image-12095\" src=\"https://goughlui.com/wp-content/uploads/2015/04/Performance-Summary.png\" alt=\"Performance Summary\" width=\"733\" height=\"223\" srcset=\"https://goughlui.com/wp-content/uploads/2015/04/Performance-Summary.png 733w, https://goughlui.com/wp-content/uploads/2015/04/Performance-Summary-300x91.png 300w\" sizes=\"(max-width: 733px) 100vw, 733px\" />Of note was that the drive was stable enough to pass H2testw in the fresh state even at varying format capacities depending on the run. This doesn’t indicate the drive will be stable in the future, as the flash wears, hence my recommendation to use stricter ECC (than the default 8 used by this manufacturer) and dynamic set to maximise the storage and improve the “safety” margin.I even tried being very negligent – doing a manufacture run at ECC 15, with ECC Enhance at 4 (loose), speed optimize, with half-capacity scan, quick-scan 1 selected, and auto size for bad blocks and the resulting unit still passed the H2testw test, however, might be unstable as flash cells wear out. It didn’t yield me any significant capacity gain, and that’s likely due to burst-error accumulation in manufactured flash.ConclusionThe quest for optimization seems to be something in most engineer’s hearts. It’s one reason why I do love overclocking. Playing with a manufacturer’s tool is like overclocking a USB key – performance and capacity was both improved at no cost! It turns a nasty key into one that more closely resembles its advertised “up to 20MB/s” and makes it tolerable for use. Of course, it may not be as stable as otherwise, depending on the options you choose – but now you’re in the driver’s seat and that’s pretty cool.Of course, none of this is without risk, and it’s equally possible to ruin a good drive, brick it or otherwise make the performance or stability worse. You can and will void warranties as well – do all of this at your own risk.In the end, I opted for ECC level 4, plus ECC Enhance Level 1 and LLF Revise ON, cache default and chose to go with Dynamic Set (4) for bad blocks to give some margin for reallocation. That should give me some more capacity, with the stability you would expect.Like it? Share it!FacebookTwitterRedditLinkedInMoreTumblrPocketPinterestWhatsAppTelegramRelatedReview, Teardown: Verbatim 8Gb Store’n’Go PinStripe USB 2.0 Drive (49062)Yet another low-cost USB flash drive gets a review, mainly because I just happened to end up receiving it. Unlike the others though, this one comes from Verbatim which is a brand which has come to be respected due to their reputation built around floppy disks and optical discs. The…December 4, 2016In \"Computing\"Quick Review: Comsol 8Gb USB 2.0 Flash Stick (UF4-8000)USB flash drives are one of the most ubiquitous storage devices, this one was anonymously donated to me for some testing. It's not a particularly special looking drive, but it is one that's available on the market today - it was worth a quick test anyway. The Comsol branding may…February 20, 2015In \"Flash Memory\"Unintentional Teardown & Repair: Kingston 128Gb UHS-I SDXC CardA while back, I posted a gripe article about how I disliked some aspects of the Secure Digital memory card format, one of the issues being the fragility of the plastic shell. Through no fault of my own, it seems it has happened to one of my largest cards. Damn.…May 23, 2015In \"Flash Memory\"Review, Teardown: Corsair Flash Voyager LS 32Gb USB 3.0 Flash DriveCorsair has been a trusted name in DRAM memory for a long time, which resulted in a brand expansion into peripherals, accessories, power supplies and more. It seems like whatever Corsair puts their brand on is a product you can trust. I've always been partial towards Corsair, having owned many…February 28, 2015In \"Computing\"Review, Teardown: Sandisk Cruzer Facet 8Gb USB 2.0 Flash DriveReaders familiar with my penchant for exotic storage would probably stare at me in confusion as to why I would be reviewing such a plain product such as this one? Interestingly enough, as with most things that I do, there is a good reason. The Sandisk Cruzer Facet 8Gb is…January 15, 2016In \"Computing\"Experiment: 8Gb USB Flash Drive Endurance TestFlash memory is, and has been, a commodity item for a while now. Almost everyone has at least a few USB flash drives (sticks) and maybe even a few memory cards. When it comes to rapidly transferring large files between devices, or even storing working documents, USB flash drives are…May 30, 2017In \"Computing\"Review & Teardown: Kogan USB 3.0 Universal Memory Card ReaderLast week, Kogan had a free shipping day, and not being one to let such opportunities slip by, I decided to pick up a Kogan USB 3.0 Universal Memory Card Reader for AU$19. It's a fair price to pay for such a product, similarly priced to my favourite Transcend RDF8.…February 17, 2014In \"Computing\"Teardown, Test: PGH Bricks & Pavers Promotional USB KeyI guess the fact that this post exists is a proof that promotional USB keys \"work\". They serve a useful purpose, and have a decent \"lifetime\", thus helping spread the promotional message. I'm no brick and pavers guy, but I managed to have one of these gifted to me, so…June 30, 2014In \"Flash Memory\"Quick Review, Teardown: “Avoid driving fast card” USB 3.0 Ethernet AdapterIt's always nice to have a few extra Ethernet interfaces on a computer - sometimes you need to run a software firewall, use a computer as a router, or you just need to sniff some network traffic running to a device for some reverse engineering or testing purposes. With the…March 17, 2016In \"Computing\"\n<img alt='' src='https://secure.gravatar.com/avatar/0843025630d91cd79ecc59aa067ac7dd?s=60&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/0843025630d91cd79ecc59aa067ac7dd?s=120&#038;d=mm&#038;r=g 2x' class='avatar avatar-60 photo' height='60' width='60' decoding='async'/>\nAbout lui_gough\nI'm a bit of a nut for electronics, computing, photography, radio, satellite and other technical hobbies. Click for more about me!\n\nView all posts by lui_gough →\nThis entry was posted in Computing, Flash Memory and tagged experiment, flash, flash memory, made-in-china, storage, teardown. Bookmark the permalink.← Unintentional Teardown: HP iPAQ h5550 Windows Mobile 2003 PDATrip to HK & CN 2014 – Part 6: Being a Tourist – Big Bus Tour (Part 1) →\n6 Responses to Teardown, Optimization: Comsol 8Gb USB Flash Stick (AU6989SN-GT & SDTNRCAMA-008G)\n<img alt='' src='https://secure.gravatar.com/avatar/0843025630d91cd79ecc59aa067ac7dd?s=40&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/0843025630d91cd79ecc59aa067ac7dd?s=80&#038;d=mm&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' decoding='async'/>\t\t\t\tlui_gough says:\nApril 11, 2015 at 12:03 am\t\t\t\t\tAfter some playing around, it seems AlcorMP tool is not without some drawbacks. One is that it installs a filter driver into your USB stack named mpfilt and on 64-bit Windows machines after the latest Windows update, you will find that it “breaks” causing all of the USB 2.0 controllers on your machine to fail with an exclamation mark and something about “associated driver failed to start.”To fix this, you will need to remove the mpfilt driver from your LowerFilters. Unless you have a USB 3.0 controller that still works without the driver (where plugging in a keyboard and mouse into those ports lets you take control of the system) or a remote VNC connection, you will have to boot into safe mode first. Get into regedit and do a search for all entries with mpfilt. Whenever you see one that has a name LowerFilters with Data “mpfilt”, delete the LowerFilters entry. DO NOT DELETE ANYTHING ELSE.Reboot your machine and you should be back to normal.\nReply\n<img alt='' src='https://secure.gravatar.com/avatar/f493cc56b8fcaa4eb141bb204b29e0fe?s=40&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/f493cc56b8fcaa4eb141bb204b29e0fe?s=80&#038;d=mm&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' decoding='async'/>\t\t\t\t- dutchman24 - says:\nAugust 1, 2016 at 6:30 pm\t\t\t\t\tThank you I was looking arround the internet to have some detailed information about this program.\nIf you have problems  (like i had) with the USB drivers because of the HUB configuration message you can still operate your pc with a ps2 (purple plug) keyboard. I menaged to restart Windows 10 with shift key pressed down and do a systeme restore and everything went back to normal.\nIts very important to start the AlcorMP program before to insert any USB devices and use USB 2.0 socket and not the 3.0 sockets (blue ones).\nBTW i think that the “Cache” option in the Advanced TAB is to configurate the USB penn to take it out of the PC without deconnecting it. If Cache is disabled it works “stand alone without putting cache into the PC.  If enabled the penn is putting cache in the PC to work faster but you have to deconnect the hardware before you can pull it out of your computer.\nRegards\nReply\n<img alt='' src='https://secure.gravatar.com/avatar/9495d9b367290ba6a36dfacf72276119?s=40&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/9495d9b367290ba6a36dfacf72276119?s=80&#038;d=mm&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' decoding='async'/>\t\t\t\ttc says:\nJuly 18, 2020 at 6:04 pm\t\t\t\t\tOr just use VMware, why would you run this kinda dodgy app on bare metal\nReply\n<img alt='' src='https://secure.gravatar.com/avatar/0843025630d91cd79ecc59aa067ac7dd?s=40&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/0843025630d91cd79ecc59aa067ac7dd?s=80&#038;d=mm&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' decoding='async'/>\t\t\t\tlui_gough says:\nJuly 18, 2020 at 7:48 pm\t\t\t\t\tThere’s a good reason for this. The reconfiguration of the flash controller relies on reprogramming the firmware of the controller itself, which often results in the device resetting and reconnecting to the bus with a new ID to perform various flash verification tasks as a sort of “warm reset”. Many manufacturing tools are very timing sensitive and will throw errors midway after loading the diagnostic code and resetting the controller since it doesn’t “reconnect” to the VM in time or correctly. Sometimes the USB pass-through drivers issue spurious resets to the USB devices which may cause them to lose this diagnostic code (often loaded in RAM temporarily) and reboot as a regular USB stick using the firmware in flash, resulting in no go either.– Gough\nReply\n<img alt='' src='https://graph.facebook.com/v6.0/1950211318481950/picture?type=large' srcset='https://secure.gravatar.com/avatar/d6dc757340a26c503ec3d4bac8c02445?s=80&#038;d=mm&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' decoding='async'/>\t\t\t\tハッピー アニメ マーケット says:\nFebruary 13, 2022 at 3:57 pm\t\t\t\t\tDo you think I can use this to recover my bad sandisk microSD card? the pins seems to match but if not, I can solder from my card to this board.\nReply\n<img alt='' src='https://secure.gravatar.com/avatar/0843025630d91cd79ecc59aa067ac7dd?s=40&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/0843025630d91cd79ecc59aa067ac7dd?s=80&#038;d=mm&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' decoding='async'/>\t\t\t\tlui_gough says:\nFebruary 18, 2022 at 5:52 am\t\t\t\t\tNo. It uses the card as raw flash memory and the controller has its own format which is likely to be incompatible with whatever SanDisk’s own card has as it bypasses it entirely. Without compatible metadata, the USB Flash memory controller would not allow access anyway.– Gough\nReplyError: Comment is Missing!Cancel reply\n\n document.addEventListener('DOMContentLoaded',function(){var commentForms=document.getElementsByClassName('jetpack_remote_comment');for(var i=0;i<commentForms.length;i++){commentForms[i].allowTransparency=!1;commentForms[i].scrolling='no'}})",
    "summary": {
      "en": "The text discusses the teardown and optimization of a Comsol 8Gb USB flash drive. The author, an engineer, describes the process of dismantling the device to explore its components, particularly the main printed circuit board (PCB) and the flash memory. \n\nKey points include:\n\n1. **Teardown Process**: The USB flash drive is encased in a metal tube, which is tricky to open without damaging it. The PCB revealed interesting features, including an unused LED and a built-in controller.\n\n2. **Flash Memory Insights**: The flash memory inside the drive is essentially a microSD card soldered directly to the PCB. Upon inspection, it was found to be a Sandisk part, but not properly formatted or certified for use.\n\n3. **Optimization Tools**: The author used a software called AlcorMP to reprogram the flash drive. This software allows for low-level formatting and optimization of the drive's performance and capacity.\n\n4. **Results of Optimization**: Through various settings adjustments, the author improved the drive’s speed and capacity significantly. It was noted that optimizing for speed yielded better performance compared to capacity.\n\n5. **Risks of Optimization**: There are risks involved in this process, including the potential to ruin the drive or void warranties. The author emphasizes the importance of being cautious and aware of these risks.\n\nOverall, the experience showcases the excitement of engineering and the potential to enhance the performance of everyday technology through hands-on experimentation.",
      "ko": "이 글에서는 Comsol 8Gb USB 플래시 드라이브의 분해와 최적화 과정을 다룹니다. 저자는 엔지니어로서 이 장치를 분해하여 주요 부품, 특히 인쇄 회로 기판(PCB)과 플래시 메모리를 살펴보는 과정을 설명합니다.\n\nUSB 플래시 드라이브는 금속 튜브에 감싸져 있어 손상 없이 열기가 어렵습니다. PCB를 살펴보니 사용되지 않는 LED와 내장된 컨트롤러 같은 흥미로운 기능들이 발견되었습니다. 드라이브 내부의 플래시 메모리는 사실상 PCB에 직접 납땜된 마이크로SD 카드입니다. 검토해본 결과, 이 메모리는 Sandisk 제품이지만 제대로 포맷되거나 인증되지 않은 상태였습니다.\n\n저자는 AlcorMP라는 소프트웨어를 사용하여 플래시 드라이브를 재프로그래밍했습니다. 이 소프트웨어는 드라이브의 성능과 용량을 저수준에서 포맷하고 최적화할 수 있게 해줍니다. 다양한 설정 조정을 통해 저자는 드라이브의 속도와 용량을 크게 향상시켰습니다. 속도를 최적화하는 것이 용량을 최적화하는 것보다 더 나은 성능을 가져온다는 점도 주목할 만합니다.\n\n하지만 이 과정에는 드라이브를 망가뜨리거나 보증을 무효화할 위험이 따릅니다. 저자는 이러한 위험을 인식하고 조심하는 것이 중요하다고 강조합니다. 전반적으로 이 경험은 엔지니어링의 흥미로움과 실험을 통해 일상 기술의 성능을 향상시킬 수 있는 가능성을 보여줍니다.",
      "ja": "このテキストでは、Comsolの8GB USBフラッシュドライブの分解と最適化について説明しています。著者はエンジニアで、デバイスを分解してその内部構造、特に主要なプリント基板（PCB）とフラッシュメモリを調査する過程を詳述しています。\n\nまず、USBフラッシュドライブは金属製の筐体に入っており、壊さずに開けるのが難しいことがわかりました。PCBには、未使用のLEDや内蔵コントローラーなど、興味深い特徴が見つかりました。\n\n次に、ドライブ内部のフラッシュメモリは、実質的にはPCBに直接はんだ付けされたマイクロSDカードです。調査の結果、Sandisk製のものであることが判明しましたが、適切にフォーマットされておらず、使用の認証も受けていませんでした。\n\n著者は、AlcorMPというソフトウェアを使用してフラッシュドライブを再プログラムしました。このソフトは、低レベルのフォーマットやドライブの性能と容量の最適化を可能にします。\n\n最適化の結果、著者はさまざまな設定を調整することで、ドライブの速度と容量を大幅に向上させることができました。特に、速度の最適化が容量よりも良いパフォーマンスをもたらすことが確認されました。\n\nただし、このプロセスにはリスクも伴います。ドライブを壊したり、保証が無効になる可能性があります。著者は、これらのリスクに注意し、慎重に行動することの重要性を強調しています。\n\n全体として、この経験はエンジニアリングの楽しさを示しており、日常の技術の性能を実践的な実験を通じて向上させる可能性を感じさせます。"
    }
  },
  {
    "id": "2159168a750d76f2",
    "title": {
      "en": "Autology: A Lisp with access to its own interpreter",
      "ko": "자기 해석 리습",
      "ja": "オートロジー：自己解釈のLisp"
    },
    "type": "story",
    "url": "https://github.com/Kimbsy/autology",
    "score": 128,
    "by": "simonpure",
    "time": 1742553436,
    "content": "Autology\nA hot take on Lisp metaprogramming, Autology is a Lisp with access\nto its own interpreter.\nWhat is it?\nAutology is a functional interpreted Lisp language, written in Clojure.\nThe Autology interpreter function is defined as a data structure which is available to the Autology program, it is bound to the variable *i*.\nWhenever an expression needs to be evaluated, Autology will retrieve the current value of *i* from the current lexically scoped execution environment and use it to evaluate the expression.\nBy binding new values of *i* we can modify the behaviour of the Autology language while our program is running.\nWhy have you done this?\nMainly for fun and to explore a neat idea.\nAutology is not a particularly useful language as-is, its very slow and resource intensive.\nMacros are the normal tool for Lisp languages to modify their syntax or behaviour, but these are normally only available at compile time. Autology allows dynamic rewriting of the language at runtime.\nEven other non-Lisp languages are able to define some form of Domain Specific Language to allow programmers to express problems more clearly, however these are generally quite restrictive in scope. Autology allows full ad-hoc re-writing of the syntax of the language, as well as defining new features or removing existing ones.\nWhat can you do with it?\nWell to start with you might like to add functions to the language. Autology doesn't have them by default, but we can rebind the *i* symbol to a data structure representing a new interpreter function, one that does have functions.\n(bind (;; grab a copy of the `:atl/eval-list` section of the\n       ;; interpreter which is responsible for evaluating lists.\n       original (get-marker *i* :atl/eval-list)\n\n       ;; define a case test+body for use when the list expression\n       ;; starts with our function special form, in this case `λ`.\n       λ-form (qu (λ (let [[_λ params body] e]\n                       (fn [& values]\n                         (autology.core/evaluate\n                          body\n                          (reduce (fn [acc-env [s v]]\n                                    (assoc acc-env s v))\n                                  env\n                                  (zipmap params values)))))))\n\n       ;; rebind `*i*` to be a new interpreter with the\n       ;; `:atl/eval-list` section replaced with a version that\n       ;; includes our lambda handling special form.\n       *i* (replace-marker *i* :atl/eval-list\n                           (list :atl/eval-list\n                                 (concat (butlast original)\n                                         λ-form\n                                         (list (last original)))))\n\n       ;; We can now immediately define functions since the\n       ;; interpreter will have already been updated to evaluate the\n       ;; remaining bindings like this one.\n       double (λ (n)\n                 (+ n n)))\n\n      ;; Finally we can invoke our new function!\n      (double (double (double (double (double 1.3125))))))\n\nAfter that, pretty much anything you want!\n\nWhy not switch form applicative order evaluation to normal order evaluation?\nMaybe modify the language so it uses a continuation passing style?\nDefine a metacircular Lisp inside your program which has access to it's own interpreter as well at the Autology interpreter.\n\nRun tests\nclojure -X:test",
    "summary": {
      "en": "**Summary of Autology**\n\nAutology is a unique Lisp programming language that allows access to its own interpreter, making it possible to change its behavior while the program runs. It is written in Clojure and is designed as a fun exploration of metaprogramming ideas.\n\n### Key Points:\n- **What is Autology?** It's a functional Lisp language that uses a special interpreter function stored in a variable called *i*. This allows dynamic evaluation of expressions by changing *i*'s value during runtime.\n  \n- **Purpose:** Autology was created mainly for enjoyment and experimentation. While it’s not practical for serious use due to its slow performance, it offers features that typical Lisp macros do not, allowing for flexible, real-time changes to the language’s syntax and behavior.\n\n- **Capabilities:** Users can enhance Autology by adding new functions or modifying existing features. For example, one can define a lambda function to create new functions dynamically.\n\n- **Examples of Modifications:** Users can change how functions are evaluated, switch to different evaluation styles, or even create a self-referential version of Lisp within Autology.\n\n- **Testing:** Autology can be tested with the command `clojure -X:test`. \n\nOverall, Autology is an experimental tool that showcases advanced programming concepts in a playful way.",
      "ko": "오토로지는 독특한 리스프 프로그래밍 언어로, 자신의 인터프리터에 접근할 수 있어 프로그램이 실행되는 동안 동작을 변경할 수 있습니다. 이 언어는 클로저로 작성되었으며, 메타프로그래밍 아이디어를 재미있게 탐구하기 위해 설계되었습니다.\n\n오토로지는 기능적 리스프 언어로, *i*라는 변수에 저장된 특별한 인터프리터 함수를 사용합니다. 이를 통해 실행 중에 *i*의 값을 변경하여 표현식을 동적으로 평가할 수 있습니다. 오토로지는 주로 즐거움과 실험을 위해 만들어졌습니다. 성능이 느려서 진지한 용도로는 적합하지 않지만, 일반적인 리스프 매크로에서는 제공하지 않는 기능을 통해 언어의 문법과 동작을 유연하게 실시간으로 변경할 수 있습니다.\n\n사용자는 새로운 함수를 추가하거나 기존 기능을 수정하여 오토로지를 확장할 수 있습니다. 예를 들어, 사용자는 람다 함수를 정의하여 동적으로 새로운 함수를 생성할 수 있습니다. 사용자는 함수의 평가 방식을 변경하거나 다른 평가 스타일로 전환할 수 있으며, 오토로지 내에서 자기 참조적인 리스프 버전을 만들 수도 있습니다.\n\n오토로지는 `clojure -X:test` 명령어로 테스트할 수 있습니다. 전반적으로 오토로지는 고급 프로그래밍 개념을 재미있게 보여주는 실험적인 도구입니다.",
      "ja": "オートロジーは、独自のインタープリターにアクセスできるユニークなLispプログラミング言語です。これにより、プログラムが実行されている間にその動作を変更することが可能になります。オートロジーはClojureで書かれており、メタプログラミングのアイデアを楽しく探求するために設計されています。\n\nオートロジーは、特別なインタープリター関数を変数*i*に格納した関数型Lisp言語です。この仕組みにより、実行中に*i*の値を変更することで、式を動的に評価することができます。\n\nオートロジーの主な目的は、楽しみと実験です。性能が遅いため、真剣な用途には向いていませんが、一般的なLispマクロではできない機能を提供し、言語の構文や動作を柔軟にリアルタイムで変更できる点が特徴です。\n\nユーザーは、新しい関数を追加したり、既存の機能を修正したりしてオートロジーを拡張できます。たとえば、ラムダ関数を定義して新しい関数を動的に作成することが可能です。\n\nユーザーは、関数の評価方法を変更したり、異なる評価スタイルに切り替えたり、オートロジー内で自己参照型のLispを作成したりすることができます。\n\nオートロジーは、コマンド`clojure -X:test`を使ってテストすることができます。\n\n全体として、オートロジーは高度なプログラミング概念を遊び心満載で示す実験的なツールです。"
    }
  },
  {
    "id": "1e53cc181f8923ea",
    "title": {
      "en": "Peano's Axioms",
      "ko": "페아노 공리",
      "ja": "ペアノの公理"
    },
    "type": "story",
    "url": "https://principlesofcryptography.com/number-theory-primer-an-axiomatic-study-of-natural-numbers-peano-axioms/",
    "score": 86,
    "by": "ulugh",
    "time": 1742844365,
    "content": "Categories\n\n\t\t\t\t\tmathematics Number Theory\n\n\t\t\tNumber Theory Primer : An Axiomatic Study Of Natural Numbers – Peano’s Axioms\n\n\t\t\t\t\t\t\t\tPost author\n\n\t\t\t\t\t\t\tBy premmi\n\n\t\t\t\t\t\t\t\tPost date\n\n\t\t\t\t\t\t\tFebruary 1, 2025\n\nauthored by Premmi andBeguène\n\nPrevious Topic: An Axiomatic Study of Numbers\n\nIntroduction\n\nThinking of numbers intuitively brings to mind the simplest and most fundamental set of numbers, namely the set of natural numbers. These numbers are used to count objects like cars, books, pens, etc. If we associate natural numbers such as 1,2,3,1, 2, 3,1,2,3, etc. with counting, then with what corresponding concepts do we relate numbers like −4,3and227-4, \\sqrt{3} \\text{ and } \\frac{22}{7}−4,3and722?\n\nTo reason about all kinds of numbers encountered during our study of mathematics, we need a precise mathematical framework for defining numbers. We will build this framework by first rigorously defining natural numbers axiomatically without relying on the intuitive notion of counting. Then, using this framework as a foundation, we will construct all the other sets of numbers such as integers, rational numbers, real numbers and complex numbers in terms of the natural numbers.\n\nA good axiomatic system assumes as little as possible, while proving as much as possible. To create an efficient axiomatization for the natural numbers, we must distill these numbers to their essential properties. Intuitively, we understand various aspects of the natural numbers, such as their existence and the basic properties of the binary operations addition, multiplication, and the ‘less than’ relation. How few of these concepts can we take as axioms, from which we can deduce everything else that we need to know about the natural numbers? It turns out that remarkably little is required for the axiomatization of the natural numbers—neither addition, nor multiplication, nor the ‘less than’ relation need to be taken as axioms; these will all be constructed from our fundamental axioms.\n\nThe standard axiomatization of the natural numbers, know as the Peano Axioms, was originally formulated by the Italian mathematician, Giuseppe Peano. In 188918891889, Peano developed the real number system based on his axioms for the natural numbers. He defined natural numbers through nine axioms, four of which established the properties of the equality relation “===” with regard to natural numbers, while the remaining five axioms provided a complete and rigorous definition of natural numbers.\n\nIn order to appreciate the intellectual feat of Peano, it’s worth noting that utilizing only his axioms, we are able to prove all the established properties of the natural numbers. Furthermore, these axioms facilitate the construction of the integer, rational, real, and complex number systems.\n\nBefore we discuss Peano’s Axioms in detail, it is a useful exercise to explore an alternative way to describe natural numbers, distinct from the usual intuitive notion of counting. Such an exploration would help us to independently arrive at Peano’s axioms.\n\nIntuition behind the Axiomatic Definition of Natural Numbers\n\nHow do we model the concept of natural numbers denoted by 0,1,2,30, 1, 2, 3 0,1,2,3 and so on without relying on the notion of counting?\n\nOne way would be to take a set-theoretic approach. That is, we can define a set of natural numbers by systematically enumerating the properties and relationships of the members of this set such that the set results in {0,1,2,3,…}\\{0, 1, 2, 3, \\ldots\\}{0,1,2,3,…}.\n\nLet us denote the set of natural numbers as N\\mathcal{N}N. First, we recognize that 000 should be a part of this set. Next, we want 000 to lead us to 111, 111 to 222 and we should be able to continue this way, naming each successive number as far as we wish. This is illustrated below.\n\n0→1→2→3→…0 \\rightarrow1 \\rightarrow 2 \\rightarrow 3 \\rightarrow \\ldots0→1→2→3→…\n\nFrom the above diagram we can see that to model this relationship we need a “next” operation that given a natural number, produces the next natural number in the sequence.\n\nThe above diagram can also be viewed as shown below.\n\n0→11→22→3⋮    ⋮\\begin{equation*}\n\\begin{split}\n0 &\\rightarrow 1 \\\\\n1 &\\rightarrow 2 \\\\\n2 &\\rightarrow 3 \\\\\n\\vdots &\\quad\\,\\,\\,\\, \\vdots \\\\\n\\end{split}\n\\end{equation*} 012⋮→1→2→3⋮\n\nWe can see from the diagram that an input of 000 yields an output of 111, an input of 111 yields 222 and so on. Therefore, we can model the “next” operation as a function S:N→NS : \\mathcal{N} \\rightarrow \\mathcal{N}S:N→N that takes a natural number as input and produces a natural number as output. Here, the letter SSS stands for ‘successor’ and we have S(0)=1,S(1)=2S(0) = 1, S(1) = 2S(0)=1,S(1)=2 and so forth. We will refer to this function SSS as the successor function since it establishes a succession within the set of natural numbers, N\\mathcal{N}N. This function SSS is illustrated below.\n\nFrom the diagram above, we can observe that the successor function SSS has the following properties:\n\nNot Surjective: There is no natural number in N\\mathcal{N}N that, when given as input to the successor function SSS, results in 000 as output. This implies that not every element of the codomain of SSS is the image of at least one element from its domain. Therefore, we can conclude that SSS is not surjective because S(n)≠0foranyn∈NS(n) \\neq 0 \\text{ for any } n \\in \\mathcal{N}S(n)=0foranyn∈N.\n\nInjective: Different inputs to SSS yield different outputs. This means that every element of the codomain of SSS is the image of at most one element from its domain; that is, S(m)=S(n)impliesthatm=nforanyn,m∈NS(m) = S(n) \\text{ implies that } m = n \\text{ for any } n, m \\in \\mathcal{N}S(m)=S(n)impliesthatm=nforanyn,m∈N. Hence, SSS is injective.\n\nFrom the diagram, we can also see that a natural number is either 000 or can be obtained from 000 by applying the successor function to 000 a finite number of times. This implies that N\\mathcal{N}N is the minimal non-empty set that contains 000 and admits a successor function satisfying conditions (1)and(2)(1) \\text{ and } (2)(1)and(2).\n\nTherefore, for the Peano Axioms to accurately describe the set of natural numbers, they must define a set that contains 000 and admits a successor function as described above.\n\nOriginal Formulation of Peano’s Axioms\n\nBefore we discuss the modern version of Peano’s Axioms, it is interesting to know how these axioms were originally stated by Giuseppe Peano. While perusing Peano’s Axioms it is worth keeping in mind that during Peano’s time, the concept of set was still nascent; it was Peano who introduced the symbol ∈\\in∈ in 188918891889 to denote “is an element of”. Being aware of Peano’s original formulation of these axioms helps us appreciate how far we have come in our mathematical journey and highlights that our journey towards better mathematical notation and abstraction still continues.\n\nThe axioms stated below appear between the pages 387−408387-408387−408 in the book “Historia Mathematica 1\\mathit{1}1” published in the year 197419741974.\n\nIn 189118911891, two years after the publication of his axioms for the natural numbers, Giuseppe Peano published an article titled “Sul concetto di numero” which translates to “On the Concept of Number,” in a journal he founded that same year. In this article, he reduced his list of axioms to five by eliminating the four axioms related to the equality relation “= ⁣ ⁣\"=\\!\\!\"=\".\n\nThe following are the five Peano’s axioms stated in the article :\n\nWe can see how the archaic notation obfuscates these axioms.\n\nThese maybe interpreted as :\n\nIn the Peano Axioms published in 1889and18911889 \\text{ and } 18911889and1891, the sequence of natural numbers began with 111, and the set of natural numbers was denoted by N\\mathit{N}N. However, in 189818981898 these axioms were modified so that the sequence began with 000 and the set was denoted by N ⁣0\\mathit{N_{\\!0}}N0.\n\nThe set of five Peano Axioms was increased to six in 190119011901 with the addition of the axiom – N ⁣0∈Cls\\mathit{N_{\\!0}} \\in \\text{Cls}N0∈Cls, i.e., the natural numbers form a class. With the addition of this last axiom, the axioms have received their final form, which are listed below.\n\nThe Axiomatization of Natural Numbers\n\nWe will first define the notion of equality as it pertains to natural numbers, and then we will formulate the Peano Axioms such that it provides an axiomatic definition of natural numbers.\n\nAlthough Giuseppe Peano omitted the four axioms related to the equality relation from his later formulation of the axioms regarding natural numbers, we will still discuss these axioms for the sake of completeness. The likely reason for this omission is because the concept of equality extends beyond natural numbers to mathematical objects in general and pertains more to the realm of logic, which specifies the conditions under which two mathematical objects can be considered equal.\n\nIn our discussion of Peano’s Axioms, we will adopt some conventions from its final form. Therefore, we will start with 000 instead of 111 in our axiomatic system and denote the set of natural numbers by N0\\mathbb{N_0}N0, where the subscript 000 reminds us that 000 is included.\n\nThe Notion of Equality\n\nBefore we define the set of natural numbers N0\\mathbb{N_0}N0 axiomatically, we will formalize the notion of equality through the four axioms of Peano, which establish the properties that the equality relation, denoted by ===, must satisfy.\n\nSuppose there exists a set N0\\mathbb{N_0}N0 that satisfies Axioms 111 t0 999 listed below.\n\nFirstly, every natural number should be equal to itself; this is called the reflexivity axiom.\n\nAxiom 1\\mathbf{1}1. For every x∈N0,x=xx \\in \\mathbb{N_0}, x = xx∈N0,x=x.\n\nSecondly, if one natural number equals a second one, then the second one should equal the first one. This is known as the symmetry axiom.\n\nAxiom 2\\mathbf{2}2. For every x,y∈N0,ifx=y,theny=xx, y \\in \\mathbb{N_0}, \\text{ if } x = y, \\text{ then } y = xx,y∈N0,ifx=y,theny=x.\n\nThirdly, if one natural number is equal to a second, and that second natural number is equal to a third, then the first and third are equal to each other. This is called the transitivity axiom.\n\nAxiom 3\\mathbf{3}3. For every x,y,z∈N0,ifx=yandy=z,thenx=zx, y, z \\in \\mathbb{N_0}, \\text{ if } x = y \\text{ and } y = z, \\text{ then } x = zx,y,z∈N0,ifx=yandy=z,thenx=z.\n\nThese three properties of reflexivity, symmetry and transitivity are applicable to any two mathematical objects that are related by the equality. As we have already discussed during our study of Set Theory, equality is an example of an equivalence relation, which is a type of homogeneous binary relation that satisfies the above three properties of reflexivity, symmetry and transitivity.\n\nSince the equality relation is defined generically for all mathematical objects and not just for natural numbers, we must make explicit the assumption that if two mathematical objects are equal—i.e., they satisfy the equality relation—and one of them is a natural number, then the other must also be a natural number. The next axiom makes this assumption explicit, namely, a natural number can only be equal to another natural number.\n\nFourthly, if any mathematical object is equal to a natural number, then that mathematical object is itself a natural number. This is called the closure of equality axiom.\n\nAxiom 4\\mathbf{4}4. For all xandy,ifx∈N0andx=y,theny∈N0x \\text{ and } y, \\text{ if } x \\in \\mathbb{N_0} \\text{ and } x = y, \\text{ then } y \\in \\mathbb{N_0}xandy,ifx∈N0andx=y,theny∈N0.\n\nThat is, the set of natural numbers is closed under equality.\n\nThe Peano Axioms\n\nWe will now discuss the five main Peano axioms that define the natural numbers. Peano aimed to formulate these axioms such that the fewest possible axioms could generate all the natural numbers. Therefore, we will construct the Peano axioms by checking, after stating each axiom, whether the axioms stated thus far can unambiguously result in the set of natural numbers that we know of. That is, we will continue constructing the Peano axioms until these axioms, when taken together, incontrovertibly result in N0={0,1,2,…}\\mathbb{N}_0 = \\{0, 1, 2, \\ldots\\}N0={0,1,2,…}.\n\nThis method of constructing the Peano axioms leads to the insight that the entire set of natural numbers can be generated by asserting the existence of at least one natural number and then defining a function called the successor function. This function takes a natural number as input and outputs another natural number, resulting in the construction of all remaining natural numbers.\n\nLet us now proceed with the construction of the Peano Axioms.\n\nSince we start counting from 000, it is unsurprising that 000 is the most obvious element to axiomatically include in the set of natural numbers.\n\nFifthly, 000 is a natural number.\n\nAxiom 5\\mathbf{5}5. 0∈N00 \\in \\mathbb{N_0}0∈N0.\n\nThus far we are only guaranteed the existence of a single natural number, 000.\n\nFrom 000 we should be able to generate the other natural numbers; that is starting with 000 we should be able to reach 111, from 111 reach 222 and so on, akin to counting. We can model this progression from one natural number to the next using a function that takes a natural number as input and produces another natural number as output. This function is called a successor function since it establishes a succession within the set of natural numbers and is written as S:N0→N0S : \\mathbb{N_0} \\rightarrow \\mathbb{N_0}S:N0→N0. The next axiom simply states that there is a function SSS whose domain and codomain are the set of natural numbers, N0\\mathbb{N}_0N0.\n\nSixthly, every natural number has a successor which is also a natural number.\n\nAxiom 6\\mathbf{6}6. If x∈N0x \\in \\mathbb{N}_0x∈N0, then S(x)∈N0S(x) \\in \\mathbb{N}_0S(x)∈N0.\n\nThat is, the set of natural numbers, N0\\mathbb{N}_0N0, is closed under the successor operation, SSS.\n\nAs this axiom implies, we will refer to S(x)S(x)S(x) as the successor of x\\mathit{x}x.\n\nTill now we have only established that the set of natural numbers contains 000 and its successor, S(0)S(0)S(0), where the function SSS takes natural numbers as input and outputs natural numbers. However, we are still quite far from having the set of natural numbers that we know of, since we could have N0={0}\\mathbb{N}_0 = \\{0\\}N0={0} and define S(0)=0S(0) = 0S(0)=0, which would still satisfy all of the above axioms. In this case, N0={0}\\mathbb{N}_0 = \\{0\\}N0={0}, but we want N0={0,1,2,3,…}\\mathbb{N}_0 = \\{0, 1, 2, 3, \\ldots\\}N0={0,1,2,3,…}.\n\nTo achieve this, we need to ensure that the successor function SSS does not output 000. Our next axiom will guarantee this by forbidding 000 from being the successor of any natural number, including itself.\n\nSeventhly, 000 is not  the successor of any natural number.\n\nAxiom 7\\mathbf{7}7. For every natural number x∈N0,S(x)≠0x \\in \\mathbb{N}_0, S(x) \\neq 0x∈N0,S(x)=0.\n\nThat means that there is no natural number whose successor is 000. Consequently, the preimage of 000 under SSS defined on the set of natural numbers is an empty set.\n\nAs a consequence of this axiom, we know that S(0)≠0S(0) \\neq 0S(0)=0. Therefore, S(0)S(0)S(0) must equal some other natural number, which we can denote by 111. Hence, we can define 111 by S(0)=1S(0) = 1S(0)=1.\n\nBased on axioms 5,6and75, 6 \\text{ and } 75,6and7 we are guaranteed the existence of at least two natural numbers, 0and10 \\text{ and } 10and1, but not necessarily others.\n\nFor example, we could define N0={0,1}\\mathbb{N}_0 = \\{0, 1\\}N0={0,1}, where S(0)=1andS(1)=1S(0) = 1 \\text{ and } S(1) = 1S(0)=1andS(1)=1. In this case, both natural numbers 0and10 \\text{ and } 10and1 have the same successor, which is 111.\n\nIf S(0)=1andS(1)=1S(0) = 1 \\text{ and } S(1) = 1S(0)=1andS(1)=1, then the two natural numbers 0and10 \\text{ and } 10and1 have the same successor, 111. This set of natural numbers together with the successor function SSS defined on it would still satisfy all the above axioms. However, if we stop here, the axioms constructed thus far do not guarantee the existence of the rest of the natural numbers that we know of, namely, 2,3,4…2, 3, 4 \\ldots2,3,4….\n\nTherefore, our next axiom should ensure that different natural numbers have different successors. This means that every natural number is the successor of at most one natural number (since 000 is not the successor of any natural number) which implies that SSS must be an injective function.\n\nEighthly, no two natural numbers have the same successor unless they are equal.\n\nAxiom 8\\mathbf{8}8. For all x,y∈N0x, y \\in \\mathbb{N}_0x,y∈N0, if S(x)=S(y)S(x) = S(y)S(x)=S(y), then x=yx = yx=y.\n\nThis axiom leads to some important consequences. It excludes the possibility of defining N0\\mathbb{N}_0N0 to be just {0,1}\\{0, 1\\}{0,1}. We already have S(0)=1S(0) = 1S(0)=1 and since SSS is an injective function, we cannot have S(1)=1S(1) = 1S(1)=1. Axiom 777 excludes the possibility that S(1)=0S(1) = 0S(1)=0. Thus S(1)S(1)S(1) must be some other natural number, which we denote as 222. Therefore, we can define 2=S(1)2 = S(1)2=S(1).\n\nBy a similar argument, S(2)S(2)S(2) cannot be 0,1or20, 1 \\text{ or } 20,1or2. Hence, it must be some other natural number, which we denote as 333. Continuing this way, we see that N0\\mathbb{N}_0N0 must contain all the natural numbers that we know of.\n\nSo far we have established that N0\\mathbb{N}_0N0 must include 000, its successor 1=S(0)1 = S(0)1=S(0), its successor’s successor 2=S(1)2 = S(1)2=S(1) and so on. Thus N0\\mathbb{N}_0N0 must include 0,S(0),S(S(0)),S(S(S(0))),…0, S(0), S(S(0)), S(S(S(0))), \\ldots0,S(0),S(S(0)),S(S(S(0))),…. In order to avoid so many nested applications of SSS we use the numerals 1,2,31, 2, 31,2,3 to denote S(0),S(S(0))andS(S(S(0)))S(0), S(S(0)) \\text{ and } S(S(S(0)))S(0),S(S(0))andS(S(S(0))), respectively.\n\nThese first eight axioms have resulted in the definition of N0\\mathbb{N}_0N0 to include all the natural numbers that we know of.\n\nTherefore, so far we only know that\n\n{0,1,2,…}⊂N0\\{0, 1, 2, \\ldots\\} \\subset \\mathbb{N}_0{0,1,2,…}⊂N0\n\nAt this point, it is interesting to ask whether our axiomatic definition of N0\\mathbb{N}_0N0 precludes the inclusion of additional elements.\n\nIn order to answer this question, let us consider a version of N0\\mathbb{N}_0N0 that satisfies all the above axioms but is not the usual set of natural numbers that we know of. That is,\n\nN0={0,1,2,3,…}∪{!,−}\\mathbb{N}_0 = \\{0, 1, 2, 3, \\ldots\\} \\cup \\{!, -\\}N0={0,1,2,3,…}∪{!,−}\n\nAs can be seen, this version of N0\\mathbb{N}_0N0 contains all the natural numbers and also includes two other symbols, !and−! \\text{ and } -!and−.\n\nWe will next define the successor function defined on this set. For the subset {0,1,2,3,…}\\{0, 1, 2, 3, \\ldots\\}{0,1,2,3,…} of N0\\mathbb{N}_0N0, we define SSS as we have described above i.e., S(0)=1,S(1)=2,S(2)=3S(0) = 1, S(1) = 2, S(2) = 3S(0)=1,S(1)=2,S(2)=3 and so on. For the subset {!,−}\\{!, -\\}{!,−} of N0\\mathbb{N}_0N0, we define S(!)=−andS(−)=!S(!) = - \\text{ and } S(-) = !S(!)=−andS(−)=!.\n\nThis version of N0\\mathbb{N}_0N0 with this successor function satisfies all the axioms, but it has more elements than what we want our set of natural numbers to have. This is shown below.\n\nSimilarly, there could be other versions of N0\\mathbb{N}_0N0 with different successor functions, each of which satisfies all of the above axioms from 5through85 \\text{ through } 85through8 but could also have elements other than natural numbers. This is illustrated below.\n\nBased on axioms 5to85 \\text{ to } 85to8, the set N0\\mathbb{N}_0N0 of natural numbers satisfies the following conditions:\n\n0∈N00 \\in \\mathbb{N}_00∈N0; and\n\nif x∈N0,thenS(x)∈N0x \\in \\mathbb{N}_0,\\text{then } S(x) \\in \\mathbb{N}_0x∈N0,thenS(x)∈N0, where S(x)S(x)S(x) denotes the successor of xxx.\n\nThis way of defining a set, where a base clause specifies the basic element of the set and an inductive clause details how to generate additional elements, is called an inductive definition of the set, and such a set is referred to as an inductive set.\n\nTherefore, the axioms 5to85 \\text{ to } 85to8 only ensure that {0,1,2,…}⊂N0\\{0, 1, 2, \\ldots\\} \\subset \\mathbb{N}_0{0,1,2,…}⊂N0, where N0\\mathbb{N}_0N0 is any set such that 0∈N00 \\in \\mathbb{N}_00∈N0 and if x∈N0,thenS(x)∈N0x \\in \\mathbb{N}_0,\\text{then } S(x) \\in \\mathbb{N}_0x∈N0,thenS(x)∈N0.\n\nHowever, as discussed earlier and shown in the diagram above, this definition of N0\\mathbb{N}_0N0 does not exclude elements other than natural numbers from being contained in the set.\n\nTo ensure that only natural numbers are included in the set N0\\mathbb{N}_0N0, i.e., N0={0,1,2,…}\\mathbb{N}_0 = \\{0, 1, 2, \\ldots\\}N0={0,1,2,…}, our next (and final) axiom should declare N0\\mathbb{N}_0N0 to be the minimal set that satisfies axioms 5through85 \\text{ through } 85through8; specifically, N0\\mathbb{N}_0N0 is the intersection of all sets that satisfy these axioms.\n\nSince axiom 999 uses the inductive definition of a set in its construction, it is referred to as the Axiom of Induction.\n\nWe will now state our ninth and final axiom.\n\nAxiom 9\\mathbf{9}9 (Axiom of Induction). If T⊂N0T \\subset \\mathbb{N}_0T⊂N0 is such that:\n\n0∈T0 \\in T0∈T; and\n\nx∈T  ⟹  S(x)∈Tx \\in T \\implies S(x) \\in Tx∈T⟹S(x)∈T for all x∈N0x \\in \\mathbb{N}_0x∈N0,\n\nthen T=N0T = \\mathbb{N}_0T=N0.\n\nAs we have already discussed, the axioms 5through85 \\text{ through } 85through8 ensure that {0,1,2,… }⊂N0\\{0, 1, 2, \\dots\\} \\subset \\mathbb{N}_0{0,1,2,…}⊂N0. Suppose T={0,1,2,… }T = \\{0, 1, 2, \\dots\\}T={0,1,2,…}. We can see that 0∈Tandx∈T  ⟹  S(x)∈Tforallx∈N00 \\in T \\text{ and } x \\in T \\implies S(x) \\in T \\text{ for all } x \\in \\mathbb{N}_00∈Tandx∈T⟹S(x)∈Tforallx∈N0. Therefore, by Axiom 999, {0,1,2,… }=N0\\{0, 1, 2, \\dots\\} = \\mathbb{N}_0{0,1,2,…}=N0.\n\nThus we finally have the set of natural numbers that we know of, namely, N0={0,1,2,… }\\mathbb{N}_0 = \\{0, 1, 2, \\dots\\}N0={0,1,2,…}.\n\nAlternate Formulations of Axiom of Induction: Set-Based and  Predicate-Based Perspectives\n\nFor the sake of completeness, we will discuss two alternate ways to formulate Peano’s Ninth Axiom, namely the Axiom of Induction.\n\nThe Axiom of Induction is a cornerstone of number theory, providing a powerful tool for proving statements about natural numbers. It can be expressed in two equivalent forms: set-based and predicate-based. Each form offers a different perspective on the same fundamental principle.\n\nSet-Based Axiom of Induction: Focusing on Subsets\n\nAxiom 9\\mathbf{9}9 (Axiom of Induction). If T⊂N0T \\subset \\mathbb{N}_0T⊂N0 is a set such that:\n\n0∈T0 \\in T0∈T; and\n\nx∈T  ⟹  S(x)∈Tx \\in T \\implies S(x) \\in Tx∈T⟹S(x)∈T for all x∈N0x \\in \\mathbb{N}_0x∈N0,\n\nthen N0⊂T\\mathbb{N}_0 \\subset TN0⊂T.\n\nIn simpler terms, if a subset of natural numbers contains 000 and is closed under the successor operation (meaning that if a natural number is in the subset, its successor is also in the subset), then that subset must contain all natural numbers.\n\nAs discussed above, Axioms 5through85 \\text{ through } 85through8 ensure that {0,1,2,…}⊂N0\\{0, 1, 2, \\ldots\\} \\subset \\mathbb{N}_0{0,1,2,…}⊂N0. Additionally, we observe that the set {0,1,2,…}\\{0, 1, 2, \\ldots\\}{0,1,2,…} satisfies the following two conditions of Axiom 999:\n\nIt contains 000; and\n\nWhenever it contains an element xxx, it also contains its successor, namely, S(x)S(x)S(x).\n\nTherefore, by Axiom 999, it follows that N0⊂{0,1,2,…}\\mathbb{N}_0 \\subset \\{0, 1, 2, \\ldots\\}N0⊂{0,1,2,…}.\n\nSince Axioms 5through85 \\text{ through } 85through8 ensure that {0,1,2,…}⊂N0\\{0, 1, 2, \\ldots\\} \\subset \\mathbb{N}_0{0,1,2,…}⊂N0 and by Axiom 999 we have show that N0⊂{0,1,2,…}\\mathbb{N}_0 \\subset \\{0, 1, 2, \\ldots\\}N0⊂{0,1,2,…}, it follows that N0={0,1,2,…}\\mathbb{N}_0 = \\{0, 1, 2, \\ldots\\}N0={0,1,2,…}.\n\nThis perspective emphasizes the structure of the subset itself. We are concerned with the elements that are members of the subset and how they relate to each other through the successor function.\n\nPredicate-Based Axiom of Induction: Focusing on Properties\n\nPeano’s axioms five through eight collectively define a superset of natural numbers, specifically {0,1,2,…}⊂N0\\{0, 1, 2, \\ldots\\} \\subset \\mathbb{N}_0{0,1,2,…}⊂N0. To ensure that this set N0\\mathbb{N}_0N0 includes only natural numbers, Peano’s ninth axiom can also be formulated as the principle of mathematical induction over natural numbers. This formulation is equivalent to the axiom of induction and serves the same purpose of removing unwanted elements from the superset N0\\mathbb{N}_0N0, ensuring that it contains only natural numbers.\n\nThe predicate-based form of the Axiom of Induction shifts the focus from subsets to properties (expressed as predicates) that natural numbers may or may not possess.\n\nThe reformulation of Axiom 999 in terms of predicates results in the Principle of Mathematical Induction which is stated as follows.\n\nAxiom 9\\mathbf{9}9 (Principle of Mathematical Induction). For any predicate P(n)P(n)P(n), where nnn is a natural number, if:\n\nP(0)P(0)P(0) is true, and\n\nfor every natural number nnn, P(n)P(n)P(n) being true implies that P(S(n))P(S(n))P(S(n)) is also true,\n\nthen P(n)P(n)P(n) is true for every natural number nnn.\n\nHere, we are concerned with a property (((represented by the predicate P(n))P(n))P(n)) that natural numbers might possess. If 000 has the property, and if natural number nnn having the property implies that its successor S(n)S(n)S(n) also has the property, then all natural numbers must have the property.\n\nThis perspective emphasizes the properties of individual natural numbers. We are concerned with whether a given natural number has a specific property.\n\nEquivalence and Connection\n\nThe set-based and predicate-based forms are logically equivalent, meaning they express the same fundamental principle. This equivalence is rooted in the relationship between subsets and predicates, as established by the Axiom of Separation.\n\nFrom Subset to Predicate:\n\nGiven T⊂N0T \\subset \\mathbb{N}_0T⊂N0, it implies from the Axiom of Separation that there exists a predicate P(n)P(n)P(n) such that P(n)P(n)P(n) is true if and only if n∈Tn \\in Tn∈T.\n\nFrom Predicate to Subset:\n\nGiven a predicate P(n)P(n)P(n) and a set N0\\mathbb{N}_0N0, we can define a subset T={n∈N0 ∣ P(n)istrue}T = \\{n \\in \\mathbb{N}_0 \\,|\\, P(n) \\text{ is true}\\}T={n∈N0∣P(n)istrue}, by the Axiom of Separation.\n\nThus, the subset and predicate perspectives are simply two ways of expressing the same fundamental idea. The set-based form emphasizes the elements within a collection, while the predicate-based form emphasizes the properties of individual elements. The Axiom of Separation is the bridge that allows us to move seamlessly between these perspectives.\n\nWhy Both Forms Are Useful\n\nBoth forms of the Axiom of Induction are valuable tools in mathematical proofs. The set-based form is often used in set theory and related areas, while the predicate-based form is commonly used in number theory and other branches of mathematics where properties of numbers are the main focus.\n\nEssentially, they are two sides of the same coin, and which one to use depends on the context of the problem and the preference of the mathematician.\n\nMethod of Definition of Natural Numbers using Peano’s Axioms\n\nIt should be noted that Peano’s Axioms only describe how to construct the set of natural numbers and do not define what natural numbers are intrinsically. A particular natural number is given when its generation under the inductive definition is given.\n\nFor example, the natural number 222 is defined as that mathematical object which is obtained by starting with the initial object 000 and applying the successor function once and then again i.e., 222 represents S(S(0))S(S(0))S(S(0)).\n\nExistence of the Set of Natural Numbers satisfying Peano’s Axioms\n\nHow do we establish the existence of a set, an element of that set, and a function from the set to itself, that satisfy Peano’s Axioms? These axioms themselves are insufficient to prove this existence. Consequently, there are two approaches to resolving this matter.\n\nOne common approach in mathematics is to take something as  axiomatic and then use it as the basis upon which we prove all our other results. Hence, such an approach requires us to be satisfied with taking the existence of a set satisfying Peano’s Axioms axiomatically. This axiom is called the existence axiom for natural numbers and guarantees that there exists a set with the properties that Peano’s axioms ascribe to it.\n\nThe statement of this axiom is as follows:\n\nExistence Axiom for Natural Numbers: There exists a set N0\\mathbb{N}_0N0 satisfying Axioms 1through91 \\text{ through } 91through9.\n\nAlternatively, if we use the Zermelo-Fraenkel Axioms as our foundation for set theory, we can prove that something satisfying Peano’s Axioms exists, so we don’t need to assume it separately. We’ll show how this is done later.\n\nProving Properties of Natural Numbers\n\nHaving established the existence of the natural numbers and their fundamental properties as defined by Peano’s Axioms, we will next explore how to rigorously prove that the natural numbers satisfy any specific property using the Axiom of Induction.\n\nNext Topic: Proving Properties of Natural Numbers Using Proof by Induction\n\nShare this:Click to share on Facebook (Opens in new window)Click to share on X (Opens in new window)Click to email a link to a friend (Opens in new window)\n\n\t\t\t\t\t←\n\t\t\t\t\tFormulas and Free Variables\n\n\t\t\t\t\t→\n\t\t\t\t\t\tNumber Theory Primer : Proving Properties of Natural Numbers Using Proof by Induction",
    "summary": {
      "en": "**Summary of Peano's Axioms and Natural Numbers**\n\nThe study of natural numbers starts with Peano's Axioms, which provide a foundational framework for defining these numbers without relying on the intuitive idea of counting. Natural numbers (0, 1, 2, 3, etc.) are essential for counting objects in the real world. \n\n1. **Purpose of Axioms**: The goal of an axiomatic system is to make minimal assumptions while proving as much as possible about natural numbers. Peano's Axioms achieve this by defining natural numbers through a few essential properties.\n\n2. **Peano's Axioms**: Originally formulated by Giuseppe Peano in the late 19th century, the standard version consists of nine axioms that describe the properties of natural numbers. These include:\n   - 0 is a natural number.\n   - Each natural number has a successor, which is also a natural number.\n   - 0 is not the successor of any natural number.\n   - Different natural numbers have different successors.\n   - Axiom of Induction: If a set contains 0 and is closed under the successor operation, then it contains all natural numbers.\n\n3. **Successor Function**: The successor function S takes a natural number and produces the next number in the sequence. For example, S(0) = 1, S(1) = 2, and so on.\n\n4. **Inductive Definition**: The natural numbers can be defined inductively, meaning we start with a base case (0) and generate further numbers using the successor function.\n\n5. **Existence of Natural Numbers**: The existence of a set of natural numbers that satisfies these axioms can be assumed axiomatically or proven using the Zermelo-Fraenkel Axioms of set theory.\n\n6. **Importance of Induction**: The Axiom of Induction is crucial for proving properties about natural numbers, allowing mathematicians to establish truths about all natural numbers based on a few base cases.\n\nIn summary, Peano's Axioms provide a rigorous framework for understanding and proving properties about natural numbers, forming the foundation for much of number theory.",
      "ko": "자연수에 대한 연구는 페아노 공리에서 시작됩니다. 이 공리는 자연수를 정의하는 기본적인 틀을 제공하며, 직관적인 수 세기 개념에 의존하지 않습니다. 자연수(0, 1, 2, 3 등)는 실제 세계의 사물을 세는 데 필수적입니다.\n\n공리의 목적은 자연수에 대해 가능한 한 많은 것을 증명하면서 최소한의 가정을 하는 것입니다. 페아노 공리는 몇 가지 필수적인 속성을 통해 자연수를 정의함으로써 이 목표를 달성합니다.\n\n페아노 공리는 19세기 후반 주세페 페아노에 의해 처음 제정되었습니다. 표준 버전은 자연수의 속성을 설명하는 아홉 개의 공리로 구성되어 있습니다. 여기에는 0이 자연수이며, 각 자연수는 다음 자연수를 가지며, 0은 어떤 자연수의 다음 수가 아니고, 서로 다른 자연수는 서로 다른 다음 수를 가지며, 귀납 공리에 따르면 0을 포함하고 다음 수 연산에 대해 닫힌 집합은 모든 자연수를 포함한다는 내용이 포함됩니다.\n\n다음 수 함수 S는 자연수를 입력받아 그 다음 수를 생성합니다. 예를 들어, S(0) = 1, S(1) = 2와 같이 이어집니다.\n\n자연수는 귀납적으로 정의될 수 있습니다. 즉, 기본 사례인 0에서 시작하여 다음 수 함수를 사용해 더 많은 수를 생성합니다.\n\n이 공리를 만족하는 자연수 집합의 존재는 공리적으로 가정하거나 집합론의 제르멜로-프레넬 공리를 사용하여 증명할 수 있습니다.\n\n귀납 공리는 자연수에 대한 속성을 증명하는 데 매우 중요합니다. 이를 통해 수학자들은 몇 가지 기본 사례를 바탕으로 모든 자연수에 대한 진리를 확립할 수 있습니다.\n\n결론적으로, 페아노 공리는 자연수에 대한 속성을 이해하고 증명하는 데 필요한 엄격한 틀을 제공하며, 이는 수론의 많은 부분의 기초를 형성합니다.",
      "ja": "自然数の研究は、ペアノの公理から始まります。これらの公理は、数を数えるという直感的な考えに頼ることなく、自然数を定義するための基本的な枠組みを提供します。自然数（0, 1, 2, 3など）は、現実世界の物体を数えるために不可欠です。\n\n公理の目的は、自然数についてできるだけ多くのことを証明するために、最小限の仮定を置くことです。ペアノの公理は、いくつかの基本的な性質を通じて自然数を定義することで、これを達成しています。\n\nペアノの公理は、19世紀後半にジュゼッペ・ペアノによって最初に提唱され、標準的なバージョンは自然数の性質を説明する9つの公理から成り立っています。これには、0は自然数であること、各自然数には次の自然数が存在すること、0はどの自然数の次でもないこと、異なる自然数は異なる次の数を持つこと、そして帰納法の公理があります。帰納法の公理は、ある集合が0を含み、次の数の操作に対して閉じているなら、その集合はすべての自然数を含むというものです。\n\n次の数を求める関数である「後続関数」は、自然数を受け取り、その次の数を生成します。例えば、S(0)は1、S(1)は2というように続きます。\n\n自然数は帰納的に定義することができ、これは基底ケース（0）から始め、後続関数を使ってさらに数を生成することを意味します。\n\nこれらの公理を満たす自然数の集合の存在は、公理的に仮定することも、集合論のゼルメロ・フレンケル公理を用いて証明することもできます。\n\n帰納法の公理は、自然数に関する性質を証明するために重要です。これにより、数学者はいくつかの基底ケースに基づいてすべての自然数に関する真実を確立することができます。\n\nペアノの公理は、自然数の性質を理解し証明するための厳密な枠組みを提供し、数論の多くの基礎を形成しています。"
    }
  },
  {
    "id": "7dab4e26703d5c25",
    "title": {
      "en": "Mastering Delphi 5 2025 Annotated Edition Is Now Complete",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://blog.marcocantu.com/blog/2025-march-mastering-delphi5-annotated-complete.html",
    "score": 185,
    "by": "mariuz",
    "time": 1742830808,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "0894f185902b17cb",
    "title": {
      "en": "Project Operation Whitecoat (2010)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://scholarworks.lib.csusb.edu/cgi/viewcontent.cgi?article=1201&context=history-in-the-making",
    "score": 102,
    "by": "smegger001",
    "time": 1742834107,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "84af1fd35b76d30e",
    "title": {
      "en": "The Wright brothers invented the airplane, right? Not if you're in Brazil",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.washingtonpost.com/world/2025/03/21/brazil-airplane-wright-brothers-santos-dumont/",
    "score": 232,
    "by": "benbreen",
    "time": 1742608551,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "361dc226f099cf47",
    "title": {
      "en": "Rickover's Lessons",
      "ko": "릭오버의 교훈",
      "ja": "リコバーの教訓"
    },
    "type": "story",
    "url": "https://www.chinatalk.media/p/rickovers-lessons-how-to-build-a",
    "score": 113,
    "by": "pepys",
    "time": 1742703526,
    "content": "Share this postChinaTalkRickover’s LessonsCopy linkFacebookEmailNotesMoreDiscover more from ChinaTalkDeep coverage of technology, China, and US policy. We feature original analysis alongside interviews with leading thinkers and policymakers.Over 50,000 subscribersSubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.Already have an account? Sign inRickover’s Lessons\"The status quo has no absolute sanctity\"Lily OttingerMar 20, 202563Share this postChinaTalkRickover’s LessonsCopy linkFacebookEmailNotesMore1212ShareCharles Yang is the executive director for the Center for Industrial Strategy, a bipartisan think tank focused on industrial policy. Previously, he served as an AI and Supply Chain Policy Advisor at the Department of Energy and was an ML Engineer at an AI hardware startup in San Francisco. Today, he’s here to present some excerpts from his research into how Admiral Hyman Rickover built the nuclear navy.Strategic competition demands more than technological innovation — it requires building industrial power. The U.S. is realizing the damage done by decades of underinvestment in the nation’s industrial base, which now jeopardizes its ability to compete on the global stage. Today, the production capacity of Chinese shipyards is over 200 times that of US shipyards, and China has used its chokehold on critical mineral processing as leverage to retaliate against US sanctions.A new bipartisan consensus is emerging around the need for industrial policy — from the passage of the CHIPS and Science Act, to the recent bipartisan introduction of the SHIPS for America Act and the Critical Minerals for the Future Act.As Congress steps into this more active role, policymakers should learn from the successes of our past. Nearly 75 years ago, Admiral Hyman G. Rickover, “Father of the Nuclear Navy”, pioneered a bold program to develop and operationalize nuclear power in the Navy. Under his leadership, the U.S. government harnessed the power of the atom, building the world’s first nuclear-powered submarine and the world’s largest fleet of nuclear reactors for civilian power.Lessons From the PastRickover spent his entire career in the Navy and is still the longest-serving naval officer in US history. He spent the first 20 years of his career as an electrical engineer, where he honed a strong technical foundation and unique management style. In 1946, he was assigned a 1-year tour of duty at the Oak Ridge site of the Manhattan Project. Rickover immediately recognized the transformative potential of nuclear technology — he spent the rest of his career building the “Nuclear Navy,” which ensured US strategic dominance of the high seas for the rest of the 20th century.Within the span of 10 years, Rickover created an entire office dedicated to nuclear propulsion, and successfully launched the world’s first nuclear-powered submarine without cost overruns. He conclusively demonstrated the strategic importance of nuclear propulsion in a timeframe no one thought possible and helped the US beat the Soviets to nuclear propulsion for submarines by 3 years. His institutional legacy is the US Navy’s safe construction and operation of nuclear reactors.As the US gears up for another strategic competition, Rickover’s story can offer helpful lessons for aspiring technocrats. Oftentimes, industrial policy is framed in terms of legislation, but Rickover demonstrates that industrial policy is as much about policy as it is about strong leadership.USS Enterprise, the world’s first nuclear-powered aircraft carrier. Source.Talent, Training, and ManagementRickover spent an inordinate amount of time focused on interviewing personnel — he made the final hiring decision for every naval officer who applied to serve on a nuclear submarine until he retired. And he was an unorthodox interviewer, screening for high agency individuals who could think on their feet — literally! To test their composure, Rickover famously made candidates sit in a chair with the front two legs shortened as he loomed over them during questioning.The infamous “Rickover chair”One interview account:For one interviewee who said he liked hiking, Rickover asked him if he had ever hiked the nearby “Goat Mountain”. When he said he had not, Rickover told him to bring him proof he had climbed it by tomorrow morning and he would be hired. It turns out that Goat Mountain was the peak of a structure for mountain goats in a zoo. He went to the zoo, asked a tourist to take his picture, jumped into the enclosure, and climbed to the top. He’s hired the next day!But it didn’t end at the interview process — Rickover believed in continued technical training for his staff and in building out a talented workforce base for this new technology:While Rickover worked to staff up quickly in the short term, he also set out to build a deep bench and a long-term pipeline of talent. He required each officer and engineer he hired to submit a self-study plan demonstrating mastery of advanced texts in metallurgy, physics, and chemistry, along with field trips to AEC facilities, totaling 854 hours of study or 16 hours per week. He also worked with MIT to develop a survey course on nuclear physics and a master's degree in nuclear engineering, with a curriculum drawn up and agreed to by Rickover, starting in June of 1949. Rickover also worked with Oak Ridge National Lab to develop a 1-year curriculum in nuclear science and technology, a program christened “Oak Ridge School of Reactor Technology (ORSORT) with the first cohort starting in March 1950. Westinghouse, GE, utilities, naval and private shipyards, and Naval Reactors all sent students to ORSORT and the program started turning out ~100 graduates a year, providing another training center to develop a nuclear industry.1 Finally, Rickover had his engineers provide training lectures to a variety of audiences, ranging from senior officials in BuShips to junior technicians, as well as to explain shipboard problems and applications to scientists at Argonne, Oak Ridge, and Westinghouse/GE.He was also known for his unique style of management. Not only did he interview every naval officer in his office, he also maintained direct lines of communication with every nuclear sub commander and project officer on-site with contractors, giving him early awareness of every issue. The demanding oversight he extended over his technical staff under his command pushed them to have greater awareness of their own direct reports:Rickover was also an intensely demanding and scrutinizing manager. As most writing then was done on carbon copy paper, every night Rickover would collect the “pinks” of every piece of writing from his various teams i.e. the carbon copied half, and read over them at home, including drafts. When his officers protested as to how they should be expected to keep track of everything in their purview, including drafts reports from staff below them, Rickovers responded “It’s up to you to see that I don’t know more about what’s going on in your shop than you do”. By enforcing tight lines of supervision over his officers, Rickover ensured that he maintained full visibility into each team, including the project facilities at Knolls, Bettis, and the shipyards, allowing him to catch problems early on. It also enforced a culture of direct accountability and oversight across the organization.Rickover’s focus on hiring, training, and close project management represented his philosophical approach to how to build complex systems managed by humans.Near the end of his career, Rickover testified to Congress after the Three Mile Island Reactor accident. He spent the vast majority of his testimony talking not about regulatory reform, but about the lack of training and inadequate culture of responsibility among the operators.“Human experience shows that people, not organizations or management systems, get things done. For this reason, subordinates must be given authority and responsibility early in their careers…Complex jobs cannot be accomplished effectively with transients. A manager must make the work challenging and rewarding so that his people will remain with the organization for many years. This allows it to benefit fully from their knowledge, experience, and corporate memory.”~ Hyman Rickover, 1982Industrial State CapacityRickover’s scrutinizing style of management extended to the private companies he worked with. He pioneered the practice of project officers, who lived on-site at the projects and who would report directly to him any delays or unforeseen issues, so that Rickover could escalate immediately and ensure the project remained on track.Government contracting was, and still is today, a largely passive and administrative activity. While Rickover acknowledged that the government was the “customer” and the contractor was the one responsible for delivering, Rickover’s unique approach to program management was exercising tight oversight over the contractors. Rickover hired technical experts into his office and then sent them out as project officers to oversee the various contractor sites. There, the project officer was expected to be the active representative of the Naval Reactors Office, reporting directly to Rickover any issues with contractors and ensuring the contractor was on track to deliver the product as expected. In every sense, Rickover’s project officer was to be his eyes and ears on the ground. Rickover took great pains to ensure there was no customer capture, telling one of his project officers, “Don’t go to dinner with them. Your wives must not get friendly with their wives. You’re not even to let your dogs get friendly with their dogs…when you do that, you become one of them…you don’t represent me anymore”.Rickover’s success in scaling industrial technology was demonstrated early on with Zirconium production. In 1949, the world had only produced a shoebox worth of purified Zirconium, but the material showed promise as a fuel cladding material due to its durability under high temperatures without blocking the emitted neutrons needed to enable fission reactions. AEC opened up a simple contract for private companies to bid to produce Zirconium, but none of the companies were able to scale up production. Rickover took over production a year later, applied his practice of close project management with the (now defunct) Bureau of Mines, and only then passed it off to industry:But by 1949, when Rickover was looking to scale up promising fuel cladding material production, the AEC had already decided to run contracts through another AEC division. Unable to exert the centralized control over the contractors, the AEC manufacturers were slow to scale up a high-quality production process. In 1950, after a year of delay, Rickover finally received permission to have the Westinghouse Bettis site directly manufacture Zirconium metal and worked with the Bureau of Mines (BuMines) to purify the Zirconium. Under Rickover’s scrutiny, Bettis scaled a novel purification process to thousands of tons of production capacity. Rickover opened up contract bids for Zirconium only after having derisked this novel technology. When the Secretary of the Navy later asked Westinghouse how they managed to scale up this process, the response he got was “Rickover made us do it”.“The man in charge must concern himself with details. If he does not consider them important, neither will his subordinates.”~ Hyman Rickover, 1982Bureaucratic InnovationBuilding big things requires lots of people. Rickover was not only an exceptional manager of people and deeply technical, but his 20-year naval career before Oak Ridge taught him how to wrangle government bureaucracy — and discern which rules mattered and which didn’t. For example, Rickover was interviewing an officer who thought the monthly reports on the gasoline usage of his base’s motorboats were pointless and wasteful. Rickover told him to simply remove the tickler file that tracked the reports from the boss’s secretary file and to send over a note the next day alerting Rickover that the task had been completed. The interviewee did and was hired.SubscribeRickover’s bureaucratic skill is exemplified by his success in rallying the Navy behind the nuclear-powered submarine. He believed this was a feasible, near-term project, despite widely-held convictions to the contrary — including those of the Atomic Energy Commission (AEC). Even Robert Oppenheimer (who served as one of the first AEC commissioners) doubted nuclear propulsion early on.In light of initial resistance from the civilian AEC, Rickover formulated a unique bureaucratic innovation to position himself within two chains of command — one within the Navy and the other within the AEC.Rickover was also able to realize his bureaucratic innovation to occupy a spot on the org chart both at AEC and in the Navy BuShips, something he first formulated while at Oak Ridge. This way, if the AEC refused something, he could respond that “this is a priority for the Navy” and vice versa. Similar to how the Manhattan Project reduced risk by pursuing parallel technological approaches, Rickover would reduce his bureaucratic risk by pursuing parallel chains of command. This unique structure lives on to this day, with Naval Reactors shared between the semi-autonomous National Nuclear Safety Administration (NNSA) in the Department of Energy (DOE) and the Navy.Joint org chart for AEC and Navy nuclear reactor efforts“The status quo has no absolute sanctity under our form of government. It must constantly justify itself to the people in whom is vested ultimate sovereignty over this nation”~ Hyman RickoverRickover firmly believed that the right team and the right culture could build incredible industrial technologies at scale, even within the government. While discourse in Washington DC often focuses on regulations or money, Rickover’s life brings a uniquely human-centered view of industrial policy: one that recognizes the importance of state capacity, technical personnel, and most importantly, public leaders with the vision and drive to build technology.You can read the full story of Rickover and how he built the world’s first nuclear-powered submarine on Charles’s substack.Rough DraftsHow Hyman Rickover Built the Nuclear NavyThere is a common misconception that the United States stopped building nuclear reactors after the 1970s. But that is not the full story: the US Navy has consistently built at least one small nuclear reactor every year since the 1950s. The fact that the Navy has managed to safely design, build, and operate nuclear reactors for decades, despite the stagn…Read more22 days ago · 15 likes · 1 comment · Charles YangSubscribe to ChinaTalkHundreds of paid subscribersDeep coverage of technology, China, and US policy. We feature original analysis alongside interviews with leading thinkers and policymakers.SubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.63 Likes∙12 Restacks63Share this postChinaTalkRickover’s LessonsCopy linkFacebookEmailNotesMore1212Share",
    "summary": {
      "en": "The article discusses the lessons learned from Admiral Hyman Rickover, known as the \"Father of the Nuclear Navy,\" and emphasizes the importance of strong industrial policy and leadership in the U.S. Rickover’s work in developing the nuclear-powered navy showcased how strategic competition requires not just technological innovation but also a robust industrial base.\n\nKey points include:\n\n1. **Industrial Policy Importance**: The U.S. has suffered from underinvestment in its industrial base, making it difficult to compete globally, especially against China, which has significantly more production capacity.\n\n2. **Rickover's Leadership**: Rickover's success in creating the nuclear navy stemmed from his technical expertise, hands-on management style, and emphasis on hiring and training talented personnel. He personally interviewed every officer for nuclear submarines and stressed continued education.\n\n3. **Management Style**: He maintained strict oversight and accountability, ensuring that his teams were well-informed and prepared to handle complex challenges. He believed in giving authority and responsibility to his subordinates early in their careers.\n\n4. **Bureaucratic Innovation**: Rickover skillfully navigated government bureaucracy, using dual command structures to advocate for nuclear propulsion within both the Navy and the Atomic Energy Commission, which helped him overcome initial resistance to his ideas.\n\n5. **Human-Centered Approach**: Rickover's philosophy was that people, not just systems, drive success in complex projects. He believed in fostering a culture that values talent and innovation.\n\nOverall, Rickover's approach offers valuable insights for current policymakers as they seek to enhance U.S. industrial capacity and compete effectively on the global stage.",
      "ko": "이 기사는 \"핵 해군의 아버지\"로 알려진 하이먼 리코버 제독의 교훈을 다루고 있으며, 미국에서 강력한 산업 정책과 리더십의 중요성을 강조합니다. 리코버의 핵 추진 해군 개발 작업은 전략적 경쟁이 기술 혁신뿐만 아니라 견고한 산업 기반도 필요하다는 것을 보여주었습니다.\n\n첫 번째로, 산업 정책의 중요성이 강조됩니다. 미국은 산업 기반에 대한 투자가 부족해져 글로벌 경쟁에서 어려움을 겪고 있으며, 특히 생산 능력이 훨씬 더 큰 중국과의 경쟁이 힘들어지고 있습니다.\n\n리코버의 리더십도 중요한 요소입니다. 그는 핵 해군을 창설하는 데 있어 기술적 전문성과 실질적인 관리 스타일, 그리고 인재 채용과 교육에 대한 강조로 성공을 거두었습니다. 그는 핵 잠수함의 모든 장교를 직접 면접하고 지속적인 교육의 중요성을 강조했습니다.\n\n그의 관리 스타일은 엄격한 감독과 책임감을 유지하는 것이었습니다. 그는 팀이 복잡한 문제를 잘 처리할 수 있도록 정보를 충분히 제공하고 준비시키는 데 신경 썼습니다. 또한, 그는 부하 직원들에게 경력 초기부터 권한과 책임을 부여하는 것을 믿었습니다.\n\n리코버는 정부의 관료제를 능숙하게 다루며, 해군과 원자력 위원회 내에서 핵 추진을 지지하기 위해 이중 지휘 체계를 활용했습니다. 이를 통해 그는 자신의 아이디어에 대한 초기 저항을 극복할 수 있었습니다.\n\n마지막으로, 리코버의 인간 중심 접근법은 복잡한 프로젝트에서 성공을 이끄는 것은 시스템이 아니라 사람이라는 철학을 바탕으로 합니다. 그는 인재와 혁신을 중시하는 문화를 조성하는 것이 중요하다고 믿었습니다.\n\n리코버의 접근 방식은 현재 정책 입안자들에게 미국의 산업 능력을 강화하고 글로벌 무대에서 효과적으로 경쟁하기 위한 귀중한 통찰을 제공합니다.",
      "ja": "この記事では、「核海軍の父」として知られるハイマン・リコバー提督から学んだ教訓について述べており、アメリカにおける強力な産業政策とリーダーシップの重要性が強調されています。リコバーの核動力海軍の開発における業績は、戦略的競争には技術革新だけでなく、強固な産業基盤も必要であることを示しています。\n\n重要なポイントとして、まず産業政策の重要性があります。アメリカは産業基盤への投資が不足しており、特に生産能力が大きい中国に対して競争が難しくなっています。次に、リコバーのリーダーシップについてです。彼の核海軍の創設における成功は、技術的な専門知識、実践的な管理スタイル、優秀な人材の採用と育成に対する重視から来ています。リコバーは核潜水艦のすべての士官を個別に面接し、継続的な教育の重要性を強調しました。\n\nまた、彼の管理スタイルは厳格な監視と責任の明確化を維持し、チームが複雑な課題に対処できるように十分な情報を提供しました。彼は部下に早い段階から権限と責任を与えることを信じていました。さらに、リコバーは政府の官僚制度を巧みに扱い、海軍と原子力委員会の両方で核推進を推進するために二重指揮構造を利用し、最初の抵抗を克服しました。\n\n最後に、リコバーの人間中心のアプローチは、成功するプロジェクトにはシステムだけでなく人が重要であるというものでした。彼は才能と革新を重視する文化を育むことを信じていました。リコバーのアプローチは、アメリカの産業能力を強化し、グローバルな舞台で効果的に競争するための貴重な洞察を提供しています。"
    }
  },
  {
    "id": "20bb7511fe9936ea",
    "title": {
      "en": "Aircraft detection at planetary scale",
      "ko": "행성 규모 항공기 탐지",
      "ja": "惑星規模の航空機検出"
    },
    "type": "story",
    "url": "https://www.planet.com/pulse/aircraft-detection-at-planetary-scale/",
    "score": 83,
    "by": "jjwiseman",
    "time": 1742848821,
    "content": "HOMETechAircraft Detection at Planetary ScaleAdam Van EttenMarch 20, 2025•5 min readTechTechIntroductionOn a typical day, there are greater than 10,000 aircraft (commercial, private, and military) in flight at any given time. The precise departure (point A) and destination (point B) locations, as well as the timing of flight activity, provide critically important information for myriad industries. For instance, in the defense and intelligence sector, being able to identify how many aircraft were stationed at one foreign government’s airbase (point A) and how many are gone the next day may reveal insights as to why an allied government’s airbase (point B) just received a new fleet on their tarmac.On the commercial side, understanding how many aircraft landed in a city on a specific day can help predict economic trends and assess the impacts of world events - such as the quantity of aircraft present for large sporting or entertainment events. Satellite imagery is a powerful information source for this type of aircraft detection. Combine automated satellite scans of airfields with a global news reporting large language model, and you can start to get a feed of insights, linking the patterns of politics and economies with global air traffic.ScaleAircraft detection with satellite imagery is nothing new. In fact, aircraft detection on the tarmac is an archetypal use case for remote sensing machine learning. Until now, though, aircraft detection has been limited to a select number of airfields with sporadic coverage and cadence. Planet’s satellites, on the other hand, scan all of Earth’s landmass on a near-daily basis, so we are now able to mitigate those limitations, leveraging novel machine learning techniques with global PlanetScope coverage to drastically scale global aircraft detection.The need for greater scale in aircraft detection is difficult to overstate. More than one hundred thousand commercial and military flights take place each day, as planes transport people and cargo to nearly fifty thousand airports across the globe. Some sites collect and share publicly available data on this complex matrix of movement. But not every flight is recorded, and not every plane is public.Monitoring aircraft activity is vital for economic, defense, and intelligence purposes, but current methods rely on tedious and expensive processes, especially at scale. The rapid pace of geopolitical developments and sheer volume of data means that analyzing airfields manually is no longer feasible, especially in conflict/sensitive zones where aircraft movements aren’t publicly reported.Planet’s new Aircraft Detection Analytic Feed automates daily detection of aircraft by leveraging a machine learning model trained using PlanetScope and SkySat data. It precisely identifies aircraft ≥25 meters in length or wingspan across airfields globally. It represents the first attempt at global detection of large aircraft on a near-daily basis and offers a scalable solution for analysis at scale. This presents a massive and unprecedented opportunity for analysts with or without geospatial expertise. Historical daily counts of aircraft broken down by size and location enables pattern-of-life analyses to be conducted, highlighting when anomalous behavior is detected. This can indicate events taking place or even actions to come.Keep reading for more information and register for our upcoming webinar. Our ApproachOur Aircraft Detection service leverages a supervised object detection model, trained on PlanetScope imagery to detect aircraft at rest on an airfield/airport tarmac. Precise labeling of aircraft can be a challenging task in medium-resolution PlanetScope imagery, so we use high-resolution SkySat imagery collected within 3 minutes of a PlanetScope scene (see Figure 1). This approach enables us to label on high-resolution imagery, then apply those high fidelity labels to medium-resolution PlanetScope data. The vast majority of aircraft are static on a 3-minute timeframe, but around ~2% of aircraft move between the two collections, and we remove those labels in our quality control phase.Figure 1. SkySat aircraft labels are applied to concurrent PlanetScope imagery.With this approach, we collected over 800 concurrent (or “crossover”) image pairs for labeling. After significant experimentation with many different model architectures, we determined that aircraft <25m in length or wingspan are too small to be reliably detected in medium resolution imagery, so we elected to focus on aircraft this size or greater. Splitting the dataset into 85% training / 15% testing, we generated a training dataset consisting of ~15,000 labels, roughly evenly split between civilian and military types. This is by no means an enormous dataset, so expansion of the label corpus would likely yield a performance boost.Figure 2. Airfield locations in the labeled dataset.Figure 3. Observation dates, covering multiple years and every season. A concerted effort was made in 2023 to leverage SkySat capacity to capture data over airfields to ensure we had a healthy amount of data to leverage.We utilize a deep learning object detection model optimized for Planet’s medium resolution imagery, which runs at sufficient speed so that we are free to analyze as many airports and airstrips as desired. The model achieves an F1 score of 0.82 for aircraft detection on our globally distributed test set. Reported errors are at the 1σ level, computed via bootstrap resampling of the test set.High resolution SkySat imagery can be leveraged for more than just training data generation. Occasionally, additional scrutiny will be required to understand exactly what’s going on at a given airfield. An example is provided in Figure 3, which shows a consistent “missed” detection with our PlanetScope model. Yet closer inspection of this airfield with SkySat revealed that this apparent “miss” was in fact merely a painting on the tarmac. The ability to rapidly probe targeted areas much deeper with SkySat is a highly valuable complement to the ubiquitous daily coverage of PlanetScope.Figure 4. PlanetScope (left) detections showing a “missed” aircraft in the upper left. High-resolution SkySat imagery (right) illustrating that only paint exists at this location.Figure 5. Animation showing analysis of a large corpus of global detections.Next StepsOur solution extends significantly beyond the archetypal problem of counting aircraft on the tarmac. We track behavior at an approximately daily cadence by quantifying the fraction of aircraft that moved or remained static. Furthermore, interactive dashboards enable on-the-fly time series analysis, outlier categorization, and trend aggregation. We’ve integrated a global news LLM into this process to further enlighten our users. Detected outliers are subsequently probed for correlation to regional events. We’ll be demonstrating this capability in an upcoming Agile EO Webinar. Register here!Tags:#Aircraft DetectionContentsIntroductionScaleOur ApproachNext StepsShareReady to Get StartedConnect with a member of our Sales team. We'll help you find the right products and pricing for your needsContact SalesOther Articles You Might Like",
    "summary": {
      "en": "**Summary of Aircraft Detection at Planetary Scale**\n\nEvery day, over 10,000 aircraft are in the air, making flight data crucial for various industries, particularly defense and economics. Knowing the number and locations of aircraft can provide insights into military actions and economic trends.\n\nTraditionally, detecting aircraft using satellite imagery has been limited to a few locations and was time-consuming. However, Planet's satellites scan the entire Earth almost daily, allowing for broader and more efficient aircraft detection using advanced machine learning techniques.\n\nPlanet has introduced an Aircraft Detection Analytic Feed that automatically identifies large aircraft (25 meters or longer) on global airfields. This service significantly enhances the ability to monitor aircraft activity, helping analysts detect unusual patterns that may indicate significant events.\n\nThe detection system uses a combination of medium-resolution PlanetScope imagery and high-resolution SkySat imagery to improve accuracy. It has successfully trained a model with around 15,000 aircraft labels, achieving a good detection rate.\n\nBeyond counting aircraft, the service tracks their movements and provides tools for real-time analysis of trends and anomalies. This integration of satellite data with a global news model helps correlate detected changes in aircraft activity with regional events.\n\nFor more details, a webinar will be held to showcase these capabilities.",
      "ko": "매일 10,000대 이상의 항공기가 비행 중이며, 이러한 비행 데이터는 방위 및 경제와 같은 다양한 산업에 매우 중요합니다. 항공기의 수와 위치를 아는 것은 군사 작전과 경제 동향에 대한 통찰력을 제공합니다.\n\n전통적으로 위성 이미지를 이용한 항공기 탐지는 몇몇 지역에 한정되어 있었고 시간이 많이 소요되었습니다. 그러나 Planet의 위성은 거의 매일 지구 전체를 스캔하여, 첨단 기계 학습 기술을 활용해 보다 넓고 효율적인 항공기 탐지가 가능해졌습니다.\n\nPlanet은 전 세계 공항에서 길이가 25미터 이상인 대형 항공기를 자동으로 식별하는 항공기 탐지 분석 피드를 도입했습니다. 이 서비스는 항공기 활동을 모니터링하는 능력을 크게 향상시켜, 분석가들이 중요한 사건을 나타낼 수 있는 비정상적인 패턴을 감지하는 데 도움을 줍니다.\n\n탐지 시스템은 중간 해상도의 PlanetScope 이미지와 고해상도의 SkySat 이미지를 결합하여 정확성을 높입니다. 약 15,000개의 항공기 레이블로 모델을 성공적으로 훈련시켜 좋은 탐지율을 달성했습니다.\n\n항공기 수를 세는 것에 그치지 않고, 이 서비스는 항공기의 이동을 추적하며 실시간으로 동향과 이상 징후를 분석할 수 있는 도구를 제공합니다. 위성 데이터와 글로벌 뉴스 모델의 통합은 항공기 활동의 변화가 지역 사건과 어떻게 연관되는지를 파악하는 데 도움을 줍니다.\n\n자세한 내용은 이러한 기능을 소개하는 웨비나가 개최될 예정입니다.",
      "ja": "毎日、1万機以上の航空機が空を飛んでおり、フライトデータは防衛や経済などさまざまな産業にとって重要です。航空機の数や位置を把握することで、軍事行動や経済動向についての洞察が得られます。\n\n従来、衛星画像を使った航空機の検出は限られた場所にしか対応できず、時間がかかるものでした。しかし、Planetの衛星はほぼ毎日地球全体をスキャンし、先進的な機械学習技術を用いてより広範囲で効率的な航空機検出を可能にしています。\n\nPlanetは、世界中の空港で25メートル以上の大型航空機を自動的に特定する「航空機検出分析フィード」を導入しました。このサービスは航空機の活動を監視する能力を大幅に向上させ、異常なパターンを検出することで重要な出来事を示唆する手助けをします。\n\nこの検出システムは、中解像度のPlanetScope画像と高解像度のSkySat画像を組み合わせて精度を向上させています。約15,000の航空機ラベルを用いてモデルを訓練し、良好な検出率を達成しました。\n\n航空機の数を数えるだけでなく、このサービスは航空機の動きを追跡し、トレンドや異常のリアルタイム分析のためのツールを提供します。衛星データとグローバルなニュースモデルを統合することで、航空機活動の変化と地域の出来事との関連を明らかにします。\n\n詳細については、これらの機能を紹介するウェビナーが開催される予定です。"
    }
  },
  {
    "id": "059ded92dec3320a",
    "title": {
      "en": "Project Aardvark: reimagining AI weather prediction",
      "ko": "AI 날씨 혁신",
      "ja": "プロジェクトアードバーク：AI天気予報の革新"
    },
    "type": "story",
    "url": "https://www.turing.ac.uk/blog/project-aardvark-reimagining-ai-weather-prediction",
    "score": 297,
    "by": "bentobean",
    "time": 1742772819,
    "content": "Section page\n                            Events from around the Turing Network\n\n                                    Conferences, workshops, and other events from around the Turing Network\n\n    Events from around the Turing Network",
    "summary": {
      "en": "This section provides information about various events related to the Turing Network, including conferences, workshops, and other activities.",
      "ko": "이 섹션에서는 튜링 네트워크와 관련된 다양한 행사에 대한 정보를 제공합니다. 여기에는 컨퍼런스, 워크숍 및 기타 활동이 포함됩니다.",
      "ja": "このセクションでは、チューリングネットワークに関連するさまざまなイベントについての情報を提供します。会議やワークショップ、その他の活動が含まれています。"
    }
  },
  {
    "id": "6cc369689d7a0ca5",
    "title": {
      "en": "Researchers search for more precise ways to measure pain",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.washingtonpost.com/science/2025/03/23/pain-measure-precision-research/",
    "score": 88,
    "by": "Petiver",
    "time": 1742788622,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "cf047133cd50e135",
    "title": {
      "en": "Quadlet: Running Podman containers under systemd",
      "ko": "쿼드렛: 시스템D로 포드맨 실행하기",
      "ja": "システムdでPodman実行"
    },
    "type": "story",
    "url": "https://mo8it.com/blog/quadlet/",
    "score": 283,
    "by": "gjvc",
    "time": 1742776025,
    "content": "Quadlet: Running Podman containers under systemd\n\n        2024-01-02, updated: 2024-02-19\n            Tags:\n            #container,#linux,#selfhosting\n\n        Reading time: ~11min\n\n        Quadlet lets you run your Podman containers as systemd services.\nThis is especially useful for running containers in the background and automatically starting them after a server reboot.\nRunning Podman containers under systemd isn't new.\nIndeed, this was supported by Podman for a long time with the command podman generate systemd.\nBut this command now shows a deprecation warning to migrate to Quadlet.\nFor some months, I was too lazy to do that migration on my home server.\nWhy even touch a working system?\nBut now that I finally found time for it, I really appreciate Quadlet!\nI think that Podman finally has a Docker Compose alternative which is even more flexible and powerful!\nIn this blog post, I will explain how to use Quadlet with rootless Podman and migrate from the old method with podman generate systemd.\n\n        Note\n        If you wonder why systemd: Something has to start containers when there is no daemon (since Podman is daemonless).\nIf you are part of the vocal minority hating against systemd, then please leave instead of trolling.\n\n    Landscape mode recommended on mobile devices\n\n            Contents\n\n                            The deprecated method\n\n                            The problem\n\n                            Quadlet\n\n                            Is it any better?\n\n                            Dependencies\n\n                            Too many files?\n\n                            Updating images\n\n                            What about podman-compose?\n\n                            Further resources\n\nThe deprecated method\nLet's see how the old method worked before comparing it to Quadlet.\nYou can skip to the Quadlet section though.\nFirst, you had to create a container.\nIn an older post about Containerized PostgreSQL with rootless Podman, I created a container with a command similar to the following:\npodman create \\\n  --name test-db \\\n  -p 5432:5432 \\\n  -v ~/volumes/test-db:/var/lib/postgresql/data:Z \\\n  -e POSTGRES_PASSWORD=CHANGE_ME \\\n  --label \"io.containers.autoupdate=registry\" \\\n  docker.io/library/postgres:16\n\nThe command details are explained in the older post.\nThe only option that isn't explained there is --label \"io.containers.autoupdate=registry\".\nThis option enables updating the container image using podman auto-update which will be explained later in this post.\nAfter creating the container, you can run the following command:\npodman generate systemd test-db -fn --new\n\nIt creates the systemd service file container-test-db.service in your current path.\nThe options of the command aren't relevant for now, but they are also explained in the older post.\nTo use this generated service file, you had to place it in the directory ~/.config/systemd/user.\nTo enable and start it, you had to run the following command:\nsystemctl --user enable --now container-test-db\n\nThe problem\nThe problem with the old method is that it required you to run commands to…\n\ncreate a container\ngenerate a service file\nmove the service file if not already in the mentioned directory\nenable the service\n\nEspecially the command for creating the container is often lengthy.\nThis means that you had to create a shell script with these commands if you wanted to be able to rerun them later.\nTo reduce duplication, I created the following fish function to be called in my fish scripts that create the containers:\nfunction podman-default-create\n    set -l container_name $argv[1]\n\n    podman create \\\n        --name $container_name \\\n        --replace \\\n        $argv[2..]; or return 1\n\n    podman generate systemd --no-header --new --name $container_name >~/.config/systemd/user/container-$container_name.service; or return 1\n\n    systemctl --user enable --now container-$container_name\nend\n\nYou don't have to understand the details of the function above.\nWhat I want to demonstrate with it is that the old method was too hacky and involved the usage of redundant commands.\nThere must be an easier way, you might think.\nEspecially if you experienced the convenience that Docker Compose provides.\nBut this is not the only problem.\nThe old method is very inflexible!\nIf you want to cutomize the service file and use all systemd features, you need to manually edit it after each generation!\nQuadlet\nLet's take a look at the new method with Quadlet.\nFirst, you create the directory ~/.config/containers/systemd.\nThen, you place a .container file inside it.\nFor example, here is the test-db.container file:\n[Container]\nImage=docker.io/library/postgres:16\nAutoUpdate=registry\nPublishPort=5432:5432\nVolume=%h/volumes/test-db:/var/lib/postgresql/data:Z\nEnvironment=POSTGRES_PASSWORD=CHANGE_ME\n\n[Service]\nRestart=always\n\n[Install]\nWantedBy=default.target\n\nIt is a normal systemd service file but with the special section [Container].\nThis section has many documented options.\nAlmost all these options map to command line options that can be used to create a container with Podman (podman create).\nThe ones that we are interested in for the example are the following:\n\nImage specifies the image (with tag) to use\nAutoUpdate=registry maps to --label \"io.containers.autoupdate=registry\" (explained later in this post)\nPublishPort maps to -p\nVolume maps to -v\nEnvironment maps to -e\n\nIt is important to use the systemd specifier %h instead of ~ for the user home directory.\nIn the [Service] section, we use the Restart option and set it to always to always restart the container (unless stopped manually).\nTo automatically start the container on boot, we set the WantedBy option in the [Install] section to default.target.\n\n        Note\n        I thought that setting WantedBy to multi-user.target would work because it is the default target on servers.\nBut it doesn't work in the case of rootless containers.\nmulti-user.target is not defined in the user mode in systemd.\nYou can verify this by running the command systemctl --user status multi-user.target.\nIt is only defined in the system mode (systemctl status multi-user.target without --user).\n\nSince we use user services for systemd, we have to enable the linger for our user to start the containers without the user being logged in:\nloginctl enable-linger\n\n        ⚠️ Warning ⚠️\n        Enabling the linger is required for the container to be automatically started after a server reboot!\n\nFor systemd to discover the new service file, run systemctl --user daemon-reload.\nNow, you can start the container with systemctl --user start test-db.\nYou can check the status of the container service by running systemctl --user status test-db.\nYou can also verify that the Podman container is running by running podman ps.\nYou should find the container systemd-test-db.\nThe container has the name of the service file (test-db.container without the .container extension) prefixed by systemd- to avoid collisions with containers not managed by systemd.\nBut you can manually set the name of the container using the ContainerName option in the [Container] section.\nIs it any better?\nMy first impression was: \"Well, now I have to map all the podman create options to their equivalents in the [Container] section. Where is the benefit?\".\nBut after migrating all containers, I found the following benefits:\n\nYou have only one file (the container service file) instead of a script that generates a service file.\nYou can use all options possible in systemd. All options that you know about in the [Unit] and [Service] sections are supported. For example, you can specify a command to run before starting the container by setting the StartExecPre option. No more manual editing of generated files anymore!\nRather subjective: Writing configuration files is easier than writing and debugging shell scripts.\nIt is easier to handle dependencies as we will see in the next section.\n\nDependencies\nLet's assume that we have an app container that depends on the database container that we created.\nYou want the database container to be automatically started when the app container is started.\nYou also want to make sure that the app container is started after the database container.\nOtherwise, the app container might fail to start.\nHow can we express this dependency?\nLet's use OxiTraffic as an example (shameless plug 😅).\nHere is the container service file oxitraffic.container that should be placed in ~/.config/containers/systemd:\n[Container]\nImage=docker.io/mo8it/oxitraffic:0.9.2\nAutoUpdate=registry\nVolume=%h/volumes/oxitraffic/config.toml:/volumes/config.toml:Z,ro\nVolume=%h/volumes/oxitraffic/logs:/var/log/oxitraffic:Z\n\n[Unit]\nRequires=test-db.service\nAfter=test-db.service\n\n[Service]\nRestart=always\n\n[Install]\nWantedBy=default.target\n\nThe new section is [Unit].\nWe set the Requires option to test-db.service to only start the app when the database is started.\nWe also set the After option to make sure that both containers aren't started in parallel.\nNote that we use test-db.service when referencing this container service and not test-db.container.\n\ntest-db.container is the file name.\ntest-db.service is the service name.\nsystemd-test-db is the default container name.\n\nFor the app to communicate with the database, a network should be added to both containers with the Network option in the [Container] section, but networking is out of the scope of this post.\nToo many files?\nIn our example, we created two files, one for the app container and one for the database container.\nDoes this mean that multi-container apps are more complex with Quadlet because you can't just put them in one file like with Docker Compose?\nIt depends on how you define complexity in this context.\nDoes splitting content over multiple files always result in a higher complexity?\nFor me, it is more complex to have everything in the same file.\nI had to maintain Docker Compose files with hundreds of lines and dozens of containers…\nThat wasn't fun!\nHaving each container in its own file has less mental overhead for me because I just have to think about this single container when I am in its file.\nOf course, you need to specify its dependencies on other containers, but you don't need to think about the details of these other containers.\nThe Docker Compose file of Mailcow is a terrifying example of huge Docker Compose files.\n\n        Note\n        Docker Compose supports splitting to multiple files.\n\nSo we need multiple files.\nBut we should still group related ones together!\nQuadlet supports placing unit files inside directories in the ~/.config/containers/systemd directory.\nFor our example, you would create an oxitraffic directory and place both files inside it.\nUpdating images\nNow, we have containers running in the background and automatically started after a server reboot.\nWouldn't it be nice to have an easy method to update the images of these containers without running podman pull for every container and then restarting the updated ones?\nFor example, if a new image is uploaded for PostgreSQL 16 (with the image tag 16 that we used), then the image should be updated and the container should be restarted.\nWith Docker, you would need something like Watchtower.\nBut Podman provides a tool out of the box!\nIf you set AutoUpdate=registry, you can just run podman auto-update and Podman will check if the registry has a newer image which is compatible with the used tag.\nIn that case, the image will be pulled and the container will be restarted.\nIt is that easy 😍\nOf course, this could be dangerous if you use a tag like latest for OxiTraffic instead of a concrete version like 0.9.2.\nBecause the next version pushed to the latest tag might include a breaking change!\nIt is even more dramatic if you use the latest tag for the PostgreSQL image because manual migrations are always required when upgrading PostgreSQL to a new major version.\nTherefore, always use a tag that can't lead to a breaking change!\nTrust me, this is not only a problem with Podman updates.\nI learned this the hard way when trying to deploy Docker containers a while ago that used the latest tag.\nPersonally, I run podman auto-update manually on the server every couple of days to see what has been updated and make sure that the containers are still healthy afterwards.\nWhat about podman-compose?\nThere is the Python script podman-compose which runs Compose files with Podman.\nBut I don't consider it a long-term alternative to Docker Compose for multiple reasons:\n\npodman-compose is just a translation layer between the Compose spec and Podman with systemd. Why yet another layer of abstraction that sacrifices flexibility? It doesn't let you use all features of systemd.\nI consider Python scripts a prototype because of Python's interpreted nature. Official Podman projects are written in a compiled language like Rust or Go.\nIt is not actively maintained. The last commit was 5 months ago.\n\nQuadlet aligns much better with the rootless, daemonless design of Podman.\nIf you are stuck with Compose files and would like to try out Quadlet, check out podlet that can help you during the migration.\nFurther resources\nTake this post as an introduction.\nI highly recommend reading the man page podman-systemd.unit to get a deeper understanding of Quadlet.\nYou don't need to read the documentation of every supported option though.\nQuadlet doesn't only work with containers.\nIt can also manage pods, networks and volumes (see man page).\nIf you are new to writing systemd unit files (like me), I also recommend checking out the man pages systemd.unit and systemd.service.\npodlet is a wonderful Rust tool that can help you during the migration.\nIt can create Quadlet files out of Podman commands or even (Docker) Compose files.\nCheck out the similar blog post on blog.while-true-do for another perspective and a second example.\nIt is my favorite blog related to Linux 🥰\nFinally, if you want to see my migration as an example, then you can compare before and after.",
    "summary": {
      "en": "**Summary of Quadlet: Running Podman Containers under systemd**\n\nQuadlet is a tool that allows you to run Podman containers as systemd services, making it easier to manage containers in the background and ensuring they start automatically after a server reboot. This approach replaces the older method of using the command `podman generate systemd`, which is now deprecated.\n\n### Key Points:\n\n1. **Why Use Quadlet?**\n   - Quadlet simplifies container management by allowing you to define containers in a single configuration file instead of multiple commands.\n   - It supports all systemd features, making it easier to customize and manage dependencies between containers.\n\n2. **Setting Up Quadlet:**\n   - Create a directory at `~/.config/containers/systemd` and add a `.container` file for each container.\n   - A typical `.container` file defines the container image, ports, volumes, environment variables, and service options like restart behavior.\n\n3. **Advantages Over the Old Method:**\n   - Only one configuration file is needed instead of multiple commands, reducing complexity.\n   - Easier to handle service dependencies, ensuring the correct startup order of containers.\n\n4. **Updating Containers:**\n   - Quadlet supports automatic updates through the `AutoUpdate` feature, allowing you to easily refresh images without manual intervention.\n\n5. **Comparison with Docker Compose:**\n   - Unlike Docker Compose, which groups multiple containers in one file, Quadlet uses separate files for each container, which some users find less complex as it focuses on individual container configurations.\n   - A tool named `podlet` can help transition from Docker Compose to Quadlet.\n\n6. **Further Learning:**\n   - For a deeper understanding, refer to the man pages for `podman-systemd.unit` and `systemd` documentation.\n   - `podlet` can assist in creating Quadlet files from existing Podman commands or Docker Compose files.\n\nIn conclusion, Quadlet provides a more flexible and powerful alternative to managing Podman containers, especially for users who prefer using systemd for service management.",
      "ko": "Quadlet은 Podman 컨테이너를 systemd 서비스로 실행할 수 있게 해주는 도구입니다. 이를 통해 백그라운드에서 컨테이너를 쉽게 관리할 수 있으며, 서버가 재부팅된 후에도 자동으로 시작되도록 할 수 있습니다. 이 방법은 더 이상 사용되지 않는 `podman generate systemd` 명령어를 대체합니다.\n\nQuadlet을 사용하는 이유는 여러 가지가 있습니다. 우선, Quadlet은 여러 명령어 대신 하나의 구성 파일에서 컨테이너를 정의할 수 있게 해주어 관리가 간편해집니다. 또한, 모든 systemd 기능을 지원하여 컨테이너 간의 의존성을 쉽게 설정하고 조정할 수 있습니다.\n\nQuadlet을 설정하려면 `~/.config/containers/systemd` 디렉토리를 만들고, 각 컨테이너에 대해 `.container` 파일을 추가해야 합니다. 일반적인 `.container` 파일에는 컨테이너 이미지, 포트, 볼륨, 환경 변수, 서비스 옵션(예: 재시작 동작) 등이 정의됩니다.\n\n구식 방법에 비해 Quadlet의 장점은 여러 개의 명령어 대신 하나의 구성 파일만 필요하다는 점입니다. 이로 인해 복잡성이 줄어들고, 서비스 간의 의존성을 더 쉽게 관리할 수 있어 컨테이너의 올바른 시작 순서를 보장합니다.\n\nQuadlet은 `AutoUpdate` 기능을 통해 자동 업데이트를 지원하여, 수동 개입 없이 이미지를 쉽게 새로 고칠 수 있습니다.\n\nDocker Compose와 비교할 때, Docker Compose는 여러 컨테이너를 하나의 파일로 묶는 반면, Quadlet은 각 컨테이너에 대해 별도의 파일을 사용합니다. 일부 사용자들은 이렇게 개별 컨테이너 구성에 집중하는 방식이 덜 복잡하다고 느낍니다. `podlet`이라는 도구는 Docker Compose에서 Quadlet로 전환하는 데 도움을 줄 수 있습니다.\n\n더 깊이 있는 이해를 원한다면 `podman-systemd.unit`의 매뉴얼 페이지와 systemd 문서를 참고하면 좋습니다. `podlet`은 기존 Podman 명령어 또는 Docker Compose 파일에서 Quadlet 파일을 생성하는 데 도움을 줄 수 있습니다.\n\nQuadlet은 Podman 컨테이너 관리를 위한 더 유연하고 강력한 대안을 제공합니다. 특히 systemd를 서비스 관리에 선호하는 사용자에게 유용합니다.",
      "ja": "Quadletは、Podmanコンテナをsystemdサービスとして実行するためのツールです。これにより、バックグラウンドでのコンテナ管理が容易になり、サーバー再起動後に自動的に起動することが保証されます。この方法は、以前の`podman generate systemd`コマンドを使う方法に代わるもので、現在は推奨されていません。\n\nQuadletを使用する理由は、コンテナ管理を簡素化できる点です。複数のコマンドを使う代わりに、単一の設定ファイルでコンテナを定義できます。また、すべてのsystemd機能をサポートしているため、コンテナ間の依存関係をカスタマイズしやすくなっています。\n\nQuadletの設定は簡単です。まず、`~/.config/containers/systemd`というディレクトリを作成し、各コンテナ用の`.container`ファイルを追加します。一般的な`.container`ファイルには、コンテナイメージ、ポート、ボリューム、環境変数、再起動の動作などのサービスオプションが定義されます。\n\n従来の方法と比べた場合、Quadletの利点は、複数のコマンドではなく一つの設定ファイルで済むため、複雑さが減ることです。また、サービスの依存関係を簡単に管理でき、コンテナの起動順序を正しく保つことができます。\n\nQuadletは、`AutoUpdate`機能を通じて自動更新をサポートしており、手動での介入なしにイメージを簡単に更新できます。\n\nDocker Composeとの比較では、Docker Composeが複数のコンテナを一つのファイルにまとめるのに対し、Quadletは各コンテナごとに別々のファイルを使用します。これにより、個々のコンテナ設定に焦点を当てるため、複雑さが軽減されると感じるユーザーもいます。また、`podlet`というツールを使うことで、Docker ComposeからQuadletへの移行が支援されます。\n\nさらに学ぶためには、`podman-systemd.unit`のマニュアルページやsystemdのドキュメントを参照することをお勧めします。`podlet`は、既存のPodmanコマンドやDocker ComposeファイルからQuadletファイルを作成するのに役立ちます。Quadletは、特にsystemdをサービス管理に使用することを好むユーザーにとって、Podmanコンテナの管理においてより柔軟で強力な代替手段を提供します。"
    }
  },
  {
    "id": "19e0e815410e9b28",
    "title": {
      "en": "A proof checker meant for education",
      "ko": "교육용 검증기",
      "ja": "教育用証明チェッカー"
    },
    "type": "story",
    "url": "https://jsiek.github.io/deduce/index.html",
    "score": 139,
    "by": "alabhyajindal",
    "time": 1742557657,
    "content": "A proof checker meant for education\n                Teaching correctness proofs of functional programs to students.\n\n                    Get Started\n                    Live Code",
    "summary": {
      "en": "This text is about a proof checker designed for educational purposes. Its main goal is to help students learn how to create correctness proofs for functional programming. The text also encourages users to get started and offers a live coding feature.",
      "ko": "이 텍스트는 교육 목적으로 설계된 증명 검사기에 대한 내용입니다. 이 도구의 주요 목표는 학생들이 함수형 프로그래밍의 정확성 증명을 만드는 방법을 배우도록 돕는 것입니다. 또한 사용자들이 쉽게 시작할 수 있도록 유도하며, 실시간 코딩 기능도 제공합니다.",
      "ja": "このテキストは、教育目的で設計された証明チェッカーについてです。このツールの主な目的は、学生が関数型プログラミングの正しさ証明を作成する方法を学ぶ手助けをすることです。また、ユーザーが始めやすいように、ライブコーディング機能も提供されています。"
    }
  },
  {
    "id": "542f250f45f190b5",
    "title": {
      "en": "Langfuse (YC W23) Is Hiring in Berlin, Germany",
      "ko": "랑퓨즈, 베를린 채용!",
      "ja": "ベルリンで採用中！"
    },
    "type": "job",
    "url": "https://langfuse.com/careers",
    "score": 1,
    "by": "clemo_ra",
    "time": 1742810172,
    "content": "Join Us!\nLangfuse is building the leading open-source LLM engineering platform (Why Langfuse?).\nWhile LLMs improve a lot, we don’t see enough applications in production. Building these applications requires a new workflow of continuous monitoring and evaluation that we enable with Langfuse.\nWe are seeing strong traction over recent months (see metrics below), thus it is the right time to grow the team to build out our backend systems, product, and how we communicate with developers.\nWe are hiring across for engineering and technical go-to-market roles. We work in-person in Berlin, Germany & are building an excellent, small team. If you are considering new opportunities — even if you might not be looking immediately, we would love to hear from you & get coffee.\nWe have raised a $4M seed round from Lightspeed, General Catalyst, Y Combinator and angels. We are growing fast (see metrics below) and work with some of the best AI teams such as Samsara, Twilio, KhanAcademy and Rocket Money.\nLearn more about Langfuse\nDocumentation\n10 min product walkthrough\nInteractive demo\nOrigin story\nLangfuse 2.0 scope\nEngineering blog on Langfuse v3\n\nTeam\n\nClemens Rawert, @rawert, Linkedin\nMarc Klingen, @marcklingen, GitHub, Linkedin\nMarlies Mayerhofer, @marliessophie, GitHub, LinkedIn\nMax Deichmann, @maxdeichmann, GitHub, Linkedin\nHassieb Pakzad, @hassiebpakzad, GitHub, LinkedIn\nSteffen Schmitz, GitHub, LinkedIn\nJannik Maierhöfer, @JMaierhoefer, GitHub, LinkedIn\n\nA few tidbits that might help you understand what kind of company we are:\n\nWe have 2 scheduled meetings per week: 15 min planning on Monday, 60 min demo on Friday\nWe are a very high-trust environment, Reed Hastings’ ‘No Rules Rules’ inspires our culture, here’s a summary\nMost team members have led teams before and are excited about being an IC again to ship more\nWe were heavily influenced by our time at Y Combinator and follow its ethos. We’re a Silicon Valley Startup in Berlin - more here\n\nPS: We are based in Berlin (Germany) and work in person. Come join us build one of the most exciting open source devtool companies in Europe!\nRoles\nCan’t find a role that’s a perfect fit? If you are excited about delivering\nexceptional developer experiences and building a high-quality open-source\nproduct alongside an insanely motivated team that ships, feel\nfree to reach out: careers@langfuse.com\n\nDesign Engineer\n\nBerlin, €70K - €130K EUR, 0.1% - 0.35%\nYou are a design-minded frontend engineer who is passionate about building beautiful, functional, and user-friendly interfaces. You own UI/UX in the Langfuse application to help our users achieve their goals and create how thousands of teams build their LLM applications.\n\nBackend Engineer\n\nBerlin, €70K - €130K EUR, 0.1% - 0.35%\nYou will work on our backend systems, powering the core infrastructure of Langfuse. You work on a data-intensive application that is used by hundreds of companies and thousands of developers ingesting hundreds of millions of events.\nThe technical blog post on the v3 release is a great example of what you could have worked on.\n\nProduct Engineer\n\nBerlin, €70K - €130K EUR, 0.1% - 0.35%\nDesign, scope, ship, and launch new full-stack (UI, backend, SDKs, integrations) features on top of our existing data platform. You’ll own what you build truly end-to-end from first conversations with users to operating it in production.\nSee launch weeks (#1, #2) and changelog for a good selection of things you could have worked on.\n\nDeveloper Advocate\n\nBerlin, €70K - €130K EUR, 0.1% - 0.35%\nRun our bottom-up go-to-market via developer-focused content, documentation, partnerships, events, and community-building. Langfuse is a platform of many building blocks and there are lots of common workflows we can educate on to drive awareness and adoption.\nExamples: Hugging Face and Posthog partnerships, LLM Security Monitoring guides/examples, AWS partnership, our YouTube channel\n\nFounding GTM Engineer\n\nBerlin, €70K - €130K EUR, 0.1% - 0.35%\nWork closely with our enterprise customers throughout the inbound sales cycle. Help them resolve complex technical questions, succeed in POCs, and expand after initial adoption.\nBuild ressources and improve the process to further increase scalability of our GTM.\n\nPlease apply via Work At a Startup or send us an email with your CV/LinkedIn to careers@langfuse.com\nYou need to be located in Berlin, Germany or willing to relocate. The first few interviews can be done remotely via Zoom. In the final stage, we will work one day onsite together (usually on an actual high-priority project).\nWhy Join Langfuse?\n\nLangfuse HQ, Berlin (Mitte), Germany\nImpactful work/product for awesome OSS community and users. We get to work with extremely smart users in our OSS community who share a ton of great feedback, and it feels great to help them build exciting products. You’ll also get to work with many large enterprises and YC companies who monitor and improve their LLM products with Langfuse. Community channels: GitHub Discussions, GitHub Issues, Discord, recordings of community town halls\nStrong in-person engineering culture. Almost everyone on the team works on the product; our go-to-market (also Enterprise) is exclusively inbound, and we enjoy building a developer tool together with our open-source community.\nInteresting challenges. Building an open source developer tool with a strong observability component is engineering-wise highly interesting due the various integrations and SDKs, the chance to design the best LLM Engineering workflows for these teams, and the necessity to scale our backend-logic and infrastructure.\nGrowth & Ownership. We believe that a product like ours is best built by a small group of excellent engineers who fully own the components that they build. This includes running them in production, product feedback and roadmap planning, and design. We are a small team that’s moving fast (changelog) with a focus on great developer experience.\nShared curiosity and interest. If you are interested in working with engineering teams who rethink what’s possible when using LLMs to power new use cases, come work with us as we all share this excitement and get to see the leading edge of what great teams build every day. Next to shipping quickly this is the most fun part for sure.\nPublic Metrics\n\nOpen dashboard in new tab ↗Last updated on March 7, 2025Was this page useful?YesCould be betterQuestions? We're here to helpGitHub Q&AGitHubChat EmailTalk to salesSubscribe to updatesGetupdates",
    "summary": {
      "en": "**Join Us at Langfuse!**\n\nLangfuse is creating a top open-source platform for LLM (Large Language Model) engineering. Despite advancements in LLMs, there aren't enough real-world applications. We provide the tools for continuous monitoring and evaluation to help build these applications.\n\nWe're experiencing rapid growth and are expanding our team in Berlin, Germany. We are looking for engineers and technical marketing roles. If you're interested, even casually, we invite you to connect with us over coffee.\n\nWe’ve raised $4M from notable investors, including Lightspeed and Y Combinator, and we work with leading AI teams like Twilio and Khan Academy.\n\n**Why Work with Us?**\n- **Collaborative Environment:** We have two weekly meetings to plan and showcase our work. Our culture is based on high trust, inspired by Reed Hastings’ principles.\n- **Team Experience:** Most team members have leadership experience and are eager to contribute directly to projects.\n- **Exciting Work:** Join us in building an open-source developer tool that addresses complex engineering challenges.\n\n**Available Roles:**\n- **Design Engineer:** Focus on creating user-friendly interfaces.\n- **Backend Engineer:** Develop and maintain our data-intensive backend systems.\n- **Product Engineer:** Design and launch new features from start to finish.\n- **Developer Advocate:** Engage the community and create educational content.\n- **Founding GTM Engineer:** Support enterprise customers and improve our go-to-market strategies.\n\nTo apply, send your CV or LinkedIn profile to careers@langfuse.com. Candidates must be based in Berlin or willing to relocate.\n\n**Why Join Langfuse?**\n- Work on impactful projects within a vibrant open-source community.\n- Enjoy a strong in-person team culture centered on product development.\n- Face interesting engineering challenges with opportunities for growth and ownership.\n- Collaborate with teams exploring innovative LLM applications.\n\nWe look forward to hearing from you!",
      "ko": "랑퓨즈에 함께하세요!\n\n랑퓨즈는 대규모 언어 모델(LLM) 엔지니어링을 위한 최고의 오픈 소스 플랫폼을 만들고 있습니다. LLM 기술이 발전하고 있지만, 실제 적용 사례는 부족합니다. 우리는 이러한 애플리케이션을 구축하는 데 필요한 지속적인 모니터링과 평가 도구를 제공합니다.\n\n현재 우리는 빠르게 성장하고 있으며 독일 베를린에서 팀을 확장하고 있습니다. 엔지니어와 기술 마케팅 역할을 찾고 있습니다. 관심이 있으시다면, 편하게 커피 한 잔 하며 이야기 나누고 싶습니다.\n\n우리는 Lightspeed와 Y Combinator를 포함한 저명한 투자자로부터 400만 달러를 유치했으며, Twilio와 Khan Academy와 같은 선도적인 AI 팀과 협력하고 있습니다.\n\n왜 우리와 함께 일해야 할까요? \n협업 환경이 잘 조성되어 있습니다. 우리는 매주 두 번 회의를 열어 작업을 계획하고 공유합니다. 우리의 문화는 Reed Hastings의 원칙에 영감을 받아 높은 신뢰를 바탕으로 하고 있습니다. 팀원 대부분이 리더십 경험을 가지고 있으며, 프로젝트에 직접 기여하고자 하는 열망이 큽니다. 복잡한 엔지니어링 문제를 해결하는 오픈 소스 개발 도구를 만드는 흥미로운 작업에 참여하세요.\n\n현재 모집 중인 역할은 다음과 같습니다. 사용자 친화적인 인터페이스를 만드는 디자인 엔지니어, 데이터 집약적인 백엔드 시스템을 개발하고 유지하는 백엔드 엔지니어, 새로운 기능을 처음부터 끝까지 설계하고 출시하는 제품 엔지니어, 커뮤니티와 소통하고 교육 콘텐츠를 만드는 개발자 옹호자, 기업 고객을 지원하고 시장 진입 전략을 개선하는 창립 GTM 엔지니어입니다.\n\n지원하려면 이력서나 LinkedIn 프로필을 careers@langfuse.com으로 보내주세요. 지원자는 베를린에 거주하거나 이사할 의향이 있어야 합니다.\n\n랑퓨즈에 합류하면 영향력 있는 프로젝트에 참여하고 활기찬 오픈 소스 커뮤니티에서 일할 수 있습니다. 제품 개발을 중심으로 한 강력한 대면 팀 문화를 즐길 수 있으며, 흥미로운 엔지니어링 도전과 성장 및 소유의 기회를 경험할 수 있습니다. 혁신적인 LLM 애플리케이션을 탐색하는 팀과 협력할 수 있습니다.\n\n여러분의 소식을 기다립니다!",
      "ja": "Langfuseでは、LLM（大規模言語モデル）エンジニアリングのための優れたオープンソースプラットフォームを構築しています。LLMの進展にもかかわらず、実際のアプリケーションはまだ十分ではありません。私たちは、これらのアプリケーションを構築するための継続的な監視と評価のツールを提供しています。\n\n私たちは急成長しており、ドイツのベルリンでチームを拡大しています。エンジニアやテクニカルマーケティングの役割を募集しています。興味がある方は、ぜひコーヒーを飲みながらお話ししましょう。\n\n私たちは、LightspeedやY Combinatorなどの著名な投資家から400万ドルを調達し、TwilioやKhan Academyなどの先進的なAIチームと協力しています。\n\n私たちと働く理由は、協力的な環境です。週に2回のミーティングを行い、作業を計画し、成果を発表します。私たちの文化は、高い信頼に基づいており、リード・ヘイスティングスの原則に触発されています。チームのほとんどのメンバーはリーダーシップの経験があり、プロジェクトに直接貢献することを熱望しています。また、複雑なエンジニアリングの課題に取り組むオープンソースの開発ツールを構築する刺激的な仕事に参加できます。\n\n現在募集している役割には、ユーザーフレンドリーなインターフェースを作成するデザインエンジニア、データ集約型のバックエンドシステムを開発・維持するバックエンドエンジニア、新機能をゼロから立ち上げるプロダクトエンジニア、コミュニティと関わり教育コンテンツを作成するデベロッパーアドボケイト、企業顧客をサポートし市場戦略を改善するファウンディングGTMエンジニアがあります。\n\n応募するには、履歴書またはLinkedInプロフィールを careers@langfuse.com に送信してください。候補者はベルリン在住または移住可能である必要があります。\n\nLangfuseに参加する理由は、活気あるオープンソースコミュニティの中で影響力のあるプロジェクトに取り組むことができる点です。製品開発を中心とした強い対面のチーム文化を楽しむことができ、興味深いエンジニアリングの課題に直面しながら成長と責任を持つ機会があります。革新的なLLMアプリケーションを探求するチームと協力することもできます。\n\n皆様からのご連絡をお待ちしています。"
    }
  },
  {
    "id": "b1a4fad7c98c2484",
    "title": {
      "en": "Mruby/C and picoruby: high level langs in low-level devices?",
      "ko": "저수준 기기, 고수준 언어?",
      "ja": "低レベルで高級言語？"
    },
    "type": "story",
    "url": "https://github.com/picoruby/picoruby",
    "score": 68,
    "by": "rahil627",
    "time": 1742721642,
    "content": "PicoRuby\nPicoRuby is an alternative mruby implementation which is:\n\nSmall foot print\n\nROM: 256 KB (depending on build config)\nRAM: 128 KB or less (depending on app code)\n(Figures in 32 bit architecture)\n\nPortable\n\nDepends on only standard C library such as glibc, Newlib or Newlib-nano\n\nReference microcontroller boards\n\nRaspberry Pi Pico - Arm Cortex-M0+, 264 KB RAM, 2 MB Flash\n\nAPI documentation with some demo videos\nhttps://picoruby.github.io/\n\nDepends on\n\nmruby/c (mrubyc/mrubyc): Another implementation of mruby virtual machine\n\nUsed by\n\nPRK Firmware: Keyboard firmware for Raspberry Pi Pico\n\npicoruby.wasm: PicoRuby WASI runtime for WebAssembly\n\nBuild\n\nPrerequisites\n\nC toolchain\ngit\nruby (should be CRuby 3.0+)\n\ngit clone --recursive https://github.com/picoruby/picoruby\ncd picoruby/\nrake\n# PICORUBY_DEBUG=1 rake                         # for debug build\n# PICORUBY_DEBUG=1 PICORUBY_NO_LIBC_ALLOC=1 rake  # for debug build using mruby/c's memory allocator\nbin/picoruby -e 'puts \"Hello World!\"'\n\nCross compilation\nSee an example: build_config/r2p2-cortex-m0plus.rb\nBinaries\nrake command will make three kinds of executable binary\n\nbin/picorbc\n\nbin/picorbc path/to/source.rb makes path/to/source.mrb that is VM code runs on an mruby-compatible virtual machine\n\nbin/picoruby\n\nbin/picoruby source.rb executes Ruby just like normal ruby command\nYou can do like bin/picoruby path/to/your_script.rb to run your script\n\nbin/r2p2\n\nPOSIX version of R2P2 (https://github.com/picoruby/R2P2)\n\nRoadmap\nPicoRuby is still developing halfway towards finishing as of 2024.\nSee implementation roadmap on issue/6\nContributing to PicoRuby\nFork, patch, then send a pull request.\nAcknowledgement\nPart of this project was coded by Monstarlab with the support of\nthe Ruby Association Grant Program\n2020\nand\n2021.\nSee also picoruby/picoruby/wiki.\nStargazers over time\n\nLicense\nCopyright © 2020-2024 HASUMI Hitoshi. See MIT-LICENSE for further details.\nCopyright © 2020-2021 Monstarlab. See MIT-LICENSE for further details.",
    "summary": {
      "en": "**PicoRuby Summary**\n\nPicoRuby is a lightweight implementation of mruby designed for small devices. Here are the key points:\n\n- **Size & Resources**: \n  - ROM usage is around 256 KB.\n  - RAM usage is 128 KB or less.\n  \n- **Portability**: \n  - It only requires standard C libraries like glibc or Newlib.\n  \n- **Supported Boards**: \n  - Works on microcontrollers, such as the Raspberry Pi Pico (Arm Cortex-M0+ with 264 KB RAM and 2 MB Flash).\n\n- **Dependencies**: \n  - Uses mruby/c for its virtual machine.\n\n- **Usage**: \n  - Commonly used for projects like keyboard firmware (PRK Firmware) and a WASI runtime for WebAssembly (picoruby.wasm).\n\n- **Building**: \n  - Requires a C toolchain, git, and Ruby (CRuby 3.0 or higher).\n  - You can clone the repository and build it using the `rake` command.\n\n- **Executable Binaries**: \n  - The build process creates three types of executables: \n    - `picorbc` (compiles Ruby code to VM code),\n    - `picoruby` (executes Ruby scripts),\n    - `r2p2` (a POSIX version of R2P2).\n\n- **Development Status**: \n  - PicoRuby is still under development and plans for future enhancements are available.\n\n- **Contribution**: \n  - Developers can contribute by forking the project and submitting pull requests.\n\n- **Acknowledgments**: \n  - The project received support from the Ruby Association and was partly developed by Monstarlab.\n\n- **License**: \n  - It is licensed under the MIT License. \n\nFor more information, visit the [PicoRuby documentation](https://picoruby.github.io/).",
      "ko": "PicoRuby는 소형 장치를 위해 설계된 mruby의 경량 구현입니다. 주요 내용은 다음과 같습니다.\n\nPicoRuby는 약 256KB의 ROM과 128KB 이하의 RAM을 사용합니다. 이 구현은 glibc나 Newlib와 같은 표준 C 라이브러리만 필요로 하여 이식성이 뛰어납니다. Raspberry Pi Pico와 같은 마이크로컨트롤러에서 작동하며, 이 장치는 Arm Cortex-M0+ 프로세서를 가지고 있고 264KB의 RAM과 2MB의 플래시 메모리를 갖추고 있습니다.\n\nPicoRuby는 가상 머신을 위해 mruby/c를 사용하며, 키보드 펌웨어(PRK Firmware)와 WebAssembly를 위한 WASI 런타임(picoruby.wasm)과 같은 프로젝트에서 일반적으로 사용됩니다. 빌드를 위해서는 C 툴체인, git, Ruby(CRuby 3.0 이상)가 필요합니다. 저장소를 클론한 후 `rake` 명령어를 사용하여 빌드할 수 있습니다.\n\n빌드 과정에서는 세 가지 유형의 실행 파일이 생성됩니다. `picorbc`는 Ruby 코드를 VM 코드로 컴파일하고, `picoruby`는 Ruby 스크립트를 실행하며, `r2p2`는 R2P2의 POSIX 버전입니다. PicoRuby는 현재 개발 중이며, 향후 개선 계획이 있습니다.\n\n개발자들은 프로젝트를 포크하고 풀 리퀘스트를 제출하여 기여할 수 있습니다. 이 프로젝트는 Ruby Association의 지원을 받았으며, Monstarlab에 의해 부분적으로 개발되었습니다. 라이선스는 MIT 라이선스입니다. 더 많은 정보는 PicoRuby 문서를 방문하면 확인할 수 있습니다.",
      "ja": "PicoRubyは、小型デバイス向けに設計されたmrubyの軽量実装です。主な特徴は以下の通りです。\n\nPicoRubyのROM使用量は約256KBで、RAM使用量は128KB以下です。標準のCライブラリであるglibcやNewlibのみが必要で、移植性に優れています。Raspberry Pi Picoなどのマイクロコントローラーで動作し、これにはArm Cortex-M0+プロセッサが搭載されており、264KBのRAMと2MBのフラッシュメモリがあります。\n\nPicoRubyは、仮想マシンとしてmruby/cを使用しています。主にキーボードファームウェア（PRKファームウェア）やWebAssembly用のWASIランタイム（picoruby.wasm）などのプロジェクトで利用されています。\n\nビルドにはCツールチェーン、git、Ruby（CRuby 3.0以上）が必要です。リポジトリをクローンし、`rake`コマンドを使ってビルドすることができます。ビルドプロセスでは、3種類の実行可能バイナリが生成されます。これには、RubyコードをVMコードにコンパイルする`picorbc`、Rubyスクリプトを実行する`picoruby`、POSIX版のR2P2である`r2p2`が含まれます。\n\nPicoRubyは現在も開発中で、今後の機能拡張の計画があります。開発者はプロジェクトをフォークし、プルリクエストを送ることで貢献できます。このプロジェクトはRuby協会からの支援を受けており、一部はMonstarlabによって開発されました。ライセンスはMITライセンスです。\n\n詳細については、PicoRubyのドキュメントを参照してください。"
    }
  },
  {
    "id": "b8521acaff2ed2e2",
    "title": {
      "en": "Yahoo is selling TechCrunch to private equity firm Regent",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://techcrunch.com/2025/03/21/techcrunch-has-personal-news/",
    "score": 71,
    "by": "Garbage",
    "time": 1742626489,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "ba3512ae44fff864",
    "title": {
      "en": "Arc-AGI-2 and ARC Prize 2025",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://arcprize.org/blog/announcing-arc-agi-2-and-arc-prize-2025",
    "score": 171,
    "by": "gkamradt",
    "time": 1742848530,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "55139fbaabff0af1",
    "title": {
      "en": "Aiter: AI Tensor Engine for ROCm",
      "ko": "아이터: ROCm을 위한 AI 텐서 엔진",
      "ja": "Aiter: AIの力"
    },
    "type": "story",
    "url": "https://rocm.blogs.amd.com/software-tools-optimization/aiter:-ai-tensor-engine-for-rocm™/README.html",
    "score": 176,
    "by": "hochmartinez",
    "time": 1742724713,
    "content": "AITER: AI Tensor Engine For ROCm#\n\n.author_string {\n    font-size: 14px;\n    font-weight: 300;\n    line-height: 22.5px;\n    text-transform: none;\n    margin-bottom: 8px;\n    font-family: var(--pst-font-family-base);\n    display: inline-block;\n    vertical-align: middle;\n}\n\n.bd-main .bd-content .bd-article-container {\n    max-width: 1750px !important;\n    width: 100% !important;\n    margin: 0 auto !important;\n}\n\n@media (min-width: 1200px) {\n    .bd-main .bd-content .bd-article-container .bd-article {\n        padding-left: 2rem;\n    }\n}\n\n.bd-container {\n    max-width: 1750px !important;\n    width: 100% !important;\n    margin: 0 auto !important;\n}\n\n.author_string {\n\n    font-size: 14px;\n\n    font-weight: 300;\n\n    line-height: 22.5px;\n\n    text-transform: none;\n\n    margin-bottom: 8px;\n\n    font-family: var(--pst-font-family-base);\n\n    display: inline-block;\n\n    vertical-align: middle;\n\n}\n\n.author_string_right {\n    font-size: 14px;\n    font-weight: 300;\n    line-height: 22.5px;\n    text-transform: none;\n    margin-bottom: 8px;\n    font-family: var(--pst-font-family-base);\n    display: inline-block;\n    vertical-align: middle;\n    flex-direction: row-reverse;\n}\n\n.author_string:last-child {\n    margin-bottom: 0px;\n}\n\n.author_string svg {\n    margin-right: 5px;\n}\n\n.bd-sidebar-primary {\n    display: none;\n}\n\n.sidebar-toggle {\n    display: none;\n}\n\nlabel.sidebar-toggle.primary-toggle.btn.btn-sm {\n    display: none;\n}\n\n.author_row {\n    display: flex;\n    justify-content: space-between;\n    align-items: flex-start;\n    width: 100%;\n    margin-bottom: 3px;\n    padding: 2px;\n}\n\n.author_row_right {\n    display: flex;\n    justify-content: space-between;\n    align-items: flex-start;\n    width: 100%;\n    margin-bottom: 3px;\n    padding: 2px;\n    flex-direction: row-reverse;\n}\n\n.author_right {\n    display: flex;\n    flex-direction: column-reverse;\n    max-width: 50%;\n    word-wrap: break-word;\n    align-items: right;\n}\n\n.author_left {\n    display: flex;\n    flex-direction: column-reverse;\n    max-width: 50%;\n    word-wrap: break-word;\n    align-items: left;\n}\n\n.image-container {\n    display: flex;\n    align-items: center;\n    justify-content: center;\n    padding: 5px;\n}\n\n.responsive-image {\n    width: 100%;\n    height: 480px;\n    object-fit: cover;\n    border: 5px solid #333;\n    box-shadow: 0 0 10px rgba(0,0,0,0.5);\n}\n\n             March 21, 2025 by Shekhar Pandey, Liz Li, Carlus Huang, Lingpeng Jin, Anshul Gupta.\n\n            5 min read. | 1331 total words.\n\n            Software tools & optimizations\n\n            AI/ML\n\n.icon-bar.fixed,\n.icon-bar.horizontal {\n    display: none;\n}\n\n@media screen and (min-width: 1520px) {\n    .icon-bar.fixed {\n        display: flex;\n        position: fixed;\n        top: 50%;\n        right: 0;\n        transform: translateY(-50%);\n        flex-direction: column;\n        z-index: 1000;\n    }\n    .icon-bar.fixed a {\n        display: block;\n        text-align: center;\n        padding: 16px;\n        font-size: 20px;\n        color: white;\n        transition: background-color 0.3s, color 0.3s;\n        background-size: 25px 25px;\n    }\n    .icon-bar.fixed a svg {\n        width: 25px;\n        height: 25px;\n    }\n    .icon-bar.fixed a:hover {\n        background-color: #000;\n    }\n}\n\n@media screen and (max-width: 1520px) {\n    .icon-bar.horizontal {\n        display: flex;\n        flex-direction: row;\n        justify-content: flex-start;\n        margin: 20px 0;\n    }\n    .icon-bar.horizontal a {\n        width: 30px;\n        height: 30px;\n        display: flex;\n        align-items: center;\n        justify-content: center;\n        font-size: 14px;\n        color: white;\n        margin-left: 8px;\n        transition: background-color 0.3s, color 0.3s;\n        background-size: 30px 30px;\n        padding: 4px;\n    }\n    .icon-bar.horizontal a svg {\n        width: 20px;\n        height: 20px;\n    }\n    .icon-bar.horizontal a:first-child {\n        margin-left: 0;\n    }\n    .icon-bar.horizontal a:hover {\n        background-color: #000;\n    }\n}\n\n.facebook {\n    background: #3B5998;\n}\n.twitter {\n    background: #55ACEE;\n}\n.reddit {\n    background: #dd4b39;\n}\n.linkedin {\n    background: #007bb5;\n}\n.google {\n    background: #bb0000;\n}\n\nPerformance optimization is critical when working with GPUs, especially for tasks involving artificial intelligence, which can be extremely demanding. To fully leverage the capabilities of advanced hardware, it’s essential to master optimization strategies and ensure every available resource is utilized efficiently. In this blog we will provide an overview ofAMD’s AI Tensor Engine for ROCm (AITER) and show you how easy it is to integrate AITER kernels in basic LLM training and inference workload.AITER helps developers to focus on creating operators while allowing customers to seamlessly integrate this operator collection into their own private, public, or any custom framework.\n\nWhat is AI Tensor Engine for ROCm (AITER)#\nAMD is introducing the AI Tensor Engine for ROCm (AITER), acentralized repository filled with high-performance AI operators [1]designed to accelerate various AI workloads. AITER serves as a unified platform where customers can easily find and integrate optimized operators into their existing frameworks—be it private, public, or custom-built as you can see in Figure 1 below. With AITER, AMD simplifies the complexity of optimization, enabling users to maximize performance while providing flexibility to meet diverse AI requirements.\n\nFigure 1: Block level diagram of AITER#\n\nKey Features#\n\nVersatile and User-Friendly Design: AITER’s architecture is carefully crafted for versatility and ease of use, allowing seamless integration into various workflows and systems.\nDual Programming Interfaces: At the highest abstraction level, AITER supports two primary interfaces—C++ and Python (Torch API). This dual-interface approach makes AITER highly accessible, catering to developers with different programming preferences and skillsets.\nRobust Kernel Infrastructure: Underneath the user-level APIs, AITER employs a powerful and robust kernel infrastructure. This infrastructure is built upon a variety of underlying technologies, including Triton, CK (Compute Kernel), ASM (Assembly), and HIP (Heterogeneous Interface for Portability).\nComprehensive Kernel Support: The AITER kernel ecosystem efficiently supports diverse computational tasks such as inference workloads, training kernels, GEMM (General Matrix Multiplication) operations, and communication kernels. Such comprehensive kernel support ensures that users can confidently handle complex and resource-intensive AI tasks.\nCustomizable and Optimizable Kernel Ecosystem: With its rich kernel environment, AITER allows developers to perform customized optimizations tailored specifically to their applications. This flexibility helps developers to bypass or overcome architectural limitations, resulting in significantly enhanced performance and adaptability.\nSeamless Integration with AMD ROCm: At its core, AITER leverages AMD’s ROCm, ensuring efficient bridging between optimized kernels and AMD GPUs. This integration unlocks the full potential and peak performance of AMD GPUs, delivering optimal efficiency across a wide range of AI workloads.\n\nBy combining user-friendly interfaces, extensive kernel capabilities, and robust GPU integration, AITER empowers developers to achieve maximum efficiency and performance in their AI applications.\n\nPerformance Gains with AITER#\nBy leveraging AITER’s advanced optimizations, users can experience significant performance improvements across various AI operations:\n\nAITER block-scale GEMM: Achieves up to 2x performance boost [2], substantially accelerating general matrix multiplication tasks.\nAITER block-scale fused MoE: Delivers up to 3x performance boost [3], optimizing the efficiency of Mixture of Experts (MoE) operations.\nAITER MLA for decode: Provides an impressive up to 17x performance boost[4], dramatically enhancing decoding efficiency.\nAITER MHA for prefill: Realizes up to 14x performance boost[5], significantly improving Multi-Head Attention (MHA) performance during prefill stages.\n\nNote: As of this blog’s publication, AITER currently provides a performance boost for DeepSeek models on Instinct GPUs via the vLLM and SGLang frameworks. Support for additional       state-of-the-art models is currently under active development and will be added in future vLLM and SGLang Docker releases.\n\nAITER’s Integration in vLLM/SGLang for DeepSeek V3/R1#\nThe integration of AITER into vLLM/SGLang for the DeepSeek v3/r1 model has led to remarkable improvements in total token throughput (tokens per second, tok/s). Before AITER’s integration, the throughput stood at 6484.76 tok/s. After incorporating AITER’s optimizations, throughput dramatically increased to 13704.36 tok/s, marking more than a 2x improvement[6] in processing speed as shown in Figure 2 below.\n\nFigure 2.Throughput Comparison: Before and After Integrating AITER in SGLang on DeepSeek Models on AMD Instinct™ MI300X.#\n\nRunning Deepseek with AITER#\nusing vLLM\nVLLM_SEED=42 VLLM_MLA_DISABLE=0 VLLM_USE_TRITON_FLASH_ATTN=0 \\\nVLLM_USE_ROCM_FP8_FLASH_ATTN=0 VLLM_FP8_PADDING=1 VLLM_USE_AITER_MOE=1 \\\nVLLM_USE_AITER_BLOCK_GEMM=1 VLLM_USE_AITER_MLA=0 vllm serve \\\n\"deepseek-ai/DeepSeek-V3\" \\\n  --host 0.0.0.0 \\\n  --port 8000 \\\n  --api-key abc-123 \\\n  --tensor-parallel-size 8 \\\n  --trust-remote-code \\\n  --seed 42\n\n  Copy to clipboard\n\nusing SGLang\nCK_BLOCK_GEMM=1 SGLANG_ROCM_AITER_BLOCK_MOE=1 RCCL_MSCCL_ENABLE=0 \\\nDEBUG_HIP_BLOCK_SYN=1024 GPU_FORCE_BLIT_COPY_SIZE=64 \\\npython3 -m sglang.launch_server  --model \"deepseek-ai/DeepSeek-V3\" \\\n --tp 8 --trust-remote-code\n\n  Copy to clipboard\n\nGetting Started with AITER#\nTo begin working with AITER, follow these simple installation steps:\n\nClone the repository:\n\ngit clone https://github.com/ROCm/aiter.git\ncd AITER\n\n  Copy to clipboard\n\nUnder the AITER root directory, run the following command to install the library in development mode:\n\npython3 setup.py develop\n\n  Copy to clipboard\n\nImplementing a Simple Linear Layer Using AITER#\nLet’s demonstrate how you can implement a simple replica of PyTorch’s linear layer using AITER’s tgemm function.\nfrom aiter.tuned_gemm import tgemm\nimport torch\n\nclass LinearLayer(torch.nn.Module):\n\tdef __init__(self, in_features, out_features):\n\t\tsuper(LinearLayer, self).__init__()\n\t\tself.weight = torch.nn.Parameter(torch.randn(out_features, in_features).cuda())\n\t\tself.bias = torch.nn.Parameter(torch.randn(out_features).cuda())\n\n\tdef forward(self, input):\n\t\tinput = input.cuda()\n\t\treturn tgemm.mm(input, self.weight, self.bias, None, None)\n\n# Define input size and layer size\nin_features = 128\nout_features = 64\nbatch_size = 32\n\n# Create custom AITER linear layer\nlayer = LinearLayer(in_features, out_features).cuda()\ninput_tensor = torch.randn(batch_size, in_features).cuda()\n\n# Get output from AITER linear layer\noutput_aiter = layer(input_tensor)\n\n# Create PyTorch linear layer with same weights and bias\npytorch_layer = torch.nn.Linear(in_features, out_features).cuda()\npytorch_layer.weight = torch.nn.Parameter(layer.weight.clone())\npytorch_layer.bias = torch.nn.Parameter(layer.bias.clone())\n\n# Get output from PyTorch linear layer\noutput_pytorch = pytorch_layer(input_tensor)\n\n# Compare outputs\nprint(\"Output difference (max absolute error):\", torch.max(torch.abs(output_aiter - output_pytorch)))\nprint(\"Output difference (mean absolute error):\", torch.mean(torch.abs(output_aiter - output_pytorch)))\n\n  Copy to clipboard\n\nIt can be very simple using AITER in daily workload, some of the other low level kernel APIs are mentioned as below which can be used to integrate in your architecture.\n\nKernel\nAPI\n\nMHA (Flash Attention)\naiter.flash_attn_func()\n\nLayerNorm\naiter.layer_norm()\n\nLayerNormFusedResidualAdd\naiter.layernorm2d_with_add_asm()\n\nRoPE forward\naiter.rope_fwd()\n\nRoPE backward\naiter.rope_bwd()\n\nRMSNorm\naiter.rms_norm()\n\nMLA Decode\naiter.ops.triton.mla_decode()\n\nAITER is not just limited to the above mentioned APIs, there are a lot of features available as mentioned in the below table and a lot are coming very soon.\n\nFeature\nType (F=Forward, B=Backward)\nDetails\n\nPrefill Attention\nF/B\nFav3 FWD FP16/BF16  Fav3 BWD FP16/BF16  MLA FP16/BF16  FA FP8 FWD+BWD (Block-Scale)  Chunked-prefill\n\nDecode Attention\nF\nPaged Attention FP16/BF16  Paged Attention FP8 per-tensor quant  Paged Attention FP8/INT8 with KV per-token quant  KVCache Update & Rotary  Batched Decoding  MLA Decoding\n\nFused-Moe\nF\nMoe-Sorting kernel and tiling solution  FP16/BF16 per-token Fused-Moe  FP8/INT8 per-token Fused-Moe  FP8 per-tensor Fused-Moe  FP8/INT4 per-tensor Fused-Moe  Fused-FFN\n\nLow Precision Gemm\nF\nFP8 per-token/channel Gemm  FP8 Block Scale Gemm  INT8 weight-only Gemm\n\nDistributed Gemm\nF/B\nDistributed GEMM\n\nNormalization and Fusion\nF\nLayernorm+quant/shortcut  RMSNorm+quant/shortcut\n\nCustom Comm.\nF\nAR/AG fused with normalization  AR/AG quantized, Optimized hipgraph support\n\nConv2d/2d\nF/B\nFP16/BF16 fwd/bwd/wrw  Fusion with bias/activation, etc.\n\nSummary#\nIn this blog we introduced AMD’s AI Tensor Engine for ROCm (AITER), our centralized high performance AI operators repository, designed to significantly accelerate AI workloads on AMD GPUs. AITER has already demonstrated its value by substantially accelerating AI workloads and significantly improving efficiency and performance. AMD remains committed to continuous innovation, with numerous further enhancements and optimization efforts currently underway. The roadmap includes even greater advancements, which promise to set new standards in AI computation. Stay tuned as AMD continues to push the boundaries of performance, ensuring machine learning engineers can consistently achieve faster, more efficient, and more powerful AI solutions.\n\nUpdated on 24 March 2025\nA note around the current scope of AITER was added under the Performance Gains with AITER section.\n\nAdditional Resources#\nAITER Github: ROCm/aiter\n\nDisclaimers#\nThird-party content is licensed to you directly by the third party that owns the\ncontent and is not licensed to you by AMD. ALL LINKED THIRD-PARTY CONTENT IS\nPROVIDED “AS IS” WITHOUT A WARRANTY OF ANY KIND. USE OF SUCH THIRD-PARTY CONTENT\nIS DONE AT YOUR SOLE DISCRETION AND UNDER NO CIRCUMSTANCES WILL AMD BE LIABLE TO\nYOU FOR ANY THIRD-PARTY CONTENT. YOU ASSUME ALL RISK AND ARE SOLELY RESPONSIBLE\nFOR ANY DAMAGES THAT MAY ARISE FROM YOUR USE OF THIRD-PARTY CONTENT.\n\n[1]\nAI operators :  optimized mathematical functions or computational kernels that perform fundamental AI and machine learning tasks, such as matrix multiplications, convolutions, and activations, which are crucial for accelerating AI workloads.\n\n[2]\nOn average, a system configured with an AMD Instinct™ MI300X GPU shows that AITER block-scale GEMM offers a 2x performance boost substantially accelerating general matrix multiplication tasks. Testing done by AMD on 03/011/2025, results may vary based on configuration, usage, software version, and optimizations.\nSYSTEM CONFIGURATION: AMD Instinct™ MI300X platform\n- System Model: Supermicro GPU A+ Server AS - 8125GS-TNMR2\n- CPU: 2x AMD EPYC 9654 96-core Processor (2 sockets, 96 cores per socket, 2 threads per core)\n- NUMA Config: 2 NUMA node per socket\n- Memory: 2.3 TiB (24 DIMMs, 4800 mts, 96 GiB/DIMM)\n- Disk: Root drive + Data drive combined:\n- 2x 960GB Samsung MZ1L2960HCJR-00A07\n- 4x 3.84TB Samsung MZQL23T8HCLS-00A07\n- GPU: 8x AMD MI300X 192GB HBM3 750W\n- Host OS: Ubuntu 22.04.4 LTS with Linux kernel 5.15.0-116-generic.\n- System BIOS: 3.2\n- System Bios Vendor:   American Megatrends International, LLC.\n- Host GPU Driver (amdgpu version): 6.10.5\n\n[3]\nOn average, a system configured with an AMD Instinct™ MI300X GPU shows AITER block-scale fused MoE offers a 3x performance boost, optimizing the efficiency of Mixture of Experts (MoE) operations.  Testing done by AMD on 03/011/2025, results may vary based on configuration, usage, software version, and optimizations.\nSYSTEM CONFIGURATION: AMD Instinct™ MI300X platform\n- System Model: Supermicro GPU A+ Server AS - 8125GS-TNMR2\n- CPU: 2x AMD EPYC 9654 96-core Processor (2 sockets, 96 cores per socket, 2 threads per core)\n- NUMA Config: 2 NUMA node per socket\n- Memory: 2.3 TiB (24 DIMMs, 4800 mts, 96 GiB/DIMM)\n- Disk: Root drive + Data drive combined:\n- 2x 960GB Samsung MZ1L2960HCJR-00A07\n- 4x 3.84TB Samsung MZQL23T8HCLS-00A07\n- GPU: 8x AMD MI300X 192GB HBM3 750W\n- Host OS: Ubuntu 22.04.4 LTS with Linux kernel 5.15.0-116-generic.\n- System BIOS: 3.2\n- System Bios Vendor:   American Megatrends International, LLC.\n- Host GPU Driver (amdgpu version): 6.10.5\n\n[4]\nOn average, a system configured with an AMD Instinct™ MI300X GPU shows that AITER MLA for decode offers a 17x performance boost enhancing decoding efficiency.  Testing done by AMD on 03/011/2025, results may vary based on configuration, usage, software version, and optimizations.\nSYSTEM CONFIGURATION: AMD Instinct™ MI300X platform\n- System Model: Supermicro GPU A+ Server AS - 8125GS-TNMR2\n- CPU: 2x AMD EPYC 9654 96-core Processor (2 sockets, 96 cores per socket, 2 threads per core)\n- NUMA Config: 2 NUMA node per socket\n- Memory: 2.3 TiB (24 DIMMs, 4800 mts, 96 GiB/DIMM)\n- Disk: Root drive + Data drive combined:\n- 2x 960GB Samsung MZ1L2960HCJR-00A07\n- 4x 3.84TB Samsung MZQL23T8HCLS-00A07\n- GPU: 8x AMD MI300X 192GB HBM3 750W\n- Host OS: Ubuntu 22.04.4 LTS with Linux kernel 5.15.0-116-generic.\n- System BIOS: 3.2\n- System Bios Vendor:   American Megatrends International, LLC.\n- Host GPU Driver (amdgpu version): 6.10.5\n\n[5]\nOn average, a system configured with an AMD Instinct™ MI300X GPU with AITER MHA for prefill shows a14x performance boost, improving Multi-Head Attention (MHA) performance during prefill stages.Testing done by AMD on 03/011/2025, results may vary based configuration, usage, software version, and optimizations.\nSYSTEM CONFIGURATION: AMD Instinct™ MI300X platform\n- System Model: Supermicro GPU A+ Server AS - 8125GS-TNMR2\n- CPU: 2x AMD EPYC 9654 96-core Processor (2 sockets, 96 cores per socket, 2 threads per core)\n- NUMA Config: 2 NUMA node per socket\n- Memory: 2.3 TiB (24 DIMMs, 4800 mts, 96 GiB/DIMM)\n- Disk: Root drive + Data drive combined:\n- 2x 960GB Samsung MZ1L2960HCJR-00A07\n- 4x 3.84TB Samsung MZQL23T8HCLS-00A07\n- GPU: 8x AMD MI300X 192GB HBM3 750W\n- Host OS: Ubuntu 22.04.4 LTS with Linux kernel 5.15.0-116-generic.\n- System BIOS: 3.2\n- System Bios Vendor:   American Megatrends International, LLC.\n- Host GPU Driver (amdgpu version): 6.10.5\n\n[6]\nOn average, a system configured with an AMD Instinct™ MI300X GPU shows that before AITER’s integration, the throughput stood at 6484.76 tok/s. After incorporating AITER’s optimizations, throughput dramatically increased to 13704.36 tok/s, marking more than a 2x improvement in processing speed. Testing done by AMD on 03/11/2025, results may vary based on configuration, usage, software version, and optimizations.\nSYSTEM CONFIGURATION: AMD Instinct™ MI300X platform\n- System Model: Supermicro GPU A+ Server AS - 8125GS-TNMR2\n- CPU: 2x AMD EPYC 9654 96-core Processor (2 sockets, 96 cores per socket, 2 threads per core)\n- NUMA Config: 2 NUMA node per socket\n- Memory: 2.3 TiB (24 DIMMs, 4800 mts, 96 GiB/DIMM)\n- Disk: Root drive + Data drive combined:\n- 2x 960GB Samsung MZ1L2960HCJR-00A07\n- 4x 3.84TB Samsung MZQL23T8HCLS-00A07\n- GPU: 8x AMD MI300X 192GB HBM3 750W\n- Host OS: Ubuntu 22.04.4 LTS with Linux kernel 5.15.0-116-generic.\n- System BIOS: 3.2\n- System Bios Vendor:   American Megatrends International, LLC.\n- Host GPU Driver (amdgpu version): 6.10.5",
    "summary": {
      "en": "**Summary of AMD's AI Tensor Engine for ROCm (AITER)**\n\nAITER is AMD's new AI Tensor Engine designed to enhance performance for artificial intelligence tasks on AMD GPUs. It provides a centralized collection of high-performance AI operators that are easy to integrate into various frameworks—private, public, or custom. \n\n**Key Features:**\n- **User-Friendly Design:** AITER is built for versatility and ease of integration.\n- **Dual Programming Interfaces:** Supports both C++ and Python (Torch API), catering to different developer preferences.\n- **Strong Kernel Infrastructure:** Utilizes advanced technologies like Triton and HIP to support various computational tasks.\n- **Comprehensive Kernel Support:** Handles tasks like inference and training efficiently, ensuring high performance for demanding AI workloads.\n- **Customizable Kernels:** Developers can optimize kernels specifically for their needs, enhancing performance.\n\n**Performance Improvements:**\nAITER offers substantial boosts in performance, such as:\n- Up to **2x** faster general matrix multiplication.\n- Up to **3x** faster efficiency for Mixture of Experts operations.\n- Up to **17x** faster decoding efficiency.\n- Up to **14x** faster Multi-Head Attention performance during prefill.\n\n**Integration and Use:**\nAITER has been integrated into models like DeepSeek, significantly increasing processing speed from 6484.76 tokens per second to 13704.36 tokens per second after optimization.\n\n**Getting Started:**\nDevelopers can start using AITER by cloning the repository, installing the library, and implementing simple functions with its APIs.\n\nIn conclusion, AITER aims to provide powerful tools for developers, facilitating faster and more efficient AI solutions while AMD continues to innovate and improve its offerings in the AI space.",
      "ko": "AITER는 AMD의 새로운 AI 텐서 엔진으로, AMD GPU에서 인공지능 작업의 성능을 향상시키기 위해 설계되었습니다. 이 엔진은 다양한 프레임워크에 쉽게 통합할 수 있는 고성능 AI 연산자의 중앙 집중형 모음을 제공합니다.\n\nAITER의 주요 특징은 사용자 친화적인 디자인으로 다재다능성과 통합의 용이성을 강조합니다. C++와 Python(토치 API) 두 가지 프로그래밍 인터페이스를 지원하여 개발자의 다양한 선호를 충족합니다. 또한, Triton과 HIP와 같은 첨단 기술을 활용하여 다양한 계산 작업을 지원하는 강력한 커널 인프라를 갖추고 있습니다. AITER는 추론과 훈련과 같은 작업을 효율적으로 처리하여 요구가 높은 AI 작업에서도 높은 성능을 보장합니다. 개발자는 자신의 필요에 맞게 커널을 최적화할 수 있어 성능을 더욱 향상시킬 수 있습니다.\n\nAITER는 성능 개선을 통해 일반 행렬 곱셈에서 최대 2배, 전문가 혼합 작업에서 최대 3배, 디코딩 효율에서 최대 17배, 프리필 중 다중 헤드 어텐션 성능에서 최대 14배의 속도 향상을 제공합니다. \n\nAITER는 DeepSeek와 같은 모델에 통합되어 최적화 후 처리 속도가 초당 6484.76 토큰에서 13704.36 토큰으로 크게 증가했습니다. \n\n개발자는 AITER를 사용하기 위해 저장소를 복제하고 라이브러리를 설치한 후 API를 활용하여 간단한 기능을 구현할 수 있습니다. AITER는 개발자에게 강력한 도구를 제공하여 더 빠르고 효율적인 AI 솔루션을 가능하게 하며, AMD는 AI 분야에서 지속적으로 혁신하고 제품을 개선해 나가고 있습니다.",
      "ja": "AITERは、AMDが開発した新しいAIテンソルエンジンで、AMDのGPU上での人工知能タスクの性能を向上させることを目的としています。このエンジンは、高性能なAIオペレーターを集中的に提供し、プライベート、パブリック、カスタムのさまざまなフレームワークに簡単に統合できるように設計されています。\n\nAITERの主な特徴には、使いやすいデザインがあり、さまざまなニーズに応じて柔軟に統合できる点が挙げられます。また、C++とPython（Torch API）の両方に対応した二重のプログラミングインターフェースを持ち、開発者の好みに応じた選択が可能です。さらに、TritonやHIPなどの先進技術を活用した強力なカーネルインフラストラクチャを備えており、さまざまな計算タスクをサポートします。推論やトレーニングなどのタスクを効率的に処理し、要求の厳しいAIワークロードに対して高い性能を確保します。開発者は、自分のニーズに合わせてカーネルを最適化することもでき、性能を向上させることが可能です。\n\nAITERは、性能の大幅な向上を提供します。例えば、一般的な行列の掛け算が最大で2倍速くなり、Mixture of Expertsの操作は最大で3倍の効率を実現します。また、デコーディングの効率は最大で17倍、プレフィル中のマルチヘッドアテンションの性能は最大で14倍向上します。\n\nAITERは、DeepSeekなどのモデルに統合されており、最適化後には処理速度が6484.76トークン毎秒から13704.36トークン毎秒に大幅に向上しました。\n\n開発者は、リポジトリをクローンし、ライブラリをインストールすることでAITERの使用を開始できます。APIを使って簡単な機能を実装することも可能です。AITERは、開発者に強力なツールを提供し、AMDがAI分野での革新を続ける中で、より迅速で効率的なAIソリューションを実現することを目指しています。"
    }
  },
  {
    "id": "9c8dde4906b72fde",
    "title": {
      "en": "Gatehouse – a composable, async-friendly authorization policy framework in Rust",
      "ko": "게이트하우스: 러스트의 비동기 인증 프레임워크",
      "ja": "ゲートハウス：Rustの認証フレームワーク"
    },
    "type": "story",
    "url": "https://github.com/thepartly/gatehouse",
    "score": 68,
    "by": "hardbyte",
    "time": 1742856647,
    "content": "Gatehouse\n\nA flexible authorization library that combines role-based (RBAC), attribute-based (ABAC), and relationship-based (ReBAC) access control policies.\n\nFeatures\n\nMulti-paradigm Authorization: Support for RBAC, ABAC, and ReBAC patterns\nPolicy Composition: Combine policies with logical operators (AND, OR, NOT)\nDetailed Evaluation Tracing: Complete decision trace for debugging and auditing\nFluent Builder API: Construct custom policies with a PolicyBuilder.\nType Safety: Strongly typed resources/actions/contexts\nAsync Ready: Built with async/await support\n\nCore Components\nPolicy Trait\nThe foundation of the authorization system:\n#[async_trait]\ntrait Policy<Subject, Resource, Action, Context> {\n    async fn evaluate_access(\n        &self,\n        subject: &Subject,\n        action: &Action,\n        resource: &Resource,\n        context: &Context,\n    ) -> PolicyEvalResult;\n}\n\nPermissionChecker\nAggregates multiple policies (e.g. RBAC, ABAC) with OR logic by default: if any policy grants access, permission is granted.\nlet mut checker = PermissionChecker::new();\nchecker.add_policy(rbac_policy);\nchecker.add_policy(owner_policy);\n\n// Check if access is granted\nlet result = checker.evaluate_access(&user, &action, &resource, &context).await;\nif result.is_granted() {\n    // Access allowed\n} else {\n    // Access denied\n}\n\nPolicyBuilder\nThe PolicyBuilder provides a fluent API to construct custom policies by chaining predicate functions for\nsubjects, actions, resources, and context. Once built, the policy can be added to a [PermissionChecker].\nlet custom_policy = PolicyBuilder::<MySubject, MyResource, MyAction, MyContext>::new(\"CustomPolicy\")\n    .subjects(|s| /* ... */)\n    .actions(|a| /* ... */)\n    .resources(|r| /* ... */)\n    .context(|c| /* ... */)\n    .when(|s, a, r, c| /* ... */)\n    .build();\n\nBuilt-in Policies\n\nRbacPolicy: Role-based access control\nAbacPolicy: Attribute-based access control\nRebacPolicy: Relationship-based access control\n\nCombinators\nAndPolicy: Grants access only if all inner policies allow access\nOrPolicy: Grants access if any inner policy allows access\nNotPolicy: Inverts the decision of an inner policy\nExamples\nSee the examples directory for complete demonstration of:\n\nRole-based access control (rbac_policy)\nRelationship-based access control (rebac_policy)\nPolicy combinators (combinator_policy)\n\nRun with:\ncargo run --example rbac_policy",
    "summary": {
      "en": "**Gatehouse Overview**\n\nGatehouse is an authorization library that supports different access control methods: role-based (RBAC), attribute-based (ABAC), and relationship-based (ReBAC).\n\n**Key Features:**\n- **Multi-Paradigm Authorization:** Supports RBAC, ABAC, and ReBAC.\n- **Policy Composition:** Combine policies using logical operators like AND, OR, and NOT.\n- **Evaluation Tracing:** Provides detailed tracking of decisions for debugging and auditing.\n- **Fluent Builder API:** Allows easy construction of custom policies.\n- **Type Safety:** Ensures strong typing for resources, actions, and contexts.\n- **Async Ready:** Compatible with async/await programming.\n\n**Core Components:**\n1. **Policy Trait:** The core of the system that evaluates access based on the subject, action, resource, and context.\n2. **PermissionChecker:** Combines multiple policies to determine access. If any policy grants access, permission is granted.\n3. **PolicyBuilder:** A user-friendly API that helps create custom policies by chaining functions for subjects, actions, resources, and context.\n\n**Built-in Policies:**\n- **RbacPolicy:** For role-based access control.\n- **AbacPolicy:** For attribute-based access control.\n- **RebacPolicy:** For relationship-based access control.\n\n**Combinators:**\n- **AndPolicy:** Grants access only if all policies allow it.\n- **OrPolicy:** Grants access if any policy allows it.\n- **NotPolicy:** Reverses the decision of a policy.\n\n**Examples:** \nYou can find examples of each access control method and policy combinators in the provided directory, with a command to run the RBAC example.",
      "ko": "게이트하우스는 다양한 접근 제어 방법을 지원하는 인증 라이브러리입니다. 이 라이브러리는 역할 기반 접근 제어(RBAC), 속성 기반 접근 제어(ABAC), 관계 기반 접근 제어(ReBAC)를 포함합니다.\n\n주요 기능으로는 여러 접근 제어 방식을 지원하는 다중 패러다임 인증, 논리 연산자(AND, OR, NOT)를 사용한 정책 조합, 디버깅과 감사용으로 결정 과정을 상세히 추적할 수 있는 평가 추적 기능이 있습니다. 또한, 사용자 정의 정책을 쉽게 구성할 수 있는 유창한 빌더 API와 자원, 행동, 맥락에 대한 강한 타입 안전성을 보장합니다. 비동기 프로그래밍을 지원하여 async/await와 호환됩니다.\n\n핵심 구성 요소로는 접근을 평가하는 정책 특성, 여러 정책을 결합하여 접근 권한을 결정하는 권한 검사기, 그리고 주체, 행동, 자원 및 맥락에 대한 함수를 연결하여 사용자 정의 정책을 쉽게 생성할 수 있는 정책 빌더가 있습니다.\n\n내장 정책으로는 역할 기반 접근 제어를 위한 RbacPolicy, 속성 기반 접근 제어를 위한 AbacPolicy, 관계 기반 접근 제어를 위한 RebacPolicy가 있습니다.\n\n정책 조합기로는 모든 정책이 허용할 경우에만 접근을 허용하는 AndPolicy, 어떤 정책이라도 허용할 경우 접근을 허용하는 OrPolicy, 정책의 결정을 반전시키는 NotPolicy가 있습니다.\n\n각 접근 제어 방법과 정책 조합기의 예시는 제공된 디렉토리에서 확인할 수 있으며, RBAC 예제를 실행할 수 있는 명령도 포함되어 있습니다.",
      "ja": "Gatehouseは、さまざまなアクセス制御方法をサポートする認可ライブラリです。具体的には、役割ベース（RBAC）、属性ベース（ABAC）、および関係ベース（ReBAC）の方法があります。\n\nGatehouseの主な特徴には、複数の認可方式をサポートするマルチパラダイム認可、AND、OR、NOTなどの論理演算子を使用してポリシーを組み合わせるポリシー合成、デバッグや監査のための詳細な決定追跡を提供する評価トレーシング、カスタムポリシーを簡単に構築できる流暢なビルダーAPI、リソース、アクション、コンテキストに対して強い型安全性を確保するタイプセーフティ、非同期プログラミングに対応した非同期準備があります。\n\nGatehouseのコアコンポーネントには、アクセスを評価するポリシートレイト、複数のポリシーを組み合わせてアクセスを判断するパーミッションチェッカー、ユーザーフレンドリーなAPIでカスタムポリシーを作成するためのポリシービルダーがあります。\n\n組み込みポリシーには、役割ベースのアクセス制御用のRbacPolicy、属性ベースのアクセス制御用のAbacPolicy、関係ベースのアクセス制御用のRebacPolicyがあります。\n\nまた、ポリシーの組み合わせには、すべてのポリシーが許可する場合にのみアクセスを許可するAndPolicy、いずれかのポリシーが許可する場合にアクセスを許可するOrPolicy、ポリシーの決定を反転させるNotPolicyがあります。\n\n各アクセス制御方法やポリシーの組み合わせの例は、提供されたディレクトリにあり、RBACの例を実行するためのコマンドも用意されています。"
    }
  },
  {
    "id": "b3941b721341aca7",
    "title": {
      "en": "David Lynch Presents Interview Project",
      "ko": "데이비드 린치 인터뷰 프로젝트",
      "ja": "リンチの対話プロジェクト"
    },
    "type": "story",
    "url": "https://www.youtube.com/channel/UC4lrTWEywA3JfR-N_dv0d4A",
    "score": 48,
    "by": "toomuchtodo",
    "time": 1742878067,
    "content": "Back\n\n  KR\n\n    Skip navigation\n\n        Search\n\n  Search with your voice\n\nSign in\n\n  KR\n\n  Home\n\n    Home\n\n  Shorts\n\n    Shorts\n\n  Subscriptions\n\n    Subscriptions\n\n  You\n\n    You\n\n  History\n\n    History\n\n          David Lynch Presents Interview Project@davidlynchpresentsIP•15.5K subscribers•158 videosMore about this channel ...more...moreinstagram.com/davidlynchinterviewprojectand 3 more linksSubscribe\n\n              HomeVideosShortsPlaylistsPosts\n\n    Search\n\n  David Lynch Presents Interview Project: TRAILERSearchInfoShoppingTap to unmute2xIf playback doesn't begin shortly, try restarting your device.•David Lynch Presents Interview ProjectSubscribeUnsubscribeYou're signed outVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.CancelConfirmShareInclude playlistAn error occurred while retrieving sharing information. Please try again later.Watch laterShareCopy link0:000:00 / 2:07•Watch full videoLive••\n\n    David Lynch Presents Interview Project: TRAILER\n\n    •\n\n    •\n\n      49,937 views\n\n      9 months ago\n\n  The Interview Project team has decided to re-release all 121 episodes of the original series on October 1, 2024  to commemorate the 15 year anniversary.\n\nEach episode will be shown in high definition, a feature that was previously unavailable to the team.\n\nFor those of you familiar with the series, we hope you enjoy revisiting them. For those new to the series, we hope you enjoy discovering them.\n\nAs David says, \"It's something that's human and you can't stay away from it.\"\n\n  Read more\n\n                Videos\n\n  3:25\n    3:25\n  Now playing\n\nDavid Lynch Presents Interview Project:  Jess\n\n32K views5 months ago\n\n  4:30\n    4:30\n  Now playing\n\nDavid Lynch Presents Interview Project:  Tommie Holliday\n\n13K views5 months ago\n\n  4:19\n    4:19\n  Now playing\n\nDavid Lynch Presents Interview Project:  Kee\n\n10K views5 months ago\n\n  3:33\n    3:33\n  Now playing\n\nDavid Lynch Presents Interview Project:  Palmer Black\n\n4.3K views5 months ago\n\n  4:03\n    4:03\n  Now playing\n\nDavid Lynch Presents Interview Project:  Gordon\n\n5.9K views5 months ago\n\n  4:46\n    4:46\n  Now playing\n\nDavid Lynch Presents Interview Project:  Clara\n\n4K views5 months ago\n\n                Created playlists\n\n    121 videosDavid Lynch Presents Interview Project PlaylistView full playlist\n\n    Shorts\n\n    David Lynch Presents Interview Project:  FRANK793 viewsDavid Lynch Presents Interview Project:  MR. SIEBERT639 viewsDavid Lynch Presents Interview Project:  ROBIN777 viewsDavid Lynch Presents Interview Project. Re-release SUMMER 2024. #davidlynch #twinpeaks #roadtrip2.3K viewsDavid Lynch Presents Interview Project:  BATSO. #davidlynch #twinpeaks #realpeople #interview941 viewsDavid Lynch Presents Interview Project:  BRENDA WRAY. #davidlynch #twinpeaks #realpeople #interviewt814 views\n\n              SearchInfoShoppingTap to unmute2xYou're signed outVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.CancelConfirmIf playback doesn't begin shortly, try restarting your device.•ShareInclude playlistAn error occurred while retrieving sharing information. Please try again later.Watch laterShareCopy link0:00 / •Watch full videoLive••Scroll for details\n\n        …\n\n      NaN / NaN",
    "summary": {
      "en": "David Lynch's Interview Project will re-release all 121 episodes of the original series on October 1, 2024, to celebrate its 15th anniversary. The episodes will be available in high definition for the first time. The creators hope both longtime fans and new viewers will enjoy the series. Lynch emphasizes the project's focus on human experiences, stating, \"It's something that's human and you can't stay away from it.\"",
      "ko": "데이비드 린치의 인터뷰 프로젝트가 원작 시리즈의 121개 에피소드를 2024년 10월 1일에 재발매합니다. 이는 시리즈의 15주년을 기념하기 위한 것입니다. 이번 에피소드는 처음으로 고화질로 제공됩니다. 제작진은 오랜 팬들과 새로운 시청자 모두가 이 시리즈를 즐기기를 바랍니다. 린치는 이 프로젝트가 인간의 경험에 중점을 두고 있다고 강조하며, \"이것은 인간적인 것이고, 피할 수 없는 것이다\"라고 말했습니다.",
      "ja": "デビッド・リンチのインタビュー・プロジェクトは、オリジナルシリーズの121エピソードを2024年10月1日に再リリースします。これはシリーズの15周年を祝うためのものです。エピソードは初めて高解像度で提供されます。制作陣は、長年のファンだけでなく新しい視聴者にも楽しんでもらえることを願っています。リンチはこのプロジェクトが人間の経験に焦点を当てていることを強調し、「それは人間的なもので、避けることはできない」と述べています。"
    }
  },
  {
    "id": "2007b33bda208fc1",
    "title": {
      "en": "An Interview with Zen Chief Architect Mike Clark",
      "ko": "젠의 수장, 마이크 클락 인터뷰",
      "ja": "禅の匠、マイク・クラーク対談"
    },
    "type": "story",
    "url": "https://www.computerenhance.com/p/an-interview-with-zen-chief-architect",
    "score": 32,
    "by": "Smaug123",
    "time": 1742843149,
    "content": "Share this postComputer, Enhance!An Interview with Zen Chief Architect Mike ClarkCopy linkFacebookEmailNotesMoreDiscover more from Computer, Enhance!Programming courses, interviews, commentary.Over 50,000 subscribersSubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.Already have an account? Sign inAn Interview with Zen Chief Architect Mike ClarkCasey MuratoriMar 25, 202596Share this postComputer, Enhance!An Interview with Zen Chief Architect Mike ClarkCopy linkFacebookEmailNotesMore115ShareZen is one of the most important microarchitectures in the history of the x86 ecosystem. Not only is it the reigning champion in many x64 benchmarks, but it is also the architecture that enabled AMD’s dramatic rise in CPU marketshare over the past eight years: from 10% when the first Zen processor was launched, to 25% at the introduction of Zen 5.I recently had the honor of interviewing none other than Zen’s chief architect, Mike Clark. I only had 30 minutes, but I tried to fit in as many of our microarchitecture questions as I could! Subscribers to Computer Enhance will recognize many of them as ones we’ve collectively wondered about during Q&A’s in the Performance-Aware Programming series - and I’m delighted to report that, as you’ll see, Mike gave detailed answers to all of them.Below is the edited transcript of our conversation. I’ve tried to keep it as accurate as possible to the original audio, while reworking the phrasing to be appropriate for reading rather than listening. I have also had AMD approve the transcript to ensure accuracy, and I will be working with them to release an extended video version as well.Now, without further ado, my interview with Mike Clark:CASEY: You will often hear “people on the internet” say that ARM as an ISA is better for low power than x64. People like me who study ISAs tend to be skeptical of this claim. As a hardware designer, are there any specific things about the x64 ISA that you find difficult to deal with for low-power designs?MIKE: Having spent my career working on x86, I might have a bias here! I do think each ISA has its own quirks that influence some of the microarchitecture. But at the base level, we can build low-power x86 designs as well as ARM can, and ARM can build high frequency, high performance designs as well as x86 can. None of the quirks are really limiting you on the microarchitecture. The reality is that the markets we've been targeting have been different, so they've driven the architectures to optimize for different design points. ARM is in much lower power markets where x86 hasn't had the market share to chase.On the x86 side, the higher performance / higher frequency devices are the established market that our devices have to compete in, so that's where our design focus is. We could build the same Zen microarchitecture with an ARM ISA on top instead. We could deliver the same performance per watt. We don't view the ISA as a fundamental input to the design as far as power or performance.CASEY: So the memory model, whether the instructions are variable length, those sorts of things don’t factor in? None of the differences are big enough to matter?MIKE: No. It may take a little bit more microarchitectural work for us to account for stronger memory order on the x86 side, but of course the software has to account for the weaker memory ordering on the ARM side. So there are tradeoffs.Variable length instructions are harder than fixed length, but we've created techniques like the uop cache, and it also gives us better density overall by having smaller instructions. x86 can put more work into each instruction byte, so we can have denser binaries and increase performance that way.So these are all just tradeoffs in the microarchitecture. They’re not fundamental issues for delivering performance per watt at the end of the day.CASEY: Similar question, but moving to the OS side of things: does the 4k page size on x64 create problems for you architecturally by limiting the L1 cache size due to how tagging works? Would architectures like Zen benefit if x64 operating systems moved to 2mb pages as the smallest page size, or perhaps a 16k or 64k page size if you were to introduce that in a future architecture?MIKE: Definitely. We always encourage developers to use larger page sizes if they can, because it gives us a lot more capacity in our TLBs and therefore less TLB pressure overall. But we have the ability to combine 4k pages into larger pages in our TLB if the OS allocates them sequentially. We can turn four 4k pages into a 16k page if they are virtually and physically sequential. That's been a technique we've used even since the original Zen to help software get the benefits of larger page sizes without moving away from 4k pages.However, 4k to 2mb is a big jump. We're always looking for ways to allow our software partners to have larger page sizes, but maybe something in between is more appropriate.CASEY: Just to poke a little further at that, for the L1 cache specifically, you're hitting up against the limit of the address bits. Have you ever wanted to put in bigger L1 caches, but found that you couldn't because the 4k page size means you can't do that without going to a larger-way cache? MIKE: No. In the past we have built L1 caches that don't follow the “ways times 4k page size is the largest index you can have” property. There are ways to do that. We've solved those problems. It is a little bit more logic, but it's a solvable problem. It doesn’t limit us in what we design.  CASEY: Moving on to the sizes of registers and cache lines, I have two questions about how CPUs seem to do things differently than GPUs.First, CPUs seem to be settling into a natural size of 64 bytes. The L1 cache lines are 64 bytes. The registers are 64 bytes. It doesn't look like anyone's trying to go beyond that. But GPUs seem to prefer at least 128 bytes for both. Is this because of the difference in clock rates? Does it have to do with CPU versus GPU workloads? In general, do you see 64 bytes as a natural settling point for CPUs, and if so, why does it seem to be different from GPUs?MIKE: We do look at increasing the line size. We're always going to a clean sheet of paper and making sure we're rethinking things and not missing anything as workloads evolve and things change. We don't want to be locked into a mindset where we think we've proven 64 bytes to be the correct size for everything on a CPU.But the reality is that CPUs are targeted at low latency, smaller datatype, integer workloads as their fundamental value proposition. We've grown that capability with all our out-of-order engines, trying to expose ILP. So far, it’s allowed us to build vector units as wide as 64 bytes.But it's been a journey to even get that wide because if you look at, say, from Zen 4 to Zen 5 - we supported 512-bit vectors on Zen 4 via a 256-bit data path. For Zen 5, we went full bore and supported the full 512-bit data path. That required a fundamental replumbing of the microarchitecture. We had to grow the width of the delivery of data from the L2 to the L1, and we had to double the delivery from the L1 to really take advantage of the wider vector units.The integer workloads that are still primarily reading data out of the cache and branching, they're not getting any benefit from that sort of fundamental change. We have to do it in a very cautious and meticulous manner, so that those highways of delivery can exist while still ensuring that if there's only one car on the highway, we’re not burning power as if all the lanes were full. It’s tricky.When you look at the GPU side, the workloads where they excel are throughput based. Not having to excel at the lowest-latency, small-datatype workloads frees them up to leverage all that extra investment. You need to have workloads that are really focused on using that much data in a wide vector to get the return on that investment.So that's always the trick. If we try to go too big, too wide, we lose our value proposition in performance per watt for the mainstream workloads people buy our new generations for.Does that make sense?CASEY: It makes perfect sense, and it leads right into my next question.Underlying what you said is the implication that, if we as software developers were taking better advantage of wider workloads, it would be worth your while to widen them. One of the problems people often have when trying to widen a data path in software is that CPUs seem to be a lot worse at scatter/gather. It’s an important feature for taking data that isn’t naturally wide and putting it through a wide datapath with some level of efficiency. For example, if I want to widen something that does an array lookup, historically it’s been hard to port that code directly because of poor gather performance.Could you give us some insight on why this is?MIKE: That's a good question. It does tie back to the previous question in the sense that it’s really not the fundamental scatter/gather concept that’s the problem. It's the amount of bandwidth needed to pull all those different elements inside the CPU to put them together to feed to the vector unit.Again, we're focused on latency, not throughput. That has permeated itself out into the interface to what we call our “data fabric”. The memory system isn't wide enough to be able to pull all the data in so it can be assembled into lanes and operated on. If we wanted to attack that, we’d have to widen the interface, and that would come with a large power cost.So again, that's the trick. You're trying to avoid the power cost when you're running workloads that don't require scatter/gather. If you widen these paths, you’ve overbuilt the design for the the baseline workloads that you normally run. We are always trying to grow and pull more applications in, but we have to balance that against the power requirements of widening the bandwidth into the CPU.CASEY: So in other words, it's a chicken and egg problem? If software developers were giving you software that ran fantastically with scatter/gather, you’d do it. But they’re not, so it’s hard to argue for it?MIKE: Right, yes.CASEY: The rest of my questions don’t group together into any particular theme, so I’ll just go through them randomly.Random question number one: previously, on the software side, we thought nontemporal stores were solely there to prevent pollution of caches. That was our mental model. But lately we have noticed that nontemporal stores seem to perform better than regular stores, separate from cache pollution - as if the memory subsystem doesn't have to do as much work, or something similar. Is there more about nontemporal stores that we need to understand, or are we mistaken?MIKE: If you were just doing nontemporal stores to data that is in the caches, obviously that would not be a good thing. So you still have to apply good judgment on when to use nontemporal stores. But tying it back to the ARM-ISA-weakly-ordered discussion, nontemporal stores, while not exactly being weakly ordered, are in some ways easier to deal with in the base case. We can process them efficiently as long as they really are nontemporal. So I think your intuition is right - we can do well with them as long as the software side ensures that the data isn’t finding itself in caches along the way.CASEY: Random question number two: for educational purposes, does anyone publish modern CPU pipeline diagrams that would be reasonably accurate? AMD and Intel, for example, both publish flow charts for new microarchitectures, but not pipeline diagrams.MIKE: It might surprise people, but if you go back to when we did publish pipeline diagrams, those are still fine for learning how a modern CPU works. We do have more complicated pipelines today, and we don't publish them because they reveal proprietary techniques we're using, or give hints that we don't want to give to the competition. But at the end of the day, it's still a fetch block, a decode block, an execute block, a retire block... there's more stages within those blocks, and you can break it down even more than that, but the fundamental pipelining is still similar.CASEY: So, for example, I think the Bulldozer pipeline diagram was the last one I saw from AMD. It’s not woefully out of date? If someone learned that pipeline, they would be able to understand what you actually do now if they were given an updated diagram?MIKE: Roughly speaking, yes.CASEY: Random question number three: if you look at a single-uop instruction like sqrtpd that has a latency longer than the pipeline depth of an execution unit, can you give a cursory explanation of how this works for those of us on the software side who don't understand hardware very well?MIKE: One way to conceptualize it is that you could have taken sqrtpd and split it up into a bunch of different uops that can operate in parallel with dependencies along the way. It can be very expensive to keep all those operations in flight, to build the pipeline to pass the data forward so you can let something new in behind it that's working on an earlier stage. The hardware cost would be too high to create a pipeline to get the execution done in a way that allows another sqrtpd to start on an earlier stage - especially if it's going to be, say, 16 stages of execution until you have achieved your answer.It's really just that cost. Is the amount of hardware worth it to make something like sqrtpd a pipelineable instruction, or can we save a lot of power and hardware by just doing one of them at a time?CASEY: Just to make sure I understand: does that mean inside an execution unit that can do one of these, the uop gets issued and it knows it’s got something special that it has to work on for a while, so it asks not to be given anything else for several cycles while some special control part inside it takes over?MIKE: Correct. The scheduler that feeds it understands that it's not a pipeline execution unit that can take another uop every cycle. But it has a known quantity where, if it has sent one in, after some number of cycles, it knows it can send another one in and it should be safe.CASEY: So the system upstream of the execution unit - the thing that's feeding it - knows not to send more?MIKE: It knows, yes.CASEY: Last question: are there things you wish we as software developers would start - or stop - doing that would help take advantage of the hardware you design, or that would make it easier for you to design new hardware in the future?MIKE: We already hit on one, which is the feedback loop when we add new ISA components - larger vectors, for example. We need software to use them to get the return on investment that we're putting in.Of course we also understand that, as a new feature comes out, it's only on the new hardware. You want your software to run well on our old hardware as well as on our new hardware. We totally understand that problem. But still, if software developers could embrace the new features more aggressively, that would definitely help.It would be great if the software could find ways to leverage wider vectors, AI, and so on - all the areas we've invested a lot of hardware in. And of course we would also like to get feedback from you guys - “if we just had this instruction or this concept, we could really leverage that in our software” and so on. We're constantly open to that, too. We want to know how to make your lives easier.And finally, one other thing I would add is that larger basic blocks are better. Taking branches versus not taking branches can have a big effect on code flow. Try to put conditional operations in the right places. I’m sure you guys probably focus on this already.CASEY: Yes, but it’s always good to hear it from you. We only ever know that something runs faster when we time it - we can't always guess what the designers are thinking on the hardware side.MIKE: Gotcha.CASEY: Well, I think we are out of time. Thank you very much! This has been fantastic. Thank you for answering all of my questions, and please keep in touch. We always have questions like this on the software side, so anytime you want to talk, or if there is anything new you want to tell us about, please let us know.MIKE: Okay, cool. It was a great conversation. And yeah, any time you're wondering what's going on in the hardware, we want to close that gap as best we can!CASEY: We all appreciate it. And we love Zen as well! I’m conducting this interview from a Zen processor as we speak. So thank you for all your hard work over the years.MIKE: Alright, thanks! Talk to you later.If you enjoyed this article, and would like to receive more like it, you can put your email address in the box below to access both our free and paid subscription options:Subscribe96 Likes∙5 Restacks96Share this postComputer, Enhance!An Interview with Zen Chief Architect Mike ClarkCopy linkFacebookEmailNotesMore115SharePrevious",
    "summary": {
      "en": "**Summary: Interview with Mike Clark, Chief Architect of Zen**\n\nIn a recent interview, Mike Clark, the chief architect behind AMD's Zen microarchitecture, discussed the evolution and capabilities of Zen, which has significantly boosted AMD's market share in CPUs. He addressed several technical questions about the differences between x64 and ARM architectures, emphasizing that while both have unique characteristics, they can achieve similar performance and power efficiency. \n\nKey points from the interview include:\n\n1. **Architecture Flexibility**: Clark believes that both x64 and ARM architectures can be optimized for low power or high performance; the choice depends more on market focus than inherent limitations in the ISA (Instruction Set Architecture).\n\n2. **Memory Management**: Larger page sizes in operating systems (like 2MB) could benefit performance by reducing pressure on Translation Lookaside Buffers (TLBs). While 4k pages are standard, Clark encourages developers to utilize larger pages when possible.\n\n3. **Cache and Register Sizes**: CPU designs typically stick to 64 bytes for cache lines and registers due to the focus on low-latency workloads. While there are considerations for wider data paths, it must align with the types of tasks CPUs are optimized for.\n\n4. **Scatter/Gather Performance**: Historically, CPUs have struggled with scatter/gather operations due to bandwidth limitations, which can affect performance when trying to use wider data paths efficiently.\n\n5. **Nontemporal Stores**: These stores can perform better under certain conditions since they reduce cache pollution, but they need to be used judiciously to maintain efficiency.\n\n6. **Software and Hardware Collaboration**: Clark emphasized the importance of developers utilizing new hardware features to maximize performance. He also encouraged feedback from software developers on potential needs for new instructions or features.\n\nIn conclusion, the interview provided insight into how the Zen architecture has evolved and the ongoing relationship between software development and hardware design.",
      "ko": "최근 인터뷰에서 AMD의 Zen 마이크로아키텍처의 수석 설계자인 마이크 클락은 Zen의 발전과 기능에 대해 이야기했습니다. Zen은 AMD의 CPU 시장 점유율을 크게 높이는 데 기여했습니다. 그는 x64와 ARM 아키텍처의 차이에 대한 여러 기술적 질문에 답하며, 두 아키텍처 모두 고유한 특성을 가지고 있지만 비슷한 성능과 전력 효율성을 달성할 수 있다고 강조했습니다.\n\n클락은 x64와 ARM 아키텍처 모두 저전력 또는 고성능으로 최적화될 수 있다고 믿으며, 선택은 ISA(명령어 집합 아키텍처)의 고유한 한계보다는 시장의 초점에 더 의존한다고 말했습니다. 운영 체제에서 더 큰 페이지 크기(예: 2MB)는 변환 참조 버퍼(TLB)에 대한 압력을 줄여 성능을 향상시킬 수 있습니다. 4k 페이지가 표준이지만, 클락은 개발자들이 가능한 경우 더 큰 페이지를 활용할 것을 권장했습니다.\n\nCPU 설계는 일반적으로 저지연 작업에 초점을 맞추기 때문에 캐시 라인과 레지스터 크기를 64바이트로 유지합니다. 더 넓은 데이터 경로에 대한 고려가 있지만, 이는 CPU가 최적화된 작업 유형과 일치해야 합니다. 역사적으로 CPU는 대역폭 제한으로 인해 스캐터/가더 작업에서 어려움을 겪어왔으며, 이는 더 넓은 데이터 경로를 효율적으로 사용하려 할 때 성능에 영향을 줄 수 있습니다.\n\n비시간적 저장소는 특정 조건에서 성능이 더 좋을 수 있으며, 이는 캐시 오염을 줄이기 때문입니다. 그러나 효율성을 유지하기 위해 신중하게 사용해야 합니다. 클락은 개발자들이 새로운 하드웨어 기능을 활용하여 성능을 극대화하는 것이 중요하다고 강조했습니다. 그는 소프트웨어 개발자들이 새로운 명령어나 기능에 대한 필요성을 피드백하는 것도 권장했습니다.\n\n이번 인터뷰는 Zen 아키텍처의 발전과 소프트웨어 개발과 하드웨어 설계 간의 지속적인 관계에 대한 통찰을 제공했습니다.",
      "ja": "最近のインタビューで、AMDのZenマイクロアーキテクチャのチーフアーキテクトであるマイク・クラーク氏が、Zenの進化とその能力について語りました。ZenはAMDのCPU市場シェアを大幅に向上させた要因となっています。クラーク氏は、x64アーキテクチャとARMアーキテクチャの違いについての技術的な質問にも答え、両者は独自の特性を持ちながらも、同様の性能と電力効率を達成できると強調しました。\n\nインタビューの主なポイントは以下の通りです。まず、アーキテクチャの柔軟性について、クラーク氏はx64とARMの両方が低消費電力または高性能に最適化できると述べました。選択は、ISA（命令セットアーキテクチャ）の固有の制限よりも市場の焦点に依存しているとしています。\n\n次に、メモリ管理については、オペレーティングシステムの大きなページサイズ（例えば2MB）が、トランスレーション・ルックアサイド・バッファ（TLB）への負担を減らすことで性能向上に寄与する可能性があると指摘しました。4kページが標準ですが、クラーク氏は可能な限り大きなページを利用することを開発者に勧めています。\n\nキャッシュとレジスタのサイズについては、CPU設計は低遅延のワークロードに焦点を当てているため、通常はキャッシュラインとレジスタを64バイトに保っています。データパスを広げる考慮もありますが、CPUが最適化されているタスクの種類と一致する必要があります。\n\nスキャッター/ギャザー性能については、歴史的にCPUは帯域幅の制限からスキャッター/ギャザー操作に苦労してきたと述べ、広いデータパスを効率的に使用する際に性能に影響を与える可能性があると説明しました。\n\nノンテンポラルストアについては、特定の条件下でキャッシュの汚染を減らすため、より良い性能を発揮することができるが、効率を維持するためには慎重に使用する必要があると述べました。\n\n最後に、ソフトウェアとハードウェアの協力について、クラーク氏は新しいハードウェア機能を活用して性能を最大化することの重要性を強調しました。また、新しい命令や機能の必要性についてソフトウェア開発者からのフィードバックを奨励しました。このインタビューは、Zenアーキテクチャの進化とソフトウェア開発とハードウェア設計の関係についての洞察を提供しました。"
    }
  },
  {
    "id": "26514204ccd4c95d",
    "title": {
      "en": "Samsung co-CEO Han Jong-hee has died",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.reuters.com/world/asia-pacific/samsung-electronics-says-co-ceo-han-jong-hee-has-died-cardiac-arrest-2025-03-25/",
    "score": 12,
    "by": "glimshe",
    "time": 1742887108,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "5a6b64c7bd1b498a",
    "title": {
      "en": "Supply Chain Attacks on Linux Distributions – Fedora Pagure",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://fenrisk.com/pagure",
    "score": 200,
    "by": "akyuu",
    "time": 1742414317,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "9e4f0badd03a1ead",
    "title": {
      "en": "Shift-to-Middle Array: A Faster Alternative to Std:Deque?",
      "ko": "중간 배열 전환: 더 빠른 대안?",
      "ja": "ミドル配列の革新"
    },
    "type": "story",
    "url": "https://github.com/attilatorda/Shift-To-Middle_Array",
    "score": 125,
    "by": "AttilaT",
    "time": 1742772027,
    "content": "Shift-To-Middle Array\nThe Shift-To-Middle Array is a dynamic array designed to optimize insertions and deletions at both ends, offering a high-performance alternative to std::deque, std::vector, and linked lists. It achieves this while maintaining contiguous memory storage, improving cache locality and enabling efficient parallel processing.\n\n🌟 Features\n✅ Amortized O(1) insertions & deletions at both ends\n✅ Fast random access (O(1))\n✅ Better cache locality than linked lists\n✅ Supports SIMD & parallel optimizations\n✅ Efficient memory usage compared to std::deque\n📌 How It Works\nUnlike std::deque, which uses a fragmented block structure, the Shift-To-Middle Array dynamically redistributes space to avoid costly shifts. When resizing, elements are moved toward the middle, ensuring efficient insertions at both ends without excessive copying.\n🚀 Time Complexity Comparison\nThe following table compares the time complexity of Shift-To-Middle Array operations with other common data structures:\n\nOperation\nArrayList (std::vector)\nLinked List\nShift-To-Middle Array\n\nAccess (by index)\nO(1)\nO(n)\nO(1)\n\nInsertion at head\nO(n)\nO(1)\nO(1) amortized\n\nInsertion at tail\nO(1) amortized\nO(1)\nO(1) amortized\n\nInsertion in middle\nO(n)\nO(n)\nO(n)\n\nDeletion at head\nO(n)\nO(1)\nO(1) amortized\n\nDeletion at tail\nO(1)\nO(1)\nO(1) amortized\n\nDeletion in middle\nO(n)\nO(n)\nO(n)\n\nCache Locality\nExcellent\nPoor\nExcellent\n\n🏆 Performance Benchmarks\nBenchmarks comparing Shift-To-Middle Array vs. std::deque vs. ExpandingRingBuffer vs. std::queue demonstrate that performance improvements depend on CPU and GPU capabilities, such as multi-core parallelism, SIMD optimizations, and cache efficiency.\nThe benchmarks were compiled using GCC with the -O3 optimization flag, ensuring high-performance execution. Results vary based on hardware specifications and workload characteristics.\n📂 Installation & Usage\nTo use Shift-To-Middle Array in your project:\n#include \"ShiftToMiddleArray.h\"\nShiftToMiddleArray<int> stmArray;\nstmArray.insert_head(42);\nstmArray.insert_tail(99);\nint value = stmArray.get_head();\nstmArray.remove_head();\n\n🔬 When To Use\n\nHigh-performance queue structures\nGame engines & real-time applications\nNetworking (packet buffering, event queues)\nDynamic sequences in computational geometry & physics\n\n📖 Documentation\nRunning Java Benchmarks\nTo run the Java benchmarks, ensure you have the Trove library installed. Compile and execute using:\njavac -cp trove-3.0.3.jar; ShiftToMiddleArrayBenchmarkTrove.java\njava -cp trove-3.0.3.jar; ShiftToMiddleArrayBenchmarkTrove\n\n📊 Benchmarks & Results\nFor full benchmark details, check out the publication. The provided Python scripts can be used to visualize performance metrics from CSV benchmark results.\n🏛 History\nThe Shift-To-Middle Array was developed as part of an effort to create a more efficient implementation strategy for lists and deques. Traditional data structures, such as std::deque and linked lists, suffer from poor cache locality or fragmented memory allocations, leading to inefficiencies. By leveraging contiguous memory, dynamic mid-shifting, and modern CPU optimizations, Shift-To-Middle Array provides a balanced solution for insertion, deletion, and access performance.\n📜 License\nThis project is licensed under the MIT License.\n🤝 Contributing\nContributions are welcome! Feel free to open an issue or pull request.\n🚀 Try Shift-To-Middle Array today and optimize your data structures!",
    "summary": {
      "en": "### Shift-To-Middle Array Summary\n\nThe Shift-To-Middle Array is a dynamic data structure that enhances performance for inserting and deleting elements at both ends. It is a faster alternative to std::deque, std::vector, and linked lists, while also keeping memory storage contiguous, which improves speed and efficiency.\n\n#### Key Features:\n- **Fast Insertions/Deletions**: O(1) time for adding or removing items at both ends.\n- **Quick Access**: O(1) time for accessing elements by index.\n- **Improved Cache Use**: More efficient than linked lists.\n- **Supports Optimizations**: Works well with SIMD (Single Instruction, Multiple Data) and parallel processing.\n- **Better Memory Efficiency**: Compared to std::deque.\n\n#### How It Works:\nInstead of using fragmented memory like std::deque, the Shift-To-Middle Array moves elements toward the center when resizing. This approach minimizes the need for excessive copying.\n\n#### Performance Comparison:\n- **Access**: O(1) for both Shift-To-Middle Array and ArrayList (std::vector), but O(n) for linked lists.\n- **Insertion**: O(1) amortized for both ends in Shift-To-Middle Array, while linked lists are O(1) at the head but O(n) in the middle.\n- **Deletion**: Similar efficient performance for deletions at both ends in Shift-To-Middle Array compared to linked lists.\n\n#### Use Cases:\n- High-performance queues\n- Game engines and real-time applications\n- Networking tasks like packet buffering\n\n#### Installation:\nTo use, include the header file and create an instance of Shift-To-Middle Array in your project.\n\n#### Additional Information:\n- **Performance Benchmarks**: Performance depends on the hardware and workload.\n- **Documentation and Contributions**: The project is open for contributions and is licensed under MIT.\n\nThe Shift-To-Middle Array is recommended for optimizing data handling in various applications.",
      "ko": "Shift-To-Middle Array는 양쪽 끝에서 요소를 삽입하고 삭제하는 성능을 향상시키는 동적 데이터 구조입니다. 이 구조는 std::deque, std::vector, 링크드 리스트보다 빠르며, 메모리 저장을 연속적으로 유지하여 속도와 효율성을 높입니다.\n\n주요 특징으로는 양쪽 끝에서 아이템을 추가하거나 제거할 때 O(1) 시간으로 빠른 삽입과 삭제가 가능하다는 점이 있습니다. 인덱스를 통해 요소에 접근할 때도 O(1) 시간이 소요됩니다. 또한 링크드 리스트보다 캐시 사용이 더 효율적이며, SIMD(단일 명령어, 다중 데이터)와 병렬 처리와 잘 호환되어 최적화를 지원합니다. 메모리 효율성 또한 std::deque에 비해 개선되었습니다.\n\n작동 방식은 std::deque처럼 단편화된 메모리를 사용하지 않고, 크기를 조정할 때 요소를 중앙으로 이동시킵니다. 이 방법은 불필요한 복사를 최소화합니다.\n\n성능 비교에서 Shift-To-Middle Array와 ArrayList(std::vector)는 접근 속도가 O(1)로 동일하지만, 링크드 리스트는 O(n)입니다. 삽입은 Shift-To-Middle Array에서 양쪽 끝 모두 O(1)로 평균적으로 빠르며, 링크드 리스트는 머리 부분에서 O(1)지만 중간에서는 O(n)입니다. 삭제 성능도 Shift-To-Middle Array는 양쪽 끝에서 링크드 리스트와 비슷한 효율성을 보입니다.\n\n이 구조는 고성능 큐, 게임 엔진 및 실시간 애플리케이션, 패킷 버퍼링과 같은 네트워킹 작업에 적합합니다. 사용하려면 헤더 파일을 포함하고 프로젝트에서 Shift-To-Middle Array의 인스턴스를 생성하면 됩니다.\n\n성능은 하드웨어와 작업 부하에 따라 달라지며, 이 프로젝트는 기여를 받을 수 있도록 열려 있으며 MIT 라이센스 하에 운영됩니다. Shift-To-Middle Array는 다양한 애플리케이션에서 데이터 처리를 최적화하는 데 추천됩니다.",
      "ja": "Shift-To-Middle Arrayは、両端での要素の挿入や削除のパフォーマンスを向上させる動的データ構造です。std::dequeやstd::vector、リンクリストに比べて高速で、メモリのストレージを連続的に保つため、速度と効率が向上します。\n\nこのデータ構造の主な特徴は、両端でのアイテムの追加や削除がO(1)の時間で行えること、インデックスによる要素へのアクセスもO(1)でできることです。また、リンクリストよりもキャッシュの利用が効率的で、SIMD（単一命令・複数データ）や並列処理との相性も良く、メモリ効率もstd::dequeに比べて優れています。\n\nShift-To-Middle Arrayは、std::dequeのように断片化されたメモリを使用するのではなく、サイズ変更時に要素を中央に移動させます。この方法により、過剰なコピーが必要なくなります。\n\nパフォーマンスの比較では、アクセスはShift-To-Middle ArrayとArrayList（std::vector）でO(1)ですが、リンクリストではO(n)です。挿入はShift-To-Middle Arrayで両端ともにO(1)の平均時間で行えますが、リンクリストは先頭でO(1)ですが、中間ではO(n)となります。削除に関しても、Shift-To-Middle Arrayは両端で効率的なパフォーマンスを発揮し、リンクリストと比較しても同様の結果が得られます。\n\nこのデータ構造は、高性能なキュー、ゲームエンジンやリアルタイムアプリケーション、パケットバッファリングなどのネットワーキングタスクに適しています。\n\n使用するには、ヘッダーファイルをインクルードし、プロジェクト内でShift-To-Middle Arrayのインスタンスを作成します。\n\nパフォーマンスはハードウェアやワークロードに依存するため、ベンチマークテストが重要です。また、このプロジェクトはMITライセンスのもとでオープンな貢献を受け付けています。\n\nShift-To-Middle Arrayは、さまざまなアプリケーションにおけるデータ処理の最適化に推奨されます。"
    }
  },
  {
    "id": "13d4eb798a832725",
    "title": {
      "en": "The Vatican's Latinist (2017)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://newcriterion.com/article/the-vaticans-latinist/",
    "score": 136,
    "by": "bookofjoe",
    "time": 1742779791,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "fd9bcc9f99741e1b",
    "title": {
      "en": "Fuckarounditis (2011)",
      "ko": "장난병 2011",
      "ja": "遊び病"
    },
    "type": "story",
    "url": "https://leangains.com/fuckarounditis/",
    "score": 16,
    "by": "fzliu",
    "time": 1742856856,
    "content": "Fuckarounditis\n\t\t\t\t\t\tSeptember 27, 2011 — Posted in Training\n\t\t\t\t\t\tGoal Setting Reverse Pyramid Training Weight Training\n\n\t\t\t\t\t\tDear readers, it is with troublesome news I break my three months of silence.\nThe statistics all point towards the same conclusion: we have a global outbreak of fuckarounditis.\n\nFuckarounditis is a behavioral disordercharacterized by a mediocre physique and complete lack of progress, despite significant amounts of time spent in the gym.\nFuckarounditismost commonly manifests itself as an intense preoccupation with crunches, curls, cable movements, belts, gloves, balance boards, Swiss Balls and Tyler Durden.Fear of squats and deadlifts is another distinguishing trait.Physical exertion is either completely lacking or misapplied (towards questionable or unproductive training practices).\nDespite an alarming increase of fuckarounditisin recent years, prevalance may vary greatly depending on location. However, in most commercial gyms, it has been estimated that 90-100% of individuals are afflicted to varying degrees.\nEnvironment andsocial networks are crucial factors for triggering the disease. It has been proposed that the roots of the disease stems from misinformation and counterproductive training advice found in popular media (“fitness magazines”) and information hubs on the Internet.\nHuman nature and the so-called “laziness”, “magic bullet” and “complacency” genes plays a permissive role for allowing the disease to take hold.\nThe disease spreads rapidly, as carriers of the disease communicate with other individuals in locker rooms, Internet discussion forums and other arenas of interaction and information exchange in real life or otherwise.\nThe onset of symptoms typically occurs in young adulthood and may go undiagnosed for a lifetime. Diagnosis is set by a professional and based on observed behaviors and physique progress.\nSymptoms, preventative measures and intervention strategies are reviewed and discussed.\nBerkhan, M. (2011)\n\t“Targeting The Fuckarounditis Epidemic: Preventative Measures and Intervention Strategies.”\nFuckarounditis: A Serious Threat\nI have been trying to cure people of fuckarounditis since the late 90’s, starting from the day I was first cleansed from it myself. Ever since I recognized the disease, I have acknowledged it as a serious threat to physique development and mental well-being. It is therefore with shock and horror I have watched an increasing number of people fall victim to the disease and the dubious training practices it inspires.\nIn educating myself of good strength training practices, I was very lucky. Yes, I wasted some time with the usual bench’n’curl/bodybuilding magazine-inspired crapola all my friends were doing, spending my time in the gym joking around more than doing any actual training. I was 16 or so when I first set my foot at the gym. (See“My Transformation”for pictures from this time period.)\nHowever, I did not spend more than a few months fooling around with my friends. I found that I enjoyed strength training, so I started to take an increasing interest in the topic. I started to explore and I eventually came across an excellent resource that set the tone for my entire approach and attitude. The resource was the book Beyond Brawn by Stuart McRobert, and it taught me the value of working hard at the right things.\nHowever, I may have never been lucky enough to find this tome of wisdom if I had first lost myself in the malicious maze that resides behind…the illusion of complexity.\nThe Illusion of Complexity\nThe Internet provides a rich soil for fuckarounditis to grow and take hold of the unsuspecting observer.Too much information, shit, clutter, woo-woo, noise, bullshit, loony toon theories, too many quacks, morons and people with good intentions giving you bad advice and uninformed answers. Ah yes, the information age.\nSome of it is bullshit wrapped up in a fancy paper with scientific terms, elaborate detail, promising cutting edge strategies based on the latest research. This makes it easier to swallow for intellectuals and those seeking a quick fix; two different groups, both equally susceptible to bullshittery and easy prey for scam artists.\nYes, if anything has surprised me so far in my work, it’s the complete disassociation between IQ and “exercise intelligence” (essentially common sense and knowledge in regards to training fundamentals). I have many clients from academic circles, many clients that are very successful financially and in their each respective field, but some were complete idiots with regards to their training before they came to me.\nThe problem at the core of the fuckarounditis epidemic is the overabundance of information we have available to us.If there are so many theories, articles and opinions on a topic, we perceive it as something complex, something hard to understand. An illusion of complexity is created.\nWe must read everything. Think long and hard about our choices. Only then can we hope to make an informed choice, we reason. And there areso many choices. Finally, that which we perceive as a good and informed choice is often the complete opposite, usually the result of whatever fad routine is trendy at the moment. Sometimes we do a little bit of everything – “can’t be bad trying to be ‘well-rounded’ now, can it?” we foolishly argue.\nWhen it comes to strength training, the right choices are limited and uncomplicated. There are right and wrong ways to do things, not “it depends”, not alternative theories based on new science that we need to investigate or try. Basic do’s and don’t’s that never change. Unfortunately, these fundamental training principles are lost to many, and stumbling over them is like finding a needle in a haystack.\nIsn’t it the same with nutrition? Do we have diet-related fuckarounditis? Sure enough, there’s diet-related fuckarounditis; people who live in the fear of insulin faeries, avoid carbs like the plague for a few days and then binge the hell out of a few boxes of cereal, and never manage to get lean, for example.\nHowever, in contrast to training-related fuckarounditis, rates of diet-related fuckarounditis have remained fairly stable. The lipophobes have merely been replaced by carbophobes. On the whole, I might even venture to say that people have been getting a bit smarter with regards to nutrition. Not so with training practices, unfortunately.\nYes, the global prevalence of fuckarounditis is increasing at an alarming rate. The plethora of bad choices increases at a much higher rate than the good choices. Soon the bad choices will all but drown out the good ones, I fear.\n“I See Weak People”\nIn my dreams? No. In gyms? Yes.Walking around like regular people. They don’t see each other. They only see what they want to see. They don’t know they’re weak.\nThe afflicted are everywhere; the Shape Magazine-reading receptionist who greets you at the gym, the 135-lb skinny-fat PT who tells you that deadlifts are off limits, the bandana wearing bro in the cable-cross machine and the guy with entire day devoted to “abs”. All of them suffer to varying degrees of the debilitating disorder known as fuckarounditis. Yes, even you might be among the afflicted. Today you shall find out if there is cause for alarm.\nKeep in mind that this is a disease that sneaks up on you and then progresses gradually.Some people walk around with a mild case of fuckarounditis that do not completely impair their results in its initial stages. In others, the disease has reached a severe state, which dramatically interferes with their progress and usually stalls it completely.\nFinally, there are those who are all but lost and for whom there is little hope of a cure. Unfortunately, these people will probably never read this. They are too busy emulating the latest bodybuilding pro, doing the Biceps Blaster routine, or rolling around on a Swiss Ball somewhere.\nHow can you tell if you are suffering from the fuckarounditis? Ultimately, it boils down to your results and whether your progress is reasonable relative to the time you put in.\nLet’s be concrete and talk numbers. After all, there needs to be some overarching and objective way of judging whether you are afflicted or not.\nProgress and Goals\nFor someone interested in aesthetics, which I assume most my readers are, relative strength is the single best measure of progress and the quality of your physique. Before seeing a picture of a client, I can easily get a very good idea of his or hers body composition by simply knowing three stats: weight, height and strength. Relative strength is therefore the parameter that will be used to determine reasonable rates of progress, which will then tell you whether you might be suffering of fuckarounditis or not.\nWithin two years of consistent training on a decent routine, the average male should be able to progress to the following levels of strength (1RM):\nStrength Goals: Intermediate\n\nBench press: body weight x 1.2\nChin-ups or pull-ups: body weight x 1.2 or 8 reps with body weight.\nSquat: body weight x 1.6\nDeadlift:body weight x 2\n\nThese numbers are for a raw (no straps, belt or knee wraps) single repetition.\nThe progress towards the intermediate strength goals should be fairly linear, meaning that there should be no plateaus that cannot be solved in an uncomplicated manner. By “consistent” training I do not mean never missing a training day, nor do I consider taking 2-3 months off from training consistent.\nBy “decent training routine”, I mean “not doing blatantly stupid shit” (training 5-6 days/week, 20-25 sets for chest and arms, etc.). I do not mean optimal and flawless.\nStrength Goals: Advanced\nUnder the exact same conditions as the previous example, 3 out of 4 of the following goals should be reached within five years, along with all of the strength goals listed under “intermediate”:\n\nBench press: body weight x 1.5\nChin-ups or pull-ups: body weight x 1.5 or 15 reps with body weight.\nSquat: body weight x 2\nDeadlift:body weight x 2.5\n\nStrength Goals: Highly Advanced\nUnder the exact same conditions, all of the following goals should be reached within ten years. Alternatively, 3 out of 4 should be reached, and one should be “Elite”:\n\nBench press: body weight x 1.5, or x 1.8 (elite)\nChin-ups or pull-ups: body weight x 1.5 or 15 reps with body weight, or x 1.8 / 20 reps(elite)\nSquat: body weight x 2, or x 2.4(elite)\nDeadlift:body weight x 2.5, or x 3(elite)\n\n“Elite” denotes one lift that is often ahead of the others. For example, people who are natural pullers (long arms) may very well hit a 3 x body weight deadlift before a 1.5 x body weight bench, and vice versa for the presser (short arms, stocky and barrel-chested) benching 1.8 x body weight but not being able to pull 2.5 x body weight in the deadlift.\nThe highly advanced strength goals falls in line with what could be considered the pinnacle of physique and strength developmentfor most average and natural trainers. At this point, progress is very slow.\nA 185-lb male that has been training consistently for 5-10 years should therefore be expected to:\n\nBench press 275-280 lbs.\nDo a chin-up with 90-95 lbs hanging from his waist.\nSquat 370 lbs.\nDeadlift 460-465 lbs.\n\nRespective goals for women:\nFor women in the 115-155-lb range, the corresponding advanced strength goals are 0.9 x body weight bench, 1.1 x body weight chin-up, 1.5 x body weight squat and 1.8 x body weight deadlift.Relative to men, women have much less muscle mass around the chest area and shoulder girdle (men have much higher androgen-receptor density in this particular area), but the lower body is comparativly strong to the upper body.\nA 135-lb woman that has been training consistently for 5-10 years should then be expected to:\n\nBench press 120-125 lbs.\nDo 4-5 chin-ups with body weight or do one with an extra 10-15 lbs hanging from her waist.\nSquat 200-205 lbs.\nDeadlift 225-230 lbs.\n\nSo where do you fall in regards to these goals? If you’ve met them, you’ve received a reasonable return on the time you’ve invested in your training and it’s safe to say that you do not suffer from fuckarounditis – even if you have some of the symptoms, remember that it’s not fucking around if what you’re doing is working for you.\nIf you have not met them, then something is amiss. And if you can’t do 8 good chin-ups or struggle with being able to bench press your body weight after a decade of working out….something is horribly amiss.\nThis is no laughing matter. I’ve had clients that spent 10-15 years working out with little or nothing to show for it and some made very basic mistakes that could have been fixed at an early stage. But as fuckarounditis took root, they seemed to only have succumbed deeper, eventually losing that last shred of training sense.\nTaking early preventative measures is key. Where this is not possible, rapid intervention can save the situation. Enter The Fuckarounditis Test.\nThe Fuckarounditis Test\nPlease review these 25 common symptoms and behaviors associated with fuckarounditis. If you recognize yourself in any of these, and have not met the strength standards recently discussed, you must immediately cease the behavior and implement the necessary changes. There is no time to waste.\n1. You don’t keep track.\nHow much can you bench, squat and deadlift? How many chin-ups? You need to be able to answer those questions right now. Don’t let me hear “I THINK I can” or “I’m not sure but…”. You need to know how much weight you can maximally lift in one set when you’re fresh.\nWhether it’s a set of 1, 4 ,6 or 8 reps doesn’t matter. You need to have concrete reference points in order to evaluate your progress. Keep track of them in a training log. Not “in your head”, write it down. The single act of writing it down is more important than you think, whether you keep those data points in a notebook, on your computer or on Post-It notes like me.\nWith tracking comes the motivation to train, the results and everything else. I can’t even imagine where people get their motivation from if they don’t keep track and just choose weights at random based on whatever feels good that day.\nYou should at all times be aware of the best performance in a few key lifts, your body weight and the conditions under which those sets were performed.\nMore on tracking progress and “checkpoints” in“How to Look Awesome Every Day”.\n\nWhen I recently evaluated client and American poker proPhil Galfond‘s progress after his first month on my training routine, I was pleased to see his bench jump from his old personal best of 225 lbs x 4 to 225 lbs x 8 – and he’s dieting. Phil says“The 4 to 8 rep jump is actually bigger than it sounds, since my 4 reps were done with terrible form, bouncing the weight as hard as I could, and the 8 reps were done very controlled.” That’s outstanding progress, but in order to spot it, you need to keep track.\n2. You are not methodical.\nThe only thing that should be changing from week to week is the load on the bar or the reps with the same load you used last time. If you’re doing it right, these should be increasing. Everything else stays the same; the movements and the order you did them in, the sets and the rest periods in between sets. You don’t add in new stuff.\nThis is the only way you can fairly evaluate your progress and see if you’re headed in the right direction.It might sound tedious to keep doing the same movements every week and the appeal of “mixing it up” can seem strong.\nHowever, the tediousness will soon be replaced by the much stronger joy you get from seeing your lifts go up on a weekly basis. Don’t fall for “muscle confusion” bullshit. The only ones confused are the people who keep talking about such nonsense.\nMixing together too much shit and being methodical about the process was one of the fuckarounditis symptoms that plagued social media expert Julien Smith, co-author of Trust Agents, before I set him straight. Here’s what I told him back when he asked me to comment on his “routine” before we started working together:\n\n…Summing it up, I suspect that your laughable progress can be explained by the following:\n1. Your bullshit training routine, which is a haphazard mixture of strength and conditioning. Some people get decent results from CrossFit and the like, but it spells death for hardgainers like yourself. If we work together you will stop flopping around like a fool and start lifting heavy, with plenty of rest in between sets.\n2. Not tracking progress, be that in some key movements like chins, bench press, etc, or benchmark CrossFit-sessions (which I use regularly for CF clients). Training whatever feels good for the moment is one of the worst ways possible to make progress, not to mention serious progress. It’s such a fucking waste of time that I was completely horrified of how you approached it all. That’s going to change if we work together.\n\nHe has since then gained muscle, added more than a hundred pounds to the main lifts, and has not gained an iota of body fat.I cured him with lots of food and a simple regimen of basic movements done consistently on a weekly basis.\n3. You don’t plan for progress.\nNever choose training weights at random. You look at what you used last session and make the choice based solely on that. Not on your ego. Not because you feel like trying higher or lower reps for shits and giggles.\nThere many good progression models but I will recommend two common models that I use depending on the situation.\nBeginners and people who need a “reboot”, i.e. they may have training experience but have been fucking around so much they’re essentially at the beginner stage (e.g., I used a slightly modified SS routine for Julien), Starting Strengthis an excellent model.\nThis is avery simple 3x/week-program with frequent training and practice of the Big Three (bench, squat and deadlift), starting at 1-3 sets of 5. Example:\nSquat: 200 x 5 x 3.\nWhen you can complete 3 sets of 5 reps with 200 lbs, resting 5 minutes between each set, increase the weight next session. The training load starts at your 7-8RM, which means that you do not go to failure initially.\nFor experienced trainers, I recommend the double progression model ofreverse pyramid training. This is what built most of my physique and it’s also by far the most common approach I use with clients. Example:\nSquat: 200 x 6-8 x 2.\nSet 1: 200 x 8\nRest 3-5 mins. Reduce the load by 10% for the second set.\nSet 2: 180 x 8 (however many reps you can get)\nWhen the highest number in the interval (6-8) is reached (“8” in this case), increase the weight by 2.5% or 5 lbs the next session. Thus in the above example, you would use 205 and 185 lbs for your sets the next time. All sets are to be performed with maximal effort and movements are only done once a week (3x/week training frequency).“Double progression” means that you progress both in reps and load; first you hit the reps you need, then you increase the load.\nRPT is very time-efficient. It’s also far superior to anything else I’ve tried for strength/muscle retention/muscle gain during a diet.\n4. You’re doing too much shit.\nBe a minimalist, like me.\n\n\tI snapped this yesterday and stay in this condition all year round.\nI built my physique with these movements primarily: squats, deadlifts, bench presses, chin-ups, triceps extensions and calf raises. These have remained staples throughout the years. I have flirted with other movements, but these were brief periods.\nOn the whole, that’s one movement per muscle group, with the exception of abs and biceps, which I never really trained except for early in my training career, and then very sporadically every once in a while.\nThe point is that most people are doing too much shit. This dilutes the focus and effort that they are able to put into that which really delivers.\n5. You think more about supplements than squats.\nNo single factor in strength training receives so much attention, yet deliver so little in return. Don’t play the fool who chases magic pills, thinking it will compensate for a lack of effort, a fundamentally flawed training routine or a poor diet.\nThere aresupplements that you might actually find useful, but they won’t turn the tide if what you’re doing right now is not working without them.\n6. You’re lifting weights for the calorie burn.\nStrength is strength. Cardio is cardio.Don’t mix, keep them separate, and use cardio sparingly on a diet or if your primary goal is strength and muscle gain.\n\nIf you’re adding 2-3 sessions of HIIT to your 3 sessions of weights, it is almost comparable to adding 2-3 days of weights. Keyword is “almost”, I’m obviously not drawing direct comparisons. That’s all fine and dandy if you think working out 5-6 days/week is a good idea on a diet. But I don’t think anyone – no matter what level of experience – needs more than 3 days a week in the gym when cutting. (Yes, this goes for competitors and beginners alike.)\nIn conclusion, if conditioning is not terribly important for you, if your goal is really about getting shredded while keeping your muscle, I highly suggest limiting moderate to high intensity cardio on a diet – or ditch it completely. Save it for some other time when your recovery is good and not limited by your diet.\nA calorie deficit is a recovery deficit. Avoid deficit spending.\n\nMy full answer here.\n\n\tRickard Nikoley is one of those I saved from fuckarounditis. Before I intervened, Richard was flopping around like a monkey in the gym. Nothing good came from that. There’s nothing primal or Paleo about being weak. I cured him with squats, bench presses, deadlifts and proper rest periods. That way he could put up the effort when it counted.\n7. You’re looking a bit too happy in there.\nIf you’re not grimacing, grunting, or experiencing some sort of discomfort, you’re not training hard enough. One good set where you have to fight for the reps is better than a week of half-ass sets.\n8. You spend more than 5 minutes on abs.\nIf you can’t see your abs, you need to unfatten yourself. Simple as that.You can’t spot reduceand you’ll get plenty of ab work with squats, deadlifts, chin-ups and overhead presses.Let me see someone at 5-6% body fat deadlift 2.5-3 x body weight for reps with with weak abs and underdeveloped abs. It ain’t happening.\nI’ll allow you one set of “abs”. Beyond that don’t waste your time with crunches, ab machines, hanging leg raises or whatever the hell you’re doing. It won’t give you shit and it will only suck time and energy from what you should be doing.\n9.Is this you?\n10. You’re afraid of “bulky muscles” and use terms like “toning.”\nWomen, you need to put down those pink dumbbells, throw that Shape Magazine in the trash can and stop with this nonsense. You seem to believe that the modest amount of strain will cause you to wake up looking like a bodybuilder in the morning. Horseshit. You’re not using enough drugs to ever come close to looking muscle bound. With the training intensity I see most women apply in the gym, they might as well skip weight training altogether and stay on the treadmill.\nhttps://youtube.com/watch?v=Ia-IdX3DIDg%3Fhl%3Den%26fs%3D1\nJenn has actually gotten stronger since she shot that video; she’s now up to 27.5 lbs added weight. That’s more than 99% of the guys at my gym…and I bet it’s more than many male readers of this site as well. Her routine?\nDay 1 (Monday)\n\n5-min walk for warm-up.\nDeadlift 2-3 sets of 4-5 reps reverse pyramid style.\nRest 5-10 mins.\nLeg extensions – Same setup as deadlifts, but reps a bit higher (6-8).\nRest 5-10 mins.\nChins – Same as deads.\n\nDay 3 (Wednesday)\n\n5-min walk for warm-up.\nBench press 2-3sets of 5-7 reps reverse pyramid style.\nRest 5-10 mins\nPull-ups 2-3sets with body weight.\nRest 5-10 mins\n\nDay 5 (Friday)\n\nSquats 2-3sets of 5-7 reps reverse pyramid style.\nRest 5-10 mins.\nWalking lunges 2-3 sets same as squats.\nRest 5-10 mins.\nOverhead Press 2-3 sets of 5-7 reps reverse pyramid style.\nRest 5-10 mins\n\nThat’s the routine Jenn was doing when she sent me the video and the one she gained all that strength on. This is a solid routine, regardless of gender, and it’s vastly superior to what you’re doing if you’re in the fuckarounditis camp. Why not try it and see for yourself? Throw in a set or two of curls, triceps and calves if you want but don’t mess with it beyond that.\n11. You’re “training the core”…and it involves a Swiss Ball, Bosu Ball or something else that makes you look like an idiot.\n“Training the core” is a phrase that is all too often used by people who are afraid to squat. You don’t need special movements for the core, because it comes with the territory if you squat, deadlift, press and chin. No one squats 2 x body weight with a weak core.\n12. You want that Tyler Durden look.\nWhenever I hear a client say this, alarm bells go off. There’s nothing wrong in having ideals. The problem is all the pain I feel after seeing the dumb shit that follows in the questionnaire, under the part where I ask them about their current training routine. I’ll often see some horrifying cardio/pump’n’tone hybrid, some celebrity workout variety or the kind you’d expect to see in those kind of routines. Nothing more faddish than made-up “celebrity workouts”. God damn, how I hate those.\nWant to look like Tyler Durden? Then you need to build up some modest amount of muscle and then you need to lose fat, period. The muscle is best built with heavy weight training, not 15-25-rep set flyes followed by an hour in the pec deck machine. Your gut is best lost with a good diet, not “fat burning workouts”/tons of cardio/pump’n’tone crapola. All those will do is leave you under-muscled, weak and with a severe case of fuckarounditis.\n\n\tThe Tyler Durden look, brought to you by squats, deadlifts, bench, chin-ups and a good diet. When this client told me that he was “closing in on his goal” of achieving a physique similar to that of Brad Pitt, AKA Tyler Durden in “Fight Club”, I told him to gain some fat, start smoking, get into a fight, and stop training his arms and shoulders. (The implication of that being that he had already surpassed his goal.)\n13. You’re using belts, gloves and straps for no good reason.\nWhat is your reason exactly? Don’t use equipment as a crutch.\n14. You avoid squats and deadlift, because you think they’ll give you a wide waist.\nBitch please. Yes, squats and deadlifts will put muscle on your obliques and thicken up your lower back. But weighted chin-ups or pull-ups will give you wider lats, and if you train all these movements, your waist-to-shoulder ratio will develop very favorably. Training all these movements will also help you grow some balls, so you can finally stop making up bullshit excuses for why you shouldn’t train hard (AKA squat and deadlift).\nPetter, the Tyler Durden lookalike, was squatting and deadlifting regularly on my routine. Last time I checked, he was squatting 2 x body weight (300 lbs) for 9 reps. He was also close to being able to complete a one-arm pull-up. Does it look like he has a wide waist? Are my clients notable for their wide waists? Take your “wide waist”-argument/excuse and shove it up your ass right now.\nWhat’s funny is that this argument is usually brought up by guys who want a “Hollywood”/underwear model type physique. They’re often a) dieting, b) not training legs and c) likely doing tons of cardio. That particular combination will strip off whatever little leg muscle they have faster than Gary Busey can do a gram of coke off his dog Chili’s back. It leaves them looking pathetic and weak, and if that sounds good to you then go ahead.\n15.Doing this?\n16. Are you still warming up? I can’t tell.\nA warm-up is a warm-up. Treat it as such. It should be light, just enough to get the blood flowing and prepare the nervous system for the real stuff. It should never cause fatigue or interfere with your performance in the work sets. All the time, I see people doing sets of 10-15 reps before they get to something that remotely resembles their real set. Which is completely retarded, because you will be fatigued and glycogen-depleted, and your performance in the work sets will be limited for metabolic reasons, and not limited by your actual strength.\nThe only standing recommendation I have regarding warm-ups is for compound movements: 1-3 sets of 4-6 reps of 40%-80% of target weight for the first work set. Warm-ups for assistance movements is whatever you feel is needed.Personally, I only warm up for squats, deadlift, bench press and chins. If you do these at the start of your workout, which you should, you’ll find that they usually get your whole body warmed up, which makes warm-ups redundant or entirely unnecessary for the rest of the workout.\n17.Are you still warming up? I can’t tell. What’s that? You’re already done and I can take the bench?\nLet’s see some fucking effort next time. Don’t take up precious space here when you can get a better workout from walking your dog through the park outside.\nYou can be doing everything perfectly, but you will not go anywhere if you’re not willing to experience discomfort on a regular basis. If you’re in the gym 5-6 days a week, lack of effort might actually be the explanation for your lack of progress. If you were actually in there training hard, most people wouldn’t be coming back for more 5-6 days a week.\n18.Was this you? 🖼️\nLearn good form early, so you don’t have to pay for it with injuries and shitty results later down the road. Don’t let your ego screw you and don’t slap on weight indiscriminately. Be methodical and patient.\nNo need to be a form nazi either. People who “practice” form in all eternity are sometimes just afraid to train hard. Striving for picture perfect form on your final reps can be counterproductive when taken to the extreme.\n19. Your workout buddy gets a set of barbell rows on the house when he “spots” you benching.\nTell him to keep his fingers off the damn bar and assist you as little as possible and only when it’s truly needed; meaning only if you are about to fail on the very last rep in the set. Don’t be one of those clowns slapping on twice of what they could lift by themselves to stroke their ego. It looks pathetic and it will make it impossible for you to properly evaluate your progress.\nAnd for me, an unwanted spotter can make it hard to evaluate the progress of the client, like I had to explain to this actor/client:\n\nI am your trainer, the guy who follows you around is only there to make sure your form is not completely retarded. His function right now should be redundant. I assume you know how to perform most movements well by now. If he interferes with your sets, he is counterproductive to the outcome.\nYou’re my only client right now with a personal trainer on top of the help you receive from me. I imagine he is eager to “correct” your form and tinker with your training in any way he can in order to not feel useless. Make sure he stays out of it beyond spotting you on some sets where it’s warranted (i.e. when and if you are about to fail). Some PTs can’t even get that part right and goes way overboard with the helping part.\nThis is another danger of having him around, as it gets impossible for me to measure your progress (strength gains) accurately (i.e. did you gain strength since the last session or did he just help you more this time?). Make it very clear to him when and how he should help (last rep you can’t complete by yourself).\n\nThis actor played a key role in an award winning and Oscar-nominated movie last year. I’m preparing him for a new movie with some very big names that start shooting early next year. I can’t tell you more beyond that, but rest assured that his “celebrity workout” does not include mickey mouse curls supersetted with cable flyes, or anything of the usual variety you read about in magazines.\n20.You obsess about “tempo”.\nForget about tempo. Tempo takes care of itself if you’re working with heavy weights (80% 1RM / 8 reps or less), because your control over the load is very limited. Lift the damn weight, repeat, it’s that simple. Don’t overthink it.\nTempo was hot stuff a few years ago and frequently discusses in articles an on the boards. It seems to have slipped off the radar now. Why? Because it’s yet another distraction from the important stuff.\n21. You’re into “functional” training.\nBut you can’t squat body weightand your “functional” muscle mass is non-existent. Unless you’re an athlete with a very good reason, stay the hell away from anything that involves Swiss Balls, balance boards or pogo sticks.\n22. You’re the guy who sent me these two direct messages 5 minutesafter I tweeted this:\n\ndo you think 8×20 abs is enough ?? i do hanging superstrict..really feel working lower abs!!. also i need bcaa if i situps in fasted state??\nalso how much bcaa u recommend\n\n23.You’re working on your “lagging biceps”, but you can’t even do 8 chin-ups with good form.\nFirst of all, you’re weak and fat.Second of all, body part specialization has its place, but it’s always the ones least qualified that are doing it. Worry about body part specialization after you achieve the major goalposts involving the muscle group that lags behind. Until then, body part specialization is a distraction that will interfere with your development as a whole.\nLet’s see if your chest and your shoulders are lagging once you can bench 1.5 x body weight or overhead press your body weight and if your legs and back are lagging with a 2 x body weight squatand 2.5 x body weight deadlift. Keep doing targeted work for muscle groups like triceps, hams and calves.\nYes, you can keep a few sets of curls in there, but 2-3 sets is enough for now. Do you really think your guns will be lagging once you’re doing chin-ups with 100 lbs hanging from your waist? My clients have told me they saw more biceps growth from the increased focus on weighted chin-ups than any amount of curls they did in the past.I never spent any time on curls and they grew just fine from all those chin-ups.I can say for damn sure you’ll put on a good deal of size on your arms if you add 50 lbs to whatever you can chin-up right now.\nAsk Paul Wilson and his 8 reps with 110 lbs of solid chin-up mastery:\nhttp://www.youtube.com/embed/qhaJWPTJU5A?hl=en&fs=1″\n\n\tPaul flexing in exclusive LG Gear, only available as an award for excellency and strength.\n24.You think ripped abs will get you laid.\nI hate to bust your bubble lil’ buddy, but women don’t give a shit beyond a guy looking reasonably fit. Lower than 10-12% body fat won’t make an ounce of a difference. If women is your main motivation for dieting, don’t bother getting shredded. Women have similar delusions in terms of thinking men like them super-skinny.\nTraining to be liked by others is a very poor motivational factor in itself. You need internal motivation, not external. No one that ends up making something out of his or hers physique, something impressive and head-turning, gets there by training to get liked by others. It might have started out as such, but it changes. I think this quote sums up why and how we make it:\n\nYou have to discipline yourself, and through this discipline will come some level of achievement, and from this achievement will come your pride.\nDavid Lee Roth.\nAnd then there’s the fact that nothing beats the feeling you get from breaking a new personal record. Some of the happiest moments I remember are from walking home after a new 20-rep squat record way back in the days. Walking up stairs was like being repeatedly stabbed with a blunt and rusty butterknife. But it was still pure bliss.\n25. You have all kinds of elaborate excuses for why you don’t need to squat and deadlift.\nYou get your leg training from playing soccer, you’re afraid you might hurt your back, you read somewhere that single-leg training is better, you talk about rear-foot elevated split squats with a 45-degree twist during the concentric being the future of leg training, etc. Horseshit, all of it.\nListen carefully. Squats and deadlifts are undisputably the two best full body movements in the god damn Universe and that’s reason enough to be doing them.\nBut that’s not the whole story. Their true greatness lies in the mental fortitude they foster if you train them hard and consistently. This will benefit your training as a whole and leave echoes in your life outside the gym.\nDid I miss something? Well, I actually left a few signs and symptoms out because this article was getting way too long for my own good and it feels like my eyes are bleeding right now. I thought I’d leave it up to you to finish it.\nThe #Fuckarounditis Contest\nYou need to do three things if you want to be eligible for participation in the contest:\n\nFollow me on Twitter.\nTweet or retweet this article.\nSend me your best/funniest/most original fuckarounditis symptom in 140 signs by using a hashtag followed by fuckarounditis, like this: #fuckarounditis (so it’ll actually be less than 125 signs not counting #fuckarounditis). That way I can click the hashtag and see all contributions.\n\nThe contest will be running from today until the end of October (ends Oct 31st). After the end of the contest, I’ll pick 5-10 winners that each will receive$100 by PayPal. That’s five, ten or any number of winners in between, I’ll just pick whatever great contributions I get. I’ll announce the winners shortly thereafter and they will be added to this article.\nImportant: Don’t PM/email/write your contribution in comments. I already get too many emails and PMs to keep up and I want to keep all contributions in one place: on Twitter. So you make your contribution by: (text goes here) #fuckarounditis. I’ll be retweeting good ones on a daily basis. When I choose the winners I’ll go through my retweets and pick 5-10 winners.\n\n\t\t\t\t\t\t\tFTC: We use income earning affiliate links. Learn More.\n\n\t\t\t\t\t\tShare this post\n\t\t\t\t\t\tFacebookRedditTwitterWhatsAppEmail\n\n\t\t\t\t\t\t\t\tMartin Berkhan\n\n\t\t\t\t\t\t\tMy name is Martin Berkhan and I work as a nutritional consultant, magazine writer and personal trainer.\n\n\t        \t\tYou might also like..\n\n\t        \t\t\t\t\tFuckarounditis\n\n\t        \t\t\t\t\tSupplements You Might Actually Find Useful (Oct 7th, 2018: EAA Update)\n\t        \t\t\t\t\tDecember 12, 2017\n\n\t        \t\t\t\t\tCheat Day Strategies For A Hedonist\n\t        \t\t\t\t\tNovember 25, 2010\n\n\t        \t\t\t\t\tTop Ten Fasting Myths Debunked\n\t        \t\t\t\t\tOctober 21, 2010\n\n\t        \t\t\t\t\tThe Leangains Guide\n\t        \t\t\t\t\tApril 14, 2010\n\n\t        \t\t\t\t\tThe Reverse Pyramid Training Guide\n\t        \t\t\t\t\tJanuary 25, 2018\n\n\t        \t\t\t\t\tSure-Fire Fat Loss\n\t        \t\t\t\t\tJune 27, 2008",
    "summary": {
      "en": "**Summary of \"Fuckarounditis\"**\n\n\"Fuckarounditis\" is a term used to describe a common issue in gym-goers characterized by a lack of progress and a mediocre physique despite spending a lot of time working out. It often involves focusing on ineffective exercises (like crunches and curls) while avoiding essential compound lifts such as squats and deadlifts.\n\nThe prevalence of fuckarounditis is alarmingly high, especially in commercial gyms, where many people are misled by bad advice from fitness magazines and online sources. This results in ineffective training habits and wasted time. Symptoms of this disorder include not tracking progress, being unmethodical in workouts, and prioritizing fancy equipment over fundamental exercises.\n\nTo combat fuckarounditis, individuals should focus on a few key principles:\n1. **Track Progress**: Keep a log of your lifts and improvements.\n2. **Be Methodical**: Stick to a consistent routine that emphasizes strength-building movements.\n3. **Plan for Progress**: Choose weights based on previous sessions rather than random feelings.\n4. **Simplify Your Routine**: Focus on a limited number of effective exercises rather than trying to do too much.\n5. **Train Hard**: Emphasize effort in your workouts and avoid distractions like excessive cardio or supplements.\n\nThe article also outlines specific strength goals for both men and women to assess whether they are making reasonable progress. If individuals are not meeting these goals after years of training, they may need to reconsider their approach.\n\nIn summary, to avoid fuckarounditis, one should adopt a straightforward, disciplined approach to training that prioritizes strength and measurable progress.",
      "ko": "\"Fuckarounditis\"는 헬스장에서 흔히 발생하는 문제를 설명하는 용어로, 많은 시간을 운동에 투자해도 진전이 없고 보통 수준의 몸매에 그치는 현상을 말합니다. 이 문제는 효과가 없는 운동, 예를 들어 크런치나 컬과 같은 운동에 집중하고, 스쿼트나 데드리프트와 같은 필수적인 복합 운동을 피하는 경우가 많습니다.\n\n상업적인 헬스장에서 fuckarounditis의 발생률은 매우 높습니다. 많은 사람들이 피트니스 잡지나 온라인 소스에서 잘못된 조언을 받아 비효율적인 훈련 습관을 형성하고 시간을 낭비하게 됩니다. 이 증상으로는 진전을 기록하지 않거나, 비체계적인 운동을 하며, 기본 운동보다 화려한 장비를 우선시하는 경향이 있습니다.\n\nfuckarounditis를 극복하기 위해서는 몇 가지 핵심 원칙에 집중해야 합니다. 첫째, 진전을 기록하세요. 자신의 운동 기록과 개선 사항을 정리하는 것이 중요합니다. 둘째, 체계적으로 운동하세요. 근력 향상 운동에 중점을 둔 일관된 루틴을 유지해야 합니다. 셋째, 진전을 계획하세요. 이전 세션을 바탕으로 적절한 중량을 선택하고, 무작위로 느끼는 대로 선택하지 않도록 합니다. 넷째, 루틴을 단순화하세요. 너무 많은 운동을 시도하기보다는 효과적인 운동 몇 가지에 집중하는 것이 좋습니다. 마지막으로, 열심히 훈련하세요. 운동 중에는 노력에 중점을 두고, 과도한 유산소 운동이나 보충제와 같은 방해 요소를 피해야 합니다.\n\n이 글에서는 남성과 여성을 위한 구체적인 근력 목표도 제시하여, 합리적인 진전을 이루고 있는지 평가할 수 있도록 돕습니다. 만약 몇 년간 훈련을 했음에도 이러한 목표를 달성하지 못한다면, 접근 방식을 재고할 필요가 있습니다.\n\n결론적으로, fuckarounditis를 피하기 위해서는 힘과 측정 가능한 진전을 우선시하는 간단하고 규칙적인 훈련 방식을 채택해야 합니다.",
      "ja": "「ファックアラウンド病」という言葉は、ジムに通う人々がよく直面する問題を指します。この病気は、長時間トレーニングをしているにもかかわらず、進歩が見られず、体型も平凡であることが特徴です。多くの場合、効果的でないエクササイズ（クランチやカールなど）に焦点を当て、スクワットやデッドリフトといった重要な複合的なリフトを避ける傾向があります。\n\nファックアラウンド病は特に商業ジムで非常に多く見られます。多くの人がフィットネス雑誌やオンラインの情報から誤ったアドバイスを受けているためです。この結果、効果的でないトレーニング習慣が身についてしまい、時間を無駄にしてしまいます。この病気の症状には、進捗を記録しないこと、トレーニングが体系的でないこと、基本的なエクササイズよりも派手な器具を優先することが含まれます。\n\nファックアラウンド病を克服するためには、いくつかの重要な原則に焦点を当てるべきです。まず、進捗を記録することが大切です。リフトや改善点をログに残しましょう。次に、体系的にトレーニングを行い、筋力を高める動きに重点を置くことが必要です。また、進捗を計画する際には、過去のセッションに基づいて重量を選ぶことが重要です。さらに、ルーチンをシンプルに保ち、効果的なエクササイズに絞ることが推奨されます。最後に、トレーニングでは努力を重視し、過度な有酸素運動やサプリメントといった気を散らす要素を避けるべきです。\n\nこの記事では、男女それぞれのための具体的な筋力目標も示されており、合理的な進捗が見られているかどうかを評価する手助けとなります。もし数年のトレーニングの後にこれらの目標を達成できていない場合は、アプローチを見直す必要があるかもしれません。\n\n要するに、ファックアラウンド病を避けるためには、筋力と測定可能な進捗を優先するシンプルで規律あるトレーニングアプローチを採用することが重要です。"
    }
  },
  {
    "id": "616488f05e0e9d76",
    "title": {
      "en": "Show HN: My iOS app to practice sight reading (10 years in the App Store)",
      "ko": "아이폰 시각 읽기 앱 출시!",
      "ja": "視読練習アプリ10年目！"
    },
    "type": "story",
    "url": "https://apps.apple.com/us/app/notes-sight-reading-trainer/id874386416",
    "score": 297,
    "by": "rooster117",
    "time": 1742765108,
    "content": ".ember3, #ember3::before {\n           width: 246px;\n           height: 246px;\n         }\n         .ember3::before {\n           padding-top: 100%;\n         }\n@media (min-width: 735px) {\n           .ember3, #ember3::before {\n           width: 217px;\n           height: 217px;\n         }\n         .ember3::before {\n           padding-top: 100%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember3, #ember3::before {\n           width: 230px;\n           height: 230px;\n         }\n         .ember3::before {\n           padding-top: 100%;\n         }\n         }\n\n          Notes - Sight Reading Trainer\n            4+\n\n            Piano sheet music\n\n            Ryan Newsome\n\n              Designed for iPad\n\n    4.8 • 2.7K Ratings\n\n                Free\n\n                Offers In-App Purchases\n\nScreenshots\n\n                  iPad\n\n                  iPhone\n\n    .ember6, #ember6::before {\n           width: 643px;\n           height: 857px;\n         }\n         .ember6::before {\n           padding-top: 133.28149300155522%;\n         }\n@media (min-width: 735px) {\n           .ember6, #ember6::before {\n           width: 217px;\n           height: 289px;\n         }\n         .ember6::before {\n           padding-top: 133.17972350230414%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember6, #ember6::before {\n           width: 313px;\n           height: 417px;\n         }\n         .ember6::before {\n           padding-top: 133.22683706070288%;\n         }\n         }\n\n    .ember7, #ember7::before {\n           width: 643px;\n           height: 857px;\n         }\n         .ember7::before {\n           padding-top: 133.28149300155522%;\n         }\n@media (min-width: 735px) {\n           .ember7, #ember7::before {\n           width: 217px;\n           height: 289px;\n         }\n         .ember7::before {\n           padding-top: 133.17972350230414%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember7, #ember7::before {\n           width: 313px;\n           height: 417px;\n         }\n         .ember7::before {\n           padding-top: 133.22683706070288%;\n         }\n         }\n\n    .ember8, #ember8::before {\n           width: 643px;\n           height: 857px;\n         }\n         .ember8::before {\n           padding-top: 133.28149300155522%;\n         }\n@media (min-width: 735px) {\n           .ember8, #ember8::before {\n           width: 217px;\n           height: 289px;\n         }\n         .ember8::before {\n           padding-top: 133.17972350230414%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember8, #ember8::before {\n           width: 313px;\n           height: 417px;\n         }\n         .ember8::before {\n           padding-top: 133.22683706070288%;\n         }\n         }\n\n    .ember9, #ember9::before {\n           width: 643px;\n           height: 857px;\n         }\n         .ember9::before {\n           padding-top: 133.28149300155522%;\n         }\n@media (min-width: 735px) {\n           .ember9, #ember9::before {\n           width: 217px;\n           height: 289px;\n         }\n         .ember9::before {\n           padding-top: 133.17972350230414%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember9, #ember9::before {\n           width: 313px;\n           height: 417px;\n         }\n         .ember9::before {\n           padding-top: 133.22683706070288%;\n         }\n         }\n\n    .ember10, #ember10::before {\n           width: 643px;\n           height: 857px;\n         }\n         .ember10::before {\n           padding-top: 133.28149300155522%;\n         }\n@media (min-width: 735px) {\n           .ember10, #ember10::before {\n           width: 217px;\n           height: 289px;\n         }\n         .ember10::before {\n           padding-top: 133.17972350230414%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember10, #ember10::before {\n           width: 313px;\n           height: 417px;\n         }\n         .ember10::before {\n           padding-top: 133.22683706070288%;\n         }\n         }\n\n    .ember11, #ember11::before {\n           width: 643px;\n           height: 857px;\n         }\n         .ember11::before {\n           padding-top: 133.28149300155522%;\n         }\n@media (min-width: 735px) {\n           .ember11, #ember11::before {\n           width: 217px;\n           height: 289px;\n         }\n         .ember11::before {\n           padding-top: 133.17972350230414%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember11, #ember11::before {\n           width: 313px;\n           height: 417px;\n         }\n         .ember11::before {\n           padding-top: 133.22683706070288%;\n         }\n         }\n\nDescription\n\n  Introducing Notes - Sight Reading Trainer, the ultimate iOS app for mastering sight reading in music! Whether you're a beginner or an experienced musician, Notes is your is your tool to become the musician you were meant to be. When you are ready to learn your instrument and music rather than just learning a song this app is for you.\n\nKey Features:\nEngaging Exercises: Explore a variety of interactive exercises designed to strengthen your sight reading abilities with a piano interface.\n\nWays to use the app: Connect with Midi, use the on screen keyboard or your devices microphone for input.\n\nCustomizable Difficulty: Tailor the difficulty level to match your proficiency, allowing you to progress at your own pace and challenge yourself when ready.\n\nComprehensive Music Notation: Learn to read sheet music, decipher key signatures, note durations and augmentations (sharp/flat) with ease.\n\nProgress Tracking: Keep track of your improvements over time with detailed performance analytics and history.\n\nPractice with songs: This app is focused on randomized practice but also has a handful of songs to practice your developing skills with. More to be added soon!\n\nGoal Setting: Set achievable note goals and receive reminders to maintain consistency in your practice.\n\nUnlock the world of sight reading and elevate your musical journey with Notes - Sight Reading Trainer. Download the app now and get serious about learning music!\n\n        more\n\nWhat’s New\n\n    Version History\n\n            Feb 26, 2025\n            Version 7.3\n\n  Fix to allow using midi inputs and microphone for the keyboarding learning lessons\n\nRatings and Reviews\n\n        See All\n\n      4.8 out of 5\n        2.7K Ratings\n\n        2.7K Ratings\n\n    Quality reviewer\n\n    ,\n\n    04/22/2023\n\n    Great app!\n\n  This is one of the best apps I’ve used to study notes. I only sing but I’m trying to learn notes and this is fantastic. You will learn how to use a piano in the process, which is key to making sure you learn to sing in tune. My only objection is that the lessons do NOT include training on sharps & flats. I was hoping the in app purchase for additional lessons (happens after you’ve learned quite a lot, no worries) included this, and there’s no info given on what these lessons contain, only that “more will be added”. The practice mode thankfully does allow you to train flats but not in an organized, piece by piece way like the lessons do. But unfortunately you cannot see flats and sharps labeled on the keys like the other ones.\n\nI have to knock off a star… sharps and flats are a big part of the music that I do and so this app didn’t teach me everything I wanted to know.\n\n        more\n\n    FPPMEDIA\n\n    ,\n\n    01/18/2018\n\n    Thank you for educational app.\n\n  This app and the ease of use has introduced a whole new world to me.\nI never thought I would understand how to play a piano.. My life has been filled with homelessness and bad fortune, so I learned a long time ago that music had a medicinal effect on me.. A personal escape of sorts... I always wanted to understand how to make beautiful sounds with a piano, but I don’t learn so well.. This app made it easy for me to understand the keyboard layout and knowing what each note is from each key.. It seems like this app will help me learn how to write songs and that will make life so happy for me.\nI’m not trying to be sad here.. I just wanted to explain how much this app can help someone like me. I can’t afford the full version but this version, I appreciate none the less.\n“Music may have to save the world someday”.\n\n        more\n\n    Churple\n\n    ,\n\n    03/26/2018\n\n    Absolutely Stunning\n\n  I rarely write reviews, but this app is so beyond amazing I’ll try my best to share my thoughts. First of all, everything is absolutely FREE, NO premium that you have to buy to basically unlock everything, I feel the developers of this app were truly trying to help instead of take all of your money. Secondly, I’ve had this app for one day and I’ve already learned more than I did searching for weeks for a good app that’s free. I’ve started to be able to play simple songs on my keyboard, like Jingle Bells, completely looking at the sheet music. I almost gave up on trying to learn how to read sheet music, but this has given me a great start. Sorry for bad grammar.\n\n        more\n\nApp Privacy\n\n    See Details\n\n    The developer, Ryan Newsome, indicated that the app’s privacy practices may include handling of data as described below. For more information, see the developer’s privacy policy.\n\n        Data Not Linked to You\n        The following data may be collected but it is not linked to your identity:\n\n                  Usage Data\n\n                  Diagnostics\n\n    Privacy practices may vary, for example, based on the features you use or your age. LearnMore\n\nInformation\n\n          Seller\n\n              Ryan Newsome\n\n          Size\n          32 MB\n\n          Category\n\n                Education\n\n        Compatibility\n\n                  iPhone\n\n                Requires iOS 14.0 or later.\n\n                  iPad\n\n                Requires iPadOS 14.0 or later.\n\n                  iPodtouch\n\n                Requires iOS 14.0 or later.\n\n                  Mac\n\n                Requires macOS11.0 or later and a Mac with AppleM1chip or later.\n\n                  Apple Vision\n\n                Requires visionOS 1.0 or later.\n\n        Languages\n\n  English, French, German, Italian, Polish, Simplified Chinese, Spanish, Traditional Chinese\n\n          Age Rating\n\n              4+\n\n        Copyright\n        © Ryan Newsome\n\n          Price\n          Free\n\n          In-App Purchases\n\n                  Premium\n                  $3.99\n\n            App Support\n\n            Privacy Policy\n\nApp Support\n\n          Privacy Policy\n\nMore By This Developer\n\n    .ember21, #ember21::before {\n           width: 320px;\n           height: 320px;\n         }\n         .ember21::before {\n           padding-top: 100%;\n         }\n@media (min-width: 735px) {\n           .ember21, #ember21::before {\n           width: 157px;\n           height: 157px;\n         }\n         .ember21::before {\n           padding-top: 100%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember21, #ember21::before {\n           width: 146px;\n           height: 146px;\n         }\n         .ember21::before {\n           padding-top: 100%;\n         }\n         }\n\n        Violin Sight Reading Trainer\n\n    Music\n\nYou Might Also Like\n\n        See All\n\n    .ember23, #ember23::before {\n           width: 320px;\n           height: 320px;\n         }\n         .ember23::before {\n           padding-top: 100%;\n         }\n@media (min-width: 735px) {\n           .ember23, #ember23::before {\n           width: 157px;\n           height: 157px;\n         }\n         .ember23::before {\n           padding-top: 100%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember23, #ember23::before {\n           width: 146px;\n           height: 146px;\n         }\n         .ember23::before {\n           padding-top: 100%;\n         }\n         }\n\n        Learn Music Notes Sight Read\n\n    Education\n\n    .ember24, #ember24::before {\n           width: 320px;\n           height: 320px;\n         }\n         .ember24::before {\n           padding-top: 100%;\n         }\n@media (min-width: 735px) {\n           .ember24, #ember24::before {\n           width: 157px;\n           height: 157px;\n         }\n         .ember24::before {\n           padding-top: 100%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember24, #ember24::before {\n           width: 146px;\n           height: 146px;\n         }\n         .ember24::before {\n           padding-top: 100%;\n         }\n         }\n\n        MyMusicTheory - Music Theory\n\n    Education\n\n    .ember25, #ember25::before {\n           width: 320px;\n           height: 320px;\n         }\n         .ember25::before {\n           padding-top: 100%;\n         }\n@media (min-width: 735px) {\n           .ember25, #ember25::before {\n           width: 157px;\n           height: 157px;\n         }\n         .ember25::before {\n           padding-top: 100%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember25, #ember25::before {\n           width: 146px;\n           height: 146px;\n         }\n         .ember25::before {\n           padding-top: 100%;\n         }\n         }\n\n        Piano Marvel - Lessons & Music\n\n    Education\n\n    .ember26, #ember26::before {\n           width: 320px;\n           height: 320px;\n         }\n         .ember26::before {\n           padding-top: 100%;\n         }\n@media (min-width: 735px) {\n           .ember26, #ember26::before {\n           width: 157px;\n           height: 157px;\n         }\n         .ember26::before {\n           padding-top: 100%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember26, #ember26::before {\n           width: 146px;\n           height: 146px;\n         }\n         .ember26::before {\n           padding-top: 100%;\n         }\n         }\n\n        Perfect Ear - Ear Trainer\n\n    Education\n\n    .ember27, #ember27::before {\n           width: 320px;\n           height: 320px;\n         }\n         .ember27::before {\n           padding-top: 100%;\n         }\n@media (min-width: 735px) {\n           .ember27, #ember27::before {\n           width: 157px;\n           height: 157px;\n         }\n         .ember27::before {\n           padding-top: 100%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember27, #ember27::before {\n           width: 146px;\n           height: 146px;\n         }\n         .ember27::before {\n           padding-top: 100%;\n         }\n         }\n\n        Sheet Music Reader Halbestunde\n\n    Education\n\n    .ember28, #ember28::before {\n           width: 320px;\n           height: 320px;\n         }\n         .ember28::before {\n           padding-top: 100%;\n         }\n@media (min-width: 735px) {\n           .ember28, #ember28::before {\n           width: 157px;\n           height: 157px;\n         }\n         .ember28::before {\n           padding-top: 100%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember28, #ember28::before {\n           width: 146px;\n           height: 146px;\n         }\n         .ember28::before {\n           padding-top: 100%;\n         }\n         }\n\n        Skoove: Learn to Play Piano\n\n    Education",
    "summary": {
      "en": "**Summary: Notes - Sight Reading Trainer**\n\n- **Overview**: Notes is an iOS app designed to help users improve their sight-reading skills in music, suitable for both beginners and experienced musicians. \n\n- **Key Features**:\n  - **Interactive Exercises**: Offers engaging activities to enhance sight-reading abilities using a piano interface.\n  - **Input Options**: Users can connect via MIDI, use an on-screen keyboard, or their device's microphone.\n  - **Customizable Difficulty**: Adjust the difficulty to match your skill level and progress at your own pace.\n  - **Comprehensive Music Notation**: Learn to read sheet music, including key signatures and note durations.\n  - **Progress Tracking**: Track improvements with detailed analytics.\n  - **Practice Songs**: Includes a selection of songs for practice, with more to be added soon.\n  - **Goal Setting**: Set note goals and receive reminders to stay consistent.\n\n- **User Ratings**: The app has a high rating of 4.8 out of 5 from 2,700 reviews, praised for its educational value and ease of use.\n\n- **Availability**: The app is free with in-app purchases for advanced features.\n\n- **Compatibility**: Requires iOS 14.0 or later for iPhone and iPad.\n\n**Conclusion**: Notes - Sight Reading Trainer is an effective tool for anyone looking to enhance their music reading skills through interactive learning and practice.",
      "ko": "Notes는 음악의 즉흥 연주 능력을 향상시키기 위해 설계된 iOS 앱으로, 초보자와 숙련된 음악가 모두에게 적합합니다. 이 앱은 사용자가 피아노 인터페이스를 통해 즉흥 연주 능력을 개발할 수 있도록 다양한 상호작용 연습을 제공합니다. \n\n사용자는 MIDI를 통해 연결하거나 화면에 표시되는 키보드, 또는 기기의 마이크를 사용할 수 있는 다양한 입력 옵션을 제공합니다. 난이도는 개인의 실력에 맞게 조정할 수 있어, 사용자가 자신의 속도에 맞춰 학습할 수 있습니다. 또한, 악보를 읽는 방법, 조표, 음표의 길이 등을 배울 수 있는 포괄적인 음악 기호 학습 기능도 포함되어 있습니다. \n\n앱은 사용자의 발전 상황을 자세히 분석하여 추적할 수 있는 기능도 제공합니다. 연습할 수 있는 곡들이 포함되어 있으며, 곧 더 많은 곡이 추가될 예정입니다. 사용자는 목표 음표를 설정하고, 일관성을 유지하기 위해 알림을 받을 수 있습니다. \n\n이 앱은 2,700개의 리뷰에서 5점 만점에 4.8점을 기록하며, 교육적 가치와 사용의 용이성으로 높은 평가를 받고 있습니다. Notes는 무료로 제공되며, 고급 기능을 위한 인앱 구매가 가능합니다. iPhone과 iPad에서는 iOS 14.0 이상이 필요합니다. \n\nNotes - Sight Reading Trainer는 상호작용 학습과 연습을 통해 음악 읽기 능력을 향상시키고자 하는 모든 이에게 효과적인 도구입니다.",
      "ja": "Notesは、音楽の視唱能力を向上させるために設計されたiOSアプリで、初心者から経験豊富な音楽家まで幅広く利用できます。\n\nこのアプリの主な特徴には、インタラクティブな練習問題があり、ピアノインターフェースを使って視唱能力を高める楽しいアクティビティが提供されています。ユーザーはMIDI接続、画面上のキーボード、またはデバイスのマイクを使用して入力できます。また、難易度をカスタマイズできるため、自分のスキルレベルに合わせて進めることができます。楽譜の読み方、調号や音符の長さについても学ぶことができ、進捗を詳細な分析で追跡する機能も備えています。練習用の曲もいくつか用意されており、今後さらに追加される予定です。目標設定機能を使って音符の目標を設定し、継続的に練習するためのリマインダーも受け取れます。\n\nこのアプリは、2,700件のレビューから4.8の高評価を得ており、教育的価値と使いやすさが評価されています。アプリは無料で提供されており、追加機能にはアプリ内購入が必要です。iPhoneやiPadではiOS 14.0以降が必要です。\n\nNotes - Sight Reading Trainerは、インタラクティブな学習と練習を通じて音楽の読み方を向上させたい人にとって、効果的なツールです。"
    }
  },
  {
    "id": "fa5bbe9249635585",
    "title": {
      "en": "The game designer playing through his own psyche",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.newyorker.com/culture/persons-of-interest/the-game-designer-playing-through-his-own-psyche",
    "score": 139,
    "by": "FinnLobsien",
    "time": 1742812772,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "2dfa01bfc8e08067",
    "title": {
      "en": "What is Growth Engineering? A deep dive with MasterClass's former Head of Growth",
      "ko": "성장 공학의 모든 것",
      "ja": "成長エンジニアリングとは？"
    },
    "type": "story",
    "url": "https://newsletter.pragmaticengineer.com/p/what-is-growth-engineering",
    "score": 3,
    "by": "stosssik",
    "time": 1742895705,
    "content": "Share this postThe Pragmatic EngineerWhat is Growth Engineering?Copy linkFacebookEmailNotesMoreDeepdivesWhat is Growth Engineering?A deep dive into the field of growth engineering, which is often positioned between product engineering and marketing. With former head of growth engineering at MasterClass, Alexey KomissaroukGergely Orosz and Alexey KomissaroukMar 21, 2025∙ Paid165Share this postThe Pragmatic EngineerWhat is Growth Engineering?Copy linkFacebookEmailNotesMore38ShareBefore we start: if you’ve already filled out the What is your tech stack? survey: thank you! If you’ve not done so, your help will be greatly appreciated. It takes 5-15 minutes to complete. Those filling out will receive results before anyone else, and additional analysis from myself and Elin. Fill out this survey here. Fill out the surveyGrowth engineering was barely known a decade ago, but today, most scaleups and many publicly traded tech companies have dedicated growth teams staffed by growth engineers. However, some software engineers are still suspicious of this new area because of its reputation for hacky code with little to no code coverage.For this reason and others, I thought it would be interesting to learn more from an expert who can tell us all about the practicalities of this controversial domain. So I turned to Alexey Komissarouk, who’s been in growth engineering since 2016, and was in charge of it at online education platform, MasterClass. These days, Alexey lives in Tokyo, Japan, where he advises on growth engineering and teaches the Growth Engineering course at Reforge.In today’s deep dive, Alexey covers:What is Growth Engineering? In the simplest terms: writing code to help a company make more money. But there are details to consider: like the company size where it makes sense to have a dedicated team do this.What do Growth Engineers work on? Business-facing work, empowerment and platform work are the main areas.Why Growth Engineers move faster than Product Engineers. Product Engineers ship to build: Growth Engineers ship to learn. Growth Engineers do take shortcuts that would make no sense when building for longevity – doing this on purpose.Tech stack. Common programming languages, monitoring and oncall, feature flags and experimentation, product analytics, review apps, and more.What makes a good Growth Engineer? Curiosity, “build to learn” mindset and a “Jack of all trades” approach.Where do Growth Engineers fit in? Usually part of the engineering department, either operating as with an “owner” or a “hitchiker” model.Becoming a Growth Engineer. A great area if you want to eventually become a founder or product manager – but even if not, it can accelerate your career growth. Working in Growth forces you to learn more about the business.With that, it’s over to Alexey:I’ll never forget the first time I made my employer a million dollars.I was running a push notification A/B test for meal delivery startup Sprig, trying to boost repeat orders.A push notification similar to what we tested to boost repeat ordersInitial results were unpromising; the push notification was not receiving many opens. Still, I wanted to be thorough: before concluding the idea was a failure, I wrote a SQL query to compare order volume for subsequent weeks between customers in test vs control.The SQL used to figure out the push notification’s efficiencyAs it turned out, our test group “beat” the control group by around 10%:‘review_5_push’ was the new type of push notification. Roughly the same amount of users clicked it, but they placed 10% more in ordersI plugged the numbers into a significance calculator, which showed it was statistically significant – or “stat-sig” – and therefore highly unlikely to be a coincidence. This meant we had a winner on our hands! But how meaningful was it, really, and what would adding the push notification mean for revenue, if rolled out to 100% of users?It turned out this experiment created an additional $1.5 million dollars, annually, with just one push notification. Wow!I was hooked. Since that day, I've shipped hundreds of experimental “winners” which generated hundreds of millions of incremental revenue for my employers. But you never forget the first one. Moments like this is what growth engineering is all about.1. What is Growth Engineering?Essentially, growth engineering is the writing of code to make a company money. Of course, all code produced by a business on some level serves this purpose, but while Product Engineers focus on creating a Product worth paying for, Growth Engineers instead focus on making that good product have a good business. To this end, they focus on optimizing and refining key parts of the customer journey, such as:Getting more people to consider the productConverting them into paying customersKeeping them as customers for longer, and spending moreWhat kinds of companies employ Growth Engineers? Places you’ve heard of, like Meta, LinkedIn, DoorDash, Coinbase, and Dropbox, are some of the ones I’ve had students from. There’s also OpenAI, Uber, Tiktok, Tinder, Airbnb, Pinterest… the list of high-profile companies goes on. Most newer public consumer companies you’ve heard have a growth engineering org, too.Typically, growth engineering orgs are started by companies at Series B stage and beyond, so long as they are selling to either consumers or businesses via SaaS. These are often places trying to grow extremely fast, and have enough software engineers that some can focus purely on growth. Before the Series B stage, a team is unlikely to be ready for growth for various reasons; likely that it hasn’t found product-market fit, or has no available headcount, or lacks the visitor traffic required to run A/B tests.Cost is a consideration. A fully-loaded growth team consisting of a handful of engineers, a PM, and a designer costs approximately 1 million dollars annually. To justify this, a rule of thumb is to have at least $5 million dollars in recurring revenue – a milestone often achieved at around the Series B stage.Despite the presence of growth engineering at many public consumer tech companies, the field itself is still quite new, as a discipline and as a proper title.Brief history of growth engineeringWhen I joined Opendoor in 2016, there was a head of growth but no dedicated growth engineers, but there were by the time I left in 2020. At MasterClass soon after, there was a growth org and a dozen dedicated growth engineers. So when did growth engineering originate?The story is that its origins lie at Facebook in 2007. The team was created by then-VP of platform and monetization Chamath Palihapitiya. Reforce founder and CEO Brian Balfour shares:“Growth (the kind found on an org chart) began at Facebook under the direction of Chamath Palihapitiya. In 2007, he joined the early team in a nebulous role that fell somewhere between Product, Marketing, and Operations. According to his retelling of the story on Recode Decode, after struggling to accomplish anything meaningful in his first year on the job, he was on the verge of being fired.Sheryl Sandberg joined soon after him, and in a hail mary move he pitched her the game-changing idea that led to the creation of the first-ever growth team. This idea not only saved his job, but earned him the lion’s share of the credit for Facebook’s unprecedented growth.At the time, Sheryl and Mark asked him, “What do you call this thing where you help change the product, do some SEO and SEM, and algorithmically do this or that?”His response: “I don’t know, I just call that, like, Growth, you know, we’re going to try to grow. I’ll be the head of growing stuff.\"And just like that, Growth became a thing.”Rather than focus on a particular product or feature, the growth team at Facebook focused on moving the needle, and figuring out which features to work on. These days, Meta employs hundreds if not thousands of growth engineers.2. What do Growth Engineers work on?Before we jump into concrete examples, let’s identify three primary focus areas that a growth engineer’s work usually involves.Business-facing work – improving the business directlyEmpowerment work – enabling other teams to improve the businessPlatform work – improving the velocity of the above activitiesLet’s go through all three:Business-facing workThis is the bread and butter of growth engineering, and follows a common pattern:Implement an idea. Try something big or small to try and move a key business metric, which differs by team but is typically related to conversion rate or retention.Quantify impact. Usually via A/B testing.Analyze impact. Await results, analyze impact, ship or roll back – then go back to the first step.Experiments can lead to sweeping or barely noticeable changes. A famous “I can’t believe they needed to test this” was when Google figured out which shade of blue generates the most clicks. At MasterClass, we tested things across the spectrum:Small: should we show the price right on the homepage, was that a winner? Yes, but we framed it in monthly terms of $15/month, not $180/year.Medium: when browsing a course page, should we include related courses, or more details about the course itself? Was it a winner? After lengthy experimentation, it was hard to tell: both are valuable and we needed to strike the right balance.Large: when a potential customer is interested, do we take them straight to checkout, or encourage them to learn more? Counterintuitively, adding steps boosted conversion!EmpowermentOne of the best ways an engineer can move a target metric is by removing themselves as a bottleneck, so colleagues from marketing can iterate and optimize freely. To this end, growth engineers can either build internal tools or integrate self-serve MarTech (Marketing Technology) vendors.With the right tool, there’s a lot that marketers can do without engineering’s involvement:Build and iterate on landing pages (Unbounce, Instapage, etc)Draft and send email, SMS and Push Notifications (Iterable, Braze, Customer.io, etc)Connect new advertising partners (Google Tag Manager, Segment, etc)We go more into detail about benefits and applications in the MarTech section of Tech Stack, below.Platform workAs a business scales, dedicated platform teams help improve stability and velocity for the teams they support. Within growth, this often includes initiatives like:Experiment Platform. Many parts of running an experiment can be standardized, from filtering the audience, to bucketing users properly, to observing statistical methodology. Historically, companies built reusable Experiment Platforms in-house, but more recently, vendors such as Eppo and Statsig have grown in popularity with fancy statistical methodologies like “Controlled Using Pre-Experiment Data” (CUPED) that give more signal with less data.Reusable components. Companies with standard front-end components for things like headlines, buttons, and images, dramatically reduce the time required to spin up a new page. No more \"did you want 5 or 6 pixels here\" with a designer; instead growth engineers rely on tools like Storybook to standardize and share reusable React components.Monitoring. Growth engineering benefits greatly from leveraging monitoring to compensate for reduced code coverage. High-quality business metric monitoring tools can detect bugs before they cause damage.When I worked at MasterClass, having monitoring at the ad layer prevented at least one six-figure incident. One Friday, a marketer accidentally broadened the audience for a particular ad from US-only, to worldwide. In response, the Facebook Ad algorithm went on a spending spree, bringing in plenty of visitors from places like Brazil and India, whom we knew from past experience were unlikely to purchase the product. Fortunately, our monitoring noticed the low-performing campaign within minutes, and an alert was sent to the growth engineer on-call, who immediately reached out to the marketer and confirmed the change was unintentional, and then shut down the campaign.Without this monitoring, a subtle targeting error like this could have gone unnoticed all weekend and would have eaten up $100,000+ of marketing budget. This episode shows that platform investment can benefit everyone; and since growth needs them most, it’s often the growth platform engineering team which implements them.As the day-to-day work of a Growth Engineer shows, A/B tests are a critical tool to both measure success and learn. It’s a numbers game: the more A/B tests a team can run in a given quarter, the more of them will end up winners, making the team successful. It’s no wonder, then, that Growth Engineering will pull out all the stops to improve velocity.3. Why Growth Engineers move faster than Product EngineersOn the surface, growth engineering teams look like product engineering ones; writing code, shipping pull requests, monitoring on-call, etc. So how do they move so much faster? The big reason lies in philosophy and focus, not technology. To quote Elena Verna, head of growth at Dropbox:“Product Engineering teams ship to build; Growth Engineering teams ship to learn.”Real-world case: price changes at MasterclassA few years ago at MasterClass, the growth team wanted to see if changing our pricing model to multiple tiers would improve revenue.Inspired in part by multiple pricing tiers for competitors such as Netflix (above), Disney Plus, and Hulu.The “multiple pricing tier” proposal for MasterClass.From a software engineering perspective, this was a highly complex project because:Backend engineering work: the backend did not yet support multiple pricing options, requiring a decent amount of engineering, and rigorous testing to make sure existing customers weren’t affected.Client app changes: on the device side, multiple platforms (iOS, iPad, Android, Roku, Apple TV, etc) would each need to be updated, including each relevant app store.The software engineering team estimated that becoming a “multi-pricing-tier” company would take months across numerous engineering teams, and engineering leadership was unwilling to greenlight that significant investment.We in growth engineering took this as a challenge. As usual, our goal was not just to add the new pricing model, but to learn how much money it might bring in. The approach we ended up proposing was a Fake Door test, which involves offering a not-yet-available option to customers to gauge interest level. This was risky, as taking a customer who’s ready to pay and telling them to join some kind of waiting list is a colossal waste, and risks making them feel like the target of a “bait and switch” trick.We found a way. The key insight was that people are only offended about a “bait and switch”, if the “switch” is worse than the “bait.” Telling customers they would pay $100 and then switching to $150 would cause a riot, but starting at $150 and then saying “just kidding, it’s only $100” is a pleasant surprise.The good kind of surprise.So long as every test “pricing tier” is less appealing – higher prices, fewer features – than the current offering, we could “upgrade” customers after their initial selection. A customer choosing the cheapest tier gets extra features at no extra cost, while a customer choosing a more expensive tier is offered a discount.We created three new tiers, at different prices. The new “premium” tier would describe the existing, original offering. Regardless of what potential customers selected, they got this “original offering,” during the experiment.The best thing about this was that no backend changes were required. There were no real, new, back-end pricing plans; everybody ended up purchasing the same version of MasterClass for the same price, with the same features. The entirety of the engineering work was on building a new pricing page, and the “congratulations, you’ve been upgraded” popup. This took just a few days.Within a couple of weeks, we had enough data to be confident the financial upside of moving to a multi-pricing-tier model would be significant. With this, we’re able to convince the rest of engineering’s leadership to invest in building the feature properly. In the end, launching multiple pricing tiers turned out to be one of the biggest revenue wins of the year.Building a skyscraper vs building a tentThe MasterClass example demonstrates the spirit of growth engineering; focusing on building to learn, instead of building to last. Consider building skyscrapers versus tents.Building a tent optimizes for speed of set-up and tear-down over longevity. You don’t think of a tent as one that is shoddy or low-quality compared to skyscrapers: it’s not even the same category of buildings! Growth engineers maximize use of lightweight materials. To stick with the tents vs skyscraper metaphor: we prioritize lightweight fabric materials over steel and concrete whenever possible. We only resort to traditional building materials when there’s no other choice, or when a direction is confirmed as correct. Quality is important – after all, a tent must keep out rain and mosquitoes. However, the speed-vs-durability tradeoff decision results in very different approaches and outcomes.4. Tech stackAt first glance, growth and product engineers use the same tooling, and contribute to the same codebases. But growth engineering tends to be high-velocity, experiment-heavy, and with limited test coverage. This means that certain “nice to have” tools for product engineering are mission-critical for growth engineers.This post is for paid subscribersSubscribeAlready a paid subscriber? Sign inA guest post byAlexey KomissaroukGrowth Engineering Advisor: alexeymk.com/growth-engSubscribe to Alexey",
    "summary": {
      "en": "**Summary of Growth Engineering**\n\nGrowth engineering is a relatively new field that sits between product engineering and marketing, focusing on writing code to help companies increase their revenue. While product engineers create products, growth engineers aim to optimize key parts of the customer journey, such as attracting new customers, converting them into paying users, and retaining them longer.\n\nKey points about growth engineering include:\n\n1. **Definition**: Growth engineers write code specifically to drive revenue, often using A/B testing to assess the impact of changes on business metrics.\n\n2. **Work Areas**:\n   - **Business-facing Work**: Directly improving business metrics through experimentation.\n   - **Empowerment Work**: Building tools that allow other teams, like marketing, to operate independently.\n   - **Platform Work**: Creating systems that enhance stability and efficiency across teams.\n\n3. **Speed of Execution**: Growth engineers typically move faster than product engineers because they prioritize learning over building long-term features. They often take shortcuts to validate ideas quickly.\n\n4. **Tech Stack**: Growth engineers use various tools and technologies to facilitate rapid experimentation and monitor results.\n\n5. **Career Path**: Growth engineering can be a great stepping stone for those aspiring to become founders or product managers, as it provides insight into business operations.\n\nIn essence, growth engineering is all about experimenting swiftly to discover what strategies can generate more revenue, often utilizing an agile and iterative approach.",
      "ko": "성장 공학은 제품 공학과 마케팅 사이에 위치한 비교적 새로운 분야로, 기업의 수익을 증가시키기 위해 코드를 작성하는 데 중점을 둡니다. 제품 엔지니어가 제품을 만드는 반면, 성장 엔지니어는 고객 여정의 핵심 요소를 최적화하는 데 목표를 두고 있습니다. 여기에는 새로운 고객을 유치하고, 이들을 유료 사용자로 전환하며, 장기적으로 유지하는 과정이 포함됩니다.\n\n성장 공학의 주요 내용은 다음과 같습니다. 첫째, 성장 엔지니어는 수익을 창출하기 위해 특별히 코드를 작성하며, A/B 테스트를 통해 변화가 비즈니스 지표에 미치는 영향을 평가합니다. 둘째, 성장 엔지니어의 작업 영역은 크게 세 가지로 나눌 수 있습니다. 비즈니스 지향 작업은 실험을 통해 비즈니스 지표를 직접 개선하는 것이고, 권한 부여 작업은 마케팅과 같은 다른 팀이 독립적으로 운영할 수 있도록 도구를 만드는 것입니다. 플랫폼 작업은 팀 간의 안정성과 효율성을 높이는 시스템을 만드는 것입니다.\n\n셋째, 성장 엔지니어는 제품 엔지니어보다 일반적으로 더 빠르게 움직입니다. 그들은 장기적인 기능 구축보다 학습을 우선시하며, 아이디어를 신속하게 검증하기 위해 종종 지름길을 선택합니다. 넷째, 성장 엔지니어는 빠른 실험을 촉진하고 결과를 모니터링하기 위해 다양한 도구와 기술을 사용합니다. 마지막으로, 성장 공학은 창업자나 제품 관리자 지망생에게 훌륭한 발판이 될 수 있습니다. 이는 비즈니스 운영에 대한 통찰력을 제공하기 때문입니다.\n\n결국, 성장 공학은 수익을 창출할 수 있는 전략을 신속하게 실험하는 데 중점을 두며, 민첩하고 반복적인 접근 방식을 활용합니다.",
      "ja": "グロースエンジニアリングは、製品エンジニアリングとマーケティングの間に位置する比較的新しい分野で、企業の収益を増やすためにコードを書くことに焦点を当てています。製品エンジニアが製品を作るのに対し、グロースエンジニアは顧客の旅路の重要な部分を最適化することを目指しています。具体的には、新しい顧客を引き付け、彼らを有料ユーザーに転換し、長期間にわたって維持することが含まれます。\n\nグロースエンジニアリングの主なポイントは以下の通りです。まず、グロースエンジニアは収益を促進するために特化してコードを書き、A/Bテストを用いて変更がビジネス指標に与える影響を評価します。次に、業務に直接影響を与える実験を通じてビジネス指標を改善する「ビジネス向けの作業」、マーケティングなど他のチームが独立して運営できるツールを構築する「エンパワーメント作業」、チーム間の安定性と効率を高めるシステムを作成する「プラットフォーム作業」があります。\n\nまた、グロースエンジニアは製品エンジニアよりも迅速に行動することが多く、長期的な機能を構築するよりも学習を優先します。アイデアを迅速に検証するためにショートカットを取ることもあります。使用する技術スタックは多様で、迅速な実験を促進し、結果を監視するためのさまざまなツールや技術を活用します。\n\nキャリアパスとしては、グロースエンジニアリングは創業者やプロダクトマネージャーを目指す人にとって素晴らしいステップとなり、ビジネス運営に関する洞察を提供します。要するに、グロースエンジニアリングは迅速に実験を行い、どの戦略が収益を増やすかを発見することに重点を置いており、アジャイルで反復的なアプローチをよく利用します。"
    }
  },
  {
    "id": "522a6ccd367a86e3",
    "title": {
      "en": "Goblin.tools: simple, single-task tools to help neurodivergent people with tasks",
      "ko": "고블린 툴즈: 간편한 도구들",
      "ja": "ゴブリンツールズ"
    },
    "type": "story",
    "url": "https://goblin.tools/",
    "score": 318,
    "by": "ValentineC",
    "time": 1742825717,
    "content": "Help\n\n                        Magic Todo acts as a standard todo list, with some special sauce. Try the  button and let it automatically come up with the steps you need to accomplish your task! The 🌶 spiciness level 🌶 gives the tool a hint about how hard or stressful you find the task. The spicier, the more steps it will attempt to break it down into. It's not an exact measure, more a hint to the system.Top level tasks are automatically assigned a category indicated by an emoji. You can filter your list to one or more categories with the  button. The filter button also allows you to hide all completed items.Common task tools (edit, remove, add subtask, and estimator) are under the  button on each item. The  icon on the left can be dragged to reorder items to anywhere.Additional actions for the entire list can be found below it. They include  synchronisation between devices,  export options , undo and redo, and  bulk actions.\n\n        .checkbox-container {\n            flex-shrink: 0;\n            width: 20px;\n        }\n\n        .spinner-border {\n            display: none;\n        }\n\n        .parent-todo {\n            margin-bottom: 8px;\n        }\n\n        .spiciness-indicator {\n            position: absolute;\n            font-size: 0.6rem;\n            transform: translateY(-100%);\n        }\n\n        .edit-input {\n            display: inline;\n            margin-right: 5px;\n        }\n\n        .edit-input-container {\n            width: 100%;\n        }\n\n        .edit-confirm,\n        .edit-cancel {\n            display: inline;\n            margin-left: 2px;\n            padding: 0 4px;\n        }\n\n        .todoText {\n            user-select: text !important;\n            -webkit-user-select: text;\n            -moz-user-select: text;\n            -ms-user-select: text;\n        }\n\n        #categoryFilterMenu {\n            --bs-dropdown-min-width: 10px !important;\n            --bs-dropdown-item-padding-x: 0;\n        }\n\n            #categoryFilterMenu .form-check {\n                padding-left: 2rem;\n                padding-right: 0.4rem;\n                text-align: center;\n                align-content: center;\n            }\n\n        .list-group-item-content {\n            margin: -6px 0 -6px -16px;\n            /*padding: var(--bs-list-group-item-padding-y) var(--bs-list-group-item-padding-x);*/\n            padding: var(--bs-list-group-item-padding-y) 8px  var(--bs-list-group-item-padding-y) 12px !important;\n        }\n\n            .list-group-item-content:hover {\n                background-color: var(--bs-tertiary-bg);\n            }\n\n        #child-frame {\n            display: none;\n        }\n\n        .highlight {\n            background-color: var(--bs-tertiary-bg);\n        }\n\n        .list-group-item {\n            outline: var(--bs-list-group-border-width) solid var(--bs-list-group-border-color) !important;\n            padding: var(--bs-list-group-item-padding-y) 0 var(--bs-list-group-item-padding-y) 18px !important;\n            margin: -1px;\n            border: 0px !important;\n            box-sizing: content-box !important;\n        }\n\n        Magic ToDo\n        Breaking things down so you don't\n\n                    🌶️🌶️🌶️\n\n                Spiciness level:\n\n                    How much breaking down do you need?\n\n                    🌶️\n                    🌶️🌶️🌶️🌶️🌶️\n\n                     Sync\n\n                        EXPERIMENTAL FEATURE\n                        use at your own risk\n\n                                Enable cloud synchronization\n\n                            Username\n\n                            Password\n\n                            Save\n                            Cancel\n\n                            Please provide credentials to synchronize your ToDo list across devices.\n                            There are no accounts, your username must simply be unique.\n                            We recommend taking a backup with  before activating sync on new devices.\n\n                                How does this work?\n\n                                All devices with the same username and password will synchronise lists.\n                                Your list is encrypted on your devices using your username and password. It can not be seen or decrypted by admins or anyone who doesn't have your credentials.\n                                Synchronization is not instant, it can take a few seconds for your changes to come through on other devices.\n\n                        Save to file\n\n                        Load from saved file\n\n                        Copy to clipboard\n\n                        Print\n\n                        Export to Todoist template\n\n                        Export to iCal file\n\n            ☑️\n\n                         Estimate all tasks\n                         Mark all as not completed\n                         Clear all estimates\n                         Clear all completed tasks\n                         Clear entire list",
    "summary": {
      "en": "**Summary of Magic Todo:**\n\nMagic Todo is a task management tool that functions like a regular to-do list but offers unique features. \n\n- **Task Breakdown:** You can use a special button to automatically generate steps for your tasks based on how difficult you find them, indicated by a \"spiciness level\" (🌶). A higher spiciness level means more detailed steps.\n\n- **Task Categories:** Each task is assigned a category represented by an emoji, and you can filter tasks by these categories.\n\n- **Task Management:** Common options like editing, removing, adding subtasks, and estimating tasks are available for each item. You can also reorder tasks by dragging them.\n\n- **Additional Features:** There are options for synchronizing your to-do list across devices, exporting tasks, undoing and redoing actions, and performing bulk actions.\n\n- **Synchronization:** To sync your tasks across devices, you need to create a unique username and password. Your data remains encrypted and secure.\n\nOverall, Magic Todo helps you organize and break down tasks effectively while providing several useful features for task management.",
      "ko": "매직 투두는 일반적인 할 일 목록처럼 작동하는 작업 관리 도구이지만, 독특한 기능들을 제공합니다. \n\n작업 분해 기능을 통해 사용자는 특별한 버튼을 눌러 작업의 난이도에 따라 자동으로 단계별 작업을 생성할 수 있습니다. 이 난이도는 '매운 정도'로 표시되며, 매운 정도가 높을수록 더 세부적인 단계가 생성됩니다.\n\n각 작업은 이모지로 표현된 카테고리에 할당되며, 사용자는 이러한 카테고리를 기준으로 작업을 필터링할 수 있습니다.\n\n작업 관리 기능으로는 편집, 삭제, 하위 작업 추가, 작업 추정 등의 일반적인 옵션이 제공됩니다. 또한, 작업을 드래그하여 순서를 변경할 수 있습니다.\n\n추가 기능으로는 여러 기기에서 할 일 목록을 동기화하는 옵션, 작업 내보내기, 작업 실행 취소 및 다시 실행, 대량 작업 수행 등이 있습니다.\n\n작업을 여러 기기에서 동기화하려면 고유한 사용자 이름과 비밀번호를 만들어야 하며, 데이터는 암호화되어 안전하게 보호됩니다.\n\n전반적으로 매직 투두는 작업을 효과적으로 정리하고 분해하는 데 도움을 주며, 작업 관리를 위한 여러 유용한 기능을 제공합니다.",
      "ja": "Magic Todoは、通常のToDoリストのように機能するタスク管理ツールですが、独自の特徴があります。\n\nタスクの分解機能では、特別なボタンを使って、タスクの難易度に応じたステップを自動的に生成できます。この難易度は「スパイシーレベル」（🌶）で示されており、レベルが高いほど詳細なステップが作成されます。\n\nタスクには、絵文字で表されたカテゴリーが割り当てられ、これに基づいてタスクをフィルタリングすることができます。\n\nタスク管理に関しては、編集、削除、サブタスクの追加、タスクの見積もりなどの一般的なオプションが各アイテムに用意されています。また、タスクをドラッグして順序を変更することも可能です。\n\nさらに、デバイス間でToDoリストを同期したり、タスクをエクスポートしたり、操作を元に戻したりやり直したり、一括操作を行うオプションもあります。\n\nデバイス間でタスクを同期するには、ユニークなユーザー名とパスワードを作成する必要があります。データは暗号化され、安全に保護されています。\n\n全体として、Magic Todoはタスクを効果的に整理し、分解する手助けをし、タスク管理に役立つ多くの機能を提供します。"
    }
  },
  {
    "id": "ed5299a42cd7c383",
    "title": {
      "en": "Btrfs Adding Fast/Realtime ZSTD Compression and Other Performance Optimizations",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.phoronix.com/news/Linux-6.15-Btrfs",
    "score": 8,
    "by": "todsacerdoti",
    "time": 1742888476,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "c32c7963edc80997",
    "title": {
      "en": "Show HN: LinkedIn sucks, so I built a better one",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://heyopenspot.com/",
    "score": 439,
    "by": "fliellerjulian",
    "time": 1742755960,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "c1017d760737a62f",
    "title": {
      "en": "Osgint – OSINT tool to find information about GitHub user",
      "ko": "오스긴 – 깃허브 정보 탐색기",
      "ja": "オスギント：GitHub情報発見ツール"
    },
    "type": "story",
    "url": "https://github.com/hippiiee/osgint",
    "score": 111,
    "by": "CHEF-KOCH",
    "time": 1742795502,
    "content": "OSGINT\n\n  Retrieve informations about a github username/email\n\nFeatures\n\n Find github username from an email\n Find email from github username (not working all the time)\n Find informations about a profile (account creation date, public gists, id, public pgp, public ssh ...)\n\nRequirements\npip3 install -r requirements.txt\n\nUsage\n$ python3 osgint.py -h\n\n .d88888b.                    d8b          888\nd88P\" \"Y88b                   Y8P          888\n888     888                                888\n888     888 .d8888b   .d88b.  888 88888b.  888888\n888     888 88K      d88P\"88b 888 888 \"88b 888\n888     888 \"Y8888b. 888  888 888 888  888 888\nY88b. .d88P      X88 Y88b 888 888 888  888 Y88b.\n \"Y88888P\"   88888P'  \"Y88888 888 888  888  \"Y888\n                          888  v1.0.0\n                     Y8b d88P\n                      \"Y88P\"\nBy Hippie | https://twitter.com/hiippiiie\n\nusage: osgint.py [-h] [-u USERNAME] [-e EMAIL] [--json]\n\noptions:\n  -h, --help            show this help message and exit\n  -u USERNAME, --username USERNAME\n                        Github username of the account to search for (default: None)\n  -e EMAIL, --email EMAIL\n                        Email of the account to search for github username (default: None)\n  --json                Return a json output (default: False)\n\nExample output\nusername\n$ ./osgint.py -u hippiiee\n[+] login : hippiiee\n[+] id : 41185722\n[+] avatar_url : https://avatars.githubusercontent.com/u/41185722?v=4\n[+] name : Hippie\n[+] blog : https://hippie.cat\n[+] bio : Hi !\n[+] public_repos : 10\n[+] public_gists : 0\n[+] followers : 8\n[+] following : 9\n[+] created_at : 2018-07-13T08:28:00Z\n[+] updated_at : 2022-08-21T13:11:36Z\n[+] public_gists : https://gist.github.com/hippiiee\n[+] GPG_keys : https://github.com/hippiiee.gpg\n[+] GPG_key_id : 27cbb171ff857c58\n[+] email : hquere@e3r4p3.42.fr hippolyte.q@gmail.com\n\n$ ./osgint.py -u hippiiee --json\n{\n    \"GPG_key_id\": \"27cbb171ff857c58\",\n    \"GPG_keys\": \"https://github.com/hippiiee.gpg\",\n    \"avatar_url\": \"https://avatars.githubusercontent.com/u/41185722?v=4\",\n    \"bio\": \"Hi !\",\n    \"blog\": \"https://hippie.cat\",\n    \"created_at\": \"2018-07-13T08:28:00Z\",\n    \"email\": [\n        \"hquere@e3r4p3.42.fr\",\n        \"hippolyte.q@gmail.com\"\n    ],\n    \"followers\": 8,\n    \"following\": 9,\n    \"id\": 41185722,\n    \"login\": \"hippiiee\",\n    \"name\": \"Hippie\",\n    \"public_gists\": \"https://gist.github.com/hippiiee\",\n    \"public_repos\": 10,\n    \"updated_at\": \"2022-08-21T13:11:36Z\"\n}\n\nEmail\n$ ./osgint.py -e chrisadr@gentoo.org\n[+] username : ChrisADR\n\n$ ./osgint.py -e chrisadr@gentoo.org --json\n{\n    \"username\": \"ChrisADR\"\n}\n\nHow does it works ?\nTo get a user email, osingt is checking :\n\nall the public commits of the user, if the email is not hidden in one of the commit it will be added to the list\nif the user have a GPG key, if he has one, it's getting the email from the content of the GPG after a base64 decode\ngithub user API\n\nTo get a user email, osgint is checking :\n\ngithub user API\n🚧 spoofing a commit with the email, then checking the name in the commit history (working every time) 🚧 (Work In Progress)\n\nProject inspired from Zen",
    "summary": {
      "en": "**OSGINT Summary**\n\nOSGINT is a tool designed to gather information about GitHub users using their usernames or email addresses. \n\n**Key Features:**\n- Find a GitHub username from an email.\n- Find an email from a GitHub username (may not always work).\n- Retrieve details about a GitHub profile, including account creation date, public repositories, and more.\n\n**Requirements:**\n- Install dependencies with: `pip3 install -r requirements.txt`.\n\n**Usage:**\n- Run the command: `$ python3 osgint.py -h` for help.\n- Options include:\n  - `-u USERNAME`: Search using a GitHub username.\n  - `-e EMAIL`: Search using an email to find the GitHub username.\n  - `--json`: Get the output in JSON format.\n\n**Examples:**\n- To find information by username:\n  ```\n  $ ./osgint.py -u hippiiee\n  ```\n- To find a username by email:\n  ```\n  $ ./osgint.py -e chrisadr@gentoo.org\n  ```\n\n**How It Works:**\n- To find a user's email, OSGINT checks public commits and GPG keys. It also uses the GitHub user API to gather information.\n- Some methods for retrieving emails may involve checking commit history to detect spoofed emails.\n\nThe project is inspired by Zen and is maintained by a user named Hippie.",
      "ko": "OSGINT는 GitHub 사용자에 대한 정보를 사용자 이름이나 이메일 주소를 사용하여 수집하는 도구입니다. 이 도구의 주요 기능으로는 이메일을 통해 GitHub 사용자 이름을 찾거나, GitHub 사용자 이름을 통해 이메일을 찾는 기능이 있습니다. 후자의 경우 항상 작동하지는 않을 수 있습니다. 또한 GitHub 프로필에 대한 세부 정보를 검색할 수 있으며, 여기에는 계정 생성 날짜, 공개 저장소 등의 정보가 포함됩니다.\n\nOSGINT를 사용하기 위해서는 먼저 필요한 의존성을 설치해야 합니다. 이를 위해 `pip3 install -r requirements.txt` 명령어를 사용합니다. 사용 방법은 간단합니다. 도움말을 보려면 `$ python3 osgint.py -h` 명령어를 실행하면 됩니다. 사용 가능한 옵션으로는 `-u USERNAME`을 사용하여 GitHub 사용자 이름으로 검색하거나, `-e EMAIL`을 사용하여 이메일로 GitHub 사용자 이름을 찾는 방법이 있습니다. 또한 `--json` 옵션을 사용하면 결과를 JSON 형식으로 받을 수 있습니다.\n\n예를 들어, 사용자 이름으로 정보를 찾으려면 다음과 같이 입력합니다. `$ ./osgint.py -u hippiiee`. 이메일로 사용자 이름을 찾고 싶다면 `$ ./osgint.py -e chrisadr@gentoo.org`와 같이 입력하면 됩니다.\n\nOSGINT는 사용자의 이메일을 찾기 위해 공개 커밋과 GPG 키를 확인합니다. 또한 GitHub 사용자 API를 사용하여 정보를 수집합니다. 이메일을 검색하는 방법 중 일부는 커밋 기록을 확인하여 스푸핑된 이메일을 감지하는 과정을 포함할 수 있습니다. 이 프로젝트는 Zen에서 영감을 받아 만들어졌으며, 사용자 Hippie가 유지 관리하고 있습니다.",
      "ja": "OSGINTは、GitHubユーザーのユーザー名やメールアドレスを使って情報を収集するためのツールです。\n\n主な機能には、メールアドレスからGitHubのユーザー名を見つけることや、GitHubのユーザー名からメールアドレスを探すことが含まれます。ただし、後者は必ずしも成功するわけではありません。また、GitHubのプロフィールに関する詳細情報、例えばアカウントの作成日や公開リポジトリの数なども取得できます。\n\nこのツールを使用するには、まず依存関係をインストールする必要があります。コマンドは「pip3 install -r requirements.txt」です。\n\n使い方は簡単で、ヘルプを表示するには「$ python3 osgint.py -h」と入力します。オプションには、GitHubのユーザー名を使って検索する「-u USERNAME」や、メールアドレスを使ってユーザー名を探す「-e EMAIL」があります。また、出力をJSON形式で取得する「--json」オプションも利用可能です。\n\n具体的な使用例として、ユーザー名で情報を探す場合は「$ ./osgint.py -u hippiiee」と入力します。メールアドレスからユーザー名を見つけたい場合は「$ ./osgint.py -e chrisadr@gentoo.org」とします。\n\nOSGINTは、ユーザーのメールアドレスを見つけるために、公開されたコミットやGPGキーをチェックします。また、GitHubのユーザーAPIを利用して情報を収集します。メールアドレスを取得するための方法の中には、コミット履歴を確認して偽装されたメールアドレスを検出するものもあります。\n\nこのプロジェクトはZenに触発されており、Hippieというユーザーによって維持されています。"
    }
  },
  {
    "id": "ac72d66503620b34",
    "title": {
      "en": "First Known Photographs of Living Specimens",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.inaturalist.org/projects/first-known-photographs-of-living-specimens",
    "score": 120,
    "by": "Morizero",
    "time": 1742501985,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "b0369bf4f1523bda",
    "title": {
      "en": "'Pathetic European free-loading': What was said in Trump officials' group chat",
      "ko": "유럽의 무임승차자들",
      "ja": "「欧州の乞食」トランプ官僚の暴露"
    },
    "type": "story",
    "url": "https://news.sky.com/story/pathetic-european-free-loading-what-was-said-in-leaked-group-chat-13335365",
    "score": 6,
    "by": "austinallegro",
    "time": 1742903144,
    "content": "document.currentScript.parentNode.config = {\"id\":\"leaderboard\",\"ad-type\":\"leaderboard\",\"targeting\":{\"platform\":\"live\",\"advert-targeting\":\"'platform': 'live'\",\"artId\":\"13335365\"},\"size\":{\"mobile\":[[320,50],[300,50]],\"tablet\":[[728,90]],\"desktop\":[[728,90],[970,250]]}}\n\n        Explainer\n            Explainer\n    'Pathetic European free-loading': What was said in Trump officials' group chat\n\n                                The editor-in-chief of The Atlantic was an accidental fly on the wall in a group chat where senior US officials discussed airstrike plans and what they thought about \"free-loading\" Europe - with a healthy sprinkling of emojis. Here's what was said.\n\n    Tuesday 25 March 2025 09:56, UK\n\n                Please use Chrome browser for a more accessible video player\n\n                2:39\n\n              Leaked security chat explainedMessages were put onto a group chat that included US vice president JD Vance and defence secretary Pete Hegseth.ShareVideo Player is loading.Play VideoPlaySkip BackwardSkip ForwardMuteCurrent Time0:00/Duration-:-Loaded: 0%0:00Stream TypeLIVESeek to live, currently behind liveLIVERemaining Time--:-1xPlayback RateChaptersChaptersDescriptionsdescriptions off, selectedCaptionsCaptions settings, opens captions settings dialogCaptions off, selectedAudio TrackFullscreenThis is a modal window.\n        Video Unavailable\n        Error Code: MEDIA_ERR_SRC_NOT_SUPPORTED\n        Technical details\n        : This video is currently unavailable\n\n      Session ID: 2025-03-25:6d36ba9a2fd37e94e097bb44 Player Element ID: id_f79d6224-8202-4ee7-be0d-aabaf493f8c5Beginning of dialog window. Escape will cancel and close the window.TextColorWhiteBlackRedGreenBlueYellowMagentaCyanOpacityOpaqueSemi-TransparentText BackgroundColorBlackWhiteRedGreenBlueYellowMagentaCyanOpacityOpaqueSemi-TransparentTransparentCaption Area BackgroundColorBlackWhiteRedGreenBlueYellowMagentaCyanOpacityTransparentSemi-TransparentOpaqueFont Size50%75%100%125%150%175%200%300%400%Text Edge StyleNoneRaisedDepressedUniformDrop shadowFont FamilyProportional Sans-SerifMonospace Sans-SerifProportional SerifMonospace SerifCasualScriptSmall CapsResetDoneClose Modal DialogEnd of dialog window.Close Modal DialogThis is a modal window. This modal can be closed by pressing the Escape key or activating the close button.Close Modal DialogThis is a modal window. This modal can be closed by pressing the Escape key or activating the close button.\n\n              Leaked security chat explained\n\n                Why you can trust Sky News\n\n      A group chat of top US officials - which a journalist was accidentally added to - discussed plans to conduct airstrikes on Yemen's Iran-backed Houthis and fiery critique of \"pathetic\" Europe.Editor-in-chief of The Atlantic, Jeffrey Goldberg, was added to the \"Houthi PC small group\" on Signal on Thursday 13 March.\n\nHe wrote about what was said in the days that followed in his article The Trump Administration Accidentally Texted Me Its War Plans.Here's what we know was said on encrypted messaging service Signal - in a group that included US vice president JD Vance, defence secretary Pete Hegseth, national security adviser Mike Waltz and director of national intelligence Tulsi Gabbard.Thursday 13 March\n\n                    document.currentScript.parentNode.config = {\"id\":\"teads\",\"ad-type\":\"teads\",\"targeting\":{\"platform\":\"live\",\"advert-targeting\":\"'platform': 'live'\",\"artId\":\"13335365\"},\"size\":{\"mobile\":[[4,4]],\"tablet\":[[4,4]],\"desktop\":[[4,4]]}}\n\nThis day was seemingly aimed at establishing who would make up the small group.A message to the group, from Michael Waltz read: \"Team - establishing a principles [sic] group for coordination on Houthis, particularly for over the next 72 hours. My deputy Alex Wong is pulling together a tiger team at deputies/agency Chief of Staff level following up from the meeting in the Sit Room this morning for action items and will be sending that out later this evening.\"The message continued: \"Pls provide the best staff POC from your team for us to coordinate with over the next couple days and over the weekend. Thx.\"One minute later, the secretary of state Marco Antonio Rubio wrote: \"Mike Needham for State,\" apparently designating the current counsellor of the state department as his representative.\n\n                                        document.currentScript.parentNode.config = {\"id\":\"mpu-1\",\"ad-type\":\"mpu-1\",\"showLabel\":true,\"targeting\":{\"platform\":\"live\",\"advert-targeting\":\"'platform': 'live'\",\"artId\":\"13335365\"},\"size\":{\"mobile\":[[300,250],[300,600]],\"tablet\":[[300,250],[300,600]],\"desktop\":[[300,250],[300,600]]}}\n\nJD Vance wrote: \"Andy baker for VP.\" One minute after that, Tulsi Gabbard, the director of national intelligence, wrote: \"Joe Kent for DNI.\"Nine minutes later, treasury secretary Scott Bessent wrote: \"Dan Katz for Treasury.\"At 4.53pm Pete Hegseth wrote: \"Dan Caldwell for DoD.\"And at 6.34pm, a user called \"Brian\" wrote \"Brian McCormack for NSC.\" Someone called \"John Ratcliffe\" then wrote with the name of a CIA official to be included in the group.\n\n        Who's who in the chat?\n\n        Eighteen people were part of the chat - here is who appears to have been added and how they were identified in the chat:\n        JD Vance - US deputy president\n        Michael Waltz - national security adviser\n        Marco Antonio Rubio - the secretary of state, identified as MAR\n        Tulsi Gabbard - the director of national intelligence, identified as TG\n        Scott Bessent - treasury secretary, identified as Scott B\n        Pete Hegseth - US defence secretary\n        Susie Wiles - White House chief of staff\n        Stephen Miller - deputy White House chief of staff, called S M in the chat (identified only by his initials, which the report says Mr Goldberg \"took to stand for Stephen Miller\")\n        Steve Witkoff - US Middle East and Ukraine negotiator\n        Jeffrey Goldberg - editor-in-chief of The Atlantic magazine, identified as JG\n        Other members included various National Security Council officials and a CIA official who was not named in the report because they are an active intelligence officer.\n\nFriday 14 MarchAt 8.05am, Mr Waltz texted the group: \"Team, you should have a statement of conclusions with taskings per the Presidents [sic] guidance this morning in your high side inboxes,\" with \"high side\" referring to classified computer systems.\"State and DOD [Department of Defence], we developed suggested notification lists for regional Allies and partners. Joint Staff is sending this am [morning] a more specific sequence of events in the coming days and we will work w DOD to ensure COS [chief of staff], OVP [office of the vice president] and POTUS [president of the United States] are briefed.\"Mr Goldberg says at this point, a \"fascinating\" policy discussion commenced, during which JD Vance wrote: \"Team, I am out for the day doing an economic event in Michigan. But I think we are making a mistake.\"3 percent of US trade runs through the suez. 40 percent of European trade does. There is a real risk that the public doesn't understand this or why it's necessary. The strongest reason to do this is, as POTUS said, to send a message.\"I am not sure the president is aware how inconsistent this is with his message on Europe right now. There's a further risk that we see a moderate to severe spike in oil prices. I am willing to support the consensus of the team and keep these concerns to myself. But there is a strong argument for delaying this a month, doing the messaging work on why this matters, seeing where the economy is, etc.\"\n\n        Spreaker\n\n                This content is provided by Spreaker, which may be using cookies and other technologies.\n                To show you this content, we need your permission to use cookies.\n                You can use the buttons below to amend your preferences to enable Spreaker cookies or to allow those cookies just once.\n                You can change your settings at any time via the Privacy Options.\n\n                Unfortunately we have been unable to verify if you have consented to Spreaker cookies.\n                To view this content you can use the button below to allow Spreaker cookies for this session only.\n\n                Enable Cookies\n                Allow Cookies Once\n\n👉 Follow Trump 100 on your podcast app 👈At 8.27am, a message arrived from the Pete Hegseth reading: \"VP: I understand your concerns - and fully support you raising w/ POTUS. Important considerations, most of which are tough to know how they play out (economy, Ukraine peace, Gaza, etc). I think messaging is going to be tough no matter what - nobody knows who the Houthis are - which is why we would need to stay focused on: 1) Biden failed & 2) Iran funded.\"Waiting a few weeks or a month does not fundamentally change the calculus. 2 immediate risks on waiting: 1) this leaks, and we look indecisive; 2) Israel takes an action first - or Gaza cease fire falls apart - and we don't get to start this on our own terms. We can manage both.\"We are prepared to execute, and if I had final go or no go vote, I believe we should. This [is] not about the Houthis. I see it as two things: 1) Restoring Freedom of Navigation, a core national interest; and 2) Reestablish deterrence, which Biden cratered. But, we can easily pause. And if we do, I will do all we can to enforce 100% OPSEC - operations security.\"I welcome other thoughts.\"Later in the conversation, Mr Waltz criticised the limited capabilities of European navies, writing: \"Whether it's now or several weeks from now, it will have to be the United States that reopens these shipping lanes. Per the president's request we are working with DOD and State to determine how to compile the cost associated and levy them on the Europeans.\"Mr Vance addressed Mr Hegseth in a message reading: \"If you think we should do it let's go. I just hate bailing Europe out again.\"Mr Hegseth replied: \"VP: I fully share your loathing of European free-loading. It's PATHETIC. But Mike is correct, we are the only ones on the planet (on our side of the ledger) who can do this. Nobody else even close. Question is timing. I feel like now is as good a time as any, given POTUS directive to reopen shipping lanes. I think we should go; but POTUS still retains 24 hours of decision space.\"An account believed to be the deputy White House chief of staff, Stephen Miller, then said: \"As I heard it, the president was clear: green light, but we soon make clear to Egypt and Europe what we expect in return. We also need to figure out how to enforce such a requirement. EG, if Europe doesn't remunerate, then what? If the US successfully restores freedom of navigation at great cost there needs to be some further economic gain extracted in return.\"This was followed by the last text of the day, from Mr Hegseth, who wrote at 9.46am: \"Agree.\"Saturday 15 MarchAt 11.44am, Mr Hegseth posted in Signal a \"TEAM UPDATE\" - but The Atlantic has not published what was said.Mr Goldberg wrote in his article: \"I will not quote from this update, or from certain other subsequent texts. The information contained in them, if they had been read by an adversary of the United States, could conceivably have been used to harm American military and intelligence personnel, particularly in the broader Middle East, Central Command's area of responsibility.\"He did say Mr Hegseth had sent lengthy text about how the first detonations in Yemen would be felt two hours from then, at 1.45pm Eastern Time. He said he waited in his car in a supermarket parking lot, waiting to see if Houthi targets would be bombed.He said he went on X and searched Yemen at about 1.55pm, when he saw reports of explosions being heard across Sanaa, its capital city.He said the only person to respond to Mr Hegseth in the morning was Mr Vance, who wrote: \"I will say a prayer for victory\", along with two prayer emojis.\n\n        Image:\n        Pic: The Atlantic\n\nAt 1.48am, Mr Waltz provided an update in the group which Mr Goldberg did not quote in full, but he did say the national security adviser described the operation as an \"amazing job\".John Ratcliffe then wrote: \"A good start.\"Mr Goldberg said Mr Waltz responded with three emojis: a fist, an American flag and fire.Others soon joined in, including Mr Rubio, who wrote, \"Good Job Pete and your team!!,\" and Susie Wiles, who texted: \"Kudos to all - most particularly those in theater and CENTCOM! Really great. God bless.\"Mr Witkoff responded with five emojis: two hands-praying, a flexed bicep, and two American flags.Tulsi Gabbard responded: \"Great work and effects!\"Mr Goldberg said the after-action discussion included assessments of damage done, including the likely death of a specific individual - he did not say who.",
    "summary": {
      "en": "A recent leak revealed a group chat among senior U.S. officials, including Vice President JD Vance and Defense Secretary Pete Hegseth, discussing military airstrikes on Yemen's Iran-backed Houthis. The editor-in-chief of The Atlantic, Jeffrey Goldberg, was accidentally added to this chat and later wrote about the conversations.\n\nKey points from the discussions included:\n\n1. **Military Coordination**: Officials were organizing a small group to manage the airstrike plans and coordinate efforts over the following days.\n   \n2. **Criticism of Europe**: There were strong sentiments expressed about Europe being \"pathetic\" and \"free-loading,\" with officials discussing how the U.S. was often left to handle military actions alone.\n\n3. **Economic Concerns**: Some officials, including Vance, raised concerns about the potential economic impact of the airstrikes, suggesting that public understanding of the situation could be lacking.\n\n4. **Decision-Making**: The group debated the timing of the airstrikes, with some arguing for immediate action and others suggesting a delay for better messaging and preparation.\n\n5. **Aftermath of the Strikes**: After the strikes, there was a discussion on the effectiveness of the operation, with officials expressing satisfaction and celebrating the military's achievements.\n\nOverall, the chat highlighted internal discussions among U.S. leaders regarding military strategy, international relations, and the role of Europe in global defense efforts.",
      "ko": "최근 유출된 정보에 따르면, 미국 고위 관리들 간의 단체 채팅이 있었고, 여기에는 부통령 JD 밴스와 국방장관 피트 헥셋이 포함되어 있었습니다. 이들은 예멘의 이란 지지 후티 반군에 대한 군사 공습에 대해 논의했습니다. 아틀란틱의 편집장 제프리 골드버그가 우연히 이 채팅에 추가되었고, 이후 대화 내용을 글로 남겼습니다.\n\n논의의 주요 내용은 다음과 같습니다. 첫째, 군사 조정에 관한 것이었습니다. 관리들은 공습 계획을 관리하고 향후 며칠 동안의 노력을 조정하기 위해 소규모 그룹을 조직하고 있었습니다. 둘째, 유럽에 대한 비판이 있었습니다. 관리들은 유럽이 \"한심하다\"며 \"공짜로 얻어먹고 있다\"는 강한 감정을 표현하며, 미국이 종종 군사 작전을 혼자 처리해야 한다고 논의했습니다.\n\n셋째, 경제적 우려도 제기되었습니다. 밴스를 포함한 일부 관리들은 공습이 경제에 미칠 잠재적 영향에 대해 우려하며, 대중이 상황을 제대로 이해하지 못할 수 있다고 언급했습니다. 넷째, 공습의 시기에 대한 논의가 있었습니다. 일부는 즉각적인 행동을 주장했고, 다른 이들은 더 나은 메시지 전달과 준비를 위해 지연할 것을 제안했습니다.\n\n마지막으로, 공습 이후 작전의 효과에 대한 논의가 있었습니다. 관리들은 작전의 성과에 만족하며 군의 성취를 축하했습니다. 전반적으로 이 채팅은 미국 지도자들 간의 군사 전략, 국제 관계, 그리고 글로벌 방어 노력에서 유럽의 역할에 대한 내부 논의를 드러냈습니다.",
      "ja": "最近の情報漏洩により、アメリカの高官たちのグループチャットが明らかになりました。このチャットには、副大統領のJDバンス氏や国防長官のピート・ヘグセス氏が参加しており、イエメンのイラン支持派フーシ派に対する軍事空爆について話し合っていました。アトランティック誌の編集長、ジェフリー・ゴールドバーグ氏が偶然このチャットに追加され、その内容について後に記事を書きました。\n\n議論の主なポイントは以下の通りです。まず、軍事調整について、高官たちは空爆計画を管理するための小グループを編成し、今後数日間の活動を調整することを話し合っていました。次に、ヨーロッパに対する批判が強く、「情けない」や「ただ乗り」といった感情が表明され、アメリカがしばしば軍事行動を単独で行うことについて議論されました。\n\nまた、経済的な懸念も取り上げられ、バンス氏を含む一部の高官が空爆の経済的影響について心配を示し、一般市民が状況を理解していない可能性があると指摘しました。さらに、空爆のタイミングについても議論があり、即時行動を求める意見と、より良いメッセージングと準備のために遅らせるべきだという意見が交わされました。\n\n空爆後には、作戦の効果についての議論があり、高官たちは軍の成果に満足し、祝賀の意を表しました。全体として、このチャットはアメリカの指導者たちの間での軍事戦略、国際関係、そしてグローバルな防衛努力におけるヨーロッパの役割についての内部議論を浮き彫りにしました。"
    }
  },
  {
    "id": "449a2c2ef5726266",
    "title": {
      "en": "Japanese scientists use stem cell treatment to restore movement in spinal injury",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://medicalxpress.com/news/2025-03-japanese-scientists-stem-cell-treatment.html",
    "score": 246,
    "by": "pseudolus",
    "time": 1742811919,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "7141c6c92b9df51f",
    "title": {
      "en": "Open Source Mythology",
      "ko": "오픈소스 신화",
      "ja": "オープンソース神話"
    },
    "type": "story",
    "url": "https://lgug2z.com/articles/on-open-source-mythology/",
    "score": 12,
    "by": "yreew",
    "time": 1742864007,
    "content": "On Open Source Mythology\n\n        2025-03-23 ::\n\n      LGUG2Z\n\n      #licensing\n\n        There are two points of popular open source mythology this post will share my\nexperience with:\n\nPeople won’t use your project if you don’t use an Open Source\nInitiative-approved license\nPeople won’t contribute to to your project if you don’t use an Open Source\nInitiative-approved license\n\nMany people have ideas about how society should be like and what must be done\nto change institutions and to work for a revolution. But this is not enough.\nOften these ideas do not conform to reality and if they do conform to reality\nthere is only one way to test them: Try to put them to work and see if they\nsucceed. Testing our ideas in concrete work is the only way we will ever know\nif they are correct.\n– James Forman - Twenty Enemies\n(1971)\n\nI maintain a tiling window manager for Windows called\nkomorebi which is made available under\nthe Komorebi License.\nThe Komorebi License is both an educational source\nlicense which\nensures the availability of source code to individuals for personal use,\nmodification, learning and reference, and a firewall license which preserves an\nindividual’s freedom to refuse by default (… like a firewall!)\nThe preservation of an individual’s freedom to refuse by default means that the\nKomorebi License is not an Open Source Initiative-approved license.\nAs of the publication of this post, komorebi has 126k\ndownloads\nand 10.6k stargazers on\nGitHub, and while I am still by far the most active and primary\ncontributor to the\nproject, there is a healthy stream of users who like to contribute patches for\nthings they have a personal interest in improving.\n\nIt often happens, however, that thinking lags behind reality; this is because\nman’s cognition is limited by numerous social conditions. We are opposed to\ndie-hards in the revolutionary ranks whose thinking fails to advance with\nchanging objective circumstances and has manifested itself historically as\nRight opportunism. These people fail to see that the struggle of opposites\nhas already pushed the objective process forward while their knowledge has\nstopped at the old stage. This is characteristic of the thinking of all\ndie-hards. Their thinking is divorced from social practice, and they cannot\nmarch ahead to guide the chariot of society; they simply trail behind,\ngrumbling that it goes too fast and trying to drag it back or turn it in the\nopposite direction.\n– 毛泽东 - ON PRACTICE\n(1937)\n\nI do not believe I am alone in having the desire to share what I have learned\nso that other people might learn from it, while not buying into the idea that\nin order to do that effectively, I also have to use a software license which\nrequires me to forfeit my freedom to refuse.\nI do not believe that there needs to be a dependency between sharing what I\nhave learned in the form of source code and permitting its use in the\ncommission of evils including but not limited\nto genocide, or participating in my own exploitation by unimaginably wealthy\ncorporations which, incidentally, are also active\nparticipants\nin the commission of evils including but not limited to genocide.\n(It’s probably no surprise that I’m also not a fan of forcing a dependency\nbetween funding for essential government services and funding a genocide)\n\nIn Russia, there was a fundamental difference between the contradiction\nresolved by the February Revolution and the contradiction resolved by the\nOctober Revolution, as well as between the methods used to resolve them.\n[…]\nThe dogmatists do not observe this principle; they do not understand that\nconditions differ in different kinds of revolution and so do not understand\nthat different methods should be used to resolve different contradictions; on\nthe contrary, they invariably adopt what they imagine to be an unalterable\nformula and arbitrarily apply it everywhere, which only causes setbacks to\nthe revolution or makes a sorry mess of what was originally well done.\n– 毛泽东 - ON CONTRADICTION\n(1937)\n\nThe conditions of today differ from the conditions of 1983 as they differ from the\nconditions of 1998. The contradictions of today differ from the contradictions\nof 1983 as they differ from the contradictions of 1998.\nIf you are a software developer who wants to share what you have learned for\nthe educational benefit of others in the world, but are uncomfortable with the\nstrings attached by open source software licensing, I invite you to reject the\ndogmatists, and to renew and share the experiences of your social practice.\n\nWe do ourselves and the revolutionary struggle a great disservice if we do\nnot begin to realize that our concrete work […] must be put into writing\nfor the benefit of all the world, especially ourselves.\nWe must constantly sum up our work, our revolutionary efforts, our social\npractices so that we might learn what is correct and positive and what must be\nfurther developed.\nAt the same time we must discard that which is irrelevant and negative. To\nassume that the Bolsheviks, the Chinese, the Vietnamese, the Cubans, the\nAlgerians and other revolutionaries have not made mistakes from which they\nprofited is not to understand that there are no blueprints for revolutions.\nThe concrete work that we do in the final analysis is the basis for truth.\n– James Forman - Twenty Enemies\n(1971)\n\n        Read other posts\n\n                Educational Source Software\n                →",
    "summary": {
      "en": "This text discusses two common beliefs about open source software: \n\n1. People won't use your project without an Open Source Initiative-approved license.\n2. People won't contribute to your project without such a license.\n\nThe author, who maintains a Windows tiling window manager called komorebi under a unique license, argues that it’s possible to share knowledge and code without giving up personal freedoms. They highlight that their project has been well-received despite not using a traditional open-source license, demonstrating that user engagement and contributions can occur outside established norms.\n\nThe author emphasizes that ideas about societal change must be tested through real-world application. They urge software developers who want to share their knowledge to reject rigid dogmas and adapt their approaches to modern conditions, learning from both successes and mistakes in past revolutionary movements.\n\nIn conclusion, the text encourages sharing experiences and practices in software development, stressing the importance of learning from concrete work rather than adhering strictly to outdated frameworks.",
      "ko": "이 글에서는 오픈 소스 소프트웨어에 대한 두 가지 일반적인 믿음에 대해 논의합니다. 첫 번째는 사람들이 오픈 소스 이니셔티브에서 승인한 라이선스가 없으면 프로젝트를 사용하지 않을 것이라는 것입니다. 두 번째는 그런 라이선스가 없으면 사람들이 프로젝트에 기여하지 않을 것이라는 믿음입니다.\n\n저자는 독특한 라이선스 아래에서 '코모레비'라는 윈도우 타일링 관리자 프로그램을 유지 관리하며, 개인의 자유를 포기하지 않고도 지식과 코드를 공유할 수 있다고 주장합니다. 그들은 전통적인 오픈 소스 라이선스를 사용하지 않음에도 불구하고 프로젝트가 잘 받아들여졌다는 점을 강조하며, 사용자 참여와 기여가 기존의 규범 밖에서도 일어날 수 있음을 보여줍니다.\n\n저자는 사회 변화에 대한 아이디어는 실제 적용을 통해 검증되어야 한다고 강조합니다. 지식을 공유하고자 하는 소프트웨어 개발자들에게는 경직된 교리를 거부하고 현대의 조건에 맞게 접근 방식을 조정할 것을 촉구합니다. 과거 혁명 운동에서의 성공과 실수로부터 배우는 것이 중요하다고 말합니다.\n\n결론적으로, 이 글은 소프트웨어 개발에서 경험과 실천을 공유할 것을 권장하며, 구식 틀에 얽매이지 않고 구체적인 작업에서 배우는 것이 중요하다고 강조합니다.",
      "ja": "この文章では、オープンソースソフトウェアに関する二つの一般的な考え方について述べています。一つ目は、オープンソースイニシアティブに承認されたライセンスがなければ、人々はあなたのプロジェクトを使わないというものです。二つ目は、そのようなライセンスがなければ、人々はあなたのプロジェクトに貢献しないという考えです。\n\n著者は、独自のライセンスのもとで「komorebi」というWindows用のタイル型ウィンドウマネージャを運営しており、個人の自由を失うことなく知識やコードを共有することが可能であると主張しています。彼のプロジェクトは、伝統的なオープンソースライセンスを使用していないにもかかわらず好評を得ており、ユーザーの関与や貢献が既存の規範の外でも行われることを示しています。\n\n著者は、社会変革に関するアイデアは実際の応用を通じて検証されるべきだと強調しています。知識を共有したいソフトウェア開発者には、硬直した教義を拒否し、現代の状況に合わせてアプローチを適応させるよう促しています。過去の革命運動からの成功や失敗を学ぶことが重要です。\n\nこの文章は、ソフトウェア開発における経験や実践を共有することを奨励し、古い枠組みに厳密に従うのではなく、具体的な作業から学ぶことの重要性を強調しています。"
    }
  },
  {
    "id": "72c8f64ceeefa7f5",
    "title": {
      "en": "The Mystery of the Oldest Writing System Remained Unsolved Until 1856",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.smithsonianmag.com/history/mystery-worlds-oldest-writing-system-remained-unsolved-until-four-scholars-raced-decipher-it-180985954/",
    "score": 78,
    "by": "wyclif",
    "time": 1742729075,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "a984f46e7f77422e",
    "title": {
      "en": "Ask HN: Should I leave the company I co-founded?",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": null,
    "score": 38,
    "by": "throwaway03902",
    "time": 1742864412,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "2349423c5fce781f",
    "title": {
      "en": "Total lunar eclipse over Teide crater, Tenerife – a project with many obstacles",
      "ko": "테이데의 완전한 달식",
      "ja": "テイデ火山の月食挑戦"
    },
    "type": "story",
    "url": "https://lrtimelapse.com/news/total-lunar-eclipse-over-teide-crater-tenerife/",
    "score": 164,
    "by": "elijahparker",
    "time": 1742569456,
    "content": "← LRTimelapse 7.3 released!Total lunar eclipse over the Teide crater, Tenerife – a project with many obstacles… Posted on 2025-03-21 by Gunther WegnerRarely have I thought so often about abandoning a photography project as during this lunar eclipse on Tenerife. Somehow, everything seemed to conspire against us this time. This is the story of an elaborately planned photo project where almost everything went wrong, but perseverance ultimately paid off. But let’s start from the beginning…Germans: Hier gibt es eine Deutsche Version.We, that is Uli and I, have made it almost a tradition to meet on Tenerife (Canary Islands, Spain) once a year. This time, Uli said, hey, on March 14th there’s a total lunar eclipse, and I’ve calculated a location from which we can photograph the fully eclipsed moon setting directly over the crater of Teide at dawn!It sounded almost too good to be true—because typical lunar eclipses are rather dull photographically, especially after you’ve seen and photographed a few. Without an interesting foreground, pictures of a red moon look the same anywhere in the world. Technically challenging, perhaps, but not very creative.A Unique ProjectNow, several exciting factors came together in Uli’s idea:We would experience a total eclipse on Tenerife, while in Germany, for example, the moon would only be partially eclipsed.We would have a foreground—the impressive crater of Teide, Tenerife’s landmark and Spain’s highest mountain at 3,715 meters.The totality would occur during dawn, meaning the sky would already be somewhat blue, and the moon would be low enough for us to use the crater as a foreground. We expected very aesthetic images from the contrast of the red eclipsed moon against the blue sky.According to my research, this opportunity to photograph a totally eclipsed moon over Teide wouldn’t occur again within the next 10 years—or possibly even much longer; I didn’t look further. These images would be genuinely unique.The PlanningUli used the smartphone app Planit for planning. This app not only displays exact eclipse timings but also shows the moon’s position relative to the observer’s location—in three dimensions. The app has quite a learning curve, but roughly speaking:You define the summit of Teide as your “target,” jump to the app’s day and time of moonset, and primarily search for a camera position reachable by foot.On the left, I’ve aligned the “scene” with the peak of Teide. The blue line on the right marks the spot where we’d need to be standing for the shot, based on where it intersects the trail.This means looking for an intersection on the blue line with a hiking trail or road. On Tenerife, there was actually only one reachable location, on a hiking path in the Canadas, below the caldera rim—about an hour’s hike from the nearest parking lot. This spot becomes your virtual camera position in Planit.Once you’ve found this spot, you can switch Planit to the VR camera mode to see Teide’s peak, the moon, and the Earth’s shadow through a virtual camera. You can even specify the focal length.We need to move further to the right to get the moon lined up over the crater. You can use the arrows to shift the camera position accordingly.Now, you can finely tune both the camera’s time and position—we want the fully shadowed moon precisely descending into the crater.Finally, you save the GPS coordinates for your chosen location.Leaving Nothing to ChanceRather than navigating blindly at night, we decided to scout the location during daylight first.Beautiful Landscapes in the Canadas del TeideOn a sunny morning, we drove an hour and a half up to Teide National Park and hiked to the spot. The route was quite strenuous, but the sun was shining, it was about 10 degrees Celsius—not too cold—and the solitude and landscape were fantastic.We didn’t want to think about the night yet, knowing temperatures would drop below freezing, coupled with the wind.In the background you can see the edge of the Caldera, the giant outer crater that surrounds the Pico del Teide.“A few meters more to the right,” said Uli, and I drew a line in the dust with my foot.Here’s where it would happen.At the moment, the sky was bright blue, and there was no sign of the moon. Hardly imaginable that in a few days, from exactly this spot, it would be precisely above the crater and eclipsed.We felt optimistic. This could really work out. On our way back, a wind began to pick up, funneling down from the direction of the Izaña observatories into our valley, like through a jet.The temperature dropped rapidly, and we already had to push a bit against the wind. A first, small taste of what was still ahead of us…Equipment ShortcomingsI was not entirely happy with the equipment I had available on the island. Usually, I don’t photograph much with telephoto lenses here, so my longest focal length was a 70-200mm f/2.8.Uli—always hardcore—had traveled to the island with 60 kg of gear, prepared for everything. Besides all his Sony lenses (200-600mm) and his Nikon Coolpix P1000 (24-3000mm!), he even had a 300mm and a Nikon 400mm f/2.8 with him. Crazy.“Since you have so many lenses – could you lend me your Nikon?” (The only lens of his that would fit my cameras.) Uli agreed. One less worry, as 200mm was quite short for moon photography.Another issue was my tripod situation. I had only one reliable tripod on the island. The second one didn’t deserve the name—I usually just use it as a holder for my tablet.Since this was a unique opportunity, I wanted to use two cameras. I had my Nikon Z8 and the Nikon Z50 II with me.So I decided to ask a friend—who was planning to visit us a few days later—to bring me a tripod. Unfortunately, he had to cancel the trip last minute, so once again, I was left tripod-less. Ordering stuff here on the island is a bit tricky, and good camera stores are few and far between. So I figured I’d head into the capital to buy one there. The selection I had briefly checked online wasn’t huge, but there was one tripod that actually deserved the name (and to be honest, I’m pretty into it now).After that little half-day trip, I could check off another item from the list. Now everything should finally go smoothly! (… though hindsight is always 20/20… ;-))Bad weather is coming…Weather ChallengesAlright, but the weather was still a big question mark. For days—actually, weeks—there’d been this weird weather pattern hanging over the Atlantic. Weird meaning strong winds from the west (instead of the usual northeast trade winds), rain, even snow up in the mountains, really bad weather in the north of the island, and high clouds—yep, even over the rim of the caldera, which was exactly where we wanted to shoot.And sure enough, the evening of the event was super cloudy. We kept checking the webcams and forecasts over and over.Uli wanted to head up the night before the lunar eclipse to set up a few wide-angle cameras for some time-lapse shots, planning to pick them up again the next morning.I was hoping to catch a bit of sleep beforehand. The alarm was set for 2 a.m.Worst-case scenario kicks inJust as I had packed everything and was about to head to bed, Uli called: “I’ve got good news and bad news. The good—weather’s actually not that bad. Not as windy as we feared.”“Sounds promising—and the bad?”“While I was out setting up my cameras, someone broke into my car and stole my second backpack—with all my telephoto lenses and other cameras…”“What??!!”Uli was surprisingly calm, considering he’d just lost two-thirds of his gear: “It’s insured…” — but still, how were we supposed to shoot the lunar eclipse now, without any long lenses?It really felt like the universe was telling us it wasn’t meant to be. I only had the 70–200 mm, and Uli’s longest lens left was a 135 mm. Not exactly ideal for capturing a lunar eclipse.If it had happened just a day earlier, we might’ve been able to borrow some lenses. But just hours before? No chance.We went back and forth. “So should we just call it off? Weather’s supposed to be unstable anyway…”Thinking about calling it offThat little inner voice—the one that already had zero interest in getting up at 2 a.m. and hiking for hours in the cold—kept chiming in: “This is going to be a bust anyway, it’s way too cold and probably all clouded over. Why don’t you just shoot it from down here in town, at the pier? You could sleep in two extra hours, it’s less windy, and way warmer.”Uli said he’d drive back down first and stop by a couple of electronics stores along the way—maybe, just maybe, he could find a telephoto lens there. Luckily, shops stay open pretty late on Tenerife…“Alright, let’s talk again once you’re back.”I kept refreshing the weather apps and checking webcams. Ran the numbers in Planit to figure out if we could at least capture the event down here on the coast. Sure, it would work… but it just wouldn’t be that once-in-a-lifetime shot.…or maybe not?At some point, Uli called back: “They don’t even carry camera gear at all anymore at places like MediaMarkt—but I just remembered I actually have another 300 mm lens lying around…” Me: “🙈 speechless…” Alright then—300 mm for him, 200 mm for me… not ideal, but worth a shot. Inner slacker, pipe down. “Alright, let’s do this. I’ll pick you up at 2:30,” I said. As you can probably guess, it was a short night. I showed up right on time at Uli’s: “Wow, traveling light, huh?” — honestly, without some dark humor, this whole thing would’ve been unbearable…Let’s go—up the mountain!Already in Vilaflor, the last village before the national park, the car beeped at 4°C. A few curves later, the roads were wet, and a little further up, they even started to sparkle with frost. I crawled up into the national park at 20 km/h. As soon as we entered the Caldera, we could see clouds spilling over the rim from the north—yep, exactly the direction we needed to head. “The observatories are still completely buried in clouds,” Uli said. Yeah… I figured as much. We drove past the massive “Roques de Garcia.” The full moon was so bright, we could actually see the landscape with the naked eye. At one point, I switched off the headlights—crazy how bright that full moon is.Is the weather about to ruin everything?“What if we just shoot the eclipse here, with the silhouette of the Roques in the foreground?”We thought about it for a moment. The downside: the moon would dip behind them later, since the rocks aren’t that tall, and the sky would already be getting brighter with dawn. Plus, there’d be a much higher chance of bumping into other people here—people who might wander straight into our timelapses. And honestly, it just wouldn’t be the shot we’d spent so much time planning for.So, we kept going. From the Teide cable car station onwards, the road got damp again, and I had to slow way down. These curves are no joke—and there are no winter tires on Tenerife.Wisps of clouds started forming above us. Just 4 km to go. The clouds kept thickening. Oh man, this is not looking good…Eventually, we reached the parking lot near El Portillo. The clouds were spilling into the caldera from the left and drifting right over us.Down there, where the clouds are drifting—that’s where we need to go. (Shot on smartphone)“Let’s walk over to the viewpoint and check things out—worst case, we head back to the Roques. Timing-wise, we’d still make it.”But honestly, the viewpoint didn’t really give us any new insights. The clouds were still there, and we couldn’t tell whether they’d get better or worse. We figured we had about a 50:50 chance of getting our shot. At the Roques? Pretty much a guaranteed 100%. But of course… it just wouldn’t be the same. So—what to do?We decided to take the riskIn the end, we went with the more uncertain, but potentially way more rewarding option—and started the hike.Inside the caldera (Smartphone)Thanks to the full moon, we didn’t need a headlamp or any other light. Even the initial bit of scrambling over some rocks worked just fine in moonlight alone. As timelapse and astro photographers, we naturally have a bit of an aversion to those insanely bright LED floodlights people call “headlamps” these days. And speaking of which—we actually saw quite a few of them, despite the unstable weather, making their way up the slopes of Teide. It never ceases to amaze me how many people head up there, no matter the conditions.And if it was already -4°C and pretty windy where we were at 2,200 meters, it must’ve been closer to -10°C up there, more than 1,000 meters higher—with some seriously brutal wind.Teide in the full moonlight (Smartphone)But we had layered up properly, and while we were hiking, we weren’t cold at all. The wind was at our backs, too.Setting upThe moon was still fairly high, so we had plenty of time to set everything up without rushing. Uli planned to use his main camera with the 300 mm on the Polarie star tracker. Getting the mount perfectly aligned with the North Star always takes a little time—but luckily, he managed to catch a clear glimpse of Polaris through a gap in the clouds.I had the 24–70 on the Z50II set to about 50 mm, aiming for a wide shot with the whole Teide in frame. Once the moon moved a bit lower, I was going to switch over to the 70–200 on my new tripod, dialed in at 200 mm, and aim it right at the crater’s summit. But first, I shot a “boring” sequence—just the already-eclipsed moon on its own, no background.A layer of ice formed on our gear in no timeI controlled the exposure of both cameras using qDslrDashboard—but mostly manually, not with the “Auto Holy Grail” mode. The reason? During a lunar eclipse, you need to be able to adjust exposure very quickly, while the “Auto Holy Grail” algorithm is designed to do the opposite: it smooths out short-term brightness changes and only adapts gradually over time—perfect for sunrises and sunsets, but not so much for a fast-changing eclipse.At one point, the moon drifted out of frame on the 200 mm, so I decided it was time to set up the final shot. The idea was to frame the crater at the bottom and start shooting just as the moon was entering the frame—this way, I wouldn’t have to touch the camera again until the moon had completely moved out.Challenge: ExposureA quick word about exposure during a lunar eclipse—because honestly, it’s one of the biggest challenges.While even just a sliver of the full moon is still visible, not even today’s sensors—with their 14 or 15 stops of dynamic range—can capture both the moon and the landscape without compromise. Either you expose for the landscape and the moon is totally blown out, or you expose for the moon and the landscape turns to pure black.The trick is finding that sweet spot: a balance where you can still pull some detail out of the landscape in post without everything being a noisy mess, and where the moon doesn’t just show up as a giant white blob.Test shots and histograms? Not much help either—the moon just shows up as a bright dot on the screen, everything else around it looks pitch black.This is what the image preview looks like on the monitor…At that point, you really have to rely on experience to judge the exposure—trying to nail it so there’s still just enough hidden in the shadows to pull the landscape out later. Even with a flat camera profile, there’s just no way to tell from the JPG or histogram what you’ll actually be able to recover from the RAW file later on.The same image (Nikon Z50 II) after RAW processing – you can now see the moon is already slightly eclipsed, and there’s detail in the landscape too.As totality approaches, exposure gets easier since the contrast drops. The Z50II performed well here, though of course it can’t quite keep up with the dynamic range of the Z8. 30mm, f/2.8, 2 sec, ISO 800.Everything changes suddenly the moment totality begins. In our case, the eclipse was right on the edge of the umbra, so even toward the end of totality, we still had a faint glow on the right side of the moon in the images.To get the best possible timelapse of a lunar eclipse, you have to react fast when totality hits and adjust your exposure accordingly. For the 50 mm wide shot, I decided to let the moon blow out just a little—as little as possible, but enough to still keep some detail in the landscape. For the telephoto shot, I was only capturing totality anyway, which made it way easier to expose properly.Meanwhile, clouds had drifted in front of Teide and we found ourselves standing in freezing rain. Thankfully, the crater and the moon were still more or less visible. Our camera bags were crusted with ice crystals by now, and during my final camera adjustments, I couldn’t feel my fingers anymore. I couldn’t even manage to turn off the display—the button just wouldn’t press. Oh well, the battery would survive. Hopefully.Would the moon actually fall into the crater? Shot with the Nikon Z8 at 200mm, f/2.8, ISO 800, and 2 sec exposure.So, we were all set: my wide shot was rolling, the telephoto sequence was up and running, and both of Uli’s cameras were clicking away too. I had set the interval to 3 seconds, by the way.Unexpected interruptionFinally, we could shove our hands into our warm pockets and just enjoy the show. Just as we were starting to relax, we suddenly saw two of those ultra-bright “searchlight” headlamps making their way down the rocky slope—right between us and Teide. WTF?? There’s no trail there, and definitely no parking spot up top—where the heck did they come from?They just kept weaving their way down toward us, totally unfazed, and their insanely bright beams kept sweeping straight in our direction—almost certainly hitting our cameras, too.You’ve got to be kidding me. “Turn off the lights!” I shouted into the night. “¡Apaguen las luces!” — no reaction. I called again, louder this time. Still nothing. Eventually, they settled on some rocks in front of us—but left their lights on the entire time.Who are these people? I mean, who else would be able to pinpoint this exact location so precisely? Probably only photographers, right? I mean, who else knows how to use Planit? 😉 But then again, photographers—especially astro nerds—usually know better than to blast everything with full-beam headlamps all night. We didn’t get the feeling they were even shooting photos… so what were they doing there?Guess we’ll never know. But seriously, running into someone out here, at this hour, was the last thing we expected.In the end, we just had to roll with it. Whether those lights would show up in our footage… well, we’d find out later. For now, we just wanted to soak in the magic of the total lunar eclipse.The freezing rain had stopped, and the clouds decided to give the summit a break—drifting by just beneath the peak.Totality approachesFrom the moment totality began to the point where the moon disappeared behind the crater, we had just 6 minutes. The sky had already started to take on a faint blue hue with the approaching dawn. And now, more clouds were looming—this time coming in from the right. Just after the moon touched the edge of the crater, one of those clouds drifted right in front of it—because of course it did. Luckily, it was a small one and didn’t mess up the final result too much. And—miraculously—the lights from those ridiculously bright headlamps didn’t show up in the timelapse at all. Sometimes, you just need a bit of luck!We did it!And here it is—the single frame from the timelapse that made all of this effort totally worth it:It worked! 🙂 Nikon Z8, 200mm, f/2.8, 2 sec, ISO 800.Once the moon had disappeared, we briefly considered running off to the right to catch one last glimpse of it beside the mountain—but it didn’t take long to realize that wasn’t going to happen. We still had to de-ice and pack up all the gear, and with how quickly the moon was setting, there was just no way we’d make it in time. So instead, we let the cameras keep rolling a little longer and simply watched as the day slowly began to break.Photo: Uli FehrOne more timelapse with the ice-covered camera, this time facing the edge of the caldera, where clouds were now spilling in from the south as well.We did it! 🙂The way backFrozen to the bone but happy, we packed up everything calmly and started the hike back to the car. Straight into the thick clouds that soon wrapped around us. We battled against the wind, and a thin layer of ice had formed on the ground. I really didn’t want to think about the drive back just yet…Somewhere out there in that fog bank… is our car.Every now and then, Teide lit up in a beautiful orange glow from the rising sun.Everything frozen…🙂When we finally got back to the car, it was completely frozen over too. Ice scrapers in Tenerife? Yeah… not exactly a booming market. So with hands that were finally starting to thaw, we scraped the ice off the windshield. My phone still showed -4°C.And once again, we got lucky: the road wasn’t too icy, so we carefully began the drive back—at a snail’s pace. The higher the sun climbed and the farther south we got, the drier the road became, and before long the thermometer crept back into positive territory.We did it!A little riddle for youTired but seriously happy, we started our descent—and ended up witnessing a super interesting physical-chemical phenomenon. Check out this photo:  In the spots where there was shadow—or just recently had been—the road had already dried. But in the sun? Still wet. How’s that even possible? If you know the answer, drop it in the comments! 🙂Uli picked up the camera he’d set up the night before, and then we headed back down to the warm coast for a well-deserved late breakfast—though not without quickly grabbing our laptops first to offload the sequences. And while we were still waiting for our food and coffee, we couldn’t help ourselves—we already started reviewing the footage. The excitement was just too much!Do you want to learn how to shoot and edit such timelapses? Get my E-Book Timelapse Photography and learn it step by step the easy way!My timelapseAnd to wrap things up, here’s a little compilation of the timelapse sequences I captured. Enjoy the show!\n\n      Total Lunar Eclipse over Crater of Pico del Teide, Tenerife (HDR Timelapse)\n    Video can&#8217;t be loaded because JavaScript is disabled: <a href=\"https://youtu.be/dSuCsWTxZ6U\" title=\"Total Lunar Eclipse over Crater of Pico del Teide, Tenerife (HDR Timelapse)\">Total Lunar Eclipse over Crater of Pico del Teide, Tenerife (HDR Timelapse) (https://youtu.be/dSuCsWTxZ6U)</a>To watch the video in HDR, you’ll need an HDR-capable TV or, on a computer, an HDR-capable monitor and browser (Chromium-based), with HDR enabled in your operating system. Make sure to watch the video directly on YouTube!P.S. Have you ever planned and pulled off a photo project this elaborate? I’d love to hear about it—drop me a comment!P.P.S. And be sure to check out Uli on Instagram and Facebook—he’s always up to some wild photo adventures. 🙂This might also be of interest to you:LA Timelapse Film Festival - the award ceremony now on Youtube! The Los Angeles Timelapse Film Festival (LATFF), first of its kind, took place last Saturday (Jan. 28, 2023) as a live stream and was…Northern Skies 4K (Cello Version, 2024) This is an update of my Film Northern Skies where the Cello in the Music track was played by Georg Roither, a professional cellist…Shorter Basic Tutorial (no Lightroom) This is the simplified, shorter basic tutorial for LRTimelapse without Lightroom Classic. You will learn how to edit your timelapse using LRTimelapse only: keyframe…LRTimelapse 7 released and new Tutorials! Dear Community, I've now released the final version of LRTimelapse 7, the latest \"major\" upgrade of LRTimelapse. I've spent an enormous amount of time…LA Timelapse Film Festival Audience Voting and Livestream Next Saturday, Jan. 28, 2023, the award ceremony for the Los Angeles Timelapse Film Festival will take place in an online ceremony - and…Basic Tutorial This is the complete basic tutorial for LRTimelapse. Additionally to the internal editing and processing in LRTimelapse it also includes editing and exporting via…← LRTimelapse 7.3 released!8 Responses to Total lunar eclipse over the Teide crater, Tenerife – a project with many obstacles… Constantin says: 2025-03-22 at 3:06 pmOn the topic of the road: Parts still in shade have humidity collected from the night in form of very light frost, barely visible. This melts when the sun hits and thereby becomes clearly visible as wet road.Reply Gunther Wegner says: 2025-03-22 at 5:15 pmYes, but here the road is wet in the sun while it is dry in the shade. That’s paradoxical and the other way around, as you would expect. I’ll wait for a few more answers, then I’ll solve it! 🤗Reply Mark Jaeger says: 2025-03-25 at 12:07 amGunther, Here are my thoughts on the wet vs dry pavement: The scene is on the island of Tenerife somewhere near maximum altitude (for the area).  You are 3+ thousand meters above sea level and the air temperature is a little lower than freezing. At altitude the air pressure is reduced, the air temperature is reduced and there is ample wind. All of this is favorable for sublimation but not so much for evaporation as, with below freezing temperatures, the vapor pressure is reduced.Here is how it goes: The entire road begins in shade. The sun is rising from the right side of the photo warming the road from left to right. The wind is blowing and sublimation is taking place.  The road is drying. There is a temperature where in sublimation the molecules will leave the surface and go into the air.  At slightly higher temperatures the molecules can re-attach to the surface (thus not leaving) The left side of the road has passed the critical temperature where sublimated molecules re-attach to the surface because it is “warm enough”. The right side of the road, in shadow, is not warm enough for sublimated molecules to re-attach and the molecules  go into the air (drying). The race to dryness is won by the area in shadow (a sublime story). The photo was taken at a very opportune time as, I think, the race was only won by a hair.Reply Gunther Wegner says: 2025-03-25 at 9:52 amThis is an interesting explanation. I’ve tried to do quite some research by myself, it’s not easy to find an explanation that says “this is true or false”. Sublimation was also what I instantly thought, when I saw it. But upon further research I found out that sublimation takes a very long time – hours or even days to transform thin layers of ice directly into vapor (skipping the melting).An easier, and for me also plausible explanation could be the following: When the sun hits the dark road, it warms the pavement above freezing so the thin layer of ice melts into liquid water—making those spots look wet. In the shaded areas, the surface stays below freezing, so the ice doesn’t turn into water, which would appear shiny and dark on the road. The ice molecules alone in the asphalt would make the road look dry. The moving shadows would then expose the areas at the borders of the shadows to the sun, where after a couple of minutes the ice would also melt, turning the road dark. While it’s not yet melted, however, it still looks dry (iced) at that “borders”. That’s why shaded patches look dry—even though they’re still icy—while sunny patches look wet from melting.Reply Gavin says: 2025-03-22 at 7:27 amI had a think about this phenomenon on the road and I suspect that the road heated by the sun is much closer to the dew point than the road in the shade. The clear night sky has kept the road dry, but with the advent of some warming, the moisture in the air is attracted to objects at or close to the dew point. Best way to find out the answer is to have a stab in the dark, like I have, and then the experts will show up to give us the science.Reply Matthew Wheeler says: 2025-03-22 at 5:07 amThank you for a great finger-nail biting account of this expensive adventure, with such a beautiful, spectacular outcome after all the work and obstacles–it is so well presented and I had to read to the end right away!  About the shadow “photography” on the road: I have seen this fair bit in Canada, where the snow can be etched away by the sun, leaving quite intricate images of the shadows. Frozen mud can become quite light in tone, and darken when it thaws. In this case it is possible the pavement looks dry because drizzle had frozen into it, and the width of the gap outside the current shadow shows the time it took to melt and darken the road surface.Reply Silvana Della Camera says: 2025-03-22 at 12:27 amWonderful! I am glad you didn’t give up. I have found that the battle within is often harder than the difficult task itself. Thank you for sharing your story and the remarkable results.Reply John Edwards says: 2025-03-21 at 7:48 pmLove the planning and perseverance.  Thanks for sharing your story and your excellent timelapse of the lunar eclipse.ReplyLeave a Reply Cancel replyYour email address will not be published. Required fields are marked *Comment *document.getElementById(\"comment\").setAttribute( \"id\", \"a122be334a6d59a99f9fafc8a51fd4ca\" );document.getElementById(\"c60cb81b37\").setAttribute( \"id\", \"comment\" );Name * Email * Website",
    "summary": {
      "en": "The blog post details the challenges faced by Gunther Wegner and his friend Uli while trying to photograph a total lunar eclipse over Teide crater in Tenerife. They had planned this unique project to capture stunning images of the eclipsed moon setting behind the iconic Teide mountain, which wouldn’t occur again for many years. \n\nThe duo meticulously prepared by scouting the location and using a smartphone app to plan their shots. However, they encountered numerous obstacles, including unexpected weather changes, equipment issues, and even a theft of Uli's camera gear the night before the event. Despite considering giving up due to these setbacks, they decided to go ahead with their plans, adapting by borrowing equipment and changing their shooting strategy.\n\nOn the night of the eclipse, they faced freezing temperatures and clouds that threatened to obscure the moon. Nevertheless, they persevered, ultimately capturing the mesmerizing moment as the moon dipped behind the crater. Their efforts paid off, resulting in beautiful timelapse footage of the eclipse.\n\nThe experience highlighted the importance of planning, adaptability, and persistence in photography, reinforcing the idea that even when things go wrong, dedication can lead to rewarding outcomes. The post concludes with a link to their timelapse video, showcasing the fruits of their labor.",
      "ko": "블로그 글에서는 건터 베그너와 그의 친구 울리가 테네리페의 테이데 분화구에서 총 월식 사진을 찍으려는 과정에서 겪은 어려움에 대해 자세히 설명하고 있다. 이들은 상징적인 테이데 산 뒤로 지는 월식의 아름다운 이미지를 담기 위해 이 특별한 프로젝트를 계획했으며, 이런 기회는 앞으로 수년간 다시는 없을 것이라고 했다.\n\n두 사람은 촬영 장소를 미리 조사하고 스마트폰 앱을 사용해 촬영 계획을 세우는 등 철저히 준비했다. 그러나 예상치 못한 날씨 변화, 장비 문제, 그리고 이벤트 전날 울리의 카메라 장비 도난 등 여러 장애물에 부딪혔다. 이러한 어려움으로 포기할까 고민했지만, 그들은 장비를 빌리고 촬영 전략을 변경하는 등 계획을 계속 진행하기로 결정했다.\n\n월식이 일어나는 밤, 그들은 영하의 기온과 달을 가릴 듯한 구름에 맞서야 했다. 그럼에도 불구하고 그들은 포기하지 않고, 결국 달이 분화구 뒤로 사라지는 매혹적인 순간을 포착했다. 그들의 노력은 결실을 맺어 아름다운 타임랩스 영상을 만들어냈다.\n\n이 경험은 사진 촬영에서 계획, 적응력, 그리고 끈기의 중요성을 강조하며, 일이 잘못될 때에도 헌신이 보람 있는 결과로 이어질 수 있다는 메시지를 강화했다. 글의 마지막에는 그들의 타임랩스 영상 링크가 포함되어 있어, 그들의 노력이 담긴 결과물을 보여준다.",
      "ja": "ブログ記事では、ガンター・ヴェグナーと彼の友人ウリが、テネリフェのテイデ火山のクレーターでの皆既月食を撮影する際に直面した課題について詳しく述べています。彼らは、象徴的なテイデ山の背後に沈む月の美しい画像を捉えるために、このユニークなプロジェクトを計画しました。このような機会は何年も訪れないため、特別な瞬間を逃すわけにはいきませんでした。\n\n二人は、撮影場所を下見し、スマートフォンのアプリを使ってショットを計画するなど、入念に準備を進めました。しかし、予期しない天候の変化や機材のトラブル、さらにはイベントの前夜にウリのカメラ機材が盗まれるという問題が発生しました。これらの困難に直面し、諦めることも考えましたが、彼らは計画を続行することに決め、機材を借りたり撮影戦略を変更したりしました。\n\n皆既月食の夜、彼らは凍えるような寒さと月を隠す恐れのある雲に直面しました。それでも、彼らは粘り強く努力し、最終的には月がクレーターの背後に沈む感動的な瞬間を捉えることに成功しました。その結果、皆既月食の美しいタイムラプス映像が得られました。\n\nこの経験は、写真撮影における計画、柔軟性、そして粘り強さの重要性を強調しました。物事がうまくいかない時でも、献身的な努力が報われることを再確認させるものでした。記事の最後には、彼らのタイムラプス動画へのリンクが掲載されており、彼らの努力の成果を示しています。"
    }
  },
  {
    "id": "5f918820000a49a2",
    "title": {
      "en": "A Sneaky Phish Just Grabbed My Mailchimp Mailing List",
      "ko": "메일침프 탈취 사건!",
      "ja": "フィッシングの罠！メールリスト奪取"
    },
    "type": "story",
    "url": "https://www.troyhunt.com/a-sneaky-phish-just-grabbed-my-mailchimp-mailing-list/",
    "score": 78,
    "by": "gpi",
    "time": 1742890020,
    "content": "A Sneaky Phish Just Grabbed my Mailchimp Mailing List\n\n            25 March 2025",
    "summary": {
      "en": "A person discovered that a phishing scam had successfully stolen their Mailchimp mailing list. The incident highlights the dangers of online scams and the importance of protecting personal information.",
      "ko": "한 사람이 피싱 사기에 의해 자신의 Mailchimp 메일링 리스트가 도난당한 사실을 발견했습니다. 이 사건은 온라인 사기의 위험성을 강조하며, 개인 정보를 보호하는 것이 얼마나 중요한지를 일깨워 줍니다.",
      "ja": "ある人が、フィッシング詐欺によって自分のMailchimpのメールリストが盗まれたことを発見しました。この出来事は、オンライン詐欺の危険性と個人情報を守ることの重要性を浮き彫りにしています。"
    }
  },
  {
    "id": "7f1f99c13740279a",
    "title": {
      "en": "Cottagecore Programmers",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://tjmorley.com/blogposts/cottagecoreprogrammers.html",
    "score": 102,
    "by": "morleytj",
    "time": 1742846905,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "e3f4dd00492cfe4a",
    "title": {
      "en": "Play the Virtual Organ from Arp Schnitger",
      "ko": "가상 오르간 연주하기",
      "ja": "アルプ・シュニッターのバーチャルオルガン"
    },
    "type": "story",
    "url": "https://www.orgelstadt-hamburg.de/play-arp/",
    "score": 123,
    "by": "ohjeez",
    "time": 1742754706,
    "content": "© Hauptkirche St. Jacobi Hamburg\n\n          ›play arp‹Virtuelle Orgel St. Jacobi\n          Hier haben Sie die weltweit einmalige Gelegenheit, eine der  international\n            wertvollsten und klangschönsten Barockorgeln selbst zum Klingen zu bringen.\n            Viele Orgelfans und Experten pilgern nach Hamburg, um die berühmte Arp-Schnitger\n            Orgel in der Hauptkirche St. Jacobi zu erleben. Wir bringen eine besondere\n            Auswahl klanglicher Perlen dieses Instrumentes exklusiv zu Ihnen. Spielen\n            Sie selbst!\n            (Für das klanglich beste Erlebnis empfehlen wir einen guten Kopfhörer zu nutzen)",
    "summary": {
      "en": "You can virtually play the unique Arp-Schnitger organ from the St. Jacobi Church in Hamburg, known for its beautiful Baroque sound. Many organ enthusiasts visit to experience this famous instrument. Now, you can enjoy selected pieces from it online. For the best sound quality, it's recommended to use good headphones.",
      "ko": "함부르크의 성 야코비 교회에 있는 독특한 아르프-슈니트거 오르간을 가상으로 체험할 수 있습니다. 이 오르간은 아름다운 바로크 사운드로 유명합니다. 많은 오르간 애호가들이 이 유명한 악기를 경험하기 위해 방문합니다. 이제 온라인에서 이 오르간의 선택된 곡들을 감상할 수 있습니다. 최상의 음질을 위해 좋은 헤드폰을 사용하는 것이 좋습니다.",
      "ja": "ハンブルクの聖ヤコビ教会にあるユニークなアルプ・シュニットガーオルガンを、オンラインで体験できます。このオルガンは美しいバロック音楽の音色で知られ、多くのオルガン愛好者が訪れます。現在、選ばれた楽曲をオンラインで楽しむことができます。最高の音質を楽しむためには、良いヘッドフォンを使用することをお勧めします。"
    }
  },
  {
    "id": "2577d1b6b5eb6922",
    "title": {
      "en": "LangManus: An Open-Source Manus Agent with LangChain + LangGraph",
      "ko": "랑체인 매뉴스",
      "ja": "オープンソースの言語エージェント"
    },
    "type": "story",
    "url": "https://github.com/langmanus/langmanus",
    "score": 124,
    "by": "gfortaine",
    "time": 1742554860,
    "content": "🦜🤖 LangManus\n\nEnglish | 简体中文 | 日本語\n\nCome From Open Source, Back to Open Source\n\nLangManus is a community-driven AI automation framework that builds upon the incredible work of the open source community. Our goal is to combine language models with specialized tools for tasks like web search, crawling, and Python code execution, while giving back to the community that made this possible.\nDemo\nTask: Calculate the influence index of DeepSeek R1 on HuggingFace. This index can be designed using a weighted sum of factors such as followers, downloads, and likes.\nLangManus's Fully Automated Plan and Solution:\n\nGather the latest information about \"DeepSeek R1\", \"HuggingFace\", and related topics through online searches.\nInteract with a Chromium instance to visit the HuggingFace official website, search for \"DeepSeek R1\" and retrieve the latest data, including followers, likes, downloads, and other relevant metrics.\nFind formulas for calculating model influence using search engines and web scraping.\nUse Python to compute the influence index of DeepSeek R1 based on the collected data.\nPresent a comprehensive report to the user.\n\nView on YouTube\n\nTable of Contents\n\nQuick Start\nProject Statement\nArchitecture\nFeatures\nWhy LangManus?\nSetup\n\nPrerequisites\nInstallation\nConfiguration\n\nUsage\nDocker\nWeb UI\nDevelopment\nFAQ\nContributing\nLicense\nAcknowledgments\n\nQuick Start\n# Clone the repository\ngit clone https://github.com/langmanus/langmanus.git\ncd langmanus\n\n# Install dependencies, uv will take care of the python interpreter and venv creation\nuv sync\n\n# Playwright install to use Chromium for browser-use by default\nuv run playwright install\n\n# Configure environment\n# Windows: copy .env.example .env\ncp .env.example .env\n# Edit .env with your API keys\n\n# Run the project\nuv run main.py\n\nProject Statement\nThis is an academically driven open-source project, developed by a group of former colleagues in our spare time. It aims to explore and exchange ideas in the fields of Multi-Agent and DeepResearch.\n\nPurpose: The primary purpose of this project is academic research, participation in the GAIA leaderboard, and the future publication of related papers.\nIndependence Statement: This project is entirely independent and unrelated to our primary job responsibilities. It does not represent the views or positions of our employers or any organizations.\nNo Association: This project has no association with Manus (whether it refers to a company, organization, or any other entity).\nClarification Statement: We have not promoted this project on any social media platforms. Any inaccurate reports related to this project are not aligned with its academic spirit.\nContribution Management: Issues and PRs will be addressed during our free time and may experience delays. We appreciate your understanding.\nDisclaimer: This project is open-sourced under the MIT License. Users assume all risks associated with its use. We disclaim any responsibility for any direct or indirect consequences arising from the use of this project.\n\n项目声明\n本项目是一个学术驱动的开源项目，由一群前同事在业余时间开发，旨在探索和交流 Multi-Agent 和 DeepResearch 相关领域的技术。\n\n项目目的：本项目的主要目的是学术研究、参与 GAIA 排行榜，并计划在未来发表相关论文。\n独立性声明：本项目完全独立，与我们的本职工作无关，不代表我们所在公司或任何组织的立场或观点。\n无关联声明：本项目与 Manus（无论是公司、组织还是其他实体）无任何关联。\n澄清声明：我们未在任何社交媒体平台上宣传过本项目，任何与本项目相关的不实报道均与本项目的学术精神无关。\n贡献管理：Issue 和 PR 将在我们空闲时间处理，可能存在延迟，敬请谅解。\n免责声明：本项目基于 MIT 协议开源，使用者需自行承担使用风险。我们对因使用本项目产生的任何直接或间接后果不承担责任。\n\nArchitecture\nLangManus implements a hierarchical multi-agent system where a supervisor coordinates specialized agents to accomplish complex tasks:\n\nThe system consists of the following agents working together:\n\nCoordinator - The entry point that handles initial interactions and routes tasks\nPlanner - Analyzes tasks and creates execution strategies\nSupervisor - Oversees and manages the execution of other agents\nResearcher - Gathers and analyzes information\nCoder - Handles code generation and modifications\nBrowser - Performs web browsing and information retrieval\nReporter - Generates reports and summaries of the workflow results\n\nFeatures\nCore Capabilities\n\n🤖 LLM Integration\n\nIt supports the integration of most models through litellm.\nSupport for open source models like Qwen\nOpenAI-compatible API interface\nMulti-tier LLM system for different task complexities\n\nTools and Integrations\n\n🔍 Search and Retrieval\n\nWeb search via Tavily API\nNeural search with Jina\nAdvanced content extraction\n\nDevelopment Features\n\n🐍 Python Integration\n\nBuilt-in Python REPL\nCode execution environment\nPackage management with uv\n\nWorkflow Management\n\n📊 Visualization and Control\n\nWorkflow graph visualization\nMulti-agent orchestration\nTask delegation and monitoring\n\nWhy LangManus?\nWe believe in the power of open source collaboration. This project wouldn't be possible without the amazing work of projects like:\n\nQwen for their open source LLMs\nTavily for search capabilities\nJina for crawl search technology\nBrowser-use for control browser\nAnd many other open source contributors\n\nWe're committed to giving back to the community and welcome contributions of all kinds - whether it's code, documentation, bug reports, or feature suggestions.\nSetup\nPrerequisites\n\nuv package manager\n\nInstallation\nLangManus leverages uv as its package manager to streamline dependency management.\nFollow the steps below to set up a virtual environment and install the necessary dependencies:\n# Step 1: Create and activate a virtual environment through uv\nuv python install 3.12\nuv venv --python 3.12\n\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Step 2: Install project dependencies\nuv sync\n\nBy completing these steps, you'll ensure your environment is properly configured and ready for development.\nConfiguration\nLangManus uses a three-layer LLM system, which are respectively used for reasoning, basic tasks, and vision-language tasks. Configuration is done using the conf.yaml file in the root directory of the project. You can copy conf.yaml.example to conf.yaml to start the configuration:\ncp conf.yaml.example conf.yaml\n\n# Setting it to true will read the conf.yaml configuration, and setting it to false will use the original .env configuration. The default is false (compatible with existing configurations)\nUSE_CONF: true\n\n# LLM Config\n## Follow the litellm configuration parameters: https://docs.litellm.ai/docs/providers. You can click on the specific provider document to view the completion parameter examples\nREASONING_MODEL:\n  model: \"volcengine/ep-xxxx\"\n  api_key: $REASONING_API_KEY # Supports referencing the environment variable ENV_KEY in the.env file through $ENV_KEY\n  api_base: $REASONING_BASE_URL\n\nBASIC_MODEL:\n  model: \"azure/gpt-4o-2024-08-06\"\n  api_base: $AZURE_API_BASE\n  api_version: $AZURE_API_VERSION\n  api_key: $AZURE_API_KEY\n\nVISION_MODEL:\n  model: \"azure/gpt-4o-2024-08-06\"\n  api_base: $AZURE_API_BASE\n  api_version: $AZURE_API_VERSION\n  api_key: $AZURE_API_KEY\n\nYou can create a .env file in the root directory of the project and configure the following environment variables. You can copy the.env.example file as a template to start:\ncp .env.example .env\n\n# Tool API Key\nTAVILY_API_KEY=your_tavily_api_key\nJINA_API_KEY=your_jina_api_key  # Optional\n\n# Browser Configuration\nCHROME_INSTANCE_PATH=/Applications/Google Chrome.app/Contents/MacOS/Google Chrome  # Optional, the path to the Chrome executable file\nCHROME_HEADLESS=False  # Optional, the default is False\nCHROME_PROXY_SERVER=http://127.0.0.1:10809  # Optional, the default is None\nCHROME_PROXY_USERNAME=  # Optional, the default is None\nCHROME_PROXY_PASSWORD=  # Optional, the default is None\n\nNote:\n\nThe system uses different models for different types of tasks:\n\nThe reasoning LLM is used for complex decision-making and analysis.\nThe basic LLM is used for simple text tasks.\nThe vision-language LLM is used for tasks involving image understanding.\n\nThe configuration of all LLMs can be customized independently.\nThe Jina API key is optional. Providing your own key can obtain a higher rate limit (you can obtain this key at jina.ai).\nThe default configuration for Tavily search is to return up to 5 results (you can obtain this key at app.tavily.com).\n\nConfigure Pre-commit Hook\nLangManus includes a pre-commit hook that runs linting and formatting checks before each commit. To set it up:\n\nMake the pre-commit script executable:\n\nchmod +x pre-commit\n\nInstall the pre-commit hook:\n\nln -s ../../pre-commit .git/hooks/pre-commit\n\nThe pre-commit hook will automatically:\n\nRun linting checks (make lint)\nRun code formatting (make format)\nAdd any reformatted files back to staging\nPrevent commits if there are any linting or formatting errors\n\nUsage\nBasic Execution\nTo run LangManus with default settings:\nuv run main.py\n\nAPI Server\nLangManus provides a FastAPI-based API server with streaming support:\n# Start the API server\nmake serve\n\n# Or run directly\nuv run server.py\n\nThe API server exposes the following endpoints:\n\nPOST /api/chat/stream: Chat endpoint for LangGraph invoke with streaming support\n\nRequest body:\n\n{\n  \"messages\": [{ \"role\": \"user\", \"content\": \"Your query here\" }],\n  \"debug\": false\n}\n\nReturns a Server-Sent Events (SSE) stream with the agent's responses\n\nAdvanced Configuration\nLangManus can be customized through various configuration files in the src/config directory:\n\nenv.py: Configure LLM models, API keys, and base URLs\ntools.py: Adjust tool-specific settings (e.g., Tavily search results limit)\nagents.py: Modify team composition and agent system prompts\n\nAgent Prompts System\nLangManus uses a sophisticated prompting system in the src/prompts directory to define agent behaviors and responsibilities:\nCore Agent Roles\n\nSupervisor (src/prompts/supervisor.md): Coordinates the team and delegates tasks by analyzing requests and determining which specialist should handle them. Makes decisions about task completion and workflow transitions.\n\nResearcher (src/prompts/researcher.md): Specializes in information gathering through web searches and data collection. Uses Tavily search and web crawling capabilities while avoiding mathematical computations or file operations.\n\nCoder (src/prompts/coder.md): Professional software engineer role focused on Python and bash scripting. Handles:\n\nPython code execution and analysis\nShell command execution\nTechnical problem-solving and implementation\n\nFile Manager (src/prompts/file_manager.md): Handles all file system operations with a focus on properly formatting and saving content in markdown format.\n\nBrowser (src/prompts/browser.md): Web interaction specialist that handles:\n\nWebsite navigation\nPage interaction (clicking, typing, scrolling)\nContent extraction from web pages\n\nPrompt System Architecture\nThe prompts system uses a template engine (src/prompts/template.py) that:\n\nLoads role-specific markdown templates\nHandles variable substitution (e.g., current time, team member information)\nFormats system prompts for each agent\n\nEach agent's prompt is defined in a separate markdown file, making it easy to modify behavior and responsibilities without changing the underlying code.\nDocker\nLangManus can be run in a Docker container. default serve api on port 8000.\nBefore run docker, you need to prepare environment variables in .env file.\ndocker build -t langmanus .\ndocker run --name langmanus -d --env-file .env -e CHROME_HEADLESS=True -p 8000:8000 langmanus\n\nYou can also just run the cli with docker.\ndocker build -t langmanus .\ndocker run --rm -it --env-file .env -e CHROME_HEADLESS=True langmanus uv run python main.py\n\nWeb UI\nLangManus provides a default web UI.\nPlease refer to the langmanus/langmanus-web-ui project for more details.\nDocker Compose (include both backend and frontend)\nLangManus provides a docker-compose setup to easily run both the backend and frontend together:\n# Start both backend and frontend\ndocker-compose up -d\n\n# The backend will be available at http://localhost:8000\n# The frontend will be available at http://localhost:3000, which could be accessed through web browser\n\nThis will:\n\nBuild and start the LangManus backend container\nBuild and start the LangManus web UI container\nConnect them using a shared network\n\n** Make sure you have your .env file prepared with the necessary API keys before starting the services. **\nDevelopment\nTesting\nRun the test suite:\n# Run all tests\nmake test\n\n# Run specific test file\npytest tests/integration/test_workflow.py\n\n# Run with coverage\nmake coverage\n\nCode Quality\n# Run linting\nmake lint\n\n# Format code\nmake format\n\nFAQ\nPlease refer to the FAQ.md for more details.\nContributing\nWe welcome contributions of all kinds! Whether you're fixing a typo, improving documentation, or adding a new feature, your help is appreciated. Please see our Contributing Guide for details on how to get started.\nLicense\nThis project is open source and available under the MIT License.\nStar History\n\nAcknowledgments\nSpecial thanks to all the open source projects and contributors that make LangManus possible. We stand on the shoulders of giants.\nIn particular, we want to express our deep appreciation for:\n\nLangChain for their exceptional framework that powers our LLM interactions and chains\nLangGraph for enabling our sophisticated multi-agent orchestration\nBrowser-use for control browser\n\nThese amazing projects form the foundation of LangManus and demonstrate the power of open source collaboration.",
    "summary": {
      "en": "**Summary of LangManus**\n\nLangManus is an open-source AI automation framework created by a community of developers. It aims to combine language models with specialized tools for tasks like web searching and executing Python code, while also giving back to the open-source community.\n\n### Key Features:\n- **Community-Driven**: Built upon contributions from various open-source projects.\n- **Multi-Agent System**: Uses different agents (Coordinator, Planner, Researcher, Coder, Browser, and Reporter) to handle complex tasks.\n- **Integration Capabilities**: Supports various language models and tools for web search and content extraction.\n- **Python Integration**: Includes a Python execution environment for coding tasks.\n- **Visualization**: Offers workflow management tools to visualize and control tasks.\n\n### Setup and Usage:\n- **Installation**: Users can clone the repository, install dependencies, and configure their environment using provided files.\n- **Running the Project**: Can be launched using commands that start the main program or API server.\n- **Docker Support**: LangManus can run in Docker containers, simplifying deployment and management.\n\n### Contributions and Community:\n- The project encourages contributions in multiple forms, such as code, documentation, and bug reports.\n- It operates independently from the developers' primary jobs and does not represent any organizations.\n\n### License:\nLangManus is open source under the MIT License, allowing users to modify and use it freely.\n\n### Acknowledgments:\nThe developers express gratitude to other open-source projects that support LangManus, highlighting the collaborative nature of their work.",
      "ko": "LangManus는 개발자 커뮤니티가 만든 오픈 소스 AI 자동화 프레임워크입니다. 이 프레임워크는 언어 모델과 웹 검색, 파이썬 코드 실행과 같은 작업을 위한 전문 도구를 결합하는 것을 목표로 하며, 오픈 소스 커뮤니티에 기여하는 데 중점을 두고 있습니다.\n\nLangManus의 주요 특징 중 하나는 다양한 오픈 소스 프로젝트의 기여를 바탕으로 한 커뮤니티 주도형 개발입니다. 이 시스템은 복잡한 작업을 처리하기 위해 조정자, 계획자, 연구자, 코더, 브라우저, 보고서 작성자 등 여러 에이전트를 사용합니다. 또한, 다양한 언어 모델과 도구를 지원하여 웹 검색 및 콘텐츠 추출을 가능하게 합니다. 파이썬 실행 환경이 포함되어 있어 코딩 작업도 수행할 수 있습니다. 작업을 시각화하고 관리할 수 있는 워크플로 관리 도구도 제공합니다.\n\n설치 과정은 사용자가 저장소를 복제하고, 의존성을 설치한 후 제공된 파일을 사용하여 환경을 설정하는 방식으로 진행됩니다. 프로젝트는 명령어를 통해 메인 프로그램이나 API 서버를 시작하여 실행할 수 있습니다. LangManus는 도커 컨테이너에서 실행할 수 있어 배포와 관리가 간편합니다.\n\n이 프로젝트는 코드, 문서화, 버그 보고 등 다양한 형태의 기여를 장려합니다. 개발자들은 이 프로젝트가 그들의 주된 직업과는 독립적으로 운영되며, 특정 조직을 대표하지 않는다고 밝히고 있습니다.\n\nLangManus는 MIT 라이선스 하에 오픈 소스로 제공되어 사용자가 자유롭게 수정하고 사용할 수 있습니다. 개발자들은 LangManus를 지원하는 다른 오픈 소스 프로젝트에 감사의 뜻을 전하며, 협력적인 작업의 중요성을 강조하고 있습니다.",
      "ja": "LangManusは、開発者コミュニティによって作られたオープンソースのAI自動化フレームワークです。このプロジェクトは、言語モデルと特定のツールを組み合わせて、ウェブ検索やPythonコードの実行などのタスクを効率的に行うことを目的としています。また、オープンソースコミュニティへの貢献も重視しています。\n\nLangManusの主な特徴には、コミュニティ主導の開発があり、さまざまなオープンソースプロジェクトからの貢献を基にしています。複雑なタスクを処理するために、コーディネーター、プランナー、リサーチャー、コーダー、ブラウザー、レポーターといった異なるエージェントを使用するマルチエージェントシステムを採用しています。また、ウェブ検索やコンテンツ抽出のためのさまざまな言語モデルやツールとの統合が可能です。さらに、コーディングタスク用のPython実行環境も含まれています。作業の可視化を行うためのワークフロー管理ツールも提供されています。\n\nインストールは、ユーザーがリポジトリをクローンし、依存関係をインストールし、提供されたファイルを使って環境を設定することで行えます。プロジェクトは、メインプログラムやAPIサーバーを起動するコマンドを使って実行できます。また、LangManusはDockerコンテナ内で動作することができ、デプロイや管理が簡単になります。\n\nこのプロジェクトは、コード、ドキュメント、バグ報告など、さまざまな形での貢献を奨励しています。開発者の主な仕事とは独立して運営されており、特定の組織を代表するものではありません。\n\nLangManusはMITライセンスの下でオープンソースとして提供されており、ユーザーは自由に修正や利用ができます。開発者たちは、LangManusを支える他のオープンソースプロジェクトに感謝の意を表しており、彼らの協力的な取り組みを強調しています。"
    }
  },
  {
    "id": "4e1d365d65298255",
    "title": {
      "en": "BeeFormer: CF and CBF hybrid approach for recommendation systems",
      "ko": "비포머: 추천 시스템의 혁신",
      "ja": "ビー former: ハイブリッド推薦法"
    },
    "type": "story",
    "url": "https://github.com/recombee/beeformer",
    "score": 42,
    "by": "klaussilveira",
    "time": 1742732424,
    "content": "beeFormer\nThis is the official implementation provided with our paper beeFormer: Bridging the Gap Between Semantic and Interaction Similarity in Recommender Systems.\nmain idea of beeFormer\n\nCollaborative filtering (CF) methods can capture patterns from interaction data that are not obvious at first sight. For example, when buying a printer, users can also buy toners, papers, or cables to connect the printer, and collaborative filtering can take such patterns into account. However, in the cold-start recommendation setup, where new items do not have any interaction at all, collaborative filtering methods cannot be used, and recommender systems are forced to use other approaches, like content-based filtering (CBF). The problem with content-based filtering is that it relies on item attributes, such as text descriptions. In our printer example, semantic similarity-trained language models will put other printers closer than accessories that users might be searching for. Our method is training language models to learn these user behavior patterns from interaction data to transfer that knowledge to previously unseen items. Our experiments show that performance benefits from this approach are enormous.\nSteps to start training the models:\n\ncreate virtual environment python3.10 -m venv beef and activate it source beef/bin/activate\nclone this repository and navigate to it cd beeformer\ninstall packages pip install -r requirements.txt\ndownload the data for movielens: navigate to the _dataset/ml20m folder and run source download_data\ndownload the data for goodbooks: navigate to the _dataset/goodbooks folder and run source download_data\ndownload the data for amazonbooks: navigate to the _dataset/amazonbooks folder and run source download_data && python preprocess.py\nin the root folder of the project run the train.py, for example like this:\n\npython train.py --seed 42 --scheduler None --lr 1e-5 --epochs 5 --dataset goodbooks --sbert \"sentence-transformers/all-mpnet-base-v2\" --max_seq_length 384 --batch_size 1024 --max_output 10000 --sbert_batch_size 200 --use_cold_start true --save_every_epoch true --model_name my_model\n\nEvaluate the results. To reproduce numbers from the paper using our hugginface repository, run for example:\n\npython evaluate_itemsplit.py --seed 42 --dataset goodbooks --sbert beeformer/Llama-goodbooks-mpnet\n\nor\npython evaluate_timesplit.py --seed 42 --dataset amazon-books --sbert beeformer/Llama-amazbooks-mpnet\n\nDatasets and preprocessing\nPreprocessing information\nWe consider ratings of 4.0 and higher as an interaction. We only keep the users with at least 5 interactions.\nLLM Data augmentations\nSince there are no text descriptions in the original data, we manually connect several datasets with the original data and train our models on it. However, this approach has several limitations: texts from different sources have different styles and different lengths, and this might influence the results. Therefore, we use the Llama-3.1-8b-instruct model to generate item descriptions for us. We use the following conversation template:\nimport pandas as pd\n\nfrom tqdm import tqdm\nfrom vllm import LLM, SamplingParams\n\nitems = pd.read_feather(\"items_with_gathered_side_info.feather\")\n\nllm = LLM(model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",dtype=\"float16\")\n\ntokenizer = llm.get_tokenizer()\nconversation = [ tokenizer.apply_chat_template(\n        [\n            {'role': 'system','content':\"You are ecomerce shop designer. Given a item description create one paragraph long summarization of the product.\"},\n            {'role': 'user', 'content': \"Item description: \"+x},\n            {'role': 'assistant', 'content': \"Sure, here is your one paragraph summary of your product:\"},\n        ],\n        tokenize=False,\n    ) for x in tqdm(items.gathered_features.to_list())]\n\noutput = llm.generate(\n    conversation,\n    SamplingParams(\n        temperature=0.1,\n        top_p=0.9,\n        max_tokens=512,\n        stop_token_ids=[tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")],\n    )\n)\n\nitems_descriptions = [o.outputs[0].text for o in output]\n\nHowever, LLM refused to generate descriptions for some items (For example, because it refuses to generate explicit content). We removed such items from the dataset. We also removed items for which we were not able to connect meaningful descriptions from other datasets, which led to LLM completely hallucinating item descriptions.\nWe share the resulting LLM-generated item descriptions in datasets/ml20m, dataset/goodbooks and datasets/amazonbooks folders.\nStatistics of datasets used for evaluation\n\nGoodBooks-10k\nMovieLens-20M\nAmazon Books\n\n# of items in X\n9975\n16902\n63305\n\n# of users in X\n53365\n136589\n634964\n\n# of interactions in X\n4119623\n9694668\n8290500\n\ndensity of X [%]\n0.7739\n0.4199\n0.0206\n\ndensity of X^TX [%]\n41.22\n26.93\n7.59\n\nPretrained models\nWe share pretrained models at https://huggingface.co/beeformer.\nHyperparameters\nWe used hyperparameters for training our models as follows.\n\nhyperparameter\ndescription\nbeeformer/Llama-goodbooks-mpnet\nbeeformer/Llama-movielens-mpnet\nbeeformer/Llama-goodlens-mpnet\nbeeformer/Llama-amazbooks-mpnet\n\nseed\nrandom seed used during training\n42\n42\n42\n42\n\nscheduler\nlearning rate scheduling strategy\nconstant learning rate\nconstant learning rate\nconstant learning rate\nconstant learning rate\n\nlr\nlearning rate\n1e-5\n1e-5\n1e-5\n1e-5\n\nepochs\nnumber of trained epochs\n5\n5\n10\n5\n\ndevices\ntraining script allow to train on multiple gpus in parallel - we used 4xV100\n[0,1,2,3]\n[0,1,2,3]\n[0,1,2,3]\n[0,1,2,3]\n\ndataset\ndataset used for training\ngoodbooks\nml20m\ngoodlens\namazon-books\n\nsbert\noriginal sentence transformer model used as an initial model for training\nsentence-transformers/all-mpnet-base-v2\nsentence-transformers/all-mpnet-base-v2\nsentence-transformers/all-mpnet-base-v2\nsentence-transformers/all-mpnet-base-v2\n\nmax_seq_length\nlimitation of sequence length; shorter sequences trains faster original mpnet model uses max 512 tokens in. sequence\n384\n384\n384\n384\n\nbatch_size\nnumber of users sampled in random batch from interaction matrix\n1024\n1024\n1024\n1024\n\nmax_output\nnegative sampling hyperparameter (m in the paper). Negatives are sampled uniformly at random.\n10000\n10000\n10000\n12500\n\nsbert_batch_size\nnumber of items processed together during training step (gradient accumulation step size)\n200\n200\n200\n200\n\nuse_cold_start\nsplit the dataset item-wise (some items are hidden to test the genralization towards new items)\ntrue\ntrue\ntrue\nfalse\n\nuse_time_split\nsort interactions by timestamp and use last 20% of interactions as a test set (generalization from the past to the future)\nfalse\nfalse\nfalse\ntrue\n\nRecSys 2024 poster\n\nCitation\nIf you find this repository helpful, feel free to cite our paper:\n@inproceedings{10.1145/3640457.3691707,\n        author = {Van\\v{c}ura, Vojt\\v{e}ch and Kord\\'{\\i}k, Pavel and Straka, Milan},\n        title = {beeFormer: Bridging the Gap Between Semantic and Interaction Similarity in Recommender Systems},\n        year = {2024},\n        isbn = {9798400705052},\n        publisher = {Association for Computing Machinery},\n        address = {New York, NY, USA},\n        url = {https://doi.org/10.1145/3640457.3691707},\n        doi = {10.1145/3640457.3691707},\n        booktitle = {Proceedings of the 18th ACM Conference on Recommender Systems},\n        pages = {1102–1107},\n        numpages = {6},\n        keywords = {Cold-start recommendation, Recommender systems, Sentence embeddings, Text mining, Zero-shot recommendation},\n        location = {Bari, Italy},\n        series = {RecSys '24}\n}",
    "summary": {
      "en": "### Summary of beeFormer\n\n**Overview:**\nbeeFormer is a new method for improving recommendation systems, especially useful when dealing with new items that have no previous user interactions (cold-start situations). Traditional collaborative filtering can identify user behavior patterns, but it struggles with new items. In contrast, content-based filtering focuses on item attributes, which may not effectively connect similar items, such as accessories for a printer.\n\n**Key Idea:**\nbeeFormer trains language models to recognize user behavior patterns from interaction data. This knowledge can then be applied to new items, enhancing recommendation accuracy.\n\n**Getting Started:**\n1. **Set Up Environment:**\n   - Create a virtual environment and activate it.\n   - Clone the beeFormer repository.\n\n2. **Install Required Packages:**\n   - Use `pip` to install necessary packages from a requirements file.\n\n3. **Download Datasets:**\n   - Navigate to specific folders and run scripts to download datasets for MovieLens, GoodBooks, and Amazon Books.\n\n4. **Train Models:**\n   - Execute the training script with various parameters to train the model.\n\n5. **Evaluate Performance:**\n   - Run evaluation scripts to reproduce results from the research paper.\n\n**Data Processing:**\n- Only interactions with ratings of 4.0 or higher are considered.\n- Use a language model to generate item descriptions due to the lack of original text in the datasets. Some items may be excluded if descriptions cannot be generated.\n\n**Datasets:**\n- The method evaluates datasets like GoodBooks, MovieLens, and Amazon Books, each with a specific number of items, users, and interactions.\n\n**Pretrained Models:**\n- Pretrained models are available for use and can be accessed online.\n\n**Hyperparameters:**\n- Specific settings for training include learning rates, batch sizes, and the number of epochs.\n\n**Citation:**\nFor reference or academic use, the paper detailing this method can be cited with the provided citation format.\n\nThis summary simplifies the main points of the beeFormer method and its implementation in recommender systems, making it easier to understand and follow.",
      "ko": "beeFormer는 추천 시스템을 개선하기 위한 새로운 방법으로, 특히 사용자 상호작용이 없는 새로운 아이템을 다룰 때 유용합니다. 기존의 협업 필터링은 사용자 행동 패턴을 파악할 수 있지만, 새로운 아이템에 대해서는 한계를 보입니다. 반면, 콘텐츠 기반 필터링은 아이템의 속성에 초점을 맞추지만, 프린터의 액세서리와 같은 유사한 아이템을 효과적으로 연결하지 못할 수 있습니다.\n\nbeeFormer의 핵심 아이디어는 언어 모델을 훈련시켜 사용자 행동 패턴을 상호작용 데이터에서 인식하는 것입니다. 이렇게 얻은 지식을 새로운 아이템에 적용하여 추천의 정확성을 높일 수 있습니다.\n\n시작하려면 먼저 가상 환경을 설정하고 활성화한 후, beeFormer 저장소를 복제합니다. 필요한 패키지는 요구 사항 파일을 통해 `pip`로 설치합니다. 그 다음, 특정 폴더로 이동하여 스크립트를 실행해 MovieLens, GoodBooks, Amazon Books의 데이터셋을 다운로드합니다. 모델을 훈련시키기 위해 다양한 매개변수를 사용하여 훈련 스크립트를 실행합니다. 마지막으로, 연구 논문의 결과를 재현하기 위해 평가 스크립트를 실행합니다.\n\n데이터 처리 과정에서는 평점이 4.0 이상인 상호작용만 고려하며, 데이터셋에 원본 텍스트가 부족하기 때문에 언어 모델을 사용해 아이템 설명을 생성합니다. 설명을 생성할 수 없는 아이템은 제외될 수 있습니다. 이 방법은 GoodBooks, MovieLens, Amazon Books와 같은 데이터셋을 평가하며, 각 데이터셋은 특정 수의 아이템, 사용자, 상호작용을 포함하고 있습니다.\n\n사전 훈련된 모델은 온라인에서 사용할 수 있으며, 훈련을 위한 특정 설정으로는 학습률, 배치 크기, 에폭 수가 포함됩니다. 이 방법에 대한 참고나 학술적 사용을 위해서는 제공된 인용 형식으로 논문을 인용할 수 있습니다. 이 요약은 beeFormer 방법과 추천 시스템에서의 구현을 간단하게 정리하여 이해하기 쉽게 전달합니다.",
      "ja": "beeFormerは、特に新しいアイテムに対する推薦システムを改善するための新しい手法です。従来の協調フィルタリングはユーザーの行動パターンを特定することができますが、新しいアイテムには対応が難しいです。一方、コンテンツベースのフィルタリングはアイテムの属性に焦点を当てますが、プリンターのアクセサリーのように類似アイテムを効果的に結びつけることができない場合があります。\n\nbeeFormerの主なアイデアは、言語モデルを訓練してユーザーの行動パターンをインタラクションデータから認識させることです。この知識を新しいアイテムに適用することで、推薦の精度を向上させます。\n\n導入手順は以下の通りです。まず、仮想環境を作成し、アクティブにします。次に、beeFormerのリポジトリをクローンします。その後、必要なパッケージをrequirementsファイルからpipを使ってインストールします。データセットをダウンロードするために、特定のフォルダに移動し、スクリプトを実行します。モデルを訓練するためには、さまざまなパラメータを用いて訓練スクリプトを実行します。最後に、評価スクリプトを実行して研究論文の結果を再現します。\n\nデータ処理では、評価対象となるのは評価が4.0以上のインタラクションのみです。データセットに元のテキストがないため、言語モデルを使用してアイテムの説明を生成します。説明が生成できないアイテムは除外されることがあります。\n\nこの手法は、GoodBooks、MovieLens、Amazon Booksなどのデータセットを評価します。それぞれ特定のアイテム数、ユーザー数、インタラクション数があります。事前に訓練されたモデルもオンラインで利用可能です。\n\n訓練に関するハイパーパラメータには、学習率、バッチサイズ、エポック数などが含まれます。参考文献や学術的な使用のためには、この手法に関する論文を指定の引用形式で引用できます。\n\nこの要約は、beeFormerの主なポイントと推薦システムへの実装を簡潔に説明しており、理解しやすくなっています。"
    }
  },
  {
    "id": "a8de69e131cf93a9",
    "title": {
      "en": "Show HN: I built a website for sharing drum patterns",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "http://drumpatterns.onether.com",
    "score": 476,
    "by": "wesz",
    "time": 1742735121,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "caba56a5a2afb958",
    "title": {
      "en": "The Software Engineering Identity Crisis",
      "ko": "소프트웨어 정체성 위기",
      "ja": "ソフトウェアの葛藤"
    },
    "type": "story",
    "url": "https://annievella.com/posts/the-software-engineering-identity-crisis/",
    "score": 129,
    "by": "napolux",
    "time": 1742755045,
    "content": "The Software Engineering Identity Crisis\n\n        March 21, 2025\n\n      Many of us became software engineers because we found our identity in building things. Not managing things. Not overseeing things. Building things. With our own hands, our own minds, our own code.\nBut that identity is being challenged.\nAI coding assistants aren’t just changing how we write software - they’re fundamentally transforming who we are. We’re shifting from creators to orchestrators, from builders to overseers. From engineers to something that looks suspiciously like… managers.\nThe irony cuts deep: for years, we’ve said that software engineering transcends mere coding. Requirements, design, testing, operations - these were all supposedly part of our craft. Yet the industry pushed us in the opposite direction. We handed these responsibilities to specialists - Product Owners, Architects, Quality Engineers, Platform Engineers - while we doubled down on our coding expertise. We became masters of our code, proud wielders of a modern magic.\nAnd now, just as we’ve perfected this craft, AI is threatening to take it away from us.\nThe Joy We’re About to Lose\nLet’s be honest about what’s at stake here. Many of us don’t just write code - we love writing code. Our identity is woven into every elegant solution we craft, every test we make pass, every problem we solve through pure logic and creativity. It’s not just work, not just a craft - it’s who we are.\nThink about those moments of deep satisfaction: when you finally track down that elusive bug that’s been haunting production, when you work out how to optimise that slow algorithm and watch response times drop from seconds to milliseconds, when you transform a maze of legacy code into something clean and maintainable. These aren’t just achievements - they’re expressions of who we are as engineers. They’re the moments that remind us why we chose this path.\nNow imagine AI taking over these moments of craftsmanship. The creators of these tools paint an optimistic picture - they say we’ll spend more time on defining intent, high-level architecture, and systems thinking. But listen carefully to what they’re really saying: we’ll become overseers rather than creators, managers rather than builders.\n\n      Are software engineers evolving into system thinkers and high-level architects?\n\nThis shift raises important questions about our identity as builders: Is oversight what drives us? Is it what makes us leap out of bed in the morning, eager to solve the next puzzle?\nThe Identity Shift: It’s Already Here\nThis isn’t some theoretical future we’re debating - it’s the reality we’re living right now. When Google recently revealed that AI generates more than a quarter of their new code, it was just the beginning. Y Combinator CEO Garry Tan reveals that for about a quarter of their startups, 95% of their code is now written by AI - marking a truly significant shift. My own Master’s research revealed a similar picture: 77% of us are spending less time writing code, and almost half believe our core skill might become secondary to prompt engineering. Think about that transition: from crafting solutions to crafting prompts.\n\n      Will Prompt Engineering Replace Traditional Coding Skills?\n\nWhen asked how to develop prompt engineering skills, software engineers emphasise refining communication skills. Getting an AI to do what you want means being able to articulate things well - providing just the right amount of context along with a clear description of the task. The better you can communicate with Gen AI, the more likely the output will match your expectations. Some even suggest being polite to these tools, treating them like team members - as if you’re leading another human being to do something for you.\nThe changes run so deep that we’re inventing new terms just to describe who we’re becoming. Take vibe coding, a term that Andrej Karpathy recently coined in a tweet. It captures a profound shift in how we write software.\nAt one end of the spectrum lies the traditional way - the craftsperson’s way. We write each line with intent, every function name and architectural decision reflecting our deep understanding of the system.\nAt the other end? We let AI fill in the blanks, “vibing” with its suggestions. We focus on the what, not the how. As Karpathy puts it: “fully give in to the vibes, embrace exponentials, and forget that the code even exists.”\nThat last part gives us pause - are we still engineers if we forget all about the code?\nIn a recent pairing session, engineering thought leaders Gene Kim and Steve Yegge demonstrated what this looks like in practice. Using an AI coding assistant, they ported a 3,500-line legacy Ruby script to Kotlin - a task that would normally take a week - in just one hour. The AI didn’t just translate the code; it improved it, adding the modular architecture and unit tests they’d wanted for years but couldn’t justify the time for.\nEven Patrick Debois, the godfather of DevOps, sees this transformation reshaping our identity. In his recent analysis of AI Native Development patterns, he outlines four fundamental shifts in how we work:\n\n      Patrick Debois: The 4 Patterns of AI Native Dev\n\nThese patterns reveal a profound shift: we’re moving from producers to managers of AI systems, from detailed implementation to expressing intent, from delivery to discovery through rapid experimentation, and from content creation to knowledge curation. Our role is evolving to blend creation with orchestration, building with oversight.\nOverall, I think it’s fair to say that the nature of our professional identity is changing at its core.\nThe Craft That Shaped Our Identity\nTo understand this identity crisis, we need to look at how deeply the craft of coding has shaped who we are. At its core, writing code is about mastery and control - skills we’ve spent years perfecting. Modern programming languages are much higher-level than those of days gone by, but they still require deep technical understanding. Few developers today deal with the nitty-gritty of pointers and memory management, yet we still take pride in knowing how things work under the hood. Even as frameworks do more heavy lifting, we’ve maintained our identity as artisans who understand our tools intimately.\nProgramming today is much more about stitching together APIs, frameworks, and libraries in creative ways to build something meaningful. In fact, recent research at Google suggests that creativity in software engineering centres on the concept of clever reuse over pure novelty. This makes sense to me - I’ve often commented that we’re all just ‘integration’ engineers nowadays, really.\nStill, there’s a strange pride that we take in knowing all the weird syntax required to build something. It’s like a secret language that only we understand. Becoming proficient with a programming language gives you a lot of control to make it do precisely what you want it to do. It’s so detailed - just one wrong character will break the entire thing and it can take a lot of time and patience to make it do what you want.\n First, one must perform perfectly. The computer resembles the magic of legend in this respect, too. If one character, one pause, of the incantation is not strictly in proper form, the magic doesn’t work.\n— Frederick P. Brooks, The Mythical Man-Month, Chapter 1, Addison-Wesley, 1975\nThe other 99% of the population thinks we’re magicians for understanding code and indeed, it can take years of deliberate practice to master it. Those who master more than one programming language have the privilege of being referred to as polyglots. Many of us take great pride in writing clean, elegant code. We passionately argue over different styles and best practices, often taking it all too seriously.\nA Reluctant Manager’s Tale\nLet me share a story about identity evolution that might resonate.\nAfter a decade of being an individual contributor, I hit the infamous ceiling of the technical track. Senior Lead Software Engineer - that was as far as the technical track went. Staff+ Engineering wasn’t a thing yet, and the sole Architecture position at the company I was working for was filled. I faced a choice that would change who I was: stay a builder or become an overseer.\nI chose management. Reluctantly. That’s where the path led me. I told myself it was still engineering, just at a different level. That managing systems wasn’t so different from managing people. That I could still keep my hands in the code in between other tasks.\nSound familiar? The parallels are uncanny. Just as I had to trade direct problem-solving for meetings and documentation, we’re now being asked to trade coding for prompt engineering. The skills that defined us as engineers - mastering syntax, architecting our code elegantly, catching and handling the edge cases, debugging complex issues - are being relegated to AI. Instead, we’re told to focus on skills that sound suspiciously like management: clear communication, systems thinking, problem definition.\nBut here’s what no one’s talking about: the identity crisis. That deep sense of loss when you realise you’re no longer building things with your own hands. When your technical mastery becomes less relevant than your ability to “manage” the tools. When your craft becomes oversight.\nCan orchestrating AI ever give us that same sense of identity? Of being a builder, a creator, a problem solver?\nWhen Machines Challenge Our Identity\nBy now the source of our identity crisis becomes clear. The craft we spent years perfecting - the one that gave us purpose, meaning, and pride - is now being done faster, cheaper, and at scale by a machine. Sure, the quality isn’t as good as your hand-written code (yet). But the speed at which code can now be written is staggering and businesses are tripping over themselves to get in on the action.\nThis is where a glimmer of hope emerges. Remember that irony - how we gave away the broader aspects of our craft to specialists? AI is pushing us to reclaim what we once knew: that software engineering transcends mere coding. That core truth remains - ultimately, software engineering is about solving problems, creating solutions, building things that matter.\nThese broader skills - what Addy Osmani calls “durable engineering skills” in his article on the human 30% of AI-assisted coding - have always separated great engineers from good ones. Communication, big-picture thinking, handling ambiguity - these become even more crucial in an AI-driven world.\nYet this emphasis on broader skills has sparked debate in our community. For some, it sounds suspiciously like management repackaged. And they’re not entirely wrong - a recent CIO article confirms that development teams are already being restructured to focus on oversight rather than creation. The article envisions future teams consisting of a product manager, a UX designer, and a software architect who primarily uses AI to generate prototypes. These architects, or senior developers, must “understand the content… who the customer is and what we’re trying to achieve” - classic management responsibilities repackaged as technical work.\n\n      Management in a Cloak\n\nThis evolution raises fundamental questions about our identity as engineers: As the traditional career ladder transforms, how will the next generation of software engineers develop their skills? How do we preserve the deep technical understanding and craftsmanship that shaped our profession while embracing these new tools? And perhaps most unsettling - as AI capabilities advance exponentially, will our role as craftspeople become as obsolete as the manual weaver’s did during the Industrial Revolution?\nThe Path Forward\nPerhaps the answer lies not in resisting this transformation, but in understanding it through the lens of history. These identity crises - these fundamental shifts in how we define ourselves through our work - aren’t new. They’re part of a pattern that repeats whenever technology reshapes a profession.\nDuring the Industrial Revolution, craftspeople faced a similar crisis. Their traditional skills - honed over generations - were being replaced by machines. But what happened next is fascinating: many adapted, becoming specialist professionals who could fix and improve these same machines that threatened to replace them. Others found ways to apply their deep understanding of materials and processes to improve overall factory operations.\nIf we draw this parallel to our AI era, a similar path emerges. The core of software engineering - solving problems and creating value - remains unchanged. Our tools are evolving, and with them, the skills needed to wield them effectively.\nThe question isn’t whether we’ll become managers of machines - it’s whether we can find the same satisfaction in this evolution of our craft.\nThe Engineer’s Dilemma\nSo where does this leave us? Are we all destined to become overseers of AI agents rather than writers of code? Is this a future to resist or embrace?\nThe truth, as always, is nuanced. Just as some engineers naturally gravitate toward management while others prefer to remain hands-on, we’ll likely see a similar spectrum emerge in how we interact with AI. Some will excel at orchestrating AI systems, focusing on high-level design and making these systems more efficient and reliable - conducting a technical symphony rather than performing solo. Others will find their calling in domains where human expertise remains crucial - perhaps in security-sensitive applications, novel areas where AI lacks training data, or systems where performance and reliability are paramount. The key isn’t to resist this evolution, but to find our place within it.\nWhat’s clear is that the definition of “software engineer” is expanding, not contracting. The skills that make someone valuable are diversifying. And this creates both challenges and opportunities.\nFor those who love the craft of coding, this shift can feel threatening. But remember that AI tools are still just that - tools. They don’t understand the “why” behind the code, the business context, or the human needs being served. They can’t innovate in the true sense of the word, at least not yet. And as far as we know, they can’t feel the satisfaction of solving a complex problem or the joy of creating something new.\nPerhaps the most valuable skill in this new landscape isn’t prompt engineering or systems architecture, but adaptability - the willingness to evolve, to learn new skills, and to find your unique place in a rapidly changing field.\nThe Bright Side\nDespite these challenges, there’s something important we need to acknowledge: these AI tools can be incredibly empowering. With agentic IDEs like Windsurf and Cursor taking software development to a whole new level, it’s like having a supportive pair-programming partner who’s always there, ready to help you tackle problems that might have seemed daunting before.\nFor junior developers or those of us who might feel a bit rusty, AI assistants can be a confidence booster - helping you get started when you’re staring at a blank file, validating your approach when you’re unsure, or explaining complex concepts in a way that makes sense to you. For experienced developers, they’re like having a tireless assistant who can handle the routine tasks while you focus on the more challenging aspects of the problem.\nThe speed at which we can now prototype ideas, explore different approaches, and learn new technologies is truly remarkable. What might have taken weeks of research and trial-and-error can often be accomplished in hours or even minutes. It’s like having a superpower - the ability to amplify our capabilities and turn our ideas into reality faster than ever before.\nThe Reality Check\nBut with great power comes great responsibility. A recent comprehensive GitClear study analysing 211 million lines of code revealed some concerning trends as AI code generation tools became more prevalent:\n\nA 17.1% increase in copy-pasted code, marking the first time AI-assisted code duplication surpassed refactored (moved) code.\nAn 8-fold rise in duplicated code blocks, with 6.66% of commits now containing repeated code sections.\nA 26% increase in code churn, with 5.7% of all code changes being revised or deleted within two weeks.\n\n      GitClear: Trends in Code Changes\n\nWhile we’re producing code faster than ever, we’re also spending more time fixing AI-generated mistakes and dealing with code that’s harder to maintain. This isn’t just about speed - it’s about the craft of writing sustainable, maintainable software.\nThe Hidden Identity Crisis\nYet beneath these surface-level changes lies a deeper challenge - one that strikes at the heart of who we are as engineers. The emerging field of Human-AI teaming is revealing uncomfortable truths about our future. A 2024 study shows that when humans and AI work together, the results often fall short of expectations. Not because AI lacks capability, but because trust works differently with machines than with humans.\nWe don’t build trust with AI the way we do with human teammates.\nWith humans, trust grows gradually, through shared success. Each problem solved together strengthens the bond. Even failures can deepen trust when handled well. With AI, trust often starts high and erodes fast.\nEvery incorrect response, every hallucinated bug fix, every misplaced confidence weakens our faith in the machine. Unlike human relationships where trust typically grows over time, AI trust often peaks early and declines.\nAnd when trust erodes, so does productivity.\nThe research reveals why:\n\nThere’s an inherent unpredictability in how AI interprets our intentions\nIt lacks the contextual awareness that makes human collaboration fluid\nIts decisions often lack transparency, making it hard to rebuild trust once lost\n\nThese challenges mirror what many of us experience when transitioning into technical leadership. Just as new engineering managers must learn to trust their team’s work without doing it themselves, we now face a similar transition with AI - learning to guide and verify rather than write every line ourselves.\nThe reality is stark: despite AI’s raw capabilities, teams often perform worse with AI than without it. Just as a team’s productivity suffers under ineffective leadership, our effectiveness diminishes when we don’t understand how to work with our AI tools.\nReclaiming Your Identity\nDrawing from both my journey as a reluctant manager and my research into this AI transformation, I see three ways we might preserve our identity as builders:\n\nResist - Some will choose to focus on domains where human creativity and deep technical expertise remain essential\nAdapt - Others will embrace AI orchestration, becoming conductors of a new kind of technical symphony\nBalance - And many, like myself, will seek a middle path - using AI for routine tasks while preserving the joy of direct problem-solving\n\nThen I had a realisation that changed my perspective: We don’t have to choose just one path.\nThe Identity Pendulum\nPerhaps the answer to our identity crisis lies in the Engineer/Manager pendulum. My own journey between these roles taught me something crucial about identity:\n\nManagement didn’t replace my engineering identity - it expanded it\nReturns to hands-on work weren’t steps backward - they were identity renewals\nThe pendulum swing itself became part of who I am - adaptable, growing, evolving\n\nAnd that’s when it hit me: This is exactly the model we need for the AI age.\nInstead of being forced into permanent “AI managers,” what if we could swing between:\n\nDeep technical work where we write and refine code directly\nStrategic orchestration where we guide AI systems\nCreative problem-solving that combines both approaches\n\nThis balanced approach resonates deeply with what I’m hearing from fellow engineers. My research shows a clear message: maintaining strong engineering fundamentals is more crucial than ever. We need deep technical knowledge to effectively review, verify, and adjust AI-generated code - because it’s often not quite right. When asked about their concerns with AI coding assistants, software engineers ranked code quality and security well above job security.\n\n      Top Concerns About AI Coding Assistants Among Software Engineers\n\nThat tells me something profound: we see ourselves as guardians of engineering excellence, ensuring that AI-generated solutions adhere to solid software engineering principles. We’re not looking to delegate our expertise to AI - we’re evolving to apply our craft in new ways.\nYour Move\nAs we navigate this transformation, a fundamental truth emerges: our identity crisis isn’t really about AI at all. The research into Human-AI teaming, the parallels with management transitions, the pendulum of roles - they all point to something deeper. Beyond choosing between builder or overseer lies the heart of who we are: creators.\nAnd now we come full circle: AI isn’t taking our jobs; it’s giving us a chance to reclaim those broader aspects of our role that we gave away to specialists. To return to a time when software engineering meant more than just writing code. When it meant understanding the whole problem space, from user needs to business impact, from system design to operational excellence.\nThe pendulum metaphor offers us wisdom here. Just as many of us have swung between engineering and management roles, we can embrace a similar fluidity with AI. Some periods we’ll dive deep into the code, experiencing that thrill of crafting elegant solutions. Other times we’ll step back to guide AI systems - not as overseers, but as master builders who understand every part of their craft. Like the Industrial Revolution’s workers who became experts at optimising the machines that transformed their craft, we can master these AI systems - making them instruments of our creativity, not replacements for it.\nIn the AI era, what matters most is preserving the essence of who we are: that pure joy of building things, of solving hard problems, of making something work exactly right. Our engineering excellence runs deeper than just verifying AI’s work - it flows from knowing systems so intimately that we can shape them, improve them, transform them.\nThe choice isn’t whether AI will change our industry - it already is. The real choice is how we evolve with it. Will we cling to an outdated sense of what it means to be an engineer? Or will we reclaim our craft, not as mere coders, but as master builders of AI-augmented systems?\nThe pendulum is swinging - will you hold your ground, or move with it?",
    "summary": {
      "en": "**Summary: The Software Engineering Identity Crisis**\n\nThe role of software engineers is changing due to the rise of AI coding assistants, shifting their identity from creators to overseers or managers. Many engineers initially entered the field to build and solve problems, finding joy in coding. However, as AI takes over more coding tasks, engineers are spending less time writing code and more time managing AI-generated outputs.\n\nRecent studies show that many engineers feel their core skills are becoming less relevant, with a significant portion of new code being produced by AI. This shift raises questions about the future identity of software engineers: Are they evolving into high-level architects, or are they losing the essence of what it means to be a builder?\n\nThe article discusses the need for engineers to adapt to these changes by embracing new skills, such as prompt engineering and systems thinking, while still preserving their technical expertise. It emphasizes that, despite the challenges, AI tools can enhance the creative process and enable faster problem-solving.\n\nUltimately, the future of software engineering may not be about resisting AI but finding a balance between hands-on coding and overseeing AI systems. The key lies in maintaining a deep understanding of engineering principles while adapting to the evolving landscape, ensuring that the core joy of creating remains intact.",
      "ko": "소프트웨어 엔지니어의 역할이 AI 코딩 도우미의 등장으로 변화하고 있습니다. 이로 인해 엔지니어들은 창조자에서 감독자나 관리자라는 새로운 정체성으로 이동하고 있습니다. 많은 엔지니어들은 문제를 해결하고 코딩하는 것에서 즐거움을 느끼며 이 분야에 진입했지만, AI가 더 많은 코딩 작업을 맡게 되면서 코드를 작성하는 시간은 줄어들고 AI가 생성한 결과물을 관리하는 데 더 많은 시간을 할애하고 있습니다.\n\n최근 연구에 따르면, 많은 엔지니어들이 자신의 핵심 기술이 점점 덜 중요해지고 있다고 느끼고 있으며, 새로운 코드의 상당 부분이 AI에 의해 생성되고 있습니다. 이러한 변화는 소프트웨어 엔지니어의 미래 정체성에 대한 질문을 제기합니다. 그들은 고급 설계자로 진화하고 있는 것인지, 아니면 건설자의 본질을 잃어가고 있는 것인지에 대한 고민이 필요합니다.\n\n이 글에서는 엔지니어들이 이러한 변화에 적응하기 위해 새로운 기술, 예를 들어 프롬프트 엔지니어링과 시스템 사고를 받아들이면서도 여전히 기술 전문성을 유지해야 한다고 강조합니다. 도전 과제가 있음에도 불구하고 AI 도구는 창의적인 과정과 문제 해결 속도를 향상시킬 수 있습니다.\n\n결국 소프트웨어 엔지니어링의 미래는 AI에 저항하는 것이 아니라, 직접 코딩하는 것과 AI 시스템을 감독하는 것 사이의 균형을 찾는 데 있을 것입니다. 핵심은 엔지니어링 원칙에 대한 깊은 이해를 유지하면서 변화하는 환경에 적응하는 것이며, 창조의 본질적인 즐거움을 지키는 것입니다.",
      "ja": "ソフトウェアエンジニアの役割が変わりつつあります。AIによるコーディングアシスタントの普及により、エンジニアのアイデンティティは創造者から監督者やマネージャーへと移行しています。多くのエンジニアは、問題を解決し、コードを書くことに喜びを感じてこの分野に入ってきました。しかし、AIがコーディングの多くの作業を引き受けるようになると、エンジニアはコードを書く時間が減り、AIが生成した成果物を管理する時間が増えています。\n\n最近の調査によると、多くのエンジニアが自分の基本的なスキルが重要でなくなっていると感じており、新しいコードのかなりの部分がAIによって生成されています。この変化は、ソフトウェアエンジニアの未来のアイデンティティについて疑問を投げかけます。エンジニアは高レベルのアーキテクトに進化しているのか、それとも創造者としての本質を失いつつあるのかということです。\n\nこの記事では、エンジニアがこれらの変化に適応する必要性について触れています。新しいスキル、例えばプロンプトエンジニアリングやシステム思考を取り入れつつ、技術的な専門知識を維持することが重要です。課題がある一方で、AIツールは創造的なプロセスを向上させ、問題解決を迅速にする手助けができると強調しています。\n\n最終的には、ソフトウェアエンジニアリングの未来はAIに抵抗することではなく、実際にコーディングを行うこととAIシステムを監督することのバランスを見つけることにあるかもしれません。エンジニアリングの原則を深く理解しながら進化する環境に適応し、創造する喜びを保つことが鍵となります。"
    }
  },
  {
    "id": "b5943bb62ae43b9d",
    "title": {
      "en": "Move on to ESM-Only",
      "ko": "ESM 전환!",
      "ja": "ESM専用へ進化"
    },
    "type": "story",
    "url": "https://antfu.me/posts/move-on-to-esm-only",
    "score": 112,
    "by": "bpierre",
    "time": 1742776525,
    "content": "The Toolings are Ready Modern Tools Top-Down & Bottom-Up Requiring ESM in Node.js The Troubles with Dual Formats Interop Issues Dependency Resolution Package Size When Should We Move to ESM-only? New Packages Browser-targeted Packages Standalone CLI Node.js Support Know Your Consumers How Far We Are? Moving Forward Three years ago, I wrote a post about shipping ESM & CJS in a single package, advocating for dual CJS/ESM formats to ease user migration and trying to make the best of both worlds. Back then, I didn’t fully agree with aggressively shipping ESM-only, as I considered the ecosystem wasn’t ready, especially since the push was mostly from low-level libraries. Over time, as tools and the ecosystem have evolved, my perspective has gradually shifted towards more and more on adopting ESM-only.As of 2025, a decade has passed since ESM was first introduced in 2015. Modern tools and libraries have increasingly adopted ESM as the primary module format. According to WOOORM’s script, the packages that ships ESM on npm in 2021 was 7.8%, and by the end of 2024, it had reached 25.8%. Although a significant portion of packages still use CJS, the trend clearly shows a good shift towards ESM.ESM adoption over time, generated by the npm-esm-vs-cjs script. Last updated at 2024-11-27Here in this post, I’d like to share my thoughts on the current state of the ecosystem and why I believe it’s time to move on to ESM-only.The Toolings are Ready #Modern Tools #With the rise of Vite as a popular modern frontend build tool, many meta-frameworks like Nuxt, SvelteKit, Astro, SolidStart, Remix, Storybook, Redwood, and many others are all built on top of Vite nowadays, that treating ESM as a first-class citizen.As a complement, we have also testing library Vitest, which was designed for ESM from the day one with powerful module mocking capability and efficient fine-grain caching support.CLI tools like tsx and jiti offer a seamless experience for running TypeScript and ESM code without requiring additional configuration. This simplifies the development process and reduces the overhead associated with setting up a project to use ESM.Other tools, for example, ESLint, in the recent v9.0, introduced a new flat config system that enables native ESM support with eslint.config.mjs, even in CJS projects.Top-Down & Bottom-Up #Back in 2021, when SINDRESORHUS first started migrating all his packages to ESM-only, for example, find-up and execa, it was a bold move. I consider this move as a bottom-up approach, as the packages that rather low-level and many their dependents are not ready for ESM yet. I was worried that this would force those dependents to stay on the old version of the packages, which might result in the ecosystem being fragmented. (As of today, I actually appreciate that move bringing us quite a lot of high-quality ESM packages, regardless that the process wasn’t super smooth).It’s way easier for an ESM or Dual formats package to depend on CJS packages, but not the other way around. In terms of smooth adoption, I believe the top-down approach is more effective in pushing the ecosystem forward. With the support of high-level frameworks and tools from top-down, it’s no longer a significant obstacle to use ESM-only packages. The remaining challenges in terms of ESM adoption primarily lie with package authors needing to migrate and ship their code in ESM format.Requiring ESM in Node.js #The capability to require() ESM modules in Node.js, initiated by JOYEECHEUNG, marks an incredible milestone. This feature allows packages to be published as ESM-only while still being consumable by CJS codebases with minimal modifications. It helps avoid the async infection (also known as Red Functions) introduced by dynamic import() ESM, which can be pretty hard, if not impossible in some cases, to migrate and adapt.This feature was recently unflagged and backported to Node.js v22 (and soon v20), which means it should be available to many developers already. Consider the top-down or bottom-up metaphor, this feature actually makes it possible to start ESM migration also from middle-out, as it allows import chains like ESM → CJS → ESM → CJS to work seamlessly.To solve the interop issue between CJS and ESM in this case, Node.js also introduced a new export { Foo as 'module.exports' } syntax in ESM to export CJS-compatible exports (by this PR). This allows package authors to publish ESM-only packages while still supporting CJS consumers, without even introducing breaking changes (expcet for changing the required Node.js version).For more details on the progress and discussions around this feature, keep track on this issue.The Troubles with Dual Formats #While dual CJS/ESM packages have been a quite helpful transition mechanism, they come with their own set of challenges. Maintaining two separate formats can be cumbersome and error-prone, especially when dealing with complex codebases. Here are some of the issues that arise when maintaining dual formats:Interop Issues #Fundamentally, CJS and ESM are different module systems with distinct design philosophies. Although Node.js has made it possible to import CJS modules in ESM, dynamically import ESM in CJS, and even require() ESM modules, there are still many tricky cases that can lead to interop issues.One key difference is that CJS typically uses a single module.exports object, while ESM supports both default and named exports. When authoring code in ESM and transpiling to CJS, handling exports can be particularly challenging, especially when the exported value is a non-object, such as a function or a class. Additionally, to make the types correct, we also need to introduce further complications with .d.mts and .d.cts declaration files. And so on…As I am trying to explain this problem deeper, I found that I actually wish you didn’t even need to be bothered with this problem at all. It’s frankly too complicated and frustrating. If you are just a user of packages, let alone the package authors to worry about that. This is one of the reasons I advocate for the entire ecosystem to transition to ESM, to leave these problems behind and spare everyone from this unnecessary hassle.Dependency Resolution #When a package has both CJS and ESM formats, the resolution of dependencies can become convoluted. For example, if a package depends on another package that only ships ESM, the consumer must ensure that the ESM version is used. This can lead to version conflicts and dependency resolution issues, especially when dealing with transitive dependencies.Also for packages that are designed to used with singleton pattern, this might introduce multiple copies of the same package and cause unexpected behaviors.Package Size #Shipping dual formats essentially doubles the package size, as both CJS and ESM bundles need to be included. While a few extra kilobytes might not seem significant for a single package, the overhead can quickly add up in projects with hundreds of dependencies, leading to the infamous node_modules bloat. Therefore, package authors should keep an eye on their package size. Moving to ESM-only is a way to optimize it, especially if the package doesn’t have strong requirements on CJS.When Should We Move to ESM-only? #This post does not intend to diminish the value of dual-format publishing. Instead, I want to encourage evaluating the current state of the ecosystem and the potential benefits of transitioning to ESM-only.There are several factors to consider when deciding whether to move to ESM-only:New Packages #I strongly recommend that all new packages be released as ESM-only, as there are no legacy dependencies to consider. New adopters are likely already using a modern, ESM-ready stack, there being ESM-only should not affect the adoption. Additionally, maintaining a single module system simplifies development, reduces maintenance overhead, and ensures that your package benefits from future ecosystem advancements.Browser-targeted Packages #If a package is primarily targeted for the browser, it makes total sense to ship ESM-only. In most cases, browser packages go through a bundler, where ESM provides significant advantages in static analysis and tree-shaking. This leads to smaller and more optimized bundles, which would also improve loading performance and reduce bandwidth consumption for end users.Standalone CLI #For a standalone CLI tool, it’s no difference to end users whether it’s ESM or CJS. However, using ESM would enable your dependencies to also be ESM, facilitating the ecosystem’s transition to ESM from a top-down approach.Node.js Support #If a package is targeting the evergreen Node.js versions, it’s a good time to consider ESM-only, especially with the recent require(ESM) support.Know Your Consumers #If a package already has certain users, it’s essential to understand the dependents’ status and requirements. For example, for an ESLint plugin/utils that requires ESLint v9, while ESLint v9’s new config system supports ESM natively even in CJS projects, there is no blocker for it to be ESM-only.Definitely, there are different factors to consider for different projects. But in general, I believe the ecosystem is ready for more packages to move to ESM-only, and it’s a good time to evaluate the benefits and potential challenges of transitioning.How Far We Are? #The transition to ESM is a gradual process that requires collaboration and effort from the entire ecosystem. Which I believe we are on a good track moving forward.To improve the transparency and visibility of the ESM adoption, I recently built a visualized tool called Node Modules Inspector for analyzing your packages’s dependencies. It provides insights into the ESM adoption status of your dependencies and helps identify potential issues when migrating to ESM.Here are some screenshots of the tool to give you a quick impression:Node Modules Inspector - OverviewNode Modules Inspector - Dependency GraphNode Modules Inspector - Reports like ESM Adoptions and Duplicated PackagesThis tool is still in its early stages, but I hope it will be a valuable resource for package authors and maintainers to track the ESM adoption progress of their dependencies and make informed decisions about transitioning to ESM-only.To learn more about how to use it and inspect your projects, check the repository node-modules-inspector.Moving Forward #I am planning to gradually transition the packages I maintain to ESM-only and take a closer look at the dependencies we rely on. We also have plenty of exciting ideas for the Node Modules Inspector, aiming to provide more useful insights and help find the best path forward.I look forward to a more portable, resilient, and optimized JavaScript/TypeScript ecosystem.I hope this post has shed some light on the benefits of moving to ESM-only and the current state of the ecosystem. If you have any thoughts or questions, feel free to reach out using the links below. Thank you for reading!",
    "summary": {
      "en": "The text discusses the transition from dual CommonJS (CJS) and ECMAScript Modules (ESM) formats to a focus on ESM-only packages in Node.js. Here are the key points simplified:\n\n1. **Current State of ESM Adoption**: The use of ESM in packages is increasing. In 2021, only 7.8% of npm packages used ESM, but by the end of 2024, this rose to 25.8%.\n\n2. **Modern Tools**: New tools and frameworks, like Vite, support ESM as a primary format, making development easier. Tools like ESLint and testing libraries are also adapting to ESM.\n\n3. **Challenges of Dual Formats**: Maintaining both CJS and ESM can lead to issues such as interoperability problems, complicated dependency resolution, and increased package sizes.\n\n4. **When to Transition to ESM-only**:\n   - **New Packages**: Should be ESM-only to simplify development and maintenance.\n   - **Browser-targeted Packages**: ESM improves performance and optimizes bundle sizes.\n   - **Standalone CLI Tools**: Transitioning to ESM can facilitate ecosystem growth.\n   - **Awareness of Consumers**: Understand the needs of current users before making the switch.\n\n5. **Moving Forward**: The transition to ESM is ongoing and requires collaboration. Tools like the Node Modules Inspector help analyze dependencies and ESM adoption, aiding developers in the transition process.\n\nThe author encourages evaluating the benefits of moving to ESM-only and believes the ecosystem is ready for more packages to make this shift.",
      "ko": "Node.js에서 이중 CommonJS(CJS)와 ECMAScript 모듈(ESM) 형식에서 ESM 전용 패키지로의 전환에 대해 논의하고 있습니다. 주요 내용을 간단히 정리하면 다음과 같습니다.\n\n현재 ESM 채택 상태는 점점 증가하고 있습니다. 2021년에는 npm 패키지의 7.8%만 ESM을 사용했지만, 2024년 말까지 이 비율이 25.8%로 증가할 것으로 예상됩니다. \n\nVite와 같은 새로운 도구와 프레임워크는 ESM을 기본 형식으로 지원하여 개발을 더 쉽게 만들어줍니다. ESLint와 테스트 라이브러리와 같은 도구들도 ESM에 맞춰 변화하고 있습니다. \n\nCJS와 ESM을 동시에 유지하는 것은 상호 운용성 문제, 복잡한 의존성 해결, 패키지 크기 증가와 같은 문제를 초래할 수 있습니다. \n\nESM 전용으로 전환할 시점에 대해 살펴보면, 새로운 패키지는 ESM 전용으로 만들어 개발과 유지보수를 간소화해야 합니다. 브라우저를 대상으로 하는 패키지는 ESM을 통해 성능을 개선하고 번들 크기를 최적화할 수 있습니다. 독립형 CLI 도구도 ESM으로 전환하면 생태계의 성장을 촉진할 수 있습니다. 전환하기 전에 현재 사용자들의 요구를 이해하는 것도 중요합니다. \n\nESM으로의 전환은 계속 진행 중이며 협력이 필요합니다. Node Modules Inspector와 같은 도구는 의존성을 분석하고 ESM 채택을 도와 개발자들이 전환 과정을 원활하게 할 수 있도록 지원합니다. 저자는 ESM 전용으로 전환하는 것의 이점을 평가할 것을 권장하며, 생태계가 더 많은 패키지가 이 전환을 할 준비가 되어 있다고 믿고 있습니다.",
      "ja": "Node.jsでは、従来のCommonJS（CJS）とECMAScript Modules（ESM）の両方の形式から、ESM専用パッケージへの移行が進んでいます。現在、ESMの採用状況は改善しています。2021年にはnpmパッケージの7.8%がESMを使用していましたが、2024年末にはその割合が25.8%に増加する見込みです。\n\n新しいツールやフレームワーク、例えばViteは、ESMを主要な形式としてサポートしており、開発を容易にしています。また、ESLintやテストライブラリなどもESMに対応しつつあります。\n\nCJSとESMの両方を維持することには、相互運用性の問題や依存関係の解決が複雑になること、パッケージサイズの増加といった課題があります。新しいパッケージはESM専用にすることで、開発やメンテナンスが簡素化されます。ブラウザ向けのパッケージでは、ESMがパフォーマンスを向上させ、バンドルサイズを最適化します。また、スタンドアロンのCLIツールもESMに移行することでエコシステムの成長を促進できます。移行を行う前に、現在のユーザーのニーズを理解することも重要です。\n\nESMへの移行は進行中であり、協力が必要です。Node Modules Inspectorのようなツールは、依存関係やESMの採用状況を分析し、開発者の移行を支援します。著者は、ESM専用への移行の利点を評価することを促し、エコシステムがこの変化に対応する準備が整っていると考えています。"
    }
  },
  {
    "id": "bc798ab915ad080e",
    "title": {
      "en": "The case of the critical section that let multiple threads enter a block of code",
      "ko": "다중 스레드의 비밀",
      "ja": "スレッドの罠"
    },
    "type": "story",
    "url": "https://devblogs.microsoft.com/oldnewthing/20250321-00/?p=110984",
    "score": 135,
    "by": "luu",
    "time": 1742717665,
    "content": "March 24, 2025\n      On how different Windows ABIs choose how to pass 32-bit values in 64-bit registers\n\n        Raymond Chen",
    "summary": {
      "en": "On March 24, 2025, Raymond Chen discussed how various Windows Application Binary Interfaces (ABIs) handle the passing of 32-bit values using 64-bit registers.",
      "ko": "2025년 3월 24일, 레이먼드 첸은 다양한 윈도우 응용 프로그램 이진 인터페이스(ABI)가 64비트 레지스터를 사용하여 32비트 값을 전달하는 방식을 논의했습니다.",
      "ja": "2025年3月24日、レイモンド・チェンは、さまざまなWindowsアプリケーションバイナリインターフェース（ABI）が、64ビットレジスタを使用して32ビットの値をどのように処理するかについて話しました。"
    }
  },
  {
    "id": "8d6f400bc4fd96b8",
    "title": {
      "en": "To Win Trust and Admiration, Fix Your Microphone",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.scientificamerican.com/article/to-win-trust-and-admiration-fix-your-microphone/",
    "score": 10,
    "by": "jimminyx",
    "time": 1742881602,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "59c3d3c0405f7a06",
    "title": {
      "en": "The belay test and the modern American climbing gym",
      "ko": "벨레이 테스트와 현대 암벽장",
      "ja": "ビレイテストと現代クライミングジム"
    },
    "type": "story",
    "url": "https://www.climbing.com/people/peter-mayfield-belay-test-modern-american-climbing-gym-history/",
    "score": 151,
    "by": "vasco",
    "time": 1742408381,
    "content": "Meet the Guy Who Invented the Belay Test—and the Modern American Climbing Gym\n\n          The story of California’s City Rock\n\n      Updated\n\n        Mar 18, 2025\n\n          Bruce Hildenbrand\n\n      Comment\n\n    Share\n\n            Share to Outside Feed\n            Create a new post with the article attached\n\n          Copy link\n\n          Email\n\n          Share on X\n\n          Share on Facebook\n\n          Share on Reddit\n\n              Photo: Peter Mayfield Collection\n\n                Peter Mayfield bouldering at City Rock Photo: Peter Mayfield Collection\n\n      Heading out the door? Read this article on the new Outside+ app available now on iOS devices for members!\n      Download the app.\n\n      Forty years ago, there was not a single purpose-built climbing gym in the U.S. In 1987, Seattle’s Vertical Club opened and a year later, the Portland Rock Gym launched. But the concept of a profitable indoor climbing gym was far from proven. This is the story of the person whose vision and determination would revolutionize climbing gyms as we know them today.\nPeter Mayfield was a climbing prodigy. At 16 years old, in 1978, he was repeating the hardest free climbs in Yosemite. He went on to establish some of the hardest aid routes on the Valley’s big walls. By the age of 19, he was guiding for the prestigious Yosemite Mountaineering School, and became chief guide a year later. When sport climbing competitions began in the late 1980s, Mayfield participated in the first ever Snowbird climbing competition as well as Jeff Lowe’s national competitions.\nIn 1988, Mayfield formed a guiding cooperative with legends Kim Schmitz and Jim Bridwell, but by his mid-twenties, he was looking for a job that would keep him closer to home. So when Bridwell returned from a trip to Europe with posters and magazines of climbing walls going up in France, Mayfield knew he’d found his next venture.\n\nPeter Mayfield at 19 years old on his first ascent in 1981 of Zenyatta Mondatta on El Cap with Jim Bridwell. Twenty years after his FA, Mayfield repeated the route with his 19-year-old son Braden. (Photo: Peter Mayfield Collection)\nAt the time, the walls in France were closer to artwork in public spaces than purpose-built climbing structures. It was around that time that the Vertical Club—a space for experienced climbers to train—went up, but Mayfield had something completely different in mind. As chief guide of the Yosemite Mountaineering School, he didn’t just want to create a place for himself and his buddies to get stronger. “I had this serious career of turning people onto this sport,” reflects Mayfield. “I was really thinking [about] kids, corporate programs … The word gym wasn’t even in my mind. It was more like an indoor climbing institute.”\nWith the popularity of indoor climbing gyms these days, it’s hard to imagine what it was like in the late ’80s, when climbing itself was still very much a niche activity. Mayfield knew that to keep the gym lights on, he’d need to attract more than just his hardcore climbing buddies.\nThen there was the problem with California’s legendary great weather. Mayfield was skeptical that climbers would want to climb indoors on days when the sun was shining, because unlike Seattle and Portland, you could climb outdoors virtually year-round. So his marketing plan was also designed to get non-climbers to come to his gym. He wanted to build a place where “10,000 eight-year-old girls could try something they never dreamed of.”\n“That was my pitch,” Mayfield says.\nThe economics of the first commercial climbing gym\nMayfield found his first investor while guiding an ascent of Alaska’s Moose’s Tooth. Tent-bound by storms for days, he discussed his climbing gym dreams with his clients. At the end of the expedition, as they were splitting up at the Oakland Airport, one of the clients asked him what he needed to start his business. He replied that he needed money for a phone line. The client pulled out his wallet and gave him $200. “I literally got handed two C-notes to start my business,” he remembers. He used that money to put in a Pacific Bell phone line in his mother’s basement in Berkeley and got to work.\n“I got no positive reaction from the [climbing] industry at all,” remembers Mayfield. “I was pretty connected because I climbed with Bridwell. I met with the CEO of The North Face. I met with the CEO of Marmot. I met with the president of REI. They all said ‘it will never work. It makes sense in Seattle where it rains. Who would ever climb indoors in California?”\nMayfield even sent his business plan to Yvon Chouinard. “He was very polite and gave me some good advice,” Mayfield recalls. “He did not invest. None of them invested. But he kind of got what I was talking about.” (Chouinard did tell Mayfield that he should try to sell milkshakes: “If you can nail it with the youth, you will kick ass.”)\nSo Mayfield widened his net, contacting ”everybody in Silicon Valley who had ever climbed” and raised half a million dollars with 48 limited partners, essentially a large group of $5,000 investors. He stalled for a while, dealing with the mountain of paperwork associated with creating such a high volume of limited partnerships. But in the midst of his fundraising, he produced the Great Outdoor Adventure Fair in San Francisco. To promote his concept to the general public, he set up a portable climbing wall at the fair. John Gage, high up in management at one of Silicon Valley’s most successful startups, Sun Microsystems, was impressed and tossed in $40,000, re-energizing Mayfield.\nBut at that time, building a climbing gym was a huge, complex undertaking. Mayfield relied on a suite of competent advisers to help him through. He eventually hooked up with Armand Tanny of Gold’s Gym, who explained the new, revolutionary process of selling and managing memberships via an automatic fund transfer. They also impressed upon him the importance of aesthetics. The other two climbing gyms had spent no more than $18,000 building their gyms. Mayfield spent $40,000 on just the bathrooms.\nChristian Griffith (designer of City Rock walls) and Peter Mayfield (Photo: Beth Wald)\nBuilding a gym for climbers—and non-climbers\n“So then I was like, ‘Okay, this is a gym. This is not an indoor sculpture garden that we climb on that’s a climbing institute. I am going to build a business model with monthly memberships,'” adds Mayfield. He decided to call it City Rock Gym. The gym was located in Emeryville, between Oakland and Berkeley.\n“I made the first gym you could take your kid to and trust that they would be well taken care of,” he explains.\nA number of climbing wall companies, such as Entre-Prises, were pressing Mayfield to help him build his gym. Instead, he turned to Christian Griffith, who he had climbed with on the competition circuit, and who felt that they could design an ”incredible space” together. “Christian posted up in south Berkeley in my mom’s yard and carved foam for two weeks and he completely created that gym,” recalls Mayfield. “He took foam core and glued graph paper to it and he did a beautiful model. He was inspired by the gritstone cliffs in England where you are always like ‘what’s around the corner?’ So City Rock had these really protuberant buttresses.”\nOverall, the City Rock Gym design was both intentional and approachable. “When people walked in the door I wanted them to see the friendly slab. I wanted them to think they can do this,” he explains. “I want them to see the six year olds climbing. I really felt that I had to prove the concept. It’s not me and my bros throwing down at the boulders.”\nInstruction served as another key pioneering element of Mayfield’s gym concept. He began teaching the climbing safety techniques he’d learned from the Sierra Club, an organization which had been studying and teaching belay practices since the 1930s. “We really did invent the belay test,” he recalls, citing the need for procedure in such a high-risk business. “I thought somebody would die at our business. I ran the statistics in my head. So many people tying knots. So many people belaying. I really took it seriously.”\nGiven the magnitude of risk, Mayfield says he really “leaned into” establishing a belay test and enforcing safety. “I had some really good, famous, climbers come in and fail the belay test,” he recalls. “These good climbers were just a little too casual with their hands. You don’t vaguely slide your hands around. They had to tighten their act up a little bit.”\nOn City Rock Gym’s grand opening weekend in 1990, 900 people came in to climb. Mayfield watched brand-new climbing ropes wear out in front of his eyes. “We installed these half-pipe sealed bearing things so the ropes wouldn’t wear out, but on Sunday morning: ‘Oh my God! I see cores in that rope. Quick, replace that,’” remembers Mayfield. “I took the climbing school really seriously. I hired the best and they were well trained … It was really high level with really good technique coaching.”\nCity Rock quickly gained a loyal following. With his emphasis on introducing new participants to the sport, Mayfield pioneered programs like a kid’s belay clinic and the first climbing-themed birthday parties. He created an event called Women on Rock and flew in Lynn Hill to teach it. The vertical dance team Bandaloop got its start at City Rock in 1991.\nThe vertical dance company Bandaloop was born at City Rock (Photo: Peter Mayfield Collection)\nBut could a commercial climbing gym keep elite climbers stoked?\nWhile he was intent on attracting non-climbers to the gym, Mayfield didn’t turn his back on his climbing buddies. He created routes for their level of difficulty and also gave some of them jobs. Many experienced climbers including Scott Cosgrove, Steve Schneider, Bird Lew, and Don Welch worked at City Rock.\n“We set some hard routes,” remembers Schneider, a 5.14 climber and one of the routesetters at City Rock. “We did some competitions there which were very successful and those routes would be left up for others to climb.”\nAs the initial buzz of City Rock’s grand opening wore off, two vital questions remained: Were established climbers willing to climb indoors on sunny days? And were the routes difficult enough to keep experienced climbers satisfied? “I was initially a little skeptical but found I really enjoyed it,” recalls Greg Murphy who, together with Chan Harrell, held the fastest known time on the Northwest Face of Half Dome. “There was nothing super steep as I recall, so the harder routes were kind of finger intensive. But it always seemed like there was a good mix of routes and some harder projects to work on.”\nAnne Smith, an excellent climber and the mother of Connor Herson, agrees: “CR [City Rock] was very much geared towards experienced climbers. [It] hosted National Championship events and set at least as many harder climbers as easier ones.”\nAs a competitive sport climber himself, Mayfield eagerly hosted national championship competitions five years in a row. Hans Florine had set the speed record on the Nose with Steve Schnieder the week before he won the 1990 nationals in both difficulty and speed at City Rock. For the next three years, Florine partnered with Mayfield and City Rock to stage the national championships there. The gym played host to top climbers such as Robyn Erbesfield, Bobbi Bensman, Dale Goddard, and Jim Karn.\nHank Levine competing in US Nationals at City Rock (Photo: Peter Mayfield Collection)\nThe inclusive climbing gym concept takes off\nThree years after City Rock’s opening, California got its second climbing gym via Tom Davis’s now-famous Pacific Edge, in Santa Cruz. (Chris Sharma and Natalia Grossman both learned to climb there.) Davis was inspired by what Mayfield had created: “It was visionary to open a gym in California at the time—but that is really hard to imagine now. From a dream to a nationwide industry.”\nBut Davis recalls that perhaps the most important thing City Rock did for the climbing gym industry was establish safety standards to manage risk, including legitimate belay tests. “It could easily have been a dirtbag climbers’ training hangout,” Davis explains. “But they pulled off something that demonstrated the true value of climbing: risk taking with the correct skillset, and on a level that a discerning public could understand.”\nIn 1992, Mayfield and Casey Newman, who ran the Colorado Rock Gym, along with several others, founded the Climbing Gym Association, which has since become the Climbing Wall Association. The purpose of this organization was to regulate the burgeoning climbing gym industry, hopefully keeping the government from stepping in with its own set of regulations. They built a gym accreditation program and traveled around the country accrediting the dozens of climbing gyms that existed by that time in the U.S. But that’s a story for another time.\nWhat ever happened to the original City Rock—and Mayfield?\nSo how did the original design of City Rock hold up? Jim Collins, a notable climber, bestselling author, and business guru who taught MBA students at Stanford University, studied City Rock as a class project. The academic analysis found that City Rock was “going to get blown out of the water by the next wave of climbing gyms,” in Mayfield’s words. Collins and his students had two major reasons to back up their conclusion. The first was that City Rock’s walls weren’t steep enough. And the second reason was that they hypothesized other entrepreneurs would come along and operate gyms more efficiently at a lower cost.\nMayfield explains the limited steepness of his gym’s original walls: “[City Rock] was designed and built before America had super steep climbing. It was before there was a Rifle. It was before there was a Jailhouse Rock. Two or three years later, the next wave of gyms did have arches and bigger, steeper, upside-down climbing structures because the cliffs that people were climbing on were like that. The hard climbing in 1989 was like Smith Rock. Dead vertical or maybe a bit past vertical, like 100 degrees.”\nIn 1997, Mayfield sold City Rock to Mark Melvin, who founded Mission Cliffs, now Touchstone Climbing. He then moved back to Yosemite, where he started an eco-tourism business to educate people about nature and conservation in the Valley.\nWhile the decision to sell the gym he poured his heart and soul into for the better part of 10 years was difficult, he made some money and his investors did well trading up into the Touchstone world. “I couldn’t care less about making money. I am just not wired that way to manipulate the world to make money.” He says that in retrospect, he was “faking it” as a business person and finds himself better adapted to social entrepreneurship.\nAfter the sale, City Rock fell victim to the skyrocketing rents associated with the Internet boom in the Bay Area. When rent tripled in the highly desirable Emeryville area, it was time to look for another location. The new owners of City Rock honored its memberships and opened Berkeley Ironworks a few miles away in 2000. It was three times the size of City Rock and also featured a large fitness area, a yoga studio, and an indoor cycling room. It represented the next generation of climbing gyms.\nNowadays, Mayfield lives in Truckee, CA, where he founded the nonprofit Gateway Mountain Center, which focuses on nature-based therapy for kids. “Even to this day, I stay interested in the effect [of climbing gyms] on youth, especially high-need youth,” says Mayfield. He also works with kids from local drug treatment centers and juvenile detention camps. “I am watching these kids just light up,” he says. “They face incredible challenges in life, but they get to climb. This is transformative for high-need youth.”\nThirty-five years have passed since Mayfield first started City Rock. While his impacts on the climbing gym industry are clear, he also continues to abide by his personal commitment to benefit youth through climbing. Mayfield still climbs, but these days, prefers Berkeley’s iconic Indian Rock to a climbing gym.\nReflecting on his pioneering foray into climbing gyms, he says: “I had kind of a classic business school case study of being the pioneer who spends all the money on R&D to prove the concept, then gets blown out of the water by the next wave, the well-funded MBAs who come after.”\n\n    Similar Reads\n\n          A Climbing Graphic Novel? Competitive Climbing Becomes Art in This New Book.\n\n          The Horrors of the Gym Belay Test\n\n          Want More From Your Climbing Gym? These 7 US Facilities Are Getting Creative.\n\n          Decolonizing Climbing in Malawi and Beyond\n\n      Tags\n\n        climbing gyms\n\n        Peter Mayfield\n\n        Recycle",
    "summary": {
      "en": "**Summary:**\n\nPeter Mayfield, a talented climber, significantly changed the climbing gym landscape in the U.S. by founding City Rock in 1990. Before then, there were no dedicated climbing gyms, and the concept seemed unlikely to succeed, especially in sunny California. Mayfield, driven by a vision to make climbing accessible to everyone, including kids and non-climbers, created a welcoming environment with a focus on safety. He introduced a belay test to ensure climbers were properly trained.\n\nDespite initial skepticism from the climbing industry, Mayfield raised funds and built City Rock in Emeryville, which quickly gained popularity, attracting both novice and experienced climbers. The gym also hosted national climbing competitions and was known for innovative programs like kids' belay clinics and climbing-themed birthday parties.\n\nAfter selling City Rock in 1997, Mayfield moved on to eco-tourism and eventually founded a nonprofit focused on helping at-risk youth through nature and climbing. His pioneering efforts laid the groundwork for the modern climbing gym industry, which has since expanded rapidly across the country. Today, Mayfield continues to influence youth through climbing, reflecting on his journey with a sense of pride and commitment to social entrepreneurship.",
      "ko": "피터 메이필드는 1990년 시티 록을 설립하며 미국의 클라이밍 체육관 풍경을 크게 변화시킨 재능 있는 클라이머입니다. 그 이전에는 전용 클라이밍 체육관이 없었고, 특히 햇살이 가득한 캘리포니아에서 이 개념이 성공할 것이라고는 생각하기 어려웠습니다. 메이필드는 아이들과 비클라이머를 포함해 모든 사람들이 클라이밍을 즐길 수 있도록 하겠다는 비전을 가지고 안전에 중점을 둔 환영하는 환경을 만들었습니다. 그는 클라이머들이 제대로 훈련받았는지 확인하기 위해 벨레이 테스트를 도입했습니다.\n\n클라이밍 업계의 초기 회의론에도 불구하고, 메이필드는 자금을 모아 에머리빌에 시티 록을 세웠고, 이곳은 빠르게 인기를 얻어 초보자와 숙련된 클라이머 모두를 끌어모았습니다. 이 체육관은 전국 클라이밍 대회를 개최했으며, 아이들을 위한 벨레이 클리닉과 클라이밍 테마의 생일 파티와 같은 혁신적인 프로그램으로도 유명했습니다.\n\n1997년 시티 록을 매각한 후, 메이필드는 에코 투어리즘으로 방향을 바꾸었고, 결국 자연과 클라이밍을 통해 위험에 처한 청소년들을 돕는 비영리 단체를 설립했습니다. 그의 선구적인 노력은 현대 클라이밍 체육관 산업의 기초를 마련했으며, 이후 이 산업은 전국적으로 빠르게 확장되었습니다. 오늘날 메이필드는 클라이밍을 통해 청소년들에게 계속해서 영향을 미치고 있으며, 자신의 여정을 자랑스럽게 되돌아보며 사회적 기업가정신에 대한 헌신을 보여주고 있습니다.",
      "ja": "ピーター・メイフィールドは、1990年にシティロックを設立し、アメリカのクライミングジムの風景を大きく変えました。それ以前は、専用のクライミングジムは存在せず、特に日差しの強いカリフォルニアでは成功するとは考えられていませんでした。メイフィールドは、子供やクライミングをしない人々も含め、誰もがクライミングを楽しめるようにしたいというビジョンを持ち、安全性を重視した温かい環境を作り出しました。彼は、クライマーが適切に訓練されていることを確認するためのビレイテストを導入しました。\n\nクライミング業界からの初期の懐疑的な反応にもかかわらず、メイフィールドは資金を集め、エメリービルにシティロックを建設しました。このジムはすぐに人気を博し、初心者から経験者まで多くのクライマーを引きつけました。また、全国的なクライミング大会を開催し、子供向けのビレイクリニックやクライミングをテーマにした誕生日パーティーなどの革新的なプログラムでも知られていました。\n\n1997年にシティロックを売却した後、メイフィールドはエコツーリズムに移行し、最終的には自然やクライミングを通じて危険にさらされている若者を支援する非営利団体を設立しました。彼の先駆的な取り組みは、現代のクライミングジム業界の基盤を築き、その後、全国に急速に広がりました。現在、メイフィールドはクライミングを通じて若者に影響を与え続けており、自身の歩みを誇りと社会的起業家精神へのコミットメントを持って振り返っています。"
    }
  },
  {
    "id": "dc82356fa3518944",
    "title": {
      "en": "Most AI value will come from broad automation, not from R & D",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://epoch.ai/gradient-updates/most-ai-value-will-come-from-broad-automation-not-from-r-d",
    "score": 202,
    "by": "ydnyshhh",
    "time": 1742668500,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "b78601fed6f74ae5",
    "title": {
      "en": "A USB Interface to the \"Mother of All Demos\" Keyset",
      "ko": "모든 데모의 USB 키셋",
      "ja": "「全てのデモの母」USB接続"
    },
    "type": "story",
    "url": "https://www.righto.com/2025/03/mother-of-all-demos-usb-keyset-interface.html",
    "score": 317,
    "by": "zdw",
    "time": 1742743886,
    "content": "Ken Shirriff's blog\n\nComputer history, restoring vintage computers, IC reverse engineering, and whatever\n\nA USB interface to the \"Mother of All Demos\" keyset\n\nIn the early 1960s, Douglas Engelbart started investigating how computers could augment human intelligence:\n\"If, in your office, you as an intellectual worker\nwere supplied with a computer display backed up by a computer that was alive for you all day and was instantly responsive to every\naction you had, how much value could you derive from that?\"\nEngelbart developed many features of modern computing that we now take for granted: the mouse,1 hypertext, shared documents, windows,\nand a graphical user interface.\nAt the 1968 Joint Computer Conference, Engelbart demonstrated these innovations in a groundbreaking presentation, now known as\n\"The Mother of All Demos.\"\n\nThe keyset with my prototype USB interface.\nEngelbart's demo also featured an input device known as the keyset, but unlike his other innovations, the keyset failed to catch on.\nThe 5-finger keyset lets you type without moving your hand, entering characters by pressing multiple keys simultaneously as a chord.\nChristina Englebart, his daughter, loaned one of Engelbart's keysets to me.\nI constructed an interface to connect the keyset to USB, so that it can be used with a modern computer.\nThe video below shows me typing with the keyset, using the mouse buttons to select upper case and special characters.2\n\nI wrote this blog post to describe my USB keyset interface.\nAlong the way, however, I got sidetracked by the history of The Mother of All Demos and how it obtained that name.\nIt turns out that Engelbart's demo isn't the first demo to be called \"The Mother of All Demos\".\nEngelbart and The Mother of All Demos\n\nEngelbart's work has its roots in\nVannevar Bush's 1945 visionary essay, \"As We May Think.\"\nBush envisioned thinking machines, along with the \"memex\", a compact machine holding a library of collective knowledge with hypertext-style links: \"The Encyclopedia Britannica could be reduced to the volume of a matchbox.\"\nThe memex could search out information based on associative search, building up a hypertext-like trail of connections.\nIn the early 1960s, Engelbart was inspired by Bush's essay and set out\nto develop means to augment human intellect: \"increasing the capability of a man to approach a complex problem situation, to gain comprehension to suit his particular needs, and to derive solutions to problems.\"3\nEngelbart founded the Augmentation Research Center at the Stanford Research Institute (now SRI), where\nhe and his team created a system called NLS (oN-Line System).\nEngelbart editing a hierarchical shopping list.\nIn 1968, Engelbart demonstrated NLS to a crowd of two thousand people\nat the Fall Joint Computer Conference.\nEngelbart gave the demo from the stage, wearing a crisp shirt and tie and a headset microphone.\nEngelbart created hierarchical documents, such as the shopping list above, and moved around them with hyperlinks.\nHe demonstrated how text could be created, moved, and edited with the keyset and mouse.\nOther documents included graphics, crude line drawing by today's standards but cutting-edge for the time.\nThe computer's output was projected onto a giant screen, along with video of Engelbart.\nEngelbart using the keyset to edit text. Note that the display doesn't support lowercase text; instead, uppercase is indicated by a line above the character. Adapted from The Mother of All Demos.\nEngelbart sat at a specially-designed Herman Miller desk6 that held the\nkeyset, keyboard, and mouse, shown above.\nWhile Engelbart was on stage in San Francisco,\nthe SDS 9404 computer that ran the NLS software was 30 miles to the south in Menlo Park.5\nTo the modern eye, the demo resembles a PowerPoint presentation over Zoom, as\nEngelbart collaborated with\nJeff Rulifson and Bill Paxton, miles away in Menlo Park.\n(Just like a modern Zoom call, the remote connection started with \"We're not hearing you. How about now?\")\nJeff Rulifson browsed the NLS code, jumping between code files with hyperlinks and expanding subroutines by clicking on them.\nNLS was written in custom high-level languages, which they developed\nwith a \"compiler compiler\" called TREE-META.\nThe NLS system held interactive documentation as well as tracking bugs and changes.\nBill Paxton interactively drew a diagram and then demonstrated how NLS could be used as a database, retrieving information by searching on keywords.\n(Although Engelbart was stressed by the live demo, Paxton told me that he was \"too young and inexperienced to be concerned.\")\nBill Paxton, in Menlo Park, communicating with the conference in San Francisco.\nBill English, an electrical engineer, not only built the first mouse for Engelbart but was also the hardware mastermind behind the demo.\nIn San Francisco, the screen images were projected on a 20-foot screen by a Volkswagen-sized\nEidophor projector, bouncing light off a modulated oil film.\nNumerous cameras, video switchers and mixers created the video image.\nTwo leased microwave links and half a dozen antennas connected SRI in Menlo Park to the demo in San Francisco.\nHigh-speed modems send the mouse, keyset, and keyboard signals from the demo back to SRI.\nBill English spent months assembling the hardware and network for the demo and then managed the demo behind the scenes, assisted by a team of about 17 people.\nAnother participant was the famed counterculturist Stewart Brand, known for the Whole Earth Catalog\nand the WELL, one of the oldest online virtual communities.\nBrand advised Engelbart on the presentation, as well as running a camera. He'd often point the camera at a monitor to generate swirling psychedelic\nfeedback patterns, reminiscent of the LSD that he and Engelbart had experimented with.\nThe demo received press attention such as\na San Francisco Chronicle article titled \"Fantastic World of Tomorrow's Computer\".\nIt stated, \"The most fantastic glimpse into the computer future was taking place in a windowless room on the third floor of the Civic Auditorium\"\nwhere Engelbart \"made a computer in Menlo Park do secretarial work for him that ten efficient secretaries couldn't do in twice the time.\"\nHis goal: \"We hope to help man do better what he does—perhaps by as much as 50 per cent.\"\nHowever, the demo received little attention in the following decades.7\nEngelbart continued his work at SRI for almost a decade, but as Engelbart commented with frustration,\n“There was a slightly less than universal perception of our value at SRI”.8\nIn 1977, SRI sold the Augmentation Research Center to Tymshare, a time-sharing computing company.\n(Timesharing was the cloud computing of the 1970s and 1980s,\nwhere companies would use time on a centralized computer.)\nAt Tymshare, Engelbart's system was renamed AUGMENT and marketed as an office automation service, but Engelbart himself was sidelined from development,\na situation that he described as\nsitting in a corner and becoming invisible.\nMeanwhile, Bill English and some other SRI researchers9 migrated four miles south to Xerox PARC and worked on the Xerox Alto computer.\nThe Xerox Alto incorporated many ideas from the Augmentation Research Center including the graphical user interface, the mouse, and the keyset.\nThe Alto's keyset\nwas almost identical to the Engelbart keyset, as can be seen in the photo below.\nThe Alto's keyset was most popular for the networked 3D shooter game \"Maze War\", with the clicking of keysets echoing through the hallways of Xerox PARC.\nA Xerox Alto with a keyset on the left.\nXerox famously failed to commercialize the ideas from the Xerox Alto, but Steve Jobs recognized the importance of interactivity, the graphical user interface, and the mouse\nwhen he visited Xerox PARC in 1979.\nSteve Jobs provided the Apple Lisa and Macintosh ended up with a graphical user interface and the mouse (streamlined to one button instead of three), but he left the keyset behind.10\nWhen McDonnell Douglas acquired Tymshare in 1984, Engelbart and his software—now called Augment—had a new home.11\nIn 1987, McDonnell Douglas released a text editor and outline processor for the IBM PC called\nMiniBASE,\none of the few PC applications that supported a keyset.\nThe functionality of MiniBASE was almost identical to Engelbart's 1968 demo, but in 1987, MiniBASE\nwas competing against GUI-based word processors such as MacWrite and Microsoft Word, so MiniBASE had little impact.\nEngelbart left McDonnell Douglas in 1988, forming a research foundation called the Bootstrap Institute to continue his research independently.\nThe name: \"The Mother of All Demos\"\nThe name \"The Mother of All Demos\" has its roots in the Gulf War.\nIn August 1990, Iraq invaded Kuwait, leading to war between Iraq and a coalition of the United States and 41 other countries.\nDuring the months of buildup prior to active conflict, Iraq's leader, Saddam Hussein,\nexhorted the Iraqi people to prepare for \"the mother of all battles\",12 a phrase that caught the attention of the media.\nThe battle didn't proceed as Hussein hoped: during exactly 100 hours of ground combat, the US-led coalition liberated Kuwait, pushed into Iraq, crushed the Iraqi forces,\nand declared a ceasefire.13\nHussein's mother of all battles became the mother of all surrenders.\nThe phrase \"mother of all ...\" became the 1990s equivalent of a meme, used as a slightly-ironic superlative.\nIt was applied to everything\nfrom The Mother of All Traffic Jams to The Mother of All Windows Books, from The Mother of All Butter Cookies to Apple calling mobile devices\nThe Mother of All Markets.14\nIn 1991, this superlative was applied to a computer demo, but it wasn't Engelbart's demo.\nAndy Grove, Intel's president, gave a keynote speech at Comdex 1991 entitled The Second Decade: Computer-Supported Collaboration,\na live demonstration of his vision for PC-based video conferencing and wireless communication in the PC's second decade.\nThis complex hour-long demo required almost six months to prepare, with 15 companies collaborating.\nIntel called this demo \"The Mother of All Demos\", a name repeated in the New York Times, San Francisco Chronicle, Fortune, and PC Week.15\nAndy Grove's demo was a hit, with over 20,000 people requesting a video tape, but the demo was soon forgotten.\nOn the eve of Comdex, the New York Times wrote about Intel's \"Mother of All Demos\". Oct 21, 1991, D1-D2.\nIn 1994, Wired writer Steven Levy wrote Insanely Great: The Life and Times of Macintosh, the Computer that Changed Everything.8\nIn the second chapter of this comprehensive book, Levy explained how Vannevar Bush and Doug Engelbart \"sparked a chain reaction\" that led to the Macintosh.\nThe chapter described Engelbart's 1968 demo in detail including a throwaway line saying, \"It was the mother of all demos.\"16\nBased on my research, I think this is the source of the name \"The Mother of All Demos\" for Engelbart's demo.\nBy the end of the century, multiple publications echoed Levy's catchy phrase.\nIn February 1999, the San Jose Mercury News had a special article on Engelbart, saying that the demonstration was \"still called 'the mother of all demos'\", a description echoed by\nthe industry publication Computerworld.17\nThe book Nerds: A Brief History of the Internet stated that the demo \"has entered legend as 'the mother of all demos'\".\nBy this point, Engelbart's fame for the \"mother of all demos\" was cemented and the phrase became near-obligatory when writing about him.\nThe classic Silicon Valley history Fire in the Valley (1984), for example,\ndidn't even mention Engelbart but in the second edition (2000),\n\"The Mother of All Demos\" had its own chapter.\nInterfacing the keyset to USB\nGetting back to the keyset interface,\nthe keyset consists of five microswitches, triggered by the five levers.\nThe switches are wired to a standard DB-25 connector.\nI used a Teensy 3.6 microcontroller board for the interface, since this board can act both as a USB device\nand as a USB host.\nAs a USB device, the Teensy can emulate a standard USB keyboard.\nAs a USB host, the Teensy can receive input from a standard USB mouse.\nConnecting the keyset to the Teensy is (almost) straightforward, wiring the switches to five data inputs on the Teensy and the common line connected to ground.\nThe Teensy's input lines can be configured with pullup resistors inside the microcontroller. The result is that a data line shows 1 by default and\n0 when the corresponding key is pressed.\nOne complication is that the keyset apparently has a 1.5 kΩ between the leftmost button and ground, maybe to indicate that the device is plugged in.\nThis resistor caused that line to always appear low to the Teensy.\nTo counteract this and allow the Teensy to read the pin, I connected a 1 kΩ pullup resistor to that one line.\nThe interface code\nReading the keyset and sending characters over USB is mostly straightforward, but there are a few complications.\nFirst, it's unlikely that the user will press multiple keyset buttons at exactly the same time. Moreover, the button contacts may bounce.\nTo deal with this, I wait until the buttons have a stable value for 100 ms (a semi-arbitrary delay) before sending a key over USB.\nThe second complication is that with five keys, the keyset only supports 32 characters. To obtain upper case, numbers, special characters, and control\ncharacters, the keyset is designed to be used in conjunction with mouse buttons.\nThus, the interface needs to act as a USB host, so I can plug in a USB mouse to the interface.\nIf I want the mouse to be usable as a mouse, not just buttons in conjunction with the keyset, the interface mus forward mouse events over USB.\nBut it's not that easy, since mouse clicks in conjunction with the keyset shouldn't be forwarded. Otherwise, unwanted clicks will happen while\nusing the keyset.\nTo emulate a keyboard, the code uses the Keyboard library. This library provides\nan API to send characters to the destination computer.\nInconveniently, the simplest method, print(), supports only regular characters, not special characters like ENTER or BACKSPACE. For those, I needed to\nuse the lower-level press() and release() methods.\nTo read the mouse buttons,\nthe code uses the USBHost_t36 library, the Teensy version of the USB Host library.\nFinally, to pass mouse motion through to the destination computer, I use the Mouse library.\nIf you want to make your own keyset, Eric Schlaepfer has a model here.\nConclusions\nEngelbart claimed  that learning a keyset wasn't\ndifficult—a six-year-old kid could learn it in less than a week—but I'm not willing to invest much time into learning it. In my brief use of the keyset, I found it very difficult to use physically.\nPressing four keys at once is difficult, with the worst being all fingers except the ring finger. Combining this with a mouse button or two at the same time\ngave me the feeling that I was sight-reading a difficult piano piece.\nMaybe it becomes easier with use, but I noticed that Alto programs tended to treat the keyset as function keys, rather than a mechanism for typing with chords.18\nDavid Liddle of Xerox PARC said, \"We found that [the keyset] was tending to slow people down, once you got away from really hot [stuff] system programmers.\nIt wasn't quite so good if you were giving it to other engineers, let alone clerical people and so on.\"\nIf anyone else has a keyset that they want to connect via USB (unlikely as it may be), my code is on\ngithub.19  Thanks to Christina Engelbart for loaning me the keyset. Thanks to Bill Paxton for answering my questions.\nFollow me on Bluesky (@righto.com) or RSS for updates.\nFootnotes and references\n\nEngelbart's use of the mouse wasn't arbitrary, but based on research.\nIn 1966, shortly after inventing the mouse, Engelbart carried out a\nNASA-sponsored study\nthat evaluated six input devices: two types of joysticks, a Graphacon positioner, the mouse,\na light pen, and a control operated by the knees (leaving the hands free).\nThe mouse, knee control, and light pen performed best, with users finding the mouse satisfying to use. Although inexperienced subjects had some trouble with the mouse, experienced subjects considered\nit the best device.\nA joystick, Graphacon, mouse, knee control, and light pen were examined as input devices. Photos from the study.\n↩\n\nThe information sheet below from the Augmentation Research Center shows what keyset chords correspond to each character.\nI used this encoding for my interface software.\nEach column corresponds to a different combination of mouse buttons.\nThe information sheet for the keyset specifies how to obtain each character.\nThe special characters above are <CD> (Command Delete, i.e. cancel a partially-entered command), <BC> (Backspace Character), <OK> (confirm command), <BW>(Backspace Word), <RC> (Replace Character), <ESC> (which does filename completion).\nNLS and the Augment software have the concept of a viewspec, a view specification that controls the\nview of a file.\nFor instance, viewspecs can expand or collapse an outline to show more or less detail, filter the content, or show authorship of sections.\nThe keyset can select viewspecs, as shown below.\nBack of the keyset information sheet.\nViewsets are explained in more detail in The Mother of All Demos.\nFor my keyset interface, I ignored viewspecs since I don't have software to use these inputs, but\nit would be easy to modify the code to output the desired viewspec characters.\n↩\n\nSee Augmenting Human Intellect: A Conceptual Framework, Engelbart's 1962 report.↩\n\nEngelbart used an SDS 940 computer running the Berkeley Timesharing System.\nThe computer had 64K words of core memory, with 4.5 MB of drum storage for swapping and 96 MB of disk storage for files.\nFor displays, the computer drove twelve 5\" high-resolution CRTs, but these weren't viewed directly.\nInstead, each CRT had a video camera pointed at it and the video was redisplayed on a larger display in a work station in each office.\nThe SDS 940 was a large 24-bit scientific computer, built by Scientific Data Systems.\nAlthough SDS built the first integrated-circuit-based commercial computer in 1965 (the SDS 92),\nthe SDS 940 was a transistorized system.\nIt consisted of multiple refrigerator-sized cabinets, as shown below. Since each memory cabinet held 16K words and the computer at SRI had 64K,\nSRI's computer had two additional cabinets of memory.\nFront view of an SDS 940 computer. From the Theory of Operation manual.\nIn the late 1960s, Xerox wanted to get into the computer industry, so Xerox\nbought Scientific Data Systems in 1969 for $900 million (about $8 billion in current dollars).\nThe acquisition was a disaster. After steadily losing money, Xerox decided to exit the mainframe computer business in 1975.\nXerox's CEO summed up the purchase: \"With hindsight, we would not have done the same thing.\"↩\n\nThe Mother of All Demos is on YouTube,\nas well as a five-minute summary for the impatient.↩\n\nThe desk for the keyset and mouse was designed by Herman Miller, the office furniture company.\nHerman Miller worked with SRI to design the\ndesks, chairs, and office walls as part of their plans for the office of the future.\nHerman Miller invented the cubicle office in 1964, creating a modern replacement for the commonly used open office arrangement.↩\n\nEngelbart's demo is famous now, but for many years it was ignored.\nFor instance, Electronic Design had a long\narticle\non Engelbart's work in 1969 (putting the system on the cover), but there was no mention of the demo.\nEngelbart's system was featured on the cover of Electronic Design. Feb 1, 1969. (slightly retouched)\nBut by the 1980s, the Engelbart demo started getting attention.\nThe 1986 documentary Silicon Valley Boomtown had a long\nsection on Engelbart's work and the demo. By 1988, the New York Times was referring to the demo as legendary.↩\n\nLevy had written about Engelbart a decade earlier, in the May 1984 issue of the magazine Popular Computing.\nThe article focused on the mouse, recently available to the public through the Apple Lisa and the IBM PC (as an option).\nThe big issue at the time was how many buttons a mouse should have: three like Engelbart's mouse, the one button that Apple used, or two buttons\nas Bill Gates preferred.\nBut Engelbart's larger vision also came through in Levy's interview along with his frustration that most of his research had been ignored,\novershadowed by the mouse.\nNotably, there was no mention of Engelbart's 1968 demo in the article.↩↩\n\nThe SRI researchers who moved to Xerox include Bill English, Charles Irby, Jeff Rulifson, Bill Duval, and Bill Paxton (details).↩\n\nIn 2023, Xerox donated the entire Xerox PARC research center to SRI. The research center remained in Palo Alto but became part of SRI.\nIn a sense, this closed the circle, since many of the people and ideas from SRI had gone to PARC in the 1970s.\nHowever, both PARC and SRI had changed radically since the 1970s, with the cutting edge of computer research moving elsewhere.↩\n\nFor a detailed discussion of the Augment system, see Tymshare's Augment: Heralding a New Era, Oct 1978.\nAugment provided a \"broad range of information handling capability\" that was not available elsewhere.\nUnlike other word processing systems, Augment was targeted at the professional, not clerical workers,\npeople who were \"eager to explore the open-ended possibilities\" of the interactive process.\nThe main complaints about Augment were its price and that it was not easy to use. Accessing Engelbart's NLS system over ARPANET cost an eye-watering $48,000 a year (over $300,000 a year in current dollars).\nTymshare's Augment service was cheaper (about $80 an hour in current dollars), but still much more expensive than a standard word processing\nservice.\nOverall, the article found that Augment users were delighted with the system: \"It is stimulating to belong to the electronic intelligentsia.\"\nUsers found it to be \"a way of life—an absorbing, enriching experience\".↩\n\nWilliam Safire provided background in the New York Times, explaining\nthat \"the mother of all battles\"\noriginally referred to the battle of Qadisiya in A.D. 636, and Saddam Hussein was referencing that ancient battle.\nA translator responded, however,\nthat the Arabic expression would be better translated as \"the great battle\" than \"the mother of all battles.\"↩\n\nThe end of the Gulf War left Saddam Hussein in control of Iraq and left thousands of US troops in Saudi Arabia.\nThese factors would turn out to be catastrophic in the following years.↩\n\nAt the Mobile '92 conference, Apple's CEO, John Sculley, said personal communicators could be \"the mother of all markets,\"\nwhile Andy Grove of Intel said that the idea of a wireless personal communicator in every pocket is \"a pipe dream driven by greed\"\n(link).\nIn hindsight, Sculley was completely right and Grove was completely wrong.↩\n\nSome references to Intel's \"Mother of all demos\" are\nComputer Industry Gathers Amid Chaos, New York Times, Oct 21, 1991\nand \"Intel's High-Tech Vision of the Future: Chipmaker proposes using computers to dramatically improve productivity\", San Francisco Chronicle, Oct 21, 1991, p24.\nThe title of an article in Microprocessor Report, \"Intel Declares Victory in the Mother of All Demos\" (Nov. 20, 1991), alluded to the recently-ended war.\nFortune wrote about Intel's demo in the  Feb 17, 1997 issue.\nA longer description of Intel's demo is in the book Strategy is Destiny.↩\n\nSeveral sources claim that Andy van Dam was the first to call Engelbart's demo \"The Mother of All Demos.\" Although van Dam attended the 1968 demo,\nI couldn't find any evidence that he coined the phrase.\nJohn Markoff, a technology journalist for The New York Times, wrote a book What the Dormouse Said: How the Sixties Counterculture Shaped the Personal Computer Industry.\nIn this book, Markoff wrote about Engelbart's demo, saying \"Years later, his talk remained 'the mother of all demos' in the words of Andries van Dam, a Brown University computer scientist.\"\nAs far as I can tell, van Dam used the phrase but only after it had already been popularized by Levy.↩\n\nIt's curious to write that the demonstration was still called the \"mother of all demos\" when the phrase was just a few years old.↩\n\nThe photo below shows a keyset from the Xerox Alto.\nThe five keys are labeled with separate functions—Copy, Undelete, Move, Draw, and Fine—\nfor use with ALE,\na program for IC design.\nALE supported\nkeyset chording\nin combination with the mouse.\n↩Keyset from a Xerox Alto, courtesy of Digibarn.\n\nAfter I implemented this interface, I came across a project that constructed a 3D-printed chording keyset, also using a Teensy for the USB interface. You can find that project here.↩\n\nEmail ThisBlogThis!Share to XShare to FacebookShare to Pinterest\n\nLabels:\nalto,\nelectronics,\nreverse-engineering\n\n12 comments:\n\nMartin Haeberli\nsaid...\n\nKen - cool project ! I have a (few?) Xerox Optical Mice that I'd like to interface to USB as well - do you know of any off-the shelf solutions? Or maybe this could be adapted? Best, Martin\n\nMarch 23, 2025 at 8:54 AM\n\nKen Shirriff\nsaid...\n\nHi Martin! I don't know of any off-the-shelf solutions, but it would probably be easy to modify my interface. (Famous last words :)\n\nMarch 23, 2025 at 9:29 AM\n\nAnonymous\nsaid...\n\nCharles Moore, creator of the FORTH programming language, famously used a 5-button chording puck for programming in the 1970s.\n\nMarch 23, 2025 at 10:56 AM\n\n<img src=\"//blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglgI5BrNqTAwhKW-P2yd3IezmYF6xIss6PS08E5EYw5JA1g37XT-ua8QrbPkhen-laj2FYOc6-DP4U5H3YyboN9QiKE2XHZuILzMAxl9-3DWkxm3I78bme2aCCItOyU-w/s45-c/Still+of+Teaching+video+headshot.png\" width=\"35\" height=\"35\" class=\"photo\" alt=\"\">\nGeoff Fellows\nsaid...\n\nJust a minor correction 1K ohm is 1024 ohms. Please use S.I. prefixes in this case lowercase k for x1000. See https://www.nist.gov/pml/owm/metric-si-prefixes\n\nMarch 23, 2025 at 1:27 PM\n\nPouya Kary\nsaid...\n\nHi! Your work is amazing! Recently I tried to create the keyset with a midi controller and so since the community of those who care about keyset is so small, I wished to show it to you. https://github.com/pouyakary/nls-keyset-with-midi-keys\n\nMarch 24, 2025 at 12:19 AM\n\nPouya Kary\nsaid...\n\nWow I didn't know herman miller was involved, how cool\n\nMarch 24, 2025 at 12:21 AM\n\nAnonymous\nsaid...\n\nThe existence of Morse code should have told Engelbart that using straight ASCII encoding might not be the optimal solution.Also, why not embrace the fact that you can't press (and release) keys at the same time? You get 24 different codes that way, pressing and releasing 2 of 4 keys in some kind of order. Add single-key presses for 28. Add the thumb key (why only one?) and you've got more possible codes than you'll be able to remember.\n\nMarch 24, 2025 at 2:09 AM\n\nAnonymous\nsaid...\n\nUppercase K would be written with a line on top of it, so it was indeed lowercase k. ;)Also, isn't it really \"Ki\" to indicate 1024, with K representing Kelvin?\n\nMarch 24, 2025 at 2:43 AM\n\nAnonymous\nsaid...\n\nI wonder if you could have gone with mouse scroll wheel for upper/lower case?Have a \"natural gravity\" to middle and when you push up, you get all uppercase, and push back down and you get all lowercase. A quick, short scroll up only uppercases the next letter.Not sure of the best way to provide visual feedback (status tray icon? just look at the letters coming out), but I think it can be tuned to work very nicely.\n\nMarch 24, 2025 at 2:47 AM\n\nAnonymous\nsaid...\n\nI know that the Robot in a Can team has a clean esp and laser cut wood keyset build. I don'tbknow the link to the files and they're currently out of stock, but I'll reach out and see if they can restock and republish the open source design.\n\nMarch 24, 2025 at 5:13 AM\n\nAnonymous\nsaid...\n\nOne place this interface did come into use is on braille devices.\n\nMarch 24, 2025 at 6:43 AM\n\nZack\nsaid...\n\nWhile using key chords to type letters may not be efficient, using key chords to type full words can enable up to 300wpm with stenotype machines [1]. The first commercially available version was sold by the United States Stenograph Corp in 1884, so they'd been around for some time [2].This continues today with the OpenSteno Project, which is compatible with normal keyboards! (and also stenotype machines) [3][1] https://en.wikipedia.org/wiki/Stenotype[2] http://www.stenograph.com/history-writers[3] http://www.openstenoproject.org/\n\nMarch 24, 2025 at 8:20 AM\n\nPost a Comment\n\nOlder Post\n\nHome\n\n      @import url('https://fonts.googleapis.com/css?family=Montserrat:300,400,500,700');\n      .form-preview {\n      display: flex;\n      flex-direction: column;\n      justify-content: center;\n      margin-top: 30px;\n      padding: clamp(17px, 5%, 40px) clamp(17px, 7%, 50px);\n      max-width: 350px;\n      min-height: 200px;\n      border-radius: 6px;\n      box-shadow: 0 5px 25px rgba(34, 60, 47, 0.25);\n      }\n      .form-preview,\n      .form-preview *{\n        box-sizing: border-box;\n      }\n      .form-preview .preview-heading {\n      width: 100%;\n      }\n      .form-preview .preview-heading h5{\n        margin-top: 0;\n        margin-bottom: 0;\n      }\n      .form-preview .preview-input-field {\n      margin-top: 20px;\n      width: 100%;\n      }\n      .form-preview .preview-input-field input {\n      width: 100%;\n      height: 40px;\n      border-radius: 6px;\n      border: 2px solid #e9e8e8;\n      background-color: #fff;\n      outline: none;\n      }\n      .form-preview .preview-input-field input::placeholder, .form-preview .preview-input-field input {\n      opacity: 0.5;\n      color: #000;\n      font-family: \"Montserrat\";\n      font-size: 14px;\n      font-weight: 500;\n      line-height: 20px;\n      text-align: center;\n      }\n      .form-preview .preview-submit-button {\n      margin-top: 10px;\n      width: 100%;\n      }\n      .form-preview .preview-submit-button button {\n      width: 100%;\n      height: 40px;\n      border: 0;\n      border-radius: 6px;\n      line-height: 0px;\n      }\n      .form-preview .preview-submit-button button:hover {\n      cursor: pointer;\n      }\n\n      Get new posts by email:  Subscribe\n\nAbout the site\n\nContact info and site index\n\nPopular Posts\n\nThe Pentium contains a complicated circuit to multiply by three\n\nA USB interface to the \"Mother of All Demos\" keyset\n\nInside a vintage aerospace navigation computer of uncertain purpose\n\nA Multi-Protocol Infrared Remote Library for the Arduino\n\nApple iPhone charger teardown: quality in a tiny expensive package\n\nA dozen USB chargers in the lab: Apple is very good, but not quite the best\n\nMining Bitcoin with pencil and paper: 0.67 hashes per day\n\nTeardown and exploration of Apple's Magsafe connector\n\nSearch This Blog\n\nLabels\n\n386\n\n6502\n\n8008\n\n8085\n\n8086\n\n8087\n\n8088\n\naerospace\n\nalto\n\nanalog\n\nApollo\n\napple\n\narc\n\narduino\n\narm\n\nbeaglebone\n\nbitcoin\n\nc#\n\ncadc\n\ncalculator\n\nchips\n\ncss\n\ndatapoint\n\ndx7\n\nelectronics\n\nf#\n\nfairchild\n\nfpga\n\nfractals\n\ngenome\n\nglobus\n\nhaskell\n\nHP\n\nhtml5\n\nibm\n\nibm1401\n\nibm360\n\nintel\n\nipv6\n\nir\n\njava\n\njavascript\n\nmath\n\nmicrocode\n\noscilloscope\n\nPentium\n\nphoto\n\npower supply\n\nrandom\n\nreverse-engineering\n\nsheevaplug\n\nsnark\n\nspace\n\nspanish\n\nsynth\n\nteardown\n\ntheory\n\nunicode\n\nZ-80\n\nBlog Archive\n\n        ▼\n\n2025\n\n(7)\n\n        ▼\n\nMarch\n\n(2)\n\nA USB interface to the \"Mother of All Demos\" keyset\nThe Pentium contains a complicated circuit to mult...\n\n        ►\n\nFebruary\n\n(1)\n\n        ►\n\nJanuary\n\n(4)\n\n        ►\n\n2024\n\n(21)\n\n        ►\n\nDecember\n\n(1)\n\n        ►\n\nNovember\n\n(1)\n\n        ►\n\nOctober\n\n(1)\n\n        ►\n\nSeptember\n\n(3)\n\n        ►\n\nAugust\n\n(2)\n\n        ►\n\nJuly\n\n(2)\n\n        ►\n\nJune\n\n(1)\n\n        ►\n\nMay\n\n(1)\n\n        ►\n\nApril\n\n(1)\n\n        ►\n\nMarch\n\n(2)\n\n        ►\n\nFebruary\n\n(3)\n\n        ►\n\nJanuary\n\n(3)\n\n        ►\n\n2023\n\n(35)\n\n        ►\n\nDecember\n\n(4)\n\n        ►\n\nNovember\n\n(2)\n\n        ►\n\nOctober\n\n(3)\n\n        ►\n\nSeptember\n\n(1)\n\n        ►\n\nAugust\n\n(2)\n\n        ►\n\nJuly\n\n(3)\n\n        ►\n\nMay\n\n(1)\n\n        ►\n\nApril\n\n(2)\n\n        ►\n\nMarch\n\n(4)\n\n        ►\n\nFebruary\n\n(5)\n\n        ►\n\nJanuary\n\n(8)\n\n        ►\n\n2022\n\n(18)\n\n        ►\n\nNovember\n\n(3)\n\n        ►\n\nAugust\n\n(1)\n\n        ►\n\nJuly\n\n(1)\n\n        ►\n\nJune\n\n(1)\n\n        ►\n\nMay\n\n(1)\n\n        ►\n\nApril\n\n(4)\n\n        ►\n\nMarch\n\n(2)\n\n        ►\n\nFebruary\n\n(3)\n\n        ►\n\nJanuary\n\n(2)\n\n        ►\n\n2021\n\n(26)\n\n        ►\n\nDecember\n\n(4)\n\n        ►\n\nNovember\n\n(2)\n\n        ►\n\nSeptember\n\n(1)\n\n        ►\n\nAugust\n\n(1)\n\n        ►\n\nJuly\n\n(2)\n\n        ►\n\nJune\n\n(2)\n\n        ►\n\nMay\n\n(1)\n\n        ►\n\nApril\n\n(2)\n\n        ►\n\nMarch\n\n(4)\n\n        ►\n\nFebruary\n\n(4)\n\n        ►\n\nJanuary\n\n(3)\n\n        ►\n\n2020\n\n(33)\n\n        ►\n\nDecember\n\n(2)\n\n        ►\n\nNovember\n\n(3)\n\n        ►\n\nOctober\n\n(2)\n\n        ►\n\nSeptember\n\n(4)\n\n        ►\n\nAugust\n\n(5)\n\n        ►\n\nJuly\n\n(2)\n\n        ►\n\nJune\n\n(3)\n\n        ►\n\nMay\n\n(4)\n\n        ►\n\nApril\n\n(2)\n\n        ►\n\nMarch\n\n(5)\n\n        ►\n\nJanuary\n\n(1)\n\n        ►\n\n2019\n\n(18)\n\n        ►\n\nNovember\n\n(3)\n\n        ►\n\nOctober\n\n(2)\n\n        ►\n\nSeptember\n\n(3)\n\n        ►\n\nAugust\n\n(1)\n\n        ►\n\nJuly\n\n(4)\n\n        ►\n\nApril\n\n(2)\n\n        ►\n\nFebruary\n\n(1)\n\n        ►\n\nJanuary\n\n(2)\n\n        ►\n\n2018\n\n(17)\n\n        ►\n\nDecember\n\n(1)\n\n        ►\n\nSeptember\n\n(4)\n\n        ►\n\nAugust\n\n(1)\n\n        ►\n\nJune\n\n(1)\n\n        ►\n\nMay\n\n(1)\n\n        ►\n\nApril\n\n(1)\n\n        ►\n\nMarch\n\n(3)\n\n        ►\n\nFebruary\n\n(1)\n\n        ►\n\nJanuary\n\n(4)\n\n        ►\n\n2017\n\n(21)\n\n        ►\n\nDecember\n\n(5)\n\n        ►\n\nNovember\n\n(2)\n\n        ►\n\nOctober\n\n(3)\n\n        ►\n\nAugust\n\n(1)\n\n        ►\n\nJuly\n\n(2)\n\n        ►\n\nJune\n\n(2)\n\n        ►\n\nApril\n\n(2)\n\n        ►\n\nMarch\n\n(2)\n\n        ►\n\nFebruary\n\n(1)\n\n        ►\n\nJanuary\n\n(1)\n\n        ►\n\n2016\n\n(34)\n\n        ►\n\nDecember\n\n(2)\n\n        ►\n\nOctober\n\n(5)\n\n        ►\n\nSeptember\n\n(8)\n\n        ►\n\nAugust\n\n(2)\n\n        ►\n\nJuly\n\n(3)\n\n        ►\n\nJune\n\n(4)\n\n        ►\n\nMay\n\n(1)\n\n        ►\n\nApril\n\n(1)\n\n        ►\n\nMarch\n\n(1)\n\n        ►\n\nFebruary\n\n(4)\n\n        ►\n\nJanuary\n\n(3)\n\n        ►\n\n2015\n\n(12)\n\n        ►\n\nDecember\n\n(2)\n\n        ►\n\nNovember\n\n(1)\n\n        ►\n\nOctober\n\n(3)\n\n        ►\n\nAugust\n\n(1)\n\n        ►\n\nMay\n\n(2)\n\n        ►\n\nMarch\n\n(2)\n\n        ►\n\nFebruary\n\n(1)\n\n        ►\n\n2014\n\n(13)\n\n        ►\n\nDecember\n\n(1)\n\n        ►\n\nOctober\n\n(1)\n\n        ►\n\nSeptember\n\n(3)\n\n        ►\n\nMay\n\n(2)\n\n        ►\n\nMarch\n\n(1)\n\n        ►\n\nFebruary\n\n(5)\n\n        ►\n\n2013\n\n(24)\n\n        ►\n\nNovember\n\n(2)\n\n        ►\n\nSeptember\n\n(4)\n\n        ►\n\nAugust\n\n(4)\n\n        ►\n\nJuly\n\n(4)\n\n        ►\n\nJune\n\n(2)\n\n        ►\n\nApril\n\n(1)\n\n        ►\n\nMarch\n\n(2)\n\n        ►\n\nFebruary\n\n(2)\n\n        ►\n\nJanuary\n\n(3)\n\n        ►\n\n2012\n\n(10)\n\n        ►\n\nDecember\n\n(1)\n\n        ►\n\nNovember\n\n(5)\n\n        ►\n\nOctober\n\n(1)\n\n        ►\n\nMay\n\n(1)\n\n        ►\n\nMarch\n\n(1)\n\n        ►\n\nFebruary\n\n(1)\n\n        ►\n\n2011\n\n(11)\n\n        ►\n\nDecember\n\n(2)\n\n        ►\n\nJuly\n\n(2)\n\n        ►\n\nMay\n\n(2)\n\n        ►\n\nApril\n\n(1)\n\n        ►\n\nMarch\n\n(1)\n\n        ►\n\nFebruary\n\n(3)\n\n        ►\n\n2010\n\n(22)\n\n        ►\n\nDecember\n\n(2)\n\n        ►\n\nNovember\n\n(4)\n\n        ►\n\nOctober\n\n(3)\n\n        ►\n\nAugust\n\n(1)\n\n        ►\n\nJune\n\n(1)\n\n        ►\n\nMay\n\n(2)\n\n        ►\n\nApril\n\n(3)\n\n        ►\n\nMarch\n\n(4)\n\n        ►\n\nJanuary\n\n(2)\n\n        ►\n\n2009\n\n(22)\n\n        ►\n\nDecember\n\n(2)\n\n        ►\n\nNovember\n\n(5)\n\n        ►\n\nSeptember\n\n(1)\n\n        ►\n\nAugust\n\n(3)\n\n        ►\n\nJuly\n\n(1)\n\n        ►\n\nJune\n\n(3)\n\n        ►\n\nApril\n\n(1)\n\n        ►\n\nMarch\n\n(3)\n\n        ►\n\nFebruary\n\n(2)\n\n        ►\n\nJanuary\n\n(1)\n\n        ►\n\n2008\n\n(27)\n\n        ►\n\nJuly\n\n(3)\n\n        ►\n\nJune\n\n(1)\n\n        ►\n\nMay\n\n(3)\n\n        ►\n\nApril\n\n(4)\n\n        ►\n\nMarch\n\n(10)\n\n        ►\n\nFebruary\n\n(6)\n\nPowered by Blogger.",
    "summary": {
      "en": "**Summary of Ken Shirriff's Blog Post**\n\nKen Shirriff discusses the history and significance of Douglas Engelbart's groundbreaking 1968 demonstration, known as \"The Mother of All Demos,\" where he introduced many features we now use in computing, such as the mouse, hypertext, and graphical user interfaces. Engelbart aimed to enhance human intelligence through technology.\n\nThe blog post focuses on Shirriff's project of creating a USB interface for Engelbart's lesser-known input device, the keyset, which allows typing by pressing multiple keys simultaneously. Engelbart's innovations were heavily influenced by Vannevar Bush's 1945 essay envisioning advanced information systems.\n\nShirriff also explains how the phrase \"The Mother of All Demos\" became associated with Engelbart's presentation. Initially, it was coined by Intel for a different demo in 1991, which was later popularized in relation to Engelbart's work by writers like Steven Levy.\n\nShirriff details the technical aspects of interfacing the keyset with modern computers, including the challenges he faced with the keyset's design and functionality. Despite Engelbart's belief that the keyset was easy to learn, Shirriff found it difficult to use.\n\nOverall, the blog highlights the importance of Engelbart's contributions to computing while showcasing Shirriff's efforts to connect historical technology with today's devices.",
      "ko": "켄 시리프는 더글라스 엥겔바트의 1968년 혁신적인 시연, 즉 \"모든 시연의 어머니\"에 대한 역사와 중요성을 다룹니다. 이 시연에서 엥겔바트는 현재 우리가 컴퓨터에서 사용하는 많은 기능, 예를 들어 마우스, 하이퍼텍스트, 그래픽 사용자 인터페이스 등을 소개했습니다. 엥겔바트는 기술을 통해 인간의 지능을 향상시키는 것을 목표로 했습니다.\n\n블로그 글에서는 엥겔바트의 덜 알려진 입력 장치인 키셋을 위한 USB 인터페이스를 만드는 시리프의 프로젝트에 중점을 두고 있습니다. 키셋은 여러 키를 동시에 눌러서 입력할 수 있게 해주는 장치입니다. 엥겔바트의 혁신은 1945년 반니바 부시가 쓴 에세이에서 영감을 받았습니다. 이 에세이는 고급 정보 시스템을 구상한 내용을 담고 있습니다.\n\n시리프는 \"모든 시연의 어머니\"라는 표현이 엥겔바트의 발표와 어떻게 연결되었는지 설명합니다. 이 표현은 원래 1991년 인텔이 다른 시연을 위해 만든 것이었고, 이후 스티븐 레비와 같은 작가들에 의해 엥겔바트의 작업과 관련하여 널리 알려지게 되었습니다.\n\n시리프는 키셋을 현대 컴퓨터와 연결하는 기술적인 측면에 대해서도 자세히 설명합니다. 그는 키셋의 디자인과 기능에서 겪은 어려움에 대해 이야기합니다. 엥겔바트는 키셋이 배우기 쉽다고 믿었지만, 시리프는 사용하기 어렵다고 느꼈습니다.\n\n전반적으로 이 블로그는 엥겔바트의 컴퓨터 분야에 대한 기여의 중요성을 강조하며, 역사적인 기술과 오늘날의 장치를 연결하려는 시리프의 노력을 보여줍니다.",
      "ja": "ケン・シリフは、ダグラス・エンゲルバートが1968年に行った画期的なデモンストレーション「すべてのデモの母」とその意義について語っています。このデモでは、マウスやハイパーテキスト、グラフィカルユーザーインターフェースなど、現在のコンピュータで使われている多くの機能が紹介されました。エンゲルバートは、技術を通じて人間の知性を高めることを目指していました。\n\nブログ記事では、シリフがエンゲルバートのあまり知られていない入力デバイス「キーセット」のUSBインターフェースを作成するプロジェクトに焦点を当てています。キーセットは、複数のキーを同時に押すことで入力ができる装置です。エンゲルバートの革新は、1945年にバンネバー・ブッシュが提唱した高度な情報システムに関するエッセイから大きな影響を受けています。\n\nシリフはまた、「すべてのデモの母」というフレーズがエンゲルバートのプレゼンテーションと結びつくようになった経緯について説明しています。このフレーズは、もともと1991年にインテルが別のデモのために作ったもので、後にスティーブン・レビーなどの作家によってエンゲルバートの業績に関連づけられました。\n\nシリフは、キーセットを現代のコンピュータに接続する際の技術的な側面や、デザインや機能に関する課題について詳しく述べています。エンゲルバートはキーセットが簡単に習得できると考えていましたが、シリフは使用するのが難しいと感じました。\n\n全体として、このブログはエンゲルバートのコンピュータへの貢献の重要性を強調し、シリフが歴史的な技術を現代のデバイスと結びつける努力を紹介しています。"
    }
  },
  {
    "id": "85899ffe934cf308",
    "title": {
      "en": "The SeL4 Microkernel: An Introduction [pdf]",
      "ko": "SeL4 마이크로커널 입문",
      "ja": "SeL4マイクロカーネル入門"
    },
    "type": "story",
    "url": "https://sel4.systems/About/seL4-whitepaper.pdf",
    "score": 259,
    "by": "snvzz",
    "time": 1742728168,
    "content": "The seL4 ®   Foundation https://sel4.systems/Foundation  The seL4 Microkernel An Introduction  Gernot Heiser  gernot@sel4.systems Revision 1.4 of 2025-01-08\n\nAbstract  This whitepaper provides an introduction to and overview of seL4. We explain what seL4 is (and is not) and explore its defining features. We explain what makes seL4 uniquely qualified as the operating-system kernel of choice for security- and safety-critical systems, and generally embedded and cyber-physical systems. In particular, we explain seL4’s assurance story, its security- and safety-relevant features, and its benchmark-setting performance. We also discuss typical usage scenarios, including incremental cyber retrofit of legacy systems.  CCS Concepts  •   Software and its engineering   →   Operating Systems  •   Security and privacy   →   Systems security  •   Security and privacy   →   Formal methods and theory of security  •   Computer systems organization   →   Real-time systems   →   Real-time operating systems  •   Computer systems organization   →   Real-time systems   →   Dependable and fault-tolerant systems and networks  Keywords  seL4, microkernel, performance  Reference Format:  Gernot Heiser. The seL4 Microkernel – An Introduction. White paper. The seL4 Foundation, Revision 1.4 of 2025-01-08.  Copyright   ©   2020 seL4 Project a Series of LF Projects, LLC. Distributed under the Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) License. seL4 is a trademark of LF Projects, LLC.\n\nContents  List of Figures   2 1   What Is seL4?   3 2   seL4 Is a Microkernel and a Hypervisor, It Is Not an OS   5  2.1   Monolithic kernels vs microkernels   . . . . . . . . . . . . . . . . . . . . .   5 2.2   seL4 Is a microkernel, not an OS . . . . . . . . . . . . . . . . . . . . . . .   7 2.3   seL4 is also a hypervisor   . . . . . . . . . . . . . . . . . . . . . . . . . . .   8 2.4   seL4 is not seLinux   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   10  3   seL4’s Verification Story   11  3.1   Correctness and security enforcement   . . . . . . . . . . . . . . . . . . .   12 3.2   The seL4 Microkit   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   15  4   About Capabilities   19  4.1   What are capabilities? . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   19 4.2   Why Capabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   20  5   Support for Hard Real-Time Systems   24  5.1   General real-time support . . . . . . . . . . . . . . . . . . . . . . . . . . .   24 5.2   Mixed-criticality systems   . . . . . . . . . . . . . . . . . . . . . . . . . . .   25  6   Security is No Excuse for Poor Performance   29 7   Real-World Deployment and Incremental Cyber Retrofit   30  7.1   General considerations   . . . . . . . . . . . . . . . . . . . . . . . . . . . .   30 7.2   Retrofitting existing systems   . . . . . . . . . . . . . . . . . . . . . . . . .   30  8   Conclusions   32 Bibliography   34  1\n\nList of Figures  2.1   Operating-system structure.   . . . . . . . . . . . . . . . . . . . . . . . . .   5 2.2   L4 microkernel family tree.   . . . . . . . . . . . . . . . . . . . . . . . . . .   7 2.3   VM-provided services   . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   9 2.4   seL4 virtualisation support with usermode VMMs. . . . . . . . . . . . . .   9 3.1   seL4’s proof chain. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   11 3.2   Translation validation proof chain. . . . . . . . . . . . . . . . . . . . . . .   13 3.3   Microkit abstractions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   15 3.4   Verified architecture mapping and system generation.   . . . . . . . . . .   17 4.1   Capabilities are keys to objects . . . . . . . . . . . . . . . . . . . . . . . .   19 4.2   The compiler as a confused deputy. . . . . . . . . . . . . . . . . . . . . .   22 5.1   Simplified example of a mixed-criticality system. . . . . . . . . . . . . . .   26 7.1   Incremental cyber-retrofit on the ULB. . . . . . . . . . . . . . . . . . . . .   31 2\n\nChapter 1 What Is seL4?  seL4 is an operating system microkernel  An operating system (OS) is the low-level system software that controls a computer system’s resources and enforces security. Unlike application software, the OS has exclusive access to a more privileged execution mode of the processor ( kernel mode ) that gives it direct access to hardware. Applications only ever execute in   user mode   and can only access hardware as permitted by the OS. An OS microkernel is a minimal core of an OS, reducing the code executing at higher privilege to a minimum. seL4 is a member of the L4 family of microkernels that goes back to the mid-1990s. (And no,   seL4 has nothing to do with seLinux. )  seL4 is also a hypervisor  seL4 supports virtual machines that can run a fully fledged guest OS such as Linux. Subject to seL4’s enforcement of communication channels, guests and their applications can communicate with each other as well as with native applications. Learn more about what it means that seL4 is a microkernel and its use as a hypervisor in Chapter 2. And learn about real-world deployment scenarios, including approaches for retrofitting security into legacy systems in Chapter 7.  seL4 is proved correct  seL4 comes with a formal, mathematical, machine-checked   proof of implementation correctness , meaning the kernel is in a very strong sense “bug free” with respect to its specification. In fact, seL4 is the world’s first OS kernel with such a proof at the code level [Klein et al., 2009].  seL4 is provably secure  Besides implementation correctness, seL4 comes with further proofs of   security enforcement   [Klein et al., 2014]. They say that in a correctly configured seL4-based system, the kernel guarantees the classical security properties of  confidentiality, integrity and availability . More about these proofs in Chapter 3.  seL4 improves security with fine-grained access control through capabilities  Capabilities are access tokens which support very fine-grained control over which entity can access a particular resource in a system. They support strong 3\n\nsecurity according to the principle of least privilege (also called principle of least authority, POLA). This is a core design principle of highly secure system, and is impossible to achieve with the way access control happens in mainstream systems such as Linux or Windows. seL4 is still the   world’s only OS that is both capability-based and formally verified , and as such has a defensible claim of being the world’s most secure OS. More about capabilities in Chapter 4.  seL4 ensures safety of time-critical systems  seL4 is the world’s only OS kernel (at least in the open literature) that has undergone a complete and sound analysis of its   worst-case execution time  (WCET) [Blackham et al., 2011, Sewell et al., 2017]. This means, if the kernel is configured appropriately, all kernel operations are bounded in time, and the bound is known. This is a prerequisite for building   hard real-time systems , where failure to react to an event within a strictly bounded time period is catastrophic.  seL4 is the world’s most advanced mixed-criticality OS  seL4 provides strong support for mixed criticality real-time systems (MCS), where the timeliness of critical activities must be ensured even if they co-exist with less trusted code executing on the same platform. seL4 achieves this with a flexible model that retains good resource utilisation, unlike the more established MCS OSes that use strict (and inflexible) time and space partitioning [Lyons et al., 2018]. More on seL4’s real-time and MCS support in Chapter 5.  seL4 is the world’s fastest microkernel  Traditionally, systems are either (sort-of) secure, or they are fast. seL4 is unique in that it is both. seL4 is designed to support a wide range of real-world use cases, whether they are security- (or safety-)critical or not, and excellent performance is a requirement. More on seL4’s performance in Chapter 6.  seL4 is pronounced “ess-e-ell-four”  The pronunciation “sell-four” is deprecated.  How to read this document  This document is meant to be approachable by a wide audience. However, for completeness, we will add some deeper technical detail in places.  Such detail will be marked with a chilli, like the one on the left. If you see this then you know you can safely skip the marked passage if you think the technical description is too “spicy” for your taste, or if you are simply not interested in this level of detail. Only other chillied passages will assume you have read it.  Technical section  Where the chilli appears in a section title, such as here, this indicates that the whole section is fairly technical and can be skipped. 4\n\nChapter 2 seL4 Is a Microkernel and a Hypervisor, It Is Not an OS  2.1   Monolithic kernels vs microkernels  To understand the difference between a mainstream OS, such as Linux, and a microkernel, such as seL4, let’s look at Figure 2.1. 10   kSLOC  Hardware  VFS  IPC, File System  Scheduler, Virtual Memory  Device Drivers, Dispatcher  Syscall  Hardware  IPC, Threads, Virtual Memory  Application  NW  Proto -  col  Stack  File  Server  Device  Driver  IPC  Kernel  Mode  User  Mode  20,000  kSLOC  Figure 2.1: Operating-system structure: Monolithic kernel (left) vs microkernel (right). The left side presents a (fairly abstracted) view of the architecture of a system such as Linux. The yellow part is the OS   kernel , it offers services such as file storage and networking to applications. All the code that implements those services executes in the   privileged mode   of the hardware, also called   kernel mode   or   supervisor mode   – the execution mode that has unfettered access and control of all resources in the system. In contrast, applications run in unprivileged, or   user mode , and do not have direct access to many hardware resources, which must be accessed through the OS. The OS is internally structured in a number of layers, where each layer provides abstractions implemented by layers below. The problem with privileged-mode code is that it is dangerous: If anything goes wrong here, there’s nothing to stop the damage. In particular, if this code has a bug that can be exploited by an attacker to run the attacker’s code in privileged mode (called a privilege-escalation or arbitrary code-execution attack) then the attacker can do what they want with the system. Such flaws are the root problem of the many system compromises we experience in mainstream systems. 5\n\nOf course, software bugs are mostly a fact of life, and OSes are not different. For example, the Linux kernel comprises of the order of 20 million lines of source code (20 MSLOC); we can estimate that it contains literally tens of thousands of bugs [Biggs et al., 2018]. This is obviously a huge attack surface! This idea is captured by saying that Linux has a large   trusted computing base   (TCB), which is defined as the subset of the overall system that must be trusted to operate correctly for the system to be secure. The idea behind a microkernel design is to drastically reduce the TCB and thus the attack surface. As schematically shown at the right of Figure 2.1, the kernel, i.e. the part of the system executing in privileged mode, is much smaller. In a well-designed microkernel, such as seL4, it is of the order of ten thousand lines of source code (10 kSLOC). This is literally three orders of magnitude smaller than the Linux kernel, and the attack surface shrinks accordingly (maybe more, as the density of bugs probably grows more than linearly with code size). Obviously, it is not possible to provide the same functionality, in terms of OS services, in such a small code base. In fact, the microkernel provides almost no services: it is just a thin wrapper around hardware, just enough to securely multiplex hardware resources. What the microkernel mostly provides is isolation, sandboxes in which programs can execute without interference from other programs. And, critically, it provides a   protected procedure call   (PPC) mechanism, which is a form of inter-process communication (IPC). For historical reasons the term IPC lives on, but I recommend avoiding it as it leads to misconceptions that result in poor designs.  For a deeper explanation of what seL4 IPC is and is not, I recommend reading my blog How to (and how not to) use seL4 IPC. The PPC mechanism allows one program to securely call a function in a different program, where the microkernel transports function inputs and outputs between the programs and, importantly, enforces interfaces: the “remote” (contained in a different sandbox) function can only be called at an exported entrypoint, and only by explicitly authorised clients (who have been given the appropriate capability, see Chapter 4). The microkernel system uses this approach to provide the services the monolithic OS implements in the kernel. In the microkernel world, these services are just programs, no different from applications, that run in their own sandboxes, and provide a PPC interface for applications to call. Should a server be compromised, that compromise is confined to the server, its sandbox protects the rest of the system. This is in stark contrast to the monolithic case, where a compromise of an OS service compromises the complete system. This effect can be quantified: Our recent study shows that of the known Linux compromises classified as   critical , i.e. most severe, 29% would be fully eliminated by a microkernel design, and another 55% would be mitigated enough to no longer qualify as critical [Biggs et al., 2018]. 6\n\n93   94   95   96   97   98   99   00   01   02   03   04   05   06   07   08   09   10   11   12   13 L3   →   L4   “X”   Hazelnut   Pistachio L4/Alpha L4/MIPS seL4 OKL4 Microkernel OKL4 Microvisor Codezero P4   →   PikeOS Fiasco   Fiasco.OC L4-embedded NOVA API Inheritance Code Inheritance GMD/IBM/TU Karlsruhe   UNSW/NICTA/Data61 TU Dresden   Commercial Clone OK Labs Creators: Figure 2.2: L4 microkernel family tree.  2.2   seL4 Is a microkernel, not an OS  seL4 is a microkernel, and designed for generality while minimising the TCB. It is a member of the L4 microkernel family, which goes back to the mid-’90s; Figure 2.2 shows seL4’s provenance. It was developed by our group at UNSW/NICTA, these days known as Trustworthy Systems (TS). At the time we had 15 years of experience in developing high-performance microkernels, and a track-record of real-world deployments: Our   OKL4 Microkernel   shipped on billions of Qualcomm cellular modem chips, and our   L4-embedded   kernel from the mid-Noughties runs on the secure enclave of all recent iOS devices (iPhones etc). Being a microkernel, seL4 contains none of the usual OS services; such services are provided by programs running in user mode. Besides the great advantages elaborated above, there are downsides to the microkernel design: These components must come from somewhere. Some can be ported from open-source OSes, such as FreeBSD or Linux, or they can be written from scratch. But in any case, this is significant work. To scale up we need the help of the community, and the seL4 Foundation is the key mechanism for enabling the community to cooperate and develop or port such services for seL4-based systems. The most important ones are device drivers, network protocol stacks, and file systems. We have a fair number of these, but much more is needed. Even compared to other microkernels, seL4’s API is very low-level, with only the minimum abstraction as required to securely manage the hardware. As such, building systems on seL4 is particularly difficult. A good way to look at this is to think of seL4 as the “assembly language of operating systems”: very primitive. No-one in their right mind would write a complex system, say a database or web server, in assembly language, we use higher-level programming languages for that. These simplify the task by providing higher-level constructs and abstractions, at the 7\n\nexpense of losing some of the power the hardware provides. In almost all cases, that is a winning trade-off. Similarly, one should not try to build a complex system directly on seL4, but should use a higher-level framework that provides more appropriate abstractions, at the expense of introducing some policy and taming the power of seL4. Specifically, such a framework should allows developers to focus on the code that implements the services, ignore hardware complexities, and automate much of the system integration. There are presently three main component frameworks for seL4, all open source: The Microkit, CAmkES and Genode. The seL4 Microkit reduces the complex seL4 API to literally a handful of simple abstractions, designed around components called   protection domains . It also provides a   software development kit   (SDK) that makes it easy to build and integrate separately compiled modules with a kernel binary to produce a bootable image. The Microkit’s simplicity is achieved by requiring the system architecture to be static, meaning the set of modules and their communications is defined at system configuration time – a model that seems to match the requirements of most (if not all) embedded systems, including complex cyberphysical systems such as cars and aircraft. CAmkES, a predecessor of the Microkit, is also a component framework for a statically-architected system. Its abstractions are higher level yet more complex than those of the Microkit. It also lacks an SDK, resulting in a much more painful build process that forces use of the kernel’s complex build system. CAmkES also introduces significant overheads. Genode is in many ways a more powerful and general framework, that supports multiple microkernels and already comes with a wealth of services and device drivers, especially for x86 platforms, and does not enforce a static system architecture. It is arguably more convenient to work with than CAmkES, and is certainly the way to get a complex system up quickly. However, Genode has drawbacks: 1. As it supports multiple microkernels, not all as powerful as seL4, Genode is based on the least common denominator. In particular, it cannot use all of seL4’s security and safety features. 2. It has no assurance story. More on this in Section 3.2. For these reasons, the Microkit is the recommended framework for building systems on seL4, at least as long as the static architecture works. Section 3.2 presents it in more detail.  2.3   seL4 is also a hypervisor  seL4 is a microkernel, but it is also a hypervisor: It is possible to run virtual machines on seL4, and inside the virtual machine (VM) a mainstream OS, such as Linux. This enables an alternative way of provisioning system services, by having a Linux VM provide them. Such a setup is shown in Figure 2.3, which shows how some services are borrowed from multiple Linux instances running as guest OSes in separate VMs. In this example, we provide two system services: networking and storage. Networking is provided by a native protocol stack running directly on seL4, lwIP or PicoTCP are 8",
    "summary": {
      "en": "The seL4 Foundation has released a whitepaper introducing seL4, a microkernel designed for security- and safety-critical systems. Here are the key points:\n\n1. **What is seL4?**\n   - seL4 is a microkernel, not a traditional operating system. It is part of the L4 family of microkernels and is known for its minimal codebase, which reduces the risk of security vulnerabilities.\n\n2. **Unique Features:**\n   - **Proven Correctness:** seL4 has a formal proof of its correctness, meaning it is highly reliable and \"bug-free\" according to its specifications.\n   - **Security:** It includes proofs for security enforcement, ensuring confidentiality, integrity, and availability when properly configured.\n   - **Capabilities:** seL4 uses a capability-based access control system, allowing precise permissions for accessing resources, which enhances security.\n   - **Real-Time Support:** It has undergone analysis for worst-case execution time, making it suitable for hard real-time systems.\n   - **Mixed-Criticality Systems:** seL4 supports systems where critical tasks run alongside less trusted applications without compromising performance.\n\n3. **Performance:** \n   - seL4 is designed to be both secure and fast, making it suitable for a variety of applications, including embedded systems.\n\n4. **Usage Scenarios:** \n   - The whitepaper discusses typical use cases, including retrofitting security into existing legacy systems.\n\n5. **Development Frameworks:**\n   - While seL4 itself is low-level, several frameworks (like the Microkit, CAmkES, and Genode) exist to help developers build systems on top of it, simplifying the process.\n\nOverall, seL4 stands out for its robustness, security features, and performance, making it an excellent choice for critical applications.",
      "ko": "seL4 재단이 보안 및 안전이 중요한 시스템을 위해 설계된 마이크로커널 seL4에 대한 백서를 발표했습니다. 주요 내용은 다음과 같습니다.\n\nseL4는 전통적인 운영 체제가 아닌 마이크로커널입니다. L4 계열의 마이크로커널 중 하나로, 최소한의 코드베이스를 가지고 있어 보안 취약점의 위험을 줄이는 데 도움을 줍니다.\n\nseL4의 특징 중 하나는 정형 검증이 이루어졌다는 점입니다. 이는 seL4가 매우 신뢰할 수 있으며, 사양에 따라 \"버그가 없는\" 시스템이라는 것을 의미합니다. 또한 보안 강화를 위한 증명이 포함되어 있어, 적절히 구성할 경우 기밀성, 무결성, 가용성을 보장합니다. seL4는 자원 접근을 위한 정밀한 권한 부여를 가능하게 하는 능력 기반 접근 제어 시스템을 사용하여 보안을 강화합니다. 최악의 실행 시간에 대한 분석이 이루어져 있어, 하드 실시간 시스템에 적합합니다. 또한, seL4는 중요한 작업과 신뢰성이 낮은 애플리케이션이 함께 실행되는 혼합 중요도 시스템을 지원합니다.\n\nseL4는 보안성과 속도를 모두 갖추도록 설계되어 있어 임베디드 시스템을 포함한 다양한 애플리케이션에 적합합니다. 백서에서는 기존 레거시 시스템에 보안을 추가하는 등의 일반적인 사용 사례도 논의하고 있습니다.\n\nseL4는 저수준 시스템이지만, Microkit, CAmkES, Genode와 같은 여러 개발 프레임워크가 있어 개발자들이 이를 기반으로 시스템을 구축하는 과정을 간소화할 수 있습니다.\n\n전반적으로 seL4는 견고함, 보안 기능, 성능 면에서 두드러져, 중요한 애플리케이션에 적합한 선택으로 평가받고 있습니다.",
      "ja": "seL4財団は、セキュリティと安全性が重要なシステム向けに設計されたマイクロカーネルであるseL4を紹介するホワイトペーパーを発表しました。以下はその主なポイントです。\n\nseL4とは、従来のオペレーティングシステムではなく、マイクロカーネルです。L4ファミリーの一部であり、最小限のコードベースを持つことでセキュリティの脆弱性を減少させることが知られています。\n\nseL4の特徴には、正式な正しさの証明があります。これは、仕様に従って非常に信頼性が高く「バグがない」とされることを意味します。また、セキュリティの強制に関する証明が含まれており、適切に設定されることで機密性、完全性、可用性を確保します。さらに、能力ベースのアクセス制御システムを採用しており、リソースへのアクセスに対する正確な権限を設定できるため、セキュリティが向上します。リアルタイムサポートもあり、最悪の実行時間の分析が行われているため、ハードリアルタイムシステムにも適しています。また、重要なタスクが信頼性の低いアプリケーションと共存する混合クリティカリティシステムもサポートしています。\n\nseL4は、安全性と速度の両方を兼ね備えており、組み込みシステムを含むさまざまなアプリケーションに適しています。ホワイトペーパーでは、既存のレガシーシステムにセキュリティを追加するなどの典型的な使用例についても言及されています。\n\nseL4自体は低レベルですが、Microkit、CAmkES、Genodeなどのフレームワークが存在し、開発者がその上にシステムを構築する際のプロセスを簡素化しています。\n\n全体として、seL4はその堅牢性、セキュリティ機能、パフォーマンスにおいて際立っており、重要なアプリケーションにとって優れた選択肢となっています。"
    }
  },
  {
    "id": "e3fa46eac06efa4b",
    "title": {
      "en": "Improving recommendation systems and search in the age of LLMs",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://eugeneyan.com/writing/recsys-llm/",
    "score": 401,
    "by": "7d7n",
    "time": 1742701205,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "b69ecdb1b76a6e20",
    "title": {
      "en": "Mathematicians uncover the logic behind how people walk in crowds",
      "ko": "군중 속 걷기의 수학적 비밀",
      "ja": "群衆の歩行法則"
    },
    "type": "story",
    "url": "https://news.mit.edu/2025/mathematicians-uncover-logic-behind-how-crowds-walk-0324",
    "score": 45,
    "by": "geox",
    "time": 1742849243,
    "content": "The findings could help planners design safer, more efficient pedestrian thoroughfares.\n\n        Jennifer Chu\n  |\n  MIT News\n\n                    Publication Date:\n\n              March 24, 2025\n\nPress Inquiries\n\n                    Press Contact:\n\n            Abby\n\n            Abazorius\n\n            Email:\n              abbya@mit.edu\n\n            Phone:\n              617-253-2709\n\n            MIT News Office\n\n            Media Download\n\n          ↓ Download Image\n\n                    Caption:\n\n              The researchers carried out controlled crowd experiments and studied how real participants walked through a crowd to reach certain locations. They identified a key measure, “angular spread,” that determines whether pedestrian traffic is ordered or disordered.\n\n                    Credits:\n\n              Image: Courtesy of the researchers\n\n  *Terms of Use:\n\n    Images for download on the MIT News office website are made available to non-commercial entities, press and the general public under a\n    Creative Commons Attribution Non-Commercial No Derivatives license.\n    You may not alter the images provided, other than to crop them to size. A credit line must be used when reproducing images; if one is not provided\n    below, credit the images to \"MIT.\"\n\n Close\n\n                    Caption:\n\n              Mathematicians studied the flow of human crowds and developed a way to predict when pedestrian paths will transition from orderly to entangled.\n\n                    Credits:\n\n              Image: iStock\n\n                    Caption:\n\n              The researchers carried out controlled crowd experiments and studied how real participants walked through a crowd to reach certain locations. They identified a key measure, “angular spread,” that determines whether pedestrian traffic is ordered or disordered.\n\n                    Credits:\n\n              Image: Courtesy of the researchers\n\nPrevious image\n          Next image\n\n            Next time you cross a crowded plaza, crosswalk, or airport concourse, take note of the pedestrian flow. Are people walking in orderly lanes, single-file, to their respective destinations? Or is it a haphazard tangle of personal trajectories, as people dodge and weave through the crowd?MIT instructor Karol Bacik and his colleagues studied the flow of human crowds and developed a first-of-its-kind way to predict when pedestrian paths will transition from orderly to entangled. Their findings may help inform the design of public spaces that promote safe and efficient thoroughfares.In a paper appearing this week in the Proceedings of the National Academy of Sciences, the researchers consider a common scenario in which pedestrians navigate a busy crosswalk. The team analyzed the scenario through mathematical analysis and simulations, considering the many angles at which individuals may cross and the dodging maneuvers they may make as they attempt to reach their destinations while avoiding bumping into other pedestrians along the way.The researchers also carried out controlled crowd experiments and studied how real participants walked through a crowd to reach certain locations. Through their mathematical and experimental work, the team identified a key measure that determines whether pedestrian traffic is ordered, such that clear lanes form in the flow, or disordered, in which there are no discernible paths through the crowd. Called “angular spread,” this parameter describes the number of people walking in different directions.If a crowd has a relatively small angular spread, this means that most pedestrians walk in opposite directions and meet the oncoming traffic head-on, such as in a crosswalk. In this case, more orderly, lane-like traffic is likely. If, however, a crowd has a larger angular spread, such as in a concourse, it means there are many more directions that pedestrians can take to cross, with more chance for disorder.In fact, the researchers calculated the point at which a moving crowd can transition from order to disorder. That point, they found, was an angular spread of around 13 degrees, meaning that if pedestrians don’t walk straight across, but instead an average pedestrian veers off at an angle larger than 13 degrees, this can tip a crowd into disordered flow.\n\nImage: Courtesy of the researchers\n\n            “This all is very commonsense,” says Bacik, who is a instructor of applied mathematics at MIT. “The question is whether we can tackle it precisely and mathematically, and where the transition is. Now we have a way to quantify when to expect lanes — this spontaneous, organized, safe flow — versus disordered, less efficient, potentially more dangerous flow.”The study’s co-authors include Grzegorz Sobota and Bogdan Bacik of theAcademy of Physical Education in Katowice, Poland, and Tim Rogers at the University of Bath in the United Kingdom.Right, left, centerBacik, who is trained in fluid dynamics and granular flow, came to study pedestrian flow during 2021, when he and his collaborators looked into the impacts of social distancing, and ways in which people might walk among each other while maintaining safe distances. That work inspired them to look more generally into the dynamics of crowd flow.In 2023, he and his collaborators explored “lane formation,” a phenomenon by which particles, grains, and, yes, people have been observed to spontaneously form lanes, moving in single-file when forced to cross a region from two opposite directions. In that work, the team identified the mechanism by which such lanes form, which Bacik sums up as “an imbalance of turning left versus right.” Essentially, they found that as soon as something in a crowd starts to look like a lane, individuals around that fledgling lane either join up, or are forced to either side of it, walking parallel to the original lane, which others can follow. In this way, a crowd can spontaneously organize into regular, structured lanes.“Now we’re asking, how robust is this mechanism?” Bacik says. “Does it only work in this very idealized situation, or can lane formation tolerate some imperfections, such as some people not going perfectly straight, as they might do in a crowd?”Lane changeFor their new study, the team looked to identify a key transition in crowd flow: When do pedestrians switch from orderly, lane-like traffic, to less organized, messy flow? The researchers first probed the question mathematically, with an equation that is typically used to describe fluid flow, in terms of the average motion of many individual molecules.“If youthink about the whole crowd flowing, rather than individuals, you can use fluid-like descriptions,” Bacik explains. “It’s this art of averaging, where, even if some people may cross more assertively than others, these effects are likely to average out in a sufficiently large crowd. If you only care about the global characteristics like, are there lanes or not, then you can make predictions without detailed knowledge of everyone in the crowd.”Bacik and his colleagues used equations of fluid flow, and applied them to the scenario of pedestrians flowing across a crosswalk. The team tweaked certain parameters in the equation, such as the width of the fluid channel (in this case, the crosswalk), and the angle at which molecules (or people) flowed across, along with various directions that people can “dodge,” or move around each other to avoid colliding.Based on these calculations, the researchers found that pedestrians in a crosswalk are more likely to form lanes, when they walk relatively straight across, from opposite directions. This order largely holds until people start veering across at more extreme angles. Then, the equation predicts that the pedestrian flow is likely to be disordered, with few to no lanes forming.The researchers were curious to see whether the math bears out in reality. For this, they carried out experiments in a gymnasium, where they recorded the movements of pedestrians using an overhead camera. Each volunteer wore a paper hat, depicting a unique barcode that the overhead camera could track.In their experiments, the team assigned volunteers various start and end positions along opposite sides of a simulated crosswalk, and tasked them with simultaneously walking across the crosswalk to their target location without bumping into anyone. They repeated the experiment many times, each time having volunteers assume different start and end positions. In the end, the researchers were able to gather visual data of multiple crowd flows, with pedestrians taking many different crossing angles.When they analyzed the data and noted when lanes spontaneously formed, and when they did not, the team found that, much like the equation predicted, the angular spread mattered. Their experiments confirmed that the transition from ordered to disordered flow occurred somewhere around the theoretically predicted 13 degrees. That is, if an average person veered more than 13 degrees away from straight ahead, the pedestrian flow could tip into disorder, with little lane formation. What’s more, they found that the more disorder there is in a crowd, the less efficiently it moves.The team plans to test their predictions on real-world crowds and pedestrian thoroughfares.“We would like to analyze footage and compare that with our theory,” Bacik says. “And we can imagine that, for anyone designing a public space, if they want to have a safe and efficient pedestrian flow, our work could provide a simpler guideline, or some rules of thumb.”This work is supported, in part, by the Engineering and Physical Sciences Research Council of UK Research and Innovation.\n\n    Share this news article on:\n\n          X\n\n          Facebook\n\n        LinkedIn\n\n        Reddit\n\n        Print\n\n      Paper\n\nPaper: “Order-disorder transition in multidirectional crowds”\n\n      Related Links\n\n      Karol BacikDepartment of MathematicsSchool of Science\n\nRelated Topics\n\n    Cities\n    Fluid dynamics\n    Mathematics\n    Research\n    Urban studies and planning\n    School of Science\n\n  Related Articles\n\n        Street smarts\n\n        Where the sidewalk ends\n\n        How the brain navigates cities\n\n        Tool for predicting pedestrian flow expands its reach\n\nPrevious item\n    Next item",
    "summary": {
      "en": "Researchers at MIT have developed a method to understand and predict pedestrian crowd flow, focusing on how orderly or chaotic movement occurs in busy areas like crosswalks. They identified a key factor called \"angular spread,\" which measures the direction pedestrians are moving. If most people walk straight across, the flow is organized; if they veer off course more than 13 degrees, the crowd becomes disordered.\n\nThe study involved mathematical modeling and experiments where participants walked across a simulated crosswalk. The results confirmed that a smaller angular spread leads to orderly lanes, while a larger spread results in disorder. This research can help urban planners design safer and more efficient public spaces by understanding these movement patterns. The team plans to further test their findings in real-world settings.",
      "ko": "MIT의 연구자들이 보행자 군중 흐름을 이해하고 예측하는 방법을 개발했습니다. 이 연구는 횡단보도와 같은 혼잡한 지역에서 사람들이 어떻게 질서 있게 또는 혼란스럽게 움직이는지를 중점적으로 다룹니다. 연구팀은 \"각도 분포\"라는 중요한 요소를 발견했는데, 이는 보행자들이 이동하는 방향을 측정하는 지표입니다. 대부분의 사람들이 직선으로 건너면 흐름이 질서 있게 유지되지만, 13도 이상으로 방향을 틀면 군중이 혼란스러워진다고 합니다.\n\n이 연구는 수학적 모델링과 참가자들이 시뮬레이션된 횡단보도를 건너는 실험을 포함했습니다. 실험 결과, 각도 분포가 작을수록 질서 있는 차선이 형성되고, 각도 분포가 클수록 혼란이 발생한다는 것을 확인했습니다. 이러한 연구 결과는 도시 계획자들이 보행자 이동 패턴을 이해함으로써 더 안전하고 효율적인 공공 공간을 설계하는 데 도움을 줄 수 있습니다. 연구팀은 앞으로 실제 환경에서 이 findings를 추가로 테스트할 계획입니다.",
      "ja": "MITの研究者たちは、歩行者の群れの流れを理解し予測する方法を開発しました。特に、交差点などの混雑した場所での秩序ある動きと混沌とした動きの違いに注目しています。彼らは「角度の広がり」という重要な要素を特定しました。これは、歩行者がどの方向に進んでいるかを測る指標です。ほとんどの人が真っ直ぐに横断歩道を渡る場合、流れは秩序立っていますが、13度以上の方向に逸れると、群れは混乱します。\n\nこの研究では、数学的なモデル化と、参加者がシミュレーションされた横断歩道を渡る実験が行われました。その結果、角度の広がりが小さいと秩序あるレーンが形成され、大きいと混乱が生じることが確認されました。この研究は、都市計画者がこれらの動きのパターンを理解することで、安全で効率的な公共スペースを設計するのに役立ちます。研究チームは、今後、実際の環境での結果をさらに検証する予定です。"
    }
  },
  {
    "id": "2852f55e961e65fa",
    "title": {
      "en": "Next.js version 15.2.3 has been released to address a security vulnerability",
      "ko": "Next.js 15.2.3 보안 업데이트",
      "ja": "Next.js 15.2.3 リリース！セキュリティ修正"
    },
    "type": "story",
    "url": "https://nextjs.org/blog/cve-2025-29927",
    "score": 237,
    "by": "makepanic",
    "time": 1742678347,
    "content": "Back to BlogSaturday, March 22nd 2025CVE-2025-29927Posted byLee Robinson@leerobNext.js version 15.2.3 has been released to address a security vulnerability (CVE-2025-29927). Additionally, backported patches are available.\nWe recommend that all self-hosted Next.js deployments using next start and output: 'standalone' should update immediately.\nContinue reading for more details on the CVE.\nTimeline\n\n2025-02-27T06:03Z: Disclosure to Next.js team via GitHub private vulnerability reporting\n2025-03-14T17:13Z: Next.js team started triaging the report\n2025-03-14T19:08Z: Patch pushed for Next.js 15.x\n2025-03-14T19:26Z: Patch pushed for Next.js 14.x\n2025-03-17T22:44Z: Next.js 14.2.25 released\n2025-03-18T00:23Z: Next.js 15.2.3 released\n2025-03-18T18:03Z: CVE-2025-29927 issued by GitHub\n2025-03-21T10:17Z: Security Advisory published\n2025-03-22T21:21Z: Next.js 13.5.9 released\n2025-03-23T06:44Z: Next.js 12.3.5 released\n\nVulnerability details\n\nNext.js uses an internal header x-middleware-subrequest to prevent recursive requests from triggering infinite loops. The security report showed it was possible to skip running Middleware, which could allow requests to skip critical checks—such as authorization cookie validation—before reaching routes.\nImpact scope\n\nAffected\n\nSelf-hosted Next.js applications using Middleware (next start with output: standalone)\nThis affects you if you rely on Middleware for auth or security checks, which are not then validated later in your application.\nApplications using Cloudflare can turn on a Managed WAF rule\n\nNot affected\n\nApplications hosted on Vercel\nApplications hosted on Netlify\nApplications deployed as static exports (Middleware not executed)\n\nPatched versions\n\nFor Next.js 15.x, this issue is fixed in 15.2.3\nFor Next.js 14.x, this issue is fixed in 14.2.25\nFor Next.js 13.x, this issue is fixed in 13.5.9\nFor Next.js 12.x, this issue is fixed in 12.3.5\n\nIf patching to a safe version is infeasible, it is recommended that you prevent external user requests which contain the x-middleware-subrequest header from reaching your Next.js application.\nOur security responsibility\n\nNext.js has published 16 security advisories since 2016. Over time, we've continued to improve how we gather, patch, and disclose vulnerabilities.\nGitHub Security Advisories and CVEs are industry-standard approaches to notifying users, vendors, and companies of vulnerabilities in software. While we have published a CVE, we missed the mark on partner communications.\nTo help us more proactively work with partners depending on Next.js, and other infrastructure providers, we are opening a partner mailing list. Please reach out to partners@nextjs.org to be included.",
    "summary": {
      "en": "Next.js has released version 15.2.3 to fix a security vulnerability (CVE-2025-29927). Users of self-hosted Next.js applications using 'next start' with 'standalone' output should update immediately. \n\n**Timeline of Events:**\n- Vulnerability reported on February 27, 2025.\n- Patches for Next.js 15.x and 14.x were released on March 14.\n- Version 15.2.3 and 14.2.25 were launched on March 18.\n- CVE-2025-29927 was issued on the same day, followed by a security advisory on March 21.\n\n**Vulnerability Details:**\nThe issue involves a header that could allow requests to bypass important checks, like authorization, which could affect security.\n\n**Who is Affected:**\n- Self-hosted Next.js applications using Middleware.\n- Applications relying on Middleware for authorization or security checks.\n\n**Who is Not Affected:**\n- Applications hosted on Vercel or Netlify.\n- Static exports where Middleware isn't executed.\n\n**Patch Information:**\n- Next.js versions 15.2.3, 14.2.25, 13.5.9, and 12.3.5 have patched this issue.\n\nIf updating is not possible, it is advised to block requests containing the specific header from reaching your application.\n\nNext.js has been proactive in addressing security issues and is establishing a mailing list for better communication with partners. Interested parties can contact partners@nextjs.org to join.",
      "ko": "Next.js가 보안 취약점(CVE-2025-29927)을 수정한 15.2.3 버전을 출시했습니다. 'standalone' 출력을 사용하는 'next start'로 자가 호스팅된 Next.js 애플리케이션 사용자들은 즉시 업데이트해야 합니다.\n\n사건의 경과는 다음과 같습니다. 2025년 2월 27일에 취약점이 보고되었고, 3월 14일에는 Next.js 15.x와 14.x에 대한 패치가 출시되었습니다. 3월 18일에는 15.2.3과 14.2.25 버전이 출시되었고, 같은 날 CVE-2025-29927이 발급되었으며, 3월 21일에는 보안 권고가 발표되었습니다.\n\n이번 취약점은 요청이 중요한 검사를 우회할 수 있는 헤더와 관련이 있습니다. 이는 인증과 같은 보안에 영향을 미칠 수 있습니다.\n\n영향을 받는 대상은 미들웨어를 사용하는 자가 호스팅된 Next.js 애플리케이션과 인증 또는 보안 검사를 위해 미들웨어에 의존하는 애플리케이션입니다. 반면, Vercel이나 Netlify에 호스팅된 애플리케이션과 미들웨어가 실행되지 않는 정적 내보내기는 영향을 받지 않습니다.\n\n이번 문제는 Next.js 15.2.3, 14.2.25, 13.5.9, 12.3.5 버전에서 패치되었습니다. 만약 업데이트가 불가능하다면, 특정 헤더를 포함한 요청이 애플리케이션에 도달하지 않도록 차단하는 것이 권장됩니다.\n\nNext.js는 보안 문제를 적극적으로 해결하고 있으며, 파트너와의 원활한 소통을 위해 메일링 리스트를 만들고 있습니다. 관심 있는 분들은 partners@nextjs.org로 연락하여 가입할 수 있습니다.",
      "ja": "Next.jsは、セキュリティの脆弱性（CVE-2025-29927）を修正するために、バージョン15.2.3をリリースしました。「standalone」出力を使用して「next start」で自己ホストされたNext.jsアプリケーションのユーザーは、すぐに更新することが推奨されています。\n\nこの脆弱性は2025年2月27日に報告され、Next.jsのバージョン15.xおよび14.x向けのパッチが3月14日にリリースされました。続いて、3月18日にバージョン15.2.3と14.2.25が発表され、同日にCVE-2025-29927が発行されました。3月21日にはセキュリティに関するアドバイザリーも発表されています。\n\n問題の内容は、特定のヘッダーが重要なチェック、例えば認証をバイパスする可能性があることです。これにより、セキュリティに影響を与える恐れがあります。\n\n影響を受けるのは、ミドルウェアを使用している自己ホストされたNext.jsアプリケーションや、認証やセキュリティチェックにミドルウェアを依存しているアプリケーションです。一方、VercelやNetlifyでホストされているアプリケーションや、ミドルウェアが実行されない静的エクスポートには影響がありません。\n\nこの問題に対するパッチは、Next.jsのバージョン15.2.3、14.2.25、13.5.9、12.3.5で適用されています。更新が難しい場合は、特定のヘッダーを含むリクエストがアプリケーションに到達しないようにブロックすることが推奨されています。\n\nNext.jsはセキュリティ問題に対して積極的に対応しており、パートナーとのコミュニケーションを改善するためのメーリングリストを設立しています。興味のある方はpartners@nextjs.orgまで連絡することで参加できます。"
    }
  },
  {
    "id": "b01e1938a4e4ad6a",
    "title": {
      "en": "Andromeda XXXV: The Faintest Dwarf Satellite of the Andromeda Galaxy",
      "ko": "안드로메다의 미세 왜소위성",
      "ja": "アンドロメダの小星"
    },
    "type": "story",
    "url": "https://iopscience.iop.org/article/10.3847/2041-8213/adb433",
    "score": 42,
    "by": "PaulHoule",
    "time": 1742449604,
    "content": "The American Astronomical Society (AAS), established in 1899 and based in Washington, DC, is the major organization of professional astronomers in North America. Its membership of about 7,000 individuals also includes physicists, mathematicians, geologists, engineers, and others whose research and educational interests lie within the broad spectrum of subjects comprising contemporary astronomy. The mission of the AAS is to enhance and share humanity's scientific understanding of the universe.\n\n                The Institute of Physics (IOP) is a leading scientific society promoting physics and bringing physicists together for the benefit of all. It has a worldwide membership of around 50 000 comprising physicists from all sectors, as well as those with an interest in physics. It works to advance physics research, application and education; and engages with policy makers and the public to develop awareness and understanding of physics. Its publishing company, IOP Publishing, is a world leader in professional scientific communications.\n            A publishing partnershipThe following article is Open accessAndromeda XXXV: The Faintest Dwarf Satellite of the Andromeda GalaxyJose Marco Arias, Eric F. Bell, Katya Gozman, In Sung Jang, Saxon Stockton, Oleg Y. Gnedin, Richard D'Souza, Antonela Monachesi, Jeremy Bailin, David Nidever, and Roelof S. de JongShow full author listPublished 2025 March 11 •\n       \t  © 2025. The Author(s). Published by the American Astronomical Society.\n        The Astrophysical Journal Letters,\n\tVolume 982,\nNumber 1Citation Jose Marco Arias et al 2025 ApJL 982 L3DOI 10.3847/2041-8213/adb433 Download Article PDFDownloadArticle ePub\n            You need an eReader or compatible software to experience the benefits of the ePub3 file format.Authors\n                                    Jose Marco AriasAFFILIATIONSDepartment of Astronomy, University of Michigan, 1085 S. University Ave., Ann Arbor, MI 48109-1107, USA; joarias@umich.eduEMAILjoarias@umich.eduhttps://orcid.org/0009-0002-9085-5928Eric F. BellAFFILIATIONSDepartment of Astronomy, University of Michigan, 1085 S. University Ave., Ann Arbor, MI 48109-1107, USA; joarias@umich.eduhttps://orcid.org/0000-0002-5564-9873Katya GozmanAFFILIATIONSDepartment of Astronomy, University of Michigan, 1085 S. University Ave., Ann Arbor, MI 48109-1107, USA; joarias@umich.eduhttps://orcid.org/0000-0003-2294-4187In Sung JangAFFILIATIONSDepartment of Astronomy and Astrophysics, University of Chicago, Chicago, IL 60637, USAhttps://orcid.org/0000-0002-2502-0070Saxon StocktonAFFILIATIONSDepartment of Physics, Utah Valley University, 800 W. University Pkwy., Orem, UT 84058, USAhttps://orcid.org/0000-0003-1677-0213Oleg Y. GnedinAFFILIATIONSDepartment of Astronomy, University of Michigan, 1085 S. University Ave., Ann Arbor, MI 48109-1107, USA; joarias@umich.eduhttps://orcid.org/0000-0001-9852-9954Richard D'SouzaAFFILIATIONSVatican Observatory, Specola Vaticana, V-00120, Vatican City Statehttps://orcid.org/0000-0001-9269-8167Antonela MonachesiAFFILIATIONSDepartamento de Astronomía, Universidad de La Serena, Avda. Rául Bitrán 1305, La Serena, Chilehttps://orcid.org/0000-0003-2325-9616Jeremy BailinAFFILIATIONSDepartment of Physics and Astronomy, University of Alabama, Box 870324, Tuscaloosa, AL 35487-0324, USAhttps://orcid.org/0000-0001-6380-010XDavid NideverAFFILIATIONSDepartment of Physics, Montana State University, P.O. Box 173840, Bozeman, MT 59717-3840, USAhttps://orcid.org/0000-0002-1793-3689Roelof S. de JongAFFILIATIONSLeibniz-Institut für Astrophysik Potsdam (AIP), An der Sternwarte 16, 14482 Potsdam, Germanyhttps://orcid.org/0000-0001-6982-4081\n                                FiguresSkip to each figure in the articleTablesSkip to each table in the articleReferencesCitationsArticle dataSkip to each data item in the articleWhat\n                                        is article data?Open science Download Article PDFDownloadArticle ePub\n            Authors\n                    Jose Marco AriasAFFILIATIONSDepartment of Astronomy, University of Michigan, 1085 S. University Ave., Ann Arbor, MI 48109-1107, USA; joarias@umich.eduEMAILjoarias@umich.eduhttps://orcid.org/0009-0002-9085-5928Eric F. BellAFFILIATIONSDepartment of Astronomy, University of Michigan, 1085 S. University Ave., Ann Arbor, MI 48109-1107, USA; joarias@umich.eduhttps://orcid.org/0000-0002-5564-9873Katya GozmanAFFILIATIONSDepartment of Astronomy, University of Michigan, 1085 S. University Ave., Ann Arbor, MI 48109-1107, USA; joarias@umich.eduhttps://orcid.org/0000-0003-2294-4187In Sung JangAFFILIATIONSDepartment of Astronomy and Astrophysics, University of Chicago, Chicago, IL 60637, USAhttps://orcid.org/0000-0002-2502-0070Saxon StocktonAFFILIATIONSDepartment of Physics, Utah Valley University, 800 W. University Pkwy., Orem, UT 84058, USAhttps://orcid.org/0000-0003-1677-0213Oleg Y. GnedinAFFILIATIONSDepartment of Astronomy, University of Michigan, 1085 S. University Ave., Ann Arbor, MI 48109-1107, USA; joarias@umich.eduhttps://orcid.org/0000-0001-9852-9954Richard D'SouzaAFFILIATIONSVatican Observatory, Specola Vaticana, V-00120, Vatican City Statehttps://orcid.org/0000-0001-9269-8167Antonela MonachesiAFFILIATIONSDepartamento de Astronomía, Universidad de La Serena, Avda. Rául Bitrán 1305, La Serena, Chilehttps://orcid.org/0000-0003-2325-9616Jeremy BailinAFFILIATIONSDepartment of Physics and Astronomy, University of Alabama, Box 870324, Tuscaloosa, AL 35487-0324, USAhttps://orcid.org/0000-0001-6380-010XDavid NideverAFFILIATIONSDepartment of Physics, Montana State University, P.O. Box 173840, Bozeman, MT 59717-3840, USAhttps://orcid.org/0000-0002-1793-3689Roelof S. de JongAFFILIATIONSLeibniz-Institut für Astrophysik Potsdam (AIP), An der Sternwarte 16, 14482 Potsdam, Germanyhttps://orcid.org/0000-0001-6982-4081\n                Article metrics5375 Total downloads0 Video abstract viewsShare this articleArticle informationDatesReceived 2024 October 30 Revised 2025 February 8 Accepted 2025 February 8 Published 2025 March 11 Unified Astronomy Thesaurus conceptsDwarf galaxies; Andromeda GalaxyJournal RSS\n\n               Create or edit your corridor alerts\n\n                        Corridor alerts\n\n                            Receive alerts on all new research papers in American Astronomical Society\n                            (AAS) journals as soon as they are published.\n                            Select your desired journals and corridors below. You will need to select a minimum of one corridor.\n\n                                Corridors\n\n                                Journals\n\n                                Please note, The Planetary Science Journal (PSJ) does not currently use the corridors.\n\n                            Create alert\n\n                    What are corridors?opens in new tab\n\n    2041-8205/982/1/L3Abstract\n            We present the discovery of Andromeda XXXV, the faintest Andromeda satellite galaxy discovered to date, identified as an overdensity of stars in the Pan-Andromeda Archaeological Survey and confirmed via Hubble Space Telescope imaging. Located at a heliocentric distance of 927−63+76 kpc and 158−45+57 kpc from Andromeda, Andromeda XXXV is an extended (rh=53−11+13 pc), elliptical (=0.4±0.2), metal-poor ([Fe/H]∼−1.9) system, and the least luminous (MV=−5.2±0.3) of Andromeda's dwarf satellites discovered so far. Andromeda XXXV's properties are consistent with the known population of dwarf galaxies around the Local Group, bearing close structural resemblance to the Canes Venatici II and Hydra II Milky Way (MW) dwarf satellite galaxies. Its stellar population, characterized by a red horizontal branch or a red clump feature, mirrors that of other Andromeda satellite galaxies in showing evidence for a spread in age and metallicity, with no signs of younger stellar generations. This age–metallicity spread is not observed in MW satellites of comparable stellar mass, ",
    "summary": {
      "en": "The American Astronomical Society (AAS), founded in 1899 and located in Washington, DC, is the main organization for professional astronomers in North America, with around 7,000 members, including physicists and engineers. Its goal is to improve and share our understanding of the universe.\n\nThe Institute of Physics (IOP) is a global scientific society that supports physics and connects physicists. It has about 50,000 members and focuses on advancing physics research, education, and public awareness. IOP Publishing is a well-known leader in scientific communications.\n\nThe recent article titled \"Andromeda XXXV: The Faintest Dwarf Satellite of the Andromeda Galaxy,\" published on March 11, 2025, reveals the discovery of Andromeda XXXV, the faintest known satellite galaxy of Andromeda. It was found through the Pan-Andromeda Archaeological Survey and confirmed by Hubble Space Telescope images. This galaxy has unique features, including a significant distance from Andromeda and characteristics similar to other dwarf galaxies in the Local Group. Its stellar population shows a range of ages and metallicities, differing from similar galaxies around the Milky Way.",
      "ko": "미국 천문학회(AAS)는 1899년에 설립되어 워싱턴 D.C.에 위치한 북미의 전문 천문학자들을 위한 주요 조직입니다. 이 단체는 약 7,000명의 회원을 보유하고 있으며, 물리학자와 엔지니어도 포함됩니다. AAS의 목표는 우주에 대한 이해를 향상시키고 이를 공유하는 것입니다.\n\n물리학 연구소(IOP)는 전 세계의 물리학자들을 연결하고 지원하는 과학 단체입니다. 약 50,000명의 회원을 두고 있으며, 물리학 연구, 교육, 대중 인식을 증진하는 데 중점을 둡니다. IOP 출판사는 과학 커뮤니케이션 분야에서 잘 알려진 선도적인 역할을 하고 있습니다.\n\n2025년 3월 11일에 발표된 \"안드로메다 XXXV: 안드로메다 은하의 가장 희미한 왜소 위성\"이라는 제목의 최근 기사에서는 안드로메다 XXXV라는 가장 희미한 위성 은하의 발견을 다루고 있습니다. 이 은하는 팬-안드로메다 고고학 조사(Pan-Andromeda Archaeological Survey)를 통해 발견되었으며, 허블 우주 망원경의 이미지로 확인되었습니다. 이 은하는 안드로메다와의 상당한 거리와 지역 그룹의 다른 왜소 은하들과 유사한 특성을 지니고 있는 독특한 특징을 가지고 있습니다. 별의 집합체는 다양한 나이와 금속성을 보여주며, 이는 우리 은하 주변의 유사한 은하들과는 다른 모습입니다.",
      "ja": "アメリカ天文学会（AAS）は、1899年に設立され、ワシントンDCに本部を置く北米のプロの天文学者の主要な組織です。約7,000人の会員が在籍しており、物理学者やエンジニアも含まれています。この組織の目的は、宇宙に対する理解を深め、共有することです。\n\n物理学協会（IOP）は、物理学を支援し、物理学者をつなぐ国際的な科学団体です。約50,000人の会員がいて、物理学の研究、教育、一般の認識を進めることに重点を置いています。IOP出版は、科学コミュニケーションの分野で著名なリーダーです。\n\n2025年3月11日に発表された「アンドロメダXXXV：アンドロメダ銀河の最も淡い矮小衛星」という記事では、アンドロメダXXXVというアンドロメダ銀河の最も淡い衛星銀河の発見が報告されています。この銀河は、パン・アンドロメダ考古学調査を通じて発見され、ハッブル宇宙望遠鏡の画像によって確認されました。この銀河は、アンドロメダからの距離が大きく、局所グループの他の矮小銀河と似た特徴を持っています。また、星の集団はさまざまな年齢や金属量を示しており、天の川銀河周辺の類似の銀河とは異なる点が見られます。"
    }
  },
  {
    "id": "cd1710a687ab12df",
    "title": {
      "en": "Show HN: XYMake – Turn Your Posts into LLM-Ready Data",
      "ko": "XYMake: 포스트를 데이터로!",
      "ja": "XYMakeで投稿をデータ化"
    },
    "type": "story",
    "url": "https://xymake.com",
    "score": 7,
    "by": "janwilmake",
    "time": 1742842935,
    "content": "Turn Your\n\n                     Into LLM-Ready Data\n\n                    Unlock your X value by letting your MCP agents and other APIs access all your X posts.\n\n                        x.com/...\n\n                        xymake.com/...\n\n                        Just add \"ymake\" to any X URL and instantly get your content in formats that LLMs can use\n\n                        Unlock Your Feed\n\n                        View on GitHub\n\n                    Unlocking your feed will make all your X posts public to anyone. Read the Terms and\n                        Conditions\n\n                    Repurpose & Sell\n\n                        Turn your X threads into sellable content and digital products\n\n                    One-Click Access\n\n                        No complex tools or coding. Just change the URL and get instant markdown format.\n\n                    LLM-Ready Format\n\n                        Perfect for AI tools such as ChatGPT and MCP (Cursor, Claude, etc.)\n\n                See It In Action\n\n                        https://x.com/janwilmake/status/1904080559422288062\n\n                        https://xymake.com/janwilmake/status/1904080559422288062\n\n                        Try It Now\n\n                    Unlock Your X Gold Mine\n\n                        Your X content has untapped value. XYMake gives you the key to unlock it.\n\n                            Free My X\n\n                            View on GitHub\n\n                        By authorizing XYMake you agree to our Terms\n                            and Conditions\n\n                    © 2025 XYMake. All rights reserved. Not affiliated with X Corp.",
    "summary": {
      "en": "### Summary\n\nUnlock the value of your X posts by using XYMake, which allows your content to be accessed in formats suitable for large language models (LLMs). Simply add \"ymake\" to any X URL to convert your posts into a markdown format that AI tools like ChatGPT can use.\n\nKey points:\n- **Easy Access**: No technical skills required; just modify the URL.\n- **Public Content**: Unlocking your feed makes your posts public.\n- **Monetization**: Transform your X threads into sellable digital products.\n- **Untapped Potential**: Your X content has valuable uses that XYMake can help you discover.\n\nFor more details, view it on GitHub and check the terms and conditions.",
      "ko": "XYMake를 사용하면 X 게시물의 가치를 극대화할 수 있습니다. 이 도구는 대형 언어 모델(LLM)에 적합한 형식으로 콘텐츠에 접근할 수 있게 해줍니다. X URL에 \"ymake\"를 추가하기만 하면, 게시물이 AI 도구인 ChatGPT와 같은 프로그램에서 사용할 수 있는 마크다운 형식으로 변환됩니다.\n\n이 서비스는 기술적인 지식이 필요하지 않아 누구나 쉽게 이용할 수 있습니다. URL을 수정하는 것만으로 간편하게 접근할 수 있습니다. 또한, 피드를 공개하면 게시물이 일반에 공개되며, X 스레드를 판매 가능한 디지털 제품으로 변환하여 수익을 창출할 수 있습니다. XYMake는 여러분의 X 콘텐츠가 가지고 있는 잠재력을 발견하는 데 도움을 줄 수 있습니다.\n\n자세한 내용은 GitHub에서 확인하고 이용 약관을 참고하세요.",
      "ja": "あなたのX投稿の価値を引き出すために、XYMakeを利用しましょう。このツールを使うと、あなたのコンテンツを大規模言語モデル（LLM）に適した形式でアクセスできるようになります。XのURLに「ymake」を追加するだけで、あなたの投稿をAIツール、例えばChatGPTが利用できるマークダウン形式に変換できます。\n\nこのサービスのポイントは、技術的なスキルが不要で、URLを変更するだけで簡単にアクセスできることです。また、フィードを公開することで、あなたの投稿が一般に見られるようになります。さらに、Xのスレッドを販売可能なデジタル商品に変えることもできます。あなたのXコンテンツには、XYMakeが発見を手助けできる貴重な利用方法が隠れています。\n\n詳細については、GitHubで確認し、利用規約をチェックしてください。"
    }
  },
  {
    "id": "01a61a7f5361a371",
    "title": {
      "en": "The Trump administration accidentally texted me its war plans",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.theatlantic.com/politics/archive/2025/03/trump-administration-accidentally-texted-me-its-war-plans/682151/",
    "score": 988,
    "by": "_tk_",
    "time": 1742833435,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "6a7e6bf13f3bfc71",
    "title": {
      "en": "Show HN: Tascli, a simple CLI task and record manager",
      "ko": "태스크리: 간편한 CLI 작업 관리기",
      "ja": "タスク管理CLI「Tascli」"
    },
    "type": "story",
    "url": "https://crates.io/crates/tascli",
    "score": 13,
    "by": "Aperocky",
    "time": 1742841176,
    "content": "tascli\nA simple CLI tool for tracking tasks and records from terminal.\nInstallation:\ncargo install tascli\n\nBasic Usage\nTasks and records are stored in ~/.local/share/tascli/tascli.db with rusqlite.\nTasks\nCreate tasks with deadlines:\n# Basic tasks\ntascli task \"Create readme\" today\ntascli task \"Publish package\" tomorrow\ntascli task \"Do taxes\" 4/15\n\n# With category\ntascli task -c work \"Read emails\" week\n\nList tasks:\n# List active tasks\n$ tascli list task\n\noutput:\nTask List:\n----------------------------------------------------------------------------------------------\n| Index  | Category            | Content                               | Deadline            |\n----------------------------------------------------------------------------------------------\n| 1      | life                | Unpack the crib                       | Today               |\n----------------------------------------------------------------------------------------------\n| 2      | tascli              | Add pagination capability for tascli  | Sunday              |\n|        |                     | list actions                          |                     |\n----------------------------------------------------------------------------------------------\n| 3      | tascli              | Add readme section on timestring      | Sunday              |\n|        |                     | format                                |                     |\n----------------------------------------------------------------------------------------------\n| 4      | life                | Do state taxes                        | Sunday              |\n----------------------------------------------------------------------------------------------\n| 5      | tascli              | Sort list output by time instead of   | Sunday              |\n|        |                     | internal id                           |                     |\n----------------------------------------------------------------------------------------------\n| 6      | tascli              | Fix length issue for unicode chars    | Sunday              |\n----------------------------------------------------------------------------------------------\n| 7      | life                | Two month pictures - follow the lead  | 4/23                |\n|        |                     | from the previous one month pictures  |                     |\n----------------------------------------------------------------------------------------------\n\nComplete tasks:\n# Mark index 1 as done\ntascli done 1\n\nList all tasks in tascli category (including completed)\ntascli list task -s all -c tascli\n\noutput:\nTask List:\n----------------------------------------------------------------------------------------------\n| Index  | Category            | Content                               | Deadline            |\n----------------------------------------------------------------------------------------------\n| 1      | tascli              | Add a tascli command to delete a row  | Today (completed)   |\n|        |                     | in the task or record table           |                     |\n----------------------------------------------------------------------------------------------\n| 2      | tascli              | Fix addition and modification commands| Today (completed)   |\n|        |                     | output to have N/A for index          |                     |\n----------------------------------------------------------------------------------------------\n| 3      | tascli              | Insert guardrail against accidental   | Today (completed)   |\n|        |                     | valid syntax like 'task list' that is |                     |\n|        |                     | mistakenly made                       |                     |\n----------------------------------------------------------------------------------------------\n| 4      | tascli              | Create a gif for readme               | Today (completed)   |\n----------------------------------------------------------------------------------------------\n| 5      | tascli              | Add pagination capability for tascli  | Sunday              |\n|        |                     | list actions                          |                     |\n----------------------------------------------------------------------------------------------\n| 6      | tascli              | Add readme section on timestring      | Sunday              |\n|        |                     | format                                |                     |\n----------------------------------------------------------------------------------------------\n\nRecords\nCreate records (for tracking events):\n# With current time\ntascli record -c feeding \"100ML\"\n\n# With specific time\ntascli record -c feeding -t 11:20AM \"100ML\"\n\nList records:\n# -d 1 stand for only get last 1 day of record\ntascli list record -d 1\n\noutput:\nRecords List:\n----------------------------------------------------------------------------------------------\n| Index  | Category            | Content                               | Created At          |\n----------------------------------------------------------------------------------------------\n| 1      | feeding             | 110ML                                 | Today 1:00AM        |\n----------------------------------------------------------------------------------------------\n| 2      | feeding             | breastfeeding                         | Today 4:10AM        |\n----------------------------------------------------------------------------------------------\n| 3      | feeding             | 100ML                                 | Today 7:30AM        |\n----------------------------------------------------------------------------------------------\n| 4      | feeding             | 110ML                                 | Today 11:20AM       |\n----------------------------------------------------------------------------------------------\n\nTime Format\nThis application accepts flexible time strings in various formats:\n\nSimple dates: today, tomorrow, yesterday, friday, eom (end of month), eoy (end of year)\nDate formats: YYYY-MM-DD, MM/DD/YYYY, MM/DD (current year)\nTime formats: HH:MM, 3:00PM, 3PM\nCombined: 2025-03-24 15:30, tomorrow 3PM\n\nWhen only a date is provided, the time defaults to end of day (23:59:59). When only a time is provided, the date defaults to today.\nHelp\ntascli uses clap for argument parsing, use --help to get help on all levels of this cli:\naperocky@~$ tascli -h\nUsage: tascli <COMMAND>\n\nCommands:\n  task    add task with end time\n  record  add record\n  done    Finish task or remove records\n  update  Update tasks or records wording/deadlines\n  list    list tasks or records\n  help    Print this message or the help of the given subcommand(s)\n\nOptions:\n  -h, --help     Print help\n  -V, --version  Print version\naperocky@~$ tascli task -h\nadd task with end time\n\nUsage: tascli task [OPTIONS] <CONTENT> [TIMESTR]\n\nArguments:\n  <CONTENT>  Description of the task\n  [TIMESTR]  Time the task is due, default to EOD\n\nOptions:\n  -c, --category <CATEGORY>  Category of the task\n  -h, --help                 Print help",
    "summary": {
      "en": "**Summary of tascli**\n\n**Overview:**\nTascli is a command-line tool for managing tasks and records from your terminal.\n\n**Installation:**\nYou can install it using the command:\n```\ncargo install tascli\n```\n\n**Basic Usage:**\n\n- **Task Management:**\n  - Create tasks with deadlines:\n    - Example: `tascli task \"Create readme\" today`\n  - List active tasks:\n    ```\n    tascli list task\n    ```\n  - Mark a task as complete:\n    - Example: `tascli done 1`\n\n- **Records Management:**\n  - Create records to track events:\n    - Example: `tascli record -c feeding \"100ML\"`\n  - List records from the last day:\n    ```\n    tascli list record -d 1\n    ```\n\n**Time Formats:**\nTascli accepts various time formats, such as:\n- Simple dates: today, tomorrow\n- Date formats: YYYY-MM-DD, MM/DD/YYYY\n- Time formats: HH:MM, 3:00PM\n- Combined formats: 2025-03-24 15:30\n\n**Help:**\nFor help with commands or options, use `tascli --help`. Key commands include:\n- `task`: Add a task\n- `record`: Add a record\n- `done`: Mark a task as complete or remove records\n- `list`: List tasks or records\n\nThis tool is useful for organizing and tracking tasks and events efficiently from the command line.",
      "ko": "타스클리(Tascli)는 터미널에서 작업과 기록을 관리하기 위한 명령줄 도구입니다. \n\n설치는 다음 명령어를 사용하여 간단하게 할 수 있습니다. cargo install tascli\n\n기본 사용법은 다음과 같습니다. 작업 관리를 위해 마감일이 있는 작업을 생성할 수 있습니다. 예를 들어, `tascli task \"README 작성\" 오늘`과 같이 입력하면 됩니다. 현재 활성화된 작업 목록을 보려면 `tascli list task`를 입력하면 됩니다. 작업을 완료로 표시하려면 `tascli done 1`과 같이 입력하면 됩니다.\n\n기록 관리를 위해 이벤트를 추적하는 기록을 생성할 수 있습니다. 예를 들어, `tascli record -c feeding \"100ML\"`와 같이 입력하면 됩니다. 지난 하루 동안의 기록을 보려면 `tascli list record -d 1`을 사용합니다.\n\n타스클리는 다양한 시간 형식을 지원합니다. 간단한 날짜로는 오늘, 내일이 있으며, 날짜 형식으로는 YYYY-MM-DD 또는 MM/DD/YYYY가 있습니다. 시간 형식으로는 HH:MM 또는 오후 3시와 같은 형식이 있으며, 결합된 형식으로는 2025-03-24 15:30과 같은 형식이 가능합니다.\n\n명령어나 옵션에 대한 도움말이 필요하면 `tascli --help`를 입력하면 됩니다. 주요 명령어로는 작업 추가를 위한 `task`, 기록 추가를 위한 `record`, 작업 완료 표시 또는 기록 삭제를 위한 `done`, 작업이나 기록 목록을 보기 위한 `list`가 있습니다. 이 도구는 명령줄에서 작업과 이벤트를 효율적으로 정리하고 추적하는 데 유용합니다.",
      "ja": "Tascliは、ターミナルからタスクや記録を管理するためのコマンドラインツールです。\n\nインストールは、以下のコマンドを使用して行います。  \ncargo install tascli\n\n基本的な使い方として、タスク管理があります。期限付きのタスクを作成するには、次のように入力します。  \n例：tascli task \"Create readme\" today  \nアクティブなタスクを一覧表示するには、次のコマンドを使います。  \ntascli list task  \nタスクを完了としてマークするには、次のように入力します。  \n例：tascli done 1\n\n記録管理も可能です。イベントを追跡するための記録を作成するには、次のようにします。  \n例：tascli record -c feeding \"100ML\"  \n過去1日の記録を一覧表示するには、次のコマンドを使用します。  \ntascli list record -d 1\n\nTascliは、さまざまな時間形式を受け付けています。例えば、シンプルな日付として「today」や「tomorrow」、日付形式として「YYYY-MM-DD」や「MM/DD/YYYY」、時間形式として「HH:MM」や「3:00PM」、組み合わせた形式として「2025-03-24 15:30」などがあります。\n\nコマンドやオプションについてのヘルプが必要な場合は、tascli --helpを使用してください。主なコマンドには、タスクを追加する「task」、記録を追加する「record」、タスクを完了としてマークするまたは記録を削除する「done」、タスクや記録を一覧表示する「list」が含まれます。\n\nこのツールは、コマンドラインから効率的にタスクやイベントを整理し、追跡するのに役立ちます。"
    }
  },
  {
    "id": "1e51268ed4d16a02",
    "title": {
      "en": "Quitting an Intel x86 Hypervisor",
      "ko": "인텔 하이퍼바이저 탈출",
      "ja": "インテル仮想化脱出"
    },
    "type": "story",
    "url": "https://halobates.de/blog/p/446",
    "score": 119,
    "by": "todsacerdoti",
    "time": 1742676124,
    "content": "Quitting an Intel x86 hypervisor\nwithout comments\n\nThis is an esoteric topic that might be of interest to people implementing Intel hypervisors. It assumes you know the basics of the Intel virtualization architecture, see Hypervisor from scratch for a tutorial. The actual full VT architecture is described in Volume 3 of the Intel SDMLet’s say we write an x86 hypervisor that starts in the UEFI environment and virtualizes the initialization phase of an OS. But the hypervisor wants to eventually quit itself to not cause extra overhead during OS run time.The way the hypervisor works is that it runs in its own memory and with its own page tables which are switched atomically on every VM exit by the VT-x implementation. This way it is isolated from the main OS.At some vm exit with the hypervisor running in its own context it decides that it is not needed anymore and wants to quit. To disable VT support the VMXOFF instruction can be used. But what we really need is an atomic VMXOFF + switch to the original OS page tables plus a jump, and all that without using any registers which need to be already restored to the original state of the OS. One trick is to use the MOV to CR3 instruction that reloads the page table as a jump. As soon as the page table is reloaded the CPU will fetch the next instruction with the translations from the freshly loaded page table, so we can transfer execution to the guest context. However to do that the MOV CR3 needs to be located just before the page offset of the target instruction. This can be done by copying a trampoline to the right page offset (potentially overlapping into the previous page). The trampoline is located in a special transfer page table mapping that places writable code pages overlapping the target mapping.But there are some complications. The hypervisor also needs to load the segmentation state (like GDT/LDT/IDT) of the guest. In theory they could just be loaded by mapping these guest pages into the transfer mapping and loading them before the transfer.  But what happens if the GDT/LDT/IDT is on the same page as the target address? This is common in real OS’ assembler startup code which is implemented in a small assembler file without any page separation between code and data. One option would be to copy them to the transfer page too and load it there, or the hypervisor first copies them to a temporary buffer and loads it from there. In the second option the base addresses of these structures will be incorrect, but in practice you can often rely on them getting reloaded eventually anyways.Another problem is the register state of the target. MOV to CR3 needs a register as the source of the reload, and it needs to be the last instruction of the trampoline. So it is impossible to restore the register it uses. But remember the hypervisor is doing this as the result of a VM exit. If we chose an exit for a condition that already clobbers a register we can use the same register for the reload and the next instruction executed in the original target (and which caused the exit originally) will just overwrite it again.\n\nA very convenient instruction for this is CPUID. It is executed multiple times in OS startup and overwrites multiple registers. In fact VMX always intercepts CPUID so it has to handle these exits in any case. So the trick to quit an hypervisor is to wait for the next CPUID exit and then use one of the registers clobbered by CPUID for the final CR3 reload.  This will have inconsistent register state for one instruction in the target, but unless the original OS is currently running a debugger it will never notice. In principle any exit as a result of an instruction that clobbers a register can be used for this.There is another potential complication if the target address of the OS conflicts with where the hypervisor is running before entering the transfer mapping. The transfer mapping needs to map the original code so that it can be jumped to. This could be solved with a third auxiliary mapping that is used before jumping to the transfer trampoline. In practice it doesn’t seem to be a problem because x86 OS typically run in a 1:1 mapping for startup, and that cannot conflict with the 1:1 mapping used by  UEFI programs as our hypervisor.\n\nHappy hypervisor hacking!\n\n    Written by therapsid\n    March 18th, 2025 at 9:34 pm\n\n    Posted in curiosities,kernel\n\n    « The browser circle closes again\n\nLeave a Reply\n\nName\n\nMail (will not be published)\n\nWebsite\n\nΔdocument.getElementById( \"ak_js_1\" ).setAttribute( \"value\", ( new Date() ).getTime() );",
    "summary": {
      "en": "This text discusses how to properly quit an Intel x86 hypervisor without causing issues during the operation of an operating system (OS). \n\nKey points include:\n\n1. **Context**: The hypervisor runs in its own memory and uses its own page tables to remain isolated from the main OS.\n\n2. **Quitting the Hypervisor**: To exit the hypervisor, the VMXOFF instruction is used, but it must be done in a way that seamlessly switches back to the OS. This requires an atomic operation that includes changing page tables and jumping to the OS code without interfering with registers.\n\n3. **Using MOV to CR3**: The instruction MOV to CR3 can help with this transition by reloading the page table and allowing the OS to take over. Special care is needed to ensure the instruction is correctly placed in memory.\n\n4. **Complications**: There are complications regarding loading the guest OS's segmentation state (like GDT/LDT/IDT) and managing register states, especially if the target address overlaps with the hypervisor's memory.\n\n5. **Utilizing CPUID**: A practical strategy is to wait for a CPUID instruction exit, which is commonly intercepted by the hypervisor. This allows the hypervisor to use a register already clobbered by CPUID for the transition.\n\n6. **Overall Solution**: While there are potential mapping conflicts, typical x86 OS setups reduce the likelihood of issues, making this approach feasible.\n\nThe article concludes with encouragement for developers working on hypervisors.",
      "ko": "이 글에서는 Intel x86 하이퍼바이저를 올바르게 종료하는 방법에 대해 설명합니다. 하이퍼바이저를 종료할 때 운영 체제(OS)의 작동에 문제가 발생하지 않도록 하는 것이 중요합니다.\n\n하이퍼바이저는 자체 메모리에서 실행되며, 주 운영 체제와 격리된 상태를 유지하기 위해 자신의 페이지 테이블을 사용합니다. 하이퍼바이저를 종료하기 위해서는 VMXOFF 명령어를 사용해야 하지만, 이 과정에서 운영 체제로 매끄럽게 전환해야 합니다. 이를 위해서는 페이지 테이블을 변경하고 레지스터에 영향을 주지 않으면서 운영 체제 코드로 점프하는 원자적 작업이 필요합니다.\n\nMOV to CR3 명령어는 페이지 테이블을 재로드하여 운영 체제가 제어를 다시 가져올 수 있도록 도와줍니다. 이 명령어가 메모리에 올바르게 배치되도록 특별한 주의가 필요합니다. 게스트 운영 체제의 세그먼트 상태(GDT, LDT, IDT 등)를 로드하고 레지스터 상태를 관리하는 데 복잡한 문제가 발생할 수 있습니다. 특히 목표 주소가 하이퍼바이저의 메모리와 겹칠 경우 더욱 그렇습니다.\n\n실용적인 전략으로는 CPUID 명령어의 종료를 기다리는 것이 있습니다. 이 명령어는 하이퍼바이저에 의해 일반적으로 가로채어지므로, 하이퍼바이저는 CPUID로 인해 이미 변경된 레지스터를 사용하여 전환할 수 있습니다. 전반적으로, 잠재적인 매핑 충돌이 있을 수 있지만, 일반적인 x86 운영 체제 설정은 문제 발생 가능성을 줄여주므로 이 접근 방식이 실행 가능하다는 점이 강조됩니다.\n\n하이퍼바이저 개발자들에게는 이러한 내용을 바탕으로 작업을 이어가기를 격려합니다.",
      "ja": "このテキストでは、Intel x86ハイパーバイザーを適切に終了させる方法について説明しています。これにより、オペレーティングシステム（OS）の運用に問題を引き起こさないようにします。\n\nまず、ハイパーバイザーは独自のメモリ内で動作し、メインのOSから隔離されるために独自のページテーブルを使用します。ハイパーバイザーを終了させるには、VMXOFF命令を使用しますが、OSにスムーズに戻る方法で行う必要があります。これには、ページテーブルを変更し、レジスタに干渉せずにOSのコードにジャンプする原子的な操作が求められます。\n\nMOV命令をCR3に使用することで、この移行を助けることができます。これにより、ページテーブルが再読み込みされ、OSが引き継ぐことが可能になります。ただし、この命令がメモリ内に正しく配置されていることを確認するために特別な注意が必要です。\n\nゲストOSのセグメンテーション状態（GDT、LDT、IDTなど）を読み込む際や、レジスタの状態を管理する際には複雑な問題が生じることがあります。特に、ターゲットアドレスがハイパーバイザーのメモリと重なる場合には注意が必要です。\n\n実用的な戦略として、CPUID命令の終了を待つ方法があります。これはハイパーバイザーによって一般的にインターセプトされます。この方法により、ハイパーバイザーはCPUIDによってすでに変更されたレジスタを使用して移行を行うことができます。\n\n全体的に見て、マッピングの競合が発生する可能性はありますが、一般的なx86 OSの設定では問題が発生する可能性が低く、このアプローチは実行可能です。ハイパーバイザーの開発に取り組む開発者に対して、励ましの言葉が送られています。"
    }
  },
  {
    "id": "56e5a0c00e5785e6",
    "title": {
      "en": "A Brief History of the Miracle Bacterium",
      "ko": "기적의 세균 역사",
      "ja": "奇跡の細菌の歴史"
    },
    "type": "story",
    "url": "https://www.asimov.press/p/miracle-bacterium",
    "score": 68,
    "by": "mailyk",
    "time": 1742745552,
    "content": "Share this postAsimov PressA Brief History of the Miracle BacteriumCopy linkFacebookEmailNotesMoreDiscover more from Asimov PressScience and technology for a flourishing future. Website: press.asimov.comSubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.Already have an account? Sign inThe Long ReadA Brief History of the Miracle BacteriumSerratia marcescens, a pathogen with an uncanny resemblance to blood, has had an outsized influence on modern science.Corrado NaiMar 24, 202520Share this postAsimov PressA Brief History of the Miracle BacteriumCopy linkFacebookEmailNotesMore13ShareArticle voiceover1×0:00-24:31Audio playback is not supported on your browser. Please upgrade.At 1:15 p.m. on Monday, August 8th, 1904, a British physician named M. H. Gordon took some soil he had “richly impregnated with a living emulsion” of the virulent bacterium,1 Serratia marcescens, and sprinkled it near a lamp post in front of the U.K. House of Commons. Gordon knew that was the exact spot Members of Parliament had to cross before their 2 p.m. session. His hope was for the politicians to step on the contaminated ground and spread the bacteria inside the Debating Chamber.Gordon wasn’t executing a terrorist attack. Rather, he had been appointed by a committee to study how germs spread inside the House of Commons following an outbreak of influenza among its members. Gordon had chosen Serratia marcescens because the bacterium forms easily recognizable red colonies. For his experiment, he placed numerous open Petri dishes inside the Debating Chamber on which the colonies could grow. Gordon’s idea was simple: politicians’ boots would carry contaminated soil and spread Serratia marcescens into the building; he would go away with his Petri dishes and reveal colonies of the bacterium to point at faults in the House of Commons’ ventilation system.But despite placing hundreds of Petri dishes around the chamber, Gordon wasn’t able to retrieve more than a handful of colonies. Suspecting the microbe might not spread easily by means of boots, he followed this up with a theatrical experiment: Inside the Debating Chamber, he gargled a suspension of Serratia marcescens and recited Shakespeare’s “King Henry V” and “Julius Caesar” for one hour to no one but an audience of open Petri dishes. This time, copious colonies of Serratia marcescens appeared, leading him to conclude that speech can transmit microbes as far as 70 feet (21 meters) away.2Indeed, Serratia marcescens’ vivid blood-red color has prompted its use in a wide range of experiments that have increased our understanding of how germs disperse within human bodies, buildings, and populations. Sightings of the striking microbe outside the lab have awakened both fear and awe in the general population.The awareness that certain strains of Serratia marcescens can cause severe harm to humans — counter-intuitively, the paler varieties are most dangerous — only became evident decades after Gordon’s investigations. Before then, hospitals deliberately sprayed Serratia marcescens inside their facilities to investigate microbial dispersion, and laboratory handbooks demonstrated transmission by handshake by having students coat their fingers in the microbe. While greater awareness of its dangers eventually led to its discontinuation in tracing experiments, Serratia marcescens remains an important subject of biomedical research.Its scientific journey began over 200 years ago with a bloody polenta.Subscribe to Asimov Press.SubscribeBlood MasqueradeWhen growing on solid substances, like foodstuffs or Petri dishes filled with agar, Serratia marcescens forms red colonies reminiscent of blood, earning it the informal moniker “masquerader of blood.” Its appearance on the polenta of a wealthy Paduan farmer during a particularly hot and humid summer in 1819 — ideal conditions for its flourishing — prompted the flurry of investigation that gave it its scientific name.Serratia marcescens looks like blood droplets when grown at room temperature on solid media. Mature colonies are mucilaginous, viscous, and tinged bright red or pink with an \"uncanny resemblance to blood.” Credit: Benutzer:BrudersohnAfter the farmer, Antonio Pittarello, discovered the red-spotted food in his house near Padua, the resultant hubbub of curiosity seekers and the spiritually appalled so disturbed the neighborhood that the local District Commissioner appointed Dr. Vicenze Sette (a physician, surgeon, and district health officer) to investigate the discovery. Also taking an interest in the mysterious “bloody polenta” was Bartolomeo Bizio (1791-1862), a young pharmacy student who joined the investigation on his own. In the meantime, red spots started to appear on food in hundreds of homes throughout Pittarello’s village as well as in villages close by.Through a series of ingenious experiments, Sette and Bizio reproduced the red spots on fresh polenta independently in their own home laboratories. Both concluded (erroneously) that the cause was a microscopic fungus.3 Sette called it Zaogalactina imetrofa (from the Latin, “slime living on food”); Bizio called it Serratia4 to honor an Italian Benedictine monk and physicist, Serafino Serrati (whom he felt had not gotten adequate credit for his contributions to the invention of the steamboat) and marcescens (from the Latin, “to decay”) since, much to his disappointment, the microbe’s red color fades easily.Racing to gain acclaim for identifying the source of the “bloody polenta” before Sette, Bizio made two deft moves. First, by citing Lazzaro Spallanzani, a pioneering biologist known for having worked to disprove the theory of spontaneous generation, he hoped to lend credibility to his findings; indeed, by demonstrating a biological origin for the “bloody polenta,” Bizio again refuted spontaneous generation decades before Louis Pasteur. And second, Bizio quickly published his discoveries in a Venetian newspaper.5 Sette was furious about being scooped by Bizio’s article, although Bizio’s results didn’t contradict his own.Bartolomeo BizioAs Bizio reported in a letter to a priest, Angelo Bellani, three years after the event:For several days succeeding its publication, there was much discussion of my experiments in the columns of this newspaper [The Official Gazette]; and to make them still more well known, an enterprising publisher printed them in a small pamphlet which he sold in the streets, so that the general public as well as educated persons came to know about them.By recreating red spots of Serratia marcescens on fresh polenta, Bizio grew pure colonies of microbes more than half a century before the “culture plate technique” developed by Robert Koch, Walther and Fanny Angelina Hesse, and Julius Petri.6 The significance of Bizio’s experiments has survived over the centuries, showing the falsehood of a dismissive remark made by renowned bacterial systematist S. T. Cowan, who said: “I believe we shall not lose anything by ignoring all work before the pioneer [sic] work of Pasteur.”The bacteriological investigations of another predecessor of Pasteur, the naturalist Christian Gottfried Ehrenberg (1795-1876), have survived as well. In 1848, several decades after the bloody polenta first appeared in Padua, red spots began popping up on boiled potatoes in Germany. Unaware of Bizio’s work, Ehrenberg observed the phenomenon and called the organism responsible Monas prodigiosa, one of the many names that Serratia marcescens has held over the centuries. Ehrenberg studied historical records and concluded that the bacterium was most likely responsible for more than 100 documented cases of so-called “miraculous blood.”!function(){\"use strict\";window.addEventListener(\"message\",(function(e){if(void 0!==e.data[\"datawrapper-height\"]){var t=document.querySelectorAll(\"iframe\");for(var a in e.data[\"datawrapper-height\"])for(var r=0;r<t.length;r++){if(t[r].contentWindow===e.source)t[r].style.height=e.data[\"datawrapper-height\"][a]+\"px\"}}}))}();Specifically, Serratia marcescens holds a large but unintended place in two of the world’s religions due to this shocking resemblance to blood. The 1264 “Miracle of Bolsena” of a Host “bleeding” with Serratia marcescens was believed to have contributed to the establishment of the Holy Communion, a central sacrament of the Greek Orthodox and Roman Catholic church. Additionally, the infamous antisemitic “blood libels” might have been due to this microbe, with thousands of Jews executed as heretics following accusations of having stabbed holy wafers in what was most likely a naturally occurring outbreak of Serratia marcescens on the starchy substratum. It is difficult to verify if Serratia marcescens was the precise biological cause of such “miracles” or “heresies,” however, since many microorganisms, such as mold or yeast, are often red or pink and grow readily on food. But it seems likely, for as researchers have pointed out: “We know of no organism [ … ] looking more like drops of fresh blood than Serratia marcescens.”With its centuries-long history, it’s perhaps no surprise that Serratia marcescens has had so many different names and presents “one of the most confusing taxonomies in the bacterial world.” In 1924, bacteriologists rehabilitated Bizio’s Linnean appellation, retaining Ehrenberg’s “term of miracle (prodigium) bacterium for use as a trivial or common name.” By the end of the 19th century, however, due to its vivid hue, Serratia marcescens had become one of “the cornerstones of modern bacteriology.” Fake Blood, Real ThreatSpecifically, Serratia marcescens’ striking color made it the microbe of choice for several decades amongst bacteriologists studying how pathogens spread through buildings and cities.In 1897, German bacteriologist Carl Flügge used the bacteria to perform experiments which served as a template for Gordon’s (but without the Shakespearean flair).7 Flügge was the first to demonstrate that mouth droplets carry bacteria. His findings spurred surgeon Johann Mikulicz to develop a precursor of today’s face mask, and were a major driver of the six feet (two meters) “social distancing” policy during the COVID-19 pandemic.In 1919, military doctors proved that utensils can indirectly transmit microbes by applying Serratia marcescens on the mouth and lips of “donor soldiers” before their meals. In 1926, bacteriologists traced how handshakes transmit microbes by smearing the bacterium on the hands of test subjects. In 1937, dentists detected bacteremia (entry of bacteria into the blood system) following dental extraction by spreading the pathogen around the gum of teeth. In 1945, military staff correlated air quality with the progression of illness by exposing test subjects to huge quantities of aerosolized Serratia marcescens. In 1957, doctors demonstrated urinary tract infections through catheters by applying Serratia marcescens on genitalia of semi-comatose patients, one of whom died shortly after.8More ominously, Nazi Germany studied the spread of the pathogen in the Paris Métro and the London Underground, as reported in a 1934 article by investigative journalist Henry Wickham Steed, Aerial Warfare: Secret German Plans. The French and U.K. governments took Steed’s article very seriously, as the underground systems in both Paris and London had functioned as shelters during WWI and would again in WWII.London residents take refuge in an Underground station during intense bombing from the Luftwaffe.The U.S. military also deployed Serratia marcescens in a spectacular way in September 1950. In a secret project called “Operation Sea Spray,” the Navy sprayed enormous quantities of the pathogen along the coast of San Francisco to study large-scale, open-air transmission of germs in biological warfare. Navy scientists also set up monitoring stations and traced the microbe up to 50 miles inland, but experts disputed any scientific and epidemiological merit of the experiments. Notably, “Operation Sea Spray” coincided with the first recorded nosocomial outbreak of Serratia marcescens, with eleven inpatients infected at Stanford Hospital between September 1950 and February 1951. Two of them got bacteremia; one died of heart failure.9(In November 1976, the Long Island newspaper Newsday leaked information about “Operation Sea Spray.” Contrary to other biowarfare experiments that flew under the radar, this led to a public Hearing at the Senate in March and May of 1977.10 Surprisingly, however, in August 1977, an investigation by the U.S. Centers for Disease Control concluded that the strain used by the U.S. Military was not related to any infection within the population.)That Serratia marcescens was so widely used in experiments involving human subjects should startle us; especially since the scientific community was accumulating evidence that the bacterium could be deadly. As early as 1903, infectologist E. Bertarelli showed that the bacterium was lethal to mice, rats, and guinea pigs “following inoculation of massive doses of Bacillus prodigiosus” (one of the microbe’s many names). The first report of human infection by Serratia marcescens followed in 1913, when a healthy young man sought medical help after becoming troubled by a foul-smelling red sputum, which he mistakenly took for blood.11 Doctors found no sign of blood cells in the sputum but saw a large number of intestinal bacteria, which grew on agar into distinctive red colonies. The patient later recovered without complications.Among those with weakened immune systems, Serratia marcescens has been involved “in every conceivable kind of infection.” It can affect an individual’s mouth and throat, lungs, gut, urinary tract, blood, heart, wounded skin, eyes, and central nervous system. When susceptible patients are infected by a virulent form of the pathogen, symptoms may include discoloration of extremities, shock, convulsions, deafness, delirium, and coma. Mortality of affected patients (especially in the case of sepsis, or the spread of bacteria in the bloodstream) runs as high as 30-40 percent. Indeed, by the late 1960s, the bacterium was unequivocally declared as a cause of serious infections, including death.Infections with Serratia marcescens haven’t ceased to be a cause for concern,12 and clinicians have wondered if there aren’t many more infections with Serratia marcescens going unrecognized. As recently as 2017, the World Health Organization listed Serratia among bacteria for which new antibiotics are urgently needed.Modern WondersSerratia marcescens striking red color comes from a pigment aptly called “prodigiosin.” Clinicians have observed an inverse correlation between a strain’s high levels of prodigiosin and its ability to cause infections. This might explain why past studies designed to take advantage of Serratia marcescens’ vivid appearance have caused little harm. By using bright red colonies as markers for their experiments, like M. H. Gordon’s in the U.K. Parliament, researchers might have unwittingly selected a less virulent strain of Serratia marcescens.Prodigiosin is an alkaloid with immunosuppressive, anticarcinogenic, and antimicrobial properties. Scientists do not yet fully understand why the bacterium produces prodigiosin, but have noticed that environmental conditions influence red pigmentation (for example, a rise in temperature reduces the bacterium’s vivid color). They speculate that the molecule might be important for cell dormancy, dispersal in the environment, or for gaining advantages over competing microbes.Prodigiosin has many uses as a biomedicine, including the inhibition of microbes such as bacteria, fungi, algae, and viruses. Researchers are still investigating the mechanisms by which prodigiosin acts on a target cell, and have observed adverse effects on the cell membrane as well as in organelles within the cell. Prodigiosin has also contributed to research on microbial “secondary metabolites;” molecules, including antibiotics, which are not involved in primary physiological functions like growth or reproduction, but nonetheless help protect an organism. Researchers showed before the discovery of penicillin, the first antibiotic, that Serratia marcescens inhibits Vibrio bacteria. In 1983, prodigiosin was also used as a molecular marker to clone, for the first time, a gene involved in the biosynthesis of an antibiotic.A formula named Coley’s Toxin containing Serratia marcescens has been used for over a century to stimulate the immune system to fight off cancers. (Its namesake, William Coley, is considered the “Father of Immunotherapy.”) Prodigiosin has shown activity against cancer cells, among others, by inducing DNA cleavage and apoptosis (cell death). Prodigiosin is also being investigated for therapies that require inhibition of the immune system.In nature, Serratia marcescens is part of a complex web of ecological interactions. It can inhibit parasites (like the malaria agent Plasmodium falciparum), kill insects (like the cockroach Blattella germanica by acting synergistically with a fungus), or enhance growth of some plants while killing others. Researchers have also used Serratia marcescens as a model to study how environmental species evolve into pathogens. When confronted with a natural predator (like the protist Tetrahymena thermophila), the bacterium acquires traits that contribute to its virulence. In 2011, researchers also observed that Serratia marcescens had begun to infect corals, the first instance of a “reverse zoonosis” in marine habitats, a situation in which a pathogen moves from humans to animals.As of early 2025, Serratia marcescens stands among the top 30 most studied microbes, topping species such as Agrobacterium tumefaciens (a widely used model microbe to genetically engineer plants), Legionella pneumophila (the cause of Legionnaire’s disease), Clostridioides difficile (a common diarrhea-causing bacterium affecting the intestinal tract), and Mycobacterium smegmatis (a model organism for the study of the pathogen causing tuberculosis, M. tuberculosis). Through surprising sightings and curious experiments, this microbe has left an enormous mark culturally, clinically, and scientifically. Its vivid blood-red color has elicited both wonder and alarm in the eyes of the beholder even as its actual dangers have often been ignored.It is no wonder, then, that Alexander Fleming, that giant of microbial investigation, also paid notice. Fleming used it in a painting method he developed, called microbial art (or “agar art”). When working in this medium, artists take a Petri dish filled with agar and, using a lab tool called a loop, inoculate sections of the plate with various species of microbes that produce different hues. As the microbes grow, their living pigments form an image. For yellow, Fleming used Staphylococcus; for blue, Bacillus violaceus; and for brilliant red, of course, Serratia marcescens.Agar art paintings by Alexander Fleming. Credit: American Society for MicrobiologyThis article is accompanied by an interview with the author. Listen on Spotify or Apple Music.Corrado Nai is a science writer with a Ph.D. in microbiology. He has published articles in Smithsonian Magazine, New Scientist, Reactor (formerly Tor.com), Small Things Considered, and elsewhere. He is writing a graphic novel about the forgotten woman who introduced agar to microbiology, Fanny Angelina Hesse (1850-1934), based on unpublished historical material he helped resurface. Corrado lives in Jakarta with his wife and daughter.Cite: Nai C. “A Brief History of the Miracle Bacterium.” Asimov Press (2025). DOI: 10.62211/48yk-73gfLead image by Ella Watkins-Dulaney, adapted from Benutzer:Brudersohn (CC BY3.0).1I use “virulent bacterium” and “pathogen” interchangeably to refer to a disease-causing microbe. Usually, “pathogen” refers to a species whereas strains (that is, lineages of bacteria) within a species can be more or less “virulent.”2Gordon’s 250+ page report is dramatic. In some experiments he made bacteriological analyses of the Ministerial versus the Opposition side of the Chamber; in others he placed open Petri dishes even on the top of the Clock Tower. In his speech experiment, he retrieved more colonies when he repeated the test with the ventilation system on than with the ventilation system off. Despite inconclusive bacteriology findings, he suggested 14 substantive modifications to the ventilation system.3Christian Gottfried Ehrenberg thought it was an animal, botanist Camille Montagne an algae.4Serratia marcescens is the first bacterium to be ever named after a person. The genus name Serratia is only predated by Vibrio (Otto Müller, 1773) and Polyangium (Heinrich Link, 1809).5The modern counterpart of Bizio’s newspaper article, published in the Gazzetta Privilegiata di Venezia on 24 August 1819, would be a preprint.6Koch was the first to streak bacteria into single colonies in the 1870s; the Hesse couple introduced agar to the laboratory in 1881; and Petri perfected his Petri dishes in 1887. The “culture plate technique” continues to be used unaltered to this day.7The original German article is available here: Carl Flügge (1897), Ueber Luftinfection. Zeitschrift für Hygiene 25:179-224.8It’s uncertain if Serratia marcescens was the cause of death, as doctors were not granted the rights for an autopsy.9The Stanford Hospital case marked the first case of endocarditis (inflammation of the heart) by Serratia marcescens.10The Hearing discussed classified Military biowarfare experiments that took place using Serratia marcescens until 1968, including in the New York City Subway. The Military admitted knowing about the Stanford Hospital outbreak.11This appearance of red sputum without the presence of blood was later named “pseudohaemoptysis.”12A dramatic outbreak happened in the mid-1970s and involved epidemics in four different hospitals in Nashville, Tennessee, caused by rotating staff likely transmitting the pathogen by hand. The virulent strain of Serratia marcescens was resistant to all antibiotics and infected over 200 patients, killing eight. Then, in one of those hospitals, a much more severe epidemic with Klebsiella pneumoniae infected about 400 patients, killing 18. Clinicians attributed this outbreak to co-infection and in vivo transmission of antibiotic-resistant genes from Serratia to Klebsiella. More recently, outbreaks of Serratia marcescens have occurred within prisons due to contaminated disinfectants and amongst intravenous opioid users due to contaminated syringes. In 2004, biotech company Chiron Corporation lost an enormous quantity of flu vaccines due to contamination with Serratia marcescens.Subscribe to Asimov PressScience and technology for a flourishing future. Website: press.asimov.comSubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.20 Likes∙3 Restacks20Share this postAsimov PressA Brief History of the Miracle BacteriumCopy linkFacebookEmailNotesMore13SharePreviousA guest post byCorrado NaiScience writer, PhD fungal ecology. Currently writing a graphic novel about the forgotten woman who introduced agar to the lab, Fanny Angelina Hesse (1850-1934): https://fanny-hesse-graphic-novel.site/",
    "summary": {
      "en": "**Summary: A Brief History of Serratia marcescens**\n\nSerratia marcescens is a bacterium known for its striking red color, resembling blood, which has significantly impacted scientific research and public health. It was first noted in 1819 when red spots appeared on polenta, leading to investigations by scientists, including Bartolomeo Bizio, who named the bacterium. \n\nIn the early 1900s, British physician M. H. Gordon used Serratia marcescens in experiments to study how germs spread, demonstrating that the microbe could be transmitted through speech and contact. Over the years, its unique color made it a popular choice in various scientific experiments related to microbial transmission.\n\nDespite its use in research, Serratia marcescens can be harmful, particularly its paler strains. It has been linked to serious infections in humans, especially those with weakened immune systems, leading to high mortality rates. Historically, this bacterium has been involved in both medical studies and military experiments, raising ethical concerns.\n\nSerratia marcescens also produces a pigment called prodigiosin, which has potential medical uses, including cancer treatment and antimicrobial properties. Today, it remains a widely studied microbe, contributing to our understanding of infections and microbial behavior in various environments.",
      "ko": "세라티아 마르세센스는 혈액과 유사한 선명한 빨간색으로 알려진 박테리아로, 과학 연구와 공공 건강에 큰 영향을 미쳤습니다. 이 박테리아는 1819년에 폴렌타에서 빨간 반점이 나타나면서 처음 주목받았고, 바르톨로메오 비지오를 포함한 과학자들이 이를 조사하게 되었습니다.\n\n1900년대 초, 영국의 의사 M. H. 고든은 세라티아 마르세센스를 사용하여 세균이 어떻게 퍼지는지를 연구하는 실험을 진행했습니다. 그는 이 미생물이 말이나 접촉을 통해 전파될 수 있음을 입증했습니다. 시간이 지나면서 이 박테리아의 독특한 색깔 덕분에 미생물 전파와 관련된 다양한 과학 실험에서 인기가 높아졌습니다.\n\n하지만 세라티아 마르세센스는 연구에 사용되기도 하지만, 특히 색이 더 연한 변종은 해로울 수 있습니다. 이 박테리아는 면역력이 약한 사람들에게 심각한 감염과 높은 사망률과 관련이 있습니다. 역사적으로 이 박테리아는 의학 연구와 군사 실험에 모두 사용되었으며, 이로 인해 윤리적 문제도 제기되었습니다.\n\n세라티아 마르세센스는 프로디지오신이라는 색소를 생성하는데, 이는 암 치료와 항균 특성 등 의학적 용도가 있을 수 있습니다. 오늘날에도 이 박테리아는 감염과 다양한 환경에서의 미생물 행동에 대한 이해를 돕기 위해 널리 연구되고 있습니다.",
      "ja": "セラチア・マルケッセンシスは、血のような鮮やかな赤色で知られる細菌で、科学研究や公衆衛生に大きな影響を与えてきました。この細菌は1819年に初めて注目され、ポレンタに赤い斑点が現れたことがきっかけで、バルトロメオ・ビツィオを含む科学者たちによる調査が行われました。\n\n1900年代初頭、イギリスの医師M.H.ゴードンは、セラチア・マルケッセンシスを使って細菌の広がりを研究しました。彼は、この微生物が話すことや接触を通じて伝播することを示しました。年月が経つにつれ、その独特の色合いから、微生物の伝播に関するさまざまな科学実験で人気のある選択肢となりました。\n\n研究に利用される一方で、セラチア・マルケッセンシスは危険な場合もあります。特に色の薄い株は、人間に深刻な感染症を引き起こすことがあり、免疫力が低下している人々においては高い死亡率が報告されています。歴史的には、この細菌は医学研究や軍事実験にも関与しており、倫理的な問題が提起されています。\n\nセラチア・マルケッセンシスは、プロディジオシンという色素を生成します。この色素は、癌治療や抗菌特性を持つなど、医療用途の可能性があります。現在でも、この微生物は広く研究されており、感染症やさまざまな環境における微生物の行動についての理解を深めるために貢献しています。"
    }
  },
  {
    "id": "b06da4e326eeff0b",
    "title": {
      "en": "The Lost Art of Commit Messages",
      "ko": "커밋 메시지의 잃어버린 미학",
      "ja": "コミットメッセージの秘訣"
    },
    "type": "story",
    "url": "https://www.seyhan.me/blog/post/lost-art-of-commit-messages",
    "score": 32,
    "by": "drac89",
    "time": 1742885542,
    "content": "The Lost Art of Commit MessagesHave you ever noticed how commit messages these days are like cryptic haikus from developers who seem to think a single word is all it takes to summarize their masterpiece?\nPeople scribbling \"fix bug\", \"update code\", or the ever descriptive \"refactor\" while pushing changes that could rival a novel in length. It's as if they think we're all mind readers or perhaps they assume their code is so flawless it needs no explanation.\n\nFor the love of clean code history, let's remember we're not monkeys banging on keyboards; we're educated, civilized human beings.\n\nThe Art of the Commit Message\nCrafting a meaningful commit message seems to be a lost art in today's fast-paced development world.\nYes! I said \"lost art\". Because many developers or let me correct that, many keyboard virtuosos apparently don't see the value in leaving a traceable breadcrumb trail of their genius.\nOkay, let's learn how to do it right.\nCommit messages are the narrative of your project's history. They help others (and your future self) understand why changes were made, making collaboration smoother and debugging less of a nightmare.\nGiven that, let's dive into some guidelines to improve your commit messages from vague to valuable.\nCommit Message Guidelines\nEach commit message should follow this structure:\n<type>(optional scope): <short description>\n<BLANK LINE>\n- Optional point 1 in brief\n- Optional point 2 in brief\n<BLANK LINE>\n<footer>\n\nComponents of the Format\n\nType: Specifies the nature of the commit.\nScope (optional): A short identifier for the component, file, or feature affected.\nShort Description: A concise summary of the change, ideally up to 50 characters.\nDetailed Points (optional): Bullet points that provide additional context or details about the changes.\nFooter (optional): Additional metadata like issue references, co-authors, or breaking changes.\n\nGuidelines for Each Component\nType\n\nRequired: Select the appropriate type based on the commit's purpose.\nUse only one of the allowed types.\n\nAllowed Types\nChoose from the following types for the <type> field:\n\nfeat: A new feature\nfix: A bug fix\nchore: Routine tasks that don't impact functionality\nrefactor: Code changes that neither fix bugs nor add features\ndocs: Documentation updates\nstyle: Code style changes that don't affect functionality\ntest: Adding or updating tests\nperf: Performance improvements\nci: Continuous Integration changes\nbuild: Changes related to build processes or dependencies\nrevert: Reverts a previous commit\n\nScope (Optional)\n\nOptional: Specifies the part of the codebase affected.\nKeep it short and descriptive.\nUse lowercase and single-word terms (hyphens are acceptable).\n\nExamples:\n\nauth, api, ui, database, config.\n\nShort Description\n\nRequired: Summarize the change in 50 characters or less.\nStart with a lowercase letter; no punctuation at the end.\nClearly convey the essence of the change.\n\nExamples:\n\nadd user authentication\nfix null pointer exception in payment module\nrefactor data processing pipeline\n\nDetailed Points (Optional)\n\nOptional: Provide additional context in bullet points.\nKeep each point brief and focused.\nUse bullets (-) and avoid lengthy explanations.\n\nExamples:\n\n- implement OAuth2 for third-party login\n- adjust timeout settings to prevent crashes\n- reorganize folder structure for clarity\n\nFooter (Optional)\nYou might wonder, \"Footers? Isn't that a bit much?\" Well, consider this: footers can automate tedious tasks, improve team communication, and make your commit history a beacon of clarity in the chaotic sea of code changes.\nNow, the optional footer section is where you can sprinkle in extra metadata that helps with automation, issue tracking, and keeping your project history as pristine as a freshly refactored codebase.\nFooters are additional lines at the end of your commit message that provide metadata about the commit. They can:\n\nAutomate issue tracking: Automatically close or reference issues and pull requests.\nProvide additional context: Include information like co-authors, sign-offs, or breaking changes.\nEnhance clarity: Make your commit history more informative and navigable.\n\nCommon Footer Tags\nBecause why settle for clarity when you can have extra clarity? Here are some standard footer tags you can use:\n\nBREAKING CHANGE: Signals that the commit introduces a backward-incompatible change.\nFixes: References an issue your commit fixes.\nCloses: Similar to \"Fixes\", but used to close issues or pull requests.\nResolves: Another way to reference and close issues.\nRelated to: Links to issues or tasks related to the commit without closing them.\nCo-authored-by: Credits additional contributors.\nReviewed-by: Indicates who reviewed the changes.\nSigned-off-by: A sign-off acknowledging you have the rights to submit the work.\nSee also: Points to additional resources or relevant information.\n\nExample Commit Messages\nExample 1: Feature Addition\nfeat(auth): add user login functionality\n\n- implement JWT authentication\n- add login endpoint to API\n\nExample 2: Bug Fix that Closes an Issue\nfix(ui): correct alignment on dashboard widgets\n\n- adjust CSS flex properties\n- test on multiple screen sizes\n\nFixes #204\n\nExample 3: Documentation Update\ndocs(readme): update installation instructions\n\n- add steps for setting up the development environment\n- include troubleshooting tips\n\nExample 4: Code Refactor\nrefactor(api): simplify request handlers\n\n- remove redundant code blocks\n- improve error handling mechanisms\n\nExample 5: Documentation Update with See Also\ndocs(README): update setup instructions\n\n- include Docker configuration\n- clarify environment variables\n\nSee also: https://example.com/setup-guide\n\nGeneral Tips\n\nBe concise and clear: Your future self (and teammates) will thank you.\nAvoid redundancy: Don't repeat the same information.\nUse present tense: \"Add feature\", not \"Added feature\".\nBe Consistent: Use standard tags to keep the commit history uniform.\nKeep It Relevant: Only include footers that add value or necessary context.\nStay Professional: This isn't the place for jokes or unnecessary commentary (that's what the code comments are for).\n\nNext time you're tempted to type \"fix stuff\" as a commit message, remember, we're not monkeys. We're educated, civilized human beings capable of conveying complex ideas with clarity. Let's make sure our commit messages reflect that if not for the sake of our teammates, then at least to prevent our future selves from cursing our past selves.\nHappy committing!",
    "summary": {
      "en": "### Summary of The Lost Art of Commit Messages\n\nCommit messages are often vague and unhelpful, resembling cryptic notes rather than clear communication. Good commit messages are essential for understanding project history and aiding collaboration.\n\n**Key Guidelines for Writing Effective Commit Messages:**\n\n1. **Structure:**\n   - Format: `<type>(optional scope): <short description>`\n   - Followed by a blank line, optional bullet points for details, and an optional footer.\n\n2. **Components:**\n   - **Type**: Indicates the nature of the commit (e.g., feat, fix, chore).\n   - **Scope (optional)**: Identifies the affected part of the codebase.\n   - **Short Description**: A concise summary (50 characters or less).\n   - **Detailed Points (optional)**: Brief bullet points for additional context.\n   - **Footer (optional)**: Extra metadata for automation and issue tracking.\n\n3. **Common Types:**\n   - feat: New feature\n   - fix: Bug fix\n   - docs: Documentation updates\n   - refactor: Code changes without new features or bug fixes\n   - Others include style, test, perf, ci, build, and revert.\n\n4. **Example Messages:**\n   - Adding a feature: `feat(auth): add user login functionality`\n   - Fixing a bug: `fix(ui): correct widget alignment` (Fixes #204)\n\n**General Tips:**\n- Be clear, concise, and consistent.\n- Use present tense.\n- Avoid redundancy and irrelevant information.\n- Maintain professionalism.\n\nBy improving commit messages, developers can enhance collaboration and make their project's history more understandable.",
      "ko": "커밋 메시지는 종종 모호하고 도움이 되지 않으며, 명확한 소통보다는 암호 같은 메모에 가깝습니다. 좋은 커밋 메시지는 프로젝트의 역사 이해와 협업에 필수적입니다.\n\n효과적인 커밋 메시지를 작성하기 위한 주요 지침은 다음과 같습니다. 첫째, 구조입니다. 형식은 `<유형>(선택적 범위): <짧은 설명>`입니다. 그 뒤에는 빈 줄이 오고, 선택적으로 세부 사항을 위한 불릿 포인트와 추가 메타데이터를 위한 풋터가 올 수 있습니다.\n\n둘째, 구성 요소입니다. 유형은 커밋의 성격을 나타내며, 예를 들어 기능 추가는 'feat', 버그 수정은 'fix', 문서 업데이트는 'docs'로 표시합니다. 선택적 범위는 코드베이스의 영향을 받는 부분을 식별합니다. 짧은 설명은 50자 이하의 간결한 요약입니다. 선택적으로 추가 맥락을 위한 간단한 불릿 포인트와 자동화 및 이슈 추적을 위한 풋터가 포함될 수 있습니다.\n\n셋째, 일반적인 유형입니다. 'feat'는 새로운 기능, 'fix'는 버그 수정, 'docs'는 문서 업데이트, 'refactor'는 새로운 기능이나 버그 수정 없이 코드 변경을 의미합니다. 그 외에도 스타일, 테스트, 성능, CI, 빌드, 되돌리기 등이 있습니다.\n\n예시 메시지로는 기능 추가를 나타내는 `feat(auth): 사용자 로그인 기능 추가`와 버그 수정을 나타내는 `fix(ui): 위젯 정렬 수정`(Fixes #204)이 있습니다.\n\n일반적인 팁으로는 명확하고 간결하며 일관성을 유지하는 것이 중요합니다. 현재 시제를 사용하고 중복되거나 관련 없는 정보는 피해야 합니다. 전문성을 유지하는 것도 중요합니다.\n\n커밋 메시지를 개선함으로써 개발자들은 협업을 강화하고 프로젝트의 역사를 더 이해하기 쉽게 만들 수 있습니다.",
      "ja": "コミットメッセージはしばしば曖昧で役に立たず、明確なコミュニケーションというよりは暗号のようなメモになってしまうことがあります。良いコミットメッセージは、プロジェクトの歴史を理解し、協力を促進するために不可欠です。\n\n効果的なコミットメッセージを書くための重要なガイドラインには、構造とコンポーネントがあります。構造は「<タイプ>(オプションのスコープ): <短い説明>」という形式で、空行の後に詳細のための箇条書きやオプションのフッターが続きます。\n\nコンポーネントには、コミットの性質を示す「タイプ」が含まれます。例えば、新機能を示す「feat」、バグ修正を示す「fix」、ドキュメントの更新を示す「docs」、新機能やバグ修正なしのコード変更を示す「refactor」などがあります。スコープはオプションで、影響を受けるコードの部分を特定します。短い説明は50文字以内の簡潔な要約で、詳細ポイントは追加の文脈を提供するための簡単な箇条書きです。フッターは自動化や問題追跡のための追加のメタデータを含むことができます。\n\n一般的なタイプには、新機能を示す「feat」、バグ修正を示す「fix」、ドキュメントの更新を示す「docs」、コード変更を示す「refactor」などがあります。具体的なメッセージの例としては、機能追加の際の「feat(auth): ユーザーログイン機能を追加」や、バグ修正の際の「fix(ui): ウィジェットの配置を修正」などがあります。\n\n一般的なアドバイスとしては、明確で簡潔、一貫性を持たせることが重要です。現在形を使用し、冗長な情報や関連性のない情報は避け、プロフェッショナルな態度を保つことが求められます。\n\nコミットメッセージを改善することで、開発者は協力を強化し、プロジェクトの歴史をより理解しやすくすることができます。"
    }
  },
  {
    "id": "a8531c88fcf194f4",
    "title": {
      "en": "Upcoming changes to how live videos are stored",
      "ko": "라이브 영상 저장 변화",
      "ja": "ライブ動画保存の新ルール"
    },
    "type": "story",
    "url": "https://www.facebook.com/business/help/1005165994166600?_fb_noscript=1",
    "score": 24,
    "by": "EGreg",
    "time": 1742693699,
    "content": "Help chosen for youSome Messaging Metrics UnavailableBusiness Help CenterWhy is My Boost Unavailable?Business Help CenterTroubleshoot a Disabled Ad AccountBusiness Help CenterHow Ad Billing Works on FacebookBusiness Help Center\n\nGet the latest updates from Meta for Business.Provide your email address to receive the latest updates from Meta for Business, including news, events and product updates. By submitting this form, you agree to receive marketing related electronic communications from Meta, including news, events, updates and promotional emails. You may withdraw your consent and unsubscribe from these at any time, for example, by clicking the unsubscribe link included on our emails. For more information about how Meta handles your data please read our Data Policy.form originlead_sourceoptin__cCampaign_IDSubscribe",
    "summary": {
      "en": "This text includes information about resources available for businesses using Meta (formerly Facebook). Key points include:\n\n- Access to help for issues like unavailable messaging metrics, disabled ad accounts, and understanding ad billing on Facebook.\n- An invitation to sign up for updates from Meta for Business, which includes news and events.\n- By signing up, users agree to receive marketing emails from Meta and can unsubscribe at any time. \n\nFor more details on how Meta manages user data, there's a mention of their Data Policy.",
      "ko": "이 텍스트는 메타(구 페이스북)를 사용하는 기업을 위한 자원에 대한 정보를 포함하고 있습니다. 주요 내용은 다음과 같습니다.\n\n메타에서는 메시징 지표가 사용 불가능한 경우, 광고 계정이 비활성화된 경우, 페이스북 광고 청구를 이해하는 데 도움이 되는 지원을 제공합니다. 또한, 메타 비즈니스의 업데이트를 받을 수 있는 초대도 포함되어 있으며, 여기에는 뉴스와 이벤트 정보가 포함됩니다. 사용자가 가입하면 메타로부터 마케팅 이메일을 받는 데 동의하게 되며, 언제든지 구독을 취소할 수 있습니다.\n\n메타가 사용자 데이터를 관리하는 방법에 대한 자세한 내용은 데이터 정책에서 확인할 수 있습니다.",
      "ja": "このテキストでは、Meta（旧Facebook）を利用する企業向けのリソースについての情報が含まれています。主なポイントは、メッセージングの指標が利用できない場合や広告アカウントが無効になった場合、Facebookの広告請求についての理解を助けるためのサポートへのアクセスがあることです。また、Meta for Businessからの最新情報やイベントについての更新を受け取るための登録を促しています。登録することで、ユーザーはMetaからのマーケティングメールを受け取ることに同意し、いつでも配信停止が可能です。さらに、Metaがユーザーデータをどのように管理しているかについての詳細は、データポリシーに記載されています。"
    }
  },
  {
    "id": "280cf1d44bdbde37",
    "title": {
      "en": "DNS Speed Test",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://dnsspeedtest.online/",
    "score": 28,
    "by": "tosh",
    "time": 1742895275,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "09e9f98019f60115",
    "title": {
      "en": "PyTorch Internals: Ezyang's Blog",
      "ko": "파이토치 내부 탐구",
      "ja": "PyTorchの裏側"
    },
    "type": "story",
    "url": "https://blog.ezyang.com/2019/05/pytorch-internals/",
    "score": 436,
    "by": "Anon84",
    "time": 1742654344,
    "content": "PyTorch internals\n\nThis post is a long form essay version of a talk about PyTorch internals, that I gave at the PyTorch NYC meetup on May 14, 2019.\n\nHi everyone!  Today I want to talk about the internals of PyTorch.\n\nThis talk is for those of you who have used PyTorch, and thought to yourself, \"It would be great if I could contribute to PyTorch,\" but were scared by PyTorch's behemoth of a C++ codebase.  I'm not going to lie: the PyTorch codebase can be a bit overwhelming at times. The purpose of this talk is to put a map in your hands: to tell you about the basic conceptual structure of a \"tensor library that supports automatic differentiation\", and give you some tools and tricks for finding your way around the codebase.  I'm going to assume that you've written some PyTorch before, but haven't necessarily delved deeper into how a machine learning library is written.\n\nThe talk is in two parts: in the first part, I'm going to first introduce you to the conceptual universe of a tensor library.  I'll start by talking about the tensor data type you know and love, and give a more detailed discussion about what exactly this data type provides, which will lead us to a better understanding of how it is actually implemented under the hood.  If you're an advanced user of PyTorch, you'll be familiar with most of this material.  We'll also talk about the trinity of \"extension points\", layout, device and dtype, which guide how we think about extensions to the tensor class.  In the live talk at PyTorch NYC, I skipped the slides about autograd, but I'll talk a little bit about them in these notes as well.\nThe second part grapples with the actual nitty gritty details involved with actually coding in PyTorch.  I'll tell you how to cut your way through swaths of autograd code, what code actually matters and what is legacy, and also all of the cool tools that PyTorch gives you for writing kernels.\n\nThe tensor is the central data structure in PyTorch.  You probably have a pretty good idea about what a tensor intuitively represents: its an n-dimensional data structure containing some sort of scalar type, e.g., floats, ints, et cetera.  We can think of a tensor as consisting of some data, and then some metadata describing the size of the tensor, the type of the elements in contains (dtype), what device the tensor lives on (CPU memory? CUDA memory?)\n\nThere's also a little piece of metadata you might be less familiar with: the stride.  Strides are actually one of the distinctive features of PyTorch, so it's worth discussing them a little more.\n\nA tensor is a mathematical concept.  But to represent it on our computers, we have to define some sort of physical representation for them.  The most common representation is to lay out each element of the tensor contiguously in memory (that's where the term contiguous comes from), writing out each row to memory, as you see above. In the example above, I've specified that the tensor contains 32-bit integers, so you can see that each integer lies in a physical address, each offset four bytes from each other.  To remember what the actual dimensions of the tensor are, we have to also record what the sizes are as extra metadata.\nSo, what do strides have to do with this picture?\n\nSuppose that I want to access the element at position tensor[1, 0] in my logical representation.  How do I translate this logical position into a location in physical memory?  Strides tell me how to do this: to find out where any element for a tensor lives, I multiply each index with the respective stride for that dimension, and sum them all together.  In the picture above, I've color coded the first dimension blue and the second dimension red, so you can follow the index and stride in the stride calculation.  Doing this sum, I get two (zero-indexed), and indeed, the number three lives two below the beginning of the contiguous array.\n(Later in the talk, I'll talk about TensorAccessor, a convenience class that handles the indexing calculation.  When you use TensorAccessor, rather than raw pointers, this calculation is handled under the covers for you.)\nStrides are the fundamental basis of how we provide views to PyTorch users.  For example, suppose that I want to extract out a tensor that represents the second row of the tensor above:\n\nUsing advanced indexing support, I can just write tensor[1, :] to get this row.  Here's the important thing: when I do this, I don't create a new tensor; instead, I just return a tensor which is a different view on the underlying data.  This means that if I, for example, edit the data in that view, it will be reflected in the original tensor.  In this case, it's not too hard to see how to do this: three and four live in contiguous memory, and all we need to do is record an offset saying that the data of this (logical) tensor lives two down from the top.  (Every tensor records an offset, but most of the time it's zero, and I'll omit it from my diagrams when that's the case.)\n\nQuestion from the talk: If I take a view on a tensor, how do I free the memory of the underlying tensor?\nAnswer: You have to make a copy of the view, thus disconnecting it from the original physical memory.  There's really not much else you can do.  By the way, if you have written Java in the old days, taking substrings of strings has a similar problem, because by default no copy is made, so the substring retains the (possibly very large string). Apparently, they fixed this in Java 7u6.\n\nA more interesting case is if I want to take the first column:\n\nWhen we look at the physical memory, we see that the elements of the column are not contiguous: there's a gap of one element between each one.  Here, strides come to the rescue: instead of specifying a stride of one, we specify a stride of two, saying that between one element and the next, you need to jump two slots.  (By the way, this is why it's called a \"stride\": if we think of an index as walking across the layout, the stride says how many locations we stride forward every time we take a step.)\nThe stride representation can actually let you represent all sorts of interesting views on tensors; if you want to play around with the possibilities, check out the Stride Visualizer.\nLet's step back for a moment, and think about how we would actually implement this functionality (after all, this is an internals talk.)  If we can have views on tensor, this means we have to decouple the notion of the tensor (the user-visible concept that you know and love), and the actual physical data that stores the data of the tensor (called storage):\n\nThere may be multiple tensors which share the same storage.  Storage defines the dtype and physical size of the tensor, while each tensor records the sizes, strides and offset, defining the logical interpretation of the physical memory.\nOne thing to realize is that there is always a pair of Tensor-Storage, even for \"simple\" cases where you don't really need a storage (e.g., you just allocated a contiguous tensor with torch.zeros(2, 2)).\n\nBy the way, we're interested in making this picture not true; instead of having a separate concept of storage, just define a view to be a tensor that is backed by a base tensor.  This is a little more complicated, but it has the benefit that contiguous tensors get a much more direct representation without the Storage indirection.  A change like this would make PyTorch's internal representation a bit more like Numpy's.\n\nWe've talked quite a bit about the data layout of tensor (some might say, if you get the data representation right, everything else falls in place).  But it's also worth briefly talking about how operations on the tensor are implemented.  At the very most abstract level, when you call torch.mm, two dispatches happen:\n\nThe first dispatch is based on the device type and layout of a tensor: e.g., whether or not it is a CPU tensor or a CUDA tensor (and also, e.g., whether or not it is a strided tensor or a sparse one).  This is a dynamic dispatch: it's a virtual function call (exactly where that virtual function call occurs will be the subject of the second half of this talk).  It should make sense that you need to do a dispatch here: the implementation of CPU matrix multiply is quite different from a CUDA implementation.  It is a dynamic dispatch because these kernels may live in separate libraries (e.g., libcaffe2.so versus libcaffe2_gpu.so), and so you have no choice: if you want to get into a library that you don't have a direct dependency on, you have to dynamic dispatch your way there.\nThe second dispatch is a dispatch on the dtype in question.  This dispatch is just a simple switch-statement for whatever dtypes a kernel chooses to support.  Upon reflection, it should also make sense that we need to a dispatch here: the CPU code (or CUDA code, as it may) that implements multiplication on float is different from the code for int.  It stands to reason you need separate kernels for each dtype.\nThis is probably the most important mental picture to have in your head, if you're trying to understand the way operators in PyTorch are invoked.  We'll return to this picture when it's time to look more at code.\n\nSince we have been talking about Tensor, I also want to take a little time to the world of tensor extensions.  After all, there's more to life than dense, CPU float tensors.  There's all sorts of interesting extensions going on, like XLA tensors, or quantized tensors, or MKL-DNN tensors, and one of the things we have to think about, as a tensor library, is how to accommodate these extensions.\n\nOur current model for extensions offers four extension points on tensors.  First, there is the trinity three parameters which uniquely determine what a tensor is:\n\nThe device, the description of where the tensor's physical memory is actually stored, e.g., on a CPU, on an NVIDIA GPU (cuda), or perhaps on an AMD GPU (hip) or a TPU (xla).  The distinguishing characteristic of a device is that it has its own allocator, that doesn't work with any other device.\nThe layout, which describes how we logically interpret this physical memory.  The most common layout is a strided tensor, but sparse tensors have a different layout involving a pair of tensors, one for indices, and one for data; MKL-DNN tensors may have even more exotic layout, like blocked layout, which can't be represented using merely strides.\nThe dtype, which describes what it is that is actually stored in each element of the tensor.  This could be floats or integers, or it could be, for example, quantized integers.\n\nIf you want to add an extension to PyTorch tensors (by the way, if that's what you want to do, please talk to us!  None of these things can be done out-of-tree at the moment), you should think about which of these parameters you would extend.  The Cartesian product of these parameters define all of the possible tensors you can make.  Now, not all of these combinations may actually have kernels (who's got kernels for sparse, quantized tensors on FPGA?) but in principle the combination could make sense, and thus we support expressing it, at the very least.\nThere's one last way you can make an \"extension\" to Tensor functionality, and that's write a wrapper class around PyTorch tensors that implements your object type.  This perhaps sounds obvious, but sometimes people reach for extending one of the three parameters when they should have just made a wrapper class instead.  One notable merit of wrapper classes is they can be developed entirely out of tree.\nWhen should you write a tensor wrapper, versus extending PyTorch itself?  The key test is whether or not you need to pass this tensor along during the autograd backwards pass.  This test, for example, tells us that sparse tensor should be a true tensor extension, and not just a Python object that contains an indices and values tensor: when doing optimization on networks involving embeddings, we want the gradient generated by the embedding to be sparse.\n\nOur philosophy on extensions also has an impact of the data layout of tensor itself.  One thing we really want out of our tensor struct is for it to have a fixed layout: we don't want fundamental (and very frequently called) operations like \"What's the size of a tensor?\" to require virtual dispatches.  So when you look at the actual layout of a Tensor (defined in the TensorImpl struct),  what we see is a common prefix of all fields that we consider all \"tensor\"-like things to universally have, plus a few fields that are only really applicable for strided tensors, but are so important that we've kept them in the main struct, and then a suffix of custom fields that can be done on a per-Tensor basis.  Sparse tensors, for example, store their indices and values in this suffix.\n\nI told you all about tensors, but if that was the only thing PyTorch provided, we'd basically just be a Numpy clone.  The distinguishing characteristic of PyTorch when it was originally released was that it provided automatic differentiation on tensors (these days, we have other cool features like TorchScript; but back then, this was it!)\nWhat does automatic differentiation do?  It's the machinery that's responsible for taking a neural network:\n\n...and fill in the missing code that actually computes the gradients of your network:\n\nTake a moment to study this diagram.  There's a lot to unpack; here's what to look at:\n\nFirst, rest your eyes on the variables in red and blue.  PyTorch implements reverse-mode automatic differentiation, which means that we effectively walk the forward computations \"backward\" to compute the gradients.  You can see this if you look at the variable names: at the bottom of the red, we compute loss; then, the first thing we do in the blue part of the program is compute grad_loss.  loss was computed from next_h2, so we compute grad_next_h2.  Technically, these variables which we call grad_ are not really gradients; they're really Jacobians left-multiplied by a vector, but in PyTorch we just call them grad and mostly everyone knows what we mean.\nIf the structure of the code stays the same, the behavior doesn't: each line from forwards is replaced with a different computation, that represents the derivative of the forward operation.  For example, the tanh operation is translated into a tanh_backward operation (these two lines are connected via a grey line on the left hand side of the diagram).  The inputs and outputs of the forward and backward operations are swapped: if the forward operation produced next_h2, the backward operation takes grad_next_h2 as an input.\n\nThe whole point of autograd is to do the computation that is described by this diagram, but without actually ever generating this source.  PyTorch autograd doesn't do a source-to-source transformation (though PyTorch JIT does know how to do symbolic differentiation).\n\nTo do this, we need to store more metadata when we carry out operations on tensors.  Let's adjust our picture of the tensor data structure: now instead of just a tensor which points to a storage, we now have a variable which wraps this tensor, and also stores more information (AutogradMeta), which is needed for performing autograd when a user calls loss.backward() in their PyTorch script.\n\nThis is yet another slide which will hopefully be out of date in the near future.  Will Feng is working on a Variable-Tensor merge in C++, following a simple merge which happened to PyTorch's frontend interface.\nWe also have to update our picture about dispatch:\n\nBefore we dispatch to CPU or CUDA implementations, there is another dispatch on variables, which is responsible for unwrapping variables, calling the underlying implementation (in green), and then rewrapping the results into variables and recording the necessary autograd metadata for backwards.\nSome implementations don't unwrap; they just call into other variable implementations.  So you might spend a while in the Variable universe.  However, once you unwrap and go into the non-Variable Tensor universe, that's it; you never go back to Variable (except by returning from your function.)\n\nIn my NY meetup talk, I skipped the following seven slides.  I'm also going to delay writeup for them; you'll have to wait for the sequel for some text.\n\nEnough about concepts, let's look at some code.\n\nPyTorch has a lot of folders, and there is a very detailed description of what they are in the CONTRIBUTING document, but really, there are only four directories you really need to know about:\n\nFirst, torch/ contains what you are most familiar with: the actual Python modules that you import and use.  This stuff is Python code and easy to hack on (just make a change and see what happens).  However, lurking not too deep below the surface is...\ntorch/csrc/, the C++ code that implements what you might call the frontend of PyTorch.  In more descriptive terms, it implements the binding code that translates between the Python and C++ universe, and also some pretty important pieces of PyTorch, like the autograd engine and the JIT compiler.  It also contains the C++ frontend code.\naten/, short for \"A Tensor Library\" (coined by Zachary DeVito), is a C++ library that implements the operations of Tensors.  If you're looking for where some kernel code lives, chances are it's in ATen.  ATen itself bifurcates into two neighborhoods of operators: the \"native\" operators, which are modern, C++ implementations of operators, and the \"legacy\" operators (TH, THC, THNN, THCUNN), which are legacy, C implementations.  The legacy operators are the bad part of town; try not to spend too much time there if you can.\nc10/, which is a pun on Caffe2 and A\"Ten\" (get it? Caffe 10) contains the core abstractions of PyTorch, including the actual implementations of the Tensor and Storage data structures.\n\nThat's a lot of places to look for code; we should probably simplify the directory structure, but that's how it is.  If you're trying to work on operators, you'll spend most of your time in aten.\nLet's see how this separation of code breaks down in practice:\n\nWhen you call a function like torch.add, what actually happens?  If you remember the discussion we had about dispatching, you already have the basic picture in your head:\n\nWe have to translate from Python realm to the C++ realm (Python argument parsing)\nWe handle variable dispatch (VariableType--Type, by the way, doesn't really have anything to do programming language types, and is just a gadget for doing dispatch.)\nWe handle device type / layout dispatch (Type)\nWe have the actual kernel, which is either a modern native function, or a legacy TH function.\n\nEach of these steps corresponds concretely to some code.  Let's cut our way through the jungle.\n\nOur initial landing point in the C++ code is the C implementation of a Python function, which we've exposed to the Python side as something like torch._C.VariableFunctions.add.  THPVariable_add is the implementation of one such implementation.\nOne important thing to know about this code is that it is auto-generated.  If you search in the GitHub repository, you won't find it, because you have to actually build PyTorch to see it.  Another important thing is, you don't have to really deeply understand what this code is doing; the idea is to skim over it and get a sense for what it is doing.  Above, I've annotated some of the most important bits in blue: you can see that there is a use of a class PythonArgParser to actually pull out C++ objects out of the Python args and kwargs; we then call a dispatch_add function (which I've inlined in red); this releases the global interpreter lock and then calls a plain old method on the C++ Tensor self.  On its way back, we rewrap the returned Tensor back into a PyObject.\n(At this point, there's an error in the slides: I'm supposed to tell you about the Variable dispatch code.  I haven't fixed it here yet.  Some magic happens, then...)\n\nWhen we call the add method on the Tensor class, no virtual dispatch happens yet.  Instead, we have an inline method which calls a virtual method on a \"Type\" object.  This method is the actual virtual method (this is why I say Type is just a \"gadget\" that gets you dynamic dispatch.)  In the particular case of this example, this virtual call dispatches to an implementation of add on a class named TypeDefault.  This happens to be because we have an implementation of add that is the same for every device type (both CPU and CUDA); if we had happened to have different implementations, we might have instead landed on something like CPUFloatType::add.  It is this implementation of the virtual method that finally gets us to the actual kernel code.\n\nHopefully, this slide will be out-of-date very soon too; Roy Li is working on replacing Type dispatch with another mechanism which will help us better support PyTorch on mobile.\nIt's worth reemphasizing that all of the code, until we got to the kernel, is automatically generated.\n\nIt's a bit twisty and turny, so once you have some basic orientation about what's going on, I recommend just jumping straight to the kernels.\n\nPyTorch offers a lot of useful tools for prospective kernel writers.  In this section, we'll walk through a few of them.  But first of all, what do you need to write a kernel?\n\nWe generally think of a kernel in PyTorch consisting of the following parts:\n\nFirst, there's some metadata which we write about the kernel, which powers the code generation and lets you get all the bindings to Python, without having to write a single line of code.\nOnce you've gotten to the kernel, you're past the device type / layout dispatch. The first thing you need to write is error checking, to make sure the input tensors are the correct dimensions.  (Error checking is really important!  Don't skimp on it!)\nNext, we generally have to allocate the result tensor which we are going to write the output into.\nTime for the kernel proper.  At this point, you now should do the second, dtype dispatch, to jump into a kernel which is specialized per dtype it operates on.  (You don't want to do this too early, because then you will be uselessly duplicating code that looks the same in any case.)\nMost performant kernels need some sort of parallelization, so that you can take advantage of multi-CPU systems.  (CUDA kernels are \"implicitly\" parallelized, since their programming model is built on top of massive parallelization).\nFinally, you need to access the data and do the computation you wanted to do!\n\nIn the subsequent slides, we'll walk through some of the tools PyTorch has for helping you implementing these steps.\n\nTo take advantage of all of the code generation which PyTorch brings, you need to write a schema for your operator.  The schema gives a mypy-esque type of your function, and also controls whether or not we generate bindings for methods or functions on Tensor.  You also tell the schema what implementations of your operator should be called for given device-layout combinations.  Check out the README in native is for more information about this format.\n\nYou also may need to define a derivative for your operation in derivatives.yaml.\n\nError checking can be done by way of either a low level or a high level API.  The low level API is just a macro, TORCH_CHECK, which takes a boolean, and then any number of arguments to make up the error string to render if the boolean is not true.  One nice thing about this macro is that you can intermix strings with non-string data; everything is formatted using their implementation of operator<<, and most important data types in PyTorch have operator<< implementations.\nThe high level API saves you from having to write up repetitive error messages over and over again.  The way it works is you first wrap each Tensor into a TensorArg, which contains information about where the tensor came from (e.g., its argument name).  It then provides a number of pre-canned functions for checking various properties; e.g., checkDim() tests if the tensor's dimensionality is a fixed number.  If it's not, the function provides a user-friendly error message based on the TensorArg metadata.\n\nOne important thing to be aware about when writing operators in PyTorch, is that you are often signing up to write three operators: abs_out, which operates on a preallocated output (this implements the out= keyword argument), abs_, which operates inplace, and abs, which is the plain old functional version of an operator.\nMost of the time, abs_out is the real workhorse, and abs and abs_ are just thin wrappers around abs_out; but sometimes writing specialized implementations for each case are warranted.\n\nTo do dtype dispatch, you should use the AT_DISPATCH_ALL_TYPES macro.  This takes in the dtype of the tensor you want to dispatch over, and a lambda which will be specialized for each dtype that is dispatchable from the macro.  Usually, this lambda just calls a templated helper function.\nThis macro doesn't just \"do dispatch\", it also decides what dtypes your kernel will support.  As such, there are actually quite a few versions of this macro, which let you pick different subsets of dtypes to generate specializations for.  Most of the time, you'll just want AT_DISPATCH_ALL_TYPES, but keep an eye out for situations when you might want to dispatch to some more types.  There's guidance in Dispatch.h for how to select the correct one for your use-case.\n\nOn CPU, you frequently want to parallelize your code.  In the past, this was usually done by directly sprinkling OpenMP pragmas in your code.\n\nAt some point, we have to actually access the data.  PyTorch offers quite a few options for doing this.\n\nIf you just want to get a value at some specific location, you should use TensorAccessor.  A tensor accessor is like a tensor, but it hard codes the dimensionality and dtype of the tensor as template parameters.  When you retrieve an accessor like x.accessor<float, 3>();, we do a runtime test to make sure that the tensor really is this format; but after that, every access is unchecked.  Tensor accessors handle strides correctly, so you should prefer using them over raw pointer access (which, unfortunately, some legacy kernels do.)  There is also a PackedTensorAccessor, which is specifically useful for sending an accessor over a CUDA launch, so that you can get accessors from inside your CUDA kernel.  (One notable gotcha: TensorAccessor defaults to 64-bit indexing, which is much slower than 32-bit indexing in CUDA!)\nIf you're writing some sort of operator with very regular element access, for example, a pointwise operation, you are much better off using a higher level of abstraction, the TensorIterator.   This helper class automatically handles broadcasting and type promotion for you, and is quite handy.\nFor true speed on CPU, you may need to write your kernel using vectorized CPU instructions.  We've got helpers for that too!  The Vec256 class represents a vector of scalars and provides a number of methods which perform vectorized operations on them all at once.  Helpers like binary_kernel_vec then let you easily run vectorized operations, and then finish everything that doesn't round nicely into vector instructions using plain old instructions.  The infrastructure here also manages compiling your kernel multiple times under different instruction sets, and then testing at runtime what instructions your CPU supports, and using the best kernel in those situations.\n\nA lot of kernels in PyTorch are still written in the legacy TH style.  (By the way, TH stands for TorcH.  It's a pretty nice acronym, but unfortunately it is a bit poisoned; if you see TH in the name, assume that it's legacy.)  What do I mean by the legacy TH style?\n\nIt's written in C style, no (or very little) use of C++.\nIt's manually refcounted (with manual calls to THTensor_free to decrease refcounts when you're done using tensors), and\nIt lives in generic/ directory, which means that we are actually going to compile the file multiple times, but with different #define scalar_t.\n\nThis code is pretty crazy, and we hate reviewing it, so please don't add to it.  One of the more useful tasks that you can do, if you like to code but don't know too much about kernel writing, is to port some of these TH functions to ATen.\n\nTo wrap up, I want to talk a little bit about working efficiently on PyTorch.  If the largeness of PyTorch's C++ codebase is the first gatekeeper that stops people from contributing to PyTorch, the efficiency of your workflow is the second gatekeeper.  If you try to work on C++ with Python habits, you will have a bad time: it will take forever to recompile PyTorch, and it will take you forever to tell if your changes worked or not.\nHow to work efficiently could probably be a talk in and of itself, but this slide calls out some of the most common anti-patterns I've seen when someone complains: \"It's hard to work on PyTorch.\"\n\nIf you edit a header, especially one that is included by many source files (and especially if it is included by CUDA files), expect a very long rebuild.  Try to stick to editing cpp files, and edit headers sparingly!\nOur CI is a very wonderful, zero-setup way to test if your changes worked or not. But expect to wait an hour or two before you get back signal.  If you are working on a change that will require lots of experimentation, spend the time setting up a local development environment.  Similarly, if you run into a hard to debug problem on a specific CI configuration, set it up locally.  You can download and run the Docker images locally\nThe CONTRIBUTING guide explains how to setup ccache; this is highly recommended, because sometimes it will help you get lucky and avoid a massive recompile when you edit a header.  It also helps cover up bugs in our build system, when we recompile files when we shouldn't.\nAt the end of the day, we have a lot of C++ code, and you will have a much more pleasant experience if you build on a beefy server with CPUs and RAM.  In particular, I don't recommend doing CUDA builds on a laptop; building CUDA is sloooooow and laptops tend to not have enough juice to turnaround quickly enough.\n\nSo that's it for a whirlwind tour of PyTorch's internals!  Many, many things have been omitted; but hopefully the descriptions and explanations here can help you get a grip on at least a substantial portion of the codebase.\nWhere should you go from here?  What kinds of contributions can you make?  A good place to start is our issue tracker.  Starting earlier this year, we have been triaging issues; issues labeled triaged mean that at least one PyTorch developer has looked at it and made an initial assessment about the issue.  You can use these labels to find out what issues we think are high priority or look up issues specific to some module, e.g., autograd or find issues which we think are small (word of warning: we're sometimes wrong!)\nEven if you don't want to get started with coding right away, there are many other useful activities like improving documentation (I love merging documentation PRs, they are so great), helping us reproduce bug reports from other users, and also just helping us discuss RFCs on the issue tracker. PyTorch would not be where it is today without our open source contributors; we hope you can join us too!\n\n          May 16, 2019\n          PyTorch",
    "summary": {
      "en": "This text summarizes a talk on the internals of PyTorch, aimed at users interested in contributing to the library but feeling intimidated by its complex C++ codebase. \n\nKey points include:\n\n1. **Understanding Tensors**: Tensors are the main data structure in PyTorch, representing n-dimensional data. They consist of data, metadata (like size and type), and strides, which help locate elements in memory.\n\n2. **Tensor Storage and Views**: Tensors can share the same underlying storage, allowing different views without duplicating data. This is managed through strides that indicate how to access non-contiguous elements.\n\n3. **Automatic Differentiation**: PyTorch uses reverse-mode automatic differentiation to compute gradients for neural networks. This requires additional metadata in tensor operations.\n\n4. **Code Structure**: The PyTorch codebase has several key directories: \n   - **torch/** for Python modules,\n   - **torch/csrc/** for C++ code interfacing with Python,\n   - **aten/** for tensor operations,\n   - **c10/** for core abstractions.\n\n5. **Kernel Writing**: Writing kernels involves metadata setup, error checking, and proper data access. PyTorch provides tools for efficient kernel writing, including macros for error handling and dispatching.\n\n6. **Best Practices**: Efficient workflow tips include minimizing header edits, using CI for testing changes, and leveraging local development environments for faster iteration.\n\n7. **Contributions**: The text encourages contributions to PyTorch, including coding, documentation, and bug reporting, and points to the issue tracker for finding areas to help.\n\nOverall, the talk aims to demystify PyTorch's internals and guide users toward effective contributions.",
      "ko": "이 텍스트는 PyTorch의 내부 구조에 대한 강연 내용을 요약한 것으로, 복잡한 C++ 코드베이스에 부담을 느끼는 사용자들이 라이브러리에 기여할 수 있도록 돕기 위한 것입니다.\n\n첫 번째로, 텐서에 대한 이해가 중요합니다. 텐서는 PyTorch의 주요 데이터 구조로, n차원 데이터를 나타냅니다. 텐서는 데이터, 메타데이터(크기와 타입 등), 그리고 메모리에서 요소를 찾는 데 도움을 주는 스트라이드로 구성됩니다.\n\n두 번째로, 텐서 저장소와 뷰에 대해 설명합니다. 텐서는 동일한 기본 저장소를 공유할 수 있어 데이터를 중복하지 않고도 다양한 뷰를 제공할 수 있습니다. 이는 비연속적인 요소에 접근하는 방법을 나타내는 스트라이드를 통해 관리됩니다.\n\n세 번째로, 자동 미분에 대해 다룹니다. PyTorch는 신경망의 그래디언트를 계산하기 위해 역전파 방식의 자동 미분을 사용합니다. 이를 위해 텐서 연산에 추가적인 메타데이터가 필요합니다.\n\n네 번째로, 코드 구조에 대해 설명합니다. PyTorch 코드베이스는 여러 주요 디렉토리로 구성되어 있습니다. Python 모듈을 위한 torch/, Python과 인터페이스하는 C++ 코드를 위한 torch/csrc/, 텐서 연산을 위한 aten/, 그리고 핵심 추상화를 위한 c10/이 있습니다.\n\n다섯 번째로, 커널 작성에 대해 설명합니다. 커널을 작성할 때는 메타데이터 설정, 오류 검사, 적절한 데이터 접근이 필요합니다. PyTorch는 오류 처리 및 디스패칭을 위한 매크로를 포함한 효율적인 커널 작성을 위한 도구를 제공합니다.\n\n여섯 번째로, 최선의 작업 관행에 대한 팁을 제공합니다. 헤더 수정을 최소화하고, CI(지속적 통합)를 사용하여 변경 사항을 테스트하며, 로컬 개발 환경을 활용하여 더 빠른 반복 작업을 할 수 있습니다.\n\n마지막으로, 기여를 장려합니다. 코딩, 문서화, 버그 보고 등 다양한 방법으로 PyTorch에 기여할 수 있으며, 도움을 줄 수 있는 영역을 찾기 위해 이슈 트래커를 참고할 것을 권장합니다.\n\n이 강연은 PyTorch의 내부 구조를 쉽게 이해할 수 있도록 돕고, 사용자들이 효과적으로 기여할 수 있는 방향으로 안내하는 것을 목표로 하고 있습니다.",
      "ja": "このテキストは、PyTorchの内部構造に関する講演をまとめたもので、ライブラリに貢献したいが複雑なC++コードに戸惑っているユーザーを対象としています。\n\nまず、テンソルについて理解することが重要です。テンソルはPyTorchの主要なデータ構造で、n次元のデータを表します。テンソルはデータそのもの、サイズや型などのメタデータ、そしてメモリ内の要素を特定するためのストライドから構成されています。\n\n次に、テンソルのストレージとビューについて説明します。テンソルは同じ基盤となるストレージを共有できるため、データを重複させることなく異なるビューを持つことができます。これは、非連続な要素にアクセスする方法を示すストライドによって管理されています。\n\n自動微分についても触れます。PyTorchは逆モードの自動微分を使用して、ニューラルネットワークの勾配を計算します。これには、テンソル操作における追加のメタデータが必要です。\n\nコードの構造については、PyTorchのコードベースにはいくつかの重要なディレクトリがあります。Pythonモジュールは「torch/」、Pythonと連携するC++コードは「torch/csrc/」、テンソル操作は「aten/」、コアの抽象化は「c10/」に格納されています。\n\nカーネルの作成については、メタデータの設定、エラーチェック、適切なデータアクセスが必要です。PyTorchは、エラーハンドリングやディスパッチ用のマクロを含む効率的なカーネル作成のためのツールを提供しています。\n\n効率的なワークフローのためのベストプラクティスとしては、ヘッダーの編集を最小限に抑えること、変更をテストするためにCIを使用すること、ローカル開発環境を活用して迅速な反復を行うことが挙げられます。\n\n最後に、PyTorchへの貢献を促しています。コーディング、ドキュメント作成、バグ報告など、さまざまな形での貢献が奨励されており、支援できる分野を見つけるためのイシュートラッカーも紹介されています。\n\nこの講演は、PyTorchの内部構造をわかりやすくし、ユーザーが効果的に貢献できるように導くことを目的としています。"
    }
  },
  {
    "id": "b6280d3a3041786d",
    "title": {
      "en": "Technicalities of Homeworld 2 Backgrounds",
      "ko": "홈월드2 배경의 비밀",
      "ja": "ホームワールド2の背景技術"
    },
    "type": "story",
    "url": "https://simonschreibt.de/gat/homeworld-2-backgrounds/",
    "score": 207,
    "by": "todsacerdoti",
    "time": 1742735655,
    "content": "I made a game! Buy on Steam\n\n\t\t\t\t\t\tA Game Art TrickHomeworld 2 – Backgrounds\n\nRussian Version by Eliiah\n\nKorean Version by Woodorl Kim\n\nWhat you see here\n\nis the stunning background art\n\nof one of the most beautiful sci-fi games.\n\n H o m e w o r l d 2\nThanks for reading.\nJust kidding. Of course i have something to say about this. In the company we look at the art of Homeworld from time to time and bow to the creators of this masterpiece. Once we talked about how great the background look and how interesting this sketched style is. There is something…some details seem…special to us.\n\nI mentioned, that this looks a bit like… a vertex color gradient. But they wouldn’t paint the background on geometry, right? I mean…that would has to be a highly tessellated sphere.\nThe discussion was over but I wasn’t satisfied and wanted at least see the textures. So i used some mod tools to extract the Homeworld 2 Demo data but there were no textures. Only some .HOD files. I used Google and found a thread how to generate these .HOD files from a .TGA. It was said:\n\n“…scans every pixel of the image then based on contrast\nit decides whether or not to add a new vertex and color…”\n\nWhat?\nCould it really be, that this is vertex color? Luckily you can watch at .HOD file with CFHodEdit. And another tool can force a wireframe mode. And now look what this brought to light:\nThis is one\n\nof the most brave\n\nsolutions for game art i ever saw.\n\nAnd here you can see how this influences the sky sphere geometry of the game. Do you see how low the resolution is in the low contrast areas? And how round the sphere is where details were painted?\n\nI never ever had thought, that this can produce such good results. Oh and don’t forget that this technique solves two major problems.\n#1 You don’t have any problems with DDS texture compression artifacts.\n#2 More important from composition perspective: since you can’t get too fine detail (it was said in the tutorial that the base TGA shouldn’t contain too sharp details), the background stays were it should:\nIn the background.\nToo often i see games where the background contains so much noise and details, that you can’t really separate fore-/midground from background.\nThe last time i saw this perfect combination of tech & composition was in Diablo 3. I talk about the 2.5D tree article.\nIf you want know more about how these spheres are generated, read my next article about this topic.\nThanks for reading.\n\nUpdate 1\n\nOskar Stålberg used the Homeworld-Background-Idea in his personal project which looks soooo gorgeous! :,)\nSource: Planet Demo by Oskar Stålberg\n\nUpdate 2\n\nChris Correia works on a space game and asked me about the stars in the Homeworld-Backgrounds because they are super-sharp. I remembered having seen a thread like this a while ago and here it is!\nIn fact, the stars are single textures/billboards:\nSource: Background Star Fields\n\n\t\t\t\t\t23 Replies\n\n\t\t\t\tWant more?\n\t\t\t\tRead more articles like this one by checking out the Game Art Trick Section! By the way: I've a Ko-fi, a Gumroad and a Discord. Have a very nice day! <3\n\n\t\t\tThis entry was posted in Game Art Tricks and tagged Backgrounds, Homeworld 2 on 15. March 2013 by Simon.",
    "summary": {
      "en": "The text discusses the impressive background art of the sci-fi game Homeworld 2. The author expresses admiration for the game's artistic style, particularly the way the backgrounds are created using a technique that generates vertex color gradients from images. This method avoids issues with texture compression artifacts and helps keep background details less distracting, allowing for better separation between foreground and background elements.\n\nThe author shares their experience of exploring the game's art further by extracting game data and analyzing it with tools. They highlight the effectiveness of this technique, comparing it to other games like Diablo 3. The article also mentions updates about other game developers inspired by Homeworld's art style.\n\nIn summary, the text celebrates the innovative art techniques used in Homeworld 2 and how they contribute to the game's beauty and composition.",
      "ko": "이 글에서는 사이언스 픽션 게임인 홈월드 2의 인상적인 배경 아트에 대해 다룹니다. 저자는 게임의 예술적 스타일에 감탄하며, 특히 배경이 이미지에서 정점 색상 그라디언트를 생성하는 기법으로 만들어진 점을 강조합니다. 이 방법은 텍스처 압축으로 인한 문제를 피할 수 있으며, 배경의 세부 사항이 덜 방해가 되도록 도와줍니다. 이렇게 하면 전경과 배경 요소 간의 구분이 더 잘 이루어집니다.\n\n저자는 게임 데이터를 추출하고 도구를 사용해 분석하면서 게임의 예술을 더 깊이 탐구한 경험을 공유합니다. 이 기법의 효과성을 강조하며, 디아블로 3와 같은 다른 게임과 비교합니다. 또한, 홈월드의 예술 스타일에 영감을 받은 다른 게임 개발자들에 대한 업데이트도 언급합니다.\n\n결론적으로, 이 글은 홈월드 2에서 사용된 혁신적인 예술 기법을 찬양하며, 이러한 기법이 게임의 아름다움과 구성에 어떻게 기여하는지를 설명합니다.",
      "ja": "このテキストでは、SFゲーム「Homeworld 2」の背景アートについての素晴らしさが語られています。著者は、このゲームのアートスタイルに感銘を受けており、特に背景が画像から生成される頂点カラーグラデーション技術を使って作られている点を評価しています。この方法は、テクスチャ圧縮によるアーティファクトの問題を回避し、背景の詳細が目立ちすぎないようにすることで、前景と背景の要素の分離をより良くしています。\n\n著者は、ゲームデータを抽出してツールを使って分析することで、ゲームのアートをさらに探求した経験を共有しています。この技術の効果を強調し、他のゲーム、例えば「Diablo 3」と比較しています。また、Homeworldのアートスタイルに触発された他のゲーム開発者に関する最新情報も紹介されています。\n\n要するに、このテキストは「Homeworld 2」で使用されている革新的なアート技術を称賛し、それがゲームの美しさや構成にどのように寄与しているかを伝えています。"
    }
  },
  {
    "id": "99e9a4ff0d6d0812",
    "title": {
      "en": "Landrun: Sandbox any Linux process using Landlock, no root or containers",
      "ko": "랜드락: 루트 없이 리눅스 프로세스 샌드박스화",
      "ja": "ランドロックでLinuxプロセスをサンドボックス化"
    },
    "type": "story",
    "url": "https://github.com/Zouuup/landrun",
    "score": 503,
    "by": "Zoup",
    "time": 1742651819,
    "content": "landrun\nA lightweight, secure sandbox for running Linux processes using Landlock. Think firejail, but with kernel-level security and minimal overhead.\nLinux Landlock is a kernel-native security module that lets unprivileged processes sandbox themselves - but nobody uses it because the API is ... hard!\nLandrun is designed to make it practical to sandbox any command with fine-grained filesystem and network access controls. No root. No containers. No SELinux/AppArmor configs.\nIt's lightweight, auditable, and wraps Landlock v5 features (file access + TCP restrictions).\nFeatures\n\n🔒 Kernel-level security using Landlock\n🚀 Lightweight and fast execution\n🛡️ Fine-grained access control for directories\n🔄 Support for read and write paths\n⚡ Path-specific execution permissions\n🌐 TCP network access control (binding and connecting)\n\nDemo\n\nRequirements\n\nLinux kernel 5.13 or later with Landlock enabled\nLinux kernel 6.7 or later for network restrictions (TCP bind/connect)\nGo 1.18 or later (for building from source)\n\nInstallation\nQuick Install\ngo install github.com/zouuup/landrun/cmd/landrun@latest\n\nFrom Source\ngit clone https://github.com/zouuup/landrun.git\ncd landrun\ngo build -o landrun cmd/landrun/main.go\nsudo cp landrun /usr/local/bin/\n\nUsage\nBasic syntax:\nlandrun [options] <command> [args...]\n\nOptions\n\n--ro <path>: Allow read-only access to specified path (can be specified multiple times or as comma-separated values)\n--rox <path>: Allow read-only access with execution to specified path (can be specified multiple times or as comma-separated values)\n--rw <path>: Allow read-write access to specified path (can be specified multiple times or as comma-separated values)\n--rwx <path>: Allow read-write access with execution to specified path (can be specified multiple times or as comma-separated values)\n--bind-tcp <port>: Allow binding to specified TCP port (can be specified multiple times or as comma-separated values)\n--connect-tcp <port>: Allow connecting to specified TCP port (can be specified multiple times or as comma-separated values)\n--env <var>: Environment variable to pass to the sandboxed command (format: KEY=VALUE or just KEY to pass current value)\n--best-effort: Use best effort mode, falling back to less restrictive sandbox if necessary [default: disabled]\n--log-level <level>: Set logging level (error, info, debug) [default: \"error\"]\n\nImportant Notes\n\nYou must explicitly add the directory to the command you want to run with --rox flag\nFor system commands, you typically need to include /usr/bin, /usr/lib, and other system directories\nUse --rwx for directories where you need both write access and the ability to execute files\nNetwork restrictions require Linux kernel 6.7 or later with Landlock ABI v4\nBy default, no environment variables are passed to the sandboxed command. Use --env to explicitly pass environment variables\nThe --best-effort flag allows graceful degradation on older kernels that don't support all requested restrictions\nPaths can be specified either using multiple flags or as comma-separated values (e.g., --ro /usr,/lib,/home)\n\nEnvironment Variables\n\nLANDRUN_LOG_LEVEL: Set logging level (error, info, debug)\n\nExamples\n\nRun a command with read-only access to a directory:\n\nlandrun --rox /usr/ --ro /path/to/dir ls /path/to/dir\n\nRun a command with write access to a directory:\n\nlandrun --rox /usr/bin --ro /lib --rw /path/to/dir touch /path/to/dir/newfile\n\nRun a command with execution permissions:\n\nlandrun --rox /usr/ --ro /lib,/lib64 /usr/bin/bash\n\nRun with debug logging:\n\nlandrun --log-level debug --rox /usr/ --ro /lib,/lib64,/path/to/dir ls /path/to/dir\n\nRun with network restrictions:\n\nlandrun --rox /usr/ --ro /lib,/lib64 --bind-tcp 8080 --connect-tcp 80 /usr/bin/my-server\n\nThis will allow the program to only bind to TCP port 8080 and connect to TCP port 80.\n\nRun a DNS client with appropriate permissions:\n\nlandrun --log-level debug --ro /etc,/usr --rox /usr/ --connect-tcp 443 nc kernel.org 443\n\nThis allows connections to port 443, requires access to /etc/resolv.conf for resolving DNS.\n\nRun a web server with selective network permissions:\n\nlandrun --rox /usr/bin --ro /lib,/lib64,/var/www --rwx /var/log --bind-tcp 80,443 /usr/bin/nginx\n\nRunning anything without providing parameters is... maximum security jail!\n\nlandrun ls\n\nIf you keep getting permission denied without knowing what exactly going on, best to use strace with it.\n\nlandrun --rox /usr strace -f -e trace=all ls\n\nRun with specific environment variables:\n\nlandrun --rox /usr --ro /etc --env HOME --env PATH --env CUSTOM_VAR=my_value -- env\n\nThis example passes the current HOME and PATH variables, plus a custom variable named CUSTOM_VAR.\nSecurity\nlandrun uses Linux's Landlock to create a secure sandbox environment. It provides:\n\nFile system access control\nDirectory access restrictions\nExecution control\nTCP network restrictions\nProcess isolation\n\nLandlock is an access-control system that enables processes to securely restrict themselves and their future children. As a stackable Linux Security Module (LSM), it creates additional security layers on top of existing system-wide access controls, helping to mitigate security impacts from bugs or malicious behavior in applications.\nLandlock Access Control Rights\nlandrun leverages Landlock's fine-grained access control mechanisms, which include:\nFile-specific rights:\n\nExecute files (LANDLOCK_ACCESS_FS_EXECUTE)\nWrite to files (LANDLOCK_ACCESS_FS_WRITE_FILE)\nRead files (LANDLOCK_ACCESS_FS_READ_FILE)\nTruncate files (LANDLOCK_ACCESS_FS_TRUNCATE) - Available since Landlock ABI v3\n\nDirectory-specific rights:\n\nRead directory contents (LANDLOCK_ACCESS_FS_READ_DIR)\nRemove directories (LANDLOCK_ACCESS_FS_REMOVE_DIR)\nRemove files (LANDLOCK_ACCESS_FS_REMOVE_FILE)\nCreate various filesystem objects (char devices, directories, regular files, sockets, etc.)\nRefer/reparent files across directories (LANDLOCK_ACCESS_FS_REFER) - Available since Landlock ABI v2\n\nNetwork-specific rights (requires Linux 6.7+ with Landlock ABI v4):\n\nBind to specific TCP ports (LANDLOCK_ACCESS_NET_BIND_TCP)\nConnect to specific TCP ports (LANDLOCK_ACCESS_NET_CONNECT_TCP)\n\nLimitations\n\nLandlock must be supported by your kernel\nNetwork restrictions require Linux kernel 6.7 or later with Landlock ABI v4\nSome operations may require additional permissions\nFiles or directories opened before sandboxing are not subject to Landlock restrictions\n\nKernel Compatibility Table\n\nFeature\nMinimum Kernel Version\nLandlock ABI Version\n\nBasic filesystem sandboxing\n5.13\n1\n\nFile referring/reparenting control\n5.19\n2\n\nFile truncation control\n6.2\n3\n\nNetwork TCP restrictions\n6.7\n4\n\nIOCTL on special files\n6.10\n5\n\nTroubleshooting\nIf you receive \"permission denied\" or similar errors:\n\nEnsure you've added all necessary paths with --ro or --rw\nTry running with --log-level debug to see detailed permission information\nCheck that Landlock is supported and enabled on your system:\ngrep -E 'landlock|lsm=' /boot/config-$(uname -r)\n# alternatively, if there are no /boot/config-* files\nzgrep -iE 'landlock|lsm=' /proc/config.gz\n# another alternate method\ngrep -iE 'landlock|lsm=' /lib/modules/$(uname -r)/config\n\nYou should see CONFIG_SECURITY_LANDLOCK=y and lsm=landlock,... in the output\nFor network restrictions, verify your kernel version is 6.7+ with Landlock ABI v4:\nuname -r\n\nTechnical Details\nImplementation\nThis project uses the landlock-lsm/go-landlock package for sandboxing, which provides both filesystem and network restrictions. The current implementation supports:\n\nRead/write/execute restrictions for files and directories\nTCP port binding restrictions\nTCP port connection restrictions\nBest-effort mode for graceful degradation on older kernels\n\nBest-Effort Mode\nWhen using --best-effort (disabled by default), landrun will gracefully degrade to using the best available Landlock version on the current kernel. This means:\n\nOn Linux 6.7+: Full filesystem and network restrictions\nOn Linux 6.2-6.6: Filesystem restrictions including truncation, but no network restrictions\nOn Linux 5.19-6.1: Basic filesystem restrictions including file reparenting, but no truncation control or network restrictions\nOn Linux 5.13-5.18: Basic filesystem restrictions without file reparenting, truncation control, or network restrictions\nOn older Linux: No restrictions (sandbox disabled)\n\nFuture Features\nBased on the Linux Landlock API capabilities, we plan to add:\n\n🔒 Enhanced filesystem controls with more fine-grained permissions\n🌐 Support for UDP and other network protocol restrictions (when supported by Linux kernel)\n🔄 Process scoping and resource controls\n🛡️ Additional security features as they become available in the Landlock API\n\nAcknowledgements\nThis project wouldn't exist without:\n\nLandlock, the kernel security module enabling unprivileged sandboxing - maintained by @l0kod\ngo-landlock, the Go bindings powering this tool - developed by @gnoack\n\nLicense\nThis project is licensed under the GNU General Public License v2\nContributing\nContributions are welcome! Please feel free to submit a Pull Request.",
    "summary": {
      "en": "**Landrun Summary**\n\nLandrun is a lightweight and secure tool for running Linux processes in a sandbox using the Landlock security module. It allows unprivileged processes to create their own secure environments without requiring root access or complex configurations. Here are the key features and details:\n\n### Key Features:\n- **Kernel-level security**: Uses Landlock for enhanced security.\n- **Lightweight**: Minimal performance overhead.\n- **Fine-grained access control**: Manage permissions for file system and network access.\n- **No root or containers needed**: Simple setup with no need for complex security frameworks like SELinux or AppArmor.\n\n### Requirements:\n- Linux kernel 5.13 or later with Landlock enabled.\n- Go 1.18 or later for building from source.\n\n### Installation:\n- Quick installation: `go install github.com/zouuup/landrun/cmd/landrun@latest`\n- From source: Clone the repository, build it, and copy the executable.\n\n### Usage:\nRun commands with specific permissions using options like:\n- `--ro <path>`: Read-only access.\n- `--rw <path>`: Read-write access.\n- `--bind-tcp <port>`: Allow binding to a TCP port.\n\n### Security Features:\n- Controls access to files and directories.\n- Isolates processes to prevent unauthorized access.\n- Supports TCP network restrictions for enhanced security.\n\n### Limitations:\n- Requires a compatible Linux kernel.\n- Some operations may require additional permissions.\n\n### Troubleshooting:\nIf you encounter permission errors, check that you've specified all necessary paths and that your kernel supports Landlock.\n\n### Future Plans:\nFuture updates may include enhanced filesystem controls, support for more network protocols, and additional security features.\n\nThis project is open for contributions and is licensed under the GNU General Public License v2.",
      "ko": "Landrun은 Landlock 보안 모듈을 사용하여 리눅스 프로세스를 샌드박스에서 실행할 수 있도록 돕는 가볍고 안전한 도구입니다. 이 도구는 권한이 없는 프로세스가 루트 접근이나 복잡한 설정 없이도 자신만의 안전한 환경을 만들 수 있게 해줍니다. 주요 특징은 다음과 같습니다.\n\n첫째, 커널 수준의 보안을 제공합니다. Landlock을 사용하여 보안을 강화하며, 성능 저하가 최소화된 경량 설계로 되어 있습니다. 둘째, 세밀한 접근 제어가 가능하여 파일 시스템과 네트워크 접근 권한을 관리할 수 있습니다. 셋째, 루트 권한이나 컨테이너가 필요 없어 SELinux나 AppArmor와 같은 복잡한 보안 프레임워크 없이 간단하게 설정할 수 있습니다.\n\nLandrun을 사용하기 위해서는 Linux 커널 5.13 이상이 필요하며, Landlock이 활성화되어 있어야 합니다. 소스에서 빌드하려면 Go 1.18 이상이 필요합니다. 설치는 간단하며, `go install github.com/zouuup/landrun/cmd/landrun@latest` 명령어로 빠르게 설치할 수 있습니다. 또는 저장소를 클론한 후 빌드하여 실행 파일을 복사하는 방법도 있습니다.\n\n사용 시에는 특정 권한으로 명령을 실행할 수 있으며, 예를 들어 `--ro <path>`를 사용하면 읽기 전용 접근이 가능하고, `--rw <path>`를 사용하면 읽기-쓰기 접근이 가능합니다. 또한 `--bind-tcp <port>` 옵션을 통해 TCP 포트에 바인딩할 수 있습니다.\n\n보안 기능으로는 파일과 디렉토리에 대한 접근을 제어하고, 프로세스를 격리하여 무단 접근을 방지합니다. TCP 네트워크 제한을 지원하여 보안을 더욱 강화합니다. 그러나 이 도구는 호환되는 리눅스 커널이 필요하며, 일부 작업은 추가 권한이 필요할 수 있습니다.\n\n문제 해결을 위해 권한 오류가 발생하면 필요한 모든 경로를 정확히 지정했는지 확인하고, 커널이 Landlock을 지원하는지 점검해야 합니다. 향후 업데이트에서는 파일 시스템 제어 강화, 더 많은 네트워크 프로토콜 지원, 추가 보안 기능이 포함될 수 있습니다. 이 프로젝트는 기여를 환영하며, GNU 일반 공용 라이선스 v2에 따라 라이선스가 부여됩니다.",
      "ja": "Landrunは、Landlockセキュリティモジュールを使用してLinuxプロセスをサンドボックス内で実行するための軽量で安全なツールです。このツールは、特権のないプロセスがルートアクセスや複雑な設定なしに独自の安全な環境を作成できるようにします。\n\n主な特徴として、カーネルレベルのセキュリティがあり、Landlockを利用してセキュリティを強化しています。また、パフォーマンスへの影響が最小限で、ファイルシステムやネットワークアクセスの権限を細かく管理できます。さらに、ルートやコンテナを必要とせず、SELinuxやAppArmorのような複雑なセキュリティフレームワークなしで簡単に設定できます。\n\n必要な条件としては、Landlockが有効なLinuxカーネル5.13以降が必要です。また、ソースからビルドするためにはGo 1.18以降が必要です。\n\nインストールは簡単で、コマンドを使ってすぐに行えます。具体的には、`go install github.com/zouuup/landrun/cmd/landrun@latest`を実行するか、リポジトリをクローンしてビルドし、実行可能ファイルをコピーします。\n\n使用方法は、特定の権限を持つコマンドを実行するために、オプションを指定します。例えば、`--ro <path>`で読み取り専用アクセス、`--rw <path>`で読み書きアクセス、`--bind-tcp <port>`でTCPポートへのバインドを許可します。\n\nセキュリティ機能としては、ファイルやディレクトリへのアクセスを制御し、プロセスを隔離して不正アクセスを防ぎます。また、TCPネットワークの制限をサポートしており、セキュリティを強化しています。\n\n制限事項としては、互換性のあるLinuxカーネルが必要であり、一部の操作には追加の権限が必要な場合があります。もし権限エラーが発生した場合は、必要なすべてのパスを指定しているか、カーネルがLandlockをサポートしているかを確認してください。\n\n今後の計画としては、ファイルシステムの制御の強化や、より多くのネットワークプロトコルのサポート、追加のセキュリティ機能が含まれる可能性があります。このプロジェクトはオープンで、GNU一般公衆ライセンスv2の下でライセンスされています。"
    }
  },
  {
    "id": "7ebe6c4ba0f90ac6",
    "title": {
      "en": "Derivatives and Logarithms of 3D Transforms",
      "ko": "3D 변환의 미적분",
      "ja": "3D変換の微分と対数"
    },
    "type": "story",
    "url": "https://nosferalatu.com/./DerivativesLogarithmsTransforms.html",
    "score": 104,
    "by": "nosferalatu123",
    "time": 1742515596,
    "content": "Derivatives, Logarithms, andTransforms\n\n    Date\n\n         Tue 07 February 2023\n\nTags\n\tProgramming\n\n                Given a transform TTT and a point x, we can find the transformed point with T∗xT∗xT * x. But what if we want to smoothly interpolate TTT so it moves xxx along the path from its initial position to its position transformed by TTT?\nWhat we want to find is the point xxx at time ttt:\nx(t)=T(t)∗x(0)x(t)=T(t)∗x(0)x(t) = T(t) *x(0)\nwhere x(0)x(0)x(0) is the point’s initial position, and T(t)T(t)T(t) is the transform at time ttt. Since we have only a single transform TTT, we need to find a way to interpolate it overtime.\nOne way to accomplish this is to raise TTT to the power of ttt, which can be done using the exponential and logarithm of the transform. Interestingly, the logarithm of a transform can also be used to easily find the velocity of a point xxx in space: the velocity vector (also called the tangent vector) is just log(T)∗xlog(T)∗xlog(T) * x. This blog post shows the relationship between the logarithm andvelocity.\nExample\nCheck out this interactive example to see how the vector field changes as you manipulate the gizmo to translate and rotate the transform. The vector field represents the velocity vector at each point in space during thetransformation.\nAs you move the gizmo, you’ll notice a white curve that traces the path from the origin to the gizmo’s transform. Along this curve, you’ll see the interpolated transform as it travels from the origin to the gizmo. As you can see, the interpolation follows the flow of the velocity vector field. The applet’s code is using the exponential and logarithm of the transform to compute the curve, interpolated transform, and vectorfield.\n\n   ControlsGizmoModeRotateTranslateRotateShowVectorFieldShowOriginAxesShowGizmoShowInterpolatedTransform\n\nThe source code for the applet can be found here, which includes an implementation of closed-form log() and exp() for rigid bodytransforms.\nNext, I’ll describe how to compute the interpolated transform and the velocity vector field you see in thisexample.\nWhat’s T(t)T(t)T(t)?\nWe have TTT, but not T(t)T(t)T(t), which changes with time. Assuming that multiplying two transforms represents the composition of those transforms, we can find T(t)T(t)T(t) bysaying\nT(0)=IT(0)=IT(0) = I (the identitytransform)\nT(1)=TT(1)=TT(1) =T\nT(2)=T∗TT(2)=T∗TT(2) = T *T\nT(3)=T∗T∗TT(3)=T∗T∗TT(3) = T * T *T\nMore generally, we can find TTT at any time bysaying\nT(t)=TtT(t)=TtT(t) = T^t.\nThe above trick is from a blog post by Fabian Giesen here but works for any transform that uses multiplication forcomposition.\nNow that we know T(t)=TtT(t)=TtT(t) = T^t, the original equation can be rearrangedto\nx(t)=Tt∗x(0)x(t)=Tt∗x(0)x(t) = T^t * x(0).\nWhat’s TtTtT^t?\nTo compute TtT^t, we need to use the matrix exponential and matrixlogarithm.\nLet’s start with two facts about a matrixX:\nelog(X)=Xe^{log(X)} = Xand\nlog(Xy)=log(X)∗ylog(X^y) = log(X) * y.\nPut together, we can saythat\nTt=elog(Tt)=elog(T)∗tT^t = e^{log(T^t)} =e^{log(T)*t}\nwhich we can plug into the earlier equation, givingus\nx(t)=elog(T)∗t∗x(0)x(t) = e^{log(T) * t} * x(0).\nThis says that to find the point x at time t, find the transform at time t using elog(T)∗te^{log(T) * t}, and use that to transform the point at its initial position (at time0).\nWhat’s thederivative?\nIn calculus, we learnedthat\nddteat=aeat\\dfrac{d}{dt}e^{a t} = a e^{at}\nwhich holds true for matrices aswell:\nddteAt=AeAt\\dfrac{d}{dt}e^{A t} = A e^{At}\nThis relationship is explained in more detail in the section the derivative of the matrix exponential.\nWe can use this property to find the derivative of our earlier equation x(t)=elog(T)tx(0)x(t) = e^{log(T)t} x(0) with respect tot:\nddtx(t)=log(T)elog(T)tx(0)\\dfrac{d}{dt}x(t) = log(T) e^{log(T) t} x(0).\nThis equation states that to find the first derivative (the velocity vector, also called the tangent vector) of the point at time t, you first transform the point’s initial position x(0)x(0) with the interpolated transform elog(T)te^{log(T)t} and then multiply it by the logarithm of the transform log(T)log(T). This expression follows the right-to-left convention of column vectors, so you would start with the initial position x(0)x(0), then apply the interpolated transform elog(T)te^{log(T)t}, and finally multiply by the logarithm log(T)log(T).\nelog(T)te^{log(T) t} acts as an operator that maps points from their initial position to their new position at time t. The matrix exponential can be thought of as like integration. At time 0, elog(T)te^{log(T) t} is the identity matrix (e0=Ie^0=I for matrix exponentials), and at time 1.0, elog(T)te^{log(T) t} is equal to the original transform matrix T (elog(T)=Te^{log(T)}=T).\nWhat’s this allmean?\nIf we take the equation at the end of “What’s TtT^t?”\nx(t)=elog(T)tx(0)x(t) = e^{log(T) t}x(0)\nand substitute that into the equation at the end of “What’s thederivative?”\nddtx(t)=log(T)elog(T)tx(0)\\dfrac{d}{dt}x(t) = log(T) e^{log(T) t} x(0),\nthen wehave:\nddtx(t)=log(T)x(t)\\dfrac{d}{dt}x(t) = log(T) x(t).\nThis relates the derivative of a moving point to the logarithm of the transformation moving thatpoint.\nOne way to think of log(T)log(T) is as a vector field of tangent vectors for the transformation. In other words, it’s the field of first derivatives. This vector field is independent of time and shows the velocity for every point inspace.\nThat equation is saying that if you transform any point in space by the logarithm of the transform, you will get the first derivative at that point. The first derivative is the velocity, so log(T)log(T) defines the velocity field (the field of tangent vectors at every point inspace).\nAs a point moves through space by the transform, it forms a curve. The tangent vector at time t is tangent to the point’s position on the curve at timet.\nYou can think of the logarithm of a matrix as the velocity field of the action performed by that matrix. The velocity field visualized in the interactive example above is thisfield.\nA more informal way of looking at this is tosay\nvelocity=log(transform)∗positionvelocity = log(transform) *position\nmeaning, to understand how a point will move in time, look at the vector field of the log of the transform as a velocity field. As the point flows along that velocity field, it moves intime.\nWhat’s the differentialequation?\nWe can also reformulate all of this as a differential equation. Earlier, wehad\nddtx(t)=log(T)x(t)\\dfrac{d}{dt} x(t) = log(T)x(t)\nwhich is a differential equation. Because log(T)log(T) is a matrix, it is more specifically a matrix differentialequation.\nScalar ordinary differential equations of theform\ny′(t)=ay(t)y'(t)=ay(t)\nhave the generalsolution\ny(t)=eaty(0)y(t)=e^{at}y(0).\nSimilarly, matrix differential equations of theform\nx′(t)=Ax(t)x'(t)=Ax(t)\nhave the generalsolution\nx(t)=eAtx(0)x(t)=e^{At}x(0).\nTherefore, given our equation fromearlier\nddtx(t)=log(T)x(t)\\dfrac{d}{dt} x(t) = log(T)x(t)\nwe have thesolution\nx(t)=elog(T)tx(0)x(t) = e^{log(T) t} x(0).\nThis is the same as our original equation, but we started with a differential equation and found a solution. To prove this solution is correct, just take the derivative of it, which is what we did earlier in the What’s the derivative?section.\nThe exponential map and logarithmmap\nThe exponential map is defined as the infiniteseries\neAt=I+At+12(At)2+13!(At)3+...=∞∑i=0(At)ii!e^{A t} = I + A t + \\frac{1}{2}(A t)^2 + \\frac{1}{3!}(A t)^3 + ... = \\sum_{i=0}^{\\infty} \\frac{(At)^i}{i!}\nand can be used to find the exponential of real numbers, complex numbers, quaternions, matrices, and more. For example, when square matrices are plugged in to the series, the result is called the matrix exponential.\nSimilarly, the logarithm is defined as the infiniteseries\nlog(A)=∞∑i=1(−1)i+1(A−I)iilog(A) = \\sum_{i=1}^{\\infty} (-1)^{i+1} \\frac{(A - I)^i}{i}\nIf you want to know more, search for the exponential map and logarithm map. You’ll find that these are important concepts in Lie group theory. The exponential map and logarithm map are inverses of each other. In Lie theory, the exponential map maps a tangent vector at a point p to a point on the manifold. The logarithm map does the opposite, mapping a point on the manifold back to the tangent vector atp.\nWhen reading about Lie groups, you’ll come across many different kinds of groups. There are only a few groups that are related to transforms, though. SO(3) is a 3D rotation matrix, SU(2) is a quaternion, SE(3) is a 3D rigid body transform (rotation and translation), SIM(3) is rotation, translation, and (positive) uniform scale, and GL(n) is an nxnmatrix.\nThere are several options for how to practically compute the exponential and logarithm map for a matrix or otherobject:\n1) Use a math library like Eigen or Armadillo. These have functions to compute the matrix exponential and matrixlogarithm.\n2) The library Sophus has code for a closed form exp/log for the groups SO(3), SE(3), and SIM(3). Beware that it clamps its quaternions to a 3D rotation angle in -π\\pi … +π\\pithough.\n3) There is an excellent PDF at the web site of Ethan Eade here which contains the closed form equations for the groups SO(3), SE(3), and SIM(3).\n4) Compute the matrix exponential and logarithm by using the infinite series definitions above, and truncating after some number of terms. In my experience, this is not robust when working with floating point numbers, as you quickly start to deal with very small and very large numbers, depending on your inputmatrix.\n5) Compute the exponential with numerical integration. Given a starting point xx, integrating it for time t is the same thing as the exponential. There are many ways to compute numerical integration, from Euler to Runge-Kutta to adaptivemethods.\nPitfalls\nThere are a few issues that you should be awareof.\nPitfall#1\nThe logarithm of a rotation matrix will return a 3D rotation angle in -π\\pi … +π\\pi. More technically, there are an infinite number of logarithms for a matrix, each corresponding to a rotation angle that is  2π\\pi greater than the previous one. Generally matrix logarithm code will return the principal logarithm, which is the logarithm in -π\\pi … +π\\pi. This can lead to discontinuities when interpolating transforms with rotations in them, such rotations from human joints (you can move your head from looking over your left shoulder to over your right shoulder and rotate a little more than 180degrees).\nOn the other hand, the logarithm of a quaternion returns a 3D rotation angle in the larger range of -2π\\pi … +2π\\pi, which makes quaternions nicer to workwith.\nPitfall#2\nWhen working with logarithms, be aware that that theproperty\nlog(AB)=log(A)+log(B)log(AB) = log(A) +log(B)\nis only true when A and B commute, which is not the case for most transforms. Real numbers always commute, though, so the property does apply to them. It’s tempting to apply the property to transforms, but it’s important to remember it only applies when A and Bcommute.\nPitfall#3\nRelated to pitfall #2, you might want to interpolate two transforms A and Bwith\ninterpolate(A,B,t)=e(1−t)∗log(A)+t∗log(B)interpolate(A, B, t) = e^{(1-t)*log(A) +t*log(B)}\nBut be careful: this only works if A and B commute, which is not usually the case for transforms. Otherwise, this interpolation is neither shortest path nor constantspeed.\nInstead, interpolate the relative (also called delta) transform from A to B, likethis:\ninterpolate(A,B,t)=elog(BA−1)tAinterpolate(A, B, t) = e^{log(B A^{-1}) t}A\nHowever, this method only works for interpolating between two transforms and not for blending more than twotransforms.\nVisualizing a matrix as a vectorfield\nIf you are wondering how you can visualize a matrix as a vector field, an eloquent explanation is in 3Blue1Brown’s video about matrix exponentiation. This part about matrices as vector fields explains that verywell:\n\nThe derivative of the matrixexponential\nEarlier we used the property ddteAt=AeAt\\dfrac{d}{dt}e^{A t} = A e^{A t}. It’s not obvious why this property is true, but it’s an important part of unlocking all ofthis.\nA good reference for this derivation is in the textbook Modern Robotics. A free copy of that book can be found here. See equation (3.43) in thatbook.\nThe matrix exponential is definedas\neAt=I+At+12(At)2+13!(At)3+...e^{A t} = I + A t + \\frac{1}{2}(A t)^2 + \\frac{1}{3!}(A t)^3 +...\nWhat then is ddteAt\\dfrac{d}{dt}e^{A t}? If we take the derivative of each term of the matrix exponential’s expanded definition, wehave\nddteAt=0+A+A2t+12A3t2+...\\dfrac{d}{dt}e^{A t} = 0 + A + A^2 t + \\frac{1}{2} A^3 t^2 +...\nPull out A, and then wehave\nddteAt=A∗(I+At+12(At)2+...)=AeAt\\dfrac{d}{dt}e^{A t} = A*(I + A t + \\frac{1}{2} (A t)^2 + ...) = A e^{A t}.\nIt’s worth noting that the matrix AA can go on the left or right, and it always holds truethat\nAeAt=eAtAAe^{A t} = e^{At}A\nfor any square matrix, as stated in equation (3.44) in ModernRobotics.\nComments\nLeave comments on this post with Github Issues here.\nif (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {\n    var align = \"center\",\n        indent = \"0em\",\n        linebreak = \"false\";\n\n    if (false) {\n        align = (screen.width < 768) ? \"left\" : align;\n        indent = (screen.width < 768) ? \"0em\" : indent;\n        linebreak = (screen.width < 768) ? 'true' : linebreak;\n    }\n\n    var mathjaxscript = document.createElement('script');\n    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';\n    mathjaxscript.type = 'text/javascript';\n    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';\n\n    var configscript = document.createElement('script');\n    configscript.type = 'text/x-mathjax-config';\n    configscript[(window.opera ? \"innerHTML\" : \"text\")] =\n        \"MathJax.Hub.Config({\" +\n        \"    config: ['MMLorHTML.js'],\" +\n        \"    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" +\n        \"    jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" +\n        \"    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" +\n        \"    displayAlign: '\"+ align +\"',\" +\n        \"    displayIndent: '\"+ indent +\"',\" +\n        \"    showMathMenu: true,\" +\n        \"    messageStyle: 'normal',\" +\n        \"    tex2jax: { \" +\n        \"        inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" +\n        \"        displayMath: [ ['$$','$$'] ],\" +\n        \"        processEscapes: true,\" +\n        \"        preview: 'TeX',\" +\n        \"    }, \" +\n        \"    'HTML-CSS': { \" +\n        \"        availableFonts: ['STIX', 'TeX'],\" +\n        \"        preferredFont: 'STIX',\" +\n        \"        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" +\n        \"        linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" +\n        \"    }, \" +\n        \"}); \" +\n        \"if ('default' !== 'default') {\" +\n            \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n            \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n        \"}\";\n\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);\n}",
    "summary": {
      "en": "### Summary of \"Derivatives, Logarithms, and Transforms\"\n\nIn this text, the author discusses how to smoothly interpolate a transform (T) over time to move a point (x) from its initial position to its transformed position. The key formula for this is:\n\n\\[ x(t) = T(t) * x(0) \\]\n\nwhere \\( T(t) \\) changes over time and \\( x(0) \\) is the initial position of the point.\n\nTo interpolate the transform over time, the author suggests using the exponential and logarithm of the transform. The logarithm can also help compute the velocity of the point:\n\n\\[ \\text{velocity} = \\log(T) * x \\]\n\nAn interactive example is provided to visualize how the point moves along a path as the transform changes, demonstrating the relationship between the transform and the velocity vector field.\n\n### Key Concepts:\n1. **Matrix Exponential and Logarithm**: These mathematical concepts are used to compute the transform over time and derive the velocity vector.\n2. **Differential Equation**: The relationship between the transform and its derivative can be expressed as a matrix differential equation:\n   \\[ \\frac{d}{dt} x(t) = \\log(T) * x(t) \\]\n   This means the velocity of the point is related to the logarithm of the transform.\n3. **Interpolation**: The author explains how to interpolate between two transforms, emphasizing the need for caution since the logarithm property only holds for commuting transforms.\n4. **Pitfalls**: The author warns of potential issues when working with matrix logarithms and rotations, as well as the limitations of certain interpolation methods.\n\n### Practical Application:\nTo compute the matrix exponential and logarithm, the author suggests using mathematical libraries or numerical integration methods, and provides resources for further exploration.\n\nOverall, this text emphasizes the mathematical foundations of transforms and their applications in visualizing motion and velocity in space.",
      "ko": "이 글에서는 저자가 변환(T)을 시간에 따라 부드럽게 보간하여 점(x)을 초기 위치에서 변환된 위치로 이동시키는 방법에 대해 설명합니다. 이 과정의 핵심 공식은 다음과 같습니다: x(t) = T(t) * x(0). 여기서 T(t)는 시간이 지남에 따라 변화하고, x(0)는 점의 초기 위치입니다.\n\n변환을 시간에 따라 보간하기 위해 저자는 변환의 지수와 로그를 사용할 것을 제안합니다. 로그는 점의 속도를 계산하는 데에도 도움이 됩니다: 속도 = log(T) * x. 저자는 변환이 변화함에 따라 점이 경로를 따라 어떻게 이동하는지를 시각화할 수 있는 인터랙티브한 예제를 제공합니다. 이를 통해 변환과 속도 벡터 필드 간의 관계를 보여줍니다.\n\n주요 개념으로는 행렬의 지수와 로그가 있습니다. 이러한 수학적 개념은 시간에 따른 변환을 계산하고 속도 벡터를 도출하는 데 사용됩니다. 변환과 그 도함수 간의 관계는 행렬 미분 방정식으로 표현될 수 있습니다: d/dt x(t) = log(T) * x(t). 이는 점의 속도가 변환의 로그와 관련이 있음을 의미합니다. 저자는 두 개의 변환 간의 보간 방법을 설명하며, 로그의 성질이 교환 가능한 변환에만 적용된다는 점에서 주의가 필요하다고 강조합니다. 또한 행렬 로그와 회전 작업을 다룰 때 발생할 수 있는 문제와 특정 보간 방법의 한계에 대해서도 경고합니다.\n\n실용적인 응용으로는 행렬의 지수와 로그를 계산하기 위해 수학 라이브러리나 수치적 적분 방법을 사용할 것을 제안하며, 추가 탐색을 위한 자료도 제공합니다. 전체적으로 이 글은 변환의 수학적 기초와 그것이 공간에서의 운동과 속도를 시각화하는 데 어떻게 활용되는지를 강조합니다.",
      "ja": "この文章では、著者が時間に沿って変換（T）を滑らかに補間し、点（x）を初期位置から変換後の位置に移動させる方法について説明しています。このための重要な式は、x(t) = T(t) * x(0) です。ここで、T(t)は時間とともに変化し、x(0)は点の初期位置を示します。\n\n変換を時間に沿って補間するために、著者は変換の指数関数と対数を使用することを提案しています。対数は点の速度を計算するのにも役立ちます。速度は、velocity = log(T) * x という式で表されます。インタラクティブな例が提供されており、変換が変化するにつれて点がどのように経路に沿って移動するかを視覚化し、変換と速度ベクトル場の関係を示しています。\n\n重要な概念として、行列の指数関数と対数が挙げられます。これらの数学的概念は、時間に沿った変換の計算や速度ベクトルの導出に使用されます。また、変換とその導関数の関係は行列微分方程式として表現できます。具体的には、d/dt x(t) = log(T) * x(t) という式が成り立ち、点の速度が変換の対数に関連していることを示しています。\n\n著者は、2つの変換の間を補間する方法についても説明しており、対数の性質が交換可能な変換にのみ適用されるため、注意が必要であることを強調しています。また、行列の対数や回転を扱う際の潜在的な問題や、特定の補間方法の限界についても警告しています。\n\n実用的な応用として、行列の指数関数と対数を計算するために、著者は数学ライブラリや数値積分法を使用することを提案し、さらなる探求のためのリソースも提供しています。この文章全体を通じて、変換の数学的基盤と、空間における動きや速度を視覚化するための応用が強調されています。"
    }
  },
  {
    "id": "edeff629e4f65bc4",
    "title": {
      "en": "Apple Now Selling USB-C to 3.5mm Audio Cable",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.macrumors.com/guide/apple-selling-usb-c-to-3-5mm-audio-cable/",
    "score": 5,
    "by": "walterbell",
    "time": 1742881259,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "edeff629e4f65bc4",
    "title": {
      "en": "Apple Now Selling USB-C to 3.5mm Audio Cable",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.macrumors.com/guide/apple-selling-usb-c-to-3-5mm-audio-cable/",
    "score": 5,
    "by": "walterbell",
    "time": 1742881259,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "edeff629e4f65bc4",
    "title": {
      "en": "Apple Now Selling USB-C to 3.5mm Audio Cable",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.macrumors.com/guide/apple-selling-usb-c-to-3-5mm-audio-cable/",
    "score": 5,
    "by": "walterbell",
    "time": 1742881259,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "edeff629e4f65bc4",
    "title": {
      "en": "Apple Now Selling USB-C to 3.5mm Audio Cable",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.macrumors.com/guide/apple-selling-usb-c-to-3-5mm-audio-cable/",
    "score": 5,
    "by": "walterbell",
    "time": 1742881259,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  }
]