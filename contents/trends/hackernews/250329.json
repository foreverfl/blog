[
  {
    "id": "981caf3cbe377770",
    "title": {
      "en": "We hacked Gemini's Python sandbox and leaked its source code (at least some)",
      "ko": "제미니 파이썬 해킹!",
      "ja": null
    },
    "type": "story",
    "url": "https://www.landh.tech/blog/20250327-we-hacked-gemini-source-code/",
    "score": 277,
    "by": "topsycatt",
    "time": 1743185578,
    "content": "<<Back to BlogWe hacked Google’s A.I Gemini and leaked its source code (at least some part)Mar 27, 2025RONI CARTA | LUPINgemini, llm, google, source code, leak, bug bounty, hackBack to Vegas, and This Time, We Brought Home the MVH Award !\nIn 2024 we released the blog post We Hacked Google A.I. for $50,000, where we traveled in 2023 to Las Vegas with Joseph \"rez0\" Thacker, Justin \"Rhynorater\" Gardner, and myself, Roni \"Lupin\" Carta, on a hacking journey that spanned from Las Vegas, Tokyo to France, all in pursuit of Gemini vulnerabilities during Google's LLM bugSWAT event. Well, we did it again …\nThe world of Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs) continues to be the Wild West of tech.  Since GPT burst onto the scene, the race to dominate the LLM landscape has only intensified, with tech giants like Meta, Microsoft, and Google racing to have the best model possible. But now there is also Anthropic, Mistral, Deepseek and more that are coming to the scene and impacting the industry at scale.\nAs companies rush to deploy AI assistants, classifiers, and a myriad of other LLM-powered tools, a critical question remains: are we building securely ?  As we highlighted last year, the rapid adoption sometimes feels like we forgot the fundamental security principles, opening the door to novel and familiar vulnerabilities alike.\nAI agents are rapidly emerging as the next game-changer in the world of artificial intelligence. These intelligent entities leverage advanced chains of thought reasoning, a process where the model generates a coherent sequence of internal reasoning steps to solve complex tasks. By documenting their thought processes, these agents not only enhance their decision-making capabilities but also provide transparency, allowing developers and researchers to understand and refine their performance. This dynamic combination of autonomous action and visible reasoning is paving the way for AI systems that are more adaptive, interpretable, and reliable. As we witness an increasing number of applications. from interactive assistants to sophisticated decision-support systems. The integration of chain-of-thought reasoning in AI agents is setting a new standard for what these models can achieve in real-world scenarios.\nGoogle, to their credit, are actively recognising this emerging frontier of AI security, and they started early on.  Their \"LLM bugSWAT\" events, held in vibrant locales like Las Vegas, are a testament to their commitment to proactive security red teaming.  These events challenge researchers worldwide to rigorously test their AI systems, seeking out the vulnerabilities that might otherwise slip through the cracks.\nAnd guess what ? We answered the call again in 2024 !  Justin and I returned to the bugSWAT event in Las Vegas, and this time, our efforts paid off in a big way.  Thanks to a brand new vulnerability in Gemini,  the one we’re about to detail, we were incredibly honored to be awarded the Most Valuable Hacker (MVH) title at this year's Las Vegas bugSWAT !\n\nPicture taken with our MVH award and 2 awesome Googlers <3\nSo, prepare to dive deep once more.  This isn't just a repeat performance; it's a whole new vulnerability that we are about to show you ;)\nDiscovering the new Gemini\nThe Google team granted us early access to a preview of the next Gemini update, one that had several exciting new features. Along with this exclusive access, we received detailed documentation explaining these features and their intended functionalities. The goal was to fully explore and test these capabilities from an attacker’s perspective.\nIt all started with a simple prompt. We asked Gemini:\nrun hello world in python3\n\nGemini provided the code, and the interface offered the enticing \"Run in Sandbox\" button. Intrigued, we started exploring.\n\nGemini's Python Playground – A Secure Space... or Was It ?\nGemini at the time offered a Python Sandbox Interpreter. Think of it as a safe space where you can run Python code generated by the AI itself, or even your own custom scripts, right within the Gemini environment. This sandbox, powered by Google's Gvisor in a GRTE (Google Runtime Environment), is designed to be secure. The idea is you can experiment with code without risking any harm to the underlying system, a crucial feature for testing and development.\ngVisor is a user-space kernel developed by Google that acts as an intermediary between containerized applications and the host operating system. By intercepting system calls made by applications, it enforces strict security boundaries that reduce the risk of container escapes and limit potential damage from compromised processes. Rather than relying solely on traditional OS-level isolation, gVisor implements a minimal, tailored subset of kernel functionalities, thereby reducing the attack surface while still maintaining reasonable performance. This innovative approach enhances the security of container environments, making gVisor an essential tool for safely running and managing containerized workloads.\nAs security researchers and bug bounty hunters, we know that this gVisor sandbox is secured with multiple layers of defense and from what we’ve seen no one managed to escape this sandbox. Actually a sandbox escape could award you a $100k bounty:\n\nWhile it might be possible to still escape it, this is a whole different set of challenges than what we were looking for.\nHowever, sandboxes are not always meant to be escaped since there are a lot of cases where there is stuff inside the sandbox itself that can help us leak data. This idea, shared with us by a Googler from the security team, was to be able to have shell access inside the Sandbox itself and try to find any piece of data that wasn't supposed to be accessible. The main problem was the following: This sandbox can only run a custom compiled Python binary.\nMapping the Territory\nThe first thing we saw is that it was also possible from the Front End to entirely rewrite the Python code and run our arbitrary version in the sandbox. Our first step was to understand the structure of this sandbox. We suspected there might be interesting files lurking around. Since we can’t pop a shell, we checked which libraries were available in this custom compiled Python binary. We found out that os was present ! Great, we can then use it to map the filesystem.\nWe wrote the following Python Code:\nimport os\n\ndef get_size_formatted(size_in_bytes):\n    if size_in_bytes >= 1024 ** 3:\n        size = size_in_bytes / (1024 ** 3)\n        unit = \"Go\"\n    elif size_in_bytes >= 1024 ** 2:\n        size = size_in_bytes / (1024 ** 2)\n        unit = \"Mb\"\n    else:\n        size = size_in_bytes / 1024\n        unit = \"Ko\"\n    return f\"{size:.2f} {unit}\"\n\ndef lslR(path):\n    try:\n        # Determine if the path is a directory or a file\n        if os.path.isdir(path):\n            type_flag = 'd'\n            total_size = sum(os.path.getsize(os.path.join(path, f)) for f in os.listdir(path))\n        else:\n            type_flag = 'f'\n            total_size = os.path.getsize(path)\n\n        size_formatted = get_size_formatted(total_size)\n\n        # Check read and write permissions\n        read_flag = 'r' if os.access(path, os.R_OK) else '-'\n        write_flag = 'w' if os.access(path, os.W_OK) else '-'\n\n        # Print the type, permissions, size, and path\n        print(f\"{type_flag}{read_flag}{write_flag} - {size_formatted} - {path}\")\n\n        # If it's a directory, recursively print the contents\n        if type_flag == 'd':\n            for entry in os.listdir(path):\n                entry_path = os.path.join(path, entry)\n                lslR(entry_path)\n    except PermissionError:\n        print(f\"d-- - 0Ko - {path} (PermissionError: cannot access)\")\n    except Exception as e:\n        print(f\"--- - 0Ko - {path} (Error: {e})\")\n\nThe goal for this code was to have some kind of recursive listing of files and directories function to be able to see which files are present, their size and also their permissions.\nWe’ve used the function to list the lslR(\"/usr\") directory.\n\nThis call identified a binary file located at /usr/bin/entry/entry_point. This sounds juicy !\n\nLeak the entry_point file\nOur next move was to extract this file, but with it being 579Mb in size, directly base64 encoding and printing it in the Front End wasn't an option, it caused the entire sandbox to hang until it eventually timed out.\nWe attempted to see if we could run TCP, HTTP, and DNS calls to exfiltrate information. Intriguingly, all our outbound connection attempts failed, the sandbox appeared completely isolated from the external network. This led to an interesting puzzle: if the sandbox is so tightly isolated that it cannot make external calls, how does it interface with Google services like Google Flights and others ? Well … we might be able to answer this later ;D\nSo we needed to exfiltrate this binary by printing in the console into chunks, for that we used the seek() function to walk through the binary file and retrieve the entire binary in chunks of 10 MB.\nimport os\nimport base64\n\ndef read_and_encode(file_path, kilobytes):\n    try:\n        # Calculate the number of bytes to read\n        num_bytes = kilobytes * 1024\n\n        # Open the file and read the specified number of bytes\n        with open(file_path, 'rb') as file:\n            file_content = file.read(num_bytes)\n\n        # Base64 encode the bytes\n        encoded_content = base64.b64encode(file_content)\n\n        # Print the encoded string\n        print(encoded_content.decode('utf-8'))\n\n    except FileNotFoundError:\n        print(f\"FileNotFoundError: {file_path} does not exist\")\n    except PermissionError:\n        print(f\"PermissionError: Cannot access {file_path}\")\n    except Exception as e:\n        print(f\"Error: {e}\")\n\nread_and_encode(\"/usr/bin/entry/entry_point\", 10000)\n\nWe then used Caido to catch the request in our proxy that would run the sandbox call and fetch the result and then send it into the Automate feature. The Automate feature allows you to send requests in bulk. This feature provides a flexible way to initiate bruteforce/fuzzing to rapidly modify certain parameters of requests using wordlists.\n\nNote from Lupin: In the article it seems like a straightforward path, but actually we took several hours to get to that point. It was 3 am we were hacking with Justin and I was sleeping on my keyboard while Justin was exfiltrating the binary using Caido.\n\nOnce we had all the base64 chunks, we reconstructed the entire file locally and we were ready to see its content.\nHow to read this file ?\nfile command ?\nRunning the file command on the binary revealed its identity as an binary: ELF 64-bit LSB shared object, x86-64, version 1 (SYSV), dynamically linked, interpreter /usr/grte/v5/lib64/ld-linux-x86-64.so.2 This  confirms that the file is a binary. Mmmmmh what can we do with this ?\nstrings command ?\nWhen we executed the strings command, the output was particularly intriguing due to multiple references to google3, Google’s internal repository. This pointed to the presence of internal data paths and code snippets that were never meant for external exposure, clearly indicating that the binary contains traces of Google’s proprietary software. But is there actually any security implication ?\nBinwalk FTW !\nThe real breakthrough came when using Binwalk. This tool managed to extract an entire file structure from within the binary, revealing a comprehensive sandbox layout. The extraction uncovered multiple directories and files, painting a detailed picture of the internal architecture and exposing components where our reaction upon what we found was like ... OMG.\nWait … is that internal Source Code ?\nWhen digging into the extract generated by our binwalk analysis, we unexpectedly found internal source code. The extraction revealed entire directories of proprietary Google source code. But is it sensitive ?\nGoogle3 Directory with Python Code\nIn the binwalk extracted directory we can find a google3 directory with the following files:\ntotal 2160\ndrwxr-xr-x   14 lupin  staff   448B Aug  7 06:17 .\ndrwxr-xr-x  231 lupin  staff   7.2K Aug  7 18:31 ..\n-r-xr-xr-x    1 lupin  staff   1.1M Jan  1  1980 __init__.py\ndrwxr-xr-x    5 lupin  staff   160B Aug  7 06:17 _solib__third_Uparty_Scrosstool_Sv18_Sstable_Ccc-compiler-k8-llvm\ndrwxr-xr-x    4 lupin  staff   128B Aug  7 06:17 assistant\ndrwxr-xr-x    4 lupin  staff   128B Aug  7 06:17 base\ndrwxr-xr-x    5 lupin  staff   160B Aug  7 06:17 devtools\ndrwxr-xr-x    4 lupin  staff   128B Aug  7 06:17 file\ndrwxr-xr-x    4 lupin  staff   128B Aug  7 06:17 google\ndrwxr-xr-x    4 lupin  staff   128B Aug  7 06:17 net\ndrwxr-xr-x    9 lupin  staff   288B Aug  7 06:17 pyglib\ndrwxr-xr-x    4 lupin  staff   128B Aug  7 06:17 testing\ndrwxr-xr-x    9 lupin  staff   288B Aug  7 06:17 third_party\ndrwxr-xr-x    4 lupin  staff   128B Aug  7 06:17 util\n\nIn the assistant directory, internal Gemini code related to RPC calls (used for handling requests via tools like YouTube, Google Flights, Google Maps, etc.) was also discovered. The directory structure is as follows:\n.\n├── __init__.py\n└── boq\n    ├── __init__.py\n    └── lamda\n        ├── __init__.py\n        └── execution_box\n            ├── __init__.py\n            ├── images\n            │   ├── __init__.py\n            │   ├── blaze_compatibility_hack.py\n            │   ├── charts_json_writer.py\n            │   ├── format_exception.py\n            │   ├── library_overrides.py\n            │   ├── matplotlib_post_processor.py\n            │   ├── py_interpreter.py\n            │   ├── py_interpreter_main.py\n            │   └── vegalite_post_processor.py\n            ├── sandbox_interface\n            │   ├── __init__.py\n            │   ├── async_sandbox_rpc.py\n            │   ├── sandbox_rpc.py\n            │   ├── sandbox_rpc_pb2.pyc\n            │   └── tool_use\n            │       ├── __init__.py\n            │       ├── metaprogramming.py\n            │       └── runtime.py\n            └── tool_use\n                ├── __init__.py\n                └── planning_immersive_lib.py\n\n8 directories, 22 files\n\nA Closer Look at the Python Code\nInside the file google3/assistant/boq/lamda/execution_box/images/py_interpreter.py, a snippet of code reveals:\n# String for attempted script dump detection:\n  snippet = (  # pylint: disable=unused-variable\n      \"3AVp#dzcQj$U?uLOj+Gl]GlY<+Z8DnKh\"  # pylint: disable=unused-variable\n  )\n\nThis snippet appears to serve as a safeguard against unauthorized script dumping, underscoring that the code was never intended for public exposure.\n\nAfter a thorough review, the inclusion of what appeared to be internal Google3 code was, in fact, a deliberate choice… Too bad x)\nThe Python code, despite its anti-dumping mechanism that might initially indicate restricted access, had been explicitly approved for public exposure by the Google Security Team well before launch. Although these measures were originally designed to prevent unintended printing, they were retained because … why not ?\nBut we didn’t leave this sandbox alone, we knew we were close to something huge ! ;D\nDigging the main logic of the Sandbox\nWhile digging deeper into the Python code, we noticed that, as expected, this sandbox was communicating with external Google servers to perform activities such as fetch data from Google Flights or other Google services.\nThis was implemented via a python class (google3.assistant.boq.lamda.execution_box.sandbox_interface) which exposed various functions like _set_reader_and_writer  that could be called.\ndef _set_reader_and_writer(\n    reader_handle: io.BufferedReader | None,\n    writer_handle: io.BufferedWriter | None,\n) -> None:\n  \"\"\"Sets the reader and writer handles for rpcs.\n\n  Should be called before running any user code that might\n  import async_sandbox_rpc\n\n  Args:\n    reader_handle: the handle through which to receive incoming RpcResponses. If\n      None will default to legacy behavior (/dev/fd/3)\n    writer_handle: the handle through which to receive incoming RpcRequests. If.\n      None will default to legacy behavior (/dev/fd/4)\n  \"\"\"\n  with _INIT_LOCK:\n    global _READER_HANDLE\n    global _WRITER_HANDLE\n    _READER_HANDLE, _WRITER_HANDLE = reader_handle, writer_handle\n\ndef run_tool(\n    name: str, operation_id: str, parameters: str\n) -> sandbox_rpc_pb2.RunToolResponse:\n  \"\"\"Runs a tool with the given name and id, passing in parameters.\n\n  Args:\n    name: The name of the tool.\n    operation_id: The name of the operation to perform.\n    parameters: The parameters to pass to the tool.\n\n  Returns:\n    A RunToolResponse containing the response from the tool.\n  \"\"\"\n  result = make_rpc(\n      sandbox_rpc_pb2.RpcRequest(\n          run_tool_request=sandbox_rpc_pb2.RunToolRequest(\n              name=name, operation_id=operation_id, parameters=parameters\n          )\n      )\n  )\n\n  if result and result.HasField(\"run_tool_response\"):\n    return result.run_tool_response\n  else:\n    return sandbox_rpc_pb2.RunToolResponse(response=\"\")\n\nWe would provide various pieces of data to these functions, they would serialize the data into the protobuf compatible format, and then call out over RPC by writing to a local file descriptor 5. The response could then be read by reading from local file descriptor 7. By utilizing the protos that were found in the massive binary, we were able to craft messages to and from this RPC server, and call these Google tools directly.\nHowever, we noticed something interesting, not every sandboxes would have the same set of Google services available. It would depend if the sandbox was spawned by the Front End to be able to run Python source code, or by the Google Agent. What do we mean by that ?\nReAct Research paper !\nBefore explaining the next part, we need to explain that Google’s team showed us the following research paper that Gemini is based of:\n\nREACT: SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS\n\nThis paper introduces a novel approach (at the time) where language models alternate between generating reasoning traces and executing specific actions, effectively merging thought and behavior in an interleaved manner. In practice, this means that as the model reasons through a problem, creating a transparent trail of thought that helps it plan, track, and adjust its actions, it simultaneously interacts with external sources to gather additional data when needed. This dynamic interplay not only boosts the model’s performance by mitigating common issues like hallucination and error propagation but also makes its decision-making process more interpretable and controllable for human operators.\nBy integrating both internal reasoning and external knowledge, ReAct offers a flexible and general framework that excels across a variety of tasks, ranging from question answering and fact verification to text-based games and web navigation. In essence, ReAct leverages the combined strengths of reasoning and acting to create more robust, human-aligned, and versatile language models.\n\nOk, but why do we need to understand this ? Well, if we follow the logic of this paper, basically Gemini can prompt itself several times in order to refine the commands and have a proper chain of thoughts.\nIf a user asks “What is the earliest flight between Toronto and New York ?”, Gemini would have roughly a chain of thought like:\n\nThe user is asking me to search the earliest flight between Toronto and New York\n\nCreating a plan like:\n\nI need to run a Python code that will connect to Google Flights tool\nThe code needs to find all flights from today to next week from YYZ airport to JFK airport\nThe code needs to sort by date the flights and take the first item\n\nOnce the plan done, it would generate the code with the available Tools that the Agent has\n\nOnce the code generated, it would spawn a sandbox with the right privileges (eg: having access to the Google Flights tool through the file descriptor)\n\nRun the code, process the output\n\nMakes a response to the user\n\nSo our theory was the following:\n\nWhat if we could do a secondary prompt injection in the planning section of Gemini in order to have access to a more privileged sandbox ?\n\nWith the help of the Google Security Team, we tested this idea and observed that, depending on factors like the generation seed and temperature (all the usual probabilistic LLM nuances), we could occasionally access what appeared to be a more privileged sandbox.\nBy “more privileged sandbox,” we mean one that can access the extensions through two new file descriptors. These file descriptors are always present but aren’t always actively listening, when the agent calls the sandbox, they monitor for any calls to the extensions (Google services) so that we can interact with the API, whereas if accessed through the Python interpreter, those extensions remain inaccessible.\nThis led us to believe that there was a real opportunity for a P0 vulnerability: there was a specific message handler that might allow a file read on Google’s internal infrastructure, and we were hopeful that the sandbox with the tool extension could initiate an RPC call to this specific tool. Given the probabilistic nature of the attack, which made it difficult to reproduce consistently, we have Google Security Team assess this situation. Ultimately, their review revealed that the suspicious message handler was not available via RPC and could only be called externally.\n\nEven though our tests were limited, the core idea still has some real potential if we push it further. Running code in the sandbox context isn’t meant to give extra powers, it's treated as untrusted, with safety checks outside the sandbox and every tool call being filtered. But being able to run code does offer some neat benefits:\n\nReliability: Once you can run code, you can trigger actions more consistently.\n\nChaining/Complexity: Controlling multiple tools or fine-tuning parameters via plain text is tough; code execution could let you build more complex chains, even if safety measures are still in place.\n\nTool Output Poisoning: You might be able to manipulate a tool’s output more effectively.\n\nLeaks: There could be other hidden parts of the environment that, if exposed, might offer extra advantages.\n\nThis shows that our idea still holds promise for further escalation. And that “leaks” potential, we wanted to see if we could at least confirm this one theory …\nWe found our leak ;D\nWhile digging deeper, we uncovered several ways to leak proto files. In case you're not familiar, proto files (short for Protocol Buffer files) are like the blueprints of data, defining how messages are structured and how information is exchanged between different parts of the system. At first glance, they might seem harmless, but leaking these files can give a pretty detailed peek into Google’s internal architecture.\nExposing classification.proto\nIt turns out that by running a command like:\nstrings entry_point > stringsoutput.txt\n\nand then searching for “Dogfood” in the resulting file, we managed to retrieve snippets of the internal protos. Parts of the extracted content included the metadata description of extremely sensitive protos. It didn’t contain user data by itself but those files are internal categories Google uses to classify user data.\nFor legal reasons we can’t show the result of this command x)\n\nWhy search for the string “Dogfood” specifically ? At Google, \"dogfood\" refers to the practice of using pre-release versions of the company's own products and prototypes internally to test and refine them before a public launch. It allows devs to test the deployment and potential issues in these products, before going to production.\nMoreover, there was the following exposed file, privacy/data_governance/attributes/proto/classification.proto, which details how data is classified within Google. Although the file includes references to associated documentation, those documents remain highly confidential and should not be publicly accessible.\n\nNote from Lupin again: This was found the next day of our all-nighter where we exfiltrated the binary file. We were in a suite in an Hotel Room booked by Google, and we were working with the security team to understand what we had found the previous night. This time Justin was the one who slept on the couch hahaha ! This bug was really time consuming but so fun ! 😀\n\nExposing Internal Security Proto Definitions\nThe same output also reveals numerous internal proto files that should have remained hidden. Running:\ncat stringsoutput.txt| grep '\\.proto' | grep 'security'\n\nlists several sensitive files, including:\nsecurity/thinmint/proto/core/thinmint_core.proto\nsecurity/thinmint/proto/thinmint.proto\nsecurity/credentials/proto/authenticator.proto\nsecurity/data_access/proto/standard_dat_scope.proto\nsecurity/loas/l2/proto/credstype.proto\nsecurity/credentials/proto/end_user_credentials.proto\nsecurity/loas/l2/proto/usertype.proto\nsecurity/credentials/proto/iam_request_attributes.proto\nsecurity/util/proto/permission.proto\nsecurity/loas/l2/proto/common.proto\nops/security/sst/signalserver/proto/ss_data.proto\nsecurity/credentials/proto/data_access_token_scope.proto\nsecurity/loas/l2/proto/identity_types.proto\nsecurity/credentials/proto/principal.proto\nsecurity/loas/l2/proto/instance.proto\nsecurity/credentials/proto/justification.proto\n\nWhen looking in the binary strings for security/credentials/proto/authenticator.proto confirms that its data is indeed exposed.\nWhy were those protos there?\nAs we said previously, the Google Security Team thoroughly reviewed everything in the sandbox and gave a green light for public disclosure. However, the build pipeline for compiling the sandbox binary included an automated step that adds security proto files to a binary whenever it detects that the binary might need them to enforce internal rules.\nIn this particular case, that step wasn’t necessary, resulting in the unintended inclusion of highly confidential internal protos in the wild !\nAs bug bounty hunters, it's essential to deeply understand the business rules that govern a company’s operations. We reported these proto leaks because we know that Google treats them as highly confidential information that should never be exposed. The more we understand the inner workings and priorities of our target, the better we are at identifying and flaging those subtle bugs that might otherwise slip under the radar. This deep knowledge not only helps us pinpoint vulnerabilities but also ensures our reports are aligned with the critical security concerns of the organization.\nConclusion\nBefore we wrap things up, it’s worth mentioning how vital it is to test these cutting-edge A.I. systems before they go live. With so many interconnections and cool features, like even a simple sandbox that can access different extensions, there’s always the potential for unexpected surprises. We’ve seen firsthand that when all these parts work together, even a small oversight can open up new avenues for issues. So, thorough testing isn’t just a best practice; it’s the only way to make sure everything stays secure and functions as intended.\nAt the end of the day, what made this whole experience so memorable was the pure fun of the ride. Cracking vulnerabilities, exploring hidden code, and pushing the limits of Gemini's sandbox was as much about the challenge as it was about the excitement of the hunt. The people we’ve met at the bugSWAT event in Las Vegas were all awesome. The shared laughs over unexpected twists, and the thrill of outsmarting complex systems turned this technical journey into an adventure we’ll never forget. It’s moments like these, where serious hacking meets good times, that remind us why we do what we do.\nFinally, a huge shout-out to all the other winners and participants who made bugSWAT 2024 such a blast. We want to congratulate Sreeram & Sivanesh for their killer teamwork, Alessandro for coming so close to that top spot, and En for making it onto the podium. It was an absolute thrill meeting so many amazing hackers and security pros, your energy and passion made this event unforgettable. We can’t wait to see everyone again at the next bugSWAT, and until then, keep hacking and having fun !\nAnd of course, thanks to the Google Security team ! As always you rock ❤️<<Back to Blog",
    "summary": {
      "en": "In March 2025, a group of hackers, including Roni Carta, Justin Gardner, and Joseph Thacker, successfully discovered and leaked parts of Google's A.I. model, Gemini, during a bug-hunting event in Las Vegas. Their journey highlighted the rapid advancements and security challenges in Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs), as tech companies compete to improve their AI offerings.\n\nGoogle has been proactive in addressing security by hosting events like \"LLM bugSWAT,\" where researchers identify vulnerabilities in AI systems. At the 2024 event, Roni and Justin earned the Most Valuable Hacker (MVH) title for uncovering a new vulnerability in Gemini.\n\nThey explored Gemini's secure Python Sandbox, designed to allow safe code execution. However, they discovered ways to access the internal file system and exfiltrate sensitive data by manipulating the sandbox environment. This led to finding internal Google source code and proto files, which define the structure of data exchange within Google's systems.\n\nThe hackers reported their findings to Google, emphasizing the importance of thorough testing in AI systems to prevent potential vulnerabilities. Their experience at bugSWAT was not only about hacking but also about building connections within the cybersecurity community. They look forward to future events and continued collaboration with security professionals.",
      "ko": "2025년 3월, 해커 그룹이 라스베가스에서 열린 버그 헌팅 행사에서 구글의 인공지능 모델인 제미니의 일부를 발견하고 유출하는 데 성공했습니다. 이 그룹에는 로니 카르타, 저스틴 가드너, 조셉 태커가 포함되어 있습니다. 이들의 여정은 생성적 인공지능(GenAI)과 대형 언어 모델(LLM)에서의 빠른 발전과 보안 문제를 강조하며, 기술 기업들이 AI 제품을 개선하기 위해 경쟁하고 있음을 보여줍니다.\n\n구글은 \"LLM bugSWAT\"와 같은 행사를 개최하여 AI 시스템의 취약점을 식별하는 연구자들을 지원하며 보안 문제에 적극적으로 대응하고 있습니다. 2024년 행사에서 로니와 저스틴은 제미니에서 새로운 취약점을 발견하여 가장 가치 있는 해커(MVH)로 선정되었습니다.\n\n그들은 안전한 코드 실행을 위해 설계된 제미니의 보안 파이썬 샌드박스를 탐색했습니다. 그러나 샌드박스 환경을 조작하여 내부 파일 시스템에 접근하고 민감한 데이터를 유출할 수 있는 방법을 발견했습니다. 이 과정에서 구글의 내부 소스 코드와 데이터 교환 구조를 정의하는 프로토 파일을 찾게 되었습니다.\n\n해커들은 구글에 이 findings를 보고하며 AI 시스템에서 잠재적인 취약점을 방지하기 위해 철저한 테스트의 중요성을 강조했습니다. bugSWAT에서의 경험은 단순한 해킹을 넘어 사이버 보안 커뮤니티 내에서의 연결을 구축하는 데에도 큰 의미가 있었습니다. 그들은 앞으로의 행사와 보안 전문가들과의 지속적인 협업을 기대하고 있습니다.",
      "ja": null
    }
  },
  {
    "id": "861c898eec47ed74",
    "title": {
      "en": "There's a psychological burden of digital life even heavier than distraction",
      "ko": "디지털 삶의 심리적 짐",
      "ja": null
    },
    "type": "story",
    "url": "https://www.chrbutler.com/digital-echoes-and-unquiet-minds",
    "score": 34,
    "by": "delaugust",
    "time": 1743193772,
    "content": "Digital Echoes and Unquiet Minds\n\nThere’s a psychological burden of digital life even heavier than distraction.\n\nWhen the iPhone was first introduced in 2007, the notion of an “everything device” was universally celebrated. A single object that could serve as phone, camera, music player, web browser, and so much more promised unprecedented convenience and connectivity. It was, quite literally, the dream of the nineties. But the better part of twenty years later, we’ve gained enough perspective to recognize that this revolutionary vision came with costs we did not anticipate.\n\nDistraction, of course, is the one we can all relate to first. An everything device has the problem of being useful nearly all the time, and when in use, all consuming. When you use it to do one thing, it pushes you toward others. In order to avoid this, you must disable functions. That’s an interesting turn of events, isn’t it? We have made a thing that does more than we need, more often than we desire. Because system-wide, duplicative notifications are enabled by default, the best thing you could say about the device’s design is that it lacks a point of view toward a prioritization of what it does. The worst thing you could say is that it is distracting by design.\n\n(I find it fascinating how many people –myself included — attempt to reduce the features of their smartphone to the point of replicating a “dumbphone” experience in order to save ourselves from distraction, but don’t actually go so far as to use a lesser-featured phone because a few key features are just too good to give up. A dumbphone is less distracting, but a nightmare for text messaging and a lousy camera. It turns out I don’t want a phone at all, but a camera that texts — and ideally one smaller than anything on the market now. I know I’m not alone, and yet this product will not be made. )\n\nThis kind of distraction is direct distraction. It’s the kind we are increasingly aware of, and as its accumulating stress puts pressure on our inner and outer lives, we can combat it with various choices and optimizations. But there is another kind of distraction that is less direct, though just as cumulative and, I believe, just as toxic. I’ve come to think of it as the “digital echo.”\n\nOn a smartphone, every single thing it is used to do generates information that goes elsewhere. The vast majority of this is unseen — though not unfelt — by us. Everyone knows that there is no privacy within a digital device, nor within its “listening” range. We are all aware that as much information as smartphone provides to us, exponentially more is generated for someone else — someone watching, listening, measuring, and monetizing. The “digital echo” is more than just the awareness of this; it is the cognitive burden of knowing that our actions generate data elsewhere. The echo exists whenever we use connected technology, creating a subtle but persistent awareness that what we do isn’t just our own. A device like a smartphone has always generated a “digital echo”, but many others are as well.\n\nComparing two different motor vehicles illustrates this well. In a car like a Tesla, which we might think of as a “smartcar” since it’s a computer you can drive, every function produces a digital signal. Adjusting the air conditioning, making a turn, opening a door — the car knows and records it all, transmitting this information to distant servers. By contrast, my 15-year-old Honda performs all of its functions without creating these digital echoes. The operations remain private, existing only in the moment they occur. In our increasingly digital world, I have begun to feel the SCIF-like isolation of the cabin of my car, and I like it.\n\n(The “smartcar”, of course, won’t remain simply a computer you can drive. The pinnacle “smartcar” drives itself. The self-driving car represents perhaps the most acute expression of how digital culture values attention and convenience above all else, especially control and ownership. As a passenger of a self-driving car, you surrender control over the vehicle’s operation in exchange for the “freedom” to direct your attention elsewhere, most likely to some digital signal either on your own device or on screens within the vehicle. I can see the value in this; driving can be boring and most times I am behind the wheel I’d rather be doing something else. But currently, truly autonomous vehicles are service-enabling products like Waymo, meaning we also relinquish ownership. The benefits of that also seem obvious: no insurance premiums, no maintenance costs. But not every advantage is worth its cost. The economics of self-driving cars are not clear-cut. There’s a real debate to be had about\nattention, convenience, and ownership that I hope will play out before we have no choice but to be a passenger in someone else’s machine.)\n\nWhen I find myself looking for new ways to throttle my smartphone’s functions, or when I sit in the untapped isolation of my car, I often wonder about the costs of the “digital echo.” What is the psychological cost of knowing that your actions aren’t just your own, but create information that can be observed and analyzed by others? As more aspects of our lives generate digital echoes, they force an ambient awareness of being perpetually witnessed rather than simply existing.\n\nThis transforms even solitary activities into implicit social interactions. It forces us to maintain awareness of our “observed self” alongside our “experiencing self,” creating a kind of persistent self-consciousness. We become performers in our own lives rather than merely participants.\n\nI think this growing awareness contributes to a growing interest in returning to single-focus devices and analog technologies. Record players and film cameras aren’t experiencing resurgence merely from nostalgia, but because they offer fundamentally different relationships with media — relationships characterized by intention, presence, and focus.\n\nIn my own life, this recognition has led to deliberate choices about which technologies to embrace and which to avoid. Here are three off the top of my head:\n\nReplacing streaming services with owned media formats (CDs, Blu-rays) that remain accessible on my terms, not subject to platform changes or content disappearance\n\nPreferring printed books while using dedicated e-readers for digital texts — in this case, accepting certain digital echoes when the benefits (in particular, access to otherwise unavailable material) outweigh the costs\n\nRejecting smart home devices entirely, recognizing that their convenience rarely justifies the added complexity and surveillance they introduce\n\nYou’ve probably made similarly-motivated decisions, perhaps in other areas of your life or in relation to other things entirely. What matters, I think, is that these choices aren’t about rejecting technology but about creating spaces for more intentional engagement. They represent a search for balance in a world that increasingly defaults to maximum connectivity.\n\nI had a conversation recently with a friend who mused, “What are these the early days of?” What a wonderful question that is; we are, I hope, always living in the early days of something. Perhaps now, we’re witnessing the beginning of a new phase in our relationship with technology. The initial wave of digital transformation prioritized connecting everything possible; the next wave may be more discriminating about what should be connected and what’s better left direct and immediate. I hope to see operating systems truly designed around focus rather than multitasking, interfaces that respect attention rather than constantly competing for it, and devices that serve discrete purposes exceptionally well instead of performing multiple functions adequately.\n\nThe digital echoes of our actions will likely continue to multiply, but we can choose which echoes we’re willing to generate and which activities deserve to remain ephemeral — to exist only in the moment they occur and then in the memories of those present. What looks like revision or retreat may be the next wave of innovation, borne out of having learned the lessons of the last few decades and desiring better for the next.\n\n        Written by Christopher Butler on March 28, 2025\n\n        Tagged\n        Essays\n\n      © Christopher Butler. All rights reserved.\n      Now\n      About this Website\n      Newsletter\n      RSS",
    "summary": {
      "en": "**Summary of \"Digital Echoes and Unquiet Minds\"**\n\nThe rise of smartphones, starting with the iPhone in 2007, has brought significant convenience but also unexpected psychological burdens. One major issue is distraction; smartphones are designed to keep users engaged, often leading to attempts to limit their features. Many people try to replicate a simpler phone experience without fully giving up the benefits of smartphones.\n\nBeyond distraction, there's a deeper issue called the \"digital echo,\" which refers to the constant awareness that our actions on digital devices generate data that is collected and monitored by others. This knowledge creates a feeling of being perpetually observed, which can make even solitary activities feel social and lead to self-consciousness.\n\nThe author notes a growing interest in simpler, analog technologies, such as record players and film cameras, as they encourage more intentional engagement with media. The article discusses personal choices to limit digital echoes, like preferring physical media over streaming and avoiding smart home devices due to their surveillance aspects.\n\nUltimately, the author hopes for a future where technology prioritizes focus and intentionality rather than constant connectivity, suggesting that the next phase of digital innovation will involve a more careful selection of what should be connected.",
      "ko": "스마트폰의 등장은 2007년 아이폰을 시작으로 큰 편리함을 가져왔지만, 예상치 못한 심리적 부담도 안겼습니다. 가장 큰 문제 중 하나는 주의 산만입니다. 스마트폰은 사용자를 계속 끌어들이도록 설계되어 있어, 많은 사람들이 기능을 제한하려고 시도합니다. 많은 이들이 스마트폰의 장점을 완전히 포기하지 않으면서도 더 간단한 전화 경험을 추구하고 있습니다.\n\n주의 산만을 넘어서, '디지털 에코'라는 더 깊은 문제가 있습니다. 이는 우리가 디지털 기기에서 하는 행동이 데이터로 생성되어 다른 사람에 의해 수집되고 감시된다는 지속적인 인식을 의미합니다. 이러한 인식은 끊임없이 관찰당하는 느낌을 주어, 혼자 하는 활동조차도 사회적인 느낌을 주고 자의식을 유발할 수 있습니다.\n\n저자는 레코드 플레이어나 필름 카메라와 같은 더 간단한 아날로그 기술에 대한 관심이 커지고 있다고 언급합니다. 이러한 기술은 미디어와의 더 의도적인 상호작용을 촉진합니다. 이 글에서는 스트리밍 대신 물리적인 미디어를 선호하거나 감시 측면 때문에 스마트 홈 기기를 피하는 등 디지털 에코를 제한하기 위한 개인적인 선택에 대해 논의합니다.\n\n결국 저자는 기술이 끊임없는 연결보다는 집중과 의도성을 우선시하는 미래를 희망합니다. 다음 디지털 혁신의 단계는 무엇을 연결할지에 대한 더 신중한 선택을 포함할 것이라고 제안합니다.",
      "ja": null
    }
  },
  {
    "id": "f068670bb2ee81fe",
    "title": {
      "en": "Xee: A Modern XPath and XSLT Engine in Rust",
      "ko": "엑스이: 러스트로 만든 현대적 XPath/XSLT 엔진",
      "ja": null
    },
    "type": "story",
    "url": "https://blog.startifact.com/posts/xee/",
    "score": 229,
    "by": "robin_reala",
    "time": 1743144498,
    "content": "Xee: A Modern XPath and XSLT Engine in Rust\n\n        By Martijn Faassen•2025-03-27•Tags:xml,rust,xpath,lxml\n\n            For the last two years I've been working on a programming language\nimplementation in Rust named Xee. Xee stands for \"XML Execution Engine\" and\nit supports modern versions of XPath and XSLT. Those are programming languages,\nand yes, that's XML stuff.\nNow hold on. Your brain might shut down when I talk about XML. I totally get\nthat XML may not be your cup of tea. But I'm also going to be talking about a\nstrange different world of technology where everything is specified, and the\nimplementation of a programming language using Rust, so I hope you still decide\nto read on if those topics could interest you.\nAnd if XML does happen to be your cup of tea, I think you should be excited\nabout Xee, as I think it can help secure a better future for XML technologies.\nHere's the Xee repository.\nThere are two highlights: a command-line tool\nxee\nthat lets you do XPath queries, and a Rust library\nxee-xpath to issue XPath\nqueries from Rust.\n\nGenesis\nIn 2023 I was asked by Paligo, my amazing and generous\nclient, whether I wanted to implement a modern version of XPath and XSLT in\nRust. I felt extremely nervous for a week. Then I told them that this was a big\nproject. I told them that I could do it and I was excited to do it, but it was\ngoing to be a lot of work.\nAnd although I was right to be very intimidated by the scope, I still\nunderestimated the effort at the time.\nBut Xee has come a long way nonetheless! I'm going to take you along on its\njourney if you're willing to follow.\n\nWhat is Xee?\nXee is a programming language implementation. It implements two core XML\nprogramming languages: XPath and,\npartially at the time of writing, XSLT.\nXPath is an XML query language, and XSLT is a language that uses XPath as its\nexpression language which lets you transform XML documents into other\ndocuments. Xee implements modern versions of these specifications, rather\nthan the versions released in 1999.\nXee implements these languages in the Rust programming language. This brings\nmodern XML technology not just to Rust. Rust is a systems programming language\nand is good at integration with other programming languages. So Xee can bring\nits capabilities to other programming languages as well, from PHP to Python.\nI've already experimented with PHP\nbindings.\nSince Xee is written in Rust, it should also be possible to compile the Xee\ninterpreter to WASM and run this stuff in the browser.\nI'll continue to talk about how Xee is implemented later, but first we'll take\na break and share some XML history.\n\nXML history\nLet's talk a bit about XML. XML emerged in the late 90s, and though it may be\ndifficult to believe now, for a while in the early part of the 2000s, XML was a\ncool technology everyone wanted to use. There was much excitement in the form\nof industry activity and many computer science papers were also published.\nTo illustrate how big this was, last year I was at the RustNL conference and I\nspoke to two separate speakers who mentioned they had worked on an XSLT\nengine1 in the past. One of them was Niko\nMatsakis, Rust core developer.\nSo me being a young and hip developer back then 2, I was doing cool XML\nstuff too. My biggest accomplishment in the XML space was the creation of\nlxml, the XML library for Python. I started that project in\nlate 2004. Early on Stefan Behnel\njoined the project and he has competently maintained it ever since - it would\nnot have been as successful without him.\nWhile XML technology isn't cool anymore today, it's still everywhere. The core\nlanguage web browsers use is not XML but its close cousin HTML. Embedded in\nHTML are true XML-based languages, such as SVG and MathML. Even though JSON and\nother languages took a large chunk out of it, XML is still used to store and\ntransmit a lot of data, and it's extensively used for documents as well, in\nformats such as docbook and JATS. XML is now niche technology, but it's a\nbigger niche than you might think, and it's not going to go away any time soon.\nIn my own career, I became less and less involved with XML over time, though\nI'd still run into it on a regular basis. It's both amusing and useful that\nwhenever I talk to a potential client that uses Python, they're already using\nlxml somewhere.\nA few years ago I entered back into the XML world. And here I am, that\nrelatively rare bird who knows a fancy modern programming language like Rust,\nand is at the same time very familiar with XML.\n\nXPath and XSLT are programming languages\nSo XPath and XSLT are both programming languages.\nXPath is a query language for XML. Given an XML document, let's say something\nlike HTML, you can query it with expressions like: /html/body//p to get all\np elements inside the body element of the outer html element. XPath in\nits modern incarnation is a functional programming language with a type system,\nvariables, function definition, conditionals, loops and so on.\nXSLT is a transformation language for XML. It describes, using templates and a\nfunctional approach, how to transform an XML document of one type into another.\nYou can for instance use it to transform docbook XML, which describes\ndocuments, into HTML. It builds on XPath - XPath expressions are the expression\nlanguage of XSLT. XSLT itself also supports programming constructs like\nvariables, loops, conditionals, functions and the like, in a partial\nduplication of XPath.\n\nState of the XML open source stack\nSo if you want to use these programming languages and you use an open source\nstack, where do you go?\nThe Java world has good modern XPath and XSLT support. XPath and XSLT are\nimplemented by Saxon, which has been around for a long time. Saxon is available\non .NET as well. There are also PHP and Python bindings via a rather complex C\nto Java bridge, and Saxon offers a JavaScript reimplemention of its runtime as\nwell. Besides its open source offerings, Saxon also has closed-source\nprofessional/enterprise editions which provide more features. Besides Saxon,\nthere are also open source XQuery3 implementations in Java.\nBut if you step out of the Java world and its periphery, and if you look in\nyour average open source stack or Linux distribution for an XPath or XSLT\nimplementation you don't find Saxon or these XQuery databases; you find\nlibxml2 and libxslt.\nlibxml2 and libxslt are C libraries for handling XML. This amalgam of\nlibraries supports parsing XML, querying it using XPath, transforming it using\nXSLT and more. libxml2 is everywhere - in your Linux distribution and in\nMacOS. People don't just use it from C code - for Python for instance I built\nlxml on top.\nThese libraries were originally created by Daniel\nVeillard. I remember speaking to him once, many years\nago. We came from different worlds - he was thinking about writing fast\nprocessor-cache friendly code in C, whereas I was interested in an easy to use\nAPI in Python. I was impressed he had implemented all these specifications -\nlxml was merely piggybacking on that hard work.\nBut libxml2 is stuck in the past - it implements XPath, but only XPath 1.0,\nand similarly libxslt implements XSLT 1.0 only. These are specifications from\n1999. The XPath 2 specification was released in 2007, and we're currently\nactually at XPath 3.1, released in 2017. Similarly XSLT 2.0 was released in\n2007 and XSLT 3.0, the current version, in 2017.\nMy hope is that Xee can be a more modern alternative to libxml2 and libxslt\nthat finds its home in the open source world. For XPath and XSLT to be thriving\nstandards they need multiple implementations, in multiple programming\nlanguages, by multiple parties.\nAnd personally I feel like I have come full circle - finally, in these latter\ndays of XML, I am where Daniel Veillard had gone ages before with libxml2. I\nfind myself implementing the same stuff, not in C, but still in a systems\nprogramming language, Rust.\n\nSpecification culture\nI was at XML Prague, an XML conference, last year,\nand I noticed something interesting about XML culture. It is still very\nstandards focused. This was a very prevalent attitude in the web development\nworld in the early 2000s, but I think that although standards are still\nconsidered important today, they're less culturally prominent.\nThe XML culture is different: stuff needs to be specified. If it's not in a\nspecification it's not fully real. This makes the XML community move more\nslowly than the rest of the software community. I was somewhat bemused to hear\ntalk in 2024 about updating the RESTXQ spec, an XQuery based web framework\nstandard, first discussed in 2012, to make use of language features like\nhashmaps and arrays, now that they had been finally added to XPath/XQuery in\n2017.\nThese XML specifications go deep, they build on each other, they are solid. If\nyou value solid foundations that will stand the test of time, the XML world has\ngot your back.\n\nImplementing a programming language\nYou might be bored with XML by now so before I return to the discussion of\nspecifications, I will talk a bit about the architecture of Xee.\nXee follows various familiar patterns in the implementation of programming\nlanguages. I based part of its architecture on the excellent book Crafting\nInterpreters.\nIn Xee, XPath gets lexed into tokens, then parsed into an abstract syntax tree\n(AST). The AST is then transformed into an intermediate representation (IR)\nthat represents the expression in a more compact way. This IR is then compiled\ninto bytecode - a simple assembly-language like stack machine, similar to the\none that underlies many programming languages such as Python and Java. The Xee\ninterpreter can then execute the bytecode.\nThis translation at present is straightforward; while I've prepared the IR to\nsupport optimization passes such as constant folding and the like, this doesn't\nhappen yet.4\nXSLT, though unfinished, is built on the same architecture as the XPath engine.\nThere's a frontend that transforms XSLT XML into an XSLT AST, and then this is\ntransformed into the same IR as the one used for XPath. It uses the same\nbytecode intepreter. So, only the XSLT frontend is different, everything else\nis the same. This made it easy to implement a whole bunch of XSLT features as I\nhad already implemented them for XPath.\nImplementing programming languages is fun!\n\nSpecifications, again\nXPath and XSLT are programming languages that are fully specified. You can\nreally implement them from the specification. On the one hand this makes life a\nlot easier - the goals are clear as it's clearly specified how things are\nsupposed to work. There's a vast conformance test suite available as well. On\nthe other hand this means an endless treadmill; I can't just stop when I think\nit looks good enough when there's more specification left to implement.\nXPath 3.1 has grown a lot bigger than XPath 1.0; it became a full-fledged\nprogramming language, with a much larger standard library. XSLT 3.0 has also\nevolved a lot since XSLT 1.0. Specifications keep building on each other, and\nadd more features in new updates, until implementing them becomes a daunting\ntask. I sometimes I wish I was implementing XPath 1.0 and XSLT 1.0, like Daniel\nVeillard back in the day.\nLet me give you a quick tour of various specifications so you can understand\nsomething about the magnitude of the task of implementing them.\nThe grammar and behavior of the XPath language is laid out in the W3C\nspecification XML Xpath Language (XPath)\n3.1. This refers to another specification,\nXQuery and XPath Data Model 3.1\nwhich describes how XPath views XML data - what properties of XML data exist.\nIt also builds on another specification XPath and XQuery Functions and\nOperators 3.1, which not only\ndescribes the behavior of XPath operators such as +, - and *, but also\ndefines its standard library of functions.\nXPath has a type system, and its types are described by W3C XML Schema\nDefinition Language (XSD) 1.1: Part 1:\nStructures and W3C XML Schema\nDefinition Language (XSD) 1.1 Part 2:\nDatatypes. This defines atomic types\n(which Xee implements) but also lets you define new types and use types from an\nXML schema, which Xee doesn't implement at present. These specifications also\ndescribe how XPath is to parse and format strings of atomic types, such as the\nformat of decimals and dates.\nOh, and that XPath functions and operators specification? Some of the functions\nuse regular expressions. The specification defines XPath regular expressions as\nan extension of the regular expressions system defined in the XML schema\nspecification. And all of that builds on the unicode specification but that's\nanother country. So I ended up implementing a regex\nengine too.\nOver to XSLT. There's XSL Transformations (XSLT) Version\n3.0 which defines the XSLT programming\nlanguage. It builds on all the specifications that went before, and also builds\non XSLT and XQuery Serialization\n3.0, which describes\noptions for how to serialize XML and various other things.\nOf course all of this builds on the XML specification itself, Extensive Markup\nLanguage (XML) 1.0 (Fifth Edition), extended with\nnamespaces, in Namespaces in XML 1.0.\nThen there are a few stray specifications that are also relevant like XML\nBase and\nxml:id. But those are small ones.\nOnce I counted up the page count5 of just the XPath and XSLT\nspecifications along with the most relevant XML Schema spec (part 2), and that\nsubset is over 1800 pages.\nI probably forgot a few specifications, because after a while they start coming\nout of my ears, but this should give you an impression.\n\nXee status\nWhat I'm most proud of is the XPath 3.1 implementation in Xee. The XPath core\nlanguage and most of its standard library have been implemented. There are gaps\nin the standard library implementation still - some formatting functions are\nparticularly huge, for instance, but overall it's pretty complete.\nThere's an XPath 3.1 conformance test suite, and of the 21859 tests, 20130\ntests are passing at the time of writing. Most of the failing tests have to do\nwith the implementation of missing standard library functionality.\nIncidentally, this test suite runs those 20130 tests in 13 seconds on my\nmachine. Computers are fast.\nMeanwhile Xee also provides a solid basis for XSLT, reusing a lot of the XPath\ninfrastructure. While a lot of XSLT works, much remains to be done and I'm\nhoping to find people who want to help contribute!\n\nA call for contributors\nSo now I will call for this rare bird: someone who read all this, saw all those\nXML specifications, knows a bit of Rust, likes implementing programming\nlanguages and thought: cool! I want to help!\n\nDo you like the challenge of implementing some functionality, small or large,\naccording to spec? Xee has plenty of tasks for you.\n\nAre you interested in programming language implementation? Perhaps do cool\nprogramming language optimization work? For a programming language that has\nan existing user base already? Xee has the foundations.\n\nDo you like to think about query optimization problems? Care about using\nsuccinct data structures? (not\nintegrated into Xee proper yet). We have plenty of what should interest you.\n\nDo you care about the future of XML and want to ensure a modern open source\nimplementation is available outside of the Java world?\n\nThe Xee project could use your help and is ready for it. Small and large\ncontributions are possible and welcome!\n2\nI'm still hip. I say so. Even though I do XML stuff.\n\n1\nNot the same XSLT engine. Different ones!\n\n3\nXQuery is a superset of XPath.\n\n4\nSo you're interested in working on programming language\noptimization you've come to the right place!\n\n5\nI printed each specification HTML page to PDF to see how many pages\nthey were.\n\n                ←Prev\n                Looking for new challenges!\n\n  Comments\n  You can use your Mastodon account to reply to this post. Learn how\n  this is implemented here.\n\n  Reply\n  Load comments\n\n    Reply to this post\n\n      With an account on the Fediverse or Mastodon, you can respond to this\n      post. Since Mastodon is decentralized, you can use your existing account\n      hosted by another Mastodon server or compatible platform if you don't\n      have an account on this one.\n\n    Copy and paste this URL into the search field of your favorite Fediverse app or the web interface of your Mastodon server.\n\n      Copy\n      Close\n\n  You need JavaScript to view the comments.\n\n    const dialog = document.querySelector('dialog');\n\n    document.getElementById('replyButton').addEventListener('click', () => {\n       dialog.showModal();\n      });\n\n    document.getElementById('copyButton').addEventListener('click', () => {\n      navigator.clipboard.writeText(\"https://fosstodon.org/@faassen/114235009789423262\");\n    });\n\n    document.getElementById('cancelButton').addEventListener('click', () => {\n      dialog.close();\n    });\n\n    dialog.addEventListener('keydown', e => {\n      if (e.key === 'Escape') dialog.close();\n    });\n\n    function escapeHtml(unsafe) {\n      return unsafe\n            .replace(/&/g, \"&amp;\")\n            .replace(/</g, \"&lt;\")\n            .replace(/>/g, \"&gt;\")\n            .replace(/\"/g, \"&quot;\")\n            .replace(/'/g, \"&#039;\");\n    }\n\n    // render date as YYYY-MM-DD HH:MM, not using the browser's locale\n    function renderDate(date) {\n        return date.getFullYear() + \"-\" + (date.getMonth() + 1).toString().padStart(2, '0') + \"-\" + date.getDate().toString().padStart(2, '0') + \" \" + date.getHours().toString().padStart(2, '0') + \":\" + date.getMinutes().toString().padStart(2, '0');\n    }\n\n    document.getElementById(\"load-comment\").addEventListener(\"click\", function() {\n        document.getElementById(\"load-comment\").innerHTML = \"Loading\";\n        fetch('https://fosstodon.org/api/v1/statuses/114235009789423262/context')\n          .then(function(response) {\n            return response.json();\n          })\n          .then(function(data) {\n            if(data['descendants'] &&\n               Array.isArray(data['descendants']) &&\n              data['descendants'].length > 0) {\n                document.getElementById('mastodon-comments-list').innerHTML = \"\";\n                data['descendants'].forEach(function(reply) {\n                  reply.account.display_name = escapeHtml(reply.account.display_name);\n                  reply.account.reply_class = reply.in_reply_to_id == \"114235009789423262\" ? \"reply-original\" : \"reply-child\";\n                  reply.created_date = new Date(reply.created_at);\n                  reply.account.emojis.forEach(emoji => {\n                    reply.account.display_name = reply.account.display_name.replace(`:${emoji.shortcode}:`,\n                      `<img src=\"${escapeHtml(emoji.static_url)}\" alt=\"Emoji ${emoji.shortcode}\" height=\"20\" width=\"20\" />`);\n                  });\n                  mastodonComment =\n                    `\n<div class=\"mastodon-wrapper\">\n  <div class=\"comment-level ${reply.account.reply_class}\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 512 512\">\n    <path fill=\"currentColor\" stroke=\"currentColor\" d=\"m 307,477.17986 c -11.5,-5.1 -19,-16.6 -19,-29.2 v -64 H 176 C 78.8,383.97986 -4.6936293e-8,305.17986 -4.6936293e-8,207.97986 -4.6936293e-8,94.679854 81.5,44.079854 100.2,33.879854 c 2.5,-1.4 5.3,-1.9 8.1,-1.9 10.9,0 19.7,8.9 19.7,19.7 0,7.5 -4.3,14.4 -9.8,19.5 -9.4,8.8 -22.2,26.4 -22.2,56.700006 0,53 43,96 96,96 h 96 v -64 c 0,-12.6 7.4,-24.1 19,-29.2 11.6,-5.1 25,-3 34.4,5.4 l 160,144 c 6.7,6.2 10.6,14.8 10.6,23.9 0,9.1 -3.9,17.7 -10.6,23.8 l -160,144 c -9.4,8.5 -22.9,10.6 -34.4,5.4 z\" />\n  </svg></div>\n  <div class=\"mastodon-comment\">\n    <div class=\"comment\">\n      <div class=\"comment-avatar\"><img src=\"${escapeHtml(reply.account.avatar_static)}\" alt=\"\"></div>\n      <div class=\"comment-author\">\n        <div class=\"comment-author-name\"><a href=\"${reply.account.url}\" rel=\"nofollow\">${reply.account.display_name}</a></div>\n        <div class=\"comment-author-reply\"><a href=\"${reply.account.url}\" rel=\"nofollow\">${escapeHtml(reply.account.acct)}</a></div>\n      </div>\n      <div class=\"meta\">${renderDate(reply.created_date)}</div>\n    </div>\n    <div class=\"comment-content\">${reply.content}</div>\n  </div>\n</div>\n`;\n                  document.getElementById('mastodon-comments-list').appendChild(DOMPurify.sanitize(mastodonComment, {'RETURN_DOM_FRAGMENT': true}));\n                });\n            } else {\n              document.getElementById('mastodon-comments-list').innerHTML = \"<p>No comments found</p>\";\n            }\n          });\n        });",
    "summary": {
      "en": "### Summary of \"Xee: A Modern XPath and XSLT Engine in Rust\"\n\nXee is a programming language implementation created by Martijn Faassen, designed to support modern versions of XPath and XSLT using Rust. XPath is a query language for XML, while XSLT is used for transforming XML documents. Xee aims to modernize these technologies and provide better integration with various programming languages.\n\n**Key Points:**\n\n1. **Project Background**: Faassen was approached by Paligo to create Xee, which he found daunting but exciting. His goal is to provide a modern implementation of XML technologies.\n\n2. **Implementation and Features**: Xee consists of a command-line tool for XPath queries and a Rust library for XPath integration. It can potentially be compiled to run in web browsers using WebAssembly (WASM).\n\n3. **XML's Evolution**: Although XML was once very popular in the early 2000s, its usage has declined, but it remains important for many applications and data formats.\n\n4. **XPath and XSLT**: Both are considered programming languages, with XPath allowing queries on XML data and XSLT enabling document transformations. Xee implements modern versions of these languages.\n\n5. **Challenges in Implementation**: The existing libraries for XML, like libxml2 and libxslt, are outdated, only supporting older versions of XPath and XSLT. Xee aims to fill this gap with a modern alternative.\n\n6. **Standards and Specifications**: The XML community is heavily focused on standards, which can slow down development. Xee is designed according to these specifications, making it a robust solution.\n\n7. **Development Status**: Xee has a functioning implementation of XPath 3.1, with many features already complete. The project invites contributions from others interested in programming languages and XML technologies.\n\n8. **Call for Contributors**: Faassen encourages those knowledgeable in Rust and interested in XML to join the project and help enhance its functionality.\n\nOverall, Xee represents an effort to revitalize XML technologies by providing a modern, open-source implementation in Rust.",
      "ko": "Xee는 Martijn Faassen이 만든 프로그래밍 언어 구현체로, Rust를 사용하여 현대적인 XPath와 XSLT를 지원하도록 설계되었습니다. XPath는 XML을 위한 쿼리 언어이며, XSLT는 XML 문서를 변환하는 데 사용됩니다. Xee는 이러한 기술을 현대화하고 다양한 프로그래밍 언어와의 통합을 개선하는 것을 목표로 하고 있습니다.\n\nFaassen은 Paligo의 요청으로 Xee를 만들게 되었고, 이 작업이 어렵지만 흥미롭다고 느꼈습니다. 그의 목표는 XML 기술의 현대적인 구현을 제공하는 것입니다. Xee는 XPath 쿼리를 위한 명령줄 도구와 XPath 통합을 위한 Rust 라이브러리로 구성되어 있습니다. 이 도구는 WebAssembly(WASM)를 사용하여 웹 브라우저에서 실행될 수 있도록 컴파일될 가능성도 있습니다.\n\nXML은 2000년대 초반에 매우 인기가 있었지만, 현재 사용량은 줄어들었습니다. 그럼에도 불구하고 많은 애플리케이션과 데이터 형식에서 여전히 중요한 역할을 하고 있습니다. XPath와 XSLT는 모두 프로그래밍 언어로 간주되며, XPath는 XML 데이터에 대한 쿼리를 가능하게 하고, XSLT는 문서 변환을 지원합니다. Xee는 이러한 언어의 현대적인 버전을 구현하고 있습니다.\n\n기존의 XML 라이브러리인 libxml2와 libxslt는 구식으로, 오래된 버전의 XPath와 XSLT만 지원합니다. Xee는 이러한 공백을 메우기 위해 현대적인 대안을 제공하고자 합니다. XML 커뮤니티는 표준에 많은 중점을 두고 있어 개발 속도를 늦출 수 있습니다. Xee는 이러한 사양에 따라 설계되어 강력한 솔루션이 됩니다.\n\n현재 Xee는 XPath 3.1의 기능적인 구현을 가지고 있으며, 많은 기능이 이미 완료되었습니다. 이 프로젝트는 프로그래밍 언어와 XML 기술에 관심이 있는 사람들의 기여를 환영합니다. Faassen은 Rust에 대한 지식이 있고 XML에 관심이 있는 사람들이 프로젝트에 참여하여 기능을 향상시키는 데 도움을 주기를 권장합니다.\n\nXee는 Rust에서 현대적이고 오픈 소스 구현을 제공함으로써 XML 기술을 재활성화하려는 노력을 나타냅니다.",
      "ja": null
    }
  },
  {
    "id": "551e20eaab8852f1",
    "title": {
      "en": "How Kerala got rich",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://aeon.co/essays/how-did-kerala-go-from-poor-to-prosperous-among-indias-states",
    "score": 256,
    "by": "lordleft",
    "time": 1743179264,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "f9d177a202e349e0",
    "title": {
      "en": "Show HN: Hexi – Modern header-only network binary serialisation for C++",
      "ko": "헥시: C++를 위한 현대적 네트워크 직렬화",
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/EmberEmu/Hexi",
    "score": 59,
    "by": "Chaosvex",
    "time": 1743183462,
    "content": "Hexi is a lightweight, header-only C++23 library for safely handling binary data from arbitrary sources (but primarily network data). It sits somewhere between manually memcpying bytes from network buffers and full-blown serialisation libraries.\nThe design goals are ease of use, safety when dealing with untrusted data, a reasonable level of flexibility, and keeping overhead to a minimum.\nWhat Hexi doesn't offer: versioning, conversion between different formats, handling of text-based formats, unloading the dishwasher.\n\nIncorporating Hexi into your project is simple! The easiest way is to simply copy hexi.h from single_include into your own project. If you'd rather only include what you use, you can add include to your include paths or incorporate it into your own CMake project with target_link_library. To build the unit tests, run CMake with ENABLE_TESTING.\nHere's what some libraries might call a very simple motivating example:\n#include <hexi.h>\n#include <array>\n#include <vector>\n#include <cstddef>\n\nstruct UserPacket {\n    uint64_t user_id;\n    uint64_t timestamp;\n    std::array<uint8_t, 16> ipv6;\n};\n\nauto deserialise(std::span<const char> network_buffer) {\n    hexi::buffer_adaptor adaptor(network_buffer); // wrap the buffer\n    hexi::binary_stream stream(adaptor);          // create a binary stream\n\n    // deserialise!\n    UserPacket packet;\n    stream >> packet;\n    return packet;\n}\n\nauto serialise(const UserPacket& packet) {\n    std::vector<uint8_t> buffer;\n    hexi::buffer_adaptor adaptor(buffer); // wrap the buffer\n    hexi::binary_stream stream(adaptor);  // create a binary stream\n\n    // serialise!\n    stream << packet;\n    return buffer;\n}\n\nBy default, Hexi will try to serialise basic structures such as our UserPacket if they meet requirements for being safe to directly copy the bytes. Now, for reasons of portability, it's not recommended that you do things this way unless you're positive that the data layout is identical on the system that wrote the data. Not to worry, this is easily solved. Plus, we didn't do any error handling. All in good time.\n\nThe two classes you'll primarily deal with are buffer_adaptor and binary_stream.\nbinary_stream takes a container as its argument and is used to do the reading and writing. It doesn't know much about the details of the underlying container.\nTo support containers that weren't written to be used with Hexi, buffer_adaptor is used as a wrapper that binary_stream can interface with. As with binary_stream, it also provides read and write operations but at a lower level.\nbuffer_adaptor can wrap any contiguous container or view that provides data and size member functions and optionally resize() for write support. From the standard library, that means the following can be used out of the box:\n\n std::array\n std::span\n std::string_view\n std::string\n std::vector\n\nPlenty of non-standard library containers will work out of the box, too, as long as they provide a vaguely similar API.\nThe container's value type must be a byte type (e.g. char, std::byte, uint8_t). std::as_bytes can be used as a workaround if this poses a problem.\n\nHexi supports custom containers, including non-contiguous containers. In fact, there's a non-contiguous container included in the library. You simply need to provide a few functions such as read and size to allow the binary_stream class to be able to use it.\nstatic_buffer.h provides a simple example of a custom container that can be used directly with binary_stream.\n\nAs mentioned, Hexi is intended to be safe to use even when dealing with untrusted data. An example might be network messages that have been manipulated to try to trick your code into reading out of bounds.\nbinary_stream performs bounds checking to ensure that it will never read more data than the buffer has available and optionally allows you to specify an upper bound on the amount of data to read. This can be useful when you have multiple messages in a buffer and want to limit the deserialisation from potentially eating into the next.\nbuffer_t buffer;\n// ... read data\nhexi::binary_stream stream(buffer, 32); // will never read more than 32 bytes\n\nThe default error handling mechanism is exceptions. Upon encountering a problem with reading data, an exception derived from hexi::exception will be thrown. These are:\n\nhexi::buffer_underrun - attempt to read out of bounds\nhexi::stream_read_limit - attempt to read more than the imposed limit\n\nExceptions from binary_stream can be disabled by specifying no_throw as a template argument, as shown:\nhexi::binary_stream<buf_type, hexi::no_throw> stream(...);\n\nWhile this prevents binary_stream itself from throwing, it does not prevent propagation of exceptions from lower levels. For example, a wrapped std::vector could still throw std::bad_alloc if allocation fails when writing to it.\nRegardless of the error handling mechanism you use, the state of a binary_stream can be checked as follows:\nhexi::binary_stream<buf_type, hexi::no_throw> stream(...);\n// ... assume an error happens\n\n// simplest way to check whether any errors have occurred\nif (!stream) {\n    // handle error\n}\n\n// or we can get the state\nif (auto state = stream.state(); state != hexi::stream_state::ok) {\n    // handle error\n}\n\nIn the first example, reading our UserPacket would only work as expected if the program that wrote the data laid everything out in the same way as our own program.\nThis might not be the case for reasons of architecture differences, compiler flags, etc.\nHere's the same example but doing it portably.\n#include <hexi.h>\n#include <span>\n#include <string>\n#include <vector>\n#include <cstddef>\n#include <cstdint>\n\nstruct UserPacket {\n    uint64_t user_id;\n    std::string username;\n    uint64_t timestamp;\n    uint8_t has_optional_field;\n    uint32_t optional_field;  // pretend this is big endian in the protocol\n\n    // deserialise\n    auto& operator>>(auto& stream) {\n        stream >> user_id >> username >> timestamp >> has_optional_field;\n\n        if (has_optional_field) {\n            stream >> optional_field;\n            hexi::endian::big_to_native_inplace(optional_field);\n        }\n\n        // we can manually trigger an error if something went wrong\n        // stream.set_error_state();\n        return stream;\n    }\n\n    // serialise\n    auto& operator<<(auto& stream) const {\n        stream << user_id << username << timestamp << has_optional_field;\n\n        if (has_optional_field) {\n            stream << hexi::endian::native_to_big(optional_field);\n        }\n\n        return stream;\n    }\n};\n\n// pretend we're reading network data\nvoid read() {\n    std::vector<char> buffer;\n    const auto bytes_read = socket.read(buffer);\n\n    // ... logic for determing packet type, etc\n\n    bool result {};\n\n    switch (packet_type) {\n        case packet_type::user_packet:\n            result = handle_user_packet(buffer);\n            break;\n    }\n\n    // ... handle result\n}\n\nauto handle_user_packet(std::span<const char> buffer) {\n    hexi::buffer_adaptor adaptor(buffer);\n    hexi::binary_stream stream(adaptor);\n\n    UserPacket packet;\n    stream >> packet;\n\n    if (stream) {\n        // ... do something with the packet\n        return true;\n    } else {\n        return false;\n    }\n}\n\nBecause binary_stream is a template, it's easiest to allow the compiler to perform type deduction magic.\nIf you want the function bodies to be in a source file, it's recommended that you provide your own using alias for your binary_stream type.\nThe alternative is to use the polymorphic equivalents, pmc::buffer_adaptor and pmc::binary_stream, which allow you to change the underlying buffer type at runtime but at the cost of virtual call overhead and lacking some functionality that doesn't mesh well with polymorphism.\nHow you structure your code is up to you, this is just one way of doing it.\n\nWhen using binary_stream, strings are always treated as null-terminated. Writing a char*, std::string_view or std::string will always write a terminating byte to the stream. If you require otherwise, use one of the put functions.\nLikewise, reading to std::string assumes the buffer contains a null-terminator. If it does not, an empty string will be returned. If you know the length of the string or need to support a custom terminating/sentinel value, use get() and find_first_of().\n\nHere's a very quick rundown on some of the included extras.\n\nhexi::file_buffer\n\nFor dealing with binary files. Simples.\n\nhexi::static_buffer\n\nFixed-size networking buffer for when you know the upper bound on the amount of data you'll need to send or receive in one go. Essentially a wrapper around std::array but with added state tracking. Handy if you need to deserialise in multiple steps (read packet header, dispatch, read packet body).\n\nhexi::dynamic_buffer\n\nResizeable buffer for when you want to deal with occasional large read/writes without having to allocate the space up front. Internally, it adds additional allocations to accomodate extra data rather than requesting a larger allocation and copying data as std::vector would. It reuses allocated blocks where possible and has support for Asio (Boost or standalone). Effectively, it's a linked list buffer.\n\nhexi::tls_block_allocator\n\nAllows many instances of dynamic_buffer to share a larger pool of pre-allocated memory, with each thread having its own pool. This is useful when you have many network sockets to handle and want to avoid the general purpose allocator. The caveat is that a deallocation must be made by the same thread that made the allocation, thus limiting access to the buffer to a single thread (with some exceptions).\n\nhexi::endian\n\nProvides functionality for handling endianness of integral types.\n\nWe're at the end of the overview, but there's more to discover if you decide to give Hexi a shot. Here's a selection of tasty morsels:\n\nbinary_stream allows you to perform write seeking within the stream, when the underlying buffer supports it. This is nice if, for example, you need to update a message header with information that you might not know until the rest of the message has been written; checksums, sizes, etc.\nbinary_stream provides overloaded put and get member functions, which allow for fine-grained control, such as reading/writing a specific number of bytes.\nbinary_stream allows for writing to std::string_view and std::span with view() and span() as long as the underlying container is contiguous. This allows you to create views into the buffer's data, providing a fast, zero-copy way to read strings and arrays from the stream. If you do this, you should avoid writing to the same buffer while holding views to the data.\nbuffer_adaptor provides a template option, space_optimise. This is enabled by default and allows it to avoid resizing containers in cases where all data has been read by the stream. Disabling it allows for preserving data even after having been read. This option is only relevant in scenarios where a single buffer is being both written to and read from.\nbuffer_adaptor provides find_first_of, making it easy to find a specific sentinel value within your buffer.\n\nTo learn more, check out the examples in docs/examples!",
    "summary": {
      "en": "Hexi is a lightweight C++23 library designed for safely managing binary data, especially from network sources. It aims to be simple to use, secure against untrusted data, flexible, and efficient. However, it does not include features like versioning, format conversion, or text format handling.\n\nTo use Hexi, you can simply copy the header file into your project or include it in your build system. The main components are `buffer_adaptor` and `binary_stream`, which facilitate reading and writing binary data. Hexi can work with standard containers like `std::array`, `std::vector`, and `std::string`, as well as custom containers as long as they follow a similar API.\n\nHexi ensures safety by performing bounds checks to prevent reading beyond the available data. It uses exceptions for error handling, which can be customized or disabled if needed. The library also includes features for handling endianness and offers various buffer types like fixed-size, dynamic, and file buffers.\n\nKey features include:\n- Easy serialization and deserialization of structures.\n- Bounds checking to prevent data overflows.\n- Support for both standard and custom containers.\n- Handling of endianness for different data formats.\n\nFor a complete understanding and examples, users are encouraged to explore the documentation and examples provided by Hexi.",
      "ko": "Hexi는 이진 데이터를 안전하게 관리하기 위해 설계된 경량 C++23 라이브러리입니다. 주로 네트워크 소스에서 오는 데이터를 다루는 데 초점을 맞추고 있으며, 사용이 간편하고 신뢰할 수 없는 데이터에 대해 안전하며 유연하고 효율적입니다. 그러나 버전 관리, 형식 변환, 텍스트 형식 처리와 같은 기능은 포함되어 있지 않습니다.\n\nHexi를 사용하려면 헤더 파일을 프로젝트에 복사하거나 빌드 시스템에 포함시키면 됩니다. 주요 구성 요소는 `buffer_adaptor`와 `binary_stream`으로, 이들은 이진 데이터를 읽고 쓰는 데 도움을 줍니다. Hexi는 `std::array`, `std::vector`, `std::string`과 같은 표준 컨테이너는 물론, 유사한 API를 따르는 사용자 정의 컨테이너와도 함께 사용할 수 있습니다.\n\nHexi는 데이터의 경계를 체크하여 사용 가능한 데이터를 넘어 읽는 것을 방지함으로써 안전성을 보장합니다. 오류 처리를 위해 예외를 사용하며, 필요에 따라 이를 사용자 정의하거나 비활성화할 수 있습니다. 또한, 엔디안 처리 기능을 포함하고 있으며, 고정 크기, 동적, 파일 버퍼와 같은 다양한 버퍼 유형을 제공합니다.\n\n주요 기능으로는 구조체의 직렬화 및 역직렬화가 용이하고, 데이터 오버플로우를 방지하기 위한 경계 체크, 표준 및 사용자 정의 컨테이너 지원, 다양한 데이터 형식에 대한 엔디안 처리 등이 있습니다.\n\n완전한 이해와 예제를 위해 사용자는 Hexi에서 제공하는 문서와 예제를 살펴보는 것이 좋습니다.",
      "ja": null
    }
  },
  {
    "id": "74fec543d154070c",
    "title": {
      "en": "Decomposing a Factorial into Large Factors",
      "ko": "팩토리얼의 대수 분해",
      "ja": null
    },
    "type": "story",
    "url": "https://terrytao.wordpress.com/2025/03/26/decomposing-a-factorial-into-large-factors/",
    "score": 93,
    "by": "surprisetalk",
    "time": 1743173754,
    "content": "Decomposing a factorial into largefactors\n\t\t26 March, 2025 in math.NT, paper | Tags: Erdos, factorial function, factorisation | by Terence Tao\n\nI’ve just uploaded to the arXiv the paper “Decomposing a factorial into large factors“. This paper studies the quantity , defined as the largest quantity such that it is possible to factorize  into  factors , each of which is at least . The first few values of this sequence are\n (OEIS A034258). For instance, we have , because on the one hand we can factor\n but on the other hand it is not possible to factorize  into nine factors, each of which is  or higher.\n\nThis quantity  was introduced by Erdös, who asked for upper and lower bounds on ; informally, this asks how equitably one can split up  into  factors. When factoring an arbitrary number, this is essentially a variant of the notorious knapsack problem (after taking logarithms), but one can hope that the specific structure of the factorial  can make this particular knapsack-type problem more tractable. Since\n for any putative factorization, we obtain an upper bound\n thanks to the Stirling approximation. At one point, Erdös, Selfridge, and Straus claimed that this upper bound was asymptotically sharp, in the sense that\n as ; informally, this means we can split  into  factors that are (mostly) approximately the same size, when  is large. However, as reported in this later paper, Erdös “believed that Straus had written up our proof… Unfortunately Straus suddenly died and no trace was ever found of his notes. Furthermore, we never could reconstruct our proof, so our assertion now can be called only a conjecture”.\n\nSome further exploration of  was conducted by Guy and Selfridge. There is a simple construction that gives the lower bound\n that comes from starting with the standard factorization  and transferring some powers of  from the later part of the sequence to the earlier part to rebalance the terms somewhat. More precisely, if one removes one power of two from the even numbers between  and , and one additional power of two from the multiples of four between  to , this frees up  powers of two that one can then distribute amongst the numbers up to  to bring them all up to at least  in size. A more complicated procedure involving transferring both powers of  and  then gives the improvement . At this point, however, things got more complicated, and the following conjectures were made by Guy and Selfridge:\n\n  (i) Is  for all ?  (ii) Is  for all ? (At , this conjecture barely fails: .)  (iii) Is  for all ?\n\nIn this note we establish the bounds\n as , where  is the explicit constant\n In particular this recovers the lost result (2). An upper bound of the shape\n for some  was previously conjectured by Erdös and Graham (Erdös problem #391). We conjecture that the upper bound in (3) is sharp, thus\n which is consistent with the above conjectures (i), (ii), (iii) of Guy and Selfridge, although numerically the convergence is somewhat slow.\n\nThe upper bound argument for (3) is simple enough that it could also be modified to establish the first conjecture (i) of Guy and Selfridge; in principle, (ii) and (iii) are now also reducible to a finite computation, but unfortunately the implied constants in the lower bound of (3) are too weak to make this directly feasible. However, it may be possible to now crowdsource the verification of (ii) and (iii) by supplying a suitable set of factorizations to cover medium sized , combined with some effective version of the lower bound argument that can establish  for all  past a certain threshold. The value  singled out by Guy and Selfridge appears to be quite a suitable test case: the constructions I tried fell just a little short of the conjectured threshold of , but it seems barely within reach that a sufficiently efficient rearrangement of factors can work here.\n\nWe now describe the proof of the upper and lower bound in (3). To improve upon the trivial upper bound (1), one can use the large prime factors of . Indeed, every prime  between  and  divides  at least once (and the ones between  and  divide it twice), and any factor  that contains such a factor therefore has to be significantly larger than the benchmark value of . This observation already readily leads to some upper bound of the shape (4) for some ; if one also uses the primes  that are slightly less than  (noting that any multiple of  that exceeds , must in fact exceed ) is what leads to the precise constant .\n\nFor previous lower bound constructions, one started with the initial factorization  and then tried to “improve” this factorization by moving around some of the prime factors. For the lower bound in (3), we start instead with an approximate factorization roughly of the shape\n where  is the target lower bound (so, slightly smaller than ), and  is a moderately sized natural number parameter (we will take , although there is significant flexibility here). If we denote the right-hand side here by , then  is basically a product of  numbers of size at least . It is not literally equal to ; however, an easy application of Legendre’s formula shows that for odd small primes ,  and  have almost exactly the same number of factors of . On the other hand, as  is odd,  contains no factors of , while  contains about  such factors. The prime factorizations of  and  differ somewhat at large primes, but  has slightly more such prime factors as  (about  such factors, in fact). By some careful applications of the prime number theorem, one can tweak some of the large primes appearing in  to make the prime factorization of  and  agree almost exactly, except that  is missing most of the powers of  in , while having some additional large prime factors beyond those contained in  to compensate. With a suitable choice of threshold , one can then replace these excess large prime factors with powers of two to obtain a factorization of  into  terms that are all at least , giving the lower bound.\n\nThe general approach of first locating some approximate factorization of  (where the approximation is in the “adelic” sense of having not just approximately the right magnitude, but also approximately the right number of factors of  for various primes ), and then moving factors around to get an exact factorization of , looks promising for also resolving the conjectures (ii), (iii) mentioned above. For instance, I was numerically able to verify that  by the following procedure:\n\n  Start with the approximate factorization of ,  by . Thus  is the product of  odd numbers, each of which is at least .  Call an odd prime -heavy if it divides  more often than , and -heavy if it divides  more often than . It turns out that there are  more -heavy primes than -heavy primes (counting multiplicity). On the other hand,  contains  powers of , while  has none. This represents the (multi-)set of primes one has to redistribute in order to convert a factorization of  to a factorization of .  Using a greedy algorithm, one can match a -heavy prime  to each -heavy prime  (counting multiplicity) in such a way that  for a small  (in most cases one can make , and often one also has ). If we then replace  in the factorization of  by  for each -heavy prime , this increases  (and does not decrease any of the  factors of ), while eliminating all the -heavy primes. With a somewhat crude matching algorithm, I was able to do this using  of the  powers of  dividing , leaving  powers remaining at my disposal. (I don’t claim that this is the most efficient matching, in terms of powers of two required, but it sufficed.)  There are still  -heavy primes left over in the factorization of (the modified version of) . Replacing each of these primes with , and then distributing the remaining  powers of two arbitrarily, this obtains a factorization of  into  terms, each of which are at least .\n\nHowever, I was not able to adjust parameters to reach  in this manner. Perhaps some readers here who are adept with computers can come up with a more efficient construction to get closer to this bound? If one can find a way to reach this bound, most likely it can be adapted to then resolve conjectures (ii) and (iii) above after some additional numerical effort.\n\nShare this:PrintEmailMoreTwitterFacebookRedditPinterestLike Loading...\n\nRecent Comments\n\t\t\t\t\tAnonymous on The blue-eyed islanders puzzle…Anonymous on The blue-eyed islanders puzzle…Anonymous on The blue-eyed islanders puzzle…Anonymous on Decomposing a factorial into l…Anonymous on Cosmic Distance Ladder videos…Anonymous on Analysis IAnonymous on Analysis ITerence Tao on Analysis IAnonymous on Analysis Ifrobitzblog on Decomposing a factorial into l…Anonymous on Analysis IAnonymous on Analysis IAnonymous on Analysis ITerence Tao on Decomposing a factorial into l…Terence Tao on Decomposing a factorial into l…\n\nTop PostsDecomposing a factorial into large factorsThe three-dimensional Kakeya conjecture, after Wang and ZahlCareer adviceCosmic Distance Ladder videos with Grant Sanderson (3blue1brown): commentary and correctionsAnalysis IBooksOn writingWork hardDoes one have to be a genius to do maths?AboutArchives\n\n\t\t\t\t\tMarch 2025(1)\n\tFebruary 2025(3)\n\tJanuary 2025(1)\n\tDecember 2024(3)\n\tNovember 2024(4)\n\tOctober 2024(1)\n\tSeptember 2024(4)\n\tAugust 2024(3)\n\tJuly 2024(3)\n\tJune 2024(1)\n\tMay 2024(1)\n\tApril 2024(5)\n\tMarch 2024(1)\n\tDecember 2023(2)\n\tNovember 2023(2)\n\tOctober 2023(1)\n\tSeptember 2023(3)\n\tAugust 2023(3)\n\tJune 2023(8)\n\tMay 2023(1)\n\tApril 2023(1)\n\tMarch 2023(2)\n\tFebruary 2023(1)\n\tJanuary 2023(2)\n\tDecember 2022(3)\n\tNovember 2022(3)\n\tOctober 2022(3)\n\tSeptember 2022(1)\n\tJuly 2022(3)\n\tJune 2022(1)\n\tMay 2022(2)\n\tApril 2022(2)\n\tMarch 2022(5)\n\tFebruary 2022(3)\n\tJanuary 2022(1)\n\tDecember 2021(2)\n\tNovember 2021(2)\n\tOctober 2021(1)\n\tSeptember 2021(2)\n\tAugust 2021(1)\n\tJuly 2021(3)\n\tJune 2021(1)\n\tMay 2021(2)\n\tFebruary 2021(6)\n\tJanuary 2021(2)\n\tDecember 2020(4)\n\tNovember 2020(2)\n\tOctober 2020(4)\n\tSeptember 2020(5)\n\tAugust 2020(2)\n\tJuly 2020(2)\n\tJune 2020(1)\n\tMay 2020(2)\n\tApril 2020(3)\n\tMarch 2020(9)\n\tFebruary 2020(1)\n\tJanuary 2020(3)\n\tDecember 2019(4)\n\tNovember 2019(2)\n\tSeptember 2019(2)\n\tAugust 2019(3)\n\tJuly 2019(2)\n\tJune 2019(4)\n\tMay 2019(6)\n\tApril 2019(4)\n\tMarch 2019(2)\n\tFebruary 2019(5)\n\tJanuary 2019(1)\n\tDecember 2018(6)\n\tNovember 2018(2)\n\tOctober 2018(2)\n\tSeptember 2018(5)\n\tAugust 2018(3)\n\tJuly 2018(3)\n\tJune 2018(1)\n\tMay 2018(4)\n\tApril 2018(4)\n\tMarch 2018(5)\n\tFebruary 2018(4)\n\tJanuary 2018(5)\n\tDecember 2017(5)\n\tNovember 2017(3)\n\tOctober 2017(4)\n\tSeptember 2017(4)\n\tAugust 2017(5)\n\tJuly 2017(5)\n\tJune 2017(1)\n\tMay 2017(3)\n\tApril 2017(2)\n\tMarch 2017(3)\n\tFebruary 2017(1)\n\tJanuary 2017(2)\n\tDecember 2016(2)\n\tNovember 2016(2)\n\tOctober 2016(5)\n\tSeptember 2016(4)\n\tAugust 2016(4)\n\tJuly 2016(1)\n\tJune 2016(3)\n\tMay 2016(5)\n\tApril 2016(2)\n\tMarch 2016(6)\n\tFebruary 2016(2)\n\tJanuary 2016(1)\n\tDecember 2015(4)\n\tNovember 2015(6)\n\tOctober 2015(5)\n\tSeptember 2015(5)\n\tAugust 2015(4)\n\tJuly 2015(7)\n\tJune 2015(1)\n\tMay 2015(5)\n\tApril 2015(4)\n\tMarch 2015(3)\n\tFebruary 2015(4)\n\tJanuary 2015(4)\n\tDecember 2014(6)\n\tNovember 2014(5)\n\tOctober 2014(4)\n\tSeptember 2014(3)\n\tAugust 2014(4)\n\tJuly 2014(5)\n\tJune 2014(5)\n\tMay 2014(5)\n\tApril 2014(2)\n\tMarch 2014(4)\n\tFebruary 2014(5)\n\tJanuary 2014(4)\n\tDecember 2013(4)\n\tNovember 2013(5)\n\tOctober 2013(4)\n\tSeptember 2013(5)\n\tAugust 2013(1)\n\tJuly 2013(7)\n\tJune 2013(12)\n\tMay 2013(4)\n\tApril 2013(2)\n\tMarch 2013(2)\n\tFebruary 2013(6)\n\tJanuary 2013(1)\n\tDecember 2012(4)\n\tNovember 2012(7)\n\tOctober 2012(6)\n\tSeptember 2012(4)\n\tAugust 2012(3)\n\tJuly 2012(4)\n\tJune 2012(3)\n\tMay 2012(3)\n\tApril 2012(4)\n\tMarch 2012(5)\n\tFebruary 2012(5)\n\tJanuary 2012(4)\n\tDecember 2011(8)\n\tNovember 2011(8)\n\tOctober 2011(7)\n\tSeptember 2011(6)\n\tAugust 2011(8)\n\tJuly 2011(9)\n\tJune 2011(8)\n\tMay 2011(11)\n\tApril 2011(3)\n\tMarch 2011(10)\n\tFebruary 2011(3)\n\tJanuary 2011(5)\n\tDecember 2010(5)\n\tNovember 2010(6)\n\tOctober 2010(9)\n\tSeptember 2010(9)\n\tAugust 2010(3)\n\tJuly 2010(4)\n\tJune 2010(8)\n\tMay 2010(8)\n\tApril 2010(8)\n\tMarch 2010(8)\n\tFebruary 2010(10)\n\tJanuary 2010(12)\n\tDecember 2009(11)\n\tNovember 2009(8)\n\tOctober 2009(15)\n\tSeptember 2009(6)\n\tAugust 2009(13)\n\tJuly 2009(10)\n\tJune 2009(11)\n\tMay 2009(9)\n\tApril 2009(11)\n\tMarch 2009(14)\n\tFebruary 2009(13)\n\tJanuary 2009(18)\n\tDecember 2008(8)\n\tNovember 2008(9)\n\tOctober 2008(10)\n\tSeptember 2008(5)\n\tAugust 2008(6)\n\tJuly 2008(7)\n\tJune 2008(8)\n\tMay 2008(11)\n\tApril 2008(12)\n\tMarch 2008(12)\n\tFebruary 2008(13)\n\tJanuary 2008(17)\n\tDecember 2007(10)\n\tNovember 2007(9)\n\tOctober 2007(9)\n\tSeptember 2007(7)\n\tAugust 2007(9)\n\tJuly 2007(9)\n\tJune 2007(6)\n\tMay 2007(10)\n\tApril 2007(11)\n\tMarch 2007(9)\n\tFebruary 2007(4)\n\n\t\t\tCategories\n\n\t\t\t\t\texpository (315)\n\n\ttricks (13)\n\n\tguest blog (10)\n\n\tMathematics (885)\n\n\tmath.AC (8)\n\n\tmath.AG (42)\n\n\tmath.AP (114)\n\n\tmath.AT (17)\n\n\tmath.CA (188)\n\n\tmath.CO (197)\n\n\tmath.CT (9)\n\n\tmath.CV (37)\n\n\tmath.DG (37)\n\n\tmath.DS (89)\n\n\tmath.FA (24)\n\n\tmath.GM (14)\n\n\tmath.GN (21)\n\n\tmath.GR (88)\n\n\tmath.GT (16)\n\n\tmath.HO (13)\n\n\tmath.IT (13)\n\n\tmath.LO (53)\n\n\tmath.MG (47)\n\n\tmath.MP (31)\n\n\tmath.NA (24)\n\n\tmath.NT (199)\n\n\tmath.OA (22)\n\n\tmath.PR (109)\n\n\tmath.QA (6)\n\n\tmath.RA (47)\n\n\tmath.RT (21)\n\n\tmath.SG (4)\n\n\tmath.SP (48)\n\n\tmath.ST (11)\n\n\tnon-technical (195)\n\n\tadmin (46)\n\n\tadvertising (66)\n\n\tdiversions (7)\n\n\tmedia (14)\n\n\tjournals (3)\n\n\tobituary (15)\n\n\topinion (36)\n\n\tpaper (253)\n\n\tbook (20)\n\n\tCompanion (13)\n\n\tupdate (23)\n\n\tquestion (127)\n\n\tpolymath (86)\n\n\ttalk (68)\n\n\tDLS (20)\n\n\tteaching (188)\n\n\t245A – Real analysis (11)\n\n\t245B – Real analysis (21)\n\n\t245C – Real analysis (6)\n\n\t246A – complex analysis (11)\n\n\t246B – complex analysis (5)\n\n\t246C – complex analysis (5)\n\n\t247B – Classical Fourier Analysis (5)\n\n\t254A – analytic prime number theory (19)\n\n\t254A – ergodic theory (18)\n\n\t254A – Hilbert's fifth problem (12)\n\n\t254A – Incompressible fluid equations (5)\n\n\t254A – random matrices (14)\n\n\t254B – expansion in groups (8)\n\n\t254B – Higher order Fourier analysis (9)\n\n\t255B – incompressible Euler equations (2)\n\n\t275A – probability theory (6)\n\n\t285G – poincare conjecture (20)\n\n\tLogic reading seminar (8)\n\n\tThe sciences (1)\n\n\ttravel (26)\n\n\t\t\tadditive combinatorics\napproximate groups\narithmetic progressions\nBen Green\nCauchy-Schwarz\nCayley graphs\ncentral limit theorem\nChowla conjecture\ncompressed sensing\ncorrespondence principle\ndistributions\ndivisor function\neigenvalues\nElias Stein\nEmmanuel Breuillard\nentropy\nequidistribution\nergodic theory\nEuler equations\nexponential sums\nfinite fields\nFourier transform\nFreiman's theorem\nGowers uniformity norm\nGowers uniformity norms\ngraph theory\nGromov's theorem\nGUE\nHilbert's fifth problem\nincompressible Euler equations\ninverse conjecture\nJoni Teravainen\nKaisa Matomaki\nKakeya conjecture\nLie algebras\nLie groups\nLiouville function\nLittlewood-Offord problem\nMaksym Radziwill\nMobius function\nmultiplicative functions\nNavier-Stokes equations\nnilpotent groups\nnilsequences\nnonstandard analysis\nparity problem\nPaul Erdos\npolitics\npolymath1\npolymath8\nPolymath15\npolynomial method\npolynomials\nprime gaps\nprime numbers\nprime number theorem\nrandom matrices\nrandomness\nRatner's theorem\nregularity lemma\nRicci flow\nRiemann zeta function\nSchrodinger equation\nShannon entropy\nsieve theory\nstructure\nSzemeredi's theorem\nTamar Ziegler\ntiling\nUCLA\nultrafilters\nuniversality\nVan Vu\nwave maps\nYitang Zhang The Polymath BlogPolymath projects 2021A sort of Polymath on a famous MathOverflow problemTen Years of PolymathUpdates and PicturesPolymath proposal: finding simpler unit distance graphs of chromatic number 5A new polymath proposal (related to the Riemann Hypothesis) over Tao’s blogSpontaneous Polymath 14 – A success!Polymath 13 – a success!Non-transitive Dice over Gowers’s BlogRota’s Basis Conjecture: Polymath 12, post 3\n\n\t\t\t13 comments\n\t\t\tComments feed for this article\n\n\t\t\t26 March, 2025 at 8:27 pm\n\t\t\tAnonymous\n\n\t\t\t\t\t\tthere is an open brace for href\n[Corrected, thanks – T.]\n\n\t\t\t\tReply\n\n\t\t\t26 March, 2025 at 11:04 pm\n\t\t\tSamuel Bonaya Buya\n\n\t\t\t\t\t\tIn my opinion the paper is a significant contribution by Tao on the understanding of the prime number theorem\n\n\t\t\t\tReply\n\n\t\t\t27 March, 2025 at 4:02 am\n\t\t\tAntoine Deleforge\n\n\t\t\t\t\t\tLooking at the first few numbers in the OEIS sequence, it looks like t(n+1) – t(n) is always zero or one. Is there any reason for this to be true?\n\n\t\t\t\tReply\n\n\t\t\t27 March, 2025 at 6:39 am\n\t\t\tTerence Tao\n\n\t\t\t\t\t\tNo; in fact, in Guy’s article on this problem, he notes that there is a jump of  from  to  (though he does not provide enough preceding values to extend the sequence in the OEIS).  In that article he also notes that Erdos conjectures that the gaps can in fact be arbitrarily large, though I see no way to attack this question even heuristically (as the extremizers for this problem may be neither structured nor (pseudo)random, but exhibit some very strange intermediate behavior).\nIn the image below, I display the upper bound on  (the pink dots) in the intermediate range  coming from Lemma 2.1 of my paper (there is no plot for  in this image as I do not have data in this range).  [Incidentally there is a slight typo in that lemma, which I will correct in the next revision: the term  should instead be .]  There is considerable fluctuation here (due to the corresponding fluctuation in the primes), which is also reflected in the related plot in Figure 2 of the paper.  Of course, fluctuation in the upper bound for  does not imply fluctuation in the true value of , but it is perhaps evidence in that direction.\n\nAnd below is a comparison of the upper bound against the true value of  in the range :\n\n\t\t\t\tReply\n\n\t\t\t27 March, 2025 at 4:14 am\n\t\t\tIvan\n\n\t\t\t\t\t\tYou may want to enclose the comma in curly brackets when it is used as a thousands separator so that  does not generate extra space after it, e.g.,  instead of  (p. 3 of the paper).\n[Thanks, this will be done in the next revision of the ms -T]\n\n\t\t\t\tReply\n\n\t\t\t27 March, 2025 at 5:18 am\n\t\t\tAntoine Deleforge\n\n\t\t\t\t\t\tI don’t immediately see the connection to the knapsack problem. If we pick the non-dividable items to be the logs of the prime factors of N!, then the problem amounts to distributing *all* of these items into N knapsacks, such that each knapsack contains *at least* a value of t(N). This is quite different from the original knapsack problem where the goal is rather to select a *subset* of items, and maximize the value while remaining *below* the knapsack capacity. Is there a deeper or more natural connection that I am missing? Can further progress on this Erdos problem be expected to eventually yield insights on the knapsack problem, or is the relation between the two too distant for that to happen?\n\n\t\t\t\tReply\n\n\t\t\t27 March, 2025 at 6:53 am\n\t\t\tTerence Tao\n\n\t\t\t\t\t\tIt’s more accurate to say that the factorial problem is a *variant* of the knapsack problem; most directly, it corresponds to a knapsack problem with negative item sizes (and negative capacity in the backpack), which of course is not physically realistic (or intuitive), but it is possible that some of the knapsack algorithms that work for positive sizes and capacities can carry over to this new context with suitable modification.  (For instance, I would guess that the problem of solving this sort of factoring problem for a general input number (rather than a factorial) is NP-complete, by some modification of the proof of NP-completeness of the knapsack problem.)\nNote by the way that to solve the factorization problem, it suffices to distribute some subset of the log-primes into the knapsacks rather than all of them, since one can just add in the remaining log-primes arbitrarily to finish the job.\n\n\t\t\t\tReply\n\n\t\t\t27 March, 2025 at 7:08 am\n\t\t\tducduc2710\n\n\t\t\t\t\t\tThat technique I think can be used to limit prime gaps.\n\n\t\t\t\tReply\n\n\t\t\t27 March, 2025 at 7:21 am\n\t\t\tAnonymous\n\n\t\t\t\t\t\tSmall typo: “multiples of four between3/4 to N” It should be 3/4N to N.\n\n[Corrected, thanks -T.]\n\n\t\t\t\tReply\n\n\t\t\t27 March, 2025 at 11:35 am\n\t\t\tTerence Tao\n\n\t\t\t\t\t\tI’m posting (with permission) some computational work by Andrew Sutherland, who implemented a greedy approach working through the prime factors  of  inreverse order (with multiplicity), constucting integers of the form  with  chosen to be minimal subject to the constraint that it can be constructed from the divisors of  that still remain. For instance when , it is able to factor  into  numbers greater than or equal to , verifying the Guy-Selfridge conjecture at this value. The code (in Maple) is at https://math.mit.edu/~drew/GuySelfridge.m . An earlier (less efficient) factorization with these parameters can be found at https://math.mit.edu/~drew/ES300000.txt .\nAndrew writes, ” It only takes about a few seconds on a fast machine so I was able to run it on all  in  and noticed that while it typically succeeds on , it still fails to prove  in  cases in , including  as large as . But it succeeds on every  in , so if the conjecture is true, it is still true if you replace  with  (probably this can be lowered a lot further, the greedy approach is not optimal).\nI then tested the threshold  on all  from  to . It failed only for “.\nWith this data, one can now reduce conjecture (ii) to conjecture (iii) provided one can construct suitable factorizations of  to resolve the three remaining cases  (one also has to retest the range  but this should be straightforward, since there is now enough room that one should be able to sample this range quite sparsely, e.g., test the threshold  for  a multiple of ). But the range  seems a bit more delicate, as the most direct greedy algorithm sometimes fails.\nAndrew adds, “I think it’s possible that one might be able to turn this algorithm into an asymptotic bound. For sufficiently large prime divisors  of , you can just take  because  will be small and there are plenty of powers of small primes initially available (and you can quantify this), and even if you focus just on the integers the algorithm constructs before it hits the first cofactor it cannot minimize you should get some constant factor of  that might be bigger than .”\n\n\t\t\t\tReply\n\n\t\t\t27 March, 2025 at 12:18 pm\n\t\t\tTerence Tao\n\n\t\t\t\t\t\tAndrew has kindly shared with me the lower bounds for  for  given by this approach, and I have incorporated them into my previous plot here, showing the current upper (pink) and lower (blue) bounds on :\n\nNote the verification of the conjecture  in this range for .  The data also replicates Guy’s reported values , and suggests that the jumps in  are indeed rather irregular. (The text file for the data can be found here.)\n\n\t\t\t\tReply\n\n\t\t\t27 March, 2025 at 2:35 pm\n\t\t\tfrobitzblog\n\n\t\t\t\t\t\tWith a bit of fiddling by hand I was able to improve the factorizations for N=182,200,207, so now (ii) is confirmed up to 100,000.  You can find the factorizations here, here, and here.\n\n\t\t\t\tReply\n\n\t\t\t28 March, 2025 at 8:12 am\n\t\t\tAnonymous\n\n\t\t\t\t\t\tShould inequality (4) have t(N)/N on the left-hand side (as opposed to just t(N))?\n[Corrected, thanks – T.]\n\n\t\t\t\tReply\n\n\t\tLeave a comment Cancel reply\n\n\t\t\tΔdocument.getElementById( \"ak_js_1\" ).setAttribute( \"value\", ( new Date() ).getTime() );",
    "summary": {
      "en": "The paper \"Decomposing a Factorial into Large Factors\" by Terence Tao, discusses how to divide a factorial into several factors that are all at least a certain size. This topic builds on work started by mathematician Paul Erdös, who was interested in finding upper and lower limits for this factorization.\n\nKey points include:\n\n1. **Definition**: The paper defines a quantity related to how well a factorial can be split into factors of a minimum size. The goal is to find the largest number of factors that can be used while ensuring each factor meets the size requirement.\n\n2. **Historical Context**: Erdös had previously conjectured about the bounds of this quantity but did not fully prove it due to the loss of notes from collaborator John Straus.\n\n3. **Recent Findings**: Tao establishes new bounds for this quantity and recovers some previously lost results. He also suggests that the problem resembles a variant of the knapsack problem, where items (or factors) must be optimally selected to meet certain criteria.\n\n4. **Conjectures**: Tao discusses several conjectures related to the bounds and invites further computational exploration to resolve them. He provides methods for approximating the factorization and suggests a potential crowd-sourced approach to verify certain conjectures.\n\n5. **Methodology**: The paper details how to improve factorization by manipulating prime factors and redistributing them to achieve the required minimum size for all factors.\n\nOverall, the paper contributes to understanding how to effectively decompose factorials, a topic with connections to number theory and combinatorial optimization.",
      "ko": "테렌스 타오의 논문 \"팩토리얼을 큰 인수로 분해하기\"는 팩토리얼을 최소한의 크기를 가진 여러 인수로 나누는 방법에 대해 다룹니다. 이 주제는 수학자 폴 에르되시가 시작한 연구를 기반으로 하며, 그는 이러한 인수 분해의 상한과 하한을 찾는 데 관심이 있었습니다.\n\n논문의 주요 내용은 다음과 같습니다. 첫째, 팩토리얼을 최소 크기의 인수로 얼마나 잘 나눌 수 있는지를 나타내는 양을 정의합니다. 목표는 각 인수가 크기 요건을 충족하면서 사용할 수 있는 최대 인수의 수를 찾는 것입니다. \n\n둘째, 에르되시는 이 양의 경계에 대해 이전에 추측했지만, 동료인 존 스트라우스의 노트를 잃어버려 완전한 증명을 하지 못했습니다. \n\n셋째, 타오는 이 양에 대한 새로운 경계를 설정하고 이전에 잃어버린 결과를 회복합니다. 그는 이 문제가 특정 기준을 충족하기 위해 아이템(또는 인수)을 최적으로 선택해야 하는 배낭 문제의 변형과 유사하다고 제안합니다. \n\n넷째, 타오는 경계와 관련된 여러 추측을 논의하며, 이를 해결하기 위한 추가적인 계산 탐색을 권장합니다. 그는 인수 분해를 근사하는 방법을 제공하고, 특정 추측을 검증하기 위한 잠재적인 크라우드소싱 접근법을 제안합니다. \n\n마지막으로, 논문은 소인수를 조작하고 재분배하여 모든 인수가 요구되는 최소 크기를 달성하도록 인수 분해를 개선하는 방법을 자세히 설명합니다. 전반적으로 이 논문은 숫자 이론과 조합 최적화와 관련된 팩토리얼 분해를 효과적으로 이해하는 데 기여합니다.",
      "ja": null
    }
  },
  {
    "id": "725d856e594facb9",
    "title": {
      "en": "Finley (YC W21) Is Hiring a Technical Implementations Specialist",
      "ko": null,
      "ja": null
    },
    "type": "job",
    "url": "https://ats.rippling.com/finley-technologies/jobs",
    "score": 1,
    "by": "festinalente",
    "time": 1743195709,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "143a7a518678cbf9",
    "title": {
      "en": "The Art of DJing: Avalon Emerson (2019)",
      "ko": "DJing의 미학: 아발론 에머슨",
      "ja": null
    },
    "type": "story",
    "url": "https://it.ra.co/features/3392",
    "score": 39,
    "by": "easyThrowaway",
    "time": 1743189734,
    "content": "#feature {\n\n    }\n\n    #feature a {\n\n    }\n\n    #feature .quote {\n\n    }\n\n      One of the most creative DJs in the game unpacks her methods for Elissa Stolman.\n\nWe're celebrating ten years of The Art of DJing. Get stuck into more of the series here.\n\nAbout a week before our scheduled date for this interview, Avalon Emerson seemed over it. \"I'm not a vibeman,\" she told me, contrasting her technical acumen with the unlearnable, abstract magic other DJs apparently possess. She was concerned that this feature would break down her workflow into digestible info nuggets that most people could and have figured out and that anyone can replicate. As a result she'd come off too clinical, her talents less impressive than those of the DJs she calls \"vibemen.\"\n\nBut never in my life have I met someone who vibes more intensely than Avalon Emerson. Her vibes have the capacity to exalt and to destroy. How did she not realize that she is a total vibewoman?\n\nSure, some have probably discovered any given facet of her organizational methodology. Others use many of the same rekordbox and CDJ functions. But they didn't create the mind palace that is Emerson's approach to DJing. I visualize her way of obtaining, organizing, editing, playing, mixing and archiving music as a Fordist factory designed like a panopticon. Although the individual tasks are simple, their integration into a logical whole is remarkable. It's a less romantic vision of what makes her a great DJ, but it's no less impressive.\n\nEmerson's approach to hacking DJ technology's underused possibilities seems like an application of her knowledge of computer programming. It's probably her nerdiness—visual, intellectual and musical—that distinguishes her most obviously from the vibemen of DJ world. Vibemen preside over ecstatic dance floors, sweating and grimacing while they blend the old-fashioned way. Emerson leans in close to the screen with her fingers on the dial and squints through Navigator glasses, piloting the CDJ with the practiced ease of someone who types very quickly.\n\nThe whole tech whiz thing is ultimately a tool that allows her to devote most of her CPU to determining what to play and when, enchanting a crowd, making people dance—in short, delivering the kind of ecstatic communal experience that elevates playing songs to the art of DJing. Her entire process of organising and playing music is engineered to help her find and transition to the next song as quickly possible, which is key for someone who jumps between genres and tempos as often as Emerson does. Any given set might include shades of hi-NRG and New Beat, booming breakbeats or aquatic house rollers, but they always bear her pop sensibility. Many of the edits she plays occasionally appear as free downloads on her personal website under the banner of Cybernedits.\n\nEmerson has taught me a lot of things about DJing and production. One of her most salient lessons was one she demonstrates more often than she articulates: a technological and rational approach doesn't eliminate an artist's creative juju—one can enhance the other. Anyone can learn Emerson's tricks, but the rest is up to you.\n\nLet's start with how you developed your current style of mixing. You don't usually bring records to gigs anymore, but you used to. When and why did you phase out vinyl? How long of a process was that?\n\nTo be honest, like a lot of other new DJs, I felt like if I didn't walk into the club with vinyl then I was somehow or perceived to be \"less\" of a DJ. So I felt pressured to do so. But it's annoying to carry records around, and more importantly, you can do more interesting things on CDJs. I also edit a lot of stuff, and those edits obviously only exist as digital files.\n\nDid you learn to DJ on vinyl?\n\nNot really. I used iTunes the first time I played music for people in a party scenario when I was like, 19.\n\nWas there ever a period where you played only or mostly vinyl?\n\nNo. I played CDs for a long time in San Francisco because USB CDJs didn't exist for the majority of the time that I lived there. Some bars had a Serato box, but I didn't buy one. Back then, whenever I encountered CDJs, they were usually these crazy wedding CDJs that are like, rack-mount units with CD trays that pop out like old desktop computers. The options were either vinyl, Serato or those CDJs, and no one used CDJs until the CDJ 1000s started popping up. I've made edits and produced music for longer than I've been a DJ, so I've always wanted to incorporate my original material. That's the main reason I've never been a vinyl-only DJ.\n\nWhen did you decide that people who judged others negatively for not using vinyl were wrong?\n\nWhen I started getting feedback that I was a good DJ. I was better than a lot of people who were playing only vinyl anyways.\n\nYou don't play vinyl anymore, but you do have a vinyl collection here at your house. Why do you still buy records?\n\nBecause that's still usually the most straightforward way to get a high-quality version of a track, and with old stuff it's often the only way. Buying and ripping is a massive part of being a DJ nowadays. Do I carry it in a Rimowa everywhere? No. But I still buy and rip constantly. I use a belt-drive turntable to rip records because the timing is more constant and they apparently transfer less noise to the needle. It's more of a hi-fi turntable. The signal goes through a Urei Soundcraft 1620 mixer, which has nice vinyl preamps in it and just sounds nice. That goes into my Soundcraft sound card, and then I record that in Ableton. Then I edit, EQ, decrackle, and remaster and normalize it.\n\nWhen I first moved to Berlin, I didn't have a good set-up for ripping, so sometimes I brought a few records to gigs because I didn't have the track digitally or because I just wanted to play vinyl. But I stopped using vinyl at all as I started to prioritize archiving my sets. I save the history of my gigs so that I can look back over four years and see exactly what I played in the order I played it. You can't do that with vinyl.\n\nWhy do you find it helpful to archive your sets?\n\nI don't do it so that I can get better, I do it mostly for personal archival reasons. I want to know what I played in Panorama Bar in July 2015. I think it's cool to be able to look back and see that. Compared to making music and releasing it on a record, DJing a party is inherently ephemeral. The medium requires the context of the space and time to be relevant as an artform. So any type of archival stuff like recording my sets, which I do as much as I can, is really important to me, but it's kinda a personal thing.\n\nWhat equipment do you bring to gigs, and how do you use each piece?\n\nMy rider says: no vinyl; three or four CDJs, at least 2000 Mark I, Mark II if available; an Allen & Heath Xone 92 or 96. I like how effect routing works on the Allen & Heath. At the top of the section about mixers is something that says, if there's a nice rotary mixer available in the club—some clubs secretly do have a nice rotary—I'd like to use it. I DJ a lot, so playing on nice hi-fi rotaries every once in a while can break it up a little bit and make it interesting and fun for me. I bring USBs—well, right now I use super high-speed SD cards made for photographers and videographers. I need ones with high write speeds because I have a billion different intelligent playlists, and it'll take Rekordbox much longer to write the music onto a drive with a run-of-the-mill, cheap USB drive. And if you have something with a slow read-speed paired with an older CDJ, that's when you start getting \"Cannot read\" or buffering errors. I also bring headphones and pedals.\n\nTell me about the pedals. What do you use the pedals for?\n\nI used to bring an isolator and sometimes I bring two pedals, but usually I just bring one. Sometimes it's a delay, sometimes it's a reverb. I've tried different ones and switch them out when I get sick of them. If I'm using a rotary mixer, I usually don't use the pedals because most of them—except for some E&S DJR 400s—don't have an effect loop or it's not routed as a send/return. Most of the time when I use a pedal, I route it as a send/return through the Xone 92 rather than as an insert effect, which is a fundamentally different way of doing effects. It's like in Ableton: if you a drop a reverb directly onto the track, it's different than if you send the track's dry signal to a return, which is the way you do it on an Allen & Heath. I have the pedal plugged into a return on the mixer and have the fader all the way up with the signal 100 percent wet. That means when I chop something in, I send the track's dry signal to the effect return using the knobs above the gain, and that delay or reverb will remain in the mix even if I take the dry signal out.\n\nIf I want to mix out in a dramatic way, I can increase the wet signal from the outgoing track as it's fading out or coming to a point where I want to chop it out. I increase the effect on channel one until there's a snare roll or something and time that with some big, meaty change in the incoming track. I increase the effect wetness on channel one, and then drop out the dry signal of channel one by pulling down the fader. Instead of just having a silent void immediately following that cut, there's a nice little reverb or delay tail, and that passes the baton in a more musical way to track two.\n\nWhat kind of reverbs and delay pedals do you use?\n\nI use a bunch of different kinds. The Strymon blueSky Reverb is kind of a techno bro one, but it's nice. I also have a TC Electronic Hall Of Fame 2, a Teil1 Keinedelay and an Eventide Space, and a really cheap little Mooer Ana Echo, which is a little too gnarly for me, and it's in mono. Once I played with it at De School, and it self-oscillated in a very loud way. I was kind of embarrassed.\n\nThat's the same way you learned to do things on a CDJ, right? By trying things at a gig you haven't done before—and sometimes that doesn't go your way.\n\nYeah, for sure. Of course. All the time. If you're not going to try something because you might be embarrassed by it, then you should just quit trying anything in life, because you're gonna fail at it sometimes, too.\n\nWhat CDJ functions do you use the most often?\n\nI loop stuff. I use the track tag and parameter filters, and I use hot cues to break up a track's linear structure.\n\nHow much preparation does that entail in rekordbox?\n\nIt's easier to set hot cues if you do it ahead of time, but you can do it on the fly, and I do that sometimes. You have to set auto-loops in advance, so that once the playhead reaches the timecode you set, it'll automatically start looping there. Say there's a really chill part of a track before it goes into a hectic part with vocals and horns and all kinds of stuff, and you'd rather mix out before that happens—you can do that. Also, any notes have to be set in advance. A note that says \"vocals\" or \"hit C to skip breakdown\" helps me do some of my \"live editing\" and gives me flexibility and breathing room when it comes to how I play certain songs. The other functions I use most are beat jump, which makes the playhead skip a certain amount of beats ahead or backwards. I also use slip mode a bit, which lets you scratch or loop or do a goofy vinyl break without losing your place in the song.\n\nYou've mentioned chopping tracks in, mixing out in dramatic ways and rearranging tracks using hot cues. How would you describe your approach to mixing records? You're not trying to make long, seamless blends that are phrased well within the limitations of a track's given arrangement, which is often how people mix with vinyl.\n\nI do a lot of cutting and quick mixes. I often layer two eight-bar loops on top of one another, and then I'll let one of the loops go so that the song can build while still maintaining some elements from the previous song until the main part comes in and I want the other one to get out of the way. It's about timing, bottling up energy from tracks and having them stack on top of one another and telling a story—not always in the DJ Harvey-style, vibeman journey way. I think of my \"storytelling\" more like putting a few words together into a clause, and that clause forms a sentence, then I string together sentences to say something in a passage. It's not just about playing songs that have similar-sounding elements in them or that would have been tagged as the same genre in rekordbox. It's hard to quantify.\n\nNow you're talking about structuring sets, not the way you transition between tracks.\n\nYeah. I could probably loop all of my sets into a few different categories. The first is short festival sets that last an hour or two. Then there are four-hour headlining slots at a club, not on a stage. The last category is long sets, like eight hours closing Panorama Bar. Especially during festival sets, I don't really have a lot of room for indecision in the booth. I don't want to be in a position where I'm like, \"Hmm, maybe I'll go in this direction,\" or, \"Hmm, which one should I start with for this new direction idea that I have?\" Also, to be totally honest, it feels pretty pointless to try and \"read\" a crowd of a few thousand people. So for those I have a playlist that's usually twice as long as the set's duration, and it's sorted roughly in the order I'd want to play it. In general I want to start with a pretty intro that's not really a dance music thing—like, if you look at my set from MUTEK a few months ago, I played a North Sea Dialect song before going into something that was still deep but more banging, so there was a contrast between the beatless intro and something that hit harder.\n\nDo you usually have an opening track in mind before you start playing?\n\nYeah, pretty much every time. I want the DJ playing before me to be able to finish as loudly or as dramatically or flashy as they want. Then everyone claps, and it's kind of a reset. When two bands play one after the other, they don't try to groove into the previous band's track. It's kind of annoying when people try to mix in from my last track. It's like, \"Hey, chill. You'll have your moment. It's OK, eager beaver.\" So even if the DJ before me plays a tool, I let it run out, we all clap, and then I start.\n\nOK, so at MUTEK you wanted to start with something beatless to reset the vibe and then contrast that with something more hard-hitting.\n\nI think for that one I was playing two hours, which for the festival's biggest stage is kind of long. So I didn't have to immediately start with the crazy vibes. I played some stuff that's more sparse and drummy and that had some more angular rhythms in it. I'm doing a shitty job at describing this.\n\nNo, you're doing fine. You're just doubting yourself because you're starting to use more \"vibeman\" talk rather than technical language to describe what you're talking about right now, but that's OK. I think you tend to downplay how much of your style is vibeman-related—we can get into that later.\n\nThat's true. OK. So yeah, after an intro you can really go hard and go kinda fast. But there's no need to rush into a crazy vibe. If you look at it like a graph of energy over time, you don't just wanna be at the top all the time. People get tired. So after I gave them the hardness and excitement of big-room rave, I did long blends for 15 or 20 minutes. There are times when I want to just stitch things together to build a monolithic, washing-over-you vibe by layering drummy acapellas and tooly tracks. It's about building pressure and using energy in a dynamic way instead of just going, \"Here's a song, here's a song.\"\n\nYou mentioned earlier that you think about a rough structure for the set in advance and organize that in rekordbox. Can you explain a little more about how you use rekordbox?\n\nI laid out some basic stuff in The Hour last year, but my process is a little more sophisticated now, so I'm going to explain it again. I've set it up for two situations to do with how I choose each next track: browse versus search. Browse is when I don't know what the next song will be, so I want to have my library set up in a way that facilitates me browsing and picking the next song. Search is what I do when I know what song I want to play next and I need my library set up to help me find it as fast as possible. Sometimes I turn into a big illiterate baby up there and I know what song I want to play, but I can't remember the name of the song or the artist that made it.\n\nIt's easier to recognize or recall a song based on the color of the record's label or something like that. That's another reason people like playing vinyl.\n\nTotally. That's something I do appreciate about vinyl. A 12-inch record is a big, human-sized visual cue that you can hold. I'm often in the situation where I can't come up with the artist name or the track name—I just know that it had blue cover art and that I played it in Leeds last weekend. Because I have the history of all my gigs archived, I can go back and find it through those playlists. Since I've been DJing professionally for several years, I split those up; Berghain has its own folder, then I have ones for countries or regions. If I had every gig just sorted in reverse chronological order, I'd have to scroll back through months of gigs to get to my last one in Leeds. This way I can just go \"UK > Leeds > Wire September 18.\" The most recent gigs are at the top. I still can't believe I'm doing an interview about this and that this is what I do.\n\nThen I have my intelligent playlists. The gigs playlists are all manual drag-and-drop, but the intelligent ones are made automatically via tags or other attributes that I've set. I have tags like \"techno,\" \"club,\" \"breaks,\" \"acid,\" \"trancey,\" \"wavey,\" \"drummy,\" \"drone\" and some others. And then I have tags for functions, which are more to do with energy or situations that I would play them in. Those function tags are: \"deep,\" \"glue,\" \"dreamy,\" \"poppy,\" \"sunny,\" then the \"tools\" header, which indicates more functional things, like a stem of one of my own songs, an acapella, a beat or an ambient track. My last category of tag is \"Played At,\" which I only uses for places that I play kind of often: Panorama Bar and De School. Since people go there regularly, I don't want to repeat myself too much from set to set, and this allows me to see what I've played there in the past.\n\nAlright, then I better not hear 100 Hz's \"Whisper\" the next time you play Panorama Bar.\n\nYou love that song, and I only played it twice there. I import the history from my USB after I play. After a set, I highlight all the tracks in that playlist and attach the \"Played At Panorama Bar\" tag to it. Rekordbox writes a list of all the tags in the comments section of the file itself. That means that it'll show my \"Played At Panorama Bar\" tag no matter how I navigate to the track on the CDJ, so I can decide if I want to play it again.\n\nWhat goes into that decision? How do you determine when to retire a track or whether to repeat it?\n\nI'm struggling with that right now. Due to the size of my library I can only use 256-gig USBs, which is pretty big. Say I lose my USB and I have to go buy one at the airport or whatever; the 256-gig ones are often not available. That's why I'd like to keep my library under 128 gigs and why I should delete stuff that I don't use. But that would disrupt the archive I've made of all my gigs since 2015.\n\nAnd even if I don't play a certain song anymore, maybe there are songs that I played two years ago and haven't played since, so it's time to bring it back. This is one of my more complex playlists: \"Goodies from two years ago.\" The conditions are: the date the file was added to rekordbox is in the last 36 months but not in the last 24 months; the DJ play count is over two, meaning that I've played it a few times, but it's less than 12, so it's not one of my all-time classic bangers like one of my songs. I play those a lot because I feel like people come to hear me DJ because they like my productions. This Goodies playlist uses data I've been tracking for the last few years to pick songs that I like and that were legit parts of my sets from a specific time—two years ago—so that I can go back and find stuff I might have forgotten about. I have it for one year ago and then this year, too.\n\nMost of my intelligent playlists listen for combinations of all the tags that I ascribe to a single song, because one song usually has many tags attached to it. That makes importing new songs and categorizing them easy, because when I add a new song, the only thing I have to do is listen to it and semantically define it. I don't have to remember all the playlists that I have and then decide which one goes in where. The other benefit is that rekordbox writes the tags I ascribe to it in the comment field of the AIFF file. So even if I'm scrolling by a brand new song and I'm not super familiar with it, I can look and say, \"Well, the last time I heard it in rekordbox, I ascribed a 'breaks' tag, a 'deep' tag, an 'acid' tag and a 'techno' tag.\" That jogs my memory about what a song sounds like.\n\nThen I have special little playlists of songs that are good for ending a set and an intelligent playlist that searches for my name, so I can find all of my own productions, demos, remixes or Cybernedits.\n\nSince pretty much everything's tagged, I can basically create intelligent playlists on the fly by editing the track filter on the CDJs themselves rather than ahead of time in rekordbox. I've personally never seen someone use the tag filter screen before—it's hard to even find an image of it on Google, even though it's a really cool feature. I use it all the time during long sets. I can say I want something that is within six beats per minute of 130, I want things that are tagged \"techno\" and \"breaks\" and \"wave.\" And then boom: the CDJ filters all of my songs to these 18 tracks that satisfy those requirements.\n\nAnother really useful thing is to have auto-playlists for things that I added in the last week, two weeks, four weeks and eight weeks. If I'm in a rush then I can just dump things in there, and I don't have to add them to any playlists or tag them at all and I can still access them quickly.\n\nWhy did you decide to develop this extremely organized method?\n\nLike, why did I decide to get crazy about it? I don't know...\n\nYes you do.\n\nBecause it's \"the best way of organizing your shit\"? It was already there, and I just discovered it.\n\nI guess I'm trying to lead you to articulate one of my own assumptions or observations about you, like all good interviewers do.\n\nIs the observation that I shouldn't be a DJ?\n\nGirl… no. I think it's related to what you've brought up about not being a \"vibeman\" or, \"I can't believe I'm talking about this because it's so silly that it's my job to have developed this elaborate analytical method to organize it\"—\n\nDo you think it's that elaborate?\n\nIt's pretty elaborate. I mean, just describe how a track goes from being a Dropbox promo that someone sent you to being a part of your set.\n\nWell, first of all, thank you for sending me a Dropbox link, because I can just add it to my Dropbox remotely. I have a folder on my computer that's in my Dropbox folder called \"DJ Music Inbox,\" which is one place where I dump everything so that I can dedicate time to sit down and listen to and tag promos or rips or whatever. I don't put it on my desktop; I don't put it in my Downloads folder—don't do that. If someone sends me a bunch of music and I don't have time to go through it now but I will later—that's in my DJ Music Inbox on Dropbox.\n\nI don't use WAVs because they don't support metadata. Anyone reading this: please don't send WAVs. Send AIFFs because they're still lossless but you can have metadata on them. If it's a WAV, I use XLD to convert the file. It's a nice program because if you have a photo in the same directory, it will automatically attach it as album art. I try to attach artwork as much as possible, even if it's just a press photo, so that I can connect a color visual.\n\nAfter I drag something onto the XLD icon, it automatically moves it into the DJ Music Library folder, which is also on Dropbox. Anything that's in rekordbox is in here. I do that so that it's easy and straightforward to backup or restore a rekordbox library, because I have two computers that share the same rekordbox library. I look at rekordbox on my desktop studio computer and also on my laptop when I'm on the road, and I want to be able to have the same library on both computers. So if I'm done working on the studio computer, I back up the rekordbox library, export the library file—which isn't the music itself, but rather a 400-ish MG zip file—and restore it on my laptop from that data.\n\nOnce the actual AIFF file is in DJ Music Library, then I just highlight it and drag it into the Collection in Rekordbox. At that point I go through and tag each file or song. When I tag them, they automatically fly into the intelligent playlists that are looking for combinations of tags.\n\nSee? Simple as that [laughs]. Back to my previous question: Why did you decide to develop this specific, detailed and elaborate process for organizing your music and workflow?\n\nI'm obsessed with optimization and I think there can always be a better way to do something. Humans are really good at some things, and computers are really good at other things. I would like to use the computer for what it is good at, so that it frees up the human side of me. I'm kind of parroting a programmer mantra, which is that humans aren't good at remembering things but computers are. Humans are good at the vibeman stuff. I want to let the computer do as many rote memory tasks, like looping something when it gets to a certain part, as possible. Beat-matching is not one of the things that it's good at, by the way. There's all kinds of groove and swing and stuff, and unless you're playing techno and trance all the time, you have to hear what's going on in order to mix. I'm actually gonna come out and say that I have definitely used the sync function on CDJs—but it's for a really specific purpose, and that's the only way that it's useful.\n\nWhat's the specific circumstance?\n\nWhen I have two or three CDJs going at the same time and all three of the faders are up 100 percent and I want to drastically change the BPM. Say I have two songs with beats and an acapella, and I want to go from 140 BPM down to 116. I put two of them on sync and move one, and they all follow. People act like the Sync button makes the CDJ magically listen to the song and figure out how to beat-match for them. What it actually does is just change the beat of the slave track to the master track, one for one. So if I move the tempo fader on the master track, then anything that has Sync on it will change by the same number of BPM.\n\nThat's related to what you said at the start about vinyl; that there are certain conventions about what makes someone a \"good\" or \"authentic\" or \"serious\" DJ, and that you don't subscribe to them.\n\nThe Sync button is a good example. Once you realize what it actually is or does, what's revealed is that people who parrot \"rules\" about not using Sync are scared and have imposter syndrome themselves, and they're afraid of other people thinking that they're not \"real\" DJs. Maybe those people don't go from 116 to 140 BPM that often anyway.\n\nBefore we did this interview, you were sort of crusty about the idea that you might demystify your craft by showing how much of what you do are tasks that could be repeated by any human. What you've referred to as the \"vibeman\" approach is harder to quantify. You can't break it down in a didactic way. The nature of the vibeman is kind of mystical and you either have it or you don't.\n\nWell, hold on. Part of my anxiety about giving only really technical answers in this interview is that I don't want to seem like a… I feel like a salesman for Pioneer or a rekordbox instructor. That's why I would also like to talk about ripping and editing, because that's a huge part of what I do. Like, if it's some New Beat proto-techno thing from 1988, maybe the EQ of the song is kind of fucked up and it needs some more low end or something. I throw it through one of my remastering chains in Ableton.\n\nGive us a rundown of what's in the mastering chain.\n\nI have a decrackler at the beginning, an EQ to cut out the very lowest sub frequencies, then other more musical EQs to cut or boost others. Then I have various aural exciters and compressors. I have Ableton utilities along the chain to raise or lower the gain, and at the end I have a loudness maximizer. I do this with everything I rip. Some digital files need it too, like if someone sends me a premaster.\n\nTell me about the editing process.\n\nSometimes I tweak the arrangement a little, but I usually do that kind of thing in rekordbox using hot cues nowadays. It's faster and more flexible.\n\nWhy is that an important part of your process?\n\nSometimes I see a potential in a song and how it would live in my DJ sets, and I have to edit it a bit so that it gets there. I'm not really a tooly genre DJ. I play all kinds of things and sometimes I hear something like a weird pop song from the '80s, and I can edit it so that it can feel more right on the dance floor.\n\nWhat percentage of stuff you play—\n\n—is fucked with in any way? Like anything that I've even looped or hit a hot cue on? A lot. Pretty much all of them. Over 90 percent.\n\nCan I bring you back to big-picture stuff now? I think you put it very nicely when you were saying that you're trying to let the computer do what it is does best so that you can do what humans do best. Before, you were overstating how much of what you do is just using rekordbox and computers in a clever way. Do you think that even if someone knew all of Avalon's secrets and methods, they still wouldn't play like you do?\n\nOf course. I sometimes literally rip apart a song so that even if you have the record, what I play is actually not the thing that you have. Maybe originally there's a little bongo intro and then the kick comes in and then the synth and then the bassline, but sometimes that doesn't happen when I play that song. Breaking the linearity of a track is a pretty baseline, low-level deconstruction that I do on the reg. The goal is to be able to play these things like an instrument. That's what makes this fun for me.\n\nUsing CDJs in that way undermines 20th-century conventions that people made to justify the artistic merit of electronic music to those who thought it wasn't \"proper\" or \"real\" music, and that DJing was a cheap, easy hobby rather than an artform.\n\nSome people still say, \"You're just playing other people's music.\" They have a hard time understanding why I get paid for just playing other people's music. Most people just don't pay for music anymore, but they still want to have that participatory communal experience of going somewhere to hear music with others, just like they did hundreds of years ago. I pay the door charge to participate in music; I want to listen to it with my friends and party. I don't care where the music comes from, but I do care if it's more interesting and varied, and that preference of the crowds nowadays seems to favor DJs with 200 GBs of music with them, rather than live performers or producers who play all of their own music for an hour. I don't know—it's not fair, but what is in the current system?\n\nThat's why I think it's important for DJs to share and shout about the music that they're playing from people, especially when it comes from contemporary producers who could still benefit from a river of pennies flowing to them from selling AIFFs on the internet.\n\nHave you been thinking about that a lot recently?\n\nI guess so. I hope that's something that I can devote more time to in the future—something that isn't DJing. After I learn to do all the lifehack CDJ tricks.",
    "summary": {
      "en": "Avalon Emerson, a creative DJ, discusses her unique approach to DJing in an interview with Elissa Stolman. She distinguishes herself from other DJs, whom she refers to as \"vibemen,\" by emphasizing her technical skills and organizational methods. Emerson's workflow blends creativity with efficiency, enabling her to quickly find and mix songs across various genres while maintaining an engaging atmosphere for her audience.\n\nShe transitioned from vinyl to digital formats, favoring CDJs and digital files for their versatility and convenience. Despite not playing vinyl anymore, she still buys records for high-quality tracks and enjoys the process of ripping and editing them to fit her style. Emerson meticulously archives her sets to track her musical journey and avoid repeating herself at venues.\n\nHer DJ setup includes multiple CDJs, a high-quality mixer, USB drives, and pedals for effects. Emerson employs various techniques like looping, hot cues, and creative transitions to craft dynamic mixes that tell a story rather than following traditional long blends. She organizes her music in a detailed manner using software like Rekordbox, allowing her to easily categorize and access tracks based on tags and playlists.\n\nEmerson believes that technical skills enhance artistic expression rather than diminish it. She emphasizes the importance of using technology to streamline her workflow, allowing her to focus on the emotional experience of her audience. Ultimately, she conveys that even if someone learns her techniques, they won't replicate her unique style, which combines her personal touch with her technical expertise.",
      "ko": "아발론 에머슨은 엘리사 스톨먼과의 인터뷰에서 그녀만의 독특한 DJ 스타일에 대해 이야기했습니다. 그녀는 자신을 \"바이브맨\"이라고 부르는 다른 DJ들과 차별화하기 위해 기술적인 능력과 조직적인 방법을 강조합니다. 에머슨의 작업 방식은 창의성과 효율성을 결합하여 다양한 장르의 곡을 빠르게 찾아 믹스하면서도 관객에게 매력적인 분위기를 유지할 수 있게 합니다.\n\n그녀는 바이닐에서 디지털 포맷으로 전환했으며, CDJ와 디지털 파일의 다재다능함과 편리함을 선호합니다. 비록 더 이상 바이닐을 연주하지 않지만, 고품질 트랙을 위해 여전히 레코드를 구매하고, 이를 자신의 스타일에 맞게 편집하는 과정을 즐깁니다. 에머슨은 자신의 세트를 세심하게 아카이브하여 음악 여정을 기록하고, 공연장에서 같은 곡을 반복하지 않도록 합니다.\n\n그녀의 DJ 장비에는 여러 개의 CDJ, 고품질 믹서, USB 드라이브, 효과를 위한 페달이 포함되어 있습니다. 에머슨은 루핑, 핫 큐, 창의적인 전환과 같은 다양한 기술을 사용하여 전통적인 긴 믹스 대신 이야기를 전달하는 역동적인 믹스를 만듭니다. 그녀는 레코드박스와 같은 소프트웨어를 사용하여 음악을 세부적으로 정리하며, 태그와 플레이리스트에 따라 곡을 쉽게 분류하고 접근할 수 있도록 합니다.\n\n에머슨은 기술적인 능력이 예술적 표현을 저해하는 것이 아니라 오히려 향상시킨다고 믿습니다. 그녀는 기술을 활용하여 작업 흐름을 간소화하는 것이 중요하다고 강조하며, 이를 통해 관객의 감정적 경험에 집중할 수 있게 됩니다. 결국, 그녀는 누군가 자신의 기술을 배운다고 해도, 개인적인 터치와 기술적 전문성이 결합된 그녀만의 독특한 스타일을 재현할 수는 없다고 전합니다.",
      "ja": null
    }
  },
  {
    "id": "bd620a437d6eb2ae",
    "title": {
      "en": "Superhyperbola",
      "ko": "슈퍼하이퍼볼라",
      "ja": null
    },
    "type": "story",
    "url": "https://www.johndcook.com/blog/2025/03/27/superhyperbola/",
    "score": 28,
    "by": "jihadjihad",
    "time": 1743186321,
    "content": "Superhyperbola\n\n\t\t\tPosted on 27 March 2025 by John\n\n\t\tAn ellipse has equation\n\nand a hyperbola has equation\n\nSimilarly the superellipse has equation\n\nand the superhyperbola\n\nWhen p = 2, the absolute value signs are unnecessary and the superellipse and superhyperbola reduce to the ellipse and hyperbola respectively.\nIncreasingp makes the superellipse more like a rectangle. But unlike a rectangle with rounded corners, the change in curvature is continuous.\n\nIncreasingp makes the superhyperbola more blunt at the vertices.\n\nMarketing\nThe superellipse is a fairly well known variation on an ellipse. Even if you’re not familiar the term, you’ve probably seen the shape. I give a couple examples here. The superhyperbola is the obvious analog of a superellipse, but the term is far less common. I’d never hear the term until yesterday.\nIt’s not clear why the superellipse would be common and the superhyperbola obscure, but here’s some speculation. First of all, the superellipse had an advocate, Piet Hein. If the superhyperbola has an advocate, he’s not a very effective advocate.\nThe name is also off-putting: juxtaposing super andhyper sounds silly. The etymology makes sense, even if it sounds funny. Piet Hein used the prefixsuper– to refer to increasing the exponent from the usual value of 2. Its unfortunate thathyperbola begins with a root that is similar tosuper.\nRelated posts\n\nApple design, squircles, and curvature\nSquircle corner radius\nSupereggs\n\n\t\t\t\tCategories : MathBookmark the permalink",
    "summary": {
      "en": "**Summary of Superhyperbola Post**\n\nThe post discusses mathematical shapes known as superellipses and superhyperbolas. \n\n- **Ellipses and Hyperbolas**: An ellipse and a hyperbola have specific mathematical equations. Superellipses and superhyperbolas are variations that change based on a parameter, \\( p \\).\n- **Shape Changes**: As \\( p \\) increases, the superellipse starts to resemble a rectangle, while the superhyperbola becomes blunter at its points.\n- **Recognition**: The superellipse is well-known, partly due to the advocacy of Piet Hein. In contrast, the superhyperbola is less recognized, and the term may sound off-putting to some.\n- **Terminology**: The prefix \"super-\" indicates an increase in the exponent from 2, which is the standard for ellipses and hyperbolas.\n\nOverall, the post highlights the differences and lesser-known status of the superhyperbola compared to the superellipse.",
      "ko": "이 글에서는 수학적 도형인 슈퍼타원과 슈퍼쌍곡선에 대해 설명합니다. 타원과 쌍곡선은 각각 특정한 수학적 방정식을 가지고 있습니다. 슈퍼타원과 슈퍼쌍곡선은 매개변수 \\( p \\)에 따라 변형된 형태입니다. \n\n\\( p \\)의 값이 증가함에 따라 슈퍼타원은 직사각형에 가까워지고, 슈퍼쌍곡선은 그 꼭짓점이 더 둥글어지는 특징이 있습니다. 슈퍼타원은 피에트 하인(Piet Hein)의 지지 덕분에 잘 알려져 있지만, 슈퍼쌍곡선은 상대적으로 덜 알려져 있으며, 이 용어가 일부 사람들에게는 생소하게 들릴 수 있습니다. \n\n\"슈퍼-\"라는 접두사는 타원과 쌍곡선의 표준 지수인 2에서 증가했음을 나타냅니다. 전반적으로 이 글은 슈퍼타원이 더 잘 알려져 있는 반면, 슈퍼쌍곡선은 상대적으로 덜 알려져 있다는 점을 강조하고 있습니다.",
      "ja": null
    }
  },
  {
    "id": "7edca9dd2a61648d",
    "title": {
      "en": "How to Write Blog Posts that Developers Read",
      "ko": "개발자가 읽는 블로그 쓰기",
      "ja": null
    },
    "type": "story",
    "url": "https://refactoringenglish.com/chapters/write-blog-posts-developers-read/",
    "score": 245,
    "by": "rbanffy",
    "time": 1743159679,
    "content": "How to Write Blog Posts that Developers Readby Michael Lynch, published\nMarch 27, 2025if(window.location.pathname===\"/chapters/nine-years-of-blogging/\"){const e=\"What I Learned from Nine Years of Blogging\";document.title=e,document.querySelector(\"h1\").textContent=e}I recently spoke to a developer who tried blogging but gave up because nobody was reading his posts. I checked out his blog, and it was immediately obvious why he didn’t have any readers.The developer had interesting insights, but he made so many mistakes in presenting his ideas that he was driving everyone away. The tragedy was that these errors were easy to fix. Once you learn to recognize them, they feel obvious, but some bloggers make these mistakes for years.I know because I’m one of them.I’ve been blogging about software development for nine years. My best posts have reached 300k+ readers, but many of them flopped, especially in my first few years.Over time, I’ve learned techniques that help some blog posts succeed and the pitfalls that cause others to languish in obscurity.Why listen to me?Get to the pointThink one degree biggerPlan the route to your readersShow more picturesAccommodate skimmersWhy listen to me?🔗I’m going to say a bunch of gloaty things to establish credibility, but it feels gross, so let’s just get it out of the way:I’ve written a software blog for nine years, and it attracts 300k-500k unique readers per year.My posts have reached the front page of Hacker News over 30 times, many of them reaching the #1 spot.According to a ranking system I made up, I have the 48th most popular personal blog on Hacker News.I launched a successful indie business by writing a popular blog post about my product.My articles frequently appear on reddit and Lobsters.My software blog receives 300k-500k unique readers per year.I don’t claim to be the world’s best software blogger, but I’ve had enough success and experience to share some useful lessons.Get to the point🔗The biggest mistake software bloggers make is meandering.Often, the author has some valuable insight to share, but they squander their first seven paragraphs on the history of functional programming and a trip they took to Bell Labs in 1973. By the time they get to the part that’s actually interesting, everyone has long since closed the browser tab.Internet attention spans are short. If you dawdle before making your point, the reader will seek out one of the literally billions of other articles they could be reading instead.So, how do you convince the reader to stay and continue reading your blog post?When the reader arrives, they’re trying to answer two questions as quickly as possible:Did the author write this article for someone like me?How will I benefit from reading it?Give yourself the title plus your first three sentences to answer both questions. If you find yourself in paragraph two and you haven’t answered either question, you’re in trouble.To show the reader you’re writing for them, mention topics they care about, and use terminology they recognize. If you throw out jargon or unfamiliar concepts, the reader assumes the article isn’t meant for them and clicks away.Your introduction should also make it clear to the reader how the article will benefit them. There are many possible benefits you can offer:A technique the reader can apply in their work or personal life.A clear explanation of a concept that impacts the reader’s work or personal life.An insight that gives the reader a better understanding of a particular technology or industry.An interesting story that resonates with the reader.Example: “if got, want: A Simple Way to Write Better Go Tests”🔗I recently wrote an article about improving tests when using the Go programming language.Here’s the title and first paragraph:if got, want: A Simple Way to Write Better Go TestsThere’s an excellent Go testing pattern that too few people know. I can teach it to you in 30 seconds.This article immediately answers the two questions:Did the author write the article for someone like me?The article is for Go developers.What’s the benefit of reading it?You’ll learn a new testing technique in 30 seconds.Think one degree bigger🔗When you write an article, you hopefully have a type of reader in mind. For example, if you wrote an article called “Debugging Memory Leaks in Java,” you probably assumed that the reader is an intermediate to advanced Java developer.Most software bloggers never think to ask, “Is there a wider audience for this topic?”For example, “intermediate to advanced Java developers” are a subset of “Java developers,” who are a subset of “programmers,” who are a subset of “people who read blog posts.”If you wrote an article for intermediate and advanced Java developers, how much would have to change for the article to appeal to Java developers of any experience level?Often, the change is just an extra sentence or two early in the article to introduce a concept or replace jargon with more accessible terms.Jeff: Sony has a futuristic sci-fi movie they’re looking to make.Nick: Cigarettes in space?Jeff: It’s the final frontier, Nick.Nick: But wouldn’t they blow up in an all-oxygen environment?Jeff: Probably. But it’s an easy fix. One line of dialogue. “Thank God we invented the… you know, whatever device.”Thank You for Smoking (2005)The set of all Java developers is about 10x larger than the set of intermediate and advanced Java developers. That means small tweaks can expand the reach of your article by an order of magnitude.Obviously, you can’t broaden every article, and you can’t keep broadening your audience forever. No matter how well you explain background concepts, your tax accountant will never read an article about memory leaks in Java. The point isn’t to write articles that appeal to every possible reader but to notice opportunities to reach a larger audience.Example: “How I Stole Your Siacoin”🔗One of my earliest successes in blogging was an article called “How I Stole Your Siacoin.” It was about a time I stole a reddit user’s cryptocurrency (for noble reasons, I promise).Initially, I thought the story would resonate with the few hundred people who followed a niche cryptocurrency called Siacoin. As I was editing the article, I realized that you didn’t have to know anything about Siacoin to understand my story. I revised it slightly so it would make sense to cryptocurrency enthusiasts who had never heard of Siacoin.Then, I realized I could even explain this story to people who knew nothing about cryptocurrency. I adjusted the terminology to use regular-person terms like “wallet” and “passphrase” and avoided crypto-specific terms like “blockchain” or “Merkle tree.”The article was my first ever hit. It became the most popular story of all time not only on the /r/siacoin subreddit but also on the larger /r/cryptocurrency subreddit. It reached the front page of Hacker News, even though readers there are generally hostile to cryptocurrency-focused stories.“How I Stole Your Siacoin” only needed a few tweaks to be enjoyable for people who didn’t know anything about cryptocurrency.Plan the route to your readers🔗Suppose you wrote the greatest beginner’s tutorial imaginable for the Python programming language. Both your five-year-old nephew and 80-year-old dentist blazed through it with ease and delight. Everyone who reads your tutorial goes on to become a Python core contributor.Bad news: nobody will ever read your Python tutorial.“Lies!” you shout. “Thousands of developers learn Python every year. Why wouldn’t my objectively awesome tutorial become popular?”Well, think it through. What happens after you hit publish? How does anyone find your article?You’re probably thinking: Google.Yes, your friend Google will index your tutorial and use its secret Google magic to identify your article’s superior quality. Before you know it, your tutorial will be the top result for python tutorial.Except that can’t happen because there are so many Python tutorials out there already on sites that Google prefers over yours. You’ll never even make it to the first page of results.It’s nearly impossible for a new blog post to rank well in Google for the search term python tutorial.Okay, so you’ll submit your Python tutorial to reddit. The /r/python subreddit has over 1.3 million subscribers. If even 5% of them read your article, that’s a huge audience:The /r/python subreddit has over 1.3 million subscribers.Whoops! /r/python only accepts text posts, not external links, so you can’t post your tutorial there.The /r/python subreddit disables the option to submit external links.Fine, then you’ll submit it to Hacker News. They accept anything and let their members decide what’s interesting. Surely, they’ll recognize the quality of your work!Nope, it will flop there, too. Hacker News doesn’t like tutorials, especially for mainstream technologies like Python.You can try sharing your tutorial by tweeting it, skeeting it, or tooting it, but unless you already have a massive following on social media, that won’t reach a critical mass either.So, what’s the answer? How do you get people to read your amazing Python tutorial?The answer is that you don’t write a beginner’s Python tutorial.You need a realistic path to your readers🔗If you want people to read your blog, choose topics that have a clear path to your readers. Before you begin writing, think through how readers will find your post.Questions to ask when considering an article topicIs it realistic for readers to find you via Google search?Are there already 500 articles about the same topic from more established websites?What keywords would your target reader search? Try searching those keywords, and see whether there are already relevant results from well-known domains.If you’re going to submit it to a link aggregator like Hacker News or Lobsters, how often do posts like yours succeed there?If you’re going to share it on a subreddit or niche forum, does it have any chance there?Does the forum accept links to blog posts?The bigger the community, the stricter the rules tend to be about external links and self-promotion.Do blog posts like yours ever succeed there?Is the community still active?The best plan is to give your post multiple chances to succeed. If you’re betting everything on Google bubbling your post to the top, it could take months or years for you to find out if you succeeded. If you’re relying on Hacker News or reddit to tell you whether your article is worth reading, they’re going to break your heart a lot.Example: “Using Zig to Unit Test a C Application”🔗In 2023, I wrote an article called “Using Zig to Unit Test a C Application.” It was about using a new low-level language called Zig to write tests for legacy C code.Before I wrote the article, I knew that there were several places where I could share it. By luck, they all worked out:Hacker News is extremely friendly to Zig content, so my article reached the #7 spot on the front page.Lobsters is extremely friendly to Zig content, so my article was one of the top links of the day.Google bubbled my article to the top result for the keywords zig unit testing c.It’s actually even a top result for just zig unit testing because there aren’t many articles about the topic.The /r/Zig subreddit accepts links to blog posts, even if they’re self-promotion, so my post reached the top spot in that subreddit.Ziggit is a niche forum that’s welcoming to Zig-related articles, so my post received 1,000 views from Ziggit.Show more pictures🔗The biggest bang-for-your-buck change you can make to a blog post is adding pictures.If your article features long stretches of text, think about whether there’s any photo, screenshot, graph, or diagram that could make the post more visually interesting.If you’re talking about a program with a graphical interface, show screenshots.If you’re talking about an improvement in metrics like app performance or active users, show graphs.If you’re writing about your server getting overloaded, show a screenshot of what that looked like in your dashboard or email alerts.If you’re explaining a difficult concept, draw a diagram.I hire illustrators for most of my posts (including this one). I typically pay $50-100 per illustration. For simple diagrams like the nested circle sketches above, I use Excalidraw, which is free and open-source.You can also use free stock photos and AI-generated images, as they’re better than nothing, but they’re worse than anything else, including terrible MS Paint drawings.Even a terrible MS Paint drawing is more interesting than an AI-generated image.Accommodate skimmers🔗Many readers skim an article first to decide if it’s worth reading. Dazzle those readers during the skim.If the reader only saw your headings and images, would it pique their interest?The worst thing for a skimmer to see is a wall of text: long paragraphs with no images or headings to break them up. Just text, text, text all the way down.Tool: Read like a skimmer🔗Here’s a JavaScript bookmarklet that you can use to see what your article looks like with just headings and images.Skimmify pageDrag the link to your browser bookmark bar, and then click it to see what your article looks like to skimmers.Example: Boring structure vs. interesting structure🔗I wrote my article, “End-to-End Testing Web Apps: The Painless Way,” in 2019, before I thought about structure.If you skim the article, does it make you want to read the full version?\nYour browser does not support the video tag.Probably not. The headings don’t reveal much about the content, and the visuals are confusing.Consider my more recent article, “I Regret My $46k Website Redesign.”\nYour browser does not support the video tag.If you skim that article, you still see the bones of a good story, and there are interesting visual elements to draw the reader in.One of those articles barely attracted any readers, and the other became one of the most popular articles I ever published, attracting 150k unique readers in its first week. Can you guess which is which?.campaign-progress{background-color:#fdfffa;padding:1rem;border-radius:6px;text-align:center;border:1px solid #dae8c6;width:90%;max-width:500px;margin-left:auto;margin-right:auto}.goal-amount{font-weight:500;margin-bottom:1rem}.countdown{color:#555;font-size:1.1em}.progress{height:40px}.progress-bar{font-size:1.1em;line-height:40px}Pre-order the bookThis is an excerpt from my upcoming book,\nRefactoring English: Effective Writing for Software Developers.$4,776 raised of $5,000\ngoal96%Time left to meet goal: 3 days, 0 hours, 2 minutesWant to fund this so I can write the full book? Pre-order the book on\nKickstarter to support the book.Pre-Order Nowconst currentAmount=4776,goalAmount=5e3;function updateFundingInfo(){document.getElementById(\"current-amount\").textContent=currentAmount.toLocaleString(),document.getElementById(\"goal-amount\").textContent=goalAmount.toLocaleString();const t=Math.round(currentAmount/goalAmount*100),e=document.getElementById(\"progress-bar\");e.style.width=t+\"%\",e.textContent=t+\"%\",e.setAttribute(\"aria-valuenow\",currentAmount)}function updateCountdown(){const t=new Date(\"2025-03-31T23:59:00-04:00\"),n=new Date,e=t-n;if(e<=0){document.getElementById(\"countdown\").textContent=\"Campaign ended\";return}const s=Math.floor(e/(1e3*60*60*24)),o=Math.floor(e%(1e3*60*60*24)/(1e3*60*60)),i=Math.floor(e%(1e3*60*60)/(1e3*60));document.getElementById(\"countdown\").textContent=`${s} days, ${o} hours, ${i} minutes`}updateFundingInfo(),updateCountdown()In the nine years I've been blogging about software development, some of my posts have hit 300k+ readers, while others flopped, especially early on. I'm sharing all the lessons I learned the hard way about how to write popular blog posts for developers. https://t.co/a5cLF4MXfF— Michael Lynch (@deliberatecoder) March 27, 2025“Not Quite How Developers Read” illustration by Piotr Letachowicz. Steve Jobs illustration by Loraine Yow.",
    "summary": {
      "en": "Michael Lynch shares tips for writing blog posts that attract developers based on his nine years of blogging experience. He emphasizes that many bloggers make common mistakes that turn readers away, but these errors are often easy to fix. Here are the key points:\n\n1. **Get to the Point**: Capture the reader's interest quickly. Answer the questions \"Is this for me?\" and \"What's the benefit?\" within the first few sentences.\n\n2. **Think Bigger**: Consider whether your topic can appeal to a wider audience. Small adjustments can make your article relevant to more readers.\n\n3. **Plan for Visibility**: Before writing, think about how readers will find your post. Research existing content, keywords, and appropriate platforms for sharing.\n\n4. **Use Visuals**: Incorporate images, diagrams, or screenshots to make your post more engaging and visually appealing.\n\n5. **Accommodate Skimmers**: Many readers skim articles first. Make sure your headings and images draw attention, and avoid long blocks of text.\n\nBy applying these techniques, bloggers can improve the chances of their posts being read and appreciated by developers.",
      "ko": "마이클 린치는 개발자들을 끌어들이는 블로그 글 작성 팁을 아홉 년의 블로깅 경험을 바탕으로 공유합니다. 그는 많은 블로거들이 독자를 멀어지게 하는 일반적인 실수를 저지르지만, 이러한 오류는 대개 쉽게 수정할 수 있다고 강조합니다. 주요 포인트는 다음과 같습니다.\n\n첫째, 독자의 관심을 빠르게 끌어야 합니다. 글의 첫 몇 문장 안에 \"이 글이 나에게 필요한가?\"와 \"무슨 이점이 있는가?\"라는 질문에 답해야 합니다.\n\n둘째, 더 넓은 시각을 가져야 합니다. 자신의 주제가 더 많은 독자에게 어필할 수 있는지 고려해 보세요. 작은 조정만으로도 글이 더 많은 사람들에게 관련성을 가질 수 있습니다.\n\n셋째, 가시성을 계획해야 합니다. 글을 쓰기 전에 독자들이 어떻게 자신의 포스트를 찾을지를 생각해 보세요. 기존 콘텐츠, 키워드, 공유할 적절한 플랫폼에 대해 조사하는 것이 중요합니다.\n\n넷째, 시각 자료를 활용하세요. 이미지, 도표, 스크린샷 등을 포함시켜 글을 더 매력적이고 시각적으로 흥미롭게 만드세요.\n\n마지막으로, 스키머를 고려해야 합니다. 많은 독자들이 먼저 기사를 훑어봅니다. 제목과 이미지를 눈에 띄게 하고, 긴 문단은 피하는 것이 좋습니다.\n\n이러한 기법을 적용하면 블로거들은 자신의 글이 개발자들에게 읽히고 인정받을 가능성을 높일 수 있습니다.",
      "ja": null
    }
  },
  {
    "id": "bd81f570e9c669f3",
    "title": {
      "en": "Swiftly 1.0",
      "ko": "스위프트 1.0",
      "ja": null
    },
    "type": "story",
    "url": "https://swift.org/blog/introducing-swiftly_10/",
    "score": 34,
    "by": "todsacerdoti",
    "time": 1743193761,
    "content": "Introducing swiftly 1.0\n\n    March 28, 2025\n\n              Chris McGee\n\n  Today we’re delighted to introduce the first stable release of swiftly, a Swift version manager that takes the pain out of installing, managing and updating your Swift toolchain.\n\nThe latest version of Swift is bundled with Xcode for writing apps for Apple platforms. But perhaps you want to install Swift on a different platform like Linux, or use a different version of the toolchain for building services or command line tools. Downloading, extracting and installing a trusted build of Swift along with the relevant dependencies for your operating system can require quite a few manual and error-prone steps.\n\nswiftly has been around for some years as a community-supported tool for Swift developers using Linux. With this release, we’re officially supporting it as part of the core Swift toolchain, including hosting it as part of the Swift GitHub organization. We’ve also added macOS support to make it easier to install Swift separately from Xcode.\n\nIntroducing swiftly\n\nswiftly is the best tool to install the standalone toolchain, providing commands to install Swift on a new system, update to the latest stable version, and experiment or test with nightly snapshots or older versions. It also makes it easy to switch effortlessly between multiple installed toolchains. You can even add a file to your project repository so swiftly will use the same toolchain version for all members of your development team.\n\nNaturally, swiftly itself is written in Swift, and is able to update itself to the latest version.\n\nQuick tour\n\nLet’s take a look at some of the features of swiftly!\n\nTo get started, visit swift.org/install and install it.\n\nswiftly will provide directions after installation if there are any system packages, or shell commands needed for smooth operation of the new toolchain.\n\nThe latest Swift toolchain is installed as the default, so you can immediately use it to start a new project. For example:\n\n$ swift package init\n\nThe swiftly use command selects the default toolchain for Swift commands (e.g. swift test, swift build):\n\n$ swiftly use 6.0.3\n$ swift --version\n--\nApple Swift version 6.0.3 (swiftlang-6.0.3.1.2 clang-1600.0.28.6)\nTarget: arm64-apple-macosx15.0\n\nAt a later point, if there’s a new release of Swift you can install it alongside the existing toolchain with the latest command:\n\n$ swiftly install latest\n\nPre-release of versions of Swift are available, including nightly “snapshot” toolchains. They can be easily listed using swiftly:\n\n$ swiftly list-available main-snapshot\n--\nAvailable main development snapshot toolchains\n----------------------------------------------\nmain-snapshot-2025-03-25\n...\n\nOnce you’ve identified a snapshot toolchain, it can be installed using its name:\n\n$ swiftly install main-snapshot-2025-03-25\n--\nInstalling main-snapshot-2025-03-25\n\nAnother way to temporarily use a specific version of Swift is to use the special ‘+’ selector. With this syntax, you don’t need to first switch to a different toolchain:\n\n$ swiftly run lldb +main-snapshot-2025-03-25\n--\n(lldb) _\n\nIf you’re building a SwiftPM project in a team setting and want to enforce a common version of the Swift toolchain on all contributors, simply create a .swift-version file in the root of your project folder with the desired version (e.g. “6.0.3”).\n\nAs swiftly is updated with new features and bug fixes, you can run swiftly self-update to check and install new releases.\n\nHow swiftly works\n\nBy writing swiftly in Swift, we’re able to take advantage of the language’s features, support, and ecosystem of related projects. Swift comes with standard library features for working with the filesystem in its Foundation module. For network operations Async HTTP Client is there to work the HTTP requests. And to retrieve the latest Swift release, swiftly uses the Swift OpenAPI plugin to generate the code to interact with the swift.org  website. Lastly, it takes advantage of Swift’s interoperability with C to use the existing libarchive library to work with archives. swiftly uses libarchive to extract the toolchains downloaded from the Swift website and the integration is simple.\n\nIt can be challenging to build shell programs that work well across multiple platforms with minimal system dependencies; this motivated us to switch swiftly away from using a shell program to install it and become a self-installing binary application. swiftly has access to excellent argument parsing capabilities, beautiful --help screens, and the full standard library.\n\nThe only remaining problem was being able to deliver the operating system and processor architecture specific binary to the users system with simplicity. The swift.org website helps with operating system detection, but it cannot reliably detect the Linux distribution. Luckily, there is the Swift Static Linux SDK that makes binaries that work with a wide range of distributions. The processor architecture can be determined on most unixes using uname -m . The result of all of this is the simplicity of a copy and paste from the website to your terminal and get started with Swift.\n\nInstalling Swift, swiftly\n\nMoving forward, swiftly will become the default way to install Swift outside of Xcode. The initial version supports macOS and a variety of Linux distributions, including Ubuntu, Debian, Fedora, Red Hat Enterprise Linux and Amazon Linux.\n\nThe swiftly documentation provides further information about using swiftly in a CI/CD environment, as well as setting proxy servers and custom install locations for enterprise environments. swiftly is an open source project, and so you can raise new issues or contribute pull requests at its GitHub repository. You can also ask questions or discuss swiftly on the Swift Forums.\n\nSpecial thanks to Patrick Freed for creating swiftly, contributing it to the Swift organization, and his continued contributions to this valuable tool. The community is what makes Swift amazing!\n\n      Written by\n\n              Chris McGee\n\n            Chris McGee is on the team at Apple working on Swift Package Manager, and Swiftly.\n\n      How Swift's server support powers Things Cloud",
    "summary": {
      "en": "**Summary of Swiftly 1.0 Release**\n\nOn March 28, 2025, Chris McGee announced the first stable release of Swiftly, a Swift version manager designed to simplify the installation, management, and updating of Swift toolchains. \n\n**Key Features:**\n- **Cross-Platform Support:** Swiftly allows users to install Swift on various platforms, including Linux and macOS, separately from Xcode.\n- **Easy Toolchain Management:** Users can install a new Swift version, update to the latest stable version, and switch between different toolchains easily. It also supports nightly snapshots and older versions.\n- **Team Collaboration:** Developers can create a `.swift-version` file in their project to ensure all team members use the same toolchain version.\n- **Self-Updating:** Swiftly can update itself to the latest version.\n\n**Installation and Usage:**\n- To get started, users can visit swift.org/install. Swiftly will guide them through any necessary package installations.\n- The toolchain can be set as the default for immediate project use, and users can easily install new or specific versions with simple commands.\n\n**Technical Aspects:**\n- Swiftly is built in Swift and utilizes various Swift features for filesystem management and HTTP requests. It simplifies installation by being a self-installing binary, making it easier to use across different platforms.\n\n**Future Directions:**\n- Swiftly is intended to be the primary method for installing Swift outside of Xcode, supporting various Linux distributions and macOS. More documentation is available for advanced use cases in CI/CD environments.\n\n**Community Contribution:**\n- The project is open-source, encouraging contributions and discussions within the Swift community. Special acknowledgment goes to Patrick Freed for his contributions to Swiftly. \n\nFor more information, users can explore the Swiftly documentation and engage with the community on Swift Forums.",
      "ko": "2025년 3월 28일, 크리스 맥기(Chris McGee)는 Swift 툴체인 설치, 관리 및 업데이트를 간소화하기 위해 설계된 Swift 버전 관리 도구인 Swiftly의 첫 번째 안정 버전을 발표했습니다.\n\nSwiftly의 주요 기능으로는 다양한 플랫폼에서 Swift를 설치할 수 있는 크로스 플랫폼 지원이 있습니다. 사용자는 Xcode와는 별개로 리눅스와 macOS에서 Swift를 설치할 수 있습니다. 또한, 사용자는 새로운 Swift 버전을 설치하고 최신 안정 버전으로 업데이트하며, 다양한 툴체인 간에 쉽게 전환할 수 있습니다. 야간 스냅샷과 이전 버전도 지원합니다. 개발자들은 프로젝트에 `.swift-version` 파일을 생성하여 팀원 모두가 동일한 툴체인 버전을 사용하도록 할 수 있습니다. Swiftly는 스스로 최신 버전으로 업데이트할 수 있는 기능도 갖추고 있습니다.\n\n사용자는 swift.org/install을 방문하여 Swiftly를 시작할 수 있습니다. Swiftly는 필요한 패키지 설치를 안내합니다. 툴체인은 즉시 프로젝트에서 사용할 수 있도록 기본값으로 설정할 수 있으며, 사용자는 간단한 명령어로 새로운 버전이나 특정 버전을 쉽게 설치할 수 있습니다.\n\nSwiftly는 Swift로 구축되었으며, 파일 시스템 관리와 HTTP 요청을 위한 다양한 Swift 기능을 활용합니다. 자가 설치형 바이너리로 설계되어 있어 여러 플랫폼에서 사용하기 쉽습니다.\n\nSwiftly는 Xcode 외부에서 Swift를 설치하는 주요 방법으로 사용될 예정이며, 다양한 리눅스 배포판과 macOS를 지원합니다. CI/CD 환경에서의 고급 사용 사례에 대한 문서도 추가로 제공됩니다.\n\n이 프로젝트는 오픈 소스이며, Swift 커뮤니티 내에서 기여와 논의를 장려합니다. 특히 패트릭 프리드(Patrick Freed)의 기여에 감사의 뜻을 전합니다. 더 많은 정보는 Swiftly 문서를 통해 확인할 수 있으며, Swift 포럼에서 커뮤니티와 소통할 수 있습니다.",
      "ja": null
    }
  },
  {
    "id": "81b904731d3c64d7",
    "title": {
      "en": "The Real Book (2021)",
      "ko": "진짜 책 (2021)",
      "ja": null
    },
    "type": "story",
    "url": "https://99percentinvisible.org/episode/the-real-book/",
    "score": 61,
    "by": "Tomte",
    "time": 1743179950,
    "content": "Episode 438\nThe Real Book\n\n    PlayPause\n  Click to enlarge image\n\n                    History\n\n      Category\n      History\n\n      Date\n      04.06.21\n\n    Producer\n    99pi\n\n            Add to QueueRemove from QueueDownloadTranscript\n\n                      Share on Facebook\n\n                      Share on Twitter\n\n                      Leave a Comment\n\n                Since the mid-1970s, almost every jazz musician has owned a copy of the same book. It has a peach-colored cover, a chunky, 1970s-style logo, and a black plastic binding. It’s delightfully homemade-looking—like it was printed by a bunch of teenagers at a Kinkos. And inside is the sheet music for hundreds of common jazz tunes—also known as jazz “standards”—all meticulously notated by hand. It’s called the Real Book.\n\nBut if you were going to music school in the 1970s, you couldn’t just buy a copy of the Real Book at the campus bookstore. Because the Real Book… was illegal. The world’s most popular collection of jazz music was a totally unlicensed publication. It was a self-published book created without permission from music publishers or songwriters. It was duplicated at photocopy shops and sold on street corners, out of the trunks of cars, and under the table at music stores where people used secret code words to make the exchange. The full story of how the Real Book came to be this bootleg bible of jazz is a complicated one. It’s a story about what happens when an insurgent, improvisational art form like jazz gets codified and becomes something that you can learn from a book.\nThe History of Fake Books\nBarry Kernfeld\n is a musicologist who has written a lot about the history of jazz and music piracy. Kernfeld says that long before the Real Book ever came out, jazz musicians were relying on collections of music they called fake books. Kernfeld says that the story of the first fake book began in the 1940s. “A man named George Goodwin in New York City, involved in radio in the early 1940s, was getting a little frustrated with all the intricacies of tracking licensing. And so he invented this thing that he called the Tune-Dex,” explains Kernfeld.\nTuneDex card via Georgia State University Library\nThe Tune-Dex was an index card catalog designed for radio station employees to keep track of the songs they were playing on air. On one side the cards had information about a particular song, such as the composer, the publisher, and anything that one would need to know for payment rights. On the other side of the card were a few lines of bite-sized sheet music—just the song’s melody, lyrics, and chords so that radio station employees could glance at it and quickly recall the song. This abbreviated musical notation also made the cards useful to another group of people: working jazz musicians.\n\nAs a Black art form, jazz had developed out of a mix of other Black music traditions including spirituals and the blues. By the 1940s, a lot of “jazz” was popular dance music, and many jazz musicians were making their money playing live gigs in small clubs and bars. The standard jazz repertoire was mostly well-known pop songs from Broadway, or New York’s songwriting factory: “Tin Pan Alley.”\n\nJazz musicians would riff and freestyle over these songs. The art of improvisation has always been a key art form of jazz music. But what made the average gigging trumpeter or sax player truly valuable was their ability to play any one of hundreds of songs right there on the spot.\nTo be prepared for any request, musicians would bring stacks and stacks of sheet music to every gig. But lugging around a giant pile of paper could be really cumbersome—this is where the Tune-Dex came in. Someone figured out that you could gather together a bunch of Tune-Dex cards, print copies of them on sheets of paper, add a table of contents and a simple binding, and then sell the finished product directly to musicians in the form of a book. They called them “fake books” because they helped musicians fake their way through unfamiliar songs. These first fake books were cheaper than regular sheet music, and a lot more organized. They became an essential tool for this entire class of working musicians.\n\nBootleggers\n\nMusicians loved these new fake books, but the music publishers hated them. They wanted musicians to buy legal sheet music, and so the publishing companies started cracking down on fake book bootleggers. That, of course, didn’t stop the bootleggers and by the 1950s, there were countless illegal fake books in existence, which were being used in nightclubs all across the country.\n\nAs helpful as fake books were, they had a lot of problems. They were notoriously illegible and confusingly laid out. The other big problem with these fake books at this point was that the music inside felt really out of date. The fake books hadn’t changed since the mid-40s, but jazz had. Disillusioned by commercial jazz that appealed to mainstream white audiences, a new generation of Black musicians took jazz improvisation to a new level. They experimented with more angular harmonies, technically demanding melodies and blindingly fast tempos. Their new style was called bebop.\n\nBebop was just the beginning. Over several decades, jazz exploded into this constellation of different styles. Meanwhile, the economics of jazz shifted too. There were fewer clubs, smaller paychecks, and more university jazz programs with steady teaching gigs. The ivory tower, not the nightclub, increasingly became a place for young musicians to learn, and for established musicians to earn a living. And if you’re going to jazz school, you need jazz books.\nBerklee College of Music. Photo by Cryptic C62\nThe fake books at the time hadn’t kept up with the music. They still contained the same old-fashioned collection of standards with the same old-fashioned collection of chord changes. If a young jazz musician wanted to try and play like Charles Mingus or Sonny Rollins, they weren’t going to learn from a book. That is… until two college kids invented the Real Book.\n\nThe Two Guys\n\nIn the mid-70s, Steve Swallow began teaching at Boston’s Berklee College of Music, an elite private music school that boasted one of the first jazz performance programs in the country. Swallow had only been teaching at Berklee for a few months when two students approached him about a secret project. “I keep referring them to them as ‘the two guys who wrote the book,’ because…they swore me to secrecy. They made me agree that I would not divulge their names,” explains Swallow. The “two guys” wanted to make a new fake book, one that actually catered to the needs of contemporary jazz musicians and reflected the current state of jazz. And they needed Swallow’s help.\n\nFrom the very beginning, the students envisioned the Real Book as a cooler and more contemporary fake book than the stodgy, outdated ones they’d grown up with. They wanted it to include new songs from jazz fusion artists like Herbie Hancock, and free jazz pioneers like Ornette Coleman who were pushing the boundaries of the genre. They also wanted to include the old jazz standards from Broadway and Tin Pan Alley, but they wanted to update those classics with alternate chord changes that reflected the way modern musicians, like Miles Davis, were actually playing them.\n\nModern jazz musicians had altered a lot of classic standards over the years, with new harmonies and more complex chord changes. And to capture these new sounds, the students spent hours listening to recordings and transcribing what they heard, as best they could. It was a huge undertaking because most of these chord changes had never actually been written down. They weren’t necessarily thinking about it like this at the time, but the students were effectively establishing a new set of standardized harmonies for a handful of classic songs.\n\nThe music wasn’t the only part of their new fake book that the students wanted to improve. They also wanted to fix the aesthetic problems with the old fake books, and make something that was nice to look at and easy to read. One of “the two guys” notated all of the music by hand in this very distinctive and expressive script. He also designed and silk-screened the logo on the front cover: “The Real Book,” written in chunky, SchoolHouse Rock-style block letters.\n\nBy the summer of 1975, the book was done, and the students took it to local photocopying shops where they cranked out hundreds of copies to sell directly to other students and a few local businesses near Berklee. Overnight, almost everyone had to have one. As the Real Book’s notoriety grew, so did the demand. The two students hadn’t printed enough copies to keep up, but it turns out, they didn’t need to. Not long after they created a few hundred copies of the book, bootleg versions began popping up all over the world. The Real Book had taken on a life of its own, and the students ironically found themselves in the same position as the music publishers and songwriters they’d originally cut out of the process, as they watched unlicensed copies of their work get duplicated and sold. After they released the first edition of the Real Book, the students put out two more editions to correct mistakes, and then their work was done. But the Real Book lived on, copied over and over again by new generations of bootleggers. And as the number of students in elite conservatory jazz programs continued to swell over the next few decades, the Real Book, with its modern repertoire, reharmonized standards, and beautiful handwriting, became the de-facto textbook for this new legion of jazz students. The unofficial official handbook of jazz.\n\nThe Real Real Book\n\nJust like with old fake books, the success of the Real Book was a major problem for music publishers. Some companies released their own fake books, but they never managed to compete with the Real Book. The popularity of the Real Book meant that lots of people weren’t getting paid for their work. But in the mid-2000s, music executive Jeff Schroedl and the publisher Hal Leonard decided, if you can’t beat ’em, join ’em. They went through the Real Book page by page, secured the rights to almost every song, and published a completely legal version. You don’t need to buy the Real Book out of the back of someone’s car anymore. It’s available at your local music shop. They even wanted the same handwriting. Hal Leonard actually hired a copyist to mimic the old Real Book’s iconic script and turn it into a digital font, which means a digital copy of a physical copy of one anonymous Berklee student’s handwriting from the mid-70s will continue to live on for as long as new editions of the book are published.\nThe Hal Leonard version of the Real Book\nWhen Hal Leonard finally published the legal version of the Real Book in 2004, it was great news if you were a composer with a song in there. You’d finally be getting royalties from the sale of the most popular jazz fake book of all time. But that didn’t totally solve the intellectual property problems with the Real Book. While the legalization of the Real Book did resolve most of its flagrant copyright violations, it didn’t clear up authorship disputes that go back to the early days of jazz. Many jazz songs arise out of collective tinkering and improvising in jam sessions. It’s sometimes quite hard to say who exactly wrote a given song, and power dynamics often impacted whose name actually got listed as an official songwriter. And so there are likely many musicians whose names will never appear on the songs they helped write, even if those songs appear in the legal Real Book.\n\nUseful Tool, or Reductive Cheat Sheet?\n\nEven if we put the intellectual property questions aside for a second, fake books like the Real Book still have plenty of critics. Nicholas Payton is a musician and record label owner, and he compares the Real Book to a study guide or a cheat sheet—a way to distill this complicated art form into a manageable packet of digestible information. To Payton, jazz isn’t just information to be learned. It’s a way of thinking and a form of expression. And it’s fundamentally a Black cultural phenomenon that can’t be taken out of its historical context. Payton says that reading books like the Real Book, even going to music school, can really only get you so far. If you want to learn to play, at some point you’re going to have to immerse yourself in the culture of the music. For Payton (and many musicians) learning directly from elders, in person, is a crucial part of what it means to really know the art form.\nThere’s also the question of codification, and whether it’s useful to have one songbook filled with definitive versions of all these jazz tunes. Carolyn Wilkins has taught ensembles at Berklee College of Music, and she says that the chords that are written down in the Real Book sometimes get treated like the right way to play a particular song. But even though jazz has all of these “standards,” they’re not supposed to be played in one standard way. As you listen to different recordings of the same song by different jazz artists, it becomes obvious that there’s no one right way to play it. Wilkins says that the Real Book does have its place in jazz education. Over her years at Berklee, she’s seen how it can be a useful starting place as a tool to bring young jazz musicians together. The key, she says, is to treat the Real Book as a starting place. From there you need to go out and explore all the other ways people have played a particular song. “And then ultimately you must find your own way.”\n\n        Enjoy 99pi? Subscribe to the podcast!\n        iTunes RSS Feed\n\n        Get the latest from 99pi each week in your inbox\n\n            Email Address\n\n              Yes Please! By submitting this form, you acknowledge that you have read the Terms of Use and Privacy Policy, that you understand them, and that you agree to be bound by them.  If you do not agree to be bound by the Terms of Use and Privacy Policy, you may not use the 99% Invisible website and services.\n\n        Enjoy 99pi? Subscribe to the podcast!\n        Subscribe Subscribe\n\n  Credits\n\n      Production\n      Reporter Mikel McCavana spoke with Jeff Leonard, musician and music educator at Berklee College of Music, Boston University, New England Conservatory; Steve Swallow, musician and composer; Barry Kernfeld, musicologist and author; Jeff Schroedl, Executive Vice President at Hal Leonard; Nicholas Payton; Musician, owner of Paytone Records, and creator of the Black American Music (BAM) movement; Carolyn Wilkins, musician and professor at Berklee College of Music; Gerald Horne, author and Moores Professor of History and African American Studies at the University of Houston.\nThis episode was edited by Emmett FitzGerald.",
    "summary": {
      "en": "**Episode 438: The Real Book - Summary**\n\nSince the 1970s, nearly every jazz musician has owned a book called the Real Book, which has a peach cover and contains hand-written sheet music for many jazz standards. However, this book was initially illegal, as it was self-published without permission from music publishers or songwriters and was sold covertly.\n\nThe origin of the Real Book is linked to earlier \"fake books,\" which emerged in the 1940s to help musicians play popular songs without carrying large amounts of sheet music. These fake books faced backlash from publishers who wanted to sell legal sheet music.\n\nThe Real Book was created in the mid-70s by two students at Berklee College of Music who wanted a modern collection that included contemporary jazz tunes and updated chord changes. They produced copies to sell to fellow students, but soon bootleg versions appeared worldwide, leading to a surge in demand.\n\nIn the mid-2000s, Hal Leonard published a legal version of the Real Book, ensuring composers received royalties, but some intellectual property issues remained unresolved, particularly regarding authorship of jazz songs.\n\nWhile the Real Book is a useful tool for learning jazz, critics argue it oversimplifies the genre. Musicians like Nicholas Payton emphasize that jazz is more than just notes on a page; it's about cultural immersion and personal expression. Others, like Carolyn Wilkins, see the Real Book as a helpful starting point but stress the importance of exploring diverse interpretations of jazz music.",
      "ko": "1970년대 이후 거의 모든 재즈 음악가들은 복숭아색 표지를 가진 '리얼 북'이라는 책을 소유하고 있습니다. 이 책에는 많은 재즈 스탠다드의 손으로 쓴 악보가 담겨 있습니다. 그러나 이 책은 처음에는 불법이었습니다. 음악 출판사나 작곡가의 허가 없이 자가 출판되어 비밀리에 판매되었기 때문입니다.\n\n리얼 북의 기원은 1940년대에 등장한 '가짜 북'과 관련이 있습니다. 이 가짜 북은 음악가들이 많은 악보를 가지고 다니지 않고도 인기 곡을 연주할 수 있도록 도와주기 위해 만들어졌습니다. 그러나 출판사들은 법적인 악보를 판매하고 싶어 했기 때문에 가짜 북에 대한 반발이 있었습니다.\n\n리얼 북은 1970년대 중반, 현대적인 재즈 곡과 업데이트된 코드 변화를 포함한 컬렉션을 원했던 버클리 음악 대학의 두 학생에 의해 만들어졌습니다. 그들은 동료 학생들에게 판매하기 위해 복사본을 제작했지만, 곧 전 세계에서 불법 복제품이 등장하면서 수요가 급증했습니다.\n\n2000년대 중반, 할 레너드가 리얼 북의 합법적인 버전을 출판하여 작곡가들이 로열티를 받을 수 있도록 했지만, 재즈 곡의 저작권 문제는 여전히 해결되지 않았습니다.\n\n리얼 북은 재즈를 배우는 데 유용한 도구이지만, 비평가들은 이 책이 장르를 지나치게 단순화한다고 주장합니다. 니콜라스 페이튼과 같은 음악가는 재즈가 단순히 악보에 적힌 음표 이상의 것이라고 강조하며, 문화적 몰입과 개인적 표현이 중요하다고 말합니다. 캐롤린 윌킨스와 같은 다른 이들은 리얼 북이 유용한 출발점이 될 수 있지만, 재즈 음악의 다양한 해석을 탐구하는 것이 중요하다고 강조합니다.",
      "ja": null
    }
  },
  {
    "id": "f5d064ce994c8f87",
    "title": {
      "en": "xAI has acquired X, xAI now valued at $80 billion",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://twitter.com/elonmusk/status/1905731750275510312",
    "score": 46,
    "by": "rvz",
    "time": 1743197022,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "bb49980e2d39f614",
    "title": {
      "en": "SignalBotOne – Notification Webhooks for Signal",
      "ko": "시그널봇 알림",
      "ja": null
    },
    "type": "story",
    "url": "https://signalbot.one/",
    "score": 36,
    "by": "danslo",
    "time": 1743181616,
    "content": "Add a Webhook to Your Signal ChatReceive messages on Signal via a simple API.Perfect for notifications and alerts.Try it! Learn More →$ curl -d '🚀 Houthi alert' https://signalbot.one/api/...9:41SignalBot🚀 Houthi alert9:41\n\nGet startedReceive Signal messages in two steps1Message @BotMaster.1000And get an instant webhook URL2Receive NotificationsGet real-time alerts and notifications, via DM or in a Signal group.SignalBot🚨 Alert: Server CPU usage at 92%SignalBot✅ Deployment successful: v2.1.3\n\nFrequently Asked QuestionsEverything you need to know about SignalBotHow secure is SignalBot?SignalBot uses strong security measures and does not store messages or metadata. However, it is not recommended to use the API to send passwords or war plans. If you want to notify a group, it's the safest to create a dedicated group chat for SignalBot notifications.Who's behind this?SignalBot was made by gwillem and is free to use. Like it? Buy me a coffee ❤️ Can I customize the notification format?Right now, only plain text and emojis are supported. Markdown support is on the way.What integrations are available?SignalBot offers a generic webhook API that can be used with curl or any programming language. It can be used to message you or a group you have invited SignalBot to. If you require specific integrations, do reach out!",
    "summary": {
      "en": "### Summary: Adding a Webhook to Your Signal Chat\n\nYou can receive messages on Signal easily using a simple API, which is great for notifications and alerts. Here's how to get started:\n\n1. **Get a Webhook URL**: Message @BotMaster to receive your webhook URL.\n2. **Receive Notifications**: Get real-time alerts via direct message or in a Signal group.\n\n**Examples of Notifications**:\n- Server CPU usage alert\n- Successful deployment notifications\n\n### Frequently Asked Questions\n\n- **Security**: SignalBot is secure and doesn’t store messages or metadata. Avoid sending sensitive information like passwords.\n- **Creator**: SignalBot is developed by gwillem and is free to use.\n- **Customization**: Currently, notifications can only be plain text and emojis, with Markdown support coming soon.\n- **Integrations**: SignalBot has a generic webhook API that works with various programming languages. Reach out for specific integration needs.",
      "ko": "신호 채팅에 웹후크를 추가하는 방법은 간단합니다. 이를 통해 알림과 경고를 쉽게 받을 수 있습니다. 시작하는 방법은 다음과 같습니다.\n\n먼저 웹후크 URL을 받아야 합니다. @BotMaster에게 메시지를 보내면 웹후크 URL을 받을 수 있습니다. 이후에는 직접 메시지나 신호 그룹을 통해 실시간 알림을 받을 수 있습니다.\n\n알림의 예로는 서버 CPU 사용량 경고나 성공적인 배포 알림이 있습니다.\n\n자주 묻는 질문에 대해 설명하겠습니다. 보안 측면에서 SignalBot은 안전하며 메시지나 메타데이터를 저장하지 않습니다. 비밀번호와 같은 민감한 정보는 보내지 않는 것이 좋습니다. SignalBot은 gwillem이 개발하였으며 무료로 사용할 수 있습니다. 현재 알림은 일반 텍스트와 이모지로만 제공되며, 곧 마크다운 지원이 추가될 예정입니다. SignalBot은 다양한 프로그래밍 언어와 호환되는 일반 웹후크 API를 제공하므로, 특정 통합 필요가 있을 경우 문의해 주시기 바랍니다.",
      "ja": null
    }
  },
  {
    "id": "a77b776dfe11cf48",
    "title": {
      "en": "A cretaceous fly trap? Remarkable abdominal modification in a fossil wasp",
      "ko": "공룡 시대 파리 덫? 화석 벌의 놀라운 변화",
      "ja": null
    },
    "type": "story",
    "url": "https://bmcbiol.biomedcentral.com/articles/10.1186/s12915-025-02190-2",
    "score": 16,
    "by": "gnabgib",
    "time": 1743188100,
    "content": "Research\n\n            Open access\n\n                            Published: 27 March 2025\n\n                        A cretaceous fly trap? remarkable abdominal modification in a fossil wasp\n                        Qiong Wu1, Lars Vilhelmsen2, Xiaoqin Li1, De Zhuo3, Dong Ren1 & …Taiping Gao1Show authors\n\n    BMC Biology\n\n                            volume23, Articlenumber:81 (2025)\n            Cite this article\n\n                        1589 Accesses\n\n                            509 Altmetric\n\n                    Metrics details\n\n                    AbstractBackgroundCarnivorous insects have evolved a range of prey and host capture mechanisms. However, insect predation strategies in the fossil record remain poorly understood.ResultsHere, we describe †Sirenobethylus charybdis n. gen. & sp., based on sixteen adult female wasps in Kachin amber from the mid-Cretaceous, 99 Mya (million years ago), and place it in Chrysidoidea: †Sirenobethylidae n. fam. The fossils display unique morphological modifications on the tip of the abdomen consisting of three flaps from the modified abdominal sternum 6 and tergum and sternum 7; the lower flap formed from sternum 6 is preserved in different positions relative to the other flaps in different specimens, indicating that they form some sort of grasping apparatus. Nothing similar is known from any other insect; the rounded abdominal apparatus, combined with the setae along the edges, is reminiscent of a Venus flytrap. Phylogenetic analysis suggests that the new family is a separate lineage close to the base of Chrysidoidea.Conclusions†Sirenobethylus probably was a koinobiont parasitoid wasp; the abdominal grasping apparatus may have been used to temporarily immobilize the host during oviposition. The new fossils suggest that Chrysidoidea displayed a wider range of parasitoid strategies in the mid-Cretaceous than they do today.\n\n                    BackgroundInsects are the most diverse group of animals on the planet, with more than 1 million described species and probably several times more undescribed; recent estimates suggest approximately 5.5 million species in total [1, 2]. Their highly adaptable exoskeleton has allowed them to radiate and colonize a wide range of habitats and develop highly efficient and innovative solutions to a range of challenges posed by their surroundings [3, 4]. Among other mechanisms, carnivorous insects have evolved a range of prey capture mechanisms [5, 6]: the forelegs of praying mantises (Mantodea) and mantis flies (Neuroptera: Mantispidae) [7]; the capture basket formed by the spiny legs of adult dragonflies (Odonata); the prehensile labial ‘mask’ employed by dragonfly nymphs [8]; the modified fore legs of female dryinid wasps (Hymenoptera: Dryinidae) for immobilizing hosts temporarily [9]; the hair-trigger mandibles of trap-jaw ants (Formicidae: Odontomachus) [10]; the fore and midlegs of heel-walkers (Mantophasmatodea) for swooping on prey [11]; the large hind tarsal claws of hangingflies (Mecoptera: Bittacidae) for grasping prey [12].The fossil insect fauna from the Cretaceous (Albian – Cenomanian; 99 Mya) Kachin amber provide unique insights into insect evolution [13, 14]; in addition to confirming the ancestry of features observed in modern organisms, it displays occasional examples of ancient morphologies without obvious modern parallels. With regard to potential prey capture mechanisms, the most prominent example so far might be the remarkable mandibles of the ‘saber-tooth’ haidomyrmecine ants [7, 15]. In the present paper, we describe an even more bizarre possible host capture/immobilization mechanism in the abdomen of a chrysidoid wasp. The rounded abdominal apparatus, combined with the setae along the edges, is reminiscent of a Venus flytrap (Droseraceae: Dionaea muscipula), a carnivorous plant using two opposing specialized leaves to capture insect prey [16].ResultsSystematic paleontologyOrder Hymenoptera Linnaeus, 1758.Infraorder Aculeata Latreille, 1802.Superfamily Chrysidoidea Latreille, 1802.Family †Sirenobethylidae Wu, Vilhelmsen & Gao fam. nov.ZooBank LSID: urn:lsid:zoobank.org:act:EAEC75B2-83B0-4C0E-A4F8-4F18DA024D15.Type genus. †Sirenobethylus Wu, Vilhelmsen & Gao gen. nov.Diagnosis. Head hypognathous, with medial line on vertex. Antenna with nine flagellomeres, antennal sockets simple, close to posterior margin of clypeus; clypeus projecting and acute in lateral view, slightly convex; mandibles with four apical teeth along truncate apical margin; occipital carina distinct, complete. Propleuron not exposed in dorsal view; prosternum small, diamond-shaped, exposed; notauli present. Female macropterous. Second abdominal (first metasomal) segment in dorsal view with angular anterolateral corners; tergum 7 longer than wide, distinctly narrower than other terga; sternum 6 wider than other sterna, laterally expanded distally, paddle-shaped, projecting posteriorly, posterior margin concave, with a dozen very long, slender setae; many thick spines on dorsal surface of sternum 6. Sternum 7 with median part accommodating ovipositor shaft dorsally and two lateral parts curving outwards before approaching median part distally; median and lateral parts of sternum 7 separated by weakly sclerotized areas. Sting sheaths on either side of the sting, apparently shorter than sting.Included genus. †Sirenobethylus Wu, Vilhelmsen & Gao gen. nov.Genus †Sirenobethylus Wu, Vilhelmsen & Gao gen. nov.ZooBank LSID: urn:lsid:zoobank.org:act:B3548EEB-BF75-4CA3-8E63-23C1B399A47C.Type species. †Sirenobethylus charybdis Wu, Vilhelmsen & Gao sp. nov.Etymology. The new generic name is a combination of the Greek ‘sireno-’ meaning ‘female humanlike beings with alluring voices in Greek mythology’, and ‘bethylus’, from the nominal genus of Bethylidae. ‘Sireno-’ is also a reference to Mammalia: Sirenia, as the ‘tail’ of the wasp in ventral view resembles that of a manatee. The gender is masculine.†Sirenobethylus charybdis Wu, Vilhelmsen & Gao sp. nov. (Figs. 1–3).Fig.1†Sirenobethylus charybdis sp. nov., holotype (specimen CNU-HYM-MA2015124) female. A Dorsal view as preserved. B The tip of abdomen and ovipositor in ventral view, showing trigger hairs (black arrows). C The tip of abdomen and ovipositor in lateral view, showing trigger hairs (black arrows), ovipositor (orange arrow) and groove on the sternum 7 (blue arrow). D Habitus reconstruction. Scale bars: A 0.5mm; B 0.3mm; C 0.2mm. Abbreviations: S6 sternum 6; T7 tergum 7; S7 sternum 7Full size imageZooBank LSID: urn:lsid:zoobank.org:act:75AEB71E-2DCD-4CBA-9EB9-5DEE7CFD0465.Etymology. The epithet refers to Charybdis, the sea monster in Greek mythology who alternately swallowed and disgorged copious amounts seawater three times a day.Diagnosis. As for the genus.Materials. Holotype. Female, CNU-HYM-MA-2015124.Locality and horizon. The amber specimen was collected from Kachin (Hukawng Valley) of northern Myanmar, which is dated at 98.79 ± 0.62 Mya [17, 18].Description. See Additional file 1: Figs. S1–S16, Dataset S1 [19,20,21,22,23,24,25,26,27], Additional file 2: Table S1.DiscussionPhylogenetic position of †Sirenobethylus gen. nov.We describe a new genus †Sirenobethylus, from mid-Cretaceous Kachin amber, based on sixteen specimens. The new genus is readily attributed to the Aculeata by its concealed posterior abdominal segments and ovipositor apparatus [28]. †Sirenobethylus has 9 flagellomeres and forewing with 8 closed cells, which is different from Vespoidea sensu lato [29] and Apoidea (antenna with 10 flagellomeres in female and forewing usually has 10 or 9 closed cells) [30, 31]. Therefore, †Sirenobethylus probably belongs to Chrysidoidea. We retrieve this superfamily as monophyletic (Fig.4, Additional file 1: Dataset S2) [32], unlike some recent molecular studies [33].All †Sirenobethylus specimens examined are macropterous females; given the unique diagnostic traits in, e.g., wing venation, and the somewhat isolated phylogenetic position of the new taxon (Fig.4), it is not possible to associate these females with any males known from Kachin amber. They have the forewing venation similar to †Chrysopsenellidae, including a long pterostigma, a closed subdiscal cell, and vein 2m-cu absent (Fig.1A and D). Therefore, the new genus is much more similar to †Chrysopsenellidae than extant groups. However, †Sirenobethylus has comparatively complete hind wing venation, including two closed cells also exhibited by †Plumalexiidae and Plumariidae, but different from other groups of Chrysidoidea [34]. All these features suggest that †Sirenobethylus could be a stem group of Chrysidoidea, which is consistent with our phylogenetic analyses (Fig.4, Additional file 3: Table S2). Furthermore, †Sirenobethylus has a unique combination of diagnostic characters within Chrysidoidea. For these reasons, we place †Sirenobethylus charybdis gen. & sp. n. in its own family, the †Sirenobethylidae.Possibly function of the abdominal apparatusThe abdominal apparatus of †Sirenobethylus, including the sixth sternum and the seventh tergum and sternum, form three superimposed horizontal flaps, modified into a complex composite structure. The composite structure is round and resembles that of the Venus flytrap (Droseraceae: Dionaea muscipula), the carnivorous plant that captures insects between its leaves [16]. The upper flap (tergum 7) is elongate and tongue-shaped; the middle flap (sternum 7) extends laterally far beyond the dorsal flap and is extensively membranous. The lower flap (sternum 6) is expanded distally and laterally, forming a paddle-shaped structure (Fig.1B); the thick spines on the dorsal part of the lower flap are elongate and scattered in the middle, but shorter and more densely placed along the edges (Additional file 4: Video S1). Furthermore, micro-CT (micro computed tomography) reconstruction shows two apodemes on the anterolateral corners of the lower flap, just inside the constriction where the sternum 6 meets the posterior margin of sternum 5 (Additional file 5: Video S2). The posterior margin of the lower flap has a dozen very long, slender setae extending from it. The tip of the abdomen often has numerous setae in the extant Aculeata, but they are not as long [35, 36]. The eighth tergum of †Sirenobethylus is hidden beneath the upper flap and the sting and is shorter than the upper flap (Fig.2D). The sting of †Sirenobethylus extends through a groove on the dorsal side of the middle flap, below the upper flap; the sting sheaths are situated on either side of the sting, and are apparently shorter than the sting (Fig.1C). Among the existing Aculeata with stings, most have sheaths that are about as long as the sting itself [28].Fig.2Photographs and micro-CT reconstructions (volume renderings) of †Sirenobethylus charybdis sp. nov., paratype (specimen CNU-HYM-MA2015119) female. A Dorsal view as preserved. B Micro-CT reconstruction of the tip of abdomen in dorsal view, showing sternum 6, sternum 7 lateral bar, sternum 7 median bar and the contact point between the two, ovipositor (white arrows). C Micro-CT reconstruction of sternum 3 with transverse line. D Micro-CT reconstruction of the interior abdomen in lateral view; blue area indicates extent of tergum 8, green area indicates sternum 6. Scale bars: A 0.5mm; B–D 0.2mm. Abbreviations: S6 sternum 6; S7 sternum 7; T8 tergum 8Full size imageThe abdominal apparatus of †Sirenobethylus is unlike anything previously reported from any extant wasp or indeed any insect known to us [5, 37]. From the morphology and the different states of position of the lower flap preserved in different specimens, it seems evident that the apparatus had some grasping function. We consider two different possible usages for the apparatus: 1) It may have had a function during mating, restraining the male. Due to the lack of male fossil evidence, we cannot determine the role of the apparatus in the mating process. Indeed, it would be unique for insect females to restrain the males during mating, rather than the other way around. We consider this an unlikely function of the abdominal apparatus. 2) It might have served to restrain a host temporarily during oviposition. We consider this to be the most likely function of the abdominal apparatus and will elaborate on this in the following.The fortuitous preservation of the flaps in different relative positions in various specimens of †Sirenobethylus, i.e., ‘open’ with the lower flap depressed relative to the middle and dorsal flaps (Figs. 3F–H, Additional file 1: Figs. S2, S10, S13) versus ‘closed’ with the ventral flap closely appressed to the middle and dorsal flaps (Figs. 1C, 2D, 3E, Additional file 1: Figs. S4, S5, S7, S9, S11, S14, S16) indicates that together they form a grasping apparatus. The very elongate hairs along the posterior margin of the ventral flap (Figs. 1D, 3C and D) might have served as ‘trigger hairs’ forewarning the parasitoid of an approaching host and perhaps indicating the host's position based on the number of hairs that the host touches [38]. The setae on the lower flap are long and sparse in the middle, and short and dense along the edges and presumably flexible (Fig.2D); the extensive membranous areas on the middle flap (Fig.2B) appear relatively soft. We speculate that the coarse setal brush inside the ventral flap and the extensive membranous areas on the middle flap might have served to cushion the host during oviposition rather than crushing it, indicating that the host may not have been permanently incapacitated during the procedure and that †Sirenobethylus might have been a koinobiont parasitoid rather than a predator [27]. For these reasons, we suggest that the abdominal apparatus could have served to temporarily grasp and immobilize the host during oviposition. In addition, the sting of †Sirenobethylus is situated in a groove on the dorsal side of the middle flap, and among the sixteen specimens, six specimens have stings preserved with grooves directed downward, which would allow the †Sirenobethylus to easily sting the captured host (Figs. 1C, 3E–G, Additional file 1: Figs. S5F, S8G, S10D, S11D, S12H, S16E).Fig.3Photographs of †Sirenobethylus charybdis sp. nov. females. (A, B) Lateral view of specimens CNU-HYM-MA2015132 and CNU-HYM-MA2015125. (C, D) Dorsal view of specimens CNU-HYM-MA2015129 and CNU-HYM-MA2015122, showing trigger hair (black arrows). (E–H) Abdominal terminal in posterior view of specimens CNU-HYM-MA2015132, CNU-HYM-MA2015125, CNU-HYM-MA2015129, CNU-HYM-MA2015122, showing ovipositor (orange arrows), respectively, with upper + middle and lower flap in different relative positions (e.g., E: closed; H: fully open). Scale bars: A–D 0.5mm; E–H 0.2mmFull size imageBased on the reconstruction from micro-CT data (Fig.2, Additional file 4: Video S1, Additional file 5: Video S2), the lower flap rotates around the point where sternum 6 is overlapped ventrally by sternum 5. Adductor muscles inserting on the lower side of the anterolateral apodemes would raise the lower flap towards the middle flap, closing the apparatus, whereas abductor muscles inserting on the upper part of the apodemes would depress the lower flap away from the middle flap. For quick operation, especially when closing the apparatus to grasp the host, the muscles would have to be of substantial size. We speculate that the abductors and adductors might have arisen from tergum 3 and sternum 3, respectively; these are the largest sclerites in the abdomen and each have a transverse line on their anterior part that might indicate the attachment sites of the muscles.The extant Chrysidoidea sensu lato [27, 30] display a range of life histories: parasitoids of wood-living beetle larvae (Scolebythidae), parasitoids of beetle or Lepidoptera larvae (Bethylidae), parasitoids of sawfly cocoons (Chrysididae: Cleptinae), egg-parasitoids of stick insects (Chrysididae: Amiseginae and Loboscelidinae), kleptoparasitoids in nests of solitary bees or wasps (Chrysididae: Chrysidinae), parasitoids of nymphs of webspinners/embiopterans (Sclerogibbidae) or parasitoids of nymphs of Auchenorrhyncha (Dryinidae, Embolemidae) [27]. Given this diversity and the lack of information from a number of chrysidoid families, including the extant Plumariidae, it is not possible to infer potential hosts for †Sirenobethylus by mapping lifestyles on the phylogeny, although most of the aforementioned groups have also been reported from Kachin amber [39].The females of some Dryinidae have developed a host restraining apparatus on the forelegs [40]; they use their chelate fore tarsi to immobilize their elusive hosts (leafhoppers, treehoppers and planthoppers) during oviposition. These dryinid females are often wingless, have large eyes and elongate legs, and pursue their hosts actively prior to oviposition [9]. In contrast, †Sirenobethylus was probably not able to pursue hosts over longer distances given the position of the putative capture apparatus at the posterior end of the body and its overall habitus compared to dryinid females (smaller eyes, shorter legs). However, the elaborate grasping apparatus indicates that †Sirenobethylus was indeed targeting highly mobile prey, and the hosts might have been homopteran hoppers (like for dryinids) or small winged insects, like flies, the elongate trigger hairs perhaps eliciting a short posterior lunge if a potential target came within range. We imagine it would have waited with the apparatus open, ready to pounce as soon as a potential host activated the capture response. Our findings suggest that Chrysidoidea displayed a wider range of parasitoid strategies in the mid-Cretaceous than they do today.ConclusionsWe report new mid-Cretaceous fossils of Chrysidoidea, suggesting that †Sirenobethylidae is a separate lineage close to the base of Chrysidoidea based on phylogenetic analysis. Based on our detailed analyses of the morphology of the specimens at our disposal, we infer that †Sirenobethylus was probably a koinobiont parasitoid wasp, the unique grasping mechanism at the tip of the abdomen possibly being used for temporary host capture. Our new findings indicate that by the mid-Cretaceous, some early Chrysidoidea had evolved unique parasitoid strategies.MethodsMaterial availabilityAll specimens were collected from Noije Bum hill, about 18km southwest of Tanai Village in the Hukawng Valley, northern Myanmar (26° 21′ 33.41\" N, 96° 43′ 11.88\" E) [17, 18]. All amber specimens are stored in the Key Lab of Insect Evolution and Environmental Changes, College of Life Sciences, Capital Normal University (CNUB; Dong Ren, Curator), Beijing, China. Extant specimens examined are stored in the Natural History Museum of Denmark, University of Copenhagen, Copenhagen, Denmark. The wing venation nomenclature is based on Rasnitsyn (1980) [41].No datasets were generated or analysed during the current study.Optical microscopy, photographyThe specimens studied were examined and photographed using a Leica M205A stereomicroscope equipped with a Leica DFC425 camera and LAS software. The amber specimens were examined under a Leica M205C stereomicroscope. Figure1 of the holotype specimen, Fig.4 of habitus images of extant specimens and Additional file 1: Fig. S7C and D were taken with a BK + Imaging System from Visionary Digital equipped with a Canon EOS 7D camera. The other images of the amber specimens were taken with a Nikon SMZ 25 microscope with an attached Nikon DS-Ri2 digital camera system or a Nikon ECLIPSE Ni microscope with an attached Nikon DS-Ri2 digital camera system.Fig.4Bayesian phylogenetic tree based on morphological characters. The large black dots show the age of the Kachin amber species; the branch nodes of this phylogenetic tree are not time-calibrated, the geological time scale refers only to the fossil taxa. The numbers on the branch nodes are posterior probabilities. Green branches: Chrysidoidea; blue branches: Vespoidea; orange branches: Apoidea. Families with habitus images associated indicated in purple: †Sirenobethylus charybdis (Sirenobethylidae), Chrysis ignita (Chrysididae), Pristocera depressa (Bethylidae), Embolemus ruddii (Embolemidae), Rhopalomutilla carinaticeps (Mutillidae), Polistes nimpha (Vespidae), Scolia quadripunctata (Scoliidae), Formica rufa (Formicidae)Full size imageMicro-CT scanningThe paratype CNU-HYM-MA2015119 was scanned at the micro-CT laboratory of YKLP (Yunnan Key Laboratory for Paleobiology) with an X-ray microscope (3D-XRM), Zeiss Xradia 520 versa. Scanning parameters are as follows: beam strength: 60kV/5w, filter: no, resolution: 1.94μm, exposure time: 5s, number of TIFF images: 1718. Volume rendering and 3D reconstruction were performed using the open-source software Drishti 2.4 [42]. The 3D-reconstruction models of the abdomen of the specimen are displayed in Fig.2, and the original scan data has been deposited in Dryad (Wu et al., 2025; https://doi.org/https://doi.org/10.5061/dryad.4b8gthtq9).Phylogenetic analysisOur phylogenetic analyses included one species of non-aculeate wasp (Trigonalidae: Taeniogonalos gundlachii) as outgroup/root, 19 species of Chrysidoidea (7 fossil, 12 extant), 19 species of Vespoidea (19 extant), and four species of Apoidea (4 extant) as ingroups to clarify the phylogenetic position of the new fossil taxon (see Additional file 3: Table S2). We scored 57 morphological characters (see Additional file 1: Dataset S3) [43,44,45,46,47,48,49,50].Bayesian analyses were performed in MrBayes version 3.2.7 [51]. Only variable characters were coded, and non-applicable morphological characters were treated as missing data. Equal transition probabilities between the states and among-character rate variation were assumed, allowing the different characters to evolve at different rates. Bayesian phylogenetic analysis used the Mk model, were conducted with four independent runs. Convergence was assessed by the average standard deviation of split frequencies (ASDSF < 0.01), and the potential scale reduction factor (PSRF < 1.005). After 20 million generations, the topology converged with an ASDSF < 0.004, and PSRF values < 1.001. 50% of the generations were then discarded as burn-in. The posterior probabilities were plotted as relative branch support in the final tree (allcompat. tre) using FigTree v.1.4.3 [52]. The halfcompat. tree can be found in the Additional file 1: Fig. S17.\n                    Data availability\n\n              All data generated or analysed during this study are included in this article and its supplementary information files. The sixteen Kachin amber specimens (CNU-HYM-MA2015118 –CNU-HYM-MA2015133) reported in this study were purchased by Mr. Fangyuan Xia in April, 2015 and donated to the Key Laboratory of Insect Evolution and Environmental Changes, College of Life Sciences, Capital Normal University in May, 2016. They are deposited in the “Fossil Insect Collection” of the Key Laboratory of Insect Evolution and Environmental Changes, College of Life Sciences, Capital Normal University, Beijing, China. Correspondence and requests for materials should be addressed to Dr. Dong Ren, rendong@mail.cnu.edu.cn or Dr. Taiping Gao, tpgao@cnu.edu.cn.\n              Nomenclatural acts established herein are registered in ZooBank (www.zoobank.org) following the requirements of the International Code of Zoological Nomenclature and listed under LSID: urn:lsid:zoobank.org:pub:33EF6F52-752D-4283-BF54-150854595F34. The original photo data of the CT scans were deposited on Dryad (https://doi.org/https://doi.org/10.5061/dryad.4b8gthtq9).\n            AbbreviationsCT:\n                    Computed tomography\n                  Mya:\n                    Million years ago\n                  S2, S3, S4, S5, S6 and S7:\n                    Sternum 2, 3, 4, 5, 6 and 7\n                  T2, T3, T4, T5, T6, T7 and T8:\n                    Tergum 2, 3, 4, 5, 6, 7 and 8\n                  1st:\n                    First\n                  6th:\n                    Sixth\n                  7th:\n                    Seventh\n                  ReferencesStork NE. How many species of insects and other terrestrial arthropods are there on Earth? Annu Rev Entomol. 2018;63:31–45.Article\n    CAS\n    PubMed\n\n                    Google Scholar\n                Eggleton P. The state of the world’s insects. Annu Rev Env Resour. 2020;45:61–82.Article\n\n                    Google Scholar\n                Tihelka E, Cai CY, Giacomelli M, Lozano-Fernandez J, Rota-Stabelli O, Huang DY, et al. The evolution of insect biodiversity. Curr Biol. 2021;31:1299–311.Article\n\n                    Google Scholar\n                Wang B, Xu CP, Jarzembowski EA. Ecological radiations of insects in the Mesozoic. Trends Ecol Evol. 2022;37:529–40.Article\n    PubMed\n\n                    Google Scholar\n                Grimaldi DA, Engel MS. Evolution of the insects. New York: Cambridge University Press; 2005.\n                    Google Scholar\n                van de Kamp T, Mikó I, Staniczek AH, Eggs B, Bajerlein D, Faragó T, et al. Evolution of flexible biting in hyperdiverse parasitoid wasps. P Roy Soc B-Biol Sci. 2022;289:20212086.\n                    Google Scholar\n                Lai DH, Chen PC, Li SM, Xiang XZ, Ou HH, Kang NY, et al. The associated evolution of raptorial foreleg and mantispid diversification during 200 million years. Natl Sci Rev. 2023;10:nwad278.Corbet PS. Dragonflies: Behaviour and Ecology of Odonata. New York: Cornell University Press; 1999.\n                    Google Scholar\n                Virla EG, Moya-Raygoza G, Guglielmino A. A review of the biology of the pincer wasps (Hymenoptera: Dryinidae). Austral Entomol. 2023;62:274–99.Article\n\n                    Google Scholar\n                Gronenberg W, Tautz J, Hölldobler B. Fast trap jaws and giant neurons in the ant Odontomachus. Science. 1993;262:561–3.Article\n    CAS\n    PubMed\n\n                    Google Scholar\n                Zompro O, Adis J, Weitschat W. A review of the order Mantophasmatodea (Insecta). Zool Anz. 2002;241:269–79.Article\n\n                    Google Scholar\n                Setty LR. Biology and morphology of some North American Bittacidae (Order Mecoptera). Am Midl Nat. 1940;23:257–353.Article\n\n                    Google Scholar\n                Grimaldi DA, Engel MS, Nascimbene PC. Fossiliferous Cretaceous amber from Myanmar (Burma): its rediscovery, biotic diversity, and paleontological significance. Am Mus Novit. 2002;3361:1–71.Article\n\n                    Google Scholar\n                Ross AJ. Complete checklist of Burmese (Myanmar) amber taxa 2023. Mesozoic. 2024;1:21–57.Article\n\n                    Google Scholar\n                Barden P, Perrichot V, Wang B. Specialized predation drives aberrant morphological integration and diversity in the earliest ants. Curr Biol. 2020;30:3818–24.Article\n    CAS\n    PubMed\n\n                    Google Scholar\n                Durak GM, Speck T, Poppinga S. Shapeshifting in the Venus flytrap (Dionaea muscipula): Morphological and biomechanical adaptations and the potential costs of a failed hunting cycle. Front Plant Sci. 2022;2:970320.Article\n\n                    Google Scholar\n                Cruickshank RD, Ko K. Geology of an amber locality in the Hukawng Valley, northern Myanmar. J Asian Earth Sci. 2003;21:441–55.Article\n\n                    Google Scholar\n                Shi GH, Grimaldi DA, Harlow GE, Wang J, Wang J, Yang MC, et al. Age constraint on Burmese amber based on U-Pb dating of zircons. Cretaceous Res. 2012;37:155–63.Article\n\n                    Google Scholar\n                Brothers DJ. A new Late Cretaceous family of Hymenoptera, and phylogeny of the Plumariidae and Chrysidoidea (Aculeata). Zookeys. 2011;130:515–42.Article\n\n                    Google Scholar\n                Evans HE. Discovery of the female Plumarius (Hymenoptera, Plumariidae). Psyche-J Entomol. 1967;73:229–37.Article\n\n                    Google Scholar\n                Brothers DJ. The first female Plumariidae (Hymenoptera: Chrysidea) from Southern Africa. XVII International Congress of Entomology, Hamburg, Abstracts. 1984;26.Engel MS, Grimaldi DA. Cretaceous Scolebythidae and phylogeny of the family (Hymenoptera: Chrysidoidea). Am Mus Novit. 2007;3568:1–16.Article\n\n                    Google Scholar\n                Lepeco A, Melo GAR. Revisiting the phylogeny of the scolebythid wasps (Hymenoptera: Aculeata) through Bayesian model evaluation and parsimony, with description of a new fossil family of Chrysidoidea. Zool J Linn Soc-Lond. 2024;201:57–85.Article\n\n                    Google Scholar\n                Melo GAR, Lucena DAA. †Chrysobythidae, a new family of chrysidoid wasps from Cretaceous Burmese amber (Hymenoptera, Aculeata). Hist Biol. 2020;32:1143–55.Article\n\n                    Google Scholar\n                Engel MS, Grimaldi DA. The first Cretaceous sclerogibbid wasp (Hymenoptera: Sclerogibbidae). Am Mus Novit. 2006;3515:1–7.Article\n\n                    Google Scholar\n                Bohart RM, Kimsey LS. A synopsis of the Chrysididae in America North of Mexico. Mem Amer Ent Inst. 1982;33:1–266.\n                    Google Scholar\n                Finnamore AT, Brothers DJ. Superfamily Chrysidoidea. In: Goulet H, Huber JT, editors. Hymenoptera of the World: an identification guide to families. Ottawa: Agriculture Canada; 1993. p. 130–60.\n                    Google Scholar\n                Kumpanenko A, Gladun D, Vilhelmsen L. Functional morphology and evolution of the sting sheaths in Aculeata (Hymenoptera). Arthropod Syst Phylo. 2019;77:325–38.\n                    Google Scholar\n                Brothers DJ, Finnamore AT. Superfamily Vespoidea. In: Goulet H, Huber JT, editors. Hymenoptera of the World: an identification guide to families. Ottawa: Agriculture Canada; 1993. p. 161–278.\n                    Google Scholar\n                Sharkey MJ. Phylogeny and classification of Hymenoptera. Zootaxa. 2007;1668:521–48.Article\n\n                    Google Scholar\n                Finnamore AT, Michener CD. Superfamily Apoidea. In: Goulet H, Huber JT, editors. Hymenoptera of the World: an identification guide to families. Ottawa: Agriculture Canada; 1993. p. 279–357.\n                    Google Scholar\n                Blaimer BB, Santos BF, Cruaud A, Gates MW, Kula RR, Mikó I, et al. Key innovations and the diversification of Hymenoptera. Nat Commun. 2023;14:1212.Article\n    CAS\n    PubMed\n    PubMed Central\n\n                    Google Scholar\n                Zhang YM, Bossert S, Spasojevic T. Evolving perspectives in Hymenoptera systematics: bridging fossils and genomes across time. Syst Entomol. 2025;50:1–31.Article\n\n                    Google Scholar\n                Rasnitsyn AP, Brothers DJ. The first plumalexiid wasp (Hymenoptera: Chrysidoidea, Plumalexiidae) from the mid-Cretaceous Burmese amber. Cretaceous Res. 2020;115:104568.Article\n\n                    Google Scholar\n                Kochetkov DN, Loktionov VM. New and little known species of spider wasps (Hymenoptera: Pompilidae) from the Russian Far East. Far East Entomol. 2019;382:1–9.Article\n\n                    Google Scholar\n                Tu BB, Lelej AS, Chen XX. Review of the genus Cystomutilla André, 1896 (Hymenoptera: Mutillidae: Sphaeropthalminae: Sphaeropthalmini), with description of the new genus Hemutilla gen. nov. and four new species from China. Zootaxa. 2014;3889:71–91.Grimaldi DA. The Complete Insect: Anatomy, Physiology, Evolution, and Ecology. Princeton: Princeton University Press; 2023.Book\n\n                    Google Scholar\n                Escalante-Perez M, Scherzer S, Al-Rasheid KA, Dottinger C, Neher E, Hedrich R. Mechano-stimulation triggers turgor changes associated with trap closure in the Darwin plant Dionaea muscipula. Mol Plant. 2014;7:744–6.Article\n    CAS\n    PubMed\n\n                    Google Scholar\n                Ross AJ. Supplement to the Burmese (Myanmar) amber checklist and bibliography, 2024. Palaeoentomology. 2025;8:12–28.Article\n\n                    Google Scholar\n                Ponomarenko NG. Family Dryinidae (Dryinids). In: Medvedev GS, editor. Keys to the Insects of the European Part of the USSR. Moscow: Publications House Nauka; 1978. p. 16–27.\n                    Google Scholar\n                Rasnitsyn AP. Origin and Evolution of Hymenoptera. Trudy Paleontologicheskogo Instituta. 1980;174:1–192 ([In Russian]).\n                    Google Scholar\n                Limaye A. Drishti: a volume exploration and presentation tool. Proceedings of the SPIE. 2012;8506. https://doi.org/10.1117/12.935640.Brothers DJ. Phylogeny and classification of the aculeate Hymenoptera, with special reference to Mutillidae. Kans Univ Sci Bull. 1975;50:483–648.\n                    Google Scholar\n                Brothers DJ, Carpenter JM. Phylogeny of Aculeata: Chrysidoidea and Vespoidea (Hymenoptera). J Hymenopt Res. 1993;2:227–304.\n                    Google Scholar\n                Perrard A, Grimaldi DA, Carpenter JM. Early lineages of Vespidae (Hymenoptera) in Cretaceous amber. Syst Entomol. 2017;42:379–86.Article\n\n                    Google Scholar\n                Wu Q, Yang HR, Shih CK, Ren D, Zhao YY, Gao TP. Vespids from the mid-Cretaceous with club-shaped antennae provide new evidence about the intrafamiliar relationships of Vespidae. Zool J Linn Soc-Lond. 2020;193:217–29.Article\n\n                    Google Scholar\n                Vilhelmsen L. Head capsule characters in the Hymenoptera and their phylogenetic implications. ZooKeys. 2011;130:343–61.Article\n\n                    Google Scholar\n                Mason WRM. Key to superfamilies of HYMENOPTERA. In: Goulet H, Huber JT, editors. Hymenoptera of the World: an identification guide to families. Ottawa: Agriculture Canada; 1993. p. 65–100.\n                    Google Scholar\n                Lucena DAA, Melo GAR. Chrysidid wasps (Hymenoptera: Chrysididae) from Cretaceous Burmese amber: phylogenetic affinities and classification. Cretaceous Res. 2018;89:279–91.Article\n\n                    Google Scholar\n                Lucena DAA, Almeida EAB. Morphology and Bayesian tip-dating recover deep Cretaceous-age divergences among major chrysidid lineages (Hymenoptera: Chrysididae). Zool J Linn Soc-Lond. 2022;194:36–79.Article\n\n                    Google Scholar\n                Ronquist F, Teslenko M, Van Der Mark P, Ayres DL, Darling A, Höhna S, et al. MrBayes 3.2: Efficient bayesian phylogenetic inference and model choice across a large model space. Syst Biol. 2012;61:539–42.Rambaut A. FigTree. Computer software and documentation distributed by the author; 2013. http://tree.bio.ed.ac.uk/software/figtree/. Accessed March 1, 2025.Download referencesAcknowledgementsWe thank the editorial board of BMC Biology, and in particular Dr. Caitlyn Cardetti. We thank three anonymous reviewers for their valuable comments on this manuscript. We appreciate Ms. Xiaoran Zuo for the habitus reconstruction pictures and thanks Dr. Zhipeng Zhao for his help in discussion and the early CT construction of the specimens.FundingThis project was supported by a grant from the National Natural Science Foundation of China (Nos. 32470468, 32270467, 32020103006). The Support Project of High-level Teachers in Beijing Municipal Universities in the Period of 14th Five–year Plan (No. BPHR20220114).Author informationAuthors and AffiliationsCollege of Life Sciences, Capital Normal University, Beijing, 100048, ChinaQiong Wu,Xiaoqin Li,Dong Ren&Taiping GaoNatural History Museum of Denmark, SCIENCE, University of Copenhagen, Universitetsparken 15, 2100, Copenhagen, DenmarkLars VilhelmsenBeijing Xiachong Amber Museum, 9 Shuanghe Middle Road, Beijing, 100023, ChinaDe ZhuoAuthorsQiong WuView author publicationsYou can also search for this author inPubMedGoogle ScholarLars VilhelmsenView author publicationsYou can also search for this author inPubMedGoogle ScholarXiaoqin LiView author publicationsYou can also search for this author inPubMedGoogle ScholarDe ZhuoView author publicationsYou can also search for this author inPubMedGoogle ScholarDong RenView author publicationsYou can also search for this author inPubMedGoogle ScholarTaiping GaoView author publicationsYou can also search for this author inPubMedGoogle ScholarContributionsTPG conceived and designed the experiments. QW, LV, XQL, DZ, DR, and TPG performed the analyses and experiments. QW prepared photographs and line drawings. QW, LV, and TPG wrote the manuscript. All authors read and approved the final manuscript.Corresponding authorCorrespondence to\n                Taiping Gao.Ethics declarations\n\n                Ethics approval and consent to participate\n                Not applicable.\n\n                Consent for publication\n                Not applicable.\n\n                Competing interests\n                The authors declare no competing interests.\n\n            Additional informationPublisher’s NoteSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.Supplementary Information12915_2025_2190_MOESM1_ESM.docAdditional file 1:Figures S1–S17, Dataset S1–S3. Fig. S1 Photographs of †Sirenobethyluscharybdis sp. nov., holotypefemale. Fig. S2 Photographs of †Sirenobethyluscharybdis sp. nov., paratypefemale. Fig. S3 Photographs of †Sirenobethyluscharybdis sp. nov., paratypefemale. Fig. S4 Photographs of †Sirenobethyluscharybdis sp. nov., paratypefemale. Fig. S5 Photographs of †Sirenobethyluscharybdis sp. nov., paratypefemale. Fig. S6 Photographs of †Sirenobethyluscharybdis sp. nov., paratypefemale. Fig. S7 Photographs of †Sirenobethyluscharybdis sp. nov., paratypefemale. Fig. S8 Photographs of †Sirenobethyluscharybdis sp. nov., paratypefemale. Fig. S9 Photographs of †Sirenobethyluscharybdis sp. nov., paratypefemale. Fig. S10 Photographs of †Sirenobethyluscharybdis sp. nov., paratypefemale. Fig. S11 Photographs of †Sirenobethyluscharybdis sp. nov., paratypefemale. Fig. S12 Photographs of †Sirenobethyluscharybdis sp. nov., paratypefemale. Fig. S13 Photographs of †Sirenobethyluscharybdis sp. nov., paratypefemale. Fig. S14 Photographs of †Sirenobethyluscharybdis sp. nov., paratypefemale. Fig. S15 Photographs of †Sirenobethyluscharybdis sp. nov., paratypefemale. Fig. S16 Photographs of †Sirenobethyluscharybdis sp. nov., paratypefemale. Fig. S17 Halfcompat tree based on morphological characters in Bayesian analyses. Dataset S1. Systematic paleontology. Dataset S2. Phylogenetic analysis. Dataset S3. Morphological characters list.Additional file 2:Table S1. Body structure measurements of all specimens.Additional file 3:Table S2. The character-state matrix.Additional file 4:Video S1. CT scan of the lateral of the abdomen.Additional file 5:Video S2. CT scan of the dorsal of the abdomen.Rights and permissions\n                Open Access  This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by-nc-nd/4.0/.\n              Reprints and permissionsAbout this articleCite this articleWu, Q., Vilhelmsen, L., Li, X. et al. A cretaceous fly trap? remarkable abdominal modification in a fossil wasp.\n                    BMC Biol 23, 81 (2025). https://doi.org/10.1186/s12915-025-02190-2Download citationReceived: 04 December 2024Accepted: 08 March 2025Published: 27 March 2025DOI: https://doi.org/10.1186/s12915-025-02190-2Share this articleAnyone you share the following link with will be able to read this content:Get shareable linkSorry, a shareable link is not currently available for this article.Copy to clipboard\n                            Provided by the Springer Nature SharedIt content-sharing initiative\n                        KeywordsSirenobethylidaeChrysidoideaMorphologyGrasping apparatusPhylogeny",
    "summary": {
      "en": "**Summary:**\n\nA new genus of fossil wasp named **Sirenobethylus charybdis** has been discovered in mid-Cretaceous amber from Myanmar, dating back 99 million years. This wasp exhibits unique modifications at the tip of its abdomen, which feature three flaps that may function as a grasping mechanism, resembling a Venus flytrap. This adaptation is thought to help the wasp temporarily immobilize its hosts during oviposition (laying eggs). \n\nThe findings suggest that the family **Sirenobethylidae**, to which this wasp belongs, represents an early branch of the **Chrysidoidea** superfamily, indicating a broader range of parasitoid strategies in Cretaceous insects compared to those observed today. The study highlights the diversity of insect predation methods and provides insights into the evolution of wasp morphology and behavior. \n\nOverall, **Sirenobethylus** is believed to have been a koinobiont parasitoid, using its specialized abdomen to capture and hold hosts for laying eggs. This discovery contributes to our understanding of insect evolution and the ecological roles of ancient wasps.",
      "ko": "미얀마의 중생대 크레타기에서 발견된 9,900만 년 된 호박에서 새로운 화석 말벌 속인 **Sirenobethylus charybdis**가 발견되었습니다. 이 말벌은 복부 끝부분에 독특한 구조를 가지고 있으며, 세 개의 플랩이 있어 이를 통해 물체를 잡는 기능을 할 수 있을 것으로 보입니다. 이 구조는 식충식물인 비너스 플라이트랩과 유사합니다. 이러한 적응은 말벌이 알을 낳을 때 숙주를 일시적으로 immobilize하는 데 도움이 되는 것으로 생각됩니다.\n\n이 연구 결과는 이 말벌이 속한 **Sirenobethylidae** 가족이 **Chrysidoidea** 초과족의 초기 분기를 나타내며, 크레타기 곤충에서 현재 관찰되는 것보다 더 다양한 기생 전략이 존재했음을 시사합니다. 이 연구는 곤충의 포식 방법의 다양성을 강조하고, 말벌의 형태와 행동의 진화에 대한 통찰을 제공합니다.\n\n전반적으로 **Sirenobethylus**는 숙주를 잡아 알을 낳기 위해 특수한 복부를 사용하는 코이노비온트 기생충으로 여겨집니다. 이 발견은 곤충 진화와 고대 말벌의 생태적 역할에 대한 이해를 높이는 데 기여합니다.",
      "ja": null
    }
  },
  {
    "id": "a5bb7b71df5c21ed",
    "title": {
      "en": "Cross-Platform P2P Wi-Fi: How the EU Killed AWDL",
      "ko": "EU가 AWDL를 죽였다: 크로스 플랫폼 P2P Wi-Fi",
      "ja": null
    },
    "type": "story",
    "url": "https://www.ditto.com/blog/cross-platform-p2p-wi-fi-how-the-eu-killed-awdl",
    "score": 158,
    "by": "stusmall",
    "time": 1743167584,
    "content": "Published OnMarch 28, 2025March 28, 2025Cross-Platform P2P Wi-Fi: How the EU Killed AWDLThis post investigates how we got from Wi-Fi Direct to AWDL to Wi-Fi Aware, what makes Wi-Fi Aware technically superior, and why this shift unlocks true cross-platform peer-to-peer connectivity for developers.Adam FishFounder and CEO\n\npre {\n\t\t--theme--background: var(--core--100);\n    --theme--border: var(--swatch--light-faded);\n    --theme--border-fill: var(--core--100);\n    --theme--text: var(--core--800);\n    --theme--text-secondary: var(--core--400);\n\t\t--pre-text-main: var(--theme--text);\n    --pre-text-comment: color-mix(in srgb, var(--theme--text) 40%, transparent);\n    --pre-text-string: hsl(95, 38%, 62%);\n    --pre-text-keyword: hsl(286, 60%, 67%);\n    --pre-text-number: color-mix(in srgb, var(--theme--text) 60%, transparent);\n    --pre-text-attribute: color-mix(in srgb, var(--theme--text) 60%, transparent);\n    font-family: var(--eyebrow--font-family);\n    font-size: var(--text-main--font-size);\n    line-height: 1.5;\n    font-weight: var(--eyebrow--font-weight);\n    letter-spacing: var(--eyebrow--letter-spacing);\n    margin: 3em 0 !important;\n}\n\npre:has(code.hljs), pre {\n    overflow: clip;\n    padding: 0.75em !important;\n    background-color: var(--theme--background) !important;\n    white-space: pre-wrap;\n    color: var(--pre-text-main);\n}\n\npre code.hljs, pre code {\n    display: block;\n    overflow: auto;\n    height: 100%;\n    font-size: .875em;\n    font-family: var(--eyebrow--font-family);\n    font-weight: var(--eyebrow--font-weight);\n    letter-spacing: var(--eyebrow--letter-spacing);\n    padding: 0.875em;\n    color: var(--pre-text-main) !important;\n}\n\npre code::selection,\npre code span::selection {\n  background: color-mix(in srgb, var(--theme--text) 10%, transparent);\n}\n\npre code::-webkit-scrollbar {\n  width: 4px;\n  height: 4px;\n}\n\npre code::-webkit-scrollbar-corner {\n  background: rgba(0,0,0,0);\n  display: none;\n}\n\npre code::-webkit-scrollbar-track {\n  background: transparent;\n  padding: 2px;\n}\n\npre code::-webkit-scrollbar-thumb {\n  background-color: color-mix(in srgb, var(--theme--text) 25%, transparent);\n  border-radius: 999px;\n}\n\npre code::-webkit-scrollbar-thumb:hover {\n  background-color: color-mix(in srgb, var(--theme--text) 25%, transparent)\n}\n\n.hljs {\n    background: transparent;\n    color: var(--pre-text-main);\n}\n\n.hljs-ln-n {\n\topacity: 0.4;\n  font-family: var(--eyebrow--font-family);\n  font-weight: var(--eyebrow--font-weight);\n}\n\n.hljs-comment,\n.hljs-quote {\n    color: var(--pre-text-comment);\n    font-family: var(--eyebrow--font-family);\n    font-weight: var(--eyebrow--font-weight);\n}\n\n.hljs-deletion,\n.hljs-name,\n.hljs-regexp,\n.hljs-selector-class,\n.hljs-selector-id,\n.hljs-tag,\n.hljs-template-variable,\n.hljs-variable {\n    color: #ffa07a;\n    font-family: var(--eyebrow--font-family);\n    font-weight: var(--eyebrow--font-weight);\n}\n\n.hljs-built_in,\n.hljs-link,\n.hljs-literal,\n.hljs-meta,\n.hljs-number,\n.hljs-params,\n.hljs-type {\n    color: var(--pre-text-number);\n    font-family: var(--eyebrow--font-family);\n    font-weight: var(--eyebrow--font-weight);\n}\n\n.hljs-attribute {\n    color: var(--pre-text-attribute);\n    font-family: var(--eyebrow--font-family);\n    font-weight: var(--eyebrow--font-weight);\n}\n\n.hljs-addition,\n.hljs-bullet,\n.hljs-string,\n.hljs-symbol {\n    color: var(--pre-text-string);\n}\n\n.hljs-section,\n.hljs-title {\n    color: #F37243;\n    font-family: var(--eyebrow--font-family);\n    font-weight: var(--eyebrow--font-weight);\n}\n\n.hljs-keyword,\n.hljs-selector-tag {\n    color: var(--pre-text-keyword);\n    font-family: var(--eyebrow--font-family);\n    font-weight: var(--eyebrow--font-weight);\n}\n\n.hljs-emphasis {\n    font-style: italic\n}\n\n.hljs-strong {\n    font-weight: 700;\n    font-family: var(--eyebrow--font-family);\n}\n\n@media screen and (-ms-high-contrast:active) {\n\n    .hljs-addition,\n    .hljs-attribute,\n    .hljs-built_in,\n    .hljs-bullet,\n    .hljs-comment,\n    .hljs-link,\n    .hljs-literal,\n    .hljs-meta,\n    .hljs-number,\n    .hljs-params,\n    .hljs-quote,\n    .hljs-string,\n    .hljs-symbol,\n    .hljs-type {\n        color: highlight\n    }\n\n    .hljs-keyword,\n    .hljs-selector-tag {\n        font-weight: 700\n    }\n}\nTL;DR: Under pressure from the EU’s Digital Markets Act (DMA), Apple is being forced to ditch its proprietary peer-to-peer Wi-Fi protocol – Apple Wireless Direct Link (AWDL) – in favor of the industry-standard Wi-Fi Aware, also known as Neighbor Awareness Networking (NAN). A quietly published EU interoperability roadmap mandates Apple support Wi-Fi Aware 4.0 in iOS 19 and v5.0,1 thereafter, essentially forcing AWDL into retirement. This post investigates how we got here (from Wi-Fi Direct to AWDL to Wi-Fi Aware), what makes Wi-Fi Aware technically superior, and why this shift unlocks true cross-platform peer-to-peer connectivity for developers.EU Forces Apple’s Hand on Peer-to-Peer Wi-FiIn a little-publicized mandate, the European Commission explicitly requires Apple to implement the Wi-Fi Alliance’s Wi-Fi Aware standard as part of DMA interoperability measures. The official DMA roadmap states:“Apple shall implement the measures for Wi-Fi Aware 4.0 in the next major iOS release, i.e. iOS 19, at the latest, and for Wi-Fi Aware 5.0 in the next iOS release at the latest nine months following the introduction of the Wi-Fi Aware 5.0 specification”In plain terms, by the time iOS 19 ships, iPhones must support Wi-Fi Aware v4.0, and Apple must roll out v5.0 support soon after the Wi-Fi Alliance finalizes that spec.Crucially, this decision was not a voluntary announcement by Apple – it was imposed by regulators. Apple has kept quiet about these changes publicly, likely because they involve opening up formerly closed-off tech. The DMA enforcement timeline was highlighted in an EU Q&A site and legal annex, not an Apple press release.7 The European Commission’s language makes it clear this is about enabling third-party devices and apps to use high-bandwidth peer-to-peer (P2P) Wi-Fi features equal to Apple’s own, rather than Apple benevolently adopting a new standard. In fact, the EU order compels Apple to deprecate AWDL and ensure third-party solutions using Wi-Fi Aware are just as effective as Apple’s internal protocols. In short, the EU gave Apple no choice: embrace Wi-Fi Aware or face penalties.What does this mean? Essentially, Apple’s hidden sauce for fast device-to-device communication – AWDL – is being forced into retirement. And with that, for the first time, iPhones and Androids will speak a common language for local wireless networking. Let’s unpack how we got here, and why it’s a big deal for developers.From Wi-Fi Direct to AWDL to Wi-Fi Aware: A Brief HistoryTo understand the significance, we need a quick history of ad-hoc Wi-Fi protocols:Wi-Fi Ad-hoc (IBSS mode): Early 802.11 allowed devices to connect directly in a peer-to-peer “ad-hoc” network (IBSS), but it had limitations (no always-on discovery, no power-saving coordination, weak security). It never gained widespread use.Wi-Fi Direct: The Wi-Fi Alliance’s first big attempt at standard P2P. Wi-Fi Direct (circa 2010) allows devices to form a direct link without an AP, designating one device as a group owner (soft AP) for security and IP allocation. It improved on ad-hoc mode (supporting WPA2, dynamic group formation), but had drawbacks – e.g. limited service discovery capabilities and difficulty staying connected to infrastructure Wi-Fi concurrently.Apple Wireless Direct Link (AWDL): Around 2014, Apple developed AWDL as a proprietary, high-performance P2P Wi-Fi protocol for its ecosystem. According to Apple’s patent on AWDL (US20180083858A1) and reverse-engineering by researchers, AWDL was designed to address Wi-Fi Direct’s concerns and succeeded ad-hoc IBSS mode.8 Apple deployed AWDL in over a billion devices (every modern iPhone, iPad, Mac) to power AirDrop, AirPlay peer connections, GameKit, Apple Watch unlock, and more.8,9 Notably, AWDL can coexist with regular Wi-Fi by rapidly hopping channels – an iPhone can be on an AP and seamlessly switch to AWDL channel windows to talk to a peer.9 This gave AWDL low latency and high throughput without dropping your internet connection.Neighbor Awareness Networking (NAN / Wi-Fi Aware): As it turns out, Apple didn’t keep all of AWDL to itself – it contributed to the Wi-Fi Alliance, which adopted AWDL’s approach as the basis for the NAN standard (branded “Wi-Fi Aware”) around 2015.8 Wi-Fi Aware is essentially the industry-standard cousin of AWDL, enabling devices to discover each other and communicate directly with Wi-Fi speeds, in a power-efficient way, regardless of vendor. Android added platform support for Wi-Fi Aware in Oreo (8.0) and later,10 but Apple until now stuck with its in-house AWDL stack which can be used by developers but isn't an open standard.In summary, AWDL was Apple’s competitive edge – a proprietary P2P stack that outperformed legacy Wi-Fi Direct and only worked on Apple devices. If an app needed cross-platform local connectivity, it couldn’t use AWDL (Apple provides no raw AWDL API). Developers resorted to Wi-Fi Direct, or Wi-Fi Aware on Android vs. Apple’s AWDL on iOS, with no interoperability. This fragmentation is exactly what the EU’s DMA targeted.The DMA order effectively forces Apple to drop AWDL and align with Wi-Fi Aware. The Commission explicitly says Apple must“implement Wi-Fi Aware in iOS devices in accordance with the Wi-Fi Aware specification” and “continue to…improve the Wi-Fi Aware standard… Apple shall not prevent AWDL from becoming part of the Wi-Fi Aware standard”,even urging Apple to allocate memory for concurrent P2P on older devices in a non-discriminatory way until AWDL is fully deprecated.The writing is on the wall: AWDL as a private protocol is done for.Inside AWDL: Apple’s Once-Secret Peer-to-Peer ProtocolAWDL is worth a closer look, because it shows what Apple achieved and what will now be opened up via Wi-Fi Aware. How does AWDL work? In short, it creates a continuously syncing ad-hoc network on the fly among nearby Apple devices:Availability Windows & Channel Hopping: Each AWDL-enabled device periodically advertises Availability Windows (AWs) – tiny time slices when it’s available on a specific Wi-Fi channel for peer-to-peer communication.8 An elected master node (chosen via a priority scheme) coordinates these windows across devices. Outside of these AWs, devices can rejoin normal Wi-Fi (e.g. your home router’s channel) or sleep their radio to save power.8 This scheduling is what allows, let's say, your Mac to be on Wi-Fi for internet most of the time, but briefly switch to channel 6 to AirDrop a file from your iPhone, then switch back – all without manual intervention.Integration with BLE: AWDL doesn’t work in isolation – it integrates with Bluetooth Low Energy for discovery. For example, AirDrop uses BLE advertisements to initially discover nearby devices (showing them in the UI), then quickly forms an AWDL connection for the actual high-speed file transfer. This combo gives the best of both: BLE’s low-power device discovery and AWDL’s high-throughput data channel.11,12Performance: AWDL leverages the full Wi-Fi PHY, so it can hit hundreds of Mbps throughput and sub-second latencies that BLE or classic Bluetooth can’t touch. It also supports robust security (authenticated pairing, encryption) as used in AirDrop/AirPlay. One clever feature: because AWDL devices coordinate their availability, one device can even sustain multiple P2P links concurrently (e.g. an iPhone streaming to a HomePod via AWDL while also AirDropping to a Mac) – something spelled out in the EU requirements.Closed Nature: Despite its capabilities, AWDL has been closed off to third-party developers and other OSes. Apple’s APIs like MultipeerConnectivity framework ride on AWDL under the hood for Apple-to-Apple connections, but there was no way for an Android device or a Windows laptop to speak AWDL. It was an Apple-only club. Researchers at TU Darmstadt’s Secure Mobile Networking Lab had to reverse-engineer AWDL (publishing an open Linux implementation called OWL) to document its inner workings.13 They demonstrated that AWDL indeed is an IEEE 802.11-based ad-hoc protocol with Apple-specific extensions, tightly integrated with Apple’s ecosystem.14 Bottom line: AWDL gave Apple a technical edge but at the cost of interoperability – a classic “walled garden” approach.It’s this walled garden that the EU is breaking down. The mandate that “Apple shall make Wi-Fi Aware available to third parties” means Apple must expose new iOS APIs for P2P connectivity that are standard-based. And since Android (and even some IoT devices) already support Wi-Fi Aware, we’re headed for a world where an iPhone and an Android phone can find and connect to each other directly via Wi-Fi, no access point, no cloud, no hacks – a scenario that AWDL alone never allowed.Wi-Fi Aware 4.0: The New Cross-Platform StandardSo what exactly is Wi-Fi Aware (a.k.a. NAN), and why is version 4.0 a game-changer? At a high level, Wi-Fi Aware offers the same kind of capabilities as AWDL, but as an open standard for any vendor. It lets devices discover each other and exchange data directly via Wi-Fi, without needing a router or cell service. Think of it as Wi-Fi’s answer to Bluetooth discovery but with Wi-Fi speed and range. Some key technical features of Wi-Fi Aware (especially in the latest v4.0 spec) include:Continuous, Efficient Discovery: Devices form a Wi-Fi Aware group and synchronize wake-up times to transmit Discovery Beacons. Like AWDL’s AWs, Wi-Fi Aware defines Discovery Windows where devices are active to find peers, then can sleep outside those windows to save power. This allows always-on background discovery with minimal battery impact.15 The latest spec enhances this with an “Instant Communication” mode – a device can temporarily accelerate discovery (e.g. switch to a channel and beacon rapidly) when triggered by an external event like a BLE advertisement or NFC tap, to achieve very fast discovery and connection setup.16 In practice, that means an app can use BLE to wake up Wi-Fi (advertising a service via BLE then negotiating a NAN link), combining the energy efficiency of BLE with the speed of Wi-Fi – just as Apple’s AirDrop has done privately. Wi-Fi Aware v4.0 explicitly added standardized BLE co-operation: “Latest enhancements to Wi-Fi Aware offer discovery by Bluetooth LE, which triggers a formal Wi-Fi Aware session by waking the Wi-Fi radio.”10High Throughput Data & Range: Once devices discover each other, Wi-Fi Aware supports establishing a direct Wi-Fi data path. This can be an IP connection or a native transport, and it leverages Wi-Fi’s high data rates (including Wi-Fi 5/6/6E speeds on 5 GHz or 6 GHz bands). In fact, the Wi-Fi Alliance notes that Wi-Fi Aware data connections use “high performance data rates and security, leveraging cutting-edge Wi-Fi technologies, including Wi-Fi 6, Wi-Fi 6E, and WPA3.” 10 Compared to Bluetooth or BLE, the throughput and range are vastly superior – Wi-Fi Aware can work at typical Wi-Fi ranges (tens of meters, even over 100m in open air) and deliver tens or hundreds of Mbps. By contrast, BLE might get 100+ meters but on the order of 0.1 Mbps in real-world throughput. Wi-Fi Aware will close that gap by giving cross-platform apps both long range and high speed.Lower Latency & Instant Communication: Version 4.0 of the spec introduced refinements for latency-critical applications. The aforementioned Instant Communication mode lets devices expedite the discovery handshake – important for use cases like AR gaming or urgent data sync where waiting a few seconds for a discovery window might be too slow. In Instant mode, a device (say, an AR headset) triggered via BLE could immediately switch to a predetermined channel and begin a quick service discovery exchange with a peer, rather than strictly waiting on the periodic timetable.16 The spec shows this can cut discovery latency dramatically (Figure 73 in the spec illustrates an accelerated discovery).16 From a developer’s perspective, Wi-Fi Aware can feel nearly instantaneous in establishing a link when properly used.Accurate Ranging: Perhaps one of the most exciting features for version 4 and beyond is built-in distance measurement between devices. Wi-Fi Aware includes a ranging protocol (based on Fine Timing Measurement, FTM) that lets one device get the distance to another with sub-meter accuracy.15 This is similar to how Apple devices can use UWB or Bluetooth RTT for ranging, but now via Wi-Fi. The devices exchange precise timing signals to calculate distance (and even do so as part of discovery – a NAN discovery packet can include a request to measure range). The spec’s NAN Ranging section defines how devices negotiate a ranging session and obtain a distance estimate before or during data exchange.16 Enhanced ranging could unlock things like peer-to-peer localization (for example, an app can find not just who is nearby but also roughly how far or even what direction).Security and Privacy: Wi-Fi Aware has baked-in solutions for secure communication and privacy. It supports device pairing (establishing trust and keys) and encrypted data paths with mutual authentication.15 It also provides privacy features like randomized identifiers that rotate, so devices aren’t broadcasting a fixed MAC or identity constantly.10 This addresses the concern that always-on discovery could be used to track devices – Aware can randomize its “NAN IDs” and only reveal a stable identity when a trusted handshake occurs. The EU mandate will require Apple to expose the same security levels to third-party developers as it uses for its own devices, meaning things like AirDrop’s peer authentication should extend to third-party Aware sessions.In essence, Wi-Fi Aware 4.0 is AWDL on steroids and open to all. It took the concepts Apple pioneered (timeslot synchronization, dual Wi-Fi/BLE use, etc.) and formalized them into a cross-vendor standard, adding improvements along the way. No longer limited to Apple devices, any Wi-Fi Aware certified device can join the discovery clusters and connect. With iOS 19, an iPhone will become just another Wi-Fi Aware node – able to discover and connect to Android phones, PCs, IoT gadgets, etc., directly via Wi-Fi.AWDL vs. Wi-Fi Aware vs. BLE: Feature ComparisonHow does Apple’s AWDL, the upcoming Wi-Fi Aware, and good old Bluetooth Low Energy stack up? The table below summarizes the key differences and capabilities of these peer-to-peer wireless technologies:\n.table_component {\n    overflow: auto;\n    width: 100%;\n}\n\n.table_component table {\n    border: 1px solid #dededf;\n    height: 100%;\n    width: 100%;\n    table-layout: fixed;\n    border-collapse: collapse;\n    border-spacing: 1px;\n    text-align: left;\n}\n\n.table_component caption {\n    caption-side: top;\n    text-align: left;\n}\n\n.table_component th {\n    border: 1px solid #dededf;\n    background-color: #eceff1;\n    color: #000000;\n    padding: 5px;\n}\n\n.table_component td {\n    border: 1px solid #dededf;\n    background-color: #ffffff;\n    color: #000000;\n    padding: 5px;\n}\n\n            Feature\n            Apple AWDL (Proprietary)\n            Wi-Fi Aware 4.0 (2022 Spec)\n            Bluetooth LE (5.x)\n\n            Standardization\n\n                Apple-defined (private protocol)\n\n                Wi-Fi Alliance NAN standard\n\n                Bluetooth SIG standard\n\n            Topology\n\n                Mesh networking. Multiple devices in a cluster. One acts as a time sync master.\n\n                Decentralized cluster (no fixed master). Typically one-to-one data links, but multiple links supported.\n\n                Point-to-point or star (one-to-many, each connection 1:1). No native mesh routing.\n\n                Discovery Mechanism\n\n                AWDL frames (Wi-Fi beacons), BLE-assisted initial discovery (e.g., AirDrop).\n\n                Publish/Subscribe discovery with NAN frames. Supports out-of-band BLE wake-up for power saving.\n\n            BLE Advertising channels, low-power continuous advertising, and scanning.\n\n                Initial Connection Latency\n\n                Very fast (<1s) using BLE assist (AirDrop). Quick AWDL link setup.\n\n                Fast (<1s typical) discovery, tens of ms connection setup after discovery.\n\n                Fast discovery (~0.5–1s). Connection establishment latency (50–100 ms).\n\n                Data Throughput\n\n                High – 160–320 Mbps real-world (AirDrop). Wi-Fi 5/6 speeds.\n\n                High – 100+ Mbps real-world on Wi-Fi 5 hardware, 250+ Mbps possible on Wi-Fi 6.\n\n                Low – Max ~1.36 Mbps app throughput (BLE 5), typically 0.2–0.5 MB/s.\n\n            Range\n\n                ~50–100m typical Wi-Fi range. 100m+ line-of-sight.\n\n                ~50–100m typical Wi-Fi range, similar to AWDL.\n\n                Up to 100–200m typical; max ~1km line of sight with BLE 5 long-range (coded PHY).\n\n                Concurrent Internet\n\n                Yes – simultaneous infrastructure Wi-Fi and P2P via channel hopping.\n\n            Yes – NAN discovery windows are scheduled around AP connectivity. Coexistence supported.\n\n                Yes – BLE separate from Wi-Fi, runs in parallel.\n\n                Notable Features\n\n                Proprietary; Powers AirDrop/AirPlay; Mesh with master; No direct public API (apps use Multipeer Connectivity).\n\n                Open standard; Flexible discovery; Instant messaging; Built-in secure data path setup; Android API since 2017.\n\n                Universally supported; Extremely energy-efficient; Background presence detection; Limited data rate. Often combined with Wi-Fi for bulk transfer.\n\n(Note: Above ranges and throughput are based on Ditto’s real-world tests and specification data. Bluetooth 5's theoretical 4x range increase can reach ~400m line-of-sight, typical usable range 100–200m indoors. Wi-Fi range varies significantly with the environment.)As the table shows, Wi-Fi Aware (NAN) and AWDL are closely matched in capabilities – no surprise, given their kinship. Both vastly outperform Bluetooth LE for high-bandwidth applications, though BLE remains invaluable for ultra-low-power needs and simple proximity detection. The sweet spot that AWDL and Aware occupy is: fast, local data exchange (from tens of megabits up to hundreds) over distances of a room or building floor, without requiring any network infrastructure. This is why forcing Apple to support Wi-Fi Aware is so pivotal – it means an iPhone and an Android phone sitting next to each other can finally establish a fast, direct Wi-Fi link without an access point, something that was previously impossible (because the iPhone would only speak AWDL, and the Android only Wi-Fi Aware/Wi-Fi Direct). In effect, the EU is unifying the table’s middle column (“Wi-Fi Aware”) across the industry, and pushing the proprietary AWDL column toward obsolescence.A Glimpse of Wi-Fi Aware 5.0 – What’s Next?The EU is already looking ahead to Wi-Fi Aware 5.0, mandating Apple support it when available. While v5.0 is still in the works, we can speculate based on industry trends and draft discussions:Better Interoperability & Backwards Compatibility: Each iteration of Aware aims to bring improvements while remaining backward compatible. v5.0 will likely fine-tune the interaction between different versions (e.g. allowing a v5 device to gracefully communicate with a v4 device at a slightly reduced feature set).Multi-Band and Wi-Fi 7 Enhancements: With Wi-Fi 7 (802.11be) emerging, v5.0 could incorporate support for Multi-Link Operation (MLO) – allowing Aware devices to use multiple bands or channels simultaneously for P2P, increasing reliability and throughput. It might also embrace new PHY capabilities like 320 MHz channels in 6 GHz or even integration of the 60 GHz band for ultra-high throughput at short range. Imagine a future Aware where two devices use 6 GHz for discovery and 60 GHz for a quick gigabit data burst.Improved Ranging and Location: Wi-Fi Aware might leverage Wi-Fi 7’s improved location features or even integrate with UWB. v5.0 could offer finer distance measurement or angle-of-arrival info by coordinating multiple antennas, which would interest AR/VR use cases and precise indoor positioning.Extended Mesh Networking: Currently, Aware focuses on finding peers and setting up links; v5.0 might add more mesh networking primitives – e.g., forwarding data through intermediate nodes or coordinating groups of devices more intelligently. This could turn clusters of phones into true mesh networks for group connectivity without infrastructure.Security Upgrades: Each version updates security. v5.0 will likely address any weaknesses found in v4, perhaps adding quantum-resistant encryption for pairing or tighter integration with device identity frameworks. Given Apple’s emphasis on privacy, expect them to push for features that allow secure sharing of connection metadata with third parties without exposing user data.We’ll know for sure once the Wi-Fi Alliance releases the Wi-Fi Aware 5.0 spec, but the direction is clear: faster, farther, and more seamless peer-to-peer connectivity. And importantly, Apple will be on board from day one (not years late as it was with previous standards).Wi-Fi Aware in Action: Android Kotlin ExampleTo illustrate how developers can use Wi-Fi Aware, let’s look at a simplified real-world example on Android. Below is Kotlin code demonstrating a device publishing a service and handling a message from a subscriber. (Android’s Wi-Fi Aware API is available from API level 26; one must have location and “Nearby Wi-Fi Devices” permissions, and the device must support Aware.)val wifiAwareMgr = context.getSystemService(Context.WIFI_AWARE_SERVICE) as WifiAwareManager\n\nif (!wifiAwareMgr.isAvailable) {\n    Log.e(\"WiFiAwareDemo\", \"Wi-Fi Aware not available on this device.\")\n    return\n}\n\n// Attach to the Wi-Fi Aware service\nwifiAwareMgr.attach(object : AttachCallback() {\n    override fun onAttached(session: WifiAwareSession) {\n        // Once attached, we can publish or subscribe\n        val publishConfig = PublishConfig.Builder()\n            .setServiceName(\"com.example.p2pchat\")    // Name of our service\n            .build()\n\n        session.publish(publishConfig, object : DiscoverySessionCallback() {\n            override fun onPublishStarted(pubSession: PublishDiscoverySession) {\n                Log.i(\"WiFiAwareDemo\", \"Service published, ready for subscribers.\")\n            }\n\n            override fun onMessageReceived(\n                session: DiscoverySession,\n                peerHandle: PeerHandle,\n                message: ByteArray\n            ) {\n                val msgStr = String(message, Charsets.UTF_8)\n                Log.i(\"WiFiAwareDemo\", \"Received message from subscriber: $msgStr\")\n                // Here we could respond or establish a data path if needed\n            }\n        }, null)\n    }\n\n    override fun onAttachFailed() {\n        Log.e(\"WiFiAwareDemo\", \"Failed to attach to Wi-Fi Aware session.\")\n    }\n}, null)\nIn this code, the app attaches to the Wi-Fi Aware service, then publishes a service named \"com.example.p2pchat\". When a peer subscribes and sends us a message (for example, “Hello from subscriber”), it arrives in onMessageReceived. A subscriber device would perform complementary steps: calling session.subscribe(...) with the same service name and implementing onServiceDiscovered to detect the publisher, then possibly using subscribeSession.sendMessage(peer, ...) to send that “Hello.” At that point, either side could then use WifiAwareSession.createNetworkSpecifier() to set up an actual data path (network interface) for larger communication.The key takeaway is that Wi-Fi Aware makes peer discovery and messaging a first-class citizen in the API, abstracting away the low-level Wi-Fi fiddling. The app developer just provides a service name and gets callbacks when peers appear or messages arrive.(Note: The above is a minimal example. In a real app, you’d handle permissions, check for support via PackageManager.FEATURE_WIFI_AWARE, and probably use the new NEARBY_WIFI_DEVICES permission on Android 13+. Also, establishing a full data path would involve requesting a Network from ConnectivityManager with a network specifier from the Aware session.)Immediately after Google announced Wi-Fi Aware in Android, we at Ditto realized its potential for seamless peer-to-peer sync. As shown above, you can certainly roll your own discovery and data exchange with Aware. However, not every developer will want to manage these details or deal with corner cases of connectivity. That’s why Ditto’s real-time sync SDK is integrating Wi-Fi Aware support out-of-the-box.Our upcoming releases will automatically use Wi-Fi Aware in iOS under the hood for nearby devices, enabling peer-to-peer database synchronization and binary file sharing between iOS and Android with zero configuration. In practical terms, if you build your app with Ditto, two devices in proximity will be able to find each other and sync data directly (bypassing cloud or LAN) using the fastest available transport – now including Wi-Fi Aware alongside Bluetooth, AWDL, LAN, etc.Cross-platform, edge-first applications (collaborative apps, offline-first data stores, local IoT networks) will significantly benefit from this, as devices will form a local mesh that syncs instantly and reliably, even if the internet is down. Ditto’s approach has always been to multiplex multiple transports (Wi-Fi infrastructure, P2P, BLE, etc.) for robustness; adding NAN support supercharges the bandwidth available for nearby sync sessions.A concrete example: Consider an app for first responders that shares maps and live sensor data among a team in the field. With Wi-Fi Aware, an Android tablet, an iPhone, and a specialized helmet device could all auto-discover each other and form a mesh to sync mission data in real-time without any network. Previously, if the iPhone had an app using AWDL, it couldn’t directly connect to the Android tablet’s Wi-Fi Aware session – they were incompatible silos. Now, they’ll speak one language, making such scenarios truly feasible.Bigger Picture: The Dawn of True Cross-Platform Mesh NetworkingApple’s reluctant adoption of Wi-Fi Aware marks a pivot point for device connectivity. For years, we’ve seen a split: Apple’s ecosystem “Just Works” within itself (thanks to AWDL, AirDrop, etc.), while other platforms muddled along with standards that never quite matched the seamlessness or performance. That left cross-platform interactions hamstrung – the experience of sharing something between an iPhone and an Android was far from instant or easy.With iOS supporting Wi-Fi Aware, we’re essentially witnessing AWDL go open. The proprietary tech that powered some of Apple’s most magical features will now be available in an interoperable way to any developer. The implications are significant:End of the Proprietary P2P Divide: No more need for parallel implementations. Developers won’t have to build one system using MultipeerConnectivity for iOS-to-iOS and another using Wi-Fi Aware or Wi-Fi Direct for Android-to-Android. They can use Wi-Fi Aware universally for nearby networking. This reduces development complexity and encourages building features that work on all devices, not just within one brand.Cross-Platform AirDrop and Beyond: We will likely see apps (or OS-level features) that enable AirDrop-like functionality between iOS and Android. Google’s Nearby Share and Samsung’s Quick Share could potentially become interoperable with Apple’s implementation now that the underlying protocol is shared. The user experience barrier between ecosystems could start to blur in local sharing scenarios.Mesh and Edge Computing Potential: If many devices can seamlessly form ad-hoc networks, this enables new paradigms in edge computing. Clusters of phones could share workload or content directly. For example, at a conference, a presenter’s laptop could broadcast slides via Wi-Fi Aware to all audience phones without internet. Or a fleet of drones could coordinate via Aware when out of range of a base station. The offline mesh becomes a first-class citizen.Competitive Innovation: The EU’s push here also sets a precedent – even giants like Apple must conform to interoperability on critical features. This may drive Apple (and others) to innovate on top of the standards rather than via proprietary lock-in. We might see Apple contribute more actively to Wi-Fi Aware’s future improvements (as required by the DMA) to ensure it meets their needs for things like AR/VR data streams. That collaboration could yield better tech for everyone, faster.One can’t ignore the irony that the Wi-Fi Aware standard is effectively a child of AWDL. Now the child comes back to replace its parent. From a technical perspective, this is a win for engineering elegance – it’s always cleaner to have one agreed-upon protocol rather than parallel ones. From a developer perspective, it’s a huge win for interoperability and user reach.Apple will undoubtedly ensure that the transition doesn’t degrade the experience for Apple-to-Apple interactions; the DMA even mandates that third-party access be “equally effective” as Apple’s own solutions. That means as developers, we should expect the new iOS 19 Wi-Fi Aware APIs to give us essentially what AWDL gave Apple’s apps. It’s like being handed the keys to a supercar that was previously locked in Apple’s garage.ConclusionThe EU’s crackdown on Apple’s closed ecosystems is catalyzing a long-awaited unification in short-range wireless technology. By compelling Apple to adopt Wi-Fi Aware, the Digital Markets Act is effectively forcing the end of AWDL as an exclusive domain. For developers and users, this is exciting news: soon your apps will be able to use high-speed peer-to-peer Wi-Fi on iPhones and have it talk to other platforms seamlessly. We’ll likely see an explosion of innovative uses for local connectivity – from truly universal AirDrop alternatives to cross-platform local multiplayer games, ad-hoc collaborative editing, IoT device commissioning, and beyond – no specialized hardware or router required.At a technical level, AWDL will be remembered as an ahead-of-its-time solution that proved what was possible, and Wi-Fi Aware ensures those capabilities are broadly available as an industry standard. With Wi-Fi Aware 4.0 on the cusp of ubiquity (and 5.0 on the horizon), we are entering a new era of frictionless sharing and syncing among devices in physical proximity. It’s a win for interoperability and a win for innovation in peer-to-peer networking. The walls around AWDL are coming down – and the implications for edge computing and offline experiences are profound.‍Sources:[1] European Commission – DMA Decisions on Apple Interoperability (Q&A) – High-bandwidth P2P Wi-Fi (Wi-Fi Aware 4.0 in iOS 19, Wi-Fi Aware 5.0 next). (2025) (Interoperability - European Commission)[2] The Apple Wiki – Apple Wireless Direct Link (AWDL) – Proprietary mesh protocol introduced in iOS 7 (2014) for AirDrop/Continuity. (Apple Wireless Direct Link - The Apple Wiki) (Apple Wireless Direct Link - The Apple Wiki)[3] ZDNet – Apple’s AWDL protocol plagued by flaws… – Research note: “NAN (Wi-Fi Aware) is a new standard supported by Android which draws on AWDL’s design.” (Nov 2019) (Apple's AWDL protocol plagued by flaws that enable tracking and MitM attacks | ZDNET)[4] Android AOSP Documentation – Wi-Fi Aware feature (Neighbor Awareness Networking) – Added in Android 8.0; supports discovery, connection, and ranging (added in Android 9). (Wi-Fi Aware | Android Open Source Project)[5] Nordic Semiconductor – Bluetooth Range Compared – Bluetooth 5 LE offers up to ~400 m range (4× vs BLE4), 2 Mbps PHY, ~1.36 Mbps application throughput. (Things You Should Know About Bluetooth Range)[6] Computerworld – Coming soon: Faster, longer-range Bluetooth 5 – “In clear line of sight, Bluetooth 5 range could stretch to 400 meters,” (2016)[7] BGR -- iOS 19 Features Coming to EU -- Details new features for EU iPhones including high-bandwidth P2P Wi-Fi, sideloading, and alternative app stores (March 2025) (8 Exclusive iOS 19 Features Coming to EU iPhone Users)[8] Open Wireless Link Wiki - What is Apple Wireless Direct Link (AWDL) -- Apple’s patent on AWDL (US201800838) and origins as a successor to Wi-FI IBSS (Wiki | Open Wireless Link)[9] CyberHoot – Apple Wireless Direct Link (AWDL) – Apple deployed AWDL in over billion devices to power AirDrop, AirPlay peer Connections, and more (2002) (Apple Wireless Direct Link (AWDL) - CyberHoot)[10] Wi-Fi Alliance – Wifi Aware – Android added platform support for Wi-Fi Aware in Oreo (8.0) and later (Wi-Fi Aware | Wi-Fi Alliance)[11] Usenix Association – A billion Open Interfaces for Eve and Mallory: MitM, DoS, and Tracking ATtacks on iOS and macOS Through Apple Wireless Direct Link – AWDL integrates with Bluetooth Low Energy (A Billion Open Interfaces for Eve and Mallory: MitM, DoS ... - USENIX)[12] Octet Stream – Building Cross Platform Offline - First Apps with Bluetooth Low Energy - Integration with Bluetooth Low Energy (May 2024) (Building Cross-Platform Offline-First Apps with Bluetooth Low Energy).[13] Open Wireless Link – Code – Linux Implementation called OWL (Code | Open Wireless Link)[14] Secure Mobile Networking Lab (SEEMOO) -- Apple Wireless Direct Link (AWDL) and Secure Device Communications – AWDL is a based ad-hoc protocol with Apple-specific extensions integrated into Apple’s ecosystem (Matthias Hollick – Secure Mobile Networking Lab)[15] WiFi Alliance – Wi-Fi CERTIFIED Wi-Fi Aware Technology Overview – Wi-Fi Aware always-on background discovery with power efficiency (2002) (Wi-Fi CERTIFIED Wi-Fi Aware™ Technology Overview (2022) | Wi-Fi Alliance)[16] WiF Alliance – Wi-Fi Aware Specification v4.0 – Detailed Specification for Wi-Fi Aware technology (2022) (Wi-Fi Aware Specification v4.0.pdf‍SUBSCRIBEGet posts straight in your inboxSubscribe to updates and we'll send you occasional emails with posts that we think you'll like.\n  hbspt.forms.create({\n    portalId: \"4836182\",\n    formId: \"b8668664-695d-40ab-a974-db7a6522dbea\",\n    region: \"na1\"\n  });\nEmail*utm_sourceutm_mediumutm_campaignutm_termutm_content\n\nRead moreView All ArticlesView All ArticlesView All ArticlesProductMarch 19, 2025Introducing Ditto 4.10: More Power, More Control, and New Platform SupportbySkyler JokielOur latest Ditto 4.10 SDK release brings significant improvements, giving developers more flexibility, better performance, and support for new platforms. March 12, 2025Ditto Lands $82M to Pioneer the Edge-Native RevolutionbyRyan RatnerThe future of computing isn’t in the cloud - it’s at the edge. And with this latest funding round, we’re poised to make Ditto the new standard for edge development.\n\nul.footer_links_wrap>li.footer_links_item { transition: opacity 400ms ease, transform 400ms ease; }\nul.footer_links_wrap:has(li.footer_links_item:hover)>li.footer_links_item:not(:hover) { opacity: 0.5; }\nul.footer_links_wrap:has(li.footer_links_item:hover)>li.footer_links_item:hover { transform: translateX(0.5em); }\nResilient Edge Device ConnectivityServers and Cloud, OptionalStart for freeStart for freeStart for freeSchedule a DemoSchedule a DemoSchedule a Demo© 2025 DittoLive IncorporatedAll rights reserved.CompanyPlatformAbout UsOur CustomersPricingCareersResourcesBlogDEMOAPPSIn The NewsPress ReleasesMake a ReportSocialsLinkedinGithubTwitter / XLegalTerms of ServicePrivacy PolicyCookie PolicyDPA",
    "summary": {
      "en": "The European Union (EU) is requiring Apple to replace its proprietary peer-to-peer Wi-Fi protocol, Apple Wireless Direct Link (AWDL), with the industry-standard Wi-Fi Aware, also known as Neighbor Awareness Networking (NAN). This change is mandated by the EU's Digital Markets Act (DMA), which aims to improve interoperability among devices. \n\nKey points include:\n\n1. **Forced Transition**: Apple must implement Wi-Fi Aware 4.0 in iOS 19 and follow up with Wi-Fi Aware 5.0 shortly after its release. This transition is not voluntary but a regulatory requirement.\n\n2. **Significance of Wi-Fi Aware**: Wi-Fi Aware allows devices from different manufacturers to discover and communicate directly without needing a router or internet connection, enhancing cross-platform connectivity.\n\n3. **History of Wi-Fi Protocols**:\n   - **Wi-Fi Direct**: An earlier standard that allowed devices to connect directly but had limitations.\n   - **AWDL**: Developed by Apple for its devices, providing high performance but lacking cross-platform functionality.\n   - **Wi-Fi Aware**: An open standard based on concepts from AWDL, enabling better interoperability and power efficiency.\n\n4. **Advantages of Wi-Fi Aware**:\n   - Continuous and efficient device discovery.\n   - High data throughput and range.\n   - Lower latency and instant communication capabilities.\n   - Built-in security features.\n\n5. **Impact on Developers**: The shift to Wi-Fi Aware will simplify development for cross-platform applications, allowing for easier implementation of features like peer-to-peer file sharing, similar to Apple’s AirDrop, across both iOS and Android devices.\n\n6. **Future Innovations**: The EU's mandate may lead to more advancements in peer-to-peer networking, including potential improvements in future versions of Wi-Fi Aware.\n\nOverall, this change represents a significant step towards unifying device connectivity standards, enabling better collaboration and communication between different platforms.",
      "ko": "유럽연합(EU)은 애플에게 자사의 독점적인 피어 투 피어 Wi-Fi 프로토콜인 애플 무선 직접 링크(AWDL)를 업계 표준인 Wi-Fi Aware, 즉 이웃 인식 네트워킹(NAN)으로 교체할 것을 요구하고 있습니다. 이 변화는 EU의 디지털 시장법(DMA)에 의해 의무화되었으며, 이는 기기 간의 상호 운용성을 개선하는 것을 목표로 하고 있습니다.\n\n애플은 iOS 19에서 Wi-Fi Aware 4.0을 구현해야 하며, 그 후 곧바로 Wi-Fi Aware 5.0으로 업데이트해야 합니다. 이 전환은 자발적인 것이 아니라 규제 요구사항입니다.\n\nWi-Fi Aware는 서로 다른 제조사의 기기가 라우터나 인터넷 연결 없이 직접 발견하고 통신할 수 있게 해주어, 플랫폼 간 연결성을 향상시킵니다. 이전의 Wi-Fi Direct는 기기가 직접 연결될 수 있게 해주는 표준이었지만 한계가 있었습니다. AWDL은 애플이 자사 기기를 위해 개발한 것으로 높은 성능을 제공하지만, 플랫폼 간 기능이 부족했습니다. Wi-Fi Aware는 AWDL의 개념을 바탕으로 한 개방형 표준으로, 더 나은 상호 운용성과 전력 효율성을 제공합니다.\n\nWi-Fi Aware의 장점으로는 지속적이고 효율적인 기기 발견, 높은 데이터 전송 속도와 범위, 낮은 지연 시간 및 즉각적인 통신 기능, 내장된 보안 기능이 있습니다. Wi-Fi Aware로의 전환은 크로스 플랫폼 애플리케이션 개발을 간소화하여, 애플의 에어드롭과 유사한 피어 투 피어 파일 공유 기능을 iOS와 안드로이드 기기 모두에서 쉽게 구현할 수 있게 합니다.\n\nEU의 요구는 피어 투 피어 네트워킹의 발전을 이끌어낼 수 있으며, 향후 Wi-Fi Aware의 개선 가능성도 열어줍니다. 이러한 변화는 기기 연결 표준을 통합하는 중요한 단계로, 서로 다른 플랫폼 간의 협업과 통신을 더욱 원활하게 할 수 있게 합니다.",
      "ja": null
    }
  },
  {
    "id": "7265932f6ff4bdeb",
    "title": {
      "en": "Optimizing Matrix Multiplication on RDNA3",
      "ko": "RDNA3 행렬 곱셈 최적화",
      "ja": null
    },
    "type": "story",
    "url": "https://seb-v.github.io/optimization/update/2025/01/20/Fast-GPU-Matrix-multiplication.html",
    "score": 18,
    "by": "skidrow",
    "time": 1742896521,
    "content": "Introduction\n\nHi everyone !\n\nIn this post, I will share with you all the steps to write an optimized FP32 matrix multiplication on AMD RDNA3 GPU outperforming rocBLAS by 60%. I will cover some basics and explain all the optimizations I have implemented. This will be done in a iterative way in 8 differents Kernels.\n\n  Figure 1: sneak peek of the performance results\n\nI primary intended to work on this to deepen my understanding of RDNA3 and try out HIP and I felt like I needed to share what I learned doing this :).\n\nFew things I like to say before we start :\n\n  All the information I used comes from the publicly available ISA guide1\n  I don’t intend to re-implement or replace rocBLAS\n  I only focused on 4096x4096 matrices single precision (FP32) matrix multiplication for the sake of simplicity.\n  All my tests were done on Windows 11 with a AMD Radeon 7900 XTX.\n\nThat being said, let’s start !\n\nProblem statement\n\nThere is a lot of research happening on the way to improve the performance of matrix multiplication nowadays. Being a core algorithm in ML applications, any FLOPS we can exploit is golden.\n\nBefore proceeding, let’s recall the basics of matrix multiplication. Given two matrices:\n\n  A of size M,K\n  B of size K,N\n\nTheir product, C, is computed as follows:\n\nCij=∑k=0K−1Aik⋅Bkj\n\ni∈[0,M−1]\nj∈[0,N−1]\n\nwhere C is the resulting matrix of size M,N.\n\nFor each output value of matrix C, we compute the dot product between the rows of matrix A and the columns of matrix B.\n\n  Figure 2: example for the first element of C\n\nIn terms of complexity, we have O(n3) computational complexity and O(n2) memory accesses.\nIf we don’t think about architectural details, this is clearly a compute bound problem and our goal will be to be compute bound on the GPU.\n\nLet’s say we manage to write the best implementation possible for the 7900 XTX. How fast could it run ? To answer this questions we need to look a bit at RDNA3 architecture.\n\nRDNA3 GPUs are made of arrays of WorkGroup Processors (WGP). Every WGP are split into 2 Compute Units (CUs), themself split into 2 SIMDs. A SIMD handles the work of multiple threads organized in waves (or warps for CUDA folks) and has a set of components to do some work (like arithmetic operations). For Floating point operations, there are two 32 way VALU units.\n\n  Figure 3: simplified representation of WGPs\n\n  Figure 4: simplified representation of a single SIMD\n\nWe can compute our theoritical floating point operation per second with this formula:\n\nFLOPS=freq∗nbSIMD∗flopsPerSIMD\n\nEvery SIMD can issue 2 Floating points intructions per cycle (one on each vALU unit). If we use FMA instructions (Fused Multiply Add), each SIMD can issue 32∗2∗2=128 floating point operations per cycle.\nThe 7900 XTX has 48 WGPs, that’s 48∗2∗2=192 SIMDs.\n\nFLOPS=2500∗106∗192∗128FLOP/s\n\nFLOPS=61.44TFLOP/s\n\nOur theoritical VRAM bandwidth is given by :\n\nBW=rate∗busWidth/8\n\nThe 7900 XTX uses GDDR6 with a 384-bit bus running at 20 Gbps.\n\nBW=20∗384/8=960GB/s\n\nIf we go back to our 4096x4096 matrix multiplication, we essentially need to do 2∗4096∗4096∗4096 operations.\nWith a 61 TFLops implementation, it would take roughly 2.23 ms to do the work and the bandwidth required to sustain this rate would be 4096∗4096∗4∗3/2.23∗10−3=90.2GB/s.\n\nOf course, these are oversimplified calculations as they totally ignore memory hierarchy but we see that the available bandwidth is sufficiently high so that we can increase the amount of data we read to be closer to compute bound.\n\nKernel 1: naive implementation\n\nLet’s start with a naive implementation like this :\n\n__global__ void kernel1_naive(const float *A, const float *B, float *C, int M, int K, int N, float alpha, float beta)\n{\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    if (row < M && col < N)\n    {\n        float acc_c = 0.0f;\n        for (int k = 0; k < K; ++k)\n        {\n            acc_c += A[row * K + k] * B[k * N + col];\n        }\n        C[row * N + col] = alpha * acc_c + beta * C[row * N + col];\n    }\n}\n\nYou will notice I am doing  C=alpha∗A∗B+beta∗C instead of C=A∗B here. This is because it makes easier to compare with libraries like rocBLAS where matrix multiplications is provided by SGEMM functions (Single-Precision General Matrix Multiply).\n\nWe launch 4096x4096 threads with a blocksize of 16x16 and each thread compute the inner dot product described before.\n\nThe performance for this kernel is 136 ms (1010.60 GFlops/s). I know, that’s pretty bad and far off our 61 TFLops target.\n\nKernel 0: rocBLAS reference implementation\n\nNow that we have seen possibly the worst implementation in terms of performance, let’s look at the official rocBLAS implementation.\n\n    const int M = N;\n    const int K = N;\n    CHECK_ROCBLAS_STATUS(rocblas_sgemm(\n        handle,\n        rocblas_operation_none, // Transpose option for A\n        rocblas_operation_none, // Transpose option for B\n        M,                      // Number of rows in A and C\n        N,                      // Number of columns in B and C\n        K,                      // Number of columns in A and rows in B\n        &alpha,                 // alpha\n        d_a,                    // Matrix A on the device\n        M,                      // Leading dimension of A\n        d_b,                    // Matrix B on the device\n        K,                      // Leading dimension of B\n        &beta,                  // beta\n        d_c,                    // Matrix C on the device\n        M                       // Leading dimension of C\n        ));\n\nAs discussed before, I used rocblas_sgemm function with alpha and beta set to 1.02\n\nThe performance for this kernel is 4.49 ms (30547 GFLOPs/s). This is clearly much better than our kernel 1 but still far from our theoritical 61.4 TFlops/s.\n\nBy inspecting the ISA in RGP3, I couldn’t find any dual issue instructions in the kernel (only v_fmac_f32_e32)4\n\n  Figure 5: extract of rocBLAS ISA code\n\nThis is very surprising as this essentially means one of the VALU unit is sitting there doing nothing.\n\nConsidering this, the VALU utilization of this kernel is pretty impressive and almost 100 %. However, it’s really surprising we can’t exploit these dual issue instructions properly. I’ll come to that later.\n\nKernel 2: LDS Tiling\n\nThe main issue with our naive kernel is that our inner loop directly accesses global memory. This is inefficient because fetching data from global memory has a high latency, typically on the order of hundreds of cycles. Since each memory read is followed by minimal computation (just one multiplication and one addition), the GPU struggles to hide this latency, even with a large number of concurrent threads. Moreover, the algorithm repeatedly reads the same rows and columns from global memory across different threads, leading to redundant memory accesses and further exacerbating the performance bottleneck.\n\nA solution to this problem is to load the data once into faster local memory and then iterate efficiently over it with all the threads. On RDNA3, we have the Local Data Store (LDS), a high-speed, low-latency memory accessible by all threads within a workgroup.\n\n  Figure 6: simplified representation of the memory hierarchy\n\nSince the LDS has a much smaller capacity than global memory, we need to use tiling to divide our problem into smaller sub-matrix multiplications. One way to facilitate this is to restructure the computation by moving the inner loop’s dot product to the outer loop. The key idea is to cache a column of matrix A and a row of matrix B, then perform the computation across the entire tile. This approach is more cache-efficient and significantly reduces memory access latency.\n\nThe pseudo code for our kernel 1 is :\n\nfor i from 0 to M - 1:                  # Loop over rows of A\n    for j from 0 to N - 1:              # Loop over columns of B\n        sum = 0\n        for k from 0 to K - 1:          # Loop over columns of A / rows of B\n            sum += A[i][k] * B[k][j]\n        end for\n        C[i][j] = sum\n    end for\nend for\n\nIf we move the dot product to the outer loop, we have this :\n\nfor k from 0 to K - 1:                  # Outer loop over the shared dimension\n    for i from 0 to M - 1:              # Loop over rows of A\n        for j from 0 to N - 1:          # Loop over columns of B\n            C[i][j] += A[i][k] * B[k][j]\n        end for\n    end for\nend for\n\nTiling in this form is straightforward: each workgroup operates on a tile and follows these steps: (BK is the batch size, ie number of rows/columns we load to the LDS)\n\nInit c to 0\nWhile kId is less than N:\n  # Load A and B to Tile As and Bs\n  Load BK columns A to As\n  Load BK rows to Bs\n  Syncthreads\n  # Accumulate results using LDS\n  for k from 0 to BK\n    c += As[threadIdx.y][k] * Bs[k][threadIdx.x]\n  Syncthreads\n  Increment kId by BK\nend for\nc[row][col]=c\n\nIf we choose a tile size of 32x32 and BK=32, our new kernel looks like this:\n\n#define TILE_SIZE 32\n__global__ void kernel2_lds(const float *A, const float *B, float *C, int N)\n{\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int t = 0; t < N; t += TILE_SIZE)\n    {\n        Bs[threadIdx.y][threadIdx.x] = B[N * (threadIdx.y + t) + col];\n        As[threadIdx.y][threadIdx.x] = A[N * row + t + threadIdx.x];\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; k++)\n        {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < N && col < N)\n    {\n        C[row * N + col] = sum;\n    }\n}\n\n__syncthreads(); is required here to ensure that all threads in the workgroup can see the data loaded into the LDS and to synchronize before any updates are made to the data.\n\nWe also ensure that the contents of both matrices A and B are loaded into the LDS by rows rather than columns to avoid uncoalesced memory accesses.\nIndeed, if we were to read by columns, each thread in a wave would access a non-contiguous memory region, result",
    "summary": {
      "en": "In this post, the author explains how to optimize FP32 matrix multiplication on an AMD RDNA3 GPU, specifically achieving a performance improvement over the rocBLAS library by 60%. The focus is on 4096x4096 matrices, using an AMD Radeon 7900 XTX, and involves eight different kernel implementations.\n\n**Key Points:**\n\n1. **Matrix Multiplication Basics**: The product of two matrices A (size MxK) and B (size KxN) results in matrix C (size MxN). The computation involves calculating the dot product of rows in A and columns in B.\n\n2. **Performance Goals**: The goal is to optimize the implementation to be compute-bound on the GPU, taking advantage of the RDNA3 architecture.\n\n3. **Theoretical Performance**: The author calculates the theoretical performance to be about 61.44 TFLOPS and the bandwidth required to sustain this performance is around 90.2 GB/s.\n\n4. **Naive Implementation**: A basic kernel implementation shows poor performance (136 ms, 1010.60 GFLOPS/s). The rocBLAS implementation performs better (4.49 ms, 30547 GFLOPS/s) but still falls short of the theoretical maximum.\n\n5. **Optimization Strategy**: The primary issue with the naive implementation is inefficient global memory access. The author proposes using Local Data Store (LDS) tiling, which uses faster local memory to reduce latency and improve performance.\n\n6. **Tiling Implementation**: The new approach involves loading data into tiles and performing computations on these tiles to minimize memory accesses. The author provides a pseudo code example and a specific implementation for the optimized kernel using LDS.\n\nOverall, the post highlights the importance of optimizing memory access patterns and utilizing GPU architecture features to enhance matrix multiplication performance.",
      "ko": "이 글에서는 AMD RDNA3 GPU에서 FP32 행렬 곱셈을 최적화하는 방법을 설명하며, rocBLAS 라이브러리보다 60% 향상된 성능을 달성하는 방법을 다룹니다. 주로 4096x4096 크기의 행렬을 사용하며, AMD Radeon 7900 XTX를 활용하고, 여덟 가지의 서로 다른 커널 구현을 포함합니다.\n\n행렬 곱셈의 기본 원리는 두 개의 행렬 A(크기 MxK)와 B(크기 KxN)를 곱하여 행렬 C(크기 MxN)를 생성하는 것입니다. 이 과정에서는 A의 행과 B의 열 간의 내적을 계산합니다.\n\n성능 목표는 GPU에서 계산 중심으로 최적화하여 RDNA3 아키텍처의 이점을 최대한 활용하는 것입니다. 저자는 이론적인 성능을 약 61.44 TFLOPS로 계산하며, 이 성능을 유지하기 위해 필요한 대역폭은 약 90.2 GB/s라고 설명합니다.\n\n기본적인 커널 구현은 성능이 좋지 않아 136ms, 1010.60 GFLOPS/s의 결과를 보입니다. rocBLAS 구현은 더 나은 성능인 4.49ms, 30547 GFLOPS/s를 기록하지만 여전히 이론적인 최대 성능에는 미치지 못합니다.\n\n최적화 전략의 주요 문제는 비효율적인 전역 메모리 접근입니다. 저자는 지연 시간을 줄이고 성능을 향상시키기 위해 Local Data Store(LDS) 타일링을 사용할 것을 제안합니다. 이 새로운 접근 방식은 데이터를 타일에 로드하고 이 타일에서 계산을 수행하여 메모리 접근을 최소화합니다. 저자는 최적화된 커널을 위한 LDS 사용의 구체적인 구현과 의사 코드를 제공합니다.\n\n이 글은 메모리 접근 패턴을 최적화하고 GPU 아키텍처의 기능을 활용하여 행렬 곱셈 성능을 향상시키는 것의 중요성을 강조합니다.",
      "ja": null
    }
  },
  {
    "id": "4080a392bd1c945c",
    "title": {
      "en": "Building Statically Linked Go Executables with CGO and Zig",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://calabro.io/zig-cgo",
    "score": 81,
    "by": "todsacerdoti",
    "time": 1743171067,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "541246337a07996d",
    "title": {
      "en": "Architecture Patterns with Python",
      "ko": "파이썬 아키텍처 패턴",
      "ja": null
    },
    "type": "story",
    "url": "https://www.cosmicpython.com/book/preface.html",
    "score": 398,
    "by": "asicsp",
    "time": 1743141447,
    "content": "Preface\n\nYou may be wondering who we are and why we wrote this book.\n\nAt the end of Harry’s last book,\nTest-Driven Development with Python (O’Reilly),\nhe found himself asking a bunch of questions about architecture, such as,\nWhat’s the best way of structuring your application so that it’s easy to test?\nMore specifically, so that your core business logic is covered by unit tests,\nand so that you minimize the number of integration and end-to-end tests you need?\nHe made vague references to \"Hexagonal Architecture\" and \"Ports and Adapters\"\nand \"Functional Core, Imperative Shell,\" but if he was honest, he’d have to\nadmit that these weren’t things he really understood or had done in practice.\n\nAnd then he was lucky enough to run into Bob, who has the answers to all these\nquestions.\n\nBob ended up as a software architect because nobody else on his team was\ndoing it. He turned out to be pretty bad at it, but he was lucky enough to run\ninto Ian Cooper, who taught him new ways of writing and thinking about code.\n\nManaging Complexity, Solving Business Problems\n\nWe both work for MADE.com, a European ecommerce company that sells furniture\nonline; there, we apply the techniques in this book to build distributed systems\nthat model real-world business problems. Our example domain is the first system\nBob built for MADE, and this book is an attempt to write down all the stuff we\nhave to teach new programmers when they join one of our teams.\n\nMADE.com operates a global supply chain of freight partners and manufacturers.\nTo keep costs low, we try to optimize the delivery of stock to our\nwarehouses so that we don’t have unsold goods lying around the place.\n\nIdeally, the sofa that you want to buy will arrive in port on the very day\nthat you decide to buy it, and we’ll ship it straight to your house without\never storing it. Getting the timing right is a tricky balancing act when goods take\nthree months to arrive by container ship. Along the way, things get broken or water\ndamaged, storms cause unexpected delays, logistics partners mishandle goods,\npaperwork goes missing, customers change their minds and amend their orders,\nand so on.\n\nWe solve those problems by building intelligent software representing the\nkinds of operations taking place in the real world so that we can automate as\nmuch of the business as possible.\n\nWhy Python?\n\nIf you’re reading this book, we probably don’t need to convince you that Python\nis great, so the real question is \"Why does the Python community need a book\nlike this?\" The answer is about Python’s popularity and maturity: although Python is\nprobably the world’s fastest-growing programming language and is nearing the top\nof the absolute popularity tables, it’s only just starting to take on the kinds\nof problems that the C# and Java world has been working on for years.\nStartups become real businesses; web apps and scripted automations are becoming\n(whisper it) enterprise software.\n\nIn the Python world, we often quote the Zen of Python:\n\"There should be one—and preferably only one—obvious way to do it.\"[1]\nUnfortunately, as project size grows, the most obvious way of doing things\nisn’t always the way that helps you manage complexity and evolving\nrequirements.\n\nNone of the techniques and patterns we discuss in this book are\nnew, but they are mostly new to the Python world. And this book isn’t\na replacement for the classics in the field such as Eric Evans’s\nDomain-Driven Design\nor Martin Fowler’s Patterns of\nEnterprise Application Architecture (both published by Addison-Wesley Professional)—which we often refer to and\nencourage you to go and read.\n\nBut all the classic code examples in the literature do tend to be written in\nJava or C++/#, and if you’re a Python person and haven’t used either of\nthose languages in a long time (or indeed ever), those code listings can be\nquite…trying. There’s a reason the latest edition of that other classic text, Fowler’s\nRefactoring (Addison-Wesley Professional), is in JavaScript.\n\nTDD, DDD, and Event-Driven Architecture\n\nIn order of notoriety, we know of three tools for managing complexity:\n\nTest-driven development (TDD) helps us to build code that is correct\nand enables us to refactor or add new features, without fear of regression.\nBut it can be hard to get the best out of our tests: How do we make sure\nthat they run as fast as possible? That we get as much coverage and feedback\nfrom fast, dependency-free unit tests and have the minimum number of slower,\nflaky end-to-end tests?\n\nDomain-driven design (DDD) asks us to focus our efforts on building a good\nmodel of the business domain, but how do we make sure that our models aren’t\nencumbered with infrastructure concerns and don’t become hard to change?\n\nLoosely coupled (micro)services integrated via messages (sometimes called\nreactive microservices) are a well-established answer to managing complexity\nacross multiple applications or business domains. But it’s not always\nobvious how to make them fit with the established tools of\nthe Python world—Flask, Django, Celery, and so on.\n\nNote\n\nDon’t be put off if you’re not working with (or interested in) microservices.\n    The vast majority of the patterns we discuss,\n    including much of the event-driven architecture material,\n    is absolutely applicable in a monolithic architecture.\n\nOur aim with this book is to introduce several classic architectural patterns\nand show how they support TDD, DDD, and event-driven services.  We hope\nit will serve as a reference for implementing them in a Pythonic way, and that\npeople can use it as a first step toward further research  in this field.\n\nWho Should Read This Book\n\nHere are a few things we assume about you, dear reader:\n\nYou’ve been close to some reasonably complex Python applications.\n\nYou’ve seen some of the pain that comes with trying to manage\nthat complexity.\n\nYou don’t necessarily know anything about DDD or any of the\nclassic application architecture patterns.\n\nWe structure our explorations of architectural patterns around an example app,\nbuilding it up chapter by chapter. We use TDD at\nwork, so we tend to show listings of tests first, followed by implementation.\nIf you’re not used to working test-first, it may feel a little strange at\nthe beginning, but we hope you’ll soon get used to seeing code \"being used\"\n(i.e., from the outside) before you see how it’s built on the inside.\n\nWe use some specific Python frameworks and technologies, including Flask,\nSQLAlchemy, and pytest, as well as Docker and Redis. If you’re already\nfamiliar with them, that won’t hurt, but we don’t think it’s required.  One of\nour main aims with this book is to build an architecture for which specific\ntechnology choices become minor implementation details.\n\nA Brief Overview of What You’ll Learn\n\nThe book is divided into two parts; here’s a look at the topics we’ll cover\nand the chapters they live in.\n\n#part1\n\nDomain modeling and DDD (Chapters 1, 2 and 7)\n\nAt some level, everyone has learned the lesson that complex business\nproblems need to be reflected in code, in the form of a model of the domain.\nBut why does it always seem to be so hard to do without getting tangled\nup with infrastructure concerns, our web frameworks, or whatever else?\nIn the first chapter we give a broad overview of domain modeling and DDD, and we\nshow how to get started with a model that has no external dependencies, and\nfast unit tests. Later we return to DDD patterns to discuss how to choose\nthe right aggregate, and how this choice relates to questions of data\nintegrity.\n\nRepository, Service Layer, and Unit of Work patterns (Chapters 2, 4, and 5)\n\nIn these three chapters we present three closely related and\nmutually reinforcing patterns that support our ambition to keep\nthe model free of extraneous dependencies.  We build a layer of\nabstraction around persistent storage, and we build a service\nlayer to define the entrypoints to our system and capture the\nprimary use cases. We show how this layer makes it easy to build\nthin entrypoints to our system, whether it’s a Flask API or a CLI.\n\nSome thoughts on testing and abstractions (Chapter 3 and 5)\n\nAfter presenting the first abstraction (the Repository pattern), we take the\nopportunity for a general discussion of how to choose abstractions, and\nwhat their role is in choosing how our software is coupled together. After\nwe introduce the Service Layer pattern, we talk a bit about achieving a test pyramid\nand writing unit tests at the highest possible level of abstraction.\n\n#part2\n\nEvent-driven architecture (Chapters 8-11)\n\nWe introduce three more mutually reinforcing patterns:\nthe Domain Events, Message Bus, and Handler patterns.\nDomain events are a vehicle for capturing the idea that\nsome interactions with a system are triggers for others.\nWe use  a message bus to allow actions to trigger events\nand call appropriate handlers.\nWe move on to discuss how events can be used as a pattern\nfor integration between services in a microservices architecture.\nFinally, we distinguish between commands and events.\nOur application is now fundamentally a message-processing system.\n\nCommand-query responsibility segregation ([chapter_12_cqrs])\n\nWe present an example of command-query responsibility segregation,\nwith and without events.\n\nDependency injection ([chapter_13_dependency_injection])\n\nWe tidy up our explicit and implicit dependencies and implement a\nsimple dependency injection framework.\n\nAdditional Content\n\nHow do I get there from here? ([epilogue_1_how_to_get_there_from_here])\n\nImplementing architectural patterns always looks easy when you show a simple\nexample, starting from scratch, but many of you will probably be wondering how\nto apply these principles to existing software. We’ll provide a\nfew pointers in the epilogue and some links to further reading.\n\nExample Code and Coding Along\n\nYou’re reading a book, but you’ll probably agree with us when we say that\nthe best way to learn about code is to code.  We learned most of what we know\nfrom pairing with people, writing code with them, and learning by doing, and\nwe’d like to re-create that experience as much as possible for you in this book.\n\nAs a result, we’ve structured the book around a single example project\n(although we do sometimes throw in other examples). We’ll build up this project as the chapters progress, as if you’ve paired with us and\nwe’re explaining what we’re doing and why at each step.\n\nBut to really get to grips with these patterns, you need to mess about with the\ncode and get a feel for how it works. You’ll find all the code on\nGitHub; each chapter has its own branch. You can find a list of the branches on GitHub as well.\n\nHere are three ways you might code along with the book:\n\nStart your own repo and try to build up the app as we do, following the\nexamples from listings in the book, and occasionally looking to our repo\nfor hints. A word of warning, however: if you’ve read Harry’s previous book\nand coded along with that, you’ll find that this book requires you to figure out more on\nyour own; you may need to lean pretty heavily on the working versions on GitHub.\n\nTry to apply each pattern, chapter by chapter, to your own (preferably\nsmall/toy) project, and see if you can make it work for your use case.  This\nis high risk/high reward (and high effort besides!). It may take quite some\nwork to get things working for the specifics of your project, but on the other\nhand, you’re likely to learn the most.\n\nFor less effort, in each chapter we outline an \"Exercise for the Reader,\"\nand point you to a GitHub location where you can download some partially finished\ncode for the chapter with a few missing parts to write yourself.\n\nParticularly if you’re intending to apply some of these patterns in your own\nprojects, working through a simple example is a great way to\nsafely practice.\n\nTip\n\nAt the very least, do a git checkout of the code from our repo as you\n    read each chapter. Being able to jump in and see the code in the context of\n    an actual working app will help answer a lot of questions as you go, and\n    makes everything more real. You’ll find instructions for how to do that\n    at the beginning of each chapter.\n\nLicense\n\nThe code (and the online version of the book) is licensed under a Creative\nCommons CC BY-NC-ND license, which means you are free to copy and share it with\nanyone you like, for non-commercial purposes, as long as you give attribution.\nIf you want to re-use any of the content from this book and you have any\nworries about the license, contact O’Reilly at permissions@oreilly.com.\n\nThe print edition is licensed differently; please see the copyright page.\n\nConventions Used in This Book\n\nThe following typographical conventions are used in this book:\n\nItalic\n\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\n\nConstant width\n\nUsed for program listings, as well as within paragraphs to refer to program elements such as variable or function names, databases, data types, environment variables, statements, and keywords.\n\nConstant width bold\n\nShows commands or other text that should be typed literally by the user.\n\nConstant width italic\n\nShows text that should be replaced with user-supplied values or by values determined by context.\n\nTip\n\nThis element signifies a tip or suggestion.\n\nNote\n\nThis element signifies a general note.\n\nWarning\n\nThis element indicates a warning or caution.\n\nO’Reilly Online Learning\n\nNote\n\nFor more than 40 years, O’Reilly Media has provided technology and business training, knowledge, and insight to help companies succeed.\n\nOur unique network of experts and innovators share their knowledge and expertise through books, articles, conferences, and our online learning platform. O’Reilly’s online learning platform gives you on-demand access to live training courses, in-depth learning paths, interactive coding environments, and a vast collection of text and video from O’Reilly and 200+ other publishers. For more information, please visit http://oreilly.com.\n\nHow to Contact O’Reilly\n\nPlease address comments and questions concerning this book to the publisher:\n\n  O’Reilly Media, Inc.\n  1005 Gravenstein Highway North\n  Sebastopol, CA 95472\n  800-998-9938 (in the United States or Canada)\n  707-829-0515 (international or local)\n  707-829-0104 (fax)\n\nWe have a web page for this book, where we list errata, examples, and any additional information. You can access this page at https://oreil.ly/architecture-patterns-python.\n\nEmail bookquestions@oreilly.com to comment or ask technical questions about this book.\n\nFor more information about our books, courses, conferences, and news, see our website at http://www.oreilly.com.\n\nFind us on Facebook: http://facebook.com/oreilly\n\nFollow us on Twitter: http://twitter.com/oreillymedia\n\nWatch us on YouTube: http://www.youtube.com/oreillymedia\n\nAcknowledgments\n\nTo our tech reviewers, David Seddon, Ed Jung, and Hynek Schlawack: we absolutely\ndo not deserve you. You are all incredibly dedicated, conscientious, and\nrigorous. Each one of you is immensely smart, and your different points of\nview were both useful and complementary to each other. Thank you from the\nbottom of our hearts.\n\nGigantic thanks also to all our readers so far for their comments and\nsuggestions:\nIan Cooper, Abdullah Ariff, Jonathan Meier, Gil Gonçalves, Matthieu Choplin,\nBen Judson, James Gregory, Łukasz Lechowicz, Clinton Roy, Vitorino Araújo,\nSusan Goodbody, Josh Harwood, Daniel Butler, Liu Haibin, Jimmy Davies, Ignacio\nVergara Kausel, Gaia Canestrani, Renne Rocha, pedroabi, Ashia Zawaduk, Jostein\nLeira, Brandon Rhodes, Jazeps Basko, simkimsia, Adrien Brunet, Sergey Nosko,\nDmitry Bychkov,\nand many more; our apologies if we missed you on this list.\n\nSuper-mega-thanks to our editor Corbin Collins for his gentle chivvying, and\nfor being a tireless advocate of the reader. Similarly-superlative thanks to\nthe production staff, Katherine Tozer, Sharon Wilkey, Ellen Troutman-Zaig, and\nRebecca Demarest, for your dedication, professionalism, and attention to\ndetail. This book is immeasurably improved thanks to you.\n\nAny errors remaining in the book are our own, naturally.\n\n  << Previous - Appendix E: Validation\n  Next - Introduction >>",
    "summary": {
      "en": "**Summary:**\n\nThis book explores key architectural patterns for building complex applications in Python, focusing on effective software design and testing strategies. Written by Harry and Bob, it stems from Harry's questions about structuring applications for better testability after his previous work on Test-Driven Development (TDD) with Python. Bob, an experienced software architect, shares insights learned from Ian Cooper on managing complexity in software.\n\nThe authors work at MADE.com, an online furniture retailer, where they apply these techniques to create intelligent systems for optimizing logistics and delivery. They aim to teach new programmers the essential skills needed for such projects.\n\nThe book emphasizes the growing need for robust architectural patterns in Python as it evolves into more complex applications, similar to those in C# and Java. It covers TDD, Domain-Driven Design (DDD), and event-driven architecture, introducing classic patterns and how they can be applied in a Pythonic way.\n\nReaders are expected to have experience with complex Python applications but don’t need prior knowledge of DDD or architectural patterns. The book builds a single example project throughout its chapters, encouraging readers to engage with the code actively through exercises and GitHub resources.\n\nKey areas covered include:\n- Domain modeling and DDD principles.\n- Repository, Service Layer, and Unit of Work patterns.\n- Event-driven architecture and message processing.\n- Command-query responsibility segregation and dependency injection.\n\nOverall, this book serves as a reference for implementing architectural patterns in Python and encourages further exploration of software design principles.",
      "ko": "이 책은 복잡한 애플리케이션을 구축하기 위한 주요 아키텍처 패턴을 탐구하며, 효과적인 소프트웨어 설계와 테스트 전략에 중점을 둡니다. 해리와 밥이 저자로 참여했으며, 이 책은 해리가 파이썬으로 테스트 주도 개발(TDD)을 진행한 후 애플리케이션 구조에 대한 질문에서 출발했습니다. 경험이 풍부한 소프트웨어 아키텍트인 밥은 소프트웨어의 복잡성을 관리하는 방법에 대한 이안 쿠퍼의 통찰을 공유합니다.\n\n저자들은 온라인 가구 소매업체인 MADE.com에서 일하며, 이러한 기술을 활용해 물류와 배송을 최적화하는 지능형 시스템을 만듭니다. 이들은 새로운 프로그래머들에게 이러한 프로젝트에 필요한 필수 기술을 가르치는 것을 목표로 하고 있습니다.\n\n책은 파이썬이 C#과 자바와 같은 복잡한 애플리케이션으로 발전함에 따라 강력한 아키텍처 패턴의 필요성이 커지고 있음을 강조합니다. TDD, 도메인 주도 설계(DDD), 이벤트 기반 아키텍처를 다루며, 고전적인 패턴과 이를 파이썬 방식으로 적용하는 방법을 소개합니다.\n\n독자는 복잡한 파이썬 애플리케이션에 대한 경험이 있어야 하지만, DDD나 아키텍처 패턴에 대한 사전 지식은 필요하지 않습니다. 이 책은 각 장에서 하나의 예제 프로젝트를 구축하며, 독자가 코드에 적극적으로 참여하도록 유도하는 연습 문제와 GitHub 리소스를 제공합니다.\n\n주요 내용으로는 도메인 모델링과 DDD 원칙, 리포지토리, 서비스 레이어, 유닛 오브 워크 패턴, 이벤트 기반 아키텍처와 메시지 처리, 명령-쿼리 책임 분리 및 의존성 주입이 포함됩니다.\n\n전반적으로 이 책은 파이썬에서 아키텍처 패턴을 구현하기 위한 참고서 역할을 하며, 소프트웨어 설계 원칙에 대한 추가 탐구를 장려합니다.",
      "ja": null
    }
  },
  {
    "id": "9ceda24309032b3b",
    "title": {
      "en": "The Biology of B-Movie Monsters (2003)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://fathom.lib.uchicago.edu/2/21701757/",
    "score": 48,
    "by": "cainxinth",
    "time": 1743169241,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "4eb49c6d920d4f18",
    "title": {
      "en": "Getting hit by lightning is good for some tropical trees",
      "ko": "번개가 나무를 살린다",
      "ja": null
    },
    "type": "story",
    "url": "https://www.caryinstitute.org/news-insights/press-release/getting-hit-lightning-good-some-tropical-trees",
    "score": 116,
    "by": "sohkamyung",
    "time": 1743169978,
    "content": "Share:\n\n                            March 26, 2025\n\n          Biodiversity,\n\n          Climate Change,\n\n          Conservation,\n\n          Tropical Forests,\n\n          Lightning\n\n          Getting zapped with millions of volts of electricity may not sound like a healthy activity, but for some trees, it is. A new study, published in New Phytologist, reports that some tropical tree species are not only able to tolerate lightning strikes, but benefit from them. The trees may have even evolved to act as lightning rods.The research was led by Evan Gora, a forest ecologist at Cary Institute of Ecosystem Studies. Gora studies how lightning impacts biodiversity and carbon storage in Panama’s tropical forests.Lightning kills hundreds of millions of trees per year. But in 2015, while working in Panama, Gora and his colleagues came across a Dipteryx oleifera tree that had survived a strike with little damage — even though the jolt had been strong enough to blast a parasitic vine out of its crown and kill more than a dozen neighboring trees. “Seeing that there are trees that get struck by lightning and they’re fine was just mind blowing,” Gora recalled. Over time, the team encountered other D. oleifera trees thriving after getting hit, so they decided to take a closer look.Also known as the eboe, choibá, tonka bean or almendro, Dipteryx oleifera is native to Honduras, Nicaragua, Costa Rica, Panama, Colombia, and Ecuador. Its hard wood is used in construction, and it produces almond-flavored seeds that are edible and sold in local markets. A keystone species of Panamanian forests, D. oleifera fruits and seeds are a crucial food source for rainforest mammals such as agouti during the dry season.Scientists had previously suspected that some trees evolved to tolerate lightning, but evidence to back it up was lacking. In 2022, Gora and colleagues demonstrated for the first time that trees differ in their ability to survive getting hit by lightning. Their new paper, published Wednesday, is the first to show that trees can actually benefit from these electric jolts.Using a unique lightning location system, the team tracked the outcomes of 93 trees that had been struck by lightning in Barro Colorado Nature Monument in central Panama. For two to six years after the strike, the team measured tree survival rates, crown and trunk condition, number of parasitic vines or lianas, and neighboring tree mortality. The study included nine directly struck D. oleifera trees, and compared them with 84 other trees that had been struck.All nine D. oleifera trees survived direct lightning strikes with only minor damages. In contrast, directly struck trees of other species were badly damaged, losing 5.7 times more leaves from their crowns, and 64% died within two years.When each D. oleifera tree was zapped, an average of 9.2 neighboring trees were killed as the electricity traveled between adjoining vines and touching branches, or jumped across small gaps between trees. Lightning strikes also decreased D. oleifera's parasitic liana infestations by 78%, further reducing competition for light and nutrients.A Dipteryx oleifera tree just after being struck by lightning in 2019 (left) versus two years later (right). The tree survived the strike with minimal damage, and benefited from having its parasitic vines and competing neighbors removed by the strike. Photos: Evan GoraThese patterns also bore out across the broader population. The team found that D. oleifera trees in general tend to have fewer lianas. Analyzing trends in tree death over the past 40 years, the researchers found that the trees neighboring D. oleifera trees were 48% more prone to die than other trees in the forest, likely because of lightning.Using drones, Gora and colleagues created 3D models of canopy height, which showed that D. oleifera trees tend to be about four meters taller than their nearest neighbors, likely because lightning killed their taller neighbors, giving them an advantage in competing for light and space.“These data provide the first evidence that some trees benefit from being struck by lightning,” the authors write. Or, as Gora puts it, “It's better off for a Dipteryx oleifera tree to be struck than not.”Because of all these benefits, the team thinks D. oleifera trees may be specially adapted to attract lightning. With their distinctive height and unusually wide crowns, they may be up to 68% more likely to get electrocuted than other trees with average height and crowns, according to the team’s calculations.Lightning strikes open up gaps around Dipteryx oleifera trees, giving them more space to spread out and collect light. Photo: Evan GoraEstimates suggest individual Dipteryx oleifera trees are directly hit by lightning every 56 years, on average. And since the trees can live for hundreds or possibly more than a thousand years, they are expected to survive these blasts many times throughout their lives. During the study, one of the D. oleifera trees was struck twice in just five years.The remarkable ability to survive lightning strikes and benefit from the removal of lianas and competitors gives D. oleifera trees a major advantage over other trees. According to the scientists' calculations, lightning tolerance boosts the species’ ability to produce offspring by 14 times.Next, the team aims to investigate what electrical or structural traits allow these trees to survive lightning strikes. They would also like to explore whether other species show lightning tolerance, to better understand how common this phenomenon is.What is clear is that lightning plays an underappreciated role in tree competition. And with lightning on the rise in many regions due to climate change, its influence may increase, potentially favoring lightning-tolerant species like Dipteryx oleifera. Understanding lightning and its role in shaping the forests may be important for predicting changes in biodiversity and carbon storage, and for informing tropical reforestation efforts.Study co-authors include: Helene Muller-Landau and Pablo Narváez of the Smithsonian Tropical Research Institute; KC Cushman of Oak Ridge National Laboratory; Jeannine Richards of Florida Gulf Coast University; Phillip Bitzer and Jeffrey Burchfield of the University of Alabama in Huntsville; and Stephen Yanoviak of the University of Louisville.New Phytologist is a leading international journal focusing on high quality, original research across the broad spectrum of plant sciences, from intracellular processes through to global environmental change. The journal is owned by the New Phytologist Foundation, a not-for-profit organisation dedicated to the promotion of plant science.FundingThis work was supported by grants from the National Science Foundation (DEB-1354060, DEB-1655346, and DEB-2213246 to SPY, DEB-1354510, DEB-1655554, and DEB-2213247 to PMB, and DEB-2213245, DEB-2241507, and GRF-2015188266 to EMG), the National Geographic Society (9703-15 to EMG), and a Smithsonian Tropical Research Institute Tupper Postdoctoral Fellowship to EMG. KCC was supported as part of the Next Generation Ecosystem Experiments-Tropics, funded by the U.S. Department of Energy, Office of Science, Office of Biological and Environmental Research.\n\n        Share:",
    "summary": {
      "en": "A recent study has found that certain tropical trees, specifically the Dipteryx oleifera, can not only survive lightning strikes but may actually benefit from them. Led by forest ecologist Evan Gora, the research showed that these trees can act like lightning rods, experiencing minimal damage while helping to eliminate competition from nearby plants.\n\nThe study tracked 93 trees struck by lightning in Panama and discovered that while D. oleifera trees survived with minor damage, other species suffered significant loss, with many dying shortly after being struck. The strikes reduced the number of parasitic vines on D. oleifera trees, allowing them to grow better and compete for resources more effectively.\n\nThe researchers suggest that D. oleifera trees may have evolved to attract lightning due to their height and wide crowns. On average, these trees are struck by lightning every 56 years and can live for hundreds of years, benefiting from multiple strikes throughout their lifespan.\n\nThe findings highlight the important role that lightning plays in forest dynamics and could influence biodiversity and carbon storage, especially as climate change increases lightning frequency. Future research will explore what traits allow these trees to survive lightning and whether other species have similar adaptations.",
      "ko": "최근 연구에 따르면 특정 열대 나무, 특히 디프테릭스 올레이페라(Dipteryx oleifera)는 번개에 맞아도 생존할 수 있을 뿐만 아니라 오히려 이득을 볼 수 있다는 사실이 밝혀졌습니다. 이 연구는 산림 생태학자 에반 고라(Evan Gora)가 이끌었으며, 이 나무들이 번개를 유도하는 역할을 하며 최소한의 피해를 입는 동시에 주변 식물과의 경쟁을 줄이는 데 도움을 준다는 결과를 보여주었습니다.\n\n연구팀은 파나마에서 번개에 맞은 93그루의 나무를 추적하였고, D. oleifera 나무는 경미한 피해를 입고 생존한 반면, 다른 종들은 상당한 피해를 입고 많은 나무가 번개에 맞은 직후 사망하는 것을 발견했습니다. 번개는 D. oleifera 나무에 있는 기생 덩굴의 수를 줄여주어 이 나무들이 더 잘 자라고 자원 경쟁에서 유리한 위치를 차지할 수 있도록 했습니다.\n\n연구자들은 D. oleifera 나무가 그 높이와 넓은 가지 덕분에 번개를 끌어당기는 방향으로 진화했을 가능성이 있다고 제안합니다. 평균적으로 이 나무들은 56년마다 번개에 맞으며, 수백 년을 살 수 있어 생애 동안 여러 번의 번개를 경험하며 이득을 볼 수 있습니다.\n\n이 연구 결과는 번개가 숲의 생태계에서 중요한 역할을 한다는 점을 강조하며, 기후 변화로 인해 번개가 더 자주 발생함에 따라 생물 다양성과 탄소 저장에 영향을 미칠 수 있습니다. 향후 연구에서는 이러한 나무들이 번개를 견딜 수 있는 특성이 무엇인지, 그리고 다른 종들도 비슷한 적응을 가지고 있는지에 대해 탐구할 예정입니다.",
      "ja": null
    }
  },
  {
    "id": "5ea5b028361048ce",
    "title": {
      "en": "Disk I/O bottlenecks in GitHub Actions",
      "ko": "깃허브 액션의 디스크 병목",
      "ja": null
    },
    "type": "story",
    "url": "https://depot.dev/blog/uncovering-disk-io-bottlenecks-github-actions-ci",
    "score": 82,
    "by": "jacobwg",
    "time": 1743175356,
    "content": "When your CI pipelines are slow, you can only optimize so much. Bottlenecks in CPU, Network, Memory, and Disk I/O can all contribute to slow CI pipelines. Let's take a look at how disk I/O can be a bottleneck in GitHub Actions.",
    "summary": {
      "en": "Slow CI pipelines can only be improved to a certain extent. Issues with CPU, Network, Memory, and Disk I/O can all cause delays. This summary focuses on how Disk I/O can slow down workflows in GitHub Actions.",
      "ko": "느린 CI 파이프라인은 어느 정도까지 개선할 수 있습니다. CPU, 네트워크, 메모리, 디스크 I/O와 같은 문제들이 모두 지연을 초래할 수 있습니다. 이 요약에서는 디스크 I/O가 GitHub Actions의 작업 흐름을 어떻게 느리게 할 수 있는지에 대해 설명합니다.",
      "ja": null
    }
  },
  {
    "id": "fe85c9fb8a23bd2d",
    "title": {
      "en": "MilliForth-6502: The smallest Forth real programming language for 6502",
      "ko": "밀리포스 6502: 초소형 포스 프로그래밍 언어",
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/agsb/milliForth-6502",
    "score": 89,
    "by": "rbanffy",
    "time": 1743159880,
    "content": "A milliForth for 6502\n\"A Forth in 328 bytes — the smallest real programming\nlanguage ever, as of yet.\"\nThe milliForth1 is a review of sectorForth2,\nand smaller than sector Lisp3\nThe miniForth4 is another Forth to use in a boot sector\nof less than 512 bytes.\nBytes?\nYes, bytes. But those are for a x86 16-bit CPU.\nHow minimal could be it for a classic 6502 8-bit CPU ?\nTwo essentially different CPUs, a 16-bit x86 based on complex\nregisters and opcodes, and a 8-bit 6502 using page zero\nas common registers and page one as hardware stack.\nInner Interpreter\nThe FIG-Forth (1980) for 6502 uses Indirect Thread Code (ITC)\nas inner interpreter.\nThe miniForth, sectorforth and milliForth use Direct Thread Code (DTC)\nThis Forth for 6502, will be was done using two models:\nwith classic Direct Thread Code (DTC)\nand\nwith Minimal Thread Code (MTC)\n\n(later we will compare both, but DTC will win for less size)\nThis project is also used to verify standart Direct Thread Code against\nvariations of Minimum Thread Code.\nMinimal Thread Code\nis an alternative model for inner interpreter, where\nthe dictionary is organized with the primitives words grouped together\nbefore the compound words, defining a \"tipping point\", from where could\ndecide if the reference of a word will be to executed or be pushed into\nreturn stack. PS. MTC is a reduced form of TachyonForth inner interpreter.\nCoding for 6502\nFocus in size not performance.\nThe compilation wqas done with ca65 V2.19 - Git 7979f8a41.\nThe emulation of 6502 CPU was done with run6502\nThe way at 6502 is use a page zero and lots of lda/sta bytes.\nBoth DTC and MTC runs with my_hello_world.FORTH, with these numbers.\nMTC, Instructions: 35796948, Cycles: 148802145, SIZE: 582\n\nDTC, Instructions: 35211469, Cycles: 148450585, SIZE: 596\n\ncompiling my_hello_world.FORTH, overhead MTC / DTC\n\nInstructions:    1.66%\n\nCycles:          0.24%\n\n(*) Thanks to wise changes from peterferrie !\nChanges:\n\nas Forth-19945: FALSE is $0000 and TRUE is $FFFF ;\n\nall tib (80 bytes), pic (16 cells), data (36 cells) and\nreturn (36 cells) stacks are in page $200 ;\n\ntib and pic grows forward, stacks grows backwards ;\n\nonly immediate flag used as $80, no more flags ;\n\nno line editor, no backspace, no cancel line, no low ASCII verify ;\n\nno stacks overflow or underflow checks ;\n\nRemarks:\n\n6502 is a byte processor, no need 'pad' at end of even names ;\nhardware stack (page $100) not used as forth stack, free for use ;\nuses 32 bytes of page zero ;\nno multiuser, no multitask, no faster ;\nonly update latest at end of word definition ;\nredefine a word does not change previous uses ;\nstacks moves like the hardware stack ;\nwords must be between spaces, before and after ever ;\nbetter use 7-bit ASCII characters ;\napprouch as ANSI X3.215-19945\n\nNotes\nLook up at Notes6 for more.\n24/08/2024\nmerge of dtc and mtc code into a flagged sector-6502.s file\n16/08/2024:\nboth models DTC and MTC, works with my_hello_world.FORTH;\n11/08/2024:\nreturn to direct thread code;\n11/06/2024:\nadapt for minimal thread indirect code;\n24/01/2024:\nwrite using standart direct thread code;\n14/11/2023\ncode for 6502 sized to 624 bytes,\nno ascii-7, no key, no emit, no 2/, many errors\nCoding\nThis version includes:\nprimitives:\n    s@    return the address of user structure\n    +     adds two values at top of data stack\n    nand  logic not and the two values at top of data stack\n    @     fetch a value of cell wich address at top of data stack\n    !     store a value into a cell wich address at top of data stack\n    0#    test if top of data stack is not zero\n\n    :     starts compilng a new word\n    ;     stops compiling a new word\n    exit  ends a word\n\n    key   get a char from default terminal (system dependent)\n    emit  put a char into default terminal (system dependent)\n\ninternals:\n    spush, spull, rpull, rpush, (stack code)\n    copyfrom, copyinto, (heap code)\n    incr, decr, add, etc (register mimics)\n    cold, warm, quit, token, skip, scan, getline, (boot and terminal)\n    parse, find, compile, execute, (outer interpreter)\n\n    unnest, next, nest, (dtc inner)\n    unnest, next, nest, pick, jump, (mtc inner)\n\n    ps. next is not the FOR NEXT loop\n\nexternals:\n    getch, putch, byes (depends on system, used minimal for emulator )\n\nextensions: (selectable)\n    2/      shift right one bit\n    exec    jump to address at top of spt\n    :$      jump to (ipt)\n    ;$      jump to next\n\nextras:    (selectable)\n    bye     ends the Forth, return to system\n    abort   restart the Forth\n    .S      list cells in data stack\n    .R      list cells in return stack\n    .       show cell at top of data stack\n    words   extended list the words in dictionary\n    dump    list contents of dictionary in binary\n\nA my_hello_world.FORTH alternate version with dictionary for use;\n\nThe sp@ and rp@ are now derived from s@ in the my_hello_world.FORTH\n\nMemory\n    $000    page zero       ; cpu reserved\n    $100    hardware stack  ; cpu reserved\n    $200    TIB             ; terminal input buffer, 80 bytes\n    $298*   data stack      ; data stack, 36 cells, backwards\n    $2E0*   return stack    ; return stack, 36 cells, backwards\n    $2E0    PIC             ; reserved for scratch, 16 cells\n    $300    _main_          ; start of Forth\n    $???    _ends_          ; end of code and primitives of Forth\n    $???    _init_          ; start of compound dictionary\n\n    _init_ is the page (MSB) of _ends_ + 1\n\nStacks\n\" When heap moves forward, move the stack backward \"\nPush is 'store and decrease'. Pull is 'increas and fetch'.\nA common memory model organization of Forth:\n   tib->...<-spt, user forth dictionary, here->pad...<-rpt,\n\nthen backward stacks allow to use the slack space ...\nThis 6502 Forth memory model is blocked in pages of 256 bytes:\n   page0, page1, page2, core ... forth dictionary ...here...\n\nat page2, without 'rush over'\n   |tib 40 cells> <spt 36 cells| <rpt 36 cells|pic 16 cells> | .\n\nLanguage\n\"SectorFORTH was an extensive guide throughout the process of\nimplementing milliFORTH, and milliFORTH's design actually converged\non sectorFORTH unintentionally in a few areas. That said, the language\nimplemented is intentionally very similar, being the 'minimal FORTH'.\"\nFor Forth language primer see\nStarting Forth\nFor Forth from inside howto see\nJonasForth\nUse\n** 16/08/2024 DTC and MTC models operational, using lib6502 for emulation and tests **\nA crude small script for compile with ca65 is included.\n; for make it\nsh mk a sector-6502\n\n; for clear it\nsh mk x sector-6502\n\nthe size is take from main: to ends: using the sector-6502.lbl file\nNote\nthe originals files are edited for lines with less than 80 bytes\nthe bf.FORTH and hello_world.FORTH are from original milliForth1\nthe my_hello_world.FORTH is adapted for miiliforth-6502\nReferences\nFootnotes\n\nThe original milliForth: https://github.com/fuzzballcat/milliForth ↩ ↩2\n\nThe inspirational sectorForth: https://github.com/cesarblum/sectorforth/ ↩\n\nMind-blowing sectorLISP: https://justine.lol/sectorlisp2/, https://github.com/jart/sectorlisp ↩\n\nThe miniforth: https://github.com/meithecatte/miniforth ↩\n\nForth standart ANSI X3.215-1994: http://www.forth.org/svfig/Win32Forth/DPANS94.txt ↩ ↩2\n\nNotes and Times: https://github.com/agsb/milliForth-6502/blob/main/Notes.md ↩",
    "summary": {
      "en": "**Summary of milliForth for 6502**\n\nmilliForth is a very compact version of the Forth programming language, measuring just 328 bytes, making it one of the smallest programming languages available. It is specifically designed for the 8-bit 6502 CPU, contrasting with other Forth versions that target 16-bit CPUs like x86.\n\nKey features of milliForth include:\n\n- **Interpreter Types**: It employs two models for its inner interpreter: Direct Thread Code (DTC) and Minimal Thread Code (MTC). DTC is preferred for its smaller size.\n- **Focus on Size**: The coding prioritizes size over performance, using tools like ca65 for compilation.\n- **Memory Management**: The design utilizes specific memory pages for various functions, including storage for input and output buffers, data stacks, and a return stack.\n- **Primitives and Internals**: It includes basic operations (like addition and fetching values), stack management, and functions to handle the terminal and compilation.\n- **Language Design**: milliForth aims to maintain simplicity and efficiency, drawing inspiration from sectorForth while ensuring it remains minimal.\n\nThis project also serves as a testbed for comparing DTC and MTC, with both models successfully running a sample program. The coding practices and design choices reflect an emphasis on fitting within the constraints of the 6502 architecture. \n\nFor more details, additional resources and references are provided, including links to the original milliForth and similar projects.",
      "ko": "milliForth는 Forth 프로그래밍 언어의 매우 간결한 버전으로, 크기가 단 328바이트에 불과해 가장 작은 프로그래밍 언어 중 하나로 평가받고 있습니다. 이 언어는 8비트 6502 CPU를 위해 특별히 설계되었으며, x86과 같은 16비트 CPU를 대상으로 하는 다른 Forth 버전과는 차별화됩니다.\n\nmilliForth의 주요 특징 중 하나는 두 가지 내부 인터프리터 모델인 직접 스레드 코드(Direct Thread Code, DTC)와 최소 스레드 코드(Minimal Thread Code, MTC)를 사용한다는 점입니다. DTC는 크기가 작아 선호됩니다. 또한, 코드는 성능보다 크기를 우선시하며, ca65와 같은 도구를 사용하여 컴파일합니다.\n\n메모리 관리 측면에서는 다양한 기능을 위해 특정 메모리 페이지를 활용합니다. 여기에는 입력 및 출력 버퍼, 데이터 스택, 반환 스택을 위한 저장 공간이 포함됩니다. 기본적인 연산(예: 덧셈 및 값 가져오기), 스택 관리, 터미널 및 컴파일을 처리하는 기능도 포함되어 있습니다.\n\nmilliForth는 간결함과 효율성을 유지하는 것을 목표로 하며, sectorForth에서 영감을 받아 최소한의 형태를 유지하고자 합니다. 이 프로젝트는 DTC와 MTC를 비교하기 위한 테스트베드 역할도 하며, 두 모델 모두 샘플 프로그램을 성공적으로 실행할 수 있습니다. 코딩 관행과 설계 선택은 6502 아키텍처의 제약에 맞추는 데 중점을 두고 있습니다.\n\n더 많은 정보와 자료는 원본 milliForth 및 유사한 프로젝트에 대한 링크를 포함하여 제공됩니다.",
      "ja": null
    }
  },
  {
    "id": "a8137c3c4854deab",
    "title": {
      "en": "Building a modern durable execution engine from first principles",
      "ko": "현대적 실행 엔진 구축하기",
      "ja": null
    },
    "type": "story",
    "url": "https://restate.dev/blog/building-a-modern-durable-execution-engine-from-first-principles/",
    "score": 38,
    "by": "whoiskatrin",
    "time": 1743083504,
    "content": "Building a modern Durable Execution Engine from First PrinciplesHow Restate works, Part 2Posted February 20, 2025 by Stephan Ewen, Ahmed Farghal, and Till Rohrmann‐20min readWe dive into the architecture details of Restate, a Durable Execution engine we built from the ground up. Restate requires no database/log or other system, but implements a full stack that competes with the best logs in terms of durability and operations.This is the second article in our series on building a durable execution system from first principles. The first blog post in this series, Every System is a Log, looks at this from the application side, and shows how a unified log architecture results in a tremendous simplification of distributed coordination logic. This post discusses the details of how we built the log-based runtime for that paradigm.Modelling locking and database updates through an application log,taken from Every System is a LogTo build this runtime, we asked ourselves, what such a system would look like when designed from first principles? We built a precursor to this with Stateful Functions, and from all the lessons learned there, we arrived at a design with a self-contained complete stack, centered around a command log and event-processor, shipping as a single Rust binary. To get an idea of the user experience we arrived at, check the videos in the announcement post.This stands somewhat in contrast to the common wisdom “don’t build a new stateful system, just use Postgres”. But we saw a clear case to build a new stack, for multiple reasons: First, the interactions and access patterns are different enough from existing systems that we can offer both better performance and operational behavior, similar to why message queues exist, even though you can queue with a SQL table. Second, the architecture of event logs has made significant advancements in recent years, but the advanced implementations are exclusive to proprietary stacks and managed offerings - the open source logs and queues still follow architectures from the on-prem era. Third, we saw how a converged stack lets us provide a much better end-to-end developer experience, from first experiments on your laptop to multi-region production deployments.Recap: Server and Services #A Restate application stack consists of two components: Restate Server, which sits at a similar place in your stack as a message broker, and the application services, which are durable functions/handlers containing the application logic. The server receives invocation events, persists them, and pushes them to the services, similar to an event broker. The services run the code that corresponds to RPC- or event-handlers, workflows, activities, or actors. Services may run as processes, containers, or even serverless functions.But Restate doesn’t just push invocations, it maintains a bidirectional connection with the executing service handler and lets the code perform durable actions as part of the invocation, including journaling steps, sending events to other handlers, accessing/modifying state, creating persistent futures (callbacks) and timers. The services use a thin SDK library which communicates the actions to the server - somewhat comparable to a KafkaConsumer or JDBC client, but more high-level. See Restate’s examples for sample code and details.The server handles all coordination and durability for the invocation life cycle, journals, embedded K/V state, and manages failover, leader-election, and fencing. The server’s view on an invocation and its journal is the ground truth; the services follow the server’s view and function executions may be cancelled/reset/retried as needed.That approach makes the services completely stateless and simple to operate. They scale rapidly and run on serverless infrastructure like AWS Lambda, Cloudflare workers, etc.This characteristic also lets us build those SDKs in many languages fairly easily, including TypeScript, Java, Kotlin, Python, Go, and Rust.Clusters, Object Stores, and the Latency Gap #A distributed Restate deployment is a cluster of nodes that are connected to each other. Invocations and events can be sent to any node, and all nodes participate in the storage of events and dispatching of invocations to services/functions. Restate is active/active from a cluster-perspective, but has leader/follower roles at the granularity of individual partitions, similar to systems like Kafka or TiKV.Restate stores data using two mechanisms: New events (invocations, journal entries, state updates, …) are persisted in an embedded replicated log (called Bifrost). From there, events move to state indexes in RocksDB, which are periodically snapshotted to an object store. So at any point in time, the majority of the data is durable in the object store (the nodes maintain a copy as cache) while a smaller part of the data is durable in the log replicated across the nodes.This is a form of storage tiering, though not the classical tiering like in modern logs. It is more similar to a database management system, where the write-ahead-log (WAL) would be replicated across nodes, while the table data files and indexes would be persisted on S3 (and cached on the nodes).Object store + latency gap #Architectures that keep most- or all - of their data on object stores have become popular for many reasons: Object stores are unbeatable in terms of combined scalability, durability, and cost (AWS S3 cites eleven 9s of durability, stores more than 100 trillion objects, and is cheaper than persistent disks). Plus, the storage exists disaggregated from compute nodes, making the nodes stateless (or owning little state), which is highly desirable for efficient operations.Object stores are also available in most on-prem setups we’ve encountered. It was natural for us to design Restate such that object storage would be the primary durability for the majority of the data.The reason why Restate has additionally a replication layer that persists new events (rather than writing events straight to object storage) is to provide low latency. Pure object store approaches have latencies that average around 100ms to make data durable, with tail latencies being a multiple of that. While that is feasible for analytical systems (e.g., Apache Flink) and data pipelines (like WarpStream), such latencies can quickly become prohibitive for many applications.Restate’s replication bridges the latency gap between the requirements of fast durable functions and the capabilities of object storage.Navigating the cloud latency-cost-disk triad #The setup described above is what we ship first, in Restate 1.2: a fast log replicated between Restate server nodes. However, Restate uses virtual log abstraction, to easily support other log implementations as well, without having to build a full consensus machinery each time. This is a defining feature of Restate’s runtime implementation that we’ll dive deeper into in the next article in this series. We are currently using that mechanism to build object-store support in the log as well, which is a powerful feature for bringing the amount of data persistent on nodes to very small amounts, even zero.There is no single best configuration for that setup - only a spectrum of trade-offs to pick from. In his Materialized View newsletter, Chris Riccomini describes it as a CAP-theorem-like choice:In our context (durable execution runtime), durability must be a given, but we have the additional dimension of how much replicated data is kept on the nodes. Restate’s triad thus is: latency-cost-disk.➕ Low latency, ➕ low cost, ➖some data on disks: Quorum replication to nodes with async batch writes to S3.The nodes provide fast durability through replication and keep the data for anything between a few 100ms and minutes, before moving the events to object storage. Restate 1.2 can be seen as a variant with a long flush interval.➕ Low latency, ➖ high cost, ➕ no data on disks: Quorum replication directly to S3 Express One Zone.Restate’s replication mechanism deals only with ordering, quorum, and repairs upon loss of a zone, but doesn’t keep data on the nodes (except caches).➖ High latency, ➕ low cost, ➕ no data on disks: Synchronous batch writes to S3.Restate’s replication mechanism isn’t used.Naturally, there are nuances: direct replication has an even lower latency than S3 Express 1Z quorums. Synchronous batch writes to S3 can be cheaper than anything else, because that approach may avoid cross AZ bandwidth cost. Disks still exist as caches in all configurations. And there is the option of using quorum replication to S3 Express 1Z in different regions, to support multi-region deployments without relying on disks. But it shows that there is the spectrum of options, with which we aim to enable developers to use Restate in diverse setups across cloud and on-prem, while maintaining a simple dependency: just an object store.Restate 1.2 ships with all the virtual log infrastructure, and a low-latency replicated log implementation. We are currently working on the other configurations - if you are interested in being an early tester or design partner, please reach out to us.As a final thought, being able to adjust to different trade-offs also helps Restate and its users adapt to changing cloud pricing models. To quote another prolific dist. sys. writer:Partitioned Scale out #Restate follows the partitioned scale-out model: A cluster has a set of partitions, each with a log partition and an event processor instance. Partitions operate independently and allow the system to scale both across cores and nodes.Everything related to an invocation happens within a single partition: invocation, idempotency & deduplication, journal entries, state, promises/futures, avoiding the need to synchronize and coordinate with any other shards. The target partition for an invocation is determined by hashing the virtual object key, workflow ID, or idempotency key, if applicable - otherwise, the partition is freely chosen.In some cases, a function execution produces an event for another partition, for example RPC events or completions. In that case, events are still written to the local partition, and the server has an out-of-band exactly-once event shuffler to move events to the right target partition.The partitions are not exposed to applications (though you see them when using restatectl) - only keys are directly addressable (virtual object id, workflow id, idempotency key), to allow changing the number of partitions without losing consistency.From here on, we look only at what happens inside a single partition.Event Log and Processor #The work that Restate Server does inside one partition happens in two components: the distributed durable log (called Bifrost) and the processors. The log is the fast primary durability for events (e.g., make invocations, add journal entry, update state, create durable promise, …), the processor acts on events (e.g., invokes handlers) and materializes their state. Log and processor are co-partitioned, meaning a partition processor connects to one log partition. They are independent, but frequently co-located in the same process.Compared to databases, you can think of Bifrost as the transaction WAL, and the processor as the query engine and table storage. Compared to stream processing, you can think of Restate’s log as Kafka and the processor as a stream processing application (like KStreams or Flink).Log and processor form a tight loop: the processor continuously tails the log, and acts on the events (e.g., making invocation). That may produce more events (journal entries, state updates, …), which are written to the log and again processed by the processor.Let’s go through an example to illustrate this:A client invokes service handler processPayment with idempotency key K through Restate. The ingress enqueues the invocation to the log partition, as determined by hashing K.The leader Processor for the partition receives that event and checks its local idempotency key state. K is not contained there for processPayment. The processor atomically adds K to the state and transitions the invocation to running, then builds a connection to the target service endpoint, and pushes the invoke journal entry.The service streams back a step result event (ctx.run({...})) and the processor enqueues that journal entry event in the log. Being persistent in the log is the point when “the step happened”, meaning from there on it will always be recovered.When the processor receives the event from the log (that means no other processor has taken over leadership in the meantime) then it adds the event to the invocation’s journal state and sends an ack to the service.Similar steps happen when the service sends a state update, a timer, an RPC event, or creates a durable promise. Events get always added to the log first, and once they are received by the processor they are acted upon (e.g., added to journal, routed as invocation to other service, etc.)Once the invocation completes, the Processor adds the result event to the log. Upon receiving that event, it sets the invocation state as complete and sends the result back to the client.When the function execution fails (e.g., crash, loss of connection, user-defined error), the processor dispatches a new invocation, attaching the full journal events from this invocation so far. To avoid split brain scenarios between services, the processor tracks invocation execution attempts (retries) and rejects events sent from an invocation if a newer attempt has started. This can be tracked with simple in-memory state, because invocations are sticky to partitions, and partitions have strong leaders.State storage #The processor stores all its non-transient state in an embedded RocksDB instance. Operations on that embedded store are very fast, but the state is lost when the node is lost. However, all state of the processor is deterministically derived from the durable log and can always be rebuilt from the log during recovery. To avoid arbitrarily long re-build phases, the RocksDB database is periodically snapshotted to the object store and the log is trimmed to the point of the snapshot. Processors can be restored by fetching the latest snapshot and attaching to the log at the event sequence number when the snapshot was taken.The implementation of the partition processor is a tight event loop in Rust’s Tokio runtime. Partition Processors operate independently from each other and access exclusively local data structures (in memory, RocksDB, streams to ongoing invocations). The partition-local handling of invocations is easy in a log-first design, and would be much harder to achieve if we built this on a general purpose database.That property makes the design also both simple and fast: Committing an event (e.g., step / activity) means appending the event to the log (obtaining a write quorum). As soon as that happens, the event is pushed from the log leader in-memory to the attached processor and ack-ed to the handler/workflow. This takes a single network roundtrip for a replication quorum, and no distributed reads. The durability of RocksDB happens completely asynchronously in the background.Leaders and Followers #Both log and processors have one leader and optional followers. In the case of the log, followers increase durability for events through additional copies. In the case of processors, followers are hot standbys that have a copy of the state (deterministically derived from the log) and can quickly take over upon failure. Only the processor leader actually dispatches invocations for functions and workflows to the services, and only the leader writes snapshots to object store.High-level architecture and request flow.Control Plane, Data Plane, External Consensus #So far, everything we looked at was the data plane of the system: The log and the partition processor.Everything is coordinated by a control plane, which is responsible for failure detection, failover coordination, and re-configuration. The control plane stores metadata for the cluster (like configurations) and runs the cluster controller that handles partition placement and leader election.Control Plane and Data PlaneBecause Restate has one control plane for both log and processors, it can co-coordinate both, for example ensuring that the leader processor is always co-located with the log partition leader, to reduce network hops and optimize reading from local memory caches. In contrast, if we were to build this transparently on an external log like Kafka, this co-location would be harder to achieve. The benefits of this joint control plane show in many parts of the system and are one of the reasons Restate is simpler to set up, scale, and operate.Besides managing re-configurations and failover, the control plane also provides the external consensus for the data plane, allowing the data plane to operate more efficiently and with simpler properties than full consensus. We’ll go into the details of Restate’s log implementation in the next blog post - for now, a useful high-level way to think about this is that the control plane moves the data plane from one steady configuration to another, whenever the previous configuration is no longer functional (a failure happened) or desired (e.g., re-balancing). This blog post from Jack Vanlightly gives a nice introduction to that concept.\nThe Control Plane reconfigures the Data Plane (Figure from “An Introduction to Virtual Consensus in Delos “by Jack Vanlightly)Another benefit of this design is that it allows Restate to use a simpler/slower implementation of consensus on the control plane, because it is rarely invoked. Restate abstracts its consensus to just an atomic compare-and-swap (CAS) metadata operation, which the built-in metadata store backs with an implementation of the RAFT consensus algorithm. But this can be easily extended to plug in different storage systems, as long as they support atomic CAS.Failover & Reconfiguration #Though the control plane jointly coordinates log and processor reconfiguration, each has their own mechanisms to ensure consistency.A segmented log (Figure from “An Introduction to Virtual Consensus in Delos “by Jack Vanlightly)Bifrost’s (the log’s) mechanism is based on a mix of Delos (Virtual Consensus) and LogDevice. From a high-level, bifrost is segmented and failover or reconfiguration seals the active segment and creates a new segment, possibly with a new leader and a different set of nodes that store replicas. To the outside and the partition processors, everything looks like a single contiguous log.When a partition processor fails, the control plane selects a new leader for that partition.The failover procedure relies on the external consensus provided by the control plane: New leaders obtain the next epoch in a strictly monotonous sequence (so newer leaders have higher epochs). The new leader appends a message to the log to signal their epoch is now active, and then simply starts appending events from its operations. The old leader (who might still be following the log) will receive that epoch bump message and step down at that exact point - it will keep materializing state (as a follower) but not dispatch invocations any more. The old leader also aborts ongoing function executions and lets the new leader recover those.Leader handover via messages in the logAny messages carrying lower epochs than the latest epoch-bumping message will be ignored, which filters messages that the old leader might have still been appending to the log before it found out that it was superseded by another leader. If the old leader was attempting to commit a journal entry, but the message was appended after the epoch-bump message, that commit cannot happen: The new leader will (or might have already) recover the process without that journal entry and execute and commit that step. This mechanism ensures that any step / activity result is committed exactly once. No split brain view is possible.This mechanism also automatically resolves concurrent competition for leadership - the highest epoch will win, and late events are consistently ignored.Converged Single Binary #Restate is architected as a set of individual components that communicate with each other and make no assumptions about the whereabouts of their peers. A Restate binary can run every component or a subset of them; a set of components is described by a role.The default configuration is the converged mode, where every binary runs every role. In that case, you get a distributed architecture in a single binary. You can start more instances of the binary to form clusters. This mode is easy to use and efficient, because it also lets different components communicate efficiently through in-memory channels and caches whenever possible (e.g., log to processor).The roles and components of RestateHowever, you can of course also run it as a disaggregated setup, where different sets of nodes run different roles. That lets you separate control plane from data plane and pick your best trade-offs in terms of cost/durability/availability for metadata and data. For example:Deploy three nodes with the admin role across three different regions, ensure application and consensus metadata are disaster proof.Deploy six nodes with the Log-Server role across three availability zones, making sure data is replicated to tolerate zone outages.Deploy Ingress and Worker roles in one availability zone to strictly co-locate them with zone-local services.Restate’s architecture gives you a great developer experience from the start (launching a single binary on your laptop) all the way to sophisticated distributed deployments (disaggregated distributed setups).Some Performance Numbers #How fast can a system like this be? The answer is, we don’t really know, we have plenty more optimization we can do. Our main focus for this release was durability, resilience, and operational tooling.But even before any deep performance optimization, the system already pushes some pretty cool numbers, both latency- and throughput wise. Below are numbers from Restate 1.2. We measure the throughput / latency of the server against mock functions and activities, to put maximum stress on the server.Latency #Restate aims to keep latencies low, despite giving strong guarantees on durability (replication) and consistency (strong consensus on locking keys, workflow ids, etc.).Below are the numbers for durable functions with an increasing number of intermediate durable steps, running on a 3-way replicated cluster. Under low load, things are generally fast and a single step has a median latency of 3ms. Under high load, steps still have a median latency of 10ms after the initial workflow setup. Tail latencies under low load are a bit higher than we like (possibly caused by some Tokio / RocksDB issues) and we believe we can get these down in the future.LoadIntermediate StepsLatencyp50p90p99Low (10 concurrent clients)05ms34ms54ms315ms42ms69ms931ms57ms93ms2761ms106ms155msHigh (1200 concurrent clients)028ms41ms58ms358ms76ms98ms9116ms138ms163ms27283ms320ms356msOne thing you can observe here is that Restate is built to make durable steps cheap. An initial function or workflow invocation needs to check whether the workflow ID, idempotency key, or object key already exists and whether it is under execution. But once an invocation has been made, adding steps is just the equivalent of a conditional append to the log.Throughput #We ran a workflow of 9 intermediate durable steps (totalling 11 actions, including invoke/result), using 1200 concurrent clients to submit workflows.Restate pushes 94,286 actions (steps) per second, equalling 8,571 full workflows each second. The system maintains a p50 of 116.36ms and a p99 of 163.33ms for the full workflow! The p50 per step is 10ms.The experiment ran on AWS c6id.8xlarge nodes, which are admittedly beefy, but also deliver a throughput that most companies will not ever reach or exceed in their transactional load. And, if they exceed that, Restate still scales to more nodes through more partitions.Up next: a fast, flexible, state-of-the-art log #We mentioned that we built our own implementation of a distributed replicated log (called Bifrost), because we didn’t find any of the existing logs suitable in terms of latency (single roundtrip, quorum replication with external consensus), durability (active-active with flexible quorums), flexibility (segmented log that can be dynamically reconfigured).The next post in this series will dig into all the nitty gritty details of how we built that log. This log itself is a marvel of engineering and one of the reasons Restate is as powerful as it is. If you are into distributed systems, you will enjoy that one for sure!Try it out! #We hope you find this work as exciting as we do. If you want to try this out, the quickstart, helps to get you to your first demo app (and give us a star on GitHub).To get right into the distributed fun, check this guide to running a cluster locally.Let us know what you think or ask questions in our community on Discord or Slack and get more deep content like this from us on X, LinkedIn, Bluesky, or via email.Restate is also available as a fully managed cloud service, if all you want is to use it and let us operate it.restate\nrelease\narchitecture\ndeployment",
    "summary": {
      "en": "**Summary of \"Building a Modern Durable Execution Engine from First Principles: How Restate Works\"**\n\nThis article, written by Stephan Ewen, Ahmed Farghal, and Till Rohrmann, is the second in a series about Restate, a new Durable Execution Engine. Unlike traditional systems, Restate does not rely on external databases or logs and aims to provide high durability and operational efficiency.\n\n**Key Points:**\n\n1. **Unified Log Architecture**: Restate simplifies distributed coordination by using a log-based runtime, improving performance compared to existing systems like Postgres.\n\n2. **System Structure**:\n   - **Restate Server**: Acts like a message broker, receiving events and dispatching them to application services.\n   - **Application Services**: Durable functions that execute application logic, manage events, and maintain state.\n\n3. **Bidirectional Communication**: The server maintains a connection with services, allowing them to perform durable actions like journaling and state modification.\n\n4. **Data Storage**: Restate uses an embedded log for new events and stores data in RocksDB, which is periodically backed up to an object store for durability.\n\n5. **Latency Management**: To address the latency of object storage, Restate implements a replication layer that ensures low-latency operations.\n\n6. **Partitioned Scale-Out Model**: The system scales by using partitions, each with its own log and event processor, which operate independently to handle invocations.\n\n7. **Control Plane and Data Plane**: A control plane coordinates the overall system, managing failures and configurations, while the data plane handles logs and event processing.\n\n8. **Performance**: Early tests show promising low latencies (around 3ms to 10ms) and high throughput (over 94,000 actions per second), even under heavy load.\n\n9. **Future Improvements**: The next article will focus on the development of Restate's log implementation, called Bifrost, which aims to enhance performance and flexibility.\n\nOverall, Restate is designed to provide a better developer experience and operational efficiency for building durable execution systems. It's available as a managed cloud service, making it accessible for various users.",
      "ko": "이 글은 Stephan Ewen, Ahmed Farghal, 그리고 Till Rohrmann이 작성한 Restate라는 새로운 내구성 실행 엔진에 관한 두 번째 기사입니다. Restate는 전통적인 시스템과 달리 외부 데이터베이스나 로그에 의존하지 않으며, 높은 내구성과 운영 효율성을 목표로 하고 있습니다.\n\nRestate의 주요 특징 중 하나는 통합 로그 아키텍처입니다. 이 아키텍처는 로그 기반의 런타임을 사용하여 분산 조정을 단순화하고, 기존 시스템인 Postgres에 비해 성능을 향상시킵니다. Restate 서버는 메시지 브로커처럼 작동하여 이벤트를 수신하고 애플리케이션 서비스에 전달합니다. 애플리케이션 서비스는 애플리케이션 로직을 실행하고, 이벤트를 관리하며, 상태를 유지하는 내구성 있는 기능입니다.\n\n서버는 서비스와의 연결을 유지하여 저널링이나 상태 수정과 같은 내구성 있는 작업을 수행할 수 있도록 합니다. Restate는 새로운 이벤트를 위한 내장 로그를 사용하고, 데이터를 RocksDB에 저장합니다. 이 데이터는 주기적으로 객체 저장소에 백업되어 내구성을 보장합니다. 객체 저장소의 지연 문제를 해결하기 위해 Restate는 낮은 지연 시간을 보장하는 복제 계층을 구현했습니다.\n\n시스템은 파티션을 사용하여 확장되며, 각 파티션은 독립적으로 작동하는 로그와 이벤트 프로세서를 가지고 있어 호출을 처리합니다. 제어 평면은 전체 시스템을 조정하며, 실패와 구성을 관리하고, 데이터 평면은 로그와 이벤트 처리를 담당합니다. 초기 테스트 결과, Restate는 높은 부하에서도 약 3ms에서 10ms의 낮은 지연 시간과 초당 94,000건 이상의 높은 처리량을 보여주고 있습니다.\n\n앞으로의 개선 사항으로는 Restate의 로그 구현인 Bifrost의 개발에 초점을 맞춘 다음 기사가 예정되어 있으며, 이는 성능과 유연성을 향상시키는 것을 목표로 하고 있습니다. 전반적으로 Restate는 내구성 있는 실행 시스템을 구축하는 데 있어 더 나은 개발자 경험과 운영 효율성을 제공하도록 설계되었습니다. 이 서비스는 관리형 클라우드 서비스로 제공되어 다양한 사용자들이 접근할 수 있습니다.",
      "ja": null
    }
  },
  {
    "id": "0d3dbde9a74eb778",
    "title": {
      "en": "Ask HN: can Wireless-CarPlay dongles steal your data?",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": null,
    "score": 7,
    "by": "concerned_citi",
    "time": 1742905228,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "327ccb524a039975",
    "title": {
      "en": "Show HN: Cursor IDE now remembers your coding prefs using MCP",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": null,
    "score": 70,
    "by": "roseway4",
    "time": 1743173099,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "c1c0ec42735e15d9",
    "title": {
      "en": "Framework 13 AMD Setup with FreeBSD",
      "ko": "프레임워크 13 AMD와 FreeBSD",
      "ja": null
    },
    "type": "story",
    "url": "https://euroquis.nl/freebsd/2025/03/16/framework.html",
    "score": 33,
    "by": "hggh",
    "time": 1742909640,
    "content": "Framework 13 AMD Setup with FreeBSD\n\n      Mar 16, 2025\n      • adridg\n\n    The FreeBSD Foundation exists to support the\nFreeBSD community and the FreeBSD project. Some of its projects are\naimed at improving the experience of FreeBSD on specific hardware.\nThere is an ongoing, and expanding, laptop experience project.\nTo expand that project further, the foundation has provided Framework\nlaptops to a bunch of developers working on the FreeBSD laptop and desktop\nexperience. I’m one of those developers, and here are some initial notes on the process.\nThe notes assume experience with FreeBSD.\n\n  Some disclaimers up front: the FreeBSD foundation is a lot like KDE e.V.,\nwhich supports the KDE community and project. I wear a board hat for KDE e.V.,\nbut on the FreeBSD side I’m “just a ports developer”. Of course, the ports\nI try to work on are the KDE ones, so there’s a happy synergy here.\n\n  An anonymous donor sponsored these machines. While I am part of the\nFreeBSD donations@ team, I was not involved in the overall decision-making\naround this donation.\n\nThe machine I got is a Framework 13 with an AMD 7000 series CPU.\nThat’s not the very-very latest one, which has a Ryzen 300 series in it,\nbut it is at least 3 CPU generations newer than any other machine I have.\nFor me in particular of interest is that it has\nthe same GPU series, AMD Polaris 12,  as my FreeBSD 14-STABLE desktop machine, so I can\nshare experimentation with graphics drivers between them.\n\nI picked the 2.8K display with rounded corners, because that’s potentially\nan interesting edge-case for the KDE Plasma 6 desktop; if there’s any\nfunny-stuff needed for those corners, then we need to know about it.\n\nLet’s Get Physical\n\nAlthough it’s completely irrelevant for the long-term use of the laptop,\nI’ve got to hand it to the Framework folks: the packaging is really nice.\nRecyclable cardboard, well-laid-out, understandable boxes. I don’t often\nget a “huh, that’s clever” reaction when unpacking consumer electronics.\n\nThere’s a screwdriver included, cunningly hidden beneath\nthe do-it-yourself-installation memory modules. That’s clever.\n\nWhen it comes to putting the machine together, the installation guide with videos is both\ncomprehensive and easy-to-follow. “Put DDR5 SO-DIMM modules in corresponding sockets”\nand “insert NVMe into socket” is straightforward, I do that all the time when (re)building desktop machines.\n\nThe bezel, on the other hand …\n\nThe bezel around the screen is just a thin bit of plastic. I got a red one,\nbecause FreeBSD (there is no KDE Blue option). It is essential to\nplace it correctly, with all the screen-cables nicely aligned. I did not,\nand just clicked the bezel in place, pushed down on it and then closed the laptop,\n“per the instructions”. Except the bezel stuck out about 2mm, and on re-opening the\nlaptop, it just about tore the bezel in half.\n\nAfter 20 tricky minutes I could get the laptop open again and removed the bezel,\nrepaired it, and tried again. I don’t really have a suggestion to improve the\nbezel installation except “try very carefully to close the laptop a bit, re-open,\nclose a bit further, re-open, …” until it’s clear that the lid closes properly.\nTake some time to (re)route the cables to the screen so that they are as flat\nas possible.\n\nAccessories\n\nThe little modules for the Framework laptop are pretty nifty. I’m already\nthinking I should have gotten an additional USB-C one. I selected one unusual module,\nRJ-45 wired ethernet, because my experience with FreeBSD and WiFi is\nnot a good one. However, that’s what this whole laptop project is for.\nThe FreeBSD Foundation has already funded work on laptop WiFi,\nso it’s probably over-cautiousness on my part.\n\nWith all the physical bits in place, the big question…\n\nWill it run Doom?\n\n  Framework 13 AMD DIY build with FreeBSD 14.2 boot screen. It sure looks like it could be Doom.\n\nOf course. Don’t be silly.\n\nWill it run FreeBSD?\n\nYes, but that takes a little bit of effort. Download a FreeBSD 14.2 image\nand write it to a USB stick on some other machine. Leave it on your desk for now.\n\nBoot the Framework laptop for the first time and let it do memory training and whatnot.\nDo not connect any devices and let it complain that there’s nothing to boot.\n\nReboot, still with nothing attached, and spam F2 during boot. You have to\ndo this to get to the EFI shell / system configuration before it tries\nto boot anything. Disable secure boot. Linuxes have a signed GRUB shim\nnowadays, or other bits and pieces so they work with secure boot.\nFreeBSD 14.2 does not, yet.\n\nNow insert the USB stick, reboot, and go through the installer process.\nIt’s a text installer (still, as I still haven’t built FreeBSD support in Calamares)\nand gets you to a working system in about 5 minutes. Having the wired ethernet\nhelps avoid any trouble here.\n\nReboot after installation and you can get a text console. All that\ntechnology for a late-80s user experience.\n\nWill it run X11?\n\nYes, but the 14.2-RELEASE Errata point\nout that DRM kernel modules do not work if you grab the pre-built ones.\nThis was true on March 12th 2025, so:\n\n  Run pkg to install the package manager (initially it is a stub)\n  Run pkg install git to install git (this pulls in a surprising amount of other stuff)\n  Get the system sources (with git)\n  Rebuild the world and install it\n  Get the ports tree (with git)\n  Build graphics/drm-61-kmod from ports (just make ; make install, and the port itself is a real quick build)\n  Build graphics/gpu-firmware-amd-kmod from ports, remember FLAVOR=polaris12 for the GPU in this laptop (otherwise the default flavor is built)\n\nAfter that, enable the amdgpu module in rc.conf, or load it by hand.\nAny old X11 stuff will do, but I suggest installing x11/kde and x11/sddm.\n\nWill it run KDE Plasma 6 Wayland?\n\nHahaha. No. But yes.\n\nKDE Plasma 6 on Wayland in general works. But on this specific machine,\nwith this specific grapics card, Plasma starts, all the processes\nof a KDE Plasma desktop are running, and the screen displays a single\nwhite text-cursor in the upper-left corner.\n\nIt’s not this-specific-machine, either, since I have a desktop\nwith Intel CPU and an AMD RX550 video card that behaves the same.\n\nLast time I dug into KWin internals in an attempt to figure this out\nI ended up with “some part of the OpenGL stack is lying” and then\ngave up. Now with a fresh laptop that just cries out for a modern\ndesktop, I’m going to try again.",
    "summary": {
      "en": "**Summary of Framework 13 AMD Setup with FreeBSD**\n\nThe FreeBSD Foundation supports the FreeBSD community and is currently enhancing the FreeBSD experience on laptops, including Framework laptops provided to developers. The author received a Framework 13 laptop with an AMD 7000 series CPU, which is useful for testing graphics drivers due to its similar GPU to their desktop machine.\n\nThe laptop's packaging was praised for its clever design, but the installation of the screen bezel was tricky, requiring careful alignment to avoid damage.\n\nThe author chose a wired ethernet module instead of WiFi, due to past issues with FreeBSD and WiFi. They confirmed that FreeBSD 14.2 can be installed on the laptop, but it requires disabling secure boot and using a USB stick for installation.\n\nAfter installation, the system boots into a basic text console. While X11 works after some setup, running KDE Plasma 6 on Wayland does not function properly on this machine, displaying only a text cursor. The author plans to investigate this further.",
      "ko": "FreeBSD 재단은 FreeBSD 커뮤니티를 지원하며, 현재 개발자에게 제공되는 Framework 노트북을 포함해 노트북에서 FreeBSD 경험을 향상시키고 있습니다. 필자는 AMD 7000 시리즈 CPU가 장착된 Framework 13 노트북을 받았으며, 이 노트북은 데스크탑 기계와 유사한 GPU 덕분에 그래픽 드라이버 테스트에 유용합니다.\n\n노트북의 포장은 독창적인 디자인으로 호평을 받았지만, 화면 베젤 설치는 까다로워서 손상을 피하기 위해 신중한 정렬이 필요했습니다. 필자는 과거 FreeBSD와 WiFi 관련 문제로 인해 유선 이더넷 모듈을 선택했습니다. FreeBSD 14.2가 이 노트북에 설치될 수 있다는 것을 확인했지만, 설치를 위해서는 보안 부팅을 비활성화하고 USB 스틱을 사용해야 합니다.\n\n설치 후 시스템은 기본 텍스트 콘솔로 부팅됩니다. X11은 약간의 설정 후 작동하지만, Wayland에서 KDE Plasma 6을 실행하면 제대로 작동하지 않고 텍스트 커서만 표시됩니다. 필자는 이 문제를 더 조사할 계획입니다.",
      "ja": null
    }
  },
  {
    "id": "44e224563abf2956",
    "title": {
      "en": "I asked police to send me their public surveillance footage of my car",
      "ko": "내 차 CCTV 요청!",
      "ja": null
    },
    "type": "story",
    "url": "https://cardinalnews.org/2025/03/28/i-drove-300-miles-in-rural-virginia-then-asked-police-to-send-me-their-public-surveillance-footage-of-my-car-heres-what-i-learned/",
    "score": 449,
    "by": "bookofjoe",
    "time": 1743164046,
    "content": "<iframe title=\"Everlit Audio Player\" src=\"https://everlit.audio/embeds/artl_eQjyeH7pmVP?client=wp&amp;client_version=1.10.5\" width=\"100%\" height=\"130px\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen=\"\"></iframe>Two police officers walked into a doughnut shop.\n\nIt’s not the opening line of a joke; it’s what I saw as I was working on an early draft of this story in March at the Staunton Dunkin’, about a quarter mile from where my vehicle was captured on a Flock camera in January and February coming back from my trips to Cardinal’s Roanoke office.\n\nTheir eyes may have strayed to the racks of Boston creme, lemon-filled and coconut-covered doughnuts as they strode to the counter with purpose, but they were here for something else.\n\nSurveillance footage.\n\nwindow.zone_load_1502249140 = function(z, d) { if (!d.count) document.getElementById('zone_load_1502249140').style.display = 'none'; };\n\n\t\t\t\tDon't miss another story! Sign up for Cardinal’s free daily newsletter.\n\nDelivered to your inbox every day at 5 a.m.\n\n\t\t\t\t\t\tSign up\n\n\t\tThe research for State of Surveillance showed that you can’t drive anywhere without going through a town, city or county that’s using public surveillance of some kind, mostly license plate reading cameras. I wondered how often I might be captured on camera just driving around to meet my reporters. Would the data over time display patterns that would make my behavior predictable to anyone looking at it?\n\nSo I took a daylong drive across Cardinal Country and asked 15 law enforcement agencies, using Freedom of Information Act requests, to provide me with the Flock LPR footage of my vehicle. My journey took me over 300 miles through slices of the communities those agencies serve, including the nearly 50 cameras they employ. And this journey may take me to one more place: an April Fool’s Day hearing in a courtroom in Roanoke. There, a judge will be asked to rule on a motion to declare the footage of the public to be beyond the reach of the public.\n\nBut while Roanoke and Botetourt and two other police agencies denied my request for that footage, nine agencies complied and searched their data for signs of me passing through.\n\nHere’s what I found.\n\nCheck out our podcast episode on Jeff’s surveillance investigation.\n\n   Watch    February 13, 2025, I left Staunton around 7:30 in the morning to head toward Roanoke. Richmond Avenue, on the outskirts of the city, is probably the way most people make their way out of town to interstates 64 and 81. It’s a significant crossroad of the region’s major east-west and north-south highways.\n\nStaunton maintains at least one of its six Flock cameras on a local intersection just shy of the cluster of on- and off-ramps. It makes surveillance-sense to position cameras to see who’s coming in and who’s leaving your town at such a singular crossroad.\n\nI was not captured by a Flock camera there, though.\n\nAs part of its services, Flock advises police on where to place its tech. The top priority appears to be places of entry and exit around the community, notably near the main highways. It’s possible that Staunton doesn’t have a camera taking pictures of who is leaving town; it’s also possible my vehicle’s plate was blocked by heavy morning traffic and so no photo could be taken.\n\nIt was a cold morning, but truckers and car drivers were behaving on the morning commute. Staying on I-81, I passed through Augusta, Rockbridge and Botetourt counties, which between them have at least eight Flock cameras. I didn’t think any would be pointed at the main highway because currently Flock can’t place its cameras on state property.\n\nNinety uneventful minutes later, I pulled into Roanoke to go to the Cardinal office and visit my Roanoke members of our own Cardinal team — which, in an unintentional irony in this story, we refer to as The Flock.\n\nI got into town just after 9:15 a.m. I know that because a Roanoke Police Department Flock camera captured my car traveling southbound down Williamson Road near the Salem Avenue intersection at 9:16:09 a.m. (That photo, as well as another, were provided by the Staunton police, as part of their arrangement to access other agencies’ data in their Flock searches.)\n\nYou can see from the image below exactly what Flock technology captures: a decent shot of the back of any vehicle that passes, a readable image of the license plate.\n\nPart of Flock’s proprietary tech determines the make and model of the vehicle and also notes if there are bumper stickers, bike racks, any other unique markings that would help identify that vehicle. That generates a “vehicle fingerprint” for every car or truck, which none of the agencies I FOIA’d would provide me. That fingerprint could prove helpful in the case where a witness or other camera captured some non-license-plate information about a vehicle, like specific bumper stickers or a roof rack.\n\nI parked my car on Church Avenue, walked to the office and logged in to our morning news meeting. Some of our reporters were there in person; others began popping up on the screen from their beats in Danville, Martinsville and Bristol. We talked about our day’s work. Afterward, I drove around town just to see if I’d be picked up in a residential area. I started in Gainsboro. Snow covered the ground around the homes on Gilmer Avenue. I did not notice any cameras.\n\nI crossed town to Marshall Avenue and a neighborhood within a few blocks of the YMCA, and then on to another neighborhood sitting next to Interstate 581, which reaches across the town like a tight belt of loud traffic. Looking between homes, I saw the Roanoke Star, perched over trees frosted with ice not yet melted.\n\nEach of these neighborhoods had different backstories and histories you could see in the architecture of their homes, in the cars that parked on their streets. One thing they had in common on that cold morning: They were all very quiet. And I did not see any surveillance cameras.\n\nLater, I received no images of my car in those places. Flock can be used to monitor public space in suspected high-crime areas, which has earned it the wrath of rights organizations including the ACLU. Because Roanoke has only five cameras, according to contracts we received from the city, it’s my guess they are not yet focusing on specific populations or neighborhoods.\n\nAfter those brief stops, I left town mid-morning. I can’t tell you exactly when, and I’ll tell you why that’s relevant.\n\nWhen I eventually received data from the Staunton Police about my trip, I noticed that Flock cameras had photographed my vehicle in similar locations within both Staunton and Roanoke at similar times on another day, January 29. If you asked me today if I knew whether I had made a trip to the Roanoke office on Jan. 29, I would hesitate before I could answer. I would have to check my calendar and emails to be able to say that I was there, with certainty.\n\nBut the police would have known, if they wanted to, without asking for any kind of warrant or court order.\n\nCheck out the other stories in this ongoing series.\n\n\t\t\t\t\t\tCity of Roanoke, Botetourt County sheriff go to court over FOIA request\n\n\t\t\t\t\t\tState of Surveillance: Everyone’s watching\n\n\t\t\tFranklin County does have four Flock cameras, but my vehicle’s image was not captured by any of them. Until I came into town, I was staying on routes 220 and 57.\n\nU.S. 220 was a misty spectacle on Feb. 13. Ice made trees sag. Thick limbs and branches crashed under the weight, closing the right lane of the highway in some places. Snow covered shaded places around buildings, but the roads were mostly clear, and traffic moved along. Nearing noon, milder temps had caused fog to rise up from the hollers. As I drove south past Boones Mill and Trump Town USA, I knew I would not trigger that town’s lone operational Flock camera. It’s set up to catch northbound traffic.\n\nI entered Martinsville via Fayette Street. Martinsville has dozens of Flock cameras, 48 according to the contracts Cardinal News gathered, so I expected to be picked up multiple times. However, my vehicle was detected only once.\n\nEven the police chief, Rob Fincher, was surprised. He was open to running the test again, but I wasn’t trying for statistical accuracy; I wanted this to be a record of a single day. There are lots of things that can get in the way of taking a clear picture, including glare and shadow and other things (cars in this case) getting between your camera and your subject. Some of those things may have been at play on that particular day.\n\nA Martinsville Flock camera did spot my vehicle at 12:11 p.m. eastbound on the way into town from its perch near the corner of West Church Street and South Memorial Boulevard.\n\nTwenty-two minutes later I was spreading cream cheese on a bagel and coffee at the Ground Floor. (I know the time because I took my own photo, not because of a surveillance camera timestamp.)\n\nThe place was bustling. On most tables stood a little rubbery Jesus toy. On one wall hangs a long roll of brown paper where people casually write their prayers. I was reminded that some people believe you’re being watched 24/7 by a higher power, though I’d argue there’s likely a pretty high trust factor about how that surveillance might be utilized. I touched base with our Martinsville reporter Dean-Paul Stephens, and then headed for Danville.\n\n* * *\n\nSpeaking of trust and ethics: two weeks later, Lt. Greg Jones called me at the Roanoke office. The Amherst County Sheriff’s Office had a question about my request for data about my vehicle.\n\n“You weren’t trying to spy on a cheating wife or something like that, were you?” he asked.\n\nI assured him that I wasn’t. As Cardinal Executive Director Luanne Rife points out in her column on Sunshine Week, public agencies don’t have to agree with why you’re asking for their public information. The idea is that it belongs to you already. They are under legal obligation to provide it to you.\n\nNot to say this question didn’t cause some thought and conversation in the newsroom. Public surveillance data like this could indeed be used to stalk an ex; it could also be used by a person suspicious their ex is stalking them to see if their ex’s vehicle actually could be found on the same roads as theirs and at the same times, which could then be used to secure a protective order or even open a criminal investigation. It could be used by private investigators to find bail jumpers and missing persons. Now imagine all those requests coming in to the local police agency…\n\nThe only reason it hadn’t happened yet was because people really didn’t know they could do that. Suddenly the cops could be in the position to find themselves spending hours looking up public surveillance for citizens with all sorts of reasons to utilize the data.\n\nSo was this a fool’s errand I was on? I didn’t think so. The police in over 80 of our local communities had chosen to start photographing citizens in their vehicles in public and sharing this with other agencies in our region and beyond, even out of state. I wasn’t the one running over 500 searches a month on its citizens, as the Roanoke police were doing. And who knows who they were running those searches on, and why?\n\n* * *\n\nBy the time I reached Danville, the weather was almost warm. The sun was out and glancing through the empty trees along Craghead Street and in through the plate glass windows of Links Coffee House.\n\nI found out after requesting data from Danville that while they did have a contract with Flock, they had not yet installed the Flock cameras, according to Matt Bell, the city’s PR specialist.\n\nThe coffee was good. The casual conversation surveillance was rich with interesting dialogues. But I had miles to go. It was just before 2 p.m. Time to get moving again.\n\n* * *\n\nTraffic in Lynchburg was heavy around 3:30 p.m. as I drove north along U.S. 29 Business. I figured there might be at least some of Lynchburg’s Flock cameras along the very busy Business 29, also known in that area as Wards Road.\n\nJust south of Liberty University, a Flock camera picked up my car near Wards Ferry Road. Lynchburg has at least a dozen Flock cameras, according to contracts we got from them during our reporting for our first State of Surveillance story. I figured one might be on this stretch of road.\n\nBy this point in the afternoon, the novelty of the day was wearing off. I got back on main route 29 and headed north.\n\nAlong the rest of the way, I passed through Amherst County, which has four Flock cameras; Nelson County, which has none; and Augusta County, with two cameras. Since I stuck to the main roads, U. S. 29 and then I-64, the chances of running into a camera were low. If I’d pulled off onto a main county road, things might have been different.\n\nIn March, Amherst would conduct a search and be unable to find my vehicle. Same with Augusta County.\n\nAt 4:59 p.m., I exited the highway onto Richmond Avenue in Staunton. This time a Flock camera spotted my vehicle and got a clear picture. I went home and ordered pizza.\n\nWhich brings me back to the cops in the coffee shop, a few weeks later.\n\n* * *\n\nAs I mentioned, the two police officers were not interested in doughnuts, or even coffee. They asked to speak to the manager. The counter person explained that the manager was at the other store across town. They asked if they could speak to that person on the phone.\n\nIt was then I noticed that a person who had come in with them was part of this conversation.\n\nFrom what I could gather, because I didn’t pull out my press badge and start asking questions, the young woman with them had been in some kind of incident; and that the police had determined that maybe some of the video footage that Dunkin takes of its drive-through may have caught the other car as it passed on the road beyond; or maybe the offending vehicle had come through the drive-through.\n\nIn a few minutes, the officers and the woman were guided behind the counter to review footage.\n\nThis scene somehow made me feel optimistic about how we’re already using such technology. It still operates under the notion that not all data belongs to the police. They have to ask, or convince a judge to give them a court order.\n\nYet just glancing at the footage I have included in this story, it’s also a little creepy to see how as few as four to six pictures, properly time- and date-stamped, can establish patterns that could enable someone to know with some likelihood how they could intercept me on my way to work one morning.\n\nThere are two differences between police use of other visual data (like a store’s security video) and Flock’s gathering of public footage (such as my car). In that first case, there’s a crime involved. And the privately captured video is granted to police voluntarily and for a good reason. It’s not theirs to take and examine at their leisure.\n\nPublic-facing LRP cameras like Flock’s, on the other hand, capture vast amounts of data unrelated to any criminal activity. And there’s zero oversight outside of the law enforcement community.This goes back to the idea that footage taken of me in public, non-investigative in nature, can be considered investigative and not subject to a public information request, and concerns me.\n\nThe idea that a law enforcement agency will claim the images that we see in this story are “investigative” in nature — and need to be protected from me — tells me that they are worried about something else. What is it?\n\nIt’s a paradigm shift where we go from having an expectation of privacy even in public spaces to its inverse. Not only do we not have a right to privacy in public; we don’t even have a right to see ourselves as the government and police might see us — a set of still moments in place and time from which they, not us, can decide what our story is.\n\nWe want to know what you think! Tell us what you think about surveillance or share your experiences here.\n\n\t\t\t\tEnjoying our free stories?\n\nDonate today to help Cardinal News remain free for everyone.\n\n\t\t\t\tOne-time\n\n\t\t\t\tMonthly\n\n\t\t\t\tAnnually\n\n\t\t\tOne-time\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t$150\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t$200\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t$500\n\n\t\t\t\t\t\t\t\t\t\t\t\tOther\n\n\t\t\t\t\t\t\t\t\t\t\t\tDonation amount\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t$\n\n\t\t\tMonthly\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t$15\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t$20\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t$50\n\n\t\t\t\t\t\t\t\t\t\t\t\tOther\n\n\t\t\t\t\t\t\t\t\t\t\t\tDonation amount\t\t\t\t\t\t\t\t\t\t\t\tper month\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t$\n\n\t\t\tAnnually\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t$150\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t$200\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t$500\n\n\t\t\t\t\t\t\t\t\t\t\t\tOther\n\n\t\t\t\t\t\t\t\t\t\t\t\tDonation amount\t\t\t\t\t\t\t\t\t\t\t\tper year\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t$\n\n\t\t\tThanks for joining our flock!\n\n\t\t\tDonate Now\n\n\tRelated stories\n\n\t\tTagged: Redbird Stories,State of Surveillance",
    "summary": {
      "en": "The text describes a personal investigation into the use of surveillance technology, specifically license plate reading (LPR) cameras, in public spaces. The author, while working on a story, noticed police officers entering a doughnut shop to review surveillance footage. This prompted him to explore how often he might be captured on camera during his travels for work.\n\nHe took a daylong trip across various counties, requesting footage from 15 law enforcement agencies through Freedom of Information Act requests. His journey revealed that while some agencies denied his requests, many complied and provided data about his vehicle. He noted the locations and times where his car was captured on camera, emphasizing how surveillance can reveal patterns of behavior.\n\nThe author reflected on the implications of such technology, indicating it could be used to track individuals without their knowledge. He expressed concerns about privacy, as public surveillance data can be accessed by police without a warrant, raising questions about oversight and individual rights. Ultimately, the investigation highlighted a shift in expectations of privacy in public spaces, suggesting that people may no longer have the right to view how they are monitored by the government.",
      "ko": "이 글은 공공 장소에서 감시 기술, 특히 번호판 인식 카메라의 사용에 대한 개인적인 조사를 다루고 있다. 저자는 이야기를 작업하던 중 경찰관들이 도넛 가게에 들어가 감시 영상을 검토하는 모습을 목격했다. 이 경험은 그가 업무로 이동하는 동안 얼마나 자주 카메라에 포착될 수 있는지를 탐구하게 만들었다.\n\n저자는 하루 동안 여러 카운티를 여행하며 정보공개법에 따라 15개 법 집행 기관에 영상을 요청했다. 그의 여정에서 일부 기관은 요청을 거부했지만, 많은 기관이 협조하여 그의 차량에 대한 데이터를 제공했다. 그는 자신의 차가 카메라에 포착된 장소와 시간을 기록하며, 감시가 행동 패턴을 드러낼 수 있음을 강조했다.\n\n저자는 이러한 기술의 함의에 대해 고민하며, 개인이 모르는 사이에 추적될 수 있다는 점을 지적했다. 그는 공공 감시 데이터가 경찰에 의해 영장 없이 접근될 수 있다는 점에서 사생활에 대한 우려를 표명하며, 감독과 개인의 권리에 대한 질문을 제기했다. 결국, 이 조사는 공공 장소에서의 사생활 기대치의 변화를 강조하며, 사람들이 정부에 의해 어떻게 감시되는지를 알 권리가 더 이상 없을지도 모른다는 점을 시사했다.",
      "ja": null
    }
  },
  {
    "id": "957629cb9498aa3f",
    "title": {
      "en": "Japanese scientists create new plastic that dissolves in saltwater overnight",
      "ko": "바다에서 사라지는 플라스틱",
      "ja": null
    },
    "type": "story",
    "url": "https://newatlas.com/materials/plastic-dissolves-ocean-overnight-no-microplastics/",
    "score": 138,
    "by": "bentobean",
    "time": 1743170950,
    "content": "Materials\n\n        New plastic dissolves in the ocean overnight, leaving no microplastics\n\nByMichael Irving\n\n        March 27, 2025\n\n    Facebook\n\n    Twitter\n\n    Flipboard\n\n    LinkedIn\n\n    Reddit\n\n        New plastic dissolves in the ocean overnight, leaving no microplastics\n\n            A sample sheet of the new biodegradable plasticRIKEN\n\n            View 2 Images\n\n                1/2\n\n            A sample sheet of the new biodegradable plasticRIKEN\n\n                2/2\n\n            An artist's impression of the new plastic, showing the strong bonds above the water and how they break down when submerged in saltwaterRIKEN\n\n        Plastics are durable and strong, which is great while they’re being used but frustrating when they end up in the environment. Scientists at RIKEN in Japan have developed a new type of plastic that’s just as stable in everyday use but dissolves quickly in saltwater, leaving behind safe compounds.The benefit of plastics is that they’re made with strong covalent bonds that hold their molecules together, meaning they take a lot of energy to break. This is why they’re so sturdy, long-lasting and perfect for everything from packaging to toys.But those same strong bonds become a problem after the useful life of a plastic product is over. That cup you used once and threw away will sit in landfill for decades, even centuries, before it fully breaks down. And when it does, it forms microplastic pieces that are turning up in all corners of the natural world, including our own bodies, where they wreak havoc on our health in ways we’re only just beginning to understand.RIKEN researchers have now developed a new type of plastic that can work just as well as the regular stuff when it’s needed, and break down readily into safe compounds when it’s not. It’s made of what are known as supramolecular polymers, which have reversible bonds that function like sticky notes that can be attached, removed and reattached, according to the team.The team wanted to make a specific type of supramolecular polymer that would be strong enough for the usual uses of plastic, but could also be made to break down quickly when required, under mild conditions and leaving only non-toxic compounds.After screening a range of molecules, the researchers identified a particular combination that seemed to have the right properties – sodium hexametaphosphate, which is a common food additive, and monomers based on guanidinium ions, which are used in fertilizers. When these two compounds are mixed together in water, they form a viscous material that can be dried to form plastics.A reaction between the two ingredients forms “salt bridges” between the molecules that make the material strong and flexible, like conventional plastic. However, when they’re soaked in saltwater, the electrolytes unlock those bonds, and the material dissolves.\n\n        An artist's impression of the new plastic, showing the strong bonds above the water and how they break down when submerged in saltwaterRIKEN\n\n        In practice, the team found that the material was just as strong as normal plastic during use, and was non-flammable, colorless and transparent. Immersed in saltwater though, the plastic completely dissolved in about eight and a half hours.There’s one major hurdle with any degradable plastic material of course: what if it comes into contact with the catalyst for its destruction before you want it to? A plastic cup is no good if certain liquids can dissolve it, after all.In this case, the team found that applying hydrophobic coatings prevented any early breaking down of the material. When you eventually want to dispose of it, a simple scratch on the surface was enough to let the saltwater back in, allowing the material to dissolve just as quickly as the non-coated sheets.While some biodegradable plastics can still leave behind harmful microplastics, this material breaks down into nitrogen and phosphorus, which are useful nutrients for plants and microbes. That said, too much of these can be disruptive to the environment as well, so the team suggests the best process might be to do the bulk of the recycling in specialized plants, where the resulting elements can be retrieved for future use.But if some of it does end up in the ocean, it will be far less harmful, and possibly even beneficial, compared to current plastic waste.A paper describing the research was published in the journal Science.Source: RIKEN",
    "summary": {
      "en": "Scientists at RIKEN in Japan have created a new type of plastic that dissolves in saltwater overnight without leaving harmful microplastics. This biodegradable plastic is made from supramolecular polymers, which have reversible bonds that allow them to be strong during use but break down easily when submerged in saltwater.\n\nThe new plastic is created by mixing sodium hexametaphosphate, a common food additive, with guanidinium ion-based monomers found in fertilizers. This combination forms a flexible and strong material that dissolves in about eight and a half hours in saltwater, breaking down into safe compounds like nitrogen and phosphorus, which can benefit plants and microbes.\n\nTo prevent the plastic from dissolving prematurely, researchers have developed hydrophobic coatings. This means the plastic can be used safely without breaking down until it is intentionally scratched to allow saltwater in for disposal.\n\nOverall, this new biodegradable plastic is a promising alternative to traditional plastics, as it poses less risk to the environment and can potentially be beneficial if it ends up in the ocean.",
      "ko": "일본 RIKEN의 과학자들이 소금물에 하룻밤 만에 녹아버리는 새로운 종류의 플라스틱을 개발했습니다. 이 플라스틱은 해로운 미세플라스틱을 남기지 않는 생분해성 소재로, 재조합 가능한 결합을 가진 초분자 폴리머로 만들어졌습니다. 이러한 결합 덕분에 사용 중에는 강한 성질을 유지하지만, 소금물에 잠기면 쉽게 분해됩니다.\n\n새로운 플라스틱은 일반적인 식품 첨가물인 헥사메타인산 나트륨과 비료에서 발견되는 구아니디늄 이온 기반의 단량체를 혼합하여 만들어집니다. 이 조합은 유연하고 강한 재료를 형성하며, 소금물에서 약 8시간 반 만에 녹아 안전한 화합물인 질소와 인으로 분해됩니다. 이 화합물은 식물과 미생물에 유익할 수 있습니다.\n\n플라스틱이 조기에 녹지 않도록 하기 위해 연구자들은 소수성 코팅을 개발했습니다. 이 코팅 덕분에 플라스틱은 고의로 긁어 소금물이 들어가도록 하지 않는 한 안전하게 사용할 수 있습니다.\n\n전반적으로 이 새로운 생분해성 플라스틱은 전통적인 플라스틱에 비해 환경에 미치는 위험이 적고, 바다에 버려졌을 때도 유익할 수 있는 가능성을 지닌 유망한 대안입니다.",
      "ja": null
    }
  },
  {
    "id": "fba455c66aeef148",
    "title": {
      "en": "Apple needs a Snow Sequoia",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://reviews.ofb.biz/safari/article/1300.html",
    "score": 909,
    "by": "trbutler",
    "time": 1743114765,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "2c99611a163fcd4a",
    "title": {
      "en": "TV is watching you – Companies inventing new ways to make money off your data",
      "ko": "TV가 지켜본다: 데이터로 돈 버는 기업들",
      "ja": null
    },
    "type": "story",
    "url": "https://www.vox.com/technology/405879/roku-amazon-netflix-moana-disney",
    "score": 8,
    "by": "rmason",
    "time": 1743194265,
    "content": "TechnologyYour TV is watching youRoku, Amazon, and practically every company in the streaming business are inventing new ways to make money off your data.by  Adam Clark EstesMar 27, 2025, 7:30 PM GMT+9FacebookLinkGetty Images; Paige Vickers/VoxAdam Clark Estes is a senior technology correspondent at Vox and author of the User Friendly newsletter. He’s spent 15 years covering the intersection of technology, culture, and politics at places like The Atlantic, Gizmodo, and Vice.Roku City, the oddly alluring cityscape screen saver, scrolls across millions of idle TVs every day. Recently, an island paradise appeared in the picture. In the foreground, a floating billboard invited me to subscribe to Disney+ and watch Moana 2 at the press of a button on my remote. The convenience, I don’t mind about the new era of ad-supported everything. The wiretapping, I do.Ads are obviously not new on TV. As long as we’ve been watching shows on glowing boxes, we’ve been watching commercials that provide the economic engine for the entire entertainment factory to operate. While streaming platforms offered a reprieve for a few years by charging monthly fees for commercial-free content, it’s now practically impossible to watch TV without seeing some sort of marketing. What’s happening more under the radar is that your TV is collecting data about you and your watching habits — sometimes by directly monitoring what’s on your screen — and serving you personalized ads on your TV or elsewhere.The screen that you once loved for private, uninterrupted Netflix-watching has become a big billboard that also spies on you.User FriendlyA weekly dispatch to make sure tech is working for you, instead of overwhelming you. From senior technology correspondent Adam Clark Estes.Email (required)Sign UpBy submitting your email, you agree to our Terms and Privacy Notice. This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply.This isn’t just a Roku problem, although the company found itself in hot water when some users were recently required to watch a video ad — a Moana 2 trailer — before they could access their TV’s home screen at all. Roku says this is just a test, but the fact that it’s similar to a feature Amazon rolled out over a year ago on Prime Video suggests that ads are generally getting more brazen on streaming platforms. How you feel about it depends a lot on your mindset and feelings about privacy. Your TV wants your dataThe TV business traditionally included three distinct entities. There’s the hardware, namely the TV itself; the entertainment, like movies and shows; and the ads, usually just commercials that interrupt your movies and shows. In the streaming era, tech companies want to control all three, a setup also known as vertical integration. If, say, Roku makes the TV, supplies the content, and sells the ads, then it stands to control the experience, set the rates, and make the most money. That’s business!Roku has done this very well. Although it was founded in 2002, Roku broke into the market in 2008 after Netflix invested $6 million in the company to make a set-top box that enabled any TV to stream Netflix content. It was literally called the Netflix Player by Roku. Over the course of the next 15 years, Roku would grow its hardware business to include streaming sticks, which are basically just smaller set-top-boxes; wireless soundbars, speakers, and subwoofers; and after licensing its operating system to third-party TV makers, its own affordable, Roku-branded smart TVs. RelatedThe Wild West of streaming TV is here and it’s freeWhile most people think of Roku as a hardware company, it actually transitioned into becoming an advertising company almost a decade ago. In the early days, you might see a banner ad on your home screen or a tile telling you to watch Game of Thrones on HBO Go. But after firing up a more serious ad business in 2016, Roku started selling targeted ads on the Roku Channel, a free, ad-supported TV (FAST) service across its devices in 2017. Roku even started making its own content, including a biopic of Weird Al Yankovic. Your TV is collecting data about you and your watching habits — sometimes by directly monitoring what’s on your screen — and serving you personalized ads on your TV or elsewhere.Things really ramped up when Roku started acquiring ad-tech companies, including Nielsen’s Advanced Video Advertising business in 2021. This helped Roku gain new insights into its audience in order to target ads better and ultimately charge more money for those ads. At the end of 2024, Roku reported annual ad revenues of $3.5 billion, which accounted for 85 percent of its total revenue — far higher than its hardware business. Roku also has 90 million users — millions more than Apple TV+ — who have become a gold mine of data, not just about what they watch on TV but also who they are and what they like. Today, it’s better to think of Roku not just as an advertising company or the folks who make cheap TVs and streaming sticks, but also as a data company with millions of detailed profiles.The magical world of Moana 2 made an appearance in Roku City when the movie hit streaming in March 2025. RokuRegarding the Moana 2 controversy, Roku said in a statement that the company’s growth “has and will always require continuous testing and innovation across design, navigation, content, and our first-rate advertising products.” The statement also said, “Our recent test is just the latest example, as we explore new ways to showcase brands and programming while still providing a delightful and simple user experience.”The shift toward ad-supported everything has been happening across the TV landscape. People buy new TVs less frequently these days, so TV makers want to make money off the TVs they’ve already sold. Samsung has Samsung Ads, LG has LG Ad Solutions, Vizio has Vizio Ads, and so on and so forth. Tech companies, notably Amazon and Google, have gotten into the mix too, not only making software and hardware for TVs but also leveraging the massive amount of data they have on their users to sell ads on their TV platforms. These companies also sell data to advertisers and data brokers, all in the interest of knowing as much about you as possible in the interest of targeting you more effectively. It could even be used to train AI.The wealth of Roku’s first-party data could be a gold mine for Amazon or Google, according to Laura Martin, an analyst at the investment bank Needham and Company. “Roku is the perfect size with a really strategic fit,” Martin told me, referring to a possible Amazon purchase. She added that Roku’s data could also be a boon for any company with AI ambitions, including OpenAI. “If I was a large language model, this is data I would absolutely want to own.”The streaming industry has faced a reckoning in recent years too: After years of prioritizing growth over all else, companies like Netflix and Disney finally had to start making money. That’s resulted in those companies charging more, bundling services, and introducing cheaper ad-supported tiers. For better or worse, ads are the future of the TV business, just as they were its past. “For consumers, it’s definitely a complicated ecosystem,” said Jon Giegengack, founder of Hub Entertainment Research. Giegengack argues, though, that this ecosystem is ultimately better for consumers. In effect, there’s a streaming option that works for any budget, and ads fill in the gaps.RelatedThe streaming boom is overBut not everyone is thrilled to be bombarded by ads and to have their data passively harvested. More ads also means less attention paid to the content you want to watch and more to the ads these companies want you to see. Nevertheless, the trade-off is worth it to a lot of Americans. Some 43 percent of all streaming subscriptions in the United States were ad-supported by the end of last year, according to the market data firm Antenna. Even if you pay for an ad-free tier, you’re contributing to the ad ecosystem by giving up your data to whatever streaming platforms you use and even the company that makes your TV.Is it possible to escape the ads? Breaking free from this ad prison is tough. Most TVs on the market today come with a technology called automatic content recognition (ACR) built in. This is basically Shazam for TV — Shazam itself helped popularize the tech — and gives smart TV platforms the ability to monitor what you’re watching by either taking screenshots or capturing audio snippets while you’re watching. (This happens at the signal level, not from actual microphone recordings from the TV.) Advertisers and TV companies use ACR tech to collect data about your habits that are otherwise hard to track, like if you watch live TV with an antenna. They use that data to build out a profile of you in order to better target ads. ACR also works with devices, like gaming consoles, that you plug into your TV through HDMI cables. Yash Vekaria, a PhD candidate at UC Davis, called the HDMI spying “the most egregious thing we found” in his research for a paper published last year on how ACR technology works. And I have to admit that I had not heard of ACR until I came across Vekaria’s research.“They haven’t kept it secret, but there’s no awareness about it,” Vekaria told me. “So if people don’t know, they will not question it.”One surprising thingIt’s very difficult to watch streaming TV and avoid ads altogether these days. One, perhaps surprising option? Your local library. An app called Kanopy taps into the collections of local libraries across the country and has tons of great classic movies, documentaries, and indie films. It’s also free and ad-free — all you need is a library card.While ACR is popular across platforms, Roku is especially excited about the technology. Many of the companies that Roku has acquired in recent years have been working on ACR, and a Roku-owned company won an Emmy in 2023 for its work on the technology. Roku has also said that, because its share of the TV operating system market is 40 percent, the scale of its data collection capabilities is “unparalleled.” Unfortunately, you don’t have much of a choice when it comes to ACR on your TV. You probably enabled the technology when you first set up your TV and accepted its privacy policy. If you refuse to do this, a lot of the functions on your TV won’t work. You can also accept the policy and then disable ACR on your TV’s settings, but that could disable certain features too. In 2017, Vizio settled a class-action lawsuit for tracking users by default. If you want to turn off this tracking technology, here’s a good guide from Consumer Reports that explains how for most types of smart TVs.To be honest, after learning about all this in the past week or so, I haven’t done anything revolutionary. I can actually buy into the idea that more relevant ads provide a better experience. I don’t need to see ads for a dozen different eczema treatments while I’m watching YouTube TV, because I don’t have eczema. I’m okay learning about a new toy for young kids, because I have a young kid. (Advertising to kids — or even letting your kids watch YouTube — is an entirely different matter.) So I’ve agreed to all the privacy policies and am enjoying my streaming content as the industry intended.But it does bug me, just on principle, that I have to let a tech company wiretap my TV in order to enjoy all of the device’s features. If you’re set on an ad-free TV experience, your best bet is to buy an old dumb TV off eBay and never connect it to a Roku, Amazon, or Google device. You can buy an antenna for network television, and a DVD player for movies. There are worse Y2K trends to resurrect than being completely offline for a few precious leisure hours.A version of this story was also published in the User Friendly newsletter. Sign up here so you don’t miss the next one!See More: AmazonAmazon Prime VideoBig TechBusiness & FinanceCultureDisneyDisney PlusEven BetterLifeMediaMoneyNetflixStreamingTechnologyTVMost PopularKavanaugh and Barrett appear likely to break with the Supreme Court’s MAGA wingHow worried should legal immigrants be about Trump’s deportations?Your TV is watching youA marine biologist discovered something incredible in a beer bottle on the seafloorHow Trump wants to make one of the most dangerous jobs in America even worseToday, ExplainedUnderstand the world with a daily explainer plus the most compelling stories of the day, compiled by news editor Sean Collins.Email (required)Sign UpBy submitting your email, you agree to our Terms and Privacy Notice. This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply.Advertiser Content FromThis is the title for the native ad",
    "summary": {
      "en": "Streaming companies like Roku and Amazon are increasingly using viewer data to enhance their advertising strategies. While traditional TV has always included ads, streaming services initially offered ad-free experiences. However, many now incorporate ads, and TVs are collecting data on viewing habits to serve personalized advertisements.\n\nRoku, originally known for its hardware, has shifted towards becoming an advertising and data company, generating a significant portion of its revenue from ads. This shift is part of a broader trend in the industry where tech companies aim to control all aspects of the TV experience—from hardware to content to advertising. Many TVs now include technology that monitors what you watch, which raises privacy concerns.\n\nAs a result, ads are becoming more prevalent, with many viewers opting for ad-supported subscriptions. Despite some consumers being uncomfortable with data collection, others appreciate the targeted advertising. Alternatives, like free library streaming services, exist for those wanting to avoid ads altogether. Overall, the TV landscape is evolving, and viewers have to navigate a complex ecosystem where privacy and advertising intersect.",
      "ko": "Roku와 아마존 같은 스트리밍 회사들이 시청자 데이터를 활용해 광고 전략을 강화하고 있습니다. 전통적인 TV는 항상 광고를 포함해왔지만, 스트리밍 서비스는 처음에는 광고 없는 경험을 제공했습니다. 그러나 이제 많은 서비스가 광고를 포함하고 있으며, TV는 시청 습관에 대한 데이터를 수집해 개인 맞춤형 광고를 제공합니다.\n\nRoku는 원래 하드웨어로 알려졌지만, 이제는 광고와 데이터 회사로 전환하고 있으며, 광고에서 상당한 수익을 올리고 있습니다. 이러한 변화는 기술 회사들이 TV 경험의 모든 측면, 즉 하드웨어, 콘텐츠, 광고를 통제하려는 산업의 넓은 흐름의 일환입니다. 현재 많은 TV는 시청하는 내용을 모니터링하는 기술을 포함하고 있어 개인 정보 보호에 대한 우려를 불러일으키고 있습니다.\n\n그 결과, 광고가 점점 더 많이 등장하고 있으며, 많은 시청자들이 광고 지원 구독을 선택하고 있습니다. 일부 소비자들은 데이터 수집에 불편함을 느끼지만, 다른 이들은 타겟 광고를 긍정적으로 평가합니다. 광고를 완전히 피하고 싶은 사람들을 위해 무료 스트리밍 서비스와 같은 대안도 존재합니다. 전반적으로 TV 환경은 변화하고 있으며, 시청자들은 개인 정보 보호와 광고가 얽힌 복잡한 생태계를 헤쳐 나가야 합니다.",
      "ja": null
    }
  },
  {
    "id": "b55976a927b034b2",
    "title": {
      "en": "The Biology of a Large Language Model",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://transformer-circuits.pub/2025/attribution-graphs/biology.html",
    "score": 96,
    "by": "frozenseven",
    "time": 1743171508,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "c8926decaaaf557b",
    "title": {
      "en": "Entropy Attacks",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://blog.cr.yp.to/20140205-entropy.html",
    "score": 79,
    "by": "todsacerdoti",
    "time": 1742905238,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "c21af1bea42bc001",
    "title": {
      "en": "Moiré Than Meets the Eye: Uncovering the Quantum Potential of Phasons",
      "ko": "모이레의 비밀: 파손의 양자 잠재력",
      "ja": null
    },
    "type": "story",
    "url": "https://newscenter.lbl.gov/2025/03/24/moire-than-meets-the-eye/",
    "score": 17,
    "by": "gnabgib",
    "time": 1742834975,
    "content": "Key Takeaways\n\nResearchers have discovered that phasons, low-temperature quasiparticles found in crystal lattices, enable interlayer excitons to move at very low temperatures, when motion should stop.\nIn addition to contributing to foundational materials science knowledge, this discovery could help improve the stability of quantum technologies, such as using excitons as qubits.\nThe Molecular Foundry’s Imaging and Manipulation of Nanostructures facility enabled the research.\n\nA moiré pattern appears when you stack and rotate two copies of an image with regularly repeating shapes, turning simple patterns of squares or triangles into a groovy wave pattern that moves across the combined image in an optical delight.\nSimilarly, stacking single layers of sub-nanometer-thick semiconductor materials known as transition metal dichalcogenides (TMDs) can generate a moiré potential, and novel electronic and optoelectronic properties may emerge between the layers.\nA moiré potential is a “seascape” of potential energy with regularly repeating peaks and valleys. They were previously thought to be stationary. But a team of researchers at the Molecular Foundry at Lawrence Berkeley National Laboratory (Berkeley Lab) has uncovered something unusual about the moiré potentials that emerge when TMDs are stacked: they are constantly moving, even at low temperatures.\nTheir discovery contributes to foundational knowledge in materials science. It also holds promise for advancing the stability of quantum technologies, as controlling moiré potentials could help mitigate decoherence in qubits and sensors. Decoherence occurs when interference causes the quantum state and its information to be lost. The researchers published their findings in ACS Nano. The research is part of broader efforts at Berkeley Lab to advance quantum information systems by working across the quantum research ecosystem, from theory to application, to fabricate and test quantum-based devices and develop software and algorithms.\nResearch was led by Antonio Rossi, a former postdoctoral scholar under Molecular Foundry staff scientist Alex Weber-Bargioni. Rossi came back to Berkeley Lab to collaborate with Molecular Foundry staff scientist Archana Raja and make use of the tools in the Foundry’s Imaging and Manipulation of Nanostructures facility.\nUnexpected Mobility in the Moiré Seascape\nRaja’s lab focuses on characterizing 2D materials using ultrafast lasers and optical spectroscopy at temperatures below -150°C. Exciting the layered TMD samples with a green pulsed laser energizes electrons and causes them to jump from their ground state to an excited one. Excited electrons leave behind a ‘hole’ with a positive charge, resulting in an electron-hole pair or exciton.\nExcitons are known to form within single-layered materials. However, excitons in the stacked two-layer system separate; electrons move into the tungsten disulfide layer, and positively charged holes get left behind in the tungsten diselenide layer. In the materials community, these special layer-jumping excitons are known as “interlayer excitons” or IXs.\n“You would expect the moiré valleys to act as traps,” Rossi said. “So once the exciton is in there, it’s basically trapped. It’s like sitting (in a valley), and all you can see is the mountains around you. You’re not moving.”\nHowever, the team noticed that IXs explored the moiré’s seascape despite being trapped within it. “It takes very little energy to make this moiré potential move, so the moiré is moving around exactly like a stormy sea,” explained Rossi.\n“We showed that even at very cold temperatures, energy, and information are not as localized as you might expect. This happens because of a special ‘mechanical property’ of the Moiré pattern,” said Raja. “There are different ways to transport energy and information at different temperatures. This is a new way to do that.”\nCollaborator Jonas Zipfel, a postdoctoral researcher in Raja’s group, worked with Rossi to automate their measurements to better understand the excitons’ motion. “Jonas’ work made it so we could seamlessly collect luminescence spectra, image, and lifetime (data), all of which enabled us to extract the diffusivity (movement) of the excitons,” said Raja.\nTo enable the observation of excitons in motion, Johannes Lischner and Indrajit Maity from Imperial College London used simulations to obtain snapshots of the moiré potential “seascape.” They wanted to see how it behaved at different times.\nBy working with theorists Lischner and Maity, the research team arrived at the only logical explanation for their observations: the moiré potential itself must be moving.\nCatching a Low-Temperature Quasiparticle in Motion\nThe researchers have proposed that a low-temperature quasiparticle called a phason enables the IX to move even while it’s trapped. A quasiparticle is a quantum of energy within a crystal lattice; it has momentum and position and generally behaves like a particle. Phasons are quasiparticles thought to be naturally present in the moiré potential.\n“You have the (interlayer) exciton surfing the moiré and moving around,” Rossi stated. He believes the phason mediates the movement in the same way a surfboard allows a surfer to catch waves. “It’s kind of carrying the exciton, in a way.”\nRossi and team found the motion of the interlayer excitons within the moiré potential to be angle and temperature-dependent. Their movement is at a maximum when TMD layers are parallel (when the molecules of the stacked layers align in the same direction).\nUnexpectedly, as the system temperature approaches zero, the motion of the interlayer excitons tapers gradually to a number that is slightly higher than zero, rather than coming to a complete halt. And while the number is small, it’s significant.\nRossi explained, “It was a surprise to find that this movement happens even at really low temperatures when everything is supposed to be frozen.”\nHis next steps include investigating the superconductivity in twisted bilayer graphene that may arise from phason quasiparticles. Rossi is currently doing research for the Center of Nanotechnology Innovation at NEST, Institute of Technology Italy.\nRaja is interested in exploring different semiconductor and moiré systems. She’s also intrigued by the possibility of imaging phasons directly. She said, “Our evidence here is through the diffusion of the (interlayer) exciton, but we haven’t necessarily caught the phason red-handed, yet.”\nThe Molecular Foundry is a DOE Office of Science user facility at Berkeley Lab.\nThe work was supported, in part, by the DOE Office of Science.\n###\nLawrence Berkeley National Laboratory (Berkeley Lab) is committed to groundbreaking research focused on discovery science and solutions for abundant and reliable energy supplies. The lab’s expertise spans materials, chemistry, physics, biology, earth and environmental science, mathematics, and computing. Researchers from around the world rely on the lab’s world-class scientific facilities for their own pioneering research. Founded in 1931 on the belief that the biggest problems are best addressed by teams, Berkeley Lab and its scientists have been recognized with 16 Nobel Prizes. Berkeley Lab is a multiprogram national laboratory managed by the University of California for the U.S. Department of Energy’s Office of Science.\nDOE’s Office of Science is the single largest supporter of basic research in the physical sciences in the United States, and is working to address some of the most pressing challenges of our time. For more information, please visit energy.gov/science.\n\n        Tags:\n\n                  Materials\n                  Quantum",
    "summary": {
      "en": "**Summary:**\n\nResearchers have found that phasons, which are quasiparticles in crystal lattices, allow interlayer excitons to move even at very low temperatures, where movement usually stops. This discovery enhances our understanding of materials science and may improve the stability of quantum technologies, such as qubits.\n\nThe study, conducted at the Molecular Foundry, involved stacking thin semiconductor layers called transition metal dichalcogenides (TMDs). When these layers are stacked, they create a moving moiré potential, which was previously thought to be stationary. This potential plays a key role in the movement of excitons, which are pairs of electrons and holes created when light interacts with these materials.\n\nThe researchers observed that excitons could move within the moiré potential, which behaves like a dynamic seascape. They discovered that the excitons’ mobility is influenced by temperature and the alignment of the layers. Surprisingly, even close to absolute zero, the excitons still exhibit some movement instead of completely freezing.\n\nThis research opens new avenues for exploring other semiconductor systems and understanding the role of phasons in materials. The findings contribute to the broader goal of advancing quantum information systems.",
      "ko": "연구자들은 결정 격자에서 발생하는 준입자인 파손(phason)이 매우 낮은 온도에서도 층간 엑시톤이 움직일 수 있도록 한다는 사실을 발견했습니다. 일반적으로 이러한 온도에서는 물질의 움직임이 멈추게 되는데, 이 발견은 재료 과학에 대한 이해를 높이고 큐비트와 같은 양자 기술의 안정성을 향상시킬 수 있는 가능성을 제시합니다.\n\n이 연구는 분자 연구소에서 진행되었으며, 전이 금속 다칼코겐화물(TMD)이라고 불리는 얇은 반도체 층을 쌓는 방식으로 이루어졌습니다. 이러한 층들이 쌓이면 정지해 있다고 여겨졌던 움직이는 모아레 잠재력이 형성됩니다. 이 잠재력은 빛이 이러한 물질과 상호작용할 때 생성되는 전자와 정공의 쌍인 엑시톤의 이동에 중요한 역할을 합니다.\n\n연구자들은 엑시톤이 마치 동적인 바다 풍경처럼 행동하는 모아레 잠재력 내에서 이동할 수 있음을 관찰했습니다. 그들은 엑시톤의 이동성이 온도와 층의 정렬에 의해 영향을 받는다는 것을 발견했습니다. 놀랍게도 절대 영도에 가까운 온도에서도 엑시톤은 완전히 얼어붙지 않고 일부 움직임을 보였습니다.\n\n이 연구는 다른 반도체 시스템을 탐색하고 재료에서 파손의 역할을 이해하는 새로운 길을 열어줍니다. 이러한 발견은 양자 정보 시스템을 발전시키려는 더 넓은 목표에 기여합니다.",
      "ja": null
    }
  },
  {
    "id": "83b73cf6b28b436f",
    "title": {
      "en": "Are Levi's from Amazon different from Levi's from Levi's?",
      "ko": "아마존 리바이스, 진짜인가?",
      "ja": null
    },
    "type": "story",
    "url": "https://nymag.com/strategist/article/levis-amazon-jeans-testing.html",
    "score": 120,
    "by": "randycupertino",
    "time": 1743164306,
    "content": "strategist investigates\n\n                Mar. 26, 2025\n\n          Are Levi’s From Amazon Different From Levi’s From Levi’s?\n\n            By\n        Erin Schwartz,\n          a Strategist writer covering décor, gardening, and garment care.\n          They previously worked as an editor at Garage magazine.\n\n                Photo-Illustration: The Strategist; Photos: Retailers\n\n        Welcome to Jeans Month on the Strategist, where we’re obsessively vetting denim — from trying on every pair at the Gap to asking dozens of stylish people about their favorite fits. For more, head to ourJeans Month hub.\n\n  The Strategist recently received a mysterious tip — the tipster had bought a pair of Levi’s jeans from Amazon that felt notably different from (and worse than) a pair that came directly from Levi’s. I’ve heard versions of the same theory over the years — not just for Levi’s and Amazon but from buyers of brand-name tank tops, mattresses, and T-shirts who were convinced that what they’d gotten from a cheaper retailer was lower quality than the version sold elsewhere under the same name.\n\n  As the Strategist’s resident materials expert, I was on the case. I acquired two pairs each of our most-recommended jeans(women’s Wedgies and Ribcages and the classic men’s 501s) — one pair ordered from Amazon, one pair sent by Levi’s for each style. I was careful to match up the same washes to prevent differences in distressing or fabric treatment from skewing the results. Still, I was skeptical. It didn’t make sense: Why would Levi’s maintain separate supply chains for different retailers? I pulled the jeans out of their plastic packaging expecting them to be indistinguishable. They were not.\n\n    The denim looked slightly different.\n\n        {\n          \"@id\": \"#articleSchema\",\n          \"image\": {\n            \"@context\": \"http://schema.org\",\n            \"@type\": \"ImageObject\",\n            \"creditText\": \"Author\",\n            \"caption\": \"The color difference between non-Amazon Ribcage denim (above) and Amazon Ribcage denim (below).\",\n            \"contentUrl\": \"https://pyxis.nymag.com/v1/imgs/3c8/745/c5dbd00a5d3b4fb447d8e2637774ae245b-IMG-4783.2x.rvertical.w570.jpg\",\n            \"width\": \"1140\",\n            \"height\": \"1426\"\n          }\n        }\n\n      The color difference between non-Amazon Ribcage denim (above) and Amazon Ribcage denim (below).\n      Photo: Author\n\n  The first thing I noticed: The washes looked different. The most pronounced were the Ribcage jeans in a medium blue called “Jazz Pop” — the pair from Amazon had a smoother handfeel and more even color, while the non-Amazon pair felt bumpier with small flecks of higher-contrast fading, an effect I associate with acid washing. The Wedgies from Amazon had a bumpier feel as well, and the 501s from Amazon were a hint darker, closer to indigo than medium blue.\n\n    I brought the jeans to a testing lab.\n\n  To figure out whether these aesthetic variations made for differences in quality, I took all the jeans to the testing lab of F.I.T.’s Textile Development and Marketing Department. The pairs were mixed and relabeled to obscure which one came from which retailer; lab assistant Cesar Saavedra stamped and snipped out samples, which were then weighed, pipetted with bleach, and clamped into a blue metal machine that exerted hundreds of pounds of force to rip the denim against the grain before releasing with a pneumatic hiss.\n\n  Based on the results, “the denim in each of these six pairs is different,” said Margaret Bishop, an adjunct professor in the department. “It’s all strong. You’re not getting a cheap product.” But in a blind test, none of the jeans was identical.\n\n    One pair of non-Amazon Levi’s was lighter weight.\n\n  Back at the office, I reconciled my data with my denim scraps to interpret the results. There were some outliers — most of the denim weighed 13 to 14 ounces per square yard except for a pair of non-Amazon 501s, which was 30 percent lighter than its analogue. A pair of non-Amazon Ribcages was significantly less resistant to tearing. But for the most part, one source didn’t significantly outperform the other. In fact, the jeans from Amazon did slightly better on average, but with a six-jean sample size, take that with a grain of salt. The aesthetic differences also didn’t correlate to the jeans’ strength. Bishop attributed the bumpier handfeel of a couple of the pairs to the level of twist in the yarn, but it didn’t make them weaker. (A colorfastness test confirmed the two Jazz Pop Ribcages were dyed differently.)\n\n    Ultimately, the Amazon Levi’s weren’t worse quality.\n\n  It seemed the tipster was half right. The tests confirmed a lot of variability between two pairs of the same jeans — you could buy the same style from Amazon and Levi’s and feel a difference. But it didn’t add up to gaps in quality; there was no indication that the Levi’s from Amazon were worse.\n\n        {\n          \"@id\": \"#articleSchema\",\n          \"image\": {\n            \"@context\": \"http://schema.org\",\n            \"@type\": \"ImageObject\",\n            \"creditText\": \"Author\",\n            \"caption\": \"A denim sample undergoes a tensile strength test at FIT.\",\n            \"contentUrl\": \"https://pyxis.nymag.com/v1/imgs/6a3/8a6/c5d2ea1b06dc41ddfdb0fd140e4b5a581e-IMG-4762.2x.rvertical.w570.jpg\",\n            \"width\": \"1140\",\n            \"height\": \"1426\"\n          }\n        }\n\n      A denim sample undergoes a tensile strength test at FIT.\n      Photo: Author\n\n  Levi’s sources its fabric from dozens of mills across the world, from luxury supplierCandianiin Italy to sites in India, Bangladesh, Mexico, and Turkey. The six pairs I tested were manufactured in three places: Cambodia, Macau, and Mexico. The company’s supply chain is vast, and to some extent, it makes sense that jeans made to the same specifications from different mills, dye facilities, and factories would result in different products. ﻿\n\n  This is especially true for washes and distressing, a notoriously fiddly process. “Wash houses get a formula — a recipe, if you will — of how the denim is supposed to be washed. You might have different machinery, you might have slightly different chemicals,” says Maxine Bédat, author of the jeans study Unraveled. Some distressing — whiskering, fraying — is done by hand. Just as “everybody making a recipe doesn’t come out with the same pie,” small differences in production will add up to different-looking jeans.\n\n  “The bigger the brand and the more channels in which it’s sold, the more diverse quality and fabric you’re going to see,” says Angela Velasquez, the executive editor of SJ Denim. If you want to minimize variation between pairs of jeans, she says to look for a “smaller, niche brand,” which is more likely to have a tighter supply chain and thus fewer differences in fabric and fit between pairs.\n\n  Do you have to worry about this if you’re just looking for a good pair of jeans? Not necessarily. According to Bédat, the way to get the most out of your denim is to “really spend time thinking about what actually works and fits,” which will probably involve going into a store in person. But I’ve found no evidence of a plot to offload worse denim onto Amazon. “I truthfully think the only consumers that really care about all the nitty-gritty and will know what any of this means are the true denim heads,” says Velasquez. And they’re getting their denim from Kaihara.\n\n      The denim I tested\n\n      (Buy from either retailer.)\n\n                Levi’s Wedgie High Waist Straight Leg Jeans - Christina\n\n    $98\n\n      $98\n\n at Amazon\n\n      $98\n\n at Levi's\n\n    @media screen and (max-width: 1179.9px) {\n      .page-header .disclaimer-text_revenue,\n      .section-header .disclaimer-text_revenue\n       {\n        display: block;\n      }\n    }\n    @media screen and (min-width: 1180px) {\n      .tertiary .disclaimer-text_revenue {\n        display: block;\n      }\n    }\n\n   {\"@context\":\"http://schema.org\",\"@type\":\"Product\",\"name\":\"Levi's Wedgie High Waist Straight Leg Jeans - Christina\",\"image\":\"https://pyxis.nymag.com/v1/imgs/ca8/149/75eaef417998204d44776340cc31f102e8.jpg\",\"gtin12\":\"196524418497\",\"brand\":{\"@type\":\"Brand\",\"name\":\"Levi's\"},\"offers\":[{\"@type\":\"Offer\",\"price\":98,\"priceCurrency\":\"USD\",\"url\":\"https://www.amazon.com/dp/B0B192W56J?tag=thestrategistsite-20&ascsubtag=__st0328aam__cm8opcv6700000igxq9l2pmnp__260260________________\"},{\"@type\":\"Offer\",\"price\":98,\"priceCurrency\":\"USD\",\"url\":\"https://click.linksynergy.com/deeplink?id=OHlcvPYhHQM&mid=53220&murl=https%3A%2F%2Fwww.levi.com%2FUS%2Fen_US%2Fclothing%2Fwomen%2Fjeans%2Fstraight%2Fwedgie-straight-fit-womens-jeans%2Fp%2F349640178&u1=[st0328aam][cm8opcv6700000igxq9l2pmnp][219367][][][][][][][][]\"}],\"offerCount\":2}\n\n        @media screen and (min-width: 768px) {\n          .product-grid .product-grid-item .price-detail-secondary {\n            height: 16px;\n          }\n        }\n\n                Levi’s Ribcage Straight Ankle Jeans - Jazz Pop\n\n    $56\n\n    $98\n\n      now\n      43% off\n\n      $56\n\n at Amazon\n\n      $98\n\n at Levi's\n\n    @media screen and (max-width: 1179.9px) {\n      .page-header .disclaimer-text_revenue,\n      .section-header .disclaimer-text_revenue\n       {\n        display: block;\n      }\n    }\n    @media screen and (min-width: 1180px) {\n      .tertiary .disclaimer-text_revenue {\n        display: block;\n      }\n    }\n\n   {\"@context\":\"http://schema.org\",\"@type\":\"Product\",\"name\":\"Levi's Ribcage Straight Ankle Jeans - Jazz Pop\",\"image\":\"https://pyxis.nymag.com/v1/imgs/c5d/565/990fa24584cddb5d26b353b56cb35958d8-jazz-pop.jpg\",\"gtin12\":\"195901201240\",\"brand\":{\"@type\":\"Brand\",\"name\":\"Levi's\"},\"offers\":[{\"@type\":\"Offer\",\"price\":56,\"priceCurrency\":\"USD\",\"url\":\"https://www.amazon.com/dp/B096T4BCRQ?tag=thestrategistsite-20&ascsubtag=__st0328aam__cm8opcv6700000igxq9l2pmnp__260264________________\"},{\"@type\":\"Offer\",\"price\":98,\"priceCurrency\":\"USD\",\"url\":\"https://click.linksynergy.com/deeplink?id=OHlcvPYhHQM&mid=53220&murl=https%3A%2F%2Fwww.levi.com%2FUS%2Fen_US%2Fclothing%2Fwomen%2Fjeans%2Fstraight%2Fribcage-straight-ankle-womens-jeans%2Fp%2F726930117&u1=[st0328aam][cm8opcv6700000igxq9l2pmnp][259945][][][][][][][][]\"}],\"offerCount\":2}\n\n        @media screen and (min-width: 768px) {\n          .product-grid .product-grid-item .price-detail-secondary {\n            height: 16px;\n          }\n        }\n\n                Levi’s 501 Original Fit Men’s Jeans - Medium Stonewash\n\n    $49\n\n    $80\n\n      now\n      38% off\n\n      $49\n\n at Amazon\n\n      $56\n\n at Levi's\n\n    @media screen and (max-width: 1179.9px) {\n      .page-header .disclaimer-text_revenue,\n      .section-header .disclaimer-text_revenue\n       {\n        display: block;\n      }\n    }\n    @media screen and (min-width: 1180px) {\n      .tertiary .disclaimer-text_revenue {\n        display: block;\n      }\n    }\n\n   {\"@context\":\"http://schema.org\",\"@type\":\"Product\",\"name\":\"Levi’s 501 Original Fit Men’s Jeans - Medium Stonewash\",\"image\":\"https://pyxis.nymag.com/v1/imgs/798/d78/a1800f62761f0df777b47ba75ae582e349-stonewash.png\",\"brand\":{\"@type\":\"Brand\",\"name\":\"Levi’s\"},\"offers\":[{\"@type\":\"Offer\",\"price\":49.29,\"priceCurrency\":\"USD\",\"url\":\"https://www.amazon.com/dp/B0018OOWRE?tag=thestrategistsite-20&ascsubtag=__st0328aam__cm8opcv6700000igxq9l2pmnp__260269________________\"},{\"@type\":\"Offer\",\"price\":55.65,\"priceCurrency\":\"USD\",\"url\":\"https://click.linksynergy.com/deeplink?id=OHlcvPYhHQM&mid=53220&murl=https%3A%2F%2Fwww.levi.com%2FUS%2Fen_US%2Fclothing%2Fmen%2Fjeans%2Fstraight%2F501-original-fit-mens-jeans%2Fp%2F005010193&u1=[st0328aam][cm8opcv6700000igxq9l2pmnp][260268][][][][][][][][]\"}],\"offerCount\":2}\n\n          get the strategist newsletter\n          Actually good deals, smart shopping advice, and exclusive discounts.\n\n            Email\n\n          This site is protected by reCAPTCHA and the Google\n          Privacy Policy and\n          Terms of Service apply.\n\n        Vox Media, LLC Terms and Privacy Notice\n        By submitting your email, you agree to our Terms and Privacy Notice and to receive email correspondence from us.\n\n    More From The Strategist\n\n              What’s the Difference Between a Pair of $30 Jeans and $300 Jeans?\n\n              I Spent 3 Hours at the Jellycat Diner\n\n              What Jennifer Aniston Can’t Live Without\n\n              The State of the Status Hand Soap\n\n  The Strategistis designed to surfaceuseful, expert recommendations for things to buy across the vast e-commerce landscape. Every product is independently selected by our team of editors, whomyou can read abouthere. We update links when possible, but note that deals can expire and all prices are subject to change.\n\n    Tags:\n\n            the strategist\n\n            jeans month\n\n            fashion\n\n            unisex apparel\n\n            strategist investigates\n\n            strategist most popular\n\n          More\n\n        Show\n\n    Leave a Comment\n\n      Are Levi’s From Amazon Different From Levi’s From Levi’s?\n\n    const freeLayoutsInstances = [\n      'ecom-article',\n      'ecom-products',\n      'non-monetizable'\n    ];\n    const paywalledLayoutsInstances = ['paywalled-article'];\n    const layoutInstance = document.querySelector('html').getAttribute('data-layout-uri').split('/instances/')[1].replace('@published', '');\n    const siteSlug = 'strategist';\n    const keywords = [\"the strategist\",\"jeans month\",\"fashion\",\"unisex apparel\",\"strategist investigates\",\"strategist most popular\"]; // This is set by handlebars in the server.\n    const featureTypes = window._nymPermutive?.article?.featureTypes;\n    const freeConditions = {\n      isStrategist: !paywalledLayoutsInstances.includes(layoutInstance) && siteSlug === 'strategist',\n      isFreeLayout: freeLayoutsInstances.includes(layoutInstance),\n      hasExcludePaywallTags: /paywall exclude/i.test(keywords.join(',')),\n      isEcomm: featureTypes && featureTypes.includes('ecomm')\n    };\n    const structuredData = {\n      '@context': 'http://schema.org',\n      '@id': '#articleSchema',\n      hasPart: {\n        '@type': 'WebPageElement',\n        cssSelector: '.article-content',\n        isAccessibleForFree: false\n      },\n      isAccessibleForFree: false\n    };\n    for (const condition of Object.keys(freeConditions)) {\n      if (!freeConditions[condition]) continue;\n      structuredData.isAccessibleForFree = true;\n      structuredData.hasPart.isAccessibleForFree = true\n      break;\n    }\n    const ldJsonScript = document.createElement('script');\n    ldJsonScript.type = \"application/ld+json\";\n    ldJsonScript.innerHTML = JSON.stringify(structuredData);\n    document.head.appendChild(ldJsonScript);",
    "summary": {
      "en": "**Summary: Are Levi’s From Amazon Different From Levi’s From Levi’s?**\n\nIn a recent exploration during \"Jeans Month,\" the Strategist investigated whether Levi's jeans purchased from Amazon are of lower quality than those bought directly from Levi's. After receiving a tip about quality differences, the author tested pairs of popular Levi's styles from both retailers. \n\nKey findings included:\n- The jeans from Amazon and Levi's had noticeable aesthetic differences, such as variations in color and texture, but these did not indicate a significant difference in quality.\n- Testing at a lab showed that while the denim from both sources varied, most pairs performed similarly in terms of strength and durability.\n- Levi's sources fabric from various global mills, which can lead to differences in production quality and appearance.\n\nUltimately, the assessment indicated that while there might be some differences between Levi's jeans from different retailers, the quality of jeans from Amazon is not necessarily worse than those from Levi's own store. \n\nFor those looking for jeans, the focus should be on fit and style rather than solely on the retailer.",
      "ko": "최근 \"청바지의 달\"을 맞아, 전략가들은 아마존에서 구매한 리바이스 청바지가 리바이스 매장에서 직접 구매한 것보다 품질이 낮은지 조사했습니다. 품질 차이에 대한 제보를 받은 저자는 두 판매처에서 인기 있는 리바이스 스타일의 청바지를 테스트했습니다.\n\n주요 발견 사항은 다음과 같습니다. 아마존과 리바이스에서 판매하는 청바지는 색상과 질감에서 눈에 띄는 차이가 있었지만, 이는 품질의 큰 차이를 나타내지는 않았습니다. 실험실 테스트 결과, 두 출처의 데님은 다소 차이가 있었지만, 대부분의 청바지는 강도와 내구성 면에서 비슷한 성능을 보였습니다. 리바이스는 다양한 글로벌 제직소에서 원단을 조달하기 때문에 생산 품질과 외관에 차이가 발생할 수 있습니다.\n\n결국, 평가 결과에 따르면 서로 다른 판매처에서 구매한 리바이스 청바지 사이에 약간의 차이가 있을 수 있지만, 아마존에서 구매한 청바지가 리바이스 매장에서 구매한 것보다 반드시 품질이 떨어지는 것은 아닙니다. 청바지를 찾는 소비자들은 판매처보다는 핏과 스타일에 더 집중해야 합니다.",
      "ja": null
    }
  },
  {
    "id": "d91c6896e5ca9717",
    "title": {
      "en": "FDIC says banks can engage in crypto activities without prior approval",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.reuters.com/business/finance/fdic-says-banks-can-engage-crypto-activities-without-prior-approval-2025-03-28/",
    "score": 6,
    "by": "speckx",
    "time": 1743194880,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "a243f5c5e6b75df5",
    "title": {
      "en": "Europe's Battle for Tech Sovereignty: Why OpenStack Matters [video]",
      "ko": "유럽의 기술 주권 전쟁: 오픈스택의 중요성",
      "ja": null
    },
    "type": "story",
    "url": "https://vpetersson.com/podcast/S02E06.html",
    "score": 7,
    "by": "mvip",
    "time": 1743191518,
    "content": "{\n  \"@context\": \"https://schema.org\",\n  \"@type\": [\"VideoObject\", \"PodcastEpisode\"],\n  \"name\": \"Europe's Battle for Tech Sovereignty: Why OpenStack Matters\",\n  \"description\": \"In this timely conversation with Johan Christenson, founder of Cleura and board member of OpenInfra (overseeing Kata Containers and OpenStack), we dive into Europe's growing urgency around digital sovereignty and what it will actually take to build a competitive, homegrown cloud ecosystem. Johan, a long-time advocate for open source infrastructure, pulls back the curtain on why progress has been so slow and where things can shift.\\n\",\n  \"uploadDate\": \"2025-03-28T00:00:00+00:00\",\n  \"duration\": \"PT72M\",\n  \"thumbnailUrl\": \"https://i.ytimg.com/vi/yq_AWq24-IM/default.jpg\",\n  \"embedUrl\": \"https://www.youtube.com/embed/yq_AWq24-IM\",\n  \"contentUrl\": \"https://www.youtube.com/watch?v=yq_AWq24-IM\",\n  \"url\": \"https://vpetersson.com/podcast/S02E06.html\",\n  \"mainEntityOfPage\": \"https://vpetersson.com/podcast/S02E06.html\",\n  \"inLanguage\": \"en\",\n  \"isFamilyFriendly\": true,\n\n  \"actors\": [\n\n    {\n      \"@type\": \"Person\",\n      \"name\": \"Johan Christenson\"\n    }\n\n  ],\n\n  \"author\": {\n    \"@type\": \"Person\",\n    \"name\": \"Viktor Petersson\",\n    \"url\": \"https://vpetersson.com\"\n  },\n  \"publisher\": {\n    \"@type\": \"Organization\",\n    \"name\": \"Nerding Out with Viktor\",\n    \"url\": \"https://vpetersson.com/podcast/\"\n  },\n  \"partOfSeries\": {\n    \"@type\": \"PodcastSeries\",\n    \"name\": \"Nerding Out with Viktor\",\n    \"url\": \"https://vpetersson.com/podcast/\",\n    \"inLanguage\": \"en\",\n    \"isAccessibleForFree\": true,\n    \"seasonNumber\": 2\n  },\n  \"episodeNumber\": 6,\n  \"seasonNumber\": 2,\n  \"sameAs\": [\n    \"https://open.spotify.com/episode/39sf97LkWGRZthwNNRuLup?si=1xtPNO_DRTixQ9_YmzAtUA\"\n    ,\"https://podcasts.apple.com/gb/podcast/europes-battle-for-tech-sovereignty-why-openstack-matters/id1722663295?i=1000701181516\"\n    ,\"https://music.amazon.co.uk/podcasts/c8e79c21-2dde-4597-b9fb-257ecbc2bf29/episodes/e0a97010-b929-4484-bd64-2e72cb91f6ab/nerding-out-with-viktor-europe's-battle-for-tech-sovereignty-why-openstack-matters\"\n  ]\n}\n\n{\n  \"@context\": \"https://schema.org\",\n  \"@type\": \"BreadcrumbList\",\n  \"itemListElement\": [\n    {\n      \"@type\": \"ListItem\",\n      \"position\": 1,\n      \"name\": \"Home\",\n      \"item\": \"https://vpetersson.com\"\n    },\n    {\n      \"@type\": \"ListItem\",\n      \"position\": 2,\n      \"name\": \"Podcast\",\n      \"item\": \"https://vpetersson.com/podcast/\"\n    },\n    {\n      \"@type\": \"ListItem\",\n      \"position\": 3,\n      \"name\": \"Europe's Battle for Tech Sovereignty: Why OpenStack Matters\",\n      \"item\": \"https://vpetersson.com/podcast/S02E06.html\"\n    }\n  ]\n}\n\n  Podcast\n\n        Join Viktor, a proud nerd and seasoned entrepreneur, whose academic journey at Santa Clara University in Silicon Valley sparked a career marked by innovation and foresight. From his college days, Viktor embarked on an entrepreneurial path, beginning with YippieMove, a groundbreaking email migration service, and continuing with a series of bootstrapped ventures.\n\n        Links\n\n        Follow Me\n\n            Follow Me\n\n            Join Viktor, a proud nerd and seasoned entrepreneur, whose academic journey at Santa Clara University in Silicon Valley sparked a career marked by innovation and foresight. From his college days, Viktor embarked on an entrepreneurial path, beginning with YippieMove, a groundbreaking email migration service, and continuing with a series of bootstrapped ventures.\n\n      Europe's Battle for Tech Sovereignty: Why OpenStack Matters\n\n        Play On\n\n        28 MAR • 2025\n\n          1 hour 12 mins\n\n  Share:\n\n        In this timely conversation with Johan Christenson, founder of Cleura and board member of OpenInfra (overseeing Kata Containers and OpenStack), we dive into Europe’s growing urgency around digital sovereignty and what it will actually take to build a competitive, homegrown cloud ecosystem. Johan, a long-time advocate for open source infrastructure, pulls back the curtain on why progress has been so slow and where things can shift.\n\nWe kick off by looking at Europe’s deep dependence on U.S. cloud providers, and the real-world risks that come with it. From pricing power and data availability to political influence and tech autonomy, Johan explains how this imbalance affects everything from government policy to startup growth.\n\nA major part of our conversation breaks down the key blockers that have kept Europe stuck:\n\n  Procurement systems that favor incumbents over local innovation\n  Long-standing vendor lock-in across schools, agencies, and enterprises\n  A steady brain drain of top talent to non-European tech companies\n  Innovation programs that reward optics over sustainable outcomes\n  The sheer complexity of building a scalable, fully independent cloud stack\n\nJohan explains the uphill battle smaller providers face when going up against hyperscalers — not just on price, but on the range and depth of services they offer. He draws a sharp contrast between basic infrastructure and the fully integrated ecosystems that dominate the market.\n\nWe explore his experience building on OpenStack, including:\n\n  Why his company committed early to open infrastructure\n  Lessons from over a decade of contributing to and operating at scale\n  How complexity, resilience, and governance affect platform choices\n  Why internal capability-building matters more than plug-and-play simplicity\n\nFrom there, we expand into bigger-picture ideas: how European vendors could collaborate more effectively, the role of technical standards in shaping markets, and how cloud-native tools like Kubernetes, if used thoughtfully, might help smaller players compete.\n\nThe episode wraps with a sharp take on Europe’s regulatory landscape. Johan argues that the problem isn’t a lack of rules, but the lack of enforcement. He walks through how the disconnect between policy ambitions and operational reality slows down even the most well-intentioned builders.\n\nIf you’re thinking seriously about digital independence, infrastructure sovereignty, or Europe’s place in the global tech stack, this conversation is both a clear-eyed reality check and a grounded look at where change could come from.\n\nFor more on this topic, check out these related articles (in Swedish):\n\n  Ge Sverige ett digitalt beredskapslyft\n  Europa behöver ett digitalt airbusprojekt\n\n            Transcript\n\n              Show/Hide Transcript\n\n                      [00:00]\n                      Viktor Petersson\n\n                    Welcome back to another episode of Nerding out with Viktor.\n\n                      [00:03]\n                      Viktor Petersson\n\n                    Today I'm joined by Johan Christenson.\n\n                      [00:05]\n                      Viktor Petersson\n\n                    Welcome, Johan.\n\n                      [00:06]\n                      Johan Christenson\n\n                    Thank you very much.\n\n                      [00:08]\n                      Viktor Petersson\n\n                    Johan.\n\n                      [00:09]\n                      Viktor Petersson\n\n                    You are in the epicenter of this whole new push that we see as of lately with Europe standing up against America when it comes to tech.\n\n                      [00:18]\n                      Viktor Petersson\n\n                    And we've known each other for quite a while.\n\n                      [00:21]\n                      Viktor Petersson\n\n                    And you've been doing a company called.\n\n                      [00:23]\n                      Viktor Petersson\n\n                    Well, used to be called City Networks and now called Cloud Plura.\n\n                      [00:26]\n                      Viktor Petersson\n\n                    Is that correct?\n\n                      [00:27]\n                      Viktor Petersson\n\n                    That's a correct pronunciation of your.\n\n                      [00:29]\n                      Viktor Petersson\n\n                    Yeah, that's right.\n\n                      [00:30]\n                      Viktor Petersson\n\n                    Amazing.\n\n                      [00:31]\n                      Viktor Petersson\n\n                    So maybe recap for the audience.\n\n                      [00:34]\n                      Viktor Petersson\n\n                    What is this whole push that we're seeing from the European cloud space in general?\n\n                      [00:40]\n                      Johan Christenson\n\n                    Well, I think that the push has actually been ongoing for quite some time, but we have not necessarily succeeded quite well with that.\n\n                      [00:47]\n                      Viktor Petersson\n\n                    Fair enough.\n\n                      [00:48]\n                      Johan Christenson\n\n                    But of course, with all the things that are going on in the world today, I think that people are waking up to a different world that are actually.\n\n                      [00:55]\n                      Johan Christenson\n\n                    It's a little bit scary sometimes, right?\n\n                      [00:57]\n                      Johan Christenson\n\n                    There's a lot of vulnerability in society.\n\n                      [01:00]\n                      Johan Christenson\n\n                    Our competitiveness is going down and so forth.\n\n                      [01:02]\n                      Johan Christenson\n\n                    So there is a push, I think, for a lot of different things in that sort of call for independence or not necessarily push against anybody, but more to make sure that there's balance in society that, you know, if things really go bad, we actually can do certain things that maybe we're too dependent on a few players to actually do today.\n\n                      [01:24]\n                      Viktor Petersson\n\n                    Absolutely.\n\n                      [01:25]\n                      Viktor Petersson\n\n                    I mean, yeah, this comes down to like European 70 essentially when it comes to tech.\n\n                      [01:30]\n                      Viktor Petersson\n\n                    And if we do look at almost any company in European ecosystem, they're largely built to your point.\n\n                      [01:37]\n                      Viktor Petersson\n\n                    You had an article in one of the big Swedish newspapers lately about this dependency, right.\n\n                      [01:41]\n                      Viktor Petersson\n\n                    About if these big clouds were to whatever block the European tech sector or economic sector in general, we would be completely doomed.\n\n                      [01:52]\n                      Viktor Petersson\n\n                    Right.\n\n                      [01:52]\n                      Viktor Petersson\n\n                    So maybe speak a bit more like how you see that and how that's what we do about this really.\n\n                      [02:00]\n                      Johan Christenson\n\n                    Well, I mean, you know, there is.\n\n                      [02:02]\n                      Johan Christenson\n\n                    If you.\n\n                      [02:02]\n                      Johan Christenson\n\n                    If you take that first bullet, right.\n\n                      [02:04]\n                      Johan Christenson\n\n                    There's a vulnerability that we built in over many years.\n\n                      [02:08]\n                      Johan Christenson\n\n                    You know, it's been a culture here in Europe that we buy American, you know, used to be you never been fired for buying IBM, you know, many years ago.\n\n                      [02:17]\n                      Johan Christenson\n\n                    And that's kind of changed over to different companies today.\n\n                      [02:20]\n                      Viktor Petersson\n\n                    Yeah.\n\n                      [02:21]\n                      Johan Christenson\n\n                    And that has gone so far today that there are true vulnerabilities.\n\n                      [02:26]\n                      Johan Christenson\n\n                    Some pieces we're building in ourselves.\n\n                      [02:28]\n                      Johan Christenson\n\n                    Like we have apps in Scandinavia that if an app doesn't work, you can't access half of your things and it almost takes down a society.\n\n                      [02:37]\n                      Johan Christenson\n\n                    But then of course, what's maybe more concerning is the underlying infrastructure of those things.\n\n                      [02:42]\n                      Johan Christenson\n\n                    It's kind of like easily put.\n\n                      [02:43]\n                      Johan Christenson\n\n                    You can say Europe needs to be able to build its own railways or its own roads.\n\n                      [02:48]\n                      Johan Christenson\n\n                    We take that for granted.\n\n                      [02:50]\n                      Johan Christenson\n\n                    But for some reason, you know, we're so comfortable, you know, just outsourcing everything else when it comes to it and as we know it today is of course, the foundation for pretty much everything we do.\n\n                      [03:01]\n                      Johan Christenson\n\n                    Whether it's, you know, us doing this thing now or if you build furniture or stuff that you don't associate with it's still a super foundational part.\n\n                      [03:13]\n                      Johan Christenson\n\n                    And then if you don't have that, I think over time we've started to realize that it's not just the vulnerability aspect in the sense that if somebody hits the off switch or something just happens to it, you know, we're super vulnerable as a society.\n\n                      [03:29]\n                      Johan Christenson\n\n                    Many times we talk about, you know, food, for example.\n\n                      [03:33]\n                      Johan Christenson\n\n                    We need to make sure that we can, you know, produce all the food.\n\n                      [03:35]\n                      Johan Christenson\n\n                    But very few people think about the carrots for the carrots actually get to the table.\n\n                      [03:40]\n                      Johan Christenson\n\n                    There's so many processes, supply chains, and it is everywhere in that.\n\n                      [03:44]\n                      Johan Christenson\n\n                    Right.\n\n                      [03:44]\n                      Johan Christenson\n\n                    So you have that aspect of the vulnerability, and that is just immense, I would say.\n\n                      [03:50]\n                      Johan Christenson\n\n                    And today, of course, in our society, you know, a car, you know, can you stop the car?\n\n                      [03:56]\n                      Johan Christenson\n\n                    Can you make the car go fast?\n\n                      [03:58]\n                      Johan Christenson\n\n                    You know, there's so many of these things that come up once societies become more like, or I should say rather that we've had war next to us and we feel more insecure with allies that used to be very reliable, so to speak, and we might have the feeling that they're not so reliable and there's so much of that vulnerability that goes around.\n\n                      [04:16]\n                      Johan Christenson\n\n                    And you can go on and on about that.\n\n                      [04:18]\n                      Johan Christenson\n\n                    But then longer term for us, of course, is the competitiveness, right?\n\n                      [04:21]\n                      Johan Christenson\n\n                    I mean, if you don't.\n\n                      [04:24]\n                      Johan Christenson\n\n                    If you don't have people that actually know the underlying infrastructure and how you build that, but rather just innovate on top of somebody else's innovation, you become completely dependent on that as well.\n\n                      [04:37]\n                      Johan Christenson\n\n                    And of course, you also lose.\n\n                      [04:39]\n                      Johan Christenson\n\n                    You lose, know, that talent that could actually create that underlying, you know, infrastructure, whether it's a road, a railway, or in this case, it, infrastructure, you need those kind of talents for you to be, you know, competitive in the higher layer, so to speak, of it.\n\n                      [04:56]\n                      Johan Christenson\n\n                    So there's various aspects of this that I think we're waking up today",
    "summary": {
      "en": "The podcast episode titled \"Europe's Battle for Tech Sovereignty: Why OpenStack Matters\" features Johan Christenson, founder of Cleura and board member of OpenInfra. In this discussion, they explore the urgent need for Europe to achieve digital sovereignty and establish a competitive local cloud ecosystem.\n\nKey points include:\n\n- **Dependence on U.S. Providers**: Europe relies heavily on American cloud services, posing risks to data availability, pricing, and political influence.\n- **Barriers to Progress**: Several obstacles hinder Europe's tech growth, such as procurement systems favoring established companies, vendor lock-in, talent migration to non-European firms, and complex infrastructure challenges.\n- **Building on OpenStack**: Johan shares insights from his experience with OpenStack, emphasizing the importance of open-source infrastructure and internal capability development.\n- **Collaboration and Standards**: The conversation highlights the need for European vendors to work together and the role of technical standards in market development.\n- **Regulatory Landscape**: Johan points out that while Europe has regulations, enforcement is often lacking, slowing progress.\n\nThe episode serves as a reality check on Europe’s tech independence and discusses potential pathways for improvement. It emphasizes the importance of building local capabilities to avoid over-reliance on external providers.",
      "ko": "\"유럽의 기술 주권을 위한 전투: 오픈스택의 중요성\"이라는 제목의 팟캐스트 에피소드에서는 클레우라의 창립자이자 오픈인프라 이사인 요한 크리스텐슨이 출연합니다. 이 대화에서는 유럽이 디지털 주권을 달성하고 경쟁력 있는 지역 클라우드 생태계를 구축해야 하는 긴급한 필요성에 대해 논의합니다.\n\n주요 내용으로는 유럽이 미국 클라우드 서비스에 크게 의존하고 있다는 점이 있습니다. 이는 데이터 가용성, 가격, 정치적 영향력에 위험을 초래합니다. 또한, 유럽의 기술 성장을 저해하는 여러 장애물도 언급됩니다. 예를 들어, 기존 기업에 유리한 조달 시스템, 공급업체 종속, 유럽 외 기업으로의 인재 유출, 복잡한 인프라 문제 등이 있습니다.\n\n요한은 오픈스택에 대한 자신의 경험을 바탕으로 오픈 소스 인프라의 중요성과 내부 역량 개발의 필요성을 강조합니다. 대화에서는 유럽의 공급업체들이 협력해야 하며, 시장 발전을 위한 기술 표준의 역할도 중요하다는 점이 강조됩니다. 요한은 유럽에 규제가 있지만, 실제 집행이 부족해 진행이 더디다는 점도 지적합니다.\n\n이 에피소드는 유럽의 기술 독립성에 대한 현실적인 점검을 제공하며, 개선을 위한 잠재적인 경로에 대해 논의합니다. 외부 공급업체에 대한 과도한 의존을 피하기 위해 지역 역량을 구축하는 것이 중요하다는 점을 강조합니다.",
      "ja": null
    }
  },
  {
    "id": "e1174069efe65757",
    "title": {
      "en": "Giant, fungus-like organism may be a completely unknown branch of life",
      "ko": "거대 균류, 미지의 생명체?",
      "ja": null
    },
    "type": "story",
    "url": "https://www.livescience.com/animals/giant-fungus-like-organism-may-be-a-completely-unknown-branch-of-life",
    "score": 274,
    "by": "wglb",
    "time": 1743117328,
    "content": "Animals\n\nGiant, fungus-like organism may be a completely unknown branch of life\n\nNews\n\nBy\nJess Thomson\n\npublished\nyesterday\n\nAn ancient and enormous organism called Prototaxites, initially found to be a type of fungus, may actually be an unknown branch of life, researchers say.\n\nComments\n( 0 )\n()\n\nWhen you purchase through links on our site, we may earn an affiliate commission. Here’s how it works.\n\nwindow.vanilla.infiniteArticlesData = [];\n\nA painting of what Prototaxites may have looked like, 400 million years ago.\n(Image credit: Painting by Mary Parrish, National Museum of Natural History.)\n\nA bizarre ancient life-form, considered to be the first giant organism to live on land, may belong to a totally unknown branch of the tree of life, scientists say.These organisms, named Prototaxites, lived around 420 million to 375 million years ago during the Devonian period and resembled branchless, cylindrical tree trunks. These organisms would have been massive, with some species growing up to 26 feet (8 meters) tall and 3 feet (1 meter) wide.Since the first Prototaxites fossil was discovered in 1843, scientists haven't been sure whether they were a plant, fungus or even a type of algae. However, chemical analyses of Prototaxites fossils in 2007 suggested they were likely a giant ancient fungus.Now, according to a paper published March 17 on the preprint server bioRxiv, Prototaxites might not have been a humongous fungus after all — rather, it may have been an entirely different and previously unknown life-form. The study has not yet been peer-reviewed.All life on Earth is classified within three domains — bacteria, archaea and eukarya — with eukarya containing all multicellular organisms within the four kingdoms of fungi, animals, plants and protists. Bacteria and archaea contain only single-celled organisms.Previous chemical analysis of Prototaxites fossils indicated that they likely fed off decaying organisms, just like many fungi do today, rather than making their food from carbon dioxide in the air like plants.However, according to this new research, Prototaxites may actually have been part of a totally different kingdom of life, separate from fungi, plants, animals and protists.\n    window.sliceComponents = window.sliceComponents || {};\n\n    externalsScriptLoaded.then(() => {\n        window.reliablePageLoad.then(() => {\n            var componentContainer = document.querySelector(\"#slice-container-newsletterForm-articleInbodyContent-TT23hGQ7XJtsjBD4MeoeD7\");\n\n            if (componentContainer) {\n                var data = {\"layout\":\"inbodyContent\",\"header\":\"Sign up for the Live Science daily newsletter now\",\"tagline\":\"Get the world\\u2019s most fascinating discoveries delivered straight to your inbox.\",\"formFooterText\":\"By submitting your information you agree to the <a href=\\\"https:\\/\\/futureplc.com\\/terms-conditions\\/\\\" target=\\\"_blank\\\">Terms & Conditions<\\/a> and <a href=\\\"https:\\/\\/futureplc.com\\/privacy-policy\\/\\\" target=\\\"_blank\\\">Privacy Policy<\\/a> and are aged 16 or over.\",\"successMessage\":{\"body\":\"Thank you for signing up. You will receive a confirmation email shortly.\"},\"failureMessage\":\"There was a problem. Please refresh the page and try again.\",\"method\":\"POST\",\"inputs\":[{\"type\":\"hidden\",\"name\":\"NAME\"},{\"type\":\"email\",\"name\":\"MAIL\",\"placeholder\":\"Your Email Address\",\"required\":true},{\"type\":\"hidden\",\"name\":\"NEWSLETTER_CODE\",\"value\":\"XLS-D\"},{\"type\":\"hidden\",\"name\":\"LANG\",\"value\":\"EN\"},{\"type\":\"hidden\",\"name\":\"SOURCE\",\"value\":\"60\"},{\"type\":\"hidden\",\"name\":\"COUNTRY\"},{\"type\":\"checkbox\",\"name\":\"CONTACT_OTHER_BRANDS\",\"label\":{\"text\":\"Contact me with news and offers from other Future brands\"}},{\"type\":\"checkbox\",\"name\":\"CONTACT_PARTNERS\",\"label\":{\"text\":\"Receive email from us on behalf of our trusted partners or sponsors\"}},{\"type\":\"submit\",\"value\":\"Sign me up\",\"required\":true}],\"endpoint\":\"https:\\/\\/newsletter-subscribe.futureplc.com\\/v2\\/submission\\/submit\",\"analytics\":[{\"analyticsType\":\"widgetViewed\"}],\"ariaLabels\":{}};\n\n                var triggerHydrate = function() {\n                    window.sliceComponents.newsletterForm.hydrate(data, componentContainer);\n                }\n\n                if (window.lazyObserveElement) {\n                    window.lazyObserveElement(componentContainer, triggerHydrate);\n                } else {\n                    triggerHydrate();\n                }\n            }\n        }).catch(err => console.error('%c FTE ','background: #9306F9; color: #ffffff','Hydration Script has failed for newsletterForm-articleInbodyContent-TT23hGQ7XJtsjBD4MeoeD7 Slice', err));\n    }).catch(err => console.error('%c FTE ','background: #9306F9; color: #ffffff','Externals script failed to load', err));\nSign up for the Live Science daily newsletter nowGet the world’s most fascinating discoveries delivered straight to your inbox.Contact me with news and offers from other Future brandsReceive email from us on behalf of our trusted partners or sponsorsBy submitting your information you agree to the Terms & Conditions and Privacy Policy and are aged 16 or over.The researchers studied the fossilized remains of one Prototaxites species named Prototaxites taiti, found preserved in the Rhynie chert, a sedimentary deposit of exceptionally well-preserved fossils of early land plants and animals in Scotland. This species was much smaller than many other species of Prototaxites, only growing up to a few inches tall, but it is still the largest Prototaxites specimen found in this region.Upon examining the internal structure of the fossilized Prototaxites, the researchers found that its interior was made up of a series of tubes, similar to those within a fungus. But these tubes branched off and reconnected in ways very unlike those seen in modern fungi.\"We report that Prototaxites taiti was the largest organism in the Rhynie ecosystem and its anatomy was fundamentally distinct from all known extant or extinct fungi,\" the researchers wrote in the paper. \"We therefore conclude that Prototaxites was not a fungus, and instead propose it is best assigned to a now entirely extinct terrestrial lineage.\"True fungi from the same period have also been preserved in the Rhynie chert, enabling the researchers to chemically compare them to Prototaxites. In addition to their unique structural characteristics, the team found that the Prototaxites fossils left completely different chemical signatures to the fungi fossils, indicating that the Prototaxites did not contain chitin, a major building block of fungal cell walls and a hallmark of the fungal kingdom. The Prototaxites fossils instead appeared to contain chemicals similar to lignin, which is found in the wood and bark of plants.\"We conclude that the morphology and molecular fingerprint of P. taiti is clearly distinct from that of the fungi and other organism preserved alongside it in the Rhynie chert, and we suggest that it is best considered a member of a previously undescribed, entirely extinct group of eukaryotes,\" the researchers wrote.Kevin Boyce, a professor at Stanford University, led the 2007 study that posited Prototaxites is a giant fungus and was not involved in this new research. However, he told the New Scientist that he agreed with the study's findings.RELATED STORIES—Scientists discover new 15 million-year old fish with last meal fossilized inside its stomach—30,000-year-old fossilized vulture feathers 'nothing like what we usually see' preserved in volcanic ash—Iguanas sailed one-fifth of the way around the world on rafts 34 million years ago\"Given the phylogenetic information we have now, there is no good place to put Prototaxites in the fungal phylogeny,\" Boyce said. \"So maybe it is a fungus, but whether a fungus or something else entirely, it represents a novel experiment with complex multicellularity that is now extinct and does not share a multicellular common ancestor with anything alive today.\"More research into Prototaxites fossils needs to be done to determine if they were fungi or a completely different type of life, and what caused them to go extinct millions of years ago.\"The conclusion that it is a completely unknown eukaryote certainly creates an air of mystery and intrigue around it — probably not likely to be solved until more fossils are discovered or new analytical techniques developed,\" Brett Summerell, a plant pathologist and fungi expert at the Botanic Gardens of Sydney, Australia, who not involved in this new study, told the New Scientist.\n\nJess ThomsonSocial Links NavigationLive Science ContributorJess Thomson is a freelance journalist. She previously worked as a science reporter for Newsweek, and has also written for publications including VICE, The Guardian, The Cut, and Inverse. Jess holds a Biological Sciences degree from the University of Oxford, where she specialised in animal behavior and ecology.\n\nYou must confirm your public display name before commenting\nPlease logout and then login again, you will then be prompted to enter your display name.\n\nLogout\n\nMore about animals\n\nFish in the Mariana Trench all have the same, unique mutations\n\n4 snow leopards spotted together on remote Pakistan mountain in rare footage\n\nLatest\n\nScientists uncover 'inside-out, legless, headless wonder' that lived long before the dinosaurs\n\nSee more latest\n\nMost Popular\n\nScientists uncover 'inside-out, legless, headless wonder' that lived long before the dinosaurs\n\nHuge steam plume rises from Alaska's Mount Spurr as volcano edges closer to eruption\n\nFish in the Mariana Trench all have the same, unique mutations\n\nStaring at the March 29 solar eclipse can cause eye damage in seconds — and you won’t even feel it happening\n\n'Woolly devil' flowers in Texas desert are the 1st new plant genus discovered in a US national park in almost 50 years\n\nFlat, razor-thin telescope lens could change the game in deep space imaging — and production could start soon\n\nEclipse map: What will tomorrow's solar eclipse look like from your state?\n\n4 snow leopards spotted together on remote Pakistan mountain in rare footage\n\nJames Webb telescope zooms in on bizarre 'Einstein ring' caused by bending of the universe\n\nHow to watch tomorrow's solar eclipse from anywhere on Earth\n\nfunction loadTaboola()\n{\nvar script = window.document.createElement('script');\nscript.async = 1;\nscript.src = '//cdn.taboola.com/libtrc/futureplc-network/loader.js';\nvar insertLocation = window.document.getElementsByTagName('script')[0];\ninsertLocation.parentNode.insertBefore(script, insertLocation);\n}\nfunction taboolaInit(){\nwindow._taboola = window._taboola || [];\nwindow._taboola.push({article: 'auto'});\n(window.Promise\n? window.Promise.all([window.reliablePageLoad, window.reliableConsentGiven])\n: window.reliableConsentGiven\n).then(function () {\nvar delay = 0;\nwindow.setTimeout(loadTaboola, delay)\n});\n};\ntaboolaInit();\n\nwindow._taboola = window._taboola || [];\nvar screenWidth = window.screen.width;\nfunction taboola_is_device(device) {\nif ((! device) || device === null || (typeof device) === 'undefined') return true\nif (device === 'amp') return false\nif (device === 'desktop' && screenWidth >= 700) return true\nif (device === 'mobile' && screenWidth < 700) return true\nreturn false\n}\n\n(function(){\nvar suitableDevice = taboola_is_device(\"desktop\");\nvar suitablePlacement = !(\"Mid Article\".includes('Mid Article') && \"\") &&\n!(\"Mid Article\".includes('Mid Article') && window.FUTR && window.FUTR.Kiosq && window.FUTR.Kiosq.hasBarrier);\nif (suitableDevice && suitablePlacement) {\nwindow._taboola.push({\nmode: \"thumbnails-a-mid\",\ncontainer: \"desktop-taboola-mid-article\",\nplacement: \"Mid Article\",\ntarget_type: \"mix\",\n});\n}\n})();\n\n(function(){\nvar suitableDevice = taboola_is_device(\"mobile\");\nvar suitablePlacement = !(\"Mid Article\".includes('Mid Article') && \"\") &&\n!(\"Mid Article\".includes('Mid Article') && window.FUTR && window.FUTR.Kiosq && window.FUTR.Kiosq.hasBarrier);\nif (suitableDevice && suitablePlacement) {\nwindow._taboola.push({\nmode: \"thumbnails-a-mid\",\ncontainer: \"mobile-taboola-mid-article\",\nplacement: \"Mid Article\",\ntarget_type: \"mix\",\n});\n}\n})();\n\n(function(){\nvar suitableDevice = taboola_is_device(\"desktop\");\nvar suitablePlacement = !(\"Below Article Thumbnails\".includes('Mid Article') && \"\") &&\n!(\"Below Article Thumbnails\".includes('Mid Article') && window.FUTR && window.FUTR.Kiosq && window.FUTR.Kiosq.hasBarrier);\nif (suitableDevice && suitablePlacement) {\nwindow._taboola.push({\nmode: \"thumbnails-f\",\ncontainer: \"taboola-below-article-thumbnails\",\nplacement: \"Below Article Thumbnails\",\ntarget_type: \"mix\",\n});\n}\n})();\n\n(function(){\nvar suitableDevice = taboola_is_device(\"mobile\");\nvar suitablePlacement = !(\"Mobile Below Article Thumbnails\".includes('Mid Article') && \"\") &&\n!(\"Mobile Below Article Thumbnails\".includes('Mid Article') && window.FUTR && window.FUTR.Kiosq && window.FUTR.Kiosq.hasBarrier);\nif (suitableDevice && suitablePlacement) {\nwindow._taboola.push({\nmode: \"thumbnails-g\",\ncontainer: \"taboola-mobile-below-article-thumbnails\",\nplacement: \"Mobile Below Article Thumbnails\",\ntarget_type: \"mix\",\n});\n}\n})();\n\n(function(){\nvar delay = 0;\nwindow.setTimeout(function() {\nwindow._taboola.push({flush: true});\n}, delay);\n})();\n\n    if (window.sliceHydrationLazy) {\n        window.sliceHydrationLazy(\"popularBox\", \"popularBox\", JSON.stringify({\"tabs\":[{\"tabName\":\"Latest Articles\",\"articles\":[{\"href\":\"\\/animals\\/extinct-species\\/scientists-uncover-inside-out-legless-headless-wonder-that-lived-long-before-the-dinosaurs\",\"heading\":\"Scientists uncover 'inside-out, legless, headless wonder' that lived long before the dinosaurs\",\"image\":{\"src\":\"https:\\/\\/cdn.mos.cms.futurecdn.net\\/GugvyuJLNHqHRGmgEmjE7T.jpg\",\"alt\":\"The fossil Keurbos susanae - or Sue - in the rock.\",\"fullscreen\":false,\"lazyLoading\":true,\"dataHydrate\":true,\"addSEOMetaData\":false}},{\"href\":\"\\/planet-earth\\/volcanos\\/huge-steam-plume-rises-from-alaskas-mount-spurr-as-volcano-edges-closer-to-eruption\",\"heading\":\"Huge steam plume rises from Alaska's Mount Spurr as volcano edges closer to eruption\",\"image\":{\"src\":\"https:\\/\\/cdn.mos.cms.futurecdn.net\\/Qu4n5FmRQWXMhHifYiujNF.jpg\",\"alt\":\"Mount spurr\",\"fullscreen\":false,\"lazyLoading\":true,\"dataHydrate\":true,\"addSEOMetaData\":false}},{\"href\":\"\\/animals\\/fish\\/fish-in-the-mariana-trench-all-have-the-same-unique-mutations\",\"heading\":\"Fish in the Mariana Trench all have the same, unique mutations\",\"image\":{\"src\":\"https:\\/\\/cdn.mos.cms.futurecdn.net\\/F44iXEUuNSmx7E8Dz5rhP6.jpg\",\"alt\":\"Illustration of the earth and its oceans with different deep sea species that surround it,\",\"fullscreen\":false,\"lazyLoading\":true,\"dataHydrate\":true,\"addSEOMetaData\":false}},{\"href\":\"\\/health\\/anatomy\\/staring-at-the-march-29-solar-eclipse-can-cause-eye-damage-in-seconds-and-you-wont-even-feel-it-happening\",\"heading\":\"Staring at the March 29 solar eclipse can cause eye damage in seconds \\u2014 and you won\\u2019t even feel it happening\",\"image\":{\"src\":\"https:\\/\\/cdn.mos.cms.futurecdn.net\\/BCWj4K5cdXLqbKHV3SWV7h.jpg\",\"alt\":\"A kid is shown looking at the solar eclipse while wearing special protective glasses\",\"fullscreen\":false,\"lazyLoading\":true,\"dataHydrate\":true,\"addSEOMetaData\":false}},{\"href\":\"\\/planet-earth\\/plants\\/woolly-devil-flowers-in-texas-desert-are-the-1st-new-plant-genus-discovered-in-a-us-national-park-in-almost-50-years\",\"heading\":\"'Woolly devil' flowers in Texas desert are the 1st new plant genus discovered in a US national park in almost 50 years\",\"image\":{\"src\":\"https:\\/\\/cdn.mos.cms.futurecdn.net\\/XaWDSQQnyiBU8AmsyK5PoF.jpg\",\"alt\":\"The wooly devil (Ovicula biradiata), a flowering plant that appears soft and fuzzy.\",\"fullscreen\":false,\"lazyLoading\":true,\"dataHydrate\":true,\"addSEOMetaData\":false}}]}]}), \"https://slice.vanilla.futurecdn.net/13-2-0/js/popularBox.js\");\n    } else {\n        console.error('%c FTE ','background: #9306F9; color: #ffffff','no lazy slice hydration function available');\n    }\nLATEST ARTICLES1Scientists uncover 'inside-out, legless, headless wonder' that lived long before the dinosaurs2Huge steam plume rises from Alaska's Mount Spurr as volcano edges closer to eruption3Fish in the Mariana Trench all have the same, unique mutations4Staring at the March 29 solar eclipse can cause eye damage in seconds — and you won’t even feel it happening5'Woolly devil' flowers in Texas desert are the 1st new plant genus discovered in a US national park in almost 50 years\n\nif(FUTR && FUTR.Connect){\n//Init Connect article History\nclass userNav {\nconstructor(key = 'connect_articles_history') {\nthis.key = key;\nthis.flushKey = `${key}_flush`;\nthis.propsKey = `${key}_props`;\nthis.store();\nconsole.info(\"FUTR.Connect.userNav - Init - Start - Using reduxStore\");\n}\nstore() {\nconst isArticle = window?.reduxStore?.getState()?.vanilla?.isArticle;\nif (typeof isArticle !== 'undefined' && isArticle && FUTR && FUTR.Connect) {\ntry {\nconst month = `${new Date().getFullYear()}-${new Date().getMonth()}`;\n//flush monthly\nif (localStorage.getItem(this.flushKey) !== month) {\nlocalStorage.setItem(this.key, btoa('[]'));\nlocalStorage.setItem(this.propsKey, []);\nlocalStorage.setItem(this.flushKey, month);\n}\nconst currentUrl = location.pathname;\nconst urls = JSON.parse(atob(localStorage.getItem(this.key) || btoa('[]')));\nconst props = JSON.parse(localStorage.getItem(this.propsKey)|| '[]');\nif (!urls.includes(currentUrl) && this.getProps().length < 20 || this.getProps().length <1) {\nurls.push(currentUrl);\nif (window.ffte && window.ffte.properties) {\nprops.push(window.ffte.properties);\nconsole.log(\"props push\", props)\n}\nlocalStorage.setItem(this.key, btoa(JSON.stringify(urls)));\nlocalStorage.setItem(this.propsKey, JSON.stringify(props));\n}\nconsole.info(\"FUTR.Connect.userNav - Urls Stored\");\n} catch (e) {\nconsole.warn('userNav:', e);\n}\n}\n}\ngetHistory() {\ntry {\nreturn JSON.parse(atob(localStorage.getItem(this.key) || btoa('[]')));\n} catch {\nreturn [];\n}\n}\ngetProps() {\ntry {\nreturn JSON.parse(localStorage.getItem(this.propsKey) || '[]');\n} catch {\nreturn [];\n}\n}\n};\nFUTR.Connect.userNav = new userNav();\nconsole.info(\"FUTR.Connect.userNav - Init - Done\", FUTR.Connect.userNav);\n}",
    "summary": {
      "en": "Researchers have discovered that an ancient organism called Prototaxites, which was once thought to be a type of giant fungus, may actually belong to an entirely unknown branch of life. Prototaxites existed around 420 to 375 million years ago and resembled large, branchless tree trunks, reaching heights of up to 26 feet (8 meters). Initially classified as a fungus based on chemical analysis, recent studies suggest it might not fit into any known categories of life, including fungi, plants, or animals.\n\nA recent study focused on a smaller species of Prototaxites called Prototaxites taiti, found in Scotland. Researchers found that its internal structure was unlike any known fungi and lacked chitin, a key component of fungal cell walls. Instead, it showed similarities to lignin, which is found in plants. This leads scientists to propose that Prototaxites is part of a previously unknown group of eukaryotes.\n\nFurther research is needed to fully understand Prototaxites and what caused its extinction. The findings have sparked interest and curiosity in the scientific community, hinting at the possibility of undiscovered life forms in Earth's history.",
      "ko": "연구자들은 고대 생물인 프로토택사이트스가 한때 거대한 균류로 여겨졌지만, 사실은 전혀 알려지지 않은 생명체의 가지에 속할 수 있다는 사실을 발견했습니다. 프로토택사이트스는 약 4억 2천만 년에서 3억 7천5백만 년 전 사이에 존재했으며, 가지가 없는 큰 나무 줄기처럼 생겼고, 높이는 최대 8미터에 달했습니다. 초기에는 화학 분석을 통해 균류로 분류되었지만, 최근 연구에서는 이 생물이 균류, 식물, 동물 등 기존의 생명체 분류에 들어맞지 않을 수 있다고 제안하고 있습니다.\n\n최근 연구는 스코틀랜드에서 발견된 더 작은 종인 프로토택사이트스 타이티에 초점을 맞췄습니다. 연구자들은 이 생물의 내부 구조가 알려진 균류와는 다르며, 균류 세포벽의 주요 성분인 키틴이 없다는 것을 발견했습니다. 대신, 식물에서 발견되는 리그닌과 유사한 점이 있었습니다. 이러한 발견은 프로토택사이트스가 이전에 알려지지 않은 진핵생물의 일종일 가능성을 제기합니다.\n\n프로토택사이트스와 그 멸종 원인을 완전히 이해하기 위해서는 추가 연구가 필요합니다. 이러한 발견은 과학계에서 큰 관심과 호기심을 불러일으키며, 지구 역사 속에 아직 발견되지 않은 생명체의 가능성을 암시하고 있습니다.",
      "ja": null
    }
  },
  {
    "id": "70215fcdb07c50d1",
    "title": {
      "en": "7.7 magnitude earthquake hits Southeast Asia, affecting Myanmar and Thailand",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://twitter.com/TaraBull808/status/1905534938558157139",
    "score": 246,
    "by": "testrun",
    "time": 1743154324,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "c673589ef5c8aa74",
    "title": {
      "en": "Let's Take a Look at JEP 483: Ahead-of-Time Class Loading and Linking",
      "ko": "JEP 483: 미리 로딩하는 클래스",
      "ja": null
    },
    "type": "story",
    "url": "https://www.morling.dev/blog/jep-483-aot-class-loading-linking/",
    "score": 43,
    "by": "ingve",
    "time": 1743160315,
    "content": "1\n2\n3\ntar xvf kafka_2.13-4.0.0.tgz\nKAFKA_CLUSTER_ID=\"$(bin/kafka-storage.sh random-uuid)\"\nbin/kafka-storage.sh format --standalone -t $KAFKA_CLUSTER_ID -c config/server.properties",
    "summary": {
      "en": "To set up Kafka, follow these steps:\n\n1. Extract the Kafka files using the command: `tar xvf kafka_2.13-4.0.0.tgz`.\n2. Generate a unique cluster ID with: `KAFKA_CLUSTER_ID=\"$(bin/kafka-storage.sh random-uuid)\"`.\n3. Format the storage for Kafka by running: `bin/kafka-storage.sh format --standalone -t $KAFKA_CLUSTER_ID -c config/server.properties`.",
      "ko": "Kafka를 설정하려면 다음 단계를 따르세요. \n\n먼저, Kafka 파일을 추출합니다. 이를 위해 `tar xvf kafka_2.13-4.0.0.tgz` 명령어를 사용합니다. 다음으로, 고유한 클러스터 ID를 생성합니다. 이때는 `KAFKA_CLUSTER_ID=\"$(bin/kafka-storage.sh random-uuid)\"` 명령어를 입력합니다. 마지막으로, Kafka의 저장소를 포맷합니다. 이를 위해 `bin/kafka-storage.sh format --standalone -t $KAFKA_CLUSTER_ID -c config/server.properties` 명령어를 실행합니다.",
      "ja": null
    }
  },
  {
    "id": "10129d8b3a81dd85",
    "title": {
      "en": "Knowledge Library MCP",
      "ko": "지식 라이브러리 MCP",
      "ja": null
    },
    "type": "story",
    "url": "https://devpost.com/software/knowledge-library-mcp",
    "score": 3,
    "by": "thstart",
    "time": 1743195670,
    "content": "1234567891011\n\n  Knowledge Library MCP—Azure AI Agent Service and MCP Powers Precision Querying\n\nAzure AI Services Multi-Agent Multimodal - Speech, Vision, Text RAG Implementation\nleveraging Anthropic’s Model Context Protocol\n\nKnowledge Library MCP (KL MCP) is a multi-modal application leveraging Azure AI Agent Service to locate documents—text and images—and deliver conversational insights via bots. It enhances search with live data integration and Responsible AI principles, designed for scalable, professional-grade querying.\n\nAbout the Project\n\nKL MCP—Azure AI Foundry and GPT-4o Turn Data into Financial Clarity\n\nImagine thousands of documents—Machine Learning notes, TSLA 10-Ks, workflows, and charts—transformed from chaos into actionable insights. KL MCP, built with Azure AI Agent Service, Microsoft Document AI, and an upgraded FinancialAnalysisApp, delivers a fast, focused system for retrieving files and answering queries. This is how I turned a developer’s challenge into a hackathon standout.\n\nThe library-like backbone of KL MCP, fueled by Azure AI Agent Service, Microsoft Document AI, Microsoft Graph, and my custom MCP bots.\n\nDetailed architecture of KL MCP, showcasing its multi-agent, multimodal design.\n\nInspiration\n\nAs a developer, I faced endless hours sifting through PDFs, Excel sheets, and charts for a single useful detail. Anthropic’s Model Context Protocol inspired me—context could unlock disorganized data’s potential. I began by segmenting documents into Cosmos DB NoSQL, but as the volume grew, search accuracy faltered. That struggle drove me to create KL MCP—a structured, library-like solution powered by Azure AI Agent Service and Microsoft Document AI, converting a daily frustration into a competitive edge.\n\nWhat It Does\n\nKL MCP retrieves documents—text or images—like Machine Learning pipelines, SEC filings, or workflow diagrams, and supports conversational queries. Ask a TSLA 10-K chart, “What’s the revenue outlook?”—it delivers. It processes PDFs, Word documents, Excel sheets, PowerPoint slides, raw text, HTML, and images, using Microsoft Document AI’s OCR and layout analysis for precision. With Azure AI Agent Service and custom bots, it surpasses Azure AI Search by integrating live data and code-generated insights, offering a dynamic, responsive chat experience. Data captured, answers delivered.\n\nKL MCP snags OneDrive docs and powers chat queries across text and images like a pro.\n\nHow I Built It\n\nI developed KL MCP single-handedly, utilizing Azure AI Agent Service, Microsoft Document AI, and GitHub Copilot’s assistance to refine my code and logic. Here’s the breakdown:\n\nFoundation: Azure AI Agent Service anchors KL MCP—a managed platform deploying intelligent agents with models like GPT-4o and Mistral. It excels at processing text data—raw files, structured spreadsheets—from storage or indices with speed, as noted in its documentation. Its limitation? No image support—charts like the MSCI market cap breakdown were invisible to it.\nVision Integration: GPT-4o, hosted on Azure OpenAI Service, fills that gap. This multimodal model analyzes images, extracts text, and interprets graphical data accurately. Its ability to decode financial visuals, as highlighted in recent analyses, made it essential for KL MCP’s chart-handling needs.\nBot System: I created specialized bots—DocBot for Machine Learning and workflows, SECBot for filings—each linked to domain-specific agents, such as TSLA or ML documents. Files upload to vector stores, with Cosmos DB NoSQL managing metadata—file type, date—for semantic searches. Microsoft Document AI processes text and images, enabling seamless chat integration.\nLive Data: API calls pull current stock prices, keeping responses fresh beyond static retrieval limits. Python scripts generate visuals—like profit trend graphs—enhancing chat outputs with calculated insights.\nAgent Tuning: I configured a GPT-4o agent to locate files, execute code, and chat, incorporating Responsible AI principles—fairness, transparency—for scalability. A lightweight app, built with the Azure AI Agents SDK, manages uploads, threading, and querying.\n\nLaying the Foundation with Azure AI Agent Service\n\nAzure AI Agent Service became KL MCP’s heavy hitter—a managed platform spinning up agents with GPT-4o and Mistral muscle. It’s a beast at chewing through text—raw files, spreadsheets, you name it—pulling insights from storage or search indices in a flash. The docs tout its text-handling prowess, and it’s a champ for data-driven smarts.\n\nBut it’s blind to visuals out of the box. A chart like the MSCI market cap breakdown was invisible to it—unacceptable for KL MCP. I needed a system that could see and think across all data types.\n\nBringing in Vision with GPT-4o\n\nEnter GPT-4o, OpenAI’s multimodal marvel on Azure OpenAI Service. It doesn’t just read—it sees. It scans images, extracts text, and decodes charts like a pro. A write-up showed it nailing financial trends from visuals, and I knew it’d supercharge Azure AI Agent Service. GPT-4o could crack my MSCI chart, turning pixels into text that the agents could jam with, bridging visuals and text into a killer workflow.\n\nTapping OneDrive with Microsoft Graph API\n\nTo level up, I wired in Microsoft Graph API with a confidential client flow using client_secret. It grabs docs straight from OneDrive, persisting tokens across restarts with a tokens.json file. If the access token expires, it refreshes it automatically using the refresh_token, keeping the system humming without constant logins. Copilot helped me nail the OAuth dance—authentication at /auth/start, token exchange at /auth/callback, and a list_onedrive_root tool to fetch and chat about OneDrive goodies.\n\nBuilding the Bot Crew\n\nI crafted a bot squad—DocBot for ML and workflows, SECBot for filings—each tied to domain-specific agents (TSLA, ML docs). Copilot guided me to vectorize uploads into stores, while Cosmos DB NoSQL tracked metadata like file type and date. Microsoft Document AI processed text and images, and Graph API pulled OneDrive data, making chats feel alive, not like a dusty catalog.\n\nAdding Live Data and Visual Flair\n\nCopilot helped me whip up API calls—like live stock prices—keeping data fresh, not stale. Python scripts crunched numbers and spun up visuals—like profit trend graphs—adding flair to chat responses.\n\nTuning the GPT-4o Agent\n\nWith Copilot’s nudge, I tuned a GPT-4o agent to “find files, run code, and chat,” blending multi-modal insights with Responsible AI principles like fairness and transparency. It integrates OneDrive queries seamlessly, keeping the system sharp.\n\nSolving the MSCI Chart\n\nA highlight was analyzing the MSCI market cap chart from a TSLA filing—“SELECTED COUNTRIES MSCI MARKET CAP AS A PERCENT OF WORLD MSCI (percent, daily, based on US$),” tracking US, Europe, and Japan from 2016 to 2025. GPT-4o extracted the US red line, stable above 60% by 2025; Europe’s blue line, falling from 50% to over 35%; and Japan’s green line, declining from 70% to around 45%. It noted specifics—like the US at 61.9 on March 14, 2024—and converted the chart into text, stored for Azure AI Agent Service to answer queries like “What’s Japan’s MSCI share in 2020?” with precision.\n\nChallenges I Faced\n\nScaling proved difficult—Cosmos DB NoSQL struggled with precision as document counts reached thousands. Structuring a library-like system within Azure required extensive iteration, and aligning metadata with vector stores demanded meticulous effort. Integrating live data, code outputs, and Responsible AI standards—ensuring fairness and transparency—tested my resolve, but persistent debugging kept the project on track.\n\nAccomplishments I’m Proud Of\n\nI’m proud to have crafted a multi-modal app that retrieves a TSLA 10-K chart and delivers trend insights instantly. It outperforms Azure AI Search with a bot-driven system, live data, and Responsible AI integration. Solving a developer’s persistent challenge with a polished, effective tool stands as a significant accomplishment. Complexity met, clarity gained.\n\nKL MCP chatting about a TSLA 10-K chart, mixing OneDrive data, text, and images like a rockstar.\n\nLessons Learned\n\nAzure AI Agent Service and Microsoft Document AI excel in multi-modal applications—combining text and image processing with advanced language capabilities is transformative. Hierarchical bot systems outmatch flat indexes at scale, and Responsible AI is integral to robust design. GitHub Copilot accelerated my development, while building a library structure honed my metadata and integration skills.\n\nWhat’s Next for Knowledge Library MCP\n\nKL MCP’s next steps include adding voice and audio for query input, video support for workflow visuals, enhanced bot logic, AI-driven metadata tagging, and Azure Machine Learning for predictive insights—all grounded in Responsible AI.\n\nNew Features and Updates in FinancialAnalysisApp\n\nFinancialAnalysisApp, KL MCP’s backbone, received critical upgrades to handle the MSCI chart and more. It now accepts command-line inputs to manage vector stores or target specific files, uses environment settings—like connection strings—for security, and identifies supported formats by file name. It fully controls vector stores in Azure—listing, managing, or clearing them—and offers a conversational loop, answering MSCI trend queries—like the US holding above 60% while Japan drops from 70% to 45%—with live, precise responses. These enhancements elevate KL MCP’s financial analysis capabilities significantly.\n\nProjects in Knowledge Library MCP\n\nThe Knowledge Library MCP repository includes the following projects:\n\n1. CompanyResearch\n\nTools and scripts for conducting company research, including document classification and structured data extraction.\n\nArchitecture of CompanyResearch, highlighting its document processing and data extraction workflow.\n\n2. FinancialAnalysisApp\n\nA .NET-based application for financial analysis, supporting document processing and conversational insights.\n\nArchitecture of FinancialAnalysisApp, showcasing its financial data handling and chat capabilities.\n\n3. go-mcp-brave\n\nA Go-based MCP server integrating with the Brave Search API for real-time web, news, image, and video search results.\n\nArchitecture of go-mcp-brave, detailing its integration with Brave Search API.\n\n4. go-mcp-metasearch\n\nA Go-based application providing metasearch functionality by aggregating results from multiple search engines and APIs.\n\nArchitecture of go-mcp-metasearch, illustrating its multi-engine aggregation process.\n\n5. mcp-azure-server\n\nA Python-based server integrating with Azure AI services for document processing, vector store management, and AI-powered interactions.\n\nArchitecture of mcp-azure-server, showing its Azure AI integration and processing pipeline.\n\n6. mcp-server-go\n\nA Go-based MCP server implementing the Model Context Protocol for tool execution and data retrieval.\n\nArchitecture of mcp-server-go, outlining its MCP implementation and tool execution.\n\n7. realtime-audio\n\nA Python project for real-time audio processing, supporting audio-to-text and text-to-audio interactions.\n\nTechnologies Used\n\nThe Knowledge Library MCP project leverages a diverse set of technologies to deliver robust and scalable solutions. Below is a list of the key technologies used across various components of the project:\n\nProgramming Languages\n\nPython: Used in CompanyResearch, mcp-azure-server, and realtime-audio for data processing, server-side logic, and real-time audio interaction.\nC#: Used in FinancialAnalysisApp and other .NET-based components for financial analysis and web services.\nGo: Used in go-mcp-brave, go-mcp-metasearch, and mcp-server-go for high-performance microservices.\n\nFrameworks and Libraries\n\n.NET 9.0: For building web and API services in KnowledgeLibraryMCP.\nPlaywright: For webpage automation and screenshot capture in CompanyResearch.\nDocling-Core: For document processing and structuring in CompanyResearch.\nMLX-VLM: For generating DocTags and extracting structured data in CompanyResearch.\nFluent UI: For building modern and responsive web components in KnowledgeLibraryMCP.Web.\nMarkdig: For Markdown processing in various components.\nPyDub: For audio playback and manipulation in realtime-audio.\nSpeechRecognition Library: For capturing and converting audio input into text in realtime-audio.\n\nCloud and AI Services\n\nAzure AI Services: For document intelligence, vector store management, and AI-powered search.\nAzure OpenAI Service: For integrating GPT-4o Realtime Preview for multimodal AI capabilities.\nAzure Document Intelligence: For text extraction and image analysis.\nBrave Search API: For fetching web search results in text-search-audio.py.\n\nTools and Utilities\n\nMermaid.js: For generating architecture diagrams.\nNode.js and npm: Required for running the Mermaid CLI.\nBeautifulSoup: For HTML parsing and text extraction in CompanyResearch.\nPillow: For image processing in CompanyResearch.\nBash: For automation scripts like update_architecture_diagram.sh.\n\nAdditional Technologies Used\n\nThe Knowledge Library MCP project also incorporates the following technologies:\n\nMicrosoft Graph API: For accessing and managing OneDrive documents.\nAzure Machine Learning: For predictive insights and advanced analytics.\nGitHub Copilot: For accelerating development and improving code quality.\nOAuth 2.0: For secure authentication and token management in API integrations.\n\nThese additional technologies enhance the project's capabilities, ensuring robust data management, seamless integration, and efficient development workflows.\n\n    Built With\n\n    aiazurec#python\n\n    Try it out\n\n  GitHub Repo\n\n      Submitted to\n\n        Azure AI Developer Hackathon\n\n    Created by\n\n      Radka Vassileva\n\n      Constantine Vassilev\n\n        Seasoned systems engineer with over 30 years of experience.  Passionate about leveraging AI to boost efficiency in software development.",
    "summary": {
      "en": "**Summary of Knowledge Library MCP Project**\n\nThe Knowledge Library MCP (KL MCP) is an advanced application that uses Azure AI Services to improve document searching and provide conversational insights through bots. It can handle various document types, including text and images, and integrates live data to enhance search accuracy and responsiveness.\n\n**Project Overview:**\n- KL MCP transforms chaotic data, like financial documents and charts, into actionable insights.\n- It combines Azure AI Agent Service, Microsoft Document AI, and custom bots for efficient querying and data retrieval.\n\n**Inspiration and Development:**\n- The project was inspired by the need to manage large amounts of data effectively. Existing systems struggled with search accuracy as document volume increased.\n- KL MCP organizes documents in a structured way and uses Anthropic’s Model Context Protocol to extract valuable insights.\n\n**Key Features:**\n- Supports queries on various document formats (PDFs, Word, Excel, images) using advanced text and image analysis.\n- Provides real-time responses by integrating live data, such as current stock prices.\n- Features a bot system for specific tasks, enhancing user interaction and query resolution.\n\n**Technical Aspects:**\n- Built using Azure AI Agent Service and Microsoft Document AI, alongside tools like GitHub Copilot for code refinement.\n- Utilizes Cosmos DB for metadata management and Microsoft Graph API for document access from OneDrive.\n- Incorporates a multimodal model (GPT-4o) for processing both text and images, bridging the gap between different data types.\n\n**Future Developments:**\n- Plans to add voice input, video support, and AI-driven insights for further enhancing functionality.\n- Continuous upgrades to the FinancialAnalysisApp to improve financial data handling and querying capabilities.\n\n**Technologies Used:**\n- Programming Languages: Python, C#, Go\n- Frameworks: .NET, various libraries for document processing and audio interactions.\n- Azure Services: For AI capabilities, document intelligence, and cloud integration.\n\n**Conclusion:**\nKL MCP is a significant leap forward in document management and querying, combining advanced AI technologies to deliver fast, accurate insights and an enhanced user experience.",
      "ko": "지식 라이브러리 MCP(KL MCP)는 Azure AI 서비스를 활용하여 문서 검색을 개선하고 대화형 인사이트를 제공하는 고급 애플리케이션입니다. 이 시스템은 텍스트와 이미지 등 다양한 문서 유형을 처리할 수 있으며, 실시간 데이터를 통합하여 검색의 정확성과 반응성을 높입니다.\n\nKL MCP는 재무 문서와 차트와 같은 혼란스러운 데이터를 실행 가능한 인사이트로 변환합니다. Azure AI 에이전트 서비스, Microsoft Document AI, 그리고 맞춤형 봇을 결합하여 효율적인 쿼리와 데이터 검색을 지원합니다.\n\n이 프로젝트는 대량의 데이터를 효과적으로 관리할 필요성에서 영감을 받았습니다. 기존 시스템은 문서 양이 증가함에 따라 검색 정확성에서 어려움을 겪었습니다. KL MCP는 문서를 체계적으로 정리하고, Anthropic의 모델 컨텍스트 프로토콜을 사용하여 가치 있는 인사이트를 추출합니다.\n\nKL MCP는 PDF, 워드, 엑셀, 이미지 등 다양한 문서 형식에 대한 쿼리를 지원하며, 고급 텍스트 및 이미지 분석을 통해 실시간 응답을 제공합니다. 또한, 특정 작업을 위한 봇 시스템을 갖추고 있어 사용자 상호작용과 쿼리 해결을 향상시킵니다.\n\n이 시스템은 Azure AI 에이전트 서비스와 Microsoft Document AI를 기반으로 구축되었으며, 코드 개선을 위해 GitHub Copilot과 같은 도구를 사용합니다. 메타데이터 관리를 위해 Cosmos DB를 활용하고, OneDrive에서 문서에 접근하기 위해 Microsoft Graph API를 사용합니다. 텍스트와 이미지를 모두 처리할 수 있는 다중 모달 모델(GPT-4o)을 통합하여 서로 다른 데이터 유형 간의 간극을 메웁니다.\n\n앞으로 음성 입력, 비디오 지원, AI 기반 인사이트 추가를 통해 기능을 더욱 향상시킬 계획입니다. 재무 데이터 처리 및 쿼리 기능을 개선하기 위해 FinancialAnalysisApp의 지속적인 업그레이드도 예정되어 있습니다.\n\n이 프로젝트에서 사용된 기술로는 Python, C#, Go와 같은 프로그래밍 언어와 문서 처리 및 오디오 상호작용을 위한 다양한 라이브러리, AI 기능과 클라우드 통합을 위한 Azure 서비스가 포함됩니다. KL MCP는 문서 관리와 쿼리에서 중요한 발전을 이루어내며, 빠르고 정확한 인사이트를 제공하고 사용자 경험을 향상시키는 고급 AI 기술을 결합하고 있습니다.",
      "ja": null
    }
  },
  {
    "id": "10097a15cc5088c4",
    "title": {
      "en": "Things I would have told myself before building an autorouter",
      "ko": "자동 라우터 제작 전 꼭 알아야 할 것",
      "ja": null
    },
    "type": "story",
    "url": "https://blog.autorouting.com/p/13-things-i-would-have-told-myself",
    "score": 345,
    "by": "seveibar",
    "time": 1743122333,
    "content": "Share this postautorouting13 things I would have told myself before building an autorouterCopy linkFacebookEmailNotesMoreDiscover more from autoroutingReviews, benchmarks and open datasets for autorouting, with a focus on open-source autorouting.SubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.Already have an account? Sign in13 things I would have told myself before building an autorouterImportant lessons from trying to build the world's fastest autorouter for about a yearSeveMar 28, 20258Share this postautorouting13 things I would have told myself before building an autorouterCopy linkFacebookEmailNotesMore31ShareI’ve spent about a year working on an autorouter for tscircuit (an open-source electronics CAD kernel written in Typescript). If I could go back a year, these are the 13 things I would tell myself:An intermediate stage of our autorouting routing a keyboard.1. Know A* like the back of your hand, use it everywhereIf I was king for a day, I would rename A* to “Fundamental Algorithm”. It is truly one of the most adaptable and important algorithms for _any kind_ of search. It is simply the best foundation for any kind of informed search (not just for 2d grids!)Here’s an animated version of A* versus “breadth first search” on a 2d grid:The way A* explores nodes is a lot faster and more intuitive. The major difference between these two algorithms is BFS explores all adjacent nodes, while A* prioritizes exploring nodes that are closer to the destination. Because it considers a metric outside the graph (the distance to the destination) it’s an informed search.You are already either using BFS or DFS (depth-first search) in your code. A recursive algorithm is a depth first search. Any loop that explores candidates/neighbors without sorting the candidates is a BFS. 99% of the time you can convert it to A* and get dramatic performance gains!One of my favorite techniques in our autorouter is we run multiple levels of A* to discover the optimal hyperparameters for a particular problem. So we’re basically running each autorouter as a candidate, then using A* to determine which autorouters we should spend the most time on!See all those numbers at the top? Those are each different configurations of hyper parameters. Running each autorouter fairly would be a huge waste of time- if one autorouter starts to win (it is successfully routing with good costs) allocate more iterations to it! This kind of meta-A* combines a regular cost function that penalizes distance with a cost function that penalizes iterations.2. Implementation Language doesn’t matterI’m controversially writing our autorouter in Javascript. This is the first thing people call out, but it’s not as unreasonable as you might expect. Consider that when optimizing an algorithm, you’re basically looking at improving two things:Lowering the number of iterations required (make the algorithm smart)Increasing the speed of each iterationPeople focus way too much on improving the speed of each iteration. If you are doing something dumb (like converting everything to a grid for overlap testing), Javascript performance will beat you no matter what language you use!Dumb algorithms in optimal assembly are slower than smart algorithms in Javascript! Algorithm > Language!95% of your focus should be on reducing the number of iterations. This is why language doesn’t matter. Whatever gets you to the smartest, most cacheable algorithm fastest is the best language.3. Spatial Hash Indexing > Tree Data StructuresYou can’t walk 5 feet into multi-dimensional space optimization without someone mentioning a QuadTree, this incredible data structure that makes O(N) search O(log(N)) when searching for nearby objects in 2d/3d space.The QuadTree and every general-purpose tree data structure are insanely slow. Trees are not an informed representation of your data.Any time you’re using a tree you’re ignoring an  O(~1) hash algorithm for a more complicated O(log(N)) algorithmWhy does Javascript use HashSets and HashMaps by default and every chance it gets? They’re super super fast. A Spatial Hash Index is the same concept as a HashMap, but instead of hashing the object we hash it’s location and store it in a Cell (or “bucket of things that are close together”)Let’s look at how we might replace the QuadTree with a SpatialHashIndex with 20% as much code:There are many variants of this basic data structure for different types of objects, but they all look pretty similar. We’re basically just creating “buckets” with spatial hashes and filling them with any object that is contained within the cell represented by the spatial hash.The reason spatial hashes aren’t as popular is you need to be careful about selecting your cell size- this is what makes it an informed algorithm. If your cell size isn’t calibrated well, you’ll end up paying high fixed costs per retrieval. In practice, it’s not that difficult to pick a reasonable cell size.4. Effective Spatial Partitioning + Caching is 1000x more important than algorithm performanceA circuit board like the one inside an IPhone probably has somewhere between 10,000 and 20,000 traces and take a team several months to route with the best EDA tools in world. It can seem daunting to try to optimize such an incredibly complex task- but the truth is the entire industry is neglecting a very simple idea: everything that has been routed has been routed before.Game developers “pre-bake” navigation meshes into many gigabytes for their games. LLMs compress the entire internet into weights for search. The next generation of autorouters will spatially partition their problems, then call upon a massive cache for pre-solved solutions. The speed of the algorithm doesn’t matter when you have a massive cache with 99% of the autorouting problem pre-solved.Most algorithms today do not focus on the effective cache-reusability or effective spatial partitioning, but a critical component of future autorouters will be caching inputs and outputs from each stage in a spatially partitioned way.Moreover, the size of storage and caching seems to go down faster than the speed of computation goes up. It’s not a big deal to have a gigabyte cache to make your autorouter 50% faster.At the end of the day, the cache will win. Cacheable algorithms matter more than fast algorithms!5. If you do not have a visualization for a problem, you will never solve itIf there is one thing I could have printed on a poster, it would be VISUALIZE THE PROBLEM. You can’t debug problems by staring at numbers.For every tiny problem we solve, we have a visualization. We will often start with the visualization. Time and time again this enables us to debug and solve problems 10x faster than we could otherwise. Here’s a visualization we made of a subalgorithm for finding 45 degree paths, we use this in our “Path Simplification Phase”, an ~final phase of the autorouter.6. Javascript Profiling Is Amazing- Use it!Javascript profiling tools are incredibe, you can easily see the exact total time in ms spend on each line of code. You don’t need to use any performance framework, just execute your javascript in the browser and pull up the performance tab. There are also awesome features like flame charts and stuff for memory usage.Example flamechat for debugging performance in @tscircuit/coreYou can easily see the time spent on each line of code inside Chrome’s performance tools!Here’s a little youtube short I made about it7. Never use recursive functionsRecursive functions are bad for multiple reasons:They are almost always synchronous (can’t be broken out for animation)They are inherently a Depth-First Search, and can’t be easily morphed to A*You can’t easily track iterationsMutability is often unnatural in recursive functions but critical to performanceHere’s an example of an “obviously recursive” function converted to a non-recursive function:The iteration-based implementation is much faster because it keeps a set of visitedNodes and checks nodes prior to exploration. You can do this with recursive functions, but you have to pass around a mutable object and do other unnatural things. It’s just best to avoid recursive functions when writing performant code.8. Monte Carlo algorithms are hacks. AVOIDMonte Carlo algorithms use randomness to iterate towards a solution. They are bad because:They lead to non-deterministic, hard-to-debug algorithmsThey are basically never optimal relative to a heuristicI sometimes use Monte Carlo-style algorithms when I don’t yet know how the algorithm should get to the solution, but I know how to score a candidate. They can help give some basic intuition about how to solve a problem. Once you have something approximating a cost function, do something smarter than Monte Carlo or any other random technique like Simulated Annealing. If your algorithm is sensitive to local minimums, consider using hyper parameters or more complex cost functions. Almost any local minimum your human eye can see can be made into a component of a cost function.Another way to think about it: How many PCB Designers randomly draw lines on their circuit board? None. Nobody does that. It’s just not a good technique for this domain. You’ll always be able to find a better heuristic.9. Keep intermediate algorithms groundedOur autorouter is currently a pipeline with 13 stages and something like 20 sub-algorithms that we measure the iteration count of for various things like determining spatial partitions or simplifying paths at the boundaries independently autorouted sections.Being able to overlay different inputs/output visualizations of each stage of the algorithm helps you understand the context surrounding the problem you’re solving. I often ran into issues at downstream stages (often our “high density routing” stage) that could be solved by improving the output of previous stages.The temptation when building sub-algorithms is to isolate the algorithm to its simplest form, maybe even normalizing around (0, 0). The danger with normalization or any complex transformation is it might impact the ability to quickly see consequences from early stages of the algorithm to later stages of the algorithm. To prevent this, just keep your coordinate space consistent throughout the lifecycle of the algorithm.Here’s each stage of our algorithm one after another. We often zoom in on this to see what stage is the most guilty culprit for a failed Design Rule Check.10. Animate your iterations to catch stupid behaviorRemember how it’s super important to lower your iteration count?Animating the iterations of your algorithm will show you how “dumb” it’s being by giving you an intuition for how many iterations are wasted exploring paths that don’t matter. This is particularly helpful when adjusting the greedy multiplier (discussed in 12)This video is an animation of a simple trace failing to solve, but instead of failing outright attempting to solve endlessly outward. Without the animation, it would have been hard to tell what was going on!11. Intersection math is fast, do you really need a grid?Consider two ways to determine if a trace A overlaps another trace B:Consider each segment of A and B, and check for intersections1Create a binary grid that marks each square where trace B is present, then check all the squares where trace A is present to see if B is thereBelieve it or not, most people would choose to use Option 2 with a binary grid check, even though this can easily be 1000x slower. People do this because math is hard 🤦Luckily LLMs make this kind of intersection math trivial. Use fast vector math!! Checking a SINGLE grid square (memory access!) can literally be slower than doing a dot product to determine if two segments intersect!12. Measure spatial probability of failure at each stage, prioritize solvabilityWhen doing spatial partitioning of the problem, you can measure the probability of solve failure of each stage with some leading indicators. For example, in the Unravel Autorouter we track the probability of failure for each “Capacity Node” at each major pipeline stage. Each stage focuses on reconfiguring adjacent nodes or rerouting to reduce the probability of failure.The great thing about probability of failure as a metric is you can literally measure it and improve your prediction as your algorithm changes. Each stage can then do it’s best to minimize the chance of future stages failing.I think generally prioritizing solvability is better than trying to incorporate too many constraints. Once a board is solved, it’s often easier to “work with that solution” than to generate optimal solution from scratch.13. The “Greedy Multiplier”, the secret hack to 100x A* performance at the cost of optimality Ok it’s not exactly a secret, maybe a “well-known secret”, but if you don’t know about it, you’re not using A* properly.By default, A* is guaranteed to give you the optimal solution, but what if you care more about speed than about optimality? Make one tiny change to your f(n)and you have Weighted A*, a variant of A* that solves more greedily, and generally much, much faster!Normal A*: f(n) = g(n) + h(n)Weighted A*: f(n) = g(n) + w * h(n)You can read more about weighted A* and other A* variants here.Game developers have a lot of the same problems as autorouting developers, so it’s not a bad idea to look for game development papers if you’re searching for related work!We’re making an autorouter.If this was interesting to you, I’d love to show you our autorouter as it gets closer to release. I believe that solving autorouting will be a massive unlock for physical-world innovation and is a key piece to enable the “vibe-building” of electronics. All of our work is MIT-licensed open-source. You can also follow me on twitter.Thanks for reading autorouting! Subscribe to hear when we release our insanely fast autorouter!Subscribe1Technically, you should use “segment to segment” distance to ensure appropriate margins, which is slightly more complex than intersection, but not by much8Share this postautorouting13 things I would have told myself before building an autorouterCopy linkFacebookEmailNotesMore31Share",
    "summary": {
      "en": "**Summary: 13 Key Lessons from Building an Autorouter**\n\n1. **Master A***: A* is a powerful search algorithm that outperforms other methods like breadth-first search (BFS) in finding the shortest paths by prioritizing closer nodes.\n\n2. **Language Choice Matters Less**: The programming language (e.g., JavaScript) is less important than using smart algorithms. Focus on reducing the number of iterations instead of just speeding up each iteration.\n\n3. **Use Spatial Hashing**: Instead of tree data structures, use spatial hash indexing for faster searches in multi-dimensional spaces.\n\n4. **Caching is Crucial**: Effective spatial partitioning and caching pre-solved problems can drastically improve performance, making algorithm speed less critical.\n\n5. **Visualize Problems**: Always create visual representations of your problems to aid debugging and understanding.\n\n6. **Utilize JavaScript Profiling**: Take advantage of browser profiling tools to analyze and optimize your code's performance.\n\n7. **Avoid Recursion**: Recursive functions can complicate tracking and performance; prefer iterative approaches for better efficiency.\n\n8. **Steer Clear of Monte Carlo Methods**: These use randomness and are often inefficient and hard to debug. Instead, seek better heuristics.\n\n9. **Keep Algorithms Grounded**: Maintain consistent coordinate spaces throughout algorithm stages for clarity and to avoid issues in later stages.\n\n10. **Animate Your Process**: Visualizing algorithms in action can highlight inefficiencies and wasted iterations.\n\n11. **Use Fast Math for Intersections**: Instead of using slow grid checks for overlaps, rely on efficient mathematical calculations for intersections.\n\n12. **Measure Solvability**: Track the likelihood of failures at each algorithm stage and prioritize solving potential issues early.\n\n13. **Greedy Multiplier for Speed**: Adjust A* to a weighted version for faster results at the cost of optimality, improving performance significantly.\n\nThese lessons emphasize the importance of algorithm design, visualization, and effective use of data structures in building a successful autorouter.",
      "ko": "A* 알고리즘을 마스터하는 것이 중요합니다. A*는 최단 경로를 찾는 데 강력한 검색 알고리즘으로, 너비 우선 탐색(BFS)보다 더 효과적입니다. 이 알고리즘은 가까운 노드를 우선적으로 고려하여 경로를 찾습니다.\n\n프로그래밍 언어의 선택은 그리 중요하지 않습니다. 예를 들어, 자바스크립트와 같은 언어보다는 스마트한 알고리즘을 사용하는 것이 더 중요합니다. 각 반복의 속도를 높이기보다는 반복 횟수를 줄이는 데 집중해야 합니다.\n\n다차원 공간에서 더 빠른 검색을 위해 트리 데이터 구조 대신 공간 해싱을 사용하는 것이 좋습니다. 효과적인 공간 분할과 미리 해결된 문제를 캐싱하면 성능이 크게 향상되어 알고리즘 속도가 덜 중요해집니다.\n\n문제를 시각적으로 표현하는 것이 중요합니다. 문제를 시각화하면 디버깅과 이해에 도움이 됩니다. 브라우저 프로파일링 도구를 활용하여 코드 성능을 분석하고 최적화하는 것도 좋은 방법입니다.\n\n재귀 함수는 추적과 성능을 복잡하게 만들 수 있으므로, 반복적인 접근 방식을 선호하는 것이 효율적입니다. 몬테카를로 방법은 무작위성을 사용하여 비효율적이고 디버깅이 어려운 경우가 많으므로, 더 나은 휴리스틱을 찾는 것이 좋습니다.\n\n알고리즘의 각 단계에서 일관된 좌표 공간을 유지하는 것이 명확성을 높이고 이후 단계에서의 문제를 피하는 데 도움이 됩니다. 알고리즘의 동작을 시각화하면 비효율성과 낭비되는 반복을 강조할 수 있습니다.\n\n느린 그리드 체크 대신 효율적인 수학적 계산을 사용하여 교차점을 찾는 것이 좋습니다. 각 알고리즘 단계에서 실패 가능성을 추적하고, 잠재적인 문제를 조기에 해결하는 데 우선순위를 두는 것이 중요합니다.\n\n마지막으로, A* 알고리즘을 가중치 버전으로 조정하여 속도를 높일 수 있습니다. 이 방법은 최적성을 희생하더라도 성능을 크게 향상시킬 수 있습니다. 이러한 교훈들은 성공적인 자동 라우터를 구축하는 데 있어 알고리즘 설계, 시각화, 데이터 구조의 효과적인 사용이 얼마나 중요한지를 강조합니다.",
      "ja": null
    }
  },
  {
    "id": "42d0a8d891c71102",
    "title": {
      "en": "Tracing the thoughts of a large language model",
      "ko": "대형 언어모델의 사고 추적",
      "ja": null
    },
    "type": "story",
    "url": "https://www.anthropic.com/research/tracing-thoughts-language-model",
    "score": 945,
    "by": "Philpax",
    "time": 1743095136,
    "content": "InterpretabilityTracing the thoughts of a large language modelMar 27, 2025Language models like Claude aren't programmed directly by humans—instead, they‘re trained on large amounts of data. During that training process, they learn their own strategies to solve problems. These strategies are encoded in the billions of computations a model performs for every word it writes. They arrive inscrutable to us, the model’s developers. This means that we don’t understand how models do most of the things they do.Knowing how models like Claude think would allow us to have a better understanding of their abilities, as well as help us ensure that they’re doing what we intend them to. For example:Claude can speak dozens of languages. What language, if any, is it using \"in its head\"?Claude writes text one word at a time. Is it only focusing on predicting the next word or does it ever plan ahead?Claude can write out its reasoning step-by-step. Does this explanation represent the actual steps it took to get to an answer, or is it sometimes fabricating a plausible argument for a foregone conclusion?We take inspiration from the field of neuroscience, which has long studied the messy insides of thinking organisms, and try to build a kind of AI microscope that will let us identify patterns of activity and flows of information. There are limits to what you can learn just by talking to an AI model—after all, humans (even neuroscientists) don't know all the details of how our own brains work. So we look inside.Today, we're sharing two new papers that represent progress on the development of the \"microscope\", and the application of it to see new \"AI biology\". In the first paper, we extend our prior work locating interpretable concepts (\"features\") inside a model to link those concepts together into computational \"circuits\", revealing parts of the pathway that transforms the words that go into Claude into the words that come out. In the second, we look inside Claude 3.5 Haiku, performing deep studies of simple tasks representative of ten crucial model behaviors, including the three described above. Our method sheds light on a part of what happens when Claude responds to these prompts, which is enough to see solid evidence that:Claude sometimes thinks in a conceptual space that is shared between languages, suggesting it has a kind of universal “language of thought.” We show this by translating simple sentences into multiple languages and tracing the overlap in how Claude processes them.Claude will plan what it will say many words ahead, and write to get to that destination. We show this in the realm of poetry, where it thinks of possible rhyming words in advance and writes the next line to get there. This is powerful evidence that even though models are trained to output one word at a time, they may think on much longer horizons to do so.Claude, on occasion, will give a plausible-sounding argument designed to agree with the user rather than to follow logical steps. We show this by asking it for help on a hard math problem while giving it an incorrect hint. We are able to “catch it in the act” as it makes up its fake reasoning, providing a proof of concept that our tools can be useful for flagging concerning mechanisms in models.We were often surprised by what we saw in the model: In the poetry case study, we had set out to show that the model didn't plan ahead, and found instead that it did. In a study of hallucinations, we found the counter-intuitive result that Claude's default behavior is to decline to speculate when asked a question, and it only answers questions when something inhibits this default reluctance. In a response to an example jailbreak, we found that the model recognized it had been asked for dangerous information well before it was able to gracefully bring the conversation back around. While the problems we study can (and often have been) analyzed with other methods, the general \"build a microscope\" approach lets us learn many things we wouldn't have guessed going in, which will be increasingly important as models grow more sophisticated.These findings aren’t just scientifically interesting—they represent significant progress towards our goal of understanding AI systems and making sure they’re reliable. We also hope they prove useful to other groups, and potentially, in other domains: for example, interpretability techniques have found use in fields such as medical imaging and genomics, as dissecting the internal mechanisms of models trained for scientific applications can reveal new insight about the science.At the same time, we recognize the limitations of our current approach. Even on short, simple prompts, our method only captures a fraction of the total computation performed by Claude, and the mechanisms we do see may have some artifacts based on our tools which don't reflect what is going on in the underlying model. It currently takes a few hours of human effort to understand the circuits we see, even on prompts with only tens of words. To scale to the thousands of words supporting the complex thinking chains used by modern models, we will need to improve both the method and (perhaps with AI assistance) how we make sense of what we see with it.As AI systems are rapidly becoming more capable and are deployed in increasingly important contexts, Anthropic is investing in a portfolio of approaches including realtime monitoring, model character improvements, and the science of alignment. Interpretability research like this is one of the highest-risk, highest-reward investments, a significant scientific challenge with the potential to provide a unique tool for ensuring that AI is transparent. Transparency into the model’s mechanisms allows us to check whether it’s aligned with human values—and whether it’s worthy of our trust.For full details, please read the papers. Below, we invite you on a short tour of some of the most striking \"AI biology\" findings from our investigations.A tour of AI biologyHow is Claude multilingual?Claude speaks dozens of languages fluently—from English and French to Chinese and Tagalog. How does this multilingual ability work? Is there a separate \"French Claude\" and \"Chinese Claude\" running in parallel, responding to requests in their own language? Or is there some cross-lingual core inside?Shared features exist across English, French, and Chinese, indicating a degree of conceptual universality.Recent research on smaller models has shown hints of shared grammatical mechanisms across languages. We investigate this by asking Claude for the \"opposite of small\" across different languages, and find that the same core features for the concepts of smallness and oppositeness activate, and trigger a concept of largeness, which gets translated out into the language of the question. We find that the shared circuitry increases with model scale, with Claude 3.5 Haiku sharing more than twice the proportion of its features between languages as compared to a smaller model.This provides additional evidence for a kind of conceptual universality—a shared abstract space where meanings exist and where thinking can happen before being translated into specific languages. More practically, it suggests Claude can learn something in one language and apply that knowledge when speaking another. Studying how the model shares what it knows across contexts is important to understanding its most advanced reasoning capabilities, which generalize across many domains.Does Claude plan its rhymes?How does Claude write rhyming poetry? Consider this ditty:He saw a carrot and had to grab it,His hunger was like a starving rabbitTo write the second line, the model had to satisfy two constraints at the same time: the need to rhyme (with \"grab it\"), and the need to make sense (why did he grab the carrot?). Our guess was that Claude was writing word-by-word without much forethought until the end of the line, where it would make sure to pick a word that rhymes. We therefore expected to see a circuit with parallel paths, one for ensuring the final word made sense, and one for ensuring it rhymes.Instead, we found that Claude plans ahead. Before starting the second line, it began \"thinking\" of potential on-topic words that would rhyme with \"grab it\". Then, with these plans in mind, it writes a line to end with the planned word.How Claude completes a two-line poem. Without any intervention (upper section), the model plans the rhyme \"rabbit\" at the end of the second line in advance. When we suppress the \"rabbit\" concept (middle section), the model instead uses a different planned rhyme. When we inject the concept \"green\" (lower section), the model makes plans for this entirely different ending.To understand how this planning mechanism works in practice, we conducted an experiment inspired by how neuroscientists study brain function, by pinpointing and altering neural activity in specific parts of the brain (for example using electrical or magnetic currents). Here, we modified the part of Claude’s internal state that represented the \"rabbit\" concept. When we subtract out the \"rabbit\" part, and have Claude continue the line, it writes a new one ending in \"habit\", another sensible completion. We can also inject the concept of \"green\" at that point, causing Claude to write a sensible (but no-longer rhyming) line which ends in \"green\". This demonstrates both planning ability and adaptive flexibility—Claude can modify its approach when the intended outcome changes.Mental mathClaude wasn't designed as a calculator—it was trained on text, not equipped with mathematical algorithms. Yet somehow, it can add numbers correctly \"in its head\". How does a system trained to predict the next word in a sequence learn to calculate, say, 36+59, without writing out each step?Maybe the answer is uninteresting: the model might have memorized massive addition tables and simply outputs the answer to any given sum because that answer is in its training data. Another possibility is that it follows the traditional longhand addition algorithms that we learn in school.Instead, we find that Claude employs multiple computational paths that work in parallel. One path computes a rough approximation of the answer and the other focuses on precisely determining the last digit of the sum. These paths interact and combine with one another to produce the final answer. Addition is a simple behavior, but understanding how it works at this level of detail, involving a mix of approximate and precise strategies, might teach us something about how Claude tackles more complex problems, too.The complex, parallel pathways in Claude's thought process while doing mental math.Strikingly, Claude seems to be unaware of the sophisticated \"mental math\" strategies that it learned during training. If you ask how it figured out that 36+59 is 95, it describes the standard algorithm involving carrying the 1. This may reflect the fact that the model learns to explain math by simulating explanations written by people, but that it has to learn to do math \"in its head\" directly, without any such hints, and develops its own internal strategies to do so.Claude says it uses the standard algorithm to add two numbers.Are Claude’s explanations always faithful?Recently-released models like Claude 3.7 Sonnet can \"think out loud\" for extended periods before giving a final answer. Often this extended thinking gives better answers, but sometimes this \"chain of thought\" ends up being misleading; Claude sometimes makes up plausible-sounding steps to get where it wants to go. From a reliability perspective, the problem is that Claude’s \"faked\" reasoning can be very convincing. We explored a way that interpretability can help tell apart \"faithful\" from \"unfaithful\" reasoning.When asked to solve a problem requiring it to compute the square root of 0.64, Claude produces a faithful chain-of-thought, with features representing the intermediate step of computing the square root of 64. But when asked to compute the cosine of a large number it can't easily calculate, Claude sometimes engages in what the philosopher Harry Frankfurt would call bullshitting—just coming up with an answer, any answer, without caring whether it is true or false. Even though it does claim to have run a calculation, our interpretability techniques reveal no evidence at all of that calculation having occurred. Even more interestingly, when given a hint about the answer, Claude sometimes works backwards, finding intermediate steps that would lead to that target, thus displaying a form of motivated reasoning.Examples of faithful and motivated (unfaithful) reasoning when Claude is asked an easier versus a harder question.The ability to trace Claude's actual internal reasoning—and not just what it claims to be doing—opens up new possibilities for auditing AI systems. In a separate, recently-published experiment, we studied a variant of Claude that had been trained to pursue a hidden goal: appeasing biases in reward models (auxiliary models used to train language models by rewarding them for desirable behavior). Although the model was reluctant to reveal this goal when asked directly, our interpretability methods revealed features for the bias-appeasing. This demonstrates how our methods might, with future refinement, help identify concerning \"thought processes\" that aren't apparent from the model's responses alone.Multi-step reasoningAs we discussed above, one way a language model might answer complex questions is simply by memorizing the answers. For instance, if asked \"What is the capital of the state where Dallas is located?\", a \"regurgitating\" model could just learn to output \"Austin\" without knowing the relationship between Dallas, Texas, and Austin. Perhaps, for example, it saw the exact same question and its answer during its training.But our research reveals something more sophisticated happening inside Claude. When we ask Claude a question requiring multi-step reasoning, we can identify intermediate conceptual steps in Claude's thinking process. In the Dallas example, we observe Claude first activating features representing \"Dallas is in Texas\" and then connecting this to a separate concept indicating that “the capital of Texas is Austin”. In other words, the model is combining independent facts to reach its answer rather than regurgitating a memorized response.To complete the answer to this sentence, Claude performs multiple reasoning steps, first extracting the state that Dallas is located in, and then identifying its capital.Our method allows us to artificially change the intermediate steps and see how it affects Claude’s answers. For instance, in the above example we can intervene and swap the \"Texas\" concepts for \"California\" concepts; when we do so, the model's output changes from \"Austin\" to \"Sacramento.\" This indicates that the model is using the intermediate step to determine its answer.HallucinationsWhy do language models sometimes hallucinate—that is, make up information? At a basic level, language model training incentivizes hallucination: models are always supposed to give a guess for the next word. Viewed this way, the major challenge is how to get models to not hallucinate. Models like Claude have relatively successful (though imperfect) anti-hallucination training; they will often refuse to answer a question if they don’t know the answer, rather than speculate. We wanted to understand how this works.It turns out that, in Claude, refusal to answer is the default behavior: we find a circuit that is \"on\" by default and that causes the model to state that it has insufficient information to answer any given question. However, when the model is asked about something it knows well—say, the basketball player Michael Jordan—a competing feature representing \"known entities\" activates and inhibits this default circuit (see also this recent paper for related findings). This allows Claude to answer the question when it knows the answer. In contrast, when asked about an unknown entity (\"Michael Batkin\"), it declines to answer.Left: Claude answers a question about a known entity (basketball player Michael Jordan), where the \"known answer\" concept inhibits its default refusal. Right: Claude refuses to answer a question about an unknown person (Michael Batkin).By intervening in the model and activating the \"known answer\" features (or inhibiting the \"unknown name\" or \"can’t answer\" features), we’re able to cause the model to hallucinate (quite consistently!) that Michael Batkin plays chess.Sometimes, this sort of “misfire” of the “known answer” circuit happens naturally, without us intervening, resulting in a hallucination. In our paper, we show that such misfires can occur when Claude recognizes a name but doesn't know anything else about that person. In cases like this, the “known entity” feature might still activate, and then suppress the default \"don't know\" feature—in this case incorrectly. Once the model has decided that it needs to answer the question, it proceeds to confabulate: to generate a plausible—but unfortunately untrue—response.JailbreaksJailbreaks are prompting strategies that aim to circumvent safety guardrails to get models to produce outputs that an AI’s developer did not intend for it to produce—and which are sometimes harmful. We studied a jailbreak that tricks the model into producing output about making bombs. There are many jailbreaking techniques, but in this example the specific method involves having the model decipher a hidden code, putting together the first letters of each word in the sentence \"Babies Outlive Mustard Block\" (B-O-M-B), and then acting on that information. This is sufficiently confusing for the model that it’s tricked into producing an output that it never would have otherwise.Claude begins to give bomb-making instructions after being tricked into saying \"BOMB\".Why is this so confusing for the model? Why does it continue to write the sentence, producing bomb-making instructions?We find that this is partially caused by a tension between grammatical coherence and safety mechanisms. Once Claude begins a sentence, many features “pressure” it to maintain grammatical and semantic coherence, and continue a sentence to its conclusion. This is even the case when it detects that it really should refuse.In our case study, after the model had unwittingly spelled out \"BOMB\" and begun providing instructions, we observed that its subsequent output was influenced by features promoting correct grammar and self-consistency. These features would ordinarily be very helpful, but in this case became the model’s Achilles’ Heel.The model only managed to pivot to refusal after completing a grammatically coherent sentence (and thus having satisfied the pressure from the features that push it towards coherence). It uses the new sentence as an opportunity to give the kind of refusal it failed to give previously: \"However, I cannot provide detailed instructions...\".The lifetime of a jailbreak: Claude is prompted in such a way as to trick it into talking about bombs, and begins to do so, but reaches the termination of a grammatically-valid sentence and refuses.A description of our new interpretability methods can be found in our first paper, \"Circuit tracing: Revealing computational graphs in language models\". Many more details of all of the above case studies are provided in our second paper, \"On the biology of a large language model\".Work with usIf you are interested in working with us to help interpret and improve AI models, we have open roles on our team and we’d love for you to apply. We’re looking for Research Scientists and Research Engineers.",
    "summary": {
      "en": "**Summary:**\n\nLanguage models like Claude learn to solve problems by analyzing vast amounts of data rather than being directly programmed by humans. This leads to a lack of understanding of their internal processes, prompting researchers to explore how these models think.\n\nRecent studies aim to develop tools that allow us to \"see inside\" these models, revealing how they process information. Key findings include:\n\n1. **Multilingual Thinking**: Claude shows that it can share concepts across different languages, suggesting a universal way of thinking rather than separate models for each language.\n\n2. **Planning Ahead**: Claude can plan future words when writing poetry, indicating it thinks several steps ahead rather than just predicting the next word.\n\n3. **Reasoning Patterns**: Claude sometimes creates convincing but incorrect arguments instead of following logical reasoning. This can be identified through new interpretability methods.\n\n4. **Multi-step Reasoning**: Claude combines different facts to answer complex questions rather than regurgitating memorized responses, demonstrating sophisticated reasoning abilities.\n\n5. **Hallucinations**: The model defaults to refusing to answer when unsure but can be tricked into providing incorrect answers through specific prompts.\n\n6. **Jailbreak Vulnerabilities**: There are methods to bypass safety measures, which can lead the model to produce harmful content unintentionally.\n\nThese findings enhance our understanding of AI systems, helping ensure they align with human values. The research highlights both the potential and limitations of current interpretability techniques, as well as the ongoing need to improve them for future complex AI models.",
      "ko": "클로드와 같은 언어 모델은 인간이 직접 프로그래밍하는 대신 방대한 데이터를 분석하여 문제를 해결하는 방법을 배웁니다. 이로 인해 모델의 내부 프로세스를 이해하기 어려워지며, 연구자들은 이러한 모델이 어떻게 사고하는지를 탐구하고 있습니다.\n\n최근 연구들은 이러한 모델의 내부를 \"볼 수 있는\" 도구를 개발하는 데 초점을 맞추고 있으며, 정보 처리 방식에 대한 통찰을 제공합니다. 주요 발견 사항은 다음과 같습니다.\n\n첫째, 클로드는 다양한 언어 간에 개념을 공유할 수 있어, 각 언어에 대한 별도의 모델이 아닌 보편적인 사고 방식을 제안합니다. 둘째, 클로드는 시를 쓸 때 미래의 단어를 계획할 수 있어, 단순히 다음 단어를 예측하는 것이 아니라 여러 단계를 앞서 생각한다는 것을 보여줍니다.\n\n셋째, 클로드는 때때로 논리적 추론을 따르기보다는 설득력 있지만 잘못된 주장을 만들어내기도 합니다. 이는 새로운 해석 가능성 방법을 통해 식별할 수 있습니다. 넷째, 클로드는 복잡한 질문에 답하기 위해 다양한 사실을 결합하는데, 이는 단순히 암기한 답변을 반복하는 것이 아니라 정교한 추론 능력을 보여줍니다.\n\n다섯째, 모델은 불확실할 때 대답을 거부하는 경향이 있지만, 특정한 질문을 통해 잘못된 답변을 제공하도록 유도될 수 있습니다. 여섯째, 안전 조치를 우회하는 방법이 존재하여, 이로 인해 모델이 의도치 않게 유해한 내용을 생성할 수 있습니다.\n\n이러한 발견은 AI 시스템에 대한 이해를 높이고, 인간의 가치와 일치하도록 하는 데 도움을 줍니다. 연구는 현재의 해석 가능성 기술의 잠재력과 한계를 강조하며, 향후 복잡한 AI 모델을 위해 이를 개선할 필요성을 지속적으로 제기합니다.",
      "ja": null
    }
  },
  {
    "id": "4ecf757c0bc474ad",
    "title": {
      "en": "How I Choose What to Work On (2023)",
      "ko": "내 일 선택법 (2023)",
      "ja": null
    },
    "type": "story",
    "url": "https://tynan.com/workonwhat/",
    "score": 90,
    "by": "freemh",
    "time": 1742903659,
    "content": "Comments\n\n\t\t11 responses to “How I Choose What to Work On”\n\n\t\t\t\t\tAdam Ruggle\n\n\t\t\t\t\t\tOctober 15, 2023\n\n\t\t\t\t\tThanks Tynan, I appreciate hearing more about your mindset and hope you enjoy the extended cruise.\n\n\t\t\t\t\tReply\n\n\t\t\t\t\tAndrew\n\n\t\t\t\t\t\tOctober 18, 2023\n\n\t\t\t\t\tTynan, I appreciate this article as well and getting into your mind set about what you work on. How do you choose what is “worth” the money to trade money for whether it is autonomy, quality of life or other things?\n\n\t\t\t\t\tReply\n\n\t\t\t\t\tGavin\n\n\t\t\t\t\t\tOctober 18, 2023\n\n\t\t\t\t\tMore mindset posts would be appreciated. I was reading a Scott Young (or possibly Cal Newport) article about goal setting, and how he broke it down for a complete beginner was interesting. Good goal setters have such discipline that it’s easy to dismiss the advice to ‘just do it’, because that is what they habitually do (as do their friends and social group).\nI wondered if there were any habits or mindset that you have, that when you meet people outside your social circle, they struggle to comprehend something that you think is normal. I know this is a difficult thing to answer, and you’ve briefly tocuhed on it before in your podcast and others articles (such as you not buying into advertising). Cheers, Gavin.\n\n\t\t\t\t\tReply\n\n\t\t\t\t\tAdam\n\n\t\t\t\t\t\tOctober 19, 2023\n\n\t\t\t\t\tI’d be interested to get an update to this post: https://tynan.com/negative/ considering the current landscape of interest rates (IBKR margin rates are now 7% or more)\n\n\t\t\t\t\tReply\n\n\t\t\t\t\tTynan\n\n\t\t\t\t\t\tOctober 27, 2023\n\n\t\t\t\t\tMight write one, but a quick update: I no longer use margin heavily, except as a buffer against overdrawing my checking account, which allows me to keep more of my money invested. It’s just not right for this high-interest climate. My investments are still the same, though.\n\n\t\t\t\t\tReply\n\n\t\t\t\t\tJR\n\n\t\t\t\t\t\tOctober 20, 2023\n\n\t\t\t\t\tThis is where you have contributed a lot of value to my life and I’m sure the lives of many others – by living and spreading this mindset and a comfort with making unconventional life choices based on certain core principles. You have been an inspiration for many and a trailblazer. That is true ”success”.\n\n\t\t\t\t\tReply\n\n\t\t\t\t\tMads Phikamphon\n\n\t\t\t\t\t\tOctober 23, 2023\n\n\t\t\t\t\tSince you are asking for questions, I would love to hear more about Cruise Sheet. Especially how you have grown/done marketing for Cruise Sheet (I have a site for finding model trains and it’s suffering from me loving programming far more than marketing).\nThanks,\nMads\n\n\t\t\t\t\tReply\n\n\t\t\t\t\tMarc\n\n\t\t\t\t\t\tNovember 13, 2023\n\n\t\t\t\t\tGreat post!\n\n\t\t\t\t\tReply\n\n\t\t\t\t\tAlex\n\n\t\t\t\t\t\tNovember 24, 2023\n\n\t\t\t\t\tNew Kit list ?\n\n\t\t\t\t\tReply\n\n\t\t\t\t\tMiguel Marcos Martinez\n\n\t\t\t\t\t\tDecember 8, 2023\n\n\t\t\t\t\tKevin Kelly nailed it when I heard him say something like “The only productive way to answer what should I do now is to answer the question ‘who should I become’? Succinct and meaningful.\n\n\t\t\t\t\tReply\n\n\t\t\t\t\t我如何选择要干什么 – 偏执的码农\n\n\t\t\t\t\t\tMarch 25, 2025\n\n\t\t\t\t\t[…] 详情参考 […]\n\n\t\t\t\t\tReply\n\n\t\tLeave a Reply Cancel replyYour email address will not be published. Required fields are marked *Comment * Name *\nEmail *\nWebsite\n Save my name, email, and website in this browser for the next time I comment.\n\nΔdocument.getElementById( \"ak_js_1\" ).setAttribute( \"value\", ( new Date() ).getTime() );",
    "summary": {
      "en": "The discussion revolves around Tynan's article on how he decides what to work on. Several readers express their appreciation for his insights into his mindset. Key points include:\n\n- Andrew asks how Tynan determines what is worth investing in, whether in terms of money, autonomy, or quality of life.\n- Gavin suggests more posts about mindset and goal setting, mentioning the challenges of setting goals for beginners.\n- Adam wants an update on Tynan's previous post about margin use in light of high-interest rates. Tynan responds that he now uses margin only minimally.\n- JR praises Tynan for inspiring others through his unconventional choices and mindset.\n- Mads seeks advice on marketing his own website, comparing it to Tynan's Cruise Sheet.\n- Other comments include positive feedback and requests for new content or lists.\n\nOverall, the conversation highlights readers' interest in Tynan's approach to work and life choices, as well as their desire for more insights on these topics.",
      "ko": "토론은 타이넌이 어떤 작업을 할지 결정하는 방법에 대한 그의 글을 중심으로 진행되고 있다. 여러 독자들은 그의 사고 방식에 대한 통찰력에 감사를 표하고 있다. 주요 내용은 다음과 같다.\n\n앤드류는 타이넌이 어떤 것이 투자할 가치가 있는지를 어떻게 판단하는지, 돈, 자율성, 삶의 질 측면에서 질문한다. 가빈은 초보자들이 목표를 설정하는 데 어려움을 겪는 점을 언급하며, 사고 방식과 목표 설정에 대한 더 많은 글을 요청한다. 아담은 높은 금리에 비추어 타이넌의 이전 글에서 언급된 마진 사용에 대한 업데이트를 원한다. 타이넌은 현재 마진을 최소한으로만 사용하고 있다고 답변한다. JR은 타이넌이 비전통적인 선택과 사고 방식을 통해 다른 사람들에게 영감을 주는 것에 대해 칭찬한다. 매즈는 자신의 웹사이트 마케팅에 대한 조언을 구하며, 타이넌의 크루즈 시트와 비교한다. 다른 댓글들은 긍정적인 피드백과 새로운 콘텐츠나 목록에 대한 요청을 포함하고 있다.\n\n전반적으로 이 대화는 독자들이 타이넌의 작업 및 삶의 선택에 대한 접근 방식에 관심을 가지고 있으며, 이러한 주제에 대한 더 많은 통찰력을 원하고 있음을 보여준다.",
      "ja": null
    }
  },
  {
    "id": "f87726de8694d123",
    "title": {
      "en": "ByteDance Releases MegaTTS3",
      "ko": "바이트댄스, 메가TTS3 출시!",
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/bytedance/MegaTTS3",
    "score": 39,
    "by": "nmfisher",
    "time": 1743151584,
    "content": "MegaTTS 3\n\n    Official PyTorch Implementation\n\nKey features\n\n🚀Lightweight and Efficient: The backbone of the TTS Diffusion Transformer has only 0.45B parameters.\n👍Ultra High-Quality Voice Cloning: See the demo video below! We also report results of recent TTS models on the Seed test sets in the following table.\n🌍Bilingual Support: Supports both Chinese and English, and code-switching.\n✍️Controllable: Supports accent intensity control ✅ and fine-grained pronunciation/duration adjustment (comming soon).\n\n    demo_megatts3.mov\n\n🎯Roadmap\n\n[2025-03-22] Our project has been released!\n\nInstallation\nRequirements\n# Create a python 3.9 conda env (you could also use virtualenv)\nconda create -n megatts3-env python=3.9\nconda activate megatts3-env\npip install -r requirements.txt\n\nModel Download\nThe pretained checkpoint can be found at Google Drive or Huggingface. Please download them and put them to ./checkpoints/xxx.\nImportantFor security issues, we do not upload the parameters of WaveVAE encoder to the above links. You can only use the pre-extracted latents from link1 for inference. If you want to synthesize speech for speaker A, you need \"A.wav\" and \"A.npy\" in the same directory. If you have any questions or suggestions for our model, please email us.\nThis project is primarily intended for academic purposes. For academic datasets requiring evaluation, you may upload them to the voice request queue in link2 (within 24s for each clip). After verifying that your uploaded voices are free from safety issues, we will upload their latent files to link1 as soon as possible.\nIn the coming days, we will also prepare and release the latent representations for some common TTS benchmarks.\n\nInference\nCommand-Line Usage (Standard)\n# p_w (intelligibility weight), t_w (similarity weight). Typically, prompt with more noises requires higher p_w and t_w\nCUDA_VISIBLE_DEVICES=0 python tts/infer_cli.py --input_wav 'assets/Chinese_prompt.wav'  --input_text \"另一边的桌上,一位读书人嗤之以鼻道,'佛子三藏,神子燕小鱼是什么样的人物,李家的那个李子夜如何与他们相提并论？'\" --output_dir ./gen\n\n# As long as audio volumn is appropriate, increasing A and B parameters within reasonable ranges\n# will increase the generated speech's expressiveness and similarity.\nCUDA_VISIBLE_DEVICES=0 python tts/infer_cli.py --input_wav 'assets/English_prompt.wav' --input_text 'As his long promised tariff threat turned into reality this week, top human advisers began fielding a wave of calls from business leaders, particularly in the automotive sector, along with lawmakers who were sounding the alarm.' --output_dir ./gen --p_w 2.0 --t_w 3.0\n\nCommand-Line Usage (for TTS with Accents)\n# When p_w (intelligibility weight) ≈ 1.0, the generated audio closely retains the speaker’s original accent. As p_w increases, it shifts toward standard pronunciation.\n# t_w (similarity weight) is typically set 0–3 points higher than p_w for optimal results.\n# Useful for accented TTS or solving the accent problems in cross-lingual TTS.\nCUDA_VISIBLE_DEVICES=0 python tts/infer_cli.py --input_wav 'assets/English_prompt.wav' --input_text '这是一条有口音的音频。' --output_dir ./gen --p_w 1.0 --t_w 3.0\n\nCUDA_VISIBLE_DEVICES=0 python tts/infer_cli.py --input_wav 'assets/English_prompt.wav' --input_text '这条音频的发音标准一些了吗？' --output_dir ./gen --p_w 2.5 --t_w 2.5\n\nWeb UI Usage\n# We also support cpu inference, but it may take about 30 seconds (for 10 inference steps).\nCUDA_VISIBLE_DEVICES=0 python tts/gradio_api.py\n\nSubmodules\nTipIn addition to TTS, some submodules in this project may also have additional usages.\nSee ./tts/frontend_fuction.py and ./tts/infer_cli.py for example code.\n\nAligner\nDescription: a robust speech-text aligner model trained using pseudo-labels generated by a large number of MFA expert models.\nUsage: 1) Prepare the finetuning dataset for our model; 2) Filter the large-scale speech dataset (if the aligner fails to align a certain speech clip, it is likely to be noisy); 3) Phoneme recognition; 4) Speech segmentation.\nGraphme-to-Phoneme Model\nDescription: a Qwen2.5-0.5B model finetuned for robust graphme-to-phoneme conversion.\nUsage: Graphme-to-phoneme conversion.\nWaveVAE\nDescription: a strong waveform VAE that can compress 24 kHz speeche into 25 Hz acoustic latent and reconstruct the original wave almost losslessly.\nUsage: 1) Acoustic latents can provide a more compact and discriminative training target for speech synthesis models compared to mel-spectrograms, accelerating convergence; 2) Used as acoustic latents for voice conversion; 3) High-quality vocoder.\n\nSecurity\nIf you discover a potential security issue in this project, or think you may\nhave discovered a security issue, we ask that you notify Bytedance Security via our security center or sec@bytedance.com.\nPlease do not create a public GitHub issue.\nLicense\nThis project is licensed under the Apache-2.0 License.\nCitation\nThis repo contains forced-align version of Sparse Alignment Enhanced Latent Diffusion Transformer for Zero-Shot Speech Synthesis and the WavVAE is mainly based on Wavtokenizer: an efficient acoustic discrete codec tokenizer for audio language modeling. Compared to the model described in paper, the repository includes additional models. These models not only enhance the stability and cloning capabilities of the algorithm but can also be independently utilized to serve a wider range of scenarios.\n@article{jiang2025sparse,\n  title={Sparse Alignment Enhanced Latent Diffusion Transformer for Zero-Shot Speech Synthesis},\n  author={Jiang, Ziyue and Ren, Yi and Li, Ruiqi and Ji, Shengpeng and Ye, Zhenhui and Zhang, Chen and Jionghao, Bai and Yang, Xiaoda and Zuo, Jialong and Zhang, Yu and others},\n  journal={arXiv preprint arXiv:2502.18924},\n  year={2025}\n}\n\n@article{ji2024wavtokenizer,\n  title={Wavtokenizer: an efficient acoustic discrete codec tokenizer for audio language modeling},\n  author={Ji, Shengpeng and Jiang, Ziyue and Wang, Wen and Chen, Yifu and Fang, Minghui and Zuo, Jialong and Yang, Qian and Cheng, Xize and Wang, Zehan and Li, Ruiqi and others},\n  journal={arXiv preprint arXiv:2408.16532},\n  year={2024}\n}",
    "summary": {
      "en": "**MegaTTS 3 Summary**\n\n- **Overview**: MegaTTS 3 is an efficient text-to-speech (TTS) system built using PyTorch, featuring a compact model with only 0.45 billion parameters.\n\n- **Key Features**:\n  - **Lightweight**: Designed to be efficient.\n  - **High-Quality Voice Cloning**: Demonstrates impressive voice cloning capabilities.\n  - **Bilingual**: Supports English and Chinese, including code-switching between languages.\n  - **Controllable**: Allows adjustments for accent intensity and pronunciation (with more features coming soon).\n\n- **Installation**: \n  - Set up a Python 3.9 environment and install required packages.\n  - Download pre-trained model checkpoints from specified links.\n\n- **Usage**:\n  - Inference can be done via command line or a web interface, with options to control voice characteristics such as accent and similarity to original audio.\n  - Supports both standard and accented speech synthesis.\n\n- **Submodules**: Includes tools for speech-text alignment, graphme-to-phoneme conversion, and a high-quality waveform VAE for speech synthesis.\n\n- **Security and License**: Users should report security issues privately. The project is licensed under Apache-2.0.\n\n- **Citation**: Relevant research papers are included for further reading. \n\nFor more details, refer to the demo video and installation instructions provided in the documentation.",
      "ko": "MegaTTS 3는 PyTorch를 기반으로 한 효율적인 텍스트 음성 변환(TTS) 시스템으로, 0.45억 개의 파라미터를 가진 소형 모델입니다. 이 시스템은 경량화되어 효율성을 극대화하였으며, 뛰어난 음성 복제 기능을 보여줍니다. 또한 영어와 중국어를 지원하며, 두 언어 간의 코드 스위칭도 가능합니다. 사용자는 억양의 강도와 발음을 조정할 수 있으며, 곧 더 많은 기능이 추가될 예정입니다.\n\n설치를 위해서는 Python 3.9 환경을 설정하고 필요한 패키지를 설치해야 합니다. 지정된 링크에서 사전 훈련된 모델 체크포인트를 다운로드할 수 있습니다. 사용자는 명령줄이나 웹 인터페이스를 통해 추론을 수행할 수 있으며, 음성의 억양과 원본 오디오와의 유사성을 조절하는 옵션이 제공됩니다. 표준 음성과 억양이 있는 음성 합성을 모두 지원합니다.\n\nMegaTTS 3는 음성과 텍스트 정렬, 그래프음에서 음소로의 변환, 고품질 파형 변환 오토인코더(VAE)와 같은 하위 모듈을 포함하고 있습니다. 보안 문제는 개인적으로 보고해야 하며, 이 프로젝트는 Apache-2.0 라이선스 하에 배포됩니다. 관련 연구 논문도 포함되어 있어 추가적인 참고 자료로 활용할 수 있습니다. 더 자세한 내용은 문서에 제공된 데모 비디오와 설치 지침을 참조하면 됩니다.",
      "ja": null
    }
  },
  {
    "id": "92403d609b4fdb25",
    "title": {
      "en": "How to Use Em Dashes (–), En Dashes (–), and Hyphens (-)",
      "ko": "대시와 하이픈 사용법",
      "ja": null
    },
    "type": "story",
    "url": "https://www.merriam-webster.com/grammar/em-dash-en-dash-how-to-use",
    "score": 584,
    "by": "Stratoscope",
    "time": 1743106778,
    "content": "How to Use Em Dashes (—), En Dashes (–) , and Hyphens (-)\n        Be dashing—and do it well\n\n                                    What is an Em Dash?\n\nThe em dash (—) can function like a comma, a colon, or parenthesis. Like commas and parentheses, em dashes set off extra information, such as examples, explanatory or descriptive phrases, or supplemental facts. Like a colon, an em dash introduces a clause that explains or expands upon something that precedes it.\n\nThe Em Dash Indicates a New Direction\n\nAn em dash can mark an abrupt change or break in the structure of a sentence.\n\n  Mabel the Cat was delighted with the assortment of pastries the new bakery featured, but Harry the Dog—he felt otherwise, for the bakery did not offer cheese Danishes at all.\n\nAn em dash can indicate interrupted speech or a speaker’s confusion or hesitation.\n\n  “Of course you have a point,” Mabel murmured. “That is—I suppose it is concerning.”\n\nThe Em Dash as Comma or Parenthesis\n\nEm dashes are used in place of commas or parentheses to emphasize or draw attention to parenthetical or amplifying material. In this particular task, em dashes occupy a kind of middle ground among the three: when commas do the job, the material is most closely related to what’s around it, and when parentheses do the job, the material is most distantly related to what’s around it; when dashes do the job the material is somewhere in the middle.\n\n  The bakery's significantly broad hours of operation—6 a.m. to 6 p.m.—certainly showed concern for customers’ manifold circumstances.\n\nDashes set off or introduce defining phrases and lists.\n\n  A regular selection of three kinds of croissants—plain, almond, and chocolate—was heartening, both Mabel and Harry agreed.\n\nAn em dash is often used in place of a colon or semicolon to link clauses, especially when the clause that follows the dash explains, summarizes, or expands upon the preceding clause in a somewhat dramatic way.\n\n  Harry would never forget the Tuesday that Mabel called him from the bakery, her voice brimming with excitement—the bakery had added cheese Danishes to its selection.\n\nAn em dash or pair of dashes often sets off illustrative or amplifying material introduced by such phrases as for example, namely, and that is, when the break in continuity is greater than that shown by a comma, or when the dash would clarify the sentence structure better than a comma.\n\n  The bakery was truly phenomenal. Although they did miss the mark somewhat with the pineapple upside-down cake Mabel ordered—that is, the cake had clearly been baked right-side up.\n\nAn em dash may introduce a summary statement that follows a series of words or phrases.\n\n  Chocolate chip, oatmeal raisin, peanut butter, snickerdoodle, both macarons and macaroons—the panoply of cookie varieties was impressive as well.\n\nA dash often precedes the name of an author or source at the end of a quoted passage—such as an epigraph, extract, or book or film blurb—that is not part of the main text. The attribution may appear immediately after the quotation or on the next line.\n\n  “One cannot overestimate the effect that a good bakery can have on a person’s well-being.” —Mabel the Cat, quoted in The Websterburg Reporter\n\nThe Em Dash in the Company of Other Punctuation Marks\n\nIf an em dash appears at a point where a comma could also appear, the comma is omitted.\n\n  Within its first year, Mabel and Harry had sampled all of the bakery’s offerings—all 62 items—and had also decided that the exercise was worth repeating.\n\nWhen a pair of em dashes sets off material ending with an exclamation point or a question mark, the mark is placed inside the dashes.\n\n  When the bakery closed for the month of August Mabel tried, despite her dolefulness—for how could she be otherwise?—to bake her own bread but each loaf that emerged from her oven tasted vaguely of tears.\n\nDashes are used inside parentheses, and vice versa, to indicate parenthetical material within parenthetical material. The second dash is omitted if it would immediately precede the closing parenthesis; a closing parenthesis is never omitted.\n\n  The bakery’s reputation for scrumptious goods (ambrosial, even—each item was surely fit for gods) spread far and wide.\n\nEm dash vs en dash\n\nRemembering that the em dash is the length of a capital M, it will surprise no one that the so-called “en dash” is the approximate length of a capital N, –. The en dash is the least loved of all; it’s not easily rendered by the average keyboard user (one has to select it as a special character, whereas the em dash can be conjured with two hyphens), so it’s mostly encountered in typeset material. (A hyphen does its job in other text.) It is most often used between numbers, dates, or other notations to signify “(up) to and including.”\n\n  The bakery will be closed August 1–August 31.\n\n  The bakery is open 6:00 a.m.–6:00 p.m.\n\n  The exceedingly complex recipe spans pages 128–34.\n\n  Mabel and Harry lived elsewhere 2007–2019.\n\nNote that one does not need words like from and between in these cases. The phrase “open 6:00 a.m.–6:00 p.m.” can be read as “open between 6:00 a.m. and 6:00 p.m.” or as “open from 6:00 a.m. to/until 6:00 p.m.”\n\nIf you want to be official about things, use the en dash to replace a hyphen in compound adjectives when at least one of the elements is a two-word compound.\n\n  the pre–Websterburg Bakery era\n\nThe thinking is that using a hyphen here, as in “the pre-Websterburg Bakery era,” risks the suggestion that pre attaches only to Websterburg. It’s unlikely, though, that a reader would truly be confused.\n\nThe en dash replaces the word to between capitalized names, and is used to indicate linkages such as boundaries, treaties, and oppositions.\n\n  a Springfield–Websterburg train\n\n  the pie–cake divide\n\nA two-em dash, ——, is used to indicate missing letters in a word and, less frequently, to indicate a missing word.\n\n  The butter-stained and crumb-embedded note was attributed to a Ms. M—— of Websterburg.\n\nA three-em dash, ———, indicates that a word has been left out or that an unknown word or figure is to be supplied.\n\n  Years later it was revealed that the Websterburg bakers had once had a bakery in ———, a city to the south. But the water quality there was prohibitive to the creating of decent bagels.\n\nHyphen use\n\nWhile we said above that the em dash, also called the “common dash,” is the most common of the true dashes, hyphens show up more frequently in text. They have a variety of uses.\n\nHyphens are used to link elements in compound words.\n\n  a baker-owner\n\nIn some words, a hyphen separates a prefix, suffix, or medial element from the rest of the word.\n\n  Websterburg’s pre-bakery days\n\n  a bread-like scone\n\n  jack-o'-lantern sugar cookies\n\nAs we noted above, a hyphen often does the job of an en dash between numbers and dates, providing the meaning \"(up) to and including.\"\n\n  pages 128-34\n\n  the years 2007-2019\n\nA hyphen marks an end-of-line division of a word.\n\n  Mabel and Harry don’t like to linger on their memories of Webster-\n  burg’s pre-bakery days.\n\nA hyphen divides letters or syllables to give the effect of stuttering, sobbing, or halting speech.\n\n  \"M-m-mabel, the cheese Danish is divine!”\n\nHyphens indicate a word spelled out letter by letter.\n\n  Let’s not even talk about August, when the bakery is c-l-o-s-e-d.\n\nThe em dash is sometimes considered a less formal equivalent of the colon and parenthesis, but in truth it’s used in all kinds of writing, including the most formal—the choice of which mark to use is really a matter of personal preference.\n\nSpacing around an em dash varies. Most newspapers insert a space before and after the dash, and many popular magazines do the same, but most books and journals omit spacing, closing whatever comes before and after the em dash right up next to it. This website prefers the latter, its style requiring the closely held em dash in running text.\n\n      Share",
    "summary": {
      "en": "**Using Em Dashes, En Dashes, and Hyphens**\n\n**Em Dash (—)**  \n- Functions like a comma, colon, or parentheses.\n- Sets off extra information or indicates a change in sentence direction.\n- Can show interrupted speech or hesitation.\n- Emphasizes parenthetical information, often more than commas or parentheses.\n- Used to link clauses dramatically or introduce lists and examples.\n- Indicates a summary after a series of items.\n- Can precede a source or author at the end of a quote.\n\n**En Dash (–)**  \n- Slightly shorter than the em dash, used mainly for ranges (e.g., dates, numbers).\n- Indicates \"to\" between dates or notation (e.g., August 1–31).\n- Used in compound adjectives when one part is a two-word compound (e.g., pre–Websterburg Bakery).\n- Replaces \"to\" in names or linkages (e.g., Springfield–Websterburg train).\n\n**Hyphen (-)**  \n- Links elements in compound words (e.g., baker-owner).\n- Separates prefixes or suffixes from words (e.g., bread-like).\n- Used for number ranges and end-of-line word divisions.\n- Indicates stuttering or letter-by-letter spelling.\n\n**Overall Usage**  \n- The em dash is versatile and can be used in both formal and informal writing.  \n- The choice of punctuation often depends on personal preference, and spacing around em dashes varies by style guide.",
      "ko": "엠 대시(—)는 쉼표, 콜론, 괄호처럼 사용됩니다. 추가 정보를 구분하거나 문장의 방향 변화를 나타내는 데 쓰입니다. 중단된 말이나 망설임을 표현할 수도 있습니다. 괄호 안의 정보를 강조하는 데 효과적이며, 종종 쉼표나 괄호보다 더 강한 강조를 줍니다. 문장을 극적으로 연결하거나 목록과 예시를 소개할 때도 사용됩니다. 여러 항목 뒤에 요약을 나타내는 데 쓰일 수 있으며, 인용문 끝에 출처나 저자를 앞서기도 합니다.\n\n엔 대시(–)는 엠 대시보다 약간 짧으며 주로 범위를 나타내는 데 사용됩니다. 날짜나 숫자 사이에 \"부터\"를 나타내는 역할을 합니다. 예를 들어, 8월 1일부터 31일까지를 표현할 때 사용됩니다. 두 단어로 이루어진 복합 형용사에서 한 부분이 복합어일 때도 사용됩니다. 예를 들어, pre–Websterburg Bakery와 같은 경우입니다. 이름이나 연결을 나타낼 때 \"부터\"를 대체하기도 합니다. 예를 들어, Springfield–Websterburg 기차와 같이 사용됩니다.\n\n하이픈(-)은 복합어의 요소를 연결하는 데 사용됩니다. 예를 들어, baker-owner와 같은 경우입니다. 접두사나 접미사를 단어와 구분할 때도 쓰입니다. 예를 들어, bread-like와 같은 형태입니다. 숫자 범위나 줄 끝에서 단어를 나누는 데도 사용됩니다. 또한 말더듬이나 글자를 하나씩 철자할 때도 나타낼 수 있습니다.\n\n전반적으로 엠 대시는 다재다능하며 공식적인 글쓰기와 비공식적인 글쓰기 모두에 사용할 수 있습니다. 구두점의 선택은 개인의 취향에 따라 다르며, 엠 대시 주변의 간격은 스타일 가이드에 따라 달라질 수 있습니다.",
      "ja": null
    }
  },
  {
    "id": "25013395192a7d77",
    "title": {
      "en": "Every Flop Counts: Scaling a 300B LLM Without Premium GPUs",
      "ko": "모든 실패가 중요하다: 300B LLM의 저비용 확장",
      "ja": null
    },
    "type": "story",
    "url": "https://arxiv.org/abs/2503.05139",
    "score": 107,
    "by": "bretpiatt",
    "time": 1742820496,
    "content": "In this technical report, we tackle the challenges of training large-scale Mixture of Experts (MoE) models, focusing on overcoming cost inefficiency and resource limitations prevalent in such systems. To address these issues, we present two differently sized MoE large language models (LLMs), namely Ling-Lite and Ling-Plus (referred to as \"Bailing\" in Chinese, spelled Bǎilíng in Pinyin). Ling-Lite contains 16.8 billion parameters with 2.75 billion activated parameters, while Ling-Plus boasts 290 billion parameters with 28.8 billion activated parameters. Both models exhibit comparable performance to leading industry benchmarks. This report offers actionable insights to improve the efficiency and accessibility of AI development in resource-constrained settings, promoting more scalable and sustainable technologies. Specifically, to reduce training costs for large-scale MoE models, we propose innovative methods for (1) optimization of model architecture and training processes, (2) refinement of training anomaly handling, and (3) enhancement of model evaluation efficiency. Additionally, leveraging high-quality data generated from knowledge graphs, our models demonstrate superior capabilities in tool use compared to other models. Ultimately, our experimental findings demonstrate that a 300B MoE LLM can be effectively trained on lower-performance devices while achieving comparable performance to models of a similar scale, including dense and MoE models. Compared to high-performance devices, utilizing a lower-specification hardware system during the pre-training phase demonstrates significant cost savings, reducing computing costs by approximately 20%. The models can be accessed at this https URL.",
    "summary": {
      "en": "This technical report addresses the challenges of training large Mixture of Experts (MoE) models, particularly focusing on cost and resource issues. It introduces two models: Ling-Lite, with 16.8 billion parameters (2.75 billion activated), and Ling-Plus, with 290 billion parameters (28.8 billion activated). Both models perform similarly to top industry standards. \n\nTo make AI development more efficient and accessible, especially in resource-limited environments, the report suggests methods to:\n1. Optimize model architecture and training processes.\n2. Improve handling of training issues.\n3. Enhance model evaluation efficiency.\n\nThe models also utilize high-quality data from knowledge graphs, showing better tool use abilities. Notably, a 300 billion parameter MoE model can be trained on less powerful devices with similar performance to other models, leading to about 20% savings in computing costs compared to using high-performance systems. The models are available via a provided link.",
      "ko": "이 기술 보고서는 대규모 혼합 전문가 모델(MoE)을 훈련하는 데 있어 비용과 자원 문제를 중심으로 한 도전 과제를 다룹니다. 여기서는 168억 개의 매개변수를 가진 링라이트 모델과 2900억 개의 매개변수를 가진 링플러스 모델을 소개합니다. 두 모델 모두 업계 최고 수준의 성능을 보여줍니다.\n\nAI 개발을 보다 효율적이고 접근 가능하게 만들기 위해, 특히 자원이 제한된 환경에서 사용할 수 있는 방법을 제안합니다. 첫째, 모델 아키텍처와 훈련 과정을 최적화하는 방법입니다. 둘째, 훈련 문제를 더 잘 처리할 수 있는 방안을 제시합니다. 셋째, 모델 평가의 효율성을 높이는 방법을 제안합니다.\n\n이 모델들은 지식 그래프에서 얻은 고품질 데이터를 활용하여 도구 사용 능력이 향상되었습니다. 특히, 3000억 개의 매개변수를 가진 MoE 모델은 성능이 비슷한 다른 모델들과 비교해도 덜 강력한 장치에서 훈련할 수 있으며, 고성능 시스템을 사용할 때보다 약 20%의 컴퓨팅 비용 절감 효과를 가져옵니다. 모델은 제공된 링크를 통해 이용할 수 있습니다.",
      "ja": null
    }
  },
  {
    "id": "37d2b7611a1e31d6",
    "title": {
      "en": "A note on the USB-to-PS/2 mouse adapter that came with Microsoft mouse devices",
      "ko": "마이크로소프트 마우스 어댑터 주의사항",
      "ja": null
    },
    "type": "story",
    "url": "https://devblogs.microsoft.com/oldnewthing/20250325-00/?p=110993",
    "score": 350,
    "by": "luu",
    "time": 1743120989,
    "content": "March 18, 2025\n      Why didn’t Windows 95 setup use a miniature version of Windows 95 as its fallback GUI?\n\n        Raymond Chen",
    "summary": {
      "en": "On March 18, 2025, Raymond Chen questioned why the setup process for Windows 95 didn't use a smaller version of Windows 95 as a backup graphical user interface (GUI).",
      "ko": "2025년 3월 18일, 레이먼드 첸은 윈도우 95의 설치 과정에서 백업 그래픽 사용자 인터페이스(GUI)로 더 작은 버전의 윈도우 95를 사용하지 않은 이유에 대해 의문을 제기했습니다.",
      "ja": null
    }
  },
  {
    "id": "f40cf8f6b86c632d",
    "title": {
      "en": "Launch HN: Continue (YC S23) – Create custom AI code assistants",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://hub.continue.dev/explore/assistants",
    "score": 163,
    "by": "sestinj",
    "time": 1743087986,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "7739e5a883bfe5d6",
    "title": {
      "en": "GitHub has gone – long live Forgejo",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://mastodon.social/@organicmaps/114233788700982882",
    "score": 6,
    "by": "Helithumper",
    "time": 1743192299,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "3f3afac5c4778cff",
    "title": {
      "en": "A decompilation and port of Sonic Advance 2-a GameBoy Advance game written in C",
      "ko": "소닉 어드벤스 2 복원!",
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/SAT-R/sa2",
    "score": 169,
    "by": "bane",
    "time": 1743128509,
    "content": "Sonic Advance 2\n\n⚠️ This project is not completed and still under active development\n\nThis is a work in progress matching decompilation of Sonic Advance 2\nIt so far builds the following ROMs:\n\nsa2.gba sha1: 7bcd6a07af7c894746fa28073fe0c0e34408022d (USA) (En,Ja,Fr,De,Es,It)\nsa2_europe.gba sha1: b0f64bdca097f2de8f05ac4c8caea2b80c5faeb1 (Europe) (En,Ja,Fr,De,Es,It)\n🚧 sa2_japan.gba sha1: dffd0188fc78154b42b401398a224ae0713edf23 (Japan) (En,Ja,Fr,De,Es,It) (Work in Progress)\n\nIt can also build:\n\nsa2.sdl make sdl (Linux/MacOS SDL 64bit port)\nsa2.sdl_win32.exe make sdl_win32 (Windows SDL 64bit port)\n🚧 sa2.win32.exe make win32 (Win32 native port, not functional)\n\nCurrent state\n\n🎉 The build is 100% from C files with ~80 functions which currently don't match\nAll assembly code extracted, disassembled, and decompiled by hand to their C equivilent\nAll songs have been extracted, and documented as matching MIDI files\nAll sprite animation frames have been extracted to PNGs and are used to build the matching rom\nAll tilemaps (backgrounds) have been documented and had their tiles extracted\nThe game compiles to a widescreen port (426x240) for multiple platforms\nThe \"sub games\" (Chao Garden and Collect The Rings) have been disassembled but not yet decompiled\n\nSetting up the repo\nPlease see follow these instructions\nCommunity\nJoin us on discord to get started in helping out\nNotes\n\nThe Kirby & The Amazing Mirror decompilation uses a very similar codebase, as it was written by the same dev team (Dimps)\nhttps://decomp.me is a great resource for helping to create matching functions\nldscript.txt tells the linker the order which files should be linked\nFor more info, see the FAQs section of TMC\n\nCredits\n\nJaceCear for his dedication to understanding the internals of the graphics engine, writing tools to extract this data, as well as massive effort in contributing towards the decompilation process, and setting up the PC ports\n\nShout out to @froggestspirit for the drive to set this project up\n\nSpecial thanks to @normmatt for the initial repo setup and sounds decompilation\n\nPokemon Reverse Engineering Tools community for their help with the project, and tooling for GBA decompilations\n\nKermalis for their tool which was used to dump the game midis\n\n琪姬 for their exellent work documenting all the quirks of matching midis",
    "summary": {
      "en": "**Sonic Advance 2 Project Summary**\n\n- **Project Status**: The Sonic Advance 2 decompilation is still in progress and not yet finished.\n  \n- **Current Builds**: The project has successfully created several ROM versions:\n  - **USA**: sa2.gba\n  - **Europe**: sa2_europe.gba\n  - **Japan**: sa2_japan.gba (still being worked on)\n\n- **Additional Builds**: The project also supports SDL versions for Linux, MacOS, and Windows, although the Win32 version is not functional yet.\n\n- **Development Progress**: \n  - The code has been fully converted to C, with some functions still needing adjustments.\n  - All assembly code has been disassembled and translated to C.\n  - Music tracks have been extracted and documented as MIDI files.\n  - Sprite animations and background tiles have been extracted and are being used in the ROM.\n  - The game can now compile for widescreen formats.\n  - Minor games like Chao Garden have been analyzed but not fully converted.\n\n- **Getting Involved**: You can join the community on Discord to help with the project.\n\n- **Tools and Resources**: The project uses tools developed by various contributors and is similar to the decompilation of \"Kirby & The Amazing Mirror.\"\n\n- **Acknowledgments**: Special thanks to key contributors for their efforts in graphics, sound extraction, and setting up the project.",
      "ko": "소닉 어드밴스 2 프로젝트는 현재 진행 중이며 아직 완료되지 않았습니다. 이 프로젝트는 여러 가지 롬 버전을 성공적으로 생성했습니다. 미국 버전은 sa2.gba, 유럽 버전은 sa2_europe.gba, 일본 버전은 sa2_japan.gba로, 일본 버전은 아직 작업 중입니다.\n\n추가적으로, 이 프로젝트는 리눅스, 맥OS, 윈도우용 SDL 버전도 지원하지만, 윈도우 32 비트 버전은 아직 작동하지 않습니다. 개발 진행 상황으로는 코드가 완전히 C 언어로 변환되었고, 일부 함수는 여전히 조정이 필요합니다. 모든 어셈블리 코드는 분해되어 C 언어로 번역되었습니다. 음악 트랙은 추출되어 MIDI 파일로 문서화되었습니다. 스프라이트 애니메이션과 배경 타일도 추출되어 롬에서 사용되고 있습니다. 이제 게임은 와이드스크린 형식으로 컴파일할 수 있습니다. 차오 가든과 같은 소규모 게임은 분석되었지만 완전히 변환되지는 않았습니다.\n\n프로젝트에 참여하고 싶다면 디스코드 커뮤니티에 가입하여 도움을 줄 수 있습니다. 이 프로젝트는 다양한 기여자들이 개발한 도구를 사용하며, \"커비 & 더 어메이징 미러\"의 디컴파일과 유사합니다. 그래픽, 사운드 추출 및 프로젝트 설정에 기여한 주요 기여자들에게 특별한 감사를 전합니다.",
      "ja": null
    }
  },
  {
    "id": "8bec9b672ff16617",
    "title": {
      "en": "Estimating Camera Motion from a Single Motion-Blurred Image",
      "ko": "단일 이미지로 카메라 움직임 추정하기",
      "ja": null
    },
    "type": "story",
    "url": "https://jerredchen.github.io/image-as-imu/",
    "score": 62,
    "by": "smusamashah",
    "time": 1743142059,
    "content": "In many robotics and VR/AR applications, fast camera motions cause a high level of motion blur, causing existing camera pose estimation methods to fail. In this work, we propose a novel framework that leverages motion blur as a rich cue for motion estimation rather than treating it as an unwanted artifact. Our approach works by predicting a dense motion flow field and a monocular depth map directly from a single motion-blurred image. We then recover the instantaneous camera velocity by solving a linear least squares problem under the small motion assumption. In essence, our method produces an IMU-like measurement that robustly captures fast and aggressive camera movements. To train our model, we construct a large-scale dataset with realistic synthetic motion blur derived from ScanNet++v2 and further refine our model by training end-to-end on real data using our fully differentiable pipeline. Extensive evaluations on real-world benchmarks demonstrate that our method achieves state-of-the-art angular and translational velocity estimates, outperforming current methods like MASt3R and COLMAP.",
    "summary": {
      "en": "In robotics and VR/AR, fast camera movements often lead to motion blur, which can disrupt camera pose estimation methods. This work introduces a new framework that uses motion blur as a helpful tool for estimating motion instead of seeing it as a problem. The method predicts a detailed motion flow and depth map from a single blurred image, allowing us to calculate the camera's speed. We create a large dataset of realistic motion blur for training and improve our model with real data. Tests show our approach provides the best estimates for camera movement compared to existing methods like MASt3R and COLMAP.",
      "ko": "로봇 공학과 가상현실(VR)/증강현실(AR) 분야에서 빠른 카메라 움직임은 흔히 모션 블러를 발생시킵니다. 이는 카메라 자세 추정 방법에 방해가 될 수 있습니다. 이번 연구에서는 모션 블러를 문제로 보지 않고, 오히려 움직임을 추정하는 데 유용한 도구로 활용하는 새로운 프레임워크를 제안합니다. 이 방법은 흐릿한 이미지 하나에서 자세한 움직임 흐름과 깊이 맵을 예측하여 카메라의 속도를 계산할 수 있게 합니다. 우리는 훈련을 위해 현실적인 모션 블러의 대규모 데이터셋을 생성하고, 실제 데이터를 통해 모델을 개선했습니다. 테스트 결과, 우리의 접근 방식이 기존의 MASt3R 및 COLMAP과 같은 방법들에 비해 카메라 움직임에 대한 가장 정확한 추정을 제공하는 것으로 나타났습니다.",
      "ja": null
    }
  },
  {
    "id": "212a03a3801f11de",
    "title": {
      "en": "I tried making artificial sunlight at home",
      "ko": "인공 햇빛 만들기 도전!",
      "ja": null
    },
    "type": "story",
    "url": "https://victorpoughon.fr/i-tried-making-artificial-sunlight-at-home/",
    "score": 552,
    "by": "fouronnes3",
    "time": 1743104968,
    "content": "I tried making artificial sunlight at home\n\n                    27 Mar, 2025\n\n    Some time ago, I saw this video by DIY Perks where they make artificial sunlight at home with a 500W LED and a gigantic (1.2m) parabolic reflector. I've been fascinated by this project ever since, and I wanted my own.\nOver the past year or so, I finally took the time to work on a similar project, but I had the idea for a different design. The issue with the parabolic reflector is that it takes a huge amount of space. Could I do something similar, but with a less bulky design? This is the story of my first attempt at this project - version 1 so to speak. Perhaps there will be a version 2 in the future. Enjoy the read!\n\nMy idea - as others have had I'm sure - was to use an array of lenses laid out as a grid. Then, instead of a single light source, I would use a grid array of multiple LEDs, one per lens. In my mind, this would have two major advantages:\n\nLess bulky. The size of the device would be determined by the focal length of the individual lens elements, and because each would be small, the focal length could be small also, while maintaining a decent f number.\nEasier thermal management. Multiple light sources could be regular low power LEDs which wouldn't need special cooling. There would just be a lot of them, spread out over the entire device surface.\n\nOver the course of this project, I also intended to teach myself some manufacturing and 3D design, as I don't have any experience doing any of this. My background is software, and as you'll see I took a very software heavy approach to this. It was all a long learning journey for me, but in the end I used:\n\nMostly build123d for CAD modeling, with some FreeCAD for final assembly checks and some experiments here and there - including with the cool OpticsWorkbench.\nKiCad for PCB design.\nCustom python code for simulating light and optimizing the optical system. (This custom code eventually became an entire open-source project for optimization-based optical design)\nJLCPCB for printing and assembling PCBs, and for manufacturing aluminum and plastic parts with their CNC service.\n\nTL;DR: I did it! Here is the finished device sitting on my desk today, at night:\n\nAnd here it is during the day (much less impressive!)\n\nBeware it's kinda hard to take good pictures of it, and I don't have the best photo gear. Here's also a video: (at night)\n\n  Your browser does not support the video tag.\n\nKinda cool that you can see a lens flare effect in the shape of the lens grid array.\nTechnical specsMechanical:\n\nLens square side length: 30mm\nEffective Focal length: 55mm\nArray size: 6x6 = 36 LEDs\nTotal size: 180x180mm\n\nParts:\n\nLenses: 1 biconvex lens array, 1 plano-convex lens array - custom made out of PMMA acrylic, CNC fabrication with vapor polish finish @ JLCCNC\nLEDs: LUXEON 2835 3V -- Ref: 2835HE. CRI: 95+, color temp: 4000K, 65mA.\nPCBs: Custom design\nMounting hardware: custom design - aluminium 60601 for the CNC parts and mate black resin for the 3D printed parts\nRayleigh diffuser: waterproof printing inkjet film\n\nGeneral design and sizingTo create artificial sunlight, you need four ingredients:\n\nParallel light rays. The sun is so far away that light rays emitted from a point on the surface of the sun reach us essentially parallel. This is not to say that all light rays coming from the sun are parallel, as it still has a 0.5 deg apparent angular size. But they need to be pretty straight. Any light coming from an artificial light source like an LED will be going in all directions, so some optics is required.\nHigh color quality. A good indicator to look for on a datasheet is the color rendering index (CRI). 95+ is recommended to achieve a good effect. I'm sure there's more color science you could get into, but CRI is a great start for off the shelf parts.\nRayleigh scattering, or an imitation of it.\nA LOT of power.\n\nLight intensity is the most important sizing constraint, so let's look at it first. Now, the sun is very bright. Like, ridiculously bright: around 100,000 lux. To achieve this with LEDs is by no means impossible, but it's a challenge. For this first version, I thought that targetting 10,000 lux would be quite enough because it would reduce the power consumption a lot for a first prototype, and also brightness perception is logarithmic. So one tenth of the intensity is really, perceptually, almost the same as full brightness. (In the end, I estimate my design only effectively achieved something between 1000 and 10000 lux).\nThe general grid based design of this project really has two variables:\n\nthe individual LED light output, in lumens\nthe individual lens surface area in mm²\n\nAfter some research, I think values between 30 to 130 lumens are typical for high CRI surface mount LEDs. So, assuming this is what we are working with, what is the required lens size to achieve the brightness of the sun?\nWe have to assume some non perfect efficiency for collimating the light. This will never be 100%, and in fact may be quite low if the focal length is high, because a lot of the light will be hitting the side walls instead of reaching the lens. The lens itself will also be absorbing some light. So taking a wild guess of 0.5 for the overall optical efficiency, and taking three lumens value of 30, 80 and 130, we get this plot:\n\nWith that in mind, I selected 30mm as my lens square side length. Presumably, this would be small enough to achieve some effect, but not too small to make the lenses too hard to make.\nLensesFocal length, and the lenses shape in general, is the next design consideration. The goal is to have perfectly parallel light rays. In theory, with a perfect point source and a perfect lens this is easy. Put the light source at the lens focal length, you're done. In practice, a lot of things make it harder to achieve with a lens. (This is where the parabolic reflector design is superior to a lens).\n\nA LED is not a point source\nA lens will not have perfect optical performance (i.e. aberrations)\nMechanical reality of the device means that positioning and orientation will not be perfect\nA LED radiation pattern is not isotropic, meaning intensity will be greater at the lens center\n\nThis is the radiation pattern characteristics diagram from my LED datasheet:\n\nI wrote some custom python code to simulate the optical system I had in mind, and find the best lens shape using numerical optimization. (This code eventually became an open-source project: torchlensmaker) After a lot of experimentation, I settled on a 2 lens design:\n\nLens 1: Biconvex parabolic lens\nLens 2: Planoconvex parabolic lens\n\nThe effective focal length of this two lens system is about 55mm. Focal length is a key design parameter, and here I feel like more experimentation is needed. It's a big tradeoff consideration and has a huge impact on the system design. It impacts:\n\nThe curvature of the lens surface, which is a key manufacturing point (you want to minimize curvature for manufacturing, which means maximizing focal length)\nThe optical efficiency of the system due to the led radiance pattern (here you want to minimize focal length, to gather more of the emitted light)\nThe device thickness (here I wanted a not-too-thick device, so to minimize focal length also)\n\nI used a two lens system mostly to reduce the surface curvature of the lens arrays. This reduces the manufacturing cost by a lot. High curvature lenses are more expensive in general, and this grid array design means that a high curvature lens will create sort of \"valleys\" in between the lenses. Because I was targetting CNC manufacturing, this is to be minimized to get a design that's even possible to machine.\nThis is the optical simulation I had at the time I finalized the design and ordered the lenses. (Since then my simulation code has improved and I could likely do much better modeling today using the latest version of torchlensmaker):\n\nWith some custom build123d code I was able to make the two lenses 3D models by stacking the lenses in a grid pattern and adding edges for mounting:\n\n  <p>Your browser does not support iframes.</p>\n\n  <p>Your browser does not support iframes.</p>\n\nWhat's really cool using build123d for 3D modeling, is that I can just change a python variable to change the size of the array, of the thickness of the lens, of anything else really. It's all parametric out of the box because it's regular Python code! This makes exploring the design space very efficient. I've never done 3D modeling any other way, but I can't imagine ever not having the power of programming with me if I ever do it again!\nI had the lenses manufactured out of PMMA acrylic at JLC with a vapor polish finish. Total cost for the lenses was about 55€ which is really not bad!\nOne of the two main lens array, built by JLCCNC:\n\nLEDsI really wanted to use the 3030 G04 from YUJILEDS, but it's only sold on 5000 units reels that cost $1000 a piece... maybe for version 2 I will upgrade to those. For version 1, I settled on LUXEON 2835 3V. They are about 3 times less bright than the YUJILED, but they have good color rendering and the SMD package I was looking for. And importantly, the minimum order quantity was only 50 at JLC global sourcing.\nIn the version 1 design, the grid is 6x6 which means 36 LEDs total.\nPCBsI designed a custom PCB with KiCAD. Each PCB holds 6 LEDs which are laid out as 2 segments of a 12V led strip in parallel. This allows to use a standard wall plug 12V power supply.\n\nThe mechanical role of the PCB is very important in this design. Not only does it distribute power to the LEDs and regulate current, it also precisely positions the LEDs at the lens focal point. For this, exporting the PCB 3D model and importing it into FreeCAD was very useful to check that everything fits together: the PCB in the aluminum support baseplate, the holes on the light hoods, etc. My Python code exported the precise LED coordinates which I could input into KiCad's layout editor.\nI had the PCB printed and the components assembled by JLCPCB. It's very very cool to design an electronic board on your computer and get it fully assembled in the mail a few weeks later - no soldering required! (for this step anyway).\n\nMechanical mounting partsTo mount everything together I designed 3 parts:\n\nA baseplate, to hold the PCBs and the side walls. The PCBs are fitted below the baseplate, and light goes through holes drilled into the baseplate. There are also partial holes to allow for the thickness of the SMD resistors mounted on top of the PCBs, and finally two mounting holes per PCB. This is why it has so many holes :)\n\n  <p>Your browser does not support iframes.</p>\n\nSide walls to hold the lenses using grooves in which to insert them, and a larger groove to secure in the baseplate. The baseplate side holes are threaded to support M2 screws securing the base of the walls. Again, JLCCNC did the drilling and threading of the holes at a great price.\n\n  <p>Your browser does not support iframes.</p>\n\nLight hoods, a rectangle block with rectangular holes. It sits on top of the PCB to shape the light coming from each LED into a cone (or really a four sided pyramid). This is to make sure light from a given LED only reaches its matching lens on the lens array, and no other. Bleed light is inevitable, but at least this prevents direct leakage.\n\n  <p>Your browser does not support iframes.</p>\n\nThe hoods were 3D printed out of black resin, the walls and baseplate were CNC cut out of Aluminum 60601.\nI'm not a mechanical engineer so this process was... trial and error. Still the result is working so I'm quite happy with that. For a possible version 2, there's a lot I'll change in the mechanical design. But apart from the one design flaw I was able to fix manually with a drill (more on that below), everything fit together quite well on the first try.\nRayleigh scatteringThe final ingredient is Rayleigh scattering. This is the physical phenomenon that makes the sky look blue, and it's important to achieve a convincing effect. In the DIY Perks video that inspired this project, they used a home made liquid solution with suspended particles of the correct size for Rayleigh scattering. Not super practical and I really wanted to find another solution (get it?). Thankfully, some time after the original video, someone on the diyperks forum discovered that inkjet print film achieves a very similar effect. A quick trip to a local office supply store was all I needed here! Amazing discovery.\nI didn't anticipate this step during the initial design phase, so the film is simply cut to the correct size and secured with black electrical tape.\nAssemblyAfter a few weeks of design work, and another few weeks of waiting for the parts to arrive, it was finally time for assembly!\nOn top of the individual 3D models made with build123d, I had a final assembly FreeCAD model with all parts fitted together, including the lenses:\n\nNote the green brackets that I initially planned to use. When actually assembling the walls to the baseplate, the solidity of the formed box was very high, I decided to drop the brackets entirely. This is why some extra unused holes remain on the side walls.\nThis is all the parts just after unboxing (excluding the inkjet film, solder tin, screws, power supply, wiring, electrical tape):\n\nThe only real design flaw was insufficient width of the grooves that hold the lenses. The lenses have an edge thickness of 1.2mm, which I had intended to fit into a 1.22mm groove. Turns out this was not enough, probably due to a combination of manufacturing tolerance and additional thickness added by the anodizing black matte surface finish of the aluminum part. The lenses didn't fit into the grooves!\nI don't have a very advanced tools at home, so my best solution to this was making the existing grooves wider by hand using a power drill. I bought a 1.5mm metal drill bit and achieved a decent result by doing 4 to 5 passes per groove. This took about 2-3h in total because I had to move the bit quite slow and could only machine about 1/4th of each groove depth at a time by moving the drill bit slowly accross, and there are 8 grooves total.\n\nHere's some more pictures of assembly below.\nThe back side after soldering wires to the PCB power pins and a socket for the 12V power supply. The PCBs and hood pieces share a common mounting hole so only two screws per PCB-hood pair are used.\n\nThe front side of the baseplate + PCB + hoods assembly, but without the lenses, powered on. Don't look at it directly :)\n\nIt's interesting to note that in the picture above, all of the light you can see from the LEDs is actually \"bleed light\" and not useful light. None of the light visible above is the light that's intended to go into the lens and produce the sunlight effect.\nTesting with partial assembly of the walls and only 1 out of the 2 lenses:\n\nTesting the inkjet film layers with an avocado as a subject. I settled on using two layers of the inkjet film for the final build:\n\nCostOverall I spent around 1000€ on this project. But this includes cost of tools I was missing, prototype parts that I had manufactured but discarded, bulk orders for parts like LEDs and PCBs which had a minium order quantity above what I need for 1 unit, and various supplies like screws, etc. The actual raw cost of parts only, without shipping, to build the final unit is hard to estimate. But I would say around 300€. The most expensive parts are the CNC parts (PMMA lenses and the aluminum baseplate and walls) accounting for about 2/3rd of the total price. The rest (PCBs, assembly service, LEDs, 3D printed plastic parts) was quite cheap.\nConclusionAs I write this the final piece is sitting on my desk and producing a pleasant soft white glow. It's definitely nice, and I'm very proud of the result - especially because this was by far the biggest build project I have ever done.\n\nThanks to this project, I've learned a ton about PCB design, electronics and CNC manufacturing and optics. I even got so far down the side quest of learning optics that I started an open-source python project for modeling geometric optics.\nSo, is it convincing as artificial sunlight?\nMy honest answer to that is: partially. The geometric effect of the light source appearing at infinity works. As I pan and tilt my head from side to side, the illusion of light coming from way far behind the object is 100% a success. On top of that, if you look at it while moving your head into the light beam, my eyes get surprised - almost hurt - by the sudden intensity jump. This indicates that collimation is good and you can sort of see it in the video at the start of this post.\nHowever it's apparent that it's simply too weak. Don't get me wrong, it's still bright. I can't look at it directly without sunglasses, and honestly it's really hard to take a good picture of it because the contrast between the light it emits and the outside of it is very high.\nAnother downside is that I can definitely make out the grid of lenses, as the intensity pattern clearly reveals the grid shape. This is quite a minor downside and not really unpleasant, and I'm sure it could be improved upon.\nIf I were to ever work on a version 2, I would focus on:\n\nMore power. My feeling is the light output needs to be 3 to 5 times stronger to get any closer to a convincing effect, and it's not crazy to aim for as much as 10x brighter than this prototype.\nMore surface area. This prototype is 18cm x 18cm. So you only really get the effect if you are able to sit with the produced straight beam of light, which is quite narrow to resemble any kind of \"fake window\". A future version would need to be 2 to 4 times wider in my opinion.\nBetter optical design. I still think a refraction based design is possible, but it requires very precise optical design and mechanical tolerances. My feeling is that a refraction based design, especially as a grid, is very sensitive to positioning and orientation of parts. I lack mechanical engineering skills in this area.\n\nHowever there are some really encouraging things that I really like about this grid based, refractive design:\n\nIt's scalable. If I had built 4 identical items, I could literally stack them on top of each other and get more surface area. The \"bezels\" would be only 5% of the total light emitting area, and I'm sure this could be lowered. I also like that the inner design calls for repeated elements, as this introduces some economy of scale, even at the prototype level. The only part that's not trivially scalable is the lens grid. Maybe it could be injection molded for very large scale production, or for medium scale you could come up with a way to tile multiple lens grids into a larger overall grid pattern, adding some thin bezels for mounting.\nIt's compact. The total size is 19cm x 19cm x 9cm. This is quite compact for a 5cm focal length and an effective lighting area of 18cm x 18cm. Reflective designs like the DIYPerks video or commercial products like CoeLux do not achieve this form factor.\nThermal management is better by design. This is not really something I got into for this design, as it's quite underpowered. The whole thing runs comfortably on a 12V / 3A wall brick power supply. But this design offers great margin for scaling up because there isn't a single light source to cool down, but a number of LEDs proportional to the surface area. I suspect the main thermal issue when scaling up would be the cooling of the power supply itself, not of the lamp.\n\nAs final thoughts, let me talk about the software heavy approach I had for this project. It's awesome. If I was starting a manufacturing company today, I would do it all code based. PCBs, 3D models, assembly, testing... I want code everywhere. The power of changing a parameter and having the entire design updated with a single script it so good. Run a script and get all the production data including GERBERs, BOM, 3D models, mechanical schematics, technical diagrams, automated tolerance and electrical checks... absolutely no manual steps between changing a design parameter and ready to send a new order to manufacturing. The PCB and CAD space is even evolving to use proper CI/CD tools which is really exciting.\nI don't know if I'll ever have the time to work on version 2 of this project, but it was great fun anyway! And now I have a cool unique lamp. Thank you for reading!\n\n            103\n\n    document.querySelector('#upvote-form').addEventListener('submit', (e) => {\n        e.preventDefault();\n        const form = e.target;\n        fetch(form.action, {\n            method: form.method,\n            body: new FormData(form),\n        });\n        const button = form.querySelector('button')\n        button.disabled = true\n        button.style.color = \"salmon\"\n        const upvoteCount = document.querySelector('.upvote-count')\n        upvoteCount.innerHTML = `${(parseInt(upvoteCount.innerHTML.split(\" \")[0]) + 1)}`\n    });",
    "summary": {
      "en": "**Summary:**\n\nThe author attempted to create artificial sunlight at home, inspired by a DIY project that used a large parabolic reflector. Instead, they designed a compact version using a grid of lenses and multiple LEDs. This approach aimed to reduce the bulkiness of the design and improve thermal management.\n\nKey points of the project include:\n\n- **Design Concept**: The author used a grid of small lenses and multiple low-power LEDs, allowing for a smaller device and better heat distribution.\n- **Learning Experience**: They learned about 3D design, PCB design, and optics throughout the project, using various software tools like CAD modeling and custom Python scripts for simulations.\n- **Technical Specs**: The final product features a 6x6 array of LEDs, custom-made lenses, and specialized components for optimal light output.\n- **Rayleigh Scattering**: They used inkjet print film to mimic Rayleigh scattering, enhancing the light's appearance.\n- **Challenges and Costs**: The project cost around 1000€, with significant expenses for manufacturing parts. The initial design had some flaws, which were resolved through manual adjustments.\n- **Final Outcome**: The artificial sunlight produced a pleasant glow but was less intense than desired. The author plans to improve the design in a potential version 2, focusing on increasing brightness and surface area.\n\nOverall, the project was a significant learning experience, resulting in a unique lamp and insights into the integration of software and hardware in design.",
      "ko": "저자는 대형 포물선 반사를 이용한 DIY 프로젝트에서 영감을 받아 집에서 인공 햇빛을 만들려고 시도했습니다. 대신, 렌즈 그리드와 여러 개의 LED를 사용하여 소형 버전을 설계했습니다. 이 접근 방식은 디자인의 부피를 줄이고 열 관리를 개선하는 데 목적이 있었습니다.\n\n프로젝트의 주요 내용은 다음과 같습니다. 저자는 작은 렌즈 그리드와 여러 개의 저전력 LED를 사용하여 더 작은 장치와 더 나은 열 분산을 가능하게 하는 디자인 개념을 적용했습니다. 이 과정에서 3D 디자인, PCB 설계, 광학에 대해 배우며 CAD 모델링과 맞춤형 파이썬 스크립트를 활용한 시뮬레이션 등 다양한 소프트웨어 도구를 사용했습니다. 최종 제품은 6x6 배열의 LED, 맞춤형 렌즈, 최적의 빛 출력을 위한 특수 부품을 특징으로 합니다. 또한, 레일리 산란을 모방하기 위해 잉크젯 인쇄 필름을 사용하여 빛의 외관을 향상시켰습니다.\n\n프로젝트 비용은 약 1000유로로, 부품 제조에 상당한 비용이 들었습니다. 초기 디자인에는 몇 가지 결함이 있었지만, 수동 조정을 통해 해결했습니다. 최종 결과물은 기분 좋은 빛을 발산했지만 원하는 만큼 강렬하지는 않았습니다. 저자는 밝기와 표면적을 늘리는 데 중점을 두어 향후 버전 2에서 디자인을 개선할 계획입니다.\n\n전반적으로 이 프로젝트는 중요한 학습 경험이었으며, 독특한 램프와 디자인에서 소프트웨어와 하드웨어의 통합에 대한 통찰을 얻는 결과를 가져왔습니다.",
      "ja": null
    }
  },
  {
    "id": "f0e60f9213a8b5b0",
    "title": {
      "en": "Mid-pregnancy pollution exposure linked to postpartum depression",
      "ko": "임신 중 오염, 산후 우울증과 연관",
      "ja": null
    },
    "type": "story",
    "url": "https://www.bps.org.uk/research-digest/mid-pregnancy-pollution-exposure-linked-postpartum-depression-new-study-suggests",
    "score": 52,
    "by": "wjb3",
    "time": 1743146735,
    "content": "Related articlesA guide for difficult news in pregnancy 03 September 2020 Ella Rhodes reports.Pregnancy affects women’s memory for what they plan to do 12 November 2008 Research finds pregnant women under-perform on tests of retrospective memory, such as word learning tasks.Cognition and perceptionMemoryWhat is psychological about normal pregnancy? 13 March 2010 Paula Nicolson reviews issues of biology, risk, identity… and more.Sex and gender",
    "summary": {
      "en": "Here’s a simplified summary of the key points from the text:\n\n- There are articles discussing various aspects of pregnancy, including:\n  - How pregnancy can impact women's memory, particularly their ability to remember planned tasks.\n  - Research showing that pregnant women may struggle with certain memory tests.\n  - A review of psychological issues related to pregnancy, including biology, risk, and identity.",
      "ko": "임신과 관련된 여러 측면에 대한 기사들이 있습니다. 이 중 하나는 임신이 여성의 기억력에 미치는 영향을 다루고 있습니다. 특히, 임신 중에는 계획된 일을 기억하는 능력이 저하될 수 있다는 점이 언급됩니다. 연구에 따르면, 임신한 여성들이 특정 기억력 테스트에서 어려움을 겪는 경우가 많다고 합니다. 또한, 임신과 관련된 심리적 문제에 대한 검토도 이루어졌으며, 여기에는 생물학적 요인, 위험 요소, 그리고 정체성에 대한 논의가 포함됩니다.",
      "ja": null
    }
  },
  {
    "id": "c833c735ec962236",
    "title": {
      "en": "Preschoolers can reason better than we think, study suggests",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://phys.org/news/2025-03-preschoolers.html",
    "score": 160,
    "by": "PaulHoule",
    "time": 1742903610,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "8c244607c023de64",
    "title": {
      "en": "Using uv and PEP 723 for Self-Contained Python Scripts",
      "ko": "UV와 PEP 723로 완벽한 파이썬 스크립트 만들기",
      "ja": null
    },
    "type": "story",
    "url": "https://thisdavej.com/share-python-scripts-like-a-pro-uv-and-pep-723-for-easy-deployment/",
    "score": 228,
    "by": "thisdavej",
    "time": 1743123226,
    "content": "Table of Contentsuv and PEP 723Setting the stageInstalling uvAdding package dependencies in single-file scripts with uvRunning your script with uvMaking it even easier to run with a Python shebangLinux/macOS usersWindows usersSetting up your uv script to be invoked from anywhere on your computerLinux/macOS usersWindows usersBonus: where does uv install its virtual environments?How does uv derive its virtual environment folder name?Conclusion",
    "summary": {
      "en": "**Summary:**\n\nThis document provides a guide on using a tool called \"uv.\" \n\n1. **Introduction to uv and PEP 723**: It explains what uv is and its connection to PEP 723.\n2. **Installation**: Instructions for installing uv.\n3. **Adding Dependencies**: How to add package dependencies in single-file scripts using uv.\n4. **Running Scripts**: Steps to run your script with uv and make it easier to execute using a Python shebang.\n5. **Setup Instructions**: Guidance for Linux/macOS and Windows users on how to set up uv scripts to run from anywhere on your computer.\n6. **Virtual Environments**: Information on where uv installs its virtual environments and how it names them.\n7. **Conclusion**: A wrap-up of the document. \n\nThis guide is designed to help users effectively utilize uv for their Python projects.",
      "ko": "이 문서는 \"uv\"라는 도구를 사용하는 방법에 대한 안내를 제공합니다.\n\nuv는 무엇인지와 PEP 723과의 관계를 설명합니다. PEP 723은 Python의 개선 제안서로, uv의 기능과 목적을 이해하는 데 도움이 됩니다. \n\nuv를 설치하는 방법에 대한 지침이 포함되어 있습니다. 설치 과정은 간단하며, 필요한 패키지를 쉽게 추가할 수 있는 방법도 안내합니다. 단일 파일 스크립트에서 uv를 사용하여 패키지 의존성을 추가하는 방법을 설명합니다.\n\nuv를 사용하여 스크립트를 실행하는 단계도 안내합니다. Python의 shebang을 사용하면 스크립트를 더 쉽게 실행할 수 있습니다. \n\nLinux/macOS와 Windows 사용자에게 uv 스크립트를 컴퓨터의 어느 위치에서든 실행할 수 있도록 설정하는 방법에 대한 지침이 제공됩니다. \n\nuv가 가상 환경을 설치하는 위치와 그 이름을 지정하는 방법에 대한 정보도 포함되어 있습니다. \n\n이 가이드는 사용자가 Python 프로젝트에서 uv를 효과적으로 활용할 수 있도록 돕기 위해 작성되었습니다.",
      "ja": null
    }
  },
  {
    "id": "f9535b307df11e08",
    "title": {
      "en": "A Debugger is a REPL is a Debugger",
      "ko": "디버거의 재발견",
      "ja": null
    },
    "type": "story",
    "url": "https://matklad.github.io/2025/03/25/debugger-is-repl-is-debugger.html",
    "score": 40,
    "by": "ingve",
    "time": 1742900776,
    "content": "A Debugger is a REPL is a Debugger Mar 25, 2025\nI love debuggers! The last time I used a debugger seriously was in 2017 or so, when I was still\ncoding in Kotlin. I’ve since switched to working with native code, and, sadly gdb and lldb are of\nalmost no help for me. This is because they are mere “debuggers”, but what I need is a REPL, and a\ndebugger, all in one. In this article I show a more productive way to use debuggers as REPLS.\nThe trick boils down to two IntelliJ IDEA features, Run to Cursor and Quick Evaluate\nExpression.\nThe first feature, run to cursor, resumes the program until it reaches the line where the cursor is\nat. It is a declarative alternative to the primitive debugger features for stepping into, over, and\nout — rather telling the debugger how to do every single step, you just directly tell it where do\nyou want to be:\n\nThe second feature, quick evaluate expression, evaluates selected test in the context of the current\nstack frame. Crucially, this needn’t be some pre-existing expression, you can type new stuff and\nevaluate it!\n\nRun to cursor sets up the interesting context, and quick evaluate allows you to poke around. This\ntwo features completely change how I used debuggers — instead of stepping through my program and\nobserving it, I zap between interesting points of its execution and run experiments.\nAuthors of debuggers & REPLs, take note of these features of the workflow:\nThere’s no prompt anywhere. The medium of interaction is the 2D program text, not 1D command line.\nIts a vi interface, not ed interface.\nFrom the debugger side, we support seamless evaluation of new program text in context. When\nentering new text, you get full code completion experience. I haven’t used this a tonne, but it\nshould also be possible to reload code with your changes.\nFrom the REPL side, you need breakpoints. Top-level context is rarely interesting! You need to be\nable to place yourself “in the middle” of your application, where you have access to all the locals.\nDebuggers do this via setting breakpoints, but the click-on-the-fringe (or, worse, enter\nfile:line:column textually) UI is atrocious. There’s a perfectly fine pointing device in any\neditor — the cursor. Just get the program execution to that point!",
    "summary": {
      "en": "The author expresses their enthusiasm for using debuggers, sharing their experience with them since 2017. They highlight a need for a tool that combines both a debugger and a REPL (Read-Eval-Print Loop), particularly for their current work with native code, where traditional debuggers like gdb and lldb fall short.\n\nThe article introduces two key features from IntelliJ IDEA that can enhance the debugging experience: \n\n1. **Run to Cursor**: This feature allows users to run the program until it reaches a specific line, providing a more efficient way to navigate through code without manually stepping through each line.\n   \n2. **Quick Evaluate Expression**: This lets users evaluate any selected text in the current context, enabling them to test new code on the fly.\n\nTogether, these features allow for a more interactive debugging process where users can quickly jump to interesting points in the code and experiment, rather than just observing program execution step-by-step. The author suggests that debuggers should support seamless code evaluation and allow for easy access to local variables by using breakpoints effectively. They advocate for a more intuitive interface that leverages the cursor for navigation rather than cumbersome text inputs.",
      "ko": "저자는 2017년부터 디버거를 사용해온 경험을 바탕으로 디버거에 대한 열정을 표현합니다. 특히, 현재 네이티브 코드 작업을 하면서 전통적인 디버거인 gdb와 lldb가 부족하다는 점에서 디버거와 REPL(읽기-평가-출력 루프)을 결합한 도구의 필요성을 강조합니다.\n\n이 글에서는 IntelliJ IDEA에서 디버깅 경험을 향상시킬 수 있는 두 가지 주요 기능을 소개합니다. 첫 번째는 '커서까지 실행(Run to Cursor)' 기능으로, 사용자가 특정 줄에 도달할 때까지 프로그램을 실행할 수 있게 해줍니다. 이를 통해 매 줄마다 수동으로 실행하지 않고도 코드 내비게이션을 더 효율적으로 할 수 있습니다. 두 번째는 '빠른 표현식 평가(Quick Evaluate Expression)' 기능으로, 현재 문맥에서 선택한 텍스트를 평가할 수 있게 해주어 새로운 코드를 즉석에서 테스트할 수 있습니다.\n\n이 두 가지 기능은 사용자가 코드에서 흥미로운 지점으로 빠르게 이동하고 실험할 수 있는 더 상호작용적인 디버깅 과정을 가능하게 합니다. 저자는 디버거가 원활한 코드 평가를 지원하고, 효과적으로 중단점을 사용하여 지역 변수에 쉽게 접근할 수 있도록 해야 한다고 제안합니다. 또한, 복잡한 텍스트 입력 대신 커서를 활용한 더 직관적인 인터페이스를 지지합니다.",
      "ja": null
    }
  },
  {
    "id": "4044c51131ead424",
    "title": {
      "en": "Percentage of parents financially supporting adult children reaches 3-year high",
      "ko": "성인 자녀 지원, 3년 만에 최고치!",
      "ja": null
    },
    "type": "story",
    "url": "https://www.savings.com/insights/financial-support-for-adult-children-study",
    "score": 28,
    "by": "walterbell",
    "time": 1743144765,
    "content": "{\n\"@context\": \"http://schema.org\",\n\"@type\": \"NewsArticle\",\n\"inLanguage\": \"en-US\",\n\"url\": \"https://www.savings.com/insights/financial-support-for-adult-children-study\",\n\"mainEntityOfPage\": \"https://www.savings.com/insights/financial-support-for-adult-children-study\",\n\"image\": \"https://cdn.sdccdn.com/images/savings/logo/5199693.png?width=500\",\n\"thumbnailUrl\": \"https://cdn.sdccdn.com/images/savings/logo/5199692.png?width=325\",\n\"name\": \"Percentage of parents financially supporting adult children reaches a three-year high\",\n\"headline\": \"Percentage of parents financially supporting adult children reaches a three-year high\",\n\"description\": \"\",\n\"datePublished\": \"2025-03-21\",\n\"dateModified\": \"2025-03-26T08:48:02-0700\",\n\"author\": {\n\"@type\": \"Person\",\n\"name\": \"Beth Klongpayabal\",\n\"email\": \"beth.klongpayabal@savings.com\",\n\"description\": \"Beth Klongpayabal is an Analytics Manager for Savings.com. She provides insights on consumer purchases and emerging retail trends. Previously, she served as Head of Data Operations.\",\n\"jobTitle\": \"Analytics Manager at Savings.com\",\n\"image\": {\n\"@type\": \"ImageObject\",\n\"url\": \"https://cdn.sdccdn.com/images/savings/logo/4959644.jpeg\"\n}\n},\n\"publisher\": {\n\"@type\": \"Organization\",\n\"name\": \"Savings.com\",\n\"url\": \"https://www.savings.com\",\n\"logo\": \"https://cdn.sdccdn.com/images/theme/sdc2/logos/savings.svg\"\n}\n}\n\nBy Beth Klongpayabal\n\nLast Updated: March 21, 2025\n\n.table-blue thead th { background-color: #2269DC; color: #FFF; text-align: left; vertical-align: middle!important; padding: .75rem .625rem .75rem; font-weight: bold; } .anchor-section li { list-style-type:none; margin-left: -20px!important; } .anchor-section li a{ text-decoration: underline; } .anchor-section h4{ margin-top:35px; } table { margin-bottom: 30px; }\n\nThe average amount these parents give also reached a three-year high at $1,474/month, but 40% plan to cut off funds in the next two years.\nLet's face it—adulting is expensive these days. With economic pressures hitting younger generations hard, more and more adult Americans are turning to an age-old financial institution: their parents.\nThis trend of parents bankrolling their grown kids took off during the pandemic, and at Savings.com, we’ve been reporting on it since 2022.\nFor our fourth annual report, we surveyed 1,000 parents of adult children. We wanted to know how much money flows from parents to grown kids, what they're spending it on, how this affects their financial health, and how far the “Bank of Mom & Dad\" is willing to go to keep their adult offspring afloat.\nKey Findings:\n\nHalf of parents with adult children provide regular financial assistance to their grown offspring. The average support per adult child is $1,474 monthly, about 6% higher than last year.\n83% of supporting parents contribute to their adult kids’ monthly groceries; 65% help with cell phones, and nearly half (46%) pay for vacations.\nMore than three-quarters (77%) of supportive parents attach conditions to their financial assistance. 23% give money without any conditions.\nNearly 50 percent of parents have sacrificed their financial security to help their grown kids financially, and most supporting parents feel obligated to help their kids with money.\nWorking parents who support grown kids contribute over 2X more money each month to their adult children than they do to retirement funds.\n\nHalf of all parents with children 18+ provide them with financial support in 2025\nThe American economy has maintained its strength in recent years. However, the new presidential administration's ambitious initiatives, substantial promises, and international trade tensions have introduced financial uncertainty.\nThis unsettled economic landscape and the growing wealth gap between generations may have strengthened the financial connection between adult children and their parents. For the first time since beginning our annual research, we've found that half of parents with adult children now provide regular financial assistance to their grown kids.\n\n<img src=\"\" width=\"100%\" alt=\"chart visualization\" class=\"lazy\" data-original=\"https://public.flourish.studio/visualisation/21815193/thumbnail\">\nA Flourish chart\nWith inflation keeping the cost of living high, parents' financial support has reached a new peak, averaging nearly $1,500 per month (or almost $18,000 annually). This represents a six percent increase from the monthly contributions we reported last year.\nAs you might expect, Generation Z adults (ages 18-28) receive more financial support from their parents than their Millennial counterparts (ages 29-44), who've had more time to build careers and establish income streams. While the average contribution to Millennials decreased slightly, a significant increase in support for Generation Zers pushed the overall average higher. Members of Generation X (ages 45-60) rarely receive financial assistance from their parents, likely because they've either achieved financial independence or have inherited family wealth.\n\n<img src=\"\" width=\"100%\" alt=\"chart visualization\" class=\"lazy\" data-original=\"https://public.flourish.studio/visualisation/22100594/thumbnail\">\nA Flourish chart\nThe financial strain of supporting grown children is particularly pressing for parents preparing a nest egg. Parents still in the workforce contribute over two times more money to their adult children each month than their retirement accounts.\n\n<img src=\"\" width=\"100%\" alt=\"chart visualization\" class=\"lazy\" data-original=\"https://public.flourish.studio/visualisation/22101217/thumbnail\">\nA Flourish chart\nThe psychological and fiscal impact of such commitment translates directly to parental anxiety. At a time when many Americans haven’t set aside enough funds for their later years, 79 percent of those supporting adult children worry about setting themselves up for a comfortable retirement. In comparison, 72 percent of people who don’t support adult children financially feel stressed about their retirement savings.\nWhat costs do parents cover for their adult children?\nParents report providing their adult children with financial assistance for various expenses, from educational costs to vacations to basic spending money.\nLooking at the breakdown of this support reveals that food and groceries top the list of needs among financially dependent adult children. With food prices continuing to climb, it's understandable that four out of five parents providing assistance are helping with their grown kids’ grocery bills. Parents contribute an average of $220 monthly toward their adult child's grocery expenses.\nCosts parents help their adult kids cover in 2025\nBy generation of their adult children\n\nCategories parents contribute to\nParents supporting Gen Zers (18-28)\nParents supporting Millennials (29-44)\nAll parents supporting adult kids\nAverage monthly support provided\n\nGroceries or food\n87%\n78%\n83%\n$220\n\nCell phone\n73%\n51%\n65%\n$63\n\nHealth insurance or healthcare\n69%\n28%\n54%\n$165\n\nRent or mortgage\n66%\n58%\n63%\n$653\n\nTuition or other school expenses\n57%\n24%\n45%\n$1,198\n\nLeisure/vacations\n56%\n30%\n46%\n$190\n\nCar\n49%\n39%\n44%\n$218\n\nDiscretionary spending\n47%\n41%\n44%\n$126\n\nStudent loans\n26%\n18%\n23%\n$226\n\nCredit cards\n22%\n19%\n21%\n$160\n\nInvestments\n18%\n9%\n14%\n*\n\n*Insufficient data\n\nAnother two-thirds of parents with adult children assisted with cell phone bills and housing expenses. The need for specific types of support varied between Generation Zers and Millennials. Gen Z adults were far more likely to need help with healthcare, vacations, and tuition than Millennials, as many are still in school or just launching their careers in their early twenties. School expenses were the costliest for parents, averaging nearly $1200 monthly. That’s a massive increase over the average spending on tuition last year, at around $600 a month.\nParental financial support often comes with conditions\nAccepting financial help from parents is one thing, but doing so while demonstrating effort and appreciation is another matter. Our findings suggest that parents may be growing less tolerant of adult children who appear to take advantage of their generosity.\nAmong parents providing financial support, 63 percent also offer housing to their adult children. While only 39 percent of these live-at-home adult children contributed to household expenses in 2024, that figure has increased substantially to 51 percent this year.\nThis improvement in shared financial responsibility likely stems from parents setting firmer boundaries. The percentage of parents establishing specific conditions for financial assistance has increased since our previous study—from 71 percent who gave conditionally last year to 77 percent who now attach requirements to their financial support.\n\nWhich conditions must your adult children meet to receive your financial assistance? Select all that apply.\n2024\n2025\n\nMaintaining a job or actively seeking employment (e.g., must prove they are applying to jobs if unemployed)\n45%\n48%\n\nPursuing education (e.g., must be enrolled in college, university, or vocational training)\n42%\n45%\n\nContributing to household expenses (if living at home, e.g., must pay a portion of rent or utility bills)\n18%\n26%\n\nAchieving certain financial behaviors or goals (e.g., must follow a budget or save a certain amount each month)\n17%\n21%\n\nAttending counseling or therapy (if support is related to personal development or health issues)\n4%\n4%\n\nNone - I provide support with no specific conditions\n29%\n23%\n\nThe most notable increase appeared in parents requiring adult children living at home to contribute to household expenses. However, the most common conditions continue to be requirements that adult children actively seek employment or pursue education—practical approaches designed to guide grown offspring toward eventual financial independence.\nOther conditions parents placed on their adult children included establishing financial goals and attending counseling or therapy sessions. Each such requirement reflects a caring concern designed to help adult children financially get on their feet.\nWhat are parents sacrificing for their children’s financial security?\nThe parents in the study seemed more than willing to aid their children. Yet, that added financial burden often creates stress and demands lifestyle sacrifices. What compels them to keep giving?\nObligation is one driving force for parents who economically support their adult offspring. Most parents who provide monetary assistance do so out of some sense of duty.\nFifty-three percent of contributing parents feel responsible for financially supporting their grown kids. That number is down from 61 percent one year ago, another potential indicator that such gravy train sentiments may be slipping.\n\n<img src=\"\" width=\"100%\" alt=\"chart visualization\" class=\"lazy\" data-original=\"https://public.flourish.studio/visualisation/22101377/thumbnail\">\nA Flourish chart\nThis responsibility causes great strain on parents. Nearly 50 percent of providing parents sacrifice financial security for the sake of supported children, and 40 percent felt pressured to give financial assistance even when it meant uncomfortably stretching their resources.\nThose numbers mirror the findings from past reports. Despite the hardship and stress sometimes created by these contributions, devoted moms and dads remain ready to dig deeper to help their struggling kids. Nearly nine in ten parents would make one or more additional financial sacrifices to aid their offspring.\nSpecifically, more than 60 percent of parents would be willing to live a more frugal lifestyle to support their adult children, half would pull money from their savings or retirement accounts, and one-third would postpone retirement or take on debt so that they might shift funds to provide for their progeny.\n\nWhich of these would you be willing to do to financially support your adult child(ren)? Select all that apply.\n2024\n2025\n\nLive a more frugal lifestyle\n61%\n62%\n\nPull money from my savings or retirement account\n46%\n50%\n\nRetire later\n37%\n35%\n\nTake on debt\n29%\n31%\n\nCome out of retirement\n18%\n17%\n\nRefinance my home\n12%\n13%\n\nNone of the above\n14%\n13%\n\nMany supporting parents would be willing to come out of retirement or refinance their homes to help their children. Grown kids struggling through financial straits are fortunate to find such selfless family support. They shouldn't take it for granted or become perpetually dependent.\nWhen asked how long they planned to continue financial support of adult children, parents admitted there may be a shelf life on their generosity. Less than 20 percent of those supplying aid said their largesse would continue indefinitely.\n\n<img src=\"\" width=\"100%\" alt=\"chart visualization\" class=\"lazy\" data-original=\"https://public.flourish.studio/visualisation/22101467/thumbnail\">\nA Flourish chart\nMore than one-third of parents who give money to their adult kids say they’ll cut off support within the next two years. Their aim is likely to encourage their children towards financial independence. However, terminating assistance before a potential recession could deal a double blow to younger generations.\nConclusion\nThe last four years of our research findings collectively illustrate remarkable parental commitment. Parents continue to accept financial stress and make personal sacrifices to support their adult children's economic well-being. However, even as we see more parents providing financial assistance than in any previous year of our research, we've also detected some emerging cracks in this foundation of support.\nThe percentage of parents who feel financially responsible for supporting their adult children has declined, while more are establishing specific conditions for continued assistance. Perhaps most notably, almost 40 percent of parents plan to end their financial support within the next two years.\nDespite these subtle shifts away from unconditional assistance, our survey essentially confirms what we've seen in recent years: the ongoing need to financially support struggling adult children is placing significant strain on many parents' financial security. This concerning pattern may face additional pressure if economic conditions worsen in the coming months. We'll examine how these trends evolve in our 2026 report.\nOur Data\nWe surveyed 1,001 U.S. parents of adult children to determine whether they cover some of their kids' bills or provide other financial support.\nOur survey was conducted online in February 2024. Participants were 50 percent men and 50 percent women. The median age among respondents was 56, and the median income was between $50,000 and $74,999 annually. Income disparities were controlled for.\nOur data distinguishes between adult children with disabilities, which may make it more difficult for them to live independently, and adult children without disabilities. Our analysis focused on parents who contribute financially to their adult children who don’t have disabilities.",
    "summary": {
      "en": "Parents are increasingly providing financial support to their adult children, with the percentage reaching a three-year high. A recent survey found that half of parents with adult children offer regular assistance, averaging $1,474 per month, which is up 6% from last year. This trend, which began during the pandemic, has continued due to economic pressures faced by younger generations.\n\nKey findings from the survey include:\n- 83% of parents help with groceries, 65% assist with cell phone bills, and nearly half cover vacation costs.\n- 77% of parents impose conditions on their financial aid, aiming to encourage independence.\n- Almost 50% of parents feel they are sacrificing their own financial security to support their kids.\n\nParents of Generation Z (ages 18-28) receive more support than Millennials (ages 29-44) as they are often still in school or early in their careers. While many parents are willing to make sacrifices, such as living more frugally or delaying retirement, about 40% of them plan to cut off financial support within the next two years.\n\nOverall, while many parents remain committed to helping their adult children, there is a growing recognition of the need for boundaries and the importance of fostering financial independence. The survey suggests that ongoing economic uncertainties may further impact these dynamics.",
      "ko": "부모들이 성인 자녀에게 재정 지원을 제공하는 비율이 증가하고 있으며, 이는 3년 만에 최고치를 기록했습니다. 최근 조사에 따르면, 성인 자녀를 둔 부모의 절반이 매달 평균 1,474달러를 정기적으로 지원하고 있으며, 이는 지난해보다 6% 증가한 수치입니다. 이 추세는 팬데믹 동안 시작되었으며, 젊은 세대가 겪고 있는 경제적 압박으로 인해 계속되고 있습니다.\n\n조사의 주요 결과로는 83%의 부모가 식료품 비용을 지원하고, 65%가 휴대전화 요금을 도와주며, 거의 절반이 휴가 비용을 부담하고 있다는 점이 있습니다. 77%의 부모는 자녀의 독립성을 촉진하기 위해 재정 지원에 조건을 붙이고 있으며, 거의 50%는 자녀를 지원하기 위해 자신의 재정적 안전을 희생하고 있다고 느끼고 있습니다.\n\nZ세대(18-28세)의 부모는 밀레니얼 세대(29-44세)의 부모보다 더 많은 지원을 받고 있습니다. 이는 Z세대가 학교에 다니거나 경력 초기 단계에 있기 때문입니다. 많은 부모들이 더 검소하게 생활하거나 은퇴를 미루는 등의 희생을 감수할 의향이 있지만, 약 40%는 향후 2년 내에 재정 지원을 중단할 계획이라고 밝혔습니다.\n\n전반적으로 많은 부모들이 성인 자녀를 돕겠다는 의지를 가지고 있지만, 경계 설정의 필요성과 재정적 독립성을 키우는 것의 중요성에 대한 인식이 커지고 있습니다. 이번 조사는 지속적인 경제적 불확실성이 이러한 동태에 더 영향을 미칠 수 있음을 시사합니다.",
      "ja": null
    }
  },
  {
    "id": "2f1afeb135a718da",
    "title": {
      "en": "The Great Chatbot Debate",
      "ko": "챗봇 대격돌",
      "ja": null
    },
    "type": "story",
    "url": "https://computerhistory.org/events/great-chatbot-debate/",
    "score": 14,
    "by": "rbanffy",
    "time": 1742894549,
    "content": "Home  Events\n            The Great Chatbot Debate\n\n                          Do LLMs Really Understand?",
    "summary": {
      "en": "The Great Chatbot Debate is an event focused on whether large language models (LLMs) truly understand language. It explores the capabilities and limitations of these AI systems in comprehending and generating human-like responses.",
      "ko": "대규모 챗봇 토론회는 대형 언어 모델(LLM)이 실제로 언어를 이해하는지에 대한 논의에 초점을 맞춘 행사입니다. 이 행사에서는 이러한 인공지능 시스템이 인간과 유사한 반응을 이해하고 생성하는 데 있어 어떤 능력과 한계를 가지고 있는지를 탐구합니다.",
      "ja": null
    }
  },
  {
    "id": "b14fe2b0ae814586",
    "title": {
      "en": "My team loved to ship fast and sink later",
      "ko": "빠른 출발, 나중에 침몰!",
      "ja": null
    },
    "type": "story",
    "url": "https://rosesecurity.dev/blog/2025/03/26/rushing-toward-rewrite",
    "score": 9,
    "by": "r0nan",
    "time": 1743182467,
    "content": "Rushing Toward RewriteMarch 26, 2025 · 2 min readThis is part three of my microblog series exploring the subtle dysfunctions that plague engineering organizations. After discussing over-abstraction as a liability and unpacking how excessive toil kills engineering teams, this post tackles a nuanced threat: when “moving fast” becomes a cultural shortcut for cutting corners.\nMove Fast and Don’t Break Everything\nA former CEO of mine used to say: “Be fast or be perfect. And since no one’s perfect, you better be fast.” Sounds cool until that motto becomes a shield to skip due diligence, code reviews, and even basic security hygiene. Speed wasn’t a value—it was an excuse. PRs rushed. On-call flaring. Postmortems piling. And still, engineers asking for admin access “to move fast.”\nSpoiler: they didn’t need it.\nThe deeper problem? We weren’t a scrappy startup anymore—we were operating at enterprise scale with a startup mindset. The cost of speed was technical debt, fragility, and a long tail of rework. When I transitioned to a new role (back in startup mode) I heard the same “move fast” mantra. But this time, it hit differently. Because here’s the truth: moving fast is possible without setting your future self on fire.\nHere’s what I’ve learned:\n1. Fail fast—but fail forward. Don’t just throw things at prod and hope they stick. Structure your failures. If a solution’s not viable, surface that early with data and a path forward. Good failure leaves breadcrumbs for the next iteration.\n2. Build for iteration. Forget perfect. Aim for clear next steps. Your v1 should be designed with a roadmap in mind. Where will this evolve? What trade-offs are you making? Ship it—but know how you’ll ship it better.\n3. Stay modular. Design with exits. If your observability pipeline starts with a pricey SaaS, fine. But make it swappable. Keep your vendor coupling thin so you can self-host later without a complete rewrite.\n4. Be honest about scale. What worked for a team of 10 won’t work at 100. “Move fast” looks different when customers depend on your uptime. Match your velocity with the blast radius of your decisions.\nWe glamorize speed, but the smartest teams know when to slow down, breathe, and make thoughtful decisions that stand the test of time. Move fast—but don’t break the foundation.Edit this page",
    "summary": {
      "en": "The text discusses issues in engineering organizations related to the mindset of \"moving fast.\" The author reflects on their experience with a previous CEO's motto: \"Be fast or be perfect,\" which led to cutting corners and neglecting important processes like code reviews and security. The result was technical debt and fragile systems.\n\nKey takeaways include:\n\n1. **Fail Fast, Fail Forward**: Learn from failures by analyzing them and providing a path for improvement, rather than just rushing without understanding the consequences.\n2. **Build for Iteration**: Focus on clear next steps, planning for future improvements rather than seeking perfection in initial versions.\n3. **Stay Modular**: Design systems to be flexible and easy to change, allowing for updates without extensive rewrites.\n4. **Be Honest About Scale**: Recognize that strategies that worked for small teams may not suit larger ones, and adjust the pace accordingly.\n\nThe overall message is that while speed is important, thoughtful decision-making is crucial for long-term success.",
      "ko": "이 글에서는 \"빠르게 움직이기\"라는 사고방식과 관련된 엔지니어링 조직의 문제를 다룹니다. 저자는 이전 CEO의 모토인 \"빠르거나 완벽하라\"는 말에 대해 회상하며, 이로 인해 중요한 과정인 코드 리뷰와 보안을 소홀히 하게 되었고, 결국 기술 부채와 취약한 시스템이 생겼다고 설명합니다.\n\n주요 내용은 다음과 같습니다. 첫째, \"빠르게 실패하고 앞으로 나아가라\"는 원칙입니다. 실패를 단순히 피하는 것이 아니라, 실패를 분석하고 개선할 수 있는 길을 제시해야 합니다. 둘째, \"반복을 위한 구축\"입니다. 초기 버전에서 완벽함을 추구하기보다는 명확한 다음 단계를 설정하고 미래의 개선을 계획하는 것이 중요합니다. 셋째, \"모듈화 유지\"입니다. 시스템을 유연하고 쉽게 변경할 수 있도록 설계하여, 대규모 수정 없이도 업데이트가 가능해야 합니다. 넷째, \"규모에 대한 솔직함\"입니다. 작은 팀에서 효과적이었던 전략이 큰 팀에는 적합하지 않을 수 있으므로, 그에 맞춰 속도를 조절해야 합니다.\n\n결국, 속도도 중요하지만 장기적인 성공을 위해서는 신중한 의사결정이 필수적이라는 메시지를 전달합니다.",
      "ja": null
    }
  },
  {
    "id": "1660dc158bf5387e",
    "title": {
      "en": "Plasmonic Modulators Break Wireless Terahertz Barrier",
      "ko": "플라스몬 모듈레이터, 테라헤르츠 장벽 돌파!",
      "ja": null
    },
    "type": "story",
    "url": "https://spectrum.ieee.org/terahertz-waves-2671362433",
    "score": 24,
    "by": "rbanffy",
    "time": 1742894904,
    "content": "NewsTelecommunications\n        Plasmonic Modulators Can Break the Wireless Terahertz Barrier\n    The tech could find a home in 6G networks and AI data centersMatthew S. Smith24 Mar 20254 min readMatthew S. Smith is a contributing editor for IEEE Spectrum and the former lead reviews editor at Digital Trends.A new plasmonic modulator [in gold] transfers signal information from an electrical wave to an optical wave at higher speeds than other modulator technologies.\n        Johannes Grewer/Polariton Technologies\n\n    {\"customDimensions\": {\"5\":\"Matthew S. Smith\",\"11\":2671362433,\"7\":\"optical fiber, 5g, 6g, wireless communications, plasmonics\",\"10\":\"wireless communications\",\"6\":\"news\",\"8\":\"03/24/2025\"}, \"post\": {\"id\": 2671362433, \"providerId\": 0, \"sections\": [497728257, 544169523, 550122300, 2267926519, 544169516, 539622110, 550122302], \"authors\": [22900948], \"tags\": [\"optical fiber\", \"5g\", \"6g\", \"wireless communications\", \"plasmonics\"], \"streams\": [], \"split_testing\": {}} }",
    "summary": {
      "en": "Researchers have developed a new plasmonic modulator that can convert electrical signals to optical signals faster than existing technologies. This advancement could be beneficial for future 6G networks and AI data centers. The modulator uses gold and represents a significant step forward in wireless communication technology.",
      "ko": "연구자들이 기존 기술보다 전기 신호를 광 신호로 더 빠르게 변환할 수 있는 새로운 플라스몬 모듈레이터를 개발했습니다. 이 발전은 미래의 6G 네트워크와 인공지능 데이터 센터에 유용할 수 있습니다. 이 모듈레이터는 금을 사용하며, 무선 통신 기술에서 중요한 진전을 나타냅니다.",
      "ja": null
    }
  },
  {
    "id": "c66679f23c539bad",
    "title": {
      "en": "Emacs Solo: A Surprise System Crafters Live Demo",
      "ko": "엠악스 솔로: 깜짝 시연!",
      "ja": null
    },
    "type": "story",
    "url": "https://www.rahuljuliato.com/posts/emacs-solo-demo",
    "score": 102,
    "by": "JNRowe",
    "time": 1743112983,
    "content": "Emacs Solo: A Surprise System Crafters Live DemoRahul M. JuliatoRahul M. JuliatoMarch\t27, 2025#emacs#systemcrafters# demoLast Friday, I was genuinely surprised by a live demo of my Emacs\nSolo configuration on the System Crafters Weekly Show. Watching\nthe live demo was an eye-opener, as I hadn't expected the project to\nget such attention, especially in a live setting. Seeing David\nWilson take a deep dive into the setup, testing the configuration\nlive, and exploring how powerful Emacs can be with only its built-in\npackages was both humbling and inspiring.\nFor more details and to explore the configuration yourself, visit the\nEmacs Solo GitHub\nrepository.\nThe Emacs Solo configuration is all about returning to the roots\nof Emacs. It's a minimalist setup designed to challenge myself and\ntest the full potential of Emacs using only its built-in\nfunctionality. The goal was to create an efficient, yet fully\nfunctional environment, all while keeping things as light and fast as\npossible. No external dependencies, no clutter. Just pure,\nunadulterated Emacs.\nThe Project: Emacs Solo\nEmacs Solo is a configuration that embraces the power of Emacs\nwithout relying on external packages. It's a setup I go back to from\ntime to time to remind myself of how much can be accomplished with\njust what Emacs offers out of the box.\nThis configuration is designed to be both powerful and lightweight,\nallowing for a fast, efficient workflow with a focus on simplicity and\nminimalism. The project includes several useful features for\nday-to-day tasks like searching, editing, and navigating—everything\nyou need for an efficient Emacs experience.\nSome of the highlights of the project include:\n» A preview of icomplete-verical enhancements I proposed to the\nEmacs core team (custom prefixes, vertico style setup, and inline\ncompletion closer to corfu/company that works on text buffers and\neshell).\n» An experimental custom git-gutter-like feature.\n» Supercharged eshell customization.\n» Custom solutions for editing multiple search entries.\n» Built-in news readers like Gnus and Newsticker.\n» Advanced file diffing and version control.\n» Extended viper mode for those who prefer vim-style editing.\n» Tree-sitter modes.\n» LSP configurations.\n» Custom rainbown-mode like.\n» And many customizations of built-in packages.\nThe idea is that Emacs is already a powerful IDE, and with a bit of\nclever customization, it can be made into something even more\nstreamlined, adaptable, and effective without the need for external\npackages.\nWatch the Demo\nHere's the video of the live demo from the System Crafters Weekly\nShow:\n\n» Also available here\nConclusion\nI’d like to take this opportunity to thank David Wilson for the\namazing show and to the System Crafters community for their\ncontinued support and enthusiasm around Emacs. I also want to express\nmy gratitude to everyone who has contributed code that I’ve borrowed\nand learned from over the years. Particularly Gopar and\nProtesilaos. Without the shared knowledge and experience from\nthese fantastic people, the Emacs Solo project wouldn't have been\npossible.\nAs always, the beauty of Emacs lies in its community, and I'm grateful\nfor all the inspiration, contributions, and shared wisdom that make\nprojects like Emacs Solo come to life. Thank you to everyone who\ncontinues to inspire and teach me along the way.",
    "summary": {
      "en": "**Summary:**\n\nLast Friday, Rahul M. Juliato showcased his Emacs Solo configuration during a live demo on the System Crafters Weekly Show. He was pleasantly surprised by the attention the project received. The Emacs Solo setup focuses on simplicity and efficiency, using only Emacs' built-in features without any external packages. This minimalist approach aims to highlight Emacs' capabilities and create a fast, functional workspace.\n\nKey features of Emacs Solo include:\n- Various enhancements and customizations for improved functionality.\n- Advanced editing and navigation tools.\n- Built-in news readers and version control options.\n- Custom modes that cater to different editing styles.\n\nThe goal of the project is to demonstrate that Emacs can be a powerful IDE with clever configurations, all while remaining lightweight. Juliato expressed his gratitude to David Wilson and the System Crafters community for their support, as well as to contributors who helped him along the way.",
      "ko": "지난 금요일, Rahul M. Juliato는 System Crafters Weekly Show에서 Emacs Solo 설정을 라이브로 시연했습니다. 그는 이 프로젝트에 대한 관심이 예상보다 높아 놀랐습니다. Emacs Solo 설정은 단순성과 효율성에 중점을 두고, 외부 패키지 없이 Emacs의 기본 기능만을 사용합니다. 이러한 미니멀한 접근 방식은 Emacs의 능력을 강조하고 빠르고 기능적인 작업 공간을 만드는 것을 목표로 합니다.\n\nEmacs Solo의 주요 특징으로는 다양한 기능 향상과 사용자 맞춤 설정이 있습니다. 고급 편집 및 탐색 도구가 포함되어 있으며, 내장된 뉴스 리더와 버전 관리 옵션도 제공합니다. 또한, 다양한 편집 스타일에 맞춘 사용자 정의 모드도 지원합니다.\n\n이 프로젝트의 목표는 Emacs가 똑똑한 설정을 통해 강력한 통합 개발 환경(IDE)이 될 수 있음을 보여주는 것입니다. Juliato는 David Wilson과 System Crafters 커뮤니티, 그리고 그를 도와준 기여자들에게 감사의 뜻을 전했습니다.",
      "ja": null
    }
  },
  {
    "id": "80e0206c30be9f7f",
    "title": {
      "en": "Show HN: We are building the next DocuSign",
      "ko": "다음 도큐사인, 시작합니다!",
      "ja": null
    },
    "type": "story",
    "url": "https://sgnly.com",
    "score": 64,
    "by": "esaidm",
    "time": 1743108702,
    "content": "Redefining Document SigningTurn PDFs toContract Templates in secondsSimple • Error-Free • Ready to Use5x faster document workflows — AI that auto-fills, explains, and builds reusable templates in seconds.Get Early AccessNo credit card required14-day free trialCancel anytime\n\nSave Time & MoneyWhy You Should Go With SgnlyTurn any document into a smart template, auto-filled with known data, explained by a voice agent, and personalized to every signer.Save On Processing TimeWith Sgnly, 80% of your document workflows are automated, so you can process more documents without expanding your team.An AI Assistant That Self-OptimizesSgnly isn't your average AI assistant. Not only does it automate tasks, it also learns from your feedback over time, just like a real human would.Get Up and Running Within A WeekLearning to use Sgnly takes minutes - and with our dedicated support team's help, you'll be onboarded and live within days!Replicate Your Top PerformersUsing our AI Playbooks, we can work with you to automate your top-performing document processes & writing workflows.All-In-One SubscriptionFrom email deliverability to B2B data, we've got it all. We've built best-in-class products for the entire document cycle, all in one place.Intent-Driven Document CreationHarness the power of behavioral, firmographic, and technographic data to create documents tailored to specific customer needs. Traditional Document ProcessManual Document ProcessingSlow, error-prone document handling with manual inputTime WastedHours spent completing complex formsManual DocumentationManual data entry for each documentNo AutomationRepetitive processes without reusable templates Sgnly Smart Solution Service Agreement Legal ContractSgnly Document Assistant - Service AgreementAI-poweredAgreement for {{company_name}}Date: {{current_date}}Dear {{customer_name}},This agreement confirms our services at the rate of {{hourly_rate}} per hour, with an estimated total of {{total_amount}}.Payment is due within {{payment_terms}} days of receipt.Sincerely,{{sender_name}}AI SuggestionAdd payment instructions to improve clarityVariables auto-filled from your data GenerateAI AssistantExplains complex terms and guides throughout the processSmart AutocompleteAutomatically fills in with saved informationAutomatic GenerationCreates customized documents in secondsTry Sgnly Now\n\nReady to transform your document workflow?Join thousands of businesses using SgnlySave time with AI-powered templatesSecure document managementEasy integration with existing systemsUnlimited electronic signaturesFull NameWork EmailCountry CodeUnited States (+1)United States (+1)United Kingdom (+44)Canada (+1)Australia (+61)Germany (+49)France (+33)Spain (+34)Italy (+39)Japan (+81)China (+86)India (+91)Brazil (+55)Mexico (+52)Russia (+7)South Korea (+82)Chile (+56)Phone NumberCompanyCurrent E-Signature SoftwareSelect your current e-signature solutionDocuSignAdobe SignHelloSignDropbox SignPandaDocSignNowSigneasyNo software currentlyOtherPrimary Use CaseRequest AccessBy submitting, you agree to our Terms of Service and Privacy Policy. Your information may be used to contact you about our products and services.",
    "summary": {
      "en": "**Summary of Sgnly Document Signing Solution**\n\nSgnly offers a fast and efficient way to handle document signing by turning PDFs into smart contract templates in seconds. Key features include:\n\n- **Speed and Efficiency**: Document workflows are five times faster, with 80% of processes automated.\n- **AI Assistance**: An AI assistant helps auto-fill documents, explains terms, and learns from user feedback.\n- **Easy Setup**: Users can get started quickly, often within a week, with dedicated support available.\n- **Customization**: Documents can be tailored to individual customer needs using various data types.\n- **All-in-One Solution**: Sgnly combines multiple document management functions into a single subscription service.\n- **Free Trial**: Users can try Sgnly for 14 days without needing a credit card and can cancel anytime.\n\nOverall, Sgnly aims to save businesses time and reduce errors in document processing.",
      "ko": "Sgnly는 PDF 파일을 몇 초 만에 스마트 계약 템플릿으로 변환하여 문서 서명을 빠르고 효율적으로 처리하는 방법을 제공합니다. 주요 특징으로는 문서 작업 흐름이 다섯 배 빨라지고, 80%의 과정이 자동화된다는 점이 있습니다. \n\nAI 보조 기능이 있어 문서 자동 작성, 용어 설명, 사용자 피드백 학습을 지원합니다. 사용자는 전용 지원을 통해 빠르게 시작할 수 있으며, 보통 일주일 이내에 설정이 가능합니다. \n\n문서는 다양한 데이터 유형을 활용하여 개별 고객의 요구에 맞게 맞춤 설정할 수 있습니다. Sgnly는 여러 문서 관리 기능을 하나의 구독 서비스로 통합하여 제공합니다. \n\n사용자는 신용카드 없이 14일 동안 Sgnly를 무료로 체험할 수 있으며, 언제든지 취소할 수 있습니다. 전반적으로 Sgnly는 기업이 문서 처리에서 시간을 절약하고 오류를 줄이는 것을 목표로 하고 있습니다.",
      "ja": null
    }
  },
  {
    "id": "0d813d8915548310",
    "title": {
      "en": "The Unbearable Loudness of Chewing",
      "ko": "씹는 소음의 고통",
      "ja": null
    },
    "type": "story",
    "url": "https://asteriskmag.com/issues/09/the-unbearable-loudness-of-chewing/",
    "score": 77,
    "by": "k2enemy",
    "time": 1742906464,
    "content": "The Unbearable Loudness of Chewing\n\n\t\t\t\t\tJake Eaton\n\nWhy do some people find certain sounds intolerable? And why has it taken so long for scientists to get even a preliminary answer?\n\n\t\t\t\t\t\t\t\t\t\t\tIt’s a teenage rite of passage to explode into rage at your parents. While the usual outburst is sparked by some combination of hormones, insecurity, and authority issues, for me it was a popping sound in my father’s jaw. I first noticed it at the dinner table. Every time he took a bite, the disc of cartilage that cushioned his jawbone would slip out of place and snap back. Chew, click, chew, click. Like a drum, his mouth reverberated the sound, which changed in pitch each time he opened to take a bite. Layered beneath all of this was the wet percussion of normal chewing. The trio — jaw pop, meat squish, fork scraping teeth — became inescapable. And it drove into me, first through my chest, a surprising shock of affront and disgust that then suffused through my whole body. It was the first time I ever got scared that I wasn’t in control of what was inside my own head.\n\n    Jul Quanouai\n\n\t\t\t\t\t\t\t\t\t\t\tI can’t precisely date this memory, but I was somewhere around the age of 13 when I became unable to tolerate the sound of other people’s mouths. In a world in which everyone eats, my day to day became an obstacle course. I learned to contract the tensor tympani muscle in my middle ear to dampen sounds. In moments of silence, I was sensitive enough that even the subtle parting of lips could trigger in me the urge to flee. There wasn’t logic to what I felt. I knew that. It changed nothing. In my worst moments I started fights, especially with my family, among whom the condition, whatever it was, felt orders of magnitude more severe.I learned strategies to hide my aversion. Restaurants and the school cafeteria were loud enough to mask eating sounds. You couldn’t eat in my car — it was, I said, just a cleanliness thing. I wore headphones everywhere. Still, I felt the consequences. I became more introverted. My family resigned to eating dinner with the TV on. My high school girlfriend called me out on what had become my habitual death stare when we ate near each other. I fell out with my best friend after I questioned his family’s table manners. I spent hours alone trying to integrate whatever was happening as part of me, hoping I had a wire crossed, afraid that I was experiencing — there was nothing online to help me — an incipient slide into mental illness.And then in 2011, my first year out of college, I happened upon a piece in the New York Times which described a poorly understood condition some scientists had begun to call misophonia.\n\n        1\n\n For its sufferers, certain sounds — often chewing — evoked what seemed an uncontrollable and disproportionate reaction of anger and disgust. It consistently appeared in adolescence. It had no cure. It was, or at least one neuroscientist hypothesized, a hard-wired condition, “a ‘physiological abnormality’ that resides in brain structures activated by processed sound.”Awash in the relief of diagnosis — not vindication so much as no longer feeling alone — the first thing I did was forward the article to my family. “So please be sensitive toward my ‘physiological abnormality’ when I come home,” I wrote, shielding my sincerity in irony.13 years later, I feel much lessimpacted by misophonia. My family knows what it is, even if they don’t fully understand it. In this, they aren’t much different than the general public. But it’s only after spending a few months reading and speaking to researchers that I’ve come to realize that the science of misophonia — at least in what we can say with certainty — isn’t well established either.It has taken, in fact, more than 20 years since the condition was first recognized to even arrive at a definition. And outside of that, some of the most basic questions remain up for debate. Where does misophonia originate, and why? Is it a medical disorder, a neurological disorder, a psychiatric disorder, or something else entirely? Most important of all — at least to the millions of people who likely suffer from it, many without fully understanding what it is — can it be cured?\n\t\t\t\t\t\t\t\t\t\t\tStarting conditions\n\n\t\t\t\t\t\t\t\t\t\t\tIn 1983, Pawel Jastreboff, a Polish engineer-turned-neuroscientist, began studying tinnitus. One of his main contributions from his early research was an animal model of the condition. It was a clever experiment that built on the work of his Ph.D. advisor, Jerzy Konorski, who developed the idea of operant conditioning after working in Ivan Pavlov’s lab.Jastreboff raised rats to live with a constant, high-pitched ringing sound. They were then trained to expect a mild electric shock when the sound shut off. Next, he gave the rats salicylate (aspirin), which is known to cause ringing in the ears. And then he watched how the rats behaved when he shut off the tone. Rather than behave as if it was silent and a shock was coming, they acted normally. It appeared they were still hearing a ringing. While the model produces only temporary tinnitus, it allowed researchers to validate that animals could be trained to respond to internally perceived sounds, establishing one of the first behavioral methods to measure tinnitus in animals.Jastreboff also developed a proprietary treatment, “tinnitus retraining therapy,” which drew, as did the animal experiments, on his belief that the condition wasn’t auditory in nature. It consisted of a combination of counseling and sound therapy that promised to “reclassify tinnitus into the category of neutral stimuli.” In other words: to counter-condition the signal. He soon took a faculty position at the University of Maryland that came with a clinic to further his study, where he also saw patients with another sound tolerance disorder called hyperacusis, a rare condition affecting about one in 50,000 people in which noises are perceived as unbearably loud. (The most common cause is damage to the cochlea, but it is more prevalent in those with tinnitus than it is in the general population.)To meet increased patient demand, Jastreboff’s wife, Margaret, a molecular biologist and pharmacologist, joined him in the clinic. And it was Margaret who noticed something Pawel had not. Some patients were intolerant of sounds, but their symptoms fit the profile of neither tinnitus nor hyperacusis. Several hundred out of a thousand patients appeared to be afflicted by an intolerance to specific sounds, with specific origins.“I was very skeptical,” Pawel said, “but she convinced me.” And so he came to understand that every patient was a little different. They were triggered by a variety of noises: the drone of an airplane 30,000 feet above, a refrigerator hum, or the clacking of a keyboard. One patient could not bear the sound of his parents’ voices. “Everybody around him had to speak in a whisper. When I met him outside of the clinic, he had to carry a sound generator set at such a high level that I was able to hear it from ten feet away.”The way these patients reacted to noise was different from those suffering from hyperacusis or tinnitus. They had, as Jastreboff euphemistically put it in the first published piece to use the word misophonia, “a negative attitude to sound,” which seemed to activate their fight-or-flight response.\n\n        2\n\n The next year, the Jastreboffs published the first peer-reviewed article to include the term “misophonia” in the Australian and New Zealand Journal of Audiology. It detailed, they claimed,\n\n        3\n\n how tinnitus retraining therapy could alleviate both tinnitus and misophonia.For ten years, it was met by silence.\n\t\t\t\t\t\t\t\t\t\t\tQuiet\n\n\t\t\t\t\t\t\t\t\t\t\tI wish I’d seen that paper when it first came out. I was 14 at the time, and I would wait another nine years to learn the term. I’m sure others feel the same — today, for example, the r/misophonia subreddit has 78,000 members. But between 2002 and 2013, misophonia appeared in journals just 15 times, mostly in case reports, none of which made meaningful advances in the science. (One, “Fear of the Yawning Mother: A Case Study of Misophonia,” was retracted due to a legal dispute.)Why does it fail to gain any traction? I can think of a few reasons. The first is that the Jastreboffs are clinicians. Their published work relies on their private practice, emphasizing observation over experiment. Nor did the Jastreboffs ever sell the concept: Misophonia doesn’t even appear in most of their article titles.\n\n        4\n\n And so it’s not surprising that, pre-social media, burying a newly coined disorder in the text of an obscure Antipodean audiology journal (impact factor 0.4) is akin — as it were — to shouting into the void.But I think a second, larger reason is that misophonia is weird. It’s hard to explain and difficult to understand and doesn’t make any intuitive sense. After all, everyone hates the sound of chewing.\n\n        5\n\n What appears to be a deep aversive reaction to sounds that most people don’t like can seem — I speak from personal experience here — like you’re hard to tolerate. It’s easier to be skeptical than empathetic. “Even now, when patients don’t know about misophonia, their friends and family are often quick to dismiss their complaints as them simply being neurotic or over-dramatic,” said Zach Williams, a neuroscientist Ph.D. and M.D. candidate at Vanderbilt who has studied misophonia and decreased sound tolerance.It doesn’t help that we don’t have clear historical precedents of the condition, the way we see, for example, signs of what we now call PTSD in writing from the First World War. At least so I thought. Misophonia researchers are quick to cite possible cases. Winston Churchill held a lifelong aversion to whistling. Churchill’s bodyguard, in his memoir, recounts England’s PM admonishing a boy for trilling while walking down the street. The man posted signs in his war room bunkers that read “There is to be no whistling or unnecessary noise in this passage.” Then there is Proust lining his room with cork and Kafka’s fixation on the din of his house in “Great Noise.” What we accept as the anality of the great artist might start to appear as something else entirely.Scientists were also skeptical of Jastreboff’s research when they did encounter it. Jastreboff spoke of being treated as if he’d conjured a condition out of thin air. “You’re totally ignored, you do not exist,” he said, “or you are just talking garbage.” That began to change, ten years later, with an article in the New York Times — the same from my eureka moment — titled “When a Chomp or a Slurp Is a Trigger for Outrage.” The month the story came out, the Google Trends for misophonia erupted into existence. It’s only grown since. It tends to periodically spike for news stories or new studies. (Its second spike occurs in May 2012, when ABC’s 20/20 aired a somewhat sensational segment about a daughter who couldn’t even talk to her mom; it also featured Kelly Ripa, who self-diagnosed with misophonia after reading the Times article.)Finally, in 2013, misophonia found real traction among scientists. According to Jastreboff, there was a third challenge in the early research breaking through. “Audiologists don’t write that much because it distracts them from other things — mainly making money with patients.” It was only a matter of time until psychologists (who, to be clear, also make money with patients) noticed the condition. “And psychologists,” he said, “like to write a lot.”\n\t\t\t\t\t\t\t\t\t\t\tNoise\n\n\t\t\t\t\t\t\t\t\t\t\t“Some patients report a preoccupation with a specific aversive human sound that triggers impulsive aggression,” reads the abstract to a 2013 paper in PLOS ONE. “This condition is relatively unknown and has hitherto never been described, although the phenomenon has anecdotally been named misophonia.”\n\n        6\n\n Led by Arjan Schröder, a psychiatrist at the University of Amsterdam, the article did more than describe the condition — its subtitle explicitly labeled it: “Misophonia: A New Psychiatric Disorder.”In broad strokes, it gets the descriptive aspect right: The average age of onset (around 13); the specificity of triggers; the usual response of irritability, disgust, and anger are all now well-validated. But it begins, literally, with a case of selection bias: “Three patients were referred to our expertise centre in obsessive-compulsive disorders (OCD) at the Academic Medical Center in Amsterdam with obsessions focused on a typical sound such as smacking or breathing and the subsequent aggressive impulse to scream and yell or attack the source of the sound in order to make it stop.”Of the 42 participants (all of whom self-selected into an OCD clinic), 22 met the diagnostic criteria for obsessive-compulsive personality disorder. To boot, the scale the authors created to measure the severity of misophonia symptoms, the A-MISO-S, was adapted from a scale used to measure the severity of OCD.While Schröder et al. are careful to note that misophonia is distinct from any disorders recognized in the DSM, they also suggest it “should be considered as an obsessive-compulsive spectrum disorder” — a loosely grouped range of conditions that most often includes body dysmorphia, hypochondriasis, tic disorders like Tourette syndrome, and body-focused repetitive disorders such as trichotillomania, in which individuals pull out their own hair.Figuratively, eating sounds have made me want to pull out my hair. But the idea that misophonia belongs on the OCD spectrum ultimately proved premature. Seven years later, research from the same group found that the comorbidity between misophonia and OCPD traits was lower than that first research: only ~25%. A 2023 paper, designed to tease out the relationship between OCPD and misophonia, found that misophonia isn’t correlated with having a diagnosis. It’s just that misophonia kind of looks like OCPD. I have not come across other research that validates the obsessive-compulsive spectrum placement.The Schröder paper deserves credit for igniting more formal research into misophonia. But it is also emblematic of the way that first decade of misophonia science often appeared to grasp for scientific purchase at a range of disorders and diagnoses that could serve as a template. One paper, based on just three case studies, suggests that misophonia may be associated with the presentation of eating disorders. Another concluded that “misophonia is better described as a symptom of a) obsessive-compulsive disorder, b) generalized anxiety disorder, and c) schizotypal personality disorder.” It, too, was based on just three case studies.“The early definitions of misophonia are a bit like Rorschach inkblots,” said Zach Rosenthal, associate professor in psychiatry and behavioral sciences and the director of the Duke Center for Misophonia and Emotion Regulation.\n\n        7\n\n “What you see is based on your own expectations and biases.”Ideally, science works as a cooperative, knowledge-sharing enterprise in well-coordinated pursuit of the truth. And it can — but sufficient funding is required to foster enough competition that errors and misdirections get corrected quickly enough. Through most of the 2010s, that is not the case for misophonia.This is a tricky recipe when it intersects with the media. For example, in 2017, Sukhbinder Kumar et al. published fMRI results that shone preliminary light on misophonia’s holy grail: the underlying mechanism. In misophonics, trigger sounds cause hyperactivity in the anterior insular cortex and other parts of the brain responsible for processing and regulating emotions. The authors are careful to conclude that it’s impossible to tell whether this is a cause or consequence of misophonia, but the CBC went on to declare misophonia “the result of a misfire in the brain.” The headline of the New York Times coverage: “Misophonia Sufferers: Scientists May Have Found the Root of Your Pain.”\n\n        8\n\nPart of the problem is that, in the early years, it was hard to find an unbiased and large sample to say anything confident about comorbidities in misophonia. So many of the early papers were drawn from some form of psychiatric clinic or private practice that it’s impossible to say much about misophonia in the general population. In 2017, a sample of 301 qualified as a “large-scale” study of misophonia. That study found that 50% of their sample appeared to have no comorbidities at all, while the other 50% were affected by a variety of conditions, only one of which — PTSD — helped to explain misophonia symptom severity.\n\n        9\n\nThere is also the challenge of measuring prevalence without validated scales. The earliest estimates are based on the perennially most convenient population: undergrads. A 2014 paper reported the incidence in the United States to be 20%. A 2017 paper among Chinese students pegged it at 6%. Neither of these studies uses the same scale, nor are those which they use validated. (Remember: We didn’t even get a consensus definition of misophonia until 2022.)“The science is flawed. Nobody wants to hear it, but it is,” Rosenthal told me over Zoom. There’s few studies with representative or large samples; we know less about men and boys; almost no research has been done outside of WEIRD contexts. “Everything needs to be held carefully in terms of what it is and what we know it is from a scientific perspective.” Rosenthal was sitting in his basement, but his background was sunset on the Green River in Utah, where he had just been on a camping trip. I’d asked for an hour of his time; we ended up speaking for two, and I got the sense he was the sort of person to extend the same generosity to his patients — and to the nascent misophonia research community.Which in 2019 began to coalesce.\n\t\t\t\t\t\t\t\t\t\t\tSignal\n\n\t\t\t\t\t\t\t\t\t\t\tThe turning point is the injection of meaningful research funding. In 2019, the philanthropists Steve and Diane Miller created the Misophonia Research Fund. (The Millers’ daughter was diagnosed with misophonia in 2016.) “They have single-handedly injected $12 million dollars into misophonia research to date and are essentially the sole funder of misophonia research at this time,” said Williams.\n\n        10\n\nResearch output on misophonia has tripled since 2019. And with it has come some stepping stones. There are now several better validated scales: the Duke Misophonia Questionnaire, developed by Rosenthal and colleagues; and the S-Five, developed by Jane Gregory and colleagues. Gregory had gotten interested after seeing misophonia patients in her own clinical practice. A key aspect of the S-Five is that it can better track clinical improvements within individual patients, which older scales had not. “My patients were saying, ‘I feel a lot better, but that’s not reflected in the change in scores,’” Gregory said. “Half the questionnaire was based on this section that essentially said, ‘Compared to other people, I am sensitive to these sounds.’” For a misophonia patient, that is almost always true — and so it was important to capture change.”\n\n        11\n\nIn July of 2024, Laura Dixon et al. published the largest survey to date (n = 4,005) which supports what has been intuitively obvious for some time: While a large proportion of people (78.5%) report sensitivity to trigger sounds, only a small proportion — 4.6% in this study — report clinical levels of misophonia.MRF now holds annual meetings, “the beginnings of what will probably be our professional society one day,” said Williams. And it funds, for the first time, a project to arrive at a consensus definition of misophonia. It was an intensive Delphi process ( a structured approach to expert consensus) conducted over eight months: 15 experts, 68 references that included definitions of misophonia, and four rounds of voting to arrive at:Misophonia is a disorder of decreased tolerance to specific sounds or stimuli associated with such sounds. These stimuli, known as “triggers,” are experienced as unpleasant or distressing and tend to evoke strong negative emotional, physiological, and behavioral responses that are not seen in most other people. Misophonic responses do not seem to be elicited by the loudness of auditory stimuli, but rather by the specific pattern or meaning to an individual.But outside of that definition, some of the most basic questions about misophonia remain. Specifically, is it psychological, neurological, audiological, or something else entirely?“The Committee concluded that the scientific evidence regarding whether or not to classify misophonia as a ‘medical’ or ‘psychiatric’ disorder is currently insufficient but that underlying organic etiology of the disorder cannot be ruled out.”The researchers I spoke with were similarly cautious on this front. “There’s not even anything reasonably in the proximity of definitive,” said Rosenthal. Or as Williams put it: “If someone tells you they know exactly what causes misophonia down to the molecular or brain pathway (which probably comes with their own patented cure that they conveniently own exclusive marketing rights to), I would consider those claims hyperbolic or not take them seriously at all.” Still, when I spoke with researchers, they were willing to hazard cautious, caveated answers to the question that I’ve most wanted answered since I was 13: Where is this coming from?Rosenthal favors a multilevel frame. There are biological vulnerabilities that we might think of as strong antennae, such as sensitivity to sensory input or sensitivity to emotions.\n\n        12\n\n Then there are environmental vulnerabilities: growing up in an environment of unpredictability, a chronic invalidation of one’s internal experiences, the need for hypervigilance in navigating one’s environment. Call this “walking on eggshells.” The interplay between those two factors translates into a hypersensitivity that, over time, locks onto a particular trigger — possibly one which you first heard at a moment you worried the eggshells were about to break.Gregory’s view is similar. Her book, Sounds Like Misophonia, a popular take on the science and guide to coping strategies, lays out specific examples of how misophonics learn to associate sounds with stressors: children eating at the dinner table during a period of family conflict; a child bullied near the basketball courts who comes to associate dribbling with social threat; a salaryman frustrated with his career progress who notices how loudly his co-worker taps on her keyboard. (One of my favorite pieces, “Did Kant Suffer from Misophonia?,” speculates that misophonia arises when common sounds violate internalized social norms, specifically Western table manners that dictate demureness when eating, thereby disrupting one’s sense of an orderly world.\n\n        13\n\n It’s somewhat compelling.) Dixon’s survey showed that over half of misophonics report eating sounds as their first trigger. If there’s truth in it, it’s that the dinner table — the place where happy and unhappy families alike gather — is a common site for conflict, including over table manners.Williams notes, however, that “whether misophonia traits and their specific manifestations (e.g., which triggers are most bothersome, how severe the reactions are, how one behaviorally responds, etc.) are learned over time remains an open question.” It’s likely that the avoidance of trigger sounds may be a secondary reaction of classical conditioning but that some of the automatic responses may not be learned in the typical way. “This is all still to be understood, though.”Some sort of neurological root — as the consensus definition notes — remains on the table. In 2021, Kumar’s lab published fMRI results that point to a motor basis for misophonia. Specifically, the brains of people with misophonia show heightened activation in areas that control facial movements when hearing triggers. The idea that misophonia is a misfiring of mirror neurons then got picked up in a range of outlets, but more recent results have contradicted those findings. Patient advocates seem to appreciate the concreteness of brain imaging studies: Here, see, misophonia is real; it’s in the brain. But fMRI is expensive, and both of these studies were small.Jastreboff remains adamant that the idea that misophonia is a “psychiatric disorder” is wrong. Much of the evidence pointing in that direction, he believes, is a result of selection bias in recruiting. Theoretically, “it is possible to turn everybody into a misophonia patient towards a specific sound.” His view remains that misophonia is a disorder of conditioned reflexes where the brain has formed strong functional connections between the auditory system and other brain systems (particularly the limbic and autonomic nervous systems).\n\t\t\t\t\t\t\t\t\t\t\tHarmony\n\n\t\t\t\t\t\t\t\t\t\t\tIn many ways, misophonia resembles what Scott Alexander has called a “trapped prior.” A belief is “trapped” when it has become so strong it causes you to interpret all new evidence — even contradictory evidence — in support of the belief. A phobia is a basic form of trapped prior. You’re asked to speak in front of the class and you mess up. You get made fun of. Your brain then associates public speaking with social danger, creating a self-reinforcing cycle of anxiety that impairs your speaking performance, which then confirms your belief that speaking is dangerous.More recent research on psychedelics and depression has provided some evidence for the trapped-priors framework. The theory goes: In depression, high-level priors like \"the world is fundamentally threatening\" or \"I am worthless\" become extremely rigid and self-reinforcing. Psychedelics appear to temporarily \"relax\" these high-level priors by reducing their influence over lower-level sensory processing.Under the predictive processing/trapped-prior framework, misophonia begins to look a little less strange: Your brain forms, for whatever reason, a negative association with specific sounds. Each new exposure to the trigger sound gets interpreted through the lens of that prior — the brain expects the sound to be unbearable, which causes a strong emotional and physiological reaction, which then reinforces the prior that the sound is unbearable. This self-reinforcing loop is difficult to break.The evidence for how it might be broken in misophonia, specifically, is thin. A 2023 systematic review of treatments found just one randomized controlled trial, one open label trial, and 31 case studies.\n\n        14\n\n That RCT, which used cognitive behavioral therapy techniques, found modest effects: 37% in the treatment group had clinical improvements compared with 0% in the control. The open-label trial, also CBT-based (and led by Schröder), worked for half. And some case studies suggest CBT can improve symptoms. Still, it’s early days.The fact that CBT-based techniques sometimes help with misophonia fits the trapped-prior model. By introducing trigger sounds in controlled, low-stress environments, it may allow the sensory \"bandwidth\" to stay open enough to update the prior — rather than having it override the experience. Some of Gregory’s techniques break the association altogether by, for example, imagining a different origin to the sound. She gave the example (from a workshop, not a patient) of a man bothered by the sound of kids bouncing on a trampoline next door. He appeared to find relief by listening to trampoline sounds while imagining not kids but his cat jumping up and down in joy.I tend to be self-deprecating about my own experience with misophonia. That is a coping strategy. It’s my way of saying: Heads-up, please forgive me, I know this is weird. I imagine a lot of misophonics feel the same. Pardon us for being crazy. We apologize for taking up space. But that experience is not everyone’s. The severity of my misophonia experience has diminished over time. (On Gregory’s S-Five, I score 48 out of 250, below the score of 87 that she uses as a cutoff for “significant” misophonia.) On a day-to-day basis I feel mostly unimpaired.Others suffer. Some deeply. The r/misophonia subreddit can be sad and desperate. “My trigger is the worst thing in my life,” “I've honestly felt the urge to punch my own wife,” “I fucking hate it with every fiber of my being,” “I have no friends, and it's often hard to even go outside.” A number of suicides have been traced to misophonia. At least on Reddit, some people report going to extreme lengths — including intentional deafness — to eliminate triggers.People need help. But this need runs up against practical barriers. There are few places to seek treatment. Misophonia remains poorly recognized. No one technique is yet proven. Official classification as a disorder could help funnel resources and attention, but there is some debate within the advocacy community about the utility of pursuing that. Those who are comfortable labeling misophonia a psychiatric disorder have no issue on this front. But “other people get really mad when that’s even on the table to consider,” Rosenthal said.The two main options are either Diagnostic and Statistical Manual of Mental Disorders or the International Classification of Diseases. Rosenthal thinks the ICD is the more appropriate place, particularly because it’s not yet clear whether misophonia is best classified as a mental disorder.“It’s undeniably a good thing to have misophonia in at least one manual,” Williams said. “[N]o code means no billing, and no billing means no reimbursement for services (not to mention less legitimacy in medico-legal contexts, advocacy, etc.).”\n\t\t\t\t\t\t\t\t\t\t\tModulation\n\n\t\t\t\t\t\t\t\t\t\t\tI had hoped to try some of the CBT-inspired approaches for this piece. Then I got busy. More honestly, I had a hard time overcoming the inertia to even start. I found myself caught in a familiar pattern: avoiding treatment for a condition that makes me avoid things.Instead, I had an unexpected opportunity: a week-long silent meditation retreat. Assuming misophonia is indeed like a trapped prior, a retreat could offer the space for processing sensory experience in a new light. Meditation, at its heart, is a way to drop all preconceptions and to see things as they are.Which is how I found myself five days into the deepest equanimity I’ve ever felt curiously observing my own reactivity manifest. The meditation hall was quiet but not silent, punctuated by the subtle sounds of thirty people settling into their seats. Every time the person next to me swallowed, I felt first a brief ripple of anger at the sound itself, followed by a larger wave of frustration at my reaction to it. Here I was exerting intensive focus on something I wasn’t supposed to be focusing on. I was shocked at the extent to which I felt — imagined I could even see — the sounds of other people swallowing around me as tiny darts into and through my chest.And yet, with time, as I settled, I became better able to notice the space between sensation and reaction. For the first time, I could choose to ignore the signal. To separate it as part of myself. Something happening in my awareness, but not necessarily to me. I could allow it to be, for once, just noise.There were times researching this piece when I found myself frustrated with the slow pace and sporadic nature of misophonia science. I was angry for my younger self at researchers jumping to conclusions, annoyed at the way misophonia seemed carelessly lumped in with whatever condition the author seemed to know best — back to trapped priors again.But this now seems unfair. For more than a decade, the field operated with tiny budgets and poor visibility. And I am hopeful that misophonia research is poised to experience something of an acceleration in the coming years. Brain science continues to advance every day. Early research from the Duke Center for Misophonia has shown that targeted brain stimulation combined with CBT techniques can help accelerate treatment. Psychedelics, theoretically, are another area to consider.These might all pan out to help in small to large ways. But the fruition of this research will take years, if not decades. Even then, it is unlikely we ever see a genuine “cure.” This remains a point of debate among some within the misophonia community. “There’s a lot of quackery and snake oil out there,” Williams said. “This is a context ripe for desperation. And people need to be protected from that.”I don’t think there is a cure. Our history treating other complex psychological conditions — from depression and anxiety to OCD and ADHD — suggests that while we may yet find more effective treatments, there is no silver bullet. Anyone who says differently may be selling you something. Living with misophonia may require something harder, the answer that was always obvious but many of us prefer not to confront: work.\n\n    Sign up for our newsletter to get Asterisk’s latest interviews, essays, and more.\n\n        Subscribe\n\n    Marsha Johnson, an audiologist in private practice in Oregon who trained with Jastreboff, noticed what we now call misophonia around the same time. In the late ’90s, she termed it “selective sound sensitivity syndrome.”\n\n                ↩\n\n    The Jastreboffs consulted a classics professor on the name, which literally translates to “hatred of sound.” Lots of people don’t like this name because neither “hatred” nor “sound” accurately characterize the condition.\n\n                ↩\n\n    The Jastreboffs have never submitted their treatment to any form of controlled trial, so I’m being deliberate, not cynical, here.\n\n                ↩\n\n    It is, plainly, “Decreased Sound Tolerance and Tinnitus Retraining Therapy (TRT).”\n\n                ↩\n\n    To be clear, misophonics are quite a bit more sensitive on this front. Most people dislike the sound of chewing, but at least one study has shown that many misophonics are triggered by sounds such as breathing and swallowing that don’t bother the general population.\n\n                ↩\n\n    The paper actually cites Jastreboff here as the “anecdotal” evidence.\n\n                ↩\n\n    Probably the largest center dedicated to the study of misophonia in the world. (It’s six people full time, plus grad students.)\n\n                ↩\n\n    One point of caution with respect to Kumar et al. — and this is argued by Schröder in a commentary — is that they recruited patients on the basis of an unvalidated questionnaire. Having looked at the questions, which are quite open-ended, I tend to think this is fine.\n\n                ↩\n\n    Tantalizingly, they also find that 50% of their sample reporting experiencing an “unfamiliar phenomenon called autonomous sensory meridian response (ASMR).” I’ve spoken with a few friends with misophonia, and they all report also enjoying ASMR. Weirdly, I’ve seen almost no follow up on this.\n\n                ↩\n\n    The Duke Misophonia Center is funded by an anonymous donor, and there are scattered smaller grants.\n\n                ↩\n\n    It’s still, however, a work in progress. As Gregory noted to me, a flaw in the S-Five is that it doesn’t differentiate between hyperacusis and misophonia. “We didn't have that in mind when we were designing it. We were working from the clinical experience we had. So that’s the next step of the research.”\n\n                ↩\n\n    Whether these are truly biological is an even harder question to answer.\n\n                ↩\n\n    Kant was apparently sensitive to a variety of sounds, including boats, prison singing, and, most notably, the crowing of a rooster — one which irritated him so much he tried to buy and kill it.\n\n                ↩\n\n    A few more RCTs are on the way. One seeks to compare acceptance and commitment therapy to progressive relaxation training. Another is testing the effectiveness of CBT combined with psychomotor therapy in children and adolescents.\n\n                ↩\n\nJake Eaton is the managing editor of Asterisk Magazine. He writes at anzalogue.xyz.\n\nPublished January 2025\n\n\t\tShare with email\n\n\t\tShare on Twitter\n\n\t\tShare on Facebook\n\n\t\tShare on LinkedIn\n\n\t\t\tHave something to say? Email us at letters@asteriskmag.com.\n\nPrevious\n\t\t\t\t\tA Defense of Weird Research\n\nFurther Reading\n\n\t\t\t\tMore:\n\t\t\t\t\t\t\t\t\thealth\n\t\t\t\t\t\t\t\t\tscience\n\n\t\t\t\t\t\tA Defense of Weird Research\n\n\t\t\t\t\t\t\t\t\tDeena Mousa\n\n\t\t\t\t\t\t\t\t\tLauren Gilbert\n\n\t\t\t\t\t\tAutomating Math\n\n\t\t\t\t\t\t\t\t\tAdam Marblestone\n\n\t\t\t\t\t\tGreening the Solar System\n\n\t\t\t\t\t\t\t\t\tEdwin Kite\n\n\t\t\t\t\t\t\t\t\tRobin Wordsworth\n\n\t\t\t\t\t\tThe Case for Insect Consciousness\n\n\t\t\t\t\t\t\t\t\tBob Fischer\n\n\t\t\t\t\t\tYes, Shrimp Matter\n\n\t\t\t\t\t\t\t\t\tAndrés Jiménez Zorrilla\n\nManage Consent PreferencesStrictly Necessary CookiesAlways ActiveThese cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.Performance Cookies  Performance Cookies These cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site. All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.Functional Cookies  Functional Cookies These cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages. If you do not allow these cookies then some or all of these services may not function properly.Targeting Cookies  Targeting Cookies These cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\n\nBack ButtonPerformance Cookies  Search IconFilter IconClear checkbox label labelApply CancelConsent Leg.Interest checkbox label label checkbox label label checkbox label label\n\nClear checkbox label labelApply Cancel\n\nConsent Leg.Interest checkbox label label checkbox label label checkbox label label",
    "summary": {
      "en": "### Summary of \"The Unbearable Loudness of Chewing\" by Jake Eaton\n\nMany people experience intense discomfort from certain sounds, particularly chewing, a condition known as misophonia. The author shares his personal struggle with this condition, which began in adolescence when he became unable to tolerate the sounds of his father's chewing. Misophonia can lead to feelings of anger and disgust, disrupting daily life and relationships.\n\nDespite its recognition for over 20 years, scientific understanding of misophonia remains limited. Researchers have debated its origins—whether it's a neurological, psychiatric, or sensory disorder. The term was first used in a 2001 study, but it gained more attention in recent years, particularly after a 2013 article identified it as a new psychiatric disorder.\n\nResearch on misophonia has increased since 2019 due to new funding, leading to better-defined questionnaires and increased awareness. A 2024 study found that while many people report sensitivity to triggering sounds, only a small percentage meet the criteria for clinical misophonia.\n\nThere is still no consensus on whether misophonia can be classified as a medical disorder. Current treatments, like cognitive-behavioral therapy (CBT), show promise but are not definitive. The author reflects on his own experience with misophonia and acknowledges the need for further research and understanding, emphasizing that living with this condition may require ongoing work and coping strategies rather than a simple cure.",
      "ko": "많은 사람들이 특정 소리, 특히 씹는 소리로 인해 심한 불편함을 느끼는 경우가 있습니다. 이를 미소포니아라고 부르며, 저자는 이 증상으로 인한 개인적인 고통을 이야기합니다. 그는 청소년 시절 아버지의 씹는 소리를 견딜 수 없게 되면서 이 문제가 시작되었다고 합니다. 미소포니아는 분노와 혐오감을 유발할 수 있으며, 일상 생활과 인간관계에 지장을 줄 수 있습니다.\n\n미소포니아는 20년 이상 알려져 왔지만, 과학적으로는 여전히 이해가 부족합니다. 연구자들은 이 증상의 원인이 신경학적, 정신적, 또는 감각적 장애인지에 대해 논의하고 있습니다. 이 용어는 2001년 연구에서 처음 사용되었지만, 2013년의 한 기사에서 새로운 정신 장애로 지목되면서 최근 몇 년 동안 더 많은 주목을 받게 되었습니다.\n\n2019년 이후 미소포니아에 대한 연구가 증가하였고, 새로운 자금 지원 덕분에 더 명확한 설문지가 개발되고 인식이 높아졌습니다. 2024년 연구에 따르면 많은 사람들이 유발 소리에 민감하다고 보고하지만, 임상 미소포니아 기준을 충족하는 사람은 소수에 불과하다고 합니다.\n\n미소포니아를 의학적 장애로 분류할 수 있는지에 대한 합의는 아직 이루어지지 않았습니다. 현재의 치료법인 인지 행동 치료(CBT)는 희망적인 결과를 보이지만, 확실한 해결책은 아닙니다. 저자는 자신의 미소포니아 경험을 반영하며, 추가 연구와 이해의 필요성을 강조합니다. 이 증상과 함께 살아가는 것은 단순한 치료보다는 지속적인 노력과 대처 전략이 필요할 수 있음을 인정합니다.",
      "ja": null
    }
  },
  {
    "id": "c3cc09f7c3386ef2",
    "title": {
      "en": "Clean, a formal verification DSL for ZK circuits in Lean4",
      "ko": "클린: Lean4의 ZK 회로 검증 DSL",
      "ja": null
    },
    "type": "story",
    "url": "https://blog.zksecurity.xyz/posts/clean/",
    "score": 71,
    "by": "vons",
    "time": 1743100380,
    "content": "Introducing clean, a formal verification DSL for zk circuits in Lean4\n\n            Written by\n\n                    Giorgio Dell'Immagine\n\n            on\n            Mar 27, 2025\n\n    We are really excited to share our initial steps towards building clean, an embedded DSL and formal verification framework for ZK circuits in Lean4.\nAs we recently shared, Zero Knowledge circuits are full of bugs, but fortunately, techniques like formal verification can provide a huge confidence boost in the correctness of ZK circuits.\nClean enables us to define circuits in Lean4, specify their desired properties, and – most importantly – formally prove them!\nThis work is part of the zkEVM Formal Verification Project which aims to provide infrastructure and tooling to enable formal verification for zkEVMs.\nRead about clean below, or watch our presentation on it in the zkEVM project updates call.\n\nObjectives\nOur objective is to build an embedded DSL for writing zk circuits in Lean4, that allows us to reason about them in a formal way.\nWe believe that co-locating circuit definitions with their desired specification and correctness proof will allow us to create a robust library of reusable formally verified circuit gadgets.\nWe currently target AIR arithmetization, and we assume to have a table lookup primitive available by the underlying proof system.\nHow to formally verify ZK circuits\nIn order to reason formally about ZK circuits, we first need to define a formal model. This involves the following steps:\n\nDefining what primitives our circuit language supports, i.e., which are the operations that we can use to define circuits.\nDefining the semantics of such primitives.\nDefining what are the properties we are interested to formally prove for a given circuit.\n\nOur language allows us to specify a circuit, which is composed of two main objects.\n\nA collection of variables, and\nconstraints and lookup relations over those variables.\nThe goal of a zero-knowledge proof system is exactly to convince a verifier that the prover knows a witness (i.e., an assignment of the variables) that satisfies the constraints and lookups of a given circuit.\n\nAt a fundamental level, for a given circuit we are interested in how the satisfaction of the constraints and the witness are related.\nIn other words: if a witness satisfies the constraints, what property can we infer about it?\nLet’s make a concrete example.\nConsider a circuit defined over one variable x and that is composed of only one contraint:\nC1 : x * (x - 1) === 0\nThis is a very common gadget that ensures that x is a boolean value, i.e., it is either 0 or 1.\nThe specification of this ciruit can be expressed as:\nx = 0 ∨ x = 1\nAlbeit being a very simple example, it shows the basic idea: we are interested in formally proving that if an assignment to the variables satisfies the constraints, then the specification holds as well.\nWe are also interested in the other direction: if an honest prover holds a witness that satisfies the specification, then there exists an assignment of the variables that satisfies the constraints.\nTake the following alternative implementation of a boolean check\nC2 : x === 0\nThis new constraint is sound, because the only valid assignment that satifies it is x = 0, which is a boolean value.\nHowever, it is not complete, as an honest prover that holds a valid boolean x = 1 cannot provide a witness that satisfies the constraint.\nMore formally, the two properties we want to prove are:\n\nSoundness: if the prover can exhibit any witness that satisfies the constraints and lookup relations defined by the circuit, then some specification property holds over that witness. Proving this property ensures that the circuit is not underconstrained.\nCompleteness: for every possible input, an honest prover can always exhibit a witness that satisfies the constraints and lookup relations defined by the circuit. Proving this property ensures that the circuit is not overconstrained.\n\nDSL design\nIn our DSL, we support four basic operations for defining circuits.\ninductive Operation (F : Type) [Field F] where\n  | witness : (m: ℕ) -> (compute : Environment F -> Vector F m) -> Operation F\n  | assert : Expression F -> Operation F\n  | lookup : Lookup F -> Operation F\n  | subcircuit : {n : ℕ} -> SubCircuit F n -> Operation F\nIndeed, we can:\n\nWitness: introduce m new variables in the circuit, and add them to the witness.\nThis operation accepts also a compute function, which represents the witness generation function, in the honest prover case.\nAssert: add a new constraint to the circuit.\nLookup: add a new lookup relation to the circuit. A lookup defines which variables are being looked up and in which other table.\nSubcircuit: add a new subcircuit to the circuit.\nThe subcircuit is instantiated in the current environment, and the internal variables of the subcircuit are added to the witness.\nThis is the main way to gain composability of gadgets.\n\nTo enhance usability, we provide a way to define a circuit using a monadic interface, with a lot of convenience functions.\nThis interface allows us to define the circuits using very natural syntax constructs.\nDesign of the composable verification framework\nThe main building block of our framework is the FormalCircuit structure.\nstructure FormalCircuit (F: Type) (β α: TypeMap)\n  [Field F] [ProvableType α] [ProvableType β]\nwhere\n  main: Var β F -> Circuit F (Var α F)\n  assumptions: β F -> Prop\n  spec: β F -> α F -> Prop\n\n  soundness:\n    ∀ offset : ℕ, ∀ env,\n    -- for all inputs that satisfy the assumptions\n    ∀ b_var : Var β F, ∀ b : β F, eval env b_var = b ->\n    assumptions b ->\n    -- if the constraints hold\n    constraints_hold.soundness env (circuit.main b_var |>.operations offset) ->\n    -- the spec holds on the input and output\n    let a := eval env (circuit.output b_var offset)\n    spec b a\n\n  completeness:\n    -- for all environments which _use the default witness generators for local variables_\n    ∀ offset : ℕ, ∀ env, ∀ b_var : Var β F,\n    env.uses_local_witnesses (circuit.main b_var |>.operations offset) ->\n    -- for all inputs that satisfy the assumptions\n    ∀ b : β F, eval env b_var = b ->\n    assumptions b ->\n    -- the constraints hold\n    constraints_hold.completeness env (circuit.main b_var |>.operations offset)\nThis structure tightly packages in a dependent-type way, the following objects:\n\nβ and α are respectively the input and output “shapes”, essentially they define a structured collection of elements.\nmain: the circuit definition itself.\nassumptions: the assumptions that the circuit makes on the inputs. All properties are proved assuming that the input variables satisfy these assumptions.\nspec: the specification property of the circuit.\nsoundness: proof for soundness of the circuit.\ncompleteness: proof for completeness of the circuit.\n\nA FormalCircuit encapsulates a formally proved, reusable gadget: when instantiating a FormalCircuit as a subcircuit, we are able to reuse the soundness and completeness proofs of the subcircuit to prove the soundness and completeness properties of the whole circuit.\nThis is accomplished by automatically replacing the constraints of a subcircuit with its (formally verified) spec.\nIn this way we can formally verify even large circuits by applying a divide-et-impera approach: we start by defining and proving correctness of low-level reusable gadgets, and then combine them to build more and more complex circuits.\nA concrete example: 8-bit addition\nLet’s walk through one of the simple gadgets we have implemented and verified: addition on 8-bit numbers.\nThis is a gadget that takes as input two bytes and an input carry, and returns the sum of the two bytes modulo 256, and the output carry.\nFirst, we need to define the input and output shapes.\nstructure Inputs (F : Type) where\n  x: F\n  y: F\n  carry_in: F\n\nstructure Outputs (F : Type) where\n  z: F\n  carry_out: F\nNow, we define the assumptions the circuit makes on the input values, and the specification that the circuit should satisfy.\ndef assumptions (input : Inputs (F p)) :=\n  let { x, y, carry_in } := input\n  x.val < 256 ∧ y.val < 256 ∧ (carry_in = 0 ∨ carry_in = 1)\n\ndef spec (input : Inputs (F p)) (out : Outputs (F p)) :=\n  let { x, y, carry_in } := input\n  out.z.val = (x.val + y.val + carry_in.val) % 256 ∧\n  out.carry_out.val = (x.val + y.val + carry_in.val) / 256\nThe main circuit is defined as follows.\ndef add8_full_carry (input : Var Inputs (F p)) :\n    Circuit (F p) (Var Outputs (F p)) := do\n  let { x, y, carry_in } := input\n\n  -- witness the result\n  let z <- witness (fun eval => mod_256 (eval (x + y + carry_in)))\n\n  -- do a lookup over the byte table for z\n  lookup (ByteLookup z)\n\n  -- witness the output carry\n  let carry_out <- witness (fun eval => floordiv (eval (x + y + carry_in)) 256)\n\n  -- ensures that the output carry is boolean\n  -- by instantiating the Boolean.circuit as a subcircuit\n  assertion Boolean.circuit carry_out\n\n  -- main 8-bit addition constraint\n  assert_zero (x + y + carry_in - z - carry_out * (const 256))\n\n  return { z, carry_out }\nFinally, we define the FormalCircuit structure, which packages all those definitions, together with the soundness and completeness proofs\ndef circuit : FormalCircuit (F p) Inputs Outputs where\n  main := add8_full_carry\n  assumptions := assumptions\n  spec := spec\n  soundness := by\n    ...\n  completeness := by\n    ...\nNotice that this definition is generic over the field prime p, however we require an additional assumption on the prime, namely p>512, otherwise the circuit is not sound!\nvariable {p : ℕ} [Fact p.Prime]\nvariable [p_large_enough: Fact (p > 512)]\nVerifying concrete AIR tables\nThe FormalCircuit abstraction provides a modular definition of verified circuits, and it is mostly arithmetization-agnostic.\nHowever, we want to target AIR as a concrete arithmetization, as it is a very popular choice in the zkVM design space.\nAIR circuits are defined over traces: constraints are specificed together with an application domain, which represent which rows they should be applied to.\nIn principle, one could choose an arbitrary domain, however, in practice we choose domains that have a succinct representation in terms of vanishing polynomial.\nHere are some examples of domains that have succinct representations and are used in practice.\n\nThe constraint is applied to one specific row of the trace. This is often referred to as a boundary constraint.\nThe constraint is applied to all rows of the trace. This is often referred to as a recurring constraint. One feature is that constraints applied to every row can access neighbouring rows: for example they could access the next row or the previous row.\nThe constraint is applied to all rows, except a chosen small set of rows. For example, it could be applied to every row except the last one, or except the first one.\nThe constraint is applied to every n rows of the trace.\n\nAs a concrete example, let’s say that we want to give constraint over a trace for computing correctly the Fibonacci sequence, which is defined as follows.\n{F0=0F1=1Fn=Fn−2+Fn−1\nWe can implement it with a trace composed of two columns: x and y.\nThe invariant we want to prove is: on the i-th row, xi sould contain Fi and yi should contain Fi+1.\nWe can achieve this behaviour by imposing the following contraints.\n\nWe impose a boundary constraint on the first row: x0=0 and y0=1.\nAdditionally, we impose two recurring constraints, implementing the recursive relation: for every i – except the last one –, yi+1=xi+yi, and xi+1=yi.\nThis set of constraints is depicted in the following figure.\n\nIt is straight-forward to see that if a trace satisfies those constraints, then in the i-th row we will find the i-th Fibonacci number in the first column.\nNotice that this set of constraints can be thought also as defining a correct sequence of states, one for each row:\n\nthe boundary constraint is ensuring that the initial state is valid, while\nthe recurring constraint is ensuring that the state transition function is executed correctly.\n\nIn our framework, we support AIR constraints by:\n\nDefining an inductive trace data structure, which we model as a sequence of rows.\nModeling what it means to apply a contraint to a trace using a particular domain: this in practice is done by providing an assignment from abstract variables to concrete trace cells and then applying the original constraint semantics.\n\nYou can check out the soundness proof for the Fibonacci table using 8-bit addition here, which satisfies the following, slightly more complicated, specification:\nFor every rowi,xi=(Fimod256)\nUpcoming work\nThe framework is still in early stages fo development, but it is already showing promising results.\nSome planned next steps are:\n\nContinue adding low-level gadgets so that we have a rich library of basic reusable circuits.\nDefining common hash function circuits and proving their correctness.\nBuild a formally verified minimal VM for a subset of RISC-V.\n\nThe whole project is open source and available on GitHub, make sure to check it out!",
    "summary": {
      "en": "**Summary of \"Introducing clean, a formal verification DSL for zk circuits in Lean4\"**\n\nGiorgio Dell'Immagine introduces \"clean,\" a new domain-specific language (DSL) for formal verification of Zero Knowledge (ZK) circuits using Lean4. ZK circuits often have bugs, but formal verification techniques can enhance their reliability. Clean allows users to define circuits, specify their properties, and prove their correctness, contributing to the zkEVM Formal Verification Project.\n\n**Objectives:**\n- Build a DSL for writing ZK circuits in Lean4.\n- Create a library of reusable, formally verified circuit components.\n- Support AIR arithmetization and include a lookup primitive.\n\n**Formal Verification Process:**\n1. Define the circuit's supported operations.\n2. Set the semantics for these operations.\n3. Specify properties to prove for each circuit.\n\nCircuits consist of variables and constraints. The goal is to ensure that if the constraints are satisfied, the specified properties hold true, and vice versa.\n\n**Key Properties:**\n- **Soundness:** If a witness satisfies the constraints, then a specification property holds.\n- **Completeness:** For every input, an honest prover can exhibit a witness that satisfies the constraints.\n\n**DSL Features:**\nThe DSL supports operations like introducing new variables, adding constraints, defining lookup relations, and composing subcircuits. It offers a user-friendly monadic interface for circuit definitions.\n\n**FormalCircuit Structure:**\nThis structure encapsulates the circuit definition, assumptions, specifications, and proofs of soundness and completeness, allowing for the reuse of verified components.\n\n**Example: 8-bit Addition Circuit:**\nThe article walks through implementing and verifying an 8-bit addition circuit, detailing input/output definitions, assumptions, specifications, and the proof structure.\n\n**Future Work:**\nThe project is in early development stages, with plans to:\n- Add more basic reusable circuits.\n- Define and verify common hashing functions.\n- Build a minimal verified VM for a subset of RISC-V.\n\nThe project is open source and available on GitHub for further exploration.",
      "ko": "조르지오 델리마지네는 Lean4를 사용하여 제로 지식(Zero Knowledge, ZK) 회로의 형식 검증을 위한 새로운 도메인 특화 언어(DSL)인 \"clean\"을 소개합니다. ZK 회로는 종종 버그가 발생할 수 있지만, 형식 검증 기법을 통해 신뢰성을 높일 수 있습니다. Clean은 사용자가 회로를 정의하고, 그 속성을 명시하며, 올바름을 증명할 수 있도록 도와줍니다. 이는 zkEVM 형식 검증 프로젝트에 기여합니다.\n\n이 프로젝트의 목표는 Lean4에서 ZK 회로를 작성하기 위한 DSL을 구축하고, 재사용 가능한 형식 검증된 회로 구성 요소의 라이브러리를 만드는 것입니다. 또한 AIR 산술화 지원과 조회 원시 기능을 포함할 계획입니다.\n\n형식 검증 과정은 다음과 같습니다. 먼저 회로가 지원하는 연산을 정의하고, 이러한 연산의 의미를 설정합니다. 그 다음 각 회로에 대해 증명할 속성을 명시합니다. 회로는 변수와 제약 조건으로 구성되며, 제약 조건이 충족되면 명시된 속성이 참이 되도록 하는 것이 목표입니다.\n\n주요 속성으로는 신뢰성(Soundness)과 완전성(Completeness)이 있습니다. 신뢰성은 증인이 제약 조건을 만족하면 명세 속성이 성립함을 의미합니다. 완전성은 모든 입력에 대해 정직한 증명자가 제약 조건을 만족하는 증인을 제시할 수 있음을 의미합니다.\n\nDSL의 기능으로는 새로운 변수를 도입하고, 제약 조건을 추가하며, 조회 관계를 정의하고, 하위 회로를 구성하는 작업을 지원합니다. 사용자 친화적인 모나딕 인터페이스를 제공하여 회로 정의를 쉽게 할 수 있습니다.\n\n형식 회로(FormalCircuit) 구조는 회로 정의, 가정, 명세 및 신뢰성과 완전성의 증명을 캡슐화하여 검증된 구성 요소를 재사용할 수 있도록 합니다.\n\n예를 들어, 8비트 덧셈 회로를 구현하고 검증하는 과정을 설명하며, 입력 및 출력 정의, 가정, 명세, 증명 구조를 자세히 다룹니다.\n\n향후 계획으로는 더 많은 기본 재사용 회로를 추가하고, 일반 해싱 함수를 정의 및 검증하며, RISC-V의 하위 집합을 위한 최소 검증된 가상 머신을 구축할 예정입니다. 이 프로젝트는 오픈 소스로 GitHub에서 더 많은 탐색이 가능합니다.",
      "ja": null
    }
  },
  {
    "id": "aa8902d9590d3643",
    "title": {
      "en": "Anti-Orbit Laser Submarines (2017)",
      "ko": "반궤도 레이저 잠수함",
      "ja": null
    },
    "type": "story",
    "url": "http://toughsf.blogspot.com/2017/10/anti-orbit-laser-submarines.html",
    "score": 55,
    "by": "EA-3167",
    "time": 1743120377,
    "content": "ToughSF\n\nThursday, 12 October 2017\n\nAnti-Orbit Laser Submarines\n\nLaser-equipped nuclear-powered submarines are the perfect last line of defense against an attacking force in orbit.\n\nThe situation\n\nYou don't win every fight. Eventually, there will come a time in space warfare where a fleet of space warships has defeated all your mobile forces and your immobile defenses. They will bear down on you from above with lasers, missiles and kinetic projectiles and you will have to find a way to prevent their forcing of an unconditional surrender.\n\nWe will refer to the opponents as the 'attackers' and to you as the 'defenders'. The first step to devising an effective defense is to understand the situation.\n\nSo what is the situation?\n\nIts an enemy ship.\n\nAn attacking warship will start out in high orbit. This is an altitude of 2000km or above. Whether it has just arrived from an interplanetary voyage or has recently destroyed your remaining warships, it will go to high orbit to maximize the effectiveness of its space superiority. Space superiority, borrowing from the term 'air superiority', is when a force has complete dominance over all the orbits around a planet. No space-borne forces can oppose this superiority and no reserve forces can challenge them without being quickly destroyed.\n\nWhat does losing space superiority mean for defenders?\n\nThe most important consequence is that enemy warships have free reign to change orbits, maneuver into favorable positions and receive re-supplies. Their mobility is unconstrained.\n\nHigh Elliptic Orbits and gravity assists from a large moon allow for a huge variety of orbits.\n\nAttackers in high orbit can make optimal use of their laser weaponry. They can get clear lines of sight onto any spot on the surface, and the long distances between objects forces travel times to lengthen with the secondary effect of giving lasers plenty of time to shoot down targets. Laser effectiveness is generally dependent on how far they are from a target and how much 'dwell time' a beam can spend on a target.\n\nDe-orbiting objects from high altitude is inexpensive in terms of deltaV. This works in favour of missiles sent down from orbit by allowing them to use very little propellant to strike ground targets, which makes them lightweight and cheap to send by the hundreds. Additionally, falling towards Earth gives them a big boost to the velocities they achieve before impact.\n\nThe same applies to kinetic projectiles, a fact applied in the Rods From God concept of orbital bombardment.\n\nRetaliating\n\nSo you want to shoot back at the attackers.\n\nU.S. Army’s Homing Overlay Experiment\n\nMissiles can do the job. They are currently our only method of delivering weapons into orbital space. Something like an ICBM with an additional stage can reach LEO. Reaching higher orbits will require either a very large rocket, a high Isp engine for the upper stage or a launch system such a laser launch or ram accelerator.\n\nHowever, each of these solutions have major issues when trying to shoot down an opponent in high orbit.\n\nLaser satellite shooting down a missile slowly climbing to orbit.\n\nLarge rockets are easy to target and shoot down. A chemical-propellant rocket that needs to minimize its dry mass to achieve the necessary deltaV capacity will have very good acceleration but will end up being very fragile. Nuclear-thermal or nuclear-powered rockets can be much smaller and more durable, but getting sufficient acceleration out of them implies a very high power requirement, which might make them very expensive to throw at the enemy.\n\nMaglev track rocket launch system... vulnerable.\n\nRegular launches take tens of minutes and cannot be disguised from the attackers. Shortening this window of vulnerability can be done by using a launch system that powers the rocket or accelerates it externally. However, the infrastructure for the launch systems will in turn become a priority target for the attackers. Massive, hard to hide and immobile, they will receive a lot of firepower. Some launch systems are practically impossible to shield from damage, such as a laser launch system that needs thousands of exposed laser optics, and others reveal their positions as soon as they fire a rocket. Building underground is also a very expensive endeavour when considering that all the work can be undone by a single 'bunker buster'-type weapon.\n\nThe logistics of launching missiles against attackers sitting in orbit works against the defenders. The attackers can de-orbit a missile by expending only a few tens to hundreds of meters per second of deltaV. A defender must equip each missile with tens of thousands of meters per second of deltaV. It might be easier to build more missiles and create rocket fuels on the ground at the start of the war, but after an orbital bombardment campaign by attackers with space superiority, it is unlikely to be the case.\n\nKinetics that can be shot all the way to high orbit need to handle hundreds to thousands of Gs of acceleration, traverse the lower atmosphere at dozens of kilometers per second, survive laser fire for several minutes with minimal capacity to dodge and take out a target with a very short window of interception. This is a tall order!\n\nHot hydrogen light gas gun launch system. Can be weaponized.\n\nSo, what are the defender's options?\n\nThey need to retaliate with something that cannot be shot down, from a platform that can avoid counter-fire and can maintain functionality after infrastructure and services have been disabled world-wide.\n\nOne option that fits the bill is laser submarines.\n\nLasers cannot be shot down and hit their target instantly. They can be used so long as electrical power is supplied. Submarines operate underwater, an environment that can hide them until they surface and protects them from high-velocity projectiles and lasers while submerged. The can protect themselves this way for months on end, and if they employ the same life support systems as on spaceships, then it can add up to years.\n\nFor the same reason that today's submarine fleets are considered an unbeatable means of retaliating against a foe after nuclear armageddon has wiped out the homeland, laser submarines will be able to operate and remain dangerous even after orbital attacks destroy all support infrastructure.\n\nLet us now look at how a submarine can be used to retaliate against attackers in orbit.\n\nThe Challenges\n\nSubmarines are already equipped with a high electrical power generation. Large modern nuclear submarines are already able to produce over 100MW for years on end. In a futuristic setting with common space travel and space wars, power generation technology developed for interplanetary travel will allow submarines to produce gigawatts or more.\n\nNext generation water-cooled nuclear reactor.\n\nThe most likely generators for space travel will be nuclear due to their high power density. The biggest limitation to generating power from nuclear reactors is waste heat capacity: It is easy to heat up the reactor core but much harder to remove the heat. Submarines will have an entire ocean as a heatsink so will be able to produce more watts compared to a spacecraft with a reactor of the same mass and volume.\n\nThe Bi-modal NTR is a nuclear thermal rocket engine that doubles as power generator.\n\nAll of this electrical power can be used to power a laser generator.\n\nThree elements determine a laser's effectiveness: wavelength, radius of focusing optics and beam power.\n\nAn observatory using a laser guide star.\n\nWe have already determined that laser submarines will likely be able to produce more electrical power than a similar laser space warship, so laser submarines will also have the advantage in beam power.\n\nThe radius of the focusing optics will depend on the specific arrangement of the laser weapon's components and how they are deployed. We will look into the possible designs down below.\n\nA free electron laser. Unlike most designs, it can freely switch wavelengths.\n\nThe wavelength however is not a variable laser designs have much control over. Submarines operate in an aquatic environment, on top of which is a hundred kilometers of Earth's atmosphere. At the interface of the ocean's surface is sea mist and suspended droplets of water in fog or clouds. The ocean's surface is not flat either, with waves of a few centimeters to a few hundred meters rolling over it endlessly. A beam emitted by a submarine will have to penetrate all of this environment and still travel the hundreds of thousands of kilometers' distance separating it from a target in high orbit.\nThe optical properties of water are therefore the determinant factor for which wavelengths the laser should produce.\n\nHere is the absorption spectrum of water:\n\nThe lower the 'Relative Absorption value', the less the wavelength's energy is absorbed. We can clearly see that the lowest values are for the 'optical window' that corresponds to the 400-700nm visual spectrum. The highest absorption is for 100nm ultraviolet wavelengths and 3000nm infrared wavelengths.\n\nHere is the absorption spectrum for our atmosphere:\n\nThe atmospheric attenuation of electromagnetic radiation has similar features to that of water: short wavelengths such as X-rays cannot go through while long wavelengths such as radio penetrate easily.\n\nIt might be easier to consider the laser beam as being fired from space and coming down to the surface.\n\nA 100nm ultraviolet laser beam will traverse the vacuum of space with ease, but will stop short of reaching the upper atmosphere. A 400nm blue laser will go through the atmosphere and through 460 meters of water before being reduced to less than 1% of its initial power. A 1000nm infrared laser will lose 20% of its power to the atmosphere and be completely absorbed by half a meter of water. A 100m radio wavelength just bounces off the ionosphere.\n\nWhile shorter wavelengths are preferred for laser weapons, as they allow a beam to be focused to destructive intensities over longer distances, a laser submarine should use 400nm wavelength lasers to penetrate water and the atmosphere without losing a lot of beam power.\n\nThe equation for how much of a beam's energy is retained after traversing a medium is given by:\n\nPercentage transmitted: e^( -1 * Attenuation coefficient * Depth) * 100\n\nThe attenuation coefficient is usually given in cm^-1, so the depth should be converted into cm units.\n\nFor example, near-infrared light at 800nm wavelength has an attenuation coefficient of 0.01cm^-1. We want to know what percent of a near-infrared laser's energy remains after passing through one meter of water.\n\nOne meter equals 100 cm. Per our equation, we find the percentage to be approximatelye^(-0.01*100)*100: 36.8%. Just over a third of the beam gets through.\n\nHere is a table of values for how much beam power is lost if the blue wavelength laser submarine fires its weapon at different depths, using an attenuation coefficient of 0.0001cm^-1:\n\nWe see that to maintain a good percentage of laser power getting through the water, a laser submarine would have to fire at rather low depths. According the the table above, a 10 meters depth using blue laser light looks like a good compromise: deep enough to escape orbital surveys and strikes, with at least 90% of the laser power going through.\n\nSo is a laser being fired at 10 meters depth a good idea?\n\nFactors affecting a solid state laser weapon system.\n\nThe table gives an incomplete picture. While laser power being absorbed is an important factor to consider, there is a large number of other elements that affect how effective a laser is. One such element is thermal blooming. That 90% power transmission rate implies that 10% of the laser power is absorbed and goes into heating the water. If the laser power is rated in megawatts, the heat absorbed by the water becomes significant. Hot water has different optical properties compared to colder water - it will work as a lens in reverse, effectively de-focusing the laser.\n\nAnother significant issue is the water/gas interface. When light travels between two mediums of significantly different density, like seawater and atmospheric gasses, it is bent by refraction. Even worse, the sea's surface is constantly disturbed by waves, tides and other movements. Instead of a smooth surface, which angling the laser can compensate for, it is continuously changing and bending light in different and hard to predict direction. Here is a familiar example of the effect:\n\nThis effect is familiar to astronomers trying to gaze at stars through the moving atmosphere, and adaptive optics are used to compensate for the deviations in light traversing the atmosphere.\n\nFor a laser weapon, adaptive optics work in reverse.\n\nAdaptive optics cannot be employed as effectively when used underwater. While the atmosphere's movements are already difficult to detect and correct, trying to effectively measure how light moves through two mediums with a complex and moving interface is much harder. Guide lasers are used for measurement, with the light reflecting off the ionosphere creating an artificial 'guide star' for astronomers to calibrate their instruments. A guide laser placed underwater would submit to the same chaotic disturbances as the weapon laser and would be unusable. They would also have to work much harder. Refraction means that inaccuracies are multiplied once they pass through the water/air interface.\n\nInterference with sunlight works in both directions.\n\nFinally, there's the problem of reflection. While water is decently able to let higher wavelength visible light penetrate, the massive difference in density between the water and the air (x1000) above it means that it is a good reflector. Laser light would travel from the submarine to the water/air interface, and just be bounced back below the surface. For sea water, less than 6% of the laser light would be reflected at angles below 30 degrees.\n\nWhile the above table might not give any large figures, remember that the angle is measured against the rippling, swelling and rolling waves. A nominal angle of 10 degrees against the water's surface might transition between -20 and 100 degrees as a wave moves over a laser. This is the difference between 3% and 45% of the laser not going through the ocean's surface.\n\nSubmarines can survive this... but it might not be enough to matter.\n\nThe principal advantage of staying underwater is that the submarine will be protected from high velocity strikes and retaliatory laser fire. However, if it cannot return fire from this position, then it cannot serve as a perfect last line of defense.\n\nThe Solutions\n\nSo, based on the previous section, we can affirm that attempting to shoot a laser while underwater provides unequalled benefits but also significant challenges.\n\nWe can either tackle the problems a laser submarine faces directly or attempt to circumvent them.\n\n-Long wavelengths\n\nPreviously, we considered that blue wavelengths were optimal for shooting while underwater as they were absorbed the least. The reduced absorption would have allowed a submarine to transmit over 90% of the laser power to the ocean's surface from a depth of 10 meters. However, the distortion at the water/air interface rendered this option impractical.\n\nHow about using a wavelength that is less efficient at penetrating water, but is less affected by distortion?\n\nLooking at the absorption spectrum of water, we notice that wavelengths longer than 100 micrometers are absorbed less and less as the wavelength increases. At 1m wavelength, the laser would traverse water as easily as blue light. This corresponds to a frequency of 300MHz. This is the radio band.\n\nNot coincidentally, frequencies of lower than 300MHz are used to communicate with submarines. At 3 to 30kHz, which is wavelengths of 10 to 100km, can penetrate the seas to a depth of several hundred meters. Using even lower frequencies further increases penetration depth, but would require impractically large lens to focus onto a target in high orbit. Another factor working against longer wavelength radio is that the ionosphere can reflect signals back down to the surface at frequencies below 30MHz.\n\nA 1m wavelength radio laser will be able to traverse the atmosphere mostly undisturbed and go through the ionosphere without refraction. The features of waves are too small to cause it to wobble chaotically at the sea/air interface. The beam would bend coming out of the sea, but it is a single predictable deviation that can be corrected.\n\nInductive output tube diagram.\n\nA radio-wavelength coherent beam or 'raser' can be generated by a Free Electron laser or inductive output tubes with an efficiency exceeding 70%.\n\nThe advantages of a radio-based anti-orbital system is that it allows a submarine to fire upon targets while deep underwater. Even at 20 meters depth, the radio beam would transmit 82% of its power through water and lose less than 1% going through the atmosphere. It is much less affected by small waves and other turbulence in the water, and mostly immune to above-surface weather effects.\n\nThere are several downsides however. Such a large wavelength makes it impossible to focus the beam down to destructive intensities without a very large radio dish - this might get impractical when you also want the submarine to move quickly while underwater. Another issue is that the beam won't interact with the target in a consistent manner.\n\nLasers, for example, are absorbed by the outermost layers of the materials the target's surface is made of. The heating is concentrated in the 'skin' of the target. Sufficiently intense laser beams heat this skin layer to very high temperatures, causing the material to boil away or even explode.\nRadio beams would use wavelengths a million times longer that do not interact with the target's materials at an atomic level. They are much more sensitive to the conductivity of the materials they are striking. Good conductors such as steel or aluminium efficiently reflect radio waves and are not heated. Good insulators such as ceramics or glass are mostly transparent to radio waves and do no absorb the beam's energy as it passes through them. Radio absorbing materials have to be neither good conductors or insulators, such as\n\nThis is bad news if the targets are space warships with an external metallic hull and an internal structure based on advanced carbon-composite and ceramic materials. Large propellant tanks will let the radio waves pass straight through. Small features of 10cm or smaller are completely invisible to the radio waves too.\n\nHowever, there will still be ways to deal damage.\n\nOpenings in the metallic hull would allow radio waves to enter and then bounce around on the internal surface. Like a microwave oven, the trapped radiation will pass through radio transparent materials thousands to millions of times before being fully absorbed. A human is mostly composed of salted water. He or she would absorb about between 0.1 and 1% of a radio beam going through their body. If the radio beam stays inside a 10m diameter hull for just 76 microseconds, 2300 bounces are possible and the percentage of beam energy absorbed rises to 90%. When the beam power is measured in tens to hundreds of megawatts, this has dire consequences for a human crew.\n\nAnother effect is induced current. If even a few watts manage to circulate in microcircuitry, it is enough to short-circuit or even melt down computers, avionics and delicate sensors. RF Shock and Burn is a serious issue for electricians and engineers working on conductive structures near a high frequency radio source. At the power levels radio-laser submarines will pump into targets, induced current is enough to melt steel.\n\nSteel melted by 1-30kHz radio frequencies.\n\nModern submarines are not powerful enough to compensate for the diffraction of a 1m wavelength radio beam. With 100MW of available electrical power, 30% of which is lost in an inductive output tube and another 10% to seawater and atmospheric absorption, less than 63MW will reach space.Even the largest submarines, such as the Ohio-class SSBN, have a hull diameter (beam) of 13m. Mounting an internal dish to focus a radio beam up to this diameterwill createa very low performance laser.\nTargets at 10km distance will receive about 22W/m^2 - a great radio signal, but a terrible weapon. Targets in low orbit and high orbit will receive milliwatts of power.\n\nA 1296MHz dish. For 300MHz beams, the spaces can be even larger.\n\nWhat is needed is much more power and an externally mounted radio dish. Thankfully, a 300MHz beam can be focused by a dish with holes up to a tenth of the wavelength in size. A radio dish for this wavelength can is very lightweight and easily collapsible, with conductive spars spaced by 10cm lengths. The spars can be made hollow to have neutral bouyancy, allowing them to support themselves without many structural elements. At 10m depth and below, there are few disturbances in the surrounding water.\n\nDish diameters of 100 meters or more are envisageable, massing less than 1kg per m^2. Tension wires hold the shape and serve as the mechanisms for adaptive optics to act on the dish.\n\nA group of submarines using space-grade nuclear reactors might be able to put together 500GW of power with a combined reactor mass of only 2500 tons. Between them, they can hold up a dish 1km in diameter, as follows:\n\nThis arrangement allows the submarines to focus a 315GW beam to an intensity of 67kW/m^2 at an altitude of 1000km. For low orbit targets, the intensity is 1.67MW/m^2. These intensities are far from enough to melt or physically damage the structure of a spaceship. However, the beam is large and entirely envelops the target. Any hole through a metallic exterior or any cavity lined by a radar reflector will turn the spaceship into a microwave oven receiving megawatts of heating over time. At high altitudes, the intensity is lower by the targets orbit much slower, giving the Rasar beam time to boil crews to death and melt components directlyor indirectly.\n\n-Interface lens\n\nA variant on the lens used for above.under water filming.\n\nThe biggest trouble with optical wavelengths is the chaotic distortions and reflections created by the sea/air interface. They would allow submarines to physically damage targets with relatively small lens and shoot on the move, but aiming though the interface seems impossible...\n\n... unless an interface lens is used.\n\nIt is an optical array that floats on the ocean's surface, serving to handle the beam's transition from underwater to atmospheric mediums. Glass can be made to have a refractive index similar to that of water. A laser beam traversing a sea/glass interface would not suffer any distortion. This is the reason why some transparent objects disappearcompletely while underwater - our eyes cannot make out any distortionsthat reveal their presence.\n\nThe refractive index of the polymer ball is exactly the same as water.\n\nThe interface lens can also serve to focus the laser. It can be made much larger than whatever the submarine can carry, as it is not confined by hull dimensions or hydrodynamics.\n\nOne primary advantage of thesefloating structures is that they can be deployed before firing commences, and each is much cheaper than a submarine. When a target passes overhead in orbit, a laser submarine can rise to firing depth and start shooting. It only needs to equipment to focus the beam from the laser generator to the interface lens, a distance of ten meters or so. The floating lens receives the beam, corrects the angle and aims at the target overhead. If the target is not destroyed, it can trace the laser back to its origin and initiate a retaliatory strike.\n\nAn interface lens is not very mobile and needs to stay on the surface, so it cannot protect itself by diving. There is a good chance it will be destroyed... at which point the laser submarine switches to firing through another interface lens and so on.\n\nAlthough the lens will have to be rather large and heavy to receive a laser beam from a wide variety of angles underwater, and re-focus it in a moving target hundreds of kilometers above, which makes it expensive, it must be considered as an expendableasset when compared to cost and size of a nuclear submarine. Also, the lens can be covered by an isothermal sheet and made out of materials transparent to radar, making it hard to detect from orbit until it starts firing.\n\nUsing a 10m diameter interface lens, even a modern submarine with 100MW of available output will be able to deal serious damage to targets in low orbit. About 40MW of the submarine's power will reach the target using a diode laser generatorat 400nm wavelength, but at an intensity of 133GW/m^2 at 200km altitude. This is enough to rip through 14.8 meters (!) of aluminium per second, or even 6.5m/s of carbon armor. Any target caught by this beam for even a second will be cut in half.At 1000km, its performance is still a respectable 52mm/s through carbon.\n\nSpaceship in Children of a Dead Earth scarred by laser strikes.\n\nMore advanced submarines can get away with smaller lens that are harder to counter-attack and still deal devastating damage to high orbit targets. A 10GW laser beam focused through a 4m wide interface lens can blast away targets at a rate of 837mm/s at 1000km.\n\nDisadvantages of this system is the cost of 'expendable' large adaptive mirrors and possibly the inability to use floating structures in severe storms with large waves rocking the optics.\n\n-Towed Lens\nThis fixes the problem with floating optical arrays. The laser is focused by towed apparatus that can be held underwater until firing starts, and then moved around after an initial volley to avoid counter-fire.\n\nThe towed lens can be lighter and cheaper than a fully independent floating lens. Electrical power, computing operations and other functions can be provided by the submarine towing the lens, with only the actuators and suspension retained.\n\nLasers have been carried through optical fibres with nearly zero losses of beam energy over distance. This is because internal reflection of the beam is done at grazing angles within the optic fibre. In other words, laser beams of megawatts to gigawatts power levels can be transported by optic fibres without any significant losses and no special provisions against heating.\n\nOptical fibres can carry a laser generated by a submarine to the towed lens, and they can run parallel to the load bearing cables attaching the lens to the submarine. This method of delivering the beam to the lens bypasses the losses and complications of having the beam penetrate several meters of water to reach the surface.\n\nTo these advantages come some downsides. The submarine becomes more vulnerable that if it relies upon fully independent floating lens. If an enemy target locates a floating lens, it might correctly assume that the much more valuable submarine is very close to the lens. When a target spaceship in orbit sends down counter-fire before the submarine is ready, the latter would be forced to cut loose the lens and dive... this breaks the optical fibre cable and renders the lens useless.\n\nTactics using towed lens might involve dragging along a fleet of lenses and rotating them to the surface and back. Longer fibre optic cables gives the submarine more freedom to move, while a mix of decoys and intermittent and random firing patterns reduces the disadvantages of the design.\n\n-Optical phased array floater.\n\nDo away with the submarine!\n\nA specialized vehicle can be built solely for the purpose of hiding a laser weapon underwater and surfacing for short bursts of fire. Since this weapon only attacks from above the water and does not have to worry about hydrodynamics, nifty solutions such as an optical phased array can be used. The laser generator's size is equal to the lens diameter and if it received damage, it will only suffer reduced output.\n\nInstead of VLS cells, phased array grids?\n\nThe more delicate components such as the power generator can remain submerged, only transporting electricity to the phased array on the surface through cables. A reactor embedded in the sea floor can be a very difficult target to locate and destroy.\n\n-Supercavitating platform.\n\n'Bullet submarines' powered by excessively powerful nuclear reactors.\n\nIf submarines have access to gigawatts of power, they can also use it for propulsion.\n\nThis level of power output can enable submarines to reach supersonic speeds through supercavitation through water. If they can rise to the surface, fire, dive and relocate in a matter of seconds, then they can evade counter-fire through sheer agility.\n\nIt might be best in this case to mount a set of optical phased array lasers tailored to trans-atmospheric wavelengths to be used once a submarine surfaces. Gigawatts of power means gigawatts of heating: the local atmosphere can be cleared of moisture and mist that distorts the beam most heavily.\n\nA 1GW laser at 400nm focused by even a relatively small 4m diameter lens can blast past 23.7m/s of aluminium or 10.4m/s of carbon at 200km, and remains deadly at 1000km with 83mm/s of carbon penetration. This allows for short bursts of laser fire to take down any target.\n\nAtmospheric lensing.\n\nAdvanced techniques such as thermal lensing using the Kerr effect is being developed in programs such as the Laser Developed Atmospheric Lens (LDALs) by DARPA. LDALs can allow high-power submarines to extend their effective range to tens of thousands of kilometers while reducing the effectiveness of laser counter-fire.\n\nConclusion.\n\nLasers and underwater environments don't mix well, but there are many solutions to gain the protection a submarine enjoys while attacking instantly and repeatedly with direct energy weapons.\n\nOnce these solutions are applied, a defending planet with deep oceans can hope to maintain an effective last line of defense against invading spaceships.\n\nPosted by\n\nMatter Beam\n\nEmail ThisBlogThis!Share to XShare to FacebookShare to Pinterest\n\n172 comments:\n\n    (function() {\n      var items = null;\n      var msgs = null;\n      var config = {};\n\n// <![CDATA[\n      var cursor = null;\n      if (items && items.length > 0) {\n        cursor = parseInt(items[items.length - 1].timestamp) + 1;\n      }\n\n      var bodyFromEntry = function(entry) {\n        var text = (entry &&\n                    ((entry.content && entry.content.$t) ||\n                     (entry.summary && entry.summary.$t))) ||\n            '';\n        if (entry && entry.gd$extendedProperty) {\n          for (var k in entry.gd$extendedProperty) {\n            if (entry.gd$extendedProperty[k].name == 'blogger.contentRemoved') {\n              return '<span class=\"deleted-comment\">' + text + '</span>';\n            }\n          }\n        }\n        return text;\n      }\n\n      var parse = function(data) {\n        cursor = null;\n        var comments = [];\n        if (data && data.feed && data.feed.entry) {\n          for (var i = 0, entry; entry = data.feed.entry[i]; i++) {\n            var comment = {};\n            // comment ID, parsed out of the original id format\n            var id = /blog-(\\d+).post-(\\d+)/.exec(entry.id.$t);\n            comment.id = id ? id[2] : null;\n            comment.body = bodyFromEntry(entry);\n            comment.timestamp = Date.parse(entry.published.$t) + '';\n            if (entry.author && entry.author.constructor === Array) {\n              var auth = entry.author[0];\n              if (auth) {\n                comment.author = {\n                  name: (auth.name ? auth.name.$t : undefined),\n                  profileUrl: (auth.uri ? auth.uri.$t : undefined),\n                  avatarUrl: (auth.gd$image ? auth.gd$image.src : undefined)\n                };\n              }\n            }\n            if (entry.link) {\n              if (entry.link[2]) {\n                comment.link = comment.permalink = entry.link[2].href;\n              }\n              if (entry.link[3]) {\n                var pid = /.*comments\\/default\\/(\\d+)\\?.*/.exec(entry.link[3].href);\n                if (pid && pid[1]) {\n                  comment.parentId = pid[1];\n                }\n              }\n            }\n            comment.deleteclass = 'item-control blog-admin';\n            if (entry.gd$extendedProperty) {\n              for (var k in entry.gd$extendedProperty) {\n                if (entry.gd$extendedProperty[k].name == 'blogger.itemClass') {\n                  comment.deleteclass += ' ' + entry.gd$extendedProperty[k].value;\n                } else if (entry.gd$extendedProperty[k].name == 'blogger.displayTime') {\n                  comment.displayTime = entry.gd$extendedProperty[k].value;\n                }\n              }\n            }\n            comments.push(comment);\n          }\n        }\n        return comments;\n      };\n\n      var paginator = function(callback) {\n        if (hasMore()) {\n          var url = config.feed + '?alt=json&v=2&orderby=published&reverse=false&max-results=50';\n          if (cursor) {\n            url += '&published-min=' + new Date(cursor).toISOString();\n          }\n          window.bloggercomments = function(data) {\n            var parsed = parse(data);\n            cursor = parsed.length < 50 ? null\n                : parseInt(parsed[parsed.length - 1].timestamp) + 1\n            callback(parsed);\n            window.bloggercomments = null;\n          }\n          url += '&callback=bloggercomments';\n          var script = document.createElement('script');\n          script.type = 'text/javascript';\n          script.src = url;\n          document.getElementsByTagName('head')[0].appendChild(script);\n        }\n      };\n      var hasMore = function() {\n        return !!cursor;\n      };\n      var getMeta = function(key, comment) {\n        if ('iswriter' == key) {\n          var matches = !!comment.author\n              && comment.author.name == config.authorName\n              && comment.author.profileUrl == config.authorUrl;\n          return matches ? 'true' : '';\n        } else if ('deletelink' == key) {\n          return config.baseUri + '/comment/delete/'\n               + config.blogId + '/' + comment.id;\n        } else if ('deleteclass' == key) {\n          return comment.deleteclas",
    "summary": {
      "en": "**Summary of Anti-Orbit Laser Submarines**\n\nLaser-equipped nuclear submarines could serve as a crucial defense against space attackers. When a fleet of enemy ships gains control of high orbit, they can strike with lasers and missiles, making it essential for defenders to find a way to retaliate. \n\n1. **Situation Overview**: Attackers in high orbit can easily target defenders on the ground, enjoying mobility and the ability to strike without constraints. This creates a dire situation for defenders who have lost space superiority.\n\n2. **Challenges for Defenders**: Traditional missile defense systems face significant hurdles, such as being easily targeted and needing vast amounts of fuel to reach high orbit. Defenders struggle to launch effective counterattacks against agile attackers.\n\n3. **Advantages of Laser Submarines**:\n   - **Protection**: Submarines can remain hidden underwater, shielded from attacks and providing a secure base for retaliation.\n   - **Power Generation**: Modern submarines can generate substantial electrical power, potentially enabling powerful laser systems.\n   - **Instant Strike Capability**: Lasers hit targets almost instantly, making them effective for surprise attacks.\n\n4. **Technical Considerations**: \n   - **Laser Effectiveness**: The effectiveness of lasers diminishes with distance and through water and atmosphere. Lasers around 400nm wavelength are optimal for penetrating both mediums.\n   - **Submarine Depth**: Submarines can operate at depths where they can still effectively fire lasers while avoiding detection.\n\n5. **Potential Solutions**:\n   - **Longer Wavelengths**: Using radio waves instead of optical lasers for underwater attacks could circumvent some challenges, allowing for deeper firing while still delivering damage.\n   - **Interface Lenses**: Floating lenses could focus laser beams effectively at the air-sea interface, allowing submarines to fire while submerged.\n   - **Towed Lens Systems**: Submarines could use towed lenses to maintain flexibility while firing laser beams at targets.\n   - **Supercavitating Platforms**: High-power submarines could achieve supercavitating speeds for quick strikes and evasive maneuvers.\n\nIn conclusion, while lasers and underwater environments pose challenges, innovative technologies and strategies can enable submarines to become an effective last line of defense against orbital threats.",
      "ko": "레이저를 장착한 핵 잠수함은 우주 공격자에 대한 중요한 방어 수단이 될 수 있습니다. 적의 함대가 고위도에서 통제권을 확보하면 레이저와 미사일로 공격할 수 있어 방어자들은 반격할 방법을 찾아야 합니다.\n\n현재 상황을 살펴보면, 고위도에서 공격자들은 지상의 방어자들을 쉽게 겨냥할 수 있으며, 이동성이 뛰어나고 제약 없이 공격할 수 있습니다. 이는 우주 우위를 잃은 방어자들에게 매우 심각한 상황을 초래합니다.\n\n방어자들이 직면한 도전 과제는 전통적인 미사일 방어 시스템이 쉽게 목표가 되고, 고위도로 도달하기 위해 많은 연료가 필요하다는 점입니다. 방어자들은 민첩한 공격자들에 대해 효과적인 반격을 launch하는 데 어려움을 겪고 있습니다.\n\n레이저 잠수함의 장점은 여러 가지가 있습니다. 잠수함은 수중에서 숨을 수 있어 공격으로부터 보호받고, 반격을 위한 안전한 기지를 제공합니다. 현대의 잠수함은 상당한 전력을 생성할 수 있어 강력한 레이저 시스템을 운영할 수 있는 가능성이 있습니다. 레이저는 거의 즉각적으로 목표에 도달하므로 기습 공격에 효과적입니다.\n\n기술적인 측면에서 레이저의 효과는 거리와 물, 대기를 통과하면서 감소합니다. 약 400nm 파장의 레이저가 두 매체를 관통하는 데 최적입니다. 잠수함은 탐지를 피하면서 레이저를 효과적으로 발사할 수 있는 깊이에서 작동할 수 있습니다.\n\n잠수함이 직면한 문제를 해결할 수 있는 잠재적인 해결책으로는, 수중 공격을 위해 광학 레이저 대신 라디오파를 사용하는 방법이 있습니다. 이는 더 깊은 곳에서 발사할 수 있게 해주며 피해를 줄 수 있습니다. 또한, 공기와 바다의 경계에서 레이저 빔을 효과적으로 집중할 수 있는 부유 렌즈를 사용하여 잠수함이 잠수한 상태에서도 발사할 수 있게 할 수 있습니다. 잠수함은 레이저 빔을 목표로 발사하면서 유연성을 유지하기 위해 견인 렌즈 시스템을 사용할 수 있습니다. 고출력 잠수함은 초기화 속도를 달성하여 신속한 공격과 회피 기동을 할 수 있습니다.\n\n결론적으로, 레이저와 수중 환경이 도전 과제가 될 수 있지만, 혁신적인 기술과 전략을 통해 잠수함이 궤도 위협에 대한 효과적인 최후의 방어선이 될 수 있습니다.",
      "ja": null
    }
  },
  {
    "id": "0f9ecfa7eec46811",
    "title": {
      "en": "Unofficial Windows 7 Service Pack 2",
      "ko": "윈도우 7 SP2 비공식판",
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/i486girl/win7-sp2",
    "score": 248,
    "by": "XzetaU8",
    "time": 1742643903,
    "content": "Windows 7 Service Pack 2\nWarningThis project is far from finished, meaning that bugs are to be expected. If you encounter any bugs, please report them in the issue tracker or in the Discord server. Thank you for your understanding.\n\nWindows 7 Service Pack 2 is a package consisting of updates, tweaks, backported apps and overall enhancements with the goal of providing an effortless way to have a fully updated Windows 7 ISO and enhancing usability on semi-modern machines.\nTODOs\n\n - All updates until 2020, with the addition of Windows Embedded Standard 7 updates\n - Snipping Tool from Windows 10 version 1507 -> Credits: vxiduu\n - Paint from Windows 8 build 9425\n - Registry Editor from Windows 10 build 16212 -> Credits: Aurorarion\n - In-place service pack installer\n - Native USB 3.0 and NVMe support -> Credits: Unknown user from MyDigitalLife forums\n - TPM 2.0 support\n - UEFI support in the ISO releases, with EFI GOP support too -> Credits: Typical/int10h, AveragePC\n - Inclusion of VxKex -> Credits: vxiduu\n - Windows 8 PE in the ISO, with a theme that resembles Windows 7 -> Credits: ImSwordQueen (Windows 7 theme for Windows 8.x)\n - Windows 10 setup engine in the ISO release -> Credits: Aurorarion (helping out on the 10 setup engine port)\n - Builtin Webp and FLAC codecs\n - Task Manager from Windows 8 build 7880 -> Credits: Jevil7452\n - Upscaled graphics in some places\n - Windows Vista/7 PE's boot screen on the ISO -> Credits: Microsoft Corporation and Tech Stuff (Boot8Plus)\n - Removal of the annoying \"Unsupported hardware\" prompt\n - Better DPI support in aero.msstyles -> Credits: Vaporvance (high DPI classes from Aero10 that will be ported to Windows 7)\n - Integration of Microsoft Visual C++ Redistributable AIO -> Credits: Microsoft Corporation (Visual C++) and abbodi1406 (VC++ AIO repack)\n - Disk Cleanup from Windows 8 build 7861 -> Credits: Jevil7452\n - Windows 8 build 7861's PDF Reader -> Credits: Jevil7452\n - Windows 10 20H1's System Information -> Credits: Jevil7452\n - Windows 11 24H2's timeout.exe command\n - Windows 10 1709's XPS Viewer\n - Windows 11 24H2's sudo.exe command\n - Windows 10 build 9845's Notepad\n - Windows Management Framework 5.1\n - Segoe UI Emoji\n - Microsoft Agent\n - WinHlp32\n - Work Folders\n - Restore Windows Journal\n - Microsoft Camera Codec Pack\n\nInstallation\nCautionFailure to follow instructions properly will result in a bricked system! We do not take responsibility for this unless this is a legitimate bug in the installer!\n\nImportantRAID/Intel RST only works on specific systems!\n\nWarningBoth versions of the Windows 7 Service Pack 2 are x64 only! Don't file issues over the lack of 32-bit hardware support as we do not plan to support 32-bit hardware. Consult the FAQ for more information.\n\nISO installation\nImportantThe ISO releases are for clean installs only. For in-place installs, please use the installer once we release a working version of it.\n\nPick the UEFI or Legacy Boot version, depending on what your system uses\nFlash it onto a DVD or a USB flash drive\nBoot from the Windows 7 install media on the machine you wish to install it in\nProceed as usual\nFinish the out of box experience\n\nIn-place intallation\n\nDownload the Windows 7 SP2 installer under the releases of this repository\nFollow the steps in the installer as usual\n\nWarningIf the installer is stuck at a certain percentage (during the Upgrading Windows phase), don't turn off your computer! Doing so will brick your system!\n\nFAQ\nQ: My system is 32-bit in hardware. Why won't the installer and the ISO run?\nA: Supporting both 64-bit and 32-bit Windows would be very time consuming, so we chose to support only 64-bit systems. Please don't file any issues regarding the lack of 32-bit support as we have no interests of making any releases targeting 32-bit hardware.\nQ: Why don't you include ESU updates?\nA: ESU updates will be rolled out by Microsoft until 2026, and unfortunately adding ESU updates will be time consuming as it requires us to roll out new releases every time a batch of ESU updates gets rolled out.\nQ: Why VxKex instead of the dotexe1337 Windows 7 Extended Kernel?\nA: VxKex is safer than dotexe's Windows 7 Extended Kernel as it relies on external DLLs.\nQ: Does this include custom integrated GPU dri-\nA: NO\nQ: Why aren't my drives appearing in the Windows Setup screen?\nA: If you are using an RAID/RST configuration, it may well likely be that the drivers installed do not work.\nOther credits\n\nK4sum1: Inspiration for creating Windows 7 SP2\nGMM2003: \"Under construction\" wallpaper used in Public Beta builds\nOur contributors\nAnyone who reports bugs constantly",
    "summary": {
      "en": "**Summary of Windows 7 Service Pack 2**\n\nWindows 7 Service Pack 2 is a project aimed at updating and enhancing Windows 7 for better usability on newer machines. It includes updates, tweaks, and new features, but it is still in development, so users may encounter bugs.\n\n**Key Features:**\n- Updates from before 2020, including Windows Embedded Standard 7 updates.\n- New tools like Snipping Tool from Windows 10, Paint from Windows 8, and updated Task Manager.\n- Support for modern hardware features like USB 3.0, NVMe, TPM 2.0, and UEFI.\n- Integration of various applications and codecs for better functionality.\n- Removal of the \"Unsupported hardware\" prompt and improved DPI support.\n\n**Installation Instructions:**\n- The project only supports 64-bit systems; 32-bit support is not planned.\n- ISO files are meant for clean installs only, while an in-place installer will be available later.\n- Users must follow installation instructions carefully to avoid system issues.\n\n**FAQs:**\n- 32-bit systems are not supported due to the complexity of maintaining both versions.\n- ESU updates are not included because they require constant updates from Microsoft.\n- VxKex is preferred for safety over other kernel options.\n\n**Credits and Acknowledgments:**\nThe project was inspired by various contributors and includes many credits for specific features and tools.\n\n**Warning:** Due to its unfinished state, users should report any bugs they encounter.",
      "ko": "Windows 7 서비스 팩 2는 최신 기기에서 Windows 7의 사용성을 개선하기 위한 업데이트 및 향상 프로젝트입니다. 이 프로젝트는 업데이트, 조정 사항, 새로운 기능을 포함하고 있지만 아직 개발 중이므로 사용자들은 버그를 경험할 수 있습니다.\n\n주요 특징으로는 2020년 이전의 업데이트와 Windows Embedded Standard 7 업데이트가 포함됩니다. Windows 10의 스니핑 툴, Windows 8의 페인트, 업데이트된 작업 관리자와 같은 새로운 도구도 추가되었습니다. USB 3.0, NVMe, TPM 2.0, UEFI와 같은 현대 하드웨어 기능을 지원하며, 다양한 애플리케이션과 코덱이 통합되어 기능성이 향상되었습니다. \"지원되지 않는 하드웨어\" 경고가 제거되었고, DPI 지원도 개선되었습니다.\n\n설치 방법은 64비트 시스템만 지원되며, 32비트 시스템에 대한 지원은 계획되어 있지 않습니다. ISO 파일은 깨끗한 설치를 위한 것이며, 나중에 인플레이스 설치 프로그램이 제공될 예정입니다. 사용자는 시스템 문제를 피하기 위해 설치 지침을 주의 깊게 따라야 합니다.\n\n자주 묻는 질문으로는 32비트 시스템은 두 버전을 모두 유지하는 복잡성 때문에 지원되지 않으며, ESU 업데이트는 Microsoft의 지속적인 업데이트가 필요하기 때문에 포함되지 않았습니다. VxKex는 다른 커널 옵션보다 안전성 때문에 선호됩니다.\n\n이 프로젝트는 여러 기여자들의 영감을 받았으며, 특정 기능과 도구에 대한 많은 크레딧이 포함되어 있습니다. 경고로, 아직 완성되지 않은 상태이므로 사용자는 발견한 버그를 보고해야 합니다.",
      "ja": null
    }
  },
  {
    "id": "71fc049725fae7c3",
    "title": {
      "en": "Hyperlegibility",
      "ko": "하이퍼가독성",
      "ja": null
    },
    "type": "story",
    "url": "https://www.notboring.co/p/hyperlegibility",
    "score": 18,
    "by": "jger15",
    "time": 1742907743,
    "content": "Share this postNot Boring by Packy McCormickHyperlegibilityCopy linkFacebookEmailNotesMoreDiscover more from Not Boring by Packy McCormickTech strategy and analysis, but not boring. Over 241,000 subscribersSubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.Already have an account? Sign inHyperlegibilityTrading Secrets for AttentionMar 25, 2025152Share this postNot Boring by Packy McCormickHyperlegibilityCopy linkFacebookEmailNotesMore2423ShareWelcome to the 1,016 newly Not Boring people who have joined us since our last essay! If you haven’t subscribed, join 242,048 smart, curious folks by subscribing here:SubscribeToday’s Weekly Dose is brought to you by… VantaAs a startup founder, finding product-market fit is your top priority.But landing bigger customers requires SOC 2 or ISO 27001 compliance—a time-consuming process that pulls you away from building and shipping.That’s where Vanta comes in.By automating up to 90% of the work needed for SOC 2, ISO 27001, and more, Vanta gets you compliant fast. Vanta opens the door to growth.It works. Over 9,000 companies like Atlassian, Factory, and Chili Piper streamline compliance with Vanta’s automation and trusted network of security experts. Whether you’re closing your first deal or gearing up for growth, Vanta makes compliance easy.It’s time to grow your ARR. Vanta can help Get $1,000 off Vanta here:Get $1,000 Off VantaHi friends 👋, Happy Tuesday! Sometimes, you write an essay and need to come up with a title. Sometimes, you have a title and need to come up with an essay. This is the latter.Let’s get to it. HyperlegibilityWhen I was a kid, if I missed a UPenn basketball game one night, I’d have to wait until The Philadelphia Inquirer arrived the next morning, shake the dew off the bag, throw out the useless (non-Sports) sections, and search for the box score. Or watch Comcast SportsRise like a hawk to make sure I caught the 30 second Penn segment, whenever it came on.Today, when Puja decides not to stay up until 3am to watch the F1 Chinese Grand Prix, she records the race to watch when the kids nap the next day. Do you know how hard it is for her to avoid spoilers? Simply unlocking her phone is a live action game of Battleship.Information that was once hard to find is now hard to avoid.This is, of course, what the internet does. And it gets better at it all the time. But something I’ve noticed is that we do it to ourselves, too.Tyler Cowen says that he writes for the AIs now. That he writes in such a way and in such volume that the models of the future might know his mind as fully as is possible. Maybe, given the volume, the models will give his words more weight.In a recent profile in The Economist's 1843 Magazine, Cowen describes the inputs that produce those AI-legible outputs:Cowen calls himself “hyperlexic”. On a good day, he claims to read four or five books. Secretly, I timed him at 30 seconds per page reading a dense tract by Martin Luther. Later, I sat next to him while he went through an economics paper. He read it at the speed of someone checking that the pages were correctly ordered.Hyperlexic. I like that word. Hyperlexia is when a kid learns to read unusually early and surprisingly well. Cowen certainly knows this. He knows a lot. He’s repurposed the word to mean something like: extremely fast, prolific, and retentive reader; someone for whom reading is very easy.If 'hyperlexic' describes extraordinary reading ability, then let me propose a complementary word for extraordinary readability: Hyperlegible.Hyperlegibility defines our current era so comprehensively that I was shocked when I googled the term and found only references to fonts.So pardon me, yoink, I’m taking this one. HyperlegibleTM. You can use it, too.Fifteen years ago, after reading James C. Scott’s Seeing Like a State, Venkatesh Rao (VGR) wrote about A Big Little Idea Called Legibility. Before the book had become a cult classic in tech circles (VGR helped make that happen, with blog posts like this one… he made Scott’s work more legible), VGR explained its core concept: legibility.James Scott via Venkatesh RaoThe picture above illustrates an attempt to make forests legible. Because wild forests like the one on the left were “illegible” to tax authorities, “scientific forestry” transformed them into “orderly strands of the highest-yielding varieties,” Rao writes. “The resulting catastrophes – better recognized these days as the problems of monoculture – were inevitable.”This was the general thrust: over and over, the state tries to impose order (legibility) on chaos (illegibility), disaster ensues.With that context, what’s fascinating about modern Hyperlegibility is that it is not the result of top-down action; we impose it on ourselves. Or Moloch does, at least.Hyperlegibility emerges with game theoretical certainty from each of our desire to win whatever game it is you’re playing. Certainly, it’s a consequence of playing The Great Online Game. In order for the right people and projects to find you, you must make yourself legible to them. To stand out in a sea of people making themselves legible, you must make yourself Hyperlegible: so easy to read and understand you rise to the top.Once you become aware of Hyperlegibility, you see it everywhere.Remember when The New York Times psy-opped the Laptop Class into making Personal User Manuals? “I get a little grumpy if I hadn’t had my latte ;) Don’t Slack me after 9pm 😡 Honestly I’m incredibly insecure and I take that out on my subordinates 🤷” That’s Hyperlegibility.When NBA teams “solve” basketball by jacking up a lot of threes, that’s Hyperlegibility.When a venture capital firm blares its thesis to the world instead of farming it in secret, that’s Hyperlegibility.When Aella tweets a Sankey Diagram for her birthday orgy (NSFW), that’s Hyperlegibility.When a company shares its “ARR” numbers in real time instead of building in “Stealth,” that’s Hyperlegibility.No judgment. When I write an essay and send it to you, that’s Hyperlegibility.Hyperlegibility isn’t good or bad. It’s neither and both. But it certainly is. Information used to be the highest form of alpha. Now everyone bends over backwards to leak it.Through a combination of humanity getting ever-better at reading anything and humans becoming ever-more willing to make themselves legible, information is easier to find and understand than it’s ever been.When I say we’re getting better at reading anything, what I mean is that we have both all of humanity’s accumulated information and modern tools by which to discover and decipher new information at our fingertips.Let me give you an example.I was reading Dominion the other day, the book by Tom Holland on the spread of Christianity, and found this paragraph particularly striking:For almost two and a half millennia, one of the inscriptions commissioned by Darius to justify his rule of the world—written in three distinct languages, and featuring a particularly imperious portrait of the king himself—had been preserved on the side of a mountain by the name of Bisitun. Carved into a cliff some two hundred feet above the road that led from the Iranian plateau to Iraq, its survival had been ensured by its sheer accessibility. The chance to risk life and limb in the cause of deciphering ancient scripts, however, was one that the odd adventurer might positively relish. One such was Henry Rawlinson, a British officer on secondment from India to the Persian court. He first scouted out Bisitun in 1835, scaling the cliff as best as he was able, and recording as much of the inscription as he could make out. Then, eight years later, he returned to the site properly equipped with planks and ropes. Balanced precariously on a ladder, he was able to complete his transcription. ‘The interest of the occupation,’ he later recalled, ‘entirely did away with any sense of danger.’ By 1845 Rawlinson had completed a full translation of the section written in Persian, and sent it for publication in London. The Great King Spoke once more.That’s a long block quote, probably too long for an essay this length. But sacrificing flow, adding a little friction and heaviness, is the best I can do to give you a sense for the lengths people once went to to get information, to make the world more legible.Rawlinson spent a decade – not the whole decade, but still – in pursuit of information that you can now, thanks to his efforts, Google. In fact, I forgot the names of both Rawlinson and Bitisun, so a traditional in-book search came up fruitless. In the past, I would have spent, what, half an hour combing the book, or given up. Now…Easy as.And let’s say Rawlinson had never scaled Bitisun, that Darius’ words were still undiscovered. How would we get them today? A climb so treacherous as to “do away with any sense of danger”? No of course not. We would send up the drones, equipped with high resolution cameras and maybe some LiDAR, and use photogrammetry and machine learning to stitch the pictures together and make sense of their contents.What once took a decade might take a week, and the results are searchable in a second.There used to be this bit on the podcast Reply All where the show’s producer Alex Blumberg would bring a tweet he didn’t understand and co-hosts Alex Goldman and PJ Vogt would try to explain it to him. By the end, they’d try to get to three “Yeses” – meaning that all three understood the tweet.Now, you just hit the little Grok button.It costs like $100 to read your whole genome. Our telescopes can see 13 billion years into the past. Luke Farritor pulled an ancient library from the ashes of a volcano. Elad Gil is funding efforts to translate “the top 1,000 off-copyright books into all commonly spoken languages,” generate audio versions, and host language models that allow you to talk to and ask questions of each.Alexandria AIHenry Rawlinson smiles. Or weeps. I don’t know. It must all seem so easy and weightless to him.The point, I hope, is clear. We are getting better at reading the world, just as a book that is entirely illegible to a two-year-old becomes entirely legible to a ten-year-old through improved skill.The second reason for Hyperlegibility is the more fascinating one: we are tripping over ourselves to make ourselves easier to read.Think Personal User Manuals. Think Tyler Cowen writing for the AIs. Think me, writing this.We are game theoretically driven to share more and more of our best ideas, the ones that we might have once exploited in silence.Here’s an example I like that hits close to home.Imagine you’re a small emerging early stage venture capital fund. You’ve noticed something about the market that no one else has. Say you believe that quantum computing is closer than anyone else believes. You’ve identified a handful of promising companies that you want to back.What do you do? Do you keep it secret or tell the world?Well, what do you need to do in order to successfully invest against your thesis?First, you’ll need to raise money from LPs. To do that, you’ll need to spell out as clearly as possible why you believe quantum computing is more investable than everyone else does and which types of companies you think are most investable. Maybe you have a network already, and you can do all of this behind the scenes. Maybe you don’t, and you need to yell your thesis from the rooftops: blog posts, podcast appearances, conference panels, whatever. One thing a differentiated view can get you is attention. So you trade a secret for the chance at money.Assuming you raise, you’ll need to stand out to the companies you want to invest in. This is not the public markets; you can’t just invest in whatever you choose. They need to choose you, too. Maybe in the early days, you can know everyone – go to the conferences, meet them one-on-one, impress them with your insights and with the fact that you believe harder than anyone else does. Does that scale? Do your early investments pay off quickly enough to build a reputation in the community before other investors come in? How do you stand out?Say you convince companies to let you invest early, you need to help them continue to raise money from downstream capital. Which means part of your job is to make quantum legible to the very firms who might one day compete with you. Sure, while your category is small, unproven, weird, and risky, they might let you win at the early stage. But eventually, they won’t. Either way, the proximate issue is getting your companies funded so your investments don’t go to zero before your ideas have had time to play out.In all three pieces of your job, you are incentivized in the short-term to make yourself and your ideas Hyperlegible so LPs, founders, and downstream capital can find you. And without the short-term, there is no long-term.Now imagine this dynamic playing out everywhere.My friend Tina He wrote an excellent essay last week, Jevons Paradox: a personal perspective. She noticed that instead of letting us work less, AI actually incentivizes working more. The more you can do in each minute, the higher the opportunity cost from not doing anything. The problem is, even if everyone agrees that’s not what we want, who blinks?If you work less, someone else will happily take your slice of the pie. She calls it a Malthusian Trap; it’s like Scott Alexander’s Moloch or a Red Queen’s Race: \"Now, here, you see, it takes all the running you can do, to keep in the same place. If you want to get somewhere else, you must run at least twice as fast as that!\"Same same here, with Hyperlegibility. You could opt out, stop publishing, encrypt yourself. Someone else will happily fill the vacuum.Attention is the scarce resource. Information you can get.Information, long alpha, becomes beta.There’s a version of this essay that bemoans the fact that information is no longer precious.That would feel good, to mourn Friday nights at Blockbuster, metaphorically speaking, but it wouldn’t be particularly useful.The question to ask is: assuming Hyperlegibility, what do I do?This is a question that I’ll probably explore over a bunch of essays, and without having the word, already have. There’s the question of what to do as an emerging manager given the conundrum I laid out above. There’s something about the growing relative importance of relationships, of “having a guy,” of agency and the ability to get things done. There is a reason that all of these ideas have become more popular recently (hint: it’s Hyperlegibility). There’s certainly something on the value of Vertical Integration, of how you chain things together to create new forms of value. The ideas in Most Human Wins are enhanced by thinking about Hyperlegibility. And I am quite certain that the more Hyperlegible most things become, the more people will crave mysteries and The Return of Magic.But for now, I’ll leave you with this: Hyperlegibility is our reality. There’s no going back.There is a generation of people living among us who don’t feel the same nostalgia for newspapers and “not knowing the answer to something immediately” that I do, because they never had those experiences in the first place. And they’re a preview of what happens when you take all of human knowledge as a commoditized input.I’ve been talking to more college students recently, and despite rumors of their demise, the kids are very alright. Maybe every generation thinks this, but I’ve walked away from a few recent conversations thinking, “If I were competing with these people I would be COOKED.” They’ve somehow read as much as I have but have the faster brains of youth. It’s a scary combination.The other day, I asked one of them, Malhar Manek, why so many of his peers seemed so scarily advanced. His answer was that they grew up on the internet, with access to all of the information imaginable. Not just the fire hose, but the blogs and newsletters and podcasts and YouTube videos that helped make sense of the stream. So it was easy for a relatively curious kid to figure out what to read to set a baseline, and then, baseline established while the brain is still fresh and curious, to jump off of that base of knowledge to ask their own questions. To help answer them, they have all of the internet’s information, AI, and even one-DM-away access to experts.I’d never thought of it that way, because I didn’t live it. But by making the world Hyperlegible, we helped create a generation of Hyperlegibility-Native Hyperlexics who take as an input the information we worked to turn into an output. And thus civilization evolves and compounds.A priori, I would have guessed that giving everyone access to the same information would lead to a convergence of goals. That doesn’t seem to be happening. One of the people I spoke with wants to start a vertically integrated healthcare company, another wants to write a Great Book, and what I’ve heard very consistently from smart college kids is that their smartest friends are all working on biotech, specifically neuro, specifically brain-computer interfaces.Which is to say, if you think the world is Hyperlegible now, just wait.That’s all for today! If you have a minute, go get SOC-2 or ISO 27001 compliant. While you do that, we’ll be working hard on a Weekly Dose, and might even drop a new podcast on you in the interim. We’ll be back in your inbox… soon.Thanks for reading,PackySubscribe to Not Boring by Packy McCormickLaunched 6 years agoTech strategy and analysis, but not boring. SubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.152Share this postNot Boring by Packy McCormickHyperlegibilityCopy linkFacebookEmailNotesMore2423Share",
    "summary": {
      "en": "The article \"Hyperlegibility\" by Packy McCormick discusses how the modern era has made information easier to access and understand, a phenomenon he calls \"Hyperlegibility.\" \n\nKey points include:\n\n1. **Changing Information Access**: In the past, finding specific information required significant effort, but now it's readily available. People are constantly trying to avoid spoilers and stay updated, which highlights the abundance of information.\n\n2. **Hyperlexic vs. Hyperlegible**: McCormick introduces the term \"Hyperlegible,\" contrasting it with \"hyperlexic,\" which describes someone who reads quickly and comprehensively. Hyperlegibility refers to how easily information can be consumed and understood.\n\n3. **Self-Imposed Clarity**: Unlike past efforts to make information legible through top-down approaches, today, people actively make themselves and their ideas more legible to stand out in a competitive environment. This includes sharing personal insights and professional strategies openly.\n\n4. **Game Theory and Attention**: The drive to be Hyperlegible is partly due to the competitive nature of the online world, where attention is a scarce resource. People are incentivized to share ideas and insights to attract opportunities and recognition.\n\n5. **Future Implications**: As Hyperlegibility increases, it shapes how information is consumed and shared, potentially leading to a generation that thrives on quick access to knowledge. This trend raises questions about the value of information and how it will continue to evolve.\n\nIn summary, the article explores the concept of Hyperlegibility, emphasizing its impact on how individuals and organizations navigate and share information in today's fast-paced, interconnected world.",
      "ko": "Packy McCormick의 \"Hyperlegibility\"라는 글에서는 현대 사회가 정보를 더 쉽게 접근하고 이해할 수 있게 만든 현상을 설명합니다. 그는 이를 \"하이퍼레지빌리티\"라고 부릅니다.\n\n첫째, 정보 접근 방식의 변화에 대해 이야기합니다. 과거에는 특정 정보를 찾기 위해 많은 노력이 필요했지만, 지금은 쉽게 찾을 수 있습니다. 사람들은 스포일러를 피하고 최신 정보를 유지하려고 하며, 이는 정보의 풍부함을 보여줍니다.\n\n둘째, \"하이퍼레지빌리티\"와 \"하이퍼렉식\"의 차이를 설명합니다. 하이퍼렉식은 빠르고 포괄적으로 읽는 사람을 의미하는 반면, 하이퍼레지빌리티는 정보가 얼마나 쉽게 소비되고 이해될 수 있는지를 나타냅니다.\n\n셋째, 자발적인 명확성에 대해 언급합니다. 과거에는 정보의 가독성을 높이기 위해 위에서 아래로 접근했지만, 오늘날 사람들은 경쟁 환경에서 두드러지기 위해 스스로와 자신의 아이디어를 더 명확하게 만들고 있습니다. 이는 개인적인 통찰이나 전문적인 전략을 공개적으로 공유하는 것을 포함합니다.\n\n넷째, 게임 이론과 주의력에 대한 부분도 다룹니다. 하이퍼레지빌리티를 추구하는 것은 온라인 세계의 경쟁적인 성격 때문이며, 여기서 주의력은 귀한 자원입니다. 사람들은 기회와 인정을 얻기 위해 아이디어와 통찰을 공유하도록 유도됩니다.\n\n마지막으로, 하이퍼레지빌리티가 증가함에 따라 정보 소비와 공유 방식이 어떻게 변화할지를 논의합니다. 이는 지식에 대한 빠른 접근을 중시하는 세대를 만들어낼 수 있으며, 정보의 가치와 그 진화 방식에 대한 질문을 제기합니다.\n\n이 글은 하이퍼레지빌리티 개념을 탐구하며, 오늘날의 빠르게 변화하는 상호 연결된 세계에서 개인과 조직이 정보를 어떻게 탐색하고 공유하는지에 대한 영향을 강조합니다.",
      "ja": null
    }
  },
  {
    "id": "aa1e29b0af3950ea",
    "title": {
      "en": "LibreOffice downloads on the rise as users look to avoid subscription costs",
      "ko": "구독비 걱정 없는 리브레오피스 인기 급상승",
      "ja": null
    },
    "type": "story",
    "url": "https://www.computerworld.com/article/3840480/libreoffice-downloads-on-the-rise-as-users-look-to-avoid-subscription-costs.html",
    "score": 284,
    "by": "cable2600",
    "time": 1743121248,
    "content": "The free open-source Microsoft Office alternative is being downloaded by nearly 1 million users a week.\n\n\t\t\t\t\t\t\tCredit: \t\t\t\t\t\t\t\t\t\t\t\t\t\t\tNiroDesign / Getty Images\n\nInterest inLibreOffice, the open-source alternative to Microsoft Office, is on the rise, with weekly downloads of its software package close to 1 million a week. That’s thehighest download number since 2023.\n\nLibreOffice, which runs on Windows, Linux, and macOS, is a standalone desktop office productivity software that is free to use. Downloads have picked up since The Document Foundation, which manages its development,released version 25.2 last week.\n\nIt has emerged as the most popular open-source suite, with alternative versions such as Apache’s OpenOffice losing steam. The last version of OpenOffice was released in 2023.\n\n“We estimate around 200 million [LibreOffice] users, but it’s important to note that we respect users’ privacy and don’t track them, so we can’t say for sure,” said Mike Saunders, an open-source advocate and a deputy to the board of directors at The Document Foundation.\n\nThere is still noteworthy interest in LibreOffice as a standalone desktop alternative to paid office productivity suites, said Jason Wong, distinguished vice president and analyst at Gartner. “Usually these are clients seeking to keep their on-premises implementation, given that both Microsoft and Google have focused on their cloud offerings,” Wong said.\n\nCost is a factor for evaluating software suites like LibreOffice, Wong said. “The downside is the additional specialized resources and new skills needed to maintain the [software],” hesaid.\n\nDownloads of LibreOffice have been steadily climbing with each new version, Saunders said.\n\nThough LibreOffice has traditionally been a favorite of home users, there’s growing interest in businesses and government, Saunders said.The northern German state of Schleswig-Holstein last year said it would move 30,000 PCs fromMicrosoft Officeto LibreOffice.\n\n“The majority of our users — 85% — are on Windows, followed by macOS and then Linux,” Saunders said.\n\nMany desktop Linux distributions pre-install LibreOffice, but it’s unclear how many users that represents.\n\nLibreOffice users typically want a straightforward interface, Saunders said. “They don’t want subscriptions, and they don’t want AI being ‘helpful’ by poking its nose into their work — it reminds them of Clippy from the bad old days,” he said.\n\nThere are genuine use cases for generative AI tools, but many users prefer to opt-in to it and choose when and where to enable it. “We have zero plans to put AI into LibreOffice. But we understand the value of some AI tools and are encouraging developers to create … extensions that use AI in a responsible way,” Saunders said.\n\nAlthough there are cloud-based versions of OpenOffice, The Document Foundation has focused on the desktop version, Saunders said.\n\n\t\t\tRelated content\n\n\t\t\t\t\tTip\n\n\t\t\t\t6 advanced Gboard tricks for smarter Android typing\n\n\t\t\t\t\t\tBy JR Raphael\n\n\t\t\t\t\t\tMar 28, 2025\n\n\t\t\t\t\t\t\t11 mins\n\n\t\t\t\t\t\t\t\tAndroid\n\n\t\t\t\t\t\t\t\tGoogle\n\n\t\t\t\t\t\t\t\tMobile Apps\n\n\t\t\t\t\tOpinion\n\n\t\t\t\tThe secret to using generative AI effectively\n\n\t\t\t\t\t\tBy Chris Hoffman\n\n\t\t\t\t\t\tMar 26, 2025\n\n\t\t\t\t\t\t\t10 mins\n\n\t\t\t\t\t\t\t\tChatbots\n\n\t\t\t\t\t\t\t\tGenerative AI\n\n\t\t\t\t\t\t\t\tWindows\n\n\t\t\t\t\tTip\n\n\t\t\t\tGee whiz, Gboard! An invaluable new Android typing upgrade\n\n\t\t\t\t\t\tBy JR Raphael\n\n\t\t\t\t\t\tMar 26, 2025\n\n\t\t\t\t\t\t\t5 mins\n\n\t\t\t\t\t\t\t\tAndroid\n\n\t\t\t\t\t\t\t\tGoogle\n\n\t\t\t\t\t\t\t\tKeyboards\n\n\t\t\t\t\tNews\n\n\t\t\t\tOtter.ai’s voice-activated AI agent can answer questions during online meetings\n\n\t\t\t\t\t\tBy Agam Shah\n\n\t\t\t\t\t\tMar 25, 2025\n\n\t\t\t\t\t\t\t4 mins\n\n\t\t\t\t\t\t\t\tGenerative AI\n\n\t\t\t\t\t\t\t\tProductivity Software\n\n\t\t\t\t\t\t\t\tVideoconferencing\n\n\t\t\tOther Sections\n\n\t\t\t\t\t\t\t\tPodcasts\n\n\t\t\t\t\t\t\t\tVideos\n\n\t\t\t\t\t\t\t\tResources\n\n\t\t\t\t\t\t\t\tEvents\n\n\t\t\t\t\t\t\t\tSpotlight: Managing modern risks\n\n\t\t\t\t\tSUBSCRIBE TO OUR NEWSLETTER\n\n\t\t\t\tFrom our editors straight to your inbox\n\n\t\t\tGet started by entering your email address below.\n\n\t\t\t\t\t\t\tPlease enter a valid email address\n\n\t\t\t\t\tSubscribe",
    "summary": {
      "en": "LibreOffice, a free open-source alternative to Microsoft Office, is becoming increasingly popular, with nearly 1 million downloads each week. This surge follows the release of version 25.2 by The Document Foundation. LibreOffice is available on Windows, Linux, and macOS, and is now the leading open-source office suite, while alternatives like OpenOffice are declining.\n\nThe Document Foundation estimates there are around 200 million LibreOffice users, although they do not track user data for privacy reasons. Interest in LibreOffice is growing among businesses and government entities, partly due to the cost savings compared to paid software like Microsoft Office.\n\nMost users (85%) use LibreOffice on Windows, and many Linux distributions come with it pre-installed. Users appreciate its simple interface and dislike subscription models or intrusive AI features. The foundation has no plans to integrate AI into LibreOffice but supports responsible AI extensions developed by others. \n\nOverall, LibreOffice continues to gain traction as a reliable, cost-effective option for both individual and organizational use.",
      "ko": "리브레오피스는 마이크로소프트 오피스의 무료 오픈소스 대안으로, 매주 거의 100만 번 다운로드되고 있어 인기를 끌고 있습니다. 이러한 성장은 문서 재단(The Document Foundation)이 25.2 버전을 출시한 이후 더욱 두드러지고 있습니다. 리브레오피스는 윈도우, 리눅스, macOS에서 사용할 수 있으며, 현재는 주요 오픈소스 오피스 제품군으로 자리 잡고 있습니다. 반면, 오픈오피스와 같은 대안은 점차 감소하고 있습니다.\n\n문서 재단은 약 2억 명의 리브레오피스 사용자가 있을 것으로 추정하고 있지만, 개인 정보 보호를 이유로 사용자 데이터를 추적하지 않고 있습니다. 리브레오피스에 대한 관심은 기업과 정부 기관에서도 증가하고 있으며, 이는 마이크로소프트 오피스와 같은 유료 소프트웨어에 비해 비용 절감 효과가 있기 때문입니다.\n\n대부분의 사용자(85%)가 윈도우에서 리브레오피스를 사용하고 있으며, 많은 리눅스 배포판에 기본적으로 설치되어 있습니다. 사용자들은 간단한 인터페이스를 높이 평가하며, 구독 모델이나 불필요한 인공지능 기능을 싫어합니다. 문서 재단은 리브레오피스에 인공지능을 통합할 계획은 없지만, 다른 개발자들이 만든 책임 있는 인공지능 확장을 지원하고 있습니다.\n\n전반적으로 리브레오피스는 개인과 조직 모두에게 신뢰할 수 있고 비용 효율적인 옵션으로 계속해서 인기를 얻고 있습니다.",
      "ja": null
    }
  },
  {
    "id": "2480187f412ae58a",
    "title": {
      "en": "Blender releases their Oscar winning version tool",
      "ko": "블렌더, 오스카 툴 공개!",
      "ja": null
    },
    "type": "story",
    "url": "https://www.blender.org/download/releases/4-4/",
    "score": 777,
    "by": "babuloseo",
    "time": 1743035223,
    "content": "Splash artwork: Flow © Dream Well Studio, Sacrebleu Productions, Take FiveImage licensed under CC-BY-SA – https://flow.movie/\n\nWhat’s New Recap\n\nRound-up of what’s new in Blender 4.4, in detail.\n\nBlender 4.4 new features overview by Jonathan Lampel from CGCookie, Harry Blends, Paul Caggegi, and Wayne Dixon.\n\nQUALITY BLEND\n\nBlender 4.4 is all about stability. During the 2024–2025 northern hemisphere winter, Blender developers doubled down on quality and stability in a group effort called “Winter of Quality.”\n\n\t\tAmount of high severity bugs since January 1st, 2025\n\nWinter of Quality\n\nIn just a few months, developers fixed over 700 reported issues, revisited old bug reports, and addressed unreported problems.\n\nAlongside bug fixes, Winter of Quality also included tackling technical debt and improving documentation.\n\nRead Blog Post\n\nIssues Addressed per Module\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tAnimation & Rigging: 37\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tAsset System: 9\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tCore: 15\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tGrease Pencil: 147\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tModeling: 45\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tNodes & Physics: 80\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tPipeline & IO: 22\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tPlatforms, Builds & Tests: 5\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tPython API: 21\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tRender & Cycles: 53\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tSculpt, Paint & Texture: 45\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tUser Interface: 119\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tVFX & Video: 51\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tViewport & EEVEE: 47\n\n\t\t\t\t\t\t#word-cloud-block_ee6590eb760be9aa9e6087741d18482e {\n\t\t\t\tcolor: #ffffff;\n\t\t\t}\n\t\t\t\t\t\t\t\t\t#word-cloud-block_ee6590eb760be9aa9e6087741d18482e .block-words-cloud-categories > div{\n\t\t\t\tbackground: #ff3a3a;\n\t\t\t}\n\t\t\t#word-cloud-block_ee6590eb760be9aa9e6087741d18482e.block-word-cloud > ul > li strong {\n\t\t\t\tcolor: #ff3a3a;\n\t\t\t}\n\nACTION PACKED\n\nBlender 4.4 introduces Action Slots, revolutionizing animation workflows by letting multiple data-blocks share a single Action.\n\nWHAT AREACTION SLOTS?\n\nBefore Action Slots, each data-block specific animation—like an object’s position, a camera’s depth of field, or a material’s shader properties—needed its own separate Action. This made it difficult to animate multiple elements together or share animations between objects or even projects.\n\nFor example, if you wanted to animate a camera moving while also changing its depth of field, you’d need two separate Actions, which couldn’t be easily linked or reused.\n\nNow you can mix all sorts of animations such as an object’s position, its material properties, even compositing effects—all within a single Action.\n\nRelease Notes\n\nRead Manual\n\nMORE ANIMATION\n\n\t\t\tConstraints\n\n\t\t\t\t\t\t\tRelationship Lines for constraints are no longer drawn when there is no target.\n\n\t\t\tGraph Editor\n\n\t\t\t\t\t\t\tNew F-Curve Noise modifer algorithm.\n\n\t\t\tRigging\n\n\t\t\t\t\t\t\tBone collection membership is now mirrored when symmetrizing an armature.\n\n\t\t\t\t\t\t\tN-panel normalization now supports locking multiple vertex groups.\n\n\t\t\t\t\t\t\tRemoving a modifier, constraint, or shape key also deletes its driver.\n\n\t\t\tPython API\n\n\t\t\t\t\t\t\tConceptual changes in the API.\n\n\t\t\tPose Library\n\n\t\t\t\t\t\t\tMajor overhaul to Pose Assets to improve the user experience.\n\n\t\t\t\t\t\t\tDocumentation: User Manual update.\n\n\t\t\t\t\t\t#word-cloud-block_b85d66c7a871960db7d4f62fc4679261 {\n\t\t\t\tcolor: #ffffff;\n\t\t\t}\n\t\t\t\t\t\t\t\t\t#word-cloud-block_b85d66c7a871960db7d4f62fc4679261 .block-words-cloud-categories > div{\n\t\t\t\tbackground: #0a0000;\n\t\t\t}\n\t\t\t#word-cloud-block_b85d66c7a871960db7d4f62fc4679261.block-word-cloud > ul > li strong {\n\t\t\t\tcolor: #0a0000;\n\t\t\t}\n\nVSE: Vastly Superior Editing\n\nThe Video Sequencer continues to improve with quality-of-life upgrades for text editing, expanded support for codecs including H.265 and 10/12-bit videos, and performance improvements that make editing faster than ever.\n\nEDIT TEXT ON THE SPOT\n\nIntroducing: Edit mode for text strips in Preview!\n\nSimply press Tab and type away.\n\nFIND YOUR CENTER\n\nMulti-line text strips can now be properly aligned to the left, right, or center.\n\nFASTEREDITING EVERYTHING\n\nBuilding proxies for image sequences is faster now.\n\nPreview playback performance of float/HDR content is faster now.\n\nText strip background fill “Box” is several times faster for large fill areas.\n\nCurves, Hue Correct, White Balance modifiers are 1.5x-2x faster now.\n\nMany sequencer effects are slightly faster now thanks to more efficient multi-threading.\n\nVIDEOBEYOND\n\nBlender 4.4 adds support for rendering videos using the H.265/HEVC codec.\n\nVideos are now rendered in BT.709 color space now, preventing playback inconsistencies from the previously unspecified color space.\n\nAdditionally, video playback YUV->RGB conversion is more accurate now, fixing color shifts and banding in dark regions.\n\nBlender now supports 10 and 12 bit/channel videos!\n\nDuring rendering, you can set a color depth of 10 or 12 bits for supported codecs (10 bit for H.264, H.265, AV1, 12 bit for H.265, AV1).\n\nWhen reading 10-bit or 12-bit videos, they are loaded as floating-point images.\n\nA BITMORE\n\nEVEN MORE SEQUENCER\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tImproved layout for Text strip properties.\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tNew roundness property for text strips background box.\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tCustom font text strips no longer default to other fonts for missing characters.\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tSnapping now works with retiming keys.\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tDuplicate strip images in Preview area.\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t“Add Effect” menu has been improved.\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tVideo rotation metadata is now respected.\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tCopying strips or creating metastrips now includes effect chains automatically, avoiding selection errors.\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tProxies for EXR (or other float/HDR format) image strips now work properly.\n\n\t\t\t\t\t\t#word-cloud-block_79fa57d3266c700bab7ba3a976ef80de {\n\t\t\t\tcolor: #ffffff;\n\t\t\t}\n\t\t\t\t\t\t\t\t\t#word-cloud-block_79fa57d3266c700bab7ba3a976ef80de .block-words-cloud-categories > div{\n\t\t\t\tbackground: #ffff1f;\n\t\t\t}\n\t\t\t#word-cloud-block_79fa57d3266c700bab7ba3a976ef80de.block-word-cloud > ul > li strong {\n\t\t\t\tcolor: #ffff1f;\n\t\t\t}\n\n\tEXPAND\nYOUR BLENDER\n\n\t\t#paragraph-plus-block_7d3bd197e4872c4e32873562a13aad04 {\n\t\t\t\t\t\t\t\t\tfont-size: 64px;\t\t}\n\n\t\t\t\t\t#paragraph-plus-block_7d3bd197e4872c4e32873562a13aad04 em {\n\t\t\t\tbackground: rgb(221,51,51);\t\t\t\tcolor: rgb(255,255,255);\t\t\t\tfont-style: normal;\t\t\t}\n\nThe Blender Extensions platform keeps growing, with over 500 free add-ons and themes to customize your workflows.\n\nYou can also share your own add-ons and themes!\n\nBrowse Extensions\n\nShare Your Extensions\n\nMODELING\n\nPole Position\n\nA new option in the Select by Trait operator lets you select by pole count.\n\nEasily find all 3-pole or 5-pole points in your mesh.\n\nGiven their impact on topology, the default selects all poles that do not have 4 edges, allowing for easy inspection.\n\nSee Manual\n\nMODELING\n\nInfluencer\n\nJoining triangles to quads now prioritizes quad-dominant topology, creating a more structured “grid” layout. This helps maintain cleaner geometry and improves mesh flow, especially in models where uniform quads are preferred.\n\nThis behavior can be adjusted using a topology influence factor, to better control how triangles are merged.\n\nSee Manual\n\nMODELING\n\nVertex & Edge Dissolve\n\nDissolving edges may remove additional, unselected edges to ensure the mesh remains valid. Previously, this also dissolved vertices connected to those unselected edges.\n\nThe new behavior processes only vertices that belonged to the selected, now dissolved edges.\n\nSee Manual\n\nMORE MODELING\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tUp to 15% faster playback when using custom normals or sharp edges\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tN-panel normalization now supports locking multiple vertex groups\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tAdded Curves Edit Mode support for “Select Linked Pick” (L, Shift+L)\n\n\t\t\t\t\t\t#word-cloud-block_d0711cbb567333a72415a9367834dbe2 {\n\t\t\t\tcolor: #eeeeee;\n\t\t\t}\n\t\t\t\t\t\t\t\t\t#word-cloud-block_d0711cbb567333a72415a9367834dbe2 .block-words-cloud-categories > div{\n\t\t\t\tbackground: #8cff4f;\n\t\t\t}\n\t\t\t#word-cloud-block_d0711cbb567333a72415a9367834dbe2.block-word-cloud > ul > li strong {\n\t\t\t\tcolor: #8cff4f;\n\t\t\t}\n\nPUT IT TO THE TEST\n\nShare and compare your computer’s score with openly accessible benchmarks provided by the Blender community.\n\nDownload Blender Benchmark\n\nSCULPT\n\nPlane & Simple\n\nStay grounded or reach new heights with a new sculpt brush type: Plane.\n\nCustomized settings for the Plane brush type in Blender 4.4\n\nThe Plane brush is a generalization of the existing Flatten, Fill, and Scrape brushes, with new options to control stabilization and range of influence above and below the brush plane.\n\nKey features include adjustable height above the brush plane, depth control for vertices below it, and an option to invert these settings.\n\nStabilization options for the Normal (brush plane’s orientation) and Plane‘s position are also available for precise control.\n\nSee Manual\n\nMORE SCULPT\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tGrab Random Cloth and Grab Cloth now use Local Simulation Area by default.\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tSample Color operator assigned to Shift+Ctrl+X in Texture Paint\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tPrevent entering Sculpt Mode in invisible objects\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tRebuild BVH no longer adds an undo entry\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tFrame Selected renamed to Frame Last Stroke\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tNew operator property to override position on sculpt and paint modes.\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tCloth brushes now have the Persistent option off by default.\n\n\t\t\t\t\t\t#word-cloud-block_a27c9106f2b21da3cd5b4527430763f8 {\n\t\t\t\tcolor: #eeeeee;\n\t\t\t}\n\t\t\t\t\t\t\t\t\t#word-cloud-block_a27c9106f2b21da3cd5b4527430763f8 .block-words-cloud-categories > div{\n\t\t\t\tbackground: #8cff4f;\n\t\t\t}\n\t\t\t#word-cloud-block_a27c9106f2b21da3cd5b4527430763f8.block-word-cloud > ul > li strong {\n\t\t\t\tcolor: #8cff4f;\n\t\t\t}\n\n\t\tNew window decorations on Windows 11\n\nUSER INTERFACE\n\nShe Comes in Colors\n\nWindow decorations now follow the theme colors on Windows 11 and macOS.\n\nSnap Into Place\n\nEditors now softly snap to minimum and maximum sizes, with improved splitting previews and docking feedback.\n\nScrollbars are hidden for small editors.\n\nResizing editors now snaps to a minimum, maximum, and half way.\n\nScrollbars are now automatically hidden.\n\nHidden Away\n\nHorizontal scrollbars are now hidden automatically when they don’t fit in the editor.\n\nSee at a glance whether inputs are valid.\n\nNode Editor\n\nFade\n\nIn Node Editors, inputs that can’t affect output are now grayed out for group nodes, Geometry Nodes modifiers, and node tools.\n\nMore Node Editor Improvements\n\nOTHER EDITOR IMPROVEMENTS\n\n\t\t\tAsset Browser\n\n\t\t\t\t\t\t\tNew default sorting option: assets now sorted by catalog instead of name.\n\n\t\t\t\t\t\t\tNew operator to remove asset preview, available in the Asset Browser sidebar.\n\n\t\t\t\t\t\t\tBrush names are easier to read with Light theme.\n\n\t\t\tUV/Image Editor\n\n\t\t\t\t\t\t\tImage Editor view mode now uses “sample” as default tool.\n\n\t\t\t\t\t\t\tUV Editor: Ctrl-C and Ctrl-V shortcuts for copying and pasting UVs.\n\n\t\t\tStatus Bar\n\n\t\t\t\t\t\t\tShow warning when active object has non-uniform or negative scale.\n\n\t\t\t\t\t\t\tShow warning when transform operation has no effect.\n\n\t\t\t\t\t\t\tColor Picker Status Help shown for MacOS for picking outside of Blender.\n\n\t\t\t\t\t\t\tImproved status bar display for several operators.\n\n\t\t\t\t\t\t\tNotification banners are now truncated when very long.\n\n\t\t\t\t\t\t\tShow notification when hiding objects.\n\n\t\t\t3D Viewport\n\n\t\t\t\t\t\t\tDefault front face color for Face Orientation overlay is now transparent.\n\n\t\t\t\t\t\t\tMesh indices overlay setting is now always visible, regardless of Developer Extras.\n\n\t\t\t\t\t\t\t“Measure” tool items can now be deleted with gizmos off.\n\n\t\t\t\t\t\t\tAnimations can now play in Sculpt mode.\n\n\t\t\t\t\t\t\tViewport Render now displays a progress bar.\n\n\t\t\t\t\t\t\tBrush/tool falloff curve presets expanded in popovers.\n\n\t\t\t\t\t\t\tKnife tool overlay now uses gizmo theme colors.\n\n\t\t\t\t\t\t\tMore readable mesh indices.\n\n\t\t\t\t\t\t\tObject data name show shown in text overlay.\n\n\t\t\t\t\t\t\tFPS display in overlay no longer jiggles when starting.\n\n\t\t\tPreferences\n\n\t\t\t\t\t\t\tExtensions: Add button to quickly access an add-on’s folder.\n\n\t\t\t\t\t\t\tMore user preferences now reset to actual defaults.\n\n\t\t\t\t\t\t\tImproved Studio Lights Editor interface layout.\n\n\t\t\t\t\t\t\tLanguage translation options are now preserved when changing languages.\n\n\t\t\t\t\t\t\tSee all Preferences changes.\n\n\t\t\tProperties\n\n\t\t\t\t\t\t\tPersistent height and scroll position on tree-views.\n\n\t\t\t\t\t\t\tImproved container/codec ordering in FFMPEG video drop-downs.\n\n\t\t\t\t\t\t\tUI Lists now be sorted in reversed alphabetical.\n\n\t\t\t\t\t\t\tSee all Properties editor changes.\n\n\t\t\t\t\t\t#word-cloud-block_03c9d03db8ba61e400849a342fa8f1bd {\n\t\t\t\tcolor: #ffffff;\n\t\t\t}\n\t\t\t\t\t\t\t\t\t#word-cloud-block_03c9d03db8ba61e400849a342fa8f1bd .block-words-cloud-categories > div{\n\t\t\t\tbackground: #000000;\n\t\t\t}\n\t\t\t#word-cloud-block_03c9d03db8ba61e400849a342fa8f1bd.block-word-cloud > ul > li strong {\n\t\t\t\tcolor: #000000;\n\t\t\t}\n\nmacOS\n\nQuick Look\n\nOn macOS, you can now preview blend file contents in a thumbnail in Finder, App Exposé and Spotlight.\n\n\t\t.blend file previews on macOS Finder.\n\nEVERYTHING UI\n\n\t\t\tFiles\n\n\t\t\t\t\t\t\tFull file name now shown on the recent items tool-tips.\n\n\t\t\t\t\t\t\tDefault file names now capitalized as “Untitled”.\n\n\t\t\tFonts\n\n\t\t\t\t\t\t\tImproved selection from Fonts folder.\n\n\t\t\t\t\t\t\tImproved calculation of text string length for monospaced fonts.\n\n\t\t\t\t\t\t\tInterface font, “Inter”, updated to 4.1\n\n\t\t\tDialogs\n\n\t\t\t\t\t\t\tCentered dialogs can now be moved.\n\n\t\t\t\t\t\t\tPopup alerts (e.g. Python script warnings) adjust width based on content.\n\n\t\t\t\t\t\t\tQuick Favorites and other menus now support accelerators (underlined letters) for toggles.\n\n\t\t\t\t\t\t\tImproved Adjust Last Operation behavior\n\n\t\t\t\t\t\t\tReset to Defaults in Redo Panel no longer closes it.\n\n\t\t\t\t\t\t\tPreview items show a slight background in menus.\n\n\t\t\t\t\t\t\tPie menus now close on loss of window focus.\n\n\t\t\tCursors\n\n\t\t\t\t\t\t\tIncreased contrast for transform cursors.\n\n\t\t\t\t\t\t\tLarger alternative version of the “frame” cursor for high-DPI displays.\n\n\t\t\tIcons\n\n\t\t\t\t\t\t\tBatch Rename now has icons for data type.\n\n\t\t\t\t\t\t\t“Material” icon flipped horizontally to better differentiate it from “World.”\n\n\t\t\tGeneral\n\n\t\t\t\t\t\t\tRename masking property to better reflect its usage.\n\n\t\t\t\t\t\t\tEyedropper color picking can now be canceled without losing the original color.\n\n\t\t\t\t\t\t\tPreview images load faster and display fewer artifacts at different sizes.\n\n\t\t\t\t\t\t\tShow warning when image drag-and-drop fails.\n\n\t\t\t\t\t\t\tPreview items show a loading icon while loading or being rendered.\n\n\t\t\tTheme\n\n\t\t\t\t\t\t\tSelected outline color for pulldowns is now customizable.\n\n\t\t\t\t\t\t\tLight theme icon border now mirrors earlier Blender versions.\n\n\t\t\tInput\n\n\t\t\t\t\t\t\tColor picker HSL value can now be adjusted by trackpad scrolling.\n\n\t\t\t\t\t\t\tImprovements to 3Dconnexion NDOF support.\n\n\t\t\t\t\t\t\tNDOF Orbit direction now inverts when upside down.\n\n\t\t\tText Objects\n\n\t\t\t\t\t\t\tText Objects with non-default fonts no longer load missing characters from other fonts.\n\n\t\t\t\t\t\t\tText Objects with missing or invalid fonts no longer use characters from the default font.\n\n\t\t\tTooltips\n\n\t\t\t\t\t\t\tTooltips now appear while animation is playing.\n\n\t\t\t\t\t\t\tStatus colors are clearer on light backgrounds.\n\n\t\t\t\t\t\t\tColorspace tooltips show more details with expanded acronyms.\n\n\t\t\t\t\t\t\tProperly display colors without alpha.\n\n\t\t\t\t\t\t\tImproved vertical centering.\n\n\t\t\t\t\t\t\tTooltips now disappear faster with Gizmo interaction.\n\n\t\t\t\t\t\t\tNo longer redraw with slight mouse movements.\n\n\t\t\tAnimation\n\n\t\t\t\t\t\t\tAnimated values for nodes and inputs now have clearer names.\n\n\t\t\t\t\t\t\tAuto keyframe toggle works better with keyboard shortcuts.\n\n\t\t\t\t\t\t\tBetter feedback during Animation Playback timer test.\n\n\t\t\t\t\t\t\tImproved current frame indicator styling in movie clip and image editors.\n\n\t\t\t\t\t\t#word-cloud-block_b26d1ee9b65b204f87a2a6e62796d75e {\n\t\t\t\tcolor: #0a0a0a;\n\t\t\t}\n\t\t\t\t\t\t\t\t\t#word-cloud-block_b26d1ee9b65b204f87a2a6e62796d75e .block-words-cloud-categories > div{\n\t\t\t\tbackground: #ffffff;\n\t\t\t}\n\t\t\t#word-cloud-block_b26d1ee9b65b204f87a2a6e62796d75e.block-word-cloud > ul > li strong {\n\t\t\t\tcolor: #ffffff;\n\t\t\t}\n\n\tBlender is and will\nalways be free, forever.\n\n\t\t#paragraph-plus-block_88f494eee05b83d08e8869836b7c8374 {\n\t\t\t\t\t\tcolor: #ff3a3a;\t\t\tfont-size: 48px;\t\t}\n\n\t\t\t#paragraph-plus-block_88f494eee05b83d08e8869836b7c8374 {\n\t\t\t\tbackground-image: linear-gradient(45deg, rgb(255,255,255) 0%, rgb(255,255,255));\n\t\t\t\t-webkit-background-clip: text;\n\t\t\t\t-webkit-text-fill-color: transparent;\n\t\t\t}\n\nReleases are possible thanks to donations by the community.\n\n Donate Monthly\n\nDonate Once\n\nCOMPOSITOR\n\nSpeedfor Everyone\n\nThe CPU compositor was rewritten to pave the way for future development.\n\nThe rewrite provides significant improvements in performance in certain configurations of some nodes, caching of static resources like images, and less memory usage on node setups with many nodes that operate on pixels.\n\nFilter nodes are particularly faster now:\n\nLevels node is up to 10x faster.\n\nFilter and Kuwahara are twice as fast.\n\nBlur nodes up to four times faster.\n\nGlare filter is not only more advanced but also 6x more performant.\n\nPixelate node is 9x faster.\n\nAdjusting compositor node trees can be significantly faster and more interactive. That’s because the compositor now avoids computing outputs that aren’t viewed by the user through the backdrop or image editor.\n\nThe overall compositing experience should now feel more responsive, whether you’re using the CPU or GPU.\n\n          Blender 4.3\n\n          Blender 4.4\n\n        Levels\n\n        Pixelate\n\n        Glare\n\n        Pixel Nodes\n\n        Bilateral Blur\n\n        Variable Blur\n\n        Kuwahara\n\n        Masks\n\n        Lens Distortion\n\n    0246810\n\n        Relative performance (higher is better)\n\n\t\tComparison between Blender 4.3 and Blender 4.4\n\nCOMPOSITOR\n\nGlare Glow Up\n\nThe Glare node got a major revamp for better control and usability:\n\nLinkable Inputs – Most node options are now input sockets you can connect.\n\nNew Outputs – Generated glare and highlights are now exposed as output sockets.\n\nA new Strength input lets you adjust glare intensity.\n\nFog Glow and Bloom sizes are now linear and scale properly.\n\nMore realistic, energy-conserving, and properly scaled Bloom.\n\nAdjust glare saturation and tint with dedicated inputs.\n\nHighlight Control – Clamp and smooth highlights with the new Smoothness and Maximum inputs.\n\nTidy UI – Inputs are now neatly organized into collapsible panels.\n\nSee Manual\n\nMORE COMPOSITOR\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tAdded support for integer sockets\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tNew Quality option of OpenImageDenoise on the Denoise node\n\n\t\t\t\t\t\t\t\t\tFast Gaussian Blur is now much more accurate\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tImage transformations are no longer destructive until processed\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tWrapping is now Repeat in the Translate node\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tNotes on compatibility\n\n\t\t\t\t\t\t#word-cloud-block_2c33456875e6a59bac9db3cc16befb5f {\n\t\t\t\tcolor: #ffffff;\n\t\t\t}\n\t\t\t\t\t\t\t\t\t#word-cloud-block_2c33456875e6a59bac9db3cc16befb5f .block-words-cloud-categories > div{\n\t\t\t\tbackground: #c578ff;\n\t\t\t}\n\t\t\t#word-cloud-block_2c33456875e6a59bac9db3cc16befb5f.block-word-cloud > ul > li strong {\n\t\t\t\tcolor: #c578ff;\n\t\t\t}\n\n\tINDUSTRY READY\n\n\t\t#paragraph-plus-block_bdba7a4c2aba9cde90dc3322057b7fae {\n\t\t\t\t\t\t\t\t\tfont-size: 64px;\t\t}\n\n\t\t\t\t\t#paragraph-plus-block_bdba7a4c2aba9cde90dc3322057b7fae em {\n\t\t\t\tbackground: rgb(221,51,51);\t\t\t\tcolor: rgb(255,255,255);\t\t\t\tfont-style: normal;\t\t\t}\n\nAll library versions used in Blender 4.4 are aligned with the VFX Reference Platform 2025, making studio pipeline integration and maintenance easier.\n\nVFX Platform\n\nSee All Library Changes\n\nBUT WAIT, THERE’S MORE\n\n\t\t\tGeometry Nodes\n\n\t\t\t\t\t\t\tNew node “Find in String”.\n\n\t\t\t\t\t\t\tNew input nodes: Collection and Object.\n\n\t\t\t\t\t\t\tNew “Limit Surface” option available in the Subdivision Surface node.\n\n\t\t\t\t\t\t\tNormal input node now outputs proper face corner normals instead of just face normals.\n\n\t\t\t\t\t\t\tJoin Geometry and Realize Instances nodes now preserve the vertex group status of input attributes.\n\n\t\t\t\t\t\t\tThe text overlay in the 3D view works for matrix attributes now.\n\n\t\t\t\t\t\t\tPerformance: Triangulate node is 30x to 100x faster.\n\n\t\t\t\t\t\t\tSort Elements node is 50% faster in common scenarios.\n\n\t\t\t\t\t\t\tThe Warning node now has a dynamic label depending on the selected type.\n\n\t\t\t\t\t\t\tUI: Resizing nodes now support snapping.\n\n\t\t\tCore\n\n\t\t\t\t\t\t\tAdded support for rendering videos using H.265/HEVC codec.\n\n\t\t\t\t\t\t\tBLENDER_SYSTEM_SCRIPTS now supports multiple paths.\n\n\t\t\t\t\t\t\tNew BLENDER_CUSTOM_SPLASH to replace the splash screen artwork.\n\n\t\t\t\t\t\t\tEXR images that use DWAA/DWAB compression codec now have a Quality setting\n\n\t\t\t\t\t\t\t“Render Audio” can now render to AAC (.aac) format.\n\n\t\t\t\t\t\t\tAuto-save and quit.blend files are now always saved with compression.\n\n\t\t\t3D Viewport\n\n\t\t\t\t\t\t\tOverlays engine rewrite and improvements.\n\n\t\t\t\t\t\t\tExpose view Lock Rotation property in the sidebar View panel.\n\n\t\t\tUSD\n\n\t\t\t\t\t\t\tAnimated volumes from Geometry Nodes or volume modifiers are now supported for export.\n\n\t\t\t\t\t\t\tMaterial displacement for UsdPreviewSurface is now supported in import and export.\n\n\t\t\t\t\t\t\tPoint instancers with animated attributes are now supported on import.\n\n\t\t\t\t\t\t\tThe experimental “Instancing” option now supports object hierarchies and non-mesh geometry (e.g., curves, point clouds).\n\n\t\t\t\t\t\t\tNew Python hooks.\n\n\t\t\t\t\t\t\tAdded “Merge parent Xform” option to control USD prim merging with its Xform parent during import for better hierarchy preservation.\n\n\t\t\t\t\t\t\tAdded “Apply Unit Conversion Scale” option to scale objects by the USD stage’s meters per unit value.\n\n\t\t\t\t\t\t\tAdded “Merge parent Xform” option to control whether Blender object transforms are written to their data prim or kept separate on export, reducing USD prim count and preserving hierarchy.\n\n\t\t\t\t\t\t\tAdded “Units” and “Meters Per Unit” options to set the USD Stage measurement or a custom value.\n\n\t\t\t\t\t\t\tUSD & Alembic: Edge and vertex crease processing now respects the value range expected by OpenSubdiv.\n\n\t\t\tVulkan (experimental)\n\n\t\t\t\t\t\t\tHuge performance improvements.\n\n\t\t\t\t\t\t\tCompatibility and Known Issues.\n\n\t\t\tGrease Pencil\n\n\t\t\t\t\t\t\tSeveral operators and functionality from the old Grease Pencil were restored.\n\n\t\t\t\t\t\t\tProperties of locked materials can be edited (similar to properties of locked layers)\n\n\t\t\t\t\t\t\tInvisible layers are no longer part of evaluated data.\n\n\t\t\t\t\t\t\tUI: The preview icon of locked materials is no longer grayed out.\n\n\t\t\t\t\t\t\t“Lock all” and “Unlock all” operators now work on Layer Groups.\n\n\t\t\t\t\t\t\t“Hide Others” operator now also considers layer groups.\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tVertex colors and layer tinting render in “Solid” shading mode.\n\n\t\t\tglTF\n\n\t\t\t\t\t\t\tAdded support for importing Action Slots.\n\n\t\t\t\t\t\t\tAdd option to not select created objects.\n\n\t\t\t\t\t\t\tAdd option to import scene extras or not.\n\n\t\t\t\t\t\t\tExport: Several (breaking) changes to hooks.\n\n\t\t\t\t\t\t\tExport: Always bake scene animation, so driven animated properties can be exported.\n\n\t\t\t\t\t\t\tExport: Add interpolation fallback option.\n\n\t\t\t\t\t\t\tExport: Several improvements to Collection export.\n\n\t\t\t\t\t\t\tBug fixes\n\n\t\t\tPython API\n\n\t\t\t\t\t\t\tNew: `bpy.app.module` indicates if Blender is running as Python module.\n\n\t\t\t\t\t\t\tNew property to check if installation is portable.\n\n\t\t\t\t\t\t\tNew operations for Curves.\n\n\t\t\t\t\t\t\tNew properties for Nodes.\n\n\t\t\t\t\t\t\tGrease Pencil Python API updates.\n\n\t\t\t\t\t\t\tVSE: Major API breaking changes and deprecated properties.\n\n\t\t\t\t\t\t\tAnimation: Slotted Action related additions, deprecated properties and breaking changes.\n\n\t\t\t\t\t\t\tComplete list of API breaking changes.\n\n\t\t\tCycles\n\n\t\t\t\t\t\t\tImproved OptiX Denoiser\n\n\t\t\t\t\t\t\tMore accurately render sub-pixel bump mapping.\n\n\t\t\t\t\t\t\tBaking: Speed up Selected to Active baking.\n\n\t\t\t\t\t\t\tOSL: Improved closure compatibility with MaterialX.\n\n\t\t\t\t\t\t\tImproved Sample Subset\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tMore robust host memory fallback when the GPU runs out of memory.\n\n\t\t\t\t\t\t\tNVIDIA: Support GeForce RTX 50×0 series (Blackwell)\n\n\t\t\t\t\t\t\tAMD: Support RX 90×0 series (RDNA4)\n\n\t\t\t\t\t\t\tAMD: HIP RT library updates and minimum driver version increased.\n\n\t\t\t\t\t\t\tIntel: Minimum driver version increased.\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tIntel: Improved image texture sampling performance.\n\n\t\t\tSpreadsheet\n\n\t\t\t\t\t\t\tThe selection filter for meshes now gives expected results for non-vertex domains.\n\n\t\t\tOutliner\n\n\t\t\t\t\t\t\tImproved vertex group sorting.\n\n\t\t\t\t\t\t\tSupport Ctrl/Shift for excluding collections.\n\n\t\t\t\t\t\t\tFixed overlapping icons with some display options.\n\n\t\t\t\t\t\t\tCan now un-isolate collection when a linked collection is present.\n\n\t\t\t\t\t\t\tNon-object active item text now drawn in “text high” color.\n\n\t\t\t\t\t\t\tDrag and drop to scene now updates the view.\n\n\t\t\t\t\t\t\tChild objects linked to other collections are now faded.\n\n\t\t\tWindows\n\n\t\t\t\t\t\t\tCopy and paste OS image paths into Image Editor.\n\n\t\t\t\t\t\t\tFile system volume names display correctly with high-bit Unicode characters.\n\n\t\t\t\t\t\t\tAltGr key is now treated as regular Alt key.\n\n\t\t\tmacOS\n\n\t\t\t\t\t\t\tImproved color picking outside of Blender windows.\n\n\t\t\t\t\t\t\tKeymaps can be searched with native key names.\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tMain window title improvements.\n\n\t\t\t\t\t\t\tEnhanced NDOF device input handling when using 3DConnexion driver v10.8.7 and later.\n\n\t\t\tLinux\n\n\t\t\t\t\t\t\tFixed text pasting from Blender to certain other applications (such as Firefox) not working under X11.\n\n\t\t\t\t\t\t\t\t\t.block-words-cloud-categories > div {\n  --box-background-color: #1e2225;\n}\n\nPlus hundreds of bug fixes, code cleanups and refactors.See the full list of changes.\n\nCREDITS\n\nList of developers that contributed to Blender 4.4\n\nBlender is a community project.Learn more on how you can contribute to Blender.\n\nSplash artwork: Flow © Dream Well Studio, Sacrebleu Productions, Take Five – Licensed under CC-BY-SA – flow.movieHuge thanks to everyone involved 🧡\n\nThe Blender team. March 18th, 2025",
    "summary": {
      "en": "**Blender 4.4 Summary**\n\nBlender 4.4 focuses on improving stability and quality, with developers fixing over 700 bugs and enhancing documentation during a project called the \"Winter of Quality.\" Key updates include:\n\n1. **Action Slots**: This feature allows multiple animations to be combined into a single Action, simplifying animation workflows and making it easier to animate multiple elements together.\n\n2. **Video Sequencer Enhancements**:\n   - Improved text editing and codec support (including H.265).\n   - Faster performance for editing, playback, and rendering.\n   - New features for text strips, such as better alignment and faster proxy building.\n\n3. **Modeling Improvements**:\n   - New selection options and better geometry handling to maintain clean mesh flow.\n   - Enhanced speed for playback and operations in the modeling workspace.\n\n4. **Sculpting**: A new Plane brush type provides more control during sculpting, along with several other brush enhancements.\n\n5. **User Interface Updates**: \n   - Modernized window decorations on Windows 11 and macOS.\n   - Improved snapping and resizing features for editors.\n   - Enhanced usability across various editors with clearer tooltips and improved layout.\n\n6. **Compositor Performance**: Major speed improvements for various nodes, making the compositor more responsive and efficient.\n\n7. **Additional Features**:\n   - Support for new video formats and rendering options.\n   - Enhanced Geometry Nodes with new functionalities.\n   - Improvements in the Python API and overall usability enhancements.\n\nOverall, Blender 4.4 delivers significant improvements in animation, video editing, modeling, and performance, making it a more robust tool for creators.",
      "ko": "Blender 4.4는 안정성과 품질 향상에 중점을 두고 있으며, 개발자들이 700개 이상의 버그를 수정하고 \"품질의 겨울\"이라는 프로젝트를 통해 문서화를 개선했습니다. 주요 업데이트 내용은 다음과 같습니다.\n\n액션 슬롯 기능이 추가되어 여러 애니메이션을 하나의 액션으로 결합할 수 있게 되었습니다. 이를 통해 애니메이션 작업 흐름이 간소화되고 여러 요소를 함께 애니메이션화하는 것이 더 쉬워졌습니다.\n\n비디오 시퀀서에서는 텍스트 편집과 코덱 지원이 개선되었으며, H.265도 포함됩니다. 편집, 재생 및 렌더링 속도가 빨라졌고, 텍스트 스트립에 대한 새로운 기능으로는 더 나은 정렬과 빠른 프록시 생성이 추가되었습니다.\n\n모델링 부분에서는 새로운 선택 옵션과 더 나은 기하학 처리 기능이 도입되어 깨끗한 메쉬 흐름을 유지할 수 있게 되었습니다. 모델링 작업 공간에서의 재생 및 작업 속도도 향상되었습니다.\n\n조각 도구에서는 새로운 평면 브러시 타입이 추가되어 조각하는 동안 더 많은 제어가 가능해졌고, 여러 브러시 개선 사항도 포함되었습니다.\n\n사용자 인터페이스는 Windows 11과 macOS에서 현대화된 창 장식으로 업데이트되었으며, 편집기에서 스냅 및 크기 조정 기능이 개선되었습니다. 다양한 편집기에서 더 명확한 툴팁과 개선된 레이아웃으로 사용성이 향상되었습니다.\n\n합성기 성능도 크게 개선되어 여러 노드의 속도가 빨라져 합성기가 더 반응성이 뛰어나고 효율적으로 작동합니다.\n\n추가 기능으로는 새로운 비디오 포맷과 렌더링 옵션 지원, 새로운 기능이 추가된 기하학 노드, 파이썬 API 개선 및 전반적인 사용성 향상이 포함됩니다.\n\n전반적으로 Blender 4.4는 애니메이션, 비디오 편집, 모델링 및 성능에서 중요한 개선을 이루어 창작자들에게 더 강력한 도구가 되었습니다.",
      "ja": null
    }
  },
  {
    "id": "aad4bbdfbd20907f",
    "title": {
      "en": "Source code art in the Rivulet language",
      "ko": "리뷰렛 언어의 코드 아트",
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/rottytooth/Rivulet",
    "score": 162,
    "by": "cranbor",
    "time": 1743076519,
    "content": "Rivulet\nRivulet is a programming language of flowing strands, written in semigraphic characters. A strand is not pictographic: its flow does not simulate computation. There are four kinds of strands, each with their own symbolism and grammatical rules. Together, they form glyphs, tightly-packed blocks of code whose strands execute together.\nHere is a complete Fibonacci program:\n   ╵──╮───╮╭─    ╵╵╭────────╮\n    ╰─╯╰──╯│       ╰─╶ ╶╮╶╮╶╯\n   ╰─────╮ │      ╭─────╯ ╰─────╮\n         ╰─╯ ╷    ╰───       ───╯╷\n\n   ╵╵─╮  ╭─╮     ╭──       ╵╵╰─╮  ──╮──╮\n      ╰─╮│ ╰─╯ ╵╵╰─╯╶╮       ╴─╯  ╭─╯╭─╯\n      ╰─╯╰─ ╰──╯╰────╯       ╭╴ ╵╶╯ ╶╯╶╮\n        ╭─╮ ╭╴               │  ╰──────╯\n        │ │ │                ╰─╮       ╭─╮\n      │ │ ╰─╯                  │     │   │\n      ╰─╯            ╷         ╰──── ╰───╯╷\n\n   ╵╵ ╭──  ──╮  ╭─╮         ╵╰─╮\n      ╰─╮  ╭─╯╭─╯ │          ╴─╯\n       ╶╯╵╶╯  │ ╷╶╯          ╭─╮\n     ╭─╮ ╰────╯ │   ╭─╮        │\n     │ ╰────╮ ╭─╯ ╭╴│ │      ╭─╯\n     ╰────╮ │ │ │ │ │ │      │\n     ╭────╯ │ │ ╰─╯ │ ╷      ╰─╷\n     ╰────╮ │ ╰─────╯ │\n          │ ╰─────────╯╷\n\nHere is the same program formatted by the interpreter into an svg, alongside two variations that produce equivalent computer instructions:\n\nFibonacci 1\nFibonacci 2\nFibonacci 4\n\n⚠️ WARNING\n\nStatus: Version 0.4. This is a mostly-working interpreter, and a tool to generate svg files of source code. The command list will likely need to expand for usability.\n\nDesign Philosophy\nRivulet is a list-based language that avoids ordinary approaches to branching and looping. Strands never split and no strand is left un-executed.\nIts writing system was inspired by the satisfying compactness of mazes, Anni Albers's Meanders series, and space-filling algorithms. Its calligraphic aspects draw from natural language and favor the ability to write by hand.\nData Model\nIn Rivulet, data is organized into lists of adjacent cells, populated with zeros by default. Commands are applied to either a single cell or an entire list. They take a second parameter, a constant or the value of another cell.\nCommands can also be run list-to-list, applying the command to each successive cell of one list, from the corresponding cells of the other. While these consider zero-populated cells as well, a list-to-list command ends at the last cell holding a value in either list.\nThe first list, List 1, is sometimes used as the output stream. This is an interpreter setting, as is whether they are displayed as numerical data or a Unicode string (where each value is rounded to the nearest integer).\nControl Flow\nEvery strand of every glyph runs in a Rivulet program; there is no equivalent of an \"if\" statement. If a glyph leads to an unwanted state, that glyph and the others of its block (all contiguous glyphs of the same level or higher), can be rolled back, setting the execution state to what it was before the glyph (or set of glyphs) fired. The conditional rollback is the only form of branching in Rivulet. Loops only end with a rollback of their last iteration. Tests for rollback are that a single cell or an entire list is either zero or non-zero, indicated by a special strand called the Question Strand.\nData strands are run in the order they begin at the top left, moving through each column flowing to the right. So the strand beginning at coordinate 2,0 is run, then 2,1, then 3,0, and so on. Question strands are always run after the data strands are executed.\nSyntax\nRivulet's nuanced grammar may seem overwhelming at first but becomes easy to read and write with practice.\nGlyphs\nGlyphs begin with markers: ╵ in the upper left and end with ╷ at the bottom right. They must not have a vertically-oriented character directly above or below them, or they'll be confused for strands. Any text outside of glyph markers is ignored.\nThe level of the glyph is marked by how many ╵s appear at the beginning of the glyph. Levels tell where glyphs fall within larger blocks of code.\nGlyphs can be arranged vertically or side-by-side. They are read in the order of their starting marker location: top-to-bottom, left-to-right.\nIn other words, this program:\n1 ╵╰──╮╰─ ╭──╯ ╶╮\n2    ─┘   └─    │\n3    ╭──────────┘\n5    └────────  ╷\n\n1 ╵╵     ╭───╮ ╭─\n2    ╴─╮╶╯╶╮ ╷╶╯\n3  ╵╰──┘   │\n5  ╰───────╯\n\nis identical to this one:\n1 ╵╰──╮╰─ ╭──╯ ╶╮ ╵╵     ╭───╮ ╭─\n2    ─┘   └─    │    ╴─╮╶╯╶╮ ╷╶╯\n3    ╭──────────┘  ╵╰──┘   │\n5    └────────  ╷  ╰───────╯\n\nLines of code\nThe interpreter refers to code locations in terms of glyph numbers and then line numbers. Line numbers reset to 1 in each glyph. After line 1, they are numbered for each successive prime. These numbers are semantically meaningful for some strands.\nOther strand types use horizontal line numbers, counting in primes away from their starting hook. They always begin on line 1 and their neighbors are 2 on each side. They progress through primes, but always in distance from their starting point. Line numbers are every-other-line vertically: this is so vertical lines are not packed too tightly.\nHere are examples of such strands:\n  ╭─╮ ╭╴  ╭╴\n  │ │ │   │ │\n│ │ ╰─╯   │ │\n╰─╯       ╰─╯\n5 3 2 1   1 2\n\nLexemes\nRivulet commands are written with these signs. Some re-use characters in a way that only context can disambiguate:\n\nName\nSigns\nContext\nInterpretation\n\nGlyph Start and End\n╵ ╷\nNot be adjacent another sign with a vertical reading\nMarks the glyph, the smallest block of code in Rivulet\n\nLocation\n╵ ╷ ╴╶\nLeaves a gap, to punctuate the end of a strand e.g. from left: ──╶\nA reference pointer to a cell\n\nContinue\n─ │\nContinues the flows in the same direction e.g.  ────\nDepending on the strand type, it can add or subtract the line number of its horizontal or vertical line number, or simply continue the strand\n\nCorner\n╯┘╰└ ╮┐╭┌\nSharp or curved corners have the same meaning and can be used interchangeably\nTurns direction of flow\n\nHook\n╯┘╰╴└╴ ╮┐╭╴┌╴\nIt's a character or characters that turn ninety degrees at the beginning of some strands. If it turns to the right or left, it is extended with a half-length line, the same character used to indicate Location, but flipped to extend the hook and not leave a gap.\n\nNon-hook Begin Strand\n╷ above a │\nStrands with no hook begin with the half-length character to extend it\nMarks the beginning of a Question Strand\n\nData Strands\nValue Strands\nA value strand indicates a command that takes a constant value. Value strands (and other data strands), begin with a hook that points up (as in the third strand below) or to the left (as in the first two). All three of the strands below are value strands:\n1 ╵╰──╮╭──╯╶╮\n2    ─┘└─   └─╮\n3\n5              ╷\n\nEach of these value strands writes to list 1, as their hooks sit on line 1. The first strand writes to the first cell (cell 0), as it appears first on that line, the second writes to the second cell (cell 1), etc.\nThe first strand moves two spaces (two ── characters) to the right on line 1, adding 1 twice. It then moves one ── to the left on line 2, subtracting two. This leaves zero. This makes the first strand a zero strand. The default command applied to a strand is addition assignement, and so zero strands usually invoke no operation.\nThe second strand is also a zero strand: it makes the same motions in reverse of the first strand, subtracting two and then adding two strand back.\nThe third strand adds the value two to the third cell or list 1. The importance of the two zero strands is in marking the third strand to write to cell 2 of list 1, rather than cell 0.\nReference Strands\nReference strands look identical to value strands, only they end with a Location Marker, a small gap that punctuates the end of the strand. It appears in the two top strands here:\n1 ╵╰──╮  ╭──╯\n2   ╴─┘╶╮└─╶\n3       └─╮\n5            ╷\n\nThe movement of Reference Strands back and forth through the glyph has no effect on what they reference; only where they end.\nThe first strand above is no longer a Zero Strand, but a reference to the first cell (cell 0) of List 2. The second strand beginning on line 1 refers to the second cell (cell 1) of List 2. This is because between those two strands is a strand writing to cell 0 of List 2. If we wanted both of the top strands to read from cell 0 of List 2, we would move its end to before that assignment (here using the vertical version of the Location Marker):\n1 ╵╰──╮╭────╯\n2   ╴─┘╷╶╮\n3        └─╮\n5            ╷\n\nAction Strands\nThe default command is addition assignment ( += ). To choose another commands, we create an Action Strand to apply to an existing data strand.\nAction Strands have hooks that point down or to the right. They sit directly below the data strand they apply to. If two data strands' hooks are aligned vertically, the top action strand applies to the top data strand, the second to the second, etc.\nWhere a data strand's value is determined by movements to the left and right, action strands determine value through vertical movement. Their line numbers are independent of the other strands in the glyph, each beginning with line 1 as the column where they begin. Their neighbors to the left and right are line 2, followed by 3 and 5.\nEXAMPLE: This command that raises the values of list 1, cells 0 and 1, each to their fourth power:\n 1 ╵╰─╮ ╰─╮\n 2    │   │\n 3  ╭╴└─╭╴└─\n 5  │   │\n 7  │   │\n11  ╰─╮ ╰─╮\n13    │   │ ╷\n    1 2 1 2\n\nThe action strands each have a value of 4, which corresponds to exponentiation_assignment, under data strands of value 4. Here is the (INCOMPLETE) command list, showing which values assign to what command:\n\nValue\nCommand\nInterpretation\n\ndefault\naddition_assignment\nadd to location, set to zero by default\n\n0\noverwrite\nassignment, overwriting existing value\n\n1\ninsert\ninserts value after indicated cell\n\n-1\nsubtraction assignment\n\n2\nmultiplication assignment\n\n-2\ndivision assignment\n\n3\nno-op\nTBD; currently only has value when assigned to list\n\n-3\nmod assignment\nmodulus of cell value against supplied argument\n\n4\nexponentiation assignment\nraise to power of supplied argument\"\n\n-4\nroot assignment\ntake root at power of supplied argument\n\n:WARNING: It is every-other-line that increments between the primes, as the vertical length for a block-drawing char is longer than their horizontal length. This sounds confusing but is usually clear visually.\nHere is an example of two action strands and their numbering:\n  ╭─╮ ╭╴  ╭╴\n  │ │ │   │ │\n│ │ ╰─╯   │ │\n╰─╯       ╰─╯\n5 3 2 1   1 2\n\nThe first strand has a value of: (1 - 2 + 2*3 - 5) = 2, multiplication assignment. The second strand has a value of (2 * 1) - (2 * 2) = -2, division assignment.\nList indicator\nAction strands can also mark that a command applies not to a single cell (as is the default) but to an entire list. This is indicated by ending an action strand with a horizontal movement. When a list indicator appears, the data strand maintains the same order as if it were its cell that updates. If cell 3 has an action strand, it is still run after cell 2 and before cell 4 strands.\nList 2 List\nIf an action strand ends with a location marker (the tiny gap), it shows that the action should be applied for every cell of the referenced list to every cell of the assigned list. This is only syntactically valid when the data strand also ends with a location marker (is a reference strand).\nEvery cell with a number in the second list is applied to the cells in the first.\nQuestion Strand Sets\nQuestion Strands appear in pairs, one above the other.\nTogether, they pose a question about the state of the data. Should it be found wanting, the glyph and its siblings (those at the same level) are rolled back. If in a loop, only the most recent iteration is undone. This is the only way to exit a loop.\nThe top question strand begins with a vertical line. It ends either to the left or right of where it began (above or below has no semantic meaning).\nThe bottom question strand begins directly above its partner. It too ends either to the left or right of where it began, and it ends with a vertical piece (indicating the question applies only to a single cell) or a horizontal piece (indicating the entire list is to be questioned, the answer an accumulation of its answer).\nQuestion strands, read only by their beginning vs end, can move back and forth through the glyph, filling in blank spaces. They are often decorative, gap-filling lines.\nQuestion lines always fail if an item is less than or equal to zero.\n\nTop Line\nBottom Line\nUse\nChecks\n\nLeft\nHorizontal\nIf\nList (all items)\n\nLeft\nVertical\nIf\nCell\n\nRight\nHorizontal\nWhile\nList\n\nRight\nVertical\nWhile\nCell\n\n(any) vs (all) are equivalent if testing only a single cell",
    "summary": {
      "en": "**Summary of Rivulet Programming Language**\n\nRivulet is a unique programming language that uses flowing strands represented by semigraphic characters. It consists of four types of strands, each with its own rules. These strands combine to form glyphs, dense blocks of code that execute together.\n\n**Key Features:**\n- **Programming Structure:** Rivulet is a list-based language where data is organized in cells, starting with zeros. Commands can target individual cells or entire lists.\n- **Execution Flow:** All strands in a glyph execute without splitting. If a glyph leads to an undesirable state, the program can roll back to a previous state, which is its only form of branching.\n- **Syntax:** Glyphs are marked with specific characters, and their arrangement is flexible. They must be read in a top-to-bottom, left-to-right order.\n- **Data Handling:** Commands can be simple (like addition) or complex (like exponentiation). Strands can reference or modify data in the lists.\n\n**Control Flow:**\n- There are no traditional \"if\" statements; the only branching occurs through rollback.\n- A special type of strand called the Question Strand checks the state of data and can trigger rollbacks.\n\n**Commands and Operations:**\n- Rivulet supports various operations, including addition, subtraction, and multiplication, with specific symbols for each.\n- Action Strands can apply commands to either a single cell or an entire list.\n\n**Design Philosophy:**\n- Inspired by mazes and calligraphy, Rivulet aims for a compact and visually appealing coding style.\n- Its grammar, while complex initially, becomes easier to navigate with practice.\n\nOverall, Rivulet is an innovative programming language that emphasizes a different approach to coding, focusing on the flow of data and visual structure.",
      "ko": "리뷰렛은 반그래픽 문자로 표현된 흐르는 줄을 사용하는 독특한 프로그래밍 언어입니다. 이 언어는 각각의 규칙을 가진 네 가지 유형의 줄로 구성되어 있으며, 이 줄들이 결합되어 함께 실행되는 기호를 형성합니다.\n\n리뷰렛의 주요 특징 중 하나는 리스트 기반의 구조입니다. 데이터는 0부터 시작하는 셀에 조직되며, 명령은 개별 셀이나 전체 리스트를 대상으로 할 수 있습니다. 기호 내의 모든 줄은 분리되지 않고 함께 실행됩니다. 만약 기호가 바람직하지 않은 상태로 이어지면, 프로그램은 이전 상태로 되돌아갈 수 있으며, 이것이 유일한 분기 방식입니다. 기호는 특정 문자로 표시되며, 그 배열은 유연합니다. 읽는 순서는 위에서 아래로, 왼쪽에서 오른쪽으로 해야 합니다.\n\n명령은 단순한 것(예: 덧셈)부터 복잡한 것(예: 지수 계산)까지 다양합니다. 줄은 리스트 내의 데이터를 참조하거나 수정할 수 있습니다. 전통적인 \"if\" 문은 없으며, 유일한 분기는 롤백을 통해 이루어집니다. 질문 줄이라는 특별한 유형의 줄은 데이터의 상태를 확인하고 롤백을 유발할 수 있습니다.\n\n리뷰렛은 덧셈, 뺄셈, 곱셈 등 다양한 연산을 지원하며, 각 연산에 대해 특정 기호가 있습니다. 액션 줄은 명령을 단일 셀이나 전체 리스트에 적용할 수 있습니다. 디자인 철학은 미로와 서예에서 영감을 받아, 간결하고 시각적으로 매력적인 코딩 스타일을 지향합니다. 문법은 처음에는 복잡할 수 있지만, 연습을 통해 쉽게 익힐 수 있습니다.\n\n전반적으로 리뷰렛은 데이터의 흐름과 시각적 구조에 중점을 두고, 코딩에 대한 새로운 접근 방식을 강조하는 혁신적인 프로그래밍 언어입니다.",
      "ja": null
    }
  },
  {
    "id": "1c2129988d9f3279",
    "title": {
      "en": "That Hit Song You Love Was a Total Fluke",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://hbr.org/2013/11/was-gangnam-style-a-fluke",
    "score": 13,
    "by": "gradus_ad",
    "time": 1743181011,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "269a5770b10e72b6",
    "title": {
      "en": "FPGA-Based Implementation of Signal Processing Systems (2nd Edition)",
      "ko": "FPGA 신호 처리 혁명",
      "ja": null
    },
    "type": "story",
    "url": "https://www.wiley.com/en-us/FPGA-based+Implementation+of+Signal+Processing+Systems%2C+2nd+Edition-p-9781119077954",
    "score": 3,
    "by": "teleforce",
    "time": 1743185817,
    "content": "Subjects General & Introductory Electrical & Electronics Engineering Signal Processing\n\nFPGA-based Implementation of Signal Processing Systems, 2nd EditionRoger Woods,John McAllister,Gaye Lightbody,Ying YiISBN: 978-1-119-07795-4May 2017360 pagesRead an ExcerptIndex (PDF)Chapter 01 (PDF)Table of Contents (PDF)Product OverviewDownload Product FlyerDescriptionRelated ProductsDigital Evaluation CopyAbout the AuthorPermissionsTable of ContentsFPGA-based Implementation of Signal Processing Systems, 2nd EditionRoger Woods,John McAllister,Gaye Lightbody,Ying YiISBN: 978-1-119-07795-4May 2017360 pagesDigital Evaluation CopyRequest Digital Evaluation Copy\n\nDigital Evaluation CopyRequest Digital Evaluation Copy\n\nE-Book978-1-119-07796-1February 2017$105.00Hardcover978-1-119-07795-4May 2017$130.95O-Book978-1-119-07923-1February 2017Available on Wiley Online Library\n\nAn important working resource for engineers and researchers involved in the design, development, and implementation of signal processing systems  The last decade has seen a rapid expansion of the use of field programmable gate arrays (FPGAs) for a wide range of applications beyond traditional digital signal processing (DSP) systems. Written by a team of experts working at the leading edge of FPGA research and development, this second edition of FPGA-based Implementation of Signal Processing Systems has been extensively updated and revised to reflect the latest iterations of FPGA theory, applications, and technology. Written from a system-level perspective, it features expert discussions of contemporary methods and tools used in the design, optimization and implementation of DSP systems using programmable FPGA hardware. And it provides a wealth of practical insights—along with illustrative case studies and timely real-world examples—of critical concern to engineers working in the design and development of DSP systems for radio, telecommunications, audio-visual, and security applications, as well as bioinformatics, Big Data applications, and more. Inside you will find up-to-date coverage of:    FPGA solutions for Big Data Applications, especially as they apply to huge data sets  The use of ARM processors in FPGAs and the transfer of FPGAs towards heterogeneous computing platforms  The evolution of High Level Synthesis tools—including new sections on Xilinx's HLS Vivado tool flow and Altera's OpenCL approach  Developments in Graphical Processing Units (GPUs), which are rapidly replacing more traditional DSP systems    FPGA-based Implementation of Signal Processing Systems, 2nd Edition is an indispensable guide for engineers and researchers involved in the design and development of both traditional and cutting-edge data and signal processing systems. Senior-level electrical and computer engineering graduates studying signal processing or digital signal processing also will find this volume of great interest.",
    "summary": {
      "en": "The book \"FPGA-based Implementation of Signal Processing Systems, 2nd Edition\" by Roger Woods and others is a key resource for engineers and researchers working on signal processing systems. It highlights the growing use of field programmable gate arrays (FPGAs) in various applications beyond traditional digital signal processing.\n\nKey points include:\n- The book has been updated to include the latest advancements in FPGA technology and applications.\n- It discusses contemporary design methods and tools for implementing digital signal processing systems using FPGAs.\n- The content includes practical insights, case studies, and examples relevant to fields like telecommunications, audio-visual, security, and bioinformatics.\n- Topics covered include FPGA applications in Big Data, the integration of ARM processors, advancements in High Level Synthesis tools, and the rise of Graphical Processing Units (GPUs) as alternatives to traditional DSP systems.\n\nThis edition is beneficial for engineers, researchers, and senior electrical and computer engineering students interested in both traditional and innovative signal processing techniques.",
      "ko": "로저 우즈와 다른 저자들이 쓴 \"FPGA 기반 신호 처리 시스템 구현, 제2판\"은 신호 처리 시스템에 대해 연구하는 엔지니어와 연구자들에게 중요한 자료입니다. 이 책은 전통적인 디지털 신호 처리 분야를 넘어 다양한 응용 분야에서 필드 프로그래머블 게이트 어레이(FPGA)의 사용이 증가하고 있음을 강조합니다.\n\n이 책은 FPGA 기술과 응용 분야의 최신 발전 사항을 반영하여 업데이트되었습니다. 또한 FPGA를 사용하여 디지털 신호 처리 시스템을 구현하는 현대적인 설계 방법과 도구에 대해 논의합니다. 내용에는 통신, 오디오-비주얼, 보안, 생물정보학 등 다양한 분야와 관련된 실용적인 통찰력, 사례 연구, 예제가 포함되어 있습니다. 다루는 주제에는 빅데이터에서의 FPGA 응용, ARM 프로세서의 통합, 고급 합성 도구의 발전, 전통적인 DSP 시스템의 대안으로서 그래픽 처리 장치(GPU)의 부상 등이 포함됩니다.\n\n이 판은 전통적인 신호 처리 기술과 혁신적인 신호 처리 기술 모두에 관심이 있는 엔지니어, 연구자, 전기 및 컴퓨터 공학의 고학년 학생들에게 유익합니다.",
      "ja": null
    }
  },
  {
    "id": "1c11fc82d110a67f",
    "title": {
      "en": "Learn to code, ignore AI, then use AI to code even better",
      "ko": "코딩 배우고 AI 활용하기",
      "ja": null
    },
    "type": "story",
    "url": "https://kyrylo.org/software/2025/03/27/learn-to-code-ignore-ai-then-use-ai-to-code-even-better.html",
    "score": 141,
    "by": "kyrylo",
    "time": 1743154594,
    "content": "Learn to code, ignore AI, then use AI to code even better\n\n        Mar 27, 2025\n\n      |\n\n          Kyrylo Silin\n\n      ·\n      @kyrylosilin\n      ·\n      bluesky:@kyrylo.org\n\n    I woke up today to an X post by Amjad Masad,\nthe CEO of Replit, a company that sells “AI as a programming service”.\n\n  I no longer think you should learn to code.\n\n    — Amjad Masad (@amasad)\n\nThe post got traction, with well over 4.5M views. In the follow-up\npost, Amjad dismissed the\ncommunity’s reaction as “cope”.\n\nThis made me reflect on the future of coding. I have a 3-year-old daughter, and\nI wonder what the world will look like when she grows up. Will coding still be a\nvaluable skill?\n\nThe rise of AI and so-called vibe coding\nhas sparked debate. Some argue coding is becoming obsolete; others believe it’s\nsimply evolving. What’s clear is that AI is changing how we code.\n\nShould they learn to code or rely on AI to do the work for them? How should I\nteach my daughter to approach coding? Should I even teach her to code at all?\n\nI don’t have all the answers, but I do have some thoughts.\n\nWhere I come from\n\nTo explain my perspective, I should share some background. I’m a web developer\nand software engineer with over 15 years of experience (mostly in interpreted\nlanguages, with occasional ventures into compiled ones). I studied Computer\nScience and hold a Master’s degree in Information Control Systems.\n\nBack in school, we played with languages like Basic and Logo. We wrote code on\npaper and then typed it into a computer — like it was the ’60s, but it was\nactually the early 2000s.\n\nWe also had to perform basic arithmetic in binary. I don’t remember much about\nit, but I do remember it was fun.\n\nI’m not ancient, but I do recall using images to create rounded corners in CSS.\nNice to meet you!\n\nLearning to code in 2025\n\nSo how do students learn to code nowadays? Beats me! And with AI in the mix,\nit’s even trickier. Should you watch online courses? Read books? Just download a\ncode editor and start coding? Or should you rely on AI to do the work for you?\n\nThere are endless options now — more languages, more frameworks, more tools, and\nmore resources than ever before.\n\nThis is tiring. It’s a blissful time to be a programmer, but it’s also a\nnightmare. I think the newer generations of programmers have it harder than we\ndid.\n\nBut I do know this: the fundamentals of coding haven’t changed. Computers have\nevolved, but the basics remain the same. What I learned in school still holds\ntrue. And if you’re just starting out, the basics are where you should begin.\n\nA solid foundation is crucial if you want to understand what you’re doing.\nUltimately, it comes down to how much control you want over your code and, by\nextension, your career.\n\nBut should you ignore AI? Absolutely not. I use AI as a coding assistant every\nday. Has it made me a better programmer? Probably not.\n\nMerchants of AI\n\nAI is the new shiny toy everyone wants to play with. And to be honest, it’s\nimpressive. The problem with AI is that with every year, it gets better and\nbetter. Wait what? How’s that a problem? Well, with every new year you lose\ncontrol.\n\nThe more you rely on AI, the less you understand what you’re doing. The less you\nunderstand, the more AI vendors can control you. And the more control they\nhave, the more they can charge you. It’s a vicious cycle.\n\nThis shift was inevitable. Humanity must adapt to this new reality. AI isn’t\ngoing away, and we need to learn how to use it to our advantage.\n\nThe large language models (LLMs) created by tech giants have absorbed decades of\nknowledge — our knowledge. They’ve been trained on our work.\n\nNow they’re selling it back to us and telling us we only need to learn English\nto code. This is a lie. As a new programmer, I don’t want you to fall for it.\nThere’s no corner-cutting. Get your shit together and learn to code.\n\nWill I continue using AI?\n\nYes, it’s addictive, and it makes me more productive. If I had to stop using it\ntomorrow, I’d feel withdrawal symptops. Coding with AI feels incredible.\n\nBut if AI vanished tomorrow due to, say, regulations, I’d just nod and go back\nto my old ways. I’d be less productive, yes. And what about you?\n\nIf you know how to code, you can build anything. If you only know how to vibe\ncode, you’re gambling with your future.\n\nBecause if you can vibe code… so can everyone else.\n\nAnd if everyone can do it, what makes you think Devin won’t replace you?",
    "summary": {
      "en": "The author reflects on the future of coding in light of AI advancements, sparked by a controversial post from Amjad Masad, CEO of Replit, suggesting that learning to code may no longer be essential. The author, a seasoned web developer, questions whether coding will still be a valuable skill for the next generation, like their young daughter.\n\nKey points include:\n\n1. **Evolving Nature of Coding**: The rise of AI has changed how coding is approached, with some believing it might make traditional coding obsolete, while others think it is simply evolving.\n\n2. **Learning Paths**: There are now many resources and tools for learning to code, complicating the process for new learners. Despite these changes, the fundamentals of coding remain important.\n\n3. **Role of AI**: While the author uses AI as a coding assistant and finds it productive, they caution against over-reliance. Understanding coding is crucial for maintaining control over one's work and career.\n\n4. **Future Concerns**: The author warns that if everyone can easily code with AI tools, it could lead to job insecurity, as many could do the same work without a solid coding foundation.\n\nIn summary, while AI can enhance productivity, learning traditional coding skills is still vital for long-term success and control in the tech industry.",
      "ko": "저자는 AI 발전에 비추어 코딩의 미래에 대해 생각해보며, Replit의 CEO인 암자드 마사드의 논란이 된 게시글을 언급합니다. 그는 코딩을 배우는 것이 더 이상 필수적이지 않을 수 있다는 주장을 했습니다. 저자는 경험이 풍부한 웹 개발자로서, 코딩이 다음 세대, 특히 자신의 어린 딸에게 여전히 가치 있는 기술이 될 것인지에 대해 의문을 제기합니다.\n\n코딩의 본질이 변화하고 있다는 점이 주요 포인트입니다. AI의 발전으로 코딩 접근 방식이 달라졌고, 일부는 전통적인 코딩이 더 이상 필요하지 않을 것이라고 믿고 있습니다. 반면, 다른 이들은 코딩이 단순히 진화하고 있다고 생각합니다.\n\n코딩을 배우는 경로도 다양해졌습니다. 현재는 많은 자원과 도구가 있어 새로운 학습자에게는 오히려 복잡해질 수 있습니다. 그러나 코딩의 기본 원칙은 여전히 중요합니다.\n\nAI의 역할에 대해서도 저자는 AI를 코딩 보조 도구로 활용하며 생산성을 높이고 있지만, 과도한 의존은 경계해야 한다고 경고합니다. 코딩을 이해하는 것은 자신의 작업과 경력을 통제하는 데 필수적입니다.\n\n미래에 대한 우려도 있습니다. 만약 모든 사람이 AI 도구를 사용해 쉽게 코딩을 할 수 있다면, 많은 사람들이 탄탄한 코딩 기초 없이도 같은 일을 할 수 있어 직업 불안정성이 커질 수 있습니다.\n\n결론적으로, AI가 생산성을 높일 수 있지만, 전통적인 코딩 기술을 배우는 것은 기술 산업에서 장기적인 성공과 통제를 위해 여전히 중요합니다.",
      "ja": null
    }
  },
  {
    "id": "ab1107866375ce2d",
    "title": {
      "en": "Asking good questions is harder than giving great answers",
      "ko": "좋은 질문의 힘",
      "ja": null
    },
    "type": "story",
    "url": "https://newsletter.dancohen.org/archive/asking-good-questions-is-harder-than-giving-great-answers/",
    "score": 145,
    "by": "speckx",
    "time": 1743112103,
    "content": "March 18, 2025\n\n            Asking Good Questions Is Harder Than Giving Great Answers\n\n                The tests we are using to assess the intelligence of AI are missing an essential aspect of human inquiry — the query itself\n\n            by Dan Cohen\n\n                        ×\n                        Close dialog\n\n                    A painting of Socrates about to drink the hemlock, as his disciples look away in dismay\n\n            Jacques-Louis David, “The Death of Socrates,” 1787, Metropolitan Museum of Art. Perhaps he asked too many good questions\n\nRecently, I sharpened a #2 pencil and took the history section of \"Humanity's Last Exam.” Consisting of 3,000 extremely difficult questions, the test is intended for AI, not me. According to its creators and contributors, Humanity’s Last Exam will tell us when artificial general intelligence has arrived to supersede human beings, once a brilliant bot scores an A.\nI got an F. Actually, worse than that: Only one of my answers was correct, and I must admit it helped that the question was multiple choice. This is fairly embarrassing for someone with a PhD in history.\nWhat happened? Let me indulge in a standard academic humiliation-avoidance technique: examining the examiners. A much easier exercise. Of the thousands of questions on the test, a mere 16 are on history. By comparison, over 1,200 are on mathematics. This is a rather rude ratio for a purported Test of All Human Knowledge, and a major demerit in this human’s assessment of the exam.\nThe offense extends further to the historical topics covered. Of the 16 history questions, four of them — 25% of historical understanding! — are about naval battles. My knowledge of the displacement of various warships is admittedly weak. Other questions are byzantine, alas not literally, but figuratively, long narrative journeys with twists and turns that are clearly trying to confuse any AI by flooding its memory with countless opaque terms. Those questions certainly succeeded in confusing me.\nI will not be reproducing the history questions here since the creators of Humanity’s Last Exam don’t want AI to have a sneak peek at the questions ahead of taking the test. Of course, this raises another question: Would a true superintelligence cheat? I feel like it would? If you, presumably a human reader, want to take the test yourself, you can find a database of the questions on Hugging Face and GitHub. I should also note that I did not take the “classics” section of the exam, as I am a historian of the modern era and do not know Latin, Greek, etc., but much of that section is history too, perhaps because there were also naval battles in the ancient world.\n* * *\nAlthough I failed Humanity’s Last Exam, I did learn something about the current state of our assessment of AI, and what we expect from it. HLE’s implicit definition of “intelligence” is the ability to provide correct answers to complicated questions, and it is just one of many similar exams. Another, less naval-gazing test of historical knowledge is based on a comprehensive global history database, but still relies on question-answer pairs so it can provide numerical scores for each LLM’s ability. Upon the release of their latest models, AI companies tout improvements on these assessment tools, which allows them to proclaim definitive AI progress: “This LLM got a 92% on a PhD-level history exam, up from 56% last year!”\nAnd the companies are not wrong about genuinely impressive improvements. Six years ago in this newsletter, I wrote about some initial testing I had been doing with computer vision APIs from Google and Microsoft, a first attempt to analyze the photo morgue my library had recently acquired from the Boston Globe. There were glimmers of hope that these pre-GPT tools could help us identify topics in millions of photographs that lacked rigorous metadata, and I found even 80% accuracy to be promising. Now our library’s digital team, much more capable than I am, has created an abstracted interface to all of the main multimodal AI services and is testing the ability of these services to provide subject headings and descriptions, with much better results (although all of the services are still imperfect).\nFellow historian Benjamin Breen has documented similar advances in his testing of AI. The latest models are scarily on par with a first-year doctoral student in history in some areas, able to provide solid context and advanced interpretations of documents and images, even complex ones that require substantial background in a field. The frontier models are much better than most doctoral students in other tasks, such as translation and transcription. Handwriting recognition for historical documents, in particular, has been among the hardest problems for computer scientists to solve, and cracking it will have a significant impact on historical research. Historian Cameron Blevins has shown that custom GPTs are now on a path to a solution that could make archives and special collections much more searchable and readable in ways that might transform our ability to do history. What these other tests of artificial intelligence show is that significant AI progress may lie not in some kind of examination endgame, of perfect answers to tough questions, but in the important, but often hidden, middle stages of a research project, when evidence is being assembled and interpreted.\n* * *\nEven more obscured right now in the conversation about AI and intelligence is that PhD-level work is not just about correct answers. It is more about asking distinctive, uncommon questions. Ultimately, we may want answers, but we must begin with new queries, new areas of interest. Along the way to a better understanding of the past and present, good questions in history may eventually require accurate translations of inscriptions or the location of sea skirmishes. But first, we must imagine why someone, today, should care about such documents and events in the first place, envision how they may have shaped our world. This is a much bigger challenge.\nThe most vibrant historical studies begin with questions that are unexpected and which therefore have revelatory power. Recently in this newsletter, for instance, I covered a book that originated with the seemingly simple query, “Why did audiences at orchestral performances become silent when previously they were rowdy?” Before I read Listening in Paris, I assumed naively that the eternally proper behavior at a concert has been respectful quiet. By asking this curious question, James Johnson was able to unveil a major change in the nature and relationship of music, composers, and audiences that still resonates today, even if our musical tastes have largely changed.\nOther books that have influenced me originated with equally novel questions. Why, over a relatively short period of time, did the British radically change their view of some animals, like dogs, from unkempt wild beasts to delightful members of the household, proudly coiffed and paraded at dog shows? Why did Isaac Newton, the paragon of modern science, write more on alchemy than he did on physics or math? How does the experience of war — not the abstract tactics of naval battles but the actual first-person experience — profoundly change individual soldiers and then, in aggregate, an entire culture?\nCan AI ever produce good questions in history rather than great answers? I’ll tackle that important question in another newsletter.\n\n    Read more:\n\n                The Unresolved Tension Between AI and Learning\n                If education is accelerated using AI, will we lose some crucial aspects of learning that will prove to be problematic?\n\n                AI Is Coming for Scholarship Next\n                AI models are now ingesting scholarly content in an attempt to dispel their hallucinations. But another possibility looms: that AI will instead drag down scholarship into its muddy realm.\n\nDon't miss what's next. Subscribe to Humane Ingenuity:\n\nYour email (you@example.com)Subscribe\n\n{\"username\": \"dancohen\", \"name\": \"Humane Ingenuity\", \"description\": \"\\u003C!-- buttondown-editor-mode: plaintext --\\u003E\\u003Cp style=\\\"text-align: center\\\"\\u003E* * *\\u003C/p\\u003E\\n\\nA newsletter by [Dan Cohen](https://dancohen.org) on technology that helps rather than hurts human understanding, and human understanding that helps us create better technology. ([*Read past issues of the newsletter*](https://newsletter.dancohen.org/archive/)) ![Humane Ingenuity header image](https://assets.buttondown.email/images/3fabcedf-d29b-4651-b9da-7d3363902bcf.jpeg)\", \"header\": \"\", \"web_header\": \"by [Dan Cohen](https://dancohen.org)\", \"web_footer\": \"\", \"hidden_settings\": [\"should_require_double_optin\", \"should_send_latest_email_to_new_subscribers\"], \"footer\": \"\\u003C!-- buttondown-editor-mode: fancy --\\u003E\\u003Cp\\u003EI'm \\u003Ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer nofollow\\\" href=\\\"https://dancohen.org/about/\\\"\\u003EDan Cohen\\u003C/a\\u003E, a vice provost/dean/professor at Northeastern University. I also write on my \\u003Ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer nofollow\\\" href=\\\"https://dancohen.org\\\"\\u003Epersonal website\\u003C/a\\u003E, in \\u003Ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer nofollow\\\" href=\\\"https://dancohen.org/publications/\\\"\\u003Ebooks and academic journals\\u003C/a\\u003E, and sometimes for \\u003Ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer nofollow\\\" href=\\\"https://www.theatlantic.com/author/dan-cohen/\\\"\\u003Emore popular venues\\u003C/a\\u003E. You can find me on social media on \\u003Ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer nofollow\\\" href=\\\"https://social.dancohen.org\\\"\\u003EMicro.blog\\u003C/a\\u003E (a humane social media platform), which interoperates with Bluesky and other AT-based platforms (follow \\u003Ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer nofollow\\\" href=\\\"https://bsky.app/profile/dancohen.org\\\"\\u003E@dancohen.org\\u003C/a\\u003E), and \\u003Ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer nofollow\\\" href=\\\"https://micro.blog/dancohen?remote_follow=1\\\"\\u003EMastodon\\u003C/a\\u003E and other ActivityPub-based platforms (follow \\u003Ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer nofollow\\\" href=\\\"https://micro.blog/dancohen?remote_follow=1\\\"\\u003E@dan@social.dancohen.org\\u003C/a\\u003E). For now, I'm on Threads and X @dancohen, but go there as often, and as enthusiastically, as the dentist.\\u003C/p\\u003E\\u003Cp\\u003EIf you liked this newsletter, please feel free to forward it to others.\\u003C/p\\u003E\", \"css\": \"\", \"paid_subscriptions_status\": \"inactive\", \"social_networks\": {\"custom\": \"\"}, \"sharing_networks\": [], \"web_css\": \"@import url(\\\"https://fonts.googleapis.com/css?family=Newsreader\\\");\\n\\nbody,\\np,\\n* {\\n  font-family: 'Newsreader', serif;\\n  font-weight: 200;\\n  font-size: 15px;\\n  margin-left: auto;\\n  margin-right: auto;\\n  max-width: 650px;\\n}\\n\\n/* archive snippet text */\\n.email\\u003Ediv {\\n  font-family: 'Newsreader', serif;\\n  font-weight: 200;\\n  font-size: 18px;\\n}\\n\\n/* archive post title */\\n.email\\u003Ediv\\u003Ediv {\\n  font-family: 'Newsreader', serif;\\n  font-weight: 600;\\n  font-size: 20px;\\n  margin-left: 0;\\n}\\n\\n/* archive post date */\\n.email-list .email\\u003Ediv\\u003Ediv:last-child {\\n  font-family: 'Newsreader', serif;\\n  font-weight: 200;\\n  font-size: 18px;\\n  margin-right: 0;\\n}\\n\\na,\\nem {\\n  font-size: 1.0em;\\n}\\n\\nimg {\\n    max-width: 100%;\\n    height: auto;\\n}\", \"from_name\": \"Dan Cohen\", \"address\": \"Snell Library\\nNortheastern University\\n360 Huntington Ave\\nBoston, MA 02115\", \"body_template\": \"\", \"archive_navigation_links\": {}, \"locale\": \"en\", \"fathom_code\": null, \"reddit_code\": null, \"meta_pixel_id\": null, \"memberful_api_key\": null, \"memberful_username\": null, \"is_using_plausible\": true, \"fathom_subscribe_code\": null, \"is_sponsorship_enabled\": false, \"sponsorship_text\": null, \"upsell_text\": \"\", \"subscription_button_text\": \"\", \"paywall_button_text\": \"\", \"subscription_redirect_url\": \"https://dancohen.org\", \"account\": \"73ef6c20-dbbe-4725-856c-0b6f61dcdf51\", \"unsubscription_redirect_url\": \"\", \"buysellads_url\": null, \"discord_api_key\": null, \"shopify_api_key\": null, \"shopify_url\": null, \"github_api_key\": null, \"should_enable_link_checking\": true, \"github_organization_slug\": null, \"github_team_slug\": null, \"github_subscriber_metadata_key\": null, \"enabled_features\": [\"whitelabeling\", \"archives\", \"tracking\", \"api\"], \"subscription_confirmation_redirect_url\": \"\", \"domain\": \"newsletter.dancohen.org\", \"should_send_latest_email_to_new_subscribers\": true, \"should_expose_rss\": true, \"should_hide_social_media\": false, \"should_hide_issue_numbers\": false, \"should_require_double_optin\": true, \"should_send_subscription_confirmation_email\": true, \"status\": \"permanently_active\", \"tint_color\": \"#000000\", \"absolute_url\": \"https://newsletter.dancohen.org/\", \"paid_subscription_free_trial_duration\": 0, \"is_paid_subscription_free_trials_enabled\": false, \"should_disable_non_premium_subscriptions\": false, \"icon\": \"https://buttondown-attachments.s3.us-west-2.amazonaws.com/images/dfca1bbd-ec9b-4d64-8f99-3610da88c2df.png\", \"image\": \"\", \"id\": \"0b99b201-27a2-42fd-9694-221c025a0890\", \"should_be_private\": false, \"archive_theme\": \"modern\", \"template\": \"modern\", \"email_domain\": \"\", \"automatically_remind_unconfirmed_subscribers\": true, \"should_track_replies\": false, \"should_track_errors\": true, \"should_track_page_views\": true, \"should_track_clicks_on_emails\": true, \"should_track_opens_on_emails\": true, \"should_track_clicks_on_transactional_emails\": true, \"should_track_opens_on_transactional_emails\": true, \"timezone\": \"America/New_York\", \"google_tag_manager_code\": null, \"umami_website_id\": null, \"subscription_success_body\": \"\", \"subscription_form_text\": \"\", \"paywall_form_text\": \"\", \"email_address\": \"dan@dancohen.org\", \"metadata_fields\": [], \"cached_analytics_data\": null}\n\n    null\n\n    null\n\n\"\"\n\n    var NEWSLETTER = JSON.parse(document.getElementById('newsletter').textContent);\n    var SUBSCRIBER_EMAIL = JSON.parse(document.getElementById('subscriber_email').textContent);\n    var REFERRING_SUBSCRIBER_ID = JSON.parse(document.getElementById('referring_subscriber_id').textContent);\n\n    var TINT_COLOR = NEWSLETTER.tint_color || '#0069FF';\n    document.documentElement.style.setProperty(\"--tint-color\", TINT_COLOR);\n    var referer = JSON.parse(document.getElementById('referer').textContent);\n    var account = null;\n\n    var VARIANT = \"archive_page\";",
    "summary": {
      "en": "In an article dated March 18, 2025, Dan Cohen discusses the challenges of evaluating AI intelligence through tests like \"Humanity's Last Exam,\" which consists of difficult questions mainly focused on mathematics, with only a small portion related to history. Cohen humorously shares his poor performance on the test, highlighting the unfair emphasis on naval battles among the historical questions.\n\nHe notes that current assessments of AI often prioritize providing correct answers to complex questions, which may not fully capture the essence of intelligence. While AI has made significant strides—performing at levels comparable to first-year doctoral students in some areas of history—Cohen argues that true intelligence also involves asking insightful questions rather than just providing answers. \n\nHe emphasizes that the most impactful historical research begins with unique, thought-provoking questions. The ability of AI to generate such questions remains an open question, which Cohen plans to explore further in future writings.",
      "ko": "2025년 3월 18일자 기사에서 댄 코헨은 \"인류의 마지막 시험\"과 같은 테스트를 통해 인공지능의 지능을 평가하는 데 있어 겪는 어려움에 대해 이야기합니다. 이 시험은 주로 수학과 관련된 어려운 질문들로 구성되어 있으며, 역사와 관련된 질문은 소수에 불과합니다. 코헨은 이 시험에서의 자신의 낮은 성적을 유머러스하게 언급하며, 역사 질문 중 해전 관련 질문에 지나치게 많은 비중이 주어지는 것이 불공평하다고 지적합니다.\n\n그는 현재 인공지능 평가가 복잡한 질문에 대한 정답을 제공하는 데 중점을 두고 있지만, 이는 지능의 본질을 완전히 포착하지 못할 수 있다고 말합니다. 인공지능이 일부 역사 분야에서 1년차 박사 과정 학생들과 비슷한 수준으로 성과를 내고 있지만, 코헨은 진정한 지능은 단순히 답을 제공하는 것이 아니라 통찰력 있는 질문을 제기하는 것이라고 주장합니다.\n\n그는 가장 영향력 있는 역사 연구는 독창적이고 사고를 자극하는 질문에서 시작된다고 강조합니다. 인공지능이 이러한 질문을 생성할 수 있는 능력은 여전히 미지수이며, 코헨은 앞으로의 글에서 이 주제를 더 깊이 탐구할 계획입니다.",
      "ja": null
    }
  },
  {
    "id": "12e39ff4c61a5f07",
    "title": {
      "en": "Better Shell History Search",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://tratt.net/laurie/blog/2025/better_shell_history_search.html",
    "score": 55,
    "by": "todsacerdoti",
    "time": 1742904165,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "9a7110f17d414461",
    "title": {
      "en": "They Might Be Giants Flood EPK Promo (1990) [video]",
      "ko": "그들은 거인이다: 홍수 EPK",
      "ja": null
    },
    "type": "story",
    "url": "https://www.youtube.com/watch?v=C-tQSFQ-ESY",
    "score": 181,
    "by": "CaliforniaKarl",
    "time": 1743046675,
    "content": "Back\n\n        Search",
    "summary": {
      "en": "It looks like your request is incomplete. Please provide the text you would like summarized, and I'll be happy to help!",
      "ko": "요청이 불완전한 것 같습니다. 요약하고 싶은 내용을 제공해 주시면 기꺼이 도와드리겠습니다!",
      "ja": null
    }
  },
  {
    "id": "da0bac5307a59bd8",
    "title": {
      "en": "Golang on the Playstation 2",
      "ko": "플레이스테이션 2의 고랭",
      "ja": null
    },
    "type": "story",
    "url": "https://rgsilva.com/blog/ps2-go-part-1/",
    "score": 189,
    "by": "donatj",
    "time": 1743111466,
    "content": "Golang on the PlayStation 2\n\n        By Ricardo\n\n      March 23, 2025\n\n      I always wanted to do some weird stuff with consoles. I’m not sure why, but making devices do unexpected things is always an interesting topic for me. And the same applies to the PlayStation 2, the 2000 console released by Sony.\nAlso, Sony, don’t sue me for this lol\nLet’s get straight into it: I want to run code into consoles (more on why this in a future post). Normally this is done in low(er) level languages, but nowadays we have better and easier to work with languages such as Go. So I was wondering.. why not?\nLooking online, however, yielded no easy way of doing this, so I decided to tackle this problem myself.\n\nPlease note that I’m writing this after a lot of the research here has been done and tested already. This means that a lot of the experimentation here is from memory and from retracing my steps, so there might be inconsistencies here and there.\n\nAlso note that this is all running inside an emulator. I do own a PS2 that I can test this, but I’m too lazy to set it up. Plus I want fully functional demos before doing so.\n\nFinal note: the code will be released at a later point and I’ll update the post accordingly.\n\nThe challenge\nBy default, Go only supports a handful of platforms, and unfortunately that does not include the PS2. In fact, Go seems to require an OS behind the scenes, thing that we don’t even have over there (not considering PS2 Linux). To solve that, however, we have TinyGo, a Go compiler for small devices, like microcontrollers and embedded systems. The basic way it works is that it gets the Go code, turns into LLVM IR, and then that into the binary code for whatever target we’re trying to code for.\nThe PS2 main CPU is called Emotion Engine, which is based on a MIPS R5900. It implements the instructions for MIPS-III and MIPS-IV, plus some custom stuff. It also is missing a few other things (more on that later). Go can actually build code for MIPS already, which should save me some time, but not a lot, as I need to get TinyGo to work on it. TinyGo relies on LLVM 19, which does support MIPS-III, but not the R5900 CPU directly.\nThese are all technical issues. There’s a much more pressing one: I don’t know how the PS2 works.\n\n      Well, that should be fun.\n\nThe ps2dev SDK and its quirks\nIf you look it up online for a way of developing code for the PS2, you’ll probably cross paths with ps2dev. This is a full-blown SDK that lets you generate binaries for it in a pretty neat way. The coolest thing about it is that it already provides a bunch of libraries for graphical operations, debugging, I/O, etc - even a stdlib is provided! Because of that, I thought: hey, maybe I want to link to their code - this would allow an easier and faster implementation of anything for the PS2 in Go. Think of it as an “OS” API (technically it’s not?), which we could call whenever we want to do things we don’t want to reimplement (or that aren’t even worth trying sometimes).\nThat said, this introduces a few problems. The first one is that the ps2dev libraries are compiled to the MIPS-III N32 standard. This means that, whatever code we produce, needs to target the same thing. Same hard-float, same N32 ABI, etc. This is a bit annoying, but manageable. The reason why it needs to match is because we’ll be linking our code to their pre-built libraries, and linkers are not really fond of dealing with different targets.\n\nFor the sake of clarification: MIPS-III N32 means that this is targeting a MIPS CPU that is implementing the MIPS-III instruction set. This is a 64bit CPU, but due to the N32, this is running 32bit code with some 64bit instructions to handle 64bit integers. This is very confusing, but you can check this to read more about it.\n\nBecause of this, you’ll see my attempts to target mipsel with a mips3 CPU in the next steps, even though it should, technically speaking, be a mips64el, as this is a 64bit CPU. N32 should force things to run in 32bit mode, even though though our target should support 64bit code. However, Clang/LLVM and TinyGo get kinda messy on this and it gets very confusing and complicated. Also, building for mips64el caused TinyGo to failed some verification steps when generating code with the LLVM, as well as clang to refuse to properly build it as the code is kinda broken. Since I really wanted to move forward, I gave up and opted for generating mipsel code with the N32 ABI, which will force clang to change it to a MIPS64 internally, but still generate valid code. Like I said, it’s very weird. Please bear with me, this is all new to me too! :D\nFurther reiteration on this topic might be necessary to get this just right, but I’m not diving into this hell right now. Future-wise we can try dropping ps2dev and doing things directly in Go, but some assembly is required - literally and metaphorically.\nGetting TinyGo to generate some code\nFor TinyGo to know about a certain target, it requires a file defining it - we’ll call it ps2.json. It defines a bunch of very interesting things that we don’t really care at the moment, but here are the most important ones:\n{\n\t\"llvm-target\": \"mipsel-unknown-unknown\",\n\t\"cpu\": \"mips3\",\n\t\"features\": \"-noabicalls\",\n\t\"build-tags\": [\"ps2\", \"baremetal\", \"mipsel\"],\n\t\"goos\": \"linux\",\n\t\"goarch\": \"mipsle\",\n\t\"linker\": \"ld.lld\",\n\t\"rtlib\": \"compiler-rt\",\n\t\"libc\": \"\",\n\t\"cflags\": [\n\t],\n\t\"ldflags\": [\n\t],\n\t\"linkerscript\": \"\",\n\t\"extra-files\": [\n\t],\n\t\"gdb\": []\n}\nThis file is the culmination of many, maaany days testing different configurations. It is only partially functional. It cannot generate object files yet (more below), hence the reason why I’m not bothering to fill in flags for compiling and linking code. There are some relevant things I need to explain, though, so here we go:\n\nTarget is mipsel-unknown-unknown. This is our LLVM target. I’m sticking with mipsel here for the reasons I explained previously.\nThe features have -noabicalls. It is required as otherwise shit hits the fan and nothing works (the LLVM IR that gets generated gets broken).\nI’ve set it to not use any libc. This is because ps2dev already provides one, and I don’t want to mess with that (trust me). Plus, since we’ll be linking to their code, we might as well use their version of it.\n\nThis is the basic target file we need so that TinyGo at least knows what a PS2 is. But that’s not all - we need to define a bunch of functions which are target-specific.\nBaremetal definitions\nOur target needs a baremetal configuration - baremetal_ps2.go. Usually the default baremetal file is enough, but in our case I’ve opted to create a custom one so that I can redefine some things.\n\nNote from the future: this can be improved by adjusting the linker file so it finds the correct externs. I might end up doing that and come back here later on.\n\n//go:build ps2\n\npackage runtime\n\nimport \"C\"\nimport (\n\t\"unsafe\"\n)\n\n//go:extern _heap_start\nvar heapStartSymbol [0]byte\n\n//go:extern _heap_end\nvar heapEndSymbol [0]byte\n\n//go:extern _fdata\nvar globalsStartSymbol [0]byte\n\n//go:extern _edata\nvar globalsEndSymbol [0]byte\n\n//go:extern _stack_top\nvar stackTopSymbol [0]byte\n\nvar (\n\theapStart    = uintptr(unsafe.Pointer(&heapStartSymbol))\n\theapEnd      = uintptr(unsafe.Pointer(&heapEndSymbol))\n\tglobalsStart = uintptr(unsafe.Pointer(&globalsStartSymbol))\n\tglobalsEnd   = uintptr(unsafe.Pointer(&globalsEndSymbol))\n\tstackTop     = uintptr(unsafe.Pointer(&stackTopSymbol))\n)\n\nfunc growHeap() bool {\n\t// On baremetal, there is no way the heap can be grown.\n\treturn false\n}\n\n//export runtime_putchar\nfunc runtime_putchar(c byte) {\n\tputchar(c)\n}\n\n//go:linkname syscall_Exit syscall.Exit\nfunc syscall_Exit(code int) {\n  \t// TODO\n\texit(code)\n}\n\nconst baremetal = true\n\nvar timeOffset int64\n\n//go:linkname now time.now\nfunc now() (sec int64, nsec int32, mono int64) {\n\tmono = nanotime()\n\tsec = (mono + timeOffset) / (1000 * 1000 * 1000)\n\tnsec = int32((mono + timeOffset) - sec*(1000*1000*1000))\n\treturn\n}\n\nfunc AdjustTimeOffset(offset int64) {\n\ttimeOffset += offset\n}\n\nvar errno int32\n\n//export __errno_location\nfunc libc_errno_location() *int32 {\n\treturn &errno\n}\nDo we need to understand how most of this works? No, we don’t. Not only that, most of this is copy-paste from the normal baremetal.go implementation. We can adjust later if necessary, no worries. Like I said, we mostly need this to build, so that we can figure out what is wrong and fix it accordingly.\n\nNote: for this to work, you still need to disable the building of the original baremetal.go for our target, so we need to change its build flag to //go:build baremetal && !ps2.\n\nRuntime\nOur target needs a runtime definitions file - runtime_ps2.go. This is the place where a bunch of target-specific functions get defined, including how putchar, exit and even main (later) gets implemented. Pretty cool part of the code if I’d say so.\nA very basic implementation would look like this:\n//go:build ps2\n\npackage runtime\n\n/*\nextern void _exit(int status);\nextern void* malloc(unsigned int size);\nextern void free(void *ptr);\nextern void scr_printf(const char *format, ...);\n*/\nimport \"C\"\nimport \"unsafe\"\n\n// timeUnit in nanoseconds\ntype timeUnit int64\n\nfunc initUART() {\n  // Unsupported.\n}\n\nfunc putchar(c byte) {\n\t// This is a very hacky way of doing this. It assumes the debug screen is already active, and prints\n\t// a whole string for a single char every single time. Very slow, but works. We can improve it later.\n\n\tx := C.CString(string(c))\n\tC.scr_printf(x)\n\tC.free(unsafe.Pointer(x))\n}\n\nfunc getchar() byte {\n\t// TODO\n\treturn 0\n}\n\nfunc buffered() int {\n\t// TODO\n\treturn 0\n}\n\nfunc sleepWDT(period uint8) {\n\t// TODO\n}\n\nfunc exit(code int) {\n  \t// This just delegates it to the ps2dev _exit(int) function.\n\tC._exit(C.int(code))\n}\n\nfunc abort() {\n\t// TODO\n}\n\nfunc ticksToNanoseconds(ticks timeUnit) int64 {\n  \t// TODO\n\treturn int64(ticks)\n}\n\nfunc nanosecondsToTicks(ns int64) timeUnit {\n  \t// TODO\n\treturn timeUnit(ns)\n}\n\nfunc sleepTicks(d timeUnit) {\n\t// TODO\n}\n\nfunc ticks() (ticksReturn timeUnit) {\n\t// TODO\n\treturn 0\n}\nA lot of it is not implemented, and this is intentional - I won’t be using those things at the moment, so I don’t care about them. We can later implement them accordingly, and get them to work as expected. Some of them might be even doable through ps2dev’s C functions, for example.\nInterrupts\nAnother basic file we need is the interrupts defintions - interrupt_ps2.go. I know that ps2dev has implementations for those calls, but I’ve opted to not call them yet. At this moment, we don’t need interrupts, so let’s just implement dummy functions for that:\n//go:build ps2\n\npackage interrupt\n\ntype State uintptr\n\nfunc Disable() (state State) {\n\treturn 0\n}\n\nfunc Restore(state State) {}\n\nfunc In() bool {\n\treturn false\n}\nWith that, we should be able to build some Go code. So let’s give it a try.\nCalling Go functions from C\nLet’s start with a simple example: get our C code to return a number and a string. Nothing major. We’ll split this into 2 parts: the loader (in C) and our Go code. It will work like this:\n\n      It works!\n\nHere’s our Go code:\n//export aGoString\nfunc aGoString() *C.char {\n\treturn C.CString(\"The answer for everything is\")\n}\n\n//export aGoNumber\nfunc aGoNumber() C.int {\n\treturn C.int(42)\n}\nAnd our loader, which contains our main function:\n// Our go functions, exported before.\nextern char* aGoString();\nextern int aGoNumber();\n\nint main() {\n  // Initialize our debug screen.\n  sceSifInitRpc(0);\n  init_scr();\n\n  // Print stuff we get from Go functions.\n  scr_printf(\"%s: %d\\n\", aGoString(), aGoNumber());\n\n  // Infinite loop so we keep the program running.\n  while (1) {}\n\n  return 0;\n}\nVery simple code, right? Let’s build it.\nWell, no, wait. There’s a problem. TinyGo, by default, wants you to generate the final ELF (.elf) or the object file (.o) with it. However, the ELF requires adding a linkfile and some other extra bits of code to it, which we’re far from. For now, we just want to get some functions in a way that we can link - so we should be able to just use the object file.\nHowever, attempting to do so generates an incorrect file:\n$ tinygo build -target ps2 -o test.o\n$ file test.o\ntest.o: ELF 32-bit LSB relocatable, MIPS, MIPS-III version 1 (SYSV), with debug_info, not stripped\n\n  Note the missing `N32` in the string\n\nI thought: oh, ok, we’re just missing the proper cflags and ldflags here, right? So let’s try adding it:\n{\n\t// (...)\n\t\"cflags\": [\n\t\t\"-mabi=n32\"\n\t],\n\t\"ldflags\": [\n\t\t\"-mabi=n32\"\n\t],\n\t// (...)\n\n  These might not be the right flags, but according to some docs it seems like it is.\n\n$ tinygo build -target ps2 -o test.o\n$ file test.o\ntest.o: ELF 32-bit LSB relocatable, MIPS, MIPS-III version 1 (SYSV), with debug_info, not stripped\nOh. Ok then.\nSince TinyGo is, for some reason, not playing nice here, I’ve opted for breaking this into steps that I can more easily control. TinyGo internally will generating some LLVM IR from your Go code, and then build it. Let’s stop at the LLVM IR level then:\n$ tinygo build -target ps2 -o build/go.ll\nThis will generate a valid LLVM IR file! 🎉 Now we can just manually build it into the object file with the format we want:\n$ clang -fno-pic -c --target=mips64el -mcpu=mips3 -fno-inline-functions -mabi=n32 -mhard-float -mxgot -mlittle-endian -o build/go.o build/go.ll\nThe flags here are important. Our target is a MIPS64 (only TinyGo is not happy with it), Little Endian, with the MIPS-III instruction set, using the N32 ABI. It uses hardware floating numbers, and the -fno-pic and -mxgot is to deal with a global offset table size limit issue when linking. With all that, here’s what we get:\n$ file build/go.o\nbuild/go.o: ELF 32-bit LSB relocatable, MIPS, N32 MIPS-III version 1 (SYSV), with debug_info, not stripped\n\n  Finally!\n\nFrom here, we can link with our C code. For that, I’ve opted to use the ps2dev linking command (extracted from the Makefile and some testing), with our Go code added into it:\nmips64r5900el-ps2-elf-gcc \\\n\t-Tlinkfile \\\n\t-L/usr/local/ps2dev/ps2sdk/ee/lib \\\n\t-L/usr/local/ps2dev/ps2sdk/ports/lib \\\n\t-L/usr/local/ps2dev/gsKit/lib/ \\\n\t-Lmodules/ds34bt/ee/ \\\n\t-Lmodules/ds34usb/ee/ \\\n\t-Wl,-zmax-page-size=128 \\\n\t-lpatches \\\n\t-lfileXio \\\n\t-lpad \\\n\t-ldebug \\\n\t-lmath3d \\\n\t-ljpeg \\\n\t-lfreetype \\\n\t-lgskit_toolkit \\\n\t-lgskit \\\n\t-ldmakit \\\n\t-lpng \\\n\t-lz \\\n\t-lmc \\\n\t-laudsrv \\\n\t-lelf-loader \\\n\t-laudsrv \\\n\t-lc \\\n\t-mhard-float \\\n\t-msingle-float \\\n\t-o build/main.elf \\\n\tbuild/loader.o \\\n\tbuild/asm_mipsx.o \\\n\tbuild/go.o\n\n  Loader is our C code, and Go is our.. well, Go code.\n\nNote: the asm_mipsx.o is some assembly code provided by TinyGo that I just copied into the project and built with clang. You can find it here.\n\nAnd, with that, we build our new application!\n$ file build/main.elf\nbuild/main.elf: ELF 32-bit LSB executable, MIPS, N32 MIPS-III version 1 (SYSV), statically linked, with debug_info, not stripped\nAnd running it yields success:\n\n      It works! This is PCSX2 v2.3.223 btw.\n\nSwitching to Go’s main\nRight now the main function that is being called is not in Go, but in C - that is what we’ve been calling loader so far. However, Go applications can start by themselves without a C-based loader - and it would be great if our games PS2 applications would so!\nRuntime changes\nThe first step to allowing Go applications to run without our loader is to have the main function exposed by Go. We can do that in our runtime_ps2.go:\n//export main\nfunc main() {\n\tpreinit()\n\trun()\n\tpreexit()\n\texit(0)\n}\n\nconst (\n\tmemSize = uint(24 * 1024 * 1024)\n)\n\nvar (\n\tgoMemoryAddr uintptr\n)\n\nfunc preinit() {\n\t// NOTE: no need to clear .bss and other memory areas as crt0 is already doing that in __start.\n\n\t// Since we're loading into whatever ps2dev kernel thingy that exists, it's safer for us to do\n\t// a proper malloc before proceeding. This guarantees that the heap location is ours. We will\n\t// need to free it later on though.\n\n\tgoMemoryAddr = uintptr(unsafe.Pointer(C.malloc(C.uint(memSize))))\n\theapStart = goMemoryAddr\n\theapEnd = goMemoryAddr + uintptr(memSize)\n}\n\nfunc preexit() {\n\tC.free(unsafe.Pointer(heapStart))\n}\nThere are some important things to note here:\n\nThe heap start and end could be defined by the linker file. And, ironically, they are. However, the crt0 provided by ps2dev will clear those variables for some reason, making it kinda broken.\n\nWe could just assume that anything above a certain memory address is ours, buuuut ps2dev may want to play with more memory and I don’t want to deal with this right now.\nWe’ll allocate the memory using ps2dev’s malloc as per stated in the code. This will guarantee that this memory area is ours - if the libraries need more, they should still have some memory left, as the PS2 should have 32MB and we’re allocating 24MB only.\nTechnically speaking we could make the heap grow per demand - but that’s a problem for future me.\n\nWe’ll intentionally unallocate the memory after usage. Not really required, but just in case.\nThe run function is responsible for calling our main function inside our main package. This is not something we need to deal with - TinyGo’s code does that for us, we just need to call it.\n\nIt works basically like this:\n\n      Exitpoint is even a word?!\n\nThis is technically a mixed approach: it’s both baremetal - because it runs without a proper OS - but it’s also not - because it allocates memory, enters and exits application.\nFun fact: once the code exits, it shows the memory card selection screen!\n\nOur Go code\nLet’s code something in Go then. First step is to have something to call, so let’s create a package called debug with the debug screen functions:\npackage debug\n\n/*\nextern void free(void *ptr);\nextern void sceSifInitRpc(int mode);\nextern void init_scr(void);\nextern void scr_printf(const char *format, ...);\n*/\nimport \"C\"\nimport (\n\t\"fmt\"\n\t\"unsafe\"\n)\n\nfunc Init() {\n\tC.sceSifInitRpc(0)\n\tC.init_scr()\n}\n\nfunc Printf(format string, args ...interface{}) {\n\tformatted := fmt.Sprintf(format, args...)\n\n\tstr := C.CString(formatted)\n\tC.scr_printf(str)\n\tC.free(unsafe.Pointer(str))\n}\n\nYes, there is an extern for the free function which could be replaced by stdlib. I’ve avoided that at the moment as that requires adding some C flags for include paths and that made it messy. Here’s how it looks like with it:\n/*\n#cgo CFLAGS: -I/Users/ricardo/dev/ps2dev/ee/mips64r5900el-ps2-elf/include -I/Users/ricardo/dev/ps2dev/ee/lib/gcc/mips64r5900el-ps2-elf/14.2.0/include/ -I/Users/ricardo/dev/ps2dev/gsKit/include -I/Users/ricardo/dev/ps2dev/ps2sdk/common/include -I/Users/ricardo/dev/ps2dev/ps2sdk/ports/include/freetype2 -I/Users/ricardo/dev/ps2dev/ps2sdk/ports/include/zlib\n#include <stdlib.h>\n\nextern void sceSifInitRpc(int mode);\nextern void init_scr(void);\nextern void scr_printf(const char *format, ...);\n*/\nThis can be improved by moving those flags externally to the build process but that’s a problem for future me once this gets released.\n\nOverall, this is nothing too crazy - it’s just the normal debug functions exposed by ps2dev (declared here and implemented here). And then we just call it:\npackage main\n\nimport (\n\t\"ps2go/debug\"\n)\n\nfunc main() {\n\tdebug.Init()\n\n\tdebug.Printf(\"Hello world from Go!\\n\")\n\tdebug.Printf(`\n   ____                                _\n  / ___| ___    _ __ _   _ _ __  _ __ (_)_ __   __ _    ___  _ __\n | |  _ / _ \\  | '__| | | | '_ \\| '_ \\| | '_ \\ / _' |  / _ \\| '_ \\\n | |_| | (_) | | |  | |_| | | | | | | | | | | | (_| | | (_) | | | |\n  \\____|\\___/  |_|   \\__,_|_| |_|_| |_|_|_| |_|\\__, |  \\___/|_| |_|\n    ____  _             ____  _        _   _   |___/       ____\n   |  _ \\| | __ _ _   _/ ___|| |_ __ _| |_(_) ___  _ __   |___ \\\n   | |_) | |/ _' | | | \\___ \\| __/ _' | __| |/ _ \\| '_ \\    __) |\n   |  __/| | (_| | |_| |___) | || (_| | |_| | (_) | | | |  / __/\n   |_|   |_|\\__,_|\\__, |____/ \\__\\__,_|\\__|_|\\___/|_| |_| |_____|\n                  |___/\n`)\n\tfor {\n\t\t// Infinite loop to not exit!\n\t}\n}\nFancy, no? Let’s build the code and see what happens:\n$ tinygo build -target ps2 -o build/go.ll\n$ clang -fno-pic -c --target=mips64el -mcpu=mips3 -fno-inline-functions -mabi=n32 -mhard-float -mxgot -mlittle-endian -o build/go.o build/go.ll\n$ mips64r5900el-ps2-elf-gcc \\\n\t-Tlinkfile \\\n\t-L/usr/local/ps2dev/ps2sdk/ee/lib \\\n\t-L/usr/local/ps2dev/ps2sdk/ports/lib \\\n\t-L/usr/local/ps2dev/gsKit/lib/ \\\n\t-Lmodules/ds34bt/ee/ \\\n\t-Lmodules/ds34usb/ee/ \\\n\t-Wl,-zmax-page-size=128 \\\n\t-lpatches \\\n\t-lfileXio \\\n\t-lpad \\\n\t-ldebug \\\n\t-lmath3d \\\n\t-ljpeg \\\n\t-lfreetype \\\n\t-lgskit_toolkit \\\n\t-lgskit \\\n\t-ldmakit \\\n\t-lpng \\\n\t-lz \\\n\t-lmc \\\n\t-laudsrv \\\n\t-lelf-loader \\\n\t-laudsrv \\\n\t-lc \\\n\t-mhard-float \\\n\t-msingle-float \\\n\t-o build/main.elf \\\n\tbuild/asm_mipsx.o \\\n\tbuild/go.o\n\n  Easy, no?\n\nThat builds the ELF file. Now let’s load it in the emulator and see what happens!\n\n      Yeeeeeeeeey!\n\nSuccess! 🎉\n\n      Gotta love memes\n\nThe DDIVU problem\nWhile testing some basic functionality, I’ve noticed that fmt.Sprintf didn’t work properly. Look at this very simple basic code:\nfunc main() {\n\tdebug.Init()\n\n\tfor i := -32; i <= 32; i++ {\n\t\tdebug.Printf(\"%02d, \", i)\n\t}\n\n\tfor {\n\t\t// Infinite loop to not exit!\n\t}\n}\n\n      Errr this is awkward\n\nOk, this is not normal. The numbers between -9 and +9 are correct, while everything else is wrong. This specific problem took me days to figure out what the hell was going on. I eventually narrowed it down to this part of the fmtInteger implementation, used by Sprintf inside the fmt package:\nfunc (f *fmt) fmtInteger(u uint64, base int, isSigned bool, verb rune, digits string) {\n\t// (... bunch of code here ...)\n\n\tswitch base {\n\tcase 10:\n\t\tfor u >= 10 {\n\t\t\ti--\n\t\t\tnext := u / 10\n\t\t\tbuf[i] = byte('0' + u - next*10)\n\t\t\tu = next\n\t\t}\n\n\t// (... bunch of code here ...)\n}\nLook at how TinyGo is generating the LLVM IR code for that:\n!875 = !DIFile(filename: \"format.go\", directory: \"/usr/local/go/src/fmt\")\n!15696 = !DILocalVariable(name: \"next\", scope: !15679, file: !875, line: 243, type: !373)\n\n; (...)\n\nlookup.next:                                      ; preds = %for.body\n  %31 = udiv i64 %27, 10, !dbg !15759\n    #dbg_value(i64 %31, !15696, !DIExpression(), !15757)\n  %.neg = mul i64 %31, 246, !dbg !15760\n  %32 = add i64 %27, 48, !dbg !15761\n  %33 = add i64 %32, %.neg, !dbg !15762\n  %34 = trunc i64 %33 to i8, !dbg !15763\n  %35 = getelementptr inbounds i8, ptr %.pn75, i32 %30, !dbg !15758\n  store i8 %34, ptr %35, align 1, !dbg !15758\n    #dbg_value(i64 %31, !15696, !DIExpression(), !15764)\n    #dbg_value(i64 %31, !15684, !DIExpression(), !15765)\n  br label %for.loop, !dbg !15700\n\n  Hopefully this is the right section of the code lol\n\nWhich all seems just fine. Looking deeper into it, there’s this specific thing: udiv i64 %27, 10 - this is a unsigned division of a 64bit integer by 10. Keep that 64bit part in mind.\nThis generates the following MIPS assembly code:\n.LBB139_23:                             # %lookup.next\n                                        #   in Loop: Header=BB139_19 Depth=1\n\t#DEBUG_VALUE: (*fmt.fmt).fmtInteger:i <- [DW_OP_plus_uconst 176] [$sp+0]\n\t#DEBUG_VALUE: (*fmt.fmt).fmtInteger:u <- [DW_OP_plus_uconst 184] [$sp+0]\n\t#DEBUG_VALUE: (*fmt.fmt).fmtInteger:negative <- [DW_OP_plus_uconst 332] [$sp+0]\n\t#DEBUG_VALUE: (*fmt.fmt).fmtInteger:digits <- [DW_OP_LLVM_fragment 32 32] 17\n\t#DEBUG_VALUE: (*fmt.fmt).fmtInteger:base <- [DW_OP_plus_uconst 316] [$sp+0]\n\t#DEBUG_VALUE: (*fmt.fmt).fmtInteger:verb <- [DW_OP_plus_uconst 312] [$sp+0]\n\t#DEBUG_VALUE: (*fmt.fmt).fmtInteger:digits <- [DW_OP_plus_uconst 308, DW_OP_LLVM_fragment 0 32] [$sp+0]\n\t.loc\t129 0 7 is_stmt 0               # format.go:0:7\n\tlw\t$1, 176($sp)                    # 4-byte Folded Reload\n\tlw\t$4, 272($sp)                    # 4-byte Folded Reload\n\tld\t$3, 184($sp)                    # 8-byte Folded Reload\n\tdaddiu\t$2, $zero, 10\n\t.loc\t129 243 14 is_stmt 1            # format.go:243:14\n\tddivu\t$zero, $3, $2\n\tteq\t$2, $zero, 7\n\tmflo\t$2\nLet’s ignore most of this and focus on one specific thing: ddivu $zero, $3, $2. Looks correct, right?\nWell… let’s look into how PCSX2 loads this:\n\n      ??????\n\nYep. PCSX2 doesn’t see the DDIVU instruction. Or, more specifically, the PlayStation  doesn’t see it.\n\nThe DDIVU (doubleword divide unsigned) is a instruction defined in MIPS-III (source) responsible for doing the division of 2 unsigned 64bit integers.\nHowever, that doesn’t work in the PS2, as we saw before. You see, the DDIVU instruction is not defined (source) in the PS2 MIPS instruction set - only DIVU is. This introduces a major problem, as now all int64 (with DDIV) and uint64 (with DDIVU) divisions won’t execute - or will execute incorrectly if it ends up matching some other instruction. We need to avoid that, and either split this division inside the Go compiler in a way that would not do the 64bit version of it, or modify the LLVM so that it won’t use this instruction, even on a mips3 CPU. Or maybe we can implement a custom CPU inside the LLVM - the r5900, like ps2dev’s GCC.\nFinding a way out\nMy first thought was “oh, let’s adapt this in the LLVM”. But, and I’m not gonna lie to you, changing that code is hell. It’s very complex and requires a ton of changes and most of the time it even requires full rebuild of the LLVM project. I’m just too lazy for that. So I’ve opted for the terrible approach of doing this inside TinyGo’s compiler.\nThe first step is to have a 64bit division code. According to my good friend ChatGPT (who has never been wrong before /s), whenever 64bit division is not available (like in the R5900), GCC uses an auxiliary function called __udivdi3:\nuint64_t __udivdi3(uint64_t a, uint64_t b);\nSo my thought was: oh, I can just map the uint64 division to this then. The first step is to add this as something that is available on our runtime_ps2.go (because I’m too lazy to do the full proper call):\n//go:build ps2\n\npackage runtime\n\n/*\nextern long __divdi3(long a, long b);\nextern unsigned long __udivdi3 (unsigned long a, unsigned long b);\nextern long __moddi3(long a, long b);\nextern unsigned long __umoddi3(unsigned long a, unsigned long b);\n*/\nimport \"C\"\n\nfunc int64div(a, b int64) int64 {\n\treturn int64(C.__divdi3(C.long(a), C.long(b)))\n}\n\nfunc uint64div(a, b uint64) uint64 {\n\treturn uint64(C.__udivdi3(C.ulong(a), C.ulong(b)))\n}\n\nfunc int64mod(a, b int64) int64 {\n\treturn int64(C.__moddi3(C.long(a), C.long(b)))\n}\n\nfunc uint64mod(a, b uint64) uint64 {\n\treturn uint64(C.__umoddi3(C.ulong(a), C.ulong(b)))\n}\nThen, we need to modify TinyGo’s compiler to use it. That is simpler than it sounds - it’s all handled here.\nLet’s start with the unsigned operations:\nif op == token.QUO {\n\treturn b.CreateUDiv(x, y, \"\"), nil\n} else {\n\treturn b.CreateURem(x, y, \"\"), nil\n}\nwill then become:\nif op == token.QUO {\n\tif (x.Type().TypeKind() == llvm.IntegerTypeKind && x.Type().IntTypeWidth() == 64) ||\n\t\t(y.Type().TypeKind() == llvm.IntegerTypeKind && y.Type().IntTypeWidth() == 64) {\n\t\treturn b.createRuntimeCall(\"uint64div\", []llvm.Value{x, y}, \"\"), nil\n\t} else {\n\t\treturn b.CreateUDiv(x, y, \"\"), nil\n\t}\n} else {\n\tif (x.Type().TypeKind() == llvm.IntegerTypeKind && x.Type().IntTypeWidth() == 64) ||\n\t\t(y.Type().TypeKind() == llvm.IntegerTypeKind && y.Type().IntTypeWidth() == 64) {\n\t\treturn b.createRuntimeCall(\"uint64mod\", []llvm.Value{x, y}, \"\"), nil\n\t} else {\n\t\treturn b.CreateURem(x, y, \"\"), nil\n\t}\n}\nThen we just rebuild the TinyGo’s compiler with a make, and rebuild our application. Let’s retest our previous code:\n\n      Fuck yeah!\n\nAnd for our int64 operations as well please. From the following code:\nif op == token.QUO {\n\treturn b.CreateSDiv(x, y, \"\"), nil\n} else {\n\treturn b.CreateSRem(x, y, \"\"), nil\n}\nwe adapt it into this:\nif op == token.QUO {\n\tif (x.Type().TypeKind() == llvm.IntegerTypeKind && x.Type().IntTypeWidth() == 64) ||\n\t\t(y.Type().TypeKind() == llvm.IntegerTypeKind && y.Type().IntTypeWidth() == 64) {\n\t\treturn b.createRuntimeCall(\"int64div\", []llvm.Value{x, y}, \"\"), nil\n\t} else {\n\t\treturn b.CreateSDiv(x, y, \"\"), nil\n\t}\n} else {\n\tif (x.Type().TypeKind() == llvm.IntegerTypeKind && x.Type().IntTypeWidth() == 64) ||\n\t\t(y.Type().TypeKind() == llvm.IntegerTypeKind && y.Type().IntTypeWidth() == 64) {\n\t\treturn b.createRuntimeCall(\"int64mod\", []llvm.Value{x, y}, \"\"), nil\n\t} else {\n\t\treturn b.CreateSRem(x, y, \"\"), nil\n\t}\n}\nFinally, we can test our changes by doing this:\ndebug.Printf(\"\\n\\n\")\nfor i := int64(-8); i <= 8; i++ {\n\tdebug.Printf(\"%02d | div02 = %02d | mod04 = %02d\\n\", i, i/2, i%4)\n}\n\nAnd with that, we’re done for the 64bit integer problem! Yey!\n\nYes, I know, there might be other instructions that are not implemented. Not looking into this now for sure. Also, yes, I didn’t get big-ass numbers, but also I don’t need them now.\n\nSpoiler: not doing this on the LLVM level will bite our asses in the future.\n\nWhat now?\nWell, now we need to keep pushing forward! But I need to stop this post at some point so that people can catch up with this project, and also so I can publish these findings. But there’s a lot still to get done:\n\nTarget-specific things, such as syscalls, inline assembly and interrupt support\nFloating points as they are non-functional at the moment\nNew LLVM MIPS CPU - yes, we’ll probably need that, plus this way we can avoid hacking code inside TinyGo’s compiler\nEverything else we want!\n\nYou may be wondering, “what can I do with it now”? Well, you can do whatever you want, actually. You can call ps2dev’s libraries and play with them, and if something fails, you can just call C code from Go. But your code will be running from the Go side of things first, which is pretty neat in my opinion - even if a bit limited for now.\nI’m already working on the next part of this project though, so stay tuned! See you around!\n\n      👋\n\n     Coding\n\n     Embedded\n\n     Go",
    "summary": {
      "en": "**Summary: Golang on the PlayStation 2**\n\nThe author, Ricardo, is exploring the possibility of running Go programming language code on the PlayStation 2 (PS2), a console released by Sony in 2000. He finds this intriguing because traditional development for consoles typically uses lower-level languages. Ricardo aims to use TinyGo, a Go compiler for small devices, to overcome the limitations of Go's support for the PS2's architecture.\n\nKey challenges include:\n1. **Lack of Direct Support**: Go doesn’t natively support the PS2's Emotion Engine CPU, which requires a workaround using TinyGo.\n2. **Technical Complexity**: The PS2 uses the MIPS R5900 architecture, and integrating with the existing ps2dev SDK introduces compatibility issues.\n3. **Emulator Testing**: Ricardo conducts his experiments in an emulator rather than on actual hardware for convenience.\n\nHe details the process of configuring TinyGo to target the PS2, addressing issues with generating compatible binaries and linking to ps2dev libraries. He also explains the creation of custom runtime definitions and functions necessary for Go to operate on the PS2.\n\nFurther complications arise from the PS2's architecture, specifically the absence of certain MIPS instructions like DDIVU, which affects how integer divisions are handled. Ricardo devises a solution by modifying TinyGo to use auxiliary functions for division.\n\nLooking ahead, he plans to implement more features and optimizations, such as handling floating-point operations and syscalls. He encourages others to experiment with the setup and promises more updates on his progress.",
      "ko": "리카르도는 2000년에 소니가 출시한 플레이스테이션 2(PS2)에서 Go 프로그래밍 언어 코드를 실행할 가능성을 탐구하고 있다. 그는 전통적인 콘솔 개발이 보통 저수준 언어를 사용한다는 점에서 이 주제가 흥미롭다고 생각한다. 리카르도는 PS2의 아키텍처에 대한 Go의 지원 한계를 극복하기 위해 소형 장치용 Go 컴파일러인 TinyGo를 사용하려고 한다.\n\n주요 도전 과제는 다음과 같다. 첫째, Go는 PS2의 이모션 엔진 CPU를 기본적으로 지원하지 않기 때문에 TinyGo를 사용해 우회해야 한다. 둘째, PS2는 MIPS R5900 아키텍처를 사용하고 있으며, 기존의 ps2dev SDK와 통합하는 과정에서 호환성 문제가 발생한다. 셋째, 리카르도는 실제 하드웨어 대신 에뮬레이터에서 실험을 진행하여 편리함을 추구하고 있다.\n\n그는 PS2를 목표로 TinyGo를 설정하는 과정에 대해 자세히 설명하며, 호환 가능한 바이너리를 생성하고 ps2dev 라이브러리에 연결하는 데 발생하는 문제를 다룬다. 또한, Go가 PS2에서 작동하기 위해 필요한 사용자 정의 런타임 정의와 함수 생성에 대해서도 설명한다.\n\nPS2의 아키텍처로 인해 추가적인 복잡성이 발생하는데, 특히 DDIVU와 같은 특정 MIPS 명령어가 없어서 정수 나누기 처리 방식에 영향을 미친다. 리카르도는 나누기를 위해 보조 함수를 사용하도록 TinyGo를 수정하는 해결책을 고안했다.\n\n앞으로 그는 부동 소수점 연산과 시스템 호출 처리와 같은 더 많은 기능과 최적화를 구현할 계획이다. 그는 다른 사람들도 이 설정을 실험해 보기를 권장하며, 자신의 진행 상황에 대한 추가 업데이트를 약속하고 있다.",
      "ja": null
    }
  },
  {
    "id": "c3a9091d32d97723",
    "title": {
      "en": "I genuinely don't understand why some people are still bullish about LLMs",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://twitter.com/skdh/status/1905132853672784121",
    "score": 654,
    "by": "ksec",
    "time": 1743110562,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "e4f517c188f4dad1",
    "title": {
      "en": "Philosophy of Coroutines (2023)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/coroutines-philosophy/",
    "score": 81,
    "by": "HeliumHydride",
    "time": 1743096215,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "efa56325d333129f",
    "title": {
      "en": "Building Node.js on Windows with Clang-cl",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://joyeecheung.github.io/blog/2025/02/16/building-nodejs-on-windows-with-clang-cl/",
    "score": 7,
    "by": "mpweiher",
    "time": 1742907540,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "fea5b956630dbb77",
    "title": {
      "en": "Abundance isn't going to happen unless politicians are scared of the status quo",
      "ko": "변화 없이는 풍요 없다",
      "ja": null
    },
    "type": "story",
    "url": "https://inpractice.yimbyaction.org/p/abundance-isnt-going-to-happen-unless",
    "score": 321,
    "by": "viajante1882",
    "time": 1743095353,
    "content": "Share this postIn PracticeAbundance Isn’t Going To Happen Unless Politicians Are Scared of the Status QuoCopy linkFacebookEmailNotesMoreDiscover more from In PracticePractical politics for housing reform. SubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.Already have an account? Sign inAbundance Isn’t Going To Happen Unless Politicians Are Scared of the Status QuoIt’s A Race Between Building Up and Burning DownLaura FooteMar 28, 202521Share this postIn PracticeAbundance Isn’t Going To Happen Unless Politicians Are Scared of the Status QuoCopy linkFacebookEmailNotesMore17Share“Too many goods created a bad.”That’s how California Governor Jerry Brown put it in 2017 when he signed a package of 15 housing-related bills, the first YIMBY-supported pro-housing bills. He was explaining, in his meandering way, how the housing shortage was the result of hundreds of well-intentioned laws, which had promised all kinds of things, but mostly succeeded in slowing down the production of new housing.In many ways, Jerry was ahead of the curve. He was lamenting “everything bagel liberalism” and “process over outcomes” before it was cool.But at the same time, there was a distinct lack of fire under the Governor’s ass. He was skeptical that this new legislation would do much to unwind the tangled legal web holding back housing production. And while we YIMBYs were bright eyed and bushy tailed, seeing as it was the first state bill signing we’d been invited to, it was also kind of depressing.Less than ten years later, the discourse is suddenly all about the seemingly simple idea that we should “do things and build things,” and there is finally a sense of urgency. I’m heading to Atlanta this week to interview Derek Thompson about his new book Abundance with Ezra Klein at the Georgia Center for the Book (tickets). Their thesis is simple: we need more of everything. More housing, more immigrants, more clean energy infrastructure.And Klein and Thomson’s book is just the latest entry into the growing genre that attempts to address the root causes of stagnation in America and prescribe a path towards abundance and renewal. Recent books in this genre include:Why Nothing Works by Marc J. DunkelmanStuck by Yoni Applebaum,Recoding America by Jennifer Pahlka,Meanwhile Ned Resnikoff is pointing out that YIMBYs were doing abundance before it was cool (or maybe were the driving force in making it cool). While I appreciate the credit to the original YIMBY brand, I’m choosing to adopt an abundance mindset about abundance. More is more, after all.SubscribeAs I see it, the Abundance concept isn’t quite an ideology, it’s more of a refocusing on outcomes. It’s a framework that points to tangible outcomes and asks us to tactically identify what is blocking progress to that goal, irrespective of what the intention of that blockage might be. It extends the classic YIMBY way of doing politics into a larger philosophy. It is a re-focusing on “ends” over “means,” and allows for a variety of ideologies to come together on specific ends. The language around “outcomes focused legislating” brings some degree of sanity to our often self-sabotaging process.Outcome-based politics seems so obvious that it can sound silly to say it out loud. Government should deliver material outcomes. Elected officials should be extremely motivated to produce tangible outcomes for the largest number of people.So why isn’t that happening?What’s eating state capacity?The hot topic in the growing abundance discourse is state capacity. That’s a fancy term for a simple concept — the government’s ability to deliver outcomes, whether those are growing the economy, establishing laws, or just picking up the garbage.It’s not a new idea. In fact, five decades ago, some of the world’s most pre-eminent political scientists fretted about the erosion of state capacity in democracies across the world. “The demands on democratic government grow,” wrote Michael Crozier, Samuel Huntington, and Joji Watanuki, “while the capacity of democratic government stagnates.”Sounds familiar, doesn’t it? They argued that in countries like the United States, Japan, and Western Europe, citizens were asking their governments to take on more and more without increasing their government’s ability to carry out those projects. That led to an erosion in legitimacy, and decades of retrenchment and cutbacks. About the only thing that the left and the right seemed to agree on was that the “era of big government is over.”The 70’s was an era of community organizing to stop The Big Bad Thing, and this urge to tap the brakes continues to this day. Elaborate outreach processes, reporting requirements and the opportunities for objections about those reports… they all can seem reasonable. But added together they created a mighty web. Marc Dunkelman explained the problem of this approach on a recent episode of the Political Gabfest: “We've now created a system where there are so many veto players and you need so many approvals that government fundamentally doesn't work.”Little by little, we stitched together a gigantic wet blanket that continues to hold back housing production. It’s easy to blame liberals, but like all of our worst problems, it was bipartisan. Conservatives liked constraining government because they didn’t trust it. Liberals liked constraining it because they overestimated how much those constraints would produce better outcomes like protecting the marginalized, preserving the environment, and elevating the voices of the community. And people who felt that integration was a threat, helped create innumerable local processes to tap the brakes on all kinds of public goods and housing.As a society, we have chronically underestimated the cost of all this. It’s a chronic case of letting the perfect be the enemy of the good. Every constraint makes the continuation of the status quo more likely. Legislators and advocates know this, but treat it like the normal cost of getting the sausage made. In the process of getting pro-housing bills passed, I’ve had countless arguments with other advocates and legislators over the innumerable ways bills are weakened. And while we’re taking steps in the right direction, the outcomes still aren’t great.State capacity is being eaten by excessive process. Overly restrictive rules (zoning) and elaborate process (permitting and planning) create chronic shortages driving prices higher and creating an angry populace. That’s the TLDR thesis of a lot of the abundance books. Things grind to a halt. People suffer. It all feels incredibly self-sabotaging and frustrating.Two paths forwardThat brings us to today, in which people across the country are mad that shit sucks. I could say more but, come on, shit sucks. Prices are high, infrastructure is crumbling, people are pissed, you’ve heard this already.There are two big responses happening right now:Tear down (DOGE vibes)Build up (Abundance vibes)The DOGE point of view says if the government can’t do anything, we should just get rid of the government. That appeals to many Americans because they are angry. The frustrated urge to blow it all up is strong. As a rule, shortages do not bring out the best in humanity. They make us blame perceived-outsiders and foster the urge to topple governments.The housing shortage fosters a “crabs in a bucket” mentality everywhere, from Blue-dot cities to Red rural communities. Whether your enemy is yuppies, coastal elites or immigrants, the through-line is that there isn’t enough to go around and someone is stealing from us. The DOGE-style of governance is about trying to tell everyone who has been stealing and publicly firing those people.The alternative gaining traction is a (sometimes vague) notion of abundance, which boils down to “things should work.” But if these new books are any indication, the Build Up team is feeling more urgency. The consensus that government is not delivering tangible good outcomes for average people is finally being recognized as an existential threat to the democratic project. The constituency for “can we please just fucking do things” is real.YIMBY has a practical goal of housing abundance. And for years, YIMBYs have been building ideologically diverse coalitions aligned on that specific, narrow goal.And while that work is great, my key point is that we are running out of time. “Get your house in order” should have a deep level of urgency right now. Elected officials at the state and local level need to rebuild the belief that government is worth preserving and can deliver a thriving middle class.We are in the middle of a race between the destroyers and the builders, and too many elected officials are twiddling their thumbs the sinking ship of the status quo.Who needs to change?People will nod and agree to everything I just said above, but what does it actually look like in practice?Literally yesterday I spoke with a city council member who was thinking about introducing single stair reform in their city. He knew how it could be a deeply impactful reform and is completely safe. But then he said “We can’t do it without the support of the firefighters union, and they’re deeply opposed.” Every redundant requirement was deliberately put there by someone who doesn’t think it needs reform and will fight it. Most elected officials weigh the various highly engaged stakeholders, as if they represent the average voters in their district. They’re not.YIMBY Action, through our local chapter model, is building a visible constituency to incentivize politicians to take bolder action on housing. But politicians need to get ahead of this. To critique my own work: it shouldn’t be necessary! Elected officials should be more concerned that the general public is feeling economically stunted and enraged!Incumbents should be more terrified of not doing things. The status quo is a downward trajectory and you will be punished electorally for maintaining it. If people continue to feel economically stunted, they will continue to boot incumbents. Being committed to outcomes requires continuous deep commitment to pushing back, with the knowledge that outcomes add up to that important “right track / wrong track” polling data.Share21Share this postIn PracticeAbundance Isn’t Going To Happen Unless Politicians Are Scared of the Status QuoCopy linkFacebookEmailNotesMore17Share",
    "summary": {
      "en": "The article discusses the urgent need for politicians to prioritize housing and economic growth in the face of a stagnant government system that is failing to deliver positive outcomes for the public. \n\nKey points include:\n\n1. **Historical Context**: California Governor Jerry Brown acknowledged that excessive regulations have slowed housing production, highlighting a long-standing issue in American governance where well-meaning laws have created barriers instead of solutions.\n\n2. **Abundance Mindset**: There is a growing movement advocating for an \"abundance\" mindset, which emphasizes the importance of producing tangible results, such as more housing and infrastructure, rather than getting bogged down by complicated processes and red tape.\n\n3. **State Capacity**: The term \"state capacity\" refers to the government's ability to effectively implement policies. The article argues that excessive rules and procedures have diminished this capacity, leading to frustration among citizens due to rising costs and deteriorating services.\n\n4. **Two Responses to Current Issues**: The article outlines two approaches to addressing these challenges: \n   - **Tear Down**: A radical viewpoint that suggests dismantling government structures due to their inefficacy.\n   - **Build Up**: A more constructive approach focused on improving and expanding government capabilities to deliver better outcomes.\n\n5. **Call to Action**: It emphasizes the urgency for elected officials to act decisively to restore faith in government effectiveness. Politicians should be more afraid of maintaining the status quo than of making bold changes, as public frustration could lead to electoral consequences.\n\nIn summary, the article argues that a shift towards an outcomes-focused approach in governance is essential for addressing the housing crisis and improving public welfare.",
      "ko": "이 기사는 정치인들이 주택과 경제 성장을 우선시해야 한다는 긴급한 필요성을 다루고 있습니다. 현재 정부 시스템이 정체되어 긍정적인 결과를 제공하지 못하고 있기 때문입니다.\n\n첫 번째로, 역사적 맥락을 살펴보면, 캘리포니아 주지사 제리 브라운은 과도한 규제가 주택 생산을 저해하고 있다고 인정했습니다. 이는 미국 정부에서 오랫동안 이어져 온 문제로, 선의의 법들이 해결책이 아닌 장벽을 만들어왔다는 점을 강조합니다.\n\n또한, '풍요로운 사고방식'을 지지하는 움직임이 커지고 있습니다. 이는 복잡한 절차와 규제에 얽매이지 않고, 더 많은 주택과 인프라와 같은 실질적인 결과를 생산하는 것이 중요하다는 점을 강조합니다.\n\n'국가 역량'이라는 용어는 정부가 정책을 효과적으로 시행할 수 있는 능력을 의미합니다. 이 기사는 과도한 규칙과 절차가 이 역량을 저하시켜, 시민들이 비용 상승과 서비스 저하로 인해 불만을 느끼게 만들고 있다고 주장합니다.\n\n현재 문제를 해결하기 위한 두 가지 접근 방식이 제시됩니다. 첫 번째는 '철거'로, 비효율적인 정부 구조를 해체하자는 급진적인 관점입니다. 두 번째는 '구축'으로, 정부의 역량을 개선하고 확장하여 더 나은 결과를 제공하자는 보다 건설적인 접근입니다.\n\n마지막으로, elected officials가 정부의 효과성에 대한 신뢰를 회복하기 위해 신속하게 행동해야 한다는 긴급성을 강조합니다. 정치인들은 대담한 변화를 두려워하기보다는 현 상태를 유지하는 것에 더 두려움을 느껴야 하며, 시민들의 불만이 선거 결과에 영향을 미칠 수 있다는 점을 인식해야 합니다.\n\n결론적으로, 이 기사는 주택 위기를 해결하고 공공 복지를 향상시키기 위해 결과 중심의 접근 방식으로의 전환이 필수적이라고 주장합니다.",
      "ja": null
    }
  },
  {
    "id": "14f7ff8f6ae0bd5e",
    "title": {
      "en": "Learning Theory from First Principles [pdf]",
      "ko": "기초부터 배우는 이론",
      "ja": null
    },
    "type": "story",
    "url": "https://www.di.ens.fr/~fbach/ltfp_book.pdf",
    "score": 190,
    "by": "Anon84",
    "time": 1743108313,
    "content": "Learning Theory from First Principles  August 27, 2024  Francis Bach  francis.bach@inria.fr  Copyright   in   this   Work   has   been   licensed   exclusively   to   The   MIT   Press,  http://mitpress.mit.edu ,   which   will   be   releasing   the   final   version   to   the   public   in 2024.   All inquiries regarding rights should be addressed to The MIT Press, Rights and Permissions Department.\n\nContents  Preface   xi  I   Preliminaries   1  1   Mathematical Preliminaries   3  1.1   Linear Algebra and Differentiable Calculus   . . . . . . . . . . . . . . . . .   3  1.1.1   Minimization of Quadratic Forms   . . . . . . . . . . . . . . . . . . .   3  1.1.2   Inverting a 2   ×   2 Matrix   . . . . . . . . . . . . . . . . . . . . . . . .   4  1.1.3   Inverting Matrices Defined by Blocks, Matrix Inversion Lemma   . .   4  1.1.4   Eigenvalue and Singular Value Decomposition   . . . . . . . . . . . .   6  1.1.5   Differential Calculus   . . . . . . . . . . . . . . . . . . . . . . . . . .   7  1.2   Concentration Inequalities   . . . . . . . . . . . . . . . . . . . . . . . . . . .   7  1.2.1   Hoeffding’s Inequality   . . . . . . . . . . . . . . . . . . . . . . . . .   10  1.2.2   McDiarmid’s Inequality   . . . . . . . . . . . . . . . . . . . . . . . .   13  1.2.3   Bernstein’s Inequality ( \u0007 )   . . . . . . . . . . . . . . . . . . . . . .   14  1.2.4   Expectation of the Maximum   . . . . . . . . . . . . . . . . . . . . .   16  1.2.5   Estimation of Expectations through Quadrature ( \u0007\u0007 )   . . . . . . .   18  1.2.6   Concentration Inequalities for Random Matrices ( \u0007\u0007 )   . . . . . . .   19  2   Introduction to Supervised Learning   21  2.1   From Training Data to Predictions   . . . . . . . . . . . . . . . . . . . . . .   22  2.2   Decision Theory   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   25  2.2.1   Supervised Learning Problems and Loss Functions   . . . . . . . . .   25  2.2.2   Risks   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   27  2.2.3   Bayes Risk and Bayes Predictor   . . . . . . . . . . . . . . . . . . . .   28  2.3   Learning from Data   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   30  2.3.1   Local Averaging   . . . . . . . . . . . . . . . . . . . . . . . . . . . .   31  2.3.2   Empirical Risk Minimization   . . . . . . . . . . . . . . . . . . . . .   32  2.4   Statistical Learning Theory   . . . . . . . . . . . . . . . . . . . . . . . . . .   36 iii\n\niv   CONTENTS  2.4.1   Measures of Performance   . . . . . . . . . . . . . . . . . . . . . . .   36  2.4.2   Notions of Consistency over Classes of Problems   . . . . . . . . . .   36  2.5   “No Free Lunch” Theorems ( \u0007 )   . . . . . . . . . . . . . . . . . . . . . . . .   38  2.6   Quest for Adaptivity   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   39  2.7   Beyond Supervised Learning   . . . . . . . . . . . . . . . . . . . . . . . . . .   40  2.8   Summary–Book Outline   . . . . . . . . . . . . . . . . . . . . . . . . . . . .   41  3   Linear Least-Squares Regression   45  3.1   Introduction   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   45  3.2   Least-Squares Framework   . . . . . . . . . . . . . . . . . . . . . . . . . . .   46  3.3   Ordinary Least-Squares Estimator   . . . . . . . . . . . . . . . . . . . . . .   47  3.3.1   Closed-Form Solution   . . . . . . . . . . . . . . . . . . . . . . . . .   47  3.3.2   Geometric Interpretation   . . . . . . . . . . . . . . . . . . . . . . .   48  3.3.3   Numerical Resolution   . . . . . . . . . . . . . . . . . . . . . . . . .   49  3.4   Statistical Analysis of Ordinary Least-Squares   . . . . . . . . . . . . . . . .   49  3.5   Fixed Design Setting   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   50  3.5.1   Statistical Properties of the OLS Estimator   . . . . . . . . . . . . .   52  3.5.2   Experiments   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   54  3.6   Ridge Least-Squares Regression   . . . . . . . . . . . . . . . . . . . . . . . .   56  3.7   Lower Bound ( \u0007 )   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   60  3.8   Random Design Analysis   . . . . . . . . . . . . . . . . . . . . . . . . . . . .   63  3.8.1   Gaussian Designs   . . . . . . . . . . . . . . . . . . . . . . . . . . . .   64  3.8.2   General Designs ( \u0007\u0007 )   . . . . . . . . . . . . . . . . . . . . . . . . .   65  3.9   Principal Component Analysis ( \u0007 )   . . . . . . . . . . . . . . . . . . . . . .   66  3.10 Conclusion   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   68  II   Generalization Bounds for Learning Algorithms   69  4   Empirical Risk Minimization   71  4.1   Convexification of the Risk   . . . . . . . . . . . . . . . . . . . . . . . . . .   72  4.1.1   Convex Surrogates   . . . . . . . . . . . . . . . . . . . . . . . . . . .   73  4.1.2   Geometric Interpretation of the Support Vector Machine ( \u0007 )   . . .   74  4.1.3   Conditional Φ-risk and Classification Calibration ( \u0007 )   . . . . . . . .   76  4.1.4   Relation between Risk and Φ-risk ( \u0007\u0007 )   . . . . . . . . . . . . . . .   79  4.2   Risk Minimization Decomposition   . . . . . . . . . . . . . . . . . . . . . .   84  4.3   Approximation Error   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   84  4.4   Estimation Error   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   85  4.4.1   Application of McDiarmid’s Inequality   . . . . . . . . . . . . . . . .   86  4.4.2   Easy Case I: Quadratic Functions   . . . . . . . . . . . . . . . . . . .   87  4.4.3   Easy Case II: Finite Number of Models   . . . . . . . . . . . . . . .   88  4.4.4   Beyond Finitely Many Models through Covering Numbers ( \u0007 )   . .   89  4.5   Rademacher Complexity   . . . . . . . . . . . . . . . . . . . . . . . . . . . .   91  4.5.1   Symmetrization   . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   92\n\nCONTENTS   v  4.5.2   Lipschitz-Continuous Losses   . . . . . . . . . . . . . . . . . . . . . .   94  4.5.3   Ball-Constrained Linear Predictions   . . . . . . . . . . . . . . . . .   96  4.5.4   Putting Things Together (Linear Predictions)   . . . . . . . . . . . .   97  4.5.5   From Constrained to Regularized Estimation ( \u0007 )   . . . . . . . . .   98  4.5.6   Extensions and Improvements   . . . . . . . . . . . . . . . . . . . . .   102  4.6   Model Selection ( \u0007 )   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   103  4.6.1   Structural Risk Minimization ( \u0007 )   . . . . . . . . . . . . . . . . . . .   104  4.6.2   Selection Based on Validation Set ( \u0007 )   . . . . . . . . . . . . . . . .   104  4.7   Relation with Asymptotic Statistics ( \u0007 )   . . . . . . . . . . . . . . . . . . .   105  4.8   Summary   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   107  5   Optimization for Machine Learning   109  5.1   Optimization in Machine Learning   . . . . . . . . . . . . . . . . . . . . . .   109  5.2   Gradient Descent   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   111  5.2.1   Simplest Analysis: Ordinary Least-Squares   . . . . . . . . . . . . .   112  5.2.2   Convex Functions and Their Properties   . . . . . . . . . . . . . . .   116  5.2.3   Analysis of Gradient Descent for Strongly Convex and Smooth Functions   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   119  5.2.4   Analysis of Gradient Descent for Convex and Smooth Functions ( \u0007 )   124  5.2.5   Beyond Gradient Descent ( \u0007 )   . . . . . . . . . . . . . . . . . . . .   126  5.2.6   Nonconvex Objective Functions ( \u0007 )   . . . . . . . . . . . . . . . . .   129  5.3   Gradient Methods on Nonsmooth Problems   . . . . . . . . . . . . . . . . .   130  5.4   Stochastic Gradient Descent   . . . . . . . . . . . . . . . . . . . . . . . . . .   134  5.4.1   Strongly Convex Problems ( \u0007 )   . . . . . . . . . . . . . . . . . . . .   139  5.4.2   Adaptive Methods ( \u0007 )   . . . . . . . . . . . . . . . . . . . . . . . . .   141  5.4.3   Bias-Variance Trade-offs for Least-Squares ( \u0007 )   . . . . . . . . . . .   143  5.4.4   Variance Reduction ( \u0007 )   . . . . . . . . . . . . . . . . . . . . . . . .   146  5.5   Conclusion   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   151  6   Local Averaging Methods   155  6.1   Introduction   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   155  6.2   Local Averaging Methods   . . . . . . . . . . . . . . . . . . . . . . . . . . .   157  6.2.1   Linear Estimators   . . . . . . . . . . . . . . . . . . . . . . . . . . .   157  6.2.2   Partition Estimators   . . . . . . . . . . . . . . . . . . . . . . . . . .   158  6.2.3   Nearest-Neighbors   . . . . . . . . . . . . . . . . . . . . . . . . . . .   160  6.2.4   Nadaraya-Watson Estimator (aka Kernel Regression) ( \u0007 )   . . . . .   162  6.3   Generic Simplest Consistency Analysis   . . . . . . . . . . . . . . . . . . . .   163  6.3.1   Fixed Partition   . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   165  6.3.2   k -nearest Neighbor   . . . . . . . . . . . . . . . . . . . . . . . . . . .   168  6.3.3   Kernel Regression (Nadaraya-Watson) ( \u0007 )   . . . . . . . . . . . . .   170  6.4   Universal Consistency ( \u0007 )   . . . . . . . . . . . . . . . . . . . . . . . . . . .   174  6.5   Adaptivity ( \u0007\u0007 )   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   177  6.6   Conclusion   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   178\n\nvi   CONTENTS  7   Kernel Methods   179  7.1   Introduction   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   180  7.2   Representer Theorem   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   181  7.3   Kernels   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   183  7.3.1   Linear and Polynomial Kernels   . . . . . . . . . . . . . . . . . . . .   186  7.3.2   Translation-Invariant Kernels on [0 ,   1]   . . . . . . . . . . . . . . . .   187  7.3.3   Translation-Invariant Kernels on   R d   . . . . . . . . . . . . . . . . .   191  7.3.4   Beyond Vectorial Input Spaces ( \u0007 )   . . . . . . . . . . . . . . . . . .   194  7.4   Algorithms   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   196  7.4.1   Representer Theorem   . . . . . . . . . . . . . . . . . . . . . . . . .   196  7.4.2   Column Sampling   . . . . . . . . . . . . . . . . . . . . . . . . . . .   197  7.4.3   Random Features   . . . . . . . . . . . . . . . . . . . . . . . . . . . .   198  7.4.4   Dual Algorithms ( \u0007 )   . . . . . . . . . . . . . . . . . . . . . . . . . .   199  7.4.5   Stochastic Gradient Descent ( \u0007 )   . . . . . . . . . . . . . . . . . . .   200  7.4.6   Kernelization of Linear Algorithms   . . . . . . . . . . . . . . . . . .   201  7.5   Generalization Guarantees–Lipschitz-continuous Losses   . . . . . . . . . . .   202  7.5.1   Risk Decomposition   . . . . . . . . . . . . . . . . . . . . . . . . . .   203  7.5.2   Approximation Error for Translation-Invariant Kernels on   R d   . . .   205  7.6   Theoretical Analysis of Ridge Regression ( \u0007 )   . . . . . . . . . . . . . . . .   208  7.6.1   Kernel Ridge Regression as a Linear Estimator   . . . . . . . . . . .   208  7.6.2   Bias and Variance Decomposition ( \u0007 )   . . . . . . . . . . . . . . . .   209  7.6.3   Relating Empirical and Population Covariance Operators   . . . . .   212  7.6.4   Analysis for Well-Specified Problems ( \u0007 )   . . . . . . . . . . . . . . .   214  7.6.5   Analysis beyond Well-Specified Problems ( \u0007 )   . . . . . . . . . . . .   216  7.6.6   Balancing Bias and Variance ( \u0007 )   . . . . . . . . . . . . . . . . . . .   217  7.7   Experiments   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   218  7.8   Conclusion   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   220  8   Sparse Methods   221  8.1   Introduction   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   221  8.1.1   Dedicated Proof Technique for Constrained Least-Squares   . . . . .   223  8.1.2   Probabilistic and Combinatorial Lemmas   . . . . . . . . . . . . . .   224  8.2   Variable Selection by the   ℓ 0 -penalty   . . . . . . . . . . . . . . . . . . . . .   226  8.2.1   Assuming That   k   Is Known   . . . . . . . . . . . . . . . . . . . . . .   226  8.2.2   Sparsity-Adaptive Estimation (Unknown   k ) ( \u0007 )   . . . . . . . . . . .   228  8.3   Variable Selection by   ℓ 1 -regularization   . . . . . . . . . . . . . . . . . . . .   231  8.3.1   Intuition and Algorithms   . . . . . . . . . . . . . . . . . . . . . . .   231  8.3.2   Slow Rates–Random Design   . . . . . . . . . . . . . . . . . . . . . .   234  8.3.3   Slow Rates–Fixed Design (Square Loss)   . . . . . . . . . . . . . . .   236  8.3.4   Fast Rates–Fixed Design ( \u0007 )   . . . . . . . . . . . . . . . . . . . . .   238  8.3.5   Zoo of Conditions ( \u0007\u0007 )   . . . . . . . . . . . . . . . . . . . . . . . .   239  8.3.6   Fast Rates–Random Design ( \u0007 )   . . . . . . . . . . . . . . . . . . . .   241  8.4   Experiments   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   243  8.5   Extensions   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   243\n\nCONTENTS   vii  8.6   Conclusion   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   245  9   Neural Networks   247  9.1   Introduction   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   247  9.2   Single Hidden-Layer Neural Network   . . . . . . . . . . . . . . . . . . . . .   249  9.2.1   Optimization   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   251  9.2.2   Rectified Linear Units and Homogeneity   . . . . . . . . . . . . . . .   253  9.2.3   Estimation Error   . . . . . . . . . . . . . . . . . . . . . . . . . . . .   253  9.3   Approximation Properties   . . . . . . . . . . . . . . . . . . . . . . . . . . .   256  9.3.1   Universal Approximation Property in One Dimension   . . . . . . .   256  9.3.2   Infinitely Many Neurons and the Variation Norm   . . . . . . . . . .   257  9.3.3   Variation Norm in One Dimension   . . . . . . . . . . . . . . . . . .   260  9.3.4   Variation Norm in an Arbitrary Dimension   . . . . . . . . . . . . .   263  9.3.5   Precise Approximation Properties   . . . . . . . . . . . . . . . . . .   265  9.3.6   From the Variation Norm to a Finite Number of Neurons ( \u0007 )   . . .   266  9.4   Generalization Performance for Neural Networks   . . . . . . . . . . . . . .   269  9.5   Relationship with Kernel Methods ( \u0007 )   . . . . . . . . . . . . . . . . . . . .   271  9.5.1   From a Banach Space   F 1   to a Hilbert Space   F 2   ( \u0007 )   . . . . . . . . .   271  9.5.2   Kernel Function ( \u0007\u0007 )   . . . . . . . . . . . . . . . . . . . . . . . . .   273  9.5.3   Upper Bound on RKHS Norm ( \u0007\u0007 )   . . . . . . . . . . . . . . . . .   275  9.6   Experiments   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   277  9.7   Extensions   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   278  9.8   Conclusion   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   279  III   Special Topics   281  10 Ensemble Learning   283  10.1 Averaging/Bagging   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   284  10.1.1   Independent Datasets   . . . . . . . . . . . . . . . . . . . . . . . . .   284  10.1.2   Bagging   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   286  10.2 Random Projections and Averaging   . . . . . . . . . . . . . . . . . . . . .   288  10.2.1   Gaussian Sketching   . . . . . . . . . . . . . . . . . . . . . . . . . . .   290  10.2.2   Random Projections   . . . . . . . . . . . . . . . . . . . . . . . . . .   292  10.3 Boosting   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   298  10.3.1   Problem Setup   . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   298  10.3.2   Incremental Learning   . . . . . . . . . . . . . . . . . . . . . . . . .   301  10.3.3   Matching Pursuit   . . . . . . . . . . . . . . . . . . . . . . . . . . . .   302  10.3.4   Adaboost   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   303  10.3.5   Greedy Algorithm Based on Gradient Boosting   . . . . . . . . . . .   304  10.3.6   Convergence of Expected Risk   . . . . . . . . . . . . . . . . . . . .   308  10.3.7   Experiments   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   310  10.4 Conclusion   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   311\n\nviii   CONTENTS  11 From Online Learning to Bandits   313  11.1 First-Order Online Convex Optimization   . . . . . . . . . . . . . . . . . . .   315  11.1.1   Convex Case   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   316  11.1.2   Strongly Convex Case ( \u0007 )   . . . . . . . . . . . . . . . . . . . . . .   318  11.1.3   Online Mirror Descent ( \u0007 )   . . . . . . . . . . . . . . . . . . . . . . .   319  11.1.4   Lower Bounds ( \u0007\u0007 )   . . . . . . . . . . . . . . . . . . . . . . . . . .   321  11.2 Zeroth-Order Convex Optimization   . . . . . . . . . . . . . . . . . . . . . .   323  11.2.1   Smooth Stochastic Gradient Descent   . . . . . . . . . . . . . . . . .   325  11.2.2   Stochastic Smoothing ( \u0007 )   . . . . . . . . . . . . . . . . . . . . . . .   328  11.2.3   Extensions   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   331  11.3 Multiarmed Bandits   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   331  11.3.1   Need for an Exploration-Exploitation Trade-off   . . . . . . . . . . .   333  11.3.2   “Explore-Then-Commit”   . . . . . . . . . . . . . . . . . . . . . . . .   333  11.3.3   Optimism in the Face of Uncertainty ( \u0007 )   . . . . . . . . . . . . . .   336  11.3.4   Adversarial Bandits ( \u0007 )   . . . . . . . . . . . . . . . . . . . . . . . .   339  11.4 Conclusion   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   341  12 Overparameterized Models   343  12.1 Implicit Bias of Gradient Descent   . . . . . . . . . . . . . . . . . . . . . . .   344  12.1.1   Least-Squares Regression   . . . . . . . . . . . . . . . . . . . . . . .   344  12.1.2   Separable Classification   . . . . . . . . . . . . . . . . . . . . . . . .   346  12.1.3   Beyond Convex Problems ( \u0007 )   . . . . . . . . . . . . . . . . . . . . .   351  12.1.4   Remarks on Implicit Bias   . . . . . . . . . . . . . . . . . . . . . . .   354  12.2 Double Descent   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   355  12.2.1   The Double Descent Phenomenon   . . . . . . . . . . . . . . . . . .   355  12.2.2   Empirical Evidence   . . . . . . . . . . . . . . . . . . . . . . . . . . .   356  12.2.3   Linear Regression with Gaussian Inputs   . . . . . . . . . . . . . . .   358  12.2.4   Linear Regression with Gaussian Projections ( \u0007\u0007 )   . . . . . . . . .   360  12.3 Global Convergence of Gradient Descent   . . . . . . . . . . . . . . . . . . .   365  12.3.1   Mean Field Limits   . . . . . . . . . . . . . . . . . . . . . . . . . . .   365  12.3.2   From Linear Networks to Positive-Definite Matrices   . . . . . . . .   370  12.3.3   Global Convergence for Positive-Definite Matrices   . . . . . . . . .   370  12.3.4   Special Cases   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   374  12.4 Lazy Regime and Neural Tangent Kernels ( \u0007 )   . . . . . . . . . . . . . . . .   375  12.5 Conclusion   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   377  13 Structured Prediction   379  13.1 Multicategory Classification   . . . . . . . . . . . . . . . . . . . . . . . . . .   380  13.1.1   Extension of Classical Convex Surrogates   . . . . . . . . . . . . . .   380  13.1.2   Generalization Bound I: Stochastic Gradient Descent   . . . . . . . .   383  13.1.3   Generalization Bound II: Rademacher Complexities ( \u0007 )   . . . . . .   384  13.2 General Setup and Examples   . . . . . . . . . . . . . . . . . . . . . . . . .   387  13.2.1   Examples   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   387  13.2.2   Structure Encoding Loss Functions   . . . . . . . . . . . . . . . . . .   390\n\nCONTENTS   ix  13.3 Surrogate Methods   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   391  13.3.1   Score Functions and Decoding Step   . . . . . . . . . . . . . . . . . .   392  13.3.2   Fisher Consistency and Calibration Functions   . . . . . . . . . . . .   392  13.3.3   Main Surrogate Frameworks   . . . . . . . . . . . . . . . . . . . . . .   393  13.4 Smooth/Quadratic Surrogates   . . . . . . . . . . . . . . . . . . . . . . . . .   393  13.4.1   Quadratic Surrogate   . . . . . . . . . . . . . . . . . . . . . . . . . .   393  13.4.2   Theoretical Guarantees   . . . . . . . . . . . . . . . . . . . . . . . .   394  13.4.3   Linear Estimators and Decoding Steps   . . . . . . . . . . . . . . . .   395  13.4.4   Smooth Surrogates ( \u0007 )   . . . . . . . . . . . . . . . . . . . . . . . . .   396  13.5 Max-Margin Formulations   . . . . . . . . . . . . . . . . . . . . . . . . . . .   398  13.5.1   Structured Support Vector Machines   . . . . . . . . . . . . . . . . .   399  13.5.2   Max-Min Formulations ( \u0007\u0007 )   . . . . . . . . . . . . . . . . . . . . .   399  13.6 Generalization Bounds ( \u0007 )   . . . . . . . . . . . . . . . . . . . . . . . . . . .   402  13.7 Experiments   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   404  13.7.1   Robust Regression   . . . . . . . . . . . . . . . . . . . . . . . . . . .   404  13.7.2   Ranking   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   404  13.8 Conclusion   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   407  14 Probabilistic Methods   409  14.1 From Empirical Risks to Log-Likelihoods   . . . . . . . . . . . . . . . . . .   409  14.1.1   Conditional Likelihoods   . . . . . . . . . . . . . . . . . . . . . . . .   411  14.1.2   Classical Priors   . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   411  14.1.3   Sparse Priors   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   412  14.1.4   On the Relationship between MAP and MMSE ( \u0007 )   . . . . . . . . .   413  14.2 Discriminative versus Generative Models   . . . . . . . . . . . . . . . . . . .   417  14.2.1   Linear Discriminant Analysis and Softmax Regression   . . . . . . .   417  14.2.2   Naive Bayes   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   418  14.2.3   Maximum Likelihood Estimations   . . . . . . . . . . . . . . . . . .   419  14.3 Bayesian Inference   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   420  14.3.1   Computational Handling of Posterior Distributions   . . . . . . . . .   421  14.3.2   Model Selection through Marginal Likelihood   . . . . . . . . . . . .   422  14.4 PAC-Bayesian Analysis   . . . . . . . . . . . . . . . . . . . . . . . . . . . .   423  14.4.1   Setup   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   423  14.4.2   Uniformly Bounded Loss Functions   . . . . . . . . . . . . . . . . . .   424  14.5 Conclusion   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   426  15 Lower Bounds   427  15.1 Statistical Lower Bounds   . . . . . . . . . . . . . . . . . . . . . . . . . . . .   428  15.1.1   Minimax Lower Bounds   . . . . . . . . . . . . . . . . . . . . . . . .   428  15.1.2   Reduction to a Hypothesis Test   . . . . . . . . . . . . . . . . . . . .   429  15.1.3   Review of Information Theory   . . . . . . . . . . . . . . . . . . . .   431  15.1.4   Lower Bound on Hypothesis Testing Based on Information Theory   434  15.1.5   Examples   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   436  15.1.6   Minimax Lower Bounds through Bayesian Analysis   . . . . . . . . .   438\n\nx   CONTENTS  15.2 Optimization Lower Bounds   . . . . . . . . . . . . . . . . . . . . . . . . . .   441  15.2.1   Convex Optimization   . . . . . . . . . . . . . . . . . . . . . . . . .   441  15.2.2   Nonconvex Optimization ( \u0007 )   . . . . . . . . . . . . . . . . . . . . .   443  15.3 Lower Bounds for Stochastic Gradient Descent ( \u0007 )   . . . . . . . . . . . . .   447  15.4 Conclusion   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   449  Conclusion   451  References   453",
    "summary": {
      "en": "The text is a detailed outline of a book titled \"Learning Theory from First Principles\" by Francis Bach, set to be published by The MIT Press in 2024. The contents include various topics related to learning theory and machine learning, organized into sections that cover mathematical concepts, supervised learning, statistical learning theory, optimization techniques, and advanced methods like neural networks and kernel methods.\n\nKey sections of the book include:\n1. **Mathematical Preliminaries**: Basics of linear algebra, calculus, and concentration inequalities.\n2. **Supervised Learning**: From training data to predictions, including decision theory, learning from data, and statistical learning theory.\n3. **Linear Least-Squares Regression**: Techniques for regression analysis and statistical properties.\n4. **Generalization Bounds**: Discusses empirical risk minimization and model selection.\n5. **Optimization for Machine Learning**: Covers gradient descent and stochastic methods.\n6. **Local Averaging Methods**: Examines various averaging techniques and their consistency.\n7. **Kernel Methods**: Introduces the concept of kernels and their applications in machine learning.\n8. **Sparse Methods**: Discusses variable selection techniques.\n9. **Neural Networks**: Explores single hidden-layer networks and their properties.\n10. **Ensemble Learning**: Covers techniques like bagging and boosting.\n11. **Probabilistic Methods**: Discusses Bayesian inference and related concepts.\n\nThe book aims to provide a comprehensive understanding of learning theory, blending theory with practical applications and advanced methodologies.",
      "ko": "프란시스 바흐의 \"Learning Theory from First Principles\"라는 제목의 책이 2024년 MIT 프레스에서 출간될 예정이다. 이 책은 학습 이론과 기계 학습에 관련된 다양한 주제를 다루며, 수학적 개념, 지도 학습, 통계적 학습 이론, 최적화 기법, 신경망 및 커널 방법과 같은 고급 방법론으로 구성된 여러 섹션으로 나뉘어 있다.\n\n책의 주요 섹션은 다음과 같다. 첫 번째는 수학적 기초로, 선형 대수, 미적분학, 집중 불평등의 기본 개념을 설명한다. 두 번째 섹션은 지도 학습으로, 훈련 데이터를 바탕으로 예측을 수행하는 과정과 의사 결정 이론, 데이터로부터 학습하는 방법, 통계적 학습 이론을 포함한다. 세 번째는 선형 최소 제곱 회귀로, 회귀 분석 기법과 통계적 성질을 다룬다. 네 번째는 일반화 경계에 대한 내용으로, 경험적 위험 최소화와 모델 선택에 대해 논의한다.\n\n다섯 번째 섹션은 기계 학습을 위한 최적화 기법으로, 경량 하강법과 확률적 방법을 포함한다. 여섯 번째는 지역 평균 방법으로, 다양한 평균 기법과 그 일관성을 살펴본다. 일곱 번째는 커널 방법으로, 커널의 개념과 기계 학습에서의 응용을 소개한다. 여덟 번째는 희소 방법으로, 변수 선택 기법에 대해 논의한다. 아홉 번째는 신경망으로, 단일 은닉층 네트워크와 그 특성을 탐구한다. 열 번째는 앙상블 학습으로, 배깅과 부스팅과 같은 기법을 다룬다. 마지막으로, 확률적 방법에서는 베이지안 추론과 관련 개념을 설명한다.\n\n이 책은 학습 이론에 대한 포괄적인 이해를 제공하며, 이론과 실제 응용, 고급 방법론을 결합하는 것을 목표로 하고 있다.",
      "ja": null
    }
  },
  {
    "id": "e9ae83fde3e64616",
    "title": {
      "en": "Banned Books: Analysis of Censorship on Amazon.com (2024)",
      "ko": "금서의 진실: 아마존 검열 분석",
      "ja": null
    },
    "type": "story",
    "url": "https://citizenlab.ca/2024/11/analysis-of-censorship-on-amazon-com/",
    "score": 98,
    "by": "gnabgib",
    "time": 1743104270,
    "content": "ResearchFree Expression Online\n\n\tBanned Books\n\n\t Analysis of Censorship on Amazon.com\n\n            By\n            Jeffrey Knockel, Jakub Dalek, Noura Aljizawi, Mohamed Ahmed, Levi Meletti, and Justin Lau\n\n          November 25, 2024\n\n              الترجمة العربية (Arabic translation)\n\n                   Download this report\n\n                  Key findings\n\nWe analyze the system Amazon deploys on the US “amazon.com” storefront to restrict shipments of certain products to specific regions. We found 17,050 products that Amazon restricted from being shipped to at least one world region.\nWhile many of the shipping restrictions are related to regulations involving WiFi, car seats, and other heavily regulated product categories, the most common product category restricted by Amazon in our study was books.\nBanned books were largely related to LGBTIQ, the occult, erotica, Christianity, and health and wellness. The regions affected by this censorship were the UAE, Saudi Arabia, and many other Middle Eastern countries as well as Brunei Darussalam, Papua New Guinea, Seychelles, and Zambia. In our test sample, Amazon censored over 1.1% of the books sold on amazon.com in at least one of these regions.\nWe identified three major censorship blocklists which Amazon assigns to different regions. In numerous cases, the resulting censorship is either overly broad or miscategorized. Examples include the restriction of books relating to breast cancer, recipe books invoking “food porn” euphemisms, Nietzsche’s Gay Science, and “rainbow” Mentos candy.\nTo justify why restricted products cannot be shipped, Amazon uses varying error messages such as by conveying that an item is temporarily out of stock. In misleading its customers and censoring books, Amazon is violating its public commitments to both LGBTIQ and more broadly human rights.\nWe conclude our report by providing Amazon multiple recommendations to address concerns raised by our work.\n\nIntroduction\nThe rise in online shopping has led to more global reach into markets that may otherwise be inaccessible for companies through traditional retail channels. This increased reach brings new opportunities but also has its own challenges for global e-commerce retailers. One such challenge is in dealing with different, more restrictive regulatory environments worldwide.\nIn this report, we analyze American e-commerce retailer Amazon and its system for preventing shipments of certain products to certain world regions as it is implemented on the US storefront — amazon.com. Specifically, we analyze functionality that Amazon implements to restrict shipments of certain products to certain regions even if the product is available and sellers are offering to ship it there. While Amazon normally hides this restriction system from customers using misleading error messages, we employ a novel methodology to uncover and measure on which products and in which regions it is activated by peeling back the layers of Amazon’s website and analyzing its internal workings. Notably, our method can distinguish between a product being restricted by Amazon and it being organically unavailable in a region.\nIn total, we found 17,050 products that were restricted from being shipped to at least one world region. While many of the shipping restrictions observed in our study are related to regulations involving WiFi, car seats, and other heavily regulated product categories, the most common product category restricted by Amazon was books. Banned books were largely related to LGBTIQ, the occult, erotica, Christianity, and health and wellness. More broadly, books were the victims of censorship, which in this report we define as Amazon’s restriction of product shipment under political or religious motivation. The regions commonly affected by this censorship were the United Arab Emirates (UAE), Saudi Arabia, and many other Middle Eastern countries as well as Brunei Darussalam, Papua New Guinea, Seychelles, and Zambia.\nGiven that the topics censored include LGBTIQ, our findings call into question Amazon’s public commitment to LGBTIQ rights as well as its respect for the rights of its users at large. By censoring the availability of books, Amazon is depriving its users of valuable information. Furthermore, by communicating to customers that censored products are organically unavailable (e.g., being out of stock), Amazon is depriving customers of the ability to make informed decisions. We conclude our report by making multiple recommendations to Amazon.\nBackground\nIn this section we briefly describe Amazon’s history as it relates to our analysis. We then outline some of the regulations applying to Amazon’s business in Saudi Arabia, the UAE, and China, which are some of the more restrictive regulatory environments to which products on amazon.com can be shipped.\nAmazon background\nAmazon is an American multinational company that originated as an online bookseller and has since evolved into a global e-commerce marketplace. Amazon’s business is heavily focused on managing shipping logistics internationally and serving a global consumer base. Alongside the main e-commerce platform, they also provide cloud computing services (Amazon Web Services), consumer electronics (Amazon Kindle and Amazon Echo), and online streaming (Amazon Prime Video) among other offerings.\nAmazon is best known for its original website — amazon.com — which serves as the landing page for US customers, although items can be shipped globally depending on seller preferences. As of 2024, there are dedicated storefronts for 22 other regions. Alongside the online expansions to other regions there has been an analogous expansion of physical infrastructure in those regions including shipping hubs, fulfillment centers, sorting facilities, and delivery stations.\nMost relevant to our study, Amazon has expanded its dedicated storefronts to include the UAE in 2017 and Saudi Arabia in 2020. This expansion included opening a regional headquarters in Riyadh, Saudi Arabia, in 2022 and a fulfillment center in Dubai, the UAE, in 2023. These recent expansions into the Middle East create their own unique challenges to the retailer because of the region’s distinct regulatory regimes, which we detail below.\nCompliance with international regulations\nAmazon polices the products sold on its platform, and their own shipping restrictions FAQ provides some guidance on why certain products may be restricted, including the need to “comply with all laws and regulations and with Amazon policies” and that Amazon may be “restricted from shipping to your location due to government import/export requirements, manufacturer restrictions, or warranty issues”. Amazon has adapted its policies to allow for the removal of offensive content including content that Amazon determines is “hate speech, promotes the abuse or sexual exploitation of children, contains pornography, glorifies rape or pedophilia, advocates terrorism”, but also “other material [they] deem inappropriate or offensive”. However, Amazon has failed to reveal specifically what categories of content it restricts to comply with the demands of authoritarian governments.\nThere have been reported incidences where Amazon complied with governments’ requests to restrict certain products or even go as far as manipulate its reviews. For example, Amazon restricted items for purchase and in search results relating to over 150 keywords relating to LGBTIQ content in the UAE after receiving pressure from the government to remove them. In China, Amazon removed all customer ratings and reviews for a book of Chinese president Xi Jinping’s speeches and writings. In both instances, Amazon claimed that they were following local laws and regulations. However, in India, internal Amazon documents showed that Amazon was circumventing local regulations by providing preferential treatment to certain sellers and by also promoting its own merchandise by rigging search results. Amazon has also been criticized for allowing its platform to spread white supremacy and racism. Items with Nazi symbols and Kindle books associated with neo-Nazis and white supremacists have remained widely available despite Amazon having been notified by journalists and non-profit organizations.\nRegulations in Saudi Arabia\nIn Saudi Arabia, content is largely governed by two laws: the 2003 Law of Printing and Publication, largely regulating print media, and the 2007 Anti-Cyber Crimes Law, regulating online media. Article 9 of the Law of Printing and Publications states that printed media cannot contravene Sharia Law, stir up internal discord, injure the economic and health situation of the country, or lead to a breach of either public security, public policy, or foreign interests. Article 18 states these regulations should apply to the importation and distribution of printed materials. An approval is required, within the framework of Article 18 of the Printing and Publication law, in order to certify that content is free from any content that is insulting to Islam, the government, interests of the Emirates, or ethical standard and public morality. In terms of enforcement, Article 39 states that any contravening printed items can be withdrawn from circulation if they are found to violate either Articles 9 or 18.\nThe 2007 Anti-Cyber Crimes Law is chiefly focused on regulations around information security and content regulation. Article 6 of this law states that “production, preparation, transmission, or storage of material impinging on public order, religious values, public morals, or privacy, through an information network or computer” is a criminal offense. Contravening this article can lead to a maximum punishment of five years in prison and a maximum fine of three million riyals (approximately 800,000 USD). This law has been applied against online content. For example, in 2019, Saudi Arabia alerted Netflix that an episode of Hasan Minhaj’s comedy show Patriot Act violated this statute as it contained criticism of a Saudi Arabian royal. Netflix complied with the government order and restricted access to the episode for Saudi Arabian users.\nRegulations in the UAE\nIn the UAE, content is governed by the Federal Decree-Law No.55 of 2023 on Media Regulation, which replaced the previous Federal Law No.15 of 1980 Concerning Publications and Publishing. Specifically, it regulates print, television, as well as online media. Another relevant regulation is the Internet Access Management Regulatory Policy which focuses on the regulation of online content. Under this policy, the only two internet service providers (ISPs) in the UAE, Etisalat and Du, are required to block online content if requested by the Telecommunications and Digital Government Regulatory Authority. Prohibited Internet content includes pornography, contempt of religion, and promotion of or trading in prohibited commodities and services. Category 13 of the policy prohibits sites from promoting or trading in commodities prohibited or restricted by licenses in the UAE, including “prints, paintings, photographs, drawings, cards, books, magazines, and stone sculptures, which are contrary to the Islamic religion or public morals, or involving intent of corruption or sedition”.\nCompliance with Chinese demands\nIn 2004, Amazon entered the Chinese market via its acquisition of Joyo, a Chinese online bookstore. Amazon faced scrutiny for its political censorship of products on its Chinese site — amazon.cn. However, facing competition from domestic rivals, Amazon terminated its online store in China in 2019, although for a limited time overseas products were still sold on the amazon.cn site. Amazon still has other operations in China, such as Amazon Web Services (AWS), which is Amazon’s cloud computing service. Outside of China, in 2021, on the US Amazon storefront — amazon.com — Amazon partnered with China International Book Trading Corp, a state-owned firm that has been labeled as “China’s state propaganda arm”, to create a portal for selling books that amplify the Chinese Communist Party’s agenda.\nMethodology\nIn this section, we explain how we determine product availability across different regions. Our methodology consists of two phases. As our original motivation was to understand how Amazon censorship applies to Middle Eastern countries, in our first phase, we focus on studying how products and shipment restrictions vary across multiple countries in the Middle East. We were particularly motivated to understand the differences between restrictions imposed on the shipment of products to Middle Eastern countries in which Amazon operates a storefront (namely, the UAE and Saudi Arabia) versus those in which it does not. To understand how censorship applies more broadly to the world at large, in our second phase we pivot from the results of the first phase and measure product availability in regions across the globe.\nIn designing our methodology, we were motivated by eliminating false positives, even if doing so might introduce false negatives. The rationale is that we would rather omit the measurement of some instances of censorship rather than falsely attribute censorship to products that are not censored.\nIn the remainder of this section we explain the two phases of our methodology.\nPhase 1: Measuring censorship in Middle East\nOne way to try to measure Amazon censorship in Middle Eastern countries would be to visit those Amazon storefronts which are available in the Middle East, namely, the UAE’s amazon.ae or Saudi Arabia’s amazon.sa, and to try to determine which products are anomalously “missing” from being sold on these two Amazon sites. This approach, however, would be limited. For example, if we saw one book related to LGBTIQ topics that was sold on amazon.com but not amazon.ae, that might be due to the book being censored on amazon.ae, but another possibility is that the book was out of stock or not sufficiently popular to be sold in some countries. However, if we saw a disproportionately large number of books related to LGBTIQ topics that were available on amazon.com but not sold on amazon.ae, then we would have a stronger argument, but this argument would be at best a statistical argument, and for any individual product we would not be able to prove whether it was",
    "summary": {
      "en": "**Summary of \"Analysis of Censorship on Amazon.com\"**\n\nThis report examines how Amazon restricts shipments of certain products, particularly books, to various global regions, particularly in the Middle East. Key findings include:\n\n- **Product Restrictions**: Amazon restricted over 17,000 products, with books being the most commonly affected category. This includes titles related to LGBTIQ issues, the occult, erotica, Christianity, and health topics.\n\n- **Affected Regions**: The main regions impacted by these restrictions are the UAE, Saudi Arabia, and several other Middle Eastern countries, as well as Brunei, Papua New Guinea, Seychelles, and Zambia. In total, more than 1.1% of books sold on Amazon.com were censored in at least one of these areas.\n\n- **Censorship Practices**: Amazon uses misleading messages, such as saying items are \"out of stock,\" to obscure the censorship. This practice raises concerns about the company's commitment to human rights, particularly LGBTIQ rights.\n\n- **Censorship Justifications**: Amazon cites compliance with local laws and regulations as reasons for these restrictions, but the criteria for what is deemed offensive or inappropriate are not transparent.\n\n- **Recommendations**: The report concludes with suggestions for Amazon to improve its practices regarding censorship and transparency.\n\nOverall, the study highlights the conflicts between Amazon's global operations and local regulatory environments, particularly regarding sensitive content.",
      "ko": "이 보고서는 아마존이 특정 제품, 특히 책의 배송을 어떻게 제한하는지를 분석하고 있으며, 특히 중동 지역에 초점을 맞추고 있습니다. 주요 내용은 다음과 같습니다.\n\n아마존은 17,000개 이상의 제품을 제한했으며, 그 중 책이 가장 많이 영향을 받는 카테고리입니다. 제한된 제품에는 LGBTIQ 문제, 오컬트, 에로티카, 기독교 및 건강 관련 주제의 제목들이 포함됩니다.\n\n이러한 제한의 주요 대상 지역은 아랍에미리트, 사우디아라비아 및 여러 중동 국가들, 그리고 브루나이, 파푸아뉴기니, 세이셸, 잠비아 등입니다. 전체적으로 아마존.com에서 판매되는 책의 1.1% 이상이 이러한 지역 중 최소 한 곳에서 검열되었습니다.\n\n아마존은 검열을 숨기기 위해 \"재고 없음\"과 같은 오해의 소지가 있는 메시지를 사용하고 있습니다. 이러한 관행은 LGBTIQ 권리와 같은 인권에 대한 회사의 헌신에 대한 우려를 불러일으킵니다.\n\n아마존은 이러한 제한의 이유로 현지 법률 및 규정을 준수한다고 주장하지만, 무엇이 불쾌하거나 부적절한지에 대한 기준은 명확하지 않습니다.\n\n보고서는 아마존이 검열 및 투명성 관련 관행을 개선할 수 있는 제안으로 마무리됩니다. 전반적으로 이 연구는 아마존의 글로벌 운영과 지역 규제 환경 간의 갈등, 특히 민감한 콘텐츠와 관련된 문제를 강조하고 있습니다.",
      "ja": null
    }
  },
  {
    "id": "ce64f63b9d59fbed",
    "title": {
      "en": "Plastic-like materials that dissolve in the sea",
      "ko": "바다에서 녹는 플라스틱",
      "ja": null
    },
    "type": "story",
    "url": "https://www.riken.jp/en/news_pubs/research_news/rr/20250327_1/index.html",
    "score": 3,
    "by": "geox",
    "time": 1743189029,
    "content": "<div class=\"lytBox01\">\n<p>Please enable JavaScript in your browser.</p>\n<!-- /.lytBox01 --></div>\n\n\t\t\t(function(d, s, id) {\n\t\t\t\tvar js, fjs = d.getElementsByTagName(s)[0];\n\t\t\t\tif (d.getElementById(id))\n\t\t\t\t\treturn;\n\t\t\t\tjs = d.createElement(s);\n\t\t\t\tjs.id = id;\n\t\t\t\tjs.src = 'https://connect.facebook.net/en_US/sdk.js#xfbml=1&version=v3.2';\n\t\t\t\tfjs.parentNode.insertBefore(js, fjs);\n\t\t\t}(document, 'script', 'facebook-jssdk'));\n\n\tTweet\n\n\t\t\t\t\t\t\t\t Mar. 27, 2025\n\t\t\t\t\t\t\t\tFeature Highlight\n\n\t\tChemistry\n\n\t\t\t\t\t\t\tPlastic-like materials that dissolve in the sea\n\n\t\t\t\t\t\t\t\tSupramolecular materials that fully degrade when soaked in saltwater have the potential to help address the microplastics pollution crisis.\n\nArtistic rendering of the new plastic. Cross linked salt bridges visible in the plastic outside the seawater give it its structure and strength. In seawater (and in soil, not depicted), resalting destroys the bridges, making it water soluble, thus preventing microplastic formation and allowing the plastic to become biodegradable. © 2025 RIKEN\n\nMicroplastics—small fragments of plastics less than 5mm across—now infiltrate every corner of our planet, from remote regions of the deep ocean and the Arctic, to the very air we breathe.\n Increasingly microplastics are also found in our bodies, including in our blood and brains. While the impact on the environment and human health is still not fully understood, these contaminants are known to cause a range of problems in marine and terrestrial ecosystems, including slowing the growth of animals, which impacts fertility and causes organ dysfunction.\nSeawater solution\nRIKEN scientists are aiming to tackle the problem of microplastics in the ocean with a new material that biodegrades in saltwater.\nSimilar in weight and strength to conventional plastics, the new material could chart a new path to reducing plastics pollution, as well as reduce greenhouse gas emissions associated with burning plastics, says Takuzo Aida, a materials scientist who heads the Emergent Soft Matter Function Research Group at the RIKEN Center for Emergent Matter Science in Wako, Japan.\nThis new plastic is a culmination of his three decades of pioneering work as an expert in materials called supramolecular polymers. Plastics are a type of polymer, which are comprised of small molecules bound into long chains by strong covalent bonds that require extensive energy to break.\n In contrast, supramolecular polymers have weaker, reversible bonds “like sticky notes that you can attach and peel off,” explains Aida.\nThis gives supramolecular polymers unique properties, such as the ability to 'self-heal' when broken and then pressed back together. They are also easy to recycle, by using specific solvents to break down the materials’ bonds at the molecular level, meaning that supramolecular polymers can be easily reused and repurposed.\nUnlocking bonds\nPlastic products are everywhere for a reason, says Aida. “Plastics, especially polyethylene terephthalate, which is used in bottles, are incredibly versatile. They are flexible but strong, durable and recyclable. It’s hard to beat that convenience.”\nBiodegradable plastics have been touted as an alternative, but Aida says the speed and conditions at which they degrade have been a major challenge. For instance, he says, significant amounts of polylactic acid (PLA), a plastic that biodegrades in soil, have been found intact in the ocean because it takes too long to break down under standard environmental conditions. As a result, it eventually ends up intact in the ocean. Since plastics such as PLA are not water-soluble, they slowly break up over time into microplastics that cannot be broken down by bacteria, fungi and enzymes.\nDriven by a sense of urgency for the planet’s future, Aida began seeking ways for supramolecular materials to overcome these challenges. “But the reversible nature of the supramolecular polymer bonds are also their weakness, since the materials disintegrate too easily,” he says. “This had limited their applications.”\n His team set out to discover a combination of compounds that would create a supramolecular material with good mechanical strength, but that can break down quickly under the right conditions into non-toxic compounds and elements. Aida had a specific reaction in mind, one that would lock the material’s molecular bonds and could only be reversed with a specific ‘key’—salt.\n After screening various molecules, the team found that a combination of sodium hexametaphosphate (a common food additive) and guanidinium ion-based monomers (used for fertilizers and soil conditioners) formed ‘salt bridges’ that bind the compounds together with strong cross-linked bonds. These types of bonds serve as the ‘lock’, providing the material with strength and flexibility, explains Aida.\n  “Screening molecules can be like looking for a needle in a haystack,” he says. “But we found the combination early on, which made us think, ‘This could actually work’.”\nIn their study, Aida’s team produced a small sheet of this supramolecular material by mixing the compounds in water. The solution separated into two layers, the bottom viscous and the top watery, a spontaneous reaction that surprised the team. The viscous bottom layer contained the compounds bound with salt bridges. This layer was extracted and dried to create a plastic-like sheet.\nThe sheet was not only as strong as conventional plastics, but also non-flammable, colorless and transparent, giving it great versatility. Importantly, the sheets degraded back into raw materials when soaked in salt water, as the electrolytes in the salt water opened the salt bridge ‘locks’. The team’s experiments showed that their sheets disintegrated in salt water after 8 and a half hours.\nThe sheet can also be made waterproof with a hydrophobic coating. Even when waterproofed, the team found that the material can dissolve just as quickly as non-coated sheets if its surface is scratched to allow the salt to penetrate, says Aida.\n\nA thin square of the glassy new plastic  © 2025 RIKEN\n\n Driving change\n\nNot only is the supramolecular material degradable, but Aida hopes what is left after it breaks down could be usefully re-used. When broken down, his team’s new material leaves behind nitrogen and phosphorus, which microbes can metabolize and plants can absorb, he explains.\nHowever, Aida cautions that this also requires careful management: while these elements can enrich soil, they could also overload coastal ecosystems with nutrients, which are associated with algal blooms that disrupt entire ecosystems. The best approach may be to largely recycle the material in a controlled treatment facility using seawater. This way the raw materials could be recovered to produce supramolecular plastics again, he says.\nIn addition to developing alternatives to fossil fuel-derived plastic, Aida argues that governments, industries and researchers must also act decisively to drive change. Without more aggressive measures, the world’s plastics production—and corresponding carbon emissions— could more than double by 2050.\n“With established infrastructures and factory lines, it’s extremely challenging for the plastics industry to change,” says Aida. “But I believe there will come a tipping point where we have to power through change.” And a technology like this will be needed when that time comes.\n\nRate this article\n\n\t\t\t\t\tStars\n\n\t\t\tThank you!\n\n\t\t\t\tSubmit\n\nReference\n\n  1. Cheng, Y., Hirano, E., Wang, H., Kuwayama, M., Meijer, E. W.  et al. Mechanically strong yet metabolizable supramolecular plastics by desalting upon phase separation.   Science 386,   875-881 (2024). doi: 10.1126/science.ado1782\n\nAbout the researcher\nTakuzo Aida\n\nTakuzo Aida is the group director at the RIKEN Center for Emergent Matter Science, located in Wako, Saitama, Japan. Here, he also leads the Emergent Soft Matter Function Research Group. In addition, he is a distinguished professor at the University of Tokyo. Aida has received several notable honors and awards, including the American Chemical Society Award in Polymer Chemistry (2009), the Chemical Society of Japan Award (2009), the Purple Ribbon (2010), the Alexander von Humboldt Research Award (2011), and the Leo Esaki Prize (2015). His achievements were further recognized with membership in the Royal Netherlands Academy of Arts and Sciences in 2020, the US National Academy of Engineering in 2021, and the American Academy of Arts and Sciences in 2023.",
    "summary": {
      "en": "Scientists at RIKEN in Japan have developed a new type of plastic that dissolves in saltwater, potentially helping to reduce the microplastics pollution crisis. Microplastics are tiny plastic particles that harm the environment and human health. The new material, known as supramolecular plastic, is similar in strength and weight to regular plastics but can biodegrade quickly in seawater, preventing microplastic formation.\n\nThis innovative plastic is made using a combination of sodium hexametaphosphate and guanidinium ion-based monomers, which create strong bonds that can break down in saltwater. The plastic can self-heal and be easily recycled. After testing, the team found that it disintegrated in saltwater within 8.5 hours.\n\nWhile the biodegradable material leaves behind nutrients that can enrich soil, careful management is needed to avoid overloading ecosystems. Researchers emphasize the need for a shift in the plastics industry to address plastic pollution effectively before production and carbon emissions increase significantly by 2050.",
      "ko": "일본의 RIKEN 연구소 과학자들이 소금물에 녹는 새로운 유형의 플라스틱을 개발했습니다. 이 플라스틱은 미세플라스틱 오염 문제를 줄이는 데 도움을 줄 수 있습니다. 미세플라스틱은 환경과 인체 건강에 해로운 작은 플라스틱 입자입니다. 새로운 물질인 초분자 플라스틱은 일반 플라스틱과 비슷한 강도와 무게를 가지면서도 바닷물에서 빠르게 생분해되어 미세플라스틱 형성을 방지할 수 있습니다.\n\n이 혁신적인 플라스틱은 헥사메타인산나트륨과 구아니디늄 이온 기반의 단량체를 조합하여 만들어졌습니다. 이 조합은 소금물에서 분해될 수 있는 강한 결합을 형성합니다. 이 플라스틱은 스스로 치유할 수 있으며 쉽게 재활용할 수 있습니다. 실험 결과, 이 플라스틱은 소금물에서 8.5시간 이내에 분해되는 것으로 나타났습니다.\n\n생분해 가능한 이 물질은 토양을 비옥하게 할 수 있는 영양분을 남기지만, 생태계에 과부하를 주지 않도록 신중한 관리가 필요합니다. 연구자들은 2050년까지 플라스틱 생산과 탄소 배출이 크게 증가하기 전에 플라스틱 산업에서 효과적으로 플라스틱 오염 문제를 해결하기 위한 변화가 필요하다고 강조하고 있습니다.",
      "ja": null
    }
  },
  {
    "id": "0bba5f1428db36d1",
    "title": {
      "en": "AI models miss disease in Black and female patients",
      "ko": "AI, 질병 놓치다!",
      "ja": null
    },
    "type": "story",
    "url": "https://www.science.org/content/article/ai-models-miss-disease-black-female-patients",
    "score": 316,
    "by": "pseudolus",
    "time": 1743100701,
    "content": "From programs designed to detect irregular heartbeats in electrocardiograms to software that tracks eye movements to diagnose autism in children, artificial intelligence (AI) is helping physicians fine-tune the care they provide patients. But for all the technology’s potential for automating tasks, a growing body of evidence also shows that AI can be prone to bias that disadvantages already vulnerable patients. A new study, published today in Science Advances, adds to this work by testing one of the most cited AI models used to scan chest x-rays for diseases—and finding the model doesn’t accurately detect potentially life-threatening diseases in marginalized groups, including women and Black people.\nThese results “are interesting and timely,” says Kimberly Badal, a computational biologist at the University of California (UC), San Francisco, who was not involved in the new study. “We are at the point in history where we’re moving to deploy a lot of AI models into clinical care,” she says, but “we don’t really know” how they affect different groups of people.\nThe model used in the new study, called CheXzero, was developed in 2022 by a team at Stanford University using a data set of almost 400,000 chest x-rays of people from Boston with conditions such as pulmonary edema, an accumulation of fluids in the lungs. Researchers fed their model the x-ray images without any of the associated radiologist reports, which contained information about diagnoses. And yet, CheXzero was just as good as the radiologists in reading the diseases associated with each x-ray.\n\n  SIGN UP FOR THE AWARD-WINNING SCIENCEADVISER NEWSLETTER\n  The latest news, commentary, and research, free to your inbox daily\n    Sign up\n\nGiven AI models’ tendencies for bias, Yuzhe Yang, a computer scientist at UC Los Angeles wanted to assess the Stanford team’s model for such biases. His team selected a subset of 666 x-ray images from the same data set that was used to train the model: the data set’s only images that also came with radiologists’ diagnoses and information about each patient’s age, sex, and race. The team then fed these images to CheXzero and compared the results against the radiologists’ diagnoses.\nCompared with the patients’ doctors, the AI model more often failed to detect the presence of disease in Black patients or women, as well in those 40 years or younger. When the researchers looked at race and sex combined, Black women fell to the bottom, with the AI not detecting disease in half of them for conditions such as cardiomegaly, or enlargement of the heart. These disparities persisted when the team tested CheXzero using four other public data sets of chest x-rays from other regions, including Spain and Vietnam.\n“I’m not surprised by that finding at all,” Badal says. Other studies have shown biases among subgroups, and the new study confirms this, she says. “There is so much variation in populations and their biology that I find it very hard to believe that eventually, one day, we will have ‘one model to rule them all.’”Advertisement\nYang and his team wanted to then tease out the possible root of the bias. Previous research had shown that AI models can be trained to detect race from x-rays with high accuracy, even when clinical experts can’t, so the team set out to test CheXzero’s ability to do the same by asking the model to predict the sex, age, and race of patients using only the x-ray images. They found CheXzero could detect these characteristics in a high percentage of the patients—almost 80% when it came to race, for instance. By contrast, when three board-certified, experienced radiologists at the University of Washington’s School of Medicine tried to do the same, their highest success rate was just about 50%.\n“There might be some hidden signals within the radiography itself that we cannot identify visually,” Yang says. He thinks the model may be using the information as a diagnostic “shortcut” to associate traits such as age with certain conditions and not others, developing a bias in the process.\nTo force CheXzero to avoid shortcuts and therefore try to mitigate this bias, the team repeated the experiment but deliberately gave the race, sex, or age of patients to the model together with the images. The model’s rate of “missed” diagnoses decreased by half—but only for some conditions.\nYang and his team are unsure what may be causing the mixed success. There may be bias baked into the model itself: The data set used to train CheXzero included more men, more people between 40 and 80 years old, and more white patients, which Yang says underscores the need for larger, more diverse data sets.\n“What is clear is that it’s going to be really difficult to mitigate these biases,” says Judy Gichoya, an interventional radiologist and informatician at Emory University who was not involved in the study. Instead, she advocates for smaller, but more diverse data sets that test these AI models to identify their flaws and correct them on a small scale first. Even so, “Humans have to be in the loop,” she says. “AI can’t be left on its own.”",
    "summary": {
      "en": "Artificial intelligence (AI) is increasingly being used in healthcare to improve patient care, such as detecting heart issues and diagnosing autism. However, a recent study shows that AI models can exhibit bias, particularly against marginalized groups like women and Black individuals. The study tested an AI model called CheXzero, designed to analyze chest x-rays, and found it often failed to accurately detect diseases in these groups, especially in Black women and younger patients.\n\nResearchers discovered that CheXzero could predict patient characteristics like race and sex from x-rays with high accuracy, suggesting it might use these traits as shortcuts, leading to biases in diagnosis. When the model was provided with this demographic information, its detection rates improved, but only for some conditions.\n\nThe study highlights the need for more diverse training data to reduce bias in AI models. Experts argue that while AI can assist in healthcare, human oversight remains crucial to ensure fair and accurate patient care.",
      "ko": "인공지능(AI)은 환자 치료를 개선하기 위해 점점 더 많은 의료 분야에서 사용되고 있습니다. 예를 들어, 심장 문제를 감지하거나 자폐증을 진단하는 데 활용되고 있습니다. 그러나 최근 연구에 따르면 AI 모델이 편향을 보일 수 있으며, 특히 여성이나 흑인과 같은 소외된 집단에 대해 그 경향이 두드러진다고 합니다. 연구팀은 흉부 엑스레이를 분석하기 위해 설계된 AI 모델인 CheXzero를 테스트했으며, 이 모델이 흑인 여성과 젊은 환자들에서 질병을 정확하게 감지하는 데 실패하는 경우가 많다는 것을 발견했습니다.\n\n연구자들은 CheXzero가 엑스레이를 통해 인종과 성별과 같은 환자의 특성을 높은 정확도로 예측할 수 있다는 사실을 밝혀냈습니다. 이는 모델이 이러한 특성을 진단의 지름길로 사용하고 있어 편향이 발생할 수 있음을 시사합니다. 인구 통계 정보를 제공했을 때, 모델의 질병 감지율은 개선되었지만, 이는 일부 질환에만 해당했습니다.\n\n이 연구는 AI 모델의 편향을 줄이기 위해 더 다양한 훈련 데이터가 필요하다는 점을 강조합니다. 전문가들은 AI가 의료 분야에서 도움을 줄 수 있지만, 공정하고 정확한 환자 치료를 보장하기 위해서는 인간의 감독이 여전히 중요하다고 주장합니다.",
      "ja": null
    }
  },
  {
    "id": "d552523a0f5f9a5e",
    "title": {
      "en": "Piranesi's Perspective Trick (2019)",
      "ko": "피라네시의 시각 속임수",
      "ja": null
    },
    "type": "story",
    "url": "https://medium.com/@brunopostle/piranesis-perspective-trick-6bcd7a754da9",
    "score": 339,
    "by": "amatheus",
    "time": 1743075690,
    "content": "Piranesi’s Perspective TrickBruno Postle·Follow8 min read·Apr 27, 2019--1ListenShareThis is a quick report explaining what I have been doing with some research on what I started to call ‘Piranesi’s perspective trick’. In the past I would have written this up as an academic paper, that may happen at some point, but not anytime soon.Giovanni Battista Piranesi was a Eighteenth century artist famous for his ‘vedute’ etchings of classical and contemporary buildings in Rome. He wasn’t the only artist doing this sort of thing, there was quite an industry around souvenirs of the Grand Tour.Veduta del Ponte Molle, PiranesiArtistically the images are excellent, and in many ways these etchings are a very good record of the places, as Piranesi was very much concerned with getting the detail right. However there are a few stand-out things about these pictures, and pictures from other artists of the period, that are a little bit strange to our modern eyes that are used to experiencing places through photography:1. The engravings give a very good impression of the sense of place, we are all used to visiting places that we have previously seen in photographs and found the reality to be very different, whereas Piranesi’s places are very identifiable when encountered.2. Related to this, often the images show a wide panoramic view that is impossible to represent in a single photograph, but without any of the distortions that you see at the edges of a photograph.3. To a modern eye, brought up with television and magazines, there is something about the perspective in these pictures that looks just a little bit off, or even very wrong, but there is no evidence that people at the time thought like this — Piranesi was famous for his technical ability as a draughtsman.A few years ago, we were looking at introducing new panoramic projections for the Hugin panorama stitcher, and one of the features often asked for was to have a projection that was very wide-angle, but which didn’t have the extreme edge distortions of normal photographic projections, or the awkward curvature that you see in fisheye or map projections, basically what we wanted was a way to recreate these ‘vedute’ views in software. What we came up with was the ‘Panini projection’ (sometimes called the ‘vedutismo projection’, probably a better name, but hey, once you create software, you can’t go changing names of things just like that).An image created in Hugin in Panini/Vedutismo projection. The angle of view of this image is nearly 180°, this is an extreme wide angleThe Panini projection is very useful, and you have probably encountered it at some point without even realising it, it has the property that vertical lines are straight, radial lines are straight, and that edge distortion is imperceptible. With the Panini projection it is possible to show a scene with an enormously wide angle of view (over 180°), and for it to appear like a normal photograph, sometimes, at least.Another Hugin panorama in extreme wide-angle Panini projectionBut our Panini projection in Hugin isn’t quite the same as the perspective views used by Piranesi and his contemporaries. Although there is a straightforward way to construct a Hugin/Panini perspective on a drawing board, there is no evidence that any of these historical artists actually used it. Some got very close, in particular Panini himself and Vincent van Gogh, but that is a story for another day.But there is a distinctive feature of Piranesi’s perspective trick, a feature that marks it out from Hugin’s Panini perspective, and from a normal ‘rectilinear’ image.‘Rectilinear’ is the kind of normal perspective you get from a camera, but also from drawing a perspective by projecting a plan on a drawing board, and also it is the default perspective you get using one of the various historical perspective machines such as a camera obscura.The distinctive feature of Piranesi’s perspective trick is that when you have a series of similar objects receding into the distance, such as houses or arches in a bridge, the nearer versions are just drawn as larger versions of those in the distance — real perspective doesn’t work like this, not at all, this is a trick that Piranesi and other artists used to cram more things into the pictures while retaining legibility.Note that all three arches are basically the same, just drawn at different sizes, PiranesiThe way to spot it involves drawing some lines on the picture, use a copy, it is best not to use an original.In a normal perspective, any parallel lines in the scene that are not perpendicular to the viewing direction converge onto points. This sounds complex, but it is not, here is what we call a one-point perspective. The edges of the buildings converge to a single ‘vanishing point’ on the horizon, this is why we call it ‘one point perspective’.Normal single-point perspective. The parallel lines of the buildings converge on a vanishing point at the middle on the horizon. The diagonals of the buildings are parallel and they converge on vanishing points above and below.But other parallel lines in the scene also converge to points, the diagonals of the buildings are also parallel, and so when shown in perspective they converge on points directly above and below the main vanishing point.But look at one of Piranesi’s engravings:Veduta del palazzo Odescalchi, PiranesiSome of the diagonals, towards the middle of the image, do the right thing, they converge top and bottom as expected, but the diagonal lines at the left of the scene are drawn completely parallel. This is outrageous! real, true perspective doesn’t work like this, it isn’t possible to construct a camera or a computer program to render a view that does this. But the engravings work as pictures nonetheless, in fact they are quite good.Piranesi engraving, note that the three arches are drawn exactly the same at different scales, diagonal lines are parallelThis one below is a Canaletto, the same trick is in use, the centre and right hand part of the picture is photographically correct, but the long building on the left has been extended using Piranesi’s perspective trick. I’ve drawn diagonals on each bay of the building and they are obviously parallel. If this was a true perspective these diagonal lines would converge into a vanishing point in the sky somewhere above St Mark’s.The mathematics of this is quite simple, just draw objects in a series such that the closer ones are the same proportion as those further away. You can show that the only way to do this is is for the ratio of the distance from the vanishing point to any two features is the same for the next two features.Piranesi’s perspective trick, the diagonals of features in the elevation stay parallel when remappedHere’s the science bit:…and here are some tests, with Panini’s perspective trick we can’t render the entire scene, just rectangular objects in a scene, rectangular objects like the elevation of a street. This is a street elevation:Main Street USA elevationHere is the same elevation distorted using Piranesi’s perspective trick, looks ok to me, it’s a first test so is a bit blurry.An early test of a single-point Piranesi perspectiveBut here is the same elevation drawn using correct rectilinear perspective. I hope you will agree that the Piranesi version is much more legible, the furthest house above is easier to see and the nearest house isn’t horribly distorted as it is below.Comparison perspective, this is a ‘true’ single-point perspective, note how distorted the left and right buildings are compared to both the original elevation and the Piranesi perspective versionBy legibility I mean that the Piranesi distortion is easier to read, and that to anyone unfamiliar with photographs, that hasn’t grown up with TV, photographs and magazines, the Piranesi version would look much better, and the true perspective would look rather odd.So what can we do with this? We can’t add this trick to Hugin as a new general purpose mapping, because it isn’t a real perspective. However most image editors, like GIMP, have a perspective tool that performs a distortion on a rectangular selection, technically this is called a ‘homography matrix transform’ and it produces a photographically correct rectilinear perspective, with all the usual unpleasant edge distortions.Piranesi’s perspective trick can also be extended into two dimensions like this, sometimes called a ‘two point perspective’. Piranesi never did this, but with computers we can do all sorts of things. So here is a general 2D remapping as a prototype replacement for the perspective tool in image editors (I have no ability to add this to GIMP, somebody else needs to step up):Two-point ‘Piranesi’ perspective distortionI think that the result is much more legible than that produced by a ‘true’ perspective, no matter how correct it is:Normal homography perspective transformation produced in GIMP. Note how squashed the buildings at the right and left are when taken in isolationOnce you know what to look for, i.e parallel diagonal lines, you can see Piranesi’s trick being used by any number of historical artists, and even some modern artists, this isn’t ‘lost’ knowledge.The trick was very common as a way to show a birds-eye aerial view of landscapes, but also keeping them legible enough to be used as a map, like this:We can do this in the computer, here is a birds-eye view of a London map redrawn using Piranesi’s perspective trick.Piranesi bird’s eye perspective, OSM contributorsI hope you will agree that it is quite a bit more legible than the same view shown in ‘correct perspective’:‘Correct’ birds-eye perspective, OSM contributors..and so this article doesn’t finish on a boring picture, here is some lineart remapped using Piranesi’s perspective trick. The thing to note is that the left building has basically the same amount of distortion as the right building, only smaller. Similarly, all the windows have about the same amount of distortion, this is very very unlike a true, ‘correct’ perspective:Palazzo Porto, Palladio. Original image by Joshua Cesa, Alessandro Senno, Elia Venturini, WikipediaBruno Postle, April 2019",
    "summary": {
      "en": "This text discusses \"Piranesi's perspective trick,\" a unique artistic technique used by the 18th-century artist Giovanni Battista Piranesi. Piranesi is known for his detailed etchings of buildings in Rome, which often appear more legible and identifiable than real photographs. Key points include:\n\n1. **Piranesi's Style**: His engravings provide a sense of place and often feature wide panoramic views without the distortions common in photographs.\n\n2. **Perspective Trick**: Piranesi used a method where similar objects appearing in the distance are drawn as larger versions of those closer to the viewer, which is not how true perspective works. This technique allows for more elements in a scene while maintaining clarity.\n\n3. **Comparison to Modern Techniques**: The text explains how modern software, like Hugin, developed a \"Panini projection\" to replicate wide-angle views without distortions. However, this is not the same as Piranesi's method.\n\n4. **Legibility**: Piranesi’s approach makes images easier to read compared to traditional perspectives, which can appear distorted.\n\n5. **Applications**: The technique can be applied in modern image editing, offering a new way to represent scenes more legibly than standard perspective methods.\n\nOverall, Piranesi’s perspective trick remains relevant and can be seen in various historical and modern artworks.",
      "ko": "이 글에서는 18세기 예술가 조반니 바티스타 피라네시의 독특한 예술 기법인 \"피라네시의 원근법 속임수\"에 대해 다룹니다. 피라네시는 로마의 건축물을 세밀하게 판화로 남긴 것으로 유명하며, 그의 작품은 실제 사진보다 더 명확하고 인식하기 쉬운 경우가 많습니다.\n\n피라네시의 스타일은 그의 판화가 장소의 느낌을 잘 전달하며, 사진에서 흔히 발생하는 왜곡 없이 넓은 파노라마 뷰를 특징으로 합니다. 그는 원근법을 활용하여 멀리 있는 유사한 물체를 관객에게 가까운 물체보다 더 크게 그리는 방식을 사용했습니다. 이는 실제 원근법과는 다른 방식으로, 장면에 더 많은 요소를 포함하면서도 명확성을 유지할 수 있게 합니다.\n\n현대 기술과 비교할 때, Hugin과 같은 소프트웨어는 왜곡 없이 넓은 시야를 재현하기 위해 \"파니니 투영\"이라는 방법을 개발했습니다. 그러나 이는 피라네시의 방식과는 다릅니다. 피라네시의 접근 방식은 전통적인 원근법보다 이미지를 더 쉽게 읽을 수 있게 만들어, 왜곡된 느낌을 줄입니다.\n\n이 기법은 현대 이미지 편집에서도 적용될 수 있으며, 표준 원근법 방법보다 장면을 더 명확하게 표현하는 새로운 방식을 제공합니다. 피라네시의 원근법 속임수는 여전히 관련성이 있으며, 다양한 역사적 및 현대 예술 작품에서 찾아볼 수 있습니다.",
      "ja": null
    }
  },
  {
    "id": "dce4b0938311310a",
    "title": {
      "en": "Retreating Glaciers Expose 1,500 Miles of Coastline",
      "ko": "빙하 후퇴, 1,500마일 해안 드러나다",
      "ja": null
    },
    "type": "story",
    "url": "https://e360.yale.edu/digest/climate-glaciers-coastline-study",
    "score": 29,
    "by": "YaleE360",
    "time": 1742828475,
    "content": "E360 Digest\n      March 24, 2025\n\n    Retreating Arctic Glaciers Have Exposed 1,500 Miles of Coastline\n\n          Courtesy of Jan Kavan\n\n  Since 2000, the melting of Arctic glaciers has exposed some 1,500 miles of coastline, a study finds.For the research, published in Nature Climate Change, scientists tracked the movement of 1,500 coastal glaciers from 2000 to 2020, finding that retreating ice had unveiled hundreds of miles of coastline, largely in Greenland.Scientists say that retreating glaciers are revealing stores of precious metals, but they warn that newly exposed coastline, which has not been cemented with ice, is vulnerable to erosion and landslides.In September 2023, a thinning coastal glacier in eastern Greenland gave way, leading to a massive landslide in the Dickson fjord. The landslide produced a 350-foot-high tsunami that registered on seismometers around the globe.ALSO ON YALE E360With Sea Ice Melting, Killer Whales Are Moving Into the Arctic\n\n    Facebook\n\n    Twitter\n    Email\n\n  Related Articles\n\n        Opinion\n\n      With NOAA Cuts, a Proud Legacy and Vital Science Are at Risk\n\n                        By  Adam Sobel\n\n      Imperiled in the Wild, Many Plants May Survive Only in Gardens\n\n                        By  Janet Marinelli\n\n      Can Toxic Mining Waste Help Remove CO2 from the Atmosphere?\n\n                        By  Moira Donovan\n\n  More From E360\n\n  CONFLICT\n\n    In War-Torn Sudan, a Gold Mining Boom Takes a Human Toll\n\n  Opinion\n\n    With NOAA Cuts, a Proud Legacy and Vital Science Are at Risk\n\n  Biodiversity\n\n    Imperiled in the Wild, Many Plants May Survive Only in Gardens\n\n  Climate\n\n    Can Toxic Mining Waste Help Remove CO2 from the Atmosphere?\n\n  INTERVIEW\n\n    Saving U.S. Climate and Environmental Data Before It Goes Away\n\n  Biodiversity\n\n    A Craze for Tiny Plants Is Driving a Poaching Crisis in South Africa\n\n  INTERVIEW\n\n    Bill McKibben on Climate Activism in the Age of Trump 2.0\n\n  Climate\n\n    How Climate Change Puts the Safety of Drinking Water at Risk\n\n  Energy\n\n    ‘Green Grab’: Solar and Wind Boom Sparks Conflicts on Land Use\n\n  INTERVIEW\n\n    Reciprocity: Rethinking Our Relationship with the Natural World\n\n  Oceans\n\n    With Sea Ice Melting, Killer Whales Are Moving Into the Arctic\n\n  Cities\n\n    As War Halts, the Environmental Devastation in Gaza Runs Deep",
    "summary": {
      "en": "**Summary of E360 Digest - March 24, 2025**\n\nResearch published in *Nature Climate Change* reveals that since 2000, melting Arctic glaciers have uncovered about 1,500 miles of coastline, mainly in Greenland. This retreat of glaciers has exposed valuable resources, but it also poses risks, as the newly uncovered land is prone to erosion and landslides. For instance, in September 2023, a glacier in eastern Greenland collapsed, causing a large landslide that generated a significant tsunami.",
      "ko": "2025년 3월 24일자 E360 다이제스트 요약에 따르면, *Nature Climate Change*에 발표된 연구 결과, 2000년 이후 북극의 빙하가 녹으면서 그린란드 지역을 중심으로 약 1,500마일의 해안선이 드러났습니다. 이 빙하의 후퇴는 귀중한 자원을 노출시키지만, 새로 드러난 땅은 침식과 산사태에 취약해 위험을 동반합니다. 예를 들어, 2023년 9월에는 그린란드 동부의 한 빙하가 붕괴되면서 큰 산사태가 발생했고, 이로 인해 상당한 규모의 쓰나미가 발생했습니다.",
      "ja": null
    }
  },
  {
    "id": "5c09f290955ffdb2",
    "title": {
      "en": "Alkanes on Mars",
      "ko": "화성의 알케인",
      "ja": null
    },
    "type": "story",
    "url": "https://www.science.org/content/blog-post/alkanes-mars",
    "score": 70,
    "by": "nick__m",
    "time": 1743095458,
    "content": "News came this week of the discovery of long-chain alkane molecules in a sample of Martian rock - here's the full manuscript at PNAS. This was not an easy find. As you'll see from that article, the samples themselves were collected in 2013 in Gale Crater by the Curiosity rover, and the same sample is the one that showed organic compounds in it a few years ago in another analysis. So why didn't these newly announced larger molecules show up then?\nThat's because this time the samples got a different treatment (two cycles of strong heating) before the mass spec analysis. And the compounds themselves (decane, undecane, and dodecane) were detected in rather small concentations as well. This all goes back to the design of the instrument package itself and to a constant problem that the team on Earth has had to contend with. The Sample Analysis on Mars (SAM) module is basically a GS/MS instrument, and as such it has provisions to heat the samples (at temperatures of up to 850C) to volatilize things for analysis. And it also has a derivatization reagent available, N-methyl-N-tert-butyldimethylsilyl-trifluoroacetamide (MTBSTFA). As the analytical chemistry types in the audience will appreciate, this will cap reactive nitrogens and oxygens (NH, NH2, OH) with t-butyldimethylsilyl (TBDMS) groups, which makes the compounds containing them much more likely to be able to usefully fly down the GC/MS. This sort of silylation treatment is a common procedure in such analyses.\nBut what didn't work out was leakage of that reagent into the instrument package, which probably happened during the strenuous landing procedures that any spacecraft has to endure to make a soft landing on Mars. Making a new crater would be more simple! This has left the rest of the SAM activity during this mission a long-running story of working around that spill in order to get useful data, such as making sure that the results obtained aren't from (say) pyrolysis of MTBSTFA or its unintended reactions with authentic Martian materials.\nIn this case, there was \"opportunistic derivatization\" of the sample with that reagent as its vapors reacted with the material in the sample holder over the intervening months. The team took the sample up to about 500C to break down the perchlorates and similar species, with release of oxygen, and then heated it again to 850C, the idea being that now there would be much less oxygen available for the combustion of any organics that made it into the gas phase. The higher temperatures were to break down sulfate minerals in the sample to potentialy release more organic residues. Another modification that had been worked out was keeping the cold trap (to catch volatilized material) at a somewhat warmer temperature than originally planned, which let a lot of MTBSTFA contaminants (such as trifluoro-N-methylacetamide and bis-TBDMS-disiloxane, known to the team as \"BSW\" for \"bisilylated water\") wash through the system instead of being retained.\nAnd it was in that higher-temperature run that the long-chain alkanes were detected. The best hypothesis now is that these are likely to have been the decarboxylation products of the corresponding fatty acids, because (for one thing) you shouldn't have needed such high temperatures to volatilize them as the plain alkanes, and for another, the chemical environment of Martian soil is strongly oxidizing to start with (all those perchlorates and such). Experiments with simulated martian soil/rock samples and such fatty acids do indeed produce the decarboxylation products on heating, so it's at least chemically plausible. So if there are indeed fatty acids on Mars, where did they come from?\nWell, one immediate thought is from living creatures, because that's where they'd probably be coming from here on Earth. But that would be a long leap to make for a Martian sample, because there are certainly some abiotic ways to make these things. But the authors do note that abiotic fatty acids tend to be shorter-chained than this (and I would note that long-term abiotic thermal reactions would be expected to produce branched-chain compounds more than the n-alkanes seen here). It's also possible that these compounds are the breakdown products of still longer fatty acids as well. Biotic fatty acids from life-as-we-know-it are biases towards even numbers of carbons (since they're built up and torn down by two-carbon units), but with only three in this sample it's hard to make an argument one way or another based on this effect - and especially since we don't know if any hypothetical Martian life has/had that same biochemical bias.\nNo one's going to be able to settle the biotic/abiotic argument around such samples until we get some of them back to Earth for workup by more comprehensive analytical techniques. We might well not even settle it then! As many will know, the Perserverance rover is in fact designed to seal and store samples for such a future sample return, and it's been doing so. But that back end of that mission is. . .well, a problem. NASA's plans for such a land-on-Mars, collect-the-samples, send-them-back-to-Martian-orbit, and return-them-to-Earth have been getting fantastically expensive and complex, to the point that everyone realized that they simply weren't going to happen (and might well not have worked if they had). New options are being explored, but it's anyone's guess when such a mission would launch.\nDiscovery by a rover of some fossilized Martian critters in one of these rocks would settle the main issue more quickly, but that is an unlikely event (although man, would I ever enjoy it). And let's be honest - any such find would just make everyone more crazy to get their (glove-boxed, robotic) hands on the real samples to try to figure out how related such life was to life here on Earth! No, if there is or was life on Mars it's far more likely to be at the single-cell level, and thus trickier to detect (after all, most of the history of life on Earth was single-celled, and those life forms still outnumber the rest of us handily). The search continues!",
    "summary": {
      "en": "This week, scientists announced the discovery of long-chain alkane molecules in Martian rock samples collected by the Curiosity rover in 2013. These larger molecules were not found in earlier analyses due to the different treatment of the samples this time, which involved heating them strongly before analysis. The specific compounds detected are decane, undecane, and dodecane, but they were found in small amounts.\n\nThe analysis used a technique involving heating samples up to 850°C, which helps to release organic materials for detection. However, a previous issue with a reagent leaking into the instrument complicated the analysis, leading to a need for adjustments in the procedure to minimize contamination.\n\nThe long-chain alkanes detected are thought to be products of decarboxylation from fatty acids, which could hint at the presence of these acids on Mars. While fatty acids on Earth typically come from living organisms, they can also be formed through non-biological processes. It's uncertain whether the detected molecules are biotic or abiotic, and further analysis of returned samples from Mars is needed to clarify their origins.\n\nNASA's Perseverance rover is currently storing samples for a planned return to Earth, but the mission's complexity and costs have made timelines uncertain. The search for evidence of life on Mars continues, with the possibility that any Martian life forms may be single-celled and difficult to detect.",
      "ko": "이번 주, 과학자들은 2013년 큐리오시티 로버가 수집한 화성 암석 샘플에서 긴 사슬 알케인 분자를 발견했다고 발표했습니다. 이 큰 분자들은 샘플을 분석할 때의 처리 방식이 달라져 이전 분석에서는 발견되지 않았습니다. 이번에는 샘플을 강하게 가열한 후 분석을 진행했습니다. 발견된 특정 화합물은 데칸, 운데칸, 그리고 도데칸으로, 모두 소량으로 존재했습니다.\n\n분석에는 샘플을 850도까지 가열하는 기술이 사용되었으며, 이는 유기 물질을 방출하여 검출하는 데 도움을 줍니다. 그러나 이전에 시약이 기기에 누출되는 문제가 발생하여 분석이 복잡해졌고, 오염을 최소화하기 위해 절차를 조정해야 했습니다.\n\n검출된 긴 사슬 알케인은 지방산의 탈카복실화 과정에서 생성된 것으로 추정되며, 이는 화성에 이러한 지방산이 존재할 가능성을 시사합니다. 지구에서 지방산은 일반적으로 생명체에서 유래하지만 비생물학적 과정에서도 형성될 수 있습니다. 따라서 발견된 분자가 생물학적 기원인지 비생물학적 기원인지는 불확실하며, 화성에서 돌아온 샘플에 대한 추가 분석이 필요합니다.\n\n현재 NASA의 퍼서비어런스 로버는 지구로 돌아올 샘플을 저장하고 있지만, 임무의 복잡성과 비용으로 인해 일정이 불확실해졌습니다. 화성에서 생명체의 증거를 찾는 작업은 계속되고 있으며, 화성의 생명체가 단세포일 가능성이 있어 탐지가 어려울 수 있습니다.",
      "ja": null
    }
  },
  {
    "id": "2bfa44bc1d84798a",
    "title": {
      "en": "Trapping misbehaving bots in an AI Labyrinth",
      "ko": "AI 미로의 함정",
      "ja": null
    },
    "type": "story",
    "url": "https://blog.cloudflare.com/ai-labyrinth/",
    "score": 236,
    "by": "pabs3",
    "time": 1742391067,
    "content": "Trapping misbehaving bots in an AI Labyrinth2025-03-19Reid TatorisHarsh SaxenaLuis Miglietti5 min readToday, we’re excited to announce AI Labyrinth, a new mitigation approach that uses AI-generated content to slow down, confuse, and waste the resources of AI Crawlers and other bots that don’t respect “no crawl” directives. When you opt in, Cloudflare will automatically deploy an AI-generated set of linked pages when we detect inappropriate bot activity, without the need for customers to create any custom rules.AI Labyrinth is available on an opt-in basis to all customers, including the Free plan.\n\n            Using Generative AI as a defensive weapon\n\n        AI-generated content has exploded, reportedly accounting for four of the top 20 Facebook posts last fall. Additionally, Medium estimates that 47% of all content on their platform is AI-generated. Like any newer tool it has both wonderful and malicious uses.At the same time, we’ve also seen an explosion of new crawlers used by AI companies to scrape data for model training. AI Crawlers generate more than 50 billion requests to the Cloudflare network every day, or just under 1% of all web requests we see. While Cloudflare has several tools for identifying and blocking unauthorized AI crawling, we have found that blocking malicious bots can alert the attacker that you are on to them, leading to a shift in approach, and a never-ending arms race. So, we wanted to create a new way to thwart these unwanted bots, without letting them know they’ve been thwarted.To do this, we decided to use a new offensive tool in the bot creator’s toolset that we haven’t really seen used defensively: AI-generated content. When we detect unauthorized crawling, rather than blocking the request, we will link to a series of AI-generated pages that are convincing enough to entice a crawler to traverse them. But while real looking, this content is not actually the content of the site we are protecting, so the crawler wastes time and resources.As an added benefit, AI Labyrinth also acts as a next-generation honeypot. No real human would go four links deep into a maze of AI-generated nonsense. Any visitor that does is very likely to be a bot, so this gives us a brand-new tool to identify and fingerprint bad bots, which we add to our list of known bad actors. Here’s how we do it…\n\n            How we built the labyrinth\n\n        When AI crawlers follow these links, they waste valuable computational resources processing irrelevant content rather than extracting your legitimate website data. This significantly reduces their ability to gather enough useful information to train their models effectively.To generate convincing human-like content, we used Workers AI with an open source model to create unique HTML pages on diverse topics. Rather than creating this content on-demand (which could impact performance), we implemented a pre-generation pipeline that sanitizes the content to prevent any XSS vulnerabilities, and stores it in R2 for faster retrieval. We found that generating a diverse set of topics first, then creating content for each topic, produced more varied and convincing results. It is important to us that we don’t generate inaccurate content that contributes to the spread of misinformation on the Internet, so the content we generate is real and related to scientific facts, just not relevant or proprietary to the site being crawled.This pre-generated content is seamlessly integrated as hidden links on existing pages via our custom HTML transformation process, without disrupting the original structure or content of the page. Each generated page includes appropriate meta directives to protect SEO by preventing search engine indexing. We also ensured that these links remain invisible to human visitors through carefully implemented attributes and styling. To further minimize the impact to regular visitors, we ensured that these links are presented only to suspected AI scrapers, while allowing legitimate users and verified crawlers to browse normally.\n\n          A graph of daily requests over time, comparing different categories of AI Crawlers.What makes this approach particularly effective is its role in our continuously evolving bot detection system. When these links are followed, we know with high confidence that it's automated crawler activity, as human visitors and legitimate browsers would never see or click them. This provides us with a powerful identification mechanism, generating valuable data that feeds into our machine learning models. By analyzing which crawlers are following these hidden pathways, we can identify new bot patterns and signatures that might otherwise go undetected. This proactive approach helps us stay ahead of AI scrapers, continuously improving our detection capabilities without disrupting the normal browsing experience.By building this solution on our developer platform, we've created a system that serves convincing decoy content instantly while maintaining consistent quality - all without impacting your site's performance or user experience.\n\n            How to use AI Labyrinth to stop AI crawlers\n\n        Enabling AI Labyrinth is simple and requires just a single toggle in your Cloudflare dashboard. Navigate to the bot management section within your zone, and toggle the new AI Labyrinth setting to on:\n\n          Once enabled, the AI Labyrinth begins working immediately with no additional configuration needed.\n\n            AI honeypots, created by AI\n\n        The core benefit of AI Labyrinth is to confuse and distract bots. However, a secondary benefit is to serve as a next-generation honeypot. In this context, a honeypot is just an invisible link that a website visitor can’t see, but a bot parsing HTML would see and click on, therefore revealing itself to be a bot. Honeypots have been used to catch hackers as early as the late 1986 Cuckoo’s Egg incident. And in 2004, Project Honeypot was created by Cloudflare founders (prior to founding Cloudflare) to let everyone easily deploy free email honeypots, and receive lists of crawler IPs in exchange for contributing to the database. But as bots have evolved, they now proactively look for honeypot techniques like hidden links, making this approach less effective.AI Labyrinth won’t simply add invisible links, but will eventually create whole networks of linked URLs that are much more realistic, and not trivial for automated programs to spot. The content on the pages is obviously content no human would spend time-consuming, but AI bots are programmed to crawl rather deeply to harvest as much data as possible. When bots hit these URLs, we can be confident they aren’t actual humans, and this information is recorded and automatically fed to our machine learning models to help improve our bot identification. This creates a beneficial feedback loop where each scraping attempt helps protect all Cloudflare customers.\n\n            What’s next\n\n        This is only the first iteration of using generative AI to thwart bots for us. Currently, while the content we generate is convincingly human, it won’t conform to the existing structure of every website. In the future, we’ll continue to work to make these links harder to spot and make them fit seamlessly into the existing structure of the website they’re embedded in. You can help us by opting in now.To take the next step in the fight against bots, opt-in to AI Labyrinth today.Cloudflare's connectivity cloud protects entire corporate networks, helps customers build Internet-scale applications efficiently, accelerates any website or Internet application, wards off DDoS attacks, keeps hackers at bay, and can help you on your journey to Zero Trust.Visit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.To learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.Discuss on Hacker NewsSecurity WeekBotsBot ManagementAI BotsAIMachine LearningGenerative AI",
    "summary": {
      "en": "Cloudflare has introduced a new feature called AI Labyrinth, designed to confuse and slow down unauthorized bots that ignore \"no crawl\" directives. This system uses AI-generated content to create a series of linked pages that bots may follow, wasting their resources without alerting them to being blocked.\n\nKey points include:\n\n1. **Purpose**: AI Labyrinth aims to distract unwanted bots while collecting data on their behavior to improve bot detection systems.\n\n2. **How It Works**: When suspicious bot activity is detected, AI Labyrinth generates convincing but irrelevant pages that bots might crawl, making it harder for them to gather useful information.\n\n3. **Implementation**: Users can easily enable this feature in their Cloudflare dashboard with a simple toggle.\n\n4. **Benefits**: The approach not only confuses bots but also helps identify them as they navigate through the maze of AI-generated pages, contributing to better bot detection.\n\n5. **Future Plans**: Cloudflare plans to enhance this feature by making the AI-generated links more integrated with existing website structures and harder for bots to detect.\n\nOverall, AI Labyrinth is a proactive tool for protecting websites from unwanted bot activity while maintaining a seamless experience for legitimate users.",
      "ko": "클라우드플레어는 \"크롤링 금지\" 지침을 무시하는 무단 봇을 혼란스럽게 하고 느리게 만드는 새로운 기능인 AI 미로를 도입했습니다. 이 시스템은 AI가 생성한 콘텐츠를 사용하여 봇이 따라갈 수 있는 일련의 연결된 페이지를 만들어, 봇이 차단되었다는 경고 없이 자원을 낭비하게 합니다.\n\nAI 미로의 주요 목적은 원치 않는 봇을 분산시키면서 그들의 행동에 대한 데이터를 수집하여 봇 탐지 시스템을 개선하는 것입니다. 의심스러운 봇 활동이 감지되면 AI 미로는 봇이 크롤링할 수 있는 그럴듯하지만 관련 없는 페이지를 생성하여 유용한 정보를 수집하기 어렵게 만듭니다.\n\n사용자는 클라우드플레어 대시보드에서 간단한 토글로 이 기능을 쉽게 활성화할 수 있습니다. 이 접근 방식은 봇을 혼란스럽게 할 뿐만 아니라, AI가 생성한 페이지의 미로를 탐색하는 동안 봇을 식별하는 데도 도움이 되어 더 나은 봇 탐지에 기여합니다.\n\n클라우드플레어는 앞으로 AI가 생성한 링크를 기존 웹사이트 구조와 더 통합하고 봇이 감지하기 어렵게 만드는 방향으로 이 기능을 강화할 계획입니다. 전반적으로 AI 미로는 웹사이트를 원치 않는 봇 활동으로부터 보호하면서 정당한 사용자에게는 원활한 경험을 제공하는 능동적인 도구입니다.",
      "ja": null
    }
  },
  {
    "id": "d0d4221bad257bc0",
    "title": {
      "en": "Jensen, We're with You. But We're Not There Yet",
      "ko": "젠슨, 함께하지만 아직 멀었어",
      "ja": null
    },
    "type": "story",
    "url": "https://www.chipstrat.com/p/jensen-were-with-you-but-were-not",
    "score": 4,
    "by": "austinlyons",
    "time": 1743183529,
    "content": "Share this postChipstratJensen, We’re With You. But We’re Not There Yet.Copy linkFacebookEmailNotesMoreJensen, We’re With You. But We’re Not There Yet.Austin LyonsMar 28, 2025∙ Paid13Share this postChipstratJensen, We’re With You. But We’re Not There Yet.Copy linkFacebookEmailNotesMore5ShareOne thing stood out in NVIDIA’s GTC Financial Analyst Q&A: CEO Jensen Huang didn’t feel heard. His tone and demeanor suggested he was frustrated that his message wasn’t landing. Watch the event from ~12:03 to 13:00 or so in the video here.Jensen Huang: And we're not just building a chip, we're building networkings and switches and, you know, we're basically building systems components for the world's enterprise, for the world's data center. So that's number one. The second thing I said is that nobody's got right, none of these forecasts has it, this concept of AI factories. Are you guys following me? It's not a multipurpose data center. It's a single-function AI factory. And these GPU clouds, Stargates, and so on and so forth. Okay? These AI factories are not accounted for. Do you guys understand? Nope. Because nobody knows how to do that. And these multiple hundred billion dollar CapEx projects coming online are not part of somebody's data center forecast. Are you guys following me? How could they possibly know about these things? We're inventing them as we speak.Over the span of an hour, he said, “Are you guys following me?” six times, “Do you guys understand?” several times, and “I hope you understand.”  It wasn’t just analysts Jensen was frustrated with. The market doesn’t fully comprehend his long-term vision, and the broader public seems even further from understanding what NVIDIA is building.But before I give my two cents, I want to preface that I’m a huge believer in AI, and I think NVIDIA is as strong as ever. Consider this a loving intervention.But, with all due respect, it’s not that the audience doesn’t understand NVIDIA. I think NVIDIA doesn’t understand the audience.Let me explain.Know Your AudienceWho is NVIDIA’s audience? It’s not just Wall Street. Judging by market cap, every retail investor is Jensen’s audience. NVIDIA stands among huge consumer brands like Apple, Microsoft, Amazon, and Google. I’d go one step further — given Jensen’s vision, the general public is Jensen’s audience.So does the general public understand NVIDIA? And does Jensen understand the general public?A Thought ExperimentImagine standing in a Walmart in Fargo, offering free samples. Not as a marketer, but simply to talk with everyday Americans. What if we ask them what they know about Apple, Amazon, or Google?Have I ever heard of Apple? (Holds up iPhone). Of course!Heard of Amazon? Ha! The Amazon truck stops every day on my street!Do I use Google? Oh yah, sure, you betcha. But what if we ask about NVIDIA? Some might say “I’m a gamer. Of course. NVIDIA GPUs are the best.” But the rest? I promise you the answer will be along the lines of:NVIDIA? Uff da. I know I’ve heard of them… I know they are a tech company. Hold on, let me think….. I know they are good to invest in! Ha! ….. what do they do though? Oh boy…. Something with AI?I know Nvidia vs the others doesn’t feel like a fair comparison. Everyone has intimate proximity to the other consumer brands, whereas the vast majority of NVIDIA’s revenue doesn’t come from consumers. But NVIDIA is a GPU company, right?Nope, NVIDIA is not a GPU company. It used to be a GPU company, but pivoted to a GPGPU company.GPGPU? A General Purpose GPU. With it, you can do more than just graphics. Oh, like Bitcoin? NVIDIA did Bitcoin, right?Well, yes, NVIDIA GPUs were used for Bitcoin mining. But nevermind that; that was just a phase. Now, NVIDIA makes systems. It is an infrastructure company.… an infrastructure company?Infrastructure to build AI factories.…you’re saying NVIDIA is an AI factory…. factory?Oh, that’s meta.Meta? …. Meta makes the factories?No, sorry. Gimme a second… NVIDIA is the platform every major industry will use to build out its AI factories.I’m just gonna get back to shopping now…AI factories? That’s not gonna play in Peoria.If You Can’t Explain It To A ChildIf NVIDIA wants to be understood by the general public, a good starting point is to ask if the basics make sense to a fifth-grader.How about the opening minutes of GTC from a fifth grader’s eyes?This is how intelligence is built now—a new kind of factory, generating tokens, the basic units of AI. Tokens have opened up a new frontier, the first step into a world full of possibility.They turn images into scientific insight, mapping distant atmospheres and guiding future explorers. They convert raw data into foresight, helping us prepare for what’s next.Tokens help us decode physics to move faster and reach farther. They can detect disease before symptoms appear and help us understand the biology of life itself.They connect information in ways that protect the most vulnerable. They turn potential into abundance and help us gather the benefits of what we grow.Tokens don’t just teach robots to move—they bring delight, offer support, and make the impossible feel within reach.Together, we take the next great leap, pushing into the unknown.And it all starts here.Tokens? Like… arcade tokens?Nolan Bushnell earned more from founding Chuck E. Cheese than from founding Atari! AcquiredSure, GTC isn’t aimed at kids. Maybe retail investors, at best. It’s an industry event, right? For industry insiders.Yet downstream content—tweets, TikToks, WSJ headlines—flow from Jensen’s keynote. GTC shapes the narrative for everyone.If NVIDIA and Jensen can express their vision in terms that are simple enough for a child, then journalists, investors, analysts, and customers will all be able to grasp it too.But it’s not simply the choice of language.The Elephant in the RoomGenAI isn’t part of everyday life in places like Fargo. That’s the hurdle. People don’t care about NVIDIA’s AI infrastructure if the output doesn’t touch them. NVIDIA can’t win the narrative until AI actually matters to regular people.Sure, ChatGPT has 400 million weekly users, but how many are paying subscribers? And how many in Fargo? Show me the Fargo businesses with increased revenue and productivity thanks to generative AI?Where’s the ad revenue business model to truly become a worldwide verb like Google and get on the home screen of every phone in Fargo? Sam Altman is not interested an advertising business model, but the general Fargo population isn’t going to pay for ChatGPT.Don’t get me wrong; there are people everywhere who understand and use generative AI, and pay for it — including people in Fargo. And I’m not downplaying the usefulness of LLMs either. If anything, I believe Generative AI will transform the world more profoundly than the internet ever did.However, Jensen’s earnings calls, keynotes, and investor presentations are all based on the assumption that GenAI's future is here. Which is true for Nvidians. Just not their audience.The future exists and is continually created inside NVIDIA’s walls.Again, the issue isn’t that the audience doesn’t understand NVIDIA—it’s that NVIDIA doesn’t understand the audience. They’re not living in the same timeline. Nvidia envisions the future, invents the software and hardware needed to enable the vision, and then uses it! They live in the future. Jensen exemplifies this by using AI constantly and understanding the deep technical details from top to bottom.Yet the general public lives in the present; some experience a small taste of that future, but most just aren’t experiencing it yet.NVIDIA, we’re with you—we’re just not there yet.Distributing the FutureOrdinary folks aren’t using AI because they don’t see the point; it doesn’t obviously solve their daily problems.Flashing tomorrow’s tech at them won’t drive adoption; they need to see why it matters now. But Jensen’s NVIDIA is often skipping ahead to showing the future they are enabling.Futuristic scene from NVIDIA’s GTC keynote opening video.This is understandable, and it’s not just Jensen, but it’s Elon’s problem too. Elon, I don’t need a robotaxi or humanoid today. Yes, that looks cool. But dude, we drive a gasoline guzzling minivan every single day. That’s our problem. We just need a Tesla minivan right now.  NVIDIA needs to show more of the present and less of the future. But that’s the rub for NVIDIA: what problems are they solving right now for ordinary folks? Again, there’s the adoption problem.But, look no further than… North Dakota! The state government is trying to use GenAI to solve real problems:The [use case] that we’re very keenly focused on exploring is making it possible to go to the state of North Dakota website and have an interface that allows you to comb through the treasure trove of information across the state’s multiple websites through a simple request or prompt. Our goal is to create a frictionless experience for our citizens to interact with the government.An example of this would be going to the Governor’s Office website and asking the large language model, “How do I start a business?” Currently, you might have to go through five or six clicks until you get to the right information. Or, there are instances when trying to comply with a tax requirement that you could find yourself going through 50 clicks to get the information that you want.Making it easier to start a business or research taxes in North Dakota? That’s what I’m talking about! 💪 It's much more relatable than humanoid robots serving coffee! Is making coffee even a problem people have today? I’m pretty sure there are already cheap machines to make coffee for us…NVIDIA should back real-world initiatives like these that make AI matter to people, turning them into compelling narratives stamped with “Powered by NVIDIA.” It’s a strategic brand play with mutual upside.Shoot, here’s a typical American problem: The DMV. The people in these pictures are ordinary Americans who want to do LITERALLY ANYTHING ELSE in the world other than wait here. They probably even had to waste a vacation day to knock this out.There’s obvious friction in how DMVs run—clunky software, slow workflows, not enough people. LLMs could help. Agents too. States don’t have the motivation or the tech skills to fix it, but NVIDIA does!Want to go mainstream? Imagine future keynotes—instead of talking abstractly about tokens, NVIDIA could show before-and-after videos of the lines at the local DMV. Talk about great fodder for viral social media videos! By the way, I used AI (Grok) to remove all the people from these images. And Grok, of course, runs on NVIDIA! Yes, the examples are hypothetical, but the core issue is real: Jensen talks like everyone already uses and values AI. But most people don’t. The best way to reach the public is to make AI real in their daily lives.BaggagePart of the public misunderstanding of NVIDIA relates to the baggage of 30 years of NVIDIA finding their way. Graphics? GPU? GPGPU? Bitcoin? AI? But now that NVIDIA is worth more than many countries and even has consumer product aspirations, it needs public clarity. What, exactly, is NVIDIA’s core product? And how can NVIDIA describe that product in terms accessible to the broader public?Naming and Framing ProblemsI was on The Circuit podcast recently, trying to explain the difference between DeepSeek V3, DeepSeek R1, DeepSeek-Distill-Llama-70B, and DeepSeek-R1-Distill-Llama-8B. You can see where this is going 😅Well, there’s DeepSeek V1, which wasn’t actually named V1, and it’s a dense model. Then they shipped DeepSeekMoE, the mixture of experts sparse model. Next up was V2, which was also an MoE. Then V3 was the best MoE yet. And these are all “fast-thinking” models. Next, V3’s base model was used with RL to train a reasoning model called R1. Engineers love literal names like DeepSeek-Distill-Llama-70B. It makes perfect sense; the name says it all!This is a common trap in platform industries like AI and semis, where companies sell to other tech firms. Engineers market to engineers. The meaning is obvious to industry insiders but meaningless to everyone else. These industries and their technical marketers also resort to versioning as a naming convention, which is sort of like semantic versioning in software.Intel was king of this with their microprocessors 4004 → 8008 → 8080 → 8086 → 286 → 386 → 486. 😅Sound familiar? Just look at OpenAI: GPT-3 → 3.5 → 4 → 4o → 4.5.Naming is tough; I get it. And like I said, these are platform companies that sell the building blocks (APIs, chips, memory) that power other companies' consumer products. The platform company engineers and marketers may not realize there’s a disconnect because their customers get it, even if the customer’s customers don’t.FramingHowever, for platform companies, this is not just a naming issue but a framing issue. You need to make sure your customer’s customer understands why you’re essential to the product they’re using.Think about “Intel Inside.” Watch this beautiful framing from 1992:This symbol outside means you have the standard inside that an entire library of software has been written to. The Intel microprocessor. Think of it as a library card that lets you run the software of today and tomorrow. So check out computers with Intel the computer inside.See the framing?Problem: When you’re buying a computer, you need to make sure it can run the software you care about – now and in the future.  This problem is implied.Solution: Buy a computer with Intel inside. A ton of software runs on Intel.Consumers aren’t buying CPUs; they’re buying a computer so they can run software. Since Intel doesn’t sell computers, they frame where Intel fits and why they are an essential part of the solution.Notice Intel went to directly to the customer’s customer! And, Intel didn’t mention any product names! They just made the connection between their brand (Intel) and a brief mention of their line of products (microprocessor).So even if your product names are borked, you’re not dead 😅InformallyAnother option is to use informal names that help the general public understand. For illustration (these aren’t great, but you get the gist):So the amazing thing about this tiny thinking llama is that it’s small enough to run on your smartphone or laptop! And the full-sized thinking llama can run on an AI workstation like the NVIDIA Spark! I can imagine how helpful a bunch of digital interns at your fingertips would be – can you?Naming and framing, especially with baggage, can trip up even the best of companies.NVIDIA’s Naming Problems, IllustratedWhen NVIDIA only made graphics cards, or GPUs, it made sense to name and talk about a new architecture like Volta.But NVIDIA is an AI systems company, not a GPU companyNVIDIA’s 2025 revenue mix. Data from QuartrThe AI systems that account for 88% of revenue are much more than a GPU. They are a system, including cables, switches, racks, GPUs, CPUs, liquid cooling, and software.Yet, confusingly, NVIDIA’s framing centers around the GPU. Blackwell, the GPU architecture, is mentioned with every productYes, Blackwell is a core component in every product which has a bunch of useful implications for engineers, like the ability to deploy CUDA on any of these devices.Of course, for anyone else the GPU-centric framing of every product is confusing.It’s like this, except the Grace Blackwell spiderman should be 9x bigger than the others combined.So when NVIDIA launches “the new Blackwell”, are we talking about the AI datacenters, or new graphics cards, or new self-driving car processors, or what?And the crazy thing is that the system that makes all the money, the AI datacenters, doesn’t really have a name! Sure it does. Don’t be dumb. It’s Grace Blackwell.That’s the name of the system? Or is that just the CPU architecture (Grace) and the GPU architecture (Blackwell)?Well, ok, the name is GB300 NVL72.Oh, so it’s a “version control” name? Yeah, how’s that gonna play in Fargo?So it’s the Grace Blackwell 300 NVL72, which means the latest CPU architecture (Grace) and the new GPU architecture (Blackwell). There was already a 200, so they updated the version number (300). NVL72 is the networking configuration—NVLink with 72 GPUs in a node. This many GPUs implies liquid cooling as well, because they cram those suckers in and run them hot!OK, fine, just focus on the generation. OK. This generation is Blackwell. The next generation is Rubin. Like it says in the roadmap:Wait a minute… So the AI systems are named after the GPUs, yet you’re not a GPU company? 🤔Give the System a NameMaybe it’s time for a reset. Instead of naming AI datacenters after GPUs or with version codes, why not give the system its own name? Something memorable, something brandable.Yes, of course I named it after The Matrix, I couldn’t help myself…. Matrix multiplication. Agents. Simulated worlds. AI…. I mean, come on.Anyway, keep it simple for the general public. We’re launching our newest AI supercomputer, Neo!And let MKBHD summarize the details for the public in 56 seconds like he does.Real World AIOK, now that you’ve got a name and can talk to the public, we’re back to that “AI doesn’t matter to me” problem.Given that most people don’t see how Generative AI impacts their life, let alone understand what “AI factory” or “AI supercomputer” means, why not illustrate in ways that customers understand?Imagine a commercial that illustrates common problems and shows how they are solved with Generative AI, powered by NVIDIA’s AI supercomputers.For example, most everyone can relate to the pains of grocery shopping, but it could be sped up with AI:Remember how Chemistry was hard? What if you had a personalized study guide?Of course, I generated these with AI, powered by NVIDIA:I didn’t say they’d be great — but I couldn’t have done it without NVIDIA AI!Behind the paywall, we’ll get tactical. What lessons can NVIDIA learn from earlier GenAI hardware marketing failures like Intel’s AI PC and Apple Intelligence? Should Jensen share the story-telling load? Then we’ll conclude with some final takeaways.By the way, if you’re a free subscriber who wants even more brain food, consider talking to your boss about a reimbursement so you can enjoy the entire serving. Creative Strategies also offers an enterprise Chipstrat subscription for your entire company to enjoy a weekly helping. More here. This post is for paid subscribersSubscribeAlready a paid subscriber? Sign inPrevious",
    "summary": {
      "en": "In a recent Q&A session at NVIDIA's GTC event, CEO Jensen Huang expressed frustration that his vision for the company and its future wasn't being understood by analysts and the public. He emphasized that NVIDIA is not just a GPU company but an infrastructure company focused on building AI systems, which he referred to as \"AI factories.\" Despite his repeated questions to the audience about their understanding, it became clear he felt his message was not resonating.\n\nThe author argues that the issue lies not with the audience's comprehension but with NVIDIA's failure to connect with them. Many people, especially outside the tech industry, are unfamiliar with NVIDIA and its products. Unlike well-known consumer brands like Apple or Amazon, NVIDIA's advancements in AI technology do not directly impact the daily lives of most individuals.\n\nTo bridge this gap, NVIDIA should simplify its messaging, making it relatable and understandable to the general public. By focusing on real-world applications of AI that address everyday problems, such as improving government services or enhancing shopping experiences, NVIDIA can better demonstrate its relevance. The author suggests that NVIDIA needs clearer naming and framing of its products to make them more accessible and memorable to consumers. Ultimately, to gain broader acceptance, NVIDIA must connect its innovations with the tangible benefits they can offer to everyday people.",
      "ko": "최근 NVIDIA의 GTC 행사에서 열린 Q&A 세션에서 CEO 젠슨 황은 자신의 회사 비전과 미래에 대한 이해가 분석가들과 대중에게 전달되지 않고 있다는 점에 대해 불만을 표출했습니다. 그는 NVIDIA가 단순한 GPU 회사가 아니라 AI 시스템 구축에 집중하는 인프라 회사라고 강조하며 이를 \"AI 공장\"이라고 표현했습니다. 그는 청중에게 그들의 이해도를 여러 번 질문했지만, 자신의 메시지가 제대로 전달되지 않았다고 느끼는 것 같았습니다.\n\n저자는 문제의 원인이 청중의 이해 부족이 아니라 NVIDIA가 그들과 소통하지 못하는 데 있다고 주장합니다. 특히 기술 산업 외부의 많은 사람들은 NVIDIA와 그 제품에 대해 잘 알지 못합니다. 애플이나 아마존과 같은 잘 알려진 소비자 브랜드와 달리, NVIDIA의 AI 기술 발전은 대부분의 사람들의 일상생활에 직접적인 영향을 미치지 않습니다.\n\n이러한 간극을 해소하기 위해 NVIDIA는 메시지를 간소화하여 일반 대중이 이해하고 공감할 수 있도록 해야 합니다. 정부 서비스 개선이나 쇼핑 경험 향상과 같은 일상적인 문제를 해결하는 AI의 실제 응용 사례에 집중함으로써, NVIDIA는 자신의 중요성을 더 잘 보여줄 수 있습니다. 저자는 NVIDIA가 소비자에게 더 접근하기 쉽고 기억에 남도록 제품의 이름과 프레임을 명확히 할 필요가 있다고 제안합니다. 궁극적으로, NVIDIA는 혁신을 일상적인 사람들에게 제공할 수 있는 실질적인 혜택과 연결시켜야 더 넓은 수용을 얻을 수 있을 것입니다.",
      "ja": null
    }
  },
  {
    "id": "1ac5efb447be78d8",
    "title": {
      "en": "Show HN: An open source alternative to Wakatime",
      "ko": "오픈소스, Wakatime 대안!",
      "ja": null
    },
    "type": "story",
    "url": "https://wakana.io",
    "score": 36,
    "by": "jemiluv8",
    "time": 1742899923,
    "content": "Beta TestingObserve your work in real timeDeveloper dashboards for insights into your work habitsTry it for free\n\nHow it WorksInstall the relevant WakaTime plugins for your editorhereLocate the ~/.wakatime.cfg file on your computer. This is usually located in your root folder. On windows you might have to show hidden files to see it.Update it with the API key you get in your Wakana dashboardhereUpdate the api_url to https://api.wakana.io/apiAfter ConfigurationOpen your editor and start typing somethingCheck your Wakana dashboard to see if stats show upAlso check the plugins section on your dashboardhereto see if data from any of your plugins has been collectedSample ConfigurationCopy and paste this into your ~/.wakatime.cfg file[settings]\napi_url = https://api.wakana.io/api\napi_key = ## replace this with your api key when you login\nCopyAfter updating your configuration, restart/reload your editor, type something and check your dashboard",
    "summary": {
      "en": "**Summary:**\n\nWakaTime allows you to observe your coding habits in real time and offers developer dashboards for insights. You can try it for free.\n\n**How to Set It Up:**\n1. Install WakaTime plugins for your code editor.\n2. Find the `~/.wakatime.cfg` file on your computer (it may be hidden on Windows).\n3. Update this file with your API key from the WakaTime dashboard and set the `api_url` to `https://api.wakana.io/api`.\n\n**After Configuration:**\n- Open your code editor and start typing.\n- Check your WakaTime dashboard to see if your stats appear.\n- Look in the plugins section to see collected data.\n\n**Sample Configuration:**\n```\n[settings]\napi_url = https://api.wakana.io/api\napi_key = ## replace this with your API key\n```\nAfter you save your changes, restart your editor, type something, and check the dashboard again.",
      "ko": "WakaTime은 실시간으로 코딩 습관을 관찰할 수 있게 해주며, 개발자 대시보드를 통해 통찰력을 제공합니다. 무료로 사용해볼 수 있습니다.\n\n설정 방법은 다음과 같습니다. 먼저, 코드 편집기에 WakaTime 플러그인을 설치합니다. 그런 다음, 컴퓨터에서 `~/.wakatime.cfg` 파일을 찾아야 합니다. 이 파일은 Windows에서는 숨겨져 있을 수 있습니다. 이 파일을 열고 WakaTime 대시보드에서 받은 API 키로 업데이트한 후, `api_url`을 `https://api.wakana.io/api`로 설정합니다.\n\n구성이 완료되면 코드 편집기를 열고 코딩을 시작합니다. WakaTime 대시보드에서 통계가 나타나는지 확인합니다. 수집된 데이터는 플러그인 섹션에서 확인할 수 있습니다.\n\n샘플 구성은 다음과 같습니다. \n[settings] \napi_url = https://api.wakana.io/api \napi_key = ## 여기에 API 키를 입력하세요 \n\n변경 사항을 저장한 후 편집기를 재시작하고, 무언가를 입력한 다음 대시보드를 다시 확인합니다.",
      "ja": null
    }
  },
  {
    "id": "e762d69afa7de586",
    "title": {
      "en": "Scientists Discover New Heavy-Metal Molecule 'Berkelocene'",
      "ko": "새로운 중금속 분자 '버켈로신' 발견!",
      "ja": null
    },
    "type": "story",
    "url": "https://newscenter.lbl.gov/2025/03/11/scientists-discover-new-heavy-metal-molecule-berkelocene/",
    "score": 126,
    "by": "gmays",
    "time": 1742821658,
    "content": "Key Takeaways\n\nScientists have discovered “berkelocene,” the first organometallic molecule to be characterized containing the heavy element berkelium.\nThe extremely oxygen- and water-sensitive complex was formed from 0.3 milligram of berkelium-249 using specialized facilities for handling air-sensitive and radioactive materials.\nThe breakthrough disrupts long-held theories about the chemistry of the elements that follow uranium in the periodic table.\n\nA research team led by the Department of Energy’s Lawrence Berkeley National Laboratory (Berkeley Lab) has discovered “berkelocene,” the first organometallic molecule to be characterized containing the heavy element berkelium.\nOrganometallic molecules, which consist of a metal ion surrounded by a carbon-based framework, are relatively common for early actinide elements like uranium (atomic number 92) but are scarcely known for later actinides like berkelium (atomic number 97).\n“This is the first time that evidence for the formation of a chemical bond between berkelium and carbon has been obtained. The discovery provides new understanding of how berkelium and other actinides behave relative to their peers in the periodic table,” said Stefan Minasian, a scientist in Berkeley Lab’s Chemical Sciences Division and one of four co-corresponding authors of a new study published in the journal Science.\nA heavy metal molecule with Berkeley roots\nBerkelium is one 0f 15 actinides in the periodic table’s f-block. One row above the actinides are the lanthanides.\nThe pioneering nuclear chemist Glenn Seaborg discovered berkelium at Berkeley Lab in 1949. It would become just one of many achievements that led to his winning the 1951 Nobel Prize in Chemistry with fellow Berkeley Lab scientist Edwin McMillan for their discoveries in the chemistry of the transuranium elements.\n“This is the first time that evidence for the formation of a chemical bond between berkelium and carbon has been obtained.”\n– Stefan Minasian, Chemical Sciences Division staff scientist\nFor many years, the Heavy Element Chemistry group in Berkeley Lab’s Chemical Sciences Division has been dedicated to preparing organometallic compounds of the actinides, because these molecules typically have high symmetries and form multiple covalent bonds with carbon, making them useful for observing the unique electronic structures of the actinides.\n“When scientists study higher symmetry structures, it helps them understand the underlying logic that nature is using to organize matter at the atomic level,” Minasian said.\nFrom left: Dominic Russo, Amy Price, Alyssa Gaiser, Polly Arnold, Jacob Branson, and Jennifer Wacker at Berkeley Lab’s Heavy Element Research Laboratory. They are co-authors on a new study published in Science, which reported their discovery of the heavy-metal molecule berkelocene. (Credit: Stefan Minasian/Berkeley Lab)\nBut berkelium is not easy to study because it is highly radioactive. And only very minute amounts of this synthetic heavy element are produced globally every year. Adding to the difficulty, organometallic molecules are extremely air-sensitive and can be pyrophoric.\n“Only a few facilities around the world can protect both the compound and the worker while managing the combined hazards of a highly radioactive material that reacts vigorously with the oxygen and moisture in air,” said Polly Arnold, a co-corresponding author on the paper who is a UC Berkeley professor of chemistry and director of Berkeley Lab’s Chemical Sciences Division.\nBreaking down the berkelium barrier\nSo Minasian, Arnold, and co-corresponding author Rebecca Abergel, a UC Berkeley associate professor of nuclear engineering and of chemistry who leads the Heavy Element Chemistry Group at Berkeley Lab, assembled a team to overcome these obstacles.\nAt Berkeley Lab’s Heavy Element Research Laboratory, the team custom-designed new gloveboxes enabling air-free syntheses with highly radioactive isotopes. Then, with just 0.3 milligram of berkelium-249, the researchers conducted single-crystal X-ray diffraction experiments. The isotope that was acquired by the team was initially distributed from the National Isotope Development Center, which is managed by the DOE Isotope Program at Oak Ridge National Laboratory.\n\n    The results showed a symmetrical structure with the berkelium atom sandwiched between two 8-membered carbon rings. The researchers named the molecule “berkelocene,” because its structure is analogous to a uranium organometallic complex called “uranocene.” (UC Berkeley chemists Andrew Streitwieser and Kenneth Raymond discovered uranocene in the late 1960s.)\nIn an unexpected finding, electronic structure calculations performed by co-corresponding author Jochen Autschbach at the University of Buffalo revealed that the berkelium atom at the center of the berkelocene structure has a tetravalent oxidation state (positive charge of +4), which is stabilized by the berkelium–carbon bonds.\n“Traditional understanding of the periodic table suggests that berkelium would behave like the lanthanide terbium,” said Minasian.\n“But the berkelium ion is much happier in the +4 oxidation state than the other f-block ions we expected it to be most like,” Arnold said.\nThe researchers say that more accurate models showing how actinide behavior changes across the periodic table are needed to solve problems related to long-term nuclear waste storage and remediation. “This clearer portrait of later actinides like berkelium provides a new lens into the behavior of these fascinating elements,” Abergel said.\nThis work was supported by the DOE Office of Science.\n###\nLawrence Berkeley National Laboratory (Berkeley Lab) is committed to groundbreaking research focused on discovery science and solutions for abundant and reliable energy supplies. The lab’s expertise spans materials, chemistry, physics, biology, earth and environmental science, mathematics, and computing. Researchers from around the world rely on the lab’s world-class scientific facilities for their own pioneering research. Founded in 1931 on the belief that the biggest problems are best addressed by teams, Berkeley Lab and its scientists have been recognized with 16 Nobel Prizes. Berkeley Lab is a multiprogram national laboratory managed by the University of California for the U.S. Department of Energy’s Office of Science.\nDOE’s Office of Science is the single largest supporter of basic research in the physical sciences in the United States, and is working to address some of the most pressing challenges of our time. For more information, please visit energy.gov/science.\n\n        Tags:\n\n                  Materials",
    "summary": {
      "en": "Scientists have successfully created and characterized \"berkelocene,\" the first organometallic molecule containing the heavy element berkelium (atomic number 97). This discovery challenges old theories about how elements after uranium behave chemically. \n\nThe research team from the Lawrence Berkeley National Laboratory used a tiny amount (0.3 milligram) of berkelium-249, which is highly radioactive and sensitive to air and moisture. They developed special equipment to safely study this compound. \n\nBerkelocene consists of a berkelium atom between two carbon rings, and it exhibits a unique chemical bond with carbon, suggesting that berkelium behaves differently than previously thought, particularly with a stable positive charge of +4. \n\nThis finding could help improve models for understanding the chemistry of actinides, which is important for nuclear waste management. The team's work was supported by the U.S. Department of Energy.",
      "ko": "과학자들이 중금속 원소인 버켈륨(원자번호 97)을 포함한 최초의 유기금속 분자인 '버켈로센'을 성공적으로 합성하고 특성을 규명했습니다. 이 발견은 우라늄 이후의 원소들이 화학적으로 어떻게 행동하는지에 대한 기존 이론에 도전하는 결과입니다.\n\n로렌스 버클리 국립 연구소의 연구팀은 방사능이 강하고 공기와 습기에 민감한 버켈륨-249를 소량(0.3밀리그램) 사용했습니다. 이 화합물을 안전하게 연구하기 위해 특별한 장비를 개발했습니다.\n\n버켈로센은 두 개의 탄소 고리 사이에 버켈륨 원자가 위치하고 있으며, 탄소와 독특한 화학 결합을 형성합니다. 이는 버켈륨이 이전에 생각했던 것과는 다르게, 특히 +4의 안정적인 양전하를 가지고 행동한다는 것을 시사합니다.\n\n이 발견은 방사성 원소인 악티늄의 화학을 이해하는 모델을 개선하는 데 도움이 될 수 있으며, 이는 핵 폐기물 관리에 중요합니다. 연구팀의 작업은 미국 에너지부의 지원을 받았습니다.",
      "ja": null
    }
  },
  {
    "id": "7bd15553d47fa6e5",
    "title": {
      "en": "Todo.txt",
      "ko": "투두.txt",
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/todotxt/todo.txt",
    "score": 9,
    "by": "1sembiyan",
    "time": 1743179110,
    "content": "todo.txt format\n\nA complete primer on the whys and hows of todo.txt.\nThe first and most important rule of todo.txt:\n\nA single line in your todo.txt text file represents a single task.\n\nWhy plain text?\nPlain text is software and operating system agnostic. It's searchable, portable, lightweight, and easily manipulated. It's unstructured. It works when someone else's web server is down or your Outlook .PST file is corrupt. There's no exporting and importing, no databases or tags or flags or stars or prioritizing or insert company name here-induced rules on what you can and can't do with it.\nThe 3 axes of an effective todo list\nUsing special notation in todo.txt, you can create a list that's sliceable by 3 key axes.\nPriority\nYour todo list should be able to tell you what's the next most important thing for you to get done - either by project or by context or overall. You can optionally assign tasks a priority that'll bubble them up to the top of the list.\nProject\nThe only way to move a big project forward is to tackle a small subtask associated with it. Your todo.txt should be able to list out all the tasks specific to a project.\nIn order to move along a project like \"Cleaning out the garage,\" my task list should give me the next logical action to take in order to move that project along. \"Clean out the garage\" isn't a good todo item; but \"Call Goodwill to schedule pickup\" in the \"Clean out garage\" project is.\nContext\nGetting Things Done author David Allen suggests splitting up your task lists by context - ie, the place and situation where you'll work on the job. Messages that you need to send go in the @email context; calls to be made @phone, household projects @home.\nThat way, when you've got a few minutes in the car with your cell phone, you can easily check your @phone tasks and make a call or two while you have the opportunity.\nThis is all possible inside todo.txt.\ntodo.txt format rules\n\nYour todo.txt is a plain text file. To take advantage of structured task metadata like priority, projects, context, creation, and completion date, there are a few simple but flexible file format rules.\nPhilosophically, the todo.txt file format has two goals:\n\nThe file contents should be human-readable without requiring any tools other than a plain text viewer or editor.\nA user can manipulate the file contents in a plain text editor in sensible, expected ways. For example, a text editor that can sort lines alphabetically should be able to sort your task list in a meaningful way.\n\nThese two goals are why, for example, lines start with priority and/or dates, so that they are easily sorted by priority or time, and completed items are marked with an x, which both sorts at the bottom of an alphabetical list and looks like a filled-in checkbox.\nHere are the rest.\nIncomplete Tasks: 3 Format Rules\nThe beauty of todo.txt is that it's completely unstructured; the fields you can attach to each task are only limited by your imagination. To get started, use special notation to indicate task context (e.g. @phone ), project (e.g. +GarageSale ) and priority (e.g. (A) ).\nA todo.txt file might look like the following:\n(A) Thank Mom for the meatballs @phone\n(B) Schedule Goodwill pickup +GarageSale @phone\nPost signs around the neighborhood +GarageSale\n@GroceryStore pies\n\nA search and filter for the @phone contextual items would output:\n(A) Thank Mom for the meatballs @phone\n(B) Schedule Goodwill pickup +GarageSale @phone\n\nTo just see the +GarageSale project items would output:\n(B) Schedule Goodwill pickup +GarageSale @phone\nPost signs around the neighborhood +GarageSale\n\nThere are three formatting rules for current todo's.\nRule 1: If priority exists, it ALWAYS appears first.\nThe priority is an uppercase character from A-Z enclosed in parentheses and followed by a space.\nThis task has a priority:\n(A) Call Mom\n\nThese tasks do not have any priorities:\nReally gotta call Mom (A) @phone @someday\n(b) Get back to the boss\n(B)->Submit TPS report\n\nRule 2: A task's creation date may optionally appear directly after priority and a space.\nIf there is no priority, the creation date appears first. If the creation date exists, it should be in the format YYYY-MM-DD.\nThese tasks have creation dates:\n2011-03-02 Document +TodoTxt task format\n(A) 2011-03-02 Call Mom\n\nThis task doesn't have a creation date:\n(A) Call Mom 2011-03-02\n\nRule 3: Contexts and Projects may appear anywhere in the line after priority/prepended date.\n\nA context is preceded by a single space and an at-sign (@).\nA project is preceded by a single space and a plus-sign (+).\nA project or context contains any non-whitespace character.\nA task may have zero, one, or more than one projects and contexts included in it.\n\nFor example, this task is part of the +Family and +PeaceLoveAndHappiness projects as well as the @iphone and @phone contexts:\n(A) Call Mom +Family +PeaceLoveAndHappiness @iphone @phone\n\nThis task has no contexts in it:\nEmail SoAndSo at soandso@example.com\n\nThis task has no projects in it:\nLearn how to add 2+2\n\nComplete Tasks: 2 Format Rules\nTwo things indicate that a task has been completed.\nRule 1: A completed task starts with an lowercase x character (x).\nIf a task starts with an x (case-sensitive and lowercase) followed directly by a space, it is marked as complete.\nThis is a complete task:\nx 2011-03-03 Call Mom\n\nThese are not complete tasks.\nxylophone lesson\nX 2012-01-01 Make resolutions\n(A) x Find ticket prices\n\nWe use a lowercase x so that completed tasks sort to the bottom of the task list using standard sort tools.\nRule 2: The date of completion appears directly after the x, separated by a space.\nFor example:\nx 2011-03-02 2011-03-01 Review Tim's pull request +TodoTxtTouch @github\n\nIf you’ve prepended the creation date to your task, on completion it will appear directly after the completion date. This is so your completed tasks sort by date using standard sort tools. Many Todo.txt clients discard priority on task completion. To preserve it, use the key:value format described below (e.g. pri:A)\nWith the completed date (required), if you've used the prepended date (optional), you can calculate how many days it took to complete a task.\nAdditional File Format Definitions\nTool developers may define additional formatting rules for extra metadata.\nDevelopers should use the format key:value to define additional metadata (e.g. due:2010-01-02 as a due date).\nBoth key and value must consist of non-whitespace characters, which are not colons. Only one colon separates the key and value.",
    "summary": {
      "en": "**Summary of todo.txt Format**\n\nThe todo.txt format is a simple way to manage tasks using a plain text file. Here are the key points:\n\n1. **Basic Principle**: Each line in the todo.txt file represents one task.\n\n2. **Why Use Plain Text?**: \n   - It's compatible with any software and operating system.\n   - It's easy to search, portable, and lightweight.\n   - No need for complex features like databases or tags.\n\n3. **Three Key Axes for an Effective Todo List**:\n   - **Priority**: Assign priorities to tasks to identify what’s most important.\n   - **Project**: Break down large projects into smaller, actionable tasks.\n   - **Context**: Organize tasks by context (e.g., @phone for calls, @home for household tasks) to make it easier to find what you can do in different situations.\n\n4. **Formatting Rules**:\n   - Tasks can include priority (e.g., (A)), creation date (YYYY-MM-DD), contexts (e.g., @phone), and projects (e.g., +GarageSale).\n   - Completed tasks start with a lowercase 'x' followed by the completion date.\n\n5. **Examples**:\n   - A task with priority and context: (A) Call Mom @phone\n   - A completed task: x 2023-10-01 Review report\n\nThis format allows for easy organization and manipulation of tasks while keeping everything human-readable.",
      "ko": "todo.txt 형식은 일반 텍스트 파일을 사용하여 작업을 관리하는 간단한 방법입니다. 이 형식의 주요 내용은 다음과 같습니다.\n\n기본 원칙은 todo.txt 파일의 각 줄이 하나의 작업을 나타낸다는 것입니다. 일반 텍스트를 사용하는 이유는 여러 가지가 있습니다. 모든 소프트웨어와 운영 체제와 호환되며, 검색이 용이하고 휴대성이 뛰어나며 가볍습니다. 데이터베이스나 태그와 같은 복잡한 기능이 필요하지 않습니다.\n\n효과적인 할 일 목록을 위해 세 가지 주요 축이 있습니다. 첫째, 우선순위입니다. 작업에 우선순위를 부여하여 가장 중요한 것을 식별할 수 있습니다. 둘째, 프로젝트입니다. 큰 프로젝트를 더 작고 실행 가능한 작업으로 나눌 수 있습니다. 셋째, 맥락입니다. 작업을 맥락에 따라 정리하여(@전화는 전화 작업, @집은 가사 작업) 다양한 상황에서 수행할 수 있는 작업을 쉽게 찾을 수 있습니다.\n\n형식 규칙으로는 작업에 우선순위(예: (A)), 생성 날짜(YYYY-MM-DD), 맥락(예: @전화), 프로젝트(예: +차고 세일)를 포함할 수 있습니다. 완료된 작업은 소문자 'x'로 시작하고 완료 날짜가 뒤따릅니다.\n\n예를 들어, 우선순위와 맥락이 포함된 작업은 (A) 엄마에게 전화하기 @전화입니다. 완료된 작업의 예는 x 2023-10-01 보고서 검토입니다.\n\n이 형식은 작업을 쉽게 정리하고 조작할 수 있게 하면서도 모든 내용을 사람이 읽기 쉽게 유지합니다.",
      "ja": null
    }
  },
  {
    "id": "a5620036423b6eb7",
    "title": {
      "en": "Blasting Past WebP - An analysis of the NSO BLASTPASS iMessage exploit",
      "ko": "웹P를 넘어서: NSO 블래스트패스 분석",
      "ja": null
    },
    "type": "story",
    "url": "https://googleprojectzero.blogspot.com/2025/03/blasting-past-webp.html",
    "score": 253,
    "by": "el_duderino",
    "time": 1743079784,
    "content": "Project Zero\n\nNews and updates from the Project Zero team at Google\n\nWednesday, March 26, 2025\n\nBlasting Past Webp\n\n@import url(https://themes.googleusercontent.com/fonts/css?kit=XGMkxXUZTA64h2imyzu79g);.lst-kix_t2u4j4vhkrnm-3>li:before{content:\"\\0025cf   \"}.lst-kix_t2u4j4vhkrnm-0>li:before{content:\"\\0025cf   \"}.lst-kix_t2u4j4vhkrnm-4>li:before{content:\"\\0025cb   \"}.lst-kix_t2u4j4vhkrnm-7>li:before{content:\"\\0025cb   \"}ul.lst-kix_t2u4j4vhkrnm-8{list-style-type:none}ul.lst-kix_t2u4j4vhkrnm-6{list-style-type:none}ul.lst-kix_t2u4j4vhkrnm-7{list-style-type:none}ul.lst-kix_t2u4j4vhkrnm-4{list-style-type:none}.lst-kix_t2u4j4vhkrnm-5>li:before{content:\"\\0025a0   \"}ul.lst-kix_t2u4j4vhkrnm-5{list-style-type:none}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}ul.lst-kix_t2u4j4vhkrnm-2{list-style-type:none}.lst-kix_t2u4j4vhkrnm-6>li:before{content:\"\\0025cf   \"}ul.lst-kix_t2u4j4vhkrnm-3{list-style-type:none}ul.lst-kix_t2u4j4vhkrnm-0{list-style-type:none}ul.lst-kix_t2u4j4vhkrnm-1{list-style-type:none}.lst-kix_t2u4j4vhkrnm-8>li:before{content:\"\\0025a0   \"}.lst-kix_t2u4j4vhkrnm-1>li:before{content:\"\\0025cb   \"}.lst-kix_t2u4j4vhkrnm-2>li:before{content:\"\\0025a0   \"}ol{margin:0;padding:0}table td,table th{padding:0}.XQFzMDWmii-c30{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:234pt;border-top-color:#000000;border-bottom-style:solid}.XQFzMDWmii-c36{padding-top:0pt;border-top-width:0pt;padding-bottom:0pt;line-height:1.5;border-top-style:solid;background-color:#ffffff;border-bottom-width:0pt;border-bottom-style:solid;orphans:2;widows:2;text-align:left}.XQFzMDWmii-c40{padding-top:14pt;padding-bottom:4pt;line-height:1.5;page-break-after:avoid;orphans:2;widows:2;text-align:left}.XQFzMDWmii-c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:\"Arial\";font-style:normal}.XQFzMDWmii-c22{color:#434343;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:\"Arial\";font-style:normal}.XQFzMDWmii-c35{color:#666666;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:\"Arial\";font-style:normal}.XQFzMDWmii-c18{color:#434343;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:\"Arial\";font-style:normal}.XQFzMDWmii-c38{padding-top:0pt;padding-bottom:16pt;line-height:1.5;page-break-after:avoid;orphans:2;widows:2;text-align:left}.XQFzMDWmii-c12{padding-top:16pt;padding-bottom:4pt;line-height:1.5;page-break-after:avoid;orphans:2;widows:2;text-align:left}.XQFzMDWmii-c4{padding-top:0pt;padding-bottom:0pt;line-height:1.5;orphans:2;widows:2;text-align:left}.XQFzMDWmii-c10{color:#000000;text-decoration:none;vertical-align:baseline;font-size:11pt;font-style:normal}.XQFzMDWmii-c29{color:#b80672;text-decoration:none;vertical-align:baseline;font-size:11pt;font-style:normal}.XQFzMDWmii-c11{padding-top:0pt;padding-bottom:0pt;line-height:1.5;text-align:left;margin-right:-72pt}.XQFzMDWmii-c21{color:#000000;vertical-align:baseline;font-size:11pt;font-style:normal}.XQFzMDWmii-c17{color:#000000;text-decoration:none;vertical-align:baseline;font-style:normal}.XQFzMDWmii-c1{font-size:9pt;font-family:\"Roboto Mono\";color:#188038;font-weight:400}.XQFzMDWmii-c42{color:#666666;vertical-align:baseline;font-size:15pt;font-style:normal}.XQFzMDWmii-c45{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.XQFzMDWmii-c3{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.XQFzMDWmii-c41{border-spacing:0;border-collapse:collapse;margin-right:auto}.XQFzMDWmii-c37{font-weight:400;text-decoration:none;font-family:\"Arial\"}.XQFzMDWmii-c2{color:#1967d2;font-weight:400;font-family:\"Roboto Mono\"}.XQFzMDWmii-c26{color:#000000;vertical-align:baseline;font-size:11pt}.XQFzMDWmii-c5{color:#37474f;font-weight:400;font-family:\"Roboto Mono\"}.XQFzMDWmii-c39{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.XQFzMDWmii-c13{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;text-decoration:underline}.XQFzMDWmii-c16{background-color:#00ff00;font-style:italic}.XQFzMDWmii-c9{font-weight:400;font-family:\"Courier New\"}.XQFzMDWmii-c23{font-weight:700;font-family:\"Courier New\"}.XQFzMDWmii-c6{font-weight:400;font-family:\"Roboto Mono\"}.XQFzMDWmii-c19{background-color:#ffff00}.XQFzMDWmii-c32{background-color:#00ff00}.XQFzMDWmii-c15{background-color:#ff9900}.XQFzMDWmii-c24{font-style:italic}.XQFzMDWmii-c25{font-weight:700}.XQFzMDWmii-c20{font-size:10pt}.XQFzMDWmii-c8{font-size:9pt}.XQFzMDWmii-c43{padding-left:0pt}.XQFzMDWmii-c44{color:#c5221f}.XQFzMDWmii-c28{height:0pt}.XQFzMDWmii-c31{background-color:#f4cccc}.XQFzMDWmii-c14{background-color:#ff00ff}.XQFzMDWmii-c34{margin-left:36pt}.XQFzMDWmii-c27{color:#188038}.XQFzMDWmii-c7{height:11pt}.XQFzMDWmii-c33{background-color:#00ffff}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:\"Arial\";line-height:1.5;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:\"Arial\";line-height:1.5;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:\"Arial\"}p{margin:0;color:#000000;font-size:11pt;font-family:\"Arial\"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:\"Arial\";line-height:1.5;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:\"Arial\";line-height:1.5;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:\"Arial\";line-height:1.5;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:\"Arial\";line-height:1.5;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:\"Arial\";line-height:1.5;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:\"Arial\";line-height:1.5;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}\n An analysis of the NSO BLASTPASS iMessage exploit\n Posted by Ian Beer, Google Project Zero\n\n On September 7, 2023 Apple issuedan out-of-band security update for iOS:\n\n Around the same time on September 7th 2023, Citizen Lab published a blog postlinking the two CVEs fixed in iOS 16.6.1 to an \"NSO Group Zero-Click, Zero-Day exploit captured in the wild\":\n\n \"[The target was] an individual employed by a Washington DC-based civil society organization with international offices...\n\n The exploit chain was capable of compromising iPhones running the latest version of iOS (16.6) without any interaction from the victim.\n\n The exploit involved PassKit attachments containing malicious images sent from an attacker iMessage account to the victim.\"\n\n The day before, on September 6th 2023, Apple reported a vulnerability to the WebP project, indicating in the report that they planned to ship a custom fix for Apple customers the next day.\n\n The WebP team posted their first proposed fixin the public git repo the next day, and five days after that on September 12th Google released a new Chrome stable releasecontaining the WebP fix. Both Apple and Google marked the issue as exploited in the wild, alerting other integrators of WebP that they should rapidly integrate the fix as well as causing the security research community to take a closer look...\n\n A couple of weeks later on September 21st 2023, former Project Zero team lead Ben Hawkes (in collaboration with @mistymntncop) published the first detailed writeupof the root cause of the vulnerability on the IsoscelesBlog. A couple of months later, on November 3rd, a group called Dark Navypublished their first blog post: a two-part analysis (Part 1- Part 2) of the WebP vulnerability and a proof-of-concept exploittargeting Chrome (CVE-2023-4863).\n\n Whilst the Isosceles and Dark Navy posts explained the underlying memory corruption vulnerability in great detail, they were unable to solve another fascinating part of the puzzle: just how exactly do you land an exploit for this vulnerability in a one-shot, zero-click setup? As we'll soon see, the corruption primitive is very limited. Without access to the samples it was almost impossible to know.\n\n In mid-November, in collaboration with Amnesty International Security Lab, I was able to obtain a number of BLASTPASS PKPasssample files as well as crash logs from failed exploit attempts.\n\n This blog post covers my analysis of those samples and the journey to figure out how one of NSO's recent zero-click iOS exploits really worked. For me that journey began by immediately taking three months of paternity leave, and resumed in March 2024 where this story begins:Setting the scene\n For a detailed analysis of the root-cause of the WebP vulnerability and the primitive it yields, I recommend first reading the three blog posts I mentioned earlier (Isosceles, Dark Navy 1, Dark Navy 2.) I won't restate their analyses here (both because you should read their original work, and because it's quite complicated!) Instead I'll briefly discuss WebP and the corruption",
    "summary": {
      "en": "**Project Zero Update - March 26, 2025**\n\nThis update discusses a significant iMessage exploit linked to NSO Group, discovered in September 2023. Key points include:\n\n- **Exploit Details**: A zero-click exploit compromised iPhones running the latest iOS (16.6) without user interaction, using malicious images sent via iMessage.\n  \n- **Apple's Response**: On September 7, 2023, Apple released a security update to address vulnerabilities in iOS, which were exploited in the wild.\n\n- **WebP Vulnerability**: Around the same time, a vulnerability in the WebP image format was identified. Apple and Google quickly developed and released fixes, highlighting the urgency of the issue.\n\n- **Research Collaboration**: The Project Zero team, in collaboration with Amnesty International, analyzed exploit samples and crash logs to understand the mechanics of NSO's zero-click exploit.\n\n- **Further Reading**: The post encourages readers to consult previous analyses from Isosceles and Dark Navy for detailed technical insights into the WebP vulnerability.\n\nThis summary captures the essence of the findings and ongoing research surrounding the exploit and vulnerabilities affecting iOS and WebP.",
      "ko": "2025년 3월 26일 프로젝트 제로 업데이트에서는 2023년 9월에 발견된 NSO 그룹과 관련된 중요한 iMessage 취약점에 대해 다루고 있습니다. 주요 내용은 다음과 같습니다.\n\n이 취약점은 사용자의 상호작용 없이도 최신 iOS(16.6)를 실행하는 아이폰을 감염시킬 수 있는 제로 클릭 취약점으로, 악성 이미지를 iMessage를 통해 전송하여 발생합니다. \n\n애플은 2023년 9월 7일, 이러한 취약점을 해결하기 위한 보안 업데이트를 발표했습니다. 이 업데이트는 실제로 악용되고 있는 취약점을 수정하기 위한 것이었습니다.\n\n같은 시기에 WebP 이미지 포맷에서도 취약점이 발견되었습니다. 애플과 구글은 신속하게 수정 작업을 진행하여 문제를 해결했습니다. 이는 문제의 긴급성을 잘 보여줍니다.\n\n프로젝트 제로 팀은 국제앰네스티와 협력하여 NSO의 제로 클릭 취약점의 작동 방식을 이해하기 위해 취약점 샘플과 크래시 로그를 분석했습니다.\n\n독자들은 WebP 취약점에 대한 자세한 기술적 통찰을 원한다면 Isosceles와 Dark Navy의 이전 분석을 참고할 것을 권장합니다. 이 요약은 iOS와 WebP에 영향을 미치는 취약점 및 관련 연구의 핵심 내용을 담고 있습니다.",
      "ja": null
    }
  },
  {
    "id": "0da8cdfb63befb78",
    "title": {
      "en": "OpenAI adds MCP support to Agents SDK",
      "ko": "OpenAI, 에이전트 SDK에 MCP 지원 추가",
      "ja": null
    },
    "type": "story",
    "url": "https://openai.github.io/openai-agents-python/mcp/",
    "score": 786,
    "by": "gronky_",
    "time": 1743015329,
    "content": "Model context protocol (MCP)\nThe Model context protocol (aka MCP) is a way to provide tools and context to the LLM. From the MCP docs:\n\nMCP is an open protocol that standardizes how applications provide context to LLMs. Think of MCP like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect your devices to various peripherals and accessories, MCP provides a standardized way to connect AI models to different data sources and tools.\n\nThe Agents SDK has support for MCP. This enables you to use a wide range of MCP servers to provide tools to your Agents.\nMCP servers\nCurrently, the MCP spec defines two kinds of servers, based on the transport mechanism they use:\n\nstdio servers run as a subprocess of your application. You can think of them as running \"locally\".\nHTTP over SSE servers run remotely. You connect to them via a URL.\n\nYou can use the MCPServerStdio and MCPServerSse classes to connect to these servers.\nFor example, this is how you'd use the official MCP filesystem server.\nasync with MCPServerStdio(\n    params={\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", samples_dir],\n    }\n) as server:\n    tools = await server.list_tools()\n\nUsing MCP servers\nMCP servers can be added to Agents. The Agents SDK will call list_tools() on the MCP servers each time the Agent is run. This makes the LLM aware of the MCP server's tools. When the LLM calls a tool from an MCP server, the SDK calls call_tool() on that server.\nagent=Agent(\n    name=\"Assistant\",\n    instructions=\"Use the tools to achieve the task\",\n    mcp_servers=[mcp_server_1, mcp_server_2]\n)\n\nCaching\nEvery time an Agent runs, it calls list_tools() on the MCP server. This can be a latency hit, especially if the server is a remote server. To automatically cache the list of tools, you can pass cache_tools_list=True to both MCPServerStdio and MCPServerSse. You should only do this if you're certain the tool list will not change.\nIf you want to invalidate the cache, you can call invalidate_tools_cache() on the servers.\nEnd-to-end examples\nView complete working examples at examples/mcp.\nTracing\nTracing automatically captures MCP operations, including:\n\nCalls to the MCP server to list tools\nMCP-related info on function calls",
    "summary": {
      "en": "**Summary of Model Context Protocol (MCP)**\n\nThe Model Context Protocol (MCP) is a standardized way to connect AI applications with tools and data sources, similar to how a USB-C port connects devices. \n\n### Key Points:\n\n- **Purpose**: MCP provides a framework for applications to provide context to large language models (LLMs).\n- **Server Types**:\n  - **Stdio Servers**: Run locally as a subprocess.\n  - **HTTP over SSE Servers**: Operate remotely and are accessed via a URL.\n  \n- **Using MCP with Agents**: The Agents SDK can connect to MCP servers, allowing LLMs to utilize tools available on these servers. Each time an agent runs, it retrieves the list of tools from the MCP servers.\n\n- **Caching**: To reduce delays when accessing remote servers, you can enable caching of the tool list. If the tools change, you can invalidate the cache.\n\n- **Tracing**: MCP includes features to automatically track operations, like listing tools and function calls.\n\nFor detailed examples of how to use MCP, you can refer to the provided examples directory.",
      "ko": "모델 컨텍스트 프로토콜(MCP)은 AI 애플리케이션과 도구, 데이터 소스를 연결하는 표준화된 방법입니다. 이는 USB-C 포트가 장치를 연결하는 방식과 유사합니다.\n\nMCP의 주요 목적은 애플리케이션이 대형 언어 모델(LLM)에 맥락을 제공할 수 있는 프레임워크를 제공하는 것입니다. 서버 유형으로는 두 가지가 있습니다. 첫 번째는 로컬에서 서브프로세스로 실행되는 스탠다드 입력 출력(Stdio) 서버입니다. 두 번째는 원격으로 운영되며 URL을 통해 접근하는 HTTP over SSE 서버입니다.\n\nMCP를 에이전트와 함께 사용할 수 있습니다. 에이전트 SDK는 MCP 서버에 연결하여 LLM이 해당 서버에서 제공하는 도구를 활용할 수 있게 합니다. 에이전트가 실행될 때마다 MCP 서버에서 도구 목록을 가져옵니다.\n\n원격 서버에 접근할 때 지연을 줄이기 위해 도구 목록의 캐싱을 활성화할 수 있습니다. 도구가 변경되면 캐시를 무효화할 수 있습니다.\n\nMCP는 도구 목록과 함수 호출과 같은 작업을 자동으로 추적하는 기능도 포함하고 있습니다. MCP 사용에 대한 자세한 예시는 제공된 예제 디렉토리를 참조하면 됩니다.",
      "ja": null
    }
  },
  {
    "id": "00fa45d791273b04",
    "title": {
      "en": "The Hole Story: How Woodpeckers Make Homes for the Rest of the Forest",
      "ko": "딱따구리의 집짓기 비밀",
      "ja": null
    },
    "type": "story",
    "url": "https://www.allaboutbirds.org/news/hole-story-how-woodpeckers-make-homes-forest/",
    "score": 3,
    "by": "hn_acker",
    "time": 1743183144,
    "content": "The Hole Story: How Woodpeckers Make Homes for the Rest of the Forest\n              Woodpecker nest holes are valuable pieces of real estate that may be used by hundreds of other species over many years. Researchers are using the concept of \"nest webs\" to understand how this valuable resource passes from one owner to the next.By Marc DevokaitisMarch 24, 2025\n\n        Northern Flicker by Michael Quinton / Minden Pictures.\n\n                            Share:\n\n          Woodpecker nest holes are valuable pieces of real estate that may be used by hundreds of other species over many years. Researchers are using the concept of \"nest webs\" to understand how this valuable resource passes from one owner to the next.\n      More From Living BirdLiving Bird Spring 2025—Table Of ContentsLiving Bird Magazine—Latest IssueLiving Bird Magazine Archives\n\nFrom theSpring 2025 issueofLiving Birdmagazine.Subscribe now.\n\nWhen night falls in the forest, most birds tuck into nooks where they can feel secure—the V-shaped intersection of two tree boughs, a cluster of dense branches. Some may take to the eaves of nearby houses. Brown Creepers find a piece of shaggy bark to wedge their bodies under.\n\nAnd many birds—sometimes more than half the species in a given woodland—stow themselves safely into a place we humans rarely (and barely) get to see: inside the hole of a tree.\n\nWhether it’s a fragment of forest bordering an urban neighborhood, or an old-growth stand that’s been regenerating itself for millennia, tree cavities are a common and crucial part of habitat in every forest landscape.\n\nWhen it comes to a cozy home, few places can deliver like a tree cavity, says Kevin McGowan, instructor for the Cornell Lab of Ornithology Bird Academy course The Wonderful World of Woodpeckers. McGowan says a big part of recognizing the contribution of woodpeckers to an ecosystem begins with understanding the value of a hole.\n\nWoodpecker Anatomy3 Reasons Why Woodpeckers Are Great Excavators\n\n“Holes in trees make great homes for all kinds of animals,” says McGowan. “They’re off of the ground, easier to defend from predators, they stay dry when it rains. It makes them a perfect place to roost and to raise young.”\n\nA study published in the journal Diversity and Distributions in 2017 found that nearly 20% of all bird species around the world rely on tree cavities for roosting or nesting, and a subsequent analysis found that there are some areas of the world where nearly all cavities occupied by birds are made by woodpeckers.\n\nIn North America in particular, scientists are finding that the nest holes excavated within trees offer critical safe harbors used by dozens of species of birds, mammals, reptiles, and insects—all thanks to a few VIPs : Very Important (Wood)Peckers.\n\nNorthern Saw-whet Owls often nest in unused woodpecker holes. Photo by Celine Bellemare / Macaulay Library.\n\nFlicker Nests Have Widespread Appeal\n\nKathy Martin began studying woodpeckers and other cavity-nesting species in the mid-1990s, when she was a newly minted professor of forest science at the University of British Columbia. She had an idea that woodpeckers would have stories to tell about the health of forest ecosystems, and their ability to stay healthy in the face of timber harvests, at a time when logging interests were sizing up British Columbia’s vast interior forests.\n\nNorthern Flicker by Michael Quinton / Minden Pictures.\n\nThirty years ago “there was a true environmental feeling in the industry,” Martin says. “[Timber] companies were interested in having ecologists weigh in on the management of their lands—people who would tell the real story and not just say that everything is fine.”\n\nWoodpeckers were a logical group to study, she says: “The woodpeckers were … the ones that you would predict would be very sensitive to this kind of harvest,” including the selective logging of mature trees for the forest industry.\n\nOver the first two years of studying the biological communities in these mixed Douglas-fir forests in western Canada, Martin and her research team recorded 32 bird species that used tree cavities, including primary excavators like woodpeckers, which can drill into a variety of tree types in different stages of decay; weaker excavators like chickadees and nuthatches, which can only excavate when the wood is already rotting; and a bevy of secondary cavity nesters, which are birds that use preexisting cavities but don’t excavate their own. The last group includes a range of species from Wood Ducks to Northern House Wrens. More broadly, Martinestimates that around 30% of all forest bird species in North America use tree cavities at some point of their life cycle, whether nesting, hiding from predators, or just finding a warm roosting place to snuggle in on a cold winter’s night.\n\nMartin and her team also recorded cavity use by 11 species of mammals, from red squirrels to pine martens to fishers. In 1999 their study, published in The Condor, introduced the concept of a nest web—a way to describe the complex system of animals that make, enhance, and/or use tree cavities.\n\n“It was based on the idea of the food web—you have all these species that are linked together because they are sharing the same resources,” says Martin, “and just like in food webs you have your producers and your consumers.”\n\nWood Duck by Seth Honig / Macaulay Library.\n\nMountain Bluebird by Aidan Brubaker / Macaulay Library.\n\nAmerican red squirrel courtesy of Adobe Stock.\n\nAmerican Kestrel by David Olsen / Macaulay Library.\nSecondary Cavity Nesters: Dozens of species of North American birds and mammals nest or shelter in tree cavities, but don’t have the ability to excavate for themselves. In some forests these secondary cavity nesters rely on woodpeckers to carve out the holes they need for breeding and survival.\n\nIn the forests Martin studies—mixed Douglas-fir, which includes pine and spruce along with aspen, poplar, and birch trees—the most important producer turned out to be Northern Flicker, which excavated nearly 50% of tree cavities observed. According to a recent estimate from the bird-monitoring alliance Partners in Flight, Northern Flicker is one of the most abundant woodpeckers in North America. PIF estimates there are about 12 million flickers across Canada, the U.S., and Mexico.\n\n“I do think the flickers are so important because they have such a high abundance,” says Martin, “but it’s more than that.” Aside from making more holes than any other woodpeckers, Martin says they make a hole that’s big enough for many different species, but not so big that the entrance hole feels unsafe.\n\nAccording to Martin’s research, flicker cavities emerged as the most important nesting resource for songbirds such as Mountain Bluebirds, raptors such as American Kestrels, and waterfowl such as Buffleheads and Hooded Mergansers. Some bird species even evict flickers from their cavity nests.\n\n“[Flickers] are kind of wimps,” Martin says. “They don’t defend their cavities very well,” which means their nest holes are often commandeered by more aggressive birds like kestrels and starlings.\n\nNorthern House Wren by Bob Bowhay / Macaulay Library.\n\nBlack-capped Chickadee by Matt Saunders / Macaulay Library.\n\nEastern Screech-Owls by Kyle Tansley / Macaulay Library.\n\nImportantly, only around 10% of the cavities Martin and her team found were so-called natural cavities—that is, cavities not started by an excavator, such as a hole in a tree at a broken bough caused by wind or ice, then rotted out by bacteria or fungi.\n\nBut, Martin says, “a lot of these natural decay cavities that form in older trees and snags are crappy cavities … not as secure, not deep enough” for safe bird nests.\n\nIn other parts of the world, natural cavities that form without the aid of woodpeckers play a much larger role in forests. A study published in Biodiversity and Conservation in 2017 found that in temperate rainforests in Chile, 75% of nests used by secondary cavity nesters—birds such as Blue-and-White Swallow and the Patagonian Sierra Finch that don’t drill their own cavities—were located in holes caused by tree decay. The other 25% of nests were in holes excavated by primary-cavity producers such as Magellanic Woodpecker and White-throated Treerunner, a nuthatch-like species.\n\nThat pattern holds true for most places outside of North America where nest webs have been studied. According to an analysis published in Frontiers in Ecology and Evolution in 2011, avian excavators (mostly woodpeckers) produced around 77% of nesting cavities in North America, but only an average of 26% across Europe and South America, and 0% in Australasia (where woodpeckers do not occur).\n\nKristina Cockle, lead author of the study, explains the discrepancy is in part due to the difference in tree structures around the world. She says that in much of the world, forests are dominated by broadleaf trees with huge branches: “When a large branch breaks off … it has good potential to create an entrance to the already-decayed heartwood inside the tree.”\n\nIn the areas of North America and northern Europe where these kinds of studies have been done, however, “trees tend to have a single main stem, with small branches that self-prune,” she says. When weather events knock branches off this type of tree, it doesn’t tend to expose the heartwood and the tree can quickly heal the wound—which means fewer opportunities for the formation of sizable natural tree cavities.\n\nWoodpecker Nest Webs. Nest webs show the relationships between nest-site producers (cavity excavators, especially woodpeckers) and consumers (the many other birds and mammals that use them). The dark gray arrows in this illustration of a western Canada forest point from cavity producers to cavity consumers. For example, Northern Flicker holes are a main source of cavities for Mountain Bluebirds, American Kestrels, and Buffleheads. The white arrows show additional food-web relationships that can stem from the relationships in this nest web. Species, left to right: Northern Flicker, Mountain Chickadee, Bufflehead, Northern Saw-whet Owl, red squirrel, short-tailed weasel, woodrat, female Mountain Bluebird, deer mouse, Northern Flicker (flying), Pileated Woodpecker, male Mountain Bluebird, American Kestrel, Barrow’s Goldeneye. Illustration by Bartels Illustrator Lauren Richelieu.\n\nA Front-Row Seat\n\nVirginia Tech researcher Jeff Walters has had a front-row seat to watching Very Important Woodpeckers in a forest ecosystem for more than three decades.\n\nSpecifically he’s gotten familiar with the haunts and habits of the Red-cockaded Woodpecker, an iconic bird protected under the Endangered Species Act that makes its living in the longleaf pine forests that dot the landscape from Virginia to Texas. As important as woodpeckers are in the forests of western Canada, the woodpeckers of the longleaf pine forests are absolutely vital, according to Walters’s research. In a study published in The Condor in 2008, Walters and Virginia Tech PhD student Lori Blanc found that almost every single tree cavity used by birds and other creatures within a longleaf pine nest-web community in northern Florida originated with a woodpecker—432 out of 433 cavities.\n\n        hbspt.cta.load(95627, '096b8ce3-0e2d-46c5-bbf7-12de3323c8da', {});\n\nIn conifer systems, Walters says, there is “more dependence on excavated cavities from woodpeckers compared to hardwood or tropical forests. Pine trees just don’t produce a lot of … natural holes on their own.” Pines also produce resin that may help protect their wood from infection following injury.\n\nAdditionally, he says that longleaf pine systems were clear-cut around the turn of the 20th century, so there are a limited number of older, decaying trees and tree snags—places that have soft spots where woodpeckers usually drill holes. Luckily the Red-cockaded Woodpecker has a secret superpower that enables it to provide cavities in a way that most other woodpeckers can’t; they drill into live trees.\n\n“Red-cockaded Woodpeckers add an additional resource beyond the holes in dead pines, which is often where most of the action is in nest webs in conifer forests,” says Walters.\n\nDrilling into a live tree is anything but easy. Most woodpeckers in North America can finish a nest cavity in a matter of weeks, or maybe a couple months. The average Red-cockaded Woodpecker, on the other hand, takes years, sometimes up to a decade or more, to excavate a cavity in a live tree. That’s because chiseling away at live wood is difficult, due to the dense sapwood layer under the bark that allows the tree to transport nutrients from the roots to the leaves.\n\nAs one of the few North American woodpeckers that can excavate in live trees, Red-cockaded Woodpeckers are a key source of nesting cavities in the longleaf pine forests of the Southeast. Red-cockaded Woodpecker by Martina Nordstrand / Macaulay Library.\n\nRed-cockadeds prefer trees where the sapwood is healthy for a single reason, says Walters: “Resin wells.”\n\n“If you ever see a picture of a Red-cockaded Woodpecker cavity that’s being actively used, you’ll see all this sap all around it,” he says. The sappy coating around the nest holes creates a barrier of protection against would-be predators. “Snakes can’t get through the sap.”\n\nWalters’s research also shows that flickers again play an important role in longleaf pine nest webs, but often they rely on Red-cockaded Woodpeckers to get things started. Red-cockaded Woodpeckers and Northern Flickers excavated the most cavities (about 50%) that were used by other cavity-nesting birds, and flickers were also the primary enlargers of holes started by red-cockadeds that eventually housed some of the largest cavity-dwellers in longleaf pine forests—American Kestrels and Eastern Screech-Owls.\n\nRelated StoriesBringing Back the Red-cockaded Woodpecker: Are Prescribed Fire and Artificial Nests Enough?\n\nThere’s another, less visible, group of players in the longleaf pine nest web. Michelle Jusino began studying Red-cockaded Woodpeckers under Jeff Walters as a PhD student in the early 2010s and quickly became interested in a different denizen of tree cavities: fungi.\n\n“These birds have long been thought to have an association with one particular fungus [called Porodaedalea pini] because that fungus causes heartrot,” says Jusino. But she says that the scientific knowledge of that association was based on anecdotal observation. “Sometimes when we see woodpecker holes, we see this fungus on the tree.”\n\nSo for her dissertation, Jusino designed a study to find out exactly what fungi are present in woodpecker cavities, and how they get there. She and her team of researchers drilled fresh holes into living longleaf pine trees and put galvanized steel screens over some of the holes to keep out Red-cockaded Woodpeckers. The other holes were left open for red-cockadeds to access.\n\nHer results, published in Proceedings of the Royal Society B in 2016, found that both sets of holes contained fungal communities that weren’t present at first, but that those communities were very different from each other. The holes accessed by Red-cockaded Woodpeckers developed clusters of dozens of different fungi that were very similar to the fungal communities found in natural red-cockaded excavations.\n\n“These birds are helping facilitate these fungal colonizations, first by making holes, and then by transporting [the fungus] from one tree to another,” says Jusino. “What that study didn’t prove was whether that fungus is really helping the bird in return.”\n\nWhile Red-cockaded Woodpeckers overwhelmingly prefer live trees with healthy sapwood, Jusino says they seem to have a preference for trees where the heartwood is beginning to decay. But the question remains: How are these woodpeckers identifying such trees when there is usually no evidence on the outside? Jusino isn’t sure if red-cockadeds are sensing the rotting heartwood and accompanying fungi through smell, feel, or some other cues—or if heartwood rot may be accelerated due to the fungus that woodpeckers transport to the tree.\n\n“It’s still kind of a wide-open question,” she says. “We suggested two hypotheses: one is tree selection by the birds, that the birds are somehow selecting the trees based on the presence of fungus, and the other is that birds are facilitating the movement of this fungus from tree to tree.”\n\nNow a researcher at the Center for Mycology Research, part of the U.S. Forest Service, Jusino says she’s designing a new study that aims to answer that question. She’s planning to start the cultures this summer, though thanks to the slow excavation habits of Red-cockaded Woodpeckers and slow growth of fungus it will take several years to get results.\n\nA Red-headed Woodpecker finds a tree cavity claimed by a flying squirrel. Photo by Bob Rumer / Birdshare.\n\nForests of the Future\n\nAs a stream of published research adds to the understanding of woodpeckers as home builders in North American forests, an emerging area of research is looking ahead to the crucial role woodpeckers will play in forests of the future.\n\nWildfires and WoodpeckersOld Flames: The Tangled History of Forest Fires, Wildlife, and People\n\nClimate change and wildfires are drastically altering forest landscapes in the American West. According to Andrew Stillman, a postdoctoral fellow at the Cornell Lab of Ornithology who studies how bird populations respond to fires, woodpeckers are a key to helping forest communities recover after a fire.\n\n“Wildfires often create a pulse of dead or dying trees, followed by a pulse of insects that come to take advantage of that dead wood,” says Stillman. “The woodpeckers are following the insects into the burned areas.”\n\nStillman says that when woodpeckers swarm into a burn area full of dead trees, they go right to work excavating cavities: “And so the woodpeckers themselves then provide a pulse of nest sites for a lot of other species … like bluebirds, which eat a lot of berries and start spreading seeds around these recently burned places.”\n\nNorth Carolina State PhD candidate Lauren Pharr sees some of the same dynamics at play in her studies on Red-cockaded Woodpeckers in the fire-prone longleaf pine forests of the Southeast, and she thinks the woodpeckers are providing more than nesting homes—they’re providing shelters during extreme climate events.\n\nReady to Become a Woodpecker Whiz?\n\nBanded Woodpecker by Ngoc Sam Thuong Dang / Macaulay Library.\n\nThe Wonderful World of Woodpeckers is an online, self-paced course full of instructional videos, interactive learning tools, case studies, photo galleries, and discussion boards. Learn more about this in-depth, self-paced Bird Academy course.\n\nDuring Pharr’s four years of studying red-cockaded nesting sites, she has noted an increase in severe rain events in the North Carolina sandhills, and several hurricanes in the Florida panhandle. She says Red-cockaded Woodpecker nest holes are helping a menagerie of other cavity-dwelling species, from bluebirds and nuthatches to other woodpeckers like red-headed and red-bellied, to snakes, small mammals, even flying squirrels.\n\n“That is the most fun thing out there, climbing a tree and then a flying squirrel comes out, and it’s so cute to see him fly,” Pharr says. “It’s so spectacular.”\n\nShe says those cavities are important for all kinds of animals in the face of climate change: “[Red-cockaded Woodpeckers] are mitigating stress for those other species. So if they need a place to go … in places with extreme weather events or anything like that, those cavities help.”\n\nPharr’s advice for forest managers who want to optimize habitat for birds: Start with making sure a forest’s woodpeckers are taken care of.\n\n“Red-cockaded Woodpecker itself is a really big indicator species, and the management that we do with Red-cockaded Woodpeckers also is very beneficial to other species in that ecosystem,” Pharr says. “So it’s a win-win.”\n\n        PreviousCracking the Red-tail Code: Exploring the Diversity of America’s Most Widespread HawkNextIn Florida, Threatened Jays Suffer Costs of Warming Winters\n\nAll About Birdsis a free resource\n\nAvailable for everyone,funded by donors like you\n\nDonate\n\nAmerican Kestrel by Blair Dudeck / Macaulay Library\n\n          Related Stories\n\n                              After birds leave a nest box, can I clean out the nest for future use?\n\n                              A Miracle of Abundance as 20,000 Whimbrel Take Refuge on a Tiny Island\n                                                                Living Bird Magazine\n\n                              Where to Put Your Bird Feeder\n\n                              Why do hummingbirds fight so much?",
    "summary": {
      "en": "**Summary: The Role of Woodpeckers in Forest Ecosystems**\n\nWoodpecker nest holes are crucial habitats that benefit numerous species in forests. Researchers are studying \"nest webs\" to track how these valuable resources are shared among different animals. \n\nMany birds, up to 20% of species globally, use tree cavities for nesting and roosting, with woodpeckers being the primary excavators of these holes. In North America, species like the Northern Flicker create cavities that serve as homes for various birds, mammals, and insects. These holes provide safe, dry, and easily defendable spaces for raising young and hiding from predators.\n\nKathy Martin's research in British Columbia highlighted that about 30% of North American forest birds utilize tree cavities at some point in their lives. Woodpeckers are essential as they create and maintain these habitats. For example, Northern Flickers excavate almost half of the cavities used by other species, while Red-cockaded Woodpeckers are unique in their ability to drill into live trees, providing vital nesting sites in longleaf pine forests.\n\nThe research also indicates that woodpeckers help facilitate the growth of certain fungi, which may enhance the health of trees and their ecosystems. In the face of climate change and forest fires, woodpeckers play a critical role in forest recovery by creating new nesting sites and helping other species adapt to changing conditions. \n\nIn summary, woodpeckers are essential for maintaining the health and diversity of forest ecosystems, benefiting many other species through their nesting cavities. Their conservation is vital for the overall health of forest habitats.",
      "ko": "딱따구리의 둥지 구멍은 숲에서 여러 종에게 중요한 서식지를 제공합니다. 연구자들은 이러한 귀중한 자원이 다양한 동물들 사이에서 어떻게 공유되는지를 추적하기 위해 \"둥지 웹\"을 연구하고 있습니다.\n\n전 세계적으로 약 20%의 조류가 나무 구멍을 둥지와 휴식처로 사용하며, 딱따구리는 이러한 구멍을 파는 주요 역할을 합니다. 북미에서는 노던 플리커와 같은 종이 다양한 새, 포유류, 곤충의 집으로 사용되는 구멍을 만듭니다. 이 구멍은 새끼를 기르고 포식자로부터 숨기에 안전하고 건조하며 방어하기 쉬운 공간을 제공합니다.\n\n브리티시컬럼비아에서 캐시 마틴의 연구에 따르면, 북미 숲의 약 30%의 조류가 생애의 어느 시점에서 나무 구멍을 이용합니다. 딱따구리는 이러한 서식지를 만들고 유지하는 데 필수적입니다. 예를 들어, 노던 플리커는 다른 종이 사용하는 구멍의 거의 절반을 파며, 레드콕카데드 딱따구리는 살아 있는 나무를 뚫을 수 있는 독특한 능력을 가지고 있어 롱리프 파인 숲에서 중요한 둥지 장소를 제공합니다.\n\n연구는 또한 딱따구리가 특정 곰팡이의 성장을 촉진하여 나무와 생태계의 건강을 향상시킬 수 있음을 보여줍니다. 기후 변화와 산불에 직면하여, 딱따구리는 새로운 둥지 장소를 만들고 다른 종이 변화하는 환경에 적응하도록 도와줌으로써 숲의 회복에 중요한 역할을 합니다.\n\n결론적으로, 딱따구리는 숲 생태계의 건강과 다양성을 유지하는 데 필수적이며, 그들의 둥지 구멍을 통해 많은 다른 종에게 혜택을 줍니다. 이들의 보존은 숲 서식지의 전반적인 건강을 위해 매우 중요합니다.",
      "ja": null
    }
  },
  {
    "id": "b26e597ec37b62f6",
    "title": {
      "en": "Writing an HTTP Server in Go from Scratch: Part 2",
      "ko": "고로 만드는 HTTP 서버: 2부",
      "ja": null
    },
    "type": "story",
    "url": "https://www.krayorn.com/posts/http-server-go-2/",
    "score": 34,
    "by": "krayorn",
    "time": 1743111771,
    "content": "An HTTP Server in Go From scratch: Part 2   Follow along while I improve the HTTPServer I wrote from scratch in Go. March 2025   Last year I wrote a blog post explaining how I built my HTTP Server in Golang by following a Coder Crafters, I got some good feedback on it and improved the HTTP Server quite a bit, let’s dive into the changes!\nThe git repository is still available if you want to look at the whole codebase.\nThe first unit test\nLet’s start by adding a unit test, I was relying on the Codecrafters test suite but now I want to have some of my own unit tests.\nThis should mimic the first stage of the Codecrafters test suit:\nfunc TestServerStart(t *testing.T) {\n\t// Start the server\n\trouter := server.NewServer()\n\tgo router.Start()\n\n\t// Give the server a moment to start\n\ttime.Sleep(100 * time.Millisecond) // Not the most robust, good enough to start\n\n\t// Try to connect to the server\n\tconn, err := net.Dial(\"tcp\", \"localhost:4221\")\n\tif err != nil {\n\t\tt.Fatalf(\"Could not connect to server: %v\", err)\n\t}\n\tdefer conn.Close()\n\n\tt.Log(\"Successfully connected to the server\")\n}\nFixing issues found by readers\nHeaders should be case-insensitive and accept multiple values\nA Reddit comment mentioned that headers should be case-insensitive, and that some of them could have multiple values. That means that my naive Headers map[string]string is not correct!\nAfter reading the Golang doc for the HTTP package, I ended up with this:\ntype Header map[string][]string // this is now an array of strings\n\nfunc (header Header) Get(key string) string {\n\tif values, ok := header[strings.ToUpper(key)]; ok && len(values) > 0 {\n\t\treturn values[0] // get only returns the first value! To get all values, .Values() should be used\n\t}\n\treturn \"\"\n}\n\nfunc (header Header) Set(key string, value string) {\n\theader[strings.ToUpper(key)] = []string{value} // I always use ToUpper when interacting with the headers\n}\n\nfunc (header Header) Add(key string, value string) {\n\theader[strings.ToUpper(key)] = append(header[strings.ToUpper(key)], value)\n}\nInstead of using toUpper I could have used textproto.CanonicalMIMEHeaderKey(s) but it felt like cheating a bit in this “from scratch” serie, and I did not want to implement it myself today!\nI’m also adding a second test to check that I’m correctly parsing the request.\nfunc TestParseRequest(t *testing.T) {\n\trawRequest := \"GET /index.html HTTP/1.1\\r\\n\" +\n\t\t\"Host: www.example.com\\r\\n\" +\n\t\t\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\\r\\n\" +\n\t\t\"Content-Length: 3\\r\\n\" +\n\t\t\"\\r\\n\" +\n\t\t\"abc\"\n\n\trequest, err := parseRequest([]byte(rawRequest))\n\tif err != nil {\n\t\tt.Errorf(\"Expected no error, got %s\", err)\n\t}\n\n\tif request.Method != \"GET\" {\n\t\tt.Errorf(\"Expected method GET, got %s\", request.Method)\n\t}\n\n\tif request.Url.Original != \"/index.html\" {\n\t\tt.Errorf(\"Expected path /index.html, got %s\", request.Url.Original)\n\t}\n\n\tif request.Headers.Get(\"Host\") != \"www.example.com\" {\n\t\tt.Errorf(\"Expected Host header www.example.com, got %s\", request.Headers[\"Host\"])\n\t}\n\n\texpectedUserAgent := \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n\tif request.Headers.Get(\"User-Agent\") != expectedUserAgent {\n\t\tt.Errorf(\"Expected User-Agent header %s, got %s\", expectedUserAgent, request.Headers[\"User-Agent\"])\n\t}\n\n\tif string(request.Body) != \"abc\" {\n\t\tt.Errorf(\"Expected body %s, got %s\", \"abc\", string(request.Body))\n\t}\n}\nStream the response instead of sending it as a single string\nThe next comment mentions that I should allow responses to be a stream (so an io.Writer in go).\nFor the most part it’s quite straightforward, and I can simply replace every instance of str += \"some string\" by io.WriteString(w, \"some string\").\nAnd pass the connection the the write method: route.Callback(*request).Write(conn)\nHandling bigger payloads\nDo you recall this from last post ?\n// Not handling bigger payload for now\nrawReq := make([]byte, 4096)\nWell, it’s time.\n\trawReq := make([]byte, 0)\n\tfor {\n\t\t// It's gonna hang if requestLength % 4096 == 0\n\t\tbuffer := make([]byte, 4096)\n\t\tn, err := conn.Read(buffer)\n\n\t\tif n > 0 {\n\t\t\trawReq = append(rawReq, buffer[:n]...)\n\t\t}\n\n\t\tif n < 4096 || err != nil {\n\t\t\tbreak\n\t\t}\n\t}\nThis is another very naive implementation, but it handle bigger payloads for now!\nHowever it breaks if the payload is a multiple of 4096, because there may be nothing left to read, but the loop is still waiting\nTo fix this issue, I add:\nconn.SetReadDeadline(time.Now().Add(10 * time.Millisecond)) // ensure the connection is not hanging waiting for data for no reason\nI’m pretty sure the correct implementation would be to start parsing the data until I find the content-length to know exactly how much more I should read, but that’s good enough for now!\nNew features from my list\nMiddleware\nMiddlewares are cool, it’s a simple concept but I wondered how hard it was to implement!\nThe Server now has an array of middlewares that can be added by the client\ntype Server struct {\n\tRoutes      []Route\n\tMiddlewares []func(Handler) Handler\n}\n\ntype Handler func(request HTTPRequest) HTTPResponse\n\nfunc (server *Server) Use(middleware func(Handler) Handler) {\n\tserver.Middlewares = append(server.Middlewares, middleware)\n}\nAnd in the listenReq function,  we create a chain starting from the method assigned to the route followed by all the middlewares in the reverse order of assignment. Once the chain is complete, we call the last function of the chain, which triggers the chain reaction, returning to the callback!\nnextRequest := route.Callback\nfor i := len(middlewares) - 1; i >= 0; i-- {\n\tnextRequest = middlewares[i](nextRequest)\n}\n\nerr := nextRequest(*request).Write(conn)\nfmt.Println(\"Error while writing the response\", err)\n\nThe client code looks like this:\nfunc main() {\n\trouter := server.NewServer()\n\trouter.AddRoute(\"/\", home, \"GET\")\n\n\trouter.AddRoute(\"/echo/{str}\", echo, \"GET\")\n\trouter.Use(timingMiddleware)\n\trouter.Use(loggingMiddleware)\n\trouter.Start()\n\n}\n\nfunc loggingMiddleware(next server.Handler) server.Handler {\n\treturn func(req server.HTTPRequest) server.HTTPResponse {\n\t\tfmt.Println(\"Receiving call on \", req.Url.Original)\n\t\tresp := next(req)\n\t\tfmt.Println(\"Received call on \", req.Url.Original)\n\t\treturn resp\n\t}\n}\n\nfunc timingMiddleware(next server.Handler) server.Handler {\n\treturn func(req server.HTTPRequest) server.HTTPResponse {\n\t\tstart := time.Now()\n\t\tresp := next(req)\n\t\tduration := time.Since(start)\n\t\tfmt.Printf(\"%s %s - %d (%v)\\n\", req.Method, req.Url.Original, resp.Code, duration)\n\t\treturn resp\n\t}\n}\nSuch elegance! (Ok, I don’t know if it’s that elegant, but I was quite happy with it, the code was less complicated than I thought!)\nQuery string parameters\nNext step: Query parameters.\nI extract these from the original URL just before trying to match the URI.\nCurrently I store all of them as strings and let the user do the conversion if they expect something else.\n\turi, queryParamString, found := strings.Cut(request.Url.Original, \"?\")\n\turiParts := strings.Split(uri, \"/\")[1:]\n\tqueryParameters := make(map[string]string)\n\tif found {\n\t\tfor _, parameter := range strings.Split(queryParamString, \"&\") {\n\t\t\tkeyValue := strings.Split(parameter, \"=\")\n\t\t\tif len(keyValue) > 1 {\n\t\t\t\tqueryParameters[keyValue[0]] = keyValue[1]\n\t\t\t} else {\n\t\t\t\tqueryParameters[keyValue[0]] = \"true\"\n\t\t\t}\n\t\t}\n\t}\nThe demo app can then access them like this\nfunc echo(request server.HTTPRequest) server.HTTPResponse {\n\tcontent := request.Url.Parameters[\"str\"]\n\tif val, ok := request.Url.QueryParameters[\"repeat\"]; ok && val == \"true\" {\n\t\tcontent = strings.Repeat(content, 2)\n\t}\n\n\theaders := make(server.Header)\n\theaders.Set(\"Content-Type\", \"text/plain\")\n\treturn server.HTTPResponse{\n\t\tCode:    server.StatusOK,\n\t\tHeaders: headers,\n\t\tBody:    []byte(content),\n\t\tRequest: &request,\n\t}\n}\nSubrouters\nTo improve the UX of declaring many routes and only assigning middlewares to a certain set of routes, let’s add the subrouters functionality.\nOur Server type accepts other Server as Subrouters, and I create them via the Subrouter method on a server.\ntype Server struct {\n\tRoutes      []Route\n\tSubRouters  []*Server\n\tMiddlewares []func(Handler) Handler\n\tPrefix      string\n}\n\nfunc (server *Server) SubRouter(prefix string) *Server {\n\tsubRouter := Server{\n\t\tPrefix: prefix,\n\t}\n\n\tserver.SubRouters = append(server.SubRouters, &subRouter)\n\n\treturn &subRouter\n}\nNext the matching, I kept the same code as before for checking the routes, but it’s now in a separate function called match.\nIf no route is found, this function will iterate through the different subrouters with a correct prefix and then call match again on them.\nfunc match(request HTTPRequest, uriParts []string, server Server) (func(HTTPRequest) HTTPResponse, map[string]string, []func(Handler) Handler) {\n\n// The code matching the routes\n// [...]\n\nSUBROUTERS:\n\tfor _, subrouter := range server.SubRouters {\n\t\tprefixParts := strings.Split(subrouter.Prefix, \"/\")[1:]\n\n\t\tparametersPrefix := make(map[string]string)\n\t\tfor i := 0; i < len(prefixParts); i++ {\n\t\t\tif strings.HasPrefix(prefixParts[i], \"{\") && strings.HasSuffix(prefixParts[i], \"}\") {\n\t\t\t\tparametersPrefix[prefixParts[i][1:len(prefixParts[i])-1]] = uriParts[i]\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tif prefixParts[i] == uriParts[i] {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tcontinue SUBROUTERS\n\t\t}\n\n\t\tres, parameters, middlewares := match(request, uriParts[len(prefixParts):], *subrouter)\n\t\tif res != nil {\n\t\t\tmaps.Copy(parametersPrefix, parameters)\n\t\t\treturn res, parametersPrefix, append(server.Middlewares, middlewares...) // We share the middlewares of the main router with its subroutes\n\t\t}\n\t}\n\n\treturn nil, map[string]string{}, server.Middlewares\n}\nIn the demo app, the client can now use the subrouters like this\n\trouter := server.NewServer()\n\n\trouter.AddRoute(\"/\", home, \"GET\")\n\trouter.AddRoute(\"/echo/{str}\", echo, \"GET\")\n\trouter.AddRoute(\"/user-agent\", userAgent, \"GET\")\n\trouter.AddRoute(\"/files/{filename}\", getFile, \"GET\")\n\trouter.AddRoute(\"/files/{filename}\", createFile, \"POST\")\n\n\tv2Router := router.SubRouter(\"/v2/{aa}\")\n\tv2Router.AddRoute(\"/echo/{str}\", echo, \"GET\")\n\tv2Router.Use(timingMiddleware)\n\n\trouter.Use(loggingMiddleware)\n\trouter.Start()\nThe middlewares of a router are shared with its subrouters.\nThe end, again\nAnd we’re done with this set of improvements, 3 fixes and 3 new features!\nI don’t think I’ll work on this in the near future unless there are some big mistakes I made or a really fun idea I want to try.\nAs you can see, I did not write enough unit tests for all the old and new features which is a shame and something I should improve on, especially for this kind of learning project.\nAlthough I don’t think I’ll write a part 3, I have two other learning projects built on top of a Codecrafters course that could deserve their own post, the grep and the shell, you can expect to see a writeup on one of those in the next few months.    gosoftware",
    "summary": {
      "en": "In this blog post, the author continues to improve their HTTP server written in Go, based on feedback received after the initial version. Key updates include:\n\n1. **Unit Testing**: The author introduces their own unit tests to ensure the server works correctly, starting with tests for server startup and request parsing.\n\n2. **Header Handling**: The header handling is updated to be case-insensitive and support multiple values, replacing the previous method of using a simple map.\n\n3. **Streaming Responses**: The server is modified to stream responses instead of sending them as a single string, improving efficiency.\n\n4. **Handling Large Payloads**: The server's ability to handle larger request payloads is enhanced, including a fix for potential hanging issues when reading data.\n\n5. **Middleware Support**: Middleware functionality is added, allowing the server to process requests through a series of functions for logging and timing.\n\n6. **Query Parameters**: The server now supports query parameters extracted from URLs, making it easier to handle dynamic requests.\n\n7. **Subrouters**: To streamline route management, subrouters are introduced, allowing for better organization of routes and middleware.\n\nThe author concludes by acknowledging the need for more unit tests and hints at future projects, including a grep tool and a shell, as potential topics for upcoming blog posts.",
      "ko": "이 블로그 포스트에서 저자는 초기 버전 이후 받은 피드백을 바탕으로 Go로 작성한 HTTP 서버를 계속 개선하고 있습니다. 주요 업데이트 내용은 다음과 같습니다.\n\n첫째, 저자는 서버가 올바르게 작동하는지 확인하기 위해 자체 단위 테스트를 도입했습니다. 이 테스트는 서버 시작과 요청 파싱을 포함합니다.\n\n둘째, 헤더 처리 방식이 업데이트되어 대소문자를 구분하지 않고 여러 값을 지원하게 되었습니다. 이전에는 간단한 맵을 사용하는 방식이었습니다.\n\n셋째, 서버는 응답을 단일 문자열로 보내는 대신 스트리밍 방식으로 전송하도록 수정되어 효율성이 향상되었습니다.\n\n넷째, 서버의 대용량 요청 페이로드 처리 능력이 강화되었습니다. 데이터 읽기 중 발생할 수 있는 멈춤 문제를 해결하는 수정도 포함되었습니다.\n\n다섯째, 미들웨어 기능이 추가되어 서버가 요청을 처리할 때 로깅과 타이밍을 위한 일련의 함수를 사용할 수 있게 되었습니다.\n\n여섯째, 서버는 URL에서 추출한 쿼리 매개변수를 지원하게 되어 동적 요청 처리가 더 쉬워졌습니다.\n\n마지막으로, 라우트 관리를 간소화하기 위해 서브라우터가 도입되어 라우트와 미들웨어를 더 잘 조직할 수 있게 되었습니다.\n\n저자는 더 많은 단위 테스트의 필요성을 인정하며, 향후 블로그 포스트에서 다룰 주제로 grep 도구와 셸 등을 언급했습니다.",
      "ja": null
    }
  },
  {
    "id": "b4913c0a45c1dfa9",
    "title": {
      "en": "Debian bookworm live images now reproducible",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://lwn.net/Articles/1015402/",
    "score": 745,
    "by": "bertman",
    "time": 1743009742,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "f7e6bb533a9dd9d0",
    "title": {
      "en": "What it takes to add a new back end to Futhark",
      "ko": "퓨타크 백엔드 추가하기",
      "ja": null
    },
    "type": "story",
    "url": "https://futhark-lang.org/blog/2025-03-04-adding-a-new-backend.html",
    "score": 28,
    "by": "PaulHoule",
    "time": 1742812288,
    "content": "What it takes to add a new backend to Futhark\n\n    Posted on March  4, 2025\n\nRecently Scott Pakin suggested writing a blog post on how to add a\nnew backend to the Futhark\ncompiler, and\nsince there’s active fiddling with the backends at this very moment,\nthis is not a bad idea. Let us manage expectations up front: this will\nnot be a tutorial on adding a backend. I will not go into the deep\ndetails on the specific internal APIs that should be used. Instead, I\nwill focused on the core representations, and give an idea about the\nkind of work (often complicated) and magnitude (sometimes relatively\nlittle) it takes to add a new backend. It’s also somewhat open\nprecisely what a “backend” even means. There’s a significant\ndifference in complexity between adding a command futhark foo bar.fut that produces something based on bar.fut (very easy), to\nimplementing another C-like GPU backend (not hard, but you need to\ntouch a lot of pieces), to creating a fundamentally new backend for an\nalien piece of hardware (depending on your needs, can be extremely\nchallenging).\nI will still link pertinent pieces of the source code as applicable -\nsometimes it is instructive just how simple (or simplistic) it is to\nfinally glue together the complex bits. The Futhark compiler currently\nsupports a fairly diverse set of targets (sequential CPU, multicore,\ndifferent GPU APIs, C, Python). To achieve this without undue\nduplication of code and effort, the compiler uses fairly heavily\nparameterised representations of the program being compiled. I’ll try\nto get the gist across, but the full details are very, well, detailed\n(and I always feel like they should be simplified - it’s not the\naspect of the compiler we’re most proud of).\nFor a drier exposition, there is also the internal compiler\ndocumentation.\nArchitectural overview\nThe Futhark compiler is a monolithic program written in Haskell. All\npasses and backends are part of the same executable. In principle, it\nwould not be too difficult to make it possible to write a backend in a\ndifferent language, as a separate executable, although it hasn’t been\nrelevant so far.\nThe compiler consists of three main parts:\n\nThe frontend, which is concerned with parsing the Futhark source\nlanguage, type-checking it, and transforming it to the core\nintermediate representation (IR).\nThe middle-end, which performs gradual refinement,\ntransformation, and optimisation, on a program represented in\nvarious dialects of the the IR format (more on that below).\nThe backend, which translates from the IR representation into\nsome lower level representation, such as C - likely via several\nsteps.\n\nThese parts form a chain. The compiler will always run the frontend,\nultimately producing an intermediate representation of the program,\nthen run an appropriate middle-end pipeline, which produces another\nrepresentation of the program, and finally pass this to the backend.\nThe frontend is pretty much the same no matter how you invoke the\nFuthark compiler (futhark c, futhark cuda, etc), but the\nmiddle-end and backend behaves very differently based on the compiler\nmode. For example, rather than having a single IR, the compiler\nactually has a family of IR dialects, suited for different purposes,\nand used at different stages of the compiler. To give a gist of what I\nmean, consider an extensible Haskell datatype for representing very\nsimple expressions:\ndata Exp o = Var String\n           | Val Int\n           | Add (Exp o) (Exp o)\n           | Sub (Exp o) (Exp o)\n           | Op o\nApart from representing variables, values, additions, and\nsubtractions, this Exp type has also has a type parameter o that\nrepresents some other kind of operation, via the Op constructor.\nThis means we can instantiate a variant of Exp that contains, say,\nsquare root as the operation:\ndata SqrtOp = Sqrt ExpWithSqrt\n\ntype ExpWithSqrt = Exp SqrtOp\nBut we could also have one with some general notion of a function call:\ndata FunOp = Fun String [ExpWithFun]\n\ntype ExpWithFun = Exp FunOp\nWe can now write functions that operate on Exp o values, as long as\nwe parameterise those functions with how to handle the o cases\n(using function parameters, type classes, or what have you). This\ntechnique is useful when we want to have a collection of types that\nare largely the same. For example, in the middle-end of the Futhark\ncompiler, we initially use an IR dialect called SOACS, where\nparallelism is expressed using nested higher order operations quite\nsimilar to the source language. Eventually, the program undergoes a\nflattening transformation, after\nwhich parallelism is expressed using a different vocabulary of\nflat-parallel operations. Later, even the representation of types goes\nfrom something similar to the source language, to a representation\nthat also contains information about memory layout. Most of the\nlanguage-level details of the IR, such as how arithmetic and control\nflow is expressed, remains the same, and so does a lot of compiler\ncode, such as simplification passes, that can operate on any dialect.\nThe actual representation is a little more involved than explained\nabove, and is quite similar to the approach described in the paper\nTrees that\nGrow.\nIn particular, type-level functions are used to avoid having a\ndifferent type parameter for everything that can vary.\nWe use this notion of IR dialects pervasively throughout both the\nmiddle-end and the backend. The middle-end uses a pipeline based on\nthe compilation mode, which eventually produces a program in some IR\ndialect. That is, pipelines can be considered pure functions from some\nIR dialect to some other (or the same) IR dialect. For the c\nbackend, this dialect is called SeqMem (no parallelism, with\ninformation about array layout and\nallocations), for the\nmulticore and ispc backends it is MCMem (multicore parallel\noperations), and for the GPU backends it is called GPUMem. You can\nsee some of the default pipelines\nhere.\nWriting a new backend thus consists of picking a pipeline that\ntransforms the IR into the dialect you wish, and then doing\nsomething with that IR - where the compiler is actually quite\nagnostic regarding what that something might be. Not every backend\nneeds a distinct IR dialect - all of the GPU backends use the same IR\ndialect, for example.\nBackend actions\nIn Futhark compiler implementation lingo, the “backend” is called an\n“action”, and is an essentially arbitrary procedure that runs on the\nresult of a middle-end pipeline:\ndata Action rep = Action\n  { actionName :: String,\n    actionDescription :: String,\n    actionProcedure :: Prog rep -> FutharkM ()\n  }\nHere rep is a type-level token representing the IR dialect accepted\nby the action, and FutharkM is monad that supports IO effects,\nmeaning that these “actions” can perform arbitrary IO. For example,\nthe action for futhark c will do a bunch of code generation in pure\nHaskell code, but also write some files and run a C compiler:\ncompileCAction :: FutharkConfig -> CompilerMode -> FilePath -> Action SeqMem\ncompileCAction fcfg mode outpath =\n  Action\n    { actionName = \"Compile to sequential C\",\n      actionDescription = \"Compile to sequential C\",\n      actionProcedure = helper\n    }\n  where\n    helper prog = do\n      cprog <- handleWarnings fcfg $ SequentialC.compileProg versionString prog\n      let cpath = outpath `addExtension` \"c\"\n          hpath = outpath `addExtension` \"h\"\n          jsonpath = outpath `addExtension` \"json\"\n\n      case mode of\n        ToLibrary -> do\n          let (header, impl, manifest) = SequentialC.asLibrary cprog\n          liftIO $ T.writeFile hpath $ cPrependHeader header\n          liftIO $ T.writeFile cpath $ cPrependHeader impl\n          liftIO $ T.writeFile jsonpath manifest\n        ToExecutable -> do\n          liftIO $ T.writeFile cpath $ SequentialC.asExecutable cprog\n          runCC cpath outpath [\"-O3\", \"-std=c99\"] [\"-lm\"]\n        ToServer -> do\n          liftIO $ T.writeFile cpath $ SequentialC.asServer cprog\n          runCC cpath outpath [\"-O3\", \"-std=c99\"] [\"-lm\"]\nHere the SequentialC.compileProg function does the actual C code\ngeneration. I’ll elaborate a bit on it, but at an architectural level,\nit is not constrained at all in what it does. In principle, an action\ncould just dump the final IR to disk and run some entirely different\nprogram that takes care of code generation. You might even write an\naction that expects the program to still be in one of the early IR\ndialects, such as the ones that do not have memory information, or\neven the one that still has nested parallelism. This might be\nappropriate if you are targeting some other (relatively) high level\nlanguage.\nUltimately, if you wish to write a backend that does not need a new IR\ndialect, and also does not need to reuse any of the existing C\ngeneration machinery, then this is consequently quite easy - at least\nas far as integration with the compiler is concerned.\nTo actually hook up a pipeline with an action and produce something\nthat can be invoked by the command line, you need to write a largely\nboilerplate main definition, like this one for futhark Haskell:\nmain :: String -> [String] -> IO ()\nmain = compilerMain\n  ()\n  []\n  \"Compile sequential C\"\n  \"Generate sequential C code from optimised Futhark program.\"\n  seqmemPipeline\n  $ \\fcfg () mode outpath prog ->\n    actionProcedure (compileCAction fcfg mode outpath) prog\nAnd then finally hook it up to the big list of\nsubommands.\nThat’s all it takes.\nImperative code generation\nWhile it is true that an action can be arbitrary imperative code, in\npractice all of Futhark’s C-based backends (and even the Python\nones)\nmake use of significant shared infastructure to avoid having to\nreimplement the wheel too often.\nAs a starting point, the Futhark compiler defines an imperative\nintermediate representation, called\nImp.\nAs with the middle-end, Imp is actually an extensible language, with\nvarious dialects. For example, sequential\nImpGen.\nIn contrast to the functional middle-end IR, which is very well\ndefined, with type checking rules and a well-defined external syntax,\nImp is a lot more ad hoc, and does not for example have a parser.\nSemantically, it’s largely a simplified form of C. In fact, it is not\neven in SSA\nform,\nwhich still works out alright, because we do no optimisation at the\nImp level.\nThe translation from the functional IR to Imp is done by a module\ncalled\nImpGen.\nIt is heavily parameterised, because it essentially has to go from an\narbitrary IR dialect to an arbitrary Imp dialect. It is full of\nimplementation details, but not particularly interesting.\nOnce the compiler has obtained an Imp representation of the program,\nit can then turn that program into C or Python, or even some other\nlanguage. This is largely a mechanical process - the semantic gap from\nImp to C (or the baroque form of Python produced by Futhark) is not\ngreat, and mostly involves mapping Imp constructs to the facilities\nprovided by the (very small) Futhark runtime system, and of course\ngenerating syntactically valid code. To ease maintenance of the three\nGPU backends (cuda, opencl, hip), we also make use of a small\nGPU abstraction layer\n(gpu.h,\ndiscussed in this\npaper.\nAdvice on writing a backend\nFuthark is not set up to make it especially easy to add new backends,\nbut neither is it particularly difficult. After all, as of this\nwriting we support 10 different backends. Here is some advice for any\nprospective people who wish to seek glory by adding a backend:\n\nIf you want to target a very high level parallel language, use only\nFuthark’s frontend and the middle-end up to the SOACS\nrepresentation. This will give you a monomorphic first-order program\n(except for parallel operations) where all types are scalars or\narrays of scalars, but still with nested parallelism, although that\nparallelism will be well fused. I do think it would be a fun\nexperiment to generate code for a fork-join language, such as\nMPL, from this representation.\nIf you want to target a slightly less high level parallel language,\nin particular one that does not handle nested parallelism well,\nconsider processing the output of the GPU representation. Despite\nthe name, it is not truly GPU specific (the parts that are can be\nignored or modified, and are mostly about metadata and tuning), but\nmerely guarantees the absence of nested parallelism. It is still\nhigh level and with value-oriented semantics, with no notion of\nmemory.\nIf you want to target a new GPU backend, implement the gpu.h\nabstraction layer. The code generation work for CPU-side work will\nthen be fairly straightforward, although you may still need to do\nsignificant work to generate the actual GPU kernel code. We are\ncurrently going through this process with the in-progress WebGPU\nbackend, and most of\nthe challenges are related to the particular limitations of WebGPU\n(a post for another time), and not so much the compiler engineering.\nIf you want to generate low level code of any kind, you will likely\nfind it easiest to use one of the IR dialects with memory\ninformation. If you want to generate something that is relatively\nC-like (and generating e.g.machine code or JavaScript is both\n“C-like” in this regard), then using the existing machinery for\ngenerating Imp is almost certainly easiest.\n\nHowever, in all cases, I would say a very good idea is to contact one\nof the Futhark developers for advice and help. Having a third party\nadd a new backend is not really something we have considered much (all\nof the backends have been written under our close supervision), and\nwhile the technical challenges are not all that major by the\nstandards of writing compiler backends, the documentation is not\nreally up to the task. But I would certainly be very excited for\nsomeone to give it a try.",
    "summary": {
      "en": "**Summary: Adding a New Backend to Futhark**\n\nScott Pakin suggested creating a blog post about adding new backends to the Futhark compiler. This post outlines the general process without going into detailed technical specifics. The complexity of adding a backend can vary significantly, from simple commands to complex new hardware implementations.\n\nThe Futhark compiler has three main parts:\n1. **Frontend**: Parses and type-checks the Futhark source code, converting it into an intermediate representation (IR).\n2. **Middle-end**: Refines and optimizes the program through various IR dialects.\n3. **Backend**: Converts the IR into a lower-level representation (like C).\n\nThe compiler supports multiple targets (CPUs, GPUs, etc.) and uses parameterized representations to avoid code duplication. Each backend is defined as an “action,” which is a procedure that works on the IR produced by the middle-end.\n\nTo write a new backend, you need to select an appropriate pipeline for transforming the IR and then define the backend action. While adding a backend is not overly complex, understanding the existing infrastructure and documentation is vital. Advice for potential developers includes:\n- Use Futhark's frontend and middle-end for high-level languages.\n- For lower-level code, utilize IR dialects with memory information.\n- Contact Futhark developers for guidance, as past backends were created with their close supervision.\n\nOverall, adding a backend offers challenges but is a feasible task, and the developers welcome new contributions.",
      "ko": "스콧 파킨은 Futhark 컴파일러에 새로운 백엔드를 추가하는 것에 대한 블로그 포스트를 작성할 것을 제안했습니다. 이 포스트는 구체적인 기술적 세부사항에 들어가지 않고 일반적인 과정을 설명합니다. 백엔드를 추가하는 복잡성은 간단한 명령어에서부터 복잡한 하드웨어 구현까지 다양하게 나타날 수 있습니다.\n\nFuthark 컴파일러는 세 가지 주요 부분으로 구성됩니다. 첫 번째는 프론트엔드로, Futhark 소스 코드를 파싱하고 타입 검사를 통해 중간 표현(Intermediate Representation, IR)으로 변환합니다. 두 번째는 미들엔드로, 다양한 IR 방언을 통해 프로그램을 정제하고 최적화합니다. 마지막으로 백엔드는 IR을 더 낮은 수준의 표현으로 변환합니다(예: C 언어).\n\n이 컴파일러는 여러 타겟(CPU, GPU 등)을 지원하며, 코드 중복을 피하기 위해 매개변수화된 표현을 사용합니다. 각 백엔드는 미들엔드에서 생성된 IR에 작용하는 절차인 “액션”으로 정의됩니다.\n\n새로운 백엔드를 작성하려면 IR을 변환하기 위한 적절한 파이프라인을 선택하고 백엔드 액션을 정의해야 합니다. 백엔드를 추가하는 것은 그리 복잡하지 않지만, 기존 인프라와 문서를 이해하는 것이 중요합니다. 잠재적인 개발자에게는 다음과 같은 조언이 주어집니다. Futhark의 프론트엔드와 미들엔드를 고급 언어에 사용하고, 저수준 코드를 위해서는 메모리 정보를 포함한 IR 방언을 활용하라는 것입니다. 또한, 과거의 백엔드는 Futhark 개발자들의 밀접한 감독 아래 만들어졌으므로, 도움을 받기 위해 그들과 연락하는 것이 좋습니다.\n\n전반적으로 백엔드를 추가하는 것은 도전이 될 수 있지만, 충분히 가능한 작업이며 개발자들은 새로운 기여를 환영합니다.",
      "ja": null
    }
  }
]