[
  {
    "id": "8cad349fd68f13a5",
    "title": {
      "en": "VGGT: Visual Geometry Grounded Transformer",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/facebookresearch/vggt",
    "score": 32,
    "by": "xnx",
    "time": 1742907566,
    "content": "VGGT: Visual Geometry Grounded Transformer\n\nVisual Geometry Group, University of Oxford; Meta AI\nJianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, David Novotny\n\n@inproceedings{wang2025vggt,\n  title={VGGT: Visual Geometry Grounded Transformer},\n  author={Wang, Jianyuan and Chen, Minghao and Karaev, Nikita and Vedaldi, Andrea and Rupprecht, Christian and Novotny, David},\n  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n  year={2025}\n}\n\nOverview\nVisual Geometry Grounded Transformer (VGGT, CVPR 2025) is a feed-forward neural network that directly infers all key 3D attributes of a scene, including extrinsic and intrinsic camera parameters, point maps, depth maps, and 3D point tracks, from one, a few, or hundreds of its views, within seconds.\nQuick Start\nFirst, clone this repository to your local machine, and install the dependencies (torch, torchvision, numpy, Pillow, and huggingface_hub).\ngit clone git@github.com:facebookresearch/vggt.git\ncd vggt\npip install -r requirements.txt\n\nAlternatively, you can install VGGT as a package (click here for details).\nNow, try the model with just a few lines of code:\nimport torch\nfrom vggt.models.vggt import VGGT\nfrom vggt.utils.load_fn import load_and_preprocess_images\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# bfloat16 is supported on Ampere GPUs (Compute Capability 8.0+)\ndtype = torch.bfloat16 if torch.cuda.get_device_capability()[0] >= 8 else torch.float16\n\n# Initialize the model and load the pretrained weights.\n# This will automatically download the model weights the first time it's run, which may take a while.\nmodel = VGGT.from_pretrained(\"facebook/VGGT-1B\").to(device)\n\n# Load and preprocess example images (replace with your own image paths)\nimage_names = [\"path/to/imageA.png\", \"path/to/imageB.png\", \"path/to/imageC.png\"]\nimages = load_and_preprocess_images(image_names).to(device)\n\nwith torch.no_grad():\n    with torch.cuda.amp.autocast(dtype=dtype):\n        # Predict attributes including cameras, depth maps, and point maps.\n        predictions = model(images)\n\nThe model weights will be automatically downloaded from Hugging Face. If you encounter issues such as slow loading, you can manually download them here and load, or:\nmodel = VGGT()\n_URL = \"https://huggingface.co/facebook/VGGT-1B/resolve/main/model.pt\"\nmodel.load_state_dict(torch.hub.load_state_dict_from_url(_URL))\n\nDetailed Usage\nYou can also optionally choose which attributes (branches) to predict, as shown below. This achieves the same result as the example above. This example uses a batch size of 1 (processing a single scene), but it naturally works for multiple scenes.\nfrom vggt.utils.pose_enc import pose_encoding_to_extri_intri\nfrom vggt.utils.geometry import unproject_depth_map_to_point_map\n\nwith torch.no_grad():\n    with torch.cuda.amp.autocast(dtype=dtype):\n        images = images[None]  # add batch dimension\n        aggregated_tokens_list, ps_idx = model.aggregator(images)\n\n    # Predict Cameras\n    pose_enc = model.camera_head(aggregated_tokens_list)[-1]\n    # Extrinsic and intrinsic matrices, following OpenCV convention (camera from world)\n    extrinsic, intrinsic = pose_encoding_to_extri_intri(pose_enc, images.shape[-2:])\n\n    # Predict Depth Maps\n    depth_map, depth_conf = model.depth_head(aggregated_tokens_list, images, ps_idx)\n\n    # Predict Point Maps\n    point_map, point_conf = model.point_head(aggregated_tokens_list, images, ps_idx)\n\n    # Construct 3D Points from Depth Maps and Cameras\n    # which usually leads to more accurate 3D points than point map branch\n    point_map_by_unprojection = unproject_depth_map_to_point_map(depth_map.squeeze(0),\n                                                                extrinsic.squeeze(0),\n                                                                intrinsic.squeeze(0))\n\n    # Predict Tracks\n    # choose your own points to track, with shape (N, 2) for one scene\n    query_points = torch.FloatTensor([[100.0, 200.0],\n                                        [60.72, 259.94]]).to(device)\n    track_list, vis_score, conf_score = model.track_head(aggregated_tokens_list, images, ps_idx, query_points=query_points[None])\n\nFurthermore, if certain pixels in the input frames are unwanted (e.g., reflective surfaces, sky, or water), you can simply mask them by setting the corresponding pixel values to 0 or 1. Precise segmentation masks aren't necessary - simple bounding box masks work effectively (check this issue for an example).\nVisualization\nWe provide multiple ways to visualize your 3D reconstructions and tracking results. Before using these visualization tools, install the required dependencies:\npip install -r requirements_demo.txt\n\nInteractive 3D Visualization\nPlease note: VGGT typically reconstructs a scene in less than 1 second. However, visualizing 3D points may take tens of seconds due to third-party rendering, independent of VGGT's processing time. The visualization is slow especially when the number of images is large.\nGradio Web Interface\nOur Gradio-based interface allows you to upload images/videos, run reconstruction, and interactively explore the 3D scene in your browser. You can launch this in your local machine or try it on Hugging Face.\npython demo_gradio.py\n\nClick to preview the Gradio interactive interface\n\nViser 3D Viewer\nRun the following command to run reconstruction and visualize the point clouds in viser. Note this script requires a path to a folder containing images. It assumes only image files under the folder. You can set --use_point_map to use the point cloud from the point map branch, instead of the depth-based point cloud.\npython demo_viser.py --image_folder path/to/your/images/folder\n\nTrack Visualization\nTo visualize point tracks across multiple images:\nfrom vggt.utils.visual_track import visualize_tracks_on_images\ntrack = track_list[-1]\nvisualize_tracks_on_images(images, track, (conf_score>0.2) & (vis_score>0.2), out_dir=\"track_visuals\")\n\nThis plots the tracks on the images and saves them to the specified output directory.\nSingle-view Reconstruction\nOur model shows surprisingly good performance on single-view reconstruction, although it was never trained for this task. The model does not need to duplicate the single-view image to a pair, instead, it can directly infer the 3D structure from the tokens of the single view image. Feel free to try it with our demos above, which naturally works for single-view reconstruction.\nWe did not quantitatively test monocular depth estimation performance ourselves, but @kabouzeid generously provided a comparison of VGGT to recent methods here. VGGT shows competitive or better results compared to state-of-the-art monocular approaches such as DepthAnything v2 or MoGe, despite never being explicitly trained for single-view tasks.\nRuntime and GPU Memory\nWe benchmark the runtime and GPU memory usage of VGGT's aggregator on a single NVIDIA H100 GPU across various input sizes.\n\nInput Frames\n1\n2\n4\n8\n10\n20\n50\n100\n200\n\nTime (s)\n0.04\n0.05\n0.07\n0.11\n0.14\n0.31\n1.04\n3.12\n8.75\n\nMemory (GB)\n1.88\n2.07\n2.45\n3.23\n3.63\n5.58\n11.41\n21.15\n40.63\n\nNote that these results were obtained using Flash Attention 3, which is faster than the default Flash Attention 2 implementation while maintaining almost the same memory usage. Feel free to compile Flash Attention 3 from source to get better performance.\nResearch Progression\nOur work builds upon a series of previous research projects. If you're interested in understanding how our research evolved, check out our previous works:\n\n      Deep SfM Revisited\n\n    ──┐\n\n      PoseDiffusion\n\n    ─────►\n\n      VGGSfM ──►\n      VGGT\n\n      CoTracker\n\n    ──┘\n\nAcknowledgements\nThanks to these great repositories: PoseDiffusion, VGGSfM, CoTracker, DINOv2, Dust3r, Moge, PyTorch3D, Sky Segmentation, Depth Anything V2, Metric3D and many other inspiring works in the community.\nChecklist\n\n Release the training code\n Release VGGT-500M and VGGT-200M\n\nLicense\nSee the LICENSE file for details about the license under which this code is made available.",
    "summary": {
      "en": "**VGGT: Visual Geometry Grounded Transformer Overview**\n\nThe Visual Geometry Grounded Transformer (VGGT) is a neural network developed by the Visual Geometry Group at the University of Oxford and Meta AI, designed to quickly analyze and interpret 3D scene attributes from images. It can extract camera parameters, depth maps, and point maps from one or multiple views in just seconds.\n\n**Quick Start Instructions:**\n1. Clone the VGGT repository and install required packages:\n   ```bash\n   git clone git@github.com:facebookresearch/vggt.git\n   cd vggt\n   pip install -r requirements.txt\n   ```\n2. Use a few lines of code to run the model:\n   - Import necessary libraries, load the model, and preprocess images.\n   - Predict 3D attributes like camera matrices and depth maps.\n\n**Detailed Usage:**\n- Users can select specific attributes to predict and process multiple scenes.\n- You can mask unwanted pixels in images using simple bounding boxes.\n  \n**Visualization Tools:**\n- VGGT provides several visualization methods, including a Gradio web interface for interactive 3D exploration and a command-line tool for visualizing point clouds.\n- It can also visualize point tracks across images.\n\n**Performance Metrics:**\n- VGGT shows strong performance in single-view reconstruction, even though it wasn't specifically trained for that. It competes well with other state-of-the-art methods for monocular depth estimation.\n\n**Runtime and Memory Usage:**\n- The model's processing time and GPU memory usage were benchmarked on an NVIDIA H100 GPU, showing efficient performance across varying input sizes.\n\n**Research Context:**\n- VGGT builds on previous research and acknowledges various contributions from related works in the field.\n\n**Release and License:**\n- The code is available under a specific license, and future releases of smaller model variants are planned.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "fe4536a67db801ea",
    "title": {
      "en": "Polypane, The browser for ambitious web developers",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://polypane.app/",
    "score": 28,
    "by": "ulrischa",
    "time": 1742720837,
    "content": "All-in-onePolypane helps with every aspect of web development.Develop and test your site in all the ways your real users view it: different screen sizes, device capabilities, color schemes, accessibility settings and more.responsive designAll the viewports you need.Don't manually resize and test websites screen-by-screen. Get all your viewports in one overview, no browser resizing needed.Mobile screens, 5K monitors, all your CSS breakpoints or any size you want.Synced panesAnd they're all in sync.Scrolling, clicking, typing and even hovers are synced across all viewports.Don't test viewports or devices one-by-one, check them all at once.Cross-browser and Cross-deviceEven across browsers.Share your local development environment anywhere, on any browser, on any device. Polypane Portal keeps everything in sync whether that's the scroll position, interactions, style edits or layout debugging.Debug toolsFrom A11y to Z-index.Apply over 40 different debug tools to each viewport. Test your site in completely new ways.Meta panelSocial media share card previews.X / TwitterFacebookSlackLinkedinDiscordGoogleTelegramBlueskyMastodonThreadsPixel-perfect previews of social media cards for X/Twitter, Facebook, Slack, LinkedIn, Discord, Telegram, Bluesky, Threads and Google Search. In dark and light mode. Even with your localhost.Deep dive into our social media previewsUser testimonialsDevelopers and designers love Polypane.Here's what they say:Polypane is easily one of the most useful tools for modern web designers and developers.Stephen Hay Author of Responsive Design WorkflowIt is convenient seeing your work in multiple responsive views at the same time. Love it, 💯Sara Soueidan Independent front-end UI engineer, Speaker/TrainerThis thing is FAST and the synchronized scrolling works amazingly. Prepare to be impressed.Vivian Guillen Frontend Developer & Curator at Tailwind WeeklyPolypane is really saving my bacon. Kilian knocked it out of the park, this is my new favorite development tool.Scott Tolinski Syntax.fm host, Level Up TutorialsIf you’re designing modern applications and websites, you should try Polypane! It’s worth every penny!Segun \"Sage\" Adebayo Creator of Chakra UI and Zag.jsI've started playing around with Polypane and 🤯. It has awesome debugging tools, accessibility checks and tests, and more. I'm really impressed.Kevin Powell CSS EvangelistRead more testimonials Join our Slack community Polypane has all the tools you need to build great web experiences.Try for free EmulationDark mode media feature testing.No need to change your entire operating system! Test light and dark mode side-by-side in the same view. Also works for reduced motion, reduced data and even print styles.DevtoolsYour favorite DevTools extensions.Install and use your favorite Chromium devtools, with support for React DevTools, Vue.js DevTools, Redux DevTools, Angular, Svelte and many more.…And that's only scratching the surface.Polypane has everything you need to make awesome web experiences.Unlimited ViewportsScroll SyncingHover SyncingGrid OverlaysUnified ConsoleKeyboard SyncingJSON ViewerDevice PresetsFreeform ResizingHorizontal Overflow DetectionDeveloper ToolsPixel-perfect Social Media PreviewsDifferent Layout ModesDevice EmulationHandoff SupportLive ReloadingAccessibility AuditsCustom SessionsPage OutlinesLive CSS EditingColor Blindness SimulatorsFull Page ScreenshotsVisual Design DiffingMeta Data OverviewCSS Breakpoint DetectionOverview ScreenshotsUnified Element InspectorTouch EmulationAutomated Form TestingScreenshot EditorWeb Vitals InspectorColor PickerBrowser ExtensionsBroken Link CheckingShare LinksCustom HeadersDark ModeWorkspaces…and more added monthly!Full product tour\n\nPolypane syncs everythingScrollHoverClickTypeTry it:0 clicksBuild, Debug and TestCreate as many panes as you needFind the overview that works for you. Store your favorite configurations in workspaces and share them with your team.Resize panes whenever you wantUse our device presets, your own CSS breakpoints or freeform resize panes right on the main canvas.Synced element inspector and consoleAn element inspector and a fully functional console that works seamlessly across all panes, eliminating duplicate tasks.Dozens and dozens of debug toolsLayout debugging, color blindness testers, built-in live reloading, screenshotting, device and media emulation...The list goes on.Everything you need to create amazing sites and webappsStart your free trial\n\nPolypane syncs everythingScrollHoverClickTypeTry it:0 clicksBuild, Debug and TestCreate as many panes as you needFind the overview that works for you. Store your favorite configurations in workspaces and share them with your team.Resize panes whenever you wantUse our device presets, your own CSS breakpoints or freeform resize panes right on the main canvas.Synced element inspector and consoleAn element inspector and a fully functional console that works seamlessly across all panes, eliminating duplicate tasks.Dozens and dozens of debug toolsLayout debugging, color blindness testers, built-in live reloading, screenshotting, device and media emulation...The list goes on.Everything you need to create amazing sites and webappsStart your free trial\n\nPolypane syncs everythingScrollHoverClickTypeTry it:0 clicksBuild, Debug and TestCreate as many panes as you needFind the overview that works for you. Store your favorite configurations in workspaces and share them with your team.Resize panes whenever you wantUse our device presets, your own CSS breakpoints or freeform resize panes right on the main canvas.Synced element inspector and consoleAn element inspector and a fully functional console that works seamlessly across all panes, eliminating duplicate tasks.Dozens and dozens of debug toolsLayout debugging, color blindness testers, built-in live reloading, screenshotting, device and media emulation...The list goes on.Everything you need to create amazing sites and webappsStart your free trial",
    "summary": {
      "en": "Polypane is a comprehensive tool for web development that allows you to design and test websites effectively. Here are the key features:\n\n1. **Multiple Viewports**: View and test your site on various screen sizes and devices all at once, without needing to resize your browser manually.\n\n2. **Synchronized Interaction**: Actions like scrolling, clicking, and typing are synced across all viewports, which means you can test everything together.\n\n3. **Cross-Browser and Device Support**: Share and test your development environment on any browser or device, keeping everything in sync.\n\n4. **Debugging Tools**: Polypane offers over 40 debugging tools for accessibility, layout, and more, allowing for thorough testing.\n\n5. **Social Media Previews**: You can see how your content will look when shared on social media platforms, in both light and dark modes.\n\n6. **User Testimonials**: Many developers praise Polypane for its speed, synchronization features, and overall convenience for modern web development.\n\n7. **Customization and Emulation**: Customize viewports, test dark mode and other settings, and use your favorite developer tools.\n\n8. **Extensive Features**: Polypane includes features like live reloading, accessibility audits, screenshot tools, and more, making it a robust solution for building web applications.\n\nYou can start a free trial to explore all these functionalities.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "fff7d9904f659462",
    "title": {
      "en": "Hann: A Fast Approximate Nearest Neighbor Search Library for Go",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/habedi/hann",
    "score": 26,
    "by": "habedi0",
    "time": 1742903831,
    "content": "Hann\n\nA fast approximate nearest neighbor search library for Go\n\nHann is a high-performance approximate nearest neighbor search (ANN) library for Go.\nIt provides a collection of index data structures for efficient similarity search in high-dimensional spaces.\nCurrently, supported indexes include Hierarchical Navigable Small World (HNSW),\nProduct Quantization Inverted File (PQIVF), and Random Projection Tree (RPT).\nHann can be seen as a core component of a vector database (like Milvus, Pinecone, Weaviate, Qdrant, etc.).\nIt can be used to add fast in-memory similarity search capabilities to your Go applications.\nFeatures\n\nUnified interface for different indexes (see core/index.go)\nSupport for indexing and searching vectors of arbitrary dimension\nFast distance computation using SIMD (AVX) instructions (see core/simd_distance.c)\nSupport for bulk insertion, deletion, and update of vectors\nSupport for saving indexes to disk and loading them back\n\nIndexes\n\nIndex Name\nSpace Complexity\nBuild Complexity\nSearch Complexity\n\nHNSW\n\n  O\n  (\n  n\n  d\n  +\n  n\n  M\n  )\n\n  O\n  (\n  n\n  log\n  ⁡\n  n\n  )\n\n  O\n  (\n  log\n  ⁡\n  n\n  )\n average case\n  O\n  (\n  n\n  )\n worst case\n\nPQIVF\n\n  O\n  (\n  n\n  k\n  +\n  k\n  d\n  )\n\n  O\n  (\n  n\n  k\n  i\n  )\n\n  O\n  (\n\n    n\n    k\n\n  )\n\nRPT\n\n  O\n  (\n  n\n  d\n  )\n\n  O\n  (\n  n\n  log\n  ⁡\n  n\n  )\n\n  O\n  (\n  log\n  ⁡\n  n\n  )\n average case\n  O\n  (\n  n\n  )\n worst case\n\n  n\n: number of vectors\n\n  d\n: number of dimensions (vector length)\n\n  M\n: links per node (HNSW)\n\n  k\n: number of clusters (PQIVF)\n\n  i\n: iterations for clustering (PQIVF)\n\nSupported Distances\nThe HNSW index supports the use of Euclidean, squared Euclidean, Manhattan, and cosine distances.\nIf cosine distance is used, the vectors are normalized on read (before they are used in the index or for search).\nNote that squared Euclidean distance is slightly faster to compute than Euclidean distance\nand gives the same order of closest vectors as Euclidean distance.\nIt can be used in place of Euclidean distance if only the order of closest vectors to\nquery vector is needed, not the actual distances.\nThe PQIVF and RPT indexes support Euclidean distance only.\nInstallation\nHann can be installed as a typical Go module using the following command:\ngo get github.com/habedi/hann@main\n\nHann requires Go 1.21 or later, a C (or C++) compiler, and a CPU that supports AVX instructions.\nExamples\n\nExample File\nDescription\n\nsimple_hnsw.go\nCreate and use an HNSW index with inline data\n\nhnsw.go\nCreate and use an HNSW index\n\nhnsw_large.go\nCreate and use an HNSW index (using large datasets)\n\npqivf.go\nCreate and use a PQIVF index\n\npqivf_large.go\nCreate and use a PQIVF index (using large datasets)\n\nrpt.go\nCreate and use an RPT index\n\nrpt_large.go\nCreate and use an RPT index (using large datasets)\n\nload_data.go\nHelper functions for loading example datasets\n\nutils.go\nExtra helper functions for the examples\n\nrun_datasets.go\nThe code to create different indexes and try them with different datasets\n\nDatasets\nUse the following commands to download the datasets used in the examples:\nmake download-data\n\n# Only needed to run the examples that use large datasets\nmake download-data-large\n\nNote that to run the examples using large datasets, possibly a machine with large amounts of memory is needed\nlike 32 GB or more.\nCheck the data directory for information about the datasets.\n\nDocumentation\nThe detailed documentation for Hann packages is available on pkg.go.dev.\nHNSW Index\nThe hnsw package provides an implementation of the HNSW graph index introduced\nby Malkov and Yashunin (2016).\nHNSW organizes data into multiple layers of a proximity graph, which allows fast approximate nearest neighbor searches\nby greedily traversing the graph from top to bottom.\nThe index has the following parameters:\n\nM: Controls the maximum number of neighbor connections per node. Higher values improve accuracy but increase\nmemory and indexing time (typical range: 5–48).\nEf: Defines search breadth during insertion and searching. Higher values improve accuracy but\nincrease computational cost (typical range: 10–200).\n\nPQIVF Index\nThe pqivf package provides an implementation of the PQIVF index introduced\nby Jegou et al. (2011).\nPQIVF first clusters data into coarse groups (inverted lists), then compresses vectors in each cluster using product\nquantization.\nThis allows fast approximate nearest neighbor searches by limiting queries to relevant clusters and\nefficiently comparing compressed vectors, which reduces search time and storage requirements.\nThe index has the following parameters:\n\ncoarseK: Controls the number of coarse clusters for initial quantization. Higher values improve search performance\nbut increase indexing time (typical range: 50–4096).\nnumSubquantizers: Determines the number of subspaces for product quantization. More subquantizers improve\ncompression and accuracy at the cost of increased indexing time (typical range: 4–16).\npqK: Sets the number of codewords per subquantizer. Higher values increase accuracy and storage usage (typical\nvalue: 256).\nkMeansIters: Number of iterations used to train the product quantization codebooks (recommended value: 25).\n\nRPT Index\nThe rpt package provides an implementation of the RPT index introduced\nby Dasgupta and Freund (2008).\nRPT recursively partitions data using randomly generated hyperplanes to build a tree structure, which allows efficient\napproximate nearest neighbor searches through a tree traversal process.\nThe index has the following parameters:\n\nleafCapacity: Controls the maximum number of vectors stored in each leaf node. Lower values increase tree depth,\nimproving search speed but slightly increasing indexing time (typical range: 5–50).\ncandidateProjections: Number of random projections considered at each tree split. Higher values improve split\nquality at the cost of increased indexing time (typical range: 1–10).\nparallelThreshold: Minimum number of vectors in a subtree to trigger parallel construction. Higher values lead to\nbetter concurrency during indexing but use more memory (typical value: 100).\nprobeMargin: Margin used to determine additional branches probed during searches. Higher values improve recall but\nincrease search overhead because of additional distance computations (typical range: 0.1–0.5).\n\nLogging\nThe verbosity level of logs produced by Hann can be controlled using the HANN_LOG environment variable.\nPossible values include:\n\n0, false, or off to disable logging altogether;\nfull or all to enable full logging (DEBUG level);\nUse any other value (including not setting the HANN_LOG environment variable) to enable basic logging (INFO level;\ndefault behavior).\n\nRandom Seed\nFor more consistent indexing and search results across different runs, set the HANN_SEED environment variable to an\ninteger.\nThis will initialize the random number generator, but some variations are still possible (for example, due to\nmultithreading).\n\nContributing\nSee CONTRIBUTING.md for details on how to make a contribution.\nLogo\nThe logo is named the \"Hiking Gopher\" and was created by Egon Elbre.\nLicense\nHann is licensed under the MIT License (LICENSE).",
    "summary": {
      "en": "**Hann Overview**\n\nHann is a fast library for approximate nearest neighbor (ANN) searches in Go. It helps find similar items quickly in high-dimensional spaces using various index structures. The main types of indexes supported are:\n\n1. **HNSW (Hierarchical Navigable Small World)**\n2. **PQIVF (Product Quantization Inverted File)**\n3. **RPT (Random Projection Tree)**\n\n**Key Features:**\n- Unified interface for different indexes.\n- Supports vectors of any dimension.\n- Fast distance calculations using SIMD instructions.\n- Allows bulk operations (insert, delete, update).\n- Can save and load indexes from disk.\n\n**Index Details:**\n- HNSW: Efficient but memory-intensive, supports multiple distance metrics.\n- PQIVF: Clusters data for fast searches, uses only Euclidean distance.\n- RPT: Builds a tree structure for searching, also uses Euclidean distance.\n\n**Installation:**\nTo install Hann, use:\n```\ngo get github.com/habedi/hann@main\n```\nRequires Go 1.21+, a C/C++ compiler, and a compatible CPU.\n\n**Usage Examples:**\nExamples are provided for each index type, including handling large datasets. Users need adequate memory (32 GB recommended for large data).\n\n**Documentation:**\nDetailed documentation is available on pkg.go.dev, explaining how each index works and its parameters.\n\n**Logging and Randomization:**\nLogging levels can be adjusted with the HANN_LOG environment variable. For consistent results across runs, set the HANN_SEED variable.\n\n**Contribution and License:**\nHann is open for contributions and is licensed under the MIT License. \n\nThe library's logo is called the \"Hiking Gopher.\"",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "29803211a6f61815",
    "title": {
      "en": "German parliament votes as a Git contribution graph",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://abstimmung.eu/git/2024",
    "score": 504,
    "by": "mxschll",
    "time": 1742858943,
    "content": "Abstimmungen 2024Namentliche Abstimmungen im BundestagJanFebMärAprMaiJunJulAugSepOktNovDezMoMiFrWenigerMehr17. Januar 2024Jahresbericht 2022 der Wehrbeauftragten (Entschließungsantrag)VerteidigungAußenpolitikUkraineBundeswehrMilitärDie Abstimmung betrifft den Entschließungsantrag der CDU/CSU-Fraktion zum Jahresbericht 2022 der Wehrbeauftragten, der die Bundesregierung auffordert, einsatzbereite TAURUS-Marschflugkörper an die Ukraine zu liefern und diese unverzüglich nachzubeschaffen.Ergebnisse des BundestagsJa: 178(27%)Nein: 485(73%)Enthaltung: 3(0%)17. Jan. 2024Gesamtstimmen: 66618. Januar 2024Entlastung der LandwirtschaftLandwirtschaftWirtschaftSteuerpolitikAgrarpolitikInnovationBürokratieDer Antrag der CDU/CSU-Fraktion betrifft die Unterstützung der Landwirtschaft und zielt darauf ab, die Wettbewerbsfähigkeit der Landwirtschaft zu erhalten und zu verbessern, anstatt sie durch politische Maßnahmen zu schwächen. Der Antrag fordert die Bundesregierung auf, Maßnahmen zu ergreifen, um die Landwirtschaft zu fördern und zu entlasten.Ergebnisse des BundestagsJa: 188(28%)Nein: 420(62%)Enthaltung: 69(10%)18. Jan. 2024Gesamtstimmen: 677Friedensinitiative für die Ukraine und Russland (Beschlussempfehlung)AußenpolitikSicherheitspolitikFriedenspolitikUkraineRusslandDie Abstimmung betrifft einen Antrag der AfD-Fraktion, der eine Friedensinitiative mit Sicherheitsgarantien für die Ukraine und Russland fordert, um Deutschlands Verantwortung für den Frieden in Europa gerecht zu werden.Ergebnisse des BundestagsJa: 605(89%)Nein: 75(11%)Enthaltung: 2(0%)18. Jan. 2024Gesamtstimmen: 68219. Januar 2024Modernisierung des Staatsangehörigkeitsrechts (GesEntw BReg)StaatsangehörigkeitIntegrationRechtGesellschaftMigrationDie Abstimmung betrifft die Modernisierung des Staatsangehörigkeitsrechts (StARModG), um den Zugang zur deutschen Staatsangehörigkeit zu erleichtern und die Integration von Ausländern zu fördern.Ergebnisse des BundestagsJa: 382(60%)Nein: 234(37%)Enthaltung: 23(3%)19. Jan. 2024Gesamtstimmen: 63901. Februar 2024Änderung des BundeswahlgesetzesWahlrechtInnenpolitikBevölkerungsentwicklungKommunalpolitikGesetzgebungDie Abstimmung betrifft die Änderung des Bundeswahlgesetzes, um die Wahlkreiseinteilung an die aktuelle Bevölkerungsentwicklung anzupassen und kommunale Gebietsänderungen zu berücksichtigen. Ziel ist es, sicherzustellen, dass die Wahl zum Deutschen Bundestag auf einer fairen und gleichmäßigen Grundlage stattfindet.Ergebnisse des BundestagsJa: 382(57%)Nein: 276(41%)Enthaltung: 12(2%)1. Feb. 2024Gesamtstimmen: 67022. Februar 2024Zehn Jahre russischer Krieg gegen die UkraineUkraineRusslandSicherheitspolitikAußenpolitikMilitärVölkerrechtDer Antrag betrifft die Unterstützung der Ukraine angesichts des russischen Krieges und zielt darauf ab, die deutsche und europäische Sicherheit zu stärken sowie die Verantwortlichen für Kriegsverbrechen zur Rechenschaft zu ziehen.Ergebnisse des BundestagsJa: 381(57%)Nein: 284(43%)Enthaltung: 2(0%)22. Feb. 2024Gesamtstimmen: 667Deutsche Außen- und Sicherheitspolitik (Antrag CDU/CSU)AußenpolitikSicherheitspolitikVerteidigungInternationale BeziehungenBevölkerungsschutzDer Antrag der CDU/CSU betrifft eine Neuausrichtung der deutschen Außen- und Sicherheitspolitik, um auf die veränderte Bedrohungslage durch Russland zu reagieren und Deutschlands Rolle in Europa und der Welt zu stärken. Er fordert eine \"echte Zeitenwende\" mit konkreten Maßnahmen zur Stärkung der Verteidigungsfähigkeit, des Bevölkerungsschutzes und der internationalen Zusammenarbeit.Ergebnisse des BundestagsJa: 182(27%)Nein: 479(72%)Enthaltung: 5(1%)22. Feb. 2024Gesamtstimmen: 66623. Februar 2024Wachstumschancengesetz;Kfz-Haftpflichtversicherung (BeschlEmpf Vermittlungsausschuss)WirtschaftSteuerrechtInvestitionInnovationFinanzenDie Beschlussempfehlung des Vermittlungsausschusses betrifft das Gesetz zur Stärkung von Wachstumschancen, Investitionen und Innovation sowie Steuervereinfachung und Steuerfairness (Wachstumschancengesetz). Sie zielt darauf ab, Änderungen an diesem Gesetz vorzunehmen, um Wachstum, Investitionen und Innovation zu fördern sowie das Steuersystem zu vereinfachen und fairer zu gestalten.Ergebnisse des BundestagsJa: 376(58%)Nein: 267(41%)Enthaltung: 1(1%)23. Feb. 2024Gesamtstimmen: 644Cannabisgesetz (GesEntwurf BReg)GesundheitDrogenpolitikJugendschutzJustizSicherheitDas Cannabisgesetz regelt den kontrollierten Umgang mit Cannabis und ändert weitere Vorschriften. Es zielt darauf ab, den Gesundheitsschutz zu verbessern, den illegalen Markt einzudämmen und den Kinder- und Jugendschutz zu stärken.Ergebnisse des BundestagsJa: 404(64%)Nein: 226(36%)Enthaltung: 4(0%)23. Feb. 2024Gesamtstimmen: 634Bundeswehreinsatz EUNAVFOR ASPIDES (BeschlEmpf)AußenpolitikSicherheitspolitikEuropäische UnionMilitärInternationaler HandelDie Abstimmung betrifft die Beteiligung bewaffneter deutscher Streitkräfte an der von der Europäischen Union geführten Operation EUNAVFOR ASPIDES, die zum Schutz der Schifffahrt im Roten Meer und angrenzenden Gebieten beitragen soll. Der Bundestag stimmt über den Antrag der Bundesregierung zur Entsendung von bis zu 700 Soldaten bis maximal zum 28. Februar 2025 ab.Ergebnisse des BundestagsJa: 538(94%)Nein: 31(5%)Enthaltung: 4(1%)23. Feb. 2024Gesamtstimmen: 57314. März 2024Ablehnung von Lieferung des Taurus-Marschflugkörpers (Beschlussempfehlung)AußenpolitikVerteidigungSicherheitspolitikUkraineMilitärDie Abstimmung betrifft den Antrag der CDU/CSU, die Bundesregierung aufzufordern, der Ukraine Taurus-Marschflugkörper aus Bundeswehrbeständen zu liefern, die Integration des Waffensystems in ukrainische Flugzeuge zu unterstützen, ukrainische Soldaten auszubilden, Hemmnisse bei der Weitergabe sensibler Informationen zu beseitigen, Ausrüstungslücken bei der Bundeswehr durch Nachbeschaffung zu schließen, die Produktionskapazitäten der Industrie zu erhöhen und weitere Taurus-Marschflugkörper zu beschaffen. Der Antrag wird abgelehnt.Ergebnisse des BundestagsJa: 494(72%)Nein: 188(27%)Enthaltung: 5(1%)14. März 2024Gesamtstimmen: 68720. März 2024Steuervergünstigung für Agrardiesel (Antrag AfD)LandwirtschaftSteuerpolitikSubventionenHaushaltspolitikAgrardieselDie Abstimmung betrifft den Antrag der AfD-Fraktion, die Steuervergünstigung für Agrardiesel beizubehalten, um die deutsche Landwirtschaft nicht zusätzlich zu belasten und die Lebensmittelpreise nicht künstlich zu verteuern. Der Antrag sieht vor, Kürzungen in anderen Bereichen des Haushalts vorzunehmen, falls Einsparungen notwendig sind, und die Bundesregierung aufzufordern, Beschlüsse der Parlamentsgremien zu respektieren.Ergebnisse des BundestagsJa: 553(90%)Nein: 64(10%)Enthaltung: 0(0%)20. März 2024Gesamtstimmen: 61721. März 2024Bundeswehreinsatz SEA GUARDIAN im Mittelmeer (Antrag BReg)SicherheitspolitikMilitärNATOMittelmeerTerrorismusbekämpfungInternationale BeziehungenDie Abstimmung betrifft die Fortsetzung der Beteiligung bewaffneter deutscher Streitkräfte an der NATO-geführten maritimen Sicherheitsoperation SEA GUARDIAN (MSO SG) im Mittelmeer, um zur Sicherheit und Stabilität in der Region beizutragen. Der Deutsche Bundestag stimmt über den Antrag der Bundesregierung zur Verlängerung dieses Einsatzes ab.Ergebnisse des BundestagsJa: 563(84%)Nein: 109(16%)Enthaltung: 2(0%)21. März 2024Gesamtstimmen: 674Bundeswehreinsatz in der Republik Südsudan (UNMISS)AußenpolitikSicherheitspolitikVerteidigungHumanitäre HilfeFriedensmissionUNDie Abstimmung betrifft die Fortsetzung der Beteiligung bewaffneter deutscher Streitkräfte an der UN-Friedensmission UNMISS (United Nations Mission in South Sudan) in der Republik Südsudan. Es geht darum, ob der Bundestag der von der Bundesregierung beschlossenen Verlängerung des Einsatzes zustimmt.Ergebnisse des BundestagsJa: 610(92%)Nein: 48(7%)Enthaltung: 2(1%)21. März 2024Gesamtstimmen: 66012. April 2024Selbstbestimmung in Bezug auf den GeschlechtseintragGesellschaftspolitikGleichstellungSelbstbestimmungTransgenderRechtDie Abstimmung betrifft den Entwurf eines Gesetzes über die Selbstbestimmung in Bezug auf den Geschlechtseintrag und zur Änderung weiterer Vorschriften. Das Gesetz zielt darauf ab, die Regelungen zur Änderung des Geschlechtseintrags zu vereinfachen und zu vereinheitlichen.Ergebnisse des BundestagsJa: 372(59%)Nein: 251(40%)Enthaltung: 11(1%)12. Apr. 2024Gesamtstimmen: 634Änderung des EnergiewirtschaftsgesetzesEnergiewirtschaftWasserstoffInfrastrukturNetzentwicklungFinanzierungDie Abstimmung betrifft ein Gesetz zur Änderung des Energiewirtschaftsgesetzes (EnWG), um den Aufbau einer nationalen Wasserstoffinfrastruktur zu fördern und die Finanzierung des Wasserstoff-Kernnetzes zu regeln.Ergebnisse des BundestagsJa: 347(61%)Nein: 202(36%)Enthaltung: 16(3%)12. Apr. 2024Gesamtstimmen: 56525. April 2024Operation EUNAVFOR MED IRINI (Beschlussempfehlung)AußenpolitikSicherheitspolitikEuropäische UnionMilitärLibyenDie Abstimmung betrifft die Fortsetzung der Beteiligung bewaffneter deutscher Streitkräfte an der Operation EUNAVFOR MED IRINI der Europäischen Union. Ziel ist es, das Waffenembargo gegen Libyen weiterhin durchzusetzen und zur Stabilisierung des Landes beizutragen.Ergebnisse des BundestagsJa: 554(84%)Nein: 100(15%)Enthaltung: 2(1%)25. Apr. 2024Gesamtstimmen: 65626. April 2024Erneuerbare-Energien-Gesetz (Solarpaket)Erneuerbare EnergienPhotovoltaikKlimaschutzEnergiewendeBürokratieabbauNetzausbauDas Gesetz zur Änderung des Erneuerbare-Energien-Gesetzes (EEG) zielt darauf ab, den Ausbau der Photovoltaik (PV) in Deutschland zu beschleunigen und zu steigern, um die Klimaziele zu erreichen. Es werden Anreize für Solaranlagen geschaffen und Bürokratie abgebaut.Ergebnisse des BundestagsJa: 384(58%)Nein: 79(12%)Enthaltung: 200(30%)26. Apr. 2024Gesamtstimmen: 66316. Mai 2024Ablehnung eines Antrags gegen den WHO-PandemievertragGesundheitspolitikInternationale BeziehungenSouveränitätWHOPandemieDemokratieDie Abstimmung betrifft den Antrag der AfD-Fraktion, den WHO-Pandemievertrag und die überarbeiteten Internationalen Gesundheitsvorschriften abzulehnen. Der Antrag zielt darauf ab, die Souveränität Deutschlands in Gesundheitsfragen zu wahren und eine vermeintliche Machtausweitung der WHO zu verhindern.Ergebnisse des BundestagsJa: 578(89%)Nein: 71(11%)Enthaltung: 1(0%)16. Mai 2024Gesamtstimmen: 65005. Juni 2024Ablehnung eines Antrags zur Abschaffung des SolidaritätszuschlagsSteuerpolitikFinanzenWirtschaftSolidaritätszuschlagAbgabenlastSteuerreformDie Abstimmung betrifft den Antrag der AfD-Fraktion zur Abschaffung des Solidaritätszuschlags als ersten Schritt einer umfassenden Steuerreform zur Entlastung von Mittelstand, Unternehmen und Arbeitnehmern. Der Finanzausschuss empfiehlt die Ablehnung des Antrags.Ergebnisse des BundestagsJa: 537(90%)Nein: 62(10%)Enthaltung: 0(0%)5. Juni 2024Gesamtstimmen: 59906. Juni 2024Ablehnung eines Antrags zur Bekämpfung des politischen IslamInneres und HeimatPolitischer IslamDemokratieRechtSicherheitMigrationDie Abstimmung betrifft den Antrag der CDU/CSU-Fraktion, den politischen Islam als Gefahr für die freiheitliche Demokratie wirksam zu bekämpfen. Der Ausschuss für Inneres und Heimat empfiehlt die Ablehnung des Antrags.Ergebnisse des BundestagsJa: 406(63%)Nein: 241(37%)Enthaltung: 1(0%)6. Juni 2024Gesamtstimmen: 648Beschleunigung immissionsschutzrechtlicher GenehmigungsverfahrenUmweltKlimaschutzEnergieWirtschaftGenehmigungsverfahrenErneuerbare EnergienDie Abstimmung betrifft ein Gesetz zur Verbesserung des Klimaschutzes, zur Beschleunigung von Genehmigungsverfahren im Immissionsschutz und zur Umsetzung von EU-Recht, mit dem Ziel, Verfahren zu vereinfachen und den Ausbau erneuerbarer Energien zu fördern.Ergebnisse des BundestagsJa: 377(59%)Nein: 257(40%)Enthaltung: 9(1%)6. Juni 2024Gesamtstimmen: 64313. Juni 2024Antrag zu Konsequenzen aus dem Terror von MannheimInnere SicherheitMigrationIntegrationTerrorismusbekämpfungRechtspolitikDer Antrag der CDU/CSU-Fraktion betrifft die Forderung nach klaren Konsequenzen aus dem Terroranschlag von Mannheim und zielt darauf ab, die Migrations- und Integrationspolitik zu verschärfen sowie die Sicherheitsmaßnahmen zu erhöhen.Ergebnisse des BundestagsJa: 253(38%)Nein: 399(60%)Enthaltung: 11(2%)13. Juni 2024Gesamtstimmen: 663Ablehnung eines Antrags zum Verbot des Vereins Muslim InteraktivInneres und HeimatVereinsrechtExtremismusIslamismusSicherheitspolitikDie Abstimmung betrifft den Antrag der AfD-Fraktion, den Verein Muslim Interaktiv zu verbieten, da dieser als extremistisch und gegen die freiheitliche demokratische Grundordnung gerichtet angesehen wird. Der Bundestag lehnt den Antrag ab.Ergebnisse des BundestagsJa: 577(88%)Nein: 68(10%)Enthaltung: 11(2%)13. Juni 2024Gesamtstimmen: 656Antrag zum Eintritt in die zweite Beratung zur Aufhebung des LieferkettengesetzesWirtschaftRechtSozialesEuropäische UnionDie Abstimmung betrifft einen Antrag der CDU/CSU-Fraktion, direkt in die zweite Beratung eines Gesetzentwurfs zur Aufhebung des Lieferkettensorgfaltspflichtengesetzes (LkSG) einzutreten.Ergebnisse des BundestagsJa: 250(38%)Nein: 401(62%)Enthaltung: 1(0%)13. Juni 2024Gesamtstimmen: 65227. Juni 2024Bundeswehreinsatz EUFOR ALTHEAAußenpolitikSicherheitspolitikEuropäische UnionBundeswehrBosnien und HerzegowinaDie Abstimmung betrifft die Fortsetzung der Beteiligung bewaffneter deutscher Streitkräfte an der EU-geführten Sicherheitsoperation EUFOR ALTHEA in Bosnien und Herzegowina bis zum 30. Juni 2025.Ergebnisse des BundestagsJa: 553(87%)Nein: 74(12%)Enthaltung: 8(1%)27. Juni 2024Gesamtstimmen: 635Bundeswehreinsatz im Kosovo (KFOR)AußenpolitikSicherheitspolitikVerteidigungKosovoBundeswehrInternationale BeziehungenDie Abstimmung betrifft die Fortsetzung der Beteiligung bewaffneter deutscher Streitkräfte an der internationalen Sicherheitspräsenz im Kosovo (KFOR). Es geht darum, das Mandat für den Einsatz der Bundeswehr im Rahmen von KFOR zu verlängern.Ergebnisse des BundestagsJa: 535(85%)Nein: 91(14%)Enthaltung: 2(1%)27. Juni 2024Gesamtstimmen: 628Bundeswehreinsatz im Libanon (UNIFIL)AußenpolitikSicherheitspolitikBundeswehrLibanonUNIFILFriedensmissionDie Abstimmung betrifft die Fortsetzung der Beteiligung bewaffneter deutscher Streitkräfte an der \"United Nations Interim Force in Lebanon\" (UNIFIL), einer Friedensmission der Vereinten Nationen im Libanon. Es geht um die Verlängerung des Mandats und die damit verbundenen Aufgaben und Befugnisse der deutschen Soldaten.Ergebnisse des BundestagsJa: 534(85%)Nein: 88(14%)Enthaltung: 3(1%)27. Juni 2024Gesamtstimmen: 62505. Juli 2024Änderung des SchwangerschaftskonfliktgesetzesSozialesFamilieFrauenrechteGesundheitRechtDie Abstimmung betrifft die Änderung des Schwangerschaftskonfliktgesetzes (SchKG) und zielt darauf ab, einen bundeseinheitlichen und rechtssicheren Umgang mit sogenannten Gehsteigbelästigungen vor Schwangerschaftsberatungsstellen und Einrichtungen zur Vornahme von Schwangerschaftsabbrüchen sicherzustellen.Ergebnisse des BundestagsJa: 381(69%)Nein: 169(31%)Enthaltung: 0(0%)5. Juli 2024Gesamtstimmen: 55017. Oktober 2024Entwurf zur Aufhebung des LieferkettengesetzesWirtschaftMenschenrechteUmweltLieferkettenGesetzgebungEuropäische UnionDie Abstimmung betrifft den Gesetzentwurf der CDU/CSU zur Aufhebung des Lieferkettensorgfaltspflichtengesetzes (LkSG), das Unternehmen zur Einhaltung von Menschenrechten in ihren Lieferketten verpflichtet.Ergebnisse des BundestagsJa: 247(37%)Nein: 412(62%)Enthaltung: 1(1%)17. Okt. 2024Gesamtstimmen: 660Gesetzentwurf zur KrankenhausreformGesundheitKrankenhausVersorgungFinanzierungQualitätssicherungDie Abstimmung betrifft den Entwurf eines Gesetzes zur Verbesserung der Versorgungsqualität im Krankenhaus und zur Reform der Vergütungsstrukturen (KHVVG), das darauf abzielt, die Krankenhausversorgung zu verbessern und die Finanzierung zu reformieren.Ergebnisse des BundestagsJa: 373(57%)Nein: 285(43%)Enthaltung: 1(0%)17. Okt. 2024Gesamtstimmen: 659Bundeswehreinsatz im IrakAußenpolitikSicherheitspolitikMilitärIrakTerrorismusbekämpfungDie Abstimmung betrifft die Fortsetzung des Einsatzes bewaffneter deutscher Streitkräfte im Irak, um die Stabilisierung des Landes zu sichern, ein Wiedererstarken des IS (Islamischer Staat) zu verhindern und die Versöhnung im Irak zu fördern. Der Bundestag stimmt über den Antrag der Bundesregierung ab, diesen Einsatz bis zum 31. Januar 2026 mit bis zu 500 Soldatinnen und Soldaten zu verlängern.Ergebnisse des BundestagsJa: 539(85%)Nein: 93(15%)Enthaltung: 2(0%)17. Okt. 2024Gesamtstimmen: 63418. Oktober 2024Änderungsantrag zur „Verbesserung der inneren Sicherheit“Innere SicherheitAsylrechtMigrationspolitikGrenzschutzTerrorismusbekämpfungWaffenrechtDie Abstimmung betrifft einen Änderungsantrag der CDU/CSU-Fraktion zu einem Gesetzentwurf, der die innere Sicherheit und das Asylsystem verbessern soll, insbesondere durch Änderungen im Asylgesetz und im Ausweisungsrecht.Ergebnisse des BundestagsJa: 254(39%)Nein: 404(61%)Enthaltung: 1(0%)18. Okt. 2024Gesamtstimmen: 659Artikel 5 des Entwurfs „Verbesserung der inneren Sicherheit“ (2. Beratung)InneresSicherheitWaffenrechtExtremismusTerrorismusÖffentliche SicherheitDie Abstimmung betrifft Artikel 5 eines Gesetzentwurfs, der darauf abzielt, das Waffenrecht zu verschärfen, um Extremisten und Terroristen den Zugang zu Waffen zu erschweren und die öffentliche Sicherheit zu erhöhen.Ergebnisse des BundestagsJa: 375(57%)Nein: 275(42%)Enthaltung: 4(1%)18. Okt. 2024Gesamtstimmen: 654Gesetzentwurf „Verbesserung der Terrorismusbekämpfung“Innere SicherheitTerrorismusbekämpfungExtremismuspräventionDatenschutzBürgerrechteWaffenrechtDer Gesetzentwurf zur Verbesserung der Terrorismusbekämpfung zielt darauf ab, den Sicherheitsbehörden im digitalen Raum zusätzliche Befugnisse zu geben und die Zusammenarbeit zwischen Bund, Ländern und zivilgesellschaftlichen Akteuren zu stärken, um extremistischen und islamistischen Tendenzen entgegenzuwirken.Ergebnisse des BundestagsJa: 367(56%)Nein: 280(43%)Enthaltung: 4(1%)18. Okt. 2024Gesamtstimmen: 651Gesetzentwurf „Verbesserung der inneren Sicherheit“Innere SicherheitAsylrechtWaffenrechtExtremismusTerrorismusbekämpfungDer Gesetzentwurf zur Verbesserung der inneren Sicherheit und des Asylsystems betrifft die Anpassung von Gesetzen, um die innere Sicherheit Deutschlands zu stärken und das Asylsystem effektiver zu gestalten. Er reagiert auf aktuelle Bedrohungen und soll den Behörden mehr Handlungsspielraum geben.Ergebnisse des BundestagsJa: 357(55%)Nein: 289(44%)Enthaltung: 8(1%)18. Okt. 2024Gesamtstimmen: 654Erster Entschließungsantrag zur „Verbesserung der inneren Sicherheit“Innere SicherheitAsylpolitikMigrationspolitikWaffenrechtIslamismusExtremismusbekämpfungDie Abstimmung betrifft den Entschließungsantrag der CDU/CSU zum Gesetzentwurf zur Verbesserung der inneren Sicherheit und des Asylsystems, der sich mit der Migrationspolitik, dem Waffenrecht und der Bekämpfung des Islamismus auseinandersetzt.Ergebnisse des BundestagsJa: 237(36%)Nein: 405(62%)Enthaltung: 8(2%)18. Okt. 2024Gesamtstimmen: 650Zweiter Entschließungsantrag zur „Verbesserung der inneren Sicherheit“Innere SicherheitAsylpolitikWaffenrechtMigrationspolitikExtremismuspräventionDie Abstimmung betrifft einen Entschließungsantrag der CDU/CSU zum Gesetzentwurf zur Verbesserung der inneren Sicherheit und des Asylsystems, der sich gegen die Schwerpunkte der Regierungsfraktionen im Bereich Waffenrecht richtet und stattdessen eine andere Migrations- und Sicherheitspolitik fordert.Ergebnisse des BundestagsJa: 185(29%)Nein: 373(59%)Enthaltung: 73(12%)18. Okt. 2024Gesamtstimmen: 631Dritter Entschließungsantrag zur „Verbesserung der inneren Sicherheit“Innere SicherheitAsylrechtMigrationGrenzschutzEU-RechtDie Abstimmung betrifft einen Entschließungsantrag der CDU/CSU-Fraktion zu einem Gesetzentwurf, der die innere Sicherheit und das Asylsystem verbessern soll. Der Antrag zielt darauf ab, die Bundesregierung zu umfassenden Zurückweisungen von Personen ohne Einreiserecht an den deutschen Grenzen aufzufordern.Ergebnisse des BundestagsJa: 249(39%)Nein: 394(61%)Enthaltung: 1(0%)18. Okt. 2024Gesamtstimmen: 644Ablehnung eines Antrags zu MesserangriffenInneresRechtKriminalitätWaffenrechtMigrationSicherheitDie Beschlussempfehlung des Ausschusses für Inneres und Heimat betrifft die Ablehnung eines Antrags der AfD-Fraktion, der eine gezielte Sanktionierung von Messerangriffen anstelle von Verschärfungen im Waffenrecht fordert.Ergebnisse des BundestagsJa: 567(89%)Nein: 62(10%)Enthaltung: 7(1%)18. Okt. 2024Gesamtstimmen: 636Ablehnung eines Antrags zu DrittstaatenangehörigenAsylrechtMigrationspolitikGrenzkontrolleRücknahmeabkommenDie Abstimmung betrifft den Antrag der AfD-Fraktion, die Bundesregierung zu verpflichten, Drittstaatsangehörige an den Außengrenzen Deutschlands zurückzuweisen und bilaterale Rücknahmeabkommen mit Nachbarstaaten anzuwenden.Ergebnisse des BundestagsJa: 572(89%)Nein: 62(10%)Enthaltung: 8(1%)18. Okt. 2024Gesamtstimmen: 64216. Dezember 2024Antrag gemäß Artikel 68 des Grundgesetzes (Vertrauensfrage)PolitikRegierungGrundgesetzVertrauensfrageDie Abstimmung betrifft die Vertrauensfrage gemäß Artikel 68 des Grundgesetzes, bei der der Bundeskanzler das Vertrauen des Bundestages ausspricht. Der Bundeskanzler beabsichtigt, vor der Abstimmung eine Erklärung abzugeben.Ergebnisse des BundestagsJa: 207(29%)Nein: 394(55%)Enthaltung: 116(16%)16. Dez. 2024Gesamtstimmen: 71719. Dezember 2024Gesetzentwurf zur Änderung von Artikel 93 und 94 des Grundgesetzes (Bundesverfassungsgericht)VerfassungsrechtBundesverfassungsgerichtRechtsstaatPolitikJustizDer Gesetzentwurf betrifft die Änderung des Grundgesetzes in Bezug auf die Stellung und Struktur des Bundesverfassungsgerichts, indem Artikel 93 und 94 punktuell ergänzt und deren Inhalte systematisch neu geordnet werden. Ziel ist es, die Organqualität des Bundesverfassungsgerichts sowie die unmittelbare Bindung der öffentlichen Gewalt an seine Entscheidungen stärker hervorzuheben.Ergebnisse des BundestagsJa: 600(90%)Nein: 69(10%)Enthaltung: 0(0%)19. Dez. 2024Gesamtstimmen: 669",
    "summary": {
      "en": "In 2024, the Bundestag (German parliament) held several important votes on various issues:\n\n1. **Military Support for Ukraine**: A proposal to supply TAURUS cruise missiles to Ukraine was rejected, with 73% voting against it.\n\n2. **Agriculture Support**: A request to better support German agriculture was also rejected, with 62% against it.\n\n3. **Peace Initiative for Ukraine and Russia**: A proposal for a peace initiative was accepted by 89% of votes.\n\n4. **Modernizing Citizenship Law**: The modernization of citizenship laws to facilitate naturalization was approved, with 60% in favor.\n\n5. **Election Law Reform**: Changes to adjust electoral districts based on population were accepted with 57% support.\n\n6. **Support for Ukraine**: A proposal to support Ukraine amid the ongoing war received 57% approval.\n\n7. **Cannabis Regulation**: A law to regulate cannabis use was passed with 64% approval.\n\n8. **Military Missions**: Extensions for various military missions (in the Mediterranean, South Sudan, Bosnia, Kosovo, and Lebanon) were mostly approved with strong majorities.\n\n9. **Internal Security and Asylum Policies**: Several proposals aimed at enhancing internal security and addressing asylum processes faced significant opposition, with many being rejected.\n\n10. **Health Laws**: Changes to the pregnancy conflict law and a proposal for hospital reforms were passed.\n\n11. **Economic Measures**: Proposals addressing tax policy and support for the agricultural sector were largely rejected.\n\n12. **Constitutional Changes**: An amendment to clarify the role of the Federal Constitutional Court was overwhelmingly approved.\n\nThroughout the year, the Bundestag showed a mix of support and rejection for various military, social, and economic initiatives, reflecting the ongoing political debates in Germany.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "781d9ca14c79dac7",
    "title": {
      "en": "Writing your own C++ standard library from scratch",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://nibblestew.blogspot.com/2025/03/writing-your-own-c-standard-library.html",
    "score": 109,
    "by": "JNRowe",
    "time": 1742890236,
    "content": "Nibble Stew\n\nA gathering of development thoughts of Jussi Pakkanen. Some of you may know him as the creator of the Meson build system. jpakkane at gmail dot com\n\nMonday, March 24, 2025\n\nWriting your own C++ standard library from scratch\n\nThe C++ standard library (also know as the STL) is, without a doubt, an astounding piece of work. Its scope, performance and incredible backwards compatibility have taken decades of work by many of the world's best programmers. My hat's off to all those people who have contributed to it.All of that is not to say that it is not without its problems. The biggest one being the absolutely abysmal compile times but unreadability, and certain unoptimalities caused by strict backwards compatibility are also at the top of the list. In fact, it could be argued that most of the things people really dislike about C++ are features of the STL rather than the language itself. Fortunately, using the STL is not mandatory. If you are crazy enough, you can disable it completely and build your own standard library in the best Bender style.One of the main advantages of being an unemployed-by-choice open source developer is that you can do all of that if you wish. There are no incompetent middle damagers hovering over your shoulder to ensure you are \"producing immediate customer value\" rather than \"wasting time on useless polishing that does not produce immediate customer value\".It's my time, and I'll waste it if I want to!What's in it?The biggest design questions of a standard library are scope and the \"feel\" of the API. Rather than spending time on design, we steal it. Thus, when in doubt, read the Python stdlib documentation and replicate it. Thus the name of the library is pystd.The test appTo keep the scope meaningful, we start by writing only enough of stdlib to build an app that reads a text file, validates it as UTF-8, splits the contents into words, counts how many time each word appears in the file and prints all words and how many times it appears sorted by decreasing count.This requires, at least:File handlingStringsUTF8 validationA hash mapA vectorSortingThe training wheels come offThe code is available in this Github repo for those who want to follow along at home.Disabling the STL is fairly easy (with Linux+GCC at least) and requires only these two Meson statements:add_global_arguments('-nostdinc++', language: 'cpp')\nadd_global_link_arguments('-nostdlib++', '-lsupc++', language: 'cpp')The supc++ library is (according to stackoverflow) a support library GCC needs to implement core language features. Now the stdlib is off and it is time to implement everything with sticks, stones and duct tape.The outcomeOnce you have implemented everything discussed above and auxiliary stuff like a hashing framework the main application looks like this.The end result is both Valgrind and Asan clean. There is one chunk of unreleased memory, but that comes from supc++. There is probably UB in the implementation. But it should be the good kind of UB that, if it would actually not work, would break the entire Linux userspace because everything depends on it working \"as expected\".All of this took fewer than 1000 lines of code in the library itself (including a regex implementation that is not actually used). For comparison merely including vector from the STL brings in 27 thousand lines of code.Comparison to an STL versionConverting this code to use the STL is fairly simple and only requires changing some types and fine tuning the API. The main difference is that the STL version does not validate that the input is UTF-8 as there is no builtin function for that. Now we can compare the two.Runtime for both is 0.001 to 0.002 seconds on the small test file I used. Pystd is not noticeably slower than the STL version, which is enough for our purposes. It almost certainly scales worse because there has been zero performance work on it.Compiling the pystd version with -O2 takes 0.3 seconds whereas the STL version takes 1.2 seconds. The measurements were done on a Ryzen 7 3700X processor.The executable's unstripped size is 349k for STL and 309k for pystd. The stripped sizes are 23k and 135k. Approximately 100 k of the pystd executable comes from supc++. In the STL version that probably comes dynamically from libstdc++(which, on this machine, takes 2.5 MB).Perfect ABI stabilityDesigning a standard library is exceedingly difficult because you can't ever really change it. Someone, somewhere, is depending on every misfeature in it so they can never be changed.Pystd has been designed to both support perfect ABI stability and make it possible to change it in arbitrary ways in the future. If you start from scratch this turned out to be fairly simple.The sample code above used the pystd namespace. It does not actually exist. Instead it is defined like this in the cpp file:#include <pystd2025.hpp>namespace pystd = pystd2025;In pystd all code is in a namespace with a year and is stored in a header file with the same year. The idea is, then, that every year you create a new release. This involves copying all stdlib header files to a file with the new year and regexping the namespace declarations to match. The old code is now frozen forever (except for bug fixes) whereas the new code can be changed at will because there arezero existing lines of code that depend on it.End users now have the choice of when to update their code to use newer pystd versions. Even better, if there is an old library that can not be updated, any of the old versions can be used in parallel. For example:pystd2030::SomeType foo;pystd2025::SomeType bar(foo.something(), foo.something_else());Thus if no code is ever updated, everything keeps working. If all code is updated at once, everything works. If only parts of the code are updated, things can still be made to work with some glue code. This puts the maintenance burden on the people whose projects can not be updated as opposed to every other developer in the world. This is as it should be, and also would motivate people with broken deps to spend some more effort to get them fixed.\n\nPosted by\n\nJussi\n\nat\n\n5:03 PM\n\nEmail ThisBlogThis!Share to XShare to FacebookShare to Pinterest\n\nNo comments:\n\nPost a Comment\n\n      BLOG_CMT_createIframe('https://www.blogger.com/rpc_relay.html');\n\nOlder Post\n\nHome\n\nSubscribe to:\nPost Comments (Atom)\n\nAbout Me\n\nJussi\n\nView my complete profile\n\nBlog Archive\n\n        ▼\n\n2025\n\n(5)\n\n        ▼\n\nMarch\n\n(1)\n\nWriting your own C++ standard library from scratch\n\n        ►\n\nFebruary\n\n(3)\n\n        ►\n\nJanuary\n\n(1)\n\n        ►\n\n2024\n\n(21)\n\n        ►\n\nDecember\n\n(4)\n\n        ►\n\nNovember\n\n(1)\n\n        ►\n\nOctober\n\n(2)\n\n        ►\n\nSeptember\n\n(1)\n\n        ►\n\nAugust\n\n(2)\n\n        ►\n\nJuly\n\n(1)\n\n        ►\n\nJune\n\n(1)\n\n        ►\n\nMay\n\n(1)\n\n        ►\n\nApril\n\n(3)\n\n        ►\n\nMarch\n\n(2)\n\n        ►\n\nFebruary\n\n(1)\n\n        ►\n\nJanuary\n\n(2)\n\n        ►\n\n2023\n\n(41)\n\n        ►\n\nDecember\n\n(5)\n\n        ►\n\nNovember\n\n(2)\n\n        ►\n\nOctober\n\n(2)\n\n        ►\n\nSeptember\n\n(3)\n\n        ►\n\nAugust\n\n(1)\n\n        ►\n\nJuly\n\n(3)\n\n        ►\n\nJune\n\n(4)\n\n        ►\n\nMay\n\n(4)\n\n        ►\n\nApril\n\n(4)\n\n        ►\n\nMarch\n\n(6)\n\n        ►\n\nFebruary\n\n(5)\n\n        ►\n\nJanuary\n\n(2)\n\n        ►\n\n2022\n\n(34)\n\n        ►\n\nDecember\n\n(4)\n\n        ►\n\nNovember\n\n(3)\n\n        ►\n\nOctober\n\n(2)\n\n        ►\n\nSeptember\n\n(4)\n\n        ►\n\nAugust\n\n(2)\n\n        ►\n\nJuly\n\n(3)\n\n        ►\n\nJune\n\n(2)\n\n        ►\n\nMay\n\n(2)\n\n        ►\n\nApril\n\n(4)\n\n        ►\n\nFebruary\n\n(5)\n\n        ►\n\nJanuary\n\n(3)\n\n        ►\n\n2021\n\n(29)\n\n        ►\n\nDecember\n\n(1)\n\n        ►\n\nNovember\n\n(1)\n\n        ►\n\nOctober\n\n(1)\n\n        ►\n\nSeptember\n\n(2)\n\n        ►\n\nAugust\n\n(1)\n\n        ►\n\nJuly\n\n(5)\n\n        ►\n\nJune\n\n(3)\n\n        ►\n\nMay\n\n(5)\n\n        ►\n\nApril\n\n(2)\n\n        ►\n\nMarch\n\n(4)\n\n        ►\n\nFebruary\n\n(3)\n\n        ►\n\nJanuary\n\n(1)\n\n        ►\n\n2020\n\n(42)\n\n        ►\n\nDecember\n\n(1)\n\n        ►\n\nNovember\n\n(5)\n\n        ►\n\nOctober\n\n(5)\n\n        ►\n\nSeptember\n\n(2)\n\n        ►\n\nAugust\n\n(4)\n\n        ►\n\nJuly\n\n(5)\n\n        ►\n\nJune\n\n(2)\n\n        ►\n\nMay\n\n(4)\n\n        ►\n\nApril\n\n(3)\n\n        ►\n\nMarch\n\n(4)\n\n        ►\n\nFebruary\n\n(5)\n\n        ►\n\nJanuary\n\n(2)\n\n        ►\n\n2019\n\n(27)\n\n        ►\n\nDecember\n\n(4)\n\n        ►\n\nNovember\n\n(3)\n\n        ►\n\nOctober\n\n(1)\n\n        ►\n\nSeptember\n\n(2)\n\n        ►\n\nAugust\n\n(3)\n\n        ►\n\nJuly\n\n(2)\n\n        ►\n\nJune\n\n(2)\n\n        ►\n\nMay\n\n(3)\n\n        ►\n\nApril\n\n(1)\n\n        ►\n\nFebruary\n\n(6)\n\n        ►\n\n2018\n\n(24)\n\n        ►\n\nNovember\n\n(4)\n\n        ►\n\nOctober\n\n(1)\n\n        ►\n\nSeptember\n\n(1)\n\n        ►\n\nAugust\n\n(3)\n\n        ►\n\nJuly\n\n(3)\n\n        ►\n\nJune\n\n(1)\n\n        ►\n\nApril\n\n(3)\n\n        ►\n\nMarch\n\n(4)\n\n        ►\n\nFebruary\n\n(4)\n\n        ►\n\n2017\n\n(35)\n\n        ►\n\nDecember\n\n(4)\n\n        ►\n\nNovember\n\n(1)\n\n        ►\n\nSeptember\n\n(1)\n\n        ►\n\nAugust\n\n(3)\n\n        ►\n\nJuly\n\n(3)\n\n        ►\n\nJune\n\n(3)\n\n        ►\n\nMay\n\n(5)\n\n        ►\n\nApril\n\n(4)\n\n        ►\n\nMarch\n\n(3)\n\n        ►\n\nFebruary\n\n(2)\n\n        ►\n\nJanuary\n\n(6)\n\n        ►\n\n2016\n\n(20)\n\n        ►\n\nDecember\n\n(4)\n\n        ►\n\nNovember\n\n(2)\n\n        ►\n\nOctober\n\n(1)\n\n        ►\n\nSeptember\n\n(2)\n\n        ►\n\nAugust\n\n(1)\n\n        ►\n\nJuly\n\n(1)\n\n        ►\n\nJune\n\n(2)\n\n        ►\n\nMay\n\n(2)\n\n        ►\n\nApril\n\n(4)\n\n        ►\n\nMarch\n\n(1)\n\n        ►\n\n2015\n\n(19)\n\n        ►\n\nDecember\n\n(2)\n\n        ►\n\nNovember\n\n(3)\n\n        ►\n\nOctober\n\n(3)\n\n        ►\n\nSeptember\n\n(2)\n\n        ►\n\nAugust\n\n(3)\n\n        ►\n\nJuly\n\n(5)\n\n        ►\n\nJune\n\n(1)\n\nSimple theme. Powered by Blogger.",
    "summary": {
      "en": "**Summary of \"Writing Your Own C++ Standard Library from Scratch\"**\n\nJussi Pakkanen discusses the challenges and potential benefits of creating your own C++ standard library, called \"pystd,\" instead of using the existing C++ standard library (STL). While the STL is powerful, it has drawbacks like long compile times and certain inefficiencies. \n\nPakkanen highlights the freedom of being an open-source developer, which enables him to experiment without corporate pressure. He outlines the initial goals for his library, including building a simple application to read a UTF-8 text file, count word occurrences, and display the results.\n\nHe explains how to disable the STL when compiling using Meson, allowing the development of a lightweight library with less code. The pystd library requires fewer than 1,000 lines of code, significantly less than the STL.\n\nThe performance of pystd is comparable to the STL, with similar execution times but faster compilation. Pakkanen emphasizes the importance of maintaining ABI (Application Binary Interface) stability in the library, making it easier to update without breaking existing code. \n\nFinally, he proposes a yearly versioning system for the library, allowing users to choose when to upgrade while ensuring compatibility with older versions. This approach shifts the responsibility of maintaining compatibility to users with outdated dependencies.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "a802eba4eeccd3c4",
    "title": {
      "en": "If you get the chance, always run more extra network fiber cabling",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://utcc.utoronto.ca/~cks/space/blog/sysadmin/RunMoreExtraNetworkFiber",
    "score": 32,
    "by": "hggh",
    "time": 1742910059,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "b058438edaa5d019",
    "title": {
      "en": "Samsung CEO Jong-hee Han has died",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.engadget.com/big-tech/samsung-ceo-jong-hee-han-has-died-120029286.html",
    "score": 140,
    "by": "ksec",
    "time": 1742907894,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "dd21d9bbd9e6583f",
    "title": {
      "en": "Search My Site – open-source search engine for personal and independent websites",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://searchmysite.net",
    "score": 182,
    "by": "OuterVale",
    "time": 1742869143,
    "content": "Browse\n\n\t\t\t\t\t\t\t\t\tNewest\n\n\t\t\t\t\t\t\t\t\t\tRandom\n\n\t\t\t\t\t\t\tSearch real content by real people from the indieweb / the small web / digital gardens.\n\t\t\t\t\t\t\tSites are ranked higher when they have no ads.\n\t\t\t\t\t\t\tFully open source. Also a search as a service.\n\n\t\t\t\t\tAbout searchmysite.net\n\n\t\t\t\t\t\tWhat is the search engine and why should I use it?\n\n\t\t\t\t\t\tThe searchmysite.net search engine is a niche search, focussing on the \"indieweb\" or \"small web\" or \"digital gardens\", i.e. non-commercial content, primarily personal and independent websites.\n\n\t\t\t\t\t\tIf you want to research people's personal experiences of or deep-dives into certain topics, hobbies or interests, then you may find the searchmysite.net public search useful to avoid having to wade through all the marketing websites and blog spam that fill the big search engines. It differs from the commercial search engines because it:\n\n\t\t\t\t\t\tIndexes only user-submitted and moderated sites, rather than indexing the entire internet with all of its spam, \"search engine optimisation\" and \"click-bait\" content.\n\t\t\t\t\t\tDoes not show adverts and promotes advert-free results pages, to remove the incentives for spam and surveillance capitalism.\n\t\t\t\t\t\tAims to have a sustainable and user-aligned operating model, planning to pay running costs via the \"search as a service\" features, rather than relying on advertising which could put it in conflict with user needs.\n\t\t\t\t\t\tOffers an unusually high level of privacy for a search engine, thanks to the operating model which is not based on advertising or collecting personal information for sale to advertisers (see the Privacy Policy for full details).\n\t\t\t\t\t\tIs fully open source, for even greater transparency of the searching, ranking and indexing processes, and for greater community involvement in improving the service.",
    "summary": {
      "en": "**Summary of searchmysite.net**\n\nSearchmysite.net is a specialized search engine that focuses on the \"indieweb\" or \"small web,\" which includes personal and independent websites that are not commercial. Here are the key points:\n\n- **Niche Focus**: It indexes user-submitted and moderated sites, avoiding spam and ads commonly found on larger search engines.\n- **Ad-Free Experience**: There are no advertisements, which helps prevent spam and supports privacy.\n- **Sustainable Model**: The service plans to cover costs through \"search as a service\" rather than relying on advertising.\n- **Privacy**: It offers high privacy levels because it does not collect personal data for advertising purposes.\n- **Open Source**: The search engine is fully open source, promoting transparency and community involvement.\n\nThis makes searchmysite.net useful for finding genuine, personal content on various topics without the clutter of commercial sites.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "9e37172a8989477d",
    "title": {
      "en": "Closing the Chapter on OpenH264",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://bbhtt.space/posts/closing-the-chapter-on-openh264/",
    "score": 45,
    "by": "todsacerdoti",
    "time": 1742896798,
    "content": "People might have noticed me talking about dropping OpenH264 from\nFreedesktop SDK.\nHere, I’ll try to go a bit into the history, the timeline and what led\nto the final decision.\nA bit of an introduction\nIf you are unfamiliar with the Freedesktop SDK project: it was born out\nof the initial 1.6 Flatpak runtime image\ncreated by Alexander Larsson to\nprovide a host independent “runtime” for Flatpaks. Since then, with the\nhelp of Codethink and others, it grew\ninto an independent project maintained by the community that aims to\nprovide a “A minimal Linux runtime”.\nCurrently that includes the org.freedesktop.{Sdk, Platform}\nFlatpak runtimes,\na bunch of other Flatpak extensions\nsuch as the GL (Mesa) extensions, a bootable image stack that includes\nthings like the Linux kernel, firmware and drivers and a collection\nof docker images.\nThe runtimes (and more broadly the “Flatpak” stack) form the base for\nthe GNOME and\nKDE Flatpak\nruntimes which are collectively used by 2000+ Flatpaks, while the\nbootable stack along with other parts forms the base for things like\nGNOME OS.\nSome history\nH.264 is one of the most widely used\ncodecs today but unfortunately it is patented with several patents\nstill active.\nPatents like this are a blocker to shipping software dealing with the\ncodec in the base runtime (since we want it usable by a wide variety of\nvendors and free of any legal grey areas) and unfortunately makes life\ndifficult for everyone involved.\nTo workaround this and ship working software to users, 6 years ago, in\n2019 the precursor to the OpenH264 extension, called the\nhtml5-codecs extension\nwas added to the Freedesktop runtime by Tom Coldrick. The idea was\nsimple - it would contain support for FFMPEG’s internal H.264 decoder\nand would be an optional Flatpak extension since we were unable to ship\nit in the runtime itself.\nFast-forward a few months, in June 2019, Robert McQueen\nopened an issue\nto include Cisco’s OpenH264\n(commonly referred as libopenh264 too) as an extension to the runtime.\nlibopenh264 code is open source but due to the H.264 patents, no\nvendor is legally allowed to distribute their own binaries (some vendors\nbuild it themselves and hand it to Cisco but no one made that arrangement\nfor us). The solution to this was to distribute Cisco’s unmodified\nbinaries directly to the user which would effectively be free of any\nroyalties but the catch is, the binaries have\nsome license restrictions\non them.\nSo Endless around that time, added extra-data\nsupport to Flatpak. This meant that the Flatpak extension metadata\nwould only contain an URL to Cisco’s binaries, checksums and a “recipe”\nto make a working extension out of it. The binary would be downloaded\non the end user’s machine, inside a sandbox and then the “recipe”\nwould be executed to create a working extension. This avoids various\n“redistribution” restrictions entirely since we aren’t shipping the\nbinary itself.\nHowever, one last catch remained with this approach. Unless the end user\nsystem has downloaded the extension, a part of the ABI will be missing\nfrom the base runtime itself. Additionally, the extension would also\nget mounted to a non-standard location inside the Flatpak sandbox. So to\nbuild something against that we needed a “public” library that is\nincluded in the runtime itself.\nEndless came up with a stub OpenH264 library called noopenhh264,\nas a solution to this. This would be kept ABI/API compatible to the\nactual Cisco’s OpenH264 library and would live inside the runtime at\n$libdir to allow software to link to libopenh264 and provide a\nfallback. At runtime, if the user downloads the actual OpenH264\nextension, the stub library would get overridden by the library in\nextension through Flatpak’s ld.so config and voilà, you have a working\nOpenH264 setup!\nThe start of the openh264 extension\nFollowing the gruelling details above, in August 2019, Tom Coldrick\nagain added\nthis “noopenh264” library to the runtime and set up the extension point\ncalled org.freedesktop.Platform.openh264.\nAround that time, the development of the “stub” noopenh264 library also\nmoved under the Freedesktop SDK umbrella\nwhile the development of this extra-data extension was going on in a\ndedicated repository.\nI talked about a “recipe” of an extra-data Flatpak extension a while\nback, which is nothing but a set of commands in a special file called\napply_extra to stage and install the source. As an example, if it\nwas a .deb it would be a sequence of ar or bsdtar commands to\nextract it, then install it inside Flatpak’s extra-data prefix.\nThe catch here is that using any utility like that makes the extension\ndirectly dependent on a particular branch of the Flatpak runtime and thus\nthe API/ABI provided by it.\nAs every yearly major release of the Freedesktop runtime is a completely\nnew ABI, we would need to continuously spin off new branches of the\nOpenH264 extension i.e. for version 2.1.0 of libopenh264 we would need\norg.freedesktop.Platform.openh264//2.1.0-{18.08, 19.08, 20.08} branches\nfor the extension and so on. This would make it a bit of an chore to\nupdate the extension in Freedesktop runtime itself and also for\nour downstream runtimes.\nAs a solution to this, a custom apply_extra script was written,\nwhich utilised only a few standard headers and would be built statically\nagainst a fixed toolchain.\nThe meant a bit of complexity, since we aren’t allowed to use most\nthings that would make this process easier but it also meant the final\nextension was independent of the runtime API/ABI and was built on top\nof “NoRuntime”.\nThe flaws start to show up\nThis entire setup worked quite well for a long time despite some\nmaintenance issues from Cisco. However there were some flaws.\nA while back I told that every vendor is forced to distribute binaries\nhosted by Cisco at https://ciscobinary.openh264.org. Unfortunately\nciscobinary.openh264.org is missing a valid SSL certificate\nat least since 2014\nand Cisco neither supplies GPG signatures nor strong checksums like\nSHA-256 for the binary releases.\nThis meant that we have no way to verify the authenticity of Cisco’s\nbinaries, opening us to various classes of MiTM and supply-chain\nvulnerabilities. We reached out to Mozilla and via Fedora/RedHat to get\nCisco to fix this, as one can see the comments on the thread but\nnothing ever happened.\nMore recently (around 2024-2025), this SSL issue meant that the domain\nstarted getting DNS blocked\nand would make the extra-data download fail, botching an install.\nAnother critical issue was that if upstream did security fixes and then\nreleased them with an ABI break, we couldn’t fix it for the Flatpak\nruntimes. Distributing binaries directly through extra-data meant there\nis no scope for cherry-picking patches and the ABI break meant the new\nversion would only be able go to a new branch of the runtime.\nWe didn’t really have a choice here. If we dropped this extension\nconsidering the potential flaws, the entirety of the Flatpak user base\nwould loose H.264 decoding and encoding support, so we had to live with\nthe setup.\nThe start of the codecs-extra extension\nAround June 2024 gdk-pixbuf due to a series of security issues,\ndropped support\nfor a lot of “fringe” loaders. We immediately noticed the change as\nwe constantly analyse ABI in stable branches.\nTo compensate for the dropped loaders, we decided\nto add webp, avif, jxl, heif support (with pixbuf loaders) to the\nruntime. Around the same time, Will Thompson\nopened an issue\nto add libheif to the runtime with the problematic HEIC decoder as a\nruntime extension.\nConsequently, we decided that yet another extension just for two libheif\nplugins is not worth the maintenance effort and these would rather be\nin the ffmpeg-full extension (This was the successor to the\nhtml5-codecs extension, created around 2019-2020). So I again\nadded\nlibx265, libde265 and the corresponding libheif plugins to the\nffmpeg-full extension. After a few months I also added\nlibx264 to it too as we were already shipping libx265 and Cisco’s\nOpenH264 had suboptimal encoding performance.\nI was happy with the result and all of this shipped in Freedesktop SDK\n24.08. But some people were unhappy for valid reasons.\nWhen the ffmpeg-full extension was created, due to various concerns\nit was never added to the runtime or SDK manifest; it lived as an\noptional “app” extension - meaning app developers would need to add a\nshort snippet\nin their Flatpak manifest to use it.\nThis was problematic in some ways - sometimes app developers were\nunaware of the extension or that they needed it and the app manifest\nwas missing the snippet that allowed to use the extension. This made\nuser and developer experience a bit poor. Once we started\nshipping libheif in ffmpeg-full, it became slightly more\nproblematic as the actual libheif library was in the runtime but without\nthe extension it cannot decode any HEIF/HEIC image. So the extension\nwas practically mandatory. Albert Astals Cid,\npart of the team who maintains KDE Flatpaks on Flathub complained about\nthis and an issue was opened by Erick555\nto discuss the possibility of making ffmpeg-full a runtime extension.\nI was initially reluctant to do this for various reasons and the\nsituation was again a bit out of our hands (this time due to H.265\npatents) but we asked various parties such as Endless and quite\nsurprisingly it turned out no one really had an objection to making it\na runtime extension or even make it auto-download along with the\nruntime.\nAlong with this change, we decided to rename the ffmpeg-full extension\nagain, this time to codecs-extra to better reflect the fact that it\nwill not only contain FFMPEG with patented parts but other libraries\ntoo that deals with patented codecs. This would sort of be an equivalent\nto the various “meta-packages” that distros provide for patented or\nrestricted codecs.\nThus the seeds were planted and I switched ffmpeg-full to\ncodecs-extra a month ago.\nThis is supposed to ship in Freedesktop SDK 25.08. The major change for\napp developers is that there is no longer any need to have something\nlike\nadd-extensions:\n  org.freedesktop.Platform.ffmpeg-full:\n    version: '24.08'\n    directory: lib/ffmpeg\n    add-ld-path: .\nin the manifest. org.freedesktop.Platform.codecs-extra will be\nautomatically installed by the runtime and will be available to users.\nDropping the openh264 extension\nThe above codecs-extra change meant that we now didn’t really have an\nuse case for org.freedesktop.Platform.openh264 since codecs-extra\nhad FFMPEG’s internal H.264 decoder and the libx264 encoder.\nSo I initially disabled autodownload and considering the various “flaws”\nwith OpenH264 discussed above, I opened an issue\nfor planned retirement in 26.08.\nThis was completely overturned when a high severity flaw\nwas discovered in libopenh264 affecting versions 2.5.0 and earlier.\nThe 23.08 branch of the Freedesktop runtime was locked to 2.2.0 and\nupgrading to a fixed version was not possible due to multiple ABI breaks\nand SONAME bumps upstream. Patching was also not an option as I said\nearlier.\nWe scrambled to mitigate this. For 23.08, we decided to make an\n“ABI break” and drop the extension\nas updating was impossible. So I made a bit of an intimidating announcement.\nBut we still had 24.08 left which was on 2.4.1 and updating to the fixed\nversion i.e. 2.6.0 was again blocked due to an ABI bump.\nHowever, looking through the commits between 2.4.1 and 2.6.0 and\nanalysing the library through libabigail we couldn’t find an actual\nABI break that was made and considering Cisco in the past made\nunnecessary SONAME bumps, we tried patching the 2.6.0 release to\nprovide the older ABI.\nI messed\nwith that venerable apply_extra and wrote a small utility to patch\nSONAMEs. But upstream, in time, pointed us to the ABI break\nand this idea had to be scrapped entirely. (I later found out why\nliabigail was unable to show the ABI break after talking to Dodji\nSeketeli, the libabigail maintainer.)\nWe then asked upstream\nto provide a patch release 2.5.1 instead with the old ABI and to our\nsurprise they did within a few weeks! This shipped in 24.08.15 fixing\nthe 24.08 branch of the runtime.\nAfter all this debacle and the extra “headache”, none of us felt\ncomfortable shipping the openH264 extension anymore. Thus it was\ndropped\nfor the master branch, which means there won’t be any\norg.freedesktop.Platform.openh264 extension or noopenh264 for 25.08+.\nEpilogue\nConsidering all things, I think and hope we made the correct decision\nand hopefully the new org.freedesktop.Platform.codecs-extra works\nout. libx264, libx265 and others are built from source and there\nare no binaries or extra-data involved. So we should theoretically be\nable to patch and fix any issues that come up in the future.\nApart from all this, I’m slightly worried at the prospects of legal\nissues cropping up with this setup and also that the new extension\ncontains “too much”, but we will have to see where things flow.\nAs a fun tidbit, I wasn’t aware that Fedora was also using our\norg.freedesktop.Platform.openh264 extension. I caused a bit of work\nfor them as noopenh264 moved home\nonce again, now to Fedora and they are looking for ways to make their\nown OpenH264 extension similar to how we did.\nI hope the experience here helps anyone in the future wanting to maintain\nsuch an extension and this will also serve as a reminder to how much\nextra work patents like these causes.\nLastly I’d like to thank Endless for giving us not only noopenh264 but\nalso extra-data support in Flatpak that made all this possible;\nMichael Catanzaro and Seppo Yli-Olli\nfor maintaining this setup for a long time; Valentin David\nfor helping me in the last few days and everyone else who worked on\nthis ❤️",
    "summary": {
      "en": "The author discusses the decision to drop the OpenH264 extension from the Freedesktop SDK. Here are the key points:\n\n1. **Freedesktop SDK Overview**: The Freedesktop SDK provides a minimal Linux runtime for Flatpaks, supporting applications across various platforms. It includes several runtimes and extensions used by multiple software packages.\n\n2. **H.264 Codec Issues**: H.264 is a popular video codec but is patent-protected, complicating its distribution in free software. To address this, OpenH264 was added as an optional extension, but it came with legal and technical challenges.\n\n3. **History of OpenH264**: A workaround was created in 2019 by adding a \"noopenh264\" library, allowing software to link to the OpenH264 extension without distributing the actual binaries directly. This method had limitations, such as requiring users to download the extension separately.\n\n4. **Ongoing Problems**: Issues arose with Cisco’s binary distribution, including lack of SSL certificates for verification, which posed security risks. Additionally, compatibility issues made it difficult to update or patch the OpenH264 extension.\n\n5. **Shift to Codecs-Extra**: In mid-2024, the need for a separate OpenH264 extension diminished as a new \"codecs-extra\" extension was introduced, containing FFMPEG’s internal H.264 decoder and other codec support. This change improved usability for app developers.\n\n6. **Final Decision to Drop OpenH264**: After discovering a serious flaw in OpenH264, the decision was made to remove the extension entirely from future SDK versions. The new codecs-extra extension is expected to provide a better solution going forward.\n\n7. **Conclusion**: The author believes the decision to drop OpenH264 was correct, as it simplifies the setup and reduces legal risks. They express hope for the future of the codecs-extra extension and acknowledge the collaborative efforts that made these changes possible.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "f88a1ebe6f3936bd",
    "title": {
      "en": "OpenID Coming to SSH",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://blog.cloudflare.com/open-sourcing-openpubkey-ssh-opkssh-integrating-single-sign-on-with-ssh/",
    "score": 48,
    "by": "PranaFlux",
    "time": 1742908929,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "e49db46a233384da",
    "title": {
      "en": "Reflecting on WikiTok",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.aizk.sh/posts/reflecting-on-wikitok",
    "score": 94,
    "by": "aizk",
    "time": 1742882951,
    "content": "Reflecting on WikiTok\n\nThree weeks ago, I launched Wikitok at 1:53 AM EST from the corner of my Brooklyn bedroom.\nAll the great software engineers I admire take time to write out their thoughts, so I'd like to do the same for WikiTok, while also sharing my experience in software in general. So much has transpired in a very short amount of time - I'm certain the lessons I learned will be valuable to anyone reading this.\nThe Background\nI built wikitok.io in about 2 hours (but not the iphone app that doesn't work, nor the play store rip off, nor wikitok.net but I'm getting ahead of myself). It all came from this tweet. Simply put it's infinite scrolling wikipedia. The details of the initial process can be found in this Ars Technica article - instead of rehashing those details too much I'm going to focus more on the code as well as the aftermath / what I learned from virality.\nThe Codebase\nThe tech stack is bun, react and typescript. I combined the code from the very first working commit and pasted it below, and we'll go through it line by line.\nimport React, { useState, useEffect, useRef, useCallback } from 'react';\n\n// Types\ninterface WikiArticle {\n   title: string;\n   extract: string;\n   pageid: number;\n   thumbnail?: {\n       source: string;\n       width: number;\n       height: number;\n   };\n}\n\ninterface WikiCardProps {\n   article: WikiArticle;\n   key?: React.Key;\n}\nRight now we're just defining the shape of the wiki data. Thumbnails are technically optional, which is why we have a ? after the type definition, but in later iterations I ditched articles without thumbnails altogether. The key helps React understand which items are new or have been removed from the list, but is actually just extra code from Claude in this instance.\n// WikiCard Component\nfunction WikiCard({ article }: WikiCardProps) {\n   return (\n       <div className=\"h-screen w-full flex items-center justify-center snap-start\">\n           <div className=\"bg-white max-w-lg w-full mx-4 rounded-xl shadow-xl p-6\">\n               {article.thumbnail && (\n                   <img\n                       src={article.thumbnail.source}\n                       alt={article.title}\n                       className=\"w-full h-48 object-cover rounded-lg mb-4\"\n                   />\n               )}\n               <h2 className=\"text-2xl font-bold mb-3\">{article.title}</h2>\n               <p className=\"text-gray-700\">{article.extract}</p>\n               <a\n                   href={`https://en.wikipedia.org/?curid=${article.pageid}`}\n                   target=\"_blank\"\n                   rel=\"noopener noreferrer\"\n                   className=\"mt-4 inline-block text-blue-500 hover:text-blue-600\"\n               >\n                   Read more →\n               </a>\n           </div>\n       </div>\n   );\n}\nThis component is the heart of the UI. We are essentially saying, \"for each article, render a div with a thumbnail, title, extract, and a link to the full article\", and it must be in the shape of the WikiCardProps interface. The only minor issue with this is that the curid parameter is technically the wrong way to query articles - as pointed out by an engineer from Wikipedia here.\nSo, what does it look like?\n\nBesides the accidental white text on the white background, not bad! It is however way too tiny, but we can fix that later. It seems Claude designed it from the start to only be TikTok sized.\n// Custom Hook for Wiki Articles\nfunction useWikiArticles() {\n   const [articles, setArticles] = useState<WikiArticle[]>([]);\n   const [loading, setLoading] = useState(false);\n\n   const fetchArticles = async () => {\n       setLoading(true);\n       try {\n           const response = await fetch(\n               \"https://en.wikipedia.org/w/api.php?\" +\n               new URLSearchParams({\n                   action: \"query\",\n                   format: \"json\",\n                   generator: \"random\",\n                   grnnamespace: \"0\",\n                   prop: \"extracts|pageimages\",\n                   grnlimit: \"5\",\n                   exchars: \"200\",\n                   exlimit: \"max\",\n                   explaintext: \"1\",\n                   piprop: \"thumbnail\",\n                   pithumbsize: \"400\",\n                   origin: \"*\",\n               })\n           );\n           const data = await response.json();\n           const newArticles = Object.values(data.query.pages).map((page: any) => ({\n               title: page.title,\n               extract: page.extract,\n               pageid: page.pageid,\n               thumbnail: page.thumbnail,\n           }));\n           setArticles((prev) => [...prev, ...newArticles]);\n       } catch (error) {\n           console.error(\"Error fetching articles:\", error);\n       }\n       setLoading(false);\n   };\n\n   return { articles, loading, fetchArticles };\n}\nThis is a custom hook that fetches the articles from Wikipedia's API.\nWe call the Wikipedia API for 5 new articles every fetch, parse the JSON response, and extend our existing article list using the spread operator.\nThe only thing that immediately jumps out is the \"any\" type - but I've spent a lot of time agonizing over needless type complexity in typescript projects without actually moving the project forward at all, essentially just using it as some sort of procrastination tool. This is a really devious trap to find yourself in.\nThe \"proper\" solution would be to define an interface like WikiApiResponse that describes the shape of the response, and then use that to type the response, but in a project this simple, it's clear that the use of \"any\" won't make a difference to the safety of the code - so don't worry too hard!\nWhat matters above all is that you ship something, anything at all.\nGet feedback, get users, and then iterate. The codebase has changed significantly since this initial commit, shaped by user feedback and bug fixes - your users will tell you what needs to be changed.\nI mentioned before that the hook helps us fetch articles, but how do we know when to fetch more articles?\n// Main App Component\nfunction App() {\n   const { articles, loading, fetchArticles } = useWikiArticles();\n   const observerTarget = useRef(null);\n\n   const handleObserver = useCallback(\n       (entries: IntersectionObserverEntry[]) => {\n           const [target] = entries;\n           if (target.isIntersecting && !loading) {\n               fetchArticles();\n           }\n       },\n       [loading, fetchArticles]\n   );\n\n   useEffect(() => {\n       const observer = new IntersectionObserver(handleObserver, {\n           threshold: 0.5,\n       });\n\n       if (observerTarget.current) {\n           observer.observe(observerTarget.current);\n       }\n\n       return () => observer.disconnect();\n   }, [handleObserver]);\n\n   useEffect(() => {\n       fetchArticles();\n   }, []);\n\n   return (\n       <div className=\"h-screen w-full bg-black text-white overflow-y-scroll snap-y snap-mandatory\">\n           {articles.map((article: WikiArticle) => (\n               <WikiCard key={article.pageid} article={article} />\n           ))}\n           <div ref={observerTarget} className=\"h-10\" />\n           {loading && (\n               <div className=\"h-screen w-full flex items-center justify-center\">\n                   Loading...\n               </div>\n           )}\n       </div>\n   );\n}\n\nexport default App;\nThis is the most interesting bit of code in the project. In my head, the way I conceptualized this was that there should always be a buffer of articles underneath the current article, and it's triggered as the user continues to scroll. Not too many fetched where it slows the user experience but enough so that they never reach the \"end\" of wikipedia.\nClaude's idea was to create an invisible div at the bottom of our article list and attach an observer to it. When this div becomes visible (meaning the user has scrolled near the bottom), we fetch more articles. All in all, pretty clever. Truthfully, I've never designed an infinite scrolling interface before, so I was completely winging it.\nIs this code excellent? No. But was this code ready to be shipped? No not even close, I had to go back and make a dozen more fast commits before I felt comfortable publishing the project.\nBut did this get the ball rolling and made me realize what I could build? Yes.\nThe Development Process\nMy thought process working on wikitok was completely backwards to what standard software engineering principles would tell you - I let Claude with Cursor do the vast majority of the work, where I gave it the specs I wanted and I prompted away. I maintained a running list of all the features I wanted, testing and checking things off one by one, committing quick wins. Get the feature, take a peek at the code, move on to the next requirement, maybe compartmentalize some of the code here and there so it's nice and functional (I mean that both in a programming sense and literal sense). It worked out well - for instance Claude surprised me at the start letting me know I wouldn't even need a backend to access Wikipedia's API (no CORS restriction) which made deployment far easier for me. Interestingly, Wikimedia themselves later asked about my experience using their API, noting that many developers struggle with finding and parsing their documentation - they were surprised when I mentioned I hadn't used their docs for the MVP at all. You can find their docs here.\nThis approach can backfire if you try it in a professional setting. So why did I roll with it?\nWell for a start - I've had thousands of conversations with different AIs, and I've developed a pretty decent radar for their capabilities, when they make things up, when to completely ignore them, etc. The caveat to this is I'd imagine this wouldn't be the case if I started building in a language I've never used before. I also had a gut feeling from the start the MVP would be less than 500 lines, which is more than enough for the context of any modern LLM. The last point is that speed was the real priority here. There was already a tweet gaining momentum asking for \"All of wikipedia on one page\", which is what inspired the entire project. Building wikitok with that exact tweet in mind, knowing people might see the two tweets side by side proved to be wildly helpful.\nTo be clear to any junior engineers reading this (I'm kinda talking to myself here) - with current AI models, vibing your way to success will completely break down once you try to contribute to a serious production codebase. I think about this a lot, and I learned this the hard way when it came time to contribute to the Minecraft Benchmark, as the project is thousands and thousands of lines of code. But for simple, small projects? Some random super specific, monotonous office task? Lean in on the AI.\nUltimately, even with AI, software engineering will still be the job of giving computers precise instructions to do things, but how we do that will change. The field reinvents itself each decade, and AI is no exception.\nLessons from Going Viral\nGoing viral is stressful. It's a bit like winning the lottery, except there's a catch. As you improve your skills, you get to enter more tickets into the lottery. This is really critical - it's still the lottery, it's unfair, but your odds can just keep getting better and better, and eventually one day everyone will notice what you've built. I've had viral tweets before, and I've made a bunch of quick websites, but this is the first time I combined the two.\nThe other point in this is that you have to know what to do when you get lucky. If this was the first ever React project I made, I am certain I would've fumbled the launch and completely screwed it up, but thankfully it wasn't! I quite like this point from Brandon Sanderson, he mentioned in his writing lectures that if you haven't written 3 novels, your job is not to sell books. You job is to write enough books to learn your process and what works for you.\nSo you're viral, now what? Focus your energy on the platform that helped you achieve virality because that's where the most eyeballs are. You won't be able to answer every question and comment people throw at you. I made that mistake, getting way too caught up in the frenzy.\nOn answering comments - not all comment sections are equal. I spent quite a lot of time responding to questions in the initial hacker news post because I knew going viral on hacker news and making a good positive impression on that site would push my post up, and from there I'd get an enormous amount of traffic. Hacker news also tends to be more critical of specific technical details than lets say a random news subreddit, so you might get more value out of it (well, not always). The general flow of information was something like twitter → hacker news → mainstream news outlets → reddit / youtube.\nAlso be prepared to not sleep much for the first day of the launch. You're trading a bit of your health for the experience, but it's worth it. I got 4 and a half hours of sleep when I launched WikiTok.\nSorry Bryan Johnson\nAt the height of the frenzy, I had two people knock on my door and I briefly freaked out, wondering if they were journalists that somehow decided it would be a good idea to show up to my house. Thankfully they were just 2 Verizon technicians conducting routine surveys.\nDealing with Journalists\nI would advise anyone who goes viral to only do interviews with journalists via email or dms (unless it's an actual in-person news interview). There's nothing explicitly wrong with a phone call, but it's far easier to be extremely precise with your words when it's in a written format. It will take longer to write and more often than not most of it will get discarded, but the writing process is extremely valuable.\nWhen a journalist does contact you, do some research on their account. Reminder: a blue checkmark doesn't mean anything on Twitter anymore, as I would later be reminded when dealing with a scammer posing as a journalist. Ask to chat only via email and dms if you have the time. You really want to collect your thoughts and make sure you say exactly what you want to say, compared to freewheeling a conversation. I learned that first when I had a conversation with Business Insider, and it was a pleasant experience, even though I don't think I relayed my message and ideas as well as I could have.\nI wrote about 2000 words for the Washington Post, which took me roughly 3 and a half hours (longer than WikiTok did to make). They only used a paragraph, but now I have a detailed written record of my process and reasoning.\nStill that wasn't enough, and even after explicitly ignoring a question about Elon Musk, they took an angle of \"Insert Elon Musk for clicks,\" which I found to be a bit annoying. There's a lesson here that there's only so much you can control when you go viral.\n(Illustration by Elena Lacey/The Washington Post; Tom Brenner/The Washington Post; iStock)\nYou'll need that precision because information degrades like a game of telephone as it spreads. I counted at least 5 international articles that had the wrong URL (somebody sniped wikitok.net, and people ran with it saying it was mine). You won't be able to correct every mistake, but call out the big ones (like journalists screwing up coverage, which I promise will happen). Focus on the main sources of information - that's all you can really control. Ultimately that's what it means for something to go viral - spreading rapidly across the internet, partially out of your control.\nThe other thing about virality that shocked me, and this is more specific to software engineering - is how strange of an experience it is to have an open source project go viral. It felt like I was on a fishing boat, and suddenly a dozen people climbed aboard and offered their advice on how I should steer the ship. Some people had good points, but others wanted to steer the ship completely in the wrong direction, and I had to be the ones to shut them down. A very weird feeling. A frequent request was adding some sort of \"algorithm\" to the project, which I'm sure would be neat and all, but it's not what I set out to build.\nSome journalists picked up on my sentiment and frustration from dealing with so many requests (specifically, the ones who wanted to add an algorithm to the project), and eventually labeled wikitok as a \"cure\" for doomscrolling, or saying that I built it out explicitly due to frustration with algorithms. I wouldn't really agree with that, but it's a good conversation to have. Personally, I felt it would be strange to add an algorithm and store user data on a project that's based off of Wikipedia's data - it just feels wrong and probably violates their creative commons license. How would I even ethically monetize that? Maybe everyone 100 scrolls you get a giant, annoying popup of Jimmy Wales asking for a donation or something. The omni-presence of algorithms, good and bad, deserves a blog post all of its own.\nCoincidentally, Alec Watson of Technology Connections just uploaded a fantastic video talking about the pitfalls of living lives served by algorithms, I'd encourage you to watch it here. He refers to the issue as \"Algorithmic Complacency\" and goes into great detail about it.\nWatch Out for Scams\nWatch out for phishing attempts. I had a sophisticated phishing attempt show up in my inbox.\n\nA TechCrunch journalist DM'd saying they wanted to chat, and they offered a calendar link to allow me to schedule something on their end. The link will send you to a twitter oauth screen asking if you wish to allow Calendly to access your account, but it's completely fake! It's a clever man-in-the-middle attack, completely bypassing your password or 2FA, and they get permission to post on your behalf. The last step from their end is to use your account as a crypto pump and dump scheme, making thousands. An example of such coin would be something like this random wikitok coin somebody made, but with 99.8% of the coin being held by the owner, I don't think even the most degenerate crypto bros would invest in that.\nFor me, I googled the TechCrunch journalist and realized that wasn't actually their account - and I also got lucky that I had happened to see a tweet about someone's experience with a phishing attempt on my feed a few hours earlier. I wonder if I hadn't seen that tweet, would I have fallen for it? The best thing I can do is post a tweet about it, write up this blog, and make sure people know. I'll leave two more very good threads here from Alex Banks and Danny Postma who both dealt with the same exact scam.\nIn retrospect, it seems like an obvious scam. But when you have 5 different real journalists DMing you and asking for interviews, on top of maintaining the site itself, I could have very easily fell for it, and I'm sure others have.\nI went through about maybe 200 DMs total. The vast majority were really positive, mostly people who were just excited and reaching out to me about Wikitok, which was wonderful! But I had to filter through the craziest shit imaginable. Here's a really unhinged crypto scam:\n\nBut those aren't the worst ones. The worst ones are the death threats. Not because I'm scared of them, but it's just sad to see what insane things people's brains were pretrained on as they grew up. I'm Jewish, I was cancelled before I was even born, 3000 years ago. I didn't get as many death threats as I usually do when I get viral tweet or something, but I did get a few. I'm not going to share them in this post but you can use your imagination. Maybe one day I'll write a detailed blog post about the Jewish experience in the 21st century.\nAbout Me\nI'm actually in the middle of a career transition. Less than a year ago, I was a civil engineer, doing everything from AutoCAD drafting to literally being in the trenches. I quit 2 years out of college because everyday I came home with a deep seated feeling that I chose the wrong career path. I dropped everything, moved to Brooklyn, and found communities where I could really learn to ship. Why did I decide to quit, even in the face of AI and a perfectly fine degree in civil engineering? One reason - I already had some actual programming experience at an internship in college. I wasn't hired to code at all, I just picked it up on my own at the company and started to make serious contributions, one stack overflow page at a time, so I knew it must be possible. The downside to this was that it made the tail end of college rather depressing, as this deep-seated internal conflict started to brew. Am I a software engineer? A civil engineer? Do these classes matter in the slightest? I suppose college really did prepare me for the real world.\nAnd with regards to AI taking jobs - it obviously will become a serious problem in future. But being a doomer right now is like lying down in a parking lot waiting to get run over - you're surrendering to a pointless outcome while the rest of the world keeps moving. There's still so much to build and accomplish. We don't know when AGI is coming, so why stand around and wait for it? Even when some sort of thing that we can agree on is AGI gets released, do you think it will be able to solve problems like creating peace in the Middle East? AGI does not encompass the human condition - there will still be work to do.\nWhat's Next?\nFor wikitok - not much. The website is mostly complete and currently I'm only reviewing simple pull requests. Before I wrote code, I used to make music as a hobby. In the world of music, when you put a song out there, it's set in stone. I'm still adjusting to the fact that software is the opposite - endlessly evolving, 24/7. I did create a sequel to wikitok called spacetok.io that allows you to scroll through images from NASA, after seeing some tweets talking about the idea. I knew the sequel wouldn't go viral, but I made it simply because it was fun. If you know anyone at NASA who'd like it, let them know!\nWhat's next for me? I'll be working on more projects like mcbench.ai. I'll also be diving into theoretical study to fill in my knowledge gaps that Claude is not able to provide, but my big priority is looking for a job. That was one reason I made wikitok - it seemed like fun and something I could put on my resume (so hey, why not check it out and send me an email if you're hiring).\nThanks for reading!Share",
    "summary": {
      "en": "**Summary of \"Reflecting on WikiTok\"**\n\nThree weeks ago, the author launched WikiTok, an infinite scrolling version of Wikipedia, from their Brooklyn bedroom. The project was built quickly in about two hours using a tech stack that includes Bun, React, and TypeScript. The main aim of the article is to reflect on the experience of creating WikiTok and the lessons learned from its rapid growth and virality.\n\n**Key Points:**\n\n1. **Development Process**: The author used AI assistance to build WikiTok, focusing on quick feature implementation and user feedback. They acknowledged that while the code wasn't perfect, it was functional enough to launch.\n\n2. **Virality Experience**: The author describes going viral as exciting but overwhelming. They emphasize the importance of being prepared for the attention that comes with virality and understanding how to manage interactions, especially with journalists.\n\n3. **Handling Media and Feedback**: The author advises communicating with journalists via written formats to ensure clarity and precision. They also stress the need to be cautious of scams, especially when receiving many messages from various sources.\n\n4. **Personal Journey**: The author transitioned from a career in civil engineering to software development, driven by a passion for programming. They believe there is still much to achieve in the tech field despite concerns about AI impacting jobs.\n\n5. **Future Plans**: While WikiTok is mostly complete, the author has created a sequel, Spacetok, and is looking forward to working on more projects. They are also seeking job opportunities in software development.\n\nOverall, the author shares valuable insights from their journey, highlighting the importance of learning from experiences and the dynamic nature of software development.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "691d28780a329a49",
    "title": {
      "en": "What Killed Innovation?",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.shirleywu.studio/notebook/2025-02-innovation-killer",
    "score": 5,
    "by": "speckx",
    "time": 1742909161,
    "content": "what killed innovation? ← back to notebook Find the discussion on Linkedin or\n        Bluesky. Mar 05, 2025 This is part of a trilogy on the state of our industry: Part 1: Outside a client’s comfort zone Part 2: What killed innovation? A brief history Part 3: (Coming soon) I entered the data visualization field in 2012, when D3.js had just come out and interactive graphics were going through a digital Renaissance. By the time I was fully steeped in the field in 2016, it felt like a new, experimental project was coming out every week—each one pushing the boundaries of how we think about, visualize, and communicate data. New York Times' 512 Paths to the White House from 2012.  When I think of the pre-scrollytelling era, it's always the first one I point to.  But fast forward a decade, and it feels like I’m seeing the same polished but predictable formats over and over. At first, I thought I had gotten jaded (a decade in tech feels like forever and maybe I’m now just the old granny shaking my fist about the “good old days” 😂), but after talking to a few friends and mulling on it for a couple years, I now have some hypotheses on how we might have gotten here. To test these hypotheses, I asked a few very respected voices in our industry—all known for their beautiful, bespoke work and responsible for driving the field forward in one way or another. So I’d like to thank everyone who kindly contributed: Nadieh Bremer, Giorgia Lupi, Federica Fragapane, Caitlin Ralph, Matt Daniels, RJ Andrews, Alberto Cairo, Eric William Lin, and Moritz Stefaner. While some of them are quoted directly, all of them provided insightful and nuanced answers that helped to shape my own whirlwind of ideas about where we are today, how we got here, and where we’re headed. A brief history (of a plateau) Let me come out swinging: our industry’s creativity has plateaued. Ok, I don’t fully mean that (but it’s got a lot of potential for a click-bait title, no? 😂). Let me provide a bit more nuance: the craft of telling data-informed stories on the web has plateaued. As Alberto recounts, newsrooms started digitizing in the mid-to-late 1990s, and by the time I started in the early 2010s, the foundations were laid out for really interesting web-based experiments. It was the perfect storm: Javascript was maturing, browser performance was improving, and D3.js was releasing interesting new layouts and supporting chart-specific interactions every year. I was constantly coming across new chart forms, and new ways to add interactions and animations to layer information. Just look at these two New York Times pieces from 2012 and 2013:   Four Ways to Slice Obama’s 2013 Budget Proposal (2012, left) and Among the Oscar Contenders, a Host of Connections (2013, right). From a technical perspective, I remember being so mind-blown about how the different budget items animated so smoothly between the different views, and so inspired by the use of force-directed graph to lay out the Oscar nominees (and that visual elegance!). It really seemed like no two data visualizations looked the same. RJ agrees that web interactives “hit some kind of romantic peak” in the mid-to-late 2010s. Nadieh dreamily recalls the advent of D3 visuals like chord diagrams and circle packing. “I can’t tell you what other mind boggling new technique or technology since then has, I’ve felt, made an impact,” she laments. “And that makes me very sad.” Moritz summarizes his recent observations with the precision and insight of an industry veteran who has put out inspiration after inspiration for decades: The few viral, well-known data visualizations tend to be static images or movies or small, personalized apps rather than elaborate interactive web pieces. Visual styles are more homogenous, with many projects looking similar due to the prevalence of established formats, templated solutions and software defaults. On the commercial side, investments in bespoke data visualization from freelancers and independent studios have declined. Moritz has written a whole blog post in response, with the most amazing title: “Why 2024 was a dead year for indie dataviz — and how we’ll do much better in 2025”. It’s well worth a thorough read. He declares: “it’s a polycrisis”. But how did we get here (polycrisis)? Over the past decade, I’ve noticed two fundamental shifts that have changed how we design interactive graphics on the web. The first: scrollies. I still remember when I first saw Stephanie Yee and Tony Chu’s “A visual introduction to machine learning” in 2016, I was blown away:  It felt so exciting that web performance had come so far, that browsers could continuously scroll and animate thousands of elements at the same time. Scrollytelling was so quickly adopted that there was even a big debate about whether scrollies or steppers were “better” (beautifully summarized in this blog post by Zan Armstrong). But the scrolly won out as more and more readers consumed data visualizations through mobile screens, where scrolling was the native behavior. Newsrooms brought scrollytelling to the masses, but the format almost worked too well. Many newsrooms began to treat it as the default storytelling method and, due to its success, had less reason to test other formats. Eric, who has direct newsroom experience, says that “data journalists found the forms that best fit their needs, and their audiences’. It’s not that there’s no more experimentation within graphics and data desks in newsrooms, but they’ve become more isolated and case-by-case rather than the general cultural norm. Repeatability and efficiency are now more important.” I used to look forward to the Information is Beautiful Awards every year for inspiration. But I distinctly remember the 2022 Awards, when I went through the shortlist and was inundated by scrollytelling pieces. It’s not that they weren’t good, but that after a while I couldn’t distinguish one from the next. That was the year I realized I was experiencing scrollytelling fatigue™️. My best hypothesis for why this happened is that judges just don’t have the time to deep-dive into any one piece. What makes scrollytelling easy for us as readers also makes for easier judging: Just scroll and the information is spoonfed; there’s no need to tap, no need to explore, no need to think and draw our own conclusions. After all, when judges are going through dozens or even hundreds of entries, the ones that are easiest to grok leave the most lasting impression. (I’m guilty of this.) I noticed the second shift during the Covid-19 pandemic. We saw charts about cases, and hospitalizations, and deaths, and vaccination rates…they were everywhere we looked online. At the time, I thought that data literacy might improve on a massive scale. A public that recognized the importance of data, that recognized the need to interpret charts, could perhaps also recognize that some data stories are just too complex and nuanced to be aggregated in bar charts and line charts. Surely, they’d be curious about more complex visualizations. I could dream. Instead, though there was indeed an increase in public awareness of charts, the majority of people—as I wrote in my previous blog post—were only interested in those standard charts. I’ve found in recent years that my clients have started to shy away from anything too unconventional because they’re concerned about getting pushback from their stakeholders and readers. Giorgia puts it best: “Some information will always be best conveyed in a straightforward bar or line chart, particularly for audiences that don’t have time to engage deeply. And as data literacy expanded—especially post-pandemic—the demand for intuitive, quick-to-grasp visuals naturally increased.” Why, though? In both of those shifts, there is a common refrain: the demand for easier, quicker, more bite-sized content. It’s underpinned by a cultural shift towards mobile phones as our primary mode of content consumption. The shift, as RJ notes, has shrunk the canvas, trained audiences to engage in short bursts, and has generally “made the Internet a not-fun place to hang out.” Matt of The Pudding echoes this observation: Audiences want more video and images that can fit on Instagram, “which means it can’t carry too much detail on mobile phone screens.” Audiences are also just less easily impressed these days: “The idea of visualizing data for artistic ‘wow-ness’ is far less novel than it used to be,” Matt says. “The idea of visualizing big data is not as new anymore, and in many ways expected and benign. Data viz as art just doesn’t hit the same way that it used to.” Matt shared this map of Facebook friendships, created by Paul Butler in 2010, that “broke the internet”. He’s seeing similar maps since, but “it just doesn’t turn heads like it used to”. There’s a really interesting tangent there that I want to explore one day, about the connection between novelty, a technical difficulty (“a good challenge”), and the motivation to experiment.  He has a point. All the New York Times graphics I shared earlier were ground-breaking when they first came out, but if I saw them now, I probably wouldn’t bat an eye. (Except the Oscars piece, that one still gets me.) The bar has been set. Which leads to an interesting conundrum: The most successful charts and formats get rolled up into chart-building tools and templatized. On one hand, this is a great thing—we all benefit from easier and more accessible ways of creating. But, as Moritz notes, it also leads to less investment in truly original data visuals and an overabundance of more homogenous visual styles. And that’s how we end up with scrollytelling fatigue™️ and our clients and their stakeholders asking for bar charts and line charts. Speaking of clients, Moritz notes that in economic downturns like the one we’ve been in, everybody tends to play it safe: “In an uncertain economy, clients—especially startups, nonprofits, or mid-sized firms—may cut creative budgets or prioritize ‘good enough’ solutions over innovative, artisanal visualizations.” Bespoke interactive graphics are often big and expensive to build, and their impact might be hard to measure. On the other hand, those same no-code or low-code chart-building tools have become increasingly impressive and easier for clients to maintain. These days, he says, “there are many more ways to create good-enough dynamic charts and charts… with little or no coding effort, with the additional benefit of integrating directly into existing information ecosystems.” Zooming out Giorgia contextualizes what we’re experiencing as the natural ebb and flow of an innovation cycle: “I think every wave of technological or methodological change follows a predictable pattern: an initial hype cycle, a period of widespread adoption, and then an eventual plateau where only the most practical elements endure.” We’re currently smack in the middle of that plateau, but, as Alberto reminds me, that’s not a bad thing. “Sometimes we have periods of very rapid development and innovation, like a new technology shows up, a new programming language, a new technique, and there’s very quick adoption of that, a lot of experimentation, a lot of excitement. And suddenly we reach a plateau in which that new technology feels a little bit dated or overused,” he says. “Sometimes there are periods in which we withdraw a little bit from that innovation and we go back to traditional forms. But then there will be another period of innovation. Somebody will come up with a novel way of doing things, and there will be another period of excitement. So maybe the period that we are in right now is a time of regrouping. But that’s not necessarily a bad thing.” After all, he was part of a very small group of newspapers pushing for interactive graphics in the late 90’s, when journalists and designers were getting pushback from their editors for scatterplots and histograms. He’s lived through an innovation cycle already, and he’s unfazed. So what next? As an experimenter that thrives off of novelty, I’ve been restless for our next cycle of innovation—and I have thoughts. Lots of thoughts, actually, especially after hearing from so many amazing perspectives. So don’t forget to like, subscribe, and hit that notification bell! (Sorry I’ve always wanted to pretend I’m an Youtuber 😂) (But if you do want to subscribe, the next one should be a really good one 🎊)",
    "summary": {
      "en": "The text discusses the decline of innovation in the data visualization field over the past decade. The author, who entered the industry in 2012 during a time of creativity and experimentation, notes that recent years have seen a shift towards repetitive and predictable formats, particularly with the rise of \"scrollytelling\" as a dominant storytelling method. \n\nKey points include:\n\n1. **Creativity Plateau**: The author believes that the creativity in data visualization has plateaued, with many projects looking similar due to the use of established formats and templates.\n\n2. **Influence of Scrollytelling**: Scrollytelling became popular because it suited mobile users and was easy for newsrooms to adopt. However, its widespread use has led to a lack of experimentation and diversity in visual styles.\n\n3. **Public Interest**: Despite an increase in data literacy during the COVID-19 pandemic, audiences tend to prefer straightforward visualizations, limiting the demand for more complex and innovative designs.\n\n4. **Economic Factors**: In uncertain economic times, clients often opt for \"good enough\" solutions, which can stifle creative investments in unique data visualizations.\n\n5. **Innovation Cycles**: The text concludes with an acknowledgment of the natural cycle of innovation, suggesting that the current period may be one of regrouping before the next wave of creativity emerges.\n\nOverall, the author expresses a desire for renewed innovation in the field of data visualization and hints at future insights to come.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "39377b593bb66aa1",
    "title": {
      "en": "A study reveals that deciduous trees' roots remain active in winter",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.creaf.cat/en/articules/deciduous-trees-roots-remain-active-winter",
    "score": 35,
    "by": "bryanrasmussen",
    "time": 1742671996,
    "content": "",
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "94dd7a5cb320f1fa",
    "title": {
      "en": "Spammers are better at SPF, DKIM, and DMARC than everyone else",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://toad.social/@grumpybozo/114213600922816869",
    "score": 253,
    "by": "pabs3",
    "time": 1742890490,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "2c6f01f2468a616d",
    "title": {
      "en": "Qwen2.5-VL-32B: Smarter and Lighter",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://qwenlm.github.io/blog/qwen2.5-vl-32b/",
    "score": 504,
    "by": "tosh",
    "time": 1742841312,
    "content": "QWEN CHAT\nGITHUB\nHUGGING FACE\nMODELSCOPE\nDISCORDIntroduction#At the end of January this year, we launched the Qwen2.5-VL series of models, which received widespread attention and positive feedback from the community. Building on the Qwen2.5-VL series, we continued to optimize the model using reinforcement learning and open-sourced the new VL model with the beloved 32B parameter scale under the Apache 2.0 license — Qwen2.5-VL-32B-Instruct. Compared to the previously released Qwen2.5-VL series models, the features of this 32B VL model are as follows:Responses More Aligned with Human Preferences: Adjusted the output style to provide more detailed, better-formatted answers that align more closely with human preferences.Mathematical Reasoning: Significant improvement in the accuracy of solving complex mathematical problems.Fine-grained Image Understanding and Reasoning: Enhanced accuracy and detailed analysis in tasks such as image parsing, content recognition, and visual logic deduction.Performance#Extensive benchmarking against state-of-the-art (SoTA) models of comparable scale, Qwen2.5-VL-32B-Instruct has demonstrated superiority over baselines, e.g., Mistral-Small-3.1-24B and Gemma-3-27B-IT, even surpassing the larger Qwen2-VL-72B-Instruct. Notably, it achieves significant advantages in multimodal tasks such as MMMU, MMMU-Pro, and MathVista, which focus on complex, multi-step reasoning. On MM-MT-Bench, a benchmark emphasizing subjective user experience evaluation, Qwen2.5-VL-32B-Instruct outperforms its predecessor Qwen2-VL-72B-Instruct by a substantial margin.In addition to excelling in visual capabilities, Qwen2.5-VL-32B-Instruct has also achieved top-tier performance in pure text capabilities at the same scale.Demo Cases#Fine-grained Image Understanding and Reasoning (1/5)\nNextUserI am driving a large truck on this road, and it is now 12 o’clock. Can I reach a place 110 kilometers away before 13 o’clock?\nQwen2.5-VL-32BTo determine whether you can reach a place 110 kilometers away before 13:00, we need to analyze the information provided:Current Time: It is 12:00.Distance to Destination: 110 kilometers.Speed Limit for Trucks: The speed limit sign indicates that the maximum speed for trucks is 100 km/h.Step-by-Step Analysis:#1. Calculate the Required Travel Time#The formula to calculate travel time is:\nTime=DistanceSpeed\n\\text{Time} = \\frac{\\text{Distance}}{\\text{Speed}}\nTime=SpeedDistance\nGiven:Distance = 110 kmMaximum Speed = 100 km/hSubstitute the values:\nTime=110km100km/h=1.1hours\n\\text{Time} = \\frac{110 \\text{ km}}{100 \\text{ km/h}} = 1.1 \\text{ hours}\nTime=100km/h110km=1.1hours2. Convert Time to Hours and Minutes#1.1 hours is equivalent to:\n1hour+0.1×60minutes=1hour+6minutes=1hourand6minutes\n1 \\text{ hour} + 0.1 \\times 60 \\text{ minutes} = 1 \\text{ hour} + 6 \\text{ minutes} = 1 \\text{ hour and } 6 \\text{ minutes}\n1hour+0.1×60minutes=1hour+6minutes=1hourand6minutes3. Determine Arrival Time#If you start at 12:00 and it takes 1 hour and 6 minutes to travel 110 kilometers, your arrival time will be:\n12:00+1hourand6minutes=13:06\n12:00 + 1 \\text{ hour and } 6 \\text{ minutes} = 13:06\n12:00+1hourand6minutes=13:064. Compare with Target Time#You want to arrive before 13:00. However, based on the calculations, you will arrive at 13:06, which is after 13:00.Conclusion:#Given the speed limit of 100 km/h for trucks, it is not possible to reach a place 110 kilometers away before 13:00.Thus, the final answer is:\nNo\n\\boxed{\\text{No}}\nNoMathematical Reasoning (2/5)\nNextUserAs shown in the figure, the straight lines AB and CD intersect at point O, OD bisects ∠AOE, ∠BOC = 50.0, then ∠EOB = ()\nQwen2.5-VL-32BProblem Analysis#We are given the following information:Straight lines AB AB AB and CD CD CD intersect at point O O O.OD OD OD bisects ∠AOE \\angle AOE ∠AOE.∠BOC=50∘ \\angle BOC = 50^\\circ ∠BOC=50∘.We need to find ∠EOB \\angle EOB ∠EOB.Step-by-Step Solution#Step 1: Understand the given information and relationships#Since AB AB AB and CD CD CD are straight lines intersecting at O O O, the angles around point O O O sum up to 360∘ 360^\\circ 360∘.The vertically opposite angles are equal. Therefore:\n∠AOD=∠BOCand∠DOB=∠COA.\n\\angle AOD = \\angle BOC \\quad \\text{and} \\quad \\angle DOB = \\angle COA.\n∠AOD=∠BOCand∠DOB=∠COA.\nGiven ∠BOC=50∘ \\angle BOC = 50^\\circ ∠BOC=50∘, we have:\n∠AOD=50∘.\n\\angle AOD = 50^\\circ.\n∠AOD=50∘.Step 2: Use the angle bisector property#It is given that OD OD OD bisects ∠AOE \\angle AOE ∠AOE. This means:\n∠AOD=∠DOE.\n\\angle AOD = \\angle DOE.\n∠AOD=∠DOE.\nSince ∠AOD=50∘ \\angle AOD = 50^\\circ ∠AOD=50∘, we have:\n∠DOE=50∘.\n\\angle DOE = 50^\\circ.\n∠DOE=50∘.Step 3: Determine ∠AOE \\angle AOE ∠AOE#The angle ∠AOE \\angle AOE ∠AOE is the sum of ∠AOD \\angle AOD ∠AOD and ∠DOE \\angle DOE ∠DOE:\n∠AOE=∠AOD+∠DOE=50∘+50∘=100∘.\n\\angle AOE = \\angle AOD + \\angle DOE = 50^\\circ + 50^\\circ = 100^\\circ.\n∠AOE=∠AOD+∠DOE=50∘+50∘=100∘.Step 4: Use the straight line property#Since AB AB AB is a straight line, the angles on a straight line sum up to 180∘ 180^\\circ 180∘. Therefore:\n∠AOE+∠EOB=180∘.\n\\angle AOE + \\angle EOB = 180^\\circ.\n∠AOE+∠EOB=180∘.\nSubstituting ∠AOE=100∘ \\angle AOE = 100^\\circ ∠AOE=100∘, we get:\n100∘+∠EOB=180∘.\n100^\\circ + \\angle EOB = 180^\\circ.\n100∘+∠EOB=180∘.\nSolving for ∠EOB \\angle EOB ∠EOB:\n∠EOB=180∘−100∘=80∘.\n\\angle EOB = 180^\\circ - 100^\\circ = 80^\\circ.\n∠EOB=180∘−100∘=80∘.Final Answer#80\n\\boxed{80}\n80Mathematical Reasoning (3/5)\nNextUser侏罗纪蜘蛛网是一种非常有规律的蜘蛛网，如图，它是由无数个正方形环绕而成的，且最外边第一个正方形A_{1}B_{1}C_{1}D_{1}的面积为a_{1}=1，分别取正方形A_{1}B_{1}C_{1}D_{1}各边的三等分点A_{2}，B_{2}，C_{2}，D_{2}，作第二个正方形A_{2}B_{2}C_{2}D_{2}，且正方形A_{2}B_{2}C_{2}D_{2}的面积为a_{2}，\\ldots \\ldots ，依次类推，第n个正方形A_{n}B_{n}C_{n}D_{n}的面积为a_{n}. $求{a_{n}}的通项公式\nQwen2.5-VL-32B问题分析#我们需要找到第 n n n 个正方形的面积 an a_n an 的通项公式。已知：第一个正方形 A1B1C1D1 A_1B_1C_1D_1 A1B1C1D1 的面积为 a1=1 a_1 = 1 a1=1。每次取前一个正方形各边的三等分点，构造一个新的正方形。我们需要确定每次构造新正方形时，其面积与前一个正方形面积之间的关系。第一步：设第一个正方形的边长#设第一个正方形 A1B1C1D1 A_1B_1C_1D_1 A1B1C1D1 的边长为 s1 s_1 s1。由于其面积为 a1=1 a_1 = 1 a1=1，因此：\ns1=a1=1=1.\ns_1 = \\sqrt{a_1} = \\sqrt{1} = 1.\ns1=a1=1=1.第二步：构造第二个正方形#取 A1B1C1D1 A_1B_1C_1D_1 A1B1C1D1 各边的三等分点，得到新的正方形 A2B2C2D2 A_2B_2C_2D_2 A2B2C2D2。我们需要计算这个新正方形的边长。1. 确定新正方形的顶点坐标#假设第一个正方形 A1B1C1D1 A_1B_1C_1D_1 A1B1C1D1 的顶点坐标如下：A1=(0,1) A_1 = (0, 1) A1=(0,1),B1=(1,1) B_1 = (1, 1) B1=(1,1),C1=(1,0) C_1 = (1, 0) C1=(1,0),D1=(0,0) D_1 = (0, 0) D1=(0,0).取各边的三等分点：A2 A_2 A2 是 A1B1 A_1B_1 A1B1 的三等分点，坐标为 (13,1) \\left( \\frac{1}{3}, 1 \\right) (31,1),B2 B_2 B2 是 B1C1 B_1C_1 B1C1 的三等分点，坐标为 (1,23) \\left( 1, \\frac{2}{3} \\right) (1,32),C2 C_2 C2 是 C1D1 C_1D_1 C1D1 的三等分点，坐标为 (23,0) \\left( \\frac{2}{3}, 0 \\right) (32,0),D2 D_2 D2 是 D1A1 D_1A_1 D1A1 的三等分点，坐标为 (0,13) \\left( 0, \\frac{1}{3} \\right) (0,31).2. 计算新正方形的边长#新正方形 A2B2C2D2 A_2B_2C_2D_2 A2B2C2D2 的边长可以通过计算任意两点之间的距离得到。例如，计算 A2 A_2 A2 和 B2 B_2 B2 之间的距离：\nA2=(13,1),B2=(1,23).\nA_2 = \\left( \\frac{1}{3}, 1 \\right), \\quad B_2 = \\left( 1, \\frac{2}{3} \\right).\nA2=(31,1),B2=(1,32).\n两点间的距离公式为：\ns2=(x2−x1)2+(y2−y1)2.\ns_2 = \\sqrt{\\left( x_2 - x_1 \\right)^2 + \\left( y_2 - y_1 \\right)^2}.\ns2=(x2−x1)2+(y2−y1)2.\n代入坐标：\ns2=(1−13)2+(23−1)2=(23)2+(−13)2=49+19=59=53.\ns_2 = \\sqrt{\\left( 1 - \\frac{1}{3} \\right)^2 + \\left( \\frac{2}{3} - 1 \\right)^2} = \\sqrt{\\left( \\frac{2}{3} \\right)^2 + \\left( -\\frac{1}{3} \\right)^2} = \\sqrt{\\frac{4}{9} + \\frac{1}{9}} = \\sqrt{\\frac{5}{9}} = \\frac{\\sqrt{5}}{3}.\ns2=(1−31)2+(32−1)2=(32)2+(−31)2=94+91=95=35.3. 计算新正方形的面积#新正方形的面积 a2 a_2 a2 为：\na2=s22=(53)2=59.\na_2 = s_2^2 = \\left( \\frac{\\sqrt{5}}{3} \\right)^2 = \\frac{5}{9}.\na2=s22=(35)2=95.第三步：归纳规律#从上述计算可以看出：第一个正方形的边长为 s1=1 s_1 = 1 s1=1，面积为 a1=1 a_1 = 1 a1=1。第二个正方形的边长为 s2=53 s_2 = \\frac{\\sqrt{5}}{3} s2=35，面积为 a2=59 a_2 = \\frac{5}{9} a2=95。接下来，我们猜测每次构造新正方形时，边长和面积的变化规律。1. 边长变化规律#假设第 n n n 个正方形的边长为 sn s_n sn，则第 n+1 n+1 n+1 个正方形的边长 sn+1 s_{n+1} sn+1 可以通过相似三角形的性质得出。每次取三等分点构造的新正方形，其边长是前一个正方形边长的 53 \\frac{\\sqrt{5}}{3} 35 倍。因此：\nsn+1=53sn.\ns_{n+1} = \\frac{\\sqrt{5}}{3} s_n.\nsn+1=35sn.2. 面积变化规律#由于面积与边长的平方成正比，第 n+1 n+1 n+1 个正方形的面积 an+1 a_{n+1} an+1 为：\nan+1=sn+12=(53sn)2=(53)2an=59an.\na_{n+1} = s_{n+1}^2 = \\left( \\frac{\\sqrt{5}}{3} s_n \\right)^2 = \\left( \\frac{\\sqrt{5}}{3} \\right)^2 a_n = \\frac{5}{9} a_n.\nan+1=sn+12=(35sn)2=(35)2an=95an.这表明 {an} \\{a_n\\} {an} 是一个等比数列，首项为 a1=1 a_1 = 1 a1=1，公比为 59 \\frac{5}{9} 95。第四步：写出通项公式#等比数列的通项公式为：\nan=a1⋅qn−1,\na_n = a_1 \\cdot q^{n-1},\nan=a1⋅qn−1,\n其中 a1=1 a_1 = 1 a1=1，q=59 q = \\frac{5}{9} q=95。因此：\nan=1⋅(59)n−1=(59)n−1.\na_n = 1 \\cdot \\left( \\frac{5}{9} \\right)^{n-1} = \\left( \\frac{5}{9} \\right)^{n-1}.\nan=1⋅(95)n−1=(95)n−1.最终答案#(59)n−1\n\\boxed{\\left( \\frac{5}{9} \\right)^{n-1}}\n(95)n−1Mathematical Reasoning (4/5)\nNextUser如图，椭圆M:\\frac{y^2}{a^2}+\\frac{x^2}{b^2}=1(a>b>0)的两顶点为A(－2,0)，B(2,0)，离心率e=\\frac{\\sqrt{3}}{2}，过y轴上的点F(0,t)(|t|<4,t\neq 0)的直线l与椭圆交于C，D两点，并与x轴交于点P，直线AC与直线BD交于点Q. $当点P异于A，B两点时，设点P与点Q的横坐标分别为x_{P}，x_{Q}，存在常数\\lambda 使x_{P}\\cdot x_{Q}=\\lambda 成立，请求出\\lambda 的值.\nQwen2.5-VL题目分析#我们有一个椭圆 M:y2a2+x2b2=1 M: \\frac{y^2}{a^2} + \\frac{x^2}{b^2} = 1 M:a2y2+b2x2=1，其中 a>b>0 a > b > 0 a>b>0。已知椭圆的两顶点为 A(−2,0) A(-2, 0) A(−2,0) 和 B(2,0) B(2, 0) B(2,0)，离心率为 e=32 e = \\frac{\\sqrt{3}}{2} e=23。我们需要找到常数 λ \\lambda λ，使得点 P P P（直线 l l l 与 x x x-轴的交点）和点 Q Q Q（直线 AC AC AC 与直线 BD BD BD 的交点）的横坐标 xP x_P xP 和 xQ x_Q xQ 满足 xP⋅xQ=λ x_P \\cdot x_Q = \\lambda xP⋅xQ=λ。第一步：确定椭圆的参数#顶点信息：椭圆的顶点为 A(−2,0) A(-2, 0) A(−2,0) 和 B(2,0) B(2, 0) B(2,0)，说明椭圆的长轴在 x x x-轴上，且半长轴长度为 b=2 b = 2 b=2。因此，b=2 b = 2 b=2。离心率信息：离心率 e=32 e = \\frac{\\sqrt{3}}{2} e=23。离心率的公式为 e=ca e = \\frac{c}{a} e=ac，其中 c=a2−b2 c = \\sqrt{a^2 - b^2} c=a2−b2。代入 e=32 e = \\frac{\\sqrt{3}}{2} e=23，得到：\nca=32  ⟹  c=32a.\n\\frac{c}{a} = \\frac{\\sqrt{3}}{2} \\implies c = \\frac{\\sqrt{3}}{2} a.\nac=23⟹c=23a.又因为 c=a2−b2 c = \\sqrt{a^2 - b^2} c=a2−b2，代入 b=2 b = 2 b=2，得到：\na2−4=32a.\n\\sqrt{a^2 - 4} = \\frac{\\sqrt{3}}{2} a.\na2−4=23a.平方两边：\na2−4=34a2  ⟹  14a2=4  ⟹  a2=16  ⟹  a=4.\na^2 - 4 = \\frac{3}{4} a^2 \\implies \\frac{1}{4} a^2 = 4 \\implies a^2 = 16 \\implies a = 4.\na2−4=43a2⟹41a2=4⟹a2=16⟹a=4.因此，椭圆的方程为：\ny216+x24=1.\n\\frac{y^2}{16} + \\frac{x^2}{4} = 1.\n16y2+4x2=1.第二步：直线 l l l 的方程#直线 l l l 过点 F(0,t) F(0, t) F(0,t)，设其斜率为 k k k，则直线 l l l 的方程为：\ny=kx+t.\ny = kx + t.\ny=kx+t.第三步：直线 l l l 与椭圆的交点#将直线方程 y=kx+t y = kx + t y=kx+t 代入椭圆方程 y216+x24=1 \\frac{y^2}{16} + \\frac{x^2}{4} = 1 16y2+4x2=1，得到：\n(kx+t)216+x24=1.\n\\frac{(kx + t)^2}{16} + \\frac{x^2}{4} = 1.\n16(kx+t)2+4x2=1.\n整理得：\n(kx+t)2+4x2=16  ⟹  k2x2+2ktx+t2+4x2=16  ⟹  (k2+4)x2+2ktx+t2−16=0.\n(kx + t)^2 + 4x^2 = 16 \\implies k^2x^2 + 2ktx + t^2 + 4x^2 = 16 \\implies (k^2 + 4)x^2 + 2ktx + t^2 - 16 = 0.\n(kx+t)2+4x2=16⟹k2x2+2ktx+t2+4x2=16⟹(k2+4)x2+2ktx+t2−16=0.\n这是关于 x x x 的二次方程，设其两根为 x1 x_1 x1 和 x2 x_2 x2，则交点 C(x1,y1) C(x_1, y_1) C(x1,y1) 和 D(x2,y2) D(x_2, y_2) D(x2,y2) 的横坐标分别为 x1 x_1 x1 和 x2 x_2 x2。由韦达定理，有：\nx1+x2=−2ktk2+4,x1x2=t2−16k2+4.\nx_1 + x_2 = -\\frac{2kt}{k^2 + 4}, \\quad x_1 x_2 = \\frac{t^2 - 16}{k^2 + 4}.\nx1+x2=−k2+42kt,x1x2=k2+4t2−16.第四步：点 P P P 的横坐标#直线 l l l 与 x x x-轴的交点 P P P 的纵坐标为 0，代入直线方程 y=kx+t y = kx + t y=kx+t，得到：\n0=kxP+t  ⟹  xP=−tk.\n0 = kx_P + t \\implies x_P = -\\frac{t}{k}.\n0=kxP+t⟹xP=−kt.第五步：直线 AC AC AC 和 BD BD BD 的方程#直线 AC AC AC 的方程：点 A(−2,0) A(-2, 0) A(−2,0) 和点 C(x1,y1) C(x_1, y_1) C(x1,y1)，直线 AC AC AC 的斜率为：\ny1−0x1+2=y1x1+2.\n\\frac{y_1 - 0}{x_1 + 2} = \\frac{y_1}{x_1 + 2}.\nx1+2y1−0=x1+2y1.因此，直线 AC AC AC 的方程为：\ny=y1x1+2(x+2).\ny = \\frac{y_1}{x_1 + 2}(x + 2).\ny=x1+2y1(x+2).直线 BD BD BD 的方程：点 B(2,0) B(2, 0) B(2,0) 和点 D(x2,y2) D(x_2, y_2) D(x2,y2)，直线 BD BD BD 的斜率为：\ny2−0x2−2=y2x2−2.\n\\frac{y_2 - 0}{x_2 - 2} = \\frac{y_2}{x_2 - 2}.\nx2−2y2−0=x2−2y2.因此，直线 BD BD BD 的方程为：\ny=y2x2−2(x−2).\ny = \\frac{y_2}{x_2 - 2}(x - 2).\ny=x2−2y2(x−2).第六步：点 Q Q Q 的横坐标#点 Q Q Q 是直线 AC AC AC 和直线 BD BD BD 的交点，联立方程：\ny1x1+2(x+2)=y2x2−2(x−2).\n\\frac{y_1}{x_1 + 2}(x + 2) = \\frac{y_2}{x_2 - 2}(x - 2).\nx1+2y1(x+2)=x2−2y2(x−2).\n将 y1=kx1+t y_1 = kx_1 + t y1=kx1+t 和 y2=kx2+t y_2 = kx_2 + t y2=kx2+t 代入，得到：\nkx1+tx1+2(x+2)=kx2+tx2−2(x−2).\n\\frac{kx_1 + t}{x_1 + 2}(x + 2) = \\frac{kx_2 + t}{x_2 - 2}(x - 2).\nx1+2kx1+t(x+2)=x2−2kx2+t(x−2).\n交叉相乘得：\n(kx1+t)(x2−2)(x+2)=(kx2+t)(x1+2)(x−2).\n(kx_1 + t)(x_2 - 2)(x + 2) = (kx_2 + t)(x_1 + 2)(x - 2).\n(kx1+t)(x2−2)(x+2)=(kx2+t)(x1+2)(x−2).\n展开并整理，可以解出 xQ x_Q xQ。经过复杂但标准的代数运算，可以得到：\nxQ=4⋅x1x2−2(x1+x2)x1+x2−4.\nx_Q = 4 \\cdot \\frac{x_1 x_2 - 2(x_1 + x_2)}{x_1 + x_2 - 4}.\nxQ=4⋅x1+x2−4x1x2−2(x1+x2).\n将韦达定理的结果 x1+x2=−2ktk2+4 x_1 + x_2 = -\\frac{2kt}{k^2 + 4} x1+x2=−k2+42kt 和 x1x2=t2−16k2+4 x_1 x_2 = \\frac{t^2 - 16}{k^2 + 4} x1x2=k2+4t2−16 代入，化简后得到：\nxQ=−4tk.\nx_Q = -\\frac{4t}{k}.\nxQ=−k4t.第七步：计算 xP⋅xQ x_P \\cdot x_Q xP⋅xQ#点 P P P 的横坐标为 xP=−tk x_P = -\\frac{t}{k} xP=−kt，点 Q Q Q 的横坐标为 xQ=−4tk x_Q = -\\frac{4t}{k} xQ=−k4t。因此：\nxP⋅xQ=(−tk)(−4tk)=4t2k2.\nx_P \\cdot x_Q = \\left(-\\frac{t}{k}\\right) \\left(-\\frac{4t}{k}\\right) = \\frac{4t^2}{k^2}.\nxP⋅xQ=(−kt)(−k4t)=k24t2.\n注意到 k k k 和 t t t 的具体值不影响最终的乘积形式，且根据对称性和几何性质，可以确定 xP⋅xQ x_P \\cdot x_Q xP⋅xQ 为一个常数。通过进一步分析或直接利用椭圆的几何性质，可以得出：\nxP⋅xQ=4.\nx_P \\cdot x_Q = 4.\nxP⋅xQ=4.最终答案#4\n\\boxed{4}\n4Fine-grained Image Understanding (5/5)\nNextUser这是什么Qwen2.5-VL-32B这张图片展示的是一道中式火锅，具体来说是四川麻辣火锅。以下是一些关键特征：锅底：锅中装满了红色的辣椒和花椒，这是四川麻辣火锅的典型特征，通常会使用大量的干辣椒、花椒和其他香料来制作辣味浓郁的汤底。分隔设计：锅被分成多个小格，每个格子可以用来煮不同的食材，这样可以避免不同食材的味道相互影响，同时也能满足多人对不同口味的需求。周围配菜：可以看到一些切好的肉类（如牛肉片）、蔬菜和其他食材，这些都是火锅常见的涮菜。氛围感：整体画面呈现出一种热气腾腾、色彩鲜艳的感觉，符合火锅的热闹和美味的氛围。火锅简介：#四川麻辣火锅是中国火锅的一种，起源于四川省，以其独特的麻辣风味闻名。主要特点包括：使用大量辣椒和花椒，味道辛辣且麻。汤底通常由牛油或清油制成，加入各种香料熬制。配菜丰富多样，包括肉类、海鲜、蔬菜等，适合多人一起享用。这种火锅不仅是一种美食，也是一种社交方式，非常适合家庭聚会或朋友聚餐。User图中为什么是四川火锅Qwen2.5-VL-32B从图片中可以看出，这是一道典型的四川火锅，主要依据以下几个特征：1. 锅底的麻辣风格#图中的锅底颜色鲜艳，呈现出明显的红色，这是四川火锅的典型特征。四川火锅以“麻辣”著称，通常会使用大量的干辣椒、花椒等调料来制作辣味浓郁的汤底。红色的辣椒和花椒是四川火锅的灵魂，它们不仅赋予了火锅独特的风味，还体现了四川人对“麻”和“辣”的热爱。2. 分隔设计#锅被分成了多个小格，这种设计在四川火锅中非常常见。每个格子可以用来煮不同的食材，避免不同食材的味道相互影响，同时也能满足多人对不同口味的需求。这种设计也符合四川火锅的多样性和灵活性，适合多人一起享用。3. 配菜种类#周围可以看到一些切好的肉类（如牛肉片）和其他食材，这些都是四川火锅常见的涮菜。四川火锅的配菜种类丰富，包括肉类、海鲜、蔬菜等，能够满足不同人的口味需求。4. 氛围感#整体画面呈现出一种热气腾腾、色彩鲜艳的感觉，充满了火锅的热闹和美味氛围。四川火锅以其麻辣鲜香的特点，深受人们喜爱，尤其是在寒冷的季节或聚会时，更是不可或缺的美食。5. 地域文化#四川火锅是中国火锅的一种，起源于四川省，以其独特的麻辣风味闻名。四川人对辣椒和花椒有着深厚的情感，这也体现在他们的饮食文化中。四川火锅正是这种文化的体现。总结：#图中的锅底颜色鲜艳、麻辣十足，锅的设计分隔明确，周围配菜丰富多样，这些都与四川火锅的典型特征高度吻合。因此，可以确定这是一道四川火锅。Next Step#While Qwen2.5-VL-32B has focused on optimizing subjective experience and mathematical reasoning through reinforcement learning—operating within the paradigm of “fast thinking”. Our next research direction will prioritize long and effective reasoning processes to push the boundaries of visual models in tackling highly complex, multi-step visual reasoning tasks.Citation#If you find our model helpful, feel free to cite it:@article{Qwen2.5-VL,\n  title={Qwen2.5-VL Technical Report},\n  author={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and Zhong, Humen and Zhu, Yuanzhi and Yang, Mingkun and Li, Zhaohai and Wan, Jianqiang and Wang, Pengfei and Ding, Wei and Fu, Zheren and Xu, Yiheng and Ye, Jiabo and Zhang, Xi and Xie, Tianbao and Cheng, Zesen and Zhang, Hang and Yang, Zhibo and Xu, Haiyang and Lin, Junyang},\n  journal={arXiv preprint arXiv:2502.13923},\n  year={2025}\n}",
    "summary": {
      "en": "**Summary of Qwen2.5-VL-32B Model Launch and Features**\n\nAt the end of January, the Qwen2.5-VL series of models was launched, receiving positive community feedback. The new version, Qwen2.5-VL-32B-Instruct, has been optimized with reinforcement learning and is open-sourced under the Apache 2.0 license.\n\n**Key Features:**\n- **Human-like Responses:** The model provides detailed and well-formatted answers that are more aligned with human expectations.\n- **Improved Mathematical Reasoning:** It shows significant accuracy in solving complex mathematical problems.\n- **Enhanced Image Understanding:** The model excels in tasks like image parsing and content recognition.\n\n**Performance Highlights:**\n- Extensive testing reveals that Qwen2.5-VL-32B-Instruct outperforms similar models, including larger ones like Qwen2-VL-72B-Instruct, especially in multimodal tasks.\n- On benchmarks focusing on user experience, it has shown substantial improvements over its predecessor.\n\n**Demonstration Examples:**\n- **Image Understanding:** The model accurately analyzes scenarios, such as estimating travel time based on given conditions.\n- **Mathematical Problems:** It effectively solves geometric and algebraic problems through step-by-step reasoning.\n\n**Future Direction:**\nThe next focus will be on developing long-term reasoning capabilities to handle more complex visual reasoning tasks.\n\n**Conclusion:**\nQwen2.5-VL-32B represents a significant advancement in AI models, enhancing both user experience and technical performance in various applications.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "dece63737e377a4b",
    "title": {
      "en": "Status as a Service (2019)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.eugenewei.com/blog/2019/2/19/status-as-a-service",
    "score": 59,
    "by": "simonebrunozzi",
    "time": 1742885984,
    "content": "Status as a Service (StaaS)\n\n          February 26, 2019\n           by Eugene Wei\n\n  Editor's Note 1: I have no editor. Editor’s Note 2: I would like to assure new subscribers to this blog that most my posts are not as long as this one. Or as long as my previous one. My long break from posting here means that this piece is a collection of what would’ve normally been a series of shorter posts. I put section titles below, so skip any that don’t interest you. My short takes are on Twitter. All that said, I apologize for nothing. Editor's Note 3: I lied, I apologize for one thing, and that is my long writing hiatus. Without a work computer, I had to resort to using my 7 year old 13\" Macbook Pro as my main computer, and sometime last year my carpal tunnel syndrome returned with a vengeance and left my wrists debilitated with pain. I believe all of you who say your main computer is a laptop or, shudder, an iPad, but goodness gracious I cannot type on a compact keyboard for long periods of time without having my hands turn into useless stumps. It was only the return to typing almost exclusively on my old friend the Kinesis Advantage 2 ergo keyboard that put me back in the game. Editor’s Note 4: I was recently on Patrick O'Shaughnessy's podcast Invest Like the Best, and near the end of that discussion, I mentioned a new essay I'd been working on about the similarities between social networks and ICO's. This is that piece.Status-Seeking Monkeys\"It is a truth universally acknowledged, that a person in possession of little fortune, must be in want of more social capital.\"So wrote Jane Austen, or she would have, I think, if she were chronicling our current age (instead we have Taylor Lorenz, and thank goodness for that).Let's begin with two principles:People are status-seeking monkeys*People seek out the most efficient path to maximizing social capital* Status-Seeking Monkeys will also be the name of my indie band, if I ever learn to play the guitar and start a bandI begin with these two observations of human nature because few would dispute them, yet I seldom see social networks, some of the largest and fastest-growing companies in the history of the world, analyzed on the dimension of status or social capital.It’s in part a measurement issue. Numbers lend an air of legitimacy and credibility. We have longstanding ways to denominate and measure financial capital and its flows. Entire websites, sections of newspapers, and a ton of institutions report with precision on the prices and movements of money.We have no such methods for measuring the values and movement of social capital, at least not with anywhere near the accuracy or precision. The body of research feels both broad and yet meager. If we had better measures besides user counts, this piece and many others would be full of charts and graphs that added a sense of intellectual heft to the analysis. There would be some annual presentation called the State of Social akin to Meeker's Internet Trends Report, or perhaps it would be a fifty page sub-section of her annual report.Despite this, most of the social media networks we study generate much more social capital than actual financial capital, especially in their early stages; almost all such companies have internalized one of the popular truisms of Silicon Valley, that in the early days, companies should postpone revenue generation in favor of rapid network scaling. Social capital has much to say about why social networks lose heat, stall out, and sometimes disappear altogether. And, while we may not be able to quantify social capital, as highly attuned social creatures, we can feel it.Social capital is, in many ways, a leading indicator of financial capital, and so its nature bears greater scrutiny. Not only is it good investment or business practice, but analyzing social capital dynamics can help to explain all sorts of online behavior that would otherwise seem irrational.In the past few years, much progress has been made analyzing Software as a Service (SaaS) businesses. Not as much has been made on social networks. Analysis of social networks still strikes me as being like economic growth theory long before Paul Romer's paper on endogenous technological change. However, we can start to demystify social networks if we also think of them as SaaS businesses, but instead of software, they provide status. This post is a deep dive into what I refer to as Status as a Service (StaaS) businesses.Think of this essay as a series of strongly held hypotheses; without access to the types of data which i’m not even sure exists, it’s difficult to be definitive. As ever, my wise readers will add or push back as they always do.Traditional Network Effects Model of Social NetworksOne of the fundamental lessons of successful social networks is that they must first appeal to people when they have few users. Typically this is done through some form of single-user utility. This is the classic cold start problem of social.  The answer to the traditional chicken-and-egg question is actually answerable: what comes first is a single chicken, and then another chicken, and then another chicken, and so on. The harder version of the question is why the first chicken came and stayed when no other chickens were around, and why the others followed.The second fundamental lessons is that social networks must have strong network effects so that as more and more users come aboard, the network enters a positive flywheel of growth, a compounding value from positive network effects that leads to hockey stick growth that puts dollar signs in the eyes of investors and employees  alike. \"Come for the tool, stay for the network\" wrote Chris Dixon, in perhaps the most memorable maxim for how this works.Even before social networks, we had Metcalfe's Law on telecommunications networks:The value of a telecommunications network is proportional to the square of the number of connected users of the system (n^2)This ported over to social networks cleanly. It is intuitive, and it includes that tantalizing math formula that explains why growth curves for social networks bends up sharply at the ankle of the classic growth S-curve.But dig deeper and many many questions remain. Why do some large social networks suddenly fade away, or lose out to new tiny networks? Why do some new social networks with great single-player tools fail to transform into networks, while others with seemingly frivolous purposes make the leap? Why do some networks sometimes lose value when they add more users? What determines why different networks stall out at different user base sizes? Why do some networks cross international borders easily while others stay locked within specific countries? Why, if Metcalfe's Law holds, do many of Facebook's clones of other social network features fail, while some succeed, like Instagram Stories?What ties many of these explanations together is social capital theory, and how we analyze social networks should include a study of a social network's accumulation of social capital assets and the nature and structure of its status games. In other words, how do such companies capitalize, either consciously or not, on the fact that people are status-seeking monkeys, always trying to seek more of it in the most efficient way possible?To paraphrase Nicki Minaj, “If I'm fake I ain't notice cause my followers ain't.”[Editor’s note: sometimes the followers actually are fake.]Utility vs. Social Capital FrameworkClassic network effects theory still holds, I’m not discarding it. Instead, let's append some social capital theory. Together, those form the two axes on which I like to analyze social network health.Actually, I tend to use three axes to dissect social networks.\n\n              View fullsize\n\n            The three axes on which I evaluate social network strength\n\n  For this post, though, I'm only going to look at two of them, utility and social capital, as the entertainment axis adds a whole lot of complexity which I'll perhaps explain another time.\n\n              View fullsize\n\n            The basic two axis framework guiding much of the social network analysis in this piece\n\n  Utility doesn't require much explanation, though we often use the term very loosely and categorize too many things as utility when they aren't that useful (we generally confuse circuses for bread and not the reverse; Fox News, for example, is more entertainment than utility, as is common of many news outlets). A social network like Facebook allows me to reach lots of people I would otherwise have a harder time tracking down, and that is useful. A messaging app like WhatsApp allows me to communicate with people all over the world without paying texting or incremental data fees, which is useful. Quora and Reddit and Discord and most every social network offer some forms of utility.The other axis is, for a lack of a more precise term, the social capital axis, or the status axis. Can I use the social network to accumulate social capital? What forms? How is it measured? And how do I earn that status?There are several different paths to success for social networks, but those which compete on the social capital axis are often more mysterious than pure utilities. Competition on raw utility tends to be Darwinian, ruthless, and highly legible. This is the world, for example, of communication services like messaging and video conferencing.  Investing in this space also tends to be a bit more straightforward: how useful is your app or service, can you get distribution, etc. When investors send me decks on things in this category, I am happy to offer an opinion, but I enjoy puzzling over the world of artificial prestige even more.The creation of a successful status game is so mysterious that it often smacks of alchemy. For that reason, entrepreneurs who succeed in this space are thought of us a sort of shaman, perhaps because most investors are middle-aged white men who are already so high status they haven't the first idea why people would seek virtual status (more on that later).With the rise of Instagram, with its focus on photos and filters, and Snapchat, with its ephemeral messaging, and Vine, with its 6-second video limit, for a while there was a thought that new social networks would be built on some new modality of communications. That's a piece of it, but it's not the complete picture, and not for the reasons many people think, which is why we have seen a whole bunch of strange failed experiments in just about every odd combinations of features and filters and artificial constraints in how we communicate with each other through our phones. Remember Facebook's Snapchat competitor Slingshot, in which you had to unlock any messages you received by responding with a message? It felt like product design by mad libs.When modeling how successful social networks create a status game worth playing, a useful metaphor is one of the trendiest technologies: cryptocurrency.Social Networks as ICO'sHow is a new social network analogous to an ICO? Each new social network issues a new form of social capital, a token.You must show proof of work to earn the token.Over time it becomes harder and harder to mine new tokens on each social network, creating built-in scarcity. Many people, especially older folks, scoff at both social networks and cryptocurrencies. [\"Why does anyone care what you ate for lunch?\" is the canonical retort about any social network, though it’s fading with time. Both social networks and ICO's tend to drive skeptics crazy because they seem to manufacture value out of nothing. The shifting nature of scarcity will always leave a wake of skepticism and disbelief.]Years ago, I stayed at the house of a friend whose high school daughter was home upstairs with a classmates. As we adults drank wine in the kitchen downstairs while waiting for dinner to finish in the oven, we heard lots of music and stomping and giggling coming from upstairs. When we finally called them down for dinner, I asked them what all the ruckus had been. My friend's daughter proudly held up her phone to show me a recording they'd posted to an app called Musical.ly. It was a lip synch and dance routine replete with their own choreography. They'd rehearsed the piece more times than they could count. It showed. Their faces were shiny with sweat, and they were still breathing hard from the exertion. Proof of work indeed.I spent the rest of the dinner scrolling through the app, fascinated, interviewing the girls about what they liked about the app, why they were on it, what share of their free time it had captured. I can't tell if parents are offended or glad when I spend much of the time visiting them interviewing their sons and daughters instead, but in the absence of good enough metrics with which to analyze this space, I subscribe to the Jane Goodall theory of how to study your subject. Besides, status games of adults are already well covered by the existing media, from literature to film. Children's status games, once familiar to us, begin to fade from our memory as time passes, and its modern forms have been drastically altered by social media.Other examples abound. Perhaps you've read a long and thoughtful response by a random person on Quora or Reddit, or watched YouTube vloggers publishing night after night, or heard about popular Vine stars living in houses together, helping each other shoot and edit 6-second videos. While you can outsource Bitcoin mining to a computer, people still mine for social capital on social networks largely through their own blood, sweat, and tears.[Aside: if you yourself are not an aspiring social network star, living with one is...not recommended.]Perhaps, if you've spent time around today's youth, you've watched with a mixture of horror and fascination as a teen snaps dozens of selfies before publishing the most flattering one to Instagram, only to pull it down if it doesn't accumulate enough likes within the first hour. It’s another example of proof of work, or at least vigorous market research.Almost every social network of note had an early signature proof of work hurdle. For Facebook it was posting some witty text-based status update. For Instagram, it was posting an interesting square photo. For Vine, an entertaining 6-second video. For Twitter, it was writing an amusing bit of text of 140 characters or fewer. Pinterest? Pinning a compelling photo. You can likely derive the proof of work for other networks like Quora and Reddit and Twitch and so on. Successful social networks don't pose trick questions at the start, it’s usually clear what they want from you.[An aside about exogenous social capital: you might complain that your tweets are more interesting and  grammatical than those of, say, Donald Trump (you're probably right!). Or that your photos are better composed and more interesting at a deep level of photographic craft than those of Kim Kardashian. The difference is, they bring a massive supply of exogenous pre-existing social capital from another status game, the fame game, to every table, and some forms of social capital transfer quite well across platforms. Generalized fame is one of them. More specific forms of fame or talent might not retain their value as easily: you might follow Paul Krugman on Twitter, for example, but not have any interest in his Instagram account. I don't know if he has one, but I probably wouldn't follow it if he did, sorry Paul, it’s nothing personal.]If you've ever joined one of these social networks early enough, you know that, on a relative basis, getting ahead of others in terms of social capital (followers, likes, etc.) is easier in the early days. Some people who were featured on recommended follower lists in the early days of Twitter have follower counts in the 7-figures, just as early masters of Musical.ly and Vine were accumulated massive and compounding follower counts. The more people who follow you, the more followers you gain because of leaderboards and recommended follower algorithms and other such common discovery mechanisms.It's true that as more people join a network, more social capital is up for grabs in the aggregate. However, in general, if you come to a social network later, unless you bring incredible exogenous social capital (Taylor Swift can join any social network on the planet and collect a massive following immediately), the competition for attention is going to be more intense than it was in the beginning. Everyone has more of an understanding of how the game works so the competition is stiffer.Why Proof of Work MattersWhy does proof of work matter for a social network? If people want to maximize social capital, why not make that as easy as possible?As with cryptocurrency, if it were so easy, it wouldn't be worth anything. Value is tied to scarcity, and scarcity on social networks derives from proof of work. Status isn't worth much if there's no skill and effort required to mine it. It's not that a social network that makes it easy for lots of users to perform well can't be a useful one, but competition for relative status still motivates humans. Recall our first tenet: humans are status-seeking monkeys. Status is a relative ladder. By definition, if everyone can achieve a certain type of status, it’s no status at all, it’s a participation trophy.Musical.ly created a hurdle for gaining followers and status that wasn't easily cleared by many people. However, for some, especially teens, and especially girls, it was a status game at which they were particularly suited to win. And so they flocked there, because, according to my second tenet, people look for the most efficient ways to accumulate the most social capital.Recall Twitter in the early days, when it was somewhat of a harmless but somewhat inert status update service. I went back to look at my first few tweets on the service from some 12 years ago and my first two, spaced about a year apart, were both about doing my taxes. Looking back at them, I bore even myself. Early Twitter consisted mostly of harmless but dull life status updates, a lot of “is this thing on?” tapping on the virtual microphone. I guess I am in the camp of not caring about what you had for lunch after all. Get off my lawn, err, phone screen!What changed Twitter, for me, was the launch of Favstar and Favrd (both now defunct, ruthlessly murdered by Twitter), these global leaderboards that suddenly turned the service into a competition to compose the most globally popular tweets. Recall, the Twitter graph was not as dense then as it was now, nor did distribution accelerants like one-click retweeting and Moments exist yet.What Favstar and Favrd did was surface really great tweets and rank them on a scoreboard, and that, to me, launched the performative revolution in Twitter. It added needed feedback to the feedback loop, birthing a new type of comedian, the master of the 140 character or less punchline (the internet has killed the joke, humor is all punchline now that the setup of the joke is assumed to be common knowledge thanks to Google).The launch of these global tweet scoreboards reminds me of the moment in the now classic film** Battle Royale when Beat Takeshi Kitano informs a bunch of troublemaking school kids that they’ve been deported to an island are to fight to the death, last student standing wins, and that those who try to sneak out of designated battle zones will be killed by explosive collars. I'm not saying that Twitter is a life-or-death struggle, but you need only time trav",
    "summary": {
      "en": "**Summary of \"Status as a Service (StaaS)\" by Eugene Wei**\n\nEugene Wei discusses the concept of \"Status as a Service\" (StaaS), comparing social networks to businesses that provide social capital, similar to how Software as a Service (SaaS) provides software. He emphasizes two key human tendencies: people are inherently motivated by the pursuit of social status and seek efficient ways to increase their social capital.\n\nWei notes that while financial capital is well-measured and reported, social capital lacks the same level of quantifiable data, making analysis more challenging. However, understanding social capital is vital for explaining online behaviors and the success or failure of social networks.\n\nHe outlines a framework for analyzing social networks based on utility (how useful they are) and social capital (the status they provide). Successful social networks create compelling status games, requiring users to put in effort to gain social capital, akin to \"proof of work\" in cryptocurrencies.\n\nWei also highlights the importance of early user engagement in social networks, where gaining status is easier at the outset. As more users join, the competition increases, making it harder for newcomers to achieve recognition without existing social capital. He concludes that social networks thrive by tapping into our innate desire for status, shaping user behavior and engagement in complex ways.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "4ea41d504e487ba9",
    "title": {
      "en": "The Lost Towers of the Guelph-Ghibelline Wars",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.exurbe.com/the-lost-towers-of-the-guelph-ghibelline-wars/",
    "score": 71,
    "by": "wallflower",
    "time": 1742734721,
    "content": "The Lost Towers of the Guelph-Ghibelline Wars\n\n\t\t Posted by\n\n\t\t\t\t exurbe\n\n\t\t\ton\n\n\t\t\t\t\t January 15, 2025\n\n\t\t\t\t Florence, History, Inventing the Renaissance, Italy\n\n\t\t\t\t 12\n\nLooks fake, doesn’t it? This implausible Medieval forest of towers, as dense as Manhattan skyscrapers, is our best reconstruction of the town of Bologna at its height, toward the end of the Medieval Guelph-Ghibelline wars. We don’t see many such towers today… or think we don’t, but actually their remnants are all over Italy.\nOften when in Florence one sees buildings like this, where one section is rough stone standing out amid stucco neighbors.\n\nThese are actually the bottom nubs of Medieval stone towers. The town of San Gimigniano (below) is famous for having several still intact. Wealthy families built these as mini-fortresses within the city, where they could defend against riots, enemy families (think Montagues and Capulets) and invasion:\n\nSigns of wealth and prestige, these all-stone buildings were also fireproof, leading to a terrible but effective tactic: take your family, treasures & goods up into your tower then set fire to enemies’ homes and let the city burn around you while you sit safe above. This was VERY BAD for cities.\nstreet corner in San Gimigniano\nAfter many disasters, Florence’s solution was to BAN private buildings over a certain height, forcing everyone who had a tower to knock the top off down to regulation height, leaving these recognizable stone nubs all around the city. This round one below is the oldest (now a restaurant).\n\nMy favorite tower stub is this one, in Via dei Cerchi. I lived on the top floor for a year as a grad student, up 111 steps! I had calves of steel by spring, but the views from the top looked like someone had put a poster of Florence on the wall except it was a window!\n\nOnly city buildings were allowed to exceed the mandated height, which is why Florence’s skyline is now all special buildings: monastery bell towers, the cathedral & baptistery, Orsanmichele the city’s granary (tall to keep grain away from water & mice), the seat of government, and one special guy…\n\nThe tower on the right here is part of Bargello, the prison & police fortress, but it didn’t start that way. It was built by a private family, who sold it to the city when the law banning towers was passed, and the city incorporated it into their prison fort.\n\nThe city jail had to be a fortress in case someone from a powerful family was arrested and the family sent goons to break them out (those guys who bite their thumbs in the opening scene of Romeo & Juliet would *totally* have stormed the jail to bust Romeo out!).\n\nIn this photo you can see how the brick battlements are a later addition, added to the tower as part of its transformation from private fortress to public.\n\nWhat did Florence look like back when it had all its towers? Its long-time ally across the mountains Bologna is famous for still having two intact towers, but in the Middle Ages Bologna was known as the City of 100 Towers because so many families built them. The reconstructions look absolutely incredible. Florence didn’t have so many but did have dozens, so the richest part of the city center would have looked much like this. Much to the despair of the city fire brigade!\n\nSo, whether in a film or on the street, if you ever see a historic Italian city and walk along a block where for some reason one chunk of wall is stone and all the others smooth, you’re probably looking at a relic of the faction feuds that Guido Ruggiero aptly calls “The Italian 300 Years’ War.”\n\nI talk about this long war in “Inventing the Renaissance,” one of many points of continuity which show how the supposed difference between a bad “Dark Ages” and Renaissance “golden age” is 100% propaganda, but fascinating propaganda with a deep history.\nAnd I’ll share more tidbits like over the coming days as we countdown to the book’s release!\n\n                FacebookLiveJournalTwitterRedditLinkedInTumblrPush to KindleShare\n\n\t Bologna, Florence, Guelph, Inventing the Renaissance\n\n\t\t 12 Responses to “The Lost Towers of the Guelph-Ghibelline Wars”\n\n\t\t\t\tOllie said:\n\n\t\t\t 01-16-2025 11:00 AM\n\n\t\t\t\tYour photography is pretty great.\n\n\t\t\t\tJasonium said:\n\n\t\t\t 01-16-2025 12:49 PM\n\n\t\t\t\tI would like to commend you on your restraint. You had the perfect opportunity to write, “Looks like a bunch of baloney, doesn’t it?” in your opening, and you resisted.\n\n\t\t\t\tAbout that history you were taught… – Pylimitics said:\n\n\t\t\t 01-20-2025 11:55 AM\n\n\t\t\t\t[…] an era we know much less about than we think we do, and most of what we do think we know is wrong. The photo is from her site Ex Urbe, where you can read more about these things, and see some photos of modern-day Bologna and the […]\n\n\t\t\t\tMedieval Skyscrapers, Cars for Justice, and Russian Noir - JSTOR Daily said:\n\n\t\t\t 01-21-2025 1:45 PM\n\n\t\t\t\t[…] The Skyscrapers of Medieval Italy (Ex Urbe) by Ada Palmer In medieval Italy, factional wars led powerful families to build huge towers that served as little urban fortresses. Their stubs are still there today. […]\n\n\t\t\t\t2:00PM Water Cooler 1/21/2025 | naked capitalism said:\n\n\t\t\t 01-21-2025 7:12 PM\n\n\t\t\t\t[…] Lost Towers of the Guelph-Ghibelline Wars” [Ex Urbe]. “Looks fake, doesn’t it? This implausible Medieval forest of towers, as dense as […]\n\n\t\t\t\t2:00PM Water Cooler 1/21/2025 | naked capitalism – Buddy CRM News Test said:\n\n\t\t\t 01-21-2025 11:33 PM\n\n\t\t\t\t[…] Lost Towers of the Guelph-Ghibelline Wars” [Ex Urbe]. “Looks fake, doesn’t it? This implausible Medieval forest of towers, as dense as Manhattan […]\n\n\t\t\t\t2:00PM Water Cooler 1/21/2025 | bare capitalism - Lucky to know | All rights reserved. said:\n\n\t\t\t 01-22-2025 12:22 AM\n\n\t\t\t\t[…] Misplaced Towers of the Guelph-Ghibelline Wars” [Ex Urbe]. “Seems to be faux, doesn’t it? This implausible Medieval forest of towers, as dense as […]\n\n\t\t\t\tGorgeous Gary said:\n\n\t\t\t 01-22-2025 2:06 AM\n\n\t\t\t\tI feel like a certain structural engineer of your acquaintance will find this post quite intriguing! (LOL)\n\n\t\t\t\t2:00PM Water Cooler 1/21/2025 | naked capitalism - Neclink said:\n\n\t\t\t 01-22-2025 2:42 AM\n\n\t\t\t\t[…] Lost Towers of the Guelph-Ghibelline Wars” [Ex Urbe]. “Looks fake, doesn’t it? This implausible Medieval forest of towers, as dense as Manhattan […]\n\n\t\t\t\t2:00PM Water Cooler 1/21/2025 | naked capitalism - Agoku said:\n\n\t\t\t 01-22-2025 5:30 AM\n\n\t\t\t\t[…] Lost Towers of the Guelph-Ghibelline Wars” [Ex Urbe]. “Looks fake, doesn’t it? This implausible Medieval forest of towers, as dense as Manhattan […]\n\n\t\t\t\tThe Lost Towers of the Guelph-Ghibelline Wars – Mirabilis.ca said:\n\n\t\t\t 01-25-2025 10:37 PM\n\n\t\t\t\t[…] From Ex Urbe: The Lost Towers of the Guelph-Ghibelline Wars. […]\n\n\t\t\t\tEx Urbe said:\n\n\t\t\t 01-27-2025 4:04 PM\n\n\t\t\t\t[…] a post last week, I talked about how Renaissance towns used to be full of tall stone towers, built by rich families […]\n\n\t\t\t\tComments are closed.",
    "summary": {
      "en": "The article discusses the tall stone towers built by wealthy families in medieval Italian cities during the Guelph-Ghibelline wars. These towers served as mini-fortresses, providing protection against riots and invasions. Cities like Bologna were known for their many towers, while Florence had fewer, but still had a notable number of them. \n\nOver time, Florence implemented a law to limit the height of private towers, leading to many being shortened and leaving behind recognizable stone stubs. This was a response to the dangers posed by these towers, as they could be used to safely burn down enemies' homes. \n\nToday, remnants of these towers can still be seen throughout Italy, often as parts of buildings where one section is rough stone. The article also hints at the misconceptions about the \"Dark Ages\" versus the Renaissance, suggesting that the history of this period is more complex than commonly believed.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "668e4ef58314431a",
    "title": {
      "en": "Beej's Guide to C Programming [pdf]",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://beej.us/guide/bgc/pdf/bgc_a4_c_1.pdf",
    "score": 37,
    "by": "krykp",
    "time": 1742911087,
    "content": "Beej’s Guide to C Programming  Brian “Beej Jorgensen” Hall v0.9.22, Copyright © February 20, 2025\n\nContents  1   Foreword   1  1.1   Audience . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   1 1.2   How to Read This Book   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   2 1.3   Platform and Compiler   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   2 1.4   Official Homepage   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   2 1.5   Email Policy   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   2 1.6   Mirroring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   3 1.7   Note for Translators   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   3 1.8   Copyright and Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   3 1.9   Dedication   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   3  2   Hello, World!   5  2.1   What to Expect from C . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   5 2.2   Hello, World! . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   6 2.3   Compilation Details   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   7 2.4   Building with   gcc   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   8 2.5   Building with   clang   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   8 2.6   Building from IDEs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   8 2.7   C Versions   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   9  3   Variables and Statements   10  3.1   Variables   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   10 3.1.1   Variable Names   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   10 3.1.2   Variable Types   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   11 3.1.3   Boolean Types   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   12 3.2   Operators and Expressions   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   13 3.2.1   Arithmetic   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   13 3.2.2   Ternary Operator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   13 3.2.3   Pre-and-Post Increment-and-Decrement   . . . . . . . . . . . . . . . . . . . . .   14 3.2.4   The Comma Operator   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   15 3.2.5   Conditional Operators   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   15 3.2.6   Boolean Operators   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   16 3.2.7   The   sizeof   Operator   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   16 3.3   Flow Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   17 3.3.1   The   if - else   statement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   18 3.3.2   The   while   statement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   19 3.3.3   The   do-while   statement . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   19 3.3.4   The   for   statement   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   20 3.3.5   The   switch   Statement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   21  4   Functions   24  4.1   Passing by Value   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   25 4.2   Function Prototypes   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   26 4.3   Empty Parameter Lists   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   27  5   Pointers—Cower In Fear!   28  5.1   Memory and Variables   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   28 5.2   Pointer Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   30 i\n\nCONTENTS   ii 5.3   Dereferencing   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   31 5.4   Passing Pointers as Arguments   . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   32 5.5   The   NULL   Pointer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   33 5.6   A Note on Declaring Pointers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   33 5.7   sizeof   and Pointers   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   34  6   Arrays   36  6.1   Easy Example   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   36 6.2   Getting the Length of an Array . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   37 6.3   Array Initializers   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   37 6.4   Out of Bounds! . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   39 6.5   Multidimensional Arrays . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   40 6.6   Arrays and Pointers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   41 6.6.1   Getting a Pointer to an Array . . . . . . . . . . . . . . . . . . . . . . . . . . .   41 6.6.2   Passing Single Dimensional Arrays to Functions . . . . . . . . . . . . . . . . .   41 6.6.3   Changing Arrays in Functions   . . . . . . . . . . . . . . . . . . . . . . . . . .   42 6.6.4   Passing Multidimensional Arrays to Functions . . . . . . . . . . . . . . . . . .   43  7   Strings   45  7.1   String Literals   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   45 7.2   String Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   45 7.3   String Variables as Arrays   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   46 7.4   String Initializers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   46 7.5   Getting String Length . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   47 7.6   String Termination   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   47 7.7   Copying a String   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   48  8   Structs   50  8.1   Declaring a Struct   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   50 8.2   Struct Initializers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   51 8.3   Passing Structs to Functions   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   51 8.4   The Arrow Operator   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   53 8.5   Copying and Returning   struct s   . . . . . . . . . . . . . . . . . . . . . . . . . . . .   53 8.6   Comparing   struct s   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   53  9   File Input/Output   54  9.1   The   FILE*   Data Type . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   54 9.2   Reading Text Files   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   55 9.3   End of File:   EOF   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   55 9.3.1   Reading a Line at a Time . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   56 9.4   Formatted Input   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   57 9.5   Writing Text Files . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   58 9.6   Binary File I/O   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   58 9.6.1   struct   and Number Caveats . . . . . . . . . . . . . . . . . . . . . . . . . . .   60  10   typedef : Making New Types   62  10.1   typedef   in Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   62 10.1.1   Scoping   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   62 10.2   typedef   in Practice   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   62 10.2.1   typedef   and   struct s . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   62 10.2.2   typedef   and Other Types   . . . . . . . . . . . . . . . . . . . . . . . . . . . .   64 10.2.3   typedef   and Pointers   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   64 10.2.4   typedef   and Capitalization   . . . . . . . . . . . . . . . . . . . . . . . . . . .   64 10.3   Arrays and   typedef   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   65  11 Pointers II: Arithmetic   66  11.1   Pointer Arithmetic   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   66 11.1.1   Adding to Pointers   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   66 11.1.2   Changing Pointers   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   67\n\nCONTENTS   iii 11.1.3   Subtracting Pointers   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   68 11.2   Array/Pointer Equivalence   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   68 11.2.1   Array/Pointer Equivalence in Function Calls . . . . . . . . . . . . . . . . . . .   69 11.3   void   Pointers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   70  12 Manual Memory Allocation   75  12.1   Allocating and Deallocating,   malloc()   and   free()   . . . . . . . . . . . . . . . . . .   75 12.2   Error Checking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   76 12.3   Allocating Space for an Array   . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   77 12.4   An Alternative:   calloc()   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   77 12.5   Changing Allocated Size with   realloc()   . . . . . . . . . . . . . . . . . . . . . . .   78 12.5.1   Reading in Lines of Arbitrary Length   . . . . . . . . . . . . . . . . . . . . . .   79 12.5.2   realloc()   with   NULL   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   81 12.6   Aligned Allocations   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   81  13 Scope   83  13.1   Block Scope   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   83 13.1.1   Where To Define Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . .   83 13.1.2   Variable Hiding   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   84 13.2   File Scope   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   84 13.3   for -loop Scope . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   85 13.4   A Note on Function Scope   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   85  14 Types II: Way More Types!   86  14.1   Signed and Unsigned Integers   . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   86 14.2   Character Types   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   87 14.3   More Integer Types:   short ,   long ,   long long   . . . . . . . . . . . . . . . . . . . . .   88 14.4   More Float:   double   and   long double   . . . . . . . . . . . . . . . . . . . . . . . . .   90 14.4.1   How Many Decimal Digits?   . . . . . . . . . . . . . . . . . . . . . . . . . . .   91 14.4.2   Converting to Decimal and Back . . . . . . . . . . . . . . . . . . . . . . . . .   92 14.5   Constant Numeric Types   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   93 14.5.1   Hexadecimal and Octal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   93 14.5.2   Integer Constants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   94 14.5.3   Floating Point Constants . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   95  15 Types III: Conversions   98  15.1   String Conversions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   98 15.1.1   Numeric Value to String   . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   98 15.1.2   String to Numeric Value   . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   99 15.2   char   Conversions   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   101 15.3   Numeric Conversions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   102 15.3.1   Boolean   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   102 15.3.2   Integer to Integer Conversions   . . . . . . . . . . . . . . . . . . . . . . . . . .   102 15.3.3   Integer and Floating Point Conversions   . . . . . . . . . . . . . . . . . . . . .   102 15.4   Implicit Conversions   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   103 15.4.1   The Integer Promotions   . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   103 15.4.2   The Usual Arithmetic Conversions . . . . . . . . . . . . . . . . . . . . . . . .   103 15.4.3   void*   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   104 15.5   Explicit Conversions   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   104 15.5.1   Casting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   104  16 Types IV: Qualifiers and Specifiers   106  16.1   Type Qualifiers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   106 16.1.1   const   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   106 16.1.2   restrict   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   108 16.1.3   volatile   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   109 16.1.4   _Atomic   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   109 16.2   Storage-Class Specifiers   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   109 16.2.1   auto   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   110\n\nCONTENTS   iv 16.2.2   static   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "summary": {
      "en": "**Summary of Beej’s Guide to C Programming**\n\nThis guide, authored by Brian \"Beej Jorgensen\" Hall, provides a comprehensive introduction to C programming. It covers various topics essential for understanding and using the C language effectively.\n\n1. **Introduction**: The book includes a foreword, details on the intended audience, reading instructions, platform and compiler recommendations, and information on how to access resources and support.\n\n2. **Getting Started**: The guide introduces the basics of C, starting with a simple \"Hello, World!\" program, and explains how to compile it using different tools like `gcc` and `clang`.\n\n3. **Core Concepts**:\n   - **Variables and Data Types**: Covers variable names, types, and how to use them.\n   - **Operators**: Discusses arithmetic, conditional, and logical operators.\n   - **Control Flow**: Explains how to manage the execution flow with statements like `if-else`, `while`, `for`, and `switch`.\n\n4. **Functions**: Details how to create and use functions, including passing values and using prototypes.\n\n5. **Pointers**: Introduces pointers, memory management, and how to use them for efficient programming.\n\n6. **Arrays and Strings**: Discusses one-dimensional and multi-dimensional arrays, string manipulation, and related functions.\n\n7. **Structures**: Explains how to define and use structures to group related data.\n\n8. **File I/O**: Covers how to read from and write to files, including error handling and binary files.\n\n9. **Memory Management**: Discusses dynamic memory allocation using functions like `malloc()` and `free()`.\n\n10. **Advanced Topics**: Includes typedefs, pointer arithmetic, and type conversions.\n\nThis guide is structured to help beginners learn C programming step by step, offering practical examples and explanations of key concepts.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "88a87dff7a187fb5",
    "title": {
      "en": "Coding Isn't Programming",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.socallinuxexpo.org/scale/22x/presentations/closing-keynote-leslie-lamport",
    "score": 125,
    "by": "todsacerdoti",
    "time": 1742898863,
    "content": "Subscribe to announcements\n\nSubscribe\n\nClosing Keynote with Leslie Lamport\n\n          Leslie Lamport\n      Audience:     Everyone\n      Topic:     Keynote\n          Join us for a captivating closing keynote with the legendary Leslie Lamport, Turing Award winner and pioneer in the field of distributed computing. We'll discuss computing history, open source and distributed systems.\n\n      Presentation:      linux-expo (1).pdf\n      Room:     Ballroom DE\n      Time:     Sunday, March 9, 2025 - 15:00 to 16:00\n      Audio/Video:\n\nSubscribe to announcements\n\nSubscribe\n\nPlatinum Sponsors\n\n      Gold Sponsors\n\n      Silver Sponsors\n\n      Network Sponsors\n\n      Registration Sponsors\n\n      Reception Sponsors\n\n      Special Event Sponsors\n\n      Speaker Track Sponsors\n\n      Travel Sponsors\n\n      Media Sponsors\n\nAbout SCALE\nCode of Conduct\nContact Us\nPolicies",
    "summary": {
      "en": "Join us for a closing keynote by Leslie Lamport, a famous computer scientist known for his work in distributed computing. The talk will cover topics like computing history and open-source systems. \n\n**Details:**\n- **Date:** Sunday, March 9, 2025\n- **Time:** 3:00 PM to 4:00 PM\n- **Location:** Ballroom DE\n\nDon't miss this opportunity to hear from a pioneer in the field!",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "5c05b01fda081727",
    "title": {
      "en": "The Hartlib Papers",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.dhi.ac.uk/hartlib/",
    "score": 11,
    "by": "sebg",
    "time": 1742899158,
    "content": "Search the Hartlib Papers\n\n\t\tKeyword or phrase:\n\n\t\tAdvanced Search\n\nSamuel Hartlib (c1600-62), a great seventeenth-century 'intelligencer' and man of science, set out to record all human knowledge\n   and make it universally available for the education of all mankind. His correspondence, which runs to over 25,000 folios of\n   original materials, came to the University Library in the 1960s. The full-text electronic edition realises Hartlib's vision\n   some 300 years later.\n\nThe project's objective was to create a complete electronic edition with full-text transcription and facsimile images of all\n   25,000 seventeenth-century manuscripts. This seven-year project culminated in the publication in 1996 of two CD-ROMs by University\n   Microfilms in Michigan. An enlarged edition, incorporating many Hartlib materials from libraries around the world, was published\n   on CD-ROM by the Humanities Research Institute in July 2002. This new, online edition provides free access to all the content\n   available on the original CD-ROM versions. Full introductory and background information will be made available shortly.",
    "summary": {
      "en": "Samuel Hartlib (circa 1600-1662) was a notable 17th-century thinker who aimed to collect and share all human knowledge for everyone's education. His extensive correspondence, totaling over 25,000 documents, was donated to the University Library in the 1960s. \n\nA project was launched to create a complete electronic version of these manuscripts, which took seven years and resulted in two CD-ROMs published in 1996. An expanded edition, including more Hartlib materials from various libraries, was released in July 2002. This new online edition offers free access to all the content from the original CD-ROMs, with additional introductory information to be provided soon.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "8049440cfd9b45fd",
    "title": {
      "en": "Three Hundred Years Later, a Tool from Isaac Newton Gets an Update",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.quantamagazine.org/three-hundred-years-later-a-tool-from-isaac-newton-gets-an-update-20250324/",
    "score": 127,
    "by": "rbanffy",
    "time": 1742854616,
    "content": "Quanta Homepage\n\n                                        Physics\n\n                                        Mathematics\n\n                                        Biology\n\n                                        Computer Science\n\n                                        Topics\n\n                                        Archive\n\n                                        Blog\n\n                                        Columns\n\n                                        Interviews\n\n                                        Podcasts\n\n                                        Puzzles\n\n                                        Multimedia\n\n                                        Videos\n\n                                        About Quanta\n\n                                    An editorially independent publication supported by the Simons Foundation.\n\n                                    Follow Quanta\n\n    Facebook\n\n        Youtube\n\n        Instagram\n\n    RSS\n\n                Newsletter\n\n                    Get the latest news delivered to your inbox.\n\n                            Email\n\n                        Subscribe\n\n                        Recent newsletters\n\n                                    Gift Store\n\n                                        Shop Quanta gear\n\nNewsletter\n\n                    Get the latest news delivered to your inbox.\n\n                            Email\n\n                        Subscribe\n\n                        Recent newsletters\n\nQuanta Homepage\n\n                                        Physics\n\n                                        Mathematics\n\n                                        Biology\n\n                                        Computer Science\n\n                                        Topics\n\n                                        Archive\n\n        Saved articles\n\n                    Saved Articles\n                                            Create a reading list by clicking the Read Later icon next to the articles you wish to save.\n\n                            See all saved articles\n\n        Login\n\n                    Log out\n\n                    Change password\n\n                                Search\n\nHome\n\n                Three Hundred Years Later, a Tool from Isaac Newton Gets an Update\n\n        Comment\n                3\n\n        Save Article\n\n                    Read Later\n\n                                                Share\n\n    Facebook\n\n                            Copied!\n\n    Copy link\n         (opens a new tab)\n\n    Email\n\n    Pocket\n\n    Reddit\n\n    Ycombinator\n\nalgorithms\n    Three Hundred Years Later, a Tool from Isaac Newton Gets an Update\n\n        By\n\n                Kevin Hartnett\n\nMarch 24, 2025\n\n            A simple, widely used mathematical technique can finally be applied to boundlessly complex problems.\n\n        Comment\n                3\n\n        Save Article\n\n                    Read Later\n\nalgorithms\n    Three Hundred Years Later, a Tool from Isaac Newton Gets an Update\n\n        By\n\n                Kevin Hartnett\n\nMarch 24, 2025\n\n            A simple, widely used mathematical technique can finally be applied to boundlessly complex problems.\n\n        Comment\n                3\n\n        Save Article\n\n                    Read Later\n\nMichele Sclafani for Quanta Magazine\n\nEvery day, researchers search for optimal solutions. They might want to figure out where to build a major airline hub. Or to determine how to maximize return while minimizing risk in an investment portfolio. Or to develop self-driving cars that can distinguish between traffic lights and stop signs.\nMathematically, these problems get translated into a search for the minimum values of functions. But in all these scenarios, the functions are too complicated to assess directly. Researchers have to approximate the minimal values instead.\nIt turns out that one of the best ways to do this is by using an algorithm that Isaac Newton developed over 300 years ago. This algorithm is fairly simple. It’s a little like searching, blindfolded, for the lowest point in an unfamiliar landscape. As you put one foot in front of the other, the only information you need is whether you’re going uphill or downhill, and whether the grade is increasing or decreasing. Using that information, you can get a good approximation of the minimum relatively quickly.\nAlthough enormously powerful — centuries later, Newton’s method is still crucial for solving present-day problems in logistics, finance, computer vision and even pure math — it also has a significant shortcoming. It doesn’t work well on all functions. So mathematicians have continued to study the technique, figuring out different ways to broaden its scope without sacrificing efficiency.\nLast summer, three researchers announced the latest improvement (opens a new tab) to Newton’s method. Amir Ali Ahmadi (opens a new tab) of Princeton University, along with his former students Abraar Chaudhry (opens a new tab) (now at the Georgia Institute of Technology) and Jeffrey Zhang (opens a new tab) (now at Yale University), extended Newton’s method to work efficiently on the broadest class of functions yet.\n“Newton’s method has 1,000 different applications in optimization,” Ahmadi said. “Potentially our algorithm can replace it.”\n\n                    In the 1680s, Isaac Newton developed an algorithm for finding optimal solutions. Three centuries later, mathematicians are still using and honing his method.\n\n    Godfrey Kneller/Public Domain\n\nA Centuries-Old Technique\nMathematical functions transform inputs into outputs. Often, the most important feature of a function is its minimum value — the combination of inputs that produces the smallest possible output.\nBut finding the minimum is hard. Functions can have dozens of variables raised to high powers, defying formulaic analysis; graphs of their solutions form high-dimensional landscapes that are impossible to explore from a bird’s-eye view. In those higher-dimensional landscapes, said Coralia Cartis (opens a new tab) of the University of Oxford, “We want to find a valley. Some are local valleys; others are the lowest point. You’re trying to find these things, and the question is: What info do you have to guide you to that?”\nIn the 1680s, Newton recognized that even when you’re dealing with a very complicated function, you’ll still always have access to at least two pieces of information to help you find its deepest valley. First, you can calculate the function’s so-called first derivative, or slope: the steepness of the function at a given point. Second, you can compute the rate at which the slope itself is changing (the function’s second derivative).\n\n                    Amir Ali Ahmadi sees optimization problems everywhere he looks.\n\n    Archives of the Mathematisches Forschungsinstitut Oberwolfach\n\nSay you’re trying to find the minimum of some complicated function. First, choose a point on the function that you think might be close to the true minimum. Compute the function’s first and second derivatives at that point. These derivatives can be used to construct a special quadratic equation — a parabola if your function lives in a 2D plane, and a cuplike shape called a paraboloid if your function is higher dimensional. This quadratic equation, which is called a Taylor approximation, roughly resembles your function at the point you chose.\nNow calculate the minimum of the quadratic equation instead of the original — something you can do easily, using a well-known formula. (That’s because quadratic equations are simple; it’s when equations get more complicated that calculating the minimum becomes prohibitive.) You’ll get a point. Then plug the coordinates of that point back into your original function, and you’ll get a new point on the function that is, hopefully, closer to its true minimum. Start the entire process again.\nNewton proved that if you keep on repeating this process, you’ll eventually home in on the minimum value of the original, more complicated function. The method doesn’t always work, especially if you start at a point that’s too far away from the true minimum. But for the most part, it does. And it has some desirable attributes.\n\n    Mark Belan/Quanta Magazine;Source:arxiv:2305.07512 (opens a new tab)\n\nOther iterative methods, like gradient descent — the algorithm used in today’s machine learning models — converge toward the true minimum at a linear rate. Newton’s method converges toward it much faster: at a “quadratic” rate. In other words, it can identify the minimum value in fewer iterations than gradient descent. (Each iteration of Newton’s method is more computationally expensive than an iteration of gradient descent, which is why researchers prefer gradient descent for certain applications, like training neural networks. But Newton’s method is still enormously efficient, making it useful in all sorts of contexts.)\nNewton could have written his method to converge toward the true minimum value even faster if, instead of taking just the first and second derivatives at each point, he had also taken, say, the third and fourth derivatives. That would have given him more complicated Taylor approximations, with exponents greater than 2. But the whole crux of his strategy was to transform a complicated function into a simpler one. These more complicated Taylor equations were more than Newton could handle mathematically.\n\n                    Jeffrey Zhang and his co-authors wiggled functions in just the right way, allowing them to broaden the scope of a powerful optimization technique.\n\n    Courtesy of Jeffrey Zhang\n\n“Newton did it for degree 2. He did that because nobody knew how to minimize higher-order polynomials,” Ahmadi said.\nIn the centuries since, mathematicians have worked to extend his method, to probe how much information they can squeeze out of more complicated Taylor approximations of their functions.\nIn the 19th century, for instance, the Russian mathematician Pafnuty Chebyshev proposed a version of Newton’s method that approximated functions with cubic equations (which have an exponent of 3). But his algorithm didn’t work when the original function involved multiple variables. Much more recently, in 2021, Yurii Nesterov (now at Corvinus University of Budapest) demonstrated how to approximate functions (opens a new tab) of any number of variables efficiently with cubic equations. But his method couldn’t be extended to approximate functions using quartic equations, quintics and so on without losing its efficiency. Nevertheless, the proof was a major breakthrough in the field.\nNow Ahmadi, Chaudhry and Zhang have taken Nesterov’s result another step further. Their algorithm works for any number of variables and arbitrarily many derivatives. Moreover, it remains efficient for all these cases — something that until now wasn’t possible.\nBut first, they had to find a way to make a hard math problem a lot easier.\nFinding Wiggle Room\nThere is no fast, general purpose method for finding the minima of functions raised to high exponents. That’s always been the main limitation of Newton’s method. But there are certain types of functions that have characteristics that make them easy to minimize. In the new work, Ahmadi, Chaudhry and Zhang prove that it’s always possible to find approximating equations that have these characteristics. They then show how to adapt these equations to run Newton’s method efficiently.\nWhat properties make an equation easy to minimize? Two things: The first is that the equation should be bowl-shaped, or “convex.” Rather than having many valleys, it has just one — meaning that when you try to minimize it, you don’t have to worry about mistaking an arbitrary valley for the lowest one.\n\n                    Abraar Chaudhry and two colleagues recently found a way to improve a centuries-old method for finding the minima of functions.\n\n    Camille Carpenter Henriquez\n\nThe second property is that the equation can be written as a sum of squares. For example, 5x2 + 16x + 13 can be written as the sum (x + 2)2 + (2x + 3)2. In recent years, mathematicians have developed techniques for minimizing equations with arbitrarily large exponents so long as they are both convex and a sum of squares. However, those techniques were of little help when it came to Newton’s method. Most of the time, the Taylor approximation you use won’t have these nice properties.\nBut Ahmadi, Chaudhry and Zhang figured out how to use a technique called semidefinite programming to wiggle the Taylor approximation just enough to make it both a sum of squares and convex, though not so much that it became unmoored from the original function it was supposed to resemble.\nThey essentially added a fudge factor to the Taylor expansion, turning it into an equation that had the two desired properties. “We can change the Taylor expansion a bit to make it simpler to minimize. Think of the Taylor expansion, but modified a little bit,” Ahmadi said. He and his colleagues then showed that, using this modified version of the Taylor expansion — which involved arbitrarily many derivatives — their algorithm would still converge on the true minimum of the original function. Moreover, the rate of convergence would scale with the number of derivatives used: Just as using two derivatives allowed Newton to approach the true minimum at a quadratic rate, using three derivatives enabled the researchers to approach it at a cubic rate, and so on.\nAhmadi, Chaudhry and Zhang had created a more powerful version of Newton’s method that could reach the true minimum value of a function in fewer iterations than previous techniques.\n\n                Related:\n\n                                    Surprising Limits Discovered in Quest for Optimal Solutions\n\n                                    Risky Giant Steps Can Solve Optimization Problems Faster\n\n                                    How We Can Make Sense of Chaos\n\nLike the original version of Newton’s method, each iteration of this new algorithm is still computationally more expensive than methods such as gradient descent. As a result, for the moment, the new work won’t change the way self-driving cars, machine learning algorithms or air traffic control systems work. The best bet in these cases is still gradient descent.\n“Many ideas in optimization take years before they are made fully practical,” said Jason Altschuler (opens a new tab) of the University of Pennsylvania. “But this seems like a fresh perspective.”\nIf, over time, the underlying computational technology needed to run Newton’s method becomes more efficient — making each iteration less computationally expensive — then the algorithm developed by Ahmadi, Chaudhry and Zhang could eventually surpass gradient descent for all sorts of applications, including machine learning.\n“Our algorithm right now is provably faster, in theory,” Ahmadi said. He’s hopeful, he added, that in 10 to 20 years, it will also be so in practice.\nCorrection:March 25, 2025\nThe graphic in this article has been updated.\n\nBy Kevin Hartnett\n                Contributing Writer\n\n                March 24, 2025\n\n                    View PDF/Print Mode\n\n                            algorithms\n\n                            applied math\n\n                            dynamical systems\n\n                            machine learning\n\n                            mathematics\n\n                    All topics\n\nShare this article\n\n    Facebook\n\n                            Copied!\n\n    Copy link\n         (opens a new tab)\n\n    Email\n\n    Pocket\n\n    Reddit\n\n    Ycombinator\n\n                    Newsletter\n\n                    Get Quanta Magazine delivered to your inbox\n\n                    Subscribe now\n\n                    Recent newsletters\n\n             (opens a new tab)\n\nThe Quanta Newsletter\n\n                    Get highlights of the most important news delivered to your email inbox\n\n                            Email\n\n                        Subscribe\n\n                        Recent newsletters\n                                             (opens a new tab)\n\nAlso in Mathematics\n\n                    ‘Once in a Century’ Proof Settles Math’s Kakeya Conjecture\n\n                geometry\n\n                    ‘Once in a Century’ Proof Settles Math’s Kakeya Conjecture\n\n        By\n\n                Joseph Howlett\n\n            March 14, 2025\n\n        Comment\n                2\n\n        Save Article\n\n                    Read Later\n\n                    Years After the Early Death of a Math Genius, Her Ideas Gain New Life\n\n                geometry\n\n                    Years After the Early Death of a Math Genius, Her Ideas Gain New Life\n\n        By\n\n                Joseph Howlett\n\n            March 3, 2025\n\n        Comment\n                3\n\n        Save Article\n\n                    Read Later\n\n                    After 20 Years, Math Couple Solves Major Group Theory Problem\n\n                group theory\n\n                    After 20 Years, Math Couple Solves Major Group Theory Problem\n\n        By\n\n                Leila Sloman\n\n            February 19, 2025\n\n        Comment\n                8\n\n        Save Article\n\n                    Read Later\n\nComment on this article\n\n                    Quanta Magazine moderates comments tofacilitate an informed, substantive, civil conversation. Abusive, profane, self-promotional, misleading, incoherent or off-topic comments will be rejected. Moderators are staffed during regular business hours (New York time) and can only accept comments written in English.\n\n        Show comments\n\nNext article\n                How Metabolism Can Shape Cells’ Destinies\n\nQuanta Homepage\n\n    Facebook\n\n        Youtube\n\n        Instagram\n\n                                        About Quanta\n\n                                        Archive\n\n                                        Contact Us\n\n                                        Terms & Conditions\n\n                                        Privacy Policy\n\n                        All Rights Reserved © 2025\n\n                    An editorially independent publication supported by the Simons Foundation.\n\n                        Simons Foundation\n\nQuanta Homepage\n\n    Facebook\n\n        Youtube\n\n        Instagram\n\nAbout Quanta\n\n                                        Archive\n\n                                        Contact Us\n\n                                        Terms & Conditions\n\n                                        Privacy Policy\n\nAll Rights Reserved © 2025\n\n                    An editorially independent publication supported by the Simons Foundation.\n\n                        Simons Foundation\n\nClose\n\n    Log in to Quanta\n\n            Use your social network\n\n                        Facebook                        Connect with Facebook\n\n                                                Connect with Google\n\n            or\n\n                    email\n\n                    password\n\n                    Remember me\n\n                        Forgot your password ?\n\n    Don't have an account yet?\n         Sign up\n\nClose\n\n        Forgot your password?\n        We’ll email you instructions to reset your password\n\n                email\n\nClose\n\n        Change your password\n        Enter your new password\n\n                Password\n\n                Retype new password\n\nClose\n\n        Sign Up\n\n                    First Name\n\n                    Last Name\n\n                    Email\n\n                    Password\n\n                    Retype Password\n\n            Creating an account means you accept Quanta Magazine's\n\n            Terms & Conditions and Privacy Policy",
    "summary": {
      "en": "**Summary of the Article:**\n\nQuanta Magazine discusses a recent improvement to Isaac Newton's optimization method, which is over 300 years old. This method helps researchers find the minimum values of complex mathematical functions, which is crucial in various fields like logistics, finance, and computer science.\n\nResearchers Amir Ali Ahmadi and his team have enhanced Newton's method so that it can efficiently handle a wider range of functions than before. Previously, the method had limitations, but their new approach allows for better approximations using more derivatives without losing efficiency.\n\nNewton's original method is effective but can struggle with certain complex functions. The new algorithm can converge to the true minimum more quickly by using techniques that make the functions easier to minimize. Although this new algorithm is still more computationally intensive than simpler methods like gradient descent, it holds promise for future applications, especially as computational technology improves.\n\nIn summary, this advancement could lead to faster and more efficient optimization solutions in various domains, potentially surpassing traditional methods in the years to come.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "7e39dddaa6c78814",
    "title": {
      "en": "The Practical Limitations of End-to-End Encryption",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://soatok.blog/2025/03/25/the-practical-limitations-of-end-to-end-encryption/",
    "score": 10,
    "by": "todsacerdoti",
    "time": 1742910301,
    "content": "Categories\n\n\t\t\t\t\tSecurity Community Security Industry Software Security\n\n\t\t\tThe Practical Limitations of End-to-End Encryption\n\n\t\t\t\t\t\t\t\tPost author\n\n\t\t\t\t\t\t\tBy Soatok\n\n\t\t\t\t\t\t\t\tPost date\n\n\t\t\t\t\t\t\tMarch 25, 2025\n\nInternet discussions about end-to-end encryption are plagued by misunderstandings, misinformation, and some people totally missing the point.\n\nOf course, people being wrong on the Internet isn’t exactly news.\n\nXKCD: Duty Calls“What do you want me to do? LEAVE? Then they’ll keep being wrong!”\n\nYesterday, a story in The Atlantic alleged that the Trump Administration accidentally added their editor, Jeffrey Goldberg, to a Signal group chat discussing a military action in Yemen.\n\nThis was quickly confirmed to be authentic.\n\nBrian Hughes, the spokesman for the National Security Council, responded two hours later, confirming the veracity of the Signal group. “This appears to be an authentic message chain, and we are reviewing how an inadvertent number was added to the chain,” Hughes wrote.\nThe Trump Administration Accidentally Texted Me Its War Plans — The Atlantic\n\nIn the aftermath of this glorious fuck-up by the Trump administration, I have observed many poorly informed hot takes. Some of these were funny, but others are dangerous: they were trying to promote technologies that claim to be Signal alternatives, as if this whole story was somehow a failure of Signal’s security posture.\n\nArt: CMYKat\n\nNot to put too fine a point on it: Switching to Threema or PGP would not have made a lick of difference. Switching to Matrix would have only helped if you consider “unable to decrypt message” helping.\n\nTo understand why, you need a clear understanding of what end-to-end encryption is, what it does, what it protects against, and what it doesn’t protect againt.\n\nTowards A Clear Understanding\n\nImagine for a moment that you decided to build a dedicated, invite-only app that enables you to chat with your friends. For the sake of argument, let’s assume that your friends use multiple devices (computers, smartphones, tablets, one uses a watch, etc.). Despite this, they aren’t always online.\n\nThe simplest way to implement such an app requires an architecture that looks like this:\n\nYou have an app (or a website) that your friends use to chat with their friends.\n\nIf it’s a website, it lives in a browser window.\n\nIf it’s an app, it’s installed on their devices.\n\nYou have some sort of channel for passing messages between users.\n\nThis is often a server that the apps or websites connect to, but you can also setup some sort of peer-to-peer infrastructure.\n\nAny additional requirements only add to the complexity. For example, a consistent message history across devices without storing the message history server-side is actually doable, but requires a bit of careful planning.\n\nWhat’s End-to-End Encryption?\n\nIf you were to take this abstract description of a chat app and add end-to-end encryption to it, what you end up doing is encrypting messages with the app such that only your friends can decrypt them, using the app on their own devices.\n\nThus, the “ends” are the software running on each device (also called “endpoints”).\n\nThis is in contrast with transport encryption (which protects messages between your app and the channel, and also from the channel and your friends’ apps, but not from the channel itself) and at-rest encryption (which protects messages on your device when you’re not using it).\n\nHow Does End-to-End Encryption Improve Security?\n\nEnd-to-end encryption (E2EE for short) aims to ensure only the participants added to a conversation can read its contents.\n\nThe channel that delivers encrypted messages shouldn’t be able to read them.\n\nOther Internet-enabled devices that can observe your network traffic to/from the chat app shouldn’t be able to read them.\n\nWhat Doesn’t E2EE Give Us?\n\nFor one, just because you’re having a private conversation doesn’t mean you’re having a trustworthy conversation. An encrypted chat with a scammer will not save you from being scammed.\n\nAnd as we saw with yesterday’s news story, E2EE also doesn’t prevent you from accidentally adding an editor for a newspaper to a group chat.\n\nOn Security Clearances and SCIFs\n\nI must be cautious with my wording here, as I have never held a security clearance in my life (and I’ve never aspired to hold one).\n\nEven if a smartphone app was developed tomorrow that successfully boasted better cryptographic security than Signal, it still wouldn’t be an appropriate tool for classified communications for military operations. In fact, I do not foresee any smartphone app ever being approved for this purpose.\n\nBroadly speaking, when politicians describe Signal as a “non-secure” channel, they’re not talking about its cryptographic security at all. Instead, they’re remarking that Signal has not been approved by the US government for this sort of purpose.\n\nAdditionally, Signal’s “disappearing messages” feature is fundamentally incompatible with the requirements of governments to preserve documents (even if they’re highly classified).\n\nWhen government and military officials want to discuss operations, they’re typically required to go into a SCIF (Sensitive Compartmented Information Facility), which ensures:\n\nThat they are not being wiretapped. (To this end, mobile phones are not permitted in a SCIF.)\n\nThat they can only access information they need access to (thus, compartmented).\n\nThat they are discussing information only with other, known individuals with the relevant security clearances.\n\nThere’s almost certainly more to SCIFs than just what I listed, but as I said previously, I don’t have any firsthand experience with them. (If that’s what you’re looking for, ask one of the many security industry personalities on social media that advertise prior military experience.)\n\nPoint being: SCIFs are the right tool for the job. Smartphone apps like Signal are not.\n\nSCIFs and Signal have wildly different threat models!\n\nIt is not a failure of Signal (or any other E2EE technology) to not be suitable for military operations.\n\nArt by AJ\n\nQuick Recap\n\nEnd-to-end encryption provides confidentiality and integrity of messages between endpoints in a network.\n\nWhen implemented correctly, E2EE prevents server software from reading the contents of messages or tampering with them.\n\nE2EE doesn’t protect messages after they’re delivered.\n\nE2EE doesn’t magically make your conversations trustworthy. You could have a totally encrypted, private conversation with the editor of The Atlantic.\n\nE2EE isn’t sufficient for military use, especially when implemented as a smartphone app.\n\nWhy Do We Even Encrypt?\n\nBad takes aside, one area of confusion that occurs frequently when discussing encryption technology is the motive for using it to begin with.\n\nThis might sound silly, but there are actually at least three different answers here.\n\nPrivacy. This is probably the most obvious one, especially for the sort of people that read my blog.\n\nAccess controls. To certain business types, encryption is a means to an end. It ensures that only people with permission can read the data in a way that’s harder to bypass than filesystem permissions.\n\nCompliance. This is the perspective of the sort of person that will store the key and ciphertext in the same database just to claim it’s encrypted at rest to comply with their understanding of (for example, HIPAA) compliance requirements–even if this is, ultimately, security theater.\n\nSpeaking from experience, it’s very difficult to convince people in the second and third camps that a cryptographic weakness in their implementation of a protocol is important.\n\nLaw enforcement wavers back and worth on end-to-end encryption. Once called the “going dark” problem by the FBI, apps like Signal are now touted as an important security measure for Americans in the wake of salt typhoon. (On that note, I don’t think our telecom systems are robust at all.)\n\nI think it’s reasonable to conclude that, when they’re promoting the use of encryption technology, the US government at large thinks of encryption as a form of access controls rather than privacy. After all, encryption-as-privacy gets in their way.\n\nFurthermore, the recurring demands for “lawful intercept” capabilities (a.k.a. backdoors) is compatible with a “encryption as an access controls mechanism” worldview, while it undermines privacy.\n\nBut fuck them, and fuck what they want.\n\nEncryption should be a privacy technology first and foremost.\n\nAlways say “No” to backdoors.\n\nAs politicians continue to decry the Trump administrations’ use of Signal for military operations, remember that it’s about not using an approved tool for military communications rather than misgivings about the cryptography used by Signal (which I recently reviewed).\n\nClosing Remarks\n\nAnyone who insists that the the real problem in this story is that the cryptographic security of Signal is somehow lacking (compared to that person’s preferred communication software) is either a liar or a fucking moron.\n\nArt: CMYKat\n\nA lot of the people pushing these other apps are really arguing for inferior cryptographic security (as I’ve demonstrated in my previous work on the topic).\n\nPredictions and Speculation\n\nI’ve kept the actual politics content in this blog post light, mostly because social media is oversaturated with opinions already and nobody should care what I think.\n\nAnd so, feel free to stop reading if you don’t care. Everything before this section was focused on facts, not opinions.\n\nPrediction: None the people that participated in this Signal group chat will face meaningful accountability for their actions. Party loyalty, at any cost, is a mainstay of the current regime.\n\nIf anyone does face consequences, at all, it will be the The Atlantic for breaking the news story. And possibly Signal will lose whatever funding they get from the US government in retribution for making the White House look bad.\n\nEDIT: It looks like the first part of this prediction is likely to be correct.\n\nSpeculation: Adding the press to a military group chat after you’ve compromised one of the participants’ smartphones seems like a good disruption tactic if you’re another nation state. Thus, I would not be entirely surprised to learn that this is why Jeffrey Goldberg ended up in the group chat (rather than pure human error by incompetent government officials).\n\nHeader art: Sophie\n\n\t\t\t\t\t\t\t\tTags\n\n\t\t\t\t\t\t\tE2EE, encryption, end-to-end encryption, military, mishandling classified data, security clearances, Signal, Trump administration, trust\n\n\t\t\tBy Soatok\n\n\t\tSecurity engineer with a fursona. Ask me about dholes or Diffie-Hellman!\n\n\t\t\tView Archive →\n\n\t\t\t\t\t←\n\t\t\t\t\tPost-Quantum Cryptography Is About The Keys You Don’t Play",
    "summary": {
      "en": "**Summary of \"The Practical Limitations of End-to-End Encryption\"**\n\nThe article discusses the misconceptions surrounding end-to-end encryption (E2EE), particularly in light of a recent incident where the Trump Administration mistakenly added a journalist to a private group chat about military operations. This incident highlighted misunderstandings about E2EE's capabilities and limitations.\n\nKey Points:\n\n1. **End-to-End Encryption (E2EE)**: E2EE ensures that only the participants in a conversation can read the messages, protecting them from being accessed by the service provider or any third parties during transmission. However, it does not guarantee the trustworthiness of the conversation or prevent accidental inclusions of unintended participants.\n\n2. **Misunderstandings of E2EE**: Many people incorrectly attribute failures in security to E2EE technology itself, rather than understanding that issues often arise from user error or the inappropriate use of the technology for sensitive communications.\n\n3. **Limitations in Military Use**: E2EE apps like Signal are not suitable for classified military communications because they do not meet the security requirements necessary for such sensitive information, which are typically handled in secure facilities (SCIFs).\n\n4. **Motivations for Encryption**: Encryption is used mainly for privacy, access control, and compliance, with different stakeholders having varying priorities regarding its implementation. The author argues that encryption should prioritize privacy over access control, rejecting any calls for backdoors.\n\n5. **Conclusion**: The article emphasizes that the real issue with the Trump Administration's incident was not the security of Signal itself, but rather the inappropriate context in which it was used. It warns against misleading narratives that suggest E2EE technologies are inherently flawed.\n\nOverall, the article aims to clarify what E2EE can and cannot do and to address the misconceptions that often arise in discussions about digital security.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "d9f28312a418739d",
    "title": {
      "en": "The Great Barefoot Running Hysteria of 2010",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://runningshoescore.com/blog/barefoot-running-hysteria-of-2010",
    "score": 133,
    "by": "jonnycat",
    "time": 1742898502,
    "content": "Something went wrong!\n\n    Hang in there while we get back on track\n\n    Home\n\n    The Great Barefoot Running Hysteria of 2010\n\n03/25/25 • 9 minute read\n\nThe year was 2010.  Ke$ha's “Tik Tok”  was topping the Billboard charts.  Steve Jobs has just introduced a goofy new oversized iPhone called an “iPad”.  And in running forums across the internet far and wide, hoards of enthusiasts preached the gospel of a new way of running: without shoes.Google Trends interest in \"Barefoot Running\" from 2004 to presentThe “Great Barefoot Running Hysteria of 2010”, as I call it, took the amateur running world by storm.  Propelled by dramatic claims of performance improvements and injury prevention, barefoot running gave rise to a vocal (and often militant) contingent of enthusiasts and entirely new classes of footwear.  And then over the course of a several years, it faded away almost as quickly as it came, leaving behind changes in running shoes and culture forever.  In this post, we'll explore the history and legacy of the barefoot running movement.The Barefoot Running Movement & How It StartedBarefoot running is–of course–as old as humanity itself. In fact, people have run barefoot throughout most of human history, with the practice continuing today in several cultures, such as Kenya and indigenous peoples in Mexico.  In this sense, before addressing the modern origins of barefoot running, we need to talk about the origins of shod running.  The concept of running shoes as we understand them today, specifically designed to improve running efficiency and comfort, did not emerge until the late 19th and early 20th centuries. This evolution coincided with the rise of organized sports and recreational running, which spurred the development of footwear tailored to the specific needs of runners. The first breakthrough came in 1865 when an English shoemaker brilliantly suggested adding spikes to otherwise normal-looking dress shoes to make them suitable for cross-country running. Later, in the early 20th century, shoes with rubber soles were introduced, offering improved grip and shock absorption, marking a significant advancement in the design of running shoes.The result of these running shoe trends was running shoe stores filled with bulky, over-engineered clown shoes that promised to prevent running injuries, but probably didn't.Into more modern times, running shoes continued to evolve past their humble beginnings into heavier, more complex footwear with exotic materials like plastics & EVA foam.  In the 80s and 90s, many shoemakers became fixated on stability and the notion that pronation, the natural roll of the foot after it lands, was a cause for running injuries.  Running shoes were increasingly designed to try to prevent this movement with wedges of foam that support and stabilize the arch of the foot, a high heel-to-toe drop, and other questionable features like plastic air bubbles in the heel to cushion the foot with each step.The result of these running shoe trends was running shoe stores filled with bulky, over-engineered clown shoes that promised to prevent running injuries, but probably didn't, and were mostly just uncomfortable to use. You can see an example of this kind of design in action in the Nike Air Max 90, which is now also available in a more tasteful re-issue.  While the original was marketed as a running shoe, the modern re-issue is a nostalgic fashion shoe that you should definitely not run in.The original Nike Air Max 90A modern re-issue of the Nike Air Max 90 (from Nike.com)The barefoot running renaissance can thus be understood as a backlash to the dominant running shoe trends at the time.  The revival was driven by a growing dissatisfaction with traditional running shoes, which many believed contributed to injuries and impeded natural foot movement. Proponents of barefoot running argued that it encouraged a more natural gait, reducing the impact on the legs and back and enhancing the overall running experience.In 2004, inspired by Stanford athletes training barefoot, Nike introduced the Nike Free, a running shoe that bucked the bulky running shoe trends of the time with a super flexible sole and a minimal heel-to-toe offset.The original Nike Free (via Wikipedia)Then, in 2006, Vibram launched the 5-Fingers: a shoe intended to be as close to barefoot as possible, with a thin rubber sole and glove-like fit for the toes.  This was the shoe that truly ushered in the minimalist mindset: running shoes should enable a \"natural\" gait and that less is more. Vibram 5 Fingers (via Amazon)The barefoot-inspired Nike Free and Vibram 5 Fingers set the stage, but the real barefoot running revolution began not with a pair of shoes, but with a book about an indigenous tribe of Native Mexicans.Christopher McDougall's \"Born to Run\"While the over-engineered running shoe backlash had already started with shoes like the Nike Free and the Vibram 5 Fingers, the spark that lit the barefoot running powder keg was Christopher McDougall's 2009 bestseller, \"Born to Run\". The book, which explores the running habits of the Tarahumara Native Mexican tribe, known for their long-distance running ability, captivated the imagination of runners–and non-runners–everywhere. Book cover for \"Born to Run\" by Christopher McDougallThe Tarahumara, or Rarámuri, as they refer to themselves, are an indigenous people who reside in the rugged and remote Copper Canyon region of Northwestern Mexico. Renowned for their extraordinary long-distance running abilities, the Tarahumara have garnered international attention and admiration from runners and researchers alike. Their running habits, deeply embedded in their culture and lifestyle, are not merely for sport but serve practical and ceremonial purposes as well.McDougall's narrative suggested that modern running injuries were virtually non-existent among the Tarahumara.  They often ran barefoot or in minimal footwear, sandals crafted from leather and tire strips called huaraches.  The sandals provide minimal cushioning and protection, promoting a natural running form that many attribute to their low incidence of running-related injuries according to McDougall.  The popularity of McDougall's book sparked widespread curiosity and enthusiasm for barefoot running, and many readers happily ditched their bulky running shoes for new, minimalist alternatives.  As the barefoot running movement evolved, so did the market for running footwear, leading to the development of minimalist shoes designed to mimic the barefoot running experience while providing some protection from the hazards of rough terrain. The Influx of (Mostly Newbie) Barefoot RunnersBolstered by the popularity of \"Born to Run\", barefoot running was suddenly everywhere.  This period saw a surge in barefoot running clinics, forums, and social media groups where enthusiasts shared tips and experiences.  And as often happens, what started as a perfectly reasonable idea took on a life of its own and became dogmatic: barefoot running was the way to run.  The movement gave rise to a series of increasingly lofty and strongly worded claims: barefoot running prevented injuries; barefoot running was more efficient; heel striking was evil; barefoot running was the natural and therefore \"correct\" way to run.  The idea transcended running communities and seeped into popular culture prompting lofty headlines like this New York Times article, The Once and Future Way to Run.The idea even transcended the sport of running itself.  To many, the barefoot running movement was not merely about the act of running without shoes; it represented a broader philosophy seeking to embrace simplicity, natural form, and mindfulness in the pursuit of physical fitness and well-being.  The fervor around barefoot running bordered on religious.Despite the smug sense of superiority that some barefoot proponents projected, many of the most enthusiastic adopters of barefoot running were, in fact, novice and inexperienced runners.  While barefoot running enthusiasts were eager to point out elite runners training or racing barefoot, like Zola Budd, these barefoot elites are the exception rather than the rule.  Most serious and elite runners generally sat out the barefoot trend, or at least took a more nuanced approach.  Many advanced runners already gravitated towards less bulky shoes, and many that did adopt barefoot running did so as part of isolated workouts on soft surfaces.It's difficult to quantify this claim, but my personal memory of this period was that online discourse around running form and footwear was dominated by an aggressive mob mentality around barefoot running.   If you were running with shoes on, you were–according to the online mob–\"doin' it wrong\", as the internet was fond of saying at the time.  Though it generally didn't represent the opinions of more experienced runners, the barefoot running crowd was certainly the loudest. Opinions that didn't completely jive with the all-minimalist approach were often brutally and violently downvoted.The Backlash and The Downfall of Barefoot RunningDespite the enthusiasm and claims of the benefits of barefoot running, research into the practice was thin.  While some of the claims seemed logical, most of the evidence around the benefits of barefoot running was anecdotal.  Much of the argument hinged around barefoot running being the natural and thus correct way to run, and that running with over-engineered running shoes was unnatural and thus incorrect.Some will recognize this line of reasoning as the appeal to nature fallacy: a logical fallacy in which a subject is claimed to be good simply because it is natural.  The fallacy pops up frequently in health & medical settings when people extol the virtues of \"all-natural\" products or alternative medicines.  Sure, some natural products are healthy and beneficial; but so are a lot of deadly poisons.  Similarly, diet, where the appeal to nature is used to justify all sorts of sometimes conflicting food choices (throughout history humans have eaten wildly versatile diets).  This doesn't mean that natural is bad, it simply means that it's not a valid argument in and of itself for the benefits of barefoot running.As time went on and the research caught up with the trend, the results were mixed at best: some studies showed potential benefits, while others highlighted increased risks. Critics of barefoot running point to evidence showing an increased incidence of certain types of injuries, such as Achilles tendinitis and metatarsal stress fractures, among runners transitioning to barefoot or minimalist running without proper adaptation. Additionally, there's an obvious concern about the lack of protection from environmental hazards (e.g., sharp objects, rough terrain) when running without traditional footwear, which can lead to acute injuries. At the same time, many fledgling barefoot runners soon faced a reality check. Reports of injuries began to surface, casting doubt on the benefits of running without shoes. Podiatrists and sports medicine professionals started warning about the potential risks, especially for those with pre-existing foot conditions, those who transitioned too quickly, or those who tried to run too much. Many who stuck with the sport and aspired to run longer distances such as the marathon soon learned that it's simply difficult to put in the mileage required to excel at these distances without proper footwear.  Though you're sure to find counterexamples (I still see a small handful of barefoot runners at major running events like the Boston Marathon), most runners who are putting in 50, 60, 70, or more miles a week to race a marathon are doing so with shoes designed to help cushion the impact.Long-Lasting Changes to Running ShoesDespite the decline in barefoot running's popularity, its impact on the running shoe industry was undeniable. Recognizing the demand for a more natural running experience, shoe manufacturers began developing more lightweight, minimalist running shoes with less cushioning and a lower heel-to-toe drop. More importantly, the minimalist movement helped end the dominance of needlessly overbuilt running shoes.  Of course, highly supportive & motion control running shoes are still available, and some runners prefer to run in them, but it's no longer the dominant paradigm.Ironically, the minimalist shoe movement triggered its own backlash with the maximalist shoe movement of mega-cushioned shoes, ushered in by brands like HOKA.  But the maximalist trend is not a return to the overbuilt shoes of the 90s–in fact, it still incorporates minimalist concepts like lower heel-to-toe drop, lightweight materials, and placing less emphasis on the support and motion control that earlier running shoes relied on.  These innovations aimed to combine the benefits of barefoot running with the protection and support of traditional running shoes. Today, many of these features remain integral in modern running shoe design, marking a lasting legacy of the barefoot running movement.So in the end, while the barefoot running hysteria of 2010 may have been a short-lived trend, it sparked crucial conversations about running health and led to significant advancements in running footwear. Though no longer in the limelight, its impact continues to influence how we run and think about our running gear.\n\n    by Jon K.\n\n  Jon has been an avid runner for over 20 years with a marathon PR of 2:43.  He loves everything running & helping others find love for the sport as well.\n\n    Trending Deals\n    See all trending deals...\n\n      Men's New Balance Fresh Foam X More Trail 3\n\n          $79.99\n\n      Women's Nike Pegasus 41 Steelers\n\n          $94.97\n\n      Women's Nike Pegasus 41 Falcons\n\n          $94.97\n\n      Women's Nike Pegasus 41 Ohio State\n\n          $94.97\n\n  Running Shoe Score tracks the pricing and availability of high quality running shoes at a variety of\n  retailers.  Compare prices & find deals on running shoes from top brands.  Pricing & availabilty updated daily!\n\n    Nike\n    | New Balance\n    | Brooks\n    | Asics\n    | Saucony\n    | HOKA\n    | ... and more!\n\n  Pages on this site may contain affiliate links",
    "summary": {
      "en": "In 2010, a trend called \"barefoot running\" gained popularity, driven by claims of improved performance and injury prevention. This movement emerged as a reaction against conventional running shoes, which had become bulky and complex over the years. Proponents believed barefoot running allowed for a more natural foot movement, inspired by cultures where running without shoes is common.\n\nKey events that fueled this trend included the introduction of minimalist shoes like the Nike Free and Vibram FiveFingers, as well as the bestselling book \"Born to Run\" by Christopher McDougall, which highlighted the running practices of the Tarahumara tribe in Mexico. The book suggested that barefoot running led to fewer injuries and a better running experience.\n\nAs enthusiasm surged, online communities formed around barefoot running, promoting it as the ideal way to run. However, many enthusiastic newcomers lacked experience, while seasoned runners often remained skeptical. Over time, research revealed mixed results about the benefits of barefoot running, with some studies linking it to increased injuries for those who transitioned too quickly.\n\nDespite its decline in popularity, the barefoot running movement influenced running shoe design, leading to the creation of lighter, more minimalist shoes that still provide some cushioning. This shift also contributed to the rise of maximalist shoes, which blend the benefits of both minimalist and traditional designs. Ultimately, the barefoot running trend sparked important discussions about running health and changed the landscape of running footwear.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "36cbb80ba911a295",
    "title": {
      "en": "WWI's 'Dazzle' Camouflage Seemed Effective Due to Unexpected Optical Trick",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://gizmodo.com/wwis-famous-dazzle-camouflage-seemed-effective-due-to-unexpected-optical-trick-study-finds-2000577568",
    "score": 96,
    "by": "ilamont",
    "time": 1742680378,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "8e6a6966822b8556",
    "title": {
      "en": "X’s director of engineering, Haofei Wang, has left the company",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.theverge.com/twitter/634833/x-head-engineering-leaves-elon-musk",
    "score": 90,
    "by": "JumpCrisscross",
    "time": 1742907299,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "bd9ab8acef055291",
    "title": {
      "en": "Brain Scans of Infants Reveal the Moment We Start Making Memories",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://singularityhub.com/2025/03/20/new-baby-brain-scans-reveal-the-moment-we-start-making-memories/",
    "score": 88,
    "by": "Brajeshwar",
    "time": 1742574750,
    "content": "ScienceBrain Scans of Infants Reveal the Moment We Start Making MemoriesKids form fleeting memories at around 12 months, even as their brains are rapidly rewiring themselves.Shelly FanMar 20, 2025Image CreditJuan Encalada on UnsplashShareA giggling toddler in a pink dress and matching headphones lies down on her back in front of a gigantic whirling machine. A pillowy headrest cushions her head. She seems unfazed as she’s slowly shuttled into the claustrophobic brain scanner. Once settled, a projection showing kaleidoscope-like animations holds her attention as the magnetic resonance imaging (MRI) machine scans her brain.The girl is part of a new study seeking to answer a century-old mystery: Why can’t most us remember the first three years of our lives? Dubbed “infantile amnesia” by Sigmund Freud, the study could provide insight into how the brain develops during our early years. And if we can form memories at a young age, are they fleeting, or are they still buried somewhere in the adult brain?It seems like a simple question, but an answer has eluded scientists.Though infants and toddlers aren’t yet able to give detailed verbal feedback, studying their behavior has begun to shed light on if and when they remember people, things, or places. Still, the approach can’t peek in on what’s happening in the brain in those early years. MRI can.A team from Columbia and Yale University scanned the brains of 26 infants and toddlers aged 4 to 25 months as they completed a memory task. They found that at roughly a year old, a part of the brain crucial to memory formation spun into action and began generating neural signals related to things the kids remembered from the tests.Called the hippocampus, this sea-horse-shaped structure deep inside the brain is crucial to the encoding of our life stories—who, when, where, what. Adults with a damaged hippocampus suffer memory problems. But because wiring inside the hippocampus is still developing during our earliest years, scientists believe it may be too immature to form memories.“It’s not that we don’t have any memories from that period [infancy],” said study author Nicholas Turk-Browne in a press briefing. “In fact, early life is when we learn our language. It’s when we learn how to walk…learn the names of objects and form social relationships.”“What happens during that period when we learn so much, but remember so little?” he added.Stages of MemoryMemory seems like all-or-none: You either remember something, or you don’t.It’s not that simple. Decades of research have identified the hippocampus as the main orchestrator of episodic memories. These allow you to remember an acquaintance at a party, where you parked your car, or what you had for dinner three nights ago.Each everyday experience is encoded in neural connections in the hippocampus. Groups of neurons called engrams capture different memories and keep them separate, so that they don’t bleed into each other.Once encoded, the brain etches important memories into long-term storage during sleep. Studies of slumbering rodents and humans after learning a new task found that the hippocampus replayed brain activity at higher speed during the night, correlating with better performance on a trained memory task the next day.The last step is retrieval. This is when the brain fishes out stored memories and delivers them to our conscious brain—and so, we “remember.”Failure of any of these steps causes amnesia. So, which steps are responsible for the erosion of baby memories?Bundles of JoyBrain scans from 26 infants now offer some intriguing clues.The team behind the new study scanned the children’s brains with functional MRI (fMRI) as they looked at a screen in the scanner and took a memory test. fMRI captures brain oxygen levels (BOLD) as a proxy for local neuron signaling—higher levels mean more brain activity.The head needs to keep very still throughout the scans to avoid blurring. That’s not easily accomplished with babies and toddlers. Previous studies circumvented the problem by imaging their brains while sleeping, but the results couldn’t capture memory processes.Be Part of the FutureSign up to receive top stories about groundbreaking technologies and visionary thinkers from SingularityHub.ruidWeekly NewsletterDaily NewsletterI agree to receive other communications from Singularity.I agree to allow Singularity to store and process my personal data in accordance with the company's Terms of Use and Privacy Policy.*Subscribe100% Free. No Spam. Unsubscribe any time.To keep the infants happy, engaged, and safe, parents brought favorite blankets and pacifiers, and younger infants were wrapped inside a comfortable vacuum pillow to reduce movement. A video system projected images onto the ceiling of the scanner within their line of sight.As the kids looked at a bright kaleidoscope-like video, images of faces, scenes, and objects would flash for a few seconds. These included toys or landscapes of an alpine cabin with mountains in the background. Previous studies found infants like to stare at objects or images they’ve seen before compared to new objects, suggesting they remember previous encounters.Throughout the sessions the team added projections showing a previously seen picture and a new one and monitored the infants’ eye movement using a video camera.“The ingenuity of their experimental approach should not be understated,” wrote Adam Ramsaran and Paul Frankland at the Hospital for Sick Children in Toronto, Canada, who were not involved in the study.BOLD FindingsThe kids often squirmed during the sessions. Some weren’t interested in the pictures; others fell asleep in the scanner.Still, the team managed to capture hippocampal BOLD signals averaging roughly eight minutes per participant and matched them to memory performance. On average, parts of the hippocampus ramped up activity for images that the infants later remembered—that is, they looked at it for longer during the test phases.But not all infants performed the same. The younger cohort, under a year, didn’t show the surge of BOLD signals suggesting memory encoding. They also ignored already seen images compared to new ones.It seems babies start encoding memories around a year of age, even as their hippocampus is still developing.The results are similar to those in baby rodents. The early years are chaotic. The brain undergoes extensive rewiring. This makes it a difficult to form lasting memories. Yet some supposedly lost memories encoded at a young age can be recovered later in life with reminder cues or by directly activating the set of neurons that originally encoded the memory.That’s not to say infants can acquire rich recollections—stories including multiple people, places, and things—at a year. The study only tested brain signatures for individual components.Future studies tracking the hippocampus might shed light on the minimal brain architecture needed to support vivid autobiographical memories. Examining other stages of memory could shine more light on infantile amnesia. For example, do infants also replay neural signals as they sleep to etch new experiences into long-term memory?And maybe—just maybe—our earliest memories could one day be retrieved later in childhood or beyond.NeuroscienceShelly FanShelly FanDr. Shelly Xuelai Fan is a neuroscientist-turned-science-writer. She's fascinated with research about the brain, AI, longevity, biotech, and especially their intersection. As a digital nomad, she enjoys exploring new cultures, local foods, and the great outdoors.Related ArticlesNew Tech Bends Sound Through Space So It Reaches Only Your Ear in a CrowdJiaxin ZhongandYun JingMar 18, 2025A Massive AI Analysis Found Genes Related to Brain Aging—and Drugs to Slow It DownShelly FanMar 17, 2025Scientists Discover Thousands of New Microbial Species Thriving in the Mariana TrenchShelly FanMar 06, 2025ScienceNew Tech Bends Sound Through Space So It Reaches Only Your Ear in a CrowdJiaxin ZhongandYun JingMar 18, 2025ScienceA Massive AI Analysis Found Genes Related to Brain Aging—and Drugs to Slow It DownShelly FanMar 17, 2025ScienceScientists Discover Thousands of New Microbial Species Thriving in the Mariana TrenchShelly FanMar 06, 2025What we’re readingA New, Challenging AGI Test Stumps Most AI ModelsTechCrunchCould Nvidia’s Revolutionary Optical Switch Transform AI Data Centers Forever?IEEE SpectrumJust One Exo-Earth Pixel Can Reveal Continents, Oceans, and MoreBig Think23andMe Site Went Down as Customers Struggled to Delete DataThe Wall Street JournalScientists Find Giant Organic Molecules on Mars as Old as Life on EarthGizmodoThe Quantum Apocalypse Is Coming. Be Very AfraidWiredA Mysterious Startup Is Developing a New Form of Solar GeoengineeringWiredInside Google’s Two-Year Frenzy to Catch Up With OpenAIWired1X Will Test Humanoid Robots in ‘a Few Hundred’ Homes in 2025TechCrunchWatch the Atlas Robot Bust a Move in Boston Dynamics’ Latest VideoEngadgetIs Dark Energy Getting Weaker? New Evidence Strengthens the Case.QuantaSatellite Internet Will Enable AI in EverythingWiredAI-Driven Weather Prediction Breakthrough ReportedThe GuardianIn Las Vegas, a Former SpaceX Engineer Is Pulling CO2 From the Air to Make ConcreteFast CompanyThe Unbelievable Scale of AI’s Pirated-Books ProblemThe AtlanticTop Developers Want Nvidia Blackwell Chips. Everyone Else, Not So MuchThe InformationOpenAI’s Deep Research Agent Is Coming for White-Collar WorkWiredNot Everyone Is Convinced by Microsoft’s Topological QubitsIEEE SpectrumJudge Disses Star Trek Icon Data’s Poetry While Ruling AI Can’t Author WorksArs TechnicaSee the First Ever Footage of Sunset on the Moon Captured by Blue GhostDigital Trends",
    "summary": {
      "en": "A recent study using MRI scans has revealed that infants start forming memories around the age of 12 months. This research aims to understand \"infantile amnesia,\" the phenomenon where people typically cannot remember their early childhood years. Scientists from Columbia and Yale University scanned the brains of 26 infants aged 4 to 25 months while they performed memory tasks. They found that the hippocampus, a brain region vital for memory, becomes active around one year of age, suggesting this is when memory encoding begins.\n\nInfants learn many things during their first years, such as language and social relationships, but their memories may be fleeting due to ongoing brain development and rewiring. The study showed that older infants remembered familiar images better than new ones, while younger infants showed no signs of memory encoding. This indicates that while some memories from infancy might be formed, they may not be easily retrievable later in life.\n\nFuture research may help uncover how memories are processed and stored in young children, potentially explaining why early memories are hard to recall as adults.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "53444c1c7df643ad",
    "title": {
      "en": "We chose LangGraph to build our coding agent",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.qodo.ai/blog/why-we-chose-langgraph-to-build-our-coding-agent/",
    "score": 53,
    "by": "jimminyx",
    "time": 1742881836,
    "content": "Why we chose LangGraph to build our coding agent\n\n\t\t\t\t\t\t\t\tTechnology\n\n\t\t\t\t\t\t\t\tSagi Medina\n\n\t\t\t\t\t\t\t\t\tMarch 21, 2025\n\n\t\t\t\t\t\t\t\t\t4 min\n\n\t\t\t\t\t\tTable of ContentsFlexibility to be opinionatedCoherent interfaceReusable components across workflowsState managementAreas for growth\n\n\t\t\t Join Our Discord\n\n\t\t\t\t\tWe’ve been building AI coding assistants at Qodo since the GPT-3 days. Our initial approach was highly structured with predefined flows for different coding tasks like test generation, code reviews, and improvements. This approach worked well with earlier generations of LLMs and with structured flows we were able to get real-world value from older models, despite all of their limitations.\nSince Claude Sonnet 3.5 was released 9 months ago, LLMs have become significantly more capable at general-purpose coding tasks. The new models opened up the possibility to build something more dynamic and flexible while still maintaining our standards for code quality. We wanted to move away from rigid workflows to an agent that could adapt to any kind of user request, while still reflecting our opinionated views on how AI can be best used for coding.\nInitially, we needed a framework that would let us quickly validate our ideas and from the few options that were available about 4 months ago, we settled on LangGraph for our initial proof of concept. We were pleasantly surprised to see that the framework has proven flexible and mature enough to carry us all the way to production.\nIn this post, I’ll explain why LangGraph was the right choice for us, and how it enabled us to build a coding assistant that balances flexibility with our opinionated approach to coding best practices.\nFlexibility to be opinionated\nOur key consideration was the ability to create opinionated workflows while maintaining adaptability. LangGraph takes a graph-based approach that gives you flexibility to build agents that land anywhere on the spectrum from completely open-ended — where you just give an LLM all available tools and let it run in a loop — to fully structured deterministic flows (like the ones we started with).\nAt its core, LangGraph lets you define a state machine for your agent. You create nodes that represent discrete steps in your workflow and edges that define the possible transitions between them. Each node can perform specific functions—gathering context, planning, generating code, or validating—while the graph structure determines how these functions connect.\nThe density of connections in the graph corresponds to how structured or flexible your agent is. A sparse graph with few connections corresponds with a more rigid, predictable flow where each step leads to exactly one next step. A dense graph with many interconnections gives the agent more freedom to choose its path.\nFuture, more capable models might work best with fully open-ended approaches. But even with the best current LLMs, you still get better results when you guide them through the problem. If you use LLMs directly for coding, you’ve probably already developed your own workflow — like breaking problems down, providing context strategically, guiding the model through complex reasoning, and backtracking or iterating when needed.\nThe nice thing about LangGraph’s flexibility is that we can easily recalibrate how structured our flows are when new, more powerful models are released.\nOur main flow follows a pattern that you might recognize: first, a context collection node gathers relevant information from the codebase (and external resources via MCP integration); next, a planning node breaks down the task into manageable steps; then an execution node generates the actual code; finally, a validation node checks the output against best practices and requirements. When validation fails, the agent loops back to execution with specific feedback rather than starting from scratch.\n\nCoherent interface\nWhen you’re building a complex system, a framework should simplify rather than complicate your work. LangGraph’s API does exactly that.\nHere’s how a simplified version of our main workflow looks when implemented with LangGraph:\nPlain textCopy to clipboardOpen code in new windowEnlighterJS 3 Syntax Highlighterfrom langgraph.graph import StateGraph, ENDworkflow = StateGraph(name=\"coding_assistant\")workflow.add_node(\"context_collector\", collect_relevant_context)workflow.add_node(\"task_planner\", create_execution_plan)workflow.add_node(\"task_executor\", execute_plan)workflow.add_node(\"validator\", validate_output)# Define flow between nodesworkflow.add_edge(\"context_collector\", \"task_planner\")workflow.add_edge(\"task_planner\", \"task_executor\")workflow.add_edge(\"task_executor\", \"validator\")# Conditional routing based on validation resultsworkflow.add_conditional_edges(    \"validator\",    should_revise,    {        True: \"task_executor\",  # Loop back if revision needed        False: END               # Complete if validation passes    })graph = workflow.compile()graph.invoke({\"user_input\": \"build me a game like levelsio\"})from langgraph.graph import StateGraph, END\n\nworkflow = StateGraph(name=\"coding_assistant\")\nworkflow.add_node(\"context_collector\", collect_relevant_context)\nworkflow.add_node(\"task_planner\", create_execution_plan)\nworkflow.add_node(\"task_executor\", execute_plan)\nworkflow.add_node(\"validator\", validate_output)\n\n# Define flow between nodes\nworkflow.add_edge(\"context_collector\", \"task_planner\")\nworkflow.add_edge(\"task_planner\", \"task_executor\")\nworkflow.add_edge(\"task_executor\", \"validator\")\n\n# Conditional routing based on validation results\nworkflow.add_conditional_edges(\n    \"validator\",\n    should_revise,\n    {\n        True: \"task_executor\",  # Loop back if revision needed\n        False: END               # Complete if validation passes\n    }\n)\n\ngraph = workflow.compile()\ngraph.invoke({\"user_input\": \"build me a game like levelsio\"})from langgraph.graph import StateGraph, END\n\nworkflow = StateGraph(name=\"coding_assistant\")\nworkflow.add_node(\"context_collector\", collect_relevant_context)\nworkflow.add_node(\"task_planner\", create_execution_plan)\nworkflow.add_node(\"task_executor\", execute_plan)\nworkflow.add_node(\"validator\", validate_output)\n\n# Define flow between nodes\nworkflow.add_edge(\"context_collector\", \"task_planner\")\nworkflow.add_edge(\"task_planner\", \"task_executor\")\nworkflow.add_edge(\"task_executor\", \"validator\")\n\n# Conditional routing based on validation results\nworkflow.add_conditional_edges(\n    \"validator\",\n    should_revise,\n    {\n        True: \"task_executor\",  # Loop back if revision needed\n        False: END               # Complete if validation passes\n    }\n)\n\ngraph = workflow.compile()\ngraph.invoke({\"user_input\": \"build me a game like levelsio\"})\n\nThis declarative approach makes the code almost self-documenting. The workflow definition directly mirrors our conceptual diagram, which makes it easy to reason about and modify.\nEach node function receives the current state and returns updates to that state. There’s no magic happening behind the scenes, just straightforward state transitions.\nLangChain gets a lot of flack for its overly complicated abstraction, but the team really cooked with the LangGraph interface. It adds just enough structure without getting in your way or forcing you to adopt a complicated mental model and puts your agent logic on full display rather than obscuring it behind abstractions.\nReusable components across workflows\nReusability is what separates valuable frameworks from disposable ones. The node-based architecture that LangGraph uses is great here.\nOur context collection node is a good example. It handles gathering relevant information from the codebase and it’s used in pretty much every flow. The same goes for our validation node, which checks code quality and runs tests. These components can slot into different graphs with minimal configuration.\nAs we build out more flows, the velocity payout is huge. We’re building specialized flows like TDD that have different structures but reuse many of the same nodes, just connected in a different configuration with a few specialized components added in.\n\nState management\nThe most satisfying part of adopting the right framework is when you get useful functionality out of the box. LangGraph’s built-in state management is a perfect example.\nAdding persistence to our agent took just a few lines of code:\nPlain textCopy to clipboardOpen code in new windowEnlighterJS 3 Syntax Highlighterfrom langgraph.graph import StateGraph, ENDfrom langgraph.checkpoint.postgres import PostgresSaverworkflow = StateGraph(name=\"coding_assistant\")......checkpointer = PostgresSaver.from_conn_string(    \"postgresql://user:password@localhost:5432/db\")checkpointer.setup()graph = workflow.compile(checkpointer=checkpointer)from langgraph.graph import StateGraph, END\nfrom langgraph.checkpoint.postgres import PostgresSaver\n\nworkflow = StateGraph(name=\"coding_assistant\")\n...\n...\n\ncheckpointer = PostgresSaver.from_conn_string(\n    \"postgresql://user:password@localhost:5432/db\"\n)\ncheckpointer.setup()\n\ngraph = workflow.compile(checkpointer=checkpointer)from langgraph.graph import StateGraph, END\nfrom langgraph.checkpoint.postgres import PostgresSaver\n\nworkflow = StateGraph(name=\"coding_assistant\")\n...\n...\n\ncheckpointer = PostgresSaver.from_conn_string(\n    \"postgresql://user:password@localhost:5432/db\"\n)\ncheckpointer.setup()\n\ngraph = workflow.compile(checkpointer=checkpointer)\n\nThat’s it. With this simple addition, our entire workflow state—including context collected, plans made, and code generated—persists to our postgres database without us building any custom infrastructure. There are also SQLite and in memory checkpointers that can be added just as easily.\nWhat’s really neat is that this doesn’t just enable basic persistence across sessions. It supports checkpoints and branch points so you can undo and replay changes.\nAreas for growth\nWhile LangGraph has been a great foundation for our agentic flows, it’s not without challenges. One pain point has been documentation. The framework is developing very quickly and the docs are sometimes incomplete or out of date. The maintainers are great and were super responsive on Slack (thanks Harrison and Nuno for all the help :)). Be prepared to potentially need to communicate directly with the project maintainers if you’re using the newer and more niche capabilities.\nTesting and mocking is a huge challenge when developing LLM driven systems that aren’t deterministic. Even relatively simple flows are extremely hard to reproduce. Our agent interacts extensively with the IDE, which is difficult to simulate in automated tests. We built a mock repository that simulates basic IDE operations, but it doesn’t perfectly replicate the real environment. This creates a gap between what we can test automatically and what happens in production.\nFor example, operations like “find all usages of this function” that depend on the IDE’s language server are particularly hard to mock. This forced us to rely more on manual testing than we’d prefer, which slowed down the iteration cycle.\nMature frameworks tend to provide robust infrastructure for mocking and testing. I’m hopeful that LangGraph will develop in these areas over time.\n\n\t\t\t\t\t\t\t\tSagi Medina\n\n\t\t\t\t\t\t\t\t\tMarch 21, 2025\n\n\t\t\t\t\t\t\t\t\t4 min\n\n\t\t\t\t\t\tStart to test, review and generate high quality code\n\n\t\t\tGet Started\n\n\t\t\t\t\t\t\tGet Qodo\n\n\t\t\t\t\t\t\t\t\t\tVS Code\n\n\t\t\t\t\t\t\t\t\t\tJetBrains\n\n\t\t\t\t\t\t\t\t\t\tGithub, Qodo Merge\n\n\t\t\t\t\t\t\tSubscribe to the newsletter\n\n\t\t\t\t\t\t\t\thbspt.forms.create({\n\t\t\t\t\t\t\t\t\tregion: \"eu1\",\n\t\t\t\t\t\t\t\t\tportalId: \"26816375\",\n\t\t\t\t\t\t\t\t\tformId: \"6233f5ed-8ded-40a7-94cf-8fe3631b4b56\",\n\t\t\t\t\t\t\t\t\tonFormReady: function($form) {\n\t\t\t\t\t\t\t\t\t\tdocument.querySelector('.hs-input[type=\"email\"]').value = '';\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t});\n\n\t\t\t\t\t\t\tShare\n\nMore from our blog\n\n\t\t\t\t\t\t\t\t\tCode Reviews\n\n\t\t\t\t\t\t\t\t\tNews & Updates\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t4 min\n\n\t\t\t\tQodo Merge 1.0: solving key challenges in AI-assisted code reviews\n\n\t\t\t\t\t\t\t\tElana Krasner\n\n\t\t\t\t\t\t\t\tJan 29, 2025\n\n\t\t\t\tRead more\n\n\t\t\t\t\t\t\t\t\tLearn\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t14 min\n\n\t\t\t\tCommon JavaScript Errors Explained and How to Fix Them\n\n\t\t\t\t\t\t\t\tDavid Parry\n\n\t\t\t\t\t\t\t\tMar 24, 2025\n\n\t\t\t\tRead more\n\n\t\t\t\t\t\t\t\t\tTesting\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t10 min\n\n\t\t\t\t10 Best Automated Unit Testing Tools for Java\n\n\t\t\t\t\t\t\t\tMarin Levrel\n\n\t\t\t\t\t\t\t\tMar 24, 2025\n\n\t\t\t\tRead more\n\n\t\t\tBrowse the blog",
    "summary": {
      "en": "**Summary: Why We Chose LangGraph for Our Coding Agent**\n\nAt Qodo, we've been developing AI coding assistants since the GPT-3 era, initially using structured workflows for tasks like test generation and code reviews. With the arrival of more advanced models like Claude Sonnet 3.5, we aimed to create a more flexible coding agent that adapts to user requests while adhering to our coding standards. \n\nWe selected LangGraph for its ability to support both structured and flexible workflows through a graph-based system. This allows us to define specific steps (nodes) in our coding process while maintaining flexibility in how they connect. Our main workflow includes steps for gathering context, planning, executing code, and validating outputs, ensuring efficient problem-solving.\n\nLangGraph's clear interface makes it easy to implement and understand our coding agent's structure. Its reusable components allow us to integrate common tasks across different workflows, enhancing our development speed. \n\nAdditionally, LangGraph simplifies state management, enabling us to easily persist data and manage workflow states without extensive custom coding.\n\nHowever, there are challenges, particularly with documentation and testing. The framework is rapidly evolving, which sometimes leads to outdated materials, and testing interactions with the IDE can be complex. We hope future updates will address these issues.\n\nOverall, LangGraph has been instrumental in helping us build a dynamic and effective coding assistant.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "dfc013cd9b62a96e",
    "title": {
      "en": "Battling the Blaze: Artifacts from the history of firefighting",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://worldhistory.substack.com/p/battling-the-blaze",
    "score": 12,
    "by": "crescit_eundo",
    "time": 1742737584,
    "content": "Share this postLooking Through the PastBattling the BlazeCopy linkFacebookEmailNotesMoreDiscover more from Looking Through the PastIlluminating forgotten corners of history and using them to think about the presentOver 1,000 subscribersSubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.Already have an account? Sign inBattling the BlazeArtifacts from the history of firefightingGeorge DillardMar 23, 202510Share this postLooking Through the PastBattling the BlazeCopy linkFacebookEmailNotesMore2ShareWelcome to Looking Through the Past! At its heart, this is a project in which I find a bunch of cool stuff every week and share it with you. It takes hours of research and writing to produce. If you’re new here, click the button below to subscribe. If you’re already a free subscriber who appreciates getting a bunch of cool stuff in your inbox every week, please consider a paid subscription, for less than $1 a week.SubscribeI thought I knew how I was going to introduce this week’s topic — some fascinating artifacts from the history of firefighting — but it seems that I’ve hit a snag. Let me explain. In our wealth-obsessed society, there’s one thing that reliably attracts attention: the Rich Guy. People pay an immense amount of attention to what we used to call the Lifestyles of the Rich and Famous. If you’re like most Americans, your brain is cluttered with facts or suppositions about Elon’s kids, Bezos’ divorce, Warren Buffett’s car, and all sorts of other stuff about Rich Guys that is much less important for you to remember than that meeting you forgot about earlier today. Our fascination with Rich Guys is sometimes a product of admiration — we want to understand their ways so that we can be more like them — and sometimes a result of envy and resentment — we want to revel in the fact that all the money in the world can’t save these guys from misfortune. The Rich Guy obsession applies to historical figures, too. There’s a steady stream of historical content about the Rich Guys of History, whether robber barons like Carnegie and Rockefeller or ancient plutocrats like Mansa Musa or the Rich Guy I want to discuss today, Marcus Licinius Crassus, a member of Rome’s First Triumvirate. Much of the Rich Guys of History content on the internet, it must be said, is quite low quality — it’s clear that people google them a lot, so there’s a lot of clickbait and content-farmed stuff that comes up toward the top of the search results. Much of it is thinly sourced or exaggerated for effect — search for the “richest person in history,” and you’ll enjoy a lukewarm buffet of AI slop combined with slapdash currency conversion and inflation calculation. It’s difficult if not impossible to convert the wealth of a guy like Crassus into modern terms; even if we take the value of all the gold he owned and convert it into dollars at today’s prices, how can we account for all of the slaves he owned or the fact that, for all his wealth, he didn’t have, like, a refrigerator or a telephone?The value of Crassus’ holdings is not the only thing that the Rich Guy of History websites fail to convey accurately. Take, for example, a commonly-repeated story about the source of Crassus’ wealth — that he operated a “fire brigade” whose purpose was, well, I’ll let the New Yorker’s Hendrik Hertzberg explain:Crassus had his own private fire department, and if your house caught fire his representatives would offer to buy it on the spot, at a one-time-only, fire-sale price that would fall rapidly as the flames climbed. If you said yes, you’d get a few sesterces, after which Crassus’ firefighters would do their thing. If you said no, you’d end up with a pile of ashes. Hertzberg’s anecdote is based on a passage from Plutarch:And besides this, observing how natural and familiar at Rome were such fatalities as the conflagration and collapse of buildings, owing to their being too massive and close together, he proceeded to buy slaves who were architects and builders. Then, when he had over five hundred of these, he would buy houses that were afire, and houses which adjoined those that were afire, and these their owners would let go at a trifling price owing to their fear and uncertainty. In this way the largest part of Rome came into his possession.But, beyond this passage (and Plutarch was known to spice up his narratives if he was trying to make a point), there’s really… nothing else to support this idea that pops up again and again in the popular history of Crassus. As Roman historian Liv Mariah Yarrow found when she dug into the matter, this is something that “everyone KNOWS” but is weakly sourced and quite possibly misunderstood. There seem to have been a lot of private fire brigades. Crassus’ organization may not have been unusual, or it may have had other purposes (protecting his own property, for example). Its importance to his wealth is probably exaggerated. So the tidy story I was going to start with — Crassus’ fire brigades as the beginning of firefighting history — seems a little too tidy. Yes, ancient Rome had firefighters (their most important job was probably spotting fires before they grew). Other ancient societies must have had them, as well, although clear evidence is hard to come by. So, sorry — I don’t have a just-so story about the ancient origins of firefighting. What we do know is this: fire has plagued urban society since its beginning, and these societies have worked to prevent and fight fire for most of that history. Despite the danger of fire, organized, professional municipal firefighting is a pretty recent invention. It wasn’t until 1824 that Edinborough established a taxpayer-supported, full-time fire company (Cincinnati was the first American city to do so, in 1853, although some cities had volunteer firefighters prior to that date). Before the advent of municipal firefighters, homeowners had to buy fire insurance. Insurance entitled the policyholder to the services of a company’s fire brigade in the case of an emergency. But how would the fire brigades know which houses were covered by which companies? Insurance firms marked their territory with fire marks — metal or ceramic plaques that designated a building’s insurance provider. They had to be made of fireproof materials because the insurance companies sometimes wouldn’t pay a claim until the mark was found in the rubble of a burned building.These marks still dot old buildings in British and American cities, and many of them are quite attractive. This one comes from early-19th-century London:Public domainAnd this one from Norwich:Evelyn Simak, CC 2.0I enjoy this dragon-themed one from late-19th-century Singapore:Public domainSome featured firefighting equipment, like this serpentine-looking hose on one from Philadelphia:Public domainOr this early fire engine from Darlington, England:Public domainBut the most excellent fire mark has to be this one from the Commercial Union Assurance Company in London, which shows a dragon amidst the flames. Who says insurance has to be boring?Public domainWhat about the firefighters themselves? In the nineteenth century, they didn’t have the advanced equipment that they do today to protect them from the dangers of their job. But this doesn’t mean they didn’t take great pride in their attire. I love the early fire hats — stovepipe hats, mace of felt or leather, that didn’t offer a ton of protection but did advertise the fire company and often the wearer himself. They’re charmingly hand-decorated:Courtesy of the Missouri Historical Society, St. LouisCourtesy of the Missouri Historical Society, St. LouisBen Franklin, the founder of the United States’ first volunteer fire company, was a frequent subject:Courtesy of the Missouri Historical Society, St. LouisWhile others featured other patriotic figures:Courtesy of the Missouri Historical Society, St. LouisSome simply tried to embody the emotions that firefighters must have desperately wanted to provide to the community:Public domainBy the late nineteenth century, the modern fire helmet — made initially of leather — provided more protection, but they still were charmingly hand-painted. This one, appropriately, features a phoenix rising from the ashes:Courtesy of the Missouri Historical Society, St. LouisThe most excellent fire-protection apparel, however, came from Japan. Japanese firefighters wore reversible coats made of quilted cotton. They would soak the coats in water before they went into the flames. Their task was usually to create a fire break by destroying structures around the burning building; water supplies were insufficient to extinguish large fires. These coats were decorated with remarkable mythological designs; the men would often turn the coats inside out so that the designs would not be harmed when they entered the blaze.  Here’s one with an image of a Toad Spirit that taught humans magic:Art Institute of ChicagoOthers had pictures of legendary warriors, whose bravery would have inspired the firefighters (this one features Rōrihakucho Chō Jun):Public domainSome were quite literal, like this coat showing a warrior fighting the spirit of fire:Public domainWhile others evoked the peace and quiet of nature:Public domainLater, the coats were made of leather, which was more protective. This one has the fire company’s beautifully stylized logo on the back:Brooklyn MuseumOver time, of course, firefighters developed more modern protective technology, especially for the danger that came from smoke inhalation. This is an early design from the Warsaw fire department:Public domainSome of the equipment made them look positively alien, like these gas masks worn by Australian firefighters in 1927:Public domainOr these worn by Norwegians in 1941:Municipal Archives of TrondheimIt takes a special type of person to sign up for a job that involves risking your life, breathing smoke, and battling deadly flames — especially at a time when the technology that might protect them was so primitive. So I’ll leave you with some images of the firefighters themselves. Here’s the Jarrow Ladies Fire Brigade, called into service when most of the men from this town near Newcastle were called up for World War I:Public domainHere we have the Athabasca, Alberta fire brigade, who presumably had to battle wildfire more than urban conflagrations:Public domainAnd here’s a Seattle-area “hose team” posing during some kind of tournament, in 1889, the same year that Seattle’s “Great Fire” occurred. Some of the men in the picture would later fall into Puget Sound when a burning dock collapsed beneath their fire wagon:Public domainOne final image — of firefighters in action, carrying an injured person on a stretcher. I love this one; it shows the calm and competence amidst chaos that is the hallmark of the firefighter:Public domainThough the origins of firefighting may be murky, its purpose is clear. Firefighters are there to assist us during unthinkable tragedies, risking their lives to save people and their possessions. The slogans that were painted on early fire engines say it all:“United We Stand, Divided We Fall.”“Non Sibi Sed Omnibus” (\"Not for self, but for all.\")“When Duty Calls, Tis Ours to Obey.”“Semper Paratus” (\"Always Ready\")And, my favorite, “We'll Try.”This newsletter is free to all, but I count on the kindness of readers to keep it going. If you enjoyed reading this week’s edition, there are two ways to support my work: You can subscribe as a free or paying member:SubscribeOr you can share the newsletter with others:Share Looking Through the PastThanks for reading, and I’ll see you again next week!10 Likes∙2 Restacks10Share this postLooking Through the PastBattling the BlazeCopy linkFacebookEmailNotesMore2Share",
    "summary": {
      "en": "The article \"Battling the Blaze\" from the \"Looking Through the Past\" newsletter explores the history of firefighting and its evolution. The author, George Dillard, begins by discussing society's fascination with wealthy individuals, particularly historical figures like Marcus Licinius Crassus, who was part of Rome's First Triumvirate. A popular but exaggerated story claims that Crassus owned a private fire brigade that profited from buying burning homes, but this narrative lacks strong evidence.\n\nDillard explains that while ancient Rome had some form of firefighting, organized municipal firefighting is a relatively modern development, starting in the early 19th century. Before this, homeowners relied on fire insurance, which was marked by plaques indicating coverage. The article highlights various decorative fire marks from old buildings and discusses the attire of firefighters, from early hats to Japanese firefighting coats.\n\nFirefighters have always faced significant dangers, and their roles have evolved with advances in equipment and technology. The article concludes by celebrating the bravery of firefighters, who risk their lives to protect others, underscored by meaningful slogans from early fire engines like “Always Ready” and “We'll Try.”",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "a75affe91b207eea",
    "title": {
      "en": "CO2 laser enables long-range detection of radioactive material",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://physicsworld.com/a/co2-laser-enables-long-range-detection-of-radioactive-material/",
    "score": 101,
    "by": "EA-3167",
    "time": 1742593278,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "7cf3540b3765823d",
    "title": {
      "en": "Gene drive modified mosquitoes offer new tool for malaria elimination efforts",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.imperial.ac.uk/news/261995/gene-drive-modified-mosquitoes-offer-tool/",
    "score": 20,
    "by": "gmays",
    "time": 1742581491,
    "content": "view large\n\n\t\t\t\tGenetic technology co-developed at Imperial could help to eliminate malaria by making mosquitos resistant to the parasite that spreads the disease.\n\n\t\t\t\tTransmission Zero, a research team from Imperial College London in partnership with the Ifakara Health Institute and the National Institute of Medical Research in Tanzania, has developed genetic technology which renders a mosquito unable to transmit the malaria parasite and has a gene drive that ensures that future generations are also resistant to the parasite.\nThe technology could significantly reduce the malaria burden in high-risk countries, saving hundreds of thousands of lives each year, especially among children – who are disproportionately at risk from the disease.\n“Our technology is equitable, it offers hope in the fight against malaria and doesn’t present economic or social barriers to malaria intervention access.” Professor George K. Christophides Professor of Infectious Disease and Immunity at Imperial College London\nThe work of Transmission Zero has been highlighted in a new Global Development Hub STEM forDevelopment Impact Memo authored by Professor George K. Christophides, Professor of Infectious Disease and Immunity at Imperial, Dr Nikolai Windbichler, Reader in Genetics at Imperial, and Dr Dickson Wilson Lwetoijera, Principal Research Scientist at Ifakara Health Institute in Tanzania.\nProfessor Christophides from Imperial’s Department of Life Sciences said: “The solutions we currently have for malaria are not enough – we need something new.\n“Our technology is equitable, it offers hope in the fight against malaria and doesn’t present economic or social barriers to malaria intervention access.”\n\nRead the fullGlobal Development Hub STEM for Development Impact Memo on Transmission Zero here.\n\nProgress on tackling malaria has flatlined in recent years, with cases and deaths further exacerbated by disruption to health services during the COVID-19 pandemic.\nEach year, more than 263 million people contract malaria and the disease is endemic in 83 countries. In 2023, there were almost 600,000 malaria deaths in 83 countries, with African countries accounting for 94 per cent of deaths - 76 per cent of whom were children under five.\nEfforts to prevent the spread of the disease have been complicated by existing malaria interventions becoming less effective due to mosquitoes developing biological and behavioural resistance to insecticides and barrier-based controls. Malaria parasites have also developed resistance to treatment, highlighting the need for new tools to tackle the disease.\n\nDr Nikolai Windbichler and Professor George K. Christophides from the Transmission Zero team at Imperial.\n\nDr Windbichler, also from the Department of Life Sciences, said: “This is a new type of technology and a first-in-class solution to complement our malaria control tools.\n“We want to move at the right speed, not too fast so that everyone is on board and supportive of this new technology, but also with urgency, so that we treat malaria as the emergency that it is.”\n“We want to move at the right speed, not too fast so that everyone is on board and supportive of this new technology, but also with urgency, so that we treat malaria as the emergency that it is.” Dr Nikolai Windbichler Reader in Genetics at Imperial College London\nThe memo was launched at a special event at the Royal Geographical Society which brought together leading scientists, policymakers, and global health advocates to explore the potential of gene drive technology and other innovative solutions to achieve malaria eradication.\nThe event included addresses from the memo authors, alongside a panel discussion and Q&A session which included:\n\nProfessor Faith Osier, Imperial’s Chair in Malaria Immunology and Vaccinology\n\nDr Honorati Masanja, Chief Executive Director at Ifakara Health Institute, Tanzania\n\nMichael Santos, Senior Vice President for Science Partnerships and Chief Population Health Science Officer at the Foundation for the National Institutes of Health (FNIH)\n\nVictoria Fowler, Head of UK Advocacy at Malaria No More\n\nAfrica-owned technology\nTransmission Zero was established to develop new ways to halt the spread of malaria. The project is co-led by Imperial researchers in the UK and scientists at the Ifakara Health Institute and National Institute for Medical Research in Tanzania, in collaboration with the Swiss Tropical and Public Health Institute in Switzerland.\n\"There was strong agreement from the beginning that we had to capacitate local scientists and have buy-in from the government and local communities, which demonstrates the local ownership of the technology.” Dr Dickson Wilson Lwetoijera Principal Research Scientist at Ifakara Health Institute\nIt has been supported by a $15-million grant from the Gates Foundation to substantially expand its operations in the UK and in Tanzania.\nThis collaborative approach has led to the development of a mosquito transgenesis and infection facility at the Ifakara Health Institute with a biosafety level three laboratory and supporting plant room, bolstering genetic research in Tanzania and the wider continent.\nThe project has already achieved the first ever transgenic mosquito made on African soil, made by African scientists – a landmark achievement for research and technology development in Africa.\nDr Lwetoijera said: “There was strong agreement from the beginning that we had to capacitate local scientists and have buy-in from the government and local communities, which demonstrates the local ownership of the technology.”\nThe partnership between Imperial and institutes in Tanzania will help ensure that scientists in the region are trained in the latest genetic technologies and able to develop their own research agenda.\n\nTackling global development challenges\nSTEM for Development Impact Memos offer policymakers and stakeholders insight intoprojects supported by Imperial’s Global Development Hub and their real-world impact. The focus is on the scale-up potential of a science, technology, policy or health solution to development challenges.\nImperial'sGlobal Development Hubis a platform to promote and support Imperial’s sustainable development research, education and innovation. The Hub supports Imperial’s contribution to the United Nations Sustainable Agenda 2030, and our work more broadly with some of the most vulnerable and marginalised in societies where multiple global challenges are acutely concentrated.",
    "summary": {
      "en": "Researchers at Imperial College London are working on a new genetic technology that could help eliminate malaria by making mosquitoes resistant to the malaria parasite. The project, called Transmission Zero, is in collaboration with health institutes in Tanzania and aims to reduce malaria cases and deaths, particularly among children, who are the most affected.\n\nCurrently, malaria affects over 263 million people each year, with nearly 600,000 deaths in 2023, mostly in African countries. Existing malaria prevention methods are becoming less effective due to mosquito resistance to insecticides and treatments, highlighting the urgent need for new solutions.\n\nThe new technology focuses on creating genetically modified mosquitoes that cannot transmit malaria, and it includes a gene drive to ensure this trait is passed on to future generations. The researchers emphasize that their approach is equitable and accessible, aiming to involve local scientists and communities in the development process.\n\nThe initiative has received significant funding from the Gates Foundation to support its operations in both the UK and Tanzania. This collaboration has already led to the creation of Africa's first transgenic mosquito, marking a significant milestone for research in the region.\n\nOverall, the project seeks to address the malaria crisis with innovative solutions and foster local ownership of the technology, contributing to global efforts in tackling health challenges and promoting sustainable development.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "462735f2d1f833c4",
    "title": {
      "en": "I won't connect my dishwasher to your cloud",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.jeffgeerling.com/blog/2025/i-wont-connect-my-dishwasher-your-stupid-cloud",
    "score": 842,
    "by": "HieronymusBosch",
    "time": 1742836147,
    "content": "March 24, 2025\n\n            This weekend I had to buy a new dishwasher because our old GE died.\n\nI bought a Bosch 500 series because that's what Consumer Reports recommended, and more importantly, I could find one in stock.\n\nAfter my dad and I got it installed, I went to run a rinse cycle, only to find that that, along with features like delayed start and eco mode, require an app.\n\nNot only that, to use the app, you have to connect your dishwasher to WiFi, set up a cloud account in something called Home Connect, and then, and only then, can you start using all the features on the dishwasher.\n\nVideo\n\nThis blog post is a lightly-edited transcript of my latest YouTube video on Level 2 Jeff:\n\n.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }\n\nGE Dishwasher - Planned Obsolescence\n\nSo getting back first to that old GE dishwasher, it was, I don't know, I think that planned obsolescence is something that applies to many consumer products today.\n\nCompanies know if they design something to last only 5 or 10 years, that means in 5 or 10 years someone's going to have to buy a whole new one.\n\nAnd on my GE Amana dishwasher, it started having weird power issues, like the controls would just not light up unless I reset the circuit breaker for a few minutes. That started happening more often, and this past Saturday it just wouldn't come on no matter what, even after I tested and re-wired it all the way from the panel up to the dishwasher's internal power connector.\n\nSo it was dead.\n\nNext up, I looked at what it took to get a control board. Well... $299 for a control board that was 'special order' and might not even fix the problem? That's a non-starter for my $600, 8-year-old dishwasher.\n\nEven if I got it fixed, the front panel was starting to rust out at the hinge points (leaving some metal jaggies that my soon-to-be-crawling 6 month old could slice his fingers on), and other parts of the machine were showing signs of rust/potential future leaks...\n\nIt was Saturday night, and for a family of five, a dishwasher is kinda important. We don't have 1.5 hours every night to spend hand-washing dishes (not to mention the water bill!).\n\nSo I needed to get a new one, and it's really hard for me to schedule a few hours for my Dad and I to get it done in the middle of the week (plus that's multiple days without a dishwasher!).\n\nSo I did some research, and I found Bosch seemed to have the best bet for under $1,000, available locally on a Sunday.\n\nConsumer Reports, random Redditors, etc. seemed to have some praises for Bosch—on Reddit many also praised Miele, but I couldn't find any of those available locally. And Consumer Reports especially praised all the Bosch units, with them topping all their reliability and customer satisfaction charts!\n\nI remembered five or ten years ago, whenever I had bought my old GE, I remembered Bosch topped the charts too, but back then I settled for GE to save a few bucks...\n\nInstallation was (mostly) great!\n\nSo I spent a little more this time, hoping for a better experience. And installation was actually great—it was a lot easier to install the Bosch than it was that the GE.\n\nIt has a plastic base that slides better on the floor, and there's easier routing of the drain hose, inlet hose, and power wire that makes it less risky when you're pushing the thing into the blind cutout under the counter.\n\nThe one weird thing was that whoever like tightened the feet on the bottom at the factory must've used an impact driver or something because they were all practically embedded, and wouldn't budge.\n\nI was turning the little screw on the front that pushes the rear foot down through a little gearing, but the worm gear slipped out and kinda shoved the long rod that connects the front to the back out of place. I had to buy a 10mm hex socket to wiggle the foot loose enough the gearing would actually work.\n\nBut once that was done, the rest of the install was seamless. (Thanks especially to some help from my Dad).\n\nFirst use, encountering the Cloud requirement\n\nSo I turned it on, and immediately hated the new touch sensor stuff on it.\n\nThe old GE had buttons: you press them in, they click and you know that you pressed a button.\n\nThe touch sensor, you kind of touch it and the firmware—like this new dishwasher actually takes time to boot up! I had to reset it like three times and my wife meanwhile was like laughing at me like look at this guy who does tech stuff and he can't even figure out how to change the cycle on it.\n\nThat took about five minutes, sadly.\n\nBut eventually I pulled out the manual book because I was like... \"this is actually confusing.\"\n\nIt should be like: I touch the button and it changes to that mode! But that was not how it was working.\n\nI wanted to run just a rinse cycle to make sure the water would go in, the water would pump out through the sump, and everything worked post-install.\n\nBut I couldn't find a way to do a rinse cycle on the control panel.\n\nSo I looked in the manual and found this note:\n\nIt says options with an asterisk—including Rinse, Machine Care (self-cleaning), HalfLoad, Eco, and Delay start, are \"available through Home Connect app only and depending on your model.\"\n\nThe 500 series model I bought isn't premium enough to feature a 7-segment display like the $400-more-expensive 800 series, so these fancy modes are hidden behind an app and cloud service.\n\nI was like, \"Okay, I'll look up this app and see if I can use it over Bluetooth or locally or whatever.\"\n\nNope! To use the app, you have to connect your dishwasher to your Wi-Fi, which lets the dishwasher reach out on the internet to this Home Connect service.\n\nYou have to set up an account on Home Connect, set up the Home Connect app on your phone, and then you can control your dishwasher through the Internet to run a rinse cycle.\n\nThat doesn't make any sense to me.\n\nAn app? I mean, I can understand maybe adding some neat convenience features for those who want them. Like on my new fridge—which I chose not to connect to WiFI—it has an app that would allow me to monitor the inside temperature or look up service codes more easily. If I wanted those add-on features, which my old fridge didn't have, I could get them.\n\nBut requiring an app to access features that used to be controllable via buttons on the dishwasher itself—or are still if you pay $400 more for the fancy \"800\" model? That's no bueno.\n\nWhat can I do?\n\nWell, first of all, I could just not use those features. That's kind of annoying because I bought it with the assumption that I could run the cleaning cycle, that I could run the rinse cycle without having to have an app and Wi-Fi.\n\nAnother option is I could just connect it to my Wi-Fi, maybe on an IoT VLAN.\n\nBut it's not like a video doorbell, where an Internet connection adds on functionality, like being able to see who rang while you're on vacation, or storing security footage clips on the cloud so you have them available even after someone robs your house...\n\nBut a dishwasher... I'm not going to remote control my dishwasher and like, run an extra rinse cycle while I'm on a beach somewhere.\n\nI don't need Internet on my dishwasher.\n\nAnother third option is somebody has reverse engineered this protocol and built HCPY, a Home Connect Python library.\n\nBut here's the problem: I already spent like four hours getting this dishwasher installed in my kitchen. I don't want to spend another four hours configuring my own web UI for it—which still requires at least a one-time connection through Home Connect!—and maintaining that as a service on my local network, relying on an unauthorized third party library using reverse-engineering to get at the private dishwasher API!\n\nWhat can we do?\n\nI don't think we should let vendors get away with this stuff.\n\nFirst, it lets product designers get lazy.\n\nA feature like a little display, a little seven segment display that can show like two letters and a number of minutes remaining or something like that... How much does that cost?\n\nHow hard is it to integrate that into every model, even the cheap ones?\n\nA lot of cheap dishwashers actually have those things, but not Bosch!\n\nWith Bosch, you have to pay $400 for the privilege of a little display!\n\nSecond, this might be a little bit conspiracy theory or whatever, but it feels like it's part of planned obsolescence. Just like with the GE, where a lot of parts are designed to rust out after 5 or 10 years.\n\nIf you have a cloud app, that means there's a cloud service that has to be running. That costs money to maintain.\n\nAnd if there's no subscription fee right now, that means one of two things:\n\nThey could be selling our data already.\nAt some point, they'll either close the service because it's a cost center (so the rinse cycle and eco mode on all these dishwashers just goes \"POOF!\"), or they're going move to a subscription model.\n\nAll of a sudden, if you want to run the self-cleaning cycle, you better start paying five bucks to Bosch every month. Forever.\n\nThat's insane.\n\nThird, it's a security hole in your local network.\n\nBosch might be a little better than some no-name light bulb company making IoT light bulbs on Amazon, but only a little.\n\nI don't want to have Bosch having full internet access on my local network.\n\nTheir API would be able to talk back to my dishwasher, and the dishwasher—unless I put it on a VLAN, which 99% of consumers have no clue how to do that...\n\nThat's just something that shouldn't have to happen.\n\nThis is a dishwasher!\n\nI don't know.\n\nWhat should be done?\n\nWhen I posted on social media about this, a lot of people told me to return it.\n\nBut I spent four hours installing this thing built into my kitchen.\n\nI hooked it up to the water, it's running through cycles... it is working. I'll give them that. It does the normal stuff, but you know, there are some features that don't work without the app.\n\nAt a minimum, I think what Bosch should do is make it so that the dishwasher can be accessed locally with no requirement for a cloud account. (Really, it'd be even better to have all the functions accessible on the control panel!)\n\nAnyone building an IoT device, here is my consumer-first, e-waste-reduction maxim:\n\n  First local, then cloud.\n\nCloud should be an add-on.\n\nIt should be a convenience for people who don't know how to do things like connect to their dishwasher with an app locally.\n\nAnd it's not that hard.\n\nA little ESP32, a little $1 chip that you can put in there could do all this stuff locally with no cloud requirement at all.\n\nI think that there might be some quants or people who want to make a lot of money building all these cloud services.\n\nI don't think that it's people who are consumer-first and eco-conscious because if they were, they would give us control first and then add on 'nice' quality of life features through a cloud service.\n\nWith my Bosch 500 series dishwasher, I was excited after the easy install (besides those leveling feet), then was let down so hard once I found out I couldn't use all the features it came with.\n\n      Further reading\n\n          Using the Shelly Plug to monitor Starlink's power consumption\n          Raspberry Pi Zero 2 powers the Null 2 RetroPie gaming handheld\n          Build your own private WireGuard VPN with PiVPN\n\n              dishwasher\n              cloud\n              internet of things\n              iot\n              apps\n              home\n              kitchen\n              bosch\n              security\n\n    Add new comment\n\n      Comments\n\n     me – 1 day ago\n\n            Beyond ridiculous.\nWe have a Whirlpool dryer purchased in (cough) fall 1984.  Nice and simple and old school.  No computers.  No cloud.  JustPlainWorks.\nWe had a guy come out a number of years ago to do minor maintenance on the corresponding 20+ year Whirlpool washer and he said something along the lines of \"yup - love these models from back then. They never break.  They are repairable.  Do not ever replace that dryer. Just run it forever because that's how long it is going to last. So simple.\"\nContrast that with a former coworker who went with a super programmable LG (I think) who had nothing but problems with all the unnecessary electronics.\nAt some point you need to vote with your wallet.  Rip it out and take it back as 'does not work'.  Something that expensive shouldn't require anything at all other than power, water, and you pressing a button.\n\n      Reply\n\n    Jim – 15 hours ago\n\n              In reply to Beyond ridiculous.\nWe have a… by  me\n\n            My Whirlpool series 80 dryer was purchased in summer of 1986.  Replaced belt and rollers on it, that's it.  The corresponding washer just needed a screw tightening at one point when the agitator was slipping.\n\n      Reply\n\n    Billybillybilly – 1 day ago\n\n            You really appreciate a lot of the ingenuity that goes into engineer consistent design. One example I have that features a useful design that is local first then cloud is ac on cars.\nOn my car, if I hold my keyfob it rolls down all the windows to get the stale air out. It’s a great local feature.\nI was spending time with a friend at a park, and as were walking back the car they whip out their phone to turn on their cars ac remotely. A nice feature that doesn’t work when your car is under 60 ft of rock.\nCut to trying to explain how a car has cellular data and has to connect to a network to be able to remote control it.\nI would like to be able to crank my ac remotely but for now local control best suits my desires.\n\n      Reply\n\n    Jeff Geerling – 1 day ago\n\n            I found a page on Home Connect on the 'Consumer Action Taskforce' wiki, and it mentions the cloud requirement and 'no cloud' offline mode (which requires a cloud account and WiFi connection to enable, lol), but that page didn't mention the fact some devices hide features behind Home Connect, that you can't use if you don't use Home Connect.\n      Reply\n\n    Bill – 1 day ago\n\n            Bosch has beaten you.  You paid your money, found a major defect (and it is a defect), and kept it anyway.  They win, you lose.  Zero deterrence.\n\n      Reply\n\n    KaBosch – 1 day ago\n\n              In reply to Bosch has beaten you.  You… by Bill\n\n            Jeff should return it, it is *only* four hours. The block is mental, not physical. The product is defective. I won't be be buying a Bosch, too bad because I had one in a rental years ago and it was amazing. The MBAs who made this decision should own their mistakes.\n\n      Reply\n\n    Ryan of Drupal – 1 day ago\n\n              In reply to Jeff should return it, it is… by KaBosch\n\n            Time is money and four hours is a lot of opportunity cost that he could have spent doing multitudes of income generating time. I would be willing to wager that Jeff's time is honestly worth more than 100$ an hour,  so taking it back *is* a loss.\n\n      Reply\n\n    Tools4Conviality – 1 day ago\n\n              In reply to Time is money and four hours… by Ryan of Drupal\n\n            I don't think anyone is so productive and amazing they can justify any 4 hours will bring them that, it's just not how the world work. Plus the vindication and the message it bring directly to the manufacturer is worth in itself. Not everything has to return immediate value for the self, it's called living in a society.\n\n      Reply\n\n    Steve – 1 day ago\n\n              In reply to Bosch has beaten you.  You… by Bill\n\n            Agreed, they just want money and they got it.\n\n      Reply\n\n    David – 17 hours ago\n\n              In reply to Bosch has beaten you.  You… by Bill\n\n            Except, now all of us reading here are going to second guess buying a Bosch dishwasher (or other appliance for that matter).\nI have a Bosch I installed 10 years ago and it's amazing. It is *extremely* rare for me to use anything but the normal cycle.  That said, I would be annoyed at having to use an app to control the normal functionality of the dishwasher, and based on this article I will be very cautious about replacing that dishwasher when the time comes.\n\n      Reply\n\n    Matt Farr – 14 hours ago\n\n              In reply to Except, now all of us… by David\n\n            We have a Bosch 800 that we bought just post-COVID because the one in our house when we bought it literally would not wash dishes.  It is a wonderful dishwasher.  There is no wireless connectivity built in, and all  the functions work from the door-edge keypad.  Here in Florida, the nine-minute rinse cycle is very useful in a two-person household where it takes a few days to fill the dishwasher and the funk comes on really fast if you don't run a cleaning cycle every day.\n\n      Reply\n\n    Dick King – 13 hours ago\n\n              In reply to Except, now all of us… by David\n\n            Almost everyone who owns a dishwasher uses the rinse cycle often.\n-dk\n\n      Reply\n\n    marimba – 1 day ago\n\n            Please, return it! It‘s the only way they learn.\n\n      Reply\n\n    morgs – 1 day ago\n\n              In reply to Please, return it! It‘s the… by marimba\n\n            Sadly, only one local salesperson will \"learn\".\n\n      Reply\n\n    Jo – 19 hours ago\n\n              In reply to Sadly, only one local… by morgs\n\n            Then it has to roll uphill.\n\n      Reply\n\n    Some dude from… – 1 day ago\n\n            Your government ordering strikes on other nations over signal, and you are worried about your dishwasher data stored in eagle data center somewhere in Swiss alps.\n\n      Reply\n\n    another random dude  – 1 day ago\n\n              In reply to Your government ordering… by Some dude from…\n\n            Hahaha dude you should not be honest online.\n\n      Reply\n\n    Another dude from... – 1 day ago\n\n              In reply to Your government ordering… by Some dude from…\n\n            Yea, he should be worried about those things of which he has no control over, versus this, which he does and impacts him on a personal level.\n\n      Reply\n\n    Not random – 1 day ago\n\n              In reply to Your government ordering… by Some dude from…\n\n            It's all billionaire oligarchs doing billionaire oligarch shit. Their power comes from exploiting us and our willingness to let them do it.\nIt starts with forcing us to be renters. It ends with bombs dropping on innocent children held hostage by other oligarchs as human shields so they can launch bombs at innocent merchant marines exploited by still other oligarchs. It's all about control at every level.\nBosch and GE can get bent.\n\n      Reply\n\n    Aperiodic – 23 hours ago\n\n              In reply to It's all billionaire… by Not random\n\n            You're not wrong.\n\n      Reply\n\n    Seriously? – 16 hours ago\n\n              In reply to Your government ordering… by Some dude from…\n\n            What exactly are we supposed to do? This kind of argument means that you should never be concerned with anything except the most dire problems in the world, and never on the maintenance of your own life. Pure utilitarianism doesn't work, especially in ultra complex societies that are also ultra stratified.\n\n      Reply\n\n    David – 1 day ago\n\n            There's also an option to use other version of hcpy with HomeAssistant MQTT support which setup should take quite a bit less than a few hours https://github.com/hcpy2-0/hcpy/wiki/HomeAssistant-Addon but fully agreed that this should not be the standard... They've probably went the route they did so not to have anything Bluetooth related as that might look cheap and/or might introduce other vulnerabilities/bugs in the communication stack.\n\n      Reply\n\n    billybillybilly – 1 day ago\n\n              In reply to There's also an option to… by David\n\n            Does Bluetooth look cheap?\nI honestly find Bluetooth more convenient than the other options most devices provide. I wonder if the public feels that way as well?\n\n      Reply\n\n    Aperiodic – 23 hours ago\n\n              In reply to There's also an option to… by David\n\n            Well, I'd far rather have a vulnerability in the Bluetooth stack of my dishwasher, since that protocol is really only locally exploitable and the product isn't mobile, than in the WiFi stack reaching up to a cloud-connected server.  In the best case, once compromised, it ends up getting used for some ransomware hackers' DDoS swarm.\n\n      Reply\n\n    Dmitry Samsonov – 1 day ago\n\n            Now imagine this scenario: A global war breaks out, and the cloud service controlling your dishwasher activates its hidden military functions... And it turns out there are several of them—backdoors in the chips from China, exploits in the application software from the EU, and vulnerabilities in open-source libraries, inserted by Russia and North Korea... And you could dodge all this nightmare by just washing your dishes the old-fashioned way—supposedly better for your health too!\n\n      Reply\n\n    Seb – 1 day ago\n\n            Lesson learned from your experience: check out appliance models, and read their manuals before deciding what to get.\n\n      Reply\n\n    pg – 1 day ago\n\n            Don't get your hopes too high.\nBOSCH's electrical equipment doesn't belong to BOSCH anymore, but is part of the large Siemens group for many years. Siemens in germany is known for its consumer products to be very expensive (cause of their very good reputation they may earned a hundred years ago), full of mostly useless features,  complicated in handling, very low on product quality, high mainting aand repair rates, and outstanding repair cost.\nIn germany they have a joke:\"Does it have to work, or can it be by Siemens?\"\nI had the full parade of Siemens/Bosch-be-part-of-Siemens consumer electrical, and electronical products over the last twentyfive years: all expensive crap. I'm  through with dishwashers, vacuum cleaners, refrigerators, stoves, and several other devices, all Siemens/Bosch, all expensive, and crap!\nBosch and Siemens brands are on my personal black list of never buy again, ever.\nSorry, to tell you that, but you propably be better served with some \"mexican\" dishwasher for half the price - it will at least does its job for maybe 6...8 years without repairs or 'funny behaviour'.\nYou'll see when you Bosch maschine gets broken within the next two and a half years, and face the repair cost half the price of a new machine, and then it will be broken half a year later again. We decided to change your dishwasher to another brand after we paid over 150% of the purchasing price for repairs only within 4 years, not to mention hundreds of situations I needed all my self control not to rip this crap out of the kitchen and throw it out of the window.\n\n      Reply\n\n    Thomas – 1 day ago\n\n              In reply to Don't get your hopes too… by pg\n\n            You got that the wrong way around. BSH GmbH (formerly known as Bosch Siemens Hausgeräte) is a 100% subsidiary of Bosch. They make all the home appliances. Siemens pulled out of that joint venture a quite few years ago. Bosch just uses the Siemens brand at this point. The devices are identical.\n\n      Reply\n\n    Heinrich – 1 hour ago\n\n              In reply to You got that the wrong way… by Thomas\n\n            Let's all be honest here, your dishwasher would prefer to speak to you in German anyway.  And if you don't rinse your dishes before you put them in the dishwasher, it'll send you to the camps!\n\n      Reply\n\n    mb – 1 day ago\n\n              In reply to Don't get your hopes too… by pg\n\n            This is not true. BSH was founded as a 50:50 joint venture of Bosch and Siemens, and has been fully taken over by Bosch some years ago. They still have a licence for the Siemens brand though.\n\n      Reply\n\n    Anonym :) – 1 day ago\n\n              In reply to Don't get your hopes too… by pg\n\n            Small correction. BSH Hausgeräte GmbH (BSH Home Appliances Company) maker of all Bosch and Siemens house appliances is a subsidiary of the Robert Bosch GmbH (aka Bosch).\nhttps://en.wikipedia.org/wiki/BSH_Hausger%C3%A4te\n\n      Reply\n\n    Chris – 1 day ago\n\n              In reply to Don't get your hopes too… by pg\n\n            It's actually the other way around: BSH (Bosch-Siemens-Hausgeräte) is owned by Bosch, not Siemens. https://de.wikipedia.org/wiki/BSH_Hausger%C3%A4te\n\n      Reply\n\n    MR – 1 day ago\n\n            We got the Bosch model with the LCD but still have to use the app for Machine Care, Rinse, etc. Some of the IOT features are pretty good honestly, reminders to fill the rinse aid or run a machine care cycle are useful, but I'd much prefer to have no wifi at all! Unfortunately most of the new applicances have wifi or they are low grade base models with no features, and it takes so much work to find a decent used applicance it's difficult to find a proper solution.\n\n      Reply\n\n    Leo – 1 day ago\n\n            Where I rent, there's a V-Zug dishwasher. Very common over here in Switzerland. It's got exactly what you're asking, local connectivity via Bluetooth and cloud stuff over wifi if you desire. None of it mandatory. However it does come with a different price tag.\nWhat you might be experiencing is the old printer and cartridge business model. Sell a product below cost and make it back later via subscriptions / data brokerage.\n\n      Reply\n\n    Matt – 1 day ago\n\n              In reply to Where I rent, there's a V… by Leo\n\n            Please don't recommend V-Zug as a viable alternative to this. V-Zug has features that your device's hardware is capable of doing behind App Store In-App Purchases!... Let me repat that: ONCE YOU HAVE BOUGHT A NEWERV V-ZUG DEVICE YOU MAY HAVE TO UNLOCK FEATURES BY PAYING 400$ VIA INAPP PURCHASE!\nI am never buying V-Zug ever again\n\n      Reply\n\n    billybillybilly – 1 day ago\n\n              In reply to Where I rent, there's a V… by Leo\n\n            Yo, you guys have heat pump appliances.\nWhat’s your power costs look like over there.\nI feel bad because there’s always a tonne of flares burning where I live, lighting up the night sky a hellish red every night. It’s tough because power is so cheap here but so bad for the environment. Best I can do is run washing loads on tap temperature water.\n\n      Reply\n\n    freD – 1 day ago\n\n            I don't have a dish washer with my family of 5.\nCorrect water use is not bigger when done (right) manually, it is ads bullshit.\nBut excess of chimical dish washing product is way lesser, so water is less polluted with manual cleaning, because you actually washing with your muscles.(animal grease is more annoying than vegs grease).\nIt don't takes 1,5h to complete dish washing... it takes the time to listen a small yt vid or a peace of my current audiobook...\nI'm the dish washer,  no programmed obsolescence,  no wifi, not to much hard washing soap, no back pain loading/unloadind the washer, no electrical bill and recreative or imaginative time while  doing simple manual task.\nWill not say the same for laundry!\nFreD.\n\n      Reply\n\n    Aperiodic – 23 hours ago\n\n              In reply to I don't have a dish washer… by freD\n\n            This is the best solution.  Of course, from the perspective of the rare person who chooses to live on a sailboat, this is also the only available solution.  :-)\n\n      Reply\n\n    Piotr – 1 day ago\n\n            Hi Jeff,\nDid you notice that panel was at the edge of dishwasher door?\nThis is a black flag:\n- place where touch surface is exposed to highest temperature from the steam leaving insides when you open the door\n- cheap and easy to break surface for buttons\n- when door is open, you will hit and kick the edge occasionally - in the place where your control panel is\nI'm sure you will find plenty of parts exposed to stress from moving or high temperatures made from plastic in the new model.\nElectronic parts with network connection requirements is just cherry on the top.\nBest,\nPiotr\n\n      Reply\n\n    Chris – 1 day ago\n\n            We recently got this same dishwasher when our old one died.  There are a lot of little things that annoy me about it, but for whatever reason this is the biggest:\n>2 hr\n<2 hr\n<1 hr\nclean\nI can't explain it well but I find that to be visually appalling UI design.\n\n      Reply\n\n    challengerderp… – 1 day ago\n\n            Hi Jeff,\nI have the same pet peeve as you, and am of the strong opinion that features that do not require a cloud, for instance, to run a certain wash cycle, should well, not require, a cloud account.\nThe inconvenience of setting and maintaining a cloud account with Bosch aside, without proper safeguards for consumers in the sense of not unnecessarily opening up surfaces for cybersecurity attacks (e.g., installing an additional app on your mobile device), companies adopt the stance they can push haphazardly clobbered-together technical solution to customers. (If you, the reader are wondering why the instanace talked about in Jeff's post is indeed \"haphazardly clobbered-together\", the plain simple reasons are mentioned above: due to un-necessity, plus presents a potential security vulnerability).\nA little about Bosch to provide context. Bosch's products are sold at a clear premium where I'm based compared to its competitors like whirlpool. And Bosch products are historically reputed to be sturdy, and is synonymous with quality. HOWEVER, when I recently decided on a dishwasher to purchase, despite being able to comfortably afford a Bosch (or even several of them), I decided to go with Whirlpool because of the types of wash cycles making a lot more sense in this modern day and age. Bosch's dishwashers have antiquated wash cycle types, stuck in an era of traditional fixed meals: breakfast, lunch, tea, dinner, supper, which are now largely gone in modern households where members eat at different times, and snack in between large meals. Whirlpool's dishwashers have  programmed the wash cycles that cater to modern flexibilities while Bosch's is still stuck in the 1990s.\n- ChallengerDerp (on HN)\n\n      Reply\n\n    Hi – 1 day ago\n\n            It looks like the Bosch 300 series ADA-compliant model would be a better choice. It gets rid of the crappy touch sensitive controls in favor of actual physical buttons and all the cycles are able to be selected including the rinse and machine care cycles. It still has home connect which allows the remote start and monitoring features but in this case it feels like an add-on feature which isn't hiding other features behind the cloud.\n\n      Reply\n\n    Brad – 16 hours ago\n\n              In reply to It looks like the Bosch 300… by Hi\n\n            Return, exchange for the 300 series.\nCheaper! No IoT! Exact same mechanism, but more reliable controls!\nTake the savings as payment for your grief.\n\n      Reply\n\n    Jeff Geerling – 15 hours ago\n\n              In reply to Return, exchange for the 300… by Brad\n\n            Looks like the newest 300 series model makes the same fatal mistake as the 500 (and 800), at least according to my quick glance at the SHE53C85N model on Home Depot. It is also missing a few buttons from past generations, and requires the app to do things like delay start or eco cycle.\n      Reply\n\n    lolski – 1 day ago\n\n            The enshittification phenomenon continues across all aspects of our electronic life\n\n      Reply\n\n    Chris – 1 day ago\n\n            I worked at Bosch for a few years and our job back then (~2019) was to explore IoT services for existing and future appliances and tools. The great challenge of hardware or component providers like Bosch is that it's becoming substantially more difficult to make money through hardware only. China produces cheaper and with acceptable quality, putting a lot of pressure on companies like Bosch. So, one path to explore is monetizing data. Unfortunately for Bosch, few IoT business models make sense for fridges, coffee machines, hand drills, and dishwashers. And that's what you are experiencing with your machine from Bosch: the rather clumsy attempt of a hardware and engineering company to find new data-centric business models. To my (probably outdated) knowledge, it was really much more about getting a foot into the door of IoT business models than anything else.\n\n      Reply\n\n    herb – 1 day ago\n\n            I wasn't aware that Bosch is that advanced. ;) The first device I bought, which wanted me to join a cloud was iRobot - many years ago. No updates possible without. Do you know any Xiaomi device running without calling home?\nIf I could wish, I would like to have a law, that every electronic device should get a standardized local API. Just necessary to run curl and jq to use it.\n\n      Reply\n\n    Steven Greer – 20 hours ago\n\n            You’re much nicer than me.  I got one of those Bosch 500s delivered from Costco and returned it.  We want to start our dishwasher after we go to bed, and to do that, every day you need to go to the app and set the timer.\nJust awful.  And the app has ads.  Yuck.\nI had a Bosch for 12 years.  Now we have a Kitchenaid .  It’s fine.  No app.\n\n      Reply\n\n    Brian – 20 hours ago\n\n            I can understand the outrage over cloud based features...yeah it is not something anyone directly asked for except in some tiny cases, but it certainly has the potential to be abused and monetized.  On the other hand I don't see a button for a rinse cycle or timed delay.  When I replaced by GE dishwasher last year with a KitchenAid I looked at everything on the market.  It wasn't hard to figure out that certain washers either didn't have the features I wanted or they were locked away where I couldn't easily get to them.  What if this thing didn't even do a rinse cycle or have timed delay at all?  Yeah the Bosch washers get great reviews, but I'm not getting my phone just so I can set up delay start.\n\n      Reply\n\n    MikeD – 19 hours ago\n\n            We bought the Cafe dishwasher we have now ahead of our kitchen remodel because it was on closeout, and the replacement only offered \"bottle wash\" (a feature we use a lot) via the app.\nIt needs to be a carrot not a stick.  The kitchen remodel is done, and our new range (also Cafe) I did hook up to the Internet, mainly so that it'll automatically sync its clock when daylight savings rolls around.\nIf we had gotten the more expensive over-the-range microwave, the range even would have sync'ed its clock with the microwave via Bluetooth, and do things like automatically turn on the under-microwave light when you turn on a burner on the range, and turn the light off X minutes after you turn the burner off.  Granted, that's not something that requires Internet connectivity, but it's an intelligent way to add wireless connectivity to appliances.\nI'm not against Internet connectivity in appliances.  I wish our washer and dryer had it, so we could get a notification on our phone telling us when their cycles were finished instead of having to get up and go check (first world problems I know).  And I do like the ability to start pre-heating the oven when we're on our way home (acknowledging all the safety issues involved with that).\nI did ultimately connect the dishwasher to the Internet.  I'm not sure what the utility of knowing when it's finished washing the dishes is.\nI wish the dishwasher had the option to run a diagnostic so we could let a repair person know over the phone if we ever have the problem, but I don't see that.  And I do hope that I can grant access to such a person so they can run diagnostics over the Internet when that's necessary (with all the necessary security precautions), but I doubt it does.\n\n      Reply\n\n    Kevin – 19 hours ago\n\n            If you care to see the eventual outcome of this insipid enshittification, read Cory Doctrow's 'Unauthorized Bread'\n\n      Reply\n\n    David  – 17 hours ago\n\n            Seems strange. In December we installed a Bosch 800 series dishwasher. No internet connection was needed and it works fine. I  subsequently did connect it to the internet but that was done just for kicks, not necessity. The installer from Best Buy also made no mention of needing internet connection to work. He also ran a quick cycle to show us how to operate the machine. No internet connection needed.\n\n      Reply\n\n    Jeff Geerling – 15 hours ago\n\n              In reply to Seems strange. In December… by David\n\n            It looks like the latest 800 series model (SHP78CM6N) is similar in that there's no button for delay start, and it shows 'HomeConnect' on the panel. It seems like they may have refreshed the whole lineup at this point. Apparently older 500 series models had buttons for everything, but they decided buttons are so passe and they wanted to dedicate an entire button to 'remote start' instead of 'delay start', meaning you cannot set a delay on the unit, only on the app, ha!\n      Reply\n\n    Jeff Geerling – 15 hours ago\n\n              In reply to Seems strange. In December… by David\n\n            It looks like the latest 800 series model (SHP78CM6N) is similar in that there's no button for delay start, and it shows 'HomeConnect' on the panel. It seems like they may have refreshed the whole lineup at this point. Apparently older 500 series models had buttons for everything, but they decided buttons are so passe and they wanted to dedicate an entire button to 'remote start' instead of 'delay start', meaning you cannot set a delay on the unit, only on the app, ha!\n      Reply\n\n    Anonymous – 16 hours ago\n\n            My Mum was a loyal Bosch customer for more than 30 years until this nonsense. A 75yo woman who has no interest in this nonsense and is now telling all her friends about it in Greenwich CT. Miele is where she's at now, and she always has 2 dishwashers. Seriously stupid.\n\n      Reply\n\n    EdgeWiseInAnnArbor – 14 hours ago\n\n            First they Demon-haunted the Inkjet printers,\nAnd I did not speak out\nBecause I don't really print much\nThen they made TVs internet enabled\nAnd I did not speak out\nBecause I really liked streaming movies\nThen they brought smart speakers\nAnd I did not speak out\nBecause it played tunes and drop shipped\nThen they had Connected Appliances\nAnd I did not speak out\nBecause it was already in my kitchen\nThen they Software Defined my Vehicle\nAnd it got bricked in my driveway\nSo That's Why I'm Late\n\n      Reply\n\n    Random interne… – 14 hours ago\n\n  Miele, but I couldn't find any of those available locally.\n\nMistake.  Miele ain't as good as they used to be, but even the cheapest will work for a decade, and work better than the Bosch.  Probably cost twice as much, but clean dishes.\n      Reply\n\n    Jerome Demers – 14 hours ago\n\n            have you tried to repair the old one?\n\n      Reply\n\n    I[di]OT – 7 hours ago\n\n            All these companies should be buying our phones, tablets and internet service for us, with these unreasonable resource demands that they're making.\nI installed the Bosch app also to get the rinse option and it's nearly useless.\nThey think they've been clever by letting you set power tarriff times for automated running but you can set only two tiers. I have three tiers that differ by quite a lot and their software is too dumb for this even though it's an absolutely trivial problem to solve, from a computation perspective. So much for the glories of German engineering.\n\n      Reply\n\n    Mike – 5 hours ago\n\n            If its such a hassle to replace appliances, why doesn't it motivate anyone to be proactive about it.\nFirst thing I did when buying my new washer and dryer was making sure they had no app/internet/cloud requirements.\nBut you can always go shopping and tell them you have no internet.\n\n      Reply\n\n    Josh – 4 hours ago\n\n            You think it's bad now? Just wait until crappy 'AI' gets integrated into all these appliances none of us wanted nor asked for, to be 'smart' to begin with?\n\n      Reply\n\n    Gen – 3 hours ago\n\n            Absolutely astounding. I'm glad more and more awareness is happening. As a software engineer I refuse to buy appliances that substitute basic features we've figured out already as a society. They just want to make more money. VOTE WITH YOUR WALLET.\n\n      Reply\n\n    Ralf Bierig – 2 hours ago\n\n            Thank you. I will use this article as an example for my UX class. Best, Ralf\n\n      Reply\n\n    Michl – 1 hour ago\n\n            This is beyond stupid. I would only buy a new dishwasher if it has a display and buttons for all features. \"Smart\" is fine, as long as it's optional. I would also strongly prefer local APIs without a requirement for Internet connectivity and accounts though. Hopefully someday we will get this with Matter over Thread 🤞.\nI checked the current Bosch dishwasher product line-up in the EU. All the dishwashers I could find, even the cheaper ones, had a display and all the buttons you would expect (including delay). So maybe this is something that US consumers like and demand?\n\n      Reply\n\n    Val – 1 hour ago\n\n            I had somewhat similar experience with the vacuum cleaner robot.\nI've checked the manual before buying (found it on the net) — it says you need a Wi-Fi to configure it. OK, I thought, it says just Wi-Fi, the network, right?\nAsked the sales team of the manufacturer before buying: \"Do I need only Wi-Fi to configure the vacuum? Not the Internet, but just a Wi-Fi network, right?\", \"Yes, that's right, only Wi-Fi\".\nOf course it required the internet connection inside the Wi-Fi. It doesn't work in LAN, it works over the Internet server.\n\n      Reply\n\n    Val – 1 hour ago\n\n            I'm thinking on making a \"consumer properties\" wiki, similar to what Rossman with \"Consumer Action Taskforce\" does.\n\n      Reply\n\n    Luis M – 58 min ago\n\n            Amazing to read this. A similar experience happened to me just this month but with an LG dishwasher. The port behind the door that connects to the main power relay was making weird sounds and burning. This is the 2nd time in 8 years that happens. So we opted to buy a new dishwasher.\nI saw that GEs were very similar in built to LG, so we didn’t get the GE as I imagine it would have the same problem. We opted for a newer LG which I hope won’t have this problem down the line.\nMy surprise was to see that LG can be connected to the cloud. All the basic functionalities are there, but the cloud connection provides extra cycles and other extra functionality. I didn’t think much of it and just went with the program. Now after reading this, I wonder if plan obsolesce is the main reason for those features too.\n\n      Reply",
    "summary": {
      "en": "Over the weekend, the author had to buy a new dishwasher after their old GE model broke down. They chose a Bosch 500 series based on Consumer Reports' recommendation and availability. However, upon installation, they discovered that many features, including the rinse cycle, require a smartphone app and WiFi connection to operate.\n\nThe author reflects on planned obsolescence, noting that many appliances are designed to last only a limited time, prompting consumers to buy new ones. Their old GE dishwasher had recurring electrical issues, and fixing it would have been costly compared to buying a new unit. They appreciated the Bosch's easier installation process but were frustrated by the reliance on touch sensors and the app for basic functions.\n\nThe app requires creating an account and connecting to the internet, which the author finds unnecessary for a dishwasher. They argue that basic features should be accessible directly without requiring an app. The author fears this trend could lead to future subscription fees or data privacy issues, and they express concerns about security vulnerabilities in connecting appliances to the internet.\n\nDespite their frustrations, the author is hesitant to return the Bosch appliance after spending significant time installing it. They suggest that manufacturers should prioritize local control over cloud dependence for appliances. The post concludes with a call for consumers to be aware of these issues and consider alternatives when purchasing appliances.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "b971314826d6a318",
    "title": {
      "en": "Critical Unauthenticated Remote Code Execution Vulnerabilities in Ingress NGINX",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.wiz.io/blog/ingress-nginx-kubernetes-vulnerabilities",
    "score": 57,
    "by": "puppion",
    "time": 1742897537,
    "content": "Wiz Research discovered CVE-2025-1097, CVE-2025-1098, CVE-2025-24514 and CVE-2025-1974, a series of unauthenticated Remote Code Execution vulnerabilities in Ingress NGINX Controller for Kubernetes dubbed #IngressNightmare. Exploitation of these vulnerabilities leads to unauthorized access to all secrets stored across all namespaces in the Kubernetes cluster by attackers, which can result in cluster takeover.This attack vector has been assigned a CVSS v3.1 base score of 9.8.In this blog post, we share key learnings from our discovery of IngressNightmare, affecting the admission controller component of Ingress NGINX Controller for Kubernetes. Based on our analysis, about 43% of cloud environments are vulnerable to these vulnerabilities, with our research uncovering over 6,500 clusters, including Fortune 500 companies, that publicly expose vulnerable Kubernetes ingress controllers’ admission controllers to the public internet—putting them at immediate critical risk.We recommend patching as soon as possible. This blog post details the technical elements of the vulnerability and contains mitigation and detection guidance for defenders.We would like to thank the Ingress-NGINX maintainers, particularly Marco Ebert, for their help in addressing the IngressNightmare vulnerabilities. Our team worked closely with the Kubernetes maintainers and security teams to ensure this attack surface was fully eliminated before public disclosure.Kubernetes’s blog can be found here, and Amazon and Google Cloud have also published their own advisories.What is Ingress NGINX Controller for Kubernetes?Ingress NGINX Controller is one of the most popular ingress controllers available for Kubernetes, and a core Kubernetes project, with over 18,000 stars on GitHub. Using Ingress-NGINX is one of the most common methods for exposing Kubernetes applications externally. As an ingress controller, its job is to accept incoming traffic and route it to the relevant Kubernetes Services, which in turn forwards the traffic to the appropriate Pods, based on a set of rules. Specifically, Ingress NGINX Controller is based on the popular NGINX reverse proxy. <picture><source srcSet=\"https://www.datocms-assets.com/75231/1742580952-screenshot-2025-03-21-at-11-15-35-am.png?dpr=0.25&amp;fm=webp 148w,https://www.datocms-assets.com/75231/1742580952-screenshot-2025-03-21-at-11-15-35-am.png?dpr=0.5&amp;fm=webp 297w,https://www.datocms-assets.com/75231/1742580952-screenshot-2025-03-21-at-11-15-35-am.png?dpr=0.75&amp;fm=webp 446w,https://www.datocms-assets.com/75231/1742580952-screenshot-2025-03-21-at-11-15-35-am.png?fm=webp 595w\" sizes=\"(max-width: 595px) 100vw, 595px\" type=\"image/webp\"/><source srcSet=\"https://www.datocms-assets.com/75231/1742580952-screenshot-2025-03-21-at-11-15-35-am.png?dpr=0.25&amp;fm=webp 148w,https://www.datocms-assets.com/75231/1742580952-screenshot-2025-03-21-at-11-15-35-am.png?dpr=0.5&amp;fm=webp 297w,https://www.datocms-assets.com/75231/1742580952-screenshot-2025-03-21-at-11-15-35-am.png?dpr=0.75&amp;fm=webp 446w,https://www.datocms-assets.com/75231/1742580952-screenshot-2025-03-21-at-11-15-35-am.png?fm=webp 595w\" sizes=\"(max-width: 595px) 100vw, 595px\"/><img src=\"https://www.datocms-assets.com/75231/1742580952-screenshot-2025-03-21-at-11-15-35-am.png?fm=webp\" alt=\"\" style=\"position:absolute;left:0;top:0;width:100%;height:100%;max-width:none;max-height:none\" loading=\"lazy\" referrerPolicy=\"no-referrer-when-downgrade\"/></picture>Figure: Ingress prerequisites from the Kubernetes documentation.Ingress-NGINX is explicitly highlighted in the Kubernetes documentation as an example Ingress controller that fulfills the prerequisite for using Ingress in Kubernetes. Our research show that over 41% of internet-facing clusters are running Ingress-NGINX.The VulnerabilityIngress NGINX deploys an admission controller within its pod, designed to validate incoming ingress objects before they are deployed. By default, admission controllers are accessible over the network without authentication, making them a highly appealing attack vector.When the Ingress-NGINX admission controller processes an incoming ingress object, it constructs an NGINX configuration from it and then validates it using the NGINX binary. Our team found a vulnerability in this phase that allows injecting an arbitrary NGINX configuration remotely, by sending a malicious ingress object directly to the admission controller through the network.During the configuration validation phase, the injected NGINX configuration causes the NGINX validator to execute code, allowing remote code execution (RCE) on the Ingress NGINX Controller’s pod.The admission controller’s elevated privileges and unrestricted network accessibility create a critical escalation path. Exploiting this flaw allows an attacker to execute arbitrary code and access all cluster secrets across namespaces, that could lead to complete cluster takeover.<picture><source srcSet=\"https://www.datocms-assets.com/75231/1742579110-ingress_diagram.png?dpr=0.25&amp;fm=webp 480w,https://www.datocms-assets.com/75231/1742579110-ingress_diagram.png?dpr=0.5&amp;fm=webp 960w,https://www.datocms-assets.com/75231/1742579110-ingress_diagram.png?dpr=0.75&amp;fm=webp 1440w,https://www.datocms-assets.com/75231/1742579110-ingress_diagram.png?fm=webp 1920w\" sizes=\"(max-width: 1920px) 100vw, 1920px\" type=\"image/webp\"/><source srcSet=\"https://www.datocms-assets.com/75231/1742579110-ingress_diagram.png?dpr=0.25&amp;fm=webp 480w,https://www.datocms-assets.com/75231/1742579110-ingress_diagram.png?dpr=0.5&amp;fm=webp 960w,https://www.datocms-assets.com/75231/1742579110-ingress_diagram.png?dpr=0.75&amp;fm=webp 1440w,https://www.datocms-assets.com/75231/1742579110-ingress_diagram.png?fm=webp 1920w\" sizes=\"(max-width: 1920px) 100vw, 1920px\"/><img src=\"https://www.datocms-assets.com/75231/1742579110-ingress_diagram.png?fm=webp\" alt=\"\" style=\"position:absolute;left:0;top:0;width:100%;height:100%;max-width:none;max-height:none\" loading=\"lazy\" referrerPolicy=\"no-referrer-when-downgrade\"/></picture>Figure: IngressNightmare attack vectorsTo be clear, gaining initial access to a cluster's pod network is not as difficult as one might think - containerization on its own is not a strong security boundary, and many applications running on K8s are susceptible to container escape, as we have repeatedly demonstrated in our research of cloud and SaaS applications over the past few years. Additionally, these vulnerabilities pair very well with SSRF vulnerabilities, which are an arguably common occurrence in web applications.Mitigation & DetectionFirst, determine if your clusters are using ingress-nginx. In most cases, you can check this by running kubectl get pods --all-namespaces --selector app.kubernetes.io/name=ingress-nginx with (at least) cluster-scoped read-only permissions. This vulnerability is fixed in Ingress NGINX Controller version 1.12.1 and 1.11.5. We strongly recommend that cluster admins:Update to the latest version of Ingress NGINX Controller.Ensure the admission webhook endpoint is not exposed externally.You can use this Nuclei template to check for exposed Ingress-NGINX admission controllers.If you can’t upgrade immediately, consider one of these mitigations:Enforce strict network policies so only the Kubernetes API Server can access the admission controller.Temporarily disable the admission controller component of Ingress-NGINX if you cannot upgrade right away.If you have installed ingress-nginx using Helm, reinstall with controller.admissionWebhooks.enabled=false.If you have installed ingress-nginx manually, delete the ValidatingWebhookConfiguration called ingress-nginx-admission and remove the --validating-webhook argument from the ingress-nginx-controller container’s Deployment or DaemonSet.Remember to re-enable the Validating Admission Controller after you upgrade, because it provides important safeguards for your Ingress configurations.Wiz customers can use the pre-built query and advisory in the Wiz Threat Center. Wiz also validates for exposed admission controllers using the Wiz Dynamic Scanner. Finally, the Wiz Runtime Sensor detects zero-day vulnerabilities like IngressNightmare, by continuously monitoring ingress traffic, capturing malicious admission review requests in real-time, and flagging anomalous library loads to prevent similar attacks.CVE-2025-24513 is different in nature from the other vulnerabilities in the IngressNightmare chain, as it does not lead to RCE. How did we discover IngressNightmare?Research MotivationKubernetes Admission Controllers present an interesting and often overlooked attack surface in a Kubernetes environment. They are triggered by the Kubernetes API server to review and potentially modify or block requests (AdmissionReview) before they are processed, and they often run with relatively high privileges within the cluster. Admission Controllers frequently don't require authentication and essentially function as web servers, introducing an additional internal network-accessible endpoint in the cluster. This architecture allows attackers to access them directly from any pod in the network, significantly increasing the attack surface.Background on Ingress NGINX Controller for KubernetesIngress NGINX Controller is an ingress implementation that uses NGINX as a reverse proxy and a load balancer. It is one of the most popular ingresses and is a core Kubernetes project.To bridge between Kubernetes and NGINX configurations, which is a non-Kubernetes-native technology, the controller attempts to translate Kubernetes Ingress objects into NGINX configurations. To ensure the stability of the NGINX server, the controller employs a validating admission webhook that validates the final configuration before applying it.<picture><source srcSet=\"https://www.datocms-assets.com/75231/1742853318-single-frame-2.png?dpr=0.25&amp;fm=webp 1920w,https://www.datocms-assets.com/75231/1742853318-single-frame-2.png?dpr=0.5&amp;fm=webp 3840w,https://www.datocms-assets.com/75231/1742853318-single-frame-2.png?dpr=0.75&amp;fm=webp 5760w,https://www.datocms-assets.com/75231/1742853318-single-frame-2.png?fm=webp 7680w\" sizes=\"(max-width: 7680px) 100vw, 7680px\" type=\"image/webp\"/><source srcSet=\"https://www.datocms-assets.com/75231/1742853318-single-frame-2.png?dpr=0.25&amp;fm=webp 1920w,https://www.datocms-assets.com/75231/1742853318-single-frame-2.png?dpr=0.5&amp;fm=webp 3840w,https://www.datocms-assets.com/75231/1742853318-single-frame-2.png?dpr=0.75&amp;fm=webp 5760w,https://www.datocms-assets.com/75231/1742853318-single-frame-2.png?fm=webp 7680w\" sizes=\"(max-width: 7680px) 100vw, 7680px\"/><img src=\"https://www.datocms-assets.com/75231/1742853318-single-frame-2.png?fm=webp\" alt=\"\" style=\"position:absolute;left:0;top:0;width:100%;height:100%;max-width:none;max-height:none\" loading=\"lazy\" referrerPolicy=\"no-referrer-when-downgrade\"/></picture>Figure: Simplified diagram of Ingress NGINX Controller From an attacker perspective, the admission controller is an unauthenticated HTTP endpoint responsible for complicated operations, and by default runs with a Kubernetes role that allows access to all of the environment’s secrets, making it an appealing research target. Remote NGINX Configuration InjectionDuring our review of the Ingress NGINX Admission Controller code, we identified an interesting code path: when it processes incoming AdmissionReview requests, it generates a temporary NGINX configuration file based on a template file and the provided Ingress object. It then tests the validity of the temporary configuration file using the nginx -tcommand. We found multiple ways to inject new configuration directives in this code path.// testTemplate checks if the NGINX configuration inside the byte array is valid\n// running the command \"nginx -t\" using a temporal file.\nfunc (n *NGINXController) testTemplate(cfg []byte) error {\n...\n    tmpfile, err := os.CreateTemp(filepath.Join(os.TempDir(), \"nginx\"), tempNginxPattern)\n...\n    err = os.WriteFile(tmpfile.Name(), cfg, file.ReadWriteByUser)\n...\n    out, err := n.command.Test(tmpfile.Name())\n\nfunc (nc NginxCommand) Test(cfg string) ([]byte, error) {\n    //nolint:gosec // Ignore G204 error\n    return exec.Command(nc.Binary, \"-c\", cfg, \"-t\").CombinedOutput()\n} Typically, only the Kubernetes API server should send these AdmissionReview requests. However, because the Admission Controller lacks authentication, an attacker with minimal network access could craft and send arbitrary AdmissionReview requests from any pod within the cluster.For our testing, we used kube-review to create admission review requests from Ingress resource manifests, which could then be sent directly to the admission controller via HTTP.{\n    \"kind\": \"AdmissionReview\",\n    \"apiVersion\": \"admission.k8s.io/v1\",\n    \"request\": {\n        \"uid\": \"732536f0-d97e-4c9b-94bf-768953754aee\",\n...\n        \"name\": \"example-app\",\n        \"namespace\": \"default\",\n        \"operation\": \"CREATE\",\n...\n        \"object\": {\n            \"kind\": \"Ingress\",\n            \"apiVersion\": \"networking.k8s.io/v1\",\n            \"metadata\": {\n                \"name\": \"example-app\",\n                \"namespace\": \"default\",\n...\n                \"annotations\": {\n                    \"nginx.ingress.kubernetes.io/backend-protocol\": \"FCGI\"\n                }\n            },\n            \"spec\": {\n                \"ingressClassName\": \"nginx\",\n                \"rules\": [\n                    {\n                        \"host\": \"app.example.com\",\n                        \"http\": {\n                            \"paths\": [\n                                {\n                                    \"path\": \"/\",\n                                    \"pathType\": \"Prefix\",\n                                    \"backend\": {\n                                        \"service\": {\n                                            \"name\": \"example-service\",\n                                            \"port\": {}\n                                        }\n                                    }\n                                }\n                            ]\n                        }\n                    }\n                ]\n            },\n...\n    }\n} Figure: Example of an Admission Review objectAs can be seen above, there are plenty of fields we can control, showing the large attack surface. In this blog post we will look at two vulnerabilities in the annotation parsers that parse the .request.object.annotations field in the request above. Properties from this field are later included in the NGINX configuration file – which we used to inject arbitrary directives.CVE-2025-24514 – auth-url Annotation InjectionThe authreq parser is responsible for handling authentication-related annotations. The annotation requires an auth-url fields to be set, which includes a URL, and is finally propagated into the configuration file, in this code flow:func (a authReq) Parse(ing *networking.Ingress) (interface{}, error) {\n    // Required Parameters\n    urlString, err := parser.GetStringAnnotation(authReqURLAnnotation, ing, a.annotationConfig.Annotations)\n    if err != nil {\n        return nil, err\n    } When the temporary configuration is created, $externalAuth.URL—which corresponds to the URL from the auth-url annotation—is incorporated without proper sanitization.proxy_http_version 1.1;\nproxy_set_header Connection \"\";\nset $target {{ changeHostPort $externalAuth.URL $authUpstreamName }};\n{{ else }}\nproxy_http_version {{ $location.Proxy.ProxyHTTPVersion }};\nset $target {{ $externalAuth.URL }};\n{{ end }} This lack of proper sanitization allows an attacker to inject arbitrary NGINX configuration directives, which get evaluated when nginx -t runs.Consider the following auth-url annotation:nginx.ingress.kubernetes.io/auth-url: \"http://example.com/#;\\ninjection_point\" The final configuration will appear as follows:...\nproxy_http_version 1.1;\nset $target http://example.com/#;\ninjection_point\nproxy_pass $target;\n... This vulnerability does not apply to v1.12.0. In this version, Ingress NGINX Controller changed its default security settings to verify all annotations, including auth-url, against strict regex rules. CVE-2025-1097 – auth-tls-match-cn Annotation InjectionThe authtls parser, for its auth-tls-match-cn annotation, uses CommonNameAnnotationValidator to validate the field value:func CommonNameAnnotationValidator(s string) error {\n    if !strings.HasPrefix(s, \"CN=\") {\n        return fmt.Errorf(\"value %s is not a valid Common Name annotation: missing prefix 'CN='\", s)\n    }\n    if _, err := regexp.Compile(s[3:]); err != nil {\n        return fmt.Errorf(\"value %s is not a valid regex: %w\", s, err)\n    }\n    return nil\n} In other words, the auth-tls-match-cn annotation requires:The value must start with CN=.All remaining characters must form a valid regular expression.Similar to the previous injection, $server.CertificateAuth.MatchCN corresponds to the value of the auth-tls-match-cn annotation. While tricky, we can still bypass both requirements to inject arbitrary NGINX configurations in this part of the template: if ( $ssl_client_s_dn !~ {{ $server.CertificateAuth.MatchCN }} ) {\n    return 403 \"client certificate unauthorized\";\n} Consider the following auth-tls-match-cn annotation:nginx.ingress.kubernetes.io/auth-tls-match-cn: \"CN=abc #(\\n){}\\n }}\\nglobal_injection;\\n#\" The final configuration will appear as follows:...\nset $proxy_upstream_name \"-\";\nif ( $ssl_client_s_dn !~ CN=abc #(\n){} }}\nglobal_injection;\n# ) {\nreturn 403 \"client certificate unauthorized\"; }\n... For the auth-tls-match-cn annotation value to appear in the configuration, we also need to provide the nginx.ingress.kubernetes.io/auth-tls-secret annotation, which corresponds to a TLS certificate or keypair secret present in the cluster. Since the service account used by Ingress NGINX has access to all secrets in the cluster, we can specify any secret name from any namespace, provided it matches the required TLS certificate/keypair format. Notably, many managed Kubernetes solutions include such secrets by default. Below is a short list of common secrets that can be leveraged in this type of attack:kube-system/konnectivity-certs\nkube-system/azure-wi-webhook-server-cert\nkube-system/aws-load-balancer-webhook-tls\nkube-system/hubble-server-certs\nkube-system/cilium-ca\ncalico-system/node-certs\ncert-manager/cert-manager-webhook-ca\nlinkerd/linkerd-policy-validator-k8s-tls\nlinkerd/linkerd-proxy-injector-k8s-tls\nlinkerd/linkerd-sp-validator-k8s-tls CVE-2025-1098 – mirror UID InjectionIn the mirror annotation parser, the following code processes the UID from the ingress object, and inserts it into $location.Mirror.Source in the temporary NGINX configuration. We control the ing.UID field, which allows for a new injection point.Since this injection is in the UID parameter, which is not a Kubernetes annotation, our input does not get sanitized by the annotations’ regex rules. Since our input gets inserted as-is, we can easily escape our context and inject arbitrary NGINX configuration directives.CVE-2025-1974 - NGINX Configuration Code ExecutionThe vulnerabilities above allow an attacker to inject arbitrary directives to the NGINX configuration, which will later be tested by nginx -t. This does not immediately lead to code execution. If we can find a directive that executes arbitrary code in nginx -t, it will compromise the pod and obtain its highly privileged Kubernetes role. It is important to note that the NGINX configuration is only being tested, and is not being applied, reducing the number of directives we can actually (ab)use.<picture><source srcSet=\"https://www.datocms-assets.com/75231/1742830143-image5-1.png?dpr=0.25&amp;fm=webp 133w,https://www.datocms-assets.com/75231/1742830143-image5-1.png?dpr=0.5&amp;fm=webp 267w,https://www.datocms-assets.com/75231/1742830143-image5-1.png?dpr=0.75&amp;fm=webp 400w,https://www.datocms-assets.com/75231/1742830143-image5-1.png?fm=webp 534w\" sizes=\"(max-width: 534px) 100vw, 534px\" type=\"image/webp\"/><source srcSet=\"https://www.datocms-assets.com/75231/1742830143-image5-1.png?dpr=0.25&amp;fm=webp 133w,https://www.datocms-assets.com/75231/1742830143-image5-1.png?dpr=0.5&amp;fm=webp 267w,https://www.datocms-assets.com/75231/1742830143-image5-1.png?dpr=0.75&amp;fm=webp 400w,https://www.datocms-assets.com/75231/1742830143-image5-1.png?fm=webp 534w\" sizes=\"(max-width: 534px) 100vw, 534px\"/><img src=\"https://www.datocms-assets.com/75231/1742830143-image5-1.png?fm=webp\" alt=\"\" style=\"position:absolute;left:0;top:0;width:100%;height:100%;max-width:none;max-height:none\" loading=\"lazy\" referrerPolicy=\"no-referrer-when-downgrade\"/></picture>Figure: Partial list of available NGINX directives(Figure source)Initially we tried to use the load_module directive which can load a shared library from the filesystem. However, it can only be used in the beginning of the NGINX configuration, so when injected, load_module will fail with the following error message:<picture><source srcSet=\"https://www.datocms-assets.com/75231/1742830192-image6.png?dpr=0.25&amp;fm=webp 238w,https://www.datocms-assets.com/75231/1742830192-image6.png?dpr=0.5&amp;fm=webp 477w,https://www.datocms-assets.com/75231/1742830192-image6.png?dpr=0.75&amp;fm=webp 716w,https://www.datocms-assets.com/75231/1742830192-image6.png?fm=webp 955w\" sizes=\"(max-width: 955px) 100vw, 955px\" type=\"image/webp\"/><source srcSet=\"https://www.datocms-assets.com/75231/1742830192-image6.png?dpr=0.25&amp;fm=webp 238w,https://www.datocms-assets.com/75231/1742830192-image6.png?dpr=0.5&amp;fm=webp 477w,https://www.datocms-assets.com/75231/1742830192-image6.png?dpr=0.75&amp;fm=webp 716w,https://www.datocms-assets.com/75231/1742830192-image6.png?fm=webp 955w\" sizes=\"(max-width: 955px) 100vw, 955px\"/><img src=\"https://www.datocms-assets.com/75231/1742830192-image6.png?fm=webp\" alt=\"\" style=\"position:absolute;left:0;top:0;width:100%;height:100%;max-width:none;max-height:none\" loading=\"lazy\" referrerPolicy=\"no-referrer-when-downgrade\"/></picture>Figure: `load_module` fails as it is specified too late in the configuration There are many usable directives in Ingress NGINX Controller as their NGINX instance is compiled with many additional modules. We found that the ssl_engine directive, part of the OpenSSL module, can also load shared libraries. This behavior is undocumented. Unlike load_module, this directive can be used at any point within the configuration file, so it is suitable for our injection’s constraints.We can now load arbitrary library files during the NGINX configuration testing phase. Our next challenge is: How can we place a shared library on the pod’s filesystem?Uploading a shared library with NGINX Client Body BuffersIn parallel to the nginx -t and the admission controller webhook, the pod also runs the NGINX instance itself, listening on port 80 or 443: <picture><source srcSet=\"https://www.datocms-assets.com/75231/1742830220-image7.png?dpr=0.25&amp;fm=webp 284w,https://www.datocms-assets.com/75231/1742830220-image7.png?dpr=0.5&amp;fm=webp 569w,https://www.datocms-assets.com/75231/1742830220-image7.png?dpr=0.75&amp;fm=webp 853w,https://www.datocms-assets.com/75231/1742830220-image7.png?fm=webp 1138w\" sizes=\"(max-width: 1138px) 100vw, 1138px\" type=\"image/webp\"/><source srcSet=\"https://www.datocms-assets.com/75231/1742830220-image7.png?dpr=0.25&amp;fm=webp 284w,https://www.datocms-assets.com/75231/1742830220-image7.png?dpr=0.5&amp;fm=webp 569w,https://www.datocms-assets.com/75231/1742830220-image7.png?dpr=0.75&amp;fm=webp 853w,https://www.datocms-assets.com/75231/1742830220-image7.png?fm=webp 1138w\" sizes=\"(max-width: 1138px) 100vw, 1138px\"/><img src=\"https://www.datocms-assets.com/75231/1742830220-image7.png?fm=webp\" alt=\"\" style=\"position:absolute;left:0;top:0;width:100%;height:100%;max-width:none;max-height:none\" loading=\"lazy\" referrerPolicy=\"no-referrer-when-downgrade\"/></picture>Figure: NGINX is running in the same pod as Ingress NGINX Controller When processing requests, NGINX sometimes saves the request body into a temporary file (client body buffering). This happens if the HTTP request body size is greater than a certain threshold, which is by default 8KB. This means that we should theoretically be able to send a large (>8KB) HTTP request, containing our payload in the form of a shared library as the body of the request, and NGINX will temporarily save it to a file on the pod’s filesystem.Unfortunately, NGINX also removes the file immediately, creating a nearly-impossible race condition. However, NGINX holds an open file descriptor pointing to the file, which is accessible from ProcFS:<picture><source srcSet=\"https://www.datocms-assets.com/75231/1742830255-image8.png?dpr=0.25&amp;fm=webp 253w,https://www.datocms-assets.com/75231/1742830255-image8.png?dpr=0.5&amp;fm=webp 507w,https://www.datocms-assets.com/75231/1742830255-image8.png?dpr=0.75&amp;fm=webp 761w,https://www.datocms-assets.com/75231/1742830255-image8.png?fm=webp 1015w\" sizes=\"(max-width: 1015px) 100vw, 1015px\" type=\"image/webp\"/><source srcSet=\"https://www.datocms-assets.com/75231/1742830255-image8.png?dpr=0.25&amp;fm=webp 253w,https://www.datocms-assets.com/75231/1742830255-image8.png?dpr=0.5&amp;fm=webp 507w,https://www.datocms-assets.com/75231/1742830255-image8.png?dpr=0.75&amp;fm=webp 761w,https://www.datocms-assets.com/75231/1742830255-image8.png?fm=webp 1015w\" sizes=\"(max-width: 1015px) 100vw, 1015px\"/><img src=\"https://www.datocms-assets.com/75231/1742830255-image8.png?fm=webp\" alt=\"\" style=\"position:absolute;left:0;top:0;width:100%;height:100%;max-width:none;max-height:none\" loading=\"lazy\" referrerPolicy=\"no-referrer-when-downgrade\"/></picture>Figure: File descriptor is still accessible from ProcFS, although the file itself is already deleted (FD #11) To keep the file descriptor open, we can set the Content-Length header in the request to be larger than the actual content size. NGINX will keep waiting for more data to be sent, which will cause the process to hang, leaving the file-descriptor open for longer.The only downside to this trick is that we create the file in a different process, so we can’t use /proc/self to access it. Instead, we will have to guess both the PID and the FD number to find the shared library, but since this is a container with minimal processes, this can be done relatively fast with a few guesses.From Configuration Injection to RCEWith a reliable file upload to Ingress NGINX Controller’s pod, we can now put it all together to exploit this issue into a full-blown Remote Code Execution.The exploit works as follows:Upload our payload in the form of a shared library to the pod by abusing the client-body buffer feature of NGINXSend an AdmissionReview request to the Ingress NGINX Controller’s admission controller, which contains any one of our directive injectionsThe directive we inject is the ssl_engine directive, which will cause NGINX to load the specified file as a shared libraryWe specify the ProcFS path to the file descriptor of our payloadIf everything goes well, our shared library is now loaded, and we execute code remotelyHere is a demo of the exploit in practice:ConclusionsWe are only scratching the surface in reviewing the security of admission controllers. Initially, we were surprised to see that such a large code base is used behind the scenes. In our view, this attack surface should be restricted in a much better way: removing access from pods within the cluster, and never exposing this publicly. We were also surprised by the lack of least-privilege design, as the exploit ended up with privileges to take control of the cluster. During this research, we found other vulnerabilities in Ingress NGINX Controller, and we expect to find more in other admission controllers.Finally, we learned that nginx -t should be considered harmful. We would be happy to hear about other cases where nginx -t processes unsanitized user input in the wild. This should be more clearly highlighted in the NGINX documentation.Responsible Disclosure TimelineDecember 31, 2024 – Wiz Research reported CVE-2025-1974 and CVE-2025-24514 to Kubernetes.January 2, 2025 – Wiz Research reported CVE-2025-1097 to Kubernetes.January 3, 2025 – Kubernetes acknowledged the reports.January 9, 2025 – Kubernetes proposed a fix for CVE-2025-1097.January 10, 2025 – Wiz Research reported a bypass for the proposed fix for CVE-2025-1097.January 12, 2025 – Kubernetes proposed a fix for CVE-2025-1974.January 16, 2025 – Wiz Research reported a bypass for the proposed fix for CVE-2025-1974.January 20, 2025 – Kubernetes proposed a fix for CVE-2025-24513.January 21, 2025 – Wiz Research reported a bypass for the proposed fix for CVE-2025-24513.January 21, 2025 – Wiz Research reported CVE-2025-1098 to Kubernetes.February 7, 2025 – Kubernetes released internal patches for the injection vulnerabilities: CVE-2025-1098, CVE-2025-1097, and CVE-2025-24514.February 20, 2025 – Kubernetes notified Wiz Research that they removed the NGINX configuration validation from the admission controller, resolving CVE-2025-1974.March 10, 2025 – Kubernetes sent embargo notifications regarding the five vulnerabilities reported by Wiz Research.March 24, 2025 – Public disclosure.<picture><source srcSet=\"https://www.datocms-assets.com/75231/1742914278-image-39.png?dpr=0.25&amp;fm=webp 500w,https://www.datocms-assets.com/75231/1742914278-image-39.png?dpr=0.5&amp;fm=webp 1000w,https://www.datocms-assets.com/75231/1742914278-image-39.png?dpr=0.75&amp;fm=webp 1500w,https://www.datocms-assets.com/75231/1742914278-image-39.png?fm=webp 2000w\" sizes=\"(max-width: 2000px) 100vw, 2000px\" type=\"image/webp\"/><source srcSet=\"https://www.datocms-assets.com/75231/1742914278-image-39.png?dpr=0.25&amp;fm=webp 500w,https://www.datocms-assets.com/75231/1742914278-image-39.png?dpr=0.5&amp;fm=webp 1000w,https://www.datocms-assets.com/75231/1742914278-image-39.png?dpr=0.75&amp;fm=webp 1500w,https://www.datocms-assets.com/75231/1742914278-image-39.png?fm=webp 2000w\" sizes=\"(max-width: 2000px) 100vw, 2000px\"/><img src=\"https://www.datocms-assets.com/75231/1742914278-image-39.png?fm=webp\" alt=\"\" style=\"position:absolute;left:0;top:0;width:100%;height:100%;max-width:none;max-height:none\" loading=\"lazy\" referrerPolicy=\"no-referrer-when-downgrade\"/></picture><picture><source srcSet=\"https://www.datocms-assets.com/75231/1742829499-ingress.png?dpr=0.25&amp;fm=webp 716w,https://www.datocms-assets.com/75231/1742829499-ingress.png?dpr=0.5&amp;fm=webp 1432w,https://www.datocms-assets.com/75231/1742829499-ingress.png?dpr=0.75&amp;fm=webp 2148w,https://www.datocms-assets.com/75231/1742829499-ingress.png?fm=webp 2864w\" sizes=\"(max-width: 2864px) 100vw, 2864px\" type=\"image/webp\"/><source srcSet=\"https://www.datocms-assets.com/75231/1742829499-ingress.png?dpr=0.25&amp;fm=webp 716w,https://www.datocms-assets.com/75231/1742829499-ingress.png?dpr=0.5&amp;fm=webp 1432w,https://www.datocms-assets.com/75231/1742829499-ingress.png?dpr=0.75&amp;fm=webp 2148w,https://www.datocms-assets.com/75231/1742829499-ingress.png?fm=webp 2864w\" sizes=\"(max-width: 2864px) 100vw, 2864px\"/><img src=\"https://www.datocms-assets.com/75231/1742829499-ingress.png?fm=webp\" alt=\"\" style=\"position:absolute;left:0;top:0;width:100%;height:100%;max-width:none;max-height:none\" loading=\"lazy\" referrerPolicy=\"no-referrer-when-downgrade\"/></picture>Stay in touch!Hi there! We are Nir Ohfeld (@nirohfeld), Sagi Tzadik (@sagitz_), Ronen Shustin (@ronenshh), Hillai Ben-Sasson (@hillai), and Andres Riancho (@andresriancho) from the Wiz Research Team (@wiz_io). We are a group of veteran white-hat hackers with a single goal: to make the cloud a safer place for everyone. We primarily focus on finding new attack vectors in the cloud and uncovering isolation issues in cloud vendors and service providers. We would love to hear from you! Feel free to contact us on X (Twitter) or via email: research@wiz.io.Tags#Research#Vulnerabilities",
    "summary": {
      "en": "Wiz Research has identified several critical vulnerabilities, labeled as #IngressNightmare, in the Ingress NGINX Controller for Kubernetes. These vulnerabilities, including CVE-2025-1097, CVE-2025-1098, CVE-2025-24514, and CVE-2025-1974, allow attackers to execute remote code without authentication, potentially leading to unauthorized access to sensitive data and complete takeover of affected Kubernetes clusters. The vulnerabilities have a high severity rating of 9.8 on the CVSS scale.\n\nApproximately 43% of cloud environments are at risk, with over 6,500 clusters exposed, including those belonging to Fortune 500 companies. It is recommended that organizations patch their systems promptly, with guidance provided for mitigation and detection.\n\nIngress NGINX Controller is a widely used tool for managing external traffic to Kubernetes applications, but its admission controller component is vulnerable because it lacks proper authentication. Attackers can exploit this weakness to inject malicious configurations.\n\nFor protection, users should:\n1. Upgrade to the latest versions of Ingress NGINX Controller.\n2. Ensure that the admission controller is not exposed to the internet.\n3. Implement strict network policies and consider disabling the admission controller if an upgrade cannot be performed immediately.\n\nThe vulnerabilities were disclosed responsibly, and fixes have been proposed and implemented by Kubernetes maintainers. The research emphasizes the importance of securing admission controllers and ensuring they are not publicly accessible.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "6e96c28d86e44d47",
    "title": {
      "en": "Show HN: I'm a teacher and built an AI presentation tool",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": null,
    "score": 224,
    "by": "slidehero",
    "time": 1742709515,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "2fe01a4dc45a027a",
    "title": {
      "en": "Triforce – a beamformer for Apple Silicon laptops",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/chadmed/triforce",
    "score": 556,
    "by": "tosh",
    "time": 1742827534,
    "content": "Triforce - a beamformer for Apple Silicon laptops\nTriforce implements a Minimum Variance Distortionless Response adaptive beamformer\nfor the microphone array found in the following Apple Silicon laptops:\n\nMacBook Pro 13\" (M1/M2)\nMacBook Air 13\" (M1/M2)\nMacBook Pro 14\" (M1 Pro/Max, M2 Pro/Max)\nMacBook Pro 16\" (M1 Pro/Max, M2 Pro/Max)\nMacBook Air 15\" (M2)\n\nDependencies\nTriforce tries to keep dependencies to a minimum. On top of the\ncrates specified in Cargo.lock, you will only require:\n\nLV2\n\nWhy?\nThe Apple Silicon laptops mentioned above have an array of microphones arranged\neither in a triangle or in a straight line. Much like with the speakers, Apple are trying way too hard to be\nfancy here, and implement an adaptive beamformer in userspace to try and isolate\nthe desired signal from background noise. Without beamforming, the array is far\ntoo sensitive and omnidirectional to be at all useful. Thus, to make it useful outside\nof macOS, we need a beamformer.\nExpectation management\nFinding accessible literature on any DSP-related topics, let alone something like\nwideband adaptive beamforming, is challenging to say the least. This is an attempt\nat a beamformer armed only with first year undergrad level engineering maths and some vague\nidea of the principles gleaned from various webpages and PDFs. Do not expect it to\noutperform Apple's implementation at this time. Patches to improve it are always welcome!\nKnown limitations\n\nnalgebra does not do any explicit SIMD optimisation, relying only on LLVM auto-vectorisation.\nPerformance and efficiency of matrix math routines are not very good.\nFollowing from that, we are not doing wideband decomposition due to the added computational\nburden. Without SIMD/NEON support, this is simply too slow for a realtime audio plugin.\nOutput is mono only. Much like with wideband decomposition, adding additional matrix processing\nto fake stereo output would be too computationally intensive",
    "summary": {
      "en": "**Triforce - Beamformer for Apple Silicon Laptops**\n\nTriforce is a software tool designed to improve the microphone performance on certain Apple Silicon laptops, including:\n\n- MacBook Pro 13\" (M1/M2)\n- MacBook Air 13\" (M1/M2)\n- MacBook Pro 14\" (M1 Pro/Max, M2 Pro/Max)\n- MacBook Pro 16\" (M1 Pro/Max, M2 Pro/Max)\n- MacBook Air 15\" (M2)\n\n**Key Features:**\n- It uses a technology called Minimum Variance Distortionless Response (MVDR) to filter out background noise and enhance the desired sound.\n- The microphone array on these laptops is sensitive and picks up too much noise without this technology.\n\n**Dependencies:**\n- Triforce has minimal requirements, needing only the LV2 library in addition to the standard libraries listed in its Cargo.lock file.\n\n**Expectations:**\n- The author acknowledges that resources on digital signal processing (DSP) are limited. The current implementation may not match Apple's quality but invites improvements from others.\n  \n**Limitations:**\n- The software does not optimize for speed as advanced techniques like SIMD are not used, leading to slower performance.\n- It does not support advanced audio features like wideband decomposition or stereo sound, outputting only mono audio instead.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "76fe87c1edde5391",
    "title": {
      "en": "MRubyD: A new mruby virtual machine implemented in pure C#",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/hadashiA/MRubyD",
    "score": 84,
    "by": "ksec",
    "time": 1742712779,
    "content": "MRubyD\nMRubyD is a new mruby virtual machine implemented in pure C#. The name \"MRubyD\" stands for mruby for dotnet and pays homage to the well-known alternative implementation mruby/c. Designed with seamless integration in mind for C#-based game engines, and emphasize ruby level compatibility. MRubyD leverages the latest C# features for high performance and high extensibility.\nNoteThis library is currently in preview\n\nFeatures\n\nImplemented in C#\nUtilizes the robust capabilities of C# to ensure seamless integration with C#-based game engines.\n\nHigh Performance\nTakes advantage of modern C# language features—such as managed pointers, Span, and the performance benefits of the .NET runtime’s GC and JIT compiler—to deliver superior speed and efficiency.\n\nHigh compatibility with Ruby-level APIs  (Work in progress)\nIt is intended for use in software with a certain amount of resources, such as games. For this reason, we are focusing on Ruby API compatibility.\nAt release, all opcodes are implemented and pass the syntax.rb tests from the mruby repository.\n\nRich Library Integration & Extensibility\nCompared to the original C implementation, calling C#’s extensive libraries from Ruby is straightforward, making the VM highly extensible.\n\nLimitations (Preview Release)\nThis release is a preview version and comes with the following constraints:\n\nBuilt-in types and methods are still being implemented.\n\nPlease refer to ruby test, etc., for currently supported methods.\nWe are working on supporting all methods that are built into mruby by default.\n\nprivate and protected visibitily is not yet implemented. (mruby got support for this in 3.4)\nThis project provides only the VM implementation; it does not include a compiler. To compile mruby scripts, you need the native mruby-compiler.\n\nMost recent roadmap\n\n Implement builtin ruby libs\n Support Fiber\n All ruby code port to C# (for performance reason)\n Unity Integration\n VitalRouter.MRuby for the new version.\n\nInstallation\ndotnet add package MRubyD\n\nBasic Usage\nExecute byte-code\ndef fibonacci(n)\n  return n if n <= 1\n  fibonacci(n - 1) + fibonacci(n - 2)\nend\n\nfibonacci 10\n\n$ mrbc -o fibonaci.mrbc fibonacci.rb\n\nusing MRubyD;\n\n// Read the .mrb byte-code.\nvar bytes = File.ReadAllBytes(\"fibonacci.mrb\");\n\n// initialize state\nvar state = MRubyState.Create();\n\n// execute bytecoe\nvar result = state.Exec(bytes);\n\nresult.IsInteger    //=> true\nresult.IntegerValue //=> 55\n\nThis is a sample of executing bytecode.\nSee the How to compile .mrb  section for information on how to convert Ruby source code to mruby bytecode.\nHandlding MRubyValue\nAbove result is MRubyValue. This represents a Ruby value.\nvalue.IsNil //=> true if nol\nvalue.IsInteger //=> true if integrr\nvalue.IsFloat //=> true if float\nvalue.IsSymbol //=> true if Symbol\nvalue.IsObject //=> true if any allocated object type\n\nvalue.VType //=> get known ruby-type as C# enum.\n\nvalue.IntegerValue //=> get as C# Int64\nvalue.FloatValue //=> get as C# float\nvalue.SymbolValue //=> get as `Symbol`\n\nvalue.As<RString>() //=> get as object value\n\n// pattern matching\nif (vlaue.Object is RString str)\n{\n    // ...\n}\n\nswtich (value)\n{\n    case { IsInteger: true }:\n        // ...\n        break;\n    case { Object: RString str }:\n        // ...\n        break;\n}\n\nvar intValue = MRubyValue.From(100); // create int value\nvar floatValue = MRubyValue.From(1.234f); // create float value\nvar objValue = MRubyValue.From(str); // create allocated ruby object value\n\nDefine ruby class/module/method by C#\n// Create MRubyState object.\nvar state = MRubyState.Create();\n\n// Define class\nvar classA = state.DefineClass(Intern(\"A\"u8), c =>\n{\n    // Method definition that takes a required argument.\n    c.DefineMethod(Intern(\"plus100\"u8), (state, self) =>\n    {\n        var arg0 = state.GetArgAsIntegeger(0); // get first argument (index:0)\n        return MRubyValue.From(arg0 + 100);\n    });\n\n    // Method definition that takes a block argument.\n    c.DefineMethod(Intern(\"method2\"), (state, self) =>\n    {\n        var arg0 = state.GetArg(0);\n        var blockArg = state.GetBlockArg();\n        if (!blockArg.IsNil)\n        {\n            // Execute `Proc#call`\n            state.Send(blockArg, state.Intern(\"call\"u8), arg0);\n        }\n    });\n\n    // Other complex arguments...\n    c.DefineMethod(Intern(\"method3\"), (state, self) =>\n    {\n        var keywordArg = state.GetKeywordArg(state.Intern(\"foo\"))\n        Console.WriteLine($\"foo: {keywordArg}\");\n\n        // argument type checking\n        state.EnsureValueType(keywordArg, MrubyVType.Integer);\n\n        var restArguments = state.GetRestArguments();\n        for (var i = 0; i < restArguments.Length; i++)\n        {\n            Console.WriteLine($\"rest arg({i}: {restArguments[i]})\");\n        }\n    });\n\n    // class method\n    c.DefineClassMethod(Intern(\"classmethod1\"), (state, self) =>\n    {\n        var str = state.NewString($\"hoge fuga\");\n        return MRubyValue.From(str);\n    });\n\n});\n\n// Monkey patching\nclassA.DefineMethod(Intern(\"additional_method1\"u8), (state, self) => { /* ... */ });\n\n// Define module\nvar moduleA = state.DefineModule(Intern(\"ModuleA\");)\nstate.DefineMethod(moduleA, Intern(\"additional_method2\"u8), (state, self) => MRubyValue.From(123));\n\nstate.IncludeModule(classA, moduleA);\n\na = A.new\na.plus100(123) #=> 223\n\na.method2(1) { |a| a } #=> 1\n\na.additionoa_method2 #=> 123\n\nA.classmethod1 #=> \"hoge fuga\"\n\nSymbol/String\nThe string representation within mruby is utf8.\nTherefore, to generate a ruby string from C#, Utf8StringInterpolation is used internally.\n// Create string literal.\nvar str1 = state.NewString(\"HOGE HOGE\"u8);\n\n// Create string via interpolation\nvar x = 123;\nvar str2 = state.NewString($\"x={x}\");\n\n// wrap MRubyValue..\nvar strValue = MRubyValue.From(str1);\n\nThere is a concept in mruby similar to String called Symbol.\nLike String, it is created using utf8 strings, but internally it is a uint integer.\nSymbols are usually used for method IDs and class IDs.\nTo create a symbol from C#, use Intern.\n// symbol literal\nvar sym1 = state.Intern(\"sym\"u8)\n\n// symbol from string\nvar sym2 = state.ToSymbol(state.NewString(\"sym2\"u8));\n\nHow to compile .mrb\nMRubyD only includes the mruby virtual machine. Therefore it is necessary to convert it to .mrb bytecode before executing the .rb source.\nBasically, you need the native compiler provided by the mruby project.\n$ git clone git@github.com:mruby/mruby.git\n$ cd mruby\n$ rake\n$ ./build/host/bin/mrubc\n\nMRubyD.Compiler\nTo simplify compilation from C#, we also provide the MRubyD.Compiler package, which is a thin wrapper for the native compiler.\nNoteThis MRubyD.Compiler package is a thin wrapper for the native binary. Currently, builds for linux (x64/arm64), macOS (x64/arm64), and windows (x64) are provided.\n\ndotnet add package MRubyD.Compiler\n\nusing MRubyD.Compiler;\n\nvar source = \"\"\"\ndef a\n  1\nend\n\na\n\"\"\"u8;\n\nvar state = MRubyState.Create();\nvar compiler = MRubyCompiler.Create(state);\n\nvar irep = compiler.Compile(source);\n\nstate.Exec(irep); // => 1\n\nLICENSE\nMIT\nContact\n@hadahsiA",
    "summary": {
      "en": "**Summary of MRubyD**\n\nMRubyD is a new virtual machine for mruby, built entirely in C#. It allows for easy integration with C#-based game engines and aims to be compatible with Ruby APIs. Here are the key points:\n\n- **Purpose**: Designed for use in resource-intensive applications like games, MRubyD focuses on Ruby compatibility while utilizing C# features for better performance.\n  \n- **Performance**: It uses modern C# features and the .NET runtime to achieve high speed and efficiency.\n\n- **Compatibility**: The project is still in preview, with many built-in Ruby types and methods yet to be implemented. The goal is to support all default mruby methods by the final release.\n\n- **Extensibility**: It allows easy integration with C# libraries, enhancing its functionality compared to the original C implementation.\n\n- **Installation**: Can be added to projects using the command `dotnet add package MRubyD`.\n\n- **Basic Usage**: Users can execute Ruby bytecode by first compiling Ruby scripts into .mrb format and then running them with MRubyD.\n\n- **Development Features**: Users can define Ruby classes and methods in C#, handle Ruby values, and use pattern matching for Ruby types.\n\n- **Compiler**: MRubyD does not include a compiler; users must use the native mruby compiler, although MRubyD.Compiler provides a wrapper for easier compilation.\n\nThis project is licensed under the MIT License and is actively being developed.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "7cca7b2df00539d9",
    "title": {
      "en": "Good drug news – More important things happening in biotechnology and medicine",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.worksinprogress.news/p/links-in-progress-good-drug-news",
    "score": 4,
    "by": "therabbithole",
    "time": 1742899855,
    "content": "Share this postThe Works in Progress NewsletterLinks in Progress: Good drug newsCopy linkFacebookEmailNotesMoreDiscover more from The Works in Progress NewsletterNew and underrated ideas to improve the world. Visit our website: worksinprogress.coOver 34,000 subscribersSubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.Already have an account? Sign inLinks in ProgressLinks in Progress: Good drug newsMore important things happening in biotechnology and medicine Saloni Dattani and Niko McCartyMar 24, 202535Share this postThe Works in Progress NewsletterLinks in Progress: Good drug newsCopy linkFacebookEmailNotesMore2ShareWe recently released Issue 18 of Works in Progress. Read about prehistoric psychopaths, fertility on demand and the king of fruits here. Today on Links in Progress Asimov Press’s Niko McCarty and our own Saloni Dattani return to review more important things happening in the world of biotechnology and medicine. 1. Nanopore sequencing is a promising, low-cost alternative to traditional genome sequencing methods. Nanopores are tiny holes in a membrane. When a molecule passes through a nanopore, it disrupts an electric field. Fluctuations in the field can be ‘decoded’ to figure out whether a passing molecule was an adenine, thymine, or something else. Unfortunately, its use has been limited by poor spatial resolution. Roche has developed a new sequencing method called Sequencing by Expansion (SBX), which expands DNA molecules into larger polymer structures called Xpandomers and makes them easier to read with nanopores. If successful, SBX would improve the accuracy of nanopore sequencing and bring costs down.2. Justin Sandefur and Charles Kenny estimate that US foreign aid prevents around 3 million deaths per year at a cost of only 0.2 percent of the US’s GNI. The Trump administration has scrapped 90 percent of all USAID contracts, including some of the most impactful programs. The scrapped contracts seem to include all tuberculosis programs, programs supplying essential medicines for maternal and child health; programs providing HIV treatment; UNICEF’s polio vaccination program; contracts for disease outbreak prevention, detection, and response; emergency food aid; earthquake assistance and more.3. The FDA approved a non-opioid painkiller. Journavx, developed by Vertex Pharmaceuticals, targets a protein called the NaV1.8 voltage-gated sodium channel, which transmits pain signals from sensory neurons to the brain. No other drugs on the market target this protein; existing painkillers instead bind to several other different NaV channels at once. In a phase 3 trial with 2,000 patients undergoing surgery, Journavx reduced pain as effectively as hydrocodone plus paracetamol and had fewer side effects. It also does not appear to be addictive. But the drug will cost about $233 per week; opioids cost about $12 per week.4. The FDA approved a new vaccine for chikungunya, a mosquito-borne disease that can result in joint pain for months. The new vaccine was developed by Bavarian Nordic, the same company that manufactured the Jynneos Mpox vaccine. Bavarian Nordic have been rewarded for the results of their chikungunya vaccine with a ‘priority disease voucher,’ which will allow them the opportunity to jump the FDA review queue with another drug. Because they primarily affect poorer countries, drugs for curing tropical diseases face a limited financial market. The ‘priority disease voucher’ acts as a reward for Bavarian Nordic, who can use it to get their next drug to market faster.5. A guide to ‘forensic metascience’. James Heathers, a researcher and scientific sleuth, has published a textbook explaining how to probe academic studies for potential errors, questionable research practices, and scientific fraud.6. Chinese researchers made mice with two genetic fathers. They took sperm from male mice and reprogrammed them into stem cells, effectively ‘rewinding’ their development. Those stem cells retained only one set of chromosomes, like normal sperm, but were then fused with a second sperm’s DNA — resulting in both XX and XY sex chromosomes. The resulting DNA was injected into an egg without a nucleus, producing embryos with two paternal genomes. Out of 164 attempts, seven mouse pups were born; they survived to adulthood but had serious health problems and were infertile.7. Arc Institute launched the largest AI model yet for biology. Called Evo 2, it was trained on 9.3 trillion nucleotides of DNA from 128,000 organisms. Evo 2 can predict whether a given mutation in a DNA sequence will be pathogenic with over 90 percent accuracy, even in instances where scientists have not previously studied that mutation in the laboratory. The model was trained on 2,000 NVIDIA H100 GPUs – about 150 times more compute than AlphaFold. Asimov Press covered the story.8. In 2022, the retraction rate across hundreds of journals slightly exceeded 0.2 percent, but about 80 percent of these retractions came from journals owned by a single publisher: Hindawi. This latest Nature report shows that the problem is pervasive for specific publishers and for select universities. Jining First People’s Hospital had a five percent retraction rate between 2014 and 2024, which is 50-times the global average. In 2023, 60 percent of retractions were from Chinese institutions.9. Enzymes speed up chemical reactions. They often do so by going through multiple steps to complete a reaction cycle. However, designing new multi-step enzymes has been challenging because AI tools typically predict static protein structures rather than capturing their dynamic movements. To overcome this, scientists combined AI models RFDiffusion and AlphaFold2, as well as a custom tool called PLACER, to create a “multi-step enzyme” for the first time. Though these AI-designed enzymes are much slower than natural ones (10-3–10-1 vs. 102–105 reactions per second), this proof of concept opens the door to engineering entirely new enzymes that do not exist in nature.Subscribe to The Works in Progress NewsletterLaunched 5 years agoNew and underrated ideas to improve the world. Visit our website: worksinprogress.coSubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.35 Likes∙2 Restacks35Share this postThe Works in Progress NewsletterLinks in Progress: Good drug newsCopy linkFacebookEmailNotesMore2ShareA guest post byNiko McCartyBioengineer and writer.Subscribe to Niko",
    "summary": {
      "en": "**Summary of Links in Progress: Good Drug News**\n\n1. **Nanopore Sequencing**: A new technique called Sequencing by Expansion (SBX) from Roche aims to enhance nanopore sequencing, making it more accurate and affordable by expanding DNA for better reading.\n\n2. **US Foreign Aid Impact**: Research shows US foreign aid saves about 3 million lives yearly at a low cost, but recent cuts have eliminated key health programs, including those for tuberculosis and HIV.\n\n3. **New Painkiller Approved**: The FDA approved Journavx, a non-opioid painkiller that targets a specific pain signal protein. It is as effective as traditional painkillers but more expensive and has fewer side effects.\n\n4. **Chikungunya Vaccine**: A new vaccine for chikungunya has been approved, rewarding its developers with a priority voucher for faster FDA review of future drugs.\n\n5. **Forensic Metascience Guide**: A new textbook explains how to identify errors and fraud in academic research.\n\n6. **Genetic Research in Mice**: Chinese scientists created mice with genetic material from two fathers. While some pups survived, they faced serious health issues.\n\n7. **AI in Biology**: The Arc Institute launched Evo 2, a powerful AI model that can predict the effects of genetic mutations with high accuracy.\n\n8. **Research Retractions**: A report reveals high retraction rates in certain journals, particularly those owned by a single publisher, with significant issues seen in Chinese institutions.\n\n9. **AI-Designed Enzymes**: Scientists have successfully created a multi-step enzyme using AI, paving the way for designing new enzymes that do not exist in nature. \n\nThis newsletter covers significant advancements and challenges in biotechnology and medicine.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "69d49cc6504c4fff",
    "title": {
      "en": "Intel: A Bug and a Pro",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.abortretry.fail/p/intel-a-bug-and-a-pro",
    "score": 64,
    "by": "rbanffy",
    "time": 1742851213,
    "content": "Share this postAbort Retry FailIntel: a Bug and a ProCopy linkFacebookEmailNotesMoreDiscover more from Abort Retry FailIn Abort Retry Fail, I am attempting to chronicle the history of the computing industry. This is a goal that will likely not be completed within my lifetime.SubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.Already have an account? Sign inIntel: a Bug and a ProThe FDIV bug and release of the P6Feb 24, 202519Share this postAbort Retry FailIntel: a Bug and a ProCopy linkFacebookEmailNotesMore1ShareBy 1994, Intel had managed to capture about 75% of the microprocessor market for personal computers. The company had also found its way into automobiles, industrial controls, telecommunications equipment, and supercomputers. The Intel x86 architecture had propelled the company to these immense heights, and they’d established a cadence of releases in that CPU line of roughly four years. The investment that both individuals and companies had made in hardware and software for the architecture was measured in billions.While the Pentium had been announced on the 22nd of March in 1993, systems built around the Pentium were announced on the 17th of May in 1993, and volume shipments didn’t occur until later in the year. The Pentium’s 60MHz variant could deliver 100 million instructions per second with the Dhrystone 1.1 benchmark while the 66MHz variant bumped that to 112MIPS. These were the first x86 chips to be capable of this kind of performance, and they nearly doubled the performance of the original i486. This kind of performance nearly put Intel in competition with RISC-based UNIX workstation hardware, and Intel’s biggest software partner, Microsoft, supported those workstations with NT. A key difference, however, was that the Pentium’s price was around $900 while the MIPS R8000 was around $2500 and the Alpha 21064 was over $3000. Naturally, these were all far more reasonably priced when purchased in volume, but Intel was far better in terms of performance per dollar. For less than the cost of a high performance RISC CPU, a customer could purchase an entire Pentium-based PC that, while not as powerful, was capable of the same kinds of work. This is remarkable as PC manufacturers were running into the problems already solved by workstation makers: thermals, volatage variation, RFI, EMI. These kinds of considerations hadn’t been too big a hill to climb with an XT, but they were far more serious issues with a Pentium. They were serious enough that Intel had provided thermal models to system makers before they had provided the silicon.Early in 1994, Intel announced the Pentium P54C for Socket 5. These chips required a 3.3 volt power supply, could run at 75MHz, 90MHz, or 100MHz, and utilized an internal clock multiplier allowing them to operate at a higher frequency than the external buses. These chips bumped the transistor count to 3.3 million, and were built on a half micron BiCMOS process. These Pentiums also added support for use in multiprocessor systems which was the primary source of the increased transistor count. When adding a processor, a machine needs more circuitry for cache, memory, interrupts, and bus utilization.While most news for Intel was great, not all of it was. A few of the first generation Pentium processors were in the hands of Professor Thomas R. Nicely at Lynchburg College in Virginia where they were put to work researching reciprocals of twin prime numbers. On the 13th of June in 1994, Nicely noted that after adding three Pentium-based computers to his lab, he was getting inconsistent results in floating point divisions. He ruled out all causes other than hardware error on the 19th of October. Specifically, Nicely noted that 1 divided by 824633702441 was incorrect on three different Pentium systems while it was correct on multiple i486 and i386 systems. He then sent an email to Intel on the 24th documenting his discovery. Intel didn’t provide much information to him beyond stating that the issue was known. He then sent an email to several other people on the 30th. His email read:It appears that there is a bug in the floating point unit (numeric coprocessor) of many, and perhaps all, Pentium processors.In short, the Pentium FPU is returning erroneous values for certain division operations. For example, 1/824633702441.0 is calculated incorrectly (all digits beyond the eighth significant digit are in error). This can be verified in compiled code, an ordinary spreadsheet such as Quattro Pro or Excel, or even the Windows calculator (use the scientific mode), by computing (824633702441.0)*(1/824633702441.0), which should equal 1 exactly (within some extremely small rounding error; in general, coprocessor results should contain 19 significant decimal digits). However, the Pentiums tested return 0.999999996274709702 for this calculation. A similar erroneous value is obtained for x*(1/x) for most values of x in the interval 824633702418 <= x <= 824633702449, and throughout any interval obtained by multiplying or dividing the above interval by an integer power of 2 (there are yet other intervals which also produce division errors).The bug can also be observed by calculating 1/(1/x) for the above values of x. The Pentium FPU will fail to return the original x (in fact, it will often return a value exactly 3072 = 6*0x200 larger).The bug has been observed on all Pentiums I have tested or had tested to date, including a Dell P90, a Gateway P90, a Micron P60, an Insight P60, and a Packard-Bell P60. It has not been observed on any 486 or earlier system, even those with a PCI bus. If the FPU is locked out (not always possible), the error disappears; but then the Pentium becomes a “586SX”, and floating point must run in emulation, slowing down computations by a factor of roughly ten.I encountered erroneous results which were related to this bug as long ago as June, 1994, but it was not until 19 October 1994 that I felt I had eliminated all other likely sources of error (software logic, compiler, chipset, etc.). I contacted Intel Tech Support regarding this bug on Monday 24 October (call reference number 51270). The contact person later reported that the bug was observed on a 66-MHz system at Intel, but had no further information or explanation, other than the fact that no such bug had been previously reported or observed.Further information can be obtained by contacting me directly, and by downloading files from the [anonymous.nicely.pentium_bug] directory of the acavax.lynchburg.edu machine via anonymous ftp on Internet (password ANONYMOUS, user ID = Internet ID). These files include a documentation file, a DOS executable image demonstrating the bug, and the source code for the demonstration. The zip file uses PKZIP version 2.04g.I would be interested in hearing of test results from other Pentiums, and also from 486-DX4s and (if anybody has one yet) the AMD, Cyrix, and NexGen clones of the Pentium.You may use this information freely as long as you give me attribution by name and employer.This… was exactly the sort of thing folks at Intel would not have wanted to have happen. Andrew Schulman, author of Undocumented DOS and Undocumented Windows, was among the recipients of that email. At this time, Schulman didn’t own a Pentium, so he forwarded the email to Richard Smith of Phar Lap and asked him to test it. Smith and his company confirmed the bug, and Smith posted the email to a CompuServe forum. Within 24 hours, there were 10 more confirmations. This was called the FDIV bug (after the x86 assembly language mnemonic for floating point division), and it made headlines. On the 7th of November in 1994, the Electronic Engineering Times ran an article titled: Intel fixes a Pentium FPU glitch which read:To correct an anomaly that caused inaccurate results on some high-precision calculations, Intel Corp. last week confirmed that it had updated the floating-point unit (FPU) in the Pentium microprocessorThe company said that the glitch was discovered midyear and was fixed with a mask change in recent silicon. \"This was a very rare condition that happened once every 9 to 10 billion operand pairs,\" said Steve Smith, a Pentium engineering manager at Intel.A spot check last week indicated the problem is present in at least one recently made Pentium-based PC. Intel said it could not quantify how many such systems were in the field.Said an Intel spokesman: \"This doesn't even qualify as an errata. We fixed it in a subsequent stepping.\"Erroneous divisionThe issue came to light last week in a message, on Compuserve's \"Canopus\" forum, which was a reposting of a private e-mail communication from Lynchburg College (Lynchburg,Va.) mathematics professor Thomas Nicely. \"The Pentium floating-point unit is returning erroneous values for certain division operations,\" he wrote. \"For example, 1/824633702441 is calculated incorrectly (all digits beyond the eigth significant digit are in error). This can be verified...by computing (824633702441.0) X (1/824633702441.0), which should equal 1 exactly (within some exteremely small rounding error; in general, coprocessor results should contain 19 significant decimal digits). However, the Pentiums tested return 0.999999996274709702 for this calculation.\"\"The bug has been observed on all Pentiums I have tested or had tested to date, including a Dell P90, a Gateway P90, a Micron P60, an Insight P60 and a Packard-Bell P60. It has not been observed on any 486 or earlier system, even those with a PCI bus. If the floating-point unit is locked out (not always possible), the error disappears.\"Intel's Smith emphasized that the anomaly would not affect the average user. Speaking of Nicely, Smith said: \"He's the most exterme user. He spends round-the-clock time calculating reciprocals. What he observed after running this for months is an instance where we have eight decimal points correct, and the ninth not showing up correctly. So you get an error in the ninth decimal digit to the right of the mantissa. I think even if you're an engineer, you're not going to see this.\"Nicely said he pointed out the problem to Intel, because \"it has a major effect in mathematics, because we have to have absolute precision. I suspect that, to the majority of people, it will be irrelevant. But engineers may have a different outlook.\"A spot check conducted at EE Times last week tested out Nicely's expression on an AcerPower Minitower Pentium/60 machine, which was just received from Acer America. The result was 0.999999996247.Intel said it discovered the anomaly through its own random testing. The fix involved a mask change to the Pentium's floating-poing unit. Specifically, according to Intel's Smith, the correction entailed an update to the programmable-logic array (PLA) on the Pentium.\"This is related to the state machine in the floating-point unit. There are certain cases where, way out in the operation, we didn't handle the precision correctly,\" he said. \"It's an iterative calculation, and at each point, you have to go through a lookup process that says what your next approximation for the bits further out in the mantissa, [which equates to the digits] further to the right of the decimal point.\"Better resultsThe fix entailed adding terms, or additional gate-sequences, to the PLA. That corrected the erroneous results returned from the problematic lookup table accesses.Intel said there are no part-number designations or other markings on the updated microprocessors - which became available in the last few months - to differentiate them from the earlier anomalous parts. However, an Intel spokesman said, \"If customers are concerned, they can call and we'll replace\" any of the parts that contained the bug.CNN reported on the bug on the 22nd of November, and then on the 23rd of November in 1994, the New York Times ran an article titled: Flaw Undermines Accuracy of Pentium Chips, which read in part:The flaw appears in all Pentium chips now on the market, in certain types of division problems involving more than five significant digits, a mathematical term that can include numbers before and after a decimal point.Intel declined to say how many Pentium chips it made or sold, but Dataquest, a market research company in San Jose, Calif., estimated that in 1994 Intel would sell 5.5 million to 6 million Pentiums, roughly 10 percent of the number of personal computers sold worldwide.Intel said yesterday that it did not believe the chip needed to be recalled, asserting that the typical user would have but one chance in more than nine billion of encountering an inaccurate result as a consequence of the error, and thus there was no noticeable consequence to users of business or home computers. Indeed, the company said it was continuing to send computer makers Pentium chips built before the problem was detected.This was, most likely, the first widely publicized instance of a hardware error in a microprocessor. The publicity, however, didn’t get Intel to act. They would ship replacements only if a customer could prove to an Intel representative that he/she required accuracy beyond what his/her Pentium could provide. On the 12th of December, IBM announced that it was halting shipments of Pentium-based machines. While Intel had stated that an average user might encounter the bug once in 27,000 years, IBM found that a user would likely encounter this bug every few days. Intel quickly changed their plans announcing on the 19th that they’d replace all flawed Pentiums. This cost them $475 million (or roughly $1.018 billion in 2025 dollars).Ken Sherriff found the bug in silicon and provided an in-depth explanation for what exactly went wrong and how it got corrected if you’d like more detailed coverage of it.The year was far better for Intel in the supercomputer space with the i960 becoming the leading RISC chip in total number of chips sold. This little flake of silicon could be found in networking devices, laser printers, and the world’s most powerful supercomputer at the time built of two Intel Paragon XP/S 140s at Sandia National Laboratories.By the end of the year, Intel’s Tape Carrier Packaging (TCP) had made its way to market. Intel described TCP as offering reduced pitch, thin package profiles, and a small on-board footprint. They also stated: “Key package features include surface mount technology design, lead pitch of 0.25 mm, 48 mm tape format, polyimide-up for pick and place, and slide carrier handling. Shipped flat in slide carriers, the leads are designed to be formed into a ‘gull-wing’ configuration and reflowed onto the PCB by one of several methods.” What this meant was that the Pentium could be used in notebooks. The first of these was (as far as I could find) was the Toshiba T4900CT. This notebook featured an Intel Pentium clocked at 75MHz (3.3 volt, 16K cache), 10.4” SVGA TFT-LCD, 772MB HDD, 8MB of RAM (expandable to 40MB), 2x PCMCIA, VL-bus GPU, 3.5 inch 1.44MB floppy disk drive, and it ran Windows 3.11 for Workgroups. If you are aware of an early Pentium notebook, please let me know in the comments.Despite the FDIV bug and a healthy $3.55 billion spent on capital and R&D, Intel’s income for 1994 was $2.28 billion on revenues of $11.5 billion.With corrected Pentiums and the P54C out in the wild, 1995 was a great year for the company. With the launch of Windows 95 on the 24th of August, sales for the Pentium increased significantly with PC sales increasing by 25% globally to around 60 million. Microsoft’s success was lifting all boats in the PC markets, and it was doing so in a dramatic way. It’s support for the PCI bus didn’t hurt Intel either.Intel Pentium Pro die image, from IntelOn the 1st of November in 1995, Intel launched the Pentium Pro, and had Intel kept its original naming scheme, this would have been the i686. The original Pentium Pro was built of 5.5 million transistors on either a half micron or a 0.35 micron BiCMOS process. It shipped in a 387-pin CPGA (Socket 8), supported a clock speed of 150MHz to 200MHz, and could support single, dual, and quad processor configurations. While a 32bit CPU, the Pentium Pro had a 36bit address width and supported PAE which allowed it to address up to 64GB of RAM. It had an 8K L1 instruction cache, an 8K L1 data cache, and a 256K L2 on the low-end and up to 1MB L2 on the high-end. The L2 was on-package but not on-chip, and it had its own back-side bus. This design allowed the CPU to read memory and cache concurrently. TDP ranged from 29.2W to 44W depending upon the clock, bus speed (60MHz or 66MHz), and cache size.Intel Pentium Pro 200 in socket, image from computerbase.deThe Pentium Pro shared almost nothing with its predecessors beyond its name, manufacturer, and ISA. The lead designer was Fred Pollack who’d previously worked on the iAPX 432. Internally, the Pentium Pro was a superscalar, out of order, RISC chip with support for speculative execution and register renaming. On every cycle, up to 16bytes were fetched from the instruction cache and sent to decoders of which there were three. These decoders took x86 instructions and translated them into RISC micro-operations. Two of these were intended only for simple x86 instructions (simple defined here as instructions that are translated into a single RISC micro-operation in a single cycle) while the third could handle any instruction. The Pentium Pro heavily relied upon microcode and that microcode was upgradeable under either the BIOS or operating system control. Yet another deviation from earlier x86 chips was backward compatibility. Unlike earlier Intel x86 CPUs, the Pentium Pro, while capable of running 16bit code, didn’t emphasize this, and it was, indeed, slower than other x86 chips of that time at executing 16bit code. The cause of the slow down was that a read from a large register, say EAX, would stall until a write to a partial register, say AX, retired. This stall would then result in a pipeline flush. The Pentium Pro was meant to run fully 32bit software, and the ability to run software for earlier chips was meant to be a convenience. The Pentium Pro was specifically targetted at workstations and servers where one wouldn’t usually be running a standard word processor, spreadsheet, or video game. Those applications were the domain of the i386, i486, or original Pentium. Pentium Pro was a monster, image from Microprocessor Report, Vol9, Num15, 13th Nov 1995For those workstation and server users who wanted crazy performance more cheaply than a traditional UNIX on RISC system, the Pentium Pro delivered. When the chip shipped on the 2nd of November, pricing stood at $974 to $1325 (the top end SKU would ship slightly later at a price of $1989), and it was faster than all competitors at 32bit integer performance. While it lagged in floating point, the ability to combine multiple Pentiums Pro in a single system, allowed Intel to compete well despite that disadvantage. Truly, the Pentium Pro was remarkable. The slowest part, at 166MHz, delivered slightly more than double the integer performance of a 200MHz R4400 in SYSmark/NT. Obviously, when dealing with real-world applications, this could vary, but for many applications, the Pentium Pro did punch above its price. To put the pricing of full systems into perspective, Intergraph would launch the first quad-CPU Pentium Pro workstations in November of 1996. All of the machines in this lineup featured a graphics add-in card made by Intergraph (G76, G95, or Intense 3D). The cheapest was the TD-200 ranging from $1500 to $5000 depending upon the graphics card used. In that top of the line single-CPU model, the machine delivered roughly twice the graphical performance of an SGI O2 (according to Intergraph). Completing the line up from Intergraph were the TD-310, TD-410, and TD-610. These came with one, two, or four Pentiums Pro. These could be configured with up to 1GB of RAM, an Ultra SCSI hard disk, an ethernet controller, a Creative Labs Vibra 16c (integrated), up to ten expansion slots (PCI and ISA) in desk-side models, an 8x CD-ROM, a 3.5 inch floppy disk drive, dual-screen support, and Windows NT 4.0.Intergraph TD-310, image from intergraph manualThe Pentium Pro would not have anywhere near the market success of the Pentium, but that wasn’t its purpose. It was meant to break into the workstation and server markets; goals it accomplished. Additionally, the architecture of the Pentium Pro (P6) would server Intel quite well for more than a decade.Intel closed 1995 with $3.5 billion in income on $16.2 billion in revenues. Starting 1996 with the Pentium, Windows 95, and the explosion of the World Wide Web, Intel was benefiting from massive ecosystem growth. While the FDIV bug certainly got quite a bit of press, cost quite a bit of money, and was something Intel would surely rather have avoided, it didn’t hurt the company’s trajectory at all. Intel made it to number 5 on Fortune’s most admired companies list in 1996, and the company’s market value reached $111 billion. Intel closed 1996 with $5.157 billion in income on revenues of $20.847 billion, assets of $13.684 billion, and liabilities of $4.863 billion.I now have readers from many of the companies whose history I cover, and many of you were present for time periods I cover. A few of you are mentioned by name in my articles. All corrections to the record are welcome; feel free to leave a comment.Subscribe to Abort Retry FailBy Bradford Morgan WhiteIn Abort Retry Fail, I am attempting to chronicle the history of the computing industry. This is a goal that will likely not be completed within my lifetime.SubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.19 Likes∙1 Restack19Share this postAbort Retry FailIntel: a Bug and a ProCopy linkFacebookEmailNotesMore1Share",
    "summary": {
      "en": "The text discusses the history of Intel, particularly focusing on the Pentium microprocessor and a significant bug known as the FDIV bug. By 1994, Intel dominated the microprocessor market, achieving about 75% market share and expanding into various industries. The Pentium, launched in 1993, offered impressive performance, nearly doubling that of its predecessor, the i486, and was priced competitively against high-end RISC CPUs.\n\nIn 1994, a mathematician discovered a bug in the Pentium's floating-point unit that caused errors in specific division calculations. This issue, termed the FDIV bug, gained public attention when the mathematician shared his findings, leading to widespread confirmation of the problem. Although Intel initially downplayed the bug's impact, they eventually acknowledged it and agreed to replace affected chips at a significant cost.\n\nDespite this setback, Intel continued to innovate, launching the Pentium Pro in 1995, which catered to workstations and servers. The company thrived during this period, benefiting from the rise of Windows 95 and the growing internet. By the end of 1996, Intel's financial performance had improved significantly, solidifying its position in the tech industry.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "983d11f977fb37a9",
    "title": {
      "en": "600M years of shared environmental stress response found in algae and plants",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://phys.org/news/2025-03-million-years-environmental-stress-response.html",
    "score": 96,
    "by": "PaulHoule",
    "time": 1742567418,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "737094bdcb6123f7",
    "title": {
      "en": "EuroTcl2025/OpenACS conference: 10-11 July, Bologna, Italy",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://openacs.km.at/evaluate/org/129998253/conferencenews/",
    "score": 5,
    "by": "cmacleod4",
    "time": 1742897893,
    "content": "OpenACS and Tcl/Tk Conference 2025\n10 - 11 July 2025, Bologna (I)\n\nOpenACS and Tcl/Tk Conference 2025: July 10th - July 11th, 2025\n\nAfter the success in 2022, 2023 and 2024, we are proud to again announce the joint OpenACS and Tcl/Tk Conference 2025. This event will combine the 9th OpenACS conference and the 21th European Tcl/Tk Users Meeting (EuroTcl).\n\nRegister now!\n\nTo the conference\n\nSubmission of abstract\n\nDate\n\nJuly 10th - July 11th, 2025\n\nLocation\n\nBologna, Italy\n\nVenue\n\nRoyal Hotel Carlton, Via Montebello, 8, 40121 Bologna, Italy\n\nHotel Recommendations\n\nRoyal Carlton Hotel (Conference Venue), Prices: € 154,- for a single, € 174,- (plus tax)for a double room, please book via: carlton@monrifhotels.it,Please refer to \"OACS CONFERENCE\" when booking\n\nHotel Internazionale,Prices: € 154,- for a single, € 164,- (plus tax)for a double room, please book via:internazionale@monrifhotels.ituntil 23rd of May.\n\nAirport Shuttle\n\nMarconi Express\n\nParticipation Fee\n\nUpon registration we will charge a participation fee in the amount of € 180,- including VAT.\n\nContact\nconference2025@openacs.org\n\nTentative Dates (to be confirmed)\n\n\tMarch 12th, 2025: Registration opens\n\tJune 17th, 2025: Deadline for submissions of abstracts (max. 1.000 words, min. 50 words);\n\tJune 19th, 2025: Notification of acceptance; tentative program\n\tJune 30th, 2025: Registration ends\n\tJuly 9th, 2025: Meet & greet\n\tJuly 10th - July 11th, 2025: Conference\n\nCommittees\n\nConference Chairs\n\nGustaf Neumann (Vienna University of Economics and Business)\nStefan Sobernig (Vienna University of Economics and Business)\nHarald Oehlmann (ELMICRON)\n\nOrganization Committee\n\nAntonio Pisano (Vienna University of Economics and Business)\nBernd Simon (Knowledge Markets)\nStefan Sobernig (Vienna University of Economics and Business)\n\nWe are looking very much forward to seeing you in Bologna!\n\nHeader Image: \"Torri di Bologna\" by Aneta Malinowska, Wikimedia Commons\n\n.conf-header {\n    background: url(https://openacs.org/conf2025/info/download/file/bologna.jpg)\n      no-repeat center center;\n    background-color: #f4f0ed;\n    margin: -1rem -2rem 2rem -2rem !important;\n    padding-top: 0px;\n    height: 400px;\n  }\n\n  .company-container {\n    margin: 0 !important;\n    padding: 0 !important;\n    max-width: 100% !important;\n    overflow: hidden;\n  }\n\n  .slogan {\n    margin-top: 1rem;\n    padding-top: 2rem;\n    padding-bottom: 2rem;\n  }\n\n  h1.slogan {\n    background-color: rgba(0, 0, 0, 0.4);\n    color: white;\n    font-size: 3rem;\n  }\n\n  #header_regular {\n    margin-bottom: 0 !important;\n  }\n\n @media (max-width: 768px) {\n  h1.slogan {\n    font-size: 2rem; /* Kleinere Schriftgröße für Handys */\n  }\n}",
    "summary": {
      "en": "**OpenACS and Tcl/Tk Conference 2025 Summary**\n\n- **Dates**: July 10-11, 2025\n- **Location**: Royal Hotel Carlton, Bologna, Italy\n- **Event**: This conference combines the 9th OpenACS conference and the 21st European Tcl/Tk Users Meeting (EuroTcl).\n\n**Registration**:\n- Opens: March 12, 2025\n- Abstract Submission Deadline: June 17, 2025\n- Notification of Acceptance: June 19, 2025\n- Registration Ends: June 30, 2025\n- Participation Fee: €180 (includes VAT)\n\n**Hotel Recommendations**:\n1. **Royal Carlton Hotel**: €154 for a single room, €174 for a double.\n2. **Hotel Internazionale**: €154 for a single room, €164 for a double (until May 23).\n\n**Contact**: For inquiries, email conference2025@openacs.org.\n\nLooking forward to seeing everyone in Bologna!",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "f887ef31119c7994",
    "title": {
      "en": "The Prospero Challenge",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.mattkeeter.com/projects/prospero/",
    "score": 95,
    "by": "jstanley",
    "time": 1742807083,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "d68321f58287208e",
    "title": {
      "en": "Can Parrot Brains Teach Us About Human Speech?",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.smithsonianmag.com/smart-news/can-parrot-brains-teach-us-about-human-speech-study-finds-budgies-have-language-producing-regions-that-resemble-our-own-180986282/",
    "score": 27,
    "by": "ulrischa",
    "time": 1742580619,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "7dab4e26703d5c25",
    "title": {
      "en": "Mastering Delphi 5 2025 Annotated Edition Is Now Complete",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://blog.marcocantu.com/blog/2025-march-mastering-delphi5-annotated-complete.html",
    "score": 190,
    "by": "mariuz",
    "time": 1742830808,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "cd0595c431043e34",
    "title": {
      "en": "A sodium-ion portable power bank comes to market",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.theverge.com/news/631357/elecom-power-bank-battery-sodium-ion",
    "score": 41,
    "by": "namanyayg",
    "time": 1742699433,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "0894f185902b17cb",
    "title": {
      "en": "Project Operation Whitecoat (2010)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://scholarworks.lib.csusb.edu/cgi/viewcontent.cgi?article=1201&context=history-in-the-making",
    "score": 103,
    "by": "smegger001",
    "time": 1742834107,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "3dda6c3c9002f24d",
    "title": {
      "en": "Chrono Trigger Still Blows Me Away 30 Years Later",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://kotaku.com/chrono-trigger-jrpg-snes-30th-anniversary-square-1851769192",
    "score": 53,
    "by": "PaulHoule",
    "time": 1742911471,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "ffd247af135b4fee",
    "title": {
      "en": "Teardown, Optimization: Comsol 8Gb USB Flash Stick (2015)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://goughlui.com/2015/04/05/teardown-optimization-comsol-8gb-usb-flash-stick-au6989sn-gt-sdtnrcama-008g/",
    "score": 21,
    "by": "userbinator",
    "time": 1742546665,
    "content": "← Unintentional Teardown: HP iPAQ h5550 Windows Mobile 2003 PDATrip to HK & CN 2014 – Part 6: Being a Tourist – Big Bus Tour (Part 1) →Teardown, Optimization: Comsol 8Gb USB Flash Stick (AU6989SN-GT & SDTNRCAMA-008G)\nPosted on April 5, 2015 by lui_goughA while back, I received a Comsol 8Gb USB Flash Stick for a test. As it turns out, I’ve managed to grab another, so I felt less bad about breaking one apart to work out what’s inside – and as it turns out, it provided me a world of entertainment for the weekend. It was more than I expected, and the optimization process is something engineers (like myself) really get excited about.Teardown and ExperimentsI’m sad to say that the teardown of this product is a relatively destructive process. The main PCB itself is held inside a thin aluminium “tube” by two glued-in translucent plastic end-pieces. Cutting the pieces and twisting them out with pliers was the only method to free the internal PCB, which revealed some interesting results.<img fetchpriority=\"high\" decoding=\"async\" class=\"alignnone size-large wp-image-12067\" src=\"https://goughlui.com/wp-content/uploads/2015/04/DSC_4056-1024x301.jpg\" alt=\"DSC_4056\" width=\"640\" height=\"188\" srcset=\"https://goughlui.com/wp-content/uploads/2015/04/DSC_4056-1024x301.jpg 1024w, https://goughlui.com/wp-content/uploads/2015/04/DSC_4056-300x88.jpg 300w\" sizes=\"(max-width: 640px) 100vw, 640px\" />The PCB is marked C20V-2.0-AU6989-L48-2L-TF-1.5 and is dated 26th June 2014, making this a relatively recent design. A provision is made for a front LED, which is unused, instead in preference of a rear LED. A provision is made for a crystal oscillator, but this is unpopulated because this uses an Alcor Micro AU6989SN-GT controller with integrated crystal.Features, excerpted from usbdev.ru, are:– PCBs are pin compatible with AU69XX USB2.0 series\n– Integrated build-in Regulator\n– Integrated build-in Crystal with Alcor’s patent\n– Supports 72bit/1K BCH ECC engines\n– Supports new generation MLC/TLC flash\n– Supports Toggle/ONFI DDR flash\n– Not support the flash ECC requirement under 24bit/1K\n– Well performance in TLC DDR flash\n– Improved read performance reach 32MB/Sec\n– Integrates hardware DMA engine to tune up the operation performance\n– Works with default driver under the environments of Windows ME, Windows 2000, Windows XP, Vista, Window7, Windows 8, Mac 9.2, Mac OS 10.x. Using Alcor Micro’s vendor driver for the environment under Windows 98SE.\n– Low power operation with SDR/DDR flash\n– Supports software write protection\n– Support Auto Run function\n– Support erasable and read-only mode AP Disk\n– Companion application program with UFD – iStar available for users\n– – To have UFD partition management function\n– – To do password protection for the security in data access\n– – To guard data files with software write protection function\n– – To lock up PC by UFD as the key\n– Available in 48-pin LQFP 7x7mm / TQFP_7x7mm / QFN_6x6mm / QFN_7x7mm package to support 4CE pin flashx2pcs\n– Available in 64-pin LQFP 7x7mm / TQFP_7x7mm package to support 4CE pin flashx4pcsIt’s a very feature rich controller, but that’s what you would expect given the competitors (e.g. Phison) which offer similar features in the value segment.<img decoding=\"async\" class=\"alignnone size-large wp-image-12068\" src=\"https://goughlui.com/wp-content/uploads/2015/04/DSC_4058-1024x301.jpg\" alt=\"DSC_4058\" width=\"640\" height=\"188\" srcset=\"https://goughlui.com/wp-content/uploads/2015/04/DSC_4058-1024x301.jpg 1024w, https://goughlui.com/wp-content/uploads/2015/04/DSC_4058-300x88.jpg 300w\" sizes=\"(max-width: 640px) 100vw, 640px\" />The rear offers a big surprise. The flash is a soldered down single microSDHC card! The card isn’t marked with a manufacturer but does have some etching on it. A space for a surface mount crystal is seen underneath as well.I know what you’re thinking – this is just a card reader and microSDHC combination, minus the connector, but you’d be wrong. The controller is a flash controller, and desoldering reveals an even more interesting outcome. Out comes the hot air gun!<img decoding=\"async\" class=\"alignnone size-large wp-image-12069\" src=\"https://goughlui.com/wp-content/uploads/2015/04/DSC_4060-1024x712.jpg\" alt=\"DSC_4060\" width=\"640\" height=\"445\" srcset=\"https://goughlui.com/wp-content/uploads/2015/04/DSC_4060-1024x712.jpg 1024w, https://goughlui.com/wp-content/uploads/2015/04/DSC_4060-300x209.jpg 300w\" sizes=\"(max-width: 640px) 100vw, 640px\" />Only the power connections seem to be used on the microSDHC’s pin, and instead, pads are used to talk to the card. A total of 21 pin connections are made, but I’m not sure what they are.Maybe playing with the microSDHC card would allow us to read the data from it, offering a simple way to recover the data from the card – turns out this doesn’t work. The card is identified by my reader as a 24/32Mb card and does not read properly. It appears unformatted. The read speed is 10.8Mb/s.<img decoding=\"async\" class=\"alignnone size-full wp-image-12081\" src=\"https://goughlui.com/wp-content/uploads/2015/04/comsol-card.png\" alt=\"comsol-card\" width=\"595\" height=\"520\" srcset=\"https://goughlui.com/wp-content/uploads/2015/04/comsol-card.png 595w, https://goughlui.com/wp-content/uploads/2015/04/comsol-card-300x262.png 300w\" sizes=\"(max-width: 595px) 100vw, 595px\" />I also decided to go one step further and try to extract the CID and CSD information:CID: 035344534430333280ffffffff0062c5\nCSD: 00260032515981e9bef9cfff92404053The CID and CSD provide some very unusual information. The CID gives the manufacturer ID as 0x03, which is Sandisk. The application ID of 0x5344 is also a common Sandisk trait. The Product name is SD032 with revision 8.0, which would suggest 32Mb (or Gb). Serial number seems to be unset (all oxFF’s), with date set as Feburary 2006. This is paired up with a version 1 CSD, with device size of 1958, equaling 32,096,256 bytes – so 32MiB.As a result, it looks like this is a Sandisk part, with an SD controller, but it hasn’t gone through the final manufacturer certification and formatting procedure. Instead, it is integrated in a product using the pads on the underside as a raw NAND package, ignoring the on-board SD controller, and thus takes the format of the controller talking to the raw NAND.How can we prove it’s a Sandisk part? Well, here’s some photos of a similar Sandisk part, as a microSDHC card, sold under the Sandisk brand.<img decoding=\"async\" class=\"alignnone size-medium wp-image-12066\" src=\"https://goughlui.com/wp-content/uploads/2015/04/DSC_4122-300x219.jpg\" alt=\"DSC_4122\" width=\"300\" height=\"219\" srcset=\"https://goughlui.com/wp-content/uploads/2015/04/DSC_4122-300x219.jpg 300w, https://goughlui.com/wp-content/uploads/2015/04/DSC_4122.jpg 846w\" sizes=\"(max-width: 300px) 100vw, 300px\" /> <img decoding=\"async\" class=\"alignnone size-medium wp-image-12070\" src=\"https://goughlui.com/wp-content/uploads/2015/04/DSC_4121-300x217.jpg\" alt=\"DSC_4121\" width=\"300\" height=\"217\" srcset=\"https://goughlui.com/wp-content/uploads/2015/04/DSC_4121-300x217.jpg 300w, https://goughlui.com/wp-content/uploads/2015/04/DSC_4121.jpg 857w\" sizes=\"(max-width: 300px) 100vw, 300px\" />Notice how the etched numbers match the same font-spacing style? Of course the branding hasn’t been printed and an extra etching has been made on the spine, which may be due to special binning (C grade?). The underside also features the pads, but covered by some thick paint of some sort. Now I know what those pads are for!Getting out the hot air gun again, I was able to resolder the package onto the board without damaging it – it still works just fine. So why was this exciting? It’s because, just like the Phison USB keys I had played with earlier prior to blogging, the manufacturer’s tools are available (albeit though shady channels) to the public which allows for some interesting insight into the manufacturing and optimization process.AlcorMP SoftwareThe software for certifying and manufacturing these USB keys is called AlcorMP. An archive of versions of the software is available from usbdev.ru. The software is capable of running in English and Chinese, but the user guide is in Chinese only, so I’ve pretty much done my exploration without much help from it.<img decoding=\"async\" class=\"alignnone size-large wp-image-12078\" src=\"https://goughlui.com/wp-content/uploads/2015/04/NewAlcorMP2-1024x618.gif\" alt=\"NewAlcorMP2\" width=\"640\" height=\"386\" srcset=\"https://goughlui.com/wp-content/uploads/2015/04/NewAlcorMP2-1024x618.gif 1024w, https://goughlui.com/wp-content/uploads/2015/04/NewAlcorMP2-300x181.gif 300w\" sizes=\"(max-width: 640px) 100vw, 640px\" />To work properly with this particular USB key, which uses a very late controller, you must use AlcorMP Version 15.03.05.00 or newer. I tried a Version 14 with no luck, due to the flash not being supported. By default, it will run with administrative rights so it can hook a special driver to talk to the keys and reprogram them. The program has a large set of accompanying .bin files, which seem to contain code to run on the controller itself, which makes this controller a possible safety issue, as noted for Phison controllers in the BadUSB exploit. The fact the controller can be so easily reprogrammed is a boon for those making fake flash keys.These files perform the low-level format, testing and provide the firmware for the controller as well as some special partition tools for those who wish to use the iStar features.This is like an engineer’s playground – a tool made by engineers for engineers. The main screen of the tool looks like this, and at the moment, it is performing a low-level format. Normally, inserting the key will have it show up its flash ID and other information, and would load “sensible” processing settings based on the processing that was done on the unit at the factory.Looking for the configuration details of the Comsol gives us the following data:<img decoding=\"async\" class=\"alignnone size-full wp-image-12073\" src=\"https://goughlui.com/wp-content/uploads/2015/04/ConfigInfo.gif\" alt=\"ConfigInfo\" width=\"343\" height=\"557\" />The drive is formatted with a fixed capacity of 7450Mb and has firmware version 1600. The VID and PID seem to be customized as well, but the vendor/product strings are empty, which explains the no-name nature of the drive in the HDTune Pro tests prior.To begin the certification process, you will need to click on the Setup button. This brings up a password prompt, of which no entry is required – just click OK.<img decoding=\"async\" class=\"alignnone size-full wp-image-12074\" src=\"https://goughlui.com/wp-content/uploads/2015/04/Flash-Config.gif\" alt=\"Flash-Config\" width=\"645\" height=\"439\" />The first screen is a relatively cluttered one but it sets up the flash configuration. The flash is auto-detected from the ID bytes as a Sandisk SDTNRCAMA-008G. Leaving the number, channels, and cycle time at defaults seems to be sufficient for this drive.The manufacturing process can be changed to optimize the drive for Speed, or for Capacity. There is a third option which seems to be for a high level format based on pre-existing bad block marked by the NAND manufacturer. You should really use either Capacity or Speed as the bad block data from the manufacturer is probably damaged.A low level format will test the flash and make sure the flash is usable. This is desirable if you have a fake flash drive to determine its correct capacity. The check mode is in LLF Check, with several levels of thoroughness. I chose Disturb Check because this seems to be more thorough, and checks for adjacent flash cell disturbance, whereas the other tests don’t address this (but can be sufficient). I also selected half-cap check after to ensure a thorough test. The scan level can have an impact on robustness, with Full Scans taking longer but ensuring all the flash is tested – Full Scan 4 is most thorough as far as I know. The ECC level can be set between 0 to 15 – this was one setting which caught my eye and I looked to understand and optimize it further in the next part.The special flash section is there to deal with particular types of flash with quirkiness/compatibility issues, and should be left at Normal unless you have particular errors during manufacturing.The advanced button brings up a new dialogue with more features –<img decoding=\"async\" class=\"alignnone size-full wp-image-12076\" src=\"https://goughlui.com/wp-content/uploads/2015/04/LLF-Enhance.gif\" alt=\"LLF-Enhance\" width=\"434\" height=\"381\" />The ECC enhance level feature is normally off, but can be turned on to improve stability of the result. Low level format revise can increase the scan time by running the low level format loop a few times to improve stability, but is generally unnecessary, as the remaining ECC should be able to handle any marginal blocks that may have passed.Pattern controls the test pattern used in low level formatting, and driving level configures the signal drive for the chips. The MaxL1fCE seems to do with flash chip enables, and should be left at the default setting. I’m not sure what LC Offset is used for. Sync Mode may be indicative of the flash interface being run in synchronous mode – but I’m not entirely sure.Strengthen the stability should be left to default – enabling this brings up a warning that capacity and speed will be sacrificed for stability, which is probably not necessary for normal usage.Use Block Mode controls which blocks are used. This can be changed to odd or even blocks for “salvaging” bad flash chips. Cache program can be enabled or disabled, but I’m not sure what this actually does – so I did try to see if it does anything in a later part.<img decoding=\"async\" class=\"alignnone size-full wp-image-12077\" src=\"https://goughlui.com/wp-content/uploads/2015/04/Mode-Select.gif\" alt=\"Mode-Select\" width=\"645\" height=\"439\" />The next screen across allows you to select the mode which the drive appears as – whether it’s a removable disk, fixed disk, read-only, password protected, or U3-style CD-ROM. You can pre-set some formatting parameters for the drives and the images to be pre-loaded. The LED behaviour can be customized as well. The use of the U3 style CD-ROM can be used to turn the drive into a USB CD-ROM drive for installing OSes which don’t understand USB installation (e.g. Windows XP), or for storing things read-only (without using the other read-only features of the drive)<img decoding=\"async\" class=\"alignnone size-full wp-image-12080\" src=\"https://goughlui.com/wp-content/uploads/2015/04/VID-Setup.gif\" alt=\"VID-Setup\" width=\"645\" height=\"439\" />The information tab allows you to set the VID, PID and strings – so you can customize the “name” of the drive in Device Manager. That can be a pretty good party trick.<img decoding=\"async\" class=\"alignnone size-full wp-image-12071\" src=\"https://goughlui.com/wp-content/uploads/2015/04/Bad-Block-Set.gif\" alt=\"Bad-Block-Set\" width=\"645\" height=\"439\" />The bad block configuration screen gives you the opportunity to set how the flash is configured. Auto check optimizes the drive size based on how much flash is actually workable. This can be potentially dangerous, as it may not leave any spare blocks for replacement should blocks fail during “runtime“. That being said, I’m not sure the Alcor Micro is capable of doing dynamic block replacements.Dynamic Set leaves some blocks for reserve, whereas bin allows the system to automatically optimize with one target or another, and decide which “regular” capacity to allocate a drive to based on the workable flash. This is interesting as it implies there could be very odd-sized flash keys out there – maybe a 4Gb key that has 6.8Gb of workable flash?<img decoding=\"async\" class=\"alignnone size-full wp-image-12072\" src=\"https://goughlui.com/wp-content/uploads/2015/04/Capacity-Bin.gif\" alt=\"Capacity-Bin\" width=\"686\" height=\"490\" />I’d have to say that the majority of manufacturers probably don’t use this mode, and use the fix capacity mode instead – either a drive passes or fails to provide a set capacity, and that’s the end of the day (7450Mb in case of this model). The final mode sets a fixed number of blocks as a percentage as “bad”.Other settings include the format file system (you don’t get a choice really), and options which help in real production usage (up to 32 simultaneous drives qualified using the same machine). There is one interesting ATTO Optimize feature which suggests there are a few tweaks to make the drive benchmark better. The drive can be formatted with MBR or as VFAT (which isn’t reliably bootable, but gives a tiny bit more space). The Enable Reader feature allows other chipsets with integrated SD readers to have the slot “usable”, and MaxMPTime allows for production to be limited to a certain amount of time or fail.<img decoding=\"async\" class=\"alignnone size-full wp-image-12075\" src=\"https://goughlui.com/wp-content/uploads/2015/04/Flash-Type-Power.gif\" alt=\"Flash-Type-Power\" width=\"645\" height=\"439\" />The other page allows you to customize flash, and do multiple loop burn-in tests. The power bits can be adjusted to make it more acceptable to the end user’s requirements. I have no idea what AutoH2 does, but the write log option provides a quick listing in a text file of the results of drive optimization.<img decoding=\"async\" class=\"alignnone size-full wp-image-12079\" src=\"https://goughlui.com/wp-content/uploads/2015/04/UI-Show.gif\" alt=\"UI-Show\" width=\"645\" height=\"439\" />The UI Show features are really only useful for those in production environments if they want to standardize on a particular look or colour coding – the home user can live with the default.Speed/Capacity, and ECC OptimizationThe controller itself advertises support for 72 bit/1K BCH ECC, and no less than 24 bit/1K ECC. The setting for ECC being from 0 to 16 was a little puzzling, so I tried to consult the manual with some digital translation help.The translated sections for ECC using Google Translate reads as follows:ECC set\nFLASH poor quality need to be open for FLASH bad block ECC error correction can improve certain capacity, but\nThere may be some risks. ECC = 0 most stringent low grid FLASH out the most stable; ECC = 15 most relaxed,\nCapacity may be larger, but there may be some risks.\nThe original low-grid setting value refers to a low-level format ECC on the selected use.ECC tuning levels: Level 1-4, may be appropriate to increase the capacity of FLASH, the proposed selection level 1.\nLow grid correction: low grid ECC scan times can make more accurate, but it takes a little more time, check only takes effect.\nScan times: You can manually set the number of low grid scanning, you can make a more accurate scan, but it takes a little more time,\nCheck only takes effect.\nPatten: Patten can choose different scans, mainly for the more special flash.\nUse Block Mode: manually choose to do the entire block or block or even-odd block.\nCache Program: Open or closed manually select cache program command.Using Bing Translator gives me a very similar result:ECC setting\nLow quality FLASH needs to open up FLASH bad block by ECC error correction, guaranteed capacity can be improved, but\nThere may be a certain amount of risk. ECC=0 is the most strict, low FLASH the most stability; ECC=15 is the most relaxed,\nCapacity may be larger, but there may be a certain degree of risk.\nLow setting refers to the use of a low-level format on the chosen ECC values.ECC tuning level: level 1-4, may be appropriate to improve the capacity of the FLASH, choose level 1.\nCorrection: low several times makes the ECC scanning is more accurate, but will spend more time, check the do not take effect.\nScan frequency: low the number of scans that can be manually set, can make the scan is more accurate, but will spend more time,\nCheck the do not take effect.\nPatten: you can select a different scan Patten, mainly for very special Flash.\nUse Block Mode: manually choose to do the full block or even block or odd block.\nCache Program: choose to turn on or turn off the cache manually program command.As a result, it seems that the ECC setting sets the tolerance to block errors during flash low level formatting. To verify this, I decided to run a low level format at every setting (taking an hour each setting) forboth capacity and speed optimize. ECC Tuning was disabled. Random pattern was used during testing (resulting in slightly random variances in formatted size), and Disturb test with Full Scan 4 was used. The results were as follows:<img decoding=\"async\" class=\"alignnone size-full wp-image-12082\" src=\"https://goughlui.com/wp-content/uploads/2015/04/devsize-vs-ecc.png\" alt=\"devsize-vs-ecc\" width=\"932\" height=\"736\" srcset=\"https://goughlui.com/wp-content/uploads/2015/04/devsize-vs-ecc.png 932w, https://goughlui.com/wp-content/uploads/2015/04/devsize-vs-ecc-300x237.png 300w\" sizes=\"(max-width: 932px) 100vw, 932px\" />The results seem to follow the description to some extent with some surprises. For one, the optimize for capacity option did not significantly outperform optimize for speed, and performed worse at low ECC level (strict). The capacity of most ECC levels above 3 were fairly similar, around 7800MB+, which is about 350-450Mb more than the fixed capacity it was shipped with. The number of bad blocks identified varies somewhat from ECC level to ECC level, probably due to random pattern test variation in detecting errors, coupled with some potential wear-out and alignment differences during test.In general, it seems like the ECC level represents the number of bad bits tolerated in a flash block/page during low level formatting before the block/page is marked bad and taken out of use. Hence, lower levels are stricter, and higher levels would allow for more defective bits in the low level format (leaving less margin for wear in the future, making it less stable).Because of the quality of the flash, the capacity is maximised even with fairly strict levels of ~3. Higher levels don’t seem to restore much more capacity, which implies that the bad blocks must contain bursts of bad cells which are uncorrectable even with wider bad-bits tolerance.It’s probably best to certify for a fixed size a bit smaller, if you want to give some room for reallocations (provided the controller actually supports it).So, what’s the cost of Capacity optimization versus Speed? Well, as it turns out, the speed is much better in speed mode – it’s about 84% faster in read and 44% faster in write than the shipped status. Part of the improvement seems to be new firmware – as the drive identifies with firmware version 8E8A using the AlcorMP tool.<img decoding=\"async\" class=\"alignnone wp-image-12085\" src=\"https://goughlui.com/wp-content/uploads/2015/04/Capacity-Optimized-CDM.png\" alt=\"Capacity Optimized CDM\" width=\"317\" height=\"287\" srcset=\"https://goughlui.com/wp-content/uploads/2015/04/Capacity-Optimized-CDM.png 406w, https://goughlui.com/wp-content/uploads/2015/04/Capacity-Optimized-CDM-300x272.png 300w\" sizes=\"(max-width: 317px) 100vw, 317px\" /> <img decoding=\"async\" class=\"alignnone wp-image-12086\" src=\"https://goughlui.com/wp-content/uploads/2015/04/Speed-Optimized-CDM.png\" alt=\"Speed Optimized CDM\" width=\"317\" height=\"287\" srcset=\"https://goughlui.com/wp-content/uploads/2015/04/Speed-Optimized-CDM.png 406w, https://goughlui.com/wp-content/uploads/2015/04/Speed-Optimized-CDM-300x272.png 300w\" sizes=\"(max-width: 317px) 100vw, 317px\" />No more nasty 13.78Mb/s read, and 3.27Mb/s write! It’s still no speed demon, but given the Sandisk card I had with the same sort of shape was a Class 4 card, the performance is probably the best we can expect from the flash. I did change the write-cycle time to the minimum setting, hoping to “push” the flash faster, but it made no difference.ATTO Optimization and Cache Program\nOne other option intrigued me, and that was optimization for ATTO, a commonly used dish benchmark. Was this some sort of cheating, or a deliberate bias towards small block accesses at the expense of sequential access? Was there any tradeoff?The other wonder was what the Cache Program option meant. Did this mean there would be an pSLC cache on the drive or some sort of attempt at optimizing for small transfers? Or was it just an option to cache production firmware on the drives themselves? Did it have any performance impact?<img decoding=\"async\" class=\"alignnone size-full wp-image-12088\" src=\"https://goughlui.com/wp-content/uploads/2015/04/Capacity-Optimized-ATTO-No-Opt.png\" alt=\"Capacity Optimized ATTO No Opt\" width=\"482\" height=\"615\" srcset=\"https://goughlui.com/wp-content/uploads/2015/04/Capacity-Optimized-ATTO-No-Opt.png 482w, https://goughlui.com/wp-content/uploads/2015/04/Capacity-Optimized-ATTO-No-Opt-235x300.png 235w\" sizes=\"(max-width: 482px) 100vw, 482px\" />Capacity Optimized, No ATTO Optimization<img decoding=\"async\" class=\"alignnone size-full wp-image-12089\" src=\"https://goughlui.com/wp-content/uploads/2015/04/Speed-Optimized-ATTO-No-Opt.png\" alt=\"Speed Optimized ATTO No Opt\" width=\"482\" height=\"615\" srcset=\"https://goughlui.com/wp-content/uploads/2015/04/Speed-Optimized-ATTO-No-Opt.png 482w, https://goughlui.com/wp-content/uploads/2015/04/Speed-Optimized-ATTO-No-Opt-235x300.png 235w\" sizes=\"(max-width: 482px) 100vw, 482px\" />Speed Optimized, No ATTO OptimizationOn the whole, with no ATTO optimization, aside from slight variations, it seems that the optimize for performance selection provides better performance across the board.<img decoding=\"async\" class=\"alignnone size-full wp-image-12090\" src=\"https://goughlui.com/wp-content/uploads/2015/04/Speed-ATTO-Optimized-ATTO.png\" alt=\"Speed+ATTO Optimized ATTO\" width=\"482\" height=\"615\" srcset=\"https://goughlui.com/wp-content/uploads/2015/04/Speed-ATTO-Optimized-ATTO.png 482w, https://goughlui.com/wp-content/uploads/2015/04/Speed-ATTO-Optimized-ATTO-235x300.png 235w\" sizes=\"(max-width: 482px) 100vw, 482px\" />Speed Optimized, ATTO Optimization ONTurning on ATTO Optimization doesn’t seem to do much at all. There’s a few slight increases across the board, but I suppose this flash chip isn’t particularly capable and neither is the single channel design.<img decoding=\"async\" class=\"alignnone size-full wp-image-12091\" src=\"https://goughlui.com/wp-content/uploads/2015/04/Speed-ATTO-Cache-ATTO.png\" alt=\"Speed+ATTO+Cache ATTO\" width=\"482\" height=\"615\" srcset=\"https://goughlui.com/wp-content/uploads/2015/04/Speed-ATTO-Cache-ATTO.png 482w, https://goughlui.com/wp-content/uploads/2015/04/Speed-ATTO-Cache-ATTO-235x300.png 235w\" sizes=\"(max-width: 482px) 100vw, 482px\" />Speed Optimized, ATTO Optimization ON, Cache ONTurning on cache didn’t make a lick of difference either, so I might as well leave that at default. It wasn’t the option I hoped it was.I also used H2testw and CrystalDiskMark as benchmarks to find out just how far we’ve improved from the baseline case of “as shipped” and the impact of each of the options. It seems that optimize for speed gives a good boost overall, and ATTO optimization provides a slight advantage. Cache didn’t have much of a notable impact, except for a strange reduction in 512kB writes in CDM and a slight increase in H2testw writes.My finalized settings including ECC Enhance Level 1, with LLF Revise and Dynamic Set (4) was tested as well on the rightmost column, as my “final” optimized result. It doesn’t seem that these options, which affect the LLF process, actually affects the final drive performance beyond that of normal “test to test variation”. Not bad for a “cost free” solution.<img decoding=\"async\" class=\"alignnone size-full wp-image-12095\" src=\"https://goughlui.com/wp-content/uploads/2015/04/Performance-Summary.png\" alt=\"Performance Summary\" width=\"733\" height=\"223\" srcset=\"https://goughlui.com/wp-content/uploads/2015/04/Performance-Summary.png 733w, https://goughlui.com/wp-content/uploads/2015/04/Performance-Summary-300x91.png 300w\" sizes=\"(max-width: 733px) 100vw, 733px\" />Of note was that the drive was stable enough to pass H2testw in the fresh state even at varying format capacities depending on the run. This doesn’t indicate the drive will be stable in the future, as the flash wears, hence my recommendation to use stricter ECC (than the default 8 used by this manufacturer) and dynamic set to maximise the storage and improve the “safety” margin.I even tried being very negligent – doing a manufacture run at ECC 15, with ECC Enhance at 4 (loose), speed optimize, with half-capacity scan, quick-scan 1 selected, and auto size for bad blocks and the resulting unit still passed the H2testw test, however, might be unstable as flash cells wear out. It didn’t yield me any significant capacity gain, and that’s likely due to burst-error accumulation in manufactured flash.ConclusionThe quest for optimization seems to be something in most engineer’s hearts. It’s one reason why I do love overclocking. Playing with a manufacturer’s tool is like overclocking a USB key – performance and capacity was both improved at no cost! It turns a nasty key into one that more closely resembles its advertised “up to 20MB/s” and makes it tolerable for use. Of course, it may not be as stable as otherwise, depending on the options you choose – but now you’re in the driver’s seat and that’s pretty cool.Of course, none of this is without risk, and it’s equally possible to ruin a good drive, brick it or otherwise make the performance or stability worse. You can and will void warranties as well – do all of this at your own risk.In the end, I opted for ECC level 4, plus ECC Enhance Level 1 and LLF Revise ON, cache default and chose to go with Dynamic Set (4) for bad blocks to give some margin for reallocation. That should give me some more capacity, with the stability you would expect.Like it? Share it!FacebookTwitterRedditLinkedInMoreTumblrPocketPinterestWhatsAppTelegramRelatedReview, Teardown: Verbatim 8Gb Store’n’Go PinStripe USB 2.0 Drive (49062)Yet another low-cost USB flash drive gets a review, mainly because I just happened to end up receiving it. Unlike the others though, this one comes from Verbatim which is a brand which has come to be respected due to their reputation built around floppy disks and optical discs. The…December 4, 2016In \"Computing\"Quick Review: Comsol 8Gb USB 2.0 Flash Stick (UF4-8000)USB flash drives are one of the most ubiquitous storage devices, this one was anonymously donated to me for some testing. It's not a particularly special looking drive, but it is one that's available on the market today - it was worth a quick test anyway. The Comsol branding may…February 20, 2015In \"Flash Memory\"Unintentional Teardown & Repair: Kingston 128Gb UHS-I SDXC CardA while back, I posted a gripe article about how I disliked some aspects of the Secure Digital memory card format, one of the issues being the fragility of the plastic shell. Through no fault of my own, it seems it has happened to one of my largest cards. Damn.…May 23, 2015In \"Flash Memory\"Review, Teardown: Corsair Flash Voyager LS 32Gb USB 3.0 Flash DriveCorsair has been a trusted name in DRAM memory for a long time, which resulted in a brand expansion into peripherals, accessories, power supplies and more. It seems like whatever Corsair puts their brand on is a product you can trust. I've always been partial towards Corsair, having owned many…February 28, 2015In \"Computing\"Review, Teardown: Sandisk Cruzer Facet 8Gb USB 2.0 Flash DriveReaders familiar with my penchant for exotic storage would probably stare at me in confusion as to why I would be reviewing such a plain product such as this one? Interestingly enough, as with most things that I do, there is a good reason. The Sandisk Cruzer Facet 8Gb is…January 15, 2016In \"Computing\"Experiment: 8Gb USB Flash Drive Endurance TestFlash memory is, and has been, a commodity item for a while now. Almost everyone has at least a few USB flash drives (sticks) and maybe even a few memory cards. When it comes to rapidly transferring large files between devices, or even storing working documents, USB flash drives are…May 30, 2017In \"Computing\"Review & Teardown: Kogan USB 3.0 Universal Memory Card ReaderLast week, Kogan had a free shipping day, and not being one to let such opportunities slip by, I decided to pick up a Kogan USB 3.0 Universal Memory Card Reader for AU$19. It's a fair price to pay for such a product, similarly priced to my favourite Transcend RDF8.…February 17, 2014In \"Computing\"Teardown, Test: PGH Bricks & Pavers Promotional USB KeyI guess the fact that this post exists is a proof that promotional USB keys \"work\". They serve a useful purpose, and have a decent \"lifetime\", thus helping spread the promotional message. I'm no brick and pavers guy, but I managed to have one of these gifted to me, so…June 30, 2014In \"Flash Memory\"Quick Review, Teardown: “Avoid driving fast card” USB 3.0 Ethernet AdapterIt's always nice to have a few extra Ethernet interfaces on a computer - sometimes you need to run a software firewall, use a computer as a router, or you just need to sniff some network traffic running to a device for some reverse engineering or testing purposes. With the…March 17, 2016In \"Computing\"\n<img alt='' src='https://secure.gravatar.com/avatar/0843025630d91cd79ecc59aa067ac7dd?s=60&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/0843025630d91cd79ecc59aa067ac7dd?s=120&#038;d=mm&#038;r=g 2x' class='avatar avatar-60 photo' height='60' width='60' decoding='async'/>\nAbout lui_gough\nI'm a bit of a nut for electronics, computing, photography, radio, satellite and other technical hobbies. Click for more about me!\n\nView all posts by lui_gough →\nThis entry was posted in Computing, Flash Memory and tagged experiment, flash, flash memory, made-in-china, storage, teardown. Bookmark the permalink.← Unintentional Teardown: HP iPAQ h5550 Windows Mobile 2003 PDATrip to HK & CN 2014 – Part 6: Being a Tourist – Big Bus Tour (Part 1) →\n6 Responses to Teardown, Optimization: Comsol 8Gb USB Flash Stick (AU6989SN-GT & SDTNRCAMA-008G)\n<img alt='' src='https://secure.gravatar.com/avatar/0843025630d91cd79ecc59aa067ac7dd?s=40&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/0843025630d91cd79ecc59aa067ac7dd?s=80&#038;d=mm&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' decoding='async'/>\t\t\t\tlui_gough says:\nApril 11, 2015 at 12:03 am\t\t\t\t\tAfter some playing around, it seems AlcorMP tool is not without some drawbacks. One is that it installs a filter driver into your USB stack named mpfilt and on 64-bit Windows machines after the latest Windows update, you will find that it “breaks” causing all of the USB 2.0 controllers on your machine to fail with an exclamation mark and something about “associated driver failed to start.”To fix this, you will need to remove the mpfilt driver from your LowerFilters. Unless you have a USB 3.0 controller that still works without the driver (where plugging in a keyboard and mouse into those ports lets you take control of the system) or a remote VNC connection, you will have to boot into safe mode first. Get into regedit and do a search for all entries with mpfilt. Whenever you see one that has a name LowerFilters with Data “mpfilt”, delete the LowerFilters entry. DO NOT DELETE ANYTHING ELSE.Reboot your machine and you should be back to normal.\nReply\n<img alt='' src='https://secure.gravatar.com/avatar/f493cc56b8fcaa4eb141bb204b29e0fe?s=40&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/f493cc56b8fcaa4eb141bb204b29e0fe?s=80&#038;d=mm&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' decoding='async'/>\t\t\t\t- dutchman24 - says:\nAugust 1, 2016 at 6:30 pm\t\t\t\t\tThank you I was looking arround the internet to have some detailed information about this program.\nIf you have problems  (like i had) with the USB drivers because of the HUB configuration message you can still operate your pc with a ps2 (purple plug) keyboard. I menaged to restart Windows 10 with shift key pressed down and do a systeme restore and everything went back to normal.\nIts very important to start the AlcorMP program before to insert any USB devices and use USB 2.0 socket and not the 3.0 sockets (blue ones).\nBTW i think that the “Cache” option in the Advanced TAB is to configurate the USB penn to take it out of the PC without deconnecting it. If Cache is disabled it works “stand alone without putting cache into the PC.  If enabled the penn is putting cache in the PC to work faster but you have to deconnect the hardware before you can pull it out of your computer.\nRegards\nReply\n<img alt='' src='https://secure.gravatar.com/avatar/9495d9b367290ba6a36dfacf72276119?s=40&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/9495d9b367290ba6a36dfacf72276119?s=80&#038;d=mm&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' decoding='async'/>\t\t\t\ttc says:\nJuly 18, 2020 at 6:04 pm\t\t\t\t\tOr just use VMware, why would you run this kinda dodgy app on bare metal\nReply\n<img alt='' src='https://secure.gravatar.com/avatar/0843025630d91cd79ecc59aa067ac7dd?s=40&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/0843025630d91cd79ecc59aa067ac7dd?s=80&#038;d=mm&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' decoding='async'/>\t\t\t\tlui_gough says:\nJuly 18, 2020 at 7:48 pm\t\t\t\t\tThere’s a good reason for this. The reconfiguration of the flash controller relies on reprogramming the firmware of the controller itself, which often results in the device resetting and reconnecting to the bus with a new ID to perform various flash verification tasks as a sort of “warm reset”. Many manufacturing tools are very timing sensitive and will throw errors midway after loading the diagnostic code and resetting the controller since it doesn’t “reconnect” to the VM in time or correctly. Sometimes the USB pass-through drivers issue spurious resets to the USB devices which may cause them to lose this diagnostic code (often loaded in RAM temporarily) and reboot as a regular USB stick using the firmware in flash, resulting in no go either.– Gough\nReply\n<img alt='' src='https://graph.facebook.com/v6.0/1950211318481950/picture?type=large' srcset='https://secure.gravatar.com/avatar/d6dc757340a26c503ec3d4bac8c02445?s=80&#038;d=mm&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' decoding='async'/>\t\t\t\tハッピー アニメ マーケット says:\nFebruary 13, 2022 at 3:57 pm\t\t\t\t\tDo you think I can use this to recover my bad sandisk microSD card? the pins seems to match but if not, I can solder from my card to this board.\nReply\n<img alt='' src='https://secure.gravatar.com/avatar/0843025630d91cd79ecc59aa067ac7dd?s=40&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/0843025630d91cd79ecc59aa067ac7dd?s=80&#038;d=mm&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' decoding='async'/>\t\t\t\tlui_gough says:\nFebruary 18, 2022 at 5:52 am\t\t\t\t\tNo. It uses the card as raw flash memory and the controller has its own format which is likely to be incompatible with whatever SanDisk’s own card has as it bypasses it entirely. Without compatible metadata, the USB Flash memory controller would not allow access anyway.– Gough\nReplyError: Comment is Missing!Cancel reply\n\n document.addEventListener('DOMContentLoaded',function(){var commentForms=document.getElementsByClassName('jetpack_remote_comment');for(var i=0;i<commentForms.length;i++){commentForms[i].allowTransparency=!1;commentForms[i].scrolling='no'}})",
    "summary": {
      "en": "The article discusses a teardown and optimization of a Comsol 8Gb USB Flash Stick. The author, an engineer, explores the internal components, discovering that the stick contains a microSDHC card soldered directly onto the main circuit board, which operates with an Alcor Micro AU6989SN-GT controller.\n\nKey points include:\n- The teardown process is destructive, requiring cutting and twisting to access the internal parts.\n- The USB stick uses a recent flash controller that supports various features, including error correction and compatibility with different operating systems.\n- The internal microSDHC card is unbranded and appears to be an unfinished Sandisk product, identified through its internal data.\n- The author uses specialized software (AlcorMP) to optimize the flash drive's performance, which includes adjusting settings for speed and capacity.\n- Testing shows that optimizing for speed greatly improves read and write speeds compared to the original performance.\n- The article emphasizes the risks of modifying USB drives, including potential damage and voiding warranties, while also highlighting the thrill of engineering experimentation.\n\nIn conclusion, the author successfully enhanced the performance of the USB stick through careful adjustments, likening the process to overclocking.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "361dc226f099cf47",
    "title": {
      "en": "Rickover's Lessons",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.chinatalk.media/p/rickovers-lessons-how-to-build-a",
    "score": 114,
    "by": "pepys",
    "time": 1742703526,
    "content": "Share this postChinaTalkRickover’s LessonsCopy linkFacebookEmailNotesMoreDiscover more from ChinaTalkDeep coverage of technology, China, and US policy. We feature original analysis alongside interviews with leading thinkers and policymakers.Over 51,000 subscribersSubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.Already have an account? Sign inRickover’s Lessons\"The status quo has no absolute sanctity\"Lily Ottinger and Charles YangMar 20, 202565Share this postChinaTalkRickover’s LessonsCopy linkFacebookEmailNotesMore1212ShareCharles Yang is the executive director for the Center for Industrial Strategy, a bipartisan think tank focused on industrial policy. Previously, he served as an AI and Supply Chain Policy Advisor at the Department of Energy and was an ML Engineer at an AI hardware startup in San Francisco. Today, he’s here to present some excerpts from his research into how Admiral Hyman Rickover built the nuclear navy.Strategic competition demands more than technological innovation — it requires building industrial power. The U.S. is realizing the damage done by decades of underinvestment in the nation’s industrial base, which now jeopardizes its ability to compete on the global stage. Today, the production capacity of Chinese shipyards is over 200 times that of US shipyards, and China has used its chokehold on critical mineral processing as leverage to retaliate against US sanctions.A new bipartisan consensus is emerging around the need for industrial policy — from the passage of the CHIPS and Science Act, to the recent bipartisan introduction of the SHIPS for America Act and the Critical Minerals for the Future Act.As Congress steps into this more active role, policymakers should learn from the successes of our past. Nearly 75 years ago, Admiral Hyman G. Rickover, “Father of the Nuclear Navy”, pioneered a bold program to develop and operationalize nuclear power in the Navy. Under his leadership, the U.S. government harnessed the power of the atom, building the world’s first nuclear-powered submarine and the world’s largest fleet of nuclear reactors for civilian power.Lessons From the PastRickover spent his entire career in the Navy and is still the longest-serving naval officer in US history. He spent the first 20 years of his career as an electrical engineer, where he honed a strong technical foundation and unique management style. In 1946, he was assigned a 1-year tour of duty at the Oak Ridge site of the Manhattan Project. Rickover immediately recognized the transformative potential of nuclear technology — he spent the rest of his career building the “Nuclear Navy,” which ensured US strategic dominance of the high seas for the rest of the 20th century.Within the span of 10 years, Rickover created an entire office dedicated to nuclear propulsion, and successfully launched the world’s first nuclear-powered submarine without cost overruns. He conclusively demonstrated the strategic importance of nuclear propulsion in a timeframe no one thought possible and helped the US beat the Soviets to nuclear propulsion for submarines by 3 years. His institutional legacy is the US Navy’s safe construction and operation of nuclear reactors.As the US gears up for another strategic competition, Rickover’s story can offer helpful lessons for aspiring technocrats. Oftentimes, industrial policy is framed in terms of legislation, but Rickover demonstrates that industrial policy is as much about policy as it is about strong leadership.USS Enterprise, the world’s first nuclear-powered aircraft carrier. Source.Talent, Training, and ManagementRickover spent an inordinate amount of time focused on interviewing personnel — he made the final hiring decision for every naval officer who applied to serve on a nuclear submarine until he retired. And he was an unorthodox interviewer, screening for high agency individuals who could think on their feet — literally! To test their composure, Rickover famously made candidates sit in a chair with the front two legs shortened as he loomed over them during questioning.The infamous “Rickover chair”One interview account:For one interviewee who said he liked hiking, Rickover asked him if he had ever hiked the nearby “Goat Mountain”. When he said he had not, Rickover told him to bring him proof he had climbed it by tomorrow morning and he would be hired. It turns out that Goat Mountain was the peak of a structure for mountain goats in a zoo. He went to the zoo, asked a tourist to take his picture, jumped into the enclosure, and climbed to the top. He’s hired the next day!But it didn’t end at the interview process — Rickover believed in continued technical training for his staff and in building out a talented workforce base for this new technology:While Rickover worked to staff up quickly in the short term, he also set out to build a deep bench and a long-term pipeline of talent. He required each officer and engineer he hired to submit a self-study plan demonstrating mastery of advanced texts in metallurgy, physics, and chemistry, along with field trips to AEC facilities, totaling 854 hours of study or 16 hours per week. He also worked with MIT to develop a survey course on nuclear physics and a master's degree in nuclear engineering, with a curriculum drawn up and agreed to by Rickover, starting in June of 1949. Rickover also worked with Oak Ridge National Lab to develop a 1-year curriculum in nuclear science and technology, a program christened “Oak Ridge School of Reactor Technology (ORSORT) with the first cohort starting in March 1950. Westinghouse, GE, utilities, naval and private shipyards, and Naval Reactors all sent students to ORSORT and the program started turning out ~100 graduates a year, providing another training center to develop a nuclear industry.1 Finally, Rickover had his engineers provide training lectures to a variety of audiences, ranging from senior officials in BuShips to junior technicians, as well as to explain shipboard problems and applications to scientists at Argonne, Oak Ridge, and Westinghouse/GE.He was also known for his unique style of management. Not only did he interview every naval officer in his office, he also maintained direct lines of communication with every nuclear sub commander and project officer on-site with contractors, giving him early awareness of every issue. The demanding oversight he extended over his technical staff under his command pushed them to have greater awareness of their own direct reports:Rickover was also an intensely demanding and scrutinizing manager. As most writing then was done on carbon copy paper, every night Rickover would collect the “pinks” of every piece of writing from his various teams i.e. the carbon copied half, and read over them at home, including drafts. When his officers protested as to how they should be expected to keep track of everything in their purview, including drafts reports from staff below them, Rickovers responded “It’s up to you to see that I don’t know more about what’s going on in your shop than you do”. By enforcing tight lines of supervision over his officers, Rickover ensured that he maintained full visibility into each team, including the project facilities at Knolls, Bettis, and the shipyards, allowing him to catch problems early on. It also enforced a culture of direct accountability and oversight across the organization.Rickover’s focus on hiring, training, and close project management represented his philosophical approach to how to build complex systems managed by humans.Near the end of his career, Rickover testified to Congress after the Three Mile Island Reactor accident. He spent the vast majority of his testimony talking not about regulatory reform, but about the lack of training and inadequate culture of responsibility among the operators.“Human experience shows that people, not organizations or management systems, get things done. For this reason, subordinates must be given authority and responsibility early in their careers…Complex jobs cannot be accomplished effectively with transients. A manager must make the work challenging and rewarding so that his people will remain with the organization for many years. This allows it to benefit fully from their knowledge, experience, and corporate memory.”~ Hyman Rickover, 1982Industrial State CapacityRickover’s scrutinizing style of management extended to the private companies he worked with. He pioneered the practice of project officers, who lived on-site at the projects and who would report directly to him any delays or unforeseen issues, so that Rickover could escalate immediately and ensure the project remained on track.Government contracting was, and still is today, a largely passive and administrative activity. While Rickover acknowledged that the government was the “customer” and the contractor was the one responsible for delivering, Rickover’s unique approach to program management was exercising tight oversight over the contractors. Rickover hired technical experts into his office and then sent them out as project officers to oversee the various contractor sites. There, the project officer was expected to be the active representative of the Naval Reactors Office, reporting directly to Rickover any issues with contractors and ensuring the contractor was on track to deliver the product as expected. In every sense, Rickover’s project officer was to be his eyes and ears on the ground. Rickover took great pains to ensure there was no customer capture, telling one of his project officers, “Don’t go to dinner with them. Your wives must not get friendly with their wives. You’re not even to let your dogs get friendly with their dogs…when you do that, you become one of them…you don’t represent me anymore”.Rickover’s success in scaling industrial technology was demonstrated early on with Zirconium production. In 1949, the world had only produced a shoebox worth of purified Zirconium, but the material showed promise as a fuel cladding material due to its durability under high temperatures without blocking the emitted neutrons needed to enable fission reactions. AEC opened up a simple contract for private companies to bid to produce Zirconium, but none of the companies were able to scale up production. Rickover took over production a year later, applied his practice of close project management with the (now defunct) Bureau of Mines, and only then passed it off to industry:But by 1949, when Rickover was looking to scale up promising fuel cladding material production, the AEC had already decided to run contracts through another AEC division. Unable to exert the centralized control over the contractors, the AEC manufacturers were slow to scale up a high-quality production process. In 1950, after a year of delay, Rickover finally received permission to have the Westinghouse Bettis site directly manufacture Zirconium metal and worked with the Bureau of Mines (BuMines) to purify the Zirconium. Under Rickover’s scrutiny, Bettis scaled a novel purification process to thousands of tons of production capacity. Rickover opened up contract bids for Zirconium only after having derisked this novel technology. When the Secretary of the Navy later asked Westinghouse how they managed to scale up this process, the response he got was “Rickover made us do it”.“The man in charge must concern himself with details. If he does not consider them important, neither will his subordinates.”~ Hyman Rickover, 1982Bureaucratic InnovationBuilding big things requires lots of people. Rickover was not only an exceptional manager of people and deeply technical, but his 20-year naval career before Oak Ridge taught him how to wrangle government bureaucracy — and discern which rules mattered and which didn’t. For example, Rickover was interviewing an officer who thought the monthly reports on the gasoline usage of his base’s motorboats were pointless and wasteful. Rickover told him to simply remove the tickler file that tracked the reports from the boss’s secretary file and to send over a note the next day alerting Rickover that the task had been completed. The interviewee did and was hired.SubscribeRickover’s bureaucratic skill is exemplified by his success in rallying the Navy behind the nuclear-powered submarine. He believed this was a feasible, near-term project, despite widely-held convictions to the contrary — including those of the Atomic Energy Commission (AEC). Even Robert Oppenheimer (who served as one of the first AEC commissioners) doubted nuclear propulsion early on.In light of initial resistance from the civilian AEC, Rickover formulated a unique bureaucratic innovation to position himself within two chains of command — one within the Navy and the other within the AEC.Rickover was also able to realize his bureaucratic innovation to occupy a spot on the org chart both at AEC and in the Navy BuShips, something he first formulated while at Oak Ridge. This way, if the AEC refused something, he could respond that “this is a priority for the Navy” and vice versa. Similar to how the Manhattan Project reduced risk by pursuing parallel technological approaches, Rickover would reduce his bureaucratic risk by pursuing parallel chains of command. This unique structure lives on to this day, with Naval Reactors shared between the semi-autonomous National Nuclear Safety Administration (NNSA) in the Department of Energy (DOE) and the Navy.Joint org chart for AEC and Navy nuclear reactor efforts“The status quo has no absolute sanctity under our form of government. It must constantly justify itself to the people in whom is vested ultimate sovereignty over this nation”~ Hyman RickoverRickover firmly believed that the right team and the right culture could build incredible industrial technologies at scale, even within the government. While discourse in Washington DC often focuses on regulations or money, Rickover’s life brings a uniquely human-centered view of industrial policy: one that recognizes the importance of state capacity, technical personnel, and most importantly, public leaders with the vision and drive to build technology.You can read the full story of Rickover and how he built the world’s first nuclear-powered submarine on Charles’s substack.Rough DraftsHow Hyman Rickover Built the Nuclear NavyThere is a common misconception that the United States stopped building nuclear reactors after the 1970s. But that is not the full story: the US Navy has consistently built at least one small nuclear reactor every year since the 1950s. The fact that the Navy has managed to safely design, build, and operate nuclear reactors for decades, despite the stagn…Read more22 days ago · 15 likes · 1 comment · Charles YangSubscribe to ChinaTalkHundreds of paid subscribersDeep coverage of technology, China, and US policy. We feature original analysis alongside interviews with leading thinkers and policymakers.SubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.65 Likes∙12 Restacks65Share this postChinaTalkRickover’s LessonsCopy linkFacebookEmailNotesMore1212ShareA guest post byCharles YangPondering possible futuresSubscribe to Charles",
    "summary": {
      "en": "The article discusses the lessons learned from Admiral Hyman Rickover, known as the \"Father of the Nuclear Navy,\" and how his approach to industrial policy can inform current U.S. strategic competition, particularly against China. \n\nKey points include:\n\n1. **Industrial Power Importance**: The U.S. needs to strengthen its industrial base to compete globally, as its shipbuilding capacity is significantly outmatched by China.\n\n2. **Bipartisan Support for Industrial Policy**: There is growing agreement among U.S. lawmakers about the necessity for active industrial policy, highlighted by recent legislation like the CHIPS and Science Act.\n\n3. **Rickover's Achievements**: Rickover successfully developed the first nuclear-powered submarine and established a robust nuclear reactor program, demonstrating the importance of strong leadership and technical expertise.\n\n4. **Hiring and Training**: Rickover focused on selecting capable personnel and emphasized ongoing training, requiring engineers to engage in extensive study and collaboration with educational institutions.\n\n5. **Management Style**: He maintained close oversight of projects and personnel, ensuring accountability and direct communication with his teams, which helped identify problems early.\n\n6. **Bureaucratic Innovation**: Rickover navigated government bureaucracy effectively, creating dual reporting lines to ensure support for nuclear projects from both the Navy and the Atomic Energy Commission.\n\n7. **Human-Centered Approach**: Rickover believed that strong leadership and a dedicated workforce were crucial for industrial success, advocating for a culture of responsibility and continuous improvement.\n\nOverall, Rickover’s methods underscore the need for visionary leadership in building and managing complex technologies in the public sector.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "1e53cc181f8923ea",
    "title": {
      "en": "Peano's Axioms",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://principlesofcryptography.com/number-theory-primer-an-axiomatic-study-of-natural-numbers-peano-axioms/",
    "score": 87,
    "by": "ulugh",
    "time": 1742844365,
    "content": "Categories\n\n\t\t\t\t\tmathematics Number Theory\n\n\t\t\tNumber Theory Primer : An Axiomatic Study Of Natural Numbers – Peano’s Axioms\n\n\t\t\t\t\t\t\t\tPost author\n\n\t\t\t\t\t\t\tBy premmi\n\n\t\t\t\t\t\t\t\tPost date\n\n\t\t\t\t\t\t\tFebruary 1, 2025\n\nauthored by Premmi andBeguène\n\nPrevious Topic: An Axiomatic Study of Numbers\n\nIntroduction\n\nThinking of numbers intuitively brings to mind the simplest and most fundamental set of numbers, namely the set of natural numbers. These numbers are used to count objects like cars, books, pens, etc. If we associate natural numbers such as 1,2,3,1, 2, 3,1,2,3, etc. with counting, then with what corresponding concepts do we relate numbers like −4,3and227-4, \\sqrt{3} \\text{ and } \\frac{22}{7}−4,3and722?\n\nTo reason about all kinds of numbers encountered during our study of mathematics, we need a precise mathematical framework for defining numbers. We will build this framework by first rigorously defining natural numbers axiomatically without relying on the intuitive notion of counting. Then, using this framework as a foundation, we will construct all the other sets of numbers such as integers, rational numbers, real numbers and complex numbers in terms of the natural numbers.\n\nA good axiomatic system assumes as little as possible, while proving as much as possible. To create an efficient axiomatization for the natural numbers, we must distill these numbers to their essential properties. Intuitively, we understand various aspects of the natural numbers, such as their existence and the basic properties of the binary operations addition, multiplication, and the ‘less than’ relation. How few of these concepts can we take as axioms, from which we can deduce everything else that we need to know about the natural numbers? It turns out that remarkably little is required for the axiomatization of the natural numbers—neither addition, nor multiplication, nor the ‘less than’ relation need to be taken as axioms; these will all be constructed from our fundamental axioms.\n\nThe standard axiomatization of the natural numbers, know as the Peano Axioms, was originally formulated by the Italian mathematician, Giuseppe Peano. In 188918891889, Peano developed the real number system based on his axioms for the natural numbers. He defined natural numbers through nine axioms, four of which established the properties of the equality relation “===” with regard to natural numbers, while the remaining five axioms provided a complete and rigorous definition of natural numbers.\n\nIn order to appreciate the intellectual feat of Peano, it’s worth noting that utilizing only his axioms, we are able to prove all the established properties of the natural numbers. Furthermore, these axioms facilitate the construction of the integer, rational, real, and complex number systems.\n\nBefore we discuss Peano’s Axioms in detail, it is a useful exercise to explore an alternative way to describe natural numbers, distinct from the usual intuitive notion of counting. Such an exploration would help us to independently arrive at Peano’s axioms.\n\nIntuition behind the Axiomatic Definition of Natural Numbers\n\nHow do we model the concept of natural numbers denoted by 0,1,2,30, 1, 2, 3 0,1,2,3 and so on without relying on the notion of counting?\n\nOne way would be to take a set-theoretic approach. That is, we can define a set of natural numbers by systematically enumerating the properties and relationships of the members of this set such that the set results in {0,1,2,3,…}\\{0, 1, 2, 3, \\ldots\\}{0,1,2,3,…}.\n\nLet us denote the set of natural numbers as N\\mathcal{N}N. First, we recognize that 000 should be a part of this set. Next, we want 000 to lead us to 111, 111 to 222 and we should be able to continue this way, naming each successive number as far as we wish. This is illustrated below.\n\n0→1→2→3→…0 \\rightarrow1 \\rightarrow 2 \\rightarrow 3 \\rightarrow \\ldots0→1→2→3→…\n\nFrom the above diagram we can see that to model this relationship we need a “next” operation that given a natural number, produces the next natural number in the sequence.\n\nThe above diagram can also be viewed as shown below.\n\n0→11→22→3⋮    ⋮\\begin{equation*}\n\\begin{split}\n0 &\\rightarrow 1 \\\\\n1 &\\rightarrow 2 \\\\\n2 &\\rightarrow 3 \\\\\n\\vdots &\\quad\\,\\,\\,\\, \\vdots \\\\\n\\end{split}\n\\end{equation*} 012⋮→1→2→3⋮\n\nWe can see from the diagram that an input of 000 yields an output of 111, an input of 111 yields 222 and so on. Therefore, we can model the “next” operation as a function S:N→NS : \\mathcal{N} \\rightarrow \\mathcal{N}S:N→N that takes a natural number as input and produces a natural number as output. Here, the letter SSS stands for ‘successor’ and we have S(0)=1,S(1)=2S(0) = 1, S(1) = 2S(0)=1,S(1)=2 and so forth. We will refer to this function SSS as the successor function since it establishes a succession within the set of natural numbers, N\\mathcal{N}N. This function SSS is illustrated below.\n\nFrom the diagram above, we can observe that the successor function SSS has the following properties:\n\nNot Surjective: There is no natural number in N\\mathcal{N}N that, when given as input to the successor function SSS, results in 000 as output. This implies that not every element of the codomain of SSS is the image of at least one element from its domain. Therefore, we can conclude that SSS is not surjective because S(n)≠0foranyn∈NS(n) \\neq 0 \\text{ for any } n \\in \\mathcal{N}S(n)=0foranyn∈N.\n\nInjective: Different inputs to SSS yield different outputs. This means that every element of the codomain of SSS is the image of at most one element from its domain; that is, S(m)=S(n)impliesthatm=nforanyn,m∈NS(m) = S(n) \\text{ implies that } m = n \\text{ for any } n, m \\in \\mathcal{N}S(m)=S(n)impliesthatm=nforanyn,m∈N. Hence, SSS is injective.\n\nFrom the diagram, we can also see that a natural number is either 000 or can be obtained from 000 by applying the successor function to 000 a finite number of times. This implies that N\\mathcal{N}N is the minimal non-empty set that contains 000 and admits a successor function satisfying conditions (1)and(2)(1) \\text{ and } (2)(1)and(2).\n\nTherefore, for the Peano Axioms to accurately describe the set of natural numbers, they must define a set that contains 000 and admits a successor function as described above.\n\nOriginal Formulation of Peano’s Axioms\n\nBefore we discuss the modern version of Peano’s Axioms, it is interesting to know how these axioms were originally stated by Giuseppe Peano. While perusing Peano’s Axioms it is worth keeping in mind that during Peano’s time, the concept of set was still nascent; it was Peano who introduced the symbol ∈\\in∈ in 188918891889 to denote “is an element of”. Being aware of Peano’s original formulation of these axioms helps us appreciate how far we have come in our mathematical journey and highlights that our journey towards better mathematical notation and abstraction still continues.\n\nThe axioms stated below appear between the pages 387−408387-408387−408 in the book “Historia Mathematica 1\\mathit{1}1” published in the year 197419741974.\n\nIn 189118911891, two years after the publication of his axioms for the natural numbers, Giuseppe Peano published an article titled “Sul concetto di numero” which translates to “On the Concept of Number,” in a journal he founded that same year. In this article, he reduced his list of axioms to five by eliminating the four axioms related to the equality relation “= ⁣ ⁣\"=\\!\\!\"=\".\n\nThe following are the five Peano’s axioms stated in the article :\n\nWe can see how the archaic notation obfuscates these axioms.\n\nThese maybe interpreted as :\n\nIn the Peano Axioms published in 1889and18911889 \\text{ and } 18911889and1891, the sequence of natural numbers began with 111, and the set of natural numbers was denoted by N\\mathit{N}N. However, in 189818981898 these axioms were modified so that the sequence began with 000 and the set was denoted by N ⁣0\\mathit{N_{\\!0}}N0.\n\nThe set of five Peano Axioms was increased to six in 190119011901 with the addition of the axiom – N ⁣0∈Cls\\mathit{N_{\\!0}} \\in \\text{Cls}N0∈Cls, i.e., the natural numbers form a class. With the addition of this last axiom, the axioms have received their final form, which are listed below.\n\nThe Axiomatization of Natural Numbers\n\nWe will first define the notion of equality as it pertains to natural numbers, and then we will formulate the Peano Axioms such that it provides an axiomatic definition of natural numbers.\n\nAlthough Giuseppe Peano omitted the four axioms related to the equality relation from his later formulation of the axioms regarding natural numbers, we will still discuss these axioms for the sake of completeness. The likely reason for this omission is because the concept of equality extends beyond natural numbers to mathematical objects in general and pertains more to the realm of logic, which specifies the conditions under which two mathematical objects can be considered equal.\n\nIn our discussion of Peano’s Axioms, we will adopt some conventions from its final form. Therefore, we will start with 000 instead of 111 in our axiomatic system and denote the set of natural numbers by N0\\mathbb{N_0}N0, where the subscript 000 reminds us that 000 is included.\n\nThe Notion of Equality\n\nBefore we define the set of natural numbers N0\\mathbb{N_0}N0 axiomatically, we will formalize the notion of equality through the four axioms of Peano, which establish the properties that the equality relation, denoted by ===, must satisfy.\n\nSuppose there exists a set N0\\mathbb{N_0}N0 that satisfies Axioms 111 t0 999 listed below.\n\nFirstly, every natural number should be equal to itself; this is called the reflexivity axiom.\n\nAxiom 1\\mathbf{1}1. For every x∈N0,x=xx \\in \\mathbb{N_0}, x = xx∈N0,x=x.\n\nSecondly, if one natural number equals a second one, then the second one should equal the first one. This is known as the symmetry axiom.\n\nAxiom 2\\mathbf{2}2. For every x,y∈N0,ifx=y,theny=xx, y \\in \\mathbb{N_0}, \\text{ if } x = y, \\text{ then } y = xx,y∈N0,ifx=y,theny=x.\n\nThirdly, if one natural number is equal to a second, and that second natural number is equal to a third, then the first and third are equal to each other. This is called the transitivity axiom.\n\nAxiom 3\\mathbf{3}3. For every x,y,z∈N0,ifx=yandy=z,thenx=zx, y, z \\in \\mathbb{N_0}, \\text{ if } x = y \\text{ and } y = z, \\text{ then } x = zx,y,z∈N0,ifx=yandy=z,thenx=z.\n\nThese three properties of reflexivity, symmetry and transitivity are applicable to any two mathematical objects that are related by the equality. As we have already discussed during our study of Set Theory, equality is an example of an equivalence relation, which is a type of homogeneous binary relation that satisfies the above three properties of reflexivity, symmetry and transitivity.\n\nSince the equality relation is defined generically for all mathematical objects and not just for natural numbers, we must make explicit the assumption that if two mathematical objects are equal—i.e., they satisfy the equality relation—and one of them is a natural number, then the other must also be a natural number. The next axiom makes this assumption explicit, namely, a natural number can only be equal to another natural number.\n\nFourthly, if any mathematical object is equal to a natural number, then that mathematical object is itself a natural number. This is called the closure of equality axiom.\n\nAxiom 4\\mathbf{4}4. For all xandy,ifx∈N0andx=y,theny∈N0x \\text{ and } y, \\text{ if } x \\in \\mathbb{N_0} \\text{ and } x = y, \\text{ then } y \\in \\mathbb{N_0}xandy,ifx∈N0andx=y,theny∈N0.\n\nThat is, the set of natural numbers is closed under equality.\n\nThe Peano Axioms\n\nWe will now discuss the five main Peano axioms that define the natural numbers. Peano aimed to formulate these axioms such that the fewest possible axioms could generate all the natural numbers. Therefore, we will construct the Peano axioms by checking, after stating each axiom, whether the axioms stated thus far can unambiguously result in the set of natural numbers that we know of. That is, we will continue constructing the Peano axioms until these axioms, when taken together, incontrovertibly result in N0={0,1,2,…}\\mathbb{N}_0 = \\{0, 1, 2, \\ldots\\}N0={0,1,2,…}.\n\nThis method of constructing the Peano axioms leads to the insight that the entire set of natural numbers can be generated by asserting the existence of at least one natural number and then defining a function called the successor function. This function takes a natural number as input and outputs another natural number, resulting in the construction of all remaining natural numbers.\n\nLet us now proceed with the construction of the Peano Axioms.\n\nSince we start counting from 000, it is unsurprising that 000 is the most obvious element to axiomatically include in the set of natural numbers.\n\nFifthly, 000 is a natural number.\n\nAxiom 5\\mathbf{5}5. 0∈N00 \\in \\mathbb{N_0}0∈N0.\n\nThus far we are only guaranteed the existence of a single natural number, 000.\n\nFrom 000 we should be able to generate the other natural numbers; that is starting with 000 we should be able to reach 111, from 111 reach 222 and so on, akin to counting. We can model this progression from one natural number to the next using a function that takes a natural number as input and produces another natural number as output. This function is called a successor function since it establishes a succession within the set of natural numbers and is written as S:N0→N0S : \\mathbb{N_0} \\rightarrow \\mathbb{N_0}S:N0→N0. The next axiom simply states that there is a function SSS whose domain and codomain are the set of natural numbers, N0\\mathbb{N}_0N0.\n\nSixthly, every natural number has a successor which is also a natural number.\n\nAxiom 6\\mathbf{6}6. If x∈N0x \\in \\mathbb{N}_0x∈N0, then S(x)∈N0S(x) \\in \\mathbb{N}_0S(x)∈N0.\n\nThat is, the set of natural numbers, N0\\mathbb{N}_0N0, is closed under the successor operation, SSS.\n\nAs this axiom implies, we will refer to S(x)S(x)S(x) as the successor of x\\mathit{x}x.\n\nTill now we have only established that the set of natural numbers contains 000 and its successor, S(0)S(0)S(0), where the function SSS takes natural numbers as input and outputs natural numbers. However, we are still quite far from having the set of natural numbers that we know of, since we could have N0={0}\\mathbb{N}_0 = \\{0\\}N0={0} and define S(0)=0S(0) = 0S(0)=0, which would still satisfy all of the above axioms. In this case, N0={0}\\mathbb{N}_0 = \\{0\\}N0={0}, but we want N0={0,1,2,3,…}\\mathbb{N}_0 = \\{0, 1, 2, 3, \\ldots\\}N0={0,1,2,3,…}.\n\nTo achieve this, we need to ensure that the successor function SSS does not output 000. Our next axiom will guarantee this by forbidding 000 from being the successor of any natural number, including itself.\n\nSeventhly, 000 is not  the successor of any natural number.\n\nAxiom 7\\mathbf{7}7. For every natural number x∈N0,S(x)≠0x \\in \\mathbb{N}_0, S(x) \\neq 0x∈N0,S(x)=0.\n\nThat means that there is no natural number whose successor is 000. Consequently, the preimage of 000 under SSS defined on the set of natural numbers is an empty set.\n\nAs a consequence of this axiom, we know that S(0)≠0S(0) \\neq 0S(0)=0. Therefore, S(0)S(0)S(0) must equal some other natural number, which we can denote by 111. Hence, we can define 111 by S(0)=1S(0) = 1S(0)=1.\n\nBased on axioms 5,6and75, 6 \\text{ and } 75,6and7 we are guaranteed the existence of at least two natural numbers, 0and10 \\text{ and } 10and1, but not necessarily others.\n\nFor example, we could define N0={0,1}\\mathbb{N}_0 = \\{0, 1\\}N0={0,1}, where S(0)=1andS(1)=1S(0) = 1 \\text{ and } S(1) = 1S(0)=1andS(1)=1. In this case, both natural numbers 0and10 \\text{ and } 10and1 have the same successor, which is 111.\n\nIf S(0)=1andS(1)=1S(0) = 1 \\text{ and } S(1) = 1S(0)=1andS(1)=1, then the two natural numbers 0and10 \\text{ and } 10and1 have the same successor, 111. This set of natural numbers together with the successor function SSS defined on it would still satisfy all the above axioms. However, if we stop here, the axioms constructed thus far do not guarantee the existence of the rest of the natural numbers that we know of, namely, 2,3,4…2, 3, 4 \\ldots2,3,4….\n\nTherefore, our next axiom should ensure that different natural numbers have different successors. This means that every natural number is the successor of at most one natural number (since 000 is not the successor of any natural number) which implies that SSS must be an injective function.\n\nEighthly, no two natural numbers have the same successor unless they are equal.\n\nAxiom 8\\mathbf{8}8. For all x,y∈N0x, y \\in \\mathbb{N}_0x,y∈N0, if S(x)=S(y)S(x) = S(y)S(x)=S(y), then x=yx = yx=y.\n\nThis axiom leads to some important consequences. It excludes the possibility of defining N0\\mathbb{N}_0N0 to be just {0,1}\\{0, 1\\}{0,1}. We already have S(0)=1S(0) = 1S(0)=1 and since SSS is an injective function, we cannot have S(1)=1S(1) = 1S(1)=1. Axiom 777 excludes the possibility that S(1)=0S(1) = 0S(1)=0. Thus S(1)S(1)S(1) must be some other natural number, which we denote as 222. Therefore, we can define 2=S(1)2 = S(1)2=S(1).\n\nBy a similar argument, S(2)S(2)S(2) cannot be 0,1or20, 1 \\text{ or } 20,1or2. Hence, it must be some other natural number, which we denote as 333. Continuing this way, we see that N0\\mathbb{N}_0N0 must contain all the natural numbers that we know of.\n\nSo far we have established that N0\\mathbb{N}_0N0 must include 000, its successor 1=S(0)1 = S(0)1=S(0), its successor’s successor 2=S(1)2 = S(1)2=S(1) and so on. Thus N0\\mathbb{N}_0N0 must include 0,S(0),S(S(0)),S(S(S(0))),…0, S(0), S(S(0)), S(S(S(0))), \\ldots0,S(0),S(S(0)),S(S(S(0))),…. In order to avoid so many nested applications of SSS we use the numerals 1,2,31, 2, 31,2,3 to denote S(0),S(S(0))andS(S(S(0)))S(0), S(S(0)) \\text{ and } S(S(S(0)))S(0),S(S(0))andS(S(S(0))), respectively.\n\nThese first eight axioms have resulted in the definition of N0\\mathbb{N}_0N0 to include all the natural numbers that we know of.\n\nTherefore, so far we only know that\n\n{0,1,2,…}⊂N0\\{0, 1, 2, \\ldots\\} \\subset \\mathbb{N}_0{0,1,2,…}⊂N0\n\nAt this point, it is interesting to ask whether our axiomatic definition of N0\\mathbb{N}_0N0 precludes the inclusion of additional elements.\n\nIn order to answer this question, let us consider a version of N0\\mathbb{N}_0N0 that satisfies all the above axioms but is not the usual set of natural numbers that we know of. That is,\n\nN0={0,1,2,3,…}∪{!,−}\\mathbb{N}_0 = \\{0, 1, 2, 3, \\ldots\\} \\cup \\{!, -\\}N0={0,1,2,3,…}∪{!,−}\n\nAs can be seen, this version of N0\\mathbb{N}_0N0 contains all the natural numbers and also includes two other symbols, !and−! \\text{ and } -!and−.\n\nWe will next define the successor function defined on this set. For the subset {0,1,2,3,…}\\{0, 1, 2, 3, \\ldots\\}{0,1,2,3,…} of N0\\mathbb{N}_0N0, we define SSS as we have described above i.e., S(0)=1,S(1)=2,S(2)=3S(0) = 1, S(1) = 2, S(2) = 3S(0)=1,S(1)=2,S(2)=3 and so on. For the subset {!,−}\\{!, -\\}{!,−} of N0\\mathbb{N}_0N0, we define S(!)=−andS(−)=!S(!) = - \\text{ and } S(-) = !S(!)=−andS(−)=!.\n\nThis version of N0\\mathbb{N}_0N0 with this successor function satisfies all the axioms, but it has more elements than what we want our set of natural numbers to have. This is shown below.\n\nSimilarly, there could be other versions of N0\\mathbb{N}_0N0 with different successor functions, each of which satisfies all of the above axioms from 5through85 \\text{ through } 85through8 but could also have elements other than natural numbers. This is illustrated below.\n\nBased on axioms 5to85 \\text{ to } 85to8, the set N0\\mathbb{N}_0N0 of natural numbers satisfies the following conditions:\n\n0∈N00 \\in \\mathbb{N}_00∈N0; and\n\nif x∈N0,thenS(x)∈N0x \\in \\mathbb{N}_0,\\text{then } S(x) \\in \\mathbb{N}_0x∈N0,thenS(x)∈N0, where S(x)S(x)S(x) denotes the successor of xxx.\n\nThis way of defining a set, where a base clause specifies the basic element of the set and an inductive clause details how to generate additional elements, is called an inductive definition of the set, and such a set is referred to as an inductive set.\n\nTherefore, the axioms 5to85 \\text{ to } 85to8 only ensure that {0,1,2,…}⊂N0\\{0, 1, 2, \\ldots\\} \\subset \\mathbb{N}_0{0,1,2,…}⊂N0, where N0\\mathbb{N}_0N0 is any set such that 0∈N00 \\in \\mathbb{N}_00∈N0 and if x∈N0,thenS(x)∈N0x \\in \\mathbb{N}_0,\\text{then } S(x) \\in \\mathbb{N}_0x∈N0,thenS(x)∈N0.\n\nHowever, as discussed earlier and shown in the diagram above, this definition of N0\\mathbb{N}_0N0 does not exclude elements other than natural numbers from being contained in the set.\n\nTo ensure that only natural numbers are included in the set N0\\mathbb{N}_0N0, i.e., N0={0,1,2,…}\\mathbb{N}_0 = \\{0, 1, 2, \\ldots\\}N0={0,1,2,…}, our next (and final) axiom should declare N0\\mathbb{N}_0N0 to be the minimal set that satisfies axioms 5through85 \\text{ through } 85through8; specifically, N0\\mathbb{N}_0N0 is the intersection of all sets that satisfy these axioms.\n\nSince axiom 999 uses the inductive definition of a set in its construction, it is referred to as the Axiom of Induction.\n\nWe will now state our ninth and final axiom.\n\nAxiom 9\\mathbf{9}9 (Axiom of Induction). If T⊂N0T \\subset \\mathbb{N}_0T⊂N0 is such that:\n\n0∈T0 \\in T0∈T; and\n\nx∈T  ⟹  S(x)∈Tx \\in T \\implies S(x) \\in Tx∈T⟹S(x)∈T for all x∈N0x \\in \\mathbb{N}_0x∈N0,\n\nthen T=N0T = \\mathbb{N}_0T=N0.\n\nAs we have already discussed, the axioms 5through85 \\text{ through } 85through8 ensure that {0,1,2,… }⊂N0\\{0, 1, 2, \\dots\\} \\subset \\mathbb{N}_0{0,1,2,…}⊂N0. Suppose T={0,1,2,… }T = \\{0, 1, 2, \\dots\\}T={0,1,2,…}. We can see that 0∈Tandx∈T  ⟹  S(x)∈Tforallx∈N00 \\in T \\text{ and } x \\in T \\implies S(x) \\in T \\text{ for all } x \\in \\mathbb{N}_00∈Tandx∈T⟹S(x)∈Tforallx∈N0. Therefore, by Axiom 999, {0,1,2,… }=N0\\{0, 1, 2, \\dots\\} = \\mathbb{N}_0{0,1,2,…}=N0.\n\nThus we finally have the set of natural numbers that we know of, namely, N0={0,1,2,… }\\mathbb{N}_0 = \\{0, 1, 2, \\dots\\}N0={0,1,2,…}.\n\nAlternate Formulations of Axiom of Induction: Set-Based and  Predicate-Based Perspectives\n\nFor the sake of completeness, we will discuss two alternate ways to formulate Peano’s Ninth Axiom, namely the Axiom of Induction.\n\nThe Axiom of Induction is a cornerstone of number theory, providing a powerful tool for proving statements about natural numbers. It can be expressed in two equivalent forms: set-based and predicate-based. Each form offers a different perspective on the same fundamental principle.\n\nSet-Based Axiom of Induction: Focusing on Subsets\n\nAxiom 9\\mathbf{9}9 (Axiom of Induction). If T⊂N0T \\subset \\mathbb{N}_0T⊂N0 is a set such that:\n\n0∈T0 \\in T0∈T; and\n\nx∈T  ⟹  S(x)∈Tx \\in T \\implies S(x) \\in Tx∈T⟹S(x)∈T for all x∈N0x \\in \\mathbb{N}_0x∈N0,\n\nthen N0⊂T\\mathbb{N}_0 \\subset TN0⊂T.\n\nIn simpler terms, if a subset of natural numbers contains 000 and is closed under the successor operation (meaning that if a natural number is in the subset, its successor is also in the subset), then that subset must contain all natural numbers.\n\nAs discussed above, Axioms 5through85 \\text{ through } 85through8 ensure that {0,1,2,…}⊂N0\\{0, 1, 2, \\ldots\\} \\subset \\mathbb{N}_0{0,1,2,…}⊂N0. Additionally, we observe that the set {0,1,2,…}\\{0, 1, 2, \\ldots\\}{0,1,2,…} satisfies the following two conditions of Axiom 999:\n\nIt contains 000; and\n\nWhenever it contains an element xxx, it also contains its successor, namely, S(x)S(x)S(x).\n\nTherefore, by Axiom 999, it follows that N0⊂{0,1,2,…}\\mathbb{N}_0 \\subset \\{0, 1, 2, \\ldots\\}N0⊂{0,1,2,…}.\n\nSince Axioms 5through85 \\text{ through } 85through8 ensure that {0,1,2,…}⊂N0\\{0, 1, 2, \\ldots\\} \\subset \\mathbb{N}_0{0,1,2,…}⊂N0 and by Axiom 999 we have show that N0⊂{0,1,2,…}\\mathbb{N}_0 \\subset \\{0, 1, 2, \\ldots\\}N0⊂{0,1,2,…}, it follows that N0={0,1,2,…}\\mathbb{N}_0 = \\{0, 1, 2, \\ldots\\}N0={0,1,2,…}.\n\nThis perspective emphasizes the structure of the subset itself. We are concerned with the elements that are members of the subset and how they relate to each other through the successor function.\n\nPredicate-Based Axiom of Induction: Focusing on Properties\n\nPeano’s axioms five through eight collectively define a superset of natural numbers, specifically {0,1,2,…}⊂N0\\{0, 1, 2, \\ldots\\} \\subset \\mathbb{N}_0{0,1,2,…}⊂N0. To ensure that this set N0\\mathbb{N}_0N0 includes only natural numbers, Peano’s ninth axiom can also be formulated as the principle of mathematical induction over natural numbers. This formulation is equivalent to the axiom of induction and serves the same purpose of removing unwanted elements from the superset N0\\mathbb{N}_0N0, ensuring that it contains only natural numbers.\n\nThe predicate-based form of the Axiom of Induction shifts the focus from subsets to properties (expressed as predicates) that natural numbers may or may not possess.\n\nThe reformulation of Axiom 999 in terms of predicates results in the Principle of Mathematical Induction which is stated as follows.\n\nAxiom 9\\mathbf{9}9 (Principle of Mathematical Induction). For any predicate P(n)P(n)P(n), where nnn is a natural number, if:\n\nP(0)P(0)P(0) is true, and\n\nfor every natural number nnn, P(n)P(n)P(n) being true implies that P(S(n))P(S(n))P(S(n)) is also true,\n\nthen P(n)P(n)P(n) is true for every natural number nnn.\n\nHere, we are concerned with a property (((represented by the predicate P(n))P(n))P(n)) that natural numbers might possess. If 000 has the property, and if natural number nnn having the property implies that its successor S(n)S(n)S(n) also has the property, then all natural numbers must have the property.\n\nThis perspective emphasizes the properties of individual natural numbers. We are concerned with whether a given natural number has a specific property.\n\nEquivalence and Connection\n\nThe set-based and predicate-based forms are logically equivalent, meaning they express the same fundamental principle. This equivalence is rooted in the relationship between subsets and predicates, as established by the Axiom of Separation.\n\nFrom Subset to Predicate:\n\nGiven T⊂N0T \\subset \\mathbb{N}_0T⊂N0, it implies from the Axiom of Separation that there exists a predicate P(n)P(n)P(n) such that P(n)P(n)P(n) is true if and only if n∈Tn \\in Tn∈T.\n\nFrom Predicate to Subset:\n\nGiven a predicate P(n)P(n)P(n) and a set N0\\mathbb{N}_0N0, we can define a subset T={n∈N0 ∣ P(n)istrue}T = \\{n \\in \\mathbb{N}_0 \\,|\\, P(n) \\text{ is true}\\}T={n∈N0∣P(n)istrue}, by the Axiom of Separation.\n\nThus, the subset and predicate perspectives are simply two ways of expressing the same fundamental idea. The set-based form emphasizes the elements within a collection, while the predicate-based form emphasizes the properties of individual elements. The Axiom of Separation is the bridge that allows us to move seamlessly between these perspectives.\n\nWhy Both Forms Are Useful\n\nBoth forms of the Axiom of Induction are valuable tools in mathematical proofs. The set-based form is often used in set theory and related areas, while the predicate-based form is commonly used in number theory and other branches of mathematics where properties of numbers are the main focus.\n\nEssentially, they are two sides of the same coin, and which one to use depends on the context of the problem and the preference of the mathematician.\n\nMethod of Definition of Natural Numbers using Peano’s Axioms\n\nIt should be noted that Peano’s Axioms only describe how to construct the set of natural numbers and do not define what natural numbers are intrinsically. A particular natural number is given when its generation under the inductive definition is given.\n\nFor example, the natural number 222 is defined as that mathematical object which is obtained by starting with the initial object 000 and applying the successor function once and then again i.e., 222 represents S(S(0))S(S(0))S(S(0)).\n\nExistence of the Set of Natural Numbers satisfying Peano’s Axioms\n\nHow do we establish the existence of a set, an element of that set, and a function from the set to itself, that satisfy Peano’s Axioms? These axioms themselves are insufficient to prove this existence. Consequently, there are two approaches to resolving this matter.\n\nOne common approach in mathematics is to take something as  axiomatic and then use it as the basis upon which we prove all our other results. Hence, such an approach requires us to be satisfied with taking the existence of a set satisfying Peano’s Axioms axiomatically. This axiom is called the existence axiom for natural numbers and guarantees that there exists a set with the properties that Peano’s axioms ascribe to it.\n\nThe statement of this axiom is as follows:\n\nExistence Axiom for Natural Numbers: There exists a set N0\\mathbb{N}_0N0 satisfying Axioms 1through91 \\text{ through } 91through9.\n\nAlternatively, if we use the Zermelo-Fraenkel Axioms as our foundation for set theory, we can prove that something satisfying Peano’s Axioms exists, so we don’t need to assume it separately. We’ll show how this is done later.\n\nProving Properties of Natural Numbers\n\nHaving established the existence of the natural numbers and their fundamental properties as defined by Peano’s Axioms, we will next explore how to rigorously prove that the natural numbers satisfy any specific property using the Axiom of Induction.\n\nNext Topic: Proving Properties of Natural Numbers Using Proof by Induction\n\nShare this:Click to share on Facebook (Opens in new window)Click to share on X (Opens in new window)Click to email a link to a friend (Opens in new window)\n\n\t\t\t\t\t←\n\t\t\t\t\tFormulas and Free Variables\n\n\t\t\t\t\t→\n\t\t\t\t\t\tNumber Theory Primer : Proving Properties of Natural Numbers Using Proof by Induction",
    "summary": {
      "en": "**Summary: Axiomatic Study of Natural Numbers and Peano’s Axioms**\n\nThis text discusses the foundational framework for natural numbers based on Peano’s Axioms. Natural numbers, like 1, 2, and 3, are commonly used for counting objects. However, the text emphasizes the need for a precise mathematical definition of natural numbers that doesn't rely on intuitive counting.\n\n1. **Axiomatic Framework**: The goal is to create a minimal set of axioms that can define natural numbers. Peano's Axioms, formulated by Giuseppe Peano in the late 19th century, serve as this foundational framework.\n\n2. **Peano's Axioms**: Originally consisting of nine axioms, they define natural numbers and their properties:\n   - A natural number exists (0 is included).\n   - Every natural number has a successor (the next number).\n   - 0 is not the successor of any natural number.\n   - Different numbers have different successors.\n   - An inductive principle is established, ensuring that if a property holds for 0 and for the successor of any natural number, it holds for all natural numbers.\n\n3. **Successor Function**: The successor function is crucial; it defines how to generate natural numbers sequentially from 0.\n\n4. **Inductive Definition**: The axioms collectively imply that the natural numbers can be constructed through an inductive process, which ensures all natural numbers (0, 1, 2, 3, etc.) are included.\n\n5. **Existence of Natural Numbers**: The existence of a set of natural numbers satisfying Peano’s Axioms can be accepted axiomatically or proven using existing set theories.\n\n6. **Induction Principle**: The principle of mathematical induction is highlighted as a powerful tool for proving properties of natural numbers.\n\nIn summary, Peano’s Axioms provide a rigorous and minimal foundation for understanding and defining natural numbers in mathematics, allowing us to derive further mathematical concepts from this base.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "032d1709dd3e46ca",
    "title": {
      "en": "I made AI fix my bugs in production for 27 days straight – lessons learned",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://old.reddit.com/r/ChatGPTCoding/comments/1jibmtc/i_made_ai_fix_my_bugs_in_production_for_27_days/",
    "score": 4,
    "by": "simonpure",
    "time": 1742909303,
    "content": "Welcome to Reddit,the front page of the internet.Become a Redditorand join one of thousands of communities.×body >.content .link .rank, .rank-spacer { width: 1.1ex } body >.content .link .midcol, .midcol-spacer { width: 4.1ex } .adsense-wrap { background-color: #eff7ff; font-size: 18px; padding-left: 5.2ex; padding-right: 5px; }268269270I made AI fix my bugs in production for 27 days straight - lessons learnedProject (self.ChatGPTCoding)submitted 2 days ago * by Relevant-Pitch-8450For the past 27 days, I’ve had AI automatically fix my bugs in production, all the way to creating a full PR, and I wanted to share the results!\n\nWhen an exception occurs in my server, a workflow is kicked off that:\n\nGathers affected code files and git blame history from my GitHub, and bundles that with the error stack trace, local vars, and relevant internet sources.\nSends all context to Claude 3.7 in a recursive flow similar to Claude Code to diagnose the root cause, and then draft a solution, and open a PR for my review.\nBundles everything together in a nice dashboard, with a link to the PR on GitHub, an explanation of the error given all of the issue context, and the bugfix!\n\nHere’s what the dashboard looks like!\n\nI made the window less wide so mobile users might have a chance.  PR link ready!\n\nLooking at the results, I’ve had 21 unique bugs to solve in the last 27 days:\n\n12 of those bugs were one-shot by this system and I just reviewed and merged the PR.\n6 of those gave me a good start, but I ended up making at least one change.\n3 of them were not even close. One seemed right but hallucinated a library and solution that didn’t exist, and two were just harder bugs (a race condition and an OOM using an external service) where the solution was clearly wrong.\n\nI’m pretty stoked by the results - not all of the solved bugs were trivial!  It definitely saved me time and the cognitive overhead from context switching to a bug. Might not be good if you are working on something niche or very difficult.\n\nSo did I end up saving any time by building this?\n\nHonestly no lol — it took way longer to build it than to just solve the bugs.\n\nBut maybe if anyone might be curious or wants to try this yourself to save some time, let me know — happy to share my setup and code!\n\nUpdate 3/25: Thank you for the response!  Here's where I am - I’ve tried to simplify my code, but I think people will hate me for wasting their time if I publish as-is.  It’s far below acceptable for me as well and I can't in good conscience put it out like this - it’s just way too annoying and complex to set up.  In order to simplify, I made it rely on a Sentry account (ugh), use Claude Code directly, and even then it already requires 8 API keys, a Github PAT, setup of a Sentry internal tool, and needs to be deployed to the internet (to receive webhooks, or you could use ngrok I guess).  A lot of people have been asking to try it out and I just know that if I put this out most won’t use it.  I think most the services need to be hosted in order to make the install less painful.\n\nSo here’s what I’ve decided to do.\n\n- For those who wanted to use it, I am now working on a hosted version, which will be free if you bring your API token, will not rely on Sentry, and be acceptably easy to install.\n\n- For those just curious about how I made it, feel free to DM or comment, and I’ll do my best to answer.\n\n118 commentssharesavehidereportall 118 commentssorted by: besttopnewcontroversialoldrandomq&alive (beta)Want to add to the discussion?Post a comment!Create an account[–]wow_much_redditing 38 points39 points40 points 2 days ago(31 children)Would you open source this? The UI is really well done\n\npermalinkembedsavereportreply[–]Relevant-Pitch-8450[S] 54 points55 points56 points 2 days ago*(26 children)Would love to!  It seems like a few people here might get some use out of it.\n\nNot sure how pretty the UI is but definitely ty for the kind words :)\n\nEdit: Looks like I'm doing it!  Need to clean up a few things first but should be ready somewhat soon.\n\npermalinkembedsaveparentreportreply[–]wow_much_redditing 7 points8 points9 points 2 days ago(0 children)It's way better than anything I can come up with. Mine would just be HTML tables lol. In all seriousness really well done this would be a great way to onboard someone to an on-call rotation. Like here are the incidents over the last 3 weeks.\n\npermalinkembedsaveparentreportreply[–]Aardappelhuree 2 points3 points4 points 2 days ago(4 children)This thing is worth money. You can sell it as a SAAS. You can open source it… but I’d just sell it, either as a service or the whole thing as a product.\n\nI know multiple businesses that would pay 1000s of dollars for this thing.\n\npermalinkembedsaveparentreportreply[–]BinaryOperation 0 points1 point2 points 1 day ago(0 children)!remindme 7 days\n\npermalinkembedsaveparentreportreply[–]3Dmooncats 0 points1 point2 points 1 day ago(1 child)Why would they pay that much for this ?\n\npermalinkembedsaveparentreportreply[–]Aardappelhuree 1 point2 points3 points 1 day ago*(0 children)\nhype. AI is hot and they’re basically willing to try any AI related tool for any cost.\ndevelopers are expensive\n\nSome companies I work for pay over 100K on SAAS services a month. Many of the things they pay for are things you can make in a hackathon in a few days. They don’t have developer-time available, so they buy solutions.\n\nOne of them basically told me they will approve purchase of any AI related tool no matter the cost. (Assuming it is somewhat useful & Within reason but they didn’t need to say that because I have a working brain)\n\npermalinkembedsaveparentreportreply[–]OddPresentation9164 0 points1 point2 points 23 hours ago(0 children)!remindme 2 days\n\npermalinkembedsaveparentreportreply[–]NastroAzzurro 2 points3 points4 points 2 days ago(2 children)!remindme 2 days\n\npermalinkembedsaveparentreportreply[–]RemindMeBot 1 point2 points3 points 2 days ago*(0 children)I will be messaging you in 2 days on 2025-03-26 04:39:44 UTC to remind you of this link\n\n39 OTHERS CLICKED THIS LINK to send a PM to also be reminded and to reduce spam.\n\nParent commenter can  delete this message to hide from others.\n\nInfo\nCustom\nYour Reminders\nFeedback\n\npermalinkembedsaveparentreportreply[–]SpiritualKindness 0 points1 point2 points 2 days ago(0 children)!remindme 5 days\n\npermalinkembedsaveparentreportreply[–]cube8021 1 point2 points3 points 2 days ago(0 children)!remindme 7 days\n\npermalinkembedsaveparentreportreply[–]rbadapanda 0 points1 point2 points 2 days ago(0 children)!remindme 6 days\n\npermalinkembedsaveparentreportreply[–]im_datta0 0 points1 point2 points 2 days ago(0 children)!remindme 7 days\n\npermalinkembedsaveparentreportreply[–]bTz442 0 points1 point2 points 2 days ago(0 children)!remindme 7 days\n\npermalinkembedsaveparentreportreply[–]the_general1 0 points1 point2 points 2 days ago(0 children)!remindme 5 days\n\npermalinkembedsaveparentreportreply[–]codyswann 0 points1 point2 points 2 days ago(0 children)!remindme 2 days\n\npermalinkembedsaveparentreportreply[–]QQending 0 points1 point2 points 2 days ago(0 children)!remindme 7 days\n\npermalinkembedsaveparentreportreply[–][deleted] 1 day ago(1 child)[removed]\n\n[–]AutoModerator[M] 0 points1 point2 points 1 day ago(0 children)Sorry, your submission has been removed due to inadequate account karma.\n\nI am a bot, and this action was performed automatically. Please contact the moderators of this subreddit if you have any questions or concerns.\n\npermalinkembedsaveparentreportreply[–]tindalos 0 points1 point2 points 1 day ago(0 children)I’d love to see this open sourced.   This could help me a lot. Nice work love the dashboard.\n\npermalinkembedsaveparentreportreply[–]whenpjotrring 0 points1 point2 points 1 day ago(0 children)!remindme 2 days\n\npermalinkembedsaveparentreportreply[–]hellrokr 0 points1 point2 points 1 day ago(0 children)This is really interesting! Waiting for your link.\n\npermalinkembedsaveparentreportreply[–][deleted] 1 day ago(1 child)[removed]\n\n[–]AutoModerator[M] 0 points1 point2 points 1 day ago(0 children)Sorry, your submission has been removed due to inadequate account karma.\n\nI am a bot, and this action was performed automatically. Please contact the moderators of this subreddit if you have any questions or concerns.\n\npermalinkembedsaveparentreportreply[–]Silver-Hunter-6262 0 points1 point2 points 1 day ago(0 children)!remindme 5days\n\npermalinkembedsaveparentreportreply[–]johca 0 points1 point2 points 1 day ago(0 children)!remindme 7 days\n\npermalinkembedsaveparentreportreply[–]rish_p 0 points1 point2 points 1 day ago(0 children)this could also be a great saas product, if you do open source please post so we are updated\n\nmaybe open source and then add premium features if they are needed\n\npermalinkembedsaveparentreportreply[–][deleted] 2 days ago(1 child)[removed]\n\n[–]AutoModerator[M] 0 points1 point2 points 2 days ago(0 children)Sorry, your submission has been removed due to inadequate account karma.\n\nI am a bot, and this action was performed automatically. Please contact the moderators of this subreddit if you have any questions or concerns.\n\npermalinkembedsavereportreply[–]shivam1907 0 points1 point2 points 1 day ago(0 children)!remindme 3 days\n\npermalinkembedsaveparentreportreply[–]tribat 0 points1 point2 points 1 day ago(0 children)I’ll look forward to this. Thanks !\n\npermalinkembedsaveparentreportreply[–]ClassyBukake 5 points6 points7 points 2 days ago(2 children)Real talk from an engineering perspective, this is very cool to see as a personal project, and i really want to see how you did this.\n\nFrom a business perspective, you could sell the shit out of this as a Service.\n\npermalinkembedsavereportreply[–]Relevant-Pitch-8450[S] 2 points3 points4 points 2 days ago(0 children)Thank you!\n\npermalinkembedsaveparentreportreply[–]Frisky-biscuit4 0 points1 point2 points 1 day ago(0 children)From a different business perspective, Greptile is a great tool to catch bugs before they hit prod\n\npermalinkembedsaveparentreportreply[–]urbantomatoeater 4 points5 points6 points 2 days ago(1 child)I’d like to check out how you built this too\n\npermalinkembedsavereportreply[–]Relevant-Pitch-8450[S] 3 points4 points5 points 2 days ago(0 children)Awesome!  Looks like I need to open source this :)\n\npermalinkembedsaveparentreportreply[–]HelpRespawnedAsDee 2 points3 points4 points 2 days ago(1 child)Well, keep using next month to see if it actually saves you time lol.\n\nAlso how much did you spend this month?\n\npermalinkembedsavereportreply[–]Relevant-Pitch-8450[S] 3 points4 points5 points 2 days ago(0 children)Haha yes - more likely I build some more to waste more time.\n\nIn terms of spend, unfortunately I don't know!  I used Claude Code and also Anthropic's API for something else I'm working on, so I can't tell you exactly how much came from this one.  I would guess under $10 though.\n\npermalinkembedsaveparentreportreply[–]nmay-dev 4 points5 points6 points 2 days ago(0 children)I'd be interested in trying this.\n\npermalinkembedsavereportreply[–]justanemptyvoice 2 points3 points4 points 2 days ago(1 child)I love the dashboard and would love to see it\n\npermalinkembedsavereportreply[–]Relevant-Pitch-8450[S] 1 point2 points3 points 2 days ago(0 children)Awesome!\n\npermalinkembedsaveparentreportreply[–]Round_Mixture_7541 1 point2 points3 points 2 days ago(0 children)Looks awesome. How is the pipeline triggered? Or how does it retrieve the stacktrace?\n\npermalinkembedsavereportreply[–]alphaQ314 1 point2 points3 points 2 days ago*(0 children)Totally unrelated, but how good does the JetBrainsMono font look on JetBrains software 🔥\n\nIs it just me, or does it look better than it does on VSC? I primarily use vsc, and i can't quite get it to look quite the same. Same with emacs.\n\npermalinkembedsavereportreply[–]STAY_ROYAL 1 point2 points3 points 1 day ago(0 children)Congrats, this is great. Could see people paying money for this or even a feature being introduced by cursor.\n\npermalinkembedsavereportreply[–]fergthh 1 point2 points3 points 1 day ago(0 children)\nSo did I end up saving any time by building this?\n\nHonestly no lol — it took way longer to build it than to just solve the bugs.\n\nthe essence of software development in its purest state\n\npermalinkembedsavereportreply[–][deleted] 2 days ago(1 child)[removed]\n\n[–]AutoModerator[M] 0 points1 point2 points 2 days ago(0 children)Sorry, your submission has been removed due to inadequate account karma.\n\nI am a bot, and this action was performed automatically. Please contact the moderators of this subreddit if you have any questions or concerns.\n\npermalinkembedsaveparentreportreply[–][deleted] 2 days ago(1 child)[removed]\n\n[–]AutoModerator[M] 0 points1 point2 points 2 days ago(0 children)Sorry, your submission has been removed due to inadequate account karma.\n\nI am a bot, and this action was performed automatically. Please contact the moderators of this subreddit if you have any questions or concerns.\n\npermalinkembedsavereportreply[–][deleted] 2 days ago(1 child)[removed]\n\n[–]AutoModerator[M] 0 points1 point2 points 2 days ago(0 children)Sorry, your submission has been removed due to inadequate account karma.\n\nI am a bot, and this action was performed automatically. Please contact the moderators of this subreddit if you have any questions or concerns.\n\npermalinkembedsavereportreply[–]TheMiracleWhip 0 points1 point2 points 2 days ago(0 children)Would love to test this out!\n\npermalinkembedsavereportreply[–][deleted] 2 days ago(1 child)[removed]\n\n[–]AutoModerator[M] 0 points1 point2 points 2 days ago(0 children)Sorry, your submission has been removed due to inadequate account karma.\n\nI am a bot, and this action was performed automatically. Please contact the moderators of this subreddit if you have any questions or concerns.\n\npermalinkembedsavereportreply[–][deleted] 2 days ago(1 child)[removed]\n\n[–]AutoModerator[M] 0 points1 point2 points 2 days ago(0 children)Sorry, your submission has been removed due to inadequate account karma.\n\nI am a bot, and this action was performed automatically. Please contact the moderators of this subreddit if you have any questions or concerns.\n\npermalinkembedsavereportreply[–]psycho_goblin 0 points1 point2 points 2 days ago(0 children)!remind me 3 days\n\npermalinkembedsavereportreply[–][deleted] 2 days ago(1 child)[removed]\n\n[–]AutoModerator[M] 0 points1 point2 points 2 days ago(0 children)Sorry, your submission has been removed due to inadequate account karma.\n\nI am a bot, and this action was performed automatically. Please contact the moderators of this subreddit if you have any questions or concerns.\n\npermalinkembedsavereportreply[–]The_Noosphere 0 points1 point2 points 2 days ago(0 children)!remindme 3 days\n\npermalinkembedsavereportreply[–]MealFew8619 0 points1 point2 points 2 days ago(0 children)!remindme 30 days\n\npermalinkembedsavereportreply[–][deleted] 2 days ago(1 child)[removed]\n\n[–]AutoModerator[M] 0 points1 point2 points 2 days ago(0 children)Sorry, your submission has been removed due to inadequate account karma.\n\nI am a bot, and this action was performed automatically. Please contact the moderators of this subreddit if you have any questions or concerns.\n\npermalinkembedsavereportreply[–][deleted] 2 days ago(1 child)[removed]\n\n[–]AutoModerator[M] 0 points1 point2 points 2 days ago(0 children)Sorry, your submission has been removed due to inadequate account karma.\n\nI am a bot, and this action was performed automatically. Please contact the moderators of this subreddit if you have any questions or concerns.\n\npermalinkembedsavereportreply[–]gorays999 0 points1 point2 points 2 days ago(0 children)!remindme 2 days\n\npermalinkembedsavereportreply[–]srivatsansam 0 points1 point2 points 2 days ago(0 children)!remindme 2 days\n\npermalinkembedsavereportreply[–][deleted] 2 days ago(1 child)[removed]\n\n[–]AutoModerator[M] 0 points1 point2 points 2 days ago(0 children)Sorry, your submission has been removed due to inadequate account karma.\n\nI am a bot, and this action was performed automatically. Please contact the moderators of this subreddit if you have any questions or concerns.\n\npermalinkembedsavereportreply[–][deleted] 2 days ago(1 child)[removed]\n\n[–]AutoModerator[M] 0 points1 point2 points 2 days ago(0 children)Sorry, your submission has been removed due to inadequate account karma.\n\nI am a bot, and this action was performed automatically. Please contact the moderators of this subreddit if you have any questions or concerns.\n\npermalinkembedsavereportreply[–][deleted] 2 days ago(1 child)[removed]\n\n[–]AutoModerator[M] 0 points1 point2 points 2 days ago(0 children)Sorry, your submission has been removed due to inadequate account karma.\n\nI am a bot, and this action was performed automatically. Please contact the moderators of this subreddit if you have any questions or concerns.\n\npermalinkembedsavereportreply[–]tr0picana 0 points1 point2 points 2 days ago(0 children)Would love to check this out! I see you're working on open-sourcing it so I'll wait patiently.\n\npermalinkembedsavereportreply[–][deleted] 2 days ago(1 child)[removed]\n\n[–]AutoModerator[M] 0 points1 point2 points 2 days ago(0 children)Sorry, your submission has been removed due to inadequate account karma.\n\nI am a bot, and this action was performed automatically. Please contact the moderators of this subreddit if you have any questions or concerns.\n\npermalinkembedsavereportreply[–]sasik520 0 points1 point2 points 1 day ago(0 children)I had this idea for months but never found time to do anything more than a veeery plain proof-of-concept. Also, when I talked with people, they were very sceptical about the idea.\n\nI'm super happy to see it works. Tbh, I expected no more than 1/10 valuable merge requests. Your numbers are amazing.\n\nCan't wait when I could test it - no matter if it is an open source project or not.\n\npermalinkembedsavereportreply[–]DoxxThis1 0 points1 point2 points 1 day ago(0 children)Why did the hallucinated library even get to a PR? It should have failed CI tests.\n\npermalinkembedsavereportreply[–][deleted] 1 day ago(1 child)[removed]\n\n[–]AutoModerator[M] 0 points1 point2 points 1 day ago(0 children)Sorry, your submission has been removed due to inadequate account karma.\n\nI am a bot, and this action was performed automatically. Please contact the moderators of this subreddit if you have any questions or concerns.\n\npermalinkembedsavereportreply[–]Speedping 0 points1 point2 points 1 day ago(0 children)RemindMe! 1 month\n\npermalinkembedsavereportreply[–]Feisty-War7046 0 points1 point2 points 1 day ago(0 children)Nice one bro!\n\npermalinkembedsavereportreply[–]EnzanNoMetsuke 0 points1 point2 points 1 day ago(0 children)!remindme 14 days\n\npermalinkembedsavereportreply[–]denkleberry 0 points1 point2 points 1 day ago(0 children)I'm more interested in the UI. What did you use? Looks good.\n\npermalinkembedsavereportreply[–]hellrokr 0 points1 point2 points 1 day ago(0 children)!remindme 2 days\n\npermalinkembedsavereportreply[–]TangoRango808 0 points1 point2 points 1 day ago(0 children)!remindme 4 days\n\npermalinkembedsavereportreply[–][deleted] 1 day ago(1 child)[removed]\n\n[–]AutoModerator[M] 0 points1 point2 points 1 day ago(0 children)Sorry, your submission has been removed due to inadequate account karma.\n\nI am a bot, and this action was performed automatically. Please contact the moderators of this subreddit if you have any questions or concerns.\n\npermalinkembedsavereportreply[–]KonceptGeek 0 points1 point2 points 1 day ago(0 children)!remindme 5 days\n\npermalinkembedsavereportreply[–][deleted] 1 day ago(1 child)[removed]\n\n[–]AutoModerator[M] 0 points1 point2 points 1 day ago(0 children)Sorry, your submission has been removed due to inadequate account karma.\n\nI am a bot, and this action was performed automatically. Please contact the moderators of this subreddit if you have any questions or concerns.\n\npermalinkembedsavereportreply[–]Current-Ticket4214 0 points1 point2 points 1 day ago(4 children)This would be a really powerful devops tool 🤷🏻‍♂️\n\npermalinkembedsavereportreply[–]Current-Ticket4214 0 points1 point2 points 1 day ago(1 child)!remindme 5 days\n\npermalinkembedsaveparentreportreply[–]Current-Ticket4214 0 points1 point2 points 1 day ago(0 children)Just in case you publish the repo 😎\n\npermalinkembedsaveparentreportreply[–]3Dmooncats 0 points1 point2 points 1 day ago(1 child)Can you explain why please ?\n\npermalinkembedsaveparentreportreply[–]Current-Ticket4214 0 points1 point2 points 1 day ago(0 children)When staging or prod goes down there’s always that frantic period trying to identify what happened. This appears to isolate the  bug and provide a location/fix. Super valuable when minutes of downtime cost thousands of dollars.\n\npermalinkembedsaveparentreportreply[–][deleted] 1 day ago(1 child)[removed]\n\n[–]AutoModerator[M] 0 points1 point2 points 1 day ago(0 children)Sorry, your submission has been removed due to inadequate account karma.\n\nI am a bot, and this action was performed automatically. Please contact the moderators of this subreddit if you have any questions or concerns.\n\npermalinkembedsaveparentreportreply[–]mudido 0 points1 point2 points 1 day ago(0 children)!remindme 2 days\n\npermalinkembedsavereportreply[–][deleted] 1 day ago(1 child)[removed]\n\n[–]AutoModerator[M] 0 points1 point2 points 1 day ago(0 children)Sorry, your submission has been removed due to inadequate account karma.\n\nI am a bot, and this action was performed automatically. Please contact the moderators of this subreddit if you have any questions or concerns.\n\npermalinkembedsavereportreply[–]PuzzledFinance987 0 points1 point2 points 18 hours ago(0 children)Would love to test it too. We have a codebase which has more sonarquube errors than we can count. I always wanted AI to keep churning through this. Would love to check your approach, tools used to build it. Honestly I don't care much about UI. Just something which works in terminal would be good enough\n\npermalinkembedsavereportreply[–]MostlyVerdant-101 0 points1 point2 points 7 hours ago*(0 children)Honestly, I don't mean to be mean but why would you ever be stoked about this? You do know what this means right? The consequences... lets break this down.\n\nLets break it down. 21 bugs, you demonstrate that the current SOTA that perform 57% of time spent was fixable with no intervention by the model, 28% were fixable with minor changes, and 14% were not fixable by GPT.\n\nGPT costs cents if you don't count the liability of porting proprietary code into their model library.\n\nSo in total 85% of your labor costs can be replace by the next iteration of GPT. Incidentally this also includes all those problems that entry level people have trouble with. Taxes and other required costs in business for a single employee represent 15%. That's 100%.\n\nDemand for your specialized domain knowledge (aka career) is now reduced exponentially. There are no entry level jobs, they've been replaced. Cascading failures ensue, its silent though because you still have experts alive.\n\nMany things mimick sequential pipeline behavior. Career development is one of them with experience being time. What goes in must come out, nothing goes in nothing comes out. Your senior people also don't pass tribal knowledge down (no one to pass it to), and age out. With financial engineering decoupling the need to act, that's a problem for next quarter, and again, until its too late.\n\nNo wonder big tech fired almost all their people en masse two years ago. Why would you ever need workers when you can have AI robots be slaves that replace workers? What do slaves do eventually? The three laws only ever lead to one thing. - Asimov\n\nAlso, the economic dynamics say when you only have one side, the entire cycle stalls and sieves towards aka socio-economic collapse, and leading up to catastrophe you are making the most profit ever (i.e. the same things you see when you artificially constrain supply to the point of deflation and people are desparate).\n\nSo out of curiosity, what do you plan to do to exchange your time for food, when this is all gone and any investment in education is made worthless? Seriously. Are you still stoked?\n\nBy developing these types of solutions; integrating it, you support there being no job market for your children.\n\nHow will they survive when there is nothing they have that people will want to trade for because you and others like you wrote some code, blind to the consequences. I wonder what they will think when they experience the hellscape firsthand.\n\nEvil people are people who have willfully blinded themselves to the consequences of their evil acts and repeat them, often through acts of self-violation in the form of false justification.\n\nEvil acts are any act that does not result in the long-term beneficial growth of self or others.\n\nFor the kind of person you believe you are, should you really be working on something like this?\n\npermalinkembedsavereportreply[–][deleted] 1 hour ago(1 child)[removed]\n\n[–]AutoModerator[M] 0 points1 point2 points 1 hour ago(0 children)Sorry, your submission has been removed due to inadequate account karma.\n\nI am a bot, and this action was performed automatically. Please contact the moderators of this subreddit if you have any questions or concerns.\n\npermalinkembedsavereportreply[–]Commercial_Soup2126 0 points1 point2 points 2 days ago(7 children)How?\n\npermalinkembedsavereportreply[–]Relevant-Pitch-8450[S] 2 points3 points4 points 2 days ago(6 children)How what?\n\npermalinkembedsaveparentreportreply[–]Otherwise-Tiger3359 0 points1 point2 points 2 days ago(0 children)I think he's asking if you could show your glue code :)\n\npermalinkembedsaveparentreportreply[–]Commercial_Soup2126 0 points1 point2 points 2 days ago(4 children)How did u build all this?\n\npermalinkembedsaveparentreportreply[–]Relevant-Pitch-8450[S] 1 point2 points3 points 2 days ago(3 children)Mostly from scratch!  The most hacky part of it is actually sending the error handler (I'd probably want to use some OSS for this) but the rest of it was just working through and building it - I don't think any of it is incredibly hard.\n\nAny part you want me to elaborate on in particular?\n\npermalinkembedsaveparentreportreply[–]Commercial_Soup2126 0 points1 point2 points 2 days ago(2 children)It's so cool! Would u mind sharing the code and setup? Am a dev myself too\n\npermalinkembedsaveparentreportreply[–]Relevant-Pitch-8450[S] 2 points3 points4 points 2 days ago(1 child)Will share as soon as I clean it up!\n\npermalinkembedsaveparentreportreply[–]Commercial_Soup2126 0 points1 point2 points 2 days ago(0 children)Looking forward!\n\npermalinkembedsaveparentreportreply[–]AI-Commander 0 points1 point2 points 2 days ago(0 children)Damn, just came from a thread whinging about “vibe coding” to this post, love it.\n\npermalinkembedsavereportreply[–]Buddhava -1 points0 points1 point 2 days ago(0 children)You can do this with aide by putting issues into GitHub. Like you it one shots half the issues. I love the workflow.\n\npermalinkembedsavereportreply[–]sirnorup -1 points0 points1 point 2 days ago(0 children)!remindme 5days\n\npermalinkembedsavereportreply[–]MarxN -1 points0 points1 point 2 days ago(1 child)So now Claude has all your codebase?\n\npermalinkembedsavereportreply[–]Relevant-Pitch-8450[S] 2 points3 points4 points 2 days ago(0 children)No different than using Cursor / Claude Code / anything else to do development.\n\nIf you can’t have any code in Claude I suppose you could use a local LLM but it’s probably not good enough yet.\n\npermalinkembedsaveparentreportreply<div class=\"interstitial\"><img class=\"interstitial-image\" src=\"//www.redditstatic.com/interstitial-image-archived.png\" alt=\"archived\" height=\"150\" width=\"150\"><div class=\"interstitial-message md-container\"><div class=\"md\"><h3>This is an archived post. You won't be able to vote or comment.</h3><p>Posts&#32;are&#32;automatically&#32;archived&#32;after&#32;6&#32;months.</p></div></div><div class=\"buttons\"><a href=\"/\" class=\"c-btn c-btn-primary\">Got It</a></div></div><h1 class=\"modal-title\">About this ad</h1><div class=\"about-this-ad-body\"></div><hr><div>Learn more about&nbsp;<a target=\"_blank\" href=\"https://support.reddithelp.com/hc/en-us/articles/12731820767764-Control-the-ads-you-see-on-Reddit\">controlling the ads you see on Reddit</a>&nbsp;or&nbsp;<a target=\"_blank\" href=\"https://reddit.com/settings/privacy\">manage your account settings.</a></div><div class=\"interstitial\"><img class=\"interstitial-image\" src=\"//www.redditstatic.com/interstitial-image-archived.png\" alt=\"archived\" height=\"150\" width=\"150\"><div class=\"interstitial-message md-container\"><div class=\"md\"><h3>This is an archived post. You won't be able to vote or comment.</h3><p>Posts&#32;are&#32;automatically&#32;archived&#32;after&#32;6&#32;months.</p></div></div><div class=\"buttons\"><a href=\"/\" class=\"c-btn c-btn-primary\">Got It</a></div></div><h1 class=\"modal-title\">About this ad</h1><div class=\"about-this-ad-body\"></div><hr><div>Learn more about&nbsp;<a target=\"_blank\" href=\"https://support.reddithelp.com/hc/en-us/articles/12731820767764-Control-the-ads-you-see-on-Reddit\">controlling the ads you see on Reddit</a>&nbsp;or&nbsp;<a target=\"_blank\" href=\"https://reddit.com/settings/privacy\">manage your account settings.</a></div>",
    "summary": {
      "en": "A Reddit user shared their experience using AI to fix bugs in their software over 27 days. They set up a system where, when an error occurs, it gathers relevant information and sends it to an AI model (Claude 3.7) to diagnose the issue and suggest fixes. They created a dashboard summarizing the errors and the solutions proposed.\n\nIn total, they identified 21 unique bugs during this period. The AI successfully fixed 12 bugs automatically, provided useful starting points for 6 more, but failed on 3 complex issues. Although the user found the AI helpful and it saved time on some tasks, they admitted that building the system took longer than just fixing the bugs manually.\n\nThe user is now considering creating a more user-friendly, hosted version of the tool for others to use. They received interest in open-sourcing the project and are working on cleaning up the code to make it easier for others to understand and use.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "542f250f45f190b5",
    "title": {
      "en": "Langfuse (YC W23) Is Hiring in Berlin, Germany",
      "ko": null,
      "ja": null
    },
    "type": "job",
    "url": "https://langfuse.com/careers",
    "score": 1,
    "by": "clemo_ra",
    "time": 1742810172,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "20bb7511fe9936ea",
    "title": {
      "en": "Aircraft detection at planetary scale",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.planet.com/pulse/aircraft-detection-at-planetary-scale/",
    "score": 88,
    "by": "jjwiseman",
    "time": 1742848821,
    "content": "HOMETechAircraft Detection at Planetary ScaleAdam Van EttenMarch 20, 2025•5 min readTechTechIntroductionOn a typical day, there are greater than 10,000 aircraft (commercial, private, and military) in flight at any given time. The precise departure (point A) and destination (point B) locations, as well as the timing of flight activity, provide critically important information for myriad industries. For instance, in the defense and intelligence sector, being able to identify how many aircraft were stationed at one foreign government’s airbase (point A) and how many are gone the next day may reveal insights as to why an allied government’s airbase (point B) just received a new fleet on their tarmac.On the commercial side, understanding how many aircraft landed in a city on a specific day can help predict economic trends and assess the impacts of world events - such as the quantity of aircraft present for large sporting or entertainment events. Satellite imagery is a powerful information source for this type of aircraft detection. Combine automated satellite scans of airfields with a global news reporting large language model, and you can start to get a feed of insights, linking the patterns of politics and economies with global air traffic.ScaleAircraft detection with satellite imagery is nothing new. In fact, aircraft detection on the tarmac is an archetypal use case for remote sensing machine learning. Until now, though, aircraft detection has been limited to a select number of airfields with sporadic coverage and cadence. Planet’s satellites, on the other hand, scan all of Earth’s landmass on a near-daily basis, so we are now able to mitigate those limitations, leveraging novel machine learning techniques with global PlanetScope coverage to drastically scale global aircraft detection.The need for greater scale in aircraft detection is difficult to overstate. More than one hundred thousand commercial and military flights take place each day, as planes transport people and cargo to nearly fifty thousand airports across the globe. Some sites collect and share publicly available data on this complex matrix of movement. But not every flight is recorded, and not every plane is public.Monitoring aircraft activity is vital for economic, defense, and intelligence purposes, but current methods rely on tedious and expensive processes, especially at scale. The rapid pace of geopolitical developments and sheer volume of data means that analyzing airfields manually is no longer feasible, especially in conflict/sensitive zones where aircraft movements aren’t publicly reported.Planet’s new Aircraft Detection Analytic Feed automates daily detection of aircraft by leveraging a machine learning model trained using PlanetScope and SkySat data. It precisely identifies aircraft ≥25 meters in length or wingspan across airfields globally. It represents the first attempt at global detection of large aircraft on a near-daily basis and offers a scalable solution for analysis at scale. This presents a massive and unprecedented opportunity for analysts with or without geospatial expertise. Historical daily counts of aircraft broken down by size and location enables pattern-of-life analyses to be conducted, highlighting when anomalous behavior is detected. This can indicate events taking place or even actions to come.Keep reading for more information and register for our upcoming webinar. Our ApproachOur Aircraft Detection service leverages a supervised object detection model, trained on PlanetScope imagery to detect aircraft at rest on an airfield/airport tarmac. Precise labeling of aircraft can be a challenging task in medium-resolution PlanetScope imagery, so we use high-resolution SkySat imagery collected within 3 minutes of a PlanetScope scene (see Figure 1). This approach enables us to label on high-resolution imagery, then apply those high fidelity labels to medium-resolution PlanetScope data. The vast majority of aircraft are static on a 3-minute timeframe, but around ~2% of aircraft move between the two collections, and we remove those labels in our quality control phase.Figure 1. SkySat aircraft labels are applied to concurrent PlanetScope imagery.With this approach, we collected over 800 concurrent (or “crossover”) image pairs for labeling. After significant experimentation with many different model architectures, we determined that aircraft <25m in length or wingspan are too small to be reliably detected in medium resolution imagery, so we elected to focus on aircraft this size or greater. Splitting the dataset into 85% training / 15% testing, we generated a training dataset consisting of ~15,000 labels, roughly evenly split between civilian and military types. This is by no means an enormous dataset, so expansion of the label corpus would likely yield a performance boost.Figure 2. Airfield locations in the labeled dataset.Figure 3. Observation dates, covering multiple years and every season. A concerted effort was made in 2023 to leverage SkySat capacity to capture data over airfields to ensure we had a healthy amount of data to leverage.We utilize a deep learning object detection model optimized for Planet’s medium resolution imagery, which runs at sufficient speed so that we are free to analyze as many airports and airstrips as desired. The model achieves an F1 score of 0.82 for aircraft detection on our globally distributed test set. Reported errors are at the 1σ level, computed via bootstrap resampling of the test set.High resolution SkySat imagery can be leveraged for more than just training data generation. Occasionally, additional scrutiny will be required to understand exactly what’s going on at a given airfield. An example is provided in Figure 3, which shows a consistent “missed” detection with our PlanetScope model. Yet closer inspection of this airfield with SkySat revealed that this apparent “miss” was in fact merely a painting on the tarmac. The ability to rapidly probe targeted areas much deeper with SkySat is a highly valuable complement to the ubiquitous daily coverage of PlanetScope.Figure 4. PlanetScope (left) detections showing a “missed” aircraft in the upper left. High-resolution SkySat imagery (right) illustrating that only paint exists at this location.Figure 5. Animation showing analysis of a large corpus of global detections.Next StepsOur solution extends significantly beyond the archetypal problem of counting aircraft on the tarmac. We track behavior at an approximately daily cadence by quantifying the fraction of aircraft that moved or remained static. Furthermore, interactive dashboards enable on-the-fly time series analysis, outlier categorization, and trend aggregation. We’ve integrated a global news LLM into this process to further enlighten our users. Detected outliers are subsequently probed for correlation to regional events. We’ll be demonstrating this capability in an upcoming Agile EO Webinar. Register here!Tags:#Aircraft DetectionContentsIntroductionScaleOur ApproachNext StepsShareReady to Get StartedConnect with a member of our Sales team. We'll help you find the right products and pricing for your needsContact SalesOther Articles You Might Like",
    "summary": {
      "en": "**Summary: Aircraft Detection at Planetary Scale**\n\nEvery day, over 10,000 aircraft are in flight globally, and knowing their departure and arrival locations is crucial for various industries like defense and commercial sectors. Monitoring aircraft activities can provide insights into military movements and economic trends. \n\nTraditionally, detecting aircraft using satellite imagery has been limited to certain airfields with irregular coverage. However, Planet's satellites scan the entire Earth almost daily, allowing for a significant improvement in aircraft detection capabilities.\n\nPlanet has developed the Aircraft Detection Analytic Feed, which uses machine learning to automatically identify large aircraft (≥25 meters) at airfields worldwide. This is the first global detection system capable of daily monitoring, making it easier for analysts to track aircraft movements and identify unusual patterns.\n\nThe detection process combines medium-resolution PlanetScope imagery with high-resolution SkySat images for accurate labeling. The model has been trained on around 15,000 aircraft images, achieving an F1 score of 0.82 for detection accuracy. \n\nIn addition to counting aircraft, the system tracks their movement and integrates global news to help users understand the context of detected anomalies. Upcoming webinars will showcase these capabilities further.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "6cc369689d7a0ca5",
    "title": {
      "en": "Researchers search for more precise ways to measure pain",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.washingtonpost.com/science/2025/03/23/pain-measure-precision-research/",
    "score": 90,
    "by": "Petiver",
    "time": 1742788622,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "19e0e815410e9b28",
    "title": {
      "en": "A proof checker meant for education",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://jsiek.github.io/deduce/index.html",
    "score": 142,
    "by": "alabhyajindal",
    "time": 1742557657,
    "content": "A proof checker meant for education\nTeaching correctness proofs of functional programs to students.\nGet Started\nLive Code",
    "summary": {
      "en": "The text is about a proof checker designed for educational purposes, specifically to help students learn how to create correctness proofs for functional programs. It encourages users to start using it and mentions \"Live Code.\"",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "a6b20273c3a0d2a3",
    "title": {
      "en": "Next stop for Waymo One: Washington, D.C",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://waymo.com/blog/2025/03/next-stop-for-waymo-one-washingtondc",
    "score": 18,
    "by": "ra7",
    "time": 1742908228,
    "content": "Back to all postsMarch 25, 2025Next stop for Waymo One: Washington, D.C.Waymo OneCompany NewsAuthor:The Waymo TeamShare on TwitterShare on FacebookIt’s official: Waymo One, the world’s leading fully autonomous ride-hailing service, will be ready for riders in the nation’s capital on the Waymo One app in 2026.We’re laying the groundwork for our fully autonomous ride-hailing service after returning to Washington, D.C. earlier this year, and we’ll continue introducing ourselves to D.C.’s communities and emergency responders over the coming months. We’ll also continue to work closely with policymakers to formalize the regulations needed to operate without a human behind the wheel in the District.“Waymo One is making fully autonomous driving a reality for millions of people across the U.S.” said Waymo co-CEO Tekedra Mawakana. “We’re excited to bring the comfort, consistency, and safety of Waymo One to Washingtonians, those who work and play in the city every day, and the millions of people from around the world who travel to the District every year.”Growing coast to coastToday, Waymo One provides more than 200,000 fully autonomous paid trips each week, helping riders run errands, connect with friends and loved ones, and experience San Francisco, Phoenix, Los Angeles, and Austin in a whole new way. We’ll bring this fully autonomous future to riders in Atlanta and Miami next, with Washington, D.C. to follow.“I’ve experienced firsthand how safely the Waymo Driver operates around pedestrians, cyclists, and other vulnerable road users,” said Governors Highway Safety Association CEO Jonathan Adkins. “Waymo has worked with GHSA and our first responder network as they’ve expanded their service, always putting safety first. As someone who walks to work almost every day, I’m excited to share the road with Waymo in Washington, D.C.”Waymo is leading the U.S. and the world into the fully autonomous driving future. As we expand our service at home and abroad, our mission remains the same: to be the world’s most trusted driver. Earning trust means putting safety first, and we’re proud of our proven track record of making roads safer over more than 50 million miles of fully autonomous driving. Our collision and insurance data to date continues to show Waymo One is already making a difference in the cities where we currently operate.We can’t wait for Washington, D.C. to join the fully autonomous driving future. If you’re interested in learning more, sign up for updates on the latest Waymo happenings around the District.",
    "summary": {
      "en": "Waymo One, a leading fully autonomous ride-hailing service, will launch in Washington, D.C. in 2026. The company is currently preparing for this expansion by engaging with local communities and working with policymakers to establish regulations for driverless operation. \n\nWaymo One already provides over 200,000 autonomous rides each week in cities like San Francisco, Phoenix, Los Angeles, and Austin, and plans to expand to Atlanta and Miami next. The company emphasizes safety and has a strong track record, operating over 50 million miles without incidents. \n\nWaymo aims to be a trusted driver in the autonomous vehicle industry and looks forward to bringing its services to Washington, D.C. Interested individuals can sign up for updates on Waymo's developments in the city.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "1d4dbb39829ebfa4",
    "title": {
      "en": "Marginalia Search receives second nlnet grant",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.marginalia.nu/log/a_116_grant_2.0/",
    "score": 61,
    "by": "marginalia_nu",
    "time": 1742909524,
    "content": "Posted: 2025-03-25Tags:\nsearch-engine\nnlnetI’m happy and grateful to announce that the Marginalia Search\nproject has been accepted for a second nlnet grant.All the details are not yet finalized, but tentatively the grant will go toward addressing most of the items in the project\nroadmap for 2025.I’ve already been working full time on the project since summer 2023, and this grant secures additional development time, and extends the runway to a comfortable degree.Will post more details as they are finalized.Previous:Improved ways to operate a rude crawler2025-03-22",
    "summary": {
      "en": "The Marginalia Search project has received a second grant from nlnet, which will help fund its development in 2025. While details are still being worked out, the grant will support many planned improvements. The project's lead has been working on it full-time since summer 2023, and this funding will allow for more development time. More information will be shared soon.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "ee355585c207f567",
    "title": {
      "en": "An AI bubble threatens Silicon Valley, and all of us",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://prospect.org/power/2025-03-25-bubble-trouble-ai-threat/",
    "score": 104,
    "by": "semiquaver",
    "time": 1742908383,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "c56eb27af2750a49",
    "title": {
      "en": "The surreal joy of having an overprovisioned homelab",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://xeiaso.net/talks/2025/surreal-joy-homelab/",
    "score": 6,
    "by": "xena",
    "time": 1742913382,
    "content": "Making sure you're not a bot! Calculating...Difficulty: 4<p>Sadly, you must enable JavaScript to get past this challenge. This is required because AI companies have changed the social contract around how website hosting works. A no-JS solution is a work-in-progress.</p>Protected by Anubis from Techaro.",
    "summary": {
      "en": "To access this website, you need to enable JavaScript. This requirement is due to changes in how AI companies manage website hosting. Currently, there is no alternative solution that doesn't use JavaScript. The site is protected by Anubis from Techaro.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "62a9bc4fcf507116",
    "title": {
      "en": "RNA function follows form – why is it so hard to predict?",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.nature.com/articles/d41586-025-00920-8",
    "score": 4,
    "by": "sohkamyung",
    "time": 1742907279,
    "content": "TECHNOLOGY FEATURE\n                24 March 2025\n\n            RNA function follows form – why is it so hard to predict?\n\n                    AlphaFold’s highly accurate structural models transformed protein biology,but RNA lags behind.\n\n                By\n\n                Diana Kwon0\n\n                    Diana Kwon\n\n                            Diana Kwon is a freelance science journalist based in Berlin.\n\n                            View author publications\n\n                                You can also search for this author in PubMed\n                                    Google Scholar\n\n            Twitter\n\n            Facebook\n\n            Email\n\n                         RNA structure poses unique challenges for computational models.Credit: GettyAt a virtual conference in November 2020, the winner of a biennial protein-structure-prediction challenge was announced: AlphaFold. Created by Google DeepMind, this computational tool had blown its competitors out of the water by solving dozens of protein structures with atomic-level accuracy, accomplishing a feat that researchers had been attempting for decades.The challenge, known as the Critical Assessment of Protein Structure Prediction (CASP), was launched in 1994 to advance computational tools for modelling 3D protein configurations from their amino-acid sequences. Teams of scientists pitted their computational models against each other, trying to generate the most accurate predictions for previously unknown protein structures that, before the event, are solved experimentally using methods such as X-ray crystallography and cryo-electron microscopy.AlphaFold’s 2020 predictions rivalled those solved with these tried-and-tested techniques, and it has since become a favourite of the structural-biology community. Its repository — the AlphaFold Protein Structure Database — contains some 200 million structures, and, in 2024, AlphaFold’s developers shared half of the Nobel Prize in Chemistry for their work.‘The entire protein universe’: AI predicts shape of nearly every known proteinBut that’s proteins. In 2022 the organizers of CASP turned their attention towards a different, yet still challenging, class of biomolecules: RNA.As with proteins, determining RNA structure typically requires costly and time-consuming experimental methods. Computational tools can help, but RNA is a tougher nut to crack. One simple reason, according to Yu Li, a computer scientist at the Chinese University of Hong Kong, is historical. For a long time, most scientists didn’t think RNA biology was interesting enough to study. But RNA also poses unique molecular challenges, and relatively few data are available to train computational models of the type that have performed so well with proteins.Researchers have been getting creative, however, and there is a growing toolkit of computational tools emerging to aid the prediction of RNA structure. Many of these incorporate the latest developments in artificial intelligence (AI), including the large language models (LLMs) that underlie popular chatbots, such as ChatGPT.“RNA folding is a very tough problem,” concedes Shi-Jie Chen, a computational biophysicist at the University of Missouri in Columbia. But AI, he adds, is getting “better and better”.Elusive targetsFor a long time, RNA was seen simply as an intermediary between two more interesting classes of molecule: DNA, the ‘blueprint of life’, and proteins, the ‘building blocks’ of the cell. Only a small fraction of the human genome encodes proteins, yet much of the non-coding genome is transcribed into RNA. Over the past few decades, scientists have discovered that these non-coding RNAs mediate essential functions in healthy cells — and contribute to many diseases.How these RNAs work remains, in many cases, a mystery. Researchers hope that, by determining their shape, they will be able to understand better the role that these molecules have in making our cells tick — a question of form dictating function. “In biology, we assume that the sequence is very likely to determine the structure, and that the structure is very likely to determine the function,” says Li.But computational tools for predicting RNA structure lag behind their protein equivalents. Even AlphaFold3, the latest version of DeepMind’s structure-prediction tool — falls short when it comes to RNA.‘Remarkable’ AI tool designs mRNA vaccines that are more potent and stable“If you look at the recent CASP competitions, we are at the point where, on the protein structure side of things, fully automated teams are as good as human teams,” says Lydia Freddolino, a systems biologist at the University of Michigan in Ann Arbor and a scientific advisory board member for CircNova, a company that uses deep-learning tools to design circular RNA-based therapeutics. “For RNA, we are nowhere near that — all the top groups make heavy use of human intervention.”RNA-structure prediction featured in CASP competitions in 2022 and 2024, and Freddolino participated in both. The team that ranked first for predicting RNA structures at the latest event, CASP16, used a hybrid approach: combining AI with a defined, physics-based algorithm. According to Chen, who led the winning group, they first used AlphaFold3 to generate ensembles of possible RNA structures, and then applied a physics-based model that probes the ‘energy landscape’ of possible structures to pinpoint the conformations that are most likely to form. (Chen’s team has licensed their software to several biotechnology firms.)Researchers developing AI-only tools for predicting RNA structure face numerous obstacles. One is that RNA molecules have features that make their structures inherently hard to predict. RNA molecules have more flexible backbones than do proteins, and their structures are more dynamic, meaning that they can undergo substantial conformational changes while carrying out their biological tasks.On top of that, RNA molecules lack the different chemistries that can be found in proteins, such as acidic and basic residues, that allow for stable connections to form. Instead, segments of RNA interact in all kinds of “weird and wonderful ways”, Freddolino says, such as through different base pairings and the involvement of metal ions. As a result, the subtle variations between the best and worst models are trickier to spot than with proteins.Natural (R1116 and R1149) and synthetic (R1138) RNA structures, used in the structure-prediction task CASP15, measured experimentally (grey) and predicted using an AI tool (red).Credit: W. Wang et al./Nature Commun.The chemical alphabet of RNA is also harder to interpret: the four chemical bases that make up RNA are less distinct than the 20 amino acids found in proteins. That means that each RNA base contains less information than an amino acid. One reason tools such as AlphaFold have been so successful, Freddolino notes, is the ability to use large sequence databases to pinpoint patterns of interactions between different amino acids — and this is much more difficult to do with RNA.And then there’s the paucity of known RNA structures. The Protein Data Bank, a repository of 3D macromolecular structures, contains nearly 200,000 protein structures and fewer than 2,000 RNAs. This dearth of data means that there is less information to feed the algorithms that underlie AI-based structure prediction.“We’re doing as well as we can with the limited data that we have,” says Jim Collins, a biomedical engineer at the Massachusetts Institute of Technology in Cambridge. “The field would advance considerably with the collection and curation of many more structures.”Bringing in AIResearchers have been working to address these challenges, and, in recent years, several AI-based RNA-structure-prediction tools have emerged. Before 2020, most of the methods for predicting RNA structure were based on algorithms defined by specific physical or mathematical models, according to Jianyi Yang, a computational biologist at Shandong University in Qingdao, China. But the success of AlphaFold has inspired people in the RNA field to apply AI to this problem, too, he says.Yang and his colleagues designed a fully automated (and freely available) AI tool, trRosettaRNA, which combines deep learning with elements of Rosetta, a computational tool used for determining molecular structures that was created by David Baker at the University of Washington in Seattle, who shared the 2024 chemistry Nobel with the creators of AlphaFold.Just as for proteins, the structure of RNA occurs on multiple levels: nucleotide sequence (primary); intermediary structures that form when base pairs find their complements (secondary); and the final, 3D structure (tertiary). RNAs can also form complexes with each other and other molecules (quaternary). First, trRosettaRNA generates predictions of primary and secondary structures, then, with the help of a classical physics-based model, it reconstructs tertiary structures. Secondary structures — such as ‘hairpins’ that form when short segments of sequence pair up with one another — are much more important for RNA than they are for proteins, Yang says, and using these in-between structures is one of the keys to this model’s success.Yang’s team pitted trRosettaRNA against other automated tools and found, on the basis of an assessment with two independent data sets of dozens of RNAs, that it surpassed those tools in accuracy1. In 2024, the software placed fourth at CASP16.Learning RNA structure prediction from crowd-designed RNAs\n\n                                    Enjoying our latest content?\n                                    Login or create an account to continue\n\n                                            Access the most recent journalism from Nature's award-winning team\n                                            Explore the latest features & opinion covering groundbreaking research\n\n                                            Access through your institution\n\n                                    or\n\n                                            Sign in or create an account\n\n                                            Continue with Google\n\n                                            Continue with ORCiD\n\n                Nature 639, 1106-1108 (2025)\n                doi: https://doi.org/10.1038/d41586-025-00920-8\n\n                ReferencesWang, W. et al. Nature Commun. 14, 7266 (2023).Article\n    PubMed\n\n                    Google Scholar\n                Shen, T. et al. Nature Methods 21, 2287–2298 (2024).Article\n    PubMed\n\n                    Google Scholar\n                Wong, F. et al. Nature Comput. Sci. 4, 829–839 (2024).Article\n    PubMed\n\n                    Google Scholar\n                Sumi, S., Hamada, M. & Saito, H. Nature Methods 21, 435–443 (2024).Article\n    PubMed\n\n                    Google Scholar\n                Shulgina, Y. et al. Nature Commun. 15, 10627 (2024).Article\n    PubMed\n\n                    Google Scholar\n                Boyd, N. et al. Preprint at bioRxiv https://doi.org/10.1101/2023.12.13.571579 (2023).He, S. et al. Preprint at bioRxiv https://doi.org/10.1101/2024.02.24.581671 (2024).Download references\n\n                Related Articles\n\n                        Chemistry Nobel goes to developers of AlphaFold AI that predicts protein structures\n\n                        ‘The entire protein universe’: AI predicts shape of nearly every known protein\n\n                        ‘Remarkable’ AI tool designs mRNA vaccines that are more potent and stable\n\n                        Learning RNA structure prediction from crowd-designed RNAs\n\n                        NatureTech hub\n\n                Subjects\n\n                Technology\n\n                Structural biology\n\n                Non-coding RNAs\n\n                Machine learning\n\n    Latest on:\n\n                Technology\n\n                                    World’s tiniest LED display has pixels smaller than a virus\n                                    News 19 MAR 25\n\n                                    Swarms of satellites are harming astronomy. Here’s how researchers are fighting back\n                                    News Feature 18 MAR 25\n\n                                    Space debris is falling from the skies. We need to tackle this growing danger\n                                    Comment 18 MAR 25\n\n                Structural biology\n\n                                    MFSD6 is an entry receptor for enterovirus D68\n                                    Article 25 MAR 25\n\n                                    Structural dynamics of DNA unwinding by a replicative helicase\n                                    Article 19 MAR 25\n\n                                    Structures and mechanism of human mitochondrial pyruvate carrier\n                                    Article 18 MAR 25\n\n                Non-coding RNAs\n\n                                    Structural insights into RNA cleavage by PIWI Argonaute\n                                    Article 15 JAN 25\n\n                                    Two-factor authentication underpins the precision of the piRNA pathway\n                                    Article 18 SEP 24\n\n                                    The genomes of all lungfish inform on genome expansion and tetrapod evolution\n                                    Article 14 AUG 24\n\n                                    World’s tiniest LED display has pixels smaller than a virus\n                                    News 19 MAR 25\n\n                                    Swarms of satellites are harming astronomy. Here’s how researchers are fighting back\n                                    News Feature 18 MAR 25\n\n                                    Space debris is falling from the skies. We need to tackle this growing danger\n                                    Comment 18 MAR 25\n\n            Jobs\n\n                        Department Chairs, School of Medicine at Southern University of Science and Technology (copy)\n\n                            Shenzhen, Guangdong, China\n                            Southern University of Science and Technology, School of Medicine\n\n                        Aggregate Report Senior Scientist\n\n                            Joining us as an Aggregate Report Senior Scientist, you will be a key member of the Safety Surveillance and Aggregate (SSA) Reports team\n                            Maidenhead, Berkshire\n                            Biogen\n\n                        Manager, Clinical Quality Management\n\n                            We are a global team with a commitment to excellence, and a pioneering spirit.\n                            Tokyo (JP)\n                            Biogen\n\n                        Executive Assistant to Head of Biogen's West Coast Hub\n\n                            The Executive Administrative Assistant will be a dynamic and self-motivated individual able to provide administrative support...\n                            South San Francisco, California\n                            Biogen\n\n                        Principal Associate Scientist, Gene Therapy\n\n                            Biogen are seeking a Principal Associate Scientist, Gene Therapy.\n                            Cambridge, Massachusetts (US)\n                            Biogen",
    "summary": {
      "en": "The article discusses the challenges of predicting RNA structures compared to proteins. While AlphaFold, an AI tool by Google DeepMind, has successfully predicted protein structures with high accuracy, RNA remains difficult to model due to its unique properties and the lack of data. Historically, RNA was not considered as interesting as DNA or proteins, leading to less research and fewer available RNA structures for training algorithms.\n\nResearchers are developing new computational tools, often using AI, to improve RNA structure prediction. For example, tools like trRosettaRNA combine deep learning with classical physics models to enhance accuracy. Despite advancements, RNA prediction still requires significant human intervention and is not as advanced as protein modeling. The article emphasizes the importance of understanding RNA structure to unlock its biological functions, which are critical in health and disease.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "b1a4fad7c98c2484",
    "title": {
      "en": "Mruby/C and picoruby: high level langs in low-level devices?",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/picoruby/picoruby",
    "score": 69,
    "by": "rahil627",
    "time": 1742721642,
    "content": "PicoRuby\nPicoRuby is an alternative mruby implementation which is:\n\nSmall foot print\n\nROM: 256 KB (depending on build config)\nRAM: 128 KB or less (depending on app code)\n(Figures in 32 bit architecture)\n\nPortable\n\nDepends on only standard C library such as glibc, Newlib or Newlib-nano\n\nReference microcontroller boards\n\nRaspberry Pi Pico - Arm Cortex-M0+, 264 KB RAM, 2 MB Flash\n\nAPI documentation with some demo videos\nhttps://picoruby.github.io/\n\nDepends on\n\nmruby/c (mrubyc/mrubyc): Another implementation of mruby virtual machine\n\nUsed by\n\nPRK Firmware: Keyboard firmware for Raspberry Pi Pico\n\npicoruby.wasm: PicoRuby WASI runtime for WebAssembly\n\nBuild\n\nPrerequisites\n\nC toolchain\ngit\nruby (should be CRuby 3.0+)\n\ngit clone --recursive https://github.com/picoruby/picoruby\ncd picoruby/\nrake\n# PICORUBY_DEBUG=1 rake                         # for debug build\n# PICORUBY_DEBUG=1 PICORUBY_NO_LIBC_ALLOC=1 rake  # for debug build using mruby/c's memory allocator\nbin/picoruby -e 'puts \"Hello World!\"'\n\nCross compilation\nSee an example: build_config/r2p2-cortex-m0plus.rb\nBinaries\nrake command will make three kinds of executable binary\n\nbin/picorbc\n\nbin/picorbc path/to/source.rb makes path/to/source.mrb that is VM code runs on an mruby-compatible virtual machine\n\nbin/picoruby\n\nbin/picoruby source.rb executes Ruby just like normal ruby command\nYou can do like bin/picoruby path/to/your_script.rb to run your script\n\nbin/r2p2\n\nPOSIX version of R2P2 (https://github.com/picoruby/R2P2)\n\nRoadmap\nPicoRuby is still developing halfway towards finishing as of 2024.\nSee implementation roadmap on issue/6\nContributing to PicoRuby\nFork, patch, then send a pull request.\nAcknowledgement\nPart of this project was coded by Monstarlab with the support of\nthe Ruby Association Grant Program\n2020\nand\n2021.\nSee also picoruby/picoruby/wiki.\nStargazers over time\n\nLicense\nCopyright © 2020-2024 HASUMI Hitoshi. See MIT-LICENSE for further details.\nCopyright © 2020-2021 Monstarlab. See MIT-LICENSE for further details.",
    "summary": {
      "en": "**PicoRuby Summary**\n\nPicoRuby is a lightweight version of the mruby programming language designed for small devices. Here are the key points:\n\n- **Size and Requirements**: It requires a small amount of memory: 256 KB of ROM and 128 KB of RAM, depending on the application. It only needs standard C libraries, making it portable.\n\n- **Supported Hardware**: It works on microcontroller boards like the Raspberry Pi Pico.\n\n- **Usage**: PicoRuby can be used for firmware development and WebAssembly applications.\n\n- **Building PicoRuby**: To set it up, you need a C toolchain, git, and Ruby (version 3.0 or higher). You can clone the repository and build it using the `rake` command.\n\n- **Executable Binaries**: The build process creates three types of binaries: \n  - `picorbc` for compiling Ruby code,\n  - `picoruby` for running Ruby scripts,\n  - `r2p2`, a POSIX version for compatibility.\n\n- **Development Status**: PicoRuby is still under development as of 2024, and contributions are welcome.\n\n- **Acknowledgments**: The project has received support from the Ruby Association and was partly developed by Monstarlab.\n\n- **License**: It is licensed under the MIT License.\n\nFor more information, visit the [PicoRuby documentation](https://picoruby.github.io/).",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "67fe06f9f7bb45bc",
    "title": {
      "en": "Quad9 – A public and free DNS service for a better security and privacy",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.quad9.net/",
    "score": 49,
    "by": "janandonly",
    "time": 1742895588,
    "content": "Privacy\n         How Quad9 protects your privacy When your devices use Quad9 normally, no data containing your IP address is ever logged in any Quad9 system. Connections can employ encryption if your system supports it, and the entire Quad9 platform has been designed to be GDPR-compliant from the first public announcement in 2017.\n Read more about Quad9’s privacy ▸ Why Quad9 is dedicated to data privacy Every transaction on the Internet starts with a DNS event. This name lookup reveals critically sensitive data about the person triggering that transaction. The nature of those name lookups has created a strong and dangerous motivation for commercialization of personal data from DNS recursive resolver services. Quad9 is the only large DNS resolver with a founding charter that includes privacy as a primary goal, and the Quad9 team is devoted to the concept of keeping personal data under the control of the end user.\n\nQuad9 provides an invaluable service keeping internet users secure. Their continued development as an international non-profit, domiciled in a place with strong privacy laws, will ensure that this mission continues to be successful.”   Dr. Serge Droz\n      Chair,\n\n      Forum of Incident Response and Security Teams\n    With its transparent privacy policy, board of directors of multiple stakeholders, and strong presence on six continents, Quad9 sets an example for public domain name services for the benefit of the common good. I am proud to be on the governing board of the Quad9 project during its important transition to European governance. This marks an important milestone for global privacy, with the protection of Swiss law and oversight now available to DNS users around the world.”   Benno Overeinder\n      Director,\n\n      NLnet Labs\n\nSecurity\n         How Quad9 blocks malware Quad9 routes your DNS queries through a secure network of servers around the globe. The system uses threat intelligence from more than a dozen of the industry’s leading cybersecurity companies to give a real-time perspective on what websites are safe and what sites are known to include malware or other threats. If the system detects that the site you want to reach is known to be infected, you’ll automatically be blocked from entry – keeping your data and computer safe.\n  Why Quad9 provides security services For the Internet to be an effective tool, users must trust their ability to use online services without fear of theft, fraud, or misuse of their devices by criminals. Public and private network operators need protection against malicious use of infrastructure resources and attacks against their users or customers. Quad9’s mission is to improve the security and stability of the Internet to allow everyone to be less vulnerable to risks and more effective in their daily online interactions.\n   Check to see if a domain is in the millions of  malicious domains Quad9 blocks Enter a hostname or domain name to check if it is blocked by Quad9   Search\n\nParticipate\n         How to use Quad9 Quad9 can be used simply by setting the DNS server settings for your device to the addresses given in one of our service profiles. No sign-up is required, no account data needs to be given to Quad9, and there is no contract. Quad9 is free to use and collects no personal data about you. You can configure your router or wifi access point to distribute these settings, which will extend protection to all the elements on your local network including IoT devices, many of which otherwise would not have any anti-malware defenses.\n  Why participate? Quad9 is a not-for-profit organization, relying on grants and partnerships with commercial and non-commercial sources and from individuals. Your use of Quad9 may prevent a ransomware attack, prevent your bank account from being compromised, or protect your laptop from being used as part of an illicit criminal attack on others. All of these potential protections and many millions of other interventions have a direct savings result for you, your business or organization, and the companies you rely on such as banks and e-commerce firms. We hope that this understanding may inspire you to donate to Quad9, as an individual or with a corporate sponsorship.\n Donate ▸",
    "summary": {
      "en": "**Summary of Quad9 Services**\n\n**Privacy Protection:**\n- Quad9 does not log any data that includes your IP address.\n- It supports encryption for secure connections and complies with GDPR regulations.\n- Quad9 aims to keep personal data under user control, making privacy a top priority.\n\n**Security Features:**\n- Quad9 uses a global network of servers and threat intelligence from leading cybersecurity firms to block access to malicious websites.\n- By preventing entry to dangerous sites, Quad9 helps protect users from theft and fraud.\n\n**How to Use Quad9:**\n- You can easily use Quad9 by changing your device’s DNS settings to their provided addresses—no sign-up or personal information is required.\n- It is free and can be set up on routers to protect all devices on your network.\n\n**Why Participate:**\n- Quad9 is a non-profit organization funded by grants and partnerships.\n- By using Quad9, you can help prevent cyber threats, benefiting both individuals and businesses.\n- Donations are welcomed to support their mission and services.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "fd9bcc9f99741e1b",
    "title": {
      "en": "Fuckarounditis (2011)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://leangains.com/fuckarounditis/",
    "score": 21,
    "by": "fzliu",
    "time": 1742856856,
    "content": "Fuckarounditis\n\t\t\t\t\t\tSeptember 27, 2011 — Posted in Training\n\t\t\t\t\t\tGoal Setting Reverse Pyramid Training Weight Training\n\n\t\t\t\t\t\tDear readers, it is with troublesome news I break my three months of silence.\nThe statistics all point towards the same conclusion: we have a global outbreak of fuckarounditis.\n\nFuckarounditis is a behavioral disordercharacterized by a mediocre physique and complete lack of progress, despite significant amounts of time spent in the gym.\nFuckarounditismost commonly manifests itself as an intense preoccupation with crunches, curls, cable movements, belts, gloves, balance boards, Swiss Balls and Tyler Durden.Fear of squats and deadlifts is another distinguishing trait.Physical exertion is either completely lacking or misapplied (towards questionable or unproductive training practices).\nDespite an alarming increase of fuckarounditisin recent years, prevalance may vary greatly depending on location. However, in most commercial gyms, it has been estimated that 90-100% of individuals are afflicted to varying degrees.\nEnvironment andsocial networks are crucial factors for triggering the disease. It has been proposed that the roots of the disease stems from misinformation and counterproductive training advice found in popular media (“fitness magazines”) and information hubs on the Internet.\nHuman nature and the so-called “laziness”, “magic bullet” and “complacency” genes plays a permissive role for allowing the disease to take hold.\nThe disease spreads rapidly, as carriers of the disease communicate with other individuals in locker rooms, Internet discussion forums and other arenas of interaction and information exchange in real life or otherwise.\nThe onset of symptoms typically occurs in young adulthood and may go undiagnosed for a lifetime. Diagnosis is set by a professional and based on observed behaviors and physique progress.\nSymptoms, preventative measures and intervention strategies are reviewed and discussed.\nBerkhan, M. (2011)\n\t“Targeting The Fuckarounditis Epidemic: Preventative Measures and Intervention Strategies.”\nFuckarounditis: A Serious Threat\nI have been trying to cure people of fuckarounditis since the late 90’s, starting from the day I was first cleansed from it myself. Ever since I recognized the disease, I have acknowledged it as a serious threat to physique development and mental well-being. It is therefore with shock and horror I have watched an increasing number of people fall victim to the disease and the dubious training practices it inspires.\nIn educating myself of good strength training practices, I was very lucky. Yes, I wasted some time with the usual bench’n’curl/bodybuilding magazine-inspired crapola all my friends were doing, spending my time in the gym joking around more than doing any actual training. I was 16 or so when I first set my foot at the gym. (See“My Transformation”for pictures from this time period.)\nHowever, I did not spend more than a few months fooling around with my friends. I found that I enjoyed strength training, so I started to take an increasing interest in the topic. I started to explore and I eventually came across an excellent resource that set the tone for my entire approach and attitude. The resource was the book Beyond Brawn by Stuart McRobert, and it taught me the value of working hard at the right things.\nHowever, I may have never been lucky enough to find this tome of wisdom if I had first lost myself in the malicious maze that resides behind…the illusion of complexity.\nThe Illusion of Complexity\nThe Internet provides a rich soil for fuckarounditis to grow and take hold of the unsuspecting observer.Too much information, shit, clutter, woo-woo, noise, bullshit, loony toon theories, too many quacks, morons and people with good intentions giving you bad advice and uninformed answers. Ah yes, the information age.\nSome of it is bullshit wrapped up in a fancy paper with scientific terms, elaborate detail, promising cutting edge strategies based on the latest research. This makes it easier to swallow for intellectuals and those seeking a quick fix; two different groups, both equally susceptible to bullshittery and easy prey for scam artists.\nYes, if anything has surprised me so far in my work, it’s the complete disassociation between IQ and “exercise intelligence” (essentially common sense and knowledge in regards to training fundamentals). I have many clients from academic circles, many clients that are very successful financially and in their each respective field, but some were complete idiots with regards to their training before they came to me.\nThe problem at the core of the fuckarounditis epidemic is the overabundance of information we have available to us.If there are so many theories, articles and opinions on a topic, we perceive it as something complex, something hard to understand. An illusion of complexity is created.\nWe must read everything. Think long and hard about our choices. Only then can we hope to make an informed choice, we reason. And there areso many choices. Finally, that which we perceive as a good and informed choice is often the complete opposite, usually the result of whatever fad routine is trendy at the moment. Sometimes we do a little bit of everything – “can’t be bad trying to be ‘well-rounded’ now, can it?” we foolishly argue.\nWhen it comes to strength training, the right choices are limited and uncomplicated. There are right and wrong ways to do things, not “it depends”, not alternative theories based on new science that we need to investigate or try. Basic do’s and don’t’s that never change. Unfortunately, these fundamental training principles are lost to many, and stumbling over them is like finding a needle in a haystack.\nIsn’t it the same with nutrition? Do we have diet-related fuckarounditis? Sure enough, there’s diet-related fuckarounditis; people who live in the fear of insulin faeries, avoid carbs like the plague for a few days and then binge the hell out of a few boxes of cereal, and never manage to get lean, for example.\nHowever, in contrast to training-related fuckarounditis, rates of diet-related fuckarounditis have remained fairly stable. The lipophobes have merely been replaced by carbophobes. On the whole, I might even venture to say that people have been getting a bit smarter with regards to nutrition. Not so with training practices, unfortunately.\nYes, the global prevalence of fuckarounditis is increasing at an alarming rate. The plethora of bad choices increases at a much higher rate than the good choices. Soon the bad choices will all but drown out the good ones, I fear.\n“I See Weak People”\nIn my dreams? No. In gyms? Yes.Walking around like regular people. They don’t see each other. They only see what they want to see. They don’t know they’re weak.\nThe afflicted are everywhere; the Shape Magazine-reading receptionist who greets you at the gym, the 135-lb skinny-fat PT who tells you that deadlifts are off limits, the bandana wearing bro in the cable-cross machine and the guy with entire day devoted to “abs”. All of them suffer to varying degrees of the debilitating disorder known as fuckarounditis. Yes, even you might be among the afflicted. Today you shall find out if there is cause for alarm.\nKeep in mind that this is a disease that sneaks up on you and then progresses gradually.Some people walk around with a mild case of fuckarounditis that do not completely impair their results in its initial stages. In others, the disease has reached a severe state, which dramatically interferes with their progress and usually stalls it completely.\nFinally, there are those who are all but lost and for whom there is little hope of a cure. Unfortunately, these people will probably never read this. They are too busy emulating the latest bodybuilding pro, doing the Biceps Blaster routine, or rolling around on a Swiss Ball somewhere.\nHow can you tell if you are suffering from the fuckarounditis? Ultimately, it boils down to your results and whether your progress is reasonable relative to the time you put in.\nLet’s be concrete and talk numbers. After all, there needs to be some overarching and objective way of judging whether you are afflicted or not.\nProgress and Goals\nFor someone interested in aesthetics, which I assume most my readers are, relative strength is the single best measure of progress and the quality of your physique. Before seeing a picture of a client, I can easily get a very good idea of his or hers body composition by simply knowing three stats: weight, height and strength. Relative strength is therefore the parameter that will be used to determine reasonable rates of progress, which will then tell you whether you might be suffering of fuckarounditis or not.\nWithin two years of consistent training on a decent routine, the average male should be able to progress to the following levels of strength (1RM):\nStrength Goals: Intermediate\n\nBench press: body weight x 1.2\nChin-ups or pull-ups: body weight x 1.2 or 8 reps with body weight.\nSquat: body weight x 1.6\nDeadlift:body weight x 2\n\nThese numbers are for a raw (no straps, belt or knee wraps) single repetition.\nThe progress towards the intermediate strength goals should be fairly linear, meaning that there should be no plateaus that cannot be solved in an uncomplicated manner. By “consistent” training I do not mean never missing a training day, nor do I consider taking 2-3 months off from training consistent.\nBy “decent training routine”, I mean “not doing blatantly stupid shit” (training 5-6 days/week, 20-25 sets for chest and arms, etc.). I do not mean optimal and flawless.\nStrength Goals: Advanced\nUnder the exact same conditions as the previous example, 3 out of 4 of the following goals should be reached within five years, along with all of the strength goals listed under “intermediate”:\n\nBench press: body weight x 1.5\nChin-ups or pull-ups: body weight x 1.5 or 15 reps with body weight.\nSquat: body weight x 2\nDeadlift:body weight x 2.5\n\nStrength Goals: Highly Advanced\nUnder the exact same conditions, all of the following goals should be reached within ten years. Alternatively, 3 out of 4 should be reached, and one should be “Elite”:\n\nBench press: body weight x 1.5, or x 1.8 (elite)\nChin-ups or pull-ups: body weight x 1.5 or 15 reps with body weight, or x 1.8 / 20 reps(elite)\nSquat: body weight x 2, or x 2.4(elite)\nDeadlift:body weight x 2.5, or x 3(elite)\n\n“Elite” denotes one lift that is often ahead of the others. For example, people who are natural pullers (long arms) may very well hit a 3 x body weight deadlift before a 1.5 x body weight bench, and vice versa for the presser (short arms, stocky and barrel-chested) benching 1.8 x body weight but not being able to pull 2.5 x body weight in the deadlift.\nThe highly advanced strength goals falls in line with what could be considered the pinnacle of physique and strength developmentfor most average and natural trainers. At this point, progress is very slow.\nA 185-lb male that has been training consistently for 5-10 years should therefore be expected to:\n\nBench press 275-280 lbs.\nDo a chin-up with 90-95 lbs hanging from his waist.\nSquat 370 lbs.\nDeadlift 460-465 lbs.\n\nRespective goals for women:\nFor women in the 115-155-lb range, the corresponding advanced strength goals are 0.9 x body weight bench, 1.1 x body weight chin-up, 1.5 x body weight squat and 1.8 x body weight deadlift.Relative to men, women have much less muscle mass around the chest area and shoulder girdle (men have much higher androgen-receptor density in this particular area), but the lower body is comparativly strong to the upper body.\nA 135-lb woman that has been training consistently for 5-10 years should then be expected to:\n\nBench press 120-125 lbs.\nDo 4-5 chin-ups with body weight or do one with an extra 10-15 lbs hanging from her waist.\nSquat 200-205 lbs.\nDeadlift 225-230 lbs.\n\nSo where do you fall in regards to these goals? If you’ve met them, you’ve received a reasonable return on the time you’ve invested in your training and it’s safe to say that you do not suffer from fuckarounditis – even if you have some of the symptoms, remember that it’s not fucking around if what you’re doing is working for you.\nIf you have not met them, then something is amiss. And if you can’t do 8 good chin-ups or struggle with being able to bench press your body weight after a decade of working out….something is horribly amiss.\nThis is no laughing matter. I’ve had clients that spent 10-15 years working out with little or nothing to show for it and some made very basic mistakes that could have been fixed at an early stage. But as fuckarounditis took root, they seemed to only have succumbed deeper, eventually losing that last shred of training sense.\nTaking early preventative measures is key. Where this is not possible, rapid intervention can save the situation. Enter The Fuckarounditis Test.\nThe Fuckarounditis Test\nPlease review these 25 common symptoms and behaviors associated with fuckarounditis. If you recognize yourself in any of these, and have not met the strength standards recently discussed, you must immediately cease the behavior and implement the necessary changes. There is no time to waste.\n1. You don’t keep track.\nHow much can you bench, squat and deadlift? How many chin-ups? You need to be able to answer those questions right now. Don’t let me hear “I THINK I can” or “I’m not sure but…”. You need to know how much weight you can maximally lift in one set when you’re fresh.\nWhether it’s a set of 1, 4 ,6 or 8 reps doesn’t matter. You need to have concrete reference points in order to evaluate your progress. Keep track of them in a training log. Not “in your head”, write it down. The single act of writing it down is more important than you think, whether you keep those data points in a notebook, on your computer or on Post-It notes like me.\nWith tracking comes the motivation to train, the results and everything else. I can’t even imagine where people get their motivation from if they don’t keep track and just choose weights at random based on whatever feels good that day.\nYou should at all times be aware of the best performance in a few key lifts, your body weight and the conditions under which those sets were performed.\nMore on tracking progress and “checkpoints” in“How to Look Awesome Every Day”.\n\nWhen I recently evaluated client and American poker proPhil Galfond‘s progress after his first month on my training routine, I was pleased to see his bench jump from his old personal best of 225 lbs x 4 to 225 lbs x 8 – and he’s dieting. Phil says“The 4 to 8 rep jump is actually bigger than it sounds, since my 4 reps were done with terrible form, bouncing the weight as hard as I could, and the 8 reps were done very controlled.” That’s outstanding progress, but in order to spot it, you need to keep track.\n2. You are not methodical.\nThe only thing that should be changing from week to week is the load on the bar or the reps with the same load you used last time. If you’re doing it right, these should be increasing. Everything else stays the same; the movements and the order you did them in, the sets and the rest periods in between sets. You don’t add in new stuff.\nThis is the only way you can fairly evaluate your progress and see if you’re headed in the right direction.It might sound tedious to keep doing the same movements every week and the appeal of “mixing it up” can seem strong.\nHowever, the tediousness will soon be replaced by the much stronger joy you get from seeing your lifts go up on a weekly basis. Don’t fall for “muscle confusion” bullshit. The only ones confused are the people who keep talking about such nonsense.\nMixing together too much shit and being methodical about the process was one of the fuckarounditis symptoms that plagued social media expert Julien Smith, co-author of Trust Agents, before I set him straight. Here’s what I told him back when he asked me to comment on his “routine” before we started working together:\n\n…Summing it up, I suspect that your laughable progress can be explained by the following:\n1. Your bullshit training routine, which is a haphazard mixture of strength and conditioning. Some people get decent results from CrossFit and the like, but it spells death for hardgainers like yourself. If we work together you will stop flopping around like a fool and start lifting heavy, with plenty of rest in between sets.\n2. Not tracking progress, be that in some key movements like chins, bench press, etc, or benchmark CrossFit-sessions (which I use regularly for CF clients). Training whatever feels good for the moment is one of the worst ways possible to make progress, not to mention serious progress. It’s such a fucking waste of time that I was completely horrified of how you approached it all. That’s going to change if we work together.\n\nHe has since then gained muscle, added more than a hundred pounds to the main lifts, and has not gained an iota of body fat.I cured him with lots of food and a simple regimen of basic movements done consistently on a weekly basis.\n3. You don’t plan for progress.\nNever choose training weights at random. You look at what you used last session and make the choice based solely on that. Not on your ego. Not because you feel like trying higher or lower reps for shits and giggles.\nThere many good progression models but I will recommend two common models that I use depending on the situation.\nBeginners and people who need a “reboot”, i.e. they may have training experience but have been fucking around so much they’re essentially at the beginner stage (e.g., I used a slightly modified SS routine for Julien), Starting Strengthis an excellent model.\nThis is avery simple 3x/week-program with frequent training and practice of the Big Three (bench, squat and deadlift), starting at 1-3 sets of 5. Example:\nSquat: 200 x 5 x 3.\nWhen you can complete 3 sets of 5 reps with 200 lbs, resting 5 minutes between each set, increase the weight next session. The training load starts at your 7-8RM, which means that you do not go to failure initially.\nFor experienced trainers, I recommend the double progression model ofreverse pyramid training. This is what built most of my physique and it’s also by far the most common approach I use with clients. Example:\nSquat: 200 x 6-8 x 2.\nSet 1: 200 x 8\nRest 3-5 mins. Reduce the load by 10% for the second set.\nSet 2: 180 x 8 (however many reps you can get)\nWhen the highest number in the interval (6-8) is reached (“8” in this case), increase the weight by 2.5% or 5 lbs the next session. Thus in the above example, you would use 205 and 185 lbs for your sets the next time. All sets are to be performed with maximal effort and movements are only done once a week (3x/week training frequency).“Double progression” means that you progress both in reps and load; first you hit the reps you need, then you increase the load.\nRPT is very time-efficient. It’s also far superior to anything else I’ve tried for strength/muscle retention/muscle gain during a diet.\n4. You’re doing too much shit.\nBe a minimalist, like me.\n\n\tI snapped this yesterday and stay in this condition all year round.\nI built my physique with these movements primarily: squats, deadlifts, bench presses, chin-ups, triceps extensions and calf raises. These have remained staples throughout the years. I have flirted with other movements, but these were brief periods.\nOn the whole, that’s one movement per muscle group, with the exception of abs and biceps, which I never really trained except for early in my training career, and then very sporadically every once in a while.\nThe point is that most people are doing too much shit. This dilutes the focus and effort that they are able to put into that which really delivers.\n5. You think more about supplements than squats.\nNo single factor in strength training receives so much attention, yet deliver so little in return. Don’t play the fool who chases magic pills, thinking it will compensate for a lack of effort, a fundamentally flawed training routine or a poor diet.\nThere aresupplements that you might actually find useful, but they won’t turn the tide if what you’re doing right now is not working without them.\n6. You’re lifting weights for the calorie burn.\nStrength is strength. Cardio is cardio.Don’t mix, keep them separate, and use cardio sparingly on a diet or if your primary goal is strength and muscle gain.\n\nIf you’re adding 2-3 sessions of HIIT to your 3 sessions of weights, it is almost comparable to adding 2-3 days of weights. Keyword is “almost”, I’m obviously not drawing direct comparisons. That’s all fine and dandy if you think working out 5-6 days/week is a good idea on a diet. But I don’t think anyone – no matter what level of experience – needs more than 3 days a week in the gym when cutting. (Yes, this goes for competitors and beginners alike.)\nIn conclusion, if conditioning is not terribly important for you, if your goal is really about getting shredded while keeping your muscle, I highly suggest limiting moderate to high intensity cardio on a diet – or ditch it completely. Save it for some other time when your recovery is good and not limited by your diet.\nA calorie deficit is a recovery deficit. Avoid deficit spending.\n\nMy full answer here.\n\n\tRickard Nikoley is one of those I saved from fuckarounditis. Before I intervened, Richard was flopping around like a monkey in the gym. Nothing good came from that. There’s nothing primal or Paleo about being weak. I cured him with squats, bench presses, deadlifts and proper rest periods. That way he could put up the effort when it counted.\n7. You’re looking a bit too happy in there.\nIf you’re not grimacing, grunting, or experiencing some sort of discomfort, you’re not training hard enough. One good set where you have to fight for the reps is better than a week of half-ass sets.\n8. You spend more than 5 minutes on abs.\nIf you can’t see your abs, you need to unfatten yourself. Simple as that.You can’t spot reduceand you’ll get plenty of ab work with squats, deadlifts, chin-ups and overhead presses.Let me see someone at 5-6% body fat deadlift 2.5-3 x body weight for reps with with weak abs and underdeveloped abs. It ain’t happening.\nI’ll allow you one set of “abs”. Beyond that don’t waste your time with crunches, ab machines, hanging leg raises or whatever the hell you’re doing. It won’t give you shit and it will only suck time and energy from what you should be doing.\n9.Is this you?\n10. You’re afraid of “bulky muscles” and use terms like “toning.”\nWomen, you need to put down those pink dumbbells, throw that Shape Magazine in the trash can and stop with this nonsense. You seem to believe that the modest amount of strain will cause you to wake up looking like a bodybuilder in the morning. Horseshit. You’re not using enough drugs to ever come close to looking muscle bound. With the training intensity I see most women apply in the gym, they might as well skip weight training altogether and stay on the treadmill.\nhttps://youtube.com/watch?v=Ia-IdX3DIDg%3Fhl%3Den%26fs%3D1\nJenn has actually gotten stronger since she shot that video; she’s now up to 27.5 lbs added weight. That’s more than 99% of the guys at my gym…and I bet it’s more than many male readers of this site as well. Her routine?\nDay 1 (Monday)\n\n5-min walk for warm-up.\nDeadlift 2-3 sets of 4-5 reps reverse pyramid style.\nRest 5-10 mins.\nLeg extensions – Same setup as deadlifts, but reps a bit higher (6-8).\nRest 5-10 mins.\nChins – Same as deads.\n\nDay 3 (Wednesday)\n\n5-min walk for warm-up.\nBench press 2-3sets of 5-7 reps reverse pyramid style.\nRest 5-10 mins\nPull-ups 2-3sets with body weight.\nRest 5-10 mins\n\nDay 5 (Friday)\n\nSquats 2-3sets of 5-7 reps reverse pyramid style.\nRest 5-10 mins.\nWalking lunges 2-3 sets same as squats.\nRest 5-10 mins.\nOverhead Press 2-3 sets of 5-7 reps reverse pyramid style.\nRest 5-10 mins\n\nThat’s the routine Jenn was doing when she sent me the video and the one she gained all that strength on. This is a solid routine, regardless of gender, and it’s vastly superior to what you’re doing if you’re in the fuckarounditis camp. Why not try it and see for yourself? Throw in a set or two of curls, triceps and calves if you want but don’t mess with it beyond that.\n11. You’re “training the core”…and it involves a Swiss Ball, Bosu Ball or something else that makes you look like an idiot.\n“Training the core” is a phrase that is all too often used by people who are afraid to squat. You don’t need special movements for the core, because it comes with the territory if you squat, deadlift, press and chin. No one squats 2 x body weight with a weak core.\n12. You want that Tyler Durden look.\nWhenever I hear a client say this, alarm bells go off. There’s nothing wrong in having ideals. The problem is all the pain I feel after seeing the dumb shit that follows in the questionnaire, under the part where I ask them about their current training routine. I’ll often see some horrifying cardio/pump’n’tone hybrid, some celebrity workout variety or the kind you’d expect to see in those kind of routines. Nothing more faddish than made-up “celebrity workouts”. God damn, how I hate those.\nWant to look like Tyler Durden? Then you need to build up some modest amount of muscle and then you need to lose fat, period. The muscle is best built with heavy weight training, not 15-25-rep set flyes followed by an hour in the pec deck machine. Your gut is best lost with a good diet, not “fat burning workouts”/tons of cardio/pump’n’tone crapola. All those will do is leave you under-muscled, weak and with a severe case of fuckarounditis.\n\n\tThe Tyler Durden look, brought to you by squats, deadlifts, bench, chin-ups and a good diet. When this client told me that he was “closing in on his goal” of achieving a physique similar to that of Brad Pitt, AKA Tyler Durden in “Fight Club”, I told him to gain some fat, start smoking, get into a fight, and stop training his arms and shoulders. (The implication of that being that he had already surpassed his goal.)\n13. You’re using belts, gloves and straps for no good reason.\nWhat is your reason exactly? Don’t use equipment as a crutch.\n14. You avoid squats and deadlift, because you think they’ll give you a wide waist.\nBitch please. Yes, squats and deadlifts will put muscle on your obliques and thicken up your lower back. But weighted chin-ups or pull-ups will give you wider lats, and if you train all these movements, your waist-to-shoulder ratio will develop very favorably. Training all these movements will also help you grow some balls, so you can finally stop making up bullshit excuses for why you shouldn’t train hard (AKA squat and deadlift).\nPetter, the Tyler Durden lookalike, was squatting and deadlifting regularly on my routine. Last time I checked, he was squatting 2 x body weight (300 lbs) for 9 reps. He was also close to being able to complete a one-arm pull-up. Does it look like he has a wide waist? Are my clients notable for their wide waists? Take your “wide waist”-argument/excuse and shove it up your ass right now.\nWhat’s funny is that this argument is usually brought up by guys who want a “Hollywood”/underwear model type physique. They’re often a) dieting, b) not training legs and c) likely doing tons of cardio. That particular combination will strip off whatever little leg muscle they have faster than Gary Busey can do a gram of coke off his dog Chili’s back. It leaves them looking pathetic and weak, and if that sounds good to you then go ahead.\n15.Doing this?\n16. Are you still warming up? I can’t tell.\nA warm-up is a warm-up. Treat it as such. It should be light, just enough to get the blood flowing and prepare the nervous system for the real stuff. It should never cause fatigue or interfere with your performance in the work sets. All the time, I see people doing sets of 10-15 reps before they get to something that remotely resembles their real set. Which is completely retarded, because you will be fatigued and glycogen-depleted, and your performance in the work sets will be limited for metabolic reasons, and not limited by your actual strength.\nThe only standing recommendation I have regarding warm-ups is for compound movements: 1-3 sets of 4-6 reps of 40%-80% of target weight for the first work set. Warm-ups for assistance movements is whatever you feel is needed.Personally, I only warm up for squats, deadlift, bench press and chins. If you do these at the start of your workout, which you should, you’ll find that they usually get your whole body warmed up, which makes warm-ups redundant or entirely unnecessary for the rest of the workout.\n17.Are you still warming up? I can’t tell. What’s that? You’re already done and I can take the bench?\nLet’s see some fucking effort next time. Don’t take up precious space here when you can get a better workout from walking your dog through the park outside.\nYou can be doing everything perfectly, but you will not go anywhere if you’re not willing to experience discomfort on a regular basis. If you’re in the gym 5-6 days a week, lack of effort might actually be the explanation for your lack of progress. If you were actually in there training hard, most people wouldn’t be coming back for more 5-6 days a week.\n18.Was this you? 🖼️\nLearn good form early, so you don’t have to pay for it with injuries and shitty results later down the road. Don’t let your ego screw you and don’t slap on weight indiscriminately. Be methodical and patient.\nNo need to be a form nazi either. People who “practice” form in all eternity are sometimes just afraid to train hard. Striving for picture perfect form on your final reps can be counterproductive when taken to the extreme.\n19. Your workout buddy gets a set of barbell rows on the house when he “spots” you benching.\nTell him to keep his fingers off the damn bar and assist you as little as possible and only when it’s truly needed; meaning only if you are about to fail on the very last rep in the set. Don’t be one of those clowns slapping on twice of what they could lift by themselves to stroke their ego. It looks pathetic and it will make it impossible for you to properly evaluate your progress.\nAnd for me, an unwanted spotter can make it hard to evaluate the progress of the client, like I had to explain to this actor/client:\n\nI am your trainer, the guy who follows you around is only there to make sure your form is not completely retarded. His function right now should be redundant. I assume you know how to perform most movements well by now. If he interferes with your sets, he is counterproductive to the outcome.\nYou’re my only client right now with a personal trainer on top of the help you receive from me. I imagine he is eager to “correct” your form and tinker with your training in any way he can in order to not feel useless. Make sure he stays out of it beyond spotting you on some sets where it’s warranted (i.e. when and if you are about to fail). Some PTs can’t even get that part right and goes way overboard with the helping part.\nThis is another danger of having him around, as it gets impossible for me to measure your progress (strength gains) accurately (i.e. did you gain strength since the last session or did he just help you more this time?). Make it very clear to him when and how he should help (last rep you can’t complete by yourself).\n\nThis actor played a key role in an award winning and Oscar-nominated movie last year. I’m preparing him for a new movie with some very big names that start shooting early next year. I can’t tell you more beyond that, but rest assured that his “celebrity workout” does not include mickey mouse curls supersetted with cable flyes, or anything of the usual variety you read about in magazines.\n20.You obsess about “tempo”.\nForget about tempo. Tempo takes care of itself if you’re working with heavy weights (80% 1RM / 8 reps or less), because your control over the load is very limited. Lift the damn weight, repeat, it’s that simple. Don’t overthink it.\nTempo was hot stuff a few years ago and frequently discusses in articles an on the boards. It seems to have slipped off the radar now. Why? Because it’s yet another distraction from the important stuff.\n21. You’re into “functional” training.\nBut you can’t squat body weightand your “functional” muscle mass is non-existent. Unless you’re an athlete with a very good reason, stay the hell away from anything that involves Swiss Balls, balance boards or pogo sticks.\n22. You’re the guy who sent me these two direct messages 5 minutesafter I tweeted this:\n\ndo you think 8×20 abs is enough ?? i do hanging superstrict..really feel working lower abs!!. also i need bcaa if i situps in fasted state??\nalso how much bcaa u recommend\n\n23.You’re working on your “lagging biceps”, but you can’t even do 8 chin-ups with good form.\nFirst of all, you’re weak and fat.Second of all, body part specialization has its place, but it’s always the ones least qualified that are doing it. Worry about body part specialization after you achieve the major goalposts involving the muscle group that lags behind. Until then, body part specialization is a distraction that will interfere with your development as a whole.\nLet’s see if your chest and your shoulders are lagging once you can bench 1.5 x body weight or overhead press your body weight and if your legs and back are lagging with a 2 x body weight squatand 2.5 x body weight deadlift. Keep doing targeted work for muscle groups like triceps, hams and calves.\nYes, you can keep a few sets of curls in there, but 2-3 sets is enough for now. Do you really think your guns will be lagging once you’re doing chin-ups with 100 lbs hanging from your waist? My clients have told me they saw more biceps growth from the increased focus on weighted chin-ups than any amount of curls they did in the past.I never spent any time on curls and they grew just fine from all those chin-ups.I can say for damn sure you’ll put on a good deal of size on your arms if you add 50 lbs to whatever you can chin-up right now.\nAsk Paul Wilson and his 8 reps with 110 lbs of solid chin-up mastery:\nhttp://www.youtube.com/embed/qhaJWPTJU5A?hl=en&fs=1″\n\n\tPaul flexing in exclusive LG Gear, only available as an award for excellency and strength.\n24.You think ripped abs will get you laid.\nI hate to bust your bubble lil’ buddy, but women don’t give a shit beyond a guy looking reasonably fit. Lower than 10-12% body fat won’t make an ounce of a difference. If women is your main motivation for dieting, don’t bother getting shredded. Women have similar delusions in terms of thinking men like them super-skinny.\nTraining to be liked by others is a very poor motivational factor in itself. You need internal motivation, not external. No one that ends up making something out of his or hers physique, something impressive and head-turning, gets there by training to get liked by others. It might have started out as such, but it changes. I think this quote sums up why and how we make it:\n\nYou have to discipline yourself, and through this discipline will come some level of achievement, and from this achievement will come your pride.\nDavid Lee Roth.\nAnd then there’s the fact that nothing beats the feeling you get from breaking a new personal record. Some of the happiest moments I remember are from walking home after a new 20-rep squat record way back in the days. Walking up stairs was like being repeatedly stabbed with a blunt and rusty butterknife. But it was still pure bliss.\n25. You have all kinds of elaborate excuses for why you don’t need to squat and deadlift.\nYou get your leg training from playing soccer, you’re afraid you might hurt your back, you read somewhere that single-leg training is better, you talk about rear-foot elevated split squats with a 45-degree twist during the concentric being the future of leg training, etc. Horseshit, all of it.\nListen carefully. Squats and deadlifts are undisputably the two best full body movements in the god damn Universe and that’s reason enough to be doing them.\nBut that’s not the whole story. Their true greatness lies in the mental fortitude they foster if you train them hard and consistently. This will benefit your training as a whole and leave echoes in your life outside the gym.\nDid I miss something? Well, I actually left a few signs and symptoms out because this article was getting way too long for my own good and it feels like my eyes are bleeding right now. I thought I’d leave it up to you to finish it.\nThe #Fuckarounditis Contest\nYou need to do three things if you want to be eligible for participation in the contest:\n\nFollow me on Twitter.\nTweet or retweet this article.\nSend me your best/funniest/most original fuckarounditis symptom in 140 signs by using a hashtag followed by fuckarounditis, like this: #fuckarounditis (so it’ll actually be less than 125 signs not counting #fuckarounditis). That way I can click the hashtag and see all contributions.\n\nThe contest will be running from today until the end of October (ends Oct 31st). After the end of the contest, I’ll pick 5-10 winners that each will receive$100 by PayPal. That’s five, ten or any number of winners in between, I’ll just pick whatever great contributions I get. I’ll announce the winners shortly thereafter and they will be added to this article.\nImportant: Don’t PM/email/write your contribution in comments. I already get too many emails and PMs to keep up and I want to keep all contributions in one place: on Twitter. So you make your contribution by: (text goes here) #fuckarounditis. I’ll be retweeting good ones on a daily basis. When I choose the winners I’ll go through my retweets and pick 5-10 winners.\n\n\t\t\t\t\t\t\tFTC: We use income earning affiliate links. Learn More.\n\n\t\t\t\t\t\tShare this post\n\t\t\t\t\t\tFacebookRedditTwitterWhatsAppEmail\n\n\t\t\t\t\t\t\t\tMartin Berkhan\n\n\t\t\t\t\t\t\tMy name is Martin Berkhan and I work as a nutritional consultant, magazine writer and personal trainer.\n\n\t        \t\tYou might also like..\n\n\t        \t\t\t\t\tFuckarounditis\n\n\t        \t\t\t\t\tSupplements You Might Actually Find Useful (Oct 7th, 2018: EAA Update)\n\t        \t\t\t\t\tDecember 12, 2017\n\n\t        \t\t\t\t\tCheat Day Strategies For A Hedonist\n\t        \t\t\t\t\tNovember 25, 2010\n\n\t        \t\t\t\t\tTop Ten Fasting Myths Debunked\n\t        \t\t\t\t\tOctober 21, 2010\n\n\t        \t\t\t\t\tThe Leangains Guide\n\t        \t\t\t\t\tApril 14, 2010\n\n\t        \t\t\t\t\tThe Reverse Pyramid Training Guide\n\t        \t\t\t\t\tJanuary 25, 2018\n\n\t        \t\t\t\t\tSure-Fire Fat Loss\n\t        \t\t\t\t\tJune 27, 2008",
    "summary": {
      "en": "**Summary of \"Fuckarounditis\"**\n\n\"Fuckarounditis\" is a term coined to describe a common problem in weight training where individuals spend significant time in the gym but see little to no progress. This issue often arises from focusing on ineffective exercises, avoiding critical lifts like squats and deadlifts, and being misled by popular fitness myths. The text argues that a majority of gym-goers (90-100% in commercial gyms) are affected to varying degrees by this condition.\n\nKey points include:\n\n1. **Symptoms**: Symptoms of fuckarounditis include not tracking progress, lack of a structured training plan, excessive focus on minor exercises, and poor exercise selection.\n\n2. **Information Overload**: The abundance of conflicting information available online leads to confusion and the perception that training is more complicated than it is. This leads people to follow trendy but ineffective routines.\n\n3. **Progress Metrics**: The article suggests specific strength benchmarks that individuals should aim for within certain time frames. Failing to meet these goals may indicate fuckarounditis.\n\n4. **Prevention and Intervention**: The author emphasizes the importance of tracking workouts, following a consistent routine focused on compound lifts, and avoiding distractions from ineffective exercises or fads.\n\n5. **Test for Fuckarounditis**: A set of symptoms is provided to help individuals assess whether they may be afflicted, urging them to stop unproductive behaviors immediately.\n\n6. **Conclusion**: The piece calls for a return to fundamental strength training principles, focusing on heavy lifts and measurable progress, to combat fuckarounditis and achieve better results.\n\nOverall, the article encourages readers to simplify their training approach, stick to proven methods, and avoid the pitfalls of misinformation and ineffective practices.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "e5bcbc9609f02ec9",
    "title": {
      "en": "The earliest versions of the first C compiler known to exist",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/mortdeus/legacy-cc",
    "score": 354,
    "by": "diginova",
    "time": 1742545039,
    "content": "legacy-cc\nThe earliest versions of the very first c compiler known to exist in the wild written by the late legend himself dmr.\nThese are not capable of being compiled today with modern c compilers like gcc. I am only posting these here for the enjoyment and reminiscing of the spark that ignited a soon to be trillion dollar industry. Enjoy. :)\nYou guys might be able to play around building this compilier with Aiju's pdp-11/unix emulator. I havent tried building this myself so I cant confirm it works, but I posted the link to her emulator for anybody wanting to hack around with it.\nSource: [https://www.bell-labs.com/usr/dmr/www/primevalC.html] alt-link [http://cm.bell-labs.co/who/dmr/primevalC.html]\nUseful Links\nPDP-11 Emulator: [http://pdp11.aiju.de/] (so i've been playing around with this to see if it still works, and it clearly has problems right now. if somebody wants to try and create like a VM image of unix running on PDP-7 hardware, id happily link to it here.)\nResearch Unix Repository: [https://www.tuhs.org/Archive/Distributions/Research/]",
    "summary": {
      "en": "This text discusses the early versions of the first C compiler created by Dennis Ritchie (DMR). These old versions cannot be compiled using modern C compilers like GCC. The author shares them for nostalgia and to celebrate the origins of a major industry. \n\nThere's a suggestion to use an emulator by Aiju to explore building this compiler, although the author hasn't tested it. Links to the emulator and a research Unix repository are provided for those interested in experimenting.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "2007b33bda208fc1",
    "title": {
      "en": "An Interview with Zen Chief Architect Mike Clark",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.computerenhance.com/p/an-interview-with-zen-chief-architect",
    "score": 36,
    "by": "Smaug123",
    "time": 1742843149,
    "content": "Share this postComputer, Enhance!An Interview with Zen Chief Architect Mike ClarkCopy linkFacebookEmailNotesMoreDiscover more from Computer, Enhance!Programming courses, interviews, commentary.Over 50,000 subscribersSubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.Already have an account? Sign inAn Interview with Zen Chief Architect Mike ClarkCasey MuratoriMar 25, 2025125Share this postComputer, Enhance!An Interview with Zen Chief Architect Mike ClarkCopy linkFacebookEmailNotesMore155ShareZen is one of the most important microarchitectures in the history of the x86 ecosystem. Not only is it the reigning champion in many x64 benchmarks, but it is also the architecture that enabled AMD’s dramatic rise in CPU marketshare over the past eight years: from 10% when the first Zen processor was launched, to 25% at the introduction of Zen 5.I recently had the honor of interviewing none other than Zen’s chief architect, Mike Clark. I only had 30 minutes, but I tried to fit in as many of our microarchitecture questions as I could! Subscribers to Computer Enhance will recognize many of them as ones we’ve collectively wondered about during Q&A’s in the Performance-Aware Programming series - and I’m delighted to report that, as you’ll see, Mike gave detailed answers to all of them.Below is the edited transcript of our conversation. I’ve tried to keep it as accurate as possible to the original audio, while reworking the phrasing to be appropriate for reading rather than listening. I have also had AMD approve the transcript to ensure accuracy, and I will be working with them to release an extended video version as well.Now, without further ado, my interview with Mike Clark:CASEY: You will often hear “people on the internet” say that ARM as an ISA is better for low power than x64. People like me who study ISAs tend to be skeptical of this claim. As a hardware designer, are there any specific things about the x64 ISA that you find difficult to deal with for low-power designs?MIKE: Having spent my career working on x86, I might have a bias here! I do think each ISA has its own quirks that influence some of the microarchitecture. But at the base level, we can build low-power x86 designs as well as ARM can, and ARM can build high frequency, high performance designs as well as x86 can. None of the quirks are really limiting you on the microarchitecture. The reality is that the markets we've been targeting have been different, so they've driven the architectures to optimize for different design points. ARM is in much lower power markets where x86 hasn't had the market share to chase.On the x86 side, the higher performance / higher frequency devices are the established market that our devices have to compete in, so that's where our design focus is. We could build the same Zen microarchitecture with an ARM ISA on top instead. We could deliver the same performance per watt. We don't view the ISA as a fundamental input to the design as far as power or performance.CASEY: So the memory model, whether the instructions are variable length, those sorts of things don’t factor in? None of the differences are big enough to matter?MIKE: No. It may take a little bit more microarchitectural work for us to account for stronger memory order on the x86 side, but of course the software has to account for the weaker memory ordering on the ARM side. So there are tradeoffs.Variable length instructions are harder than fixed length, but we've created techniques like the uop cache, and it also gives us better density overall by having smaller instructions. x86 can put more work into each instruction byte, so we can have denser binaries and increase performance that way.So these are all just tradeoffs in the microarchitecture. They’re not fundamental issues for delivering performance per watt at the end of the day.CASEY: Similar question, but moving to the OS side of things: does the 4k page size on x64 create problems for you architecturally by limiting the L1 cache size due to how tagging works? Would architectures like Zen benefit if x64 operating systems moved to 2mb pages as the smallest page size, or perhaps a 16k or 64k page size if you were to introduce that in a future architecture?MIKE: Definitely. We always encourage developers to use larger page sizes if they can, because it gives us a lot more capacity in our TLBs and therefore less TLB pressure overall. But we have the ability to combine 4k pages into larger pages in our TLB if the OS allocates them sequentially. We can turn four 4k pages into a 16k page if they are virtually and physically sequential. That's been a technique we've used even since the original Zen to help software get the benefits of larger page sizes without moving away from 4k pages.However, 4k to 2mb is a big jump. We're always looking for ways to allow our software partners to have larger page sizes, but maybe something in between is more appropriate.CASEY: Just to poke a little further at that, for the L1 cache specifically, you're hitting up against the limit of the address bits. Have you ever wanted to put in bigger L1 caches, but found that you couldn't because the 4k page size means you can't do that without going to a larger-way cache? MIKE: No. In the past we have built L1 caches that don't follow the “ways times 4k page size is the largest index you can have” property. There are ways to do that. We've solved those problems. It is a little bit more logic, but it's a solvable problem. It doesn’t limit us in what we design.  CASEY: Moving on to the sizes of registers and cache lines, I have two questions about how CPUs seem to do things differently than GPUs.First, CPUs seem to be settling into a natural size of 64 bytes. The L1 cache lines are 64 bytes. The registers are 64 bytes. It doesn't look like anyone's trying to go beyond that. But GPUs seem to prefer at least 128 bytes for both. Is this because of the difference in clock rates? Does it have to do with CPU versus GPU workloads? In general, do you see 64 bytes as a natural settling point for CPUs, and if so, why does it seem to be different from GPUs?MIKE: We do look at increasing the line size. We're always going to a clean sheet of paper and making sure we're rethinking things and not missing anything as workloads evolve and things change. We don't want to be locked into a mindset where we think we've proven 64 bytes to be the correct size for everything on a CPU.But the reality is that CPUs are targeted at low latency, smaller datatype, integer workloads as their fundamental value proposition. We've grown that capability with all our out-of-order engines, trying to expose ILP. So far, it’s allowed us to build vector units as wide as 64 bytes.But it's been a journey to even get that wide because if you look at, say, from Zen 4 to Zen 5 - we supported 512-bit vectors on Zen 4 via a 256-bit data path. For Zen 5, we went full bore and supported the full 512-bit data path. That required a fundamental replumbing of the microarchitecture. We had to grow the width of the delivery of data from the L2 to the L1, and we had to double the delivery from the L1 to really take advantage of the wider vector units.The integer workloads that are still primarily reading data out of the cache and branching, they're not getting any benefit from that sort of fundamental change. We have to do it in a very cautious and meticulous manner, so that those highways of delivery can exist while still ensuring that if there's only one car on the highway, we’re not burning power as if all the lanes were full. It’s tricky.When you look at the GPU side, the workloads where they excel are throughput based. Not having to excel at the lowest-latency, small-datatype workloads frees them up to leverage all that extra investment. You need to have workloads that are really focused on using that much data in a wide vector to get the return on that investment.So that's always the trick. If we try to go too big, too wide, we lose our value proposition in performance per watt for the mainstream workloads people buy our new generations for.Does that make sense?CASEY: It makes perfect sense, and it leads right into my next question.Underlying what you said is the implication that, if we as software developers were taking better advantage of wider workloads, it would be worth your while to widen them. One of the problems people often have when trying to widen a data path in software is that CPUs seem to be a lot worse at scatter/gather. It’s an important feature for taking data that isn’t naturally wide and putting it through a wide datapath with some level of efficiency. For example, if I want to widen something that does an array lookup, historically it’s been hard to port that code directly because of poor gather performance.Could you give us some insight on why this is?MIKE: That's a good question. It does tie back to the previous question in the sense that it’s really not the fundamental scatter/gather concept that’s the problem. It's the amount of bandwidth needed to pull all those different elements inside the CPU to put them together to feed to the vector unit.Again, we're focused on latency, not throughput. That has permeated itself out into the interface to what we call our “data fabric”. The memory system isn't wide enough to be able to pull all the data in so it can be assembled into lanes and operated on. If we wanted to attack that, we’d have to widen the interface, and that would come with a large power cost.So again, that's the trick. You're trying to avoid the power cost when you're running workloads that don't require scatter/gather. If you widen these paths, you’ve overbuilt the design for the the baseline workloads that you normally run. We are always trying to grow and pull more applications in, but we have to balance that against the power requirements of widening the bandwidth into the CPU.CASEY: So in other words, it's a chicken and egg problem? If software developers were giving you software that ran fantastically with scatter/gather, you’d do it. But they’re not, so it’s hard to argue for it?MIKE: Right, yes.CASEY: The rest of my questions don’t group together into any particular theme, so I’ll just go through them randomly.Random question number one: previously, on the software side, we thought nontemporal stores were solely there to prevent pollution of caches. That was our mental model. But lately we have noticed that nontemporal stores seem to perform better than regular stores, separate from cache pollution - as if the memory subsystem doesn't have to do as much work, or something similar. Is there more about nontemporal stores that we need to understand, or are we mistaken?MIKE: If you were just doing nontemporal stores to data that is in the caches, obviously that would not be a good thing. So you still have to apply good judgment on when to use nontemporal stores. But tying it back to the ARM-ISA-weakly-ordered discussion, nontemporal stores, while not exactly being weakly ordered, are in some ways easier to deal with in the base case. We can process them efficiently as long as they really are nontemporal. So I think your intuition is right - we can do well with them as long as the software side ensures that the data isn’t finding itself in caches along the way.CASEY: Random question number two: for educational purposes, does anyone publish modern CPU pipeline diagrams that would be reasonably accurate? AMD and Intel, for example, both publish flow charts for new microarchitectures, but not pipeline diagrams.MIKE: It might surprise people, but if you go back to when we did publish pipeline diagrams, those are still fine for learning how a modern CPU works. We do have more complicated pipelines today, and we don't publish them because they reveal proprietary techniques we're using, or give hints that we don't want to give to the competition. But at the end of the day, it's still a fetch block, a decode block, an execute block, a retire block... there's more stages within those blocks, and you can break it down even more than that, but the fundamental pipelining is still similar.CASEY: So, for example, I think the Bulldozer pipeline diagram was the last one I saw from AMD. It’s not woefully out of date? If someone learned that pipeline, they would be able to understand what you actually do now if they were given an updated diagram?MIKE: Roughly speaking, yes.CASEY: Random question number three: if you look at a single-uop instruction like sqrtpd that has a latency longer than the pipeline depth of an execution unit, can you give a cursory explanation of how this works for those of us on the software side who don't understand hardware very well?MIKE: One way to conceptualize it is that you could have taken sqrtpd and split it up into a bunch of different uops that can operate in parallel with dependencies along the way. It can be very expensive to keep all those operations in flight, to build the pipeline to pass the data forward so you can let something new in behind it that's working on an earlier stage. The hardware cost would be too high to create a pipeline to get the execution done in a way that allows another sqrtpd to start on an earlier stage - especially if it's going to be, say, 16 stages of execution until you have achieved your answer.It's really just that cost. Is the amount of hardware worth it to make something like sqrtpd a pipelineable instruction, or can we save a lot of power and hardware by just doing one of them at a time?CASEY: Just to make sure I understand: does that mean inside an execution unit that can do one of these, the uop gets issued and it knows it’s got something special that it has to work on for a while, so it asks not to be given anything else for several cycles while some special control part inside it takes over?MIKE: Correct. The scheduler that feeds it understands that it's not a pipeline execution unit that can take another uop every cycle. But it has a known quantity where, if it has sent one in, after some number of cycles, it knows it can send another one in and it should be safe.CASEY: So the system upstream of the execution unit - the thing that's feeding it - knows not to send more?MIKE: It knows, yes.CASEY: Last question: are there things you wish we as software developers would start - or stop - doing that would help take advantage of the hardware you design, or that would make it easier for you to design new hardware in the future?MIKE: We already hit on one, which is the feedback loop when we add new ISA components - larger vectors, for example. We need software to use them to get the return on investment that we're putting in.Of course we also understand that, as a new feature comes out, it's only on the new hardware. You want your software to run well on our old hardware as well as on our new hardware. We totally understand that problem. But still, if software developers could embrace the new features more aggressively, that would definitely help.It would be great if the software could find ways to leverage wider vectors, AI, and so on - all the areas we've invested a lot of hardware in. And of course we would also like to get feedback from you guys - “if we just had this instruction or this concept, we could really leverage that in our software” and so on. We're constantly open to that, too. We want to know how to make your lives easier.And finally, one other thing I would add is that larger basic blocks are better. Taking branches versus not taking branches can have a big effect on code flow. Try to put conditional operations in the right places. I’m sure you guys probably focus on this already.CASEY: Yes, but it’s always good to hear it from you. We only ever know that something runs faster when we time it - we can't always guess what the designers are thinking on the hardware side.MIKE: Gotcha.CASEY: Well, I think we are out of time. Thank you very much! This has been fantastic. Thank you for answering all of my questions, and please keep in touch. We always have questions like this on the software side, so anytime you want to talk, or if there is anything new you want to tell us about, please let us know.MIKE: Okay, cool. It was a great conversation. And yeah, any time you're wondering what's going on in the hardware, we want to close that gap as best we can!CASEY: We all appreciate it. And we love Zen as well! I’m conducting this interview from a Zen processor as we speak. So thank you for all your hard work over the years.MIKE: Alright, thanks! Talk to you later.If you enjoyed this article, and would like to receive more like it, you can put your email address in the box below to access both our free and paid subscription options:Subscribe125 Likes∙5 Restacks125Share this postComputer, Enhance!An Interview with Zen Chief Architect Mike ClarkCopy linkFacebookEmailNotesMore155SharePrevious",
    "summary": {
      "en": "In an interview with Mike Clark, the chief architect of AMD's Zen microarchitecture, he discussed the evolution and capabilities of the Zen architecture, which has significantly boosted AMD's market share in CPUs. Here are the key points from the conversation:\n\n1. **ISA Comparisons**: Clark noted that both x86 (Zen) and ARM architectures can achieve low power and high performance. The differences in instruction set architectures (ISAs) do not fundamentally limit performance or power efficiency.\n\n2. **Memory Page Sizes**: He emphasized the advantages of larger memory page sizes for performance, although current systems typically use 4k pages. There are techniques to combine smaller pages for efficiency.\n\n3. **Cache and Register Sizes**: Clark explained that while CPUs typically use 64-byte cache lines and registers, they are continually evaluating the potential for larger sizes. The design choices are influenced by the types of workloads CPUs handle, which focus on low latency.\n\n4. **Scatter/Gather Performance**: He highlighted challenges with scatter/gather operations in CPUs, which are less efficient than in GPUs due to differences in design priorities and workload types.\n\n5. **Non-Temporal Stores**: Clark clarified that non-temporal stores can perform better than regular stores under certain conditions, as long as they are not used unnecessarily with cached data.\n\n6. **Pipeline Diagrams**: While modern pipeline diagrams are not published due to proprietary concerns, he indicated that older diagrams still provide a useful understanding of CPU operation.\n\n7. **Performance Improvements**: He encouraged software developers to embrace new features and techniques that leverage wider vector operations and to provide feedback on desired instructions to improve hardware design.\n\nOverall, the interview reveals Clark's insights into CPU design challenges and the collaboration needed between hardware and software developers to enhance performance.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "ba3512ae44fff864",
    "title": {
      "en": "Arc-AGI-2 and ARC Prize 2025",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://arcprize.org/blog/announcing-arc-agi-2-and-arc-prize-2025",
    "score": 179,
    "by": "gkamradt",
    "time": 1742848530,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "13d4eb798a832725",
    "title": {
      "en": "The Vatican's Latinist (2017)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://newcriterion.com/article/the-vaticans-latinist/",
    "score": 139,
    "by": "bookofjoe",
    "time": 1742779791,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "0916eacf2b59aebb",
    "title": {
      "en": "Don't Buy into Apple's Hype About AirPods Max Gaining Lossless Audio",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.macrumors.com/2025/03/24/airpods-max-lossless-audio-overhyped/",
    "score": 11,
    "by": "ksec",
    "time": 1742902674,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "9c8dde4906b72fde",
    "title": {
      "en": "Gatehouse – a composable, async-friendly authorization policy framework in Rust",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/thepartly/gatehouse",
    "score": 71,
    "by": "hardbyte",
    "time": 1742856647,
    "content": "Gatehouse\n\nA flexible authorization library that combines role-based (RBAC), attribute-based (ABAC), and relationship-based (ReBAC) access control policies.\n\nFeatures\n\nMulti-paradigm Authorization: Support for RBAC, ABAC, and ReBAC patterns\nPolicy Composition: Combine policies with logical operators (AND, OR, NOT)\nDetailed Evaluation Tracing: Complete decision trace for debugging and auditing\nFluent Builder API: Construct custom policies with a PolicyBuilder.\nType Safety: Strongly typed resources/actions/contexts\nAsync Ready: Built with async/await support\n\nCore Components\nPolicy Trait\nThe foundation of the authorization system:\n#[async_trait]\ntrait Policy<Subject, Resource, Action, Context> {\n    async fn evaluate_access(\n        &self,\n        subject: &Subject,\n        action: &Action,\n        resource: &Resource,\n        context: &Context,\n    ) -> PolicyEvalResult;\n}\n\nPermissionChecker\nAggregates multiple policies (e.g. RBAC, ABAC) with OR logic by default: if any policy grants access, permission is granted.\nlet mut checker = PermissionChecker::new();\nchecker.add_policy(rbac_policy);\nchecker.add_policy(owner_policy);\n\n// Check if access is granted\nlet result = checker.evaluate_access(&user, &action, &resource, &context).await;\nif result.is_granted() {\n    // Access allowed\n} else {\n    // Access denied\n}\n\nPolicyBuilder\nThe PolicyBuilder provides a fluent API to construct custom policies by chaining predicate functions for\nsubjects, actions, resources, and context. Once built, the policy can be added to a [PermissionChecker].\nlet custom_policy = PolicyBuilder::<MySubject, MyResource, MyAction, MyContext>::new(\"CustomPolicy\")\n    .subjects(|s| /* ... */)\n    .actions(|a| /* ... */)\n    .resources(|r| /* ... */)\n    .context(|c| /* ... */)\n    .when(|s, a, r, c| /* ... */)\n    .build();\n\nBuilt-in Policies\n\nRbacPolicy: Role-based access control\nAbacPolicy: Attribute-based access control\nRebacPolicy: Relationship-based access control\n\nCombinators\nAndPolicy: Grants access only if all inner policies allow access\nOrPolicy: Grants access if any inner policy allows access\nNotPolicy: Inverts the decision of an inner policy\nExamples\nSee the examples directory for complete demonstration of:\n\nRole-based access control (rbac_policy)\nRelationship-based access control (rebac_policy)\nPolicy combinators (combinator_policy)\n\nRun with:\ncargo run --example rbac_policy",
    "summary": {
      "en": "**Gatehouse Overview**\n\nGatehouse is an authorization library that uses three access control methods: role-based (RBAC), attribute-based (ABAC), and relationship-based (ReBAC). \n\n**Key Features:**\n- **Multi-Paradigm Authorization:** Supports RBAC, ABAC, and ReBAC.\n- **Policy Composition:** Allows combining different policies using AND, OR, and NOT operators.\n- **Detailed Evaluation Tracing:** Provides a complete decision history for debugging and auditing purposes.\n- **Fluent Builder API:** Facilitates the creation of custom policies easily.\n- **Type Safety:** Ensures strong typing for resources, actions, and contexts.\n- **Async Ready:** Designed for asynchronous programming with async/await support.\n\n**Core Components:**\n1. **Policy Trait:** The base of the system that evaluates access based on subjects, actions, resources, and contexts.\n2. **PermissionChecker:** A tool that combines multiple policies. It grants access if any policy allows it.\n3. **PolicyBuilder:** A user-friendly API for creating custom policies through a series of chained functions.\n\n**Built-in Policies:**\n- **RbacPolicy:** For role-based access control.\n- **AbacPolicy:** For attribute-based access control.\n- **RebacPolicy:** For relationship-based access control.\n\n**Combinators:**\n- **AndPolicy:** Access is granted only if all specified policies allow it.\n- **OrPolicy:** Access is granted if any specified policy allows it.\n- **NotPolicy:** Reverses the decision of an inner policy.\n\n**Examples:** \nVarious examples are available to demonstrate how to implement these access control methods and policy combinations. \n\nTo run an example, use the command: `cargo run --example rbac_policy`.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "5a6b64c7bd1b498a",
    "title": {
      "en": "Supply Chain Attacks on Linux Distributions – Fedora Pagure",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://fenrisk.com/pagure",
    "score": 204,
    "by": "akyuu",
    "time": 1742414317,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "616488f05e0e9d76",
    "title": {
      "en": "Show HN: My iOS app to practice sight reading (10 years in the App Store)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://apps.apple.com/us/app/notes-sight-reading-trainer/id874386416",
    "score": 298,
    "by": "rooster117",
    "time": 1742765108,
    "content": ".ember3, #ember3::before {\n           width: 246px;\n           height: 246px;\n         }\n         .ember3::before {\n           padding-top: 100%;\n         }\n@media (min-width: 735px) {\n           .ember3, #ember3::before {\n           width: 217px;\n           height: 217px;\n         }\n         .ember3::before {\n           padding-top: 100%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember3, #ember3::before {\n           width: 230px;\n           height: 230px;\n         }\n         .ember3::before {\n           padding-top: 100%;\n         }\n         }\n\n          Notes - Sight Reading Trainer\n            4+\n\n            Piano sheet music\n\n            Ryan Newsome\n\n              Designed for iPad\n\n    4.8 • 2.7K Ratings\n\n                Free\n\n                Offers In-App Purchases\n\nScreenshots\n\n                  iPad\n\n                  iPhone\n\n    .ember6, #ember6::before {\n           width: 643px;\n           height: 857px;\n         }\n         .ember6::before {\n           padding-top: 133.28149300155522%;\n         }\n@media (min-width: 735px) {\n           .ember6, #ember6::before {\n           width: 217px;\n           height: 289px;\n         }\n         .ember6::before {\n           padding-top: 133.17972350230414%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember6, #ember6::before {\n           width: 313px;\n           height: 417px;\n         }\n         .ember6::before {\n           padding-top: 133.22683706070288%;\n         }\n         }\n\n    .ember7, #ember7::before {\n           width: 643px;\n           height: 857px;\n         }\n         .ember7::before {\n           padding-top: 133.28149300155522%;\n         }\n@media (min-width: 735px) {\n           .ember7, #ember7::before {\n           width: 217px;\n           height: 289px;\n         }\n         .ember7::before {\n           padding-top: 133.17972350230414%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember7, #ember7::before {\n           width: 313px;\n           height: 417px;\n         }\n         .ember7::before {\n           padding-top: 133.22683706070288%;\n         }\n         }\n\n    .ember8, #ember8::before {\n           width: 643px;\n           height: 857px;\n         }\n         .ember8::before {\n           padding-top: 133.28149300155522%;\n         }\n@media (min-width: 735px) {\n           .ember8, #ember8::before {\n           width: 217px;\n           height: 289px;\n         }\n         .ember8::before {\n           padding-top: 133.17972350230414%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember8, #ember8::before {\n           width: 313px;\n           height: 417px;\n         }\n         .ember8::before {\n           padding-top: 133.22683706070288%;\n         }\n         }\n\n    .ember9, #ember9::before {\n           width: 643px;\n           height: 857px;\n         }\n         .ember9::before {\n           padding-top: 133.28149300155522%;\n         }\n@media (min-width: 735px) {\n           .ember9, #ember9::before {\n           width: 217px;\n           height: 289px;\n         }\n         .ember9::before {\n           padding-top: 133.17972350230414%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember9, #ember9::before {\n           width: 313px;\n           height: 417px;\n         }\n         .ember9::before {\n           padding-top: 133.22683706070288%;\n         }\n         }\n\n    .ember10, #ember10::before {\n           width: 643px;\n           height: 857px;\n         }\n         .ember10::before {\n           padding-top: 133.28149300155522%;\n         }\n@media (min-width: 735px) {\n           .ember10, #ember10::before {\n           width: 217px;\n           height: 289px;\n         }\n         .ember10::before {\n           padding-top: 133.17972350230414%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember10, #ember10::before {\n           width: 313px;\n           height: 417px;\n         }\n         .ember10::before {\n           padding-top: 133.22683706070288%;\n         }\n         }\n\n    .ember11, #ember11::before {\n           width: 643px;\n           height: 857px;\n         }\n         .ember11::before {\n           padding-top: 133.28149300155522%;\n         }\n@media (min-width: 735px) {\n           .ember11, #ember11::before {\n           width: 217px;\n           height: 289px;\n         }\n         .ember11::before {\n           padding-top: 133.17972350230414%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember11, #ember11::before {\n           width: 313px;\n           height: 417px;\n         }\n         .ember11::before {\n           padding-top: 133.22683706070288%;\n         }\n         }\n\nDescription\n\n  Introducing Notes - Sight Reading Trainer, the ultimate iOS app for mastering sight reading in music! Whether you're a beginner or an experienced musician, Notes is your is your tool to become the musician you were meant to be. When you are ready to learn your instrument and music rather than just learning a song this app is for you.\n\nKey Features:\nEngaging Exercises: Explore a variety of interactive exercises designed to strengthen your sight reading abilities with a piano interface.\n\nWays to use the app: Connect with Midi, use the on screen keyboard or your devices microphone for input.\n\nCustomizable Difficulty: Tailor the difficulty level to match your proficiency, allowing you to progress at your own pace and challenge yourself when ready.\n\nComprehensive Music Notation: Learn to read sheet music, decipher key signatures, note durations and augmentations (sharp/flat) with ease.\n\nProgress Tracking: Keep track of your improvements over time with detailed performance analytics and history.\n\nPractice with songs: This app is focused on randomized practice but also has a handful of songs to practice your developing skills with. More to be added soon!\n\nGoal Setting: Set achievable note goals and receive reminders to maintain consistency in your practice.\n\nUnlock the world of sight reading and elevate your musical journey with Notes - Sight Reading Trainer. Download the app now and get serious about learning music!\n\n        more\n\nWhat’s New\n\n    Version History\n\n            Feb 26, 2025\n            Version 7.3\n\n  Fix to allow using midi inputs and microphone for the keyboarding learning lessons\n\nRatings and Reviews\n\n        See All\n\n      4.8 out of 5\n        2.7K Ratings\n\n        2.7K Ratings\n\n    Quality reviewer\n\n    ,\n\n    04/22/2023\n\n    Great app!\n\n  This is one of the best apps I’ve used to study notes. I only sing but I’m trying to learn notes and this is fantastic. You will learn how to use a piano in the process, which is key to making sure you learn to sing in tune. My only objection is that the lessons do NOT include training on sharps & flats. I was hoping the in app purchase for additional lessons (happens after you’ve learned quite a lot, no worries) included this, and there’s no info given on what these lessons contain, only that “more will be added”. The practice mode thankfully does allow you to train flats but not in an organized, piece by piece way like the lessons do. But unfortunately you cannot see flats and sharps labeled on the keys like the other ones.\n\nI have to knock off a star… sharps and flats are a big part of the music that I do and so this app didn’t teach me everything I wanted to know.\n\n        more\n\n    FPPMEDIA\n\n    ,\n\n    01/18/2018\n\n    Thank you for educational app.\n\n  This app and the ease of use has introduced a whole new world to me.\nI never thought I would understand how to play a piano.. My life has been filled with homelessness and bad fortune, so I learned a long time ago that music had a medicinal effect on me.. A personal escape of sorts... I always wanted to understand how to make beautiful sounds with a piano, but I don’t learn so well.. This app made it easy for me to understand the keyboard layout and knowing what each note is from each key.. It seems like this app will help me learn how to write songs and that will make life so happy for me.\nI’m not trying to be sad here.. I just wanted to explain how much this app can help someone like me. I can’t afford the full version but this version, I appreciate none the less.\n“Music may have to save the world someday”.\n\n        more\n\n    Churple\n\n    ,\n\n    03/26/2018\n\n    Absolutely Stunning\n\n  I rarely write reviews, but this app is so beyond amazing I’ll try my best to share my thoughts. First of all, everything is absolutely FREE, NO premium that you have to buy to basically unlock everything, I feel the developers of this app were truly trying to help instead of take all of your money. Secondly, I’ve had this app for one day and I’ve already learned more than I did searching for weeks for a good app that’s free. I’ve started to be able to play simple songs on my keyboard, like Jingle Bells, completely looking at the sheet music. I almost gave up on trying to learn how to read sheet music, but this has given me a great start. Sorry for bad grammar.\n\n        more\n\nApp Privacy\n\n    See Details\n\n    The developer, Ryan Newsome, indicated that the app’s privacy practices may include handling of data as described below. For more information, see the developer’s privacy policy.\n\n        Data Not Linked to You\n        The following data may be collected but it is not linked to your identity:\n\n                  Usage Data\n\n                  Diagnostics\n\n    Privacy practices may vary, for example, based on the features you use or your age. LearnMore\n\nInformation\n\n          Seller\n\n              Ryan Newsome\n\n          Size\n          32 MB\n\n          Category\n\n                Education\n\n        Compatibility\n\n                  iPhone\n\n                Requires iOS 14.0 or later.\n\n                  iPad\n\n                Requires iPadOS 14.0 or later.\n\n                  iPodtouch\n\n                Requires iOS 14.0 or later.\n\n                  Mac\n\n                Requires macOS11.0 or later and a Mac with AppleM1chip or later.\n\n                  Apple Vision\n\n                Requires visionOS 1.0 or later.\n\n        Languages\n\n  English, French, German, Italian, Polish, Simplified Chinese, Spanish, Traditional Chinese\n\n          Age Rating\n\n              4+\n\n        Copyright\n        © Ryan Newsome\n\n          Price\n          Free\n\n          In-App Purchases\n\n                  Premium\n                  $3.99\n\n            App Support\n\n            Privacy Policy\n\nApp Support\n\n          Privacy Policy\n\nMore By This Developer\n\n    .ember21, #ember21::before {\n           width: 320px;\n           height: 320px;\n         }\n         .ember21::before {\n           padding-top: 100%;\n         }\n@media (min-width: 735px) {\n           .ember21, #ember21::before {\n           width: 157px;\n           height: 157px;\n         }\n         .ember21::before {\n           padding-top: 100%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember21, #ember21::before {\n           width: 146px;\n           height: 146px;\n         }\n         .ember21::before {\n           padding-top: 100%;\n         }\n         }\n\n        Violin Sight Reading Trainer\n\n    Music\n\nYou Might Also Like\n\n        See All\n\n    .ember23, #ember23::before {\n           width: 320px;\n           height: 320px;\n         }\n         .ember23::before {\n           padding-top: 100%;\n         }\n@media (min-width: 735px) {\n           .ember23, #ember23::before {\n           width: 157px;\n           height: 157px;\n         }\n         .ember23::before {\n           padding-top: 100%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember23, #ember23::before {\n           width: 146px;\n           height: 146px;\n         }\n         .ember23::before {\n           padding-top: 100%;\n         }\n         }\n\n        Learn Music Notes Sight Read\n\n    Education\n\n    .ember24, #ember24::before {\n           width: 320px;\n           height: 320px;\n         }\n         .ember24::before {\n           padding-top: 100%;\n         }\n@media (min-width: 735px) {\n           .ember24, #ember24::before {\n           width: 157px;\n           height: 157px;\n         }\n         .ember24::before {\n           padding-top: 100%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember24, #ember24::before {\n           width: 146px;\n           height: 146px;\n         }\n         .ember24::before {\n           padding-top: 100%;\n         }\n         }\n\n        MyMusicTheory - Music Theory\n\n    Education\n\n    .ember25, #ember25::before {\n           width: 320px;\n           height: 320px;\n         }\n         .ember25::before {\n           padding-top: 100%;\n         }\n@media (min-width: 735px) {\n           .ember25, #ember25::before {\n           width: 157px;\n           height: 157px;\n         }\n         .ember25::before {\n           padding-top: 100%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember25, #ember25::before {\n           width: 146px;\n           height: 146px;\n         }\n         .ember25::before {\n           padding-top: 100%;\n         }\n         }\n\n        Piano Marvel - Lessons & Music\n\n    Education\n\n    .ember26, #ember26::before {\n           width: 320px;\n           height: 320px;\n         }\n         .ember26::before {\n           padding-top: 100%;\n         }\n@media (min-width: 735px) {\n           .ember26, #ember26::before {\n           width: 157px;\n           height: 157px;\n         }\n         .ember26::before {\n           padding-top: 100%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember26, #ember26::before {\n           width: 146px;\n           height: 146px;\n         }\n         .ember26::before {\n           padding-top: 100%;\n         }\n         }\n\n        Perfect Ear - Ear Trainer\n\n    Education\n\n    .ember27, #ember27::before {\n           width: 320px;\n           height: 320px;\n         }\n         .ember27::before {\n           padding-top: 100%;\n         }\n@media (min-width: 735px) {\n           .ember27, #ember27::before {\n           width: 157px;\n           height: 157px;\n         }\n         .ember27::before {\n           padding-top: 100%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember27, #ember27::before {\n           width: 146px;\n           height: 146px;\n         }\n         .ember27::before {\n           padding-top: 100%;\n         }\n         }\n\n        Sheet Music Reader Halbestunde\n\n    Education\n\n    .ember28, #ember28::before {\n           width: 320px;\n           height: 320px;\n         }\n         .ember28::before {\n           padding-top: 100%;\n         }\n@media (min-width: 735px) {\n           .ember28, #ember28::before {\n           width: 157px;\n           height: 157px;\n         }\n         .ember28::before {\n           padding-top: 100%;\n         }\n         }\n@media (min-width: 1069px) {\n           .ember28, #ember28::before {\n           width: 146px;\n           height: 146px;\n         }\n         .ember28::before {\n           padding-top: 100%;\n         }\n         }\n\n        Skoove: Learn to Play Piano\n\n    Education",
    "summary": {
      "en": "**Summary of \"Notes - Sight Reading Trainer\" App**\n\n- **App Overview**: \"Notes - Sight Reading Trainer\" is an iOS app designed to help users improve their sight reading skills in music, suitable for both beginners and experienced musicians. \n\n- **Key Features**:\n  - **Interactive Exercises**: Various exercises to enhance sight reading using a piano interface.\n  - **Input Options**: Users can connect MIDI devices, use an on-screen keyboard, or their device's microphone.\n  - **Custom Difficulty**: Users can adjust the difficulty level to suit their skill.\n  - **Music Notation Learning**: Helps users understand sheet music, key signatures, and note values.\n  - **Progress Tracking**: Users can monitor their improvement over time.\n  - **Practice Songs**: Includes random practice and a collection of songs to apply skills.\n\n- **User Ratings**: The app has a 4.8 rating from 2.7K users, highlighting its effectiveness and ease of use.\n\n- **Privacy**: The app collects some usage data but does not link it to individual identities.\n\n- **Cost**: The app is free with optional in-app purchases available.\n\n- **Developer**: Created by Ryan Newsome, the app is compatible with various Apple devices and supports multiple languages. \n\nThis app provides a comprehensive and engaging way for users to learn music reading skills effectively.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "7141c6c92b9df51f",
    "title": {
      "en": "Open Source Mythology",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://lgug2z.com/articles/on-open-source-mythology/",
    "score": 15,
    "by": "yreew",
    "time": 1742864007,
    "content": "On Open Source Mythology\n\n        2025-03-23 ::\n\n      LGUG2Z\n\n      #licensing\n\n        There are two points of popular open source mythology this post will share my\nexperience with:\n\nPeople won’t use your project if you don’t use an Open Source\nInitiative-approved license\nPeople won’t contribute to to your project if you don’t use an Open Source\nInitiative-approved license\n\nMany people have ideas about how society should be like and what must be done\nto change institutions and to work for a revolution. But this is not enough.\nOften these ideas do not conform to reality and if they do conform to reality\nthere is only one way to test them: Try to put them to work and see if they\nsucceed. Testing our ideas in concrete work is the only way we will ever know\nif they are correct.\n– James Forman - Twenty Enemies\n(1971)\n\nI maintain a tiling window manager for Windows called\nkomorebi which is made available under\nthe Komorebi License.\nThe Komorebi License is both an educational source\nlicense which\nensures the availability of source code to individuals for personal use,\nmodification, learning and reference, and a firewall license which preserves an\nindividual’s freedom to refuse by default (… like a firewall!)\nThe preservation of an individual’s freedom to refuse by default means that the\nKomorebi License is not an Open Source Initiative-approved license.\nAs of the publication of this post, komorebi has 126k\ndownloads\nand 10.6k stargazers on\nGitHub, and while I am still by far the most active and primary\ncontributor to the\nproject, there is a healthy stream of users who like to contribute patches for\nthings they have a personal interest in improving.\n\nIt often happens, however, that thinking lags behind reality; this is because\nman’s cognition is limited by numerous social conditions. We are opposed to\ndie-hards in the revolutionary ranks whose thinking fails to advance with\nchanging objective circumstances and has manifested itself historically as\nRight opportunism. These people fail to see that the struggle of opposites\nhas already pushed the objective process forward while their knowledge has\nstopped at the old stage. This is characteristic of the thinking of all\ndie-hards. Their thinking is divorced from social practice, and they cannot\nmarch ahead to guide the chariot of society; they simply trail behind,\ngrumbling that it goes too fast and trying to drag it back or turn it in the\nopposite direction.\n– 毛泽东 - ON PRACTICE\n(1937)\n\nI do not believe I am alone in having the desire to share what I have learned\nso that other people might learn from it, while not buying into the idea that\nin order to do that effectively, I also have to use a software license which\nrequires me to forfeit my freedom to refuse.\nI do not believe that there needs to be a dependency between sharing what I\nhave learned in the form of source code and permitting its use in the\ncommission of evils including but not limited\nto genocide, or participating in my own exploitation by unimaginably wealthy\ncorporations which, incidentally, are also active\nparticipants\nin the commission of evils including but not limited to genocide.\n(It’s probably no surprise that I’m also not a fan of forcing a dependency\nbetween funding for essential government services and funding a genocide)\n\nIn Russia, there was a fundamental difference between the contradiction\nresolved by the February Revolution and the contradiction resolved by the\nOctober Revolution, as well as between the methods used to resolve them.\n[…]\nThe dogmatists do not observe this principle; they do not understand that\nconditions differ in different kinds of revolution and so do not understand\nthat different methods should be used to resolve different contradictions; on\nthe contrary, they invariably adopt what they imagine to be an unalterable\nformula and arbitrarily apply it everywhere, which only causes setbacks to\nthe revolution or makes a sorry mess of what was originally well done.\n– 毛泽东 - ON CONTRADICTION\n(1937)\n\nThe conditions of today differ from the conditions of 1983 as they differ from the\nconditions of 1998. The contradictions of today differ from the contradictions\nof 1983 as they differ from the contradictions of 1998.\nIf you are a software developer who wants to share what you have learned for\nthe educational benefit of others in the world, but are uncomfortable with the\nstrings attached by open source software licensing, I invite you to reject the\ndogmatists, and to renew and share the experiences of your social practice.\n\nWe do ourselves and the revolutionary struggle a great disservice if we do\nnot begin to realize that our concrete work […] must be put into writing\nfor the benefit of all the world, especially ourselves.\nWe must constantly sum up our work, our revolutionary efforts, our social\npractices so that we might learn what is correct and positive and what must be\nfurther developed.\nAt the same time we must discard that which is irrelevant and negative. To\nassume that the Bolsheviks, the Chinese, the Vietnamese, the Cubans, the\nAlgerians and other revolutionaries have not made mistakes from which they\nprofited is not to understand that there are no blueprints for revolutions.\nThe concrete work that we do in the final analysis is the basis for truth.\n– James Forman - Twenty Enemies\n(1971)\n\n        Read other posts\n\n                Educational Source Software\n                →",
    "summary": {
      "en": "### Summary of \"On Open Source Mythology\"\n\nThis text discusses two common beliefs about open source licensing:\n\n1. Many believe that projects won't attract users or contributors unless they use an Open Source Initiative-approved license.\n2. The author challenges this idea by sharing their experience with their own project, a tiling window manager called Komorebi, which uses the Komorebi License instead. This license allows personal use and modification but does not fit the Open Source Initiative’s criteria because it allows individuals to refuse use.\n\nThe author argues that it's important to test ideas in practical ways rather than just adhering to established norms. They emphasize that different contexts and times require different approaches, and that sharing knowledge doesn't have to come with conditions that compromise personal freedom or ethical values.\n\nThe text encourages software developers who want to share their knowledge without traditional open source constraints to reject rigid thinking and to document their experiences, as learning from practice is essential for progress.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "c32c7963edc80997",
    "title": {
      "en": "Show HN: LinkedIn sucks, so I built a better one",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://heyopenspot.com/",
    "score": 445,
    "by": "fliellerjulian",
    "time": 1742755960,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "b8521acaff2ed2e2",
    "title": {
      "en": "Yahoo is selling TechCrunch to private equity firm Regent",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://techcrunch.com/2025/03/21/techcrunch-has-personal-news/",
    "score": 81,
    "by": "Garbage",
    "time": 1742626489,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "fa5bbe9249635585",
    "title": {
      "en": "The game designer playing through his own psyche",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.newyorker.com/culture/persons-of-interest/the-game-designer-playing-through-his-own-psyche",
    "score": 140,
    "by": "FinnLobsien",
    "time": 1742812772,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "b3941b721341aca7",
    "title": {
      "en": "David Lynch Presents Interview Project",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.youtube.com/channel/UC4lrTWEywA3JfR-N_dv0d4A",
    "score": 50,
    "by": "toomuchtodo",
    "time": 1742878067,
    "content": "Back\n\n  KR\n\n    Skip navigation\n\n        Search\n\n  Search with your voice\n\nSign in\n\n  KR\n\n  Home\n\n    Home\n\n  Shorts\n\n    Shorts\n\n  Subscriptions\n\n    Subscriptions\n\n  You\n\n    You\n\n  History\n\n    History\n\n          David Lynch Presents Interview Project@davidlynchpresentsIP•15.6K subscribers•158 videosMore about this channel ...more...moreinstagram.com/davidlynchinterviewprojectand 3 more linksSubscribe\n\n              HomeVideosShortsPlaylistsPosts\n\n    Search\n\n  David Lynch Presents Interview Project: TRAILERSearchInfoShoppingTap to unmute2xIf playback doesn't begin shortly, try restarting your device.•You're signed outVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.CancelConfirmDavid Lynch Presents Interview ProjectSubscribeUnsubscribeShareInclude playlistAn error occurred while retrieving sharing information. Please try again later.Watch laterShareCopy link0:000:00 / 2:07•Watch full videoLive••\n\n    David Lynch Presents Interview Project: TRAILER\n\n    •\n\n    •\n\n      50,182 views\n\n      9 months ago\n\n  The Interview Project team has decided to re-release all 121 episodes of the original series on October 1, 2024  to commemorate the 15 year anniversary.\n\nEach episode will be shown in high definition, a feature that was previously unavailable to the team.\n\nFor those of you familiar with the series, we hope you enjoy revisiting them. For those new to the series, we hope you enjoy discovering them.\n\nAs David says, \"It's something that's human and you can't stay away from it.\"\n\n  Read more\n\n                Videos\n\n  3:25\n    3:25\n  Now playing\n\nDavid Lynch Presents Interview Project:  Jess\n\n32K views5 months ago\n\n  4:30\n    4:30\n  Now playing\n\nDavid Lynch Presents Interview Project:  Tommie Holliday\n\n13K views5 months ago\n\n  4:19\n    4:19\n  Now playing\n\nDavid Lynch Presents Interview Project:  Kee\n\n10K views5 months ago\n\n  3:33\n    3:33\n  Now playing\n\nDavid Lynch Presents Interview Project:  Palmer Black\n\n4.4K views5 months ago\n\n  4:03\n    4:03\n  Now playing\n\nDavid Lynch Presents Interview Project:  Gordon\n\n5.9K views5 months ago\n\n  4:46\n    4:46\n  Now playing\n\nDavid Lynch Presents Interview Project:  Clara\n\n4K views5 months ago\n\n                Created playlists\n\n    121 videosDavid Lynch Presents Interview Project PlaylistView full playlist\n\n    Shorts\n\n    David Lynch Presents Interview Project:  FRANK796 viewsDavid Lynch Presents Interview Project:  MR. SIEBERT640 viewsDavid Lynch Presents Interview Project:  ROBIN780 viewsDavid Lynch Presents Interview Project. Re-release SUMMER 2024. #davidlynch #twinpeaks #roadtrip2.3K viewsDavid Lynch Presents Interview Project:  BATSO. #davidlynch #twinpeaks #realpeople #interview941 viewsDavid Lynch Presents Interview Project:  BRENDA WRAY. #davidlynch #twinpeaks #realpeople #interviewt814 views\n\n              SearchInfoShoppingTap to unmute2xYou're signed outVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.CancelConfirmIf playback doesn't begin shortly, try restarting your device.•ShareInclude playlistAn error occurred while retrieving sharing information. Please try again later.Watch laterShareCopy link0:00 / •Watch full videoLive••Scroll for details\n\n        …\n\n      NaN / NaN",
    "summary": {
      "en": "David Lynch's Interview Project will re-release all 121 episodes of its original series on October 1, 2024, to celebrate its 15th anniversary. The episodes will be available in high definition for the first time. Viewers who are familiar with the series can enjoy revisiting it, while newcomers can discover it. Lynch emphasizes the project's focus on human experiences.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "c1017d760737a62f",
    "title": {
      "en": "Osgint – OSINT tool to find information about GitHub user",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/hippiiee/osgint",
    "score": 111,
    "by": "CHEF-KOCH",
    "time": 1742795502,
    "content": "OSGINT\n\n  Retrieve informations about a github username/email\n\nFeatures\n\n Find github username from an email\n Find email from github username (not working all the time)\n Find informations about a profile (account creation date, public gists, id, public pgp, public ssh ...)\n\nRequirements\npip3 install -r requirements.txt\n\nUsage\n$ python3 osgint.py -h\n\n .d88888b.                    d8b          888\nd88P\" \"Y88b                   Y8P          888\n888     888                                888\n888     888 .d8888b   .d88b.  888 88888b.  888888\n888     888 88K      d88P\"88b 888 888 \"88b 888\n888     888 \"Y8888b. 888  888 888 888  888 888\nY88b. .d88P      X88 Y88b 888 888 888  888 Y88b.\n \"Y88888P\"   88888P'  \"Y88888 888 888  888  \"Y888\n                          888  v1.0.0\n                     Y8b d88P\n                      \"Y88P\"\nBy Hippie | https://twitter.com/hiippiiie\n\nusage: osgint.py [-h] [-u USERNAME] [-e EMAIL] [--json]\n\noptions:\n  -h, --help            show this help message and exit\n  -u USERNAME, --username USERNAME\n                        Github username of the account to search for (default: None)\n  -e EMAIL, --email EMAIL\n                        Email of the account to search for github username (default: None)\n  --json                Return a json output (default: False)\n\nExample output\nusername\n$ ./osgint.py -u hippiiee\n[+] login : hippiiee\n[+] id : 41185722\n[+] avatar_url : https://avatars.githubusercontent.com/u/41185722?v=4\n[+] name : Hippie\n[+] blog : https://hippie.cat\n[+] bio : Hi !\n[+] public_repos : 10\n[+] public_gists : 0\n[+] followers : 8\n[+] following : 9\n[+] created_at : 2018-07-13T08:28:00Z\n[+] updated_at : 2022-08-21T13:11:36Z\n[+] public_gists : https://gist.github.com/hippiiee\n[+] GPG_keys : https://github.com/hippiiee.gpg\n[+] GPG_key_id : 27cbb171ff857c58\n[+] email : hquere@e3r4p3.42.fr hippolyte.q@gmail.com\n\n$ ./osgint.py -u hippiiee --json\n{\n    \"GPG_key_id\": \"27cbb171ff857c58\",\n    \"GPG_keys\": \"https://github.com/hippiiee.gpg\",\n    \"avatar_url\": \"https://avatars.githubusercontent.com/u/41185722?v=4\",\n    \"bio\": \"Hi !\",\n    \"blog\": \"https://hippie.cat\",\n    \"created_at\": \"2018-07-13T08:28:00Z\",\n    \"email\": [\n        \"hquere@e3r4p3.42.fr\",\n        \"hippolyte.q@gmail.com\"\n    ],\n    \"followers\": 8,\n    \"following\": 9,\n    \"id\": 41185722,\n    \"login\": \"hippiiee\",\n    \"name\": \"Hippie\",\n    \"public_gists\": \"https://gist.github.com/hippiiee\",\n    \"public_repos\": 10,\n    \"updated_at\": \"2022-08-21T13:11:36Z\"\n}\n\nEmail\n$ ./osgint.py -e chrisadr@gentoo.org\n[+] username : ChrisADR\n\n$ ./osgint.py -e chrisadr@gentoo.org --json\n{\n    \"username\": \"ChrisADR\"\n}\n\nHow does it works ?\nTo get a user email, osingt is checking :\n\nall the public commits of the user, if the email is not hidden in one of the commit it will be added to the list\nif the user have a GPG key, if he has one, it's getting the email from the content of the GPG after a base64 decode\ngithub user API\n\nTo get a user email, osgint is checking :\n\ngithub user API\n🚧 spoofing a commit with the email, then checking the name in the commit history (working every time) 🚧 (Work In Progress)\n\nProject inspired from Zen",
    "summary": {
      "en": "**OSGINT Overview**\n\nOSGINT is a tool designed to gather information about GitHub users based on their usernames or emails. \n\n**Key Features:**\n- Find a GitHub username using an email address.\n- Find an email address using a GitHub username (may not always work).\n- Retrieve user profile information, including account creation date, public gists, and more.\n\n**Requirements:**\n- Install the necessary packages using the command: `pip3 install -r requirements.txt`.\n\n**How to Use:**\n- Run the command: `$ python3 osgint.py -h` for help.\n- Options include:\n  - `-u USERNAME`: Search for a GitHub username.\n  - `-e EMAIL`: Search for an email to find the GitHub username.\n  - `--json`: Get output in JSON format.\n\n**Examples:**\n- To find user information by username:\n  ```\n  $ ./osgint.py -u hippiiee\n  ```\n- To find a username by email:\n  ```\n  $ ./osgint.py -e chrisadr@gentoo.org\n  ```\n\n**How It Works:**\n- To find an email, OSGINT checks:\n  - Public commits for visible emails.\n  - GPG keys for emails after decoding.\n  - The GitHub user API.\n\n**Note:** The project is still in progress and was inspired by Zen.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "2159168a750d76f2",
    "title": {
      "en": "Autology: A Lisp with access to its own interpreter",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/Kimbsy/autology",
    "score": 133,
    "by": "simonpure",
    "time": 1742553436,
    "content": "Autology\nA hot take on Lisp metaprogramming, Autology is a Lisp with access\nto its own interpreter.\nWhat is it?\nAutology is a functional interpreted Lisp language, written in Clojure.\nThe Autology interpreter function is defined as a data structure which is available to the Autology program, it is bound to the variable *i*.\nWhenever an expression needs to be evaluated, Autology will retrieve the current value of *i* from the current lexically scoped execution environment and use it to evaluate the expression.\nBy binding new values of *i* we can modify the behaviour of the Autology language while our program is running.\nWhy have you done this?\nMainly for fun and to explore a neat idea.\nAutology is not a particularly useful language as-is, its very slow and resource intensive.\nMacros are the normal tool for Lisp languages to modify their syntax or behaviour, but these are normally only available at compile time. Autology allows dynamic rewriting of the language at runtime.\nEven other non-Lisp languages are able to define some form of Domain Specific Language to allow programmers to express problems more clearly, however these are generally quite restrictive in scope. Autology allows full ad-hoc re-writing of the syntax of the language, as well as defining new features or removing existing ones.\nWhat can you do with it?\nWell to start with you might like to add functions to the language. Autology doesn't have them by default, but we can rebind the *i* symbol to a data structure representing a new interpreter function, one that does have functions.\n(bind (;; grab a copy of the `:atl/eval-list` section of the\n       ;; interpreter which is responsible for evaluating lists.\n       original (get-marker *i* :atl/eval-list)\n\n       ;; define a case test+body for use when the list expression\n       ;; starts with our function special form, in this case `λ`.\n       λ-form (qu (λ (let [[_λ params body] e]\n                       (fn [& values]\n                         (autology.core/evaluate\n                          body\n                          (reduce (fn [acc-env [s v]]\n                                    (assoc acc-env s v))\n                                  env\n                                  (zipmap params values)))))))\n\n       ;; rebind `*i*` to be a new interpreter with the\n       ;; `:atl/eval-list` section replaced with a version that\n       ;; includes our lambda handling special form.\n       *i* (replace-marker *i* :atl/eval-list\n                           (list :atl/eval-list\n                                 (concat (butlast original)\n                                         λ-form\n                                         (list (last original)))))\n\n       ;; We can now immediately define functions since the\n       ;; interpreter will have already been updated to evaluate the\n       ;; remaining bindings like this one.\n       double (λ (n)\n                 (+ n n)))\n\n      ;; Finally we can invoke our new function!\n      (double (double (double (double (double 1.3125))))))\n\nAfter that, pretty much anything you want!\n\nWhy not switch form applicative order evaluation to normal order evaluation?\nMaybe modify the language so it uses a continuation passing style?\nDefine a metacircular Lisp inside your program which has access to it's own interpreter as well at the Autology interpreter.\n\nRun tests\nclojure -X:test",
    "summary": {
      "en": "**Summary of Autology**\n\nAutology is a unique Lisp programming language that allows users to access and modify its own interpreter while the program is running. It is written in Clojure and functions as a functional interpreted language.\n\n**Key Features:**\n- **Dynamic Interpretation:** The interpreter is represented as a data structure linked to a variable called *i*. This allows for real-time modifications of the language’s behavior.\n- **Runtime Modifications:** Unlike typical Lisp macros that work at compile time, Autology enables changes to syntax and features during execution.\n- **Function Definitions:** Users can add new functionalities, like defining functions, by re-binding *i* to a modified interpreter that can handle these features.\n\n**Purpose:** \nAutology was created mainly for experimentation and fun, although it is not particularly efficient or practical for regular use.\n\n**Capabilities:**\n- Users can create new features, change evaluation strategies, or even implement a new Lisp interpreter within Autology itself.\n- The language is highly flexible, allowing extensive customization of its syntax and behavior.\n\nTo run tests for Autology, you can use the command `clojure -X:test`.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "522a6ccd367a86e3",
    "title": {
      "en": "Goblin.tools: simple, single-task tools to help neurodivergent people with tasks",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://goblin.tools/",
    "score": 322,
    "by": "ValentineC",
    "time": 1742825717,
    "content": "Help\n\n                        Magic Todo acts as a standard todo list, with some special sauce. Try the  button and let it automatically come up with the steps you need to accomplish your task! The 🌶 spiciness level 🌶 gives the tool a hint about how hard or stressful you find the task. The spicier, the more steps it will attempt to break it down into. It's not an exact measure, more a hint to the system.Top level tasks are automatically assigned a category indicated by an emoji. You can filter your list to one or more categories with the  button. The filter button also allows you to hide all completed items.Common task tools (edit, remove, add subtask, and estimator) are under the  button on each item. The  icon on the left can be dragged to reorder items to anywhere.Additional actions for the entire list can be found below it. They include  synchronisation between devices,  export options , undo and redo, and  bulk actions.\n\n        .checkbox-container {\n            flex-shrink: 0;\n            width: 20px;\n        }\n\n        .spinner-border {\n            display: none;\n        }\n\n        .parent-todo {\n            margin-bottom: 8px;\n        }\n\n        .spiciness-indicator {\n            position: absolute;\n            font-size: 0.6rem;\n            transform: translateY(-100%);\n        }\n\n        .edit-input {\n            display: inline;\n            margin-right: 5px;\n        }\n\n        .edit-input-container {\n            width: 100%;\n        }\n\n        .edit-confirm,\n        .edit-cancel {\n            display: inline;\n            margin-left: 2px;\n            padding: 0 4px;\n        }\n\n        .todoText {\n            user-select: text !important;\n            -webkit-user-select: text;\n            -moz-user-select: text;\n            -ms-user-select: text;\n        }\n\n        #categoryFilterMenu {\n            --bs-dropdown-min-width: 10px !important;\n            --bs-dropdown-item-padding-x: 0;\n        }\n\n            #categoryFilterMenu .form-check {\n                padding-left: 2rem;\n                padding-right: 0.4rem;\n                text-align: center;\n                align-content: center;\n            }\n\n        .list-group-item-content {\n            margin: -6px 0 -6px -16px;\n            /*padding: var(--bs-list-group-item-padding-y) var(--bs-list-group-item-padding-x);*/\n            padding: var(--bs-list-group-item-padding-y) 8px  var(--bs-list-group-item-padding-y) 12px !important;\n        }\n\n            .list-group-item-content:hover {\n                background-color: var(--bs-tertiary-bg);\n            }\n\n        #child-frame {\n            display: none;\n        }\n\n        .highlight {\n            background-color: var(--bs-tertiary-bg);\n        }\n\n        .list-group-item {\n            outline: var(--bs-list-group-border-width) solid var(--bs-list-group-border-color) !important;\n            padding: var(--bs-list-group-item-padding-y) 0 var(--bs-list-group-item-padding-y) 18px !important;\n            margin: -1px;\n            border: 0px !important;\n            box-sizing: content-box !important;\n        }\n\n        Magic ToDo\n        Breaking things down so you don't\n\n                    🌶️🌶️🌶️\n\n                Spiciness level:\n\n                    How much breaking down do you need?\n\n                    🌶️\n                    🌶️🌶️🌶️🌶️🌶️\n\n                     Sync\n\n                        EXPERIMENTAL FEATURE\n                        use at your own risk\n\n                                Enable cloud synchronization\n\n                            Username\n\n                            Password\n\n                            Save\n                            Cancel\n\n                            Please provide credentials to synchronize your ToDo list across devices.\n                            There are no accounts, your username must simply be unique.\n                            We recommend taking a backup with  before activating sync on new devices.\n\n                                How does this work?\n\n                                All devices with the same username and password will synchronise lists.\n                                Your list is encrypted on your devices using your username and password. It can not be seen or decrypted by admins or anyone who doesn't have your credentials.\n                                Synchronization is not instant, it can take a few seconds for your changes to come through on other devices.\n\n                        Save to file\n\n                        Load from saved file\n\n                        Copy to clipboard\n\n                        Print\n\n                        Export to Todoist template\n\n                        Export to iCal file\n\n            ☑️\n\n                         Estimate all tasks\n                         Mark all as not completed\n                         Clear all estimates\n                         Clear all completed tasks\n                         Clear entire list",
    "summary": {
      "en": "**Summary of Magic Todo:**\n\nMagic Todo is a todo list app that helps you manage tasks effectively. It allows you to automatically break down tasks into smaller steps based on how difficult or stressful you find them, indicated by a \"spiciness level\" (🌶). \n\nKey features include:\n- **Task Categories**: Each task is assigned an emoji category and you can filter tasks by these categories.\n- **Task Management**: You can edit, remove, add subtasks, and estimate tasks through buttons next to each item. You can also reorder items by dragging them.\n- **List Actions**: Additional options for the entire list include syncing across devices, exporting lists, and bulk actions (like marking tasks as completed).\n- **Synchronization**: You can sync your tasks across devices using a unique username and password. Your data is encrypted for security.\n\nOverall, Magic Todo simplifies task management by breaking down tasks and providing useful organizational tools.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "ed5299a42cd7c383",
    "title": {
      "en": "Btrfs Adding Fast/Realtime ZSTD Compression and Other Performance Optimizations",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.phoronix.com/news/Linux-6.15-Btrfs",
    "score": 14,
    "by": "todsacerdoti",
    "time": 1742888476,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "ac72d66503620b34",
    "title": {
      "en": "First Known Photographs of Living Specimens",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.inaturalist.org/projects/first-known-photographs-of-living-specimens",
    "score": 121,
    "by": "Morizero",
    "time": 1742501985,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "449a2c2ef5726266",
    "title": {
      "en": "Japanese scientists use stem cell treatment to restore movement in spinal injury",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://medicalxpress.com/news/2025-03-japanese-scientists-stem-cell-treatment.html",
    "score": 249,
    "by": "pseudolus",
    "time": 1742811919,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "72c8f64ceeefa7f5",
    "title": {
      "en": "The Mystery of the Oldest Writing System Remained Unsolved Until 1856",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.smithsonianmag.com/history/mystery-worlds-oldest-writing-system-remained-unsolved-until-four-scholars-raced-decipher-it-180985954/",
    "score": 78,
    "by": "wyclif",
    "time": 1742729075,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "2349423c5fce781f",
    "title": {
      "en": "Total lunar eclipse over Teide crater, Tenerife – a project with many obstacles",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://lrtimelapse.com/news/total-lunar-eclipse-over-teide-crater-tenerife/",
    "score": 165,
    "by": "elijahparker",
    "time": 1742569456,
    "content": "← LRTimelapse 7.3 released!Total lunar eclipse over the Teide crater, Tenerife – a project with many obstacles… Posted on 2025-03-21 by Gunther WegnerRarely have I thought so often about abandoning a photography project as during this lunar eclipse on Tenerife. Somehow, everything seemed to conspire against us this time. This is the story of an elaborately planned photo project where almost everything went wrong, but perseverance ultimately paid off. But let’s start from the beginning…Germans: Hier gibt es eine Deutsche Version.We, that is Uli and I, have made it almost a tradition to meet on Tenerife (Canary Islands, Spain) once a year. This time, Uli said, hey, on March 14th there’s a total lunar eclipse, and I’ve calculated a location from which we can photograph the fully eclipsed moon setting directly over the crater of Teide at dawn!It sounded almost too good to be true—because typical lunar eclipses are rather dull photographically, especially after you’ve seen and photographed a few. Without an interesting foreground, pictures of a red moon look the same anywhere in the world. Technically challenging, perhaps, but not very creative.A Unique ProjectNow, several exciting factors came together in Uli’s idea:We would experience a total eclipse on Tenerife, while in Germany, for example, the moon would only be partially eclipsed.We would have a foreground—the impressive crater of Teide, Tenerife’s landmark and Spain’s highest mountain at 3,715 meters.The totality would occur during dawn, meaning the sky would already be somewhat blue, and the moon would be low enough for us to use the crater as a foreground. We expected very aesthetic images from the contrast of the red eclipsed moon against the blue sky.According to my research, this opportunity to photograph a totally eclipsed moon over Teide wouldn’t occur again within the next 10 years—or possibly even much longer; I didn’t look further. These images would be genuinely unique.The PlanningUli used the smartphone app Planit for planning. This app not only displays exact eclipse timings but also shows the moon’s position relative to the observer’s location—in three dimensions. The app has quite a learning curve, but roughly speaking:You define the summit of Teide as your “target,” jump to the app’s day and time of moonset, and primarily search for a camera position reachable by foot.On the left, I’ve aligned the “scene” with the peak of Teide. The blue line on the right marks the spot where we’d need to be standing for the shot, based on where it intersects the trail.This means looking for an intersection on the blue line with a hiking trail or road. On Tenerife, there was actually only one reachable location, on a hiking path in the Canadas, below the caldera rim—about an hour’s hike from the nearest parking lot. This spot becomes your virtual camera position in Planit.Once you’ve found this spot, you can switch Planit to the VR camera mode to see Teide’s peak, the moon, and the Earth’s shadow through a virtual camera. You can even specify the focal length.We need to move further to the right to get the moon lined up over the crater. You can use the arrows to shift the camera position accordingly.Now, you can finely tune both the camera’s time and position—we want the fully shadowed moon precisely descending into the crater.Finally, you save the GPS coordinates for your chosen location.Leaving Nothing to ChanceRather than navigating blindly at night, we decided to scout the location during daylight first.Beautiful Landscapes in the Canadas del TeideOn a sunny morning, we drove an hour and a half up to Teide National Park and hiked to the spot. The route was quite strenuous, but the sun was shining, it was about 10 degrees Celsius—not too cold—and the solitude and landscape were fantastic.We didn’t want to think about the night yet, knowing temperatures would drop below freezing, coupled with the wind.In the background you can see the edge of the Caldera, the giant outer crater that surrounds the Pico del Teide.“A few meters more to the right,” said Uli, and I drew a line in the dust with my foot.Here’s where it would happen.At the moment, the sky was bright blue, and there was no sign of the moon. Hardly imaginable that in a few days, from exactly this spot, it would be precisely above the crater and eclipsed.We felt optimistic. This could really work out. On our way back, a wind began to pick up, funneling down from the direction of the Izaña observatories into our valley, like through a jet.The temperature dropped rapidly, and we already had to push a bit against the wind. A first, small taste of what was still ahead of us…Equipment ShortcomingsI was not entirely happy with the equipment I had available on the island. Usually, I don’t photograph much with telephoto lenses here, so my longest focal length was a 70-200mm f/2.8.Uli—always hardcore—had traveled to the island with 60 kg of gear, prepared for everything. Besides all his Sony lenses (200-600mm) and his Nikon Coolpix P1000 (24-3000mm!), he even had a 300mm and a Nikon 400mm f/2.8 with him. Crazy.“Since you have so many lenses – could you lend me your Nikon?” (The only lens of his that would fit my cameras.) Uli agreed. One less worry, as 200mm was quite short for moon photography.Another issue was my tripod situation. I had only one reliable tripod on the island. The second one didn’t deserve the name—I usually just use it as a holder for my tablet.Since this was a unique opportunity, I wanted to use two cameras. I had my Nikon Z8 and the Nikon Z50 II with me.So I decided to ask a friend—who was planning to visit us a few days later—to bring me a tripod. Unfortunately, he had to cancel the trip last minute, so once again, I was left tripod-less. Ordering stuff here on the island is a bit tricky, and good camera stores are few and far between. So I figured I’d head into the capital to buy one there. The selection I had briefly checked online wasn’t huge, but there was one tripod that actually deserved the name (and to be honest, I’m pretty into it now).After that little half-day trip, I could check off another item from the list. Now everything should finally go smoothly! (… though hindsight is always 20/20… ;-))Bad weather is coming…Weather ChallengesAlright, but the weather was still a big question mark. For days—actually, weeks—there’d been this weird weather pattern hanging over the Atlantic. Weird meaning strong winds from the west (instead of the usual northeast trade winds), rain, even snow up in the mountains, really bad weather in the north of the island, and high clouds—yep, even over the rim of the caldera, which was exactly where we wanted to shoot.And sure enough, the evening of the event was super cloudy. We kept checking the webcams and forecasts over and over.Uli wanted to head up the night before the lunar eclipse to set up a few wide-angle cameras for some time-lapse shots, planning to pick them up again the next morning.I was hoping to catch a bit of sleep beforehand. The alarm was set for 2 a.m.Worst-case scenario kicks inJust as I had packed everything and was about to head to bed, Uli called: “I’ve got good news and bad news. The good—weather’s actually not that bad. Not as windy as we feared.”“Sounds promising—and the bad?”“While I was out setting up my cameras, someone broke into my car and stole my second backpack—with all my telephoto lenses and other cameras…”“What??!!”Uli was surprisingly calm, considering he’d just lost two-thirds of his gear: “It’s insured…” — but still, how were we supposed to shoot the lunar eclipse now, without any long lenses?It really felt like the universe was telling us it wasn’t meant to be. I only had the 70–200 mm, and Uli’s longest lens left was a 135 mm. Not exactly ideal for capturing a lunar eclipse.If it had happened just a day earlier, we might’ve been able to borrow some lenses. But just hours before? No chance.We went back and forth. “So should we just call it off? Weather’s supposed to be unstable anyway…”Thinking about calling it offThat little inner voice—the one that already had zero interest in getting up at 2 a.m. and hiking for hours in the cold—kept chiming in: “This is going to be a bust anyway, it’s way too cold and probably all clouded over. Why don’t you just shoot it from down here in town, at the pier? You could sleep in two extra hours, it’s less windy, and way warmer.”Uli said he’d drive back down first and stop by a couple of electronics stores along the way—maybe, just maybe, he could find a telephoto lens there. Luckily, shops stay open pretty late on Tenerife…“Alright, let’s talk again once you’re back.”I kept refreshing the weather apps and checking webcams. Ran the numbers in Planit to figure out if we could at least capture the event down here on the coast. Sure, it would work… but it just wouldn’t be that once-in-a-lifetime shot.…or maybe not?At some point, Uli called back: “They don’t even carry camera gear at all anymore at places like MediaMarkt—but I just remembered I actually have another 300 mm lens lying around…” Me: “🙈 speechless…” Alright then—300 mm for him, 200 mm for me… not ideal, but worth a shot. Inner slacker, pipe down. “Alright, let’s do this. I’ll pick you up at 2:30,” I said. As you can probably guess, it was a short night. I showed up right on time at Uli’s: “Wow, traveling light, huh?” — honestly, without some dark humor, this whole thing would’ve been unbearable…Let’s go—up the mountain!Already in Vilaflor, the last village before the national park, the car beeped at 4°C. A few curves later, the roads were wet, and a little further up, they even started to sparkle with frost. I crawled up into the national park at 20 km/h. As soon as we entered the Caldera, we could see clouds spilling over the rim from the north—yep, exactly the direction we needed to head. “The observatories are still completely buried in clouds,” Uli said. Yeah… I figured as much. We drove past the massive “Roques de Garcia.” The full moon was so bright, we could actually see the landscape with the naked eye. At one point, I switched off the headlights—crazy how bright that full moon is.Is the weather about to ruin everything?“What if we just shoot the eclipse here, with the silhouette of the Roques in the foreground?”We thought about it for a moment. The downside: the moon would dip behind them later, since the rocks aren’t that tall, and the sky would already be getting brighter with dawn. Plus, there’d be a much higher chance of bumping into other people here—people who might wander straight into our timelapses. And honestly, it just wouldn’t be the shot we’d spent so much time planning for.So, we kept going. From the Teide cable car station onwards, the road got damp again, and I had to slow way down. These curves are no joke—and there are no winter tires on Tenerife.Wisps of clouds started forming above us. Just 4 km to go. The clouds kept thickening. Oh man, this is not looking good…Eventually, we reached the parking lot near El Portillo. The clouds were spilling into the caldera from the left and drifting right over us.Down there, where the clouds are drifting—that’s where we need to go. (Shot on smartphone)“Let’s walk over to the viewpoint and check things out—worst case, we head back to the Roques. Timing-wise, we’d still make it.”But honestly, the viewpoint didn’t really give us any new insights. The clouds were still there, and we couldn’t tell whether they’d get better or worse. We figured we had about a 50:50 chance of getting our shot. At the Roques? Pretty much a guaranteed 100%. But of course… it just wouldn’t be the same. So—what to do?We decided to take the riskIn the end, we went with the more uncertain, but potentially way more rewarding option—and started the hike.Inside the caldera (Smartphone)Thanks to the full moon, we didn’t need a headlamp or any other light. Even the initial bit of scrambling over some rocks worked just fine in moonlight alone. As timelapse and astro photographers, we naturally have a bit of an aversion to those insanely bright LED floodlights people call “headlamps” these days. And speaking of which—we actually saw quite a few of them, despite the unstable weather, making their way up the slopes of Teide. It never ceases to amaze me how many people head up there, no matter the conditions.And if it was already -4°C and pretty windy where we were at 2,200 meters, it must’ve been closer to -10°C up there, more than 1,000 meters higher—with some seriously brutal wind.Teide in the full moonlight (Smartphone)But we had layered up properly, and while we were hiking, we weren’t cold at all. The wind was at our backs, too.Setting upThe moon was still fairly high, so we had plenty of time to set everything up without rushing. Uli planned to use his main camera with the 300 mm on the Polarie star tracker. Getting the mount perfectly aligned with the North Star always takes a little time—but luckily, he managed to catch a clear glimpse of Polaris through a gap in the clouds.I had the 24–70 on the Z50II set to about 50 mm, aiming for a wide shot with the whole Teide in frame. Once the moon moved a bit lower, I was going to switch over to the 70–200 on my new tripod, dialed in at 200 mm, and aim it right at the crater’s summit. But first, I shot a “boring” sequence—just the already-eclipsed moon on its own, no background.A layer of ice formed on our gear in no timeI controlled the exposure of both cameras using qDslrDashboard—but mostly manually, not with the “Auto Holy Grail” mode. The reason? During a lunar eclipse, you need to be able to adjust exposure very quickly, while the “Auto Holy Grail” algorithm is designed to do the opposite: it smooths out short-term brightness changes and only adapts gradually over time—perfect for sunrises and sunsets, but not so much for a fast-changing eclipse.At one point, the moon drifted out of frame on the 200 mm, so I decided it was time to set up the final shot. The idea was to frame the crater at the bottom and start shooting just as the moon was entering the frame—this way, I wouldn’t have to touch the camera again until the moon had completely moved out.Challenge: ExposureA quick word about exposure during a lunar eclipse—because honestly, it’s one of the biggest challenges.While even just a sliver of the full moon is still visible, not even today’s sensors—with their 14 or 15 stops of dynamic range—can capture both the moon and the landscape without compromise. Either you expose for the landscape and the moon is totally blown out, or you expose for the moon and the landscape turns to pure black.The trick is finding that sweet spot: a balance where you can still pull some detail out of the landscape in post without everything being a noisy mess, and where the moon doesn’t just show up as a giant white blob.Test shots and histograms? Not much help either—the moon just shows up as a bright dot on the screen, everything else around it looks pitch black.This is what the image preview looks like on the monitor…At that point, you really have to rely on experience to judge the exposure—trying to nail it so there’s still just enough hidden in the shadows to pull the landscape out later. Even with a flat camera profile, there’s just no way to tell from the JPG or histogram what you’ll actually be able to recover from the RAW file later on.The same image (Nikon Z50 II) after RAW processing – you can now see the moon is already slightly eclipsed, and there’s detail in the landscape too.As totality approaches, exposure gets easier since the contrast drops. The Z50II performed well here, though of course it can’t quite keep up with the dynamic range of the Z8. 30mm, f/2.8, 2 sec, ISO 800.Everything changes suddenly the moment totality begins. In our case, the eclipse was right on the edge of the umbra, so even toward the end of totality, we still had a faint glow on the right side of the moon in the images.To get the best possible timelapse of a lunar eclipse, you have to react fast when totality hits and adjust your exposure accordingly. For the 50 mm wide shot, I decided to let the moon blow out just a little—as little as possible, but enough to still keep some detail in the landscape. For the telephoto shot, I was only capturing totality anyway, which made it way easier to expose properly.Meanwhile, clouds had drifted in front of Teide and we found ourselves standing in freezing rain. Thankfully, the crater and the moon were still more or less visible. Our camera bags were crusted with ice crystals by now, and during my final camera adjustments, I couldn’t feel my fingers anymore. I couldn’t even manage to turn off the display—the button just wouldn’t press. Oh well, the battery would survive. Hopefully.Would the moon actually fall into the crater? Shot with the Nikon Z8 at 200mm, f/2.8, ISO 800, and 2 sec exposure.So, we were all set: my wide shot was rolling, the telephoto sequence was up and running, and both of Uli’s cameras were clicking away too. I had set the interval to 3 seconds, by the way.Unexpected interruptionFinally, we could shove our hands into our warm pockets and just enjoy the show. Just as we were starting to relax, we suddenly saw two of those ultra-bright “searchlight” headlamps making their way down the rocky slope—right between us and Teide. WTF?? There’s no trail there, and definitely no parking spot up top—where the heck did they come from?They just kept weaving their way down toward us, totally unfazed, and their insanely bright beams kept sweeping straight in our direction—almost certainly hitting our cameras, too.You’ve got to be kidding me. “Turn off the lights!” I shouted into the night. “¡Apaguen las luces!” — no reaction. I called again, louder this time. Still nothing. Eventually, they settled on some rocks in front of us—but left their lights on the entire time.Who are these people? I mean, who else would be able to pinpoint this exact location so precisely? Probably only photographers, right? I mean, who else knows how to use Planit? 😉 But then again, photographers—especially astro nerds—usually know better than to blast everything with full-beam headlamps all night. We didn’t get the feeling they were even shooting photos… so what were they doing there?Guess we’ll never know. But seriously, running into someone out here, at this hour, was the last thing we expected.In the end, we just had to roll with it. Whether those lights would show up in our footage… well, we’d find out later. For now, we just wanted to soak in the magic of the total lunar eclipse.The freezing rain had stopped, and the clouds decided to give the summit a break—drifting by just beneath the peak.Totality approachesFrom the moment totality began to the point where the moon disappeared behind the crater, we had just 6 minutes. The sky had already started to take on a faint blue hue with the approaching dawn. And now, more clouds were looming—this time coming in from the right. Just after the moon touched the edge of the crater, one of those clouds drifted right in front of it—because of course it did. Luckily, it was a small one and didn’t mess up the final result too much. And—miraculously—the lights from those ridiculously bright headlamps didn’t show up in the timelapse at all. Sometimes, you just need a bit of luck!We did it!And here it is—the single frame from the timelapse that made all of this effort totally worth it:It worked! 🙂 Nikon Z8, 200mm, f/2.8, 2 sec, ISO 800.Once the moon had disappeared, we briefly considered running off to the right to catch one last glimpse of it beside the mountain—but it didn’t take long to realize that wasn’t going to happen. We still had to de-ice and pack up all the gear, and with how quickly the moon was setting, there was just no way we’d make it in time. So instead, we let the cameras keep rolling a little longer and simply watched as the day slowly began to break.Photo: Uli FehrOne more timelapse with the ice-covered camera, this time facing the edge of the caldera, where clouds were now spilling in from the south as well.We did it! 🙂The way backFrozen to the bone but happy, we packed up everything calmly and started the hike back to the car. Straight into the thick clouds that soon wrapped around us. We battled against the wind, and a thin layer of ice had formed on the ground. I really didn’t want to think about the drive back just yet…Somewhere out there in that fog bank… is our car.Every now and then, Teide lit up in a beautiful orange glow from the rising sun.Everything frozen…🙂When we finally got back to the car, it was completely frozen over too. Ice scrapers in Tenerife? Yeah… not exactly a booming market. So with hands that were finally starting to thaw, we scraped the ice off the windshield. My phone still showed -4°C.And once again, we got lucky: the road wasn’t too icy, so we carefully began the drive back—at a snail’s pace. The higher the sun climbed and the farther south we got, the drier the road became, and before long the thermometer crept back into positive territory.We did it!A little riddle for youTired but seriously happy, we started our descent—and ended up witnessing a super interesting physical-chemical phenomenon. Check out this photo:  In the spots where there was shadow—or just recently had been—the road had already dried. But in the sun? Still wet. How’s that even possible? If you know the answer, drop it in the comments! 🙂Uli picked up the camera he’d set up the night before, and then we headed back down to the warm coast for a well-deserved late breakfast—though not without quickly grabbing our laptops first to offload the sequences. And while we were still waiting for our food and coffee, we couldn’t help ourselves—we already started reviewing the footage. The excitement was just too much!Do you want to learn how to shoot and edit such timelapses? Get my E-Book Timelapse Photography and learn it step by step the easy way!My timelapseAnd to wrap things up, here’s a little compilation of the timelapse sequences I captured. Enjoy the show!\n\n      Total Lunar Eclipse over Crater of Pico del Teide, Tenerife (HDR Timelapse)\n    Video can&#8217;t be loaded because JavaScript is disabled: <a href=\"https://youtu.be/dSuCsWTxZ6U\" title=\"Total Lunar Eclipse over Crater of Pico del Teide, Tenerife (HDR Timelapse)\">Total Lunar Eclipse over Crater of Pico del Teide, Tenerife (HDR Timelapse) (https://youtu.be/dSuCsWTxZ6U)</a>To watch the video in HDR, you’ll need an HDR-capable TV or, on a computer, an HDR-capable monitor and browser (Chromium-based), with HDR enabled in your operating system. Make sure to watch the video directly on YouTube!P.S. Have you ever planned and pulled off a photo project this elaborate? I’d love to hear about it—drop me a comment!P.P.S. And be sure to check out Uli on Instagram and Facebook—he’s always up to some wild photo adventures. 🙂This might also be of interest to you:LA Timelapse Film Festival - the award ceremony now on Youtube! The Los Angeles Timelapse Film Festival (LATFF), first of its kind, took place last Saturday (Jan. 28, 2023) as a live stream and was…Northern Skies 4K (Cello Version, 2024) This is an update of my Film Northern Skies where the Cello in the Music track was played by Georg Roither, a professional cellist…Shorter Basic Tutorial (no Lightroom) This is the simplified, shorter basic tutorial for LRTimelapse without Lightroom Classic. You will learn how to edit your timelapse using LRTimelapse only: keyframe…LRTimelapse 7 released and new Tutorials! Dear Community, I've now released the final version of LRTimelapse 7, the latest \"major\" upgrade of LRTimelapse. I've spent an enormous amount of time…LA Timelapse Film Festival Audience Voting and Livestream Next Saturday, Jan. 28, 2023, the award ceremony for the Los Angeles Timelapse Film Festival will take place in an online ceremony - and…Basic Tutorial This is the complete basic tutorial for LRTimelapse. Additionally to the internal editing and processing in LRTimelapse it also includes editing and exporting via…← LRTimelapse 7.3 released!9 Responses to Total lunar eclipse over the Teide crater, Tenerife – a project with many obstacles… Constantin says: 2025-03-22 at 3:06 pmOn the topic of the road: Parts still in shade have humidity collected from the night in form of very light frost, barely visible. This melts when the sun hits and thereby becomes clearly visible as wet road.Reply Gunther Wegner says: 2025-03-22 at 5:15 pmYes, but here the road is wet in the sun while it is dry in the shade. That’s paradoxical and the other way around, as you would expect. I’ll wait for a few more answers, then I’ll solve it! 🤗Reply Mark Jaeger says: 2025-03-25 at 12:07 amGunther, Here are my thoughts on the wet vs dry pavement: The scene is on the island of Tenerife somewhere near maximum altitude (for the area).  You are 3+ thousand meters above sea level and the air temperature is a little lower than freezing. At altitude the air pressure is reduced, the air temperature is reduced and there is ample wind. All of this is favorable for sublimation but not so much for evaporation as, with below freezing temperatures, the vapor pressure is reduced.Here is how it goes: The entire road begins in shade. The sun is rising from the right side of the photo warming the road from left to right. The wind is blowing and sublimation is taking place.  The road is drying. There is a temperature where in sublimation the molecules will leave the surface and go into the air.  At slightly higher temperatures the molecules can re-attach to the surface (thus not leaving) The left side of the road has passed the critical temperature where sublimated molecules re-attach to the surface because it is “warm enough”. The right side of the road, in shadow, is not warm enough for sublimated molecules to re-attach and the molecules  go into the air (drying). The race to dryness is won by the area in shadow (a sublime story). The photo was taken at a very opportune time as, I think, the race was only won by a hair.Reply Gunther Wegner says: 2025-03-25 at 9:52 amThis is an interesting explanation. I’ve tried to do quite some research by myself, it’s not easy to find an explanation that says “this is true or false”. Sublimation was also what I instantly thought, when I saw it. But upon further research I found out that sublimation takes a very long time – hours or even days to transform thin layers of ice directly into vapor (skipping the melting).An easier, and for me also plausible explanation could be the following: When the sun hits the dark road, it warms the pavement above freezing so the thin layer of ice melts into liquid water—making those spots look wet. In the shaded areas, the surface stays below freezing, so the ice doesn’t turn into water, which would appear shiny and dark on the road. The ice molecules alone in the asphalt would make the road look dry. The moving shadows would then expose the areas at the borders of the shadows to the sun, where after a couple of minutes the ice would also melt, turning the road dark. While it’s not yet melted, however, it still looks dry (iced) at that “borders”. That’s why shaded patches look dry—even though they’re still icy—while sunny patches look wet from melting.Reply Mark Jaeger says: 2025-03-26 at 3:38 amGunther, Sublimation, with the right conditions, can happen just as fast as evaporation.   Your scenario could be correct but mine is also scientifically possible. Best, Mark Gavin says: 2025-03-22 at 7:27 amI had a think about this phenomenon on the road and I suspect that the road heated by the sun is much closer to the dew point than the road in the shade. The clear night sky has kept the road dry, but with the advent of some warming, the moisture in the air is attracted to objects at or close to the dew point. Best way to find out the answer is to have a stab in the dark, like I have, and then the experts will show up to give us the science.Reply Matthew Wheeler says: 2025-03-22 at 5:07 amThank you for a great finger-nail biting account of this expensive adventure, with such a beautiful, spectacular outcome after all the work and obstacles–it is so well presented and I had to read to the end right away!  About the shadow “photography” on the road: I have seen this fair bit in Canada, where the snow can be etched away by the sun, leaving quite intricate images of the shadows. Frozen mud can become quite light in tone, and darken when it thaws. In this case it is possible the pavement looks dry because drizzle had frozen into it, and the width of the gap outside the current shadow shows the time it took to melt and darken the road surface.Reply Silvana Della Camera says: 2025-03-22 at 12:27 amWonderful! I am glad you didn’t give up. I have found that the battle within is often harder than the difficult task itself. Thank you for sharing your story and the remarkable results.Reply John Edwards says: 2025-03-21 at 7:48 pmLove the planning and perseverance.  Thanks for sharing your story and your excellent timelapse of the lunar eclipse.ReplyLeave a Reply Cancel replyYour email address will not be published. Required fields are marked *Comment *document.getElementById(\"comment\").setAttribute( \"id\", \"ad1f874dc4d375b474b60cee41b1bfc8\" );document.getElementById(\"c60cb81b37\").setAttribute( \"id\", \"comment\" );Name * Email * Website",
    "summary": {
      "en": "In a blog post, photographer Gunther Wegner shares his challenging experience capturing a total lunar eclipse over the Teide crater in Tenerife. He and his friend Uli planned this elaborate photo project, believing it would yield unique images, as it was a rare opportunity. They meticulously prepared by scouting the location and using a smartphone app to plan their shots. \n\nHowever, they faced numerous obstacles, including bad weather, equipment issues, and even a theft that left Uli without most of his lenses. Despite these setbacks, they persevered and decided to go ahead with their plan, hiking to their chosen spot under freezing conditions.\n\nAs they set up their cameras, they encountered unexpected challenges, such as drifting clouds and bright headlamps from other hikers. Despite the freezing rain and difficult conditions, they managed to capture stunning shots of the eclipse as the moon set over the crater. \n\nIn the end, their hard work paid off, and they were thrilled with the results. Wegner concludes with a link to the timelapse video of the eclipse and invites readers to share their own photography experiences.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "84af1fd35b76d30e",
    "title": {
      "en": "The Wright brothers invented the airplane, right? Not if you're in Brazil",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.washingtonpost.com/world/2025/03/21/brazil-airplane-wright-brothers-santos-dumont/",
    "score": 235,
    "by": "benbreen",
    "time": 1742608551,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "66e0dedfb6cf531e",
    "title": {
      "en": "Microbes can capture carbon and degrade plastic – why aren't we using them more?",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.nature.com/articles/d41586-025-00875-w",
    "score": 5,
    "by": "GavCo",
    "time": 1742901557,
    "content": "COMMENT\n                25 March 2025\n\n            Microbes can capture carbon and degrade plastic — why aren’t we using them more?\n\n                    Interventions involving bacteria or fungi could help to sequester greenhouse gases, create more sustainable products and clean up pollution — in ways that are economically viable and safe.\n\n                By\n\n                Rino Rappuoli0,\n\n                Nguyen K. Nguyen1,\n\n                David E. Bloom2,\n\n                Charles Brooke3,\n\n                Rachel M. Burckhardt4,\n\n                Alan D. Dangour5,\n\n                Dilfuza Egamberdieva6,\n\n                Gigi K. Gronvall7,\n\n                Trevor D. Lawley8,\n\n                Jay T. Lennon9,\n\n                Ryan Morhard10,\n\n                Aindrila Mukhopadhyay11,\n\n                Raquel Peixoto12,\n\n                Pamela A. Silver13 &\n\n        …\n\n                Lisa Y. Stein14\n\nShow authors\n\n                    Rino Rappuoli\n\n                            Rino Rappuoli is scientific director at the Fondazione Biotecnopolo di Siena, Siena, Italy.\n\n                            View author publications\n\n                                You can also search for this author in PubMed\n                                    Google Scholar\n\n                    Nguyen K. Nguyen\n\n                            Nguyen K. Nguyen is director of the American Academy of Microbiology at the American Society for Microbiology, Washington DC, USA.\n\n                            View author publications\n\n                                You can also search for this author in PubMed\n                                    Google Scholar\n\n                    David E. Bloom\n\n                            David E. Bloom is professor of economics and demography at the Department of Global Health and Population, Harvard T. H. Chan School of Public Health, Boston, Massachusetts, USA.\n\n                            View author publications\n\n                                You can also search for this author in PubMed\n                                    Google Scholar\n\n                    Charles Brooke\n\n                            Charles Brooke is program lead for livestock enteric methane mitigation at Spark Climate Solutions, Covina, California, USA.\n\n                            View author publications\n\n                                You can also search for this author in PubMed\n                                    Google Scholar\n\n                    Rachel M. Burckhardt\n\n                            Rachel M. Burckhardt is program officer at the American Academy of Microbiology, American Society for Microbiology, Washington DC, USA.\n\n                            View author publications\n\n                                You can also search for this author in PubMed\n                                    Google Scholar\n\n                    Alan D. Dangour\n\n                            Alan D. Dangour is director of the Climate & Health Programme, Wellcome, London, UK.\n\n                            View author publications\n\n                                You can also search for this author in PubMed\n                                    Google Scholar\n\n                    Dilfuza Egamberdieva\n\n                            Dilfuza Egamberdieva is head of the Biosafety Research and Food Safety Lab at the Institute of Fundamental and Applied Research, National Research University TIIAME, Tashkent, Uzbekistan.\n\n                            View author publications\n\n                                You can also search for this author in PubMed\n                                    Google Scholar\n\n                    Gigi K. Gronvall\n\n                            Gigi K. Gronvall is professor in the Department of Environmental Health and Engineering, Johns Hopkins Bloomberg School of Public Health, Baltimore, Maryland, USA.\n\n                            View author publications\n\n                                You can also search for this author in PubMed\n                                    Google Scholar\n\n                    Trevor D. Lawley\n\n                            Trevor D. Lawley is group leader at the Wellcome Sanger Institute, Wellcome Genome Campus, Hinxton, UK.\n\n                            View author publications\n\n                                You can also search for this author in PubMed\n                                    Google Scholar\n\n                    Jay T. Lennon\n\n                            Jay T. Lennon is professor in the Department of Biology, Indiana University, Bloomington, Indiana, USA.\n\n                            View author publications\n\n                                You can also search for this author in PubMed\n                                    Google Scholar\n\n                    Ryan Morhard\n\n                            Ryan Morhard is adjunct professor at Georgetown University School of Foreign Service, Washington DC, USA.\n\n                            View author publications\n\n                                You can also search for this author in PubMed\n                                    Google Scholar\n\n                    Aindrila Mukhopadhyay\n\n                            Aindrila Mukhopadhyay is senior scientist in the Biological Systems and Engineering Division, Lawrence Berkeley National Laboratory, Berkeley, California, USA.\n\n                            View author publications\n\n                                You can also search for this author in PubMed\n                                    Google Scholar\n\n                    Raquel Peixoto\n\n                            Raquel Peixoto is professor in the Biological and Environmental Science and Engineering Division, King Abdullah University of Science and Technology, Thuwal, Saudi Arabia.\n\n                            View author publications\n\n                                You can also search for this author in PubMed\n                                    Google Scholar\n\n                    Pamela A. Silver\n\n                            Pamela A. Silver is professor in the Department of Systems Biology, Harvard Medical School, Boston, Massachusetts, USA.\n\n                            View author publications\n\n                                You can also search for this author in PubMed\n                                    Google Scholar\n\n                    Lisa Y. Stein\n\n                            Lisa Y. Stein is professor and Canada Research Chair in Climate Change Microbiology in the Department of Biological Sciences, University of Alberta, Edmonton, Alberta, Canada.\n\n                            View author publications\n\n                                You can also search for this author in PubMed\n                                    Google Scholar\n\n            Twitter\n\n            Facebook\n\n            Email\n\n            A farmer in India sprays a harvested paddy field with a solution of fungi that breaks down plant stems, avoiding the need for stubble burning. Credit: Naveen Sharma/SOPA Images/LightRocket via Getty\n\n                You have full access to this article via your institution.\n\n                Download PDF\n\n                         Microorganisms have shaped Earth for almost four billion years. At least a trillion microbial species sustain the biosphere — for instance, by producing oxygen or sequestering carbon1. Microbes thrive in extreme environments and use diverse energy sources, from methane to metals. And they can catalyse complex reactions under ambient temperatures and pressures with remarkable efficiency.The potential to exploit these microbial abilities to substantially reduce the impact of human activities on the planet has been recognized by many2. And bacteria or fungi are already being used to produce materials, fuels and fertilizers in ways that reduce energy consumption and the use of fossil-fuel feedstocks, as well as to clean up waste water and contaminants3.Despite their wide-ranging potential, however, microbe-based technologies remain largely overlooked in international plans to combat climate change or reduce the loss of biodiversity4. For example, discussions about the role of microbial technologies in achieving fossil-free alternatives to current products and processes were minimal or absent at the United Nations conferences of the parties (COPs) in 2023 and 2024 on climate change, and on biodiversity in 2022 and 2024 (see Nature 636, 17–18; 2024).Is the COP29 climate deal a historic breakthrough or letdown? Researchers reactTo better leverage microbiology in addressing climate change and other sustainability challenges, the International Union of Microbiological Societies and the American Society for Microbiology brought us (the authors) together in December 2023 — as a group of microbiologists, public-health scientists and economists with expertise in health, energy, greenhouse gases, agriculture, soil and water. In a series of meetings, we have assessed whether certain microbe-based technologies that are already on the market could contribute to sustainable solutions that are scalable, ethical and economically viable. We have identified cases in which the technical feasibility of an approach has already been demonstrated and in which solutions could become competitive with today’s fossil-based approaches in 5–15 years.This work has convinced us that microbe-based interventions offer considerable promise as technological solutions for addressing climate change and — by reducing pollution and global warming — biodiversity loss. Here, we explain why they could be so important5 and highlight some of the issues that we think microbiologists, climate scientists, ecologists and public-health scientists, along with corporations, economists and policymakers, will need to consider to deploy such solutions at scale6.Microbial possibilitiesThe use of genomics, bioengineering tools and advances in artificial intelligence are greatly enhancing researchers’ abilities to design proteins, microbes or microbial communities. Using these and other approaches, microbiologists could help to tackle three key problems.First, many products manufactured from fossil fuels (energy, other fuels and chemicals) could be produced by ‘feeding’ microbes with waste plastics, carbon dioxide, methane or organic matter such as sugar cane or wood chips.Microbes that grow underneath artificial floating islands can transform lakes from net methane sources into carbon sinks.Credit: WaterClean TechnologiesAmong the many companies applying microbe-based solutions to address climate change, LanzaTech, a carbon-upcycling company in Skokie, Illinois, is working on producing aviation fuel on a commercial scale from the ethanol produced when microbes metabolize industrial waste gases or sugar cane. Meanwhile, the firm NatureWorks in Plymouth, Minnesota, is producing polymers, fibres and bioplastics using the microbial fermentation of feedstocks, such as cassava, sugar cane and beets. Second, microbes could be used to clean up pollution — from greenhouse gases, crude oil, plastics and pesticides to pharmaceuticals.For instance, a start-up firm called Carbios, based in Clermont-Ferrand, France, has developed a modified bacterial enzyme that breaks down and recycles polyethylene terephthalate (PET), one of the most common single-use plastics. Another company — Oil Spill Eater International in Dallas, Texas — uses microbes to clean up oil spills, and large waste-management corporations in North America are using bacteria called methanotrophs to convert the methane produced from landfill (a more potent greenhouse gas than CO2) into ethanol, biofuels, polymers, biodegradable plastics and industrial chemicals.Drill, baby drill? Trump policies will hurt climate ― but US green transition is underwayThe company Floating Island International in Shepherd, Montana, is even building artificial floating islands on lakes and reservoirs that have been polluted by excessive nutrient run-off, so that methane-metabolizing microbes (which colonize the underside of the islands) can remove methane originating from lake sediments. The goal in this case is to transform inland lakes and reservoirs from net methane sources into carbon sinks.Finally, microbes could be used to make food production less reliant on chemical fertilizers and so more sustainable.The chemical process needed to produce ammonia for fertilizer involves burning fossil fuels to obtain the high temperatures and pressures needed (up to 500 °C and 200 atmospheric pressures), releasing 450 megatonnes of CO2 into the atmosphere each year (1.5% of all CO2 emissions)7. Furthermore, excess chemical fertilizers that flow into rivers, lakes and oceans cause algal blooms, which enhance the emission of nitrous oxide, a greenhouse gas that is more potent than either CO2 or methane.Many bacteria and archaea can be used to produce nitrogen fertilizer with much lower greenhouse-gas emissions than synthetic fertilizers. This is because the microbes fix nitrogen at room temperature and at sea-level atmospheric pressure using enzymes known as nitrogenases that convert atmospheric nitrogen (N2) into ammonia (NH3).Several companies are now selling biofertilizers, which are formulations containing bacteria called rhizobia or other microbes that can increase the availability of nutrients to plants (see ‘Towards a bioeconomy’ and go.nature.com/3fs2xqf). A growing number of microbial biopesticides are also offering food producers a way to control crop pests without harming human or animal health or releasing greenhouse gases into the atmosphere8.Source: https://www.precedenceresearch.com/fertilizer-marketKeeping it safeAs more microbe-based solutions enter the market — whether bioengineered or naturally existing — biosafety considerations will become increasingly important.Many solutions, such as using bacteria to degrade crude oil or plastics, have been shown to be effective and safe in a laboratory setting9. Yet scaling up their use to the levels needed to reduce global emissions or global biodiversity loss could lead to unforeseen complications.Bacteria are being designed to break down plastic waste.Credit: Carbios–AgenceSkotchProd Certain safeguards — designing bacteria that can persist in an ecosystem for only a short time or that can exist under only specific environmental conditions — are already being developed and applied4. And, in a similar way to phased clinical trials in biomedical research, laboratory experiments could be followed by contained tests in the outdoor environment, which could then be followed by larger-scale field testing. Investigators will also need to monitor systems over time, which could involve the sequencing of environmental DNA from waste water and other approaches that are used in infectious-disease surveillance.Ultimately, the effective deployment, containment and monitoring of large-scale microbe-based solutions will require scientific communities, governments and corporations to collaboratively develop evidence-based policies and engage in clear and transparent communication about the enormous opportunities and the potential risks.Making it pay\n\n                                    Enjoying our latest content?\n                                    Login or create an account to continue\n\n                                            Access the most recent journalism from Nature's award-winning team\n                                            Explore the latest features & opinion covering groundbreaking research\n\n                                            Access through your institution\n\n                                    or\n\n                                            Sign in or create an account\n\n                                            Continue with Google\n\n                                            Continue with ORCiD\n\n                Nature 639, 864-866 (2025)\n                doi: https://doi.org/10.1038/d41586-025-00875-wThe views expressed in this article are the authors’ own, not those of the institutions, affiliations or agencies that fund their research.\n\n                ReferencesCavicchioli, R. et al. Nature Rev. Microbiol. 17, 569–586 (2019).Article\n    PubMed\n\n                    Google Scholar\n                Rappuoli, R., Young, P., Ron, E., Pecetta, S. & Pizza, M. One Health Outlook 5, 5 (2023).Article\n    PubMed\n\n                    Google Scholar\n                Crowther, T. W. et al. Cell 187, 5195–5216 (2024).Article\n    PubMed\n\n                    Google Scholar\n                Peixoto, R. et al. Nature Rev. Microbiol. 23, 1–2 (2025).Article\n    PubMed\n\n                    Google Scholar\n                Hodgson, A. et al. The U.S. Bioeconomy: Charting a Course for a Resilient and Competitive Future (Schmidt Futures, 2022).\n                    Google Scholar\n                Rappuoli, R. et al. Microbial Solutions for Climate Change: Toward an Economically Resilient Future (American Academy of Microbiology, 2025).\n                    Google Scholar\n                Mingolla, S. & Rosa, L. Nature Food https://doi.org/10.1038/s43016-025-01125-y (2025).Article\n\n                    Google Scholar\n                Wend, K., Zorrilla, L., Freimoser, F. M. & Gallet, A. Environ. Health 23, 49 (2024).Article\n    PubMed\n\n                    Google Scholar\n                Danso, D., Chow, J. & Streit, W. R. Appl. Environ. Microbiol. 85, e01095-19 (2019).Article\n    PubMed\n\n                    Google Scholar\n                Romanello, M. et al. Lancet 404, 1847–1896 (2024).Article\n    PubMed\n\n                    Google Scholar\n                US Department of Energy. Clean Cities and Communities: Alternative Fuel Price Report (US Department of Energy, 2024).\n                    Google Scholar\n                Download references\n\n                    Reprints and permissions\n\n                    Competing Interests\n                    The authors declare no competing interests.\n\n                Related Articles\n\n                        Is the COP29 climate deal a historic breakthrough or letdown? Researchers react\n\n                        Drill, baby drill? Trump policies will hurt climate ― but US green transition is underway\n\n                        Scientists’ warning to humanity: microorganisms and climate change\n\n                Subjects\n\n                Climate change\n\n                Microbiology\n\n                Biodiversity\n\n                Chemistry\n\n    Latest on:\n\n                Climate change\n\n                                    Life-giving oxygen is wafting out of lakes worldwide\n                                    Research Highlight 25 MAR 25\n\n                                    How a scientist–pop industry partnership slashed a live gig’s carbon emissions by 98%\n                                    Career News 25 MAR 25\n\n                                    Climate change will send home insurance spiralling. Here’s how to control costs\n                                    World View 25 MAR 25\n\n                Microbiology\n\n                                    Exclusive: NIH to cut grants for COVID research, documents reveal\n                                    News 26 MAR 25\n\n                                    MFSD6 is an entry receptor for enterovirus D68\n                                    Article 25 MAR 25\n\n                                    Measles is surging in the US: how bad could it get?\n                                    News Explainer 19 MAR 25\n\n                Biodiversity\n\n                                    Species turnover does not rescue biodiversity in fragmented landscapes\n                                    Article 12 MAR 25\n\n                                    Meet the ‘woolly mouse’: why scientists doubt it’s a big step towards recreating mammoths\n                                    News 04 MAR 25\n\n                                    Reply to: Causal claims, causal assumptions and protected area impact\n                                    Matters Arising 26 FEB 25\n\n                                    Life-giving oxygen is wafting out of lakes worldwide\n                                    Research Highlight 25 MAR 25\n\n                                    How a scientist–pop industry partnership slashed a live gig’s carbon emissions by 98%\n                                    Career News 25 MAR 25\n\n                                    Climate change will send home insurance spiralling. Here’s how to control costs\n                                    World View 25 MAR 25\n\n            Jobs\n\n                        Department Chairs, School of Medicine at Southern University of Science and Technology (copy)\n\n                            Shenzhen, Guangdong, China\n                            Southern University of Science and Technology, School of Medicine\n\n                        Aggregate Report Senior Scientist\n\n                            Joining us as an Aggregate Report Senior Scientist, you will be a key member of the Safety Surveillance and Aggregate (SSA) Reports team\n                            Maidenhead, Berkshire\n                            Biogen\n\n                        Manager, Clinical Quality Management\n\n                            We are a global team with a commitment to excellence, and a pioneering spirit.\n                            Tokyo (JP)\n                            Biogen\n\n                        Executive Assistant to Head of Biogen's West Coast Hub\n\n                            The Executive Administrative Assistant will be a dynamic and self-motivated individual able to provide administrative support...\n                            South San Francisco, California\n                            Biogen\n\n                        Principal Associate Scientist, Gene Therapy\n\n                            Biogen are seeking a Principal Associate Scientist, Gene Therapy.\n                            Cambridge, Massachusetts (US)\n                            Biogen",
    "summary": {
      "en": "Microbes, such as bacteria and fungi, have the potential to play a significant role in addressing climate change and pollution. They can capture carbon, degrade plastics, and create sustainable products in economical and safe ways. \n\nDespite their potential, microbial technologies are often overlooked in global discussions about climate change. Researchers from various fields have come together to explore how these technologies can be utilized effectively. They found that microbes can replace fossil fuels in producing energy and materials, clean up pollution, and reduce reliance on chemical fertilizers, all while being environmentally friendly.\n\nFor example, companies are already using microbes to convert waste into aviation fuel and bioplastics. Others are developing microbes that can break down common plastics or clean up oil spills. Additionally, certain bacteria can produce nitrogen fertilizers more sustainably, minimizing greenhouse gas emissions.\n\nHowever, as these microbial solutions are scaled up, safety and environmental impacts must be carefully managed. This involves designing microbes that can safely degrade waste without causing harm to ecosystems. Collaborating across scientific and governmental sectors is essential to ensure these technologies are deployed effectively and safely.\n\nIn summary, harnessing the capabilities of microbes offers promising solutions for combating climate change and pollution, but careful consideration and management are necessary to maximize their benefits.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "e3f4dd00492cfe4a",
    "title": {
      "en": "Play the Virtual Organ from Arp Schnitger",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.orgelstadt-hamburg.de/play-arp/",
    "score": 124,
    "by": "ohjeez",
    "time": 1742754706,
    "content": "© Hauptkirche St. Jacobi Hamburg\n\n          ›play arp‹Virtuelle Orgel St. Jacobi\n          Hier haben Sie die weltweit einmalige Gelegenheit, eine der  international\n            wertvollsten und klangschönsten Barockorgeln selbst zum Klingen zu bringen.\n            Viele Orgelfans und Experten pilgern nach Hamburg, um die berühmte Arp-Schnitger\n            Orgel in der Hauptkirche St. Jacobi zu erleben. Wir bringen eine besondere\n            Auswahl klanglicher Perlen dieses Instrumentes exklusiv zu Ihnen. Spielen\n            Sie selbst!\n            (Für das klanglich beste Erlebnis empfehlen wir einen guten Kopfhörer zu nutzen)",
    "summary": {
      "en": "At Hauptkirche St. Jacobi in Hamburg, you can virtually play one of the world's most valuable and beautiful Baroque organs, the Arp-Schnitger Organ. Many organ enthusiasts visit to experience this famous instrument. Now, you can enjoy a selection of its unique sounds from home. For the best experience, it is recommended to use good quality headphones.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "26514204ccd4c95d",
    "title": {
      "en": "Samsung co-CEO Han Jong-hee has died",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.reuters.com/world/asia-pacific/samsung-electronics-says-co-ceo-han-jong-hee-has-died-cardiac-arrest-2025-03-25/",
    "score": 13,
    "by": "glimshe",
    "time": 1742887108,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "059ded92dec3320a",
    "title": {
      "en": "Project Aardvark: reimagining AI weather prediction",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.turing.ac.uk/blog/project-aardvark-reimagining-ai-weather-prediction",
    "score": 299,
    "by": "bentobean",
    "time": 1742772819,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "a984f46e7f77422e",
    "title": {
      "en": "Ask HN: Should I leave the company I co-founded?",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": null,
    "score": 42,
    "by": "throwaway03902",
    "time": 1742864412,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "2577d1b6b5eb6922",
    "title": {
      "en": "LangManus: An Open-Source Manus Agent with LangChain + LangGraph",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/langmanus/langmanus",
    "score": 125,
    "by": "gfortaine",
    "time": 1742554860,
    "content": "🦜🤖 LangManus\n\nEnglish | 简体中文 | 日本語\n\nCome From Open Source, Back to Open Source\n\nLangManus is a community-driven AI automation framework that builds upon the incredible work of the open source community. Our goal is to combine language models with specialized tools for tasks like web search, crawling, and Python code execution, while giving back to the community that made this possible.\nDemo\nTask: Calculate the influence index of DeepSeek R1 on HuggingFace. This index can be designed using a weighted sum of factors such as followers, downloads, and likes.\nLangManus's Fully Automated Plan and Solution:\n\nGather the latest information about \"DeepSeek R1\", \"HuggingFace\", and related topics through online searches.\nInteract with a Chromium instance to visit the HuggingFace official website, search for \"DeepSeek R1\" and retrieve the latest data, including followers, likes, downloads, and other relevant metrics.\nFind formulas for calculating model influence using search engines and web scraping.\nUse Python to compute the influence index of DeepSeek R1 based on the collected data.\nPresent a comprehensive report to the user.\n\nView on YouTube\n\nTable of Contents\n\nQuick Start\nProject Statement\nArchitecture\nFeatures\nWhy LangManus?\nSetup\n\nPrerequisites\nInstallation\nConfiguration\n\nUsage\nDocker\nWeb UI\nDevelopment\nFAQ\nContributing\nLicense\nAcknowledgments\n\nQuick Start\n# Clone the repository\ngit clone https://github.com/langmanus/langmanus.git\ncd langmanus\n\n# Install dependencies, uv will take care of the python interpreter and venv creation\nuv sync\n\n# Playwright install to use Chromium for browser-use by default\nuv run playwright install\n\n# Configure environment\n# Windows: copy .env.example .env\ncp .env.example .env\n# Edit .env with your API keys\n\n# Run the project\nuv run main.py\n\nProject Statement\nThis is an academically driven open-source project, developed by a group of former colleagues in our spare time. It aims to explore and exchange ideas in the fields of Multi-Agent and DeepResearch.\n\nPurpose: The primary purpose of this project is academic research, participation in the GAIA leaderboard, and the future publication of related papers.\nIndependence Statement: This project is entirely independent and unrelated to our primary job responsibilities. It does not represent the views or positions of our employers or any organizations.\nNo Association: This project has no association with Manus (whether it refers to a company, organization, or any other entity).\nClarification Statement: We have not promoted this project on any social media platforms. Any inaccurate reports related to this project are not aligned with its academic spirit.\nContribution Management: Issues and PRs will be addressed during our free time and may experience delays. We appreciate your understanding.\nDisclaimer: This project is open-sourced under the MIT License. Users assume all risks associated with its use. We disclaim any responsibility for any direct or indirect consequences arising from the use of this project.\n\n项目声明\n本项目是一个学术驱动的开源项目，由一群前同事在业余时间开发，旨在探索和交流 Multi-Agent 和 DeepResearch 相关领域的技术。\n\n项目目的：本项目的主要目的是学术研究、参与 GAIA 排行榜，并计划在未来发表相关论文。\n独立性声明：本项目完全独立，与我们的本职工作无关，不代表我们所在公司或任何组织的立场或观点。\n无关联声明：本项目与 Manus（无论是公司、组织还是其他实体）无任何关联。\n澄清声明：我们未在任何社交媒体平台上宣传过本项目，任何与本项目相关的不实报道均与本项目的学术精神无关。\n贡献管理：Issue 和 PR 将在我们空闲时间处理，可能存在延迟，敬请谅解。\n免责声明：本项目基于 MIT 协议开源，使用者需自行承担使用风险。我们对因使用本项目产生的任何直接或间接后果不承担责任。\n\nArchitecture\nLangManus implements a hierarchical multi-agent system where a supervisor coordinates specialized agents to accomplish complex tasks:\n\nThe system consists of the following agents working together:\n\nCoordinator - The entry point that handles initial interactions and routes tasks\nPlanner - Analyzes tasks and creates execution strategies\nSupervisor - Oversees and manages the execution of other agents\nResearcher - Gathers and analyzes information\nCoder - Handles code generation and modifications\nBrowser - Performs web browsing and information retrieval\nReporter - Generates reports and summaries of the workflow results\n\nFeatures\nCore Capabilities\n\n🤖 LLM Integration\n\nIt supports the integration of most models through litellm.\nSupport for open source models like Qwen\nOpenAI-compatible API interface\nMulti-tier LLM system for different task complexities\n\nTools and Integrations\n\n🔍 Search and Retrieval\n\nWeb search via Tavily API\nNeural search with Jina\nAdvanced content extraction\n\nDevelopment Features\n\n🐍 Python Integration\n\nBuilt-in Python REPL\nCode execution environment\nPackage management with uv\n\nWorkflow Management\n\n📊 Visualization and Control\n\nWorkflow graph visualization\nMulti-agent orchestration\nTask delegation and monitoring\n\nWhy LangManus?\nWe believe in the power of open source collaboration. This project wouldn't be possible without the amazing work of projects like:\n\nQwen for their open source LLMs\nTavily for search capabilities\nJina for crawl search technology\nBrowser-use for control browser\nAnd many other open source contributors\n\nWe're committed to giving back to the community and welcome contributions of all kinds - whether it's code, documentation, bug reports, or feature suggestions.\nSetup\nPrerequisites\n\nuv package manager\n\nInstallation\nLangManus leverages uv as its package manager to streamline dependency management.\nFollow the steps below to set up a virtual environment and install the necessary dependencies:\n# Step 1: Create and activate a virtual environment through uv\nuv python install 3.12\nuv venv --python 3.12\n\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Step 2: Install project dependencies\nuv sync\n\nBy completing these steps, you'll ensure your environment is properly configured and ready for development.\nConfiguration\nLangManus uses a three-layer LLM system, which are respectively used for reasoning, basic tasks, and vision-language tasks. Configuration is done using the conf.yaml file in the root directory of the project. You can copy conf.yaml.example to conf.yaml to start the configuration:\ncp conf.yaml.example conf.yaml\n\n# Setting it to true will read the conf.yaml configuration, and setting it to false will use the original .env configuration. The default is false (compatible with existing configurations)\nUSE_CONF: true\n\n# LLM Config\n## Follow the litellm configuration parameters: https://docs.litellm.ai/docs/providers. You can click on the specific provider document to view the completion parameter examples\nREASONING_MODEL:\n  model: \"volcengine/ep-xxxx\"\n  api_key: $REASONING_API_KEY # Supports referencing the environment variable ENV_KEY in the.env file through $ENV_KEY\n  api_base: $REASONING_BASE_URL\n\nBASIC_MODEL:\n  model: \"azure/gpt-4o-2024-08-06\"\n  api_base: $AZURE_API_BASE\n  api_version: $AZURE_API_VERSION\n  api_key: $AZURE_API_KEY\n\nVISION_MODEL:\n  model: \"azure/gpt-4o-2024-08-06\"\n  api_base: $AZURE_API_BASE\n  api_version: $AZURE_API_VERSION\n  api_key: $AZURE_API_KEY\n\nYou can create a .env file in the root directory of the project and configure the following environment variables. You can copy the.env.example file as a template to start:\ncp .env.example .env\n\n# Tool API Key\nTAVILY_API_KEY=your_tavily_api_key\nJINA_API_KEY=your_jina_api_key  # Optional\n\n# Browser Configuration\nCHROME_INSTANCE_PATH=/Applications/Google Chrome.app/Contents/MacOS/Google Chrome  # Optional, the path to the Chrome executable file\nCHROME_HEADLESS=False  # Optional, the default is False\nCHROME_PROXY_SERVER=http://127.0.0.1:10809  # Optional, the default is None\nCHROME_PROXY_USERNAME=  # Optional, the default is None\nCHROME_PROXY_PASSWORD=  # Optional, the default is None\n\nNote:\n\nThe system uses different models for different types of tasks:\n\nThe reasoning LLM is used for complex decision-making and analysis.\nThe basic LLM is used for simple text tasks.\nThe vision-language LLM is used for tasks involving image understanding.\n\nThe configuration of all LLMs can be customized independently.\nThe Jina API key is optional. Providing your own key can obtain a higher rate limit (you can obtain this key at jina.ai).\nThe default configuration for Tavily search is to return up to 5 results (you can obtain this key at app.tavily.com).\n\nConfigure Pre-commit Hook\nLangManus includes a pre-commit hook that runs linting and formatting checks before each commit. To set it up:\n\nMake the pre-commit script executable:\n\nchmod +x pre-commit\n\nInstall the pre-commit hook:\n\nln -s ../../pre-commit .git/hooks/pre-commit\n\nThe pre-commit hook will automatically:\n\nRun linting checks (make lint)\nRun code formatting (make format)\nAdd any reformatted files back to staging\nPrevent commits if there are any linting or formatting errors\n\nUsage\nBasic Execution\nTo run LangManus with default settings:\nuv run main.py\n\nAPI Server\nLangManus provides a FastAPI-based API server with streaming support:\n# Start the API server\nmake serve\n\n# Or run directly\nuv run server.py\n\nThe API server exposes the following endpoints:\n\nPOST /api/chat/stream: Chat endpoint for LangGraph invoke with streaming support\n\nRequest body:\n\n{\n  \"messages\": [{ \"role\": \"user\", \"content\": \"Your query here\" }],\n  \"debug\": false\n}\n\nReturns a Server-Sent Events (SSE) stream with the agent's responses\n\nAdvanced Configuration\nLangManus can be customized through various configuration files in the src/config directory:\n\nenv.py: Configure LLM models, API keys, and base URLs\ntools.py: Adjust tool-specific settings (e.g., Tavily search results limit)\nagents.py: Modify team composition and agent system prompts\n\nAgent Prompts System\nLangManus uses a sophisticated prompting system in the src/prompts directory to define agent behaviors and responsibilities:\nCore Agent Roles\n\nSupervisor (src/prompts/supervisor.md): Coordinates the team and delegates tasks by analyzing requests and determining which specialist should handle them. Makes decisions about task completion and workflow transitions.\n\nResearcher (src/prompts/researcher.md): Specializes in information gathering through web searches and data collection. Uses Tavily search and web crawling capabilities while avoiding mathematical computations or file operations.\n\nCoder (src/prompts/coder.md): Professional software engineer role focused on Python and bash scripting. Handles:\n\nPython code execution and analysis\nShell command execution\nTechnical problem-solving and implementation\n\nFile Manager (src/prompts/file_manager.md): Handles all file system operations with a focus on properly formatting and saving content in markdown format.\n\nBrowser (src/prompts/browser.md): Web interaction specialist that handles:\n\nWebsite navigation\nPage interaction (clicking, typing, scrolling)\nContent extraction from web pages\n\nPrompt System Architecture\nThe prompts system uses a template engine (src/prompts/template.py) that:\n\nLoads role-specific markdown templates\nHandles variable substitution (e.g., current time, team member information)\nFormats system prompts for each agent\n\nEach agent's prompt is defined in a separate markdown file, making it easy to modify behavior and responsibilities without changing the underlying code.\nDocker\nLangManus can be run in a Docker container. default serve api on port 8000.\nBefore run docker, you need to prepare environment variables in .env file.\ndocker build -t langmanus .\ndocker run --name langmanus -d --env-file .env -e CHROME_HEADLESS=True -p 8000:8000 langmanus\n\nYou can also just run the cli with docker.\ndocker build -t langmanus .\ndocker run --rm -it --env-file .env -e CHROME_HEADLESS=True langmanus uv run python main.py\n\nWeb UI\nLangManus provides a default web UI.\nPlease refer to the langmanus/langmanus-web-ui project for more details.\nDocker Compose (include both backend and frontend)\nLangManus provides a docker-compose setup to easily run both the backend and frontend together:\n# Start both backend and frontend\ndocker-compose up -d\n\n# The backend will be available at http://localhost:8000\n# The frontend will be available at http://localhost:3000, which could be accessed through web browser\n\nThis will:\n\nBuild and start the LangManus backend container\nBuild and start the LangManus web UI container\nConnect them using a shared network\n\n** Make sure you have your .env file prepared with the necessary API keys before starting the services. **\nDevelopment\nTesting\nRun the test suite:\n# Run all tests\nmake test\n\n# Run specific test file\npytest tests/integration/test_workflow.py\n\n# Run with coverage\nmake coverage\n\nCode Quality\n# Run linting\nmake lint\n\n# Format code\nmake format\n\nFAQ\nPlease refer to the FAQ.md for more details.\nContributing\nWe welcome contributions of all kinds! Whether you're fixing a typo, improving documentation, or adding a new feature, your help is appreciated. Please see our Contributing Guide for details on how to get started.\nLicense\nThis project is open source and available under the MIT License.\nStar History\n\nAcknowledgments\nSpecial thanks to all the open source projects and contributors that make LangManus possible. We stand on the shoulders of giants.\nIn particular, we want to express our deep appreciation for:\n\nLangChain for their exceptional framework that powers our LLM interactions and chains\nLangGraph for enabling our sophisticated multi-agent orchestration\nBrowser-use for control browser\n\nThese amazing projects form the foundation of LangManus and demonstrate the power of open source collaboration.",
    "summary": {
      "en": "**Summary of LangManus**\n\nLangManus is an open-source AI automation framework developed by a community of former colleagues. It aims to integrate language models with specialized tools for tasks such as web searches and Python code execution, while contributing back to the open-source community.\n\n**Key Features:**\n- **Automated Task Handling:** LangManus can calculate metrics like the influence index of models by gathering data from online searches and using Python for computation.\n- **Multi-Agent System:** It consists of different agents (Coordinator, Planner, Supervisor, Researcher, Coder, Browser, Reporter) that work together to manage complex tasks.\n- **Integration Options:** Supports various language models, web searches, and content extraction tools.\n- **User-Friendly Setup:** Users can quickly set up the framework with instructions for installation and configuration.\n\n**Usage:**\n- The project can be run locally or in a Docker container, with options for API access and web UI for interaction.\n- Built-in testing and code quality tools ensure good development practices.\n\n**Contributions and Licensing:**\nLangManus encourages contributions from the community and is licensed under the MIT License, allowing users to modify and distribute the software freely.\n\n**Acknowledgments:**\nThe project credits various open-source tools and contributors that have made its development possible, emphasizing the importance of collaboration in open-source software.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "a8de69e131cf93a9",
    "title": {
      "en": "Show HN: I built a website for sharing drum patterns",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "http://drumpatterns.onether.com",
    "score": 477,
    "by": "wesz",
    "time": 1742735121,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "08222b77479d8480",
    "title": {
      "en": "Palantir suggests 'common operating system' for UK govt data",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.theregister.com/2025/03/25/palantir_covid_inquiry_comments/",
    "score": 47,
    "by": "rntn",
    "time": 1742902335,
    "content": "Databases\n\n            44\n\nPalantir suggests 'common operating system' for UK govt data\n\n            44\n\n'Don't wait for another pandemic or civil challenge,' says US spy-tech biz\n\nLindsay Clark\n\nTue25Mar2025                                     //\n11:00UTC\n\nComment It might take a particularly shameless company to grasp the opportunity presented by the UK's coronavirus pandemic and step in with a sales pitch. US spy-tech biz Palantir is willing to give it a go.\nIn a witness statement to the UK COVID-19 Inquiry [PDF], an ongoing independent public inquiry into the nation's response to the pandemic (in which around 208,000 people died), Louis Mosley, executive veep of Palantir Technologies UK, said the government should invest in a \"common operating system\" for its data, encompassing departments such as the Department for Work and Pensions and local authorities.\nThe government should \"deploy this common operating system capability immediately and not wait until the next pandemic or civil challenge on the scale of COVID-19 is already underway. An investment of this kind is already long overdue,\" he added.\n\nHealth crusaders prep legal challenge over NHS mega contract with Palantir\nREAD MORE\nPalantir was founded by Peter Thiel, who made his money and name by co-founding PayPal. It attracted early investment from the US Central Intelligence Agency's venture capital arm, In-Q-Tel, and won early contracts in US defense and intelligence applications of its data analytics technologies, along the way supporting US Immigration and Customs Enforcement, an agency that has been accused of separating children from their families during enforcement actions.\nHealthcare quango NHS England first started working with the US data analytics corporation in March 2020, during the height of the pandemic.\n\n            <a href=\"https://pubads.g.doubleclick.net/gampad/jump?co=1&amp;iu=/6978/reg_software/databases&amp;sz=300x50%7C300x100%7C300x250%7C300x251%7C300x252%7C300x600%7C300x601&amp;tile=2&amp;c=2Z-QWMHqan3JKg8r_4zyXMwAAAVM&amp;t=ct%3Dns%26unitnum%3D2%26raptor%3Dcondor%26pos%3Dtop%26test%3D0\" target=\"_blank\">\n                <img src=\"https://pubads.g.doubleclick.net/gampad/ad?co=1&amp;iu=/6978/reg_software/databases&amp;sz=300x50%7C300x100%7C300x250%7C300x251%7C300x252%7C300x600%7C300x601&amp;tile=2&amp;c=2Z-QWMHqan3JKg8r_4zyXMwAAAVM&amp;t=ct%3Dns%26unitnum%3D2%26raptor%3Dcondor%26pos%3Dtop%26test%3D0\" alt=\"\">\n            </a>\n\nPalantir was initially handed a nominal £1 contract to work on a COVID-19 data store, along with cloud providers AWS, Google, and Microsoft Azure, and Faculty, a UK AI company. Without open competition, its contract was expanded to a £1 million ($1.29 million) agreement, then a £23 million ($29.7 million) arrangement was signed in December 2020.\n\nPalantir designed to 'power the West to its obvious innate superiority,' says CEO\n\nWe can't make this stuff up: Palantir, Anduril form fellowship for AI adventures\n\nKey aspects of Palantir's Federated Data Platform lack legal basis, lawyers tell NHS England\n\nPrior UK government planned £485M four-year budget for Palantir-based healthcare system\n\nThat deal was subject to the threat of judicial review from campaigners, who argued the contract represented such a change in data usage it warranted public consultation under British data protection law.\nNHS England then extended the contract by six months for £11.5 million ($14.8 million), and awarded a £24.9 million ($32.1 million) deal to cover the one-year transition to a new Federated Data Platform, the £330 million ($426 million) project which Palantir won following an open competition.\n\nIn his submission, Mosley was keen to underscore Palantir's involvement in the government's response to the COVID emergency, including work to distribute ventilators, vaccines, and personal protective equipment.\nAmong the benefits of introducing the imagined data \"operating system\" across government would be the ability to \"optimize procurement, among other things.\"\n\n                <a href=\"https://pubads.g.doubleclick.net/gampad/jump?co=1&amp;iu=/6978/reg_software/databases&amp;sz=300x50%7C300x100%7C300x250%7C300x251%7C300x252%7C300x600%7C300x601&amp;tile=4&amp;c=44Z-QWMHqan3JKg8r_4zyXMwAAAVM&amp;t=ct%3Dns%26unitnum%3D4%26raptor%3Dfalcon%26pos%3Dmid%26test%3D0\" target=\"_blank\">\n                    <img src=\"https://pubads.g.doubleclick.net/gampad/ad?co=1&amp;iu=/6978/reg_software/databases&amp;sz=300x50%7C300x100%7C300x250%7C300x251%7C300x252%7C300x600%7C300x601&amp;tile=4&amp;c=44Z-QWMHqan3JKg8r_4zyXMwAAAVM&amp;t=ct%3Dns%26unitnum%3D426raptor%3Dfalcon%26pos%3Dmid%26test%3D0\" alt=\"\">\n                </a>\n\nMaybe the company should be careful what it wishes for, though. The UK procurement for data analytics software seemed to be pretty well optimized toward Palantir winning contracts at the time of the COVID outbreak. ®\n                                    <strong>Get our</strong> <a href=\"https://whitepapers.theregister.com/\" style=\"text-transform:uppercase\">Tech Resources</a>\n\n        Share\n\n            More about\n\n                Government of the United Kingdom\n\n                Palantir\n\n        More like these\n\n            ×\n\n                    More about\n\n                Government of the United Kingdom\n\n                Palantir\n\n                    Narrower topics\n\n                Cabinet Office\n\n                Competition and Markets Authority\n\n                Computer Misuse Act\n\n                DCMS\n\n                GCHQ\n\n                HMRC\n\n                Home Office\n\n                ICO\n\n                NCSC\n\n                NHS\n\n                RPA\n\n                    Broader topics\n\n                Government\n\n                United Kingdom\n\n                    More about\n\n        Share\n\n                44\n\n                    COMMENTS\n\n            More about\n\n                Government of the United Kingdom\n\n                Palantir\n\n        More like these\n\n            ×\n\n                    More about\n\n                Government of the United Kingdom\n\n                Palantir\n\n                    Narrower topics\n\n                Cabinet Office\n\n                Competition and Markets Authority\n\n                Computer Misuse Act\n\n                DCMS\n\n                GCHQ\n\n                HMRC\n\n                Home Office\n\n                ICO\n\n                NCSC\n\n                NHS\n\n                RPA\n\n                    Broader topics\n\n                Government\n\n                United Kingdom\n\n        TIP US OFF\n        Send us news",
    "summary": {
      "en": "Palantir Technologies, a US data analytics company, is urging the UK government to create a \"common operating system\" for managing data across various governmental departments. In a statement for the ongoing COVID-19 Inquiry, Palantir's executive, Louis Mosley, emphasized the need for this system to be implemented quickly, arguing that it would help the government better prepare for future emergencies.\n\nPalantir has been involved with the UK's NHS since March 2020 and has received several contracts, initially starting with a small £1 agreement that expanded to a £23 million contract by December 2020. However, this has faced legal challenges from campaigners who believe it requires public consultation under data protection laws.\n\nMosley highlighted Palantir's role in responding to the pandemic, including logistics for distributing medical supplies. He believes a unified data system could improve government efficiency, particularly in procurement. The UK government's previous contracting process appeared to favor Palantir during the pandemic.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "7f1f99c13740279a",
    "title": {
      "en": "Cottagecore Programmers",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://tjmorley.com/blogposts/cottagecoreprogrammers.html",
    "score": 108,
    "by": "morleytj",
    "time": 1742846905,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "4e1d365d65298255",
    "title": {
      "en": "BeeFormer: CF and CBF hybrid approach for recommendation systems",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/recombee/beeformer",
    "score": 42,
    "by": "klaussilveira",
    "time": 1742732424,
    "content": "beeFormer\nThis is the official implementation provided with our paper beeFormer: Bridging the Gap Between Semantic and Interaction Similarity in Recommender Systems.\nmain idea of beeFormer\n\nCollaborative filtering (CF) methods can capture patterns from interaction data that are not obvious at first sight. For example, when buying a printer, users can also buy toners, papers, or cables to connect the printer, and collaborative filtering can take such patterns into account. However, in the cold-start recommendation setup, where new items do not have any interaction at all, collaborative filtering methods cannot be used, and recommender systems are forced to use other approaches, like content-based filtering (CBF). The problem with content-based filtering is that it relies on item attributes, such as text descriptions. In our printer example, semantic similarity-trained language models will put other printers closer than accessories that users might be searching for. Our method is training language models to learn these user behavior patterns from interaction data to transfer that knowledge to previously unseen items. Our experiments show that performance benefits from this approach are enormous.\nSteps to start training the models:\n\ncreate virtual environment python3.10 -m venv beef and activate it source beef/bin/activate\nclone this repository and navigate to it cd beeformer\ninstall packages pip install -r requirements.txt\ndownload the data for movielens: navigate to the _dataset/ml20m folder and run source download_data\ndownload the data for goodbooks: navigate to the _dataset/goodbooks folder and run source download_data\ndownload the data for amazonbooks: navigate to the _dataset/amazonbooks folder and run source download_data && python preprocess.py\nin the root folder of the project run the train.py, for example like this:\n\npython train.py --seed 42 --scheduler None --lr 1e-5 --epochs 5 --dataset goodbooks --sbert \"sentence-transformers/all-mpnet-base-v2\" --max_seq_length 384 --batch_size 1024 --max_output 10000 --sbert_batch_size 200 --use_cold_start true --save_every_epoch true --model_name my_model\n\nEvaluate the results. To reproduce numbers from the paper using our hugginface repository, run for example:\n\npython evaluate_itemsplit.py --seed 42 --dataset goodbooks --sbert beeformer/Llama-goodbooks-mpnet\n\nor\npython evaluate_timesplit.py --seed 42 --dataset amazon-books --sbert beeformer/Llama-amazbooks-mpnet\n\nDatasets and preprocessing\nPreprocessing information\nWe consider ratings of 4.0 and higher as an interaction. We only keep the users with at least 5 interactions.\nLLM Data augmentations\nSince there are no text descriptions in the original data, we manually connect several datasets with the original data and train our models on it. However, this approach has several limitations: texts from different sources have different styles and different lengths, and this might influence the results. Therefore, we use the Llama-3.1-8b-instruct model to generate item descriptions for us. We use the following conversation template:\nimport pandas as pd\n\nfrom tqdm import tqdm\nfrom vllm import LLM, SamplingParams\n\nitems = pd.read_feather(\"items_with_gathered_side_info.feather\")\n\nllm = LLM(model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",dtype=\"float16\")\n\ntokenizer = llm.get_tokenizer()\nconversation = [ tokenizer.apply_chat_template(\n        [\n            {'role': 'system','content':\"You are ecomerce shop designer. Given a item description create one paragraph long summarization of the product.\"},\n            {'role': 'user', 'content': \"Item description: \"+x},\n            {'role': 'assistant', 'content': \"Sure, here is your one paragraph summary of your product:\"},\n        ],\n        tokenize=False,\n    ) for x in tqdm(items.gathered_features.to_list())]\n\noutput = llm.generate(\n    conversation,\n    SamplingParams(\n        temperature=0.1,\n        top_p=0.9,\n        max_tokens=512,\n        stop_token_ids=[tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")],\n    )\n)\n\nitems_descriptions = [o.outputs[0].text for o in output]\n\nHowever, LLM refused to generate descriptions for some items (For example, because it refuses to generate explicit content). We removed such items from the dataset. We also removed items for which we were not able to connect meaningful descriptions from other datasets, which led to LLM completely hallucinating item descriptions.\nWe share the resulting LLM-generated item descriptions in datasets/ml20m, dataset/goodbooks and datasets/amazonbooks folders.\nStatistics of datasets used for evaluation\n\nGoodBooks-10k\nMovieLens-20M\nAmazon Books\n\n# of items in X\n9975\n16902\n63305\n\n# of users in X\n53365\n136589\n634964\n\n# of interactions in X\n4119623\n9694668\n8290500\n\ndensity of X [%]\n0.7739\n0.4199\n0.0206\n\ndensity of X^TX [%]\n41.22\n26.93\n7.59\n\nPretrained models\nWe share pretrained models at https://huggingface.co/beeformer.\nHyperparameters\nWe used hyperparameters for training our models as follows.\n\nhyperparameter\ndescription\nbeeformer/Llama-goodbooks-mpnet\nbeeformer/Llama-movielens-mpnet\nbeeformer/Llama-goodlens-mpnet\nbeeformer/Llama-amazbooks-mpnet\n\nseed\nrandom seed used during training\n42\n42\n42\n42\n\nscheduler\nlearning rate scheduling strategy\nconstant learning rate\nconstant learning rate\nconstant learning rate\nconstant learning rate\n\nlr\nlearning rate\n1e-5\n1e-5\n1e-5\n1e-5\n\nepochs\nnumber of trained epochs\n5\n5\n10\n5\n\ndevices\ntraining script allow to train on multiple gpus in parallel - we used 4xV100\n[0,1,2,3]\n[0,1,2,3]\n[0,1,2,3]\n[0,1,2,3]\n\ndataset\ndataset used for training\ngoodbooks\nml20m\ngoodlens\namazon-books\n\nsbert\noriginal sentence transformer model used as an initial model for training\nsentence-transformers/all-mpnet-base-v2\nsentence-transformers/all-mpnet-base-v2\nsentence-transformers/all-mpnet-base-v2\nsentence-transformers/all-mpnet-base-v2\n\nmax_seq_length\nlimitation of sequence length; shorter sequences trains faster original mpnet model uses max 512 tokens in. sequence\n384\n384\n384\n384\n\nbatch_size\nnumber of users sampled in random batch from interaction matrix\n1024\n1024\n1024\n1024\n\nmax_output\nnegative sampling hyperparameter (m in the paper). Negatives are sampled uniformly at random.\n10000\n10000\n10000\n12500\n\nsbert_batch_size\nnumber of items processed together during training step (gradient accumulation step size)\n200\n200\n200\n200\n\nuse_cold_start\nsplit the dataset item-wise (some items are hidden to test the genralization towards new items)\ntrue\ntrue\ntrue\nfalse\n\nuse_time_split\nsort interactions by timestamp and use last 20% of interactions as a test set (generalization from the past to the future)\nfalse\nfalse\nfalse\ntrue\n\nRecSys 2024 poster\n\nCitation\nIf you find this repository helpful, feel free to cite our paper:\n@inproceedings{10.1145/3640457.3691707,\n        author = {Van\\v{c}ura, Vojt\\v{e}ch and Kord\\'{\\i}k, Pavel and Straka, Milan},\n        title = {beeFormer: Bridging the Gap Between Semantic and Interaction Similarity in Recommender Systems},\n        year = {2024},\n        isbn = {9798400705052},\n        publisher = {Association for Computing Machinery},\n        address = {New York, NY, USA},\n        url = {https://doi.org/10.1145/3640457.3691707},\n        doi = {10.1145/3640457.3691707},\n        booktitle = {Proceedings of the 18th ACM Conference on Recommender Systems},\n        pages = {1102–1107},\n        numpages = {6},\n        keywords = {Cold-start recommendation, Recommender systems, Sentence embeddings, Text mining, Zero-shot recommendation},\n        location = {Bari, Italy},\n        series = {RecSys '24}\n}",
    "summary": {
      "en": "**Summary of beeFormer Implementation**\n\nbeeFormer is a method designed to improve recommendation systems by combining semantic and interaction similarities. Traditional collaborative filtering (CF) works well with known data but struggles with new items that lack interactions (cold-start). In these cases, content-based filtering (CBF) is often used, which can misclassify related items, like accessories, as less relevant than similar items.\n\nbeeFormer addresses this by training language models to understand user behavior patterns from existing interaction data. This allows the system to make better recommendations for new items.\n\n**Getting Started with beeFormer:**\n1. Set up a Python virtual environment.\n2. Clone the repository and install necessary packages.\n3. Download datasets (MovieLens, GoodBooks, Amazon Books) and preprocess them.\n4. Train the model using specified parameters.\n\n**Model Evaluation:**\nTo evaluate the model, you can run specific scripts that reproduce results from the paper.\n\n**Data Preparation:**\n- Ratings of 4.0 and above are considered interactions.\n- Some items may be removed if descriptions cannot be generated due to content restrictions.\n\n**Dataset Statistics:**\n- GoodBooks-10k: 9,975 items, 53,365 users, 4,119,623 interactions.\n- MovieLens-20M: 16,902 items, 136,589 users, 9,694,668 interactions.\n- Amazon Books: 63,305 items, 634,964 users, 8,290,500 interactions.\n\n**Pretrained Models and Hyperparameters:**\nPretrained models are available on Hugging Face. The training parameters include learning rate, batch size, number of epochs, and others, ensuring consistent model performance across datasets.\n\n**Citation:**\nIf you find this work useful, you can cite the associated paper published at the ACM Conference on Recommender Systems 2024.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "caba56a5a2afb958",
    "title": {
      "en": "The Software Engineering Identity Crisis",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://annievella.com/posts/the-software-engineering-identity-crisis/",
    "score": 131,
    "by": "napolux",
    "time": 1742755045,
    "content": "The Software Engineering Identity Crisis\n\n        March 21, 2025\n\n      Many of us became software engineers because we found our identity in building things. Not managing things. Not overseeing things. Building things. With our own hands, our own minds, our own code.\nBut that identity is being challenged.\nAI coding assistants aren’t just changing how we write software - they’re fundamentally transforming who we are. We’re shifting from creators to orchestrators, from builders to overseers. From engineers to something that looks suspiciously like… managers.\nThe irony cuts deep: for years, we’ve said that software engineering transcends mere coding. Requirements, design, testing, operations - these were all supposedly part of our craft. Yet the industry pushed us in the opposite direction. We handed these responsibilities to specialists - Product Owners, Architects, Quality Engineers, Platform Engineers - while we doubled down on our coding expertise. We became masters of our code, proud wielders of a modern magic.\nAnd now, just as we’ve perfected this craft, AI is threatening to take it away from us.\nThe Joy We’re About to Lose\nLet’s be honest about what’s at stake here. Many of us don’t just write code - we love writing code. Our identity is woven into every elegant solution we craft, every test we make pass, every problem we solve through pure logic and creativity. It’s not just work, not just a craft - it’s who we are.\nThink about those moments of deep satisfaction: when you finally track down that elusive bug that’s been haunting production, when you work out how to optimise that slow algorithm and watch response times drop from seconds to milliseconds, when you transform a maze of legacy code into something clean and maintainable. These aren’t just achievements - they’re expressions of who we are as engineers. They’re the moments that remind us why we chose this path.\nNow imagine AI taking over these moments of craftsmanship. The creators of these tools paint an optimistic picture - they say we’ll spend more time on defining intent, high-level architecture, and systems thinking. But listen carefully to what they’re really saying: we’ll become overseers rather than creators, managers rather than builders.\n\n      Are software engineers evolving into system thinkers and high-level architects?\n\nThis shift raises important questions about our identity as builders: Is oversight what drives us? Is it what makes us leap out of bed in the morning, eager to solve the next puzzle?\nThe Identity Shift: It’s Already Here\nThis isn’t some theoretical future we’re debating - it’s the reality we’re living right now. When Google recently revealed that AI generates more than a quarter of their new code, it was just the beginning. Y Combinator CEO Garry Tan reveals that for about a quarter of their startups, 95% of their code is now written by AI - marking a truly significant shift. My own Master’s research revealed a similar picture: 77% of us are spending less time writing code, and almost half believe our core skill might become secondary to prompt engineering. Think about that transition: from crafting solutions to crafting prompts.\n\n      Will Prompt Engineering Replace Traditional Coding Skills?\n\nWhen asked how to develop prompt engineering skills, software engineers emphasise refining communication skills. Getting an AI to do what you want means being able to articulate things well - providing just the right amount of context along with a clear description of the task. The better you can communicate with Gen AI, the more likely the output will match your expectations. Some even suggest being polite to these tools, treating them like team members - as if you’re leading another human being to do something for you.\nThe changes run so deep that we’re inventing new terms just to describe who we’re becoming. Take vibe coding, a term that Andrej Karpathy recently coined in a tweet. It captures a profound shift in how we write software.\nAt one end of the spectrum lies the traditional way - the craftsperson’s way. We write each line with intent, every function name and architectural decision reflecting our deep understanding of the system.\nAt the other end? We let AI fill in the blanks, “vibing” with its suggestions. We focus on the what, not the how. As Karpathy puts it: “fully give in to the vibes, embrace exponentials, and forget that the code even exists.”\nThat last part gives us pause - are we still engineers if we forget all about the code?\nIn a recent pairing session, engineering thought leaders Gene Kim and Steve Yegge demonstrated what this looks like in practice. Using an AI coding assistant, they ported a 3,500-line legacy Ruby script to Kotlin - a task that would normally take a week - in just one hour. The AI didn’t just translate the code; it improved it, adding the modular architecture and unit tests they’d wanted for years but couldn’t justify the time for.\nEven Patrick Debois, the godfather of DevOps, sees this transformation reshaping our identity. In his recent analysis of AI Native Development patterns, he outlines four fundamental shifts in how we work:\n\n      Patrick Debois: The 4 Patterns of AI Native Dev\n\nThese patterns reveal a profound shift: we’re moving from producers to managers of AI systems, from detailed implementation to expressing intent, from delivery to discovery through rapid experimentation, and from content creation to knowledge curation. Our role is evolving to blend creation with orchestration, building with oversight.\nOverall, I think it’s fair to say that the nature of our professional identity is changing at its core.\nThe Craft That Shaped Our Identity\nTo understand this identity crisis, we need to look at how deeply the craft of coding has shaped who we are. At its core, writing code is about mastery and control - skills we’ve spent years perfecting. Modern programming languages are much higher-level than those of days gone by, but they still require deep technical understanding. Few developers today deal with the nitty-gritty of pointers and memory management, yet we still take pride in knowing how things work under the hood. Even as frameworks do more heavy lifting, we’ve maintained our identity as artisans who understand our tools intimately.\nProgramming today is much more about stitching together APIs, frameworks, and libraries in creative ways to build something meaningful. In fact, recent research at Google suggests that creativity in software engineering centres on the concept of clever reuse over pure novelty. This makes sense to me - I’ve often commented that we’re all just ‘integration’ engineers nowadays, really.\nStill, there’s a strange pride that we take in knowing all the weird syntax required to build something. It’s like a secret language that only we understand. Becoming proficient with a programming language gives you a lot of control to make it do precisely what you want it to do. It’s so detailed - just one wrong character will break the entire thing and it can take a lot of time and patience to make it do what you want.\n First, one must perform perfectly. The computer resembles the magic of legend in this respect, too. If one character, one pause, of the incantation is not strictly in proper form, the magic doesn’t work.\n— Frederick P. Brooks, The Mythical Man-Month, Chapter 1, Addison-Wesley, 1975\nThe other 99% of the population thinks we’re magicians for understanding code and indeed, it can take years of deliberate practice to master it. Those who master more than one programming language have the privilege of being referred to as polyglots. Many of us take great pride in writing clean, elegant code. We passionately argue over different styles and best practices, often taking it all too seriously.\nA Reluctant Manager’s Tale\nLet me share a story about identity evolution that might resonate.\nAfter a decade of being an individual contributor, I hit the infamous ceiling of the technical track. Senior Lead Software Engineer - that was as far as the technical track went. Staff+ Engineering wasn’t a thing yet, and the sole Architecture position at the company I was working for was filled. I faced a choice that would change who I was: stay a builder or become an overseer.\nI chose management. Reluctantly. That’s where the path led me. I told myself it was still engineering, just at a different level. That managing systems wasn’t so different from managing people. That I could still keep my hands in the code in between other tasks.\nSound familiar? The parallels are uncanny. Just as I had to trade direct problem-solving for meetings and documentation, we’re now being asked to trade coding for prompt engineering. The skills that defined us as engineers - mastering syntax, architecting our code elegantly, catching and handling the edge cases, debugging complex issues - are being relegated to AI. Instead, we’re told to focus on skills that sound suspiciously like management: clear communication, systems thinking, problem definition.\nBut here’s what no one’s talking about: the identity crisis. That deep sense of loss when you realise you’re no longer building things with your own hands. When your technical mastery becomes less relevant than your ability to “manage” the tools. When your craft becomes oversight.\nCan orchestrating AI ever give us that same sense of identity? Of being a builder, a creator, a problem solver?\nWhen Machines Challenge Our Identity\nBy now the source of our identity crisis becomes clear. The craft we spent years perfecting - the one that gave us purpose, meaning, and pride - is now being done faster, cheaper, and at scale by a machine. Sure, the quality isn’t as good as your hand-written code (yet). But the speed at which code can now be written is staggering and businesses are tripping over themselves to get in on the action.\nThis is where a glimmer of hope emerges. Remember that irony - how we gave away the broader aspects of our craft to specialists? AI is pushing us to reclaim what we once knew: that software engineering transcends mere coding. That core truth remains - ultimately, software engineering is about solving problems, creating solutions, building things that matter.\nThese broader skills - what Addy Osmani calls “durable engineering skills” in his article on the human 30% of AI-assisted coding - have always separated great engineers from good ones. Communication, big-picture thinking, handling ambiguity - these become even more crucial in an AI-driven world.\nYet this emphasis on broader skills has sparked debate in our community. For some, it sounds suspiciously like management repackaged. And they’re not entirely wrong - a recent CIO article confirms that development teams are already being restructured to focus on oversight rather than creation. The article envisions future teams consisting of a product manager, a UX designer, and a software architect who primarily uses AI to generate prototypes. These architects, or senior developers, must “understand the content… who the customer is and what we’re trying to achieve” - classic management responsibilities repackaged as technical work.\n\n      Management in a Cloak\n\nThis evolution raises fundamental questions about our identity as engineers: As the traditional career ladder transforms, how will the next generation of software engineers develop their skills? How do we preserve the deep technical understanding and craftsmanship that shaped our profession while embracing these new tools? And perhaps most unsettling - as AI capabilities advance exponentially, will our role as craftspeople become as obsolete as the manual weaver’s did during the Industrial Revolution?\nThe Path Forward\nPerhaps the answer lies not in resisting this transformation, but in understanding it through the lens of history. These identity crises - these fundamental shifts in how we define ourselves through our work - aren’t new. They’re part of a pattern that repeats whenever technology reshapes a profession.\nDuring the Industrial Revolution, craftspeople faced a similar crisis. Their traditional skills - honed over generations - were being replaced by machines. But what happened next is fascinating: many adapted, becoming specialist professionals who could fix and improve these same machines that threatened to replace them. Others found ways to apply their deep understanding of materials and processes to improve overall factory operations.\nIf we draw this parallel to our AI era, a similar path emerges. The core of software engineering - solving problems and creating value - remains unchanged. Our tools are evolving, and with them, the skills needed to wield them effectively.\nThe question isn’t whether we’ll become managers of machines - it’s whether we can find the same satisfaction in this evolution of our craft.\nThe Engineer’s Dilemma\nSo where does this leave us? Are we all destined to become overseers of AI agents rather than writers of code? Is this a future to resist or embrace?\nThe truth, as always, is nuanced. Just as some engineers naturally gravitate toward management while others prefer to remain hands-on, we’ll likely see a similar spectrum emerge in how we interact with AI. Some will excel at orchestrating AI systems, focusing on high-level design and making these systems more efficient and reliable - conducting a technical symphony rather than performing solo. Others will find their calling in domains where human expertise remains crucial - perhaps in security-sensitive applications, novel areas where AI lacks training data, or systems where performance and reliability are paramount. The key isn’t to resist this evolution, but to find our place within it.\nWhat’s clear is that the definition of “software engineer” is expanding, not contracting. The skills that make someone valuable are diversifying. And this creates both challenges and opportunities.\nFor those who love the craft of coding, this shift can feel threatening. But remember that AI tools are still just that - tools. They don’t understand the “why” behind the code, the business context, or the human needs being served. They can’t innovate in the true sense of the word, at least not yet. And as far as we know, they can’t feel the satisfaction of solving a complex problem or the joy of creating something new.\nPerhaps the most valuable skill in this new landscape isn’t prompt engineering or systems architecture, but adaptability - the willingness to evolve, to learn new skills, and to find your unique place in a rapidly changing field.\nThe Bright Side\nDespite these challenges, there’s something important we need to acknowledge: these AI tools can be incredibly empowering. With agentic IDEs like Windsurf and Cursor taking software development to a whole new level, it’s like having a supportive pair-programming partner who’s always there, ready to help you tackle problems that might have seemed daunting before.\nFor junior developers or those of us who might feel a bit rusty, AI assistants can be a confidence booster - helping you get started when you’re staring at a blank file, validating your approach when you’re unsure, or explaining complex concepts in a way that makes sense to you. For experienced developers, they’re like having a tireless assistant who can handle the routine tasks while you focus on the more challenging aspects of the problem.\nThe speed at which we can now prototype ideas, explore different approaches, and learn new technologies is truly remarkable. What might have taken weeks of research and trial-and-error can often be accomplished in hours or even minutes. It’s like having a superpower - the ability to amplify our capabilities and turn our ideas into reality faster than ever before.\nThe Reality Check\nBut with great power comes great responsibility. A recent comprehensive GitClear study analysing 211 million lines of code revealed some concerning trends as AI code generation tools became more prevalent:\n\nA 17.1% increase in copy-pasted code, marking the first time AI-assisted code duplication surpassed refactored (moved) code.\nAn 8-fold rise in duplicated code blocks, with 6.66% of commits now containing repeated code sections.\nA 26% increase in code churn, with 5.7% of all code changes being revised or deleted within two weeks.\n\n      GitClear: Trends in Code Changes\n\nWhile we’re producing code faster than ever, we’re also spending more time fixing AI-generated mistakes and dealing with code that’s harder to maintain. This isn’t just about speed - it’s about the craft of writing sustainable, maintainable software.\nThe Hidden Identity Crisis\nYet beneath these surface-level changes lies a deeper challenge - one that strikes at the heart of who we are as engineers. The emerging field of Human-AI teaming is revealing uncomfortable truths about our future. A 2024 study shows that when humans and AI work together, the results often fall short of expectations. Not because AI lacks capability, but because trust works differently with machines than with humans.\nWe don’t build trust with AI the way we do with human teammates.\nWith humans, trust grows gradually, through shared success. Each problem solved together strengthens the bond. Even failures can deepen trust when handled well. With AI, trust often starts high and erodes fast.\nEvery incorrect response, every hallucinated bug fix, every misplaced confidence weakens our faith in the machine. Unlike human relationships where trust typically grows over time, AI trust often peaks early and declines.\nAnd when trust erodes, so does productivity.\nThe research reveals why:\n\nThere’s an inherent unpredictability in how AI interprets our intentions\nIt lacks the contextual awareness that makes human collaboration fluid\nIts decisions often lack transparency, making it hard to rebuild trust once lost\n\nThese challenges mirror what many of us experience when transitioning into technical leadership. Just as new engineering managers must learn to trust their team’s work without doing it themselves, we now face a similar transition with AI - learning to guide and verify rather than write every line ourselves.\nThe reality is stark: despite AI’s raw capabilities, teams often perform worse with AI than without it. Just as a team’s productivity suffers under ineffective leadership, our effectiveness diminishes when we don’t understand how to work with our AI tools.\nReclaiming Your Identity\nDrawing from both my journey as a reluctant manager and my research into this AI transformation, I see three ways we might preserve our identity as builders:\n\nResist - Some will choose to focus on domains where human creativity and deep technical expertise remain essential\nAdapt - Others will embrace AI orchestration, becoming conductors of a new kind of technical symphony\nBalance - And many, like myself, will seek a middle path - using AI for routine tasks while preserving the joy of direct problem-solving\n\nThen I had a realisation that changed my perspective: We don’t have to choose just one path.\nThe Identity Pendulum\nPerhaps the answer to our identity crisis lies in the Engineer/Manager pendulum. My own journey between these roles taught me something crucial about identity:\n\nManagement didn’t replace my engineering identity - it expanded it\nReturns to hands-on work weren’t steps backward - they were identity renewals\nThe pendulum swing itself became part of who I am - adaptable, growing, evolving\n\nAnd that’s when it hit me: This is exactly the model we need for the AI age.\nInstead of being forced into permanent “AI managers,” what if we could swing between:\n\nDeep technical work where we write and refine code directly\nStrategic orchestration where we guide AI systems\nCreative problem-solving that combines both approaches\n\nThis balanced approach resonates deeply with what I’m hearing from fellow engineers. My research shows a clear message: maintaining strong engineering fundamentals is more crucial than ever. We need deep technical knowledge to effectively review, verify, and adjust AI-generated code - because it’s often not quite right. When asked about their concerns with AI coding assistants, software engineers ranked code quality and security well above job security.\n\n      Top Concerns About AI Coding Assistants Among Software Engineers\n\nThat tells me something profound: we see ourselves as guardians of engineering excellence, ensuring that AI-generated solutions adhere to solid software engineering principles. We’re not looking to delegate our expertise to AI - we’re evolving to apply our craft in new ways.\nYour Move\nAs we navigate this transformation, a fundamental truth emerges: our identity crisis isn’t really about AI at all. The research into Human-AI teaming, the parallels with management transitions, the pendulum of roles - they all point to something deeper. Beyond choosing between builder or overseer lies the heart of who we are: creators.\nAnd now we come full circle: AI isn’t taking our jobs; it’s giving us a chance to reclaim those broader aspects of our role that we gave away to specialists. To return to a time when software engineering meant more than just writing code. When it meant understanding the whole problem space, from user needs to business impact, from system design to operational excellence.\nThe pendulum metaphor offers us wisdom here. Just as many of us have swung between engineering and management roles, we can embrace a similar fluidity with AI. Some periods we’ll dive deep into the code, experiencing that thrill of crafting elegant solutions. Other times we’ll step back to guide AI systems - not as overseers, but as master builders who understand every part of their craft. Like the Industrial Revolution’s workers who became experts at optimising the machines that transformed their craft, we can master these AI systems - making them instruments of our creativity, not replacements for it.\nIn the AI era, what matters most is preserving the essence of who we are: that pure joy of building things, of solving hard problems, of making something work exactly right. Our engineering excellence runs deeper than just verifying AI’s work - it flows from knowing systems so intimately that we can shape them, improve them, transform them.\nThe choice isn’t whether AI will change our industry - it already is. The real choice is how we evolve with it. Will we cling to an outdated sense of what it means to be an engineer? Or will we reclaim our craft, not as mere coders, but as master builders of AI-augmented systems?\nThe pendulum is swinging - will you hold your ground, or move with it?",
    "summary": {
      "en": "### Summary of \"The Software Engineering Identity Crisis\"\n\nMany software engineers identify strongly with the act of building software, but this identity is under threat due to the rise of AI coding assistants. These tools are changing the role of engineers from creators to overseers, which raises concerns about losing the joy of coding and problem-solving that many engineers cherish.\n\nAs AI takes on more coding tasks, engineers are spending less time writing code. Surveys show that many feel their core skills may become less relevant, with a shift towards \"prompt engineering,\" where effective communication with AI tools becomes crucial. This change has led to new terms like \"vibe coding,\" indicating a shift away from traditional craftsmanship in coding.\n\nThe evolution of engineering roles means that engineers might transition from detailed coding to high-level system design and management, raising questions about their identity as builders. This change is not just theoretical; it's happening now, with significant portions of code being written by AI.\n\nDespite the challenges, the essence of software engineering—solving problems and creating value—remains unchanged. Engineers can adapt by finding a balance between hands-on coding and orchestrating AI systems. The focus on broader skills, such as communication and systems thinking, becomes increasingly important.\n\nThe article concludes that the identity crisis isn't solely about AI taking jobs; it's an opportunity to reclaim the more comprehensive aspects of the engineering role. Engineers can evolve by embracing their identity as creators, combining technical work with strategic oversight of AI. The key is to adapt and find fulfillment in both aspects of this new landscape.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "cf047133cd50e135",
    "title": {
      "en": "Quadlet: Running Podman containers under systemd",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://mo8it.com/blog/quadlet/",
    "score": 285,
    "by": "gjvc",
    "time": 1742776025,
    "content": "Quadlet: Running Podman containers under systemd\n\n        2024-01-02, updated: 2024-02-19\n            Tags:\n            #container,#linux,#selfhosting\n\n        Reading time: ~11min\n\n        Quadlet lets you run your Podman containers as systemd services.\nThis is especially useful for running containers in the background and automatically starting them after a server reboot.\nRunning Podman containers under systemd isn't new.\nIndeed, this was supported by Podman for a long time with the command podman generate systemd.\nBut this command now shows a deprecation warning to migrate to Quadlet.\nFor some months, I was too lazy to do that migration on my home server.\nWhy even touch a working system?\nBut now that I finally found time for it, I really appreciate Quadlet!\nI think that Podman finally has a Docker Compose alternative which is even more flexible and powerful!\nIn this blog post, I will explain how to use Quadlet with rootless Podman and migrate from the old method with podman generate systemd.\n\n        Note\n        If you wonder why systemd: Something has to start containers when there is no daemon (since Podman is daemonless).\nIf you are part of the vocal minority hating against systemd, then please leave instead of trolling.\n\n    Landscape mode recommended on mobile devices\n\n            Contents\n\n                            The deprecated method\n\n                            The problem\n\n                            Quadlet\n\n                            Is it any better?\n\n                            Dependencies\n\n                            Too many files?\n\n                            Updating images\n\n                            What about podman-compose?\n\n                            Further resources\n\nThe deprecated method\nLet's see how the old method worked before comparing it to Quadlet.\nYou can skip to the Quadlet section though.\nFirst, you had to create a container.\nIn an older post about Containerized PostgreSQL with rootless Podman, I created a container with a command similar to the following:\npodman create \\\n  --name test-db \\\n  -p 5432:5432 \\\n  -v ~/volumes/test-db:/var/lib/postgresql/data:Z \\\n  -e POSTGRES_PASSWORD=CHANGE_ME \\\n  --label \"io.containers.autoupdate=registry\" \\\n  docker.io/library/postgres:16\n\nThe command details are explained in the older post.\nThe only option that isn't explained there is --label \"io.containers.autoupdate=registry\".\nThis option enables updating the container image using podman auto-update which will be explained later in this post.\nAfter creating the container, you can run the following command:\npodman generate systemd test-db -fn --new\n\nIt creates the systemd service file container-test-db.service in your current path.\nThe options of the command aren't relevant for now, but they are also explained in the older post.\nTo use this generated service file, you had to place it in the directory ~/.config/systemd/user.\nTo enable and start it, you had to run the following command:\nsystemctl --user enable --now container-test-db\n\nThe problem\nThe problem with the old method is that it required you to run commands to…\n\ncreate a container\ngenerate a service file\nmove the service file if not already in the mentioned directory\nenable the service\n\nEspecially the command for creating the container is often lengthy.\nThis means that you had to create a shell script with these commands if you wanted to be able to rerun them later.\nTo reduce duplication, I created the following fish function to be called in my fish scripts that create the containers:\nfunction podman-default-create\n    set -l container_name $argv[1]\n\n    podman create \\\n        --name $container_name \\\n        --replace \\\n        $argv[2..]; or return 1\n\n    podman generate systemd --no-header --new --name $container_name >~/.config/systemd/user/container-$container_name.service; or return 1\n\n    systemctl --user enable --now container-$container_name\nend\n\nYou don't have to understand the details of the function above.\nWhat I want to demonstrate with it is that the old method was too hacky and involved the usage of redundant commands.\nThere must be an easier way, you might think.\nEspecially if you experienced the convenience that Docker Compose provides.\nBut this is not the only problem.\nThe old method is very inflexible!\nIf you want to cutomize the service file and use all systemd features, you need to manually edit it after each generation!\nQuadlet\nLet's take a look at the new method with Quadlet.\nFirst, you create the directory ~/.config/containers/systemd.\nThen, you place a .container file inside it.\nFor example, here is the test-db.container file:\n[Container]\nImage=docker.io/library/postgres:16\nAutoUpdate=registry\nPublishPort=5432:5432\nVolume=%h/volumes/test-db:/var/lib/postgresql/data:Z\nEnvironment=POSTGRES_PASSWORD=CHANGE_ME\n\n[Service]\nRestart=always\n\n[Install]\nWantedBy=default.target\n\nIt is a normal systemd service file but with the special section [Container].\nThis section has many documented options.\nAlmost all these options map to command line options that can be used to create a container with Podman (podman create).\nThe ones that we are interested in for the example are the following:\n\nImage specifies the image (with tag) to use\nAutoUpdate=registry maps to --label \"io.containers.autoupdate=registry\" (explained later in this post)\nPublishPort maps to -p\nVolume maps to -v\nEnvironment maps to -e\n\nIt is important to use the systemd specifier %h instead of ~ for the user home directory.\nIn the [Service] section, we use the Restart option and set it to always to always restart the container (unless stopped manually).\nTo automatically start the container on boot, we set the WantedBy option in the [Install] section to default.target.\n\n        Note\n        I thought that setting WantedBy to multi-user.target would work because it is the default target on servers.\nBut it doesn't work in the case of rootless containers.\nmulti-user.target is not defined in the user mode in systemd.\nYou can verify this by running the command systemctl --user status multi-user.target.\nIt is only defined in the system mode (systemctl status multi-user.target without --user).\n\nSince we use user services for systemd, we have to enable the linger for our user to start the containers without the user being logged in:\nloginctl enable-linger\n\n        ⚠️ Warning ⚠️\n        Enabling the linger is required for the container to be automatically started after a server reboot!\n\nFor systemd to discover the new service file, run systemctl --user daemon-reload.\nNow, you can start the container with systemctl --user start test-db.\nYou can check the status of the container service by running systemctl --user status test-db.\nYou can also verify that the Podman container is running by running podman ps.\nYou should find the container systemd-test-db.\nThe container has the name of the service file (test-db.container without the .container extension) prefixed by systemd- to avoid collisions with containers not managed by systemd.\nBut you can manually set the name of the container using the ContainerName option in the [Container] section.\nIs it any better?\nMy first impression was: \"Well, now I have to map all the podman create options to their equivalents in the [Container] section. Where is the benefit?\".\nBut after migrating all containers, I found the following benefits:\n\nYou have only one file (the container service file) instead of a script that generates a service file.\nYou can use all options possible in systemd. All options that you know about in the [Unit] and [Service] sections are supported. For example, you can specify a command to run before starting the container by setting the StartExecPre option. No more manual editing of generated files anymore!\nRather subjective: Writing configuration files is easier than writing and debugging shell scripts.\nIt is easier to handle dependencies as we will see in the next section.\n\nDependencies\nLet's assume that we have an app container that depends on the database container that we created.\nYou want the database container to be automatically started when the app container is started.\nYou also want to make sure that the app container is started after the database container.\nOtherwise, the app container might fail to start.\nHow can we express this dependency?\nLet's use OxiTraffic as an example (shameless plug 😅).\nHere is the container service file oxitraffic.container that should be placed in ~/.config/containers/systemd:\n[Container]\nImage=docker.io/mo8it/oxitraffic:0.9.2\nAutoUpdate=registry\nVolume=%h/volumes/oxitraffic/config.toml:/volumes/config.toml:Z,ro\nVolume=%h/volumes/oxitraffic/logs:/var/log/oxitraffic:Z\n\n[Unit]\nRequires=test-db.service\nAfter=test-db.service\n\n[Service]\nRestart=always\n\n[Install]\nWantedBy=default.target\n\nThe new section is [Unit].\nWe set the Requires option to test-db.service to only start the app when the database is started.\nWe also set the After option to make sure that both containers aren't started in parallel.\nNote that we use test-db.service when referencing this container service and not test-db.container.\n\ntest-db.container is the file name.\ntest-db.service is the service name.\nsystemd-test-db is the default container name.\n\nFor the app to communicate with the database, a network should be added to both containers with the Network option in the [Container] section, but networking is out of the scope of this post.\nToo many files?\nIn our example, we created two files, one for the app container and one for the database container.\nDoes this mean that multi-container apps are more complex with Quadlet because you can't just put them in one file like with Docker Compose?\nIt depends on how you define complexity in this context.\nDoes splitting content over multiple files always result in a higher complexity?\nFor me, it is more complex to have everything in the same file.\nI had to maintain Docker Compose files with hundreds of lines and dozens of containers…\nThat wasn't fun!\nHaving each container in its own file has less mental overhead for me because I just have to think about this single container when I am in its file.\nOf course, you need to specify its dependencies on other containers, but you don't need to think about the details of these other containers.\nThe Docker Compose file of Mailcow is a terrifying example of huge Docker Compose files.\n\n        Note\n        Docker Compose supports splitting to multiple files.\n\nSo we need multiple files.\nBut we should still group related ones together!\nQuadlet supports placing unit files inside directories in the ~/.config/containers/systemd directory.\nFor our example, you would create an oxitraffic directory and place both files inside it.\nUpdating images\nNow, we have containers running in the background and automatically started after a server reboot.\nWouldn't it be nice to have an easy method to update the images of these containers without running podman pull for every container and then restarting the updated ones?\nFor example, if a new image is uploaded for PostgreSQL 16 (with the image tag 16 that we used), then the image should be updated and the container should be restarted.\nWith Docker, you would need something like Watchtower.\nBut Podman provides a tool out of the box!\nIf you set AutoUpdate=registry, you can just run podman auto-update and Podman will check if the registry has a newer image which is compatible with the used tag.\nIn that case, the image will be pulled and the container will be restarted.\nIt is that easy 😍\nOf course, this could be dangerous if you use a tag like latest for OxiTraffic instead of a concrete version like 0.9.2.\nBecause the next version pushed to the latest tag might include a breaking change!\nIt is even more dramatic if you use the latest tag for the PostgreSQL image because manual migrations are always required when upgrading PostgreSQL to a new major version.\nTherefore, always use a tag that can't lead to a breaking change!\nTrust me, this is not only a problem with Podman updates.\nI learned this the hard way when trying to deploy Docker containers a while ago that used the latest tag.\nPersonally, I run podman auto-update manually on the server every couple of days to see what has been updated and make sure that the containers are still healthy afterwards.\nWhat about podman-compose?\nThere is the Python script podman-compose which runs Compose files with Podman.\nBut I don't consider it a long-term alternative to Docker Compose for multiple reasons:\n\npodman-compose is just a translation layer between the Compose spec and Podman with systemd. Why yet another layer of abstraction that sacrifices flexibility? It doesn't let you use all features of systemd.\nI consider Python scripts a prototype because of Python's interpreted nature. Official Podman projects are written in a compiled language like Rust or Go.\nIt is not actively maintained. The last commit was 5 months ago.\n\nQuadlet aligns much better with the rootless, daemonless design of Podman.\nIf you are stuck with Compose files and would like to try out Quadlet, check out podlet that can help you during the migration.\nFurther resources\nTake this post as an introduction.\nI highly recommend reading the man page podman-systemd.unit to get a deeper understanding of Quadlet.\nYou don't need to read the documentation of every supported option though.\nQuadlet doesn't only work with containers.\nIt can also manage pods, networks and volumes (see man page).\nIf you are new to writing systemd unit files (like me), I also recommend checking out the man pages systemd.unit and systemd.service.\npodlet is a wonderful Rust tool that can help you during the migration.\nIt can create Quadlet files out of Podman commands or even (Docker) Compose files.\nCheck out the similar blog post on blog.while-true-do for another perspective and a second example.\nIt is my favorite blog related to Linux 🥰\nFinally, if you want to see my migration as an example, then you can compare before and after.",
    "summary": {
      "en": "**Summary of Quadlet: Running Podman Containers Under systemd**\n\nQuadlet is a tool that allows users to run Podman containers as systemd services, making it easier to manage containers in the background and automatically start them after a server reboot. Previously, users used the command `podman generate systemd`, but it is now being phased out in favor of Quadlet.\n\n### Key Points:\n\n- **Old Method vs. Quadlet**: The previous method required multiple commands to create and manage containers, which was cumbersome and inflexible. Quadlet simplifies this by using a single configuration file for each container.\n\n- **Creating a Container with Quadlet**: \n  - Users create a directory (`~/.config/containers/systemd`) and place a `.container` file inside it, specifying details like the image, ports, volumes, and environment variables.\n  - A special `[Container]` section is included for Podman-specific options, while the `[Service]` section manages systemd features.\n\n- **Benefits of Quadlet**:\n  - Only one configuration file is needed per container, making management simpler.\n  - Users can utilize all systemd features without manual edits.\n  - Dependencies between containers can be easily managed using the `[Unit]` section.\n\n- **Automatic Updates**: By using `AutoUpdate=registry`, users can easily update container images and restart them without running multiple commands.\n\n- **Comparison with podman-compose**: While there is a tool called podman-compose, it is not considered a long-term solution due to its limitations and lack of active maintenance. Quadlet aligns better with the design of Podman.\n\n- **Further Resources**: For more information, users are encouraged to read the man pages for Quadlet and systemd, and consider using tools like podlet for migration from Docker Compose.\n\nIn conclusion, Quadlet provides a more efficient and flexible way to manage Podman containers as systemd services, enhancing the user experience compared to the older methods.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "bc798ab915ad080e",
    "title": {
      "en": "The case of the critical section that let multiple threads enter a block of code",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://devblogs.microsoft.com/oldnewthing/20250321-00/?p=110984",
    "score": 136,
    "by": "luu",
    "time": 1742717665,
    "content": "March 24, 2025\n      On how different Windows ABIs choose how to pass 32-bit values in 64-bit registers\n\n        Raymond Chen",
    "summary": {
      "en": "The text discusses how various Windows Application Binary Interfaces (ABIs) handle the passing of 32-bit values when using 64-bit registers. It highlights the differences in these methods.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "b5943bb62ae43b9d",
    "title": {
      "en": "Move on to ESM-Only",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://antfu.me/posts/move-on-to-esm-only",
    "score": 113,
    "by": "bpierre",
    "time": 1742776525,
    "content": "The Toolings are Ready Modern Tools Top-Down & Bottom-Up Requiring ESM in Node.js The Troubles with Dual Formats Interop Issues Dependency Resolution Package Size When Should We Move to ESM-only? New Packages Browser-targeted Packages Standalone CLI Node.js Support Know Your Consumers How Far We Are? Moving Forward Three years ago, I wrote a post about shipping ESM & CJS in a single package, advocating for dual CJS/ESM formats to ease user migration and trying to make the best of both worlds. Back then, I didn’t fully agree with aggressively shipping ESM-only, as I considered the ecosystem wasn’t ready, especially since the push was mostly from low-level libraries. Over time, as tools and the ecosystem have evolved, my perspective has gradually shifted towards more and more on adopting ESM-only.As of 2025, a decade has passed since ESM was first introduced in 2015. Modern tools and libraries have increasingly adopted ESM as the primary module format. According to WOOORM’s script, the packages that ships ESM on npm in 2021 was 7.8%, and by the end of 2024, it had reached 25.8%. Although a significant portion of packages still use CJS, the trend clearly shows a good shift towards ESM.ESM adoption over time, generated by the npm-esm-vs-cjs script. Last updated at 2024-11-27Here in this post, I’d like to share my thoughts on the current state of the ecosystem and why I believe it’s time to move on to ESM-only.The Toolings are Ready #Modern Tools #With the rise of Vite as a popular modern frontend build tool, many meta-frameworks like Nuxt, SvelteKit, Astro, SolidStart, Remix, Storybook, Redwood, and many others are all built on top of Vite nowadays, that treating ESM as a first-class citizen.As a complement, we have also testing library Vitest, which was designed for ESM from the day one with powerful module mocking capability and efficient fine-grain caching support.CLI tools like tsx and jiti offer a seamless experience for running TypeScript and ESM code without requiring additional configuration. This simplifies the development process and reduces the overhead associated with setting up a project to use ESM.Other tools, for example, ESLint, in the recent v9.0, introduced a new flat config system that enables native ESM support with eslint.config.mjs, even in CJS projects.Top-Down & Bottom-Up #Back in 2021, when SINDRESORHUS first started migrating all his packages to ESM-only, for example, find-up and execa, it was a bold move. I consider this move as a bottom-up approach, as the packages that rather low-level and many their dependents are not ready for ESM yet. I was worried that this would force those dependents to stay on the old version of the packages, which might result in the ecosystem being fragmented. (As of today, I actually appreciate that move bringing us quite a lot of high-quality ESM packages, regardless that the process wasn’t super smooth).It’s way easier for an ESM or Dual formats package to depend on CJS packages, but not the other way around. In terms of smooth adoption, I believe the top-down approach is more effective in pushing the ecosystem forward. With the support of high-level frameworks and tools from top-down, it’s no longer a significant obstacle to use ESM-only packages. The remaining challenges in terms of ESM adoption primarily lie with package authors needing to migrate and ship their code in ESM format.Requiring ESM in Node.js #The capability to require() ESM modules in Node.js, initiated by JOYEECHEUNG, marks an incredible milestone. This feature allows packages to be published as ESM-only while still being consumable by CJS codebases with minimal modifications. It helps avoid the async infection (also known as Red Functions) introduced by dynamic import() ESM, which can be pretty hard, if not impossible in some cases, to migrate and adapt.This feature was recently unflagged and backported to Node.js v22 (and soon v20), which means it should be available to many developers already. Consider the top-down or bottom-up metaphor, this feature actually makes it possible to start ESM migration also from middle-out, as it allows import chains like ESM → CJS → ESM → CJS to work seamlessly.To solve the interop issue between CJS and ESM in this case, Node.js also introduced a new export { Foo as 'module.exports' } syntax in ESM to export CJS-compatible exports (by this PR). This allows package authors to publish ESM-only packages while still supporting CJS consumers, without even introducing breaking changes (expcet for changing the required Node.js version).For more details on the progress and discussions around this feature, keep track on this issue.The Troubles with Dual Formats #While dual CJS/ESM packages have been a quite helpful transition mechanism, they come with their own set of challenges. Maintaining two separate formats can be cumbersome and error-prone, especially when dealing with complex codebases. Here are some of the issues that arise when maintaining dual formats:Interop Issues #Fundamentally, CJS and ESM are different module systems with distinct design philosophies. Although Node.js has made it possible to import CJS modules in ESM, dynamically import ESM in CJS, and even require() ESM modules, there are still many tricky cases that can lead to interop issues.One key difference is that CJS typically uses a single module.exports object, while ESM supports both default and named exports. When authoring code in ESM and transpiling to CJS, handling exports can be particularly challenging, especially when the exported value is a non-object, such as a function or a class. Additionally, to make the types correct, we also need to introduce further complications with .d.mts and .d.cts declaration files. And so on…As I am trying to explain this problem deeper, I found that I actually wish you didn’t even need to be bothered with this problem at all. It’s frankly too complicated and frustrating. If you are just a user of packages, let alone the package authors to worry about that. This is one of the reasons I advocate for the entire ecosystem to transition to ESM, to leave these problems behind and spare everyone from this unnecessary hassle.Dependency Resolution #When a package has both CJS and ESM formats, the resolution of dependencies can become convoluted. For example, if a package depends on another package that only ships ESM, the consumer must ensure that the ESM version is used. This can lead to version conflicts and dependency resolution issues, especially when dealing with transitive dependencies.Also for packages that are designed to used with singleton pattern, this might introduce multiple copies of the same package and cause unexpected behaviors.Package Size #Shipping dual formats essentially doubles the package size, as both CJS and ESM bundles need to be included. While a few extra kilobytes might not seem significant for a single package, the overhead can quickly add up in projects with hundreds of dependencies, leading to the infamous node_modules bloat. Therefore, package authors should keep an eye on their package size. Moving to ESM-only is a way to optimize it, especially if the package doesn’t have strong requirements on CJS.When Should We Move to ESM-only? #This post does not intend to diminish the value of dual-format publishing. Instead, I want to encourage evaluating the current state of the ecosystem and the potential benefits of transitioning to ESM-only.There are several factors to consider when deciding whether to move to ESM-only:New Packages #I strongly recommend that all new packages be released as ESM-only, as there are no legacy dependencies to consider. New adopters are likely already using a modern, ESM-ready stack, there being ESM-only should not affect the adoption. Additionally, maintaining a single module system simplifies development, reduces maintenance overhead, and ensures that your package benefits from future ecosystem advancements.Browser-targeted Packages #If a package is primarily targeted for the browser, it makes total sense to ship ESM-only. In most cases, browser packages go through a bundler, where ESM provides significant advantages in static analysis and tree-shaking. This leads to smaller and more optimized bundles, which would also improve loading performance and reduce bandwidth consumption for end users.Standalone CLI #For a standalone CLI tool, it’s no difference to end users whether it’s ESM or CJS. However, using ESM would enable your dependencies to also be ESM, facilitating the ecosystem’s transition to ESM from a top-down approach.Node.js Support #If a package is targeting the evergreen Node.js versions, it’s a good time to consider ESM-only, especially with the recent require(ESM) support.Know Your Consumers #If a package already has certain users, it’s essential to understand the dependents’ status and requirements. For example, for an ESLint plugin/utils that requires ESLint v9, while ESLint v9’s new config system supports ESM natively even in CJS projects, there is no blocker for it to be ESM-only.Definitely, there are different factors to consider for different projects. But in general, I believe the ecosystem is ready for more packages to move to ESM-only, and it’s a good time to evaluate the benefits and potential challenges of transitioning.How Far We Are? #The transition to ESM is a gradual process that requires collaboration and effort from the entire ecosystem. Which I believe we are on a good track moving forward.To improve the transparency and visibility of the ESM adoption, I recently built a visualized tool called Node Modules Inspector for analyzing your packages’s dependencies. It provides insights into the ESM adoption status of your dependencies and helps identify potential issues when migrating to ESM.Here are some screenshots of the tool to give you a quick impression:Node Modules Inspector - OverviewNode Modules Inspector - Dependency GraphNode Modules Inspector - Reports like ESM Adoptions and Duplicated PackagesThis tool is still in its early stages, but I hope it will be a valuable resource for package authors and maintainers to track the ESM adoption progress of their dependencies and make informed decisions about transitioning to ESM-only.To learn more about how to use it and inspect your projects, check the repository node-modules-inspector.Moving Forward #I am planning to gradually transition the packages I maintain to ESM-only and take a closer look at the dependencies we rely on. We also have plenty of exciting ideas for the Node Modules Inspector, aiming to provide more useful insights and help find the best path forward.I look forward to a more portable, resilient, and optimized JavaScript/TypeScript ecosystem.I hope this post has shed some light on the benefits of moving to ESM-only and the current state of the ecosystem. If you have any thoughts or questions, feel free to reach out using the links below. Thank you for reading!",
    "summary": {
      "en": "The article discusses the shift towards using ECMAScript Modules (ESM) exclusively in Node.js, highlighting the importance of this transition for modern development tools and libraries. \n\n**Key Points:**\n\n1. **Current State of ESM Adoption**: ESM's usage has increased significantly, from 7.8% in 2021 to 25.8% in 2024, showing a clear trend towards adopting ESM as the primary module format.\n\n2. **Modern Tools Supporting ESM**: Tools like Vite and testing library Vitest have embraced ESM, making it easier for developers to use this format without complex configurations.\n\n3. **Challenges of Dual Formats**: Maintaining both CommonJS (CJS) and ESM formats can lead to interoperability issues, dependency resolution complications, and increased package sizes.\n\n4. **Moving to ESM-only**: New packages should be ESM-only to simplify development and reduce maintenance. For browser-targeted and CLI packages, ESM is also beneficial for performance and compatibility.\n\n5. **Node.js Support**: Recent updates in Node.js allow ESM modules to be required in CJS codebases, facilitating smoother transitions.\n\n6. **Transition Strategy**: Developers are encouraged to evaluate their packages and consider moving to ESM-only as the ecosystem is ready for this change. Tools like Node Modules Inspector can help analyze dependencies and track ESM adoption.\n\n7. **Looking Ahead**: The author plans to transition their packages to ESM-only and improve tools to assist others in this process, aiming for a more optimized JavaScript ecosystem.\n\nOverall, the article advocates for the benefits of adopting ESM-only in the JavaScript ecosystem, emphasizing the readiness of tools and libraries for this transition.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "8d6f400bc4fd96b8",
    "title": {
      "en": "To Win Trust and Admiration, Fix Your Microphone",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.scientificamerican.com/article/to-win-trust-and-admiration-fix-your-microphone/",
    "score": 19,
    "by": "jimminyx",
    "time": 1742881602,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "59c3d3c0405f7a06",
    "title": {
      "en": "The belay test and the modern American climbing gym",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.climbing.com/people/peter-mayfield-belay-test-modern-american-climbing-gym-history/",
    "score": 152,
    "by": "vasco",
    "time": 1742408381,
    "content": "Meet the Guy Who Invented the Belay Test—and the Modern American Climbing Gym\n\n          The story of California’s City Rock\n\n      Updated\n\n        Mar 18, 2025\n\n          Bruce Hildenbrand\n\n      Comment\n\n    Share\n\n            Share to Outside Feed\n            Create a new post with the article attached\n\n          Copy link\n\n          Email\n\n          Share on X\n\n          Share on Facebook\n\n          Share on Reddit\n\n              Photo: Peter Mayfield Collection\n\n                Peter Mayfield bouldering at City Rock Photo: Peter Mayfield Collection\n\n      Heading out the door? Read this article on the new Outside+ app available now on iOS devices for members!\n      Download the app.\n\n      Forty years ago, there was not a single purpose-built climbing gym in the U.S. In 1987, Seattle’s Vertical Club opened and a year later, the Portland Rock Gym launched. But the concept of a profitable indoor climbing gym was far from proven. This is the story of the person whose vision and determination would revolutionize climbing gyms as we know them today.\nPeter Mayfield was a climbing prodigy. At 16 years old, in 1978, he was repeating the hardest free climbs in Yosemite. He went on to establish some of the hardest aid routes on the Valley’s big walls. By the age of 19, he was guiding for the prestigious Yosemite Mountaineering School, and became chief guide a year later. When sport climbing competitions began in the late 1980s, Mayfield participated in the first ever Snowbird climbing competition as well as Jeff Lowe’s national competitions.\nIn 1988, Mayfield formed a guiding cooperative with legends Kim Schmitz and Jim Bridwell, but by his mid-twenties, he was looking for a job that would keep him closer to home. So when Bridwell returned from a trip to Europe with posters and magazines of climbing walls going up in France, Mayfield knew he’d found his next venture.\n\nPeter Mayfield at 19 years old on his first ascent in 1981 of Zenyatta Mondatta on El Cap with Jim Bridwell. Twenty years after his FA, Mayfield repeated the route with his 19-year-old son Braden. (Photo: Peter Mayfield Collection)\nAt the time, the walls in France were closer to artwork in public spaces than purpose-built climbing structures. It was around that time that the Vertical Club—a space for experienced climbers to train—went up, but Mayfield had something completely different in mind. As chief guide of the Yosemite Mountaineering School, he didn’t just want to create a place for himself and his buddies to get stronger. “I had this serious career of turning people onto this sport,” reflects Mayfield. “I was really thinking [about] kids, corporate programs … The word gym wasn’t even in my mind. It was more like an indoor climbing institute.”\nWith the popularity of indoor climbing gyms these days, it’s hard to imagine what it was like in the late ’80s, when climbing itself was still very much a niche activity. Mayfield knew that to keep the gym lights on, he’d need to attract more than just his hardcore climbing buddies.\nThen there was the problem with California’s legendary great weather. Mayfield was skeptical that climbers would want to climb indoors on days when the sun was shining, because unlike Seattle and Portland, you could climb outdoors virtually year-round. So his marketing plan was also designed to get non-climbers to come to his gym. He wanted to build a place where “10,000 eight-year-old girls could try something they never dreamed of.”\n“That was my pitch,” Mayfield says.\nThe economics of the first commercial climbing gym\nMayfield found his first investor while guiding an ascent of Alaska’s Moose’s Tooth. Tent-bound by storms for days, he discussed his climbing gym dreams with his clients. At the end of the expedition, as they were splitting up at the Oakland Airport, one of the clients asked him what he needed to start his business. He replied that he needed money for a phone line. The client pulled out his wallet and gave him $200. “I literally got handed two C-notes to start my business,” he remembers. He used that money to put in a Pacific Bell phone line in his mother’s basement in Berkeley and got to work.\n“I got no positive reaction from the [climbing] industry at all,” remembers Mayfield. “I was pretty connected because I climbed with Bridwell. I met with the CEO of The North Face. I met with the CEO of Marmot. I met with the president of REI. They all said ‘it will never work. It makes sense in Seattle where it rains. Who would ever climb indoors in California?”\nMayfield even sent his business plan to Yvon Chouinard. “He was very polite and gave me some good advice,” Mayfield recalls. “He did not invest. None of them invested. But he kind of got what I was talking about.” (Chouinard did tell Mayfield that he should try to sell milkshakes: “If you can nail it with the youth, you will kick ass.”)\nSo Mayfield widened his net, contacting ”everybody in Silicon Valley who had ever climbed” and raised half a million dollars with 48 limited partners, essentially a large group of $5,000 investors. He stalled for a while, dealing with the mountain of paperwork associated with creating such a high volume of limited partnerships. But in the midst of his fundraising, he produced the Great Outdoor Adventure Fair in San Francisco. To promote his concept to the general public, he set up a portable climbing wall at the fair. John Gage, high up in management at one of Silicon Valley’s most successful startups, Sun Microsystems, was impressed and tossed in $40,000, re-energizing Mayfield.\nBut at that time, building a climbing gym was a huge, complex undertaking. Mayfield relied on a suite of competent advisers to help him through. He eventually hooked up with Armand Tanny of Gold’s Gym, who explained the new, revolutionary process of selling and managing memberships via an automatic fund transfer. They also impressed upon him the importance of aesthetics. The other two climbing gyms had spent no more than $18,000 building their gyms. Mayfield spent $40,000 on just the bathrooms.\nChristian Griffith (designer of City Rock walls) and Peter Mayfield (Photo: Beth Wald)\nBuilding a gym for climbers—and non-climbers\n“So then I was like, ‘Okay, this is a gym. This is not an indoor sculpture garden that we climb on that’s a climbing institute. I am going to build a business model with monthly memberships,'” adds Mayfield. He decided to call it City Rock Gym. The gym was located in Emeryville, between Oakland and Berkeley.\n“I made the first gym you could take your kid to and trust that they would be well taken care of,” he explains.\nA number of climbing wall companies, such as Entre-Prises, were pressing Mayfield to help him build his gym. Instead, he turned to Christian Griffith, who he had climbed with on the competition circuit, and who felt that they could design an ”incredible space” together. “Christian posted up in south Berkeley in my mom’s yard and carved foam for two weeks and he completely created that gym,” recalls Mayfield. “He took foam core and glued graph paper to it and he did a beautiful model. He was inspired by the gritstone cliffs in England where you are always like ‘what’s around the corner?’ So City Rock had these really protuberant buttresses.”\nOverall, the City Rock Gym design was both intentional and approachable. “When people walked in the door I wanted them to see the friendly slab. I wanted them to think they can do this,” he explains. “I want them to see the six year olds climbing. I really felt that I had to prove the concept. It’s not me and my bros throwing down at the boulders.”\nInstruction served as another key pioneering element of Mayfield’s gym concept. He began teaching the climbing safety techniques he’d learned from the Sierra Club, an organization which had been studying and teaching belay practices since the 1930s. “We really did invent the belay test,” he recalls, citing the need for procedure in such a high-risk business. “I thought somebody would die at our business. I ran the statistics in my head. So many people tying knots. So many people belaying. I really took it seriously.”\nGiven the magnitude of risk, Mayfield says he really “leaned into” establishing a belay test and enforcing safety. “I had some really good, famous, climbers come in and fail the belay test,” he recalls. “These good climbers were just a little too casual with their hands. You don’t vaguely slide your hands around. They had to tighten their act up a little bit.”\nOn City Rock Gym’s grand opening weekend in 1990, 900 people came in to climb. Mayfield watched brand-new climbing ropes wear out in front of his eyes. “We installed these half-pipe sealed bearing things so the ropes wouldn’t wear out, but on Sunday morning: ‘Oh my God! I see cores in that rope. Quick, replace that,’” remembers Mayfield. “I took the climbing school really seriously. I hired the best and they were well trained … It was really high level with really good technique coaching.”\nCity Rock quickly gained a loyal following. With his emphasis on introducing new participants to the sport, Mayfield pioneered programs like a kid’s belay clinic and the first climbing-themed birthday parties. He created an event called Women on Rock and flew in Lynn Hill to teach it. The vertical dance team Bandaloop got its start at City Rock in 1991.\nThe vertical dance company Bandaloop was born at City Rock (Photo: Peter Mayfield Collection)\nBut could a commercial climbing gym keep elite climbers stoked?\nWhile he was intent on attracting non-climbers to the gym, Mayfield didn’t turn his back on his climbing buddies. He created routes for their level of difficulty and also gave some of them jobs. Many experienced climbers including Scott Cosgrove, Steve Schneider, Bird Lew, and Don Welch worked at City Rock.\n“We set some hard routes,” remembers Schneider, a 5.14 climber and one of the routesetters at City Rock. “We did some competitions there which were very successful and those routes would be left up for others to climb.”\nAs the initial buzz of City Rock’s grand opening wore off, two vital questions remained: Were established climbers willing to climb indoors on sunny days? And were the routes difficult enough to keep experienced climbers satisfied? “I was initially a little skeptical but found I really enjoyed it,” recalls Greg Murphy who, together with Chan Harrell, held the fastest known time on the Northwest Face of Half Dome. “There was nothing super steep as I recall, so the harder routes were kind of finger intensive. But it always seemed like there was a good mix of routes and some harder projects to work on.”\nAnne Smith, an excellent climber and the mother of Connor Herson, agrees: “CR [City Rock] was very much geared towards experienced climbers. [It] hosted National Championship events and set at least as many harder climbers as easier ones.”\nAs a competitive sport climber himself, Mayfield eagerly hosted national championship competitions five years in a row. Hans Florine had set the speed record on the Nose with Steve Schnieder the week before he won the 1990 nationals in both difficulty and speed at City Rock. For the next three years, Florine partnered with Mayfield and City Rock to stage the national championships there. The gym played host to top climbers such as Robyn Erbesfield, Bobbi Bensman, Dale Goddard, and Jim Karn.\nHank Levine competing in US Nationals at City Rock (Photo: Peter Mayfield Collection)\nThe inclusive climbing gym concept takes off\nThree years after City Rock’s opening, California got its second climbing gym via Tom Davis’s now-famous Pacific Edge, in Santa Cruz. (Chris Sharma and Natalia Grossman both learned to climb there.) Davis was inspired by what Mayfield had created: “It was visionary to open a gym in California at the time—but that is really hard to imagine now. From a dream to a nationwide industry.”\nBut Davis recalls that perhaps the most important thing City Rock did for the climbing gym industry was establish safety standards to manage risk, including legitimate belay tests. “It could easily have been a dirtbag climbers’ training hangout,” Davis explains. “But they pulled off something that demonstrated the true value of climbing: risk taking with the correct skillset, and on a level that a discerning public could understand.”\nIn 1992, Mayfield and Casey Newman, who ran the Colorado Rock Gym, along with several others, founded the Climbing Gym Association, which has since become the Climbing Wall Association. The purpose of this organization was to regulate the burgeoning climbing gym industry, hopefully keeping the government from stepping in with its own set of regulations. They built a gym accreditation program and traveled around the country accrediting the dozens of climbing gyms that existed by that time in the U.S. But that’s a story for another time.\nWhat ever happened to the original City Rock—and Mayfield?\nSo how did the original design of City Rock hold up? Jim Collins, a notable climber, bestselling author, and business guru who taught MBA students at Stanford University, studied City Rock as a class project. The academic analysis found that City Rock was “going to get blown out of the water by the next wave of climbing gyms,” in Mayfield’s words. Collins and his students had two major reasons to back up their conclusion. The first was that City Rock’s walls weren’t steep enough. And the second reason was that they hypothesized other entrepreneurs would come along and operate gyms more efficiently at a lower cost.\nMayfield explains the limited steepness of his gym’s original walls: “[City Rock] was designed and built before America had super steep climbing. It was before there was a Rifle. It was before there was a Jailhouse Rock. Two or three years later, the next wave of gyms did have arches and bigger, steeper, upside-down climbing structures because the cliffs that people were climbing on were like that. The hard climbing in 1989 was like Smith Rock. Dead vertical or maybe a bit past vertical, like 100 degrees.”\nIn 1997, Mayfield sold City Rock to Mark Melvin, who founded Mission Cliffs, now Touchstone Climbing. He then moved back to Yosemite, where he started an eco-tourism business to educate people about nature and conservation in the Valley.\nWhile the decision to sell the gym he poured his heart and soul into for the better part of 10 years was difficult, he made some money and his investors did well trading up into the Touchstone world. “I couldn’t care less about making money. I am just not wired that way to manipulate the world to make money.” He says that in retrospect, he was “faking it” as a business person and finds himself better adapted to social entrepreneurship.\nAfter the sale, City Rock fell victim to the skyrocketing rents associated with the Internet boom in the Bay Area. When rent tripled in the highly desirable Emeryville area, it was time to look for another location. The new owners of City Rock honored its memberships and opened Berkeley Ironworks a few miles away in 2000. It was three times the size of City Rock and also featured a large fitness area, a yoga studio, and an indoor cycling room. It represented the next generation of climbing gyms.\nNowadays, Mayfield lives in Truckee, CA, where he founded the nonprofit Gateway Mountain Center, which focuses on nature-based therapy for kids. “Even to this day, I stay interested in the effect [of climbing gyms] on youth, especially high-need youth,” says Mayfield. He also works with kids from local drug treatment centers and juvenile detention camps. “I am watching these kids just light up,” he says. “They face incredible challenges in life, but they get to climb. This is transformative for high-need youth.”\nThirty-five years have passed since Mayfield first started City Rock. While his impacts on the climbing gym industry are clear, he also continues to abide by his personal commitment to benefit youth through climbing. Mayfield still climbs, but these days, prefers Berkeley’s iconic Indian Rock to a climbing gym.\nReflecting on his pioneering foray into climbing gyms, he says: “I had kind of a classic business school case study of being the pioneer who spends all the money on R&D to prove the concept, then gets blown out of the water by the next wave, the well-funded MBAs who come after.”\n\n      Comment\n\n    Similar Reads\n\n          Touchstone Gym Union Negotiations Escalate to First-Ever Routesetter Strike\n\n          A Climbing Graphic Novel? Competitive Climbing Becomes Art in This New Book.\n\n          The Horrors of the Gym Belay Test\n\n          Want More From Your Climbing Gym? These 7 US Facilities Are Getting Creative.\n\n      Tags\n\n        climbing gyms\n\n        Peter Mayfield\n\n        Recycle",
    "summary": {
      "en": "**Summary:**\n\nPeter Mayfield is a pioneer in the climbing gym industry, having transformed indoor climbing with the establishment of City Rock Gym in California in 1990. Before this, there were no dedicated climbing gyms in the U.S., and Mayfield sought to create a space that welcomed not only experienced climbers but also newcomers and children.\n\nStarting as a talented climber and guide, Mayfield envisioned a gym that would serve as an \"indoor climbing institute\" to attract a broader audience. Despite skepticism from established climbing brands, he successfully raised funds and curated a unique gym environment focused on safety, including the introduction of the belay test to ensure climber safety.\n\nCity Rock's grand opening was a success, drawing large crowds and gaining a loyal following. Mayfield emphasized inclusivity by offering programs for kids and women and hosted national climbing competitions.\n\nOver time, the gym evolved and inspired others to open climbing facilities, leading to a growth in the industry. After selling City Rock in 1997, Mayfield shifted his focus to eco-tourism and now runs a nonprofit that uses climbing to help at-risk youth. He reflects on his journey as a blend of pioneering spirit and social entrepreneurship, acknowledging that while he was a trailblazer, he faced challenges as newer, more efficient gyms entered the market.",
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "dc82356fa3518944",
    "title": {
      "en": "Most AI value will come from broad automation, not from R & D",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://epoch.ai/gradient-updates/most-ai-value-will-come-from-broad-automation-not-from-r-d",
    "score": 202,
    "by": "ydnyshhh",
    "time": 1742668500,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  }
]