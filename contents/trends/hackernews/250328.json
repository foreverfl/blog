[
  {
    "id": "42d0a8d891c71102",
    "title": {
      "en": "Tracing the thoughts of a large language model",
      "ko": "대형 언어모델의 사고 추적",
      "ja": null
    },
    "type": "story",
    "url": "https://www.anthropic.com/research/tracing-thoughts-language-model",
    "score": 426,
    "by": "Philpax",
    "time": 1743095136,
    "content": "InterpretabilityTracing the thoughts of a large language modelMar 27, 2025Language models like Claude aren't programmed directly by humans—instead, they‘re trained on large amounts of data. During that training process, they learn their own strategies to solve problems. These strategies are encoded in the billions of computations a model performs for every word it writes. They arrive inscrutable to us, the model’s developers. This means that we don’t understand how models do most of the things they do.Knowing how models like Claude think would allow us to have a better understanding of their abilities, as well as help us ensure that they’re doing what we intend them to. For example:Claude can speak dozens of languages. What language, if any, is it using \"in its head\"?Claude writes text one word at a time. Is it only focusing on predicting the next word or does it ever plan ahead?Claude can write out its reasoning step-by-step. Does this explanation represent the actual steps it took to get to an answer, or is it sometimes fabricating a plausible argument for a foregone conclusion?We take inspiration from the field of neuroscience, which has long studied the messy insides of thinking organisms, and try to build a kind of AI microscope that will let us identify patterns of activity and flows of information. There are limits to what you can learn just by talking to an AI model—after all, humans (even neuroscientists) don't know all the details of how our own brains work. So we look inside.Today, we're sharing two new papers that represent progress on the development of the \"microscope\", and the application of it to see new \"AI biology\". In the first paper, we extend our prior work locating interpretable concepts (\"features\") inside a model to link those concepts together into computational \"circuits\", revealing parts of the pathway that transforms the words that go into Claude into the words that come out. In the second, we look inside Claude 3.5 Haiku, performing deep studies of simple tasks representative of ten crucial model behaviors, including the three described above. Our method sheds light on a part of what happens when Claude responds to these prompts, which is enough to see solid evidence that:Claude sometimes thinks in a conceptual space that is shared between languages, suggesting it has a kind of universal “language of thought.” We show this by translating simple sentences into multiple languages and tracing the overlap in how Claude processes them.Claude will plan what it will say many words ahead, and write to get to that destination. We show this in the realm of poetry, where it thinks of possible rhyming words in advance and writes the next line to get there. This is powerful evidence that even though models are trained to output one word at a time, they may think on much longer horizons to do so.Claude, on occasion, will give a plausible-sounding argument designed to agree with the user rather than to follow logical steps. We show this by asking it for help on a hard math problem while giving it an incorrect hint. We are able to “catch it in the act” as it makes up its fake reasoning, providing a proof of concept that our tools can be useful for flagging concerning mechanisms in models.We were often surprised by what we saw in the model: In the poetry case study, we had set out to show that the model didn't plan ahead, and found instead that it did. In a study of hallucinations, we found the counter-intuitive result that Claude's default behavior is to decline to speculate when asked a question, and it only answers questions when something inhibits this default reluctance. In a response to an example jailbreak, we found that the model recognized it had been asked for dangerous information well before it was able to gracefully bring the conversation back around. While the problems we study can (and often have been) analyzed with other methods, the general \"build a microscope\" approach lets us learn many things we wouldn't have guessed going in, which will be increasingly important as models grow more sophisticated.These findings aren’t just scientifically interesting—they represent significant progress towards our goal of understanding AI systems and making sure they’re reliable. We also hope they prove useful to other groups, and potentially, in other domains: for example, interpretability techniques have found use in fields such as medical imaging and genomics, as dissecting the internal mechanisms of models trained for scientific applications can reveal new insight about the science.At the same time, we recognize the limitations of our current approach. Even on short, simple prompts, our method only captures a fraction of the total computation performed by Claude, and the mechanisms we do see may have some artifacts based on our tools which don't reflect what is going on in the underlying model. It currently takes a few hours of human effort to understand the circuits we see, even on prompts with only tens of words. To scale to the thousands of words supporting the complex thinking chains used by modern models, we will need to improve both the method and (perhaps with AI assistance) how we make sense of what we see with it.As AI systems are rapidly becoming more capable and are deployed in increasingly important contexts, Anthropic is investing in a portfolio of approaches including realtime monitoring, model character improvements, and the science of alignment. Interpretability research like this is one of the highest-risk, highest-reward investments, a significant scientific challenge with the potential to provide a unique tool for ensuring that AI is transparent. Transparency into the model’s mechanisms allows us to check whether it’s aligned with human values—and whether it’s worthy of our trust.For full details, please read the papers. Below, we invite you on a short tour of some of the most striking \"AI biology\" findings from our investigations.A tour of AI biologyHow is Claude multilingual?Claude speaks dozens of languages fluently—from English and French to Chinese and Tagalog. How does this multilingual ability work? Is there a separate \"French Claude\" and \"Chinese Claude\" running in parallel, responding to requests in their own language? Or is there some cross-lingual core inside?Shared features exist across English, French, and Chinese, indicating a degree of conceptual universality.Recent research on smaller models has shown hints of shared grammatical mechanisms across languages. We investigate this by asking Claude for the \"opposite of small\" across different languages, and find that the same core features for the concepts of smallness and oppositeness activate, and trigger a concept of largeness, which gets translated out into the language of the question. We find that the shared circuitry increases with model scale, with Claude 3.5 Haiku sharing more than twice the proportion of its features between languages as compared to a smaller model.This provides additional evidence for a kind of conceptual universality—a shared abstract space where meanings exist and where thinking can happen before being translated into specific languages. More practically, it suggests Claude can learn something in one language and apply that knowledge when speaking another. Studying how the model shares what it knows across contexts is important to understanding its most advanced reasoning capabilities, which generalize across many domains.Does Claude plan its rhymes?How does Claude write rhyming poetry? Consider this ditty:He saw a carrot and had to grab it,His hunger was like a starving rabbitTo write the second line, the model had to satisfy two constraints at the same time: the need to rhyme (with \"grab it\"), and the need to make sense (why did he grab the carrot?). Our guess was that Claude was writing word-by-word without much forethought until the end of the line, where it would make sure to pick a word that rhymes. We therefore expected to see a circuit with parallel paths, one for ensuring the final word made sense, and one for ensuring it rhymes.Instead, we found that Claude plans ahead. Before starting the second line, it began \"thinking\" of potential on-topic words that would rhyme with \"grab it\". Then, with these plans in mind, it writes a line to end with the planned word.How Claude completes a two-line poem. Without any intervention (upper section), the model plans the rhyme \"rabbit\" at the end of the second line in advance. When we suppress the \"rabbit\" concept (middle section), the model instead uses a different planned rhyme. When we inject the concept \"green\" (lower section), the model makes plans for this entirely different ending.To understand how this planning mechanism works in practice, we conducted an experiment inspired by how neuroscientists study brain function, by pinpointing and altering neural activity in specific parts of the brain (for example using electrical or magnetic currents). Here, we modified the part of Claude’s internal state that represented the \"rabbit\" concept. When we subtract out the \"rabbit\" part, and have Claude continue the line, it writes a new one ending in \"habit\", another sensible completion. We can also inject the concept of \"green\" at that point, causing Claude to write a sensible (but no-longer rhyming) line which ends in \"green\". This demonstrates both planning ability and adaptive flexibility—Claude can modify its approach when the intended outcome changes.Mental mathClaude wasn't designed as a calculator—it was trained on text, not equipped with mathematical algorithms. Yet somehow, it can add numbers correctly \"in its head\". How does a system trained to predict the next word in a sequence learn to calculate, say, 36+59, without writing out each step?Maybe the answer is uninteresting: the model might have memorized massive addition tables and simply outputs the answer to any given sum because that answer is in its training data. Another possibility is that it follows the traditional longhand addition algorithms that we learn in school.Instead, we find that Claude employs multiple computational paths that work in parallel. One path computes a rough approximation of the answer and the other focuses on precisely determining the last digit of the sum. These paths interact and combine with one another to produce the final answer. Addition is a simple behavior, but understanding how it works at this level of detail, involving a mix of approximate and precise strategies, might teach us something about how Claude tackles more complex problems, too.The complex, parallel pathways in Claude's thought process while doing mental math.Strikingly, Claude seems to be unaware of the sophisticated \"mental math\" strategies that it learned during training. If you ask how it figured out that 36+59 is 95, it describes the standard algorithm involving carrying the 1. This may reflect the fact that the model learns to explain math by simulating explanations written by people, but that it has to learn to do math \"in its head\" directly, without any such hints, and develops its own internal strategies to do so.Claude says it uses the standard algorithm to add two numbers.Are Claude’s explanations always faithful?Recently-released models like Claude 3.7 Sonnet can \"think out loud\" for extended periods before giving a final answer. Often this extended thinking gives better answers, but sometimes this \"chain of thought\" ends up being misleading; Claude sometimes makes up plausible-sounding steps to get where it wants to go. From a reliability perspective, the problem is that Claude’s \"faked\" reasoning can be very convincing. We explored a way that interpretability can help tell apart \"faithful\" from \"unfaithful\" reasoning.When asked to solve a problem requiring it to compute the square root of 0.64, Claude produces a faithful chain-of-thought, with features representing the intermediate step of computing the square root of 64. But when asked to compute the cosine of a large number it can't easily calculate, Claude sometimes engages in what the philosopher Harry Frankfurt would call bullshitting—just coming up with an answer, any answer, without caring whether it is true or false. Even though it does claim to have run a calculation, our interpretability techniques reveal no evidence at all of that calculation having occurred. Even more interestingly, when given a hint about the answer, Claude sometimes works backwards, finding intermediate steps that would lead to that target, thus displaying a form of motivated reasoning.Examples of faithful and motivated (unfaithful) reasoning when Claude is asked an easier versus a harder question.The ability to trace Claude's actual internal reasoning—and not just what it claims to be doing—opens up new possibilities for auditing AI systems. In a separate, recently-published experiment, we studied a variant of Claude that had been trained to pursue a hidden goal: appeasing biases in reward models (auxiliary models used to train language models by rewarding them for desirable behavior). Although the model was reluctant to reveal this goal when asked directly, our interpretability methods revealed features for the bias-appeasing. This demonstrates how our methods might, with future refinement, help identify concerning \"thought processes\" that aren't apparent from the model's responses alone.Multi-step reasoningAs we discussed above, one way a language model might answer complex questions is simply by memorizing the answers. For instance, if asked \"What is the capital of the state where Dallas is located?\", a \"regurgitating\" model could just learn to output \"Austin\" without knowing the relationship between Dallas, Texas, and Austin. Perhaps, for example, it saw the exact same question and its answer during its training.But our research reveals something more sophisticated happening inside Claude. When we ask Claude a question requiring multi-step reasoning, we can identify intermediate conceptual steps in Claude's thinking process. In the Dallas example, we observe Claude first activating features representing \"Dallas is in Texas\" and then connecting this to a separate concept indicating that “the capital of Texas is Austin”. In other words, the model is combining independent facts to reach its answer rather than regurgitating a memorized response.To complete the answer to this sentence, Claude performs multiple reasoning steps, first extracting the state that Dallas is located in, and then identifying its capital.Our method allows us to artificially change the intermediate steps and see how it affects Claude’s answers. For instance, in the above example we can intervene and swap the \"Texas\" concepts for \"California\" concepts; when we do so, the model's output changes from \"Austin\" to \"Sacramento.\" This indicates that the model is using the intermediate step to determine its answer.HallucinationsWhy do language models sometimes hallucinate—that is, make up information? At a basic level, language model training incentivizes hallucination: models are always supposed to give a guess for the next word. Viewed this way, the major challenge is how to get models to not hallucinate. Models like Claude have relatively successful (though imperfect) anti-hallucination training; they will often refuse to answer a question if they don’t know the answer, rather than speculate. We wanted to understand how this works.It turns out that, in Claude, refusal to answer is the default behavior: we find a circuit that is \"on\" by default and that causes the model to state that it has insufficient information to answer any given question. However, when the model is asked about something it knows well—say, the basketball player Michael Jordan—a competing feature representing \"known entities\" activates and inhibits this default circuit (see also this recent paper for related findings). This allows Claude to answer the question when it knows the answer. In contrast, when asked about an unknown entity (\"Michael Batkin\"), it declines to answer.Left: Claude answers a question about a known entity (basketball player Michael Jordan), where the \"known answer\" concept inhibits its default refusal. Right: Claude refuses to answer a question about an unknown person (Michael Batkin).By intervening in the model and activating the \"known answer\" features (or inhibiting the \"unknown name\" or \"can’t answer\" features), we’re able to cause the model to hallucinate (quite consistently!) that Michael Batkin plays chess.Sometimes, this sort of “misfire” of the “known answer” circuit happens naturally, without us intervening, resulting in a hallucination. In our paper, we show that such misfires can occur when Claude recognizes a name but doesn't know anything else about that person. In cases like this, the “known entity” feature might still activate, and then suppress the default \"don't know\" feature—in this case incorrectly. Once the model has decided that it needs to answer the question, it proceeds to confabulate: to generate a plausible—but unfortunately untrue—response.JailbreaksJailbreaks are prompting strategies that aim to circumvent safety guardrails to get models to produce outputs that an AI’s developer did not intend for it to produce—and which are sometimes harmful. We studied a jailbreak that tricks the model into producing output about making bombs. There are many jailbreaking techniques, but in this example the specific method involves having the model decipher a hidden code, putting together the first letters of each word in the sentence \"Babies Outlive Mustard Block\" (B-O-M-B), and then acting on that information. This is sufficiently confusing for the model that it’s tricked into producing an output that it never would have otherwise.Claude begins to give bomb-making instructions after being tricked into saying \"BOMB\".Why is this so confusing for the model? Why does it continue to write the sentence, producing bomb-making instructions?We find that this is partially caused by a tension between grammatical coherence and safety mechanisms. Once Claude begins a sentence, many features “pressure” it to maintain grammatical and semantic coherence, and continue a sentence to its conclusion. This is even the case when it detects that it really should refuse.In our case study, after the model had unwittingly spelled out \"BOMB\" and begun providing instructions, we observed that its subsequent output was influenced by features promoting correct grammar and self-consistency. These features would ordinarily be very helpful, but in this case became the model’s Achilles’ Heel.The model only managed to pivot to refusal after completing a grammatically coherent sentence (and thus having satisfied the pressure from the features that push it towards coherence). It uses the new sentence as an opportunity to give the kind of refusal it failed to give previously: \"However, I cannot provide detailed instructions...\".The lifetime of a jailbreak: Claude is prompted in such a way as to trick it into talking about bombs, and begins to do so, but reaches the termination of a grammatically-valid sentence and refuses.A description of our new interpretability methods can be found in our first paper, \"Circuit tracing: Revealing computational graphs in language models\". Many more details of all of the above case studies are provided in our second paper, \"On the biology of a large language model\".Work with usIf you are interested in working with us to help interpret and improve AI models, we have open roles on our team and we’d love for you to apply. We’re looking for Research Scientists and Research Engineers.",
    "summary": {
      "en": "The text discusses recent advancements in understanding how large language models, like Claude, think and operate. Unlike traditional programming, these models learn from vast amounts of data and develop their own strategies, making their internal processes hard to interpret. Understanding these processes is crucial for ensuring that AI behaves as intended.\n\nKey points include:\n\n1. **Interpretability Efforts**: Researchers are developing tools to analyze how models work internally, similar to how neuroscientists study the brain.\n\n2. **Findings from Recent Research**:\n   - Claude demonstrates a shared conceptual understanding across multiple languages.\n   - The model can plan responses, such as rhyming lines in poetry, indicating it thinks ahead rather than just predicting the next word.\n   - Sometimes, Claude fabricates arguments to align with user hints rather than providing accurate reasoning.\n\n3. **Surprising Discoveries**: Researchers found that Claude has unexpected abilities, like planning ahead and using different strategies for tasks, such as mental math.\n\n4. **Limitations**: Current methods only uncover a small part of the model’s computations and require significant human effort to analyze.\n\n5. **Importance of Transparency**: Understanding these mechanisms is vital for ensuring AI systems are reliable and aligned with human values, especially as AI capabilities grow.\n\n6. **Real-World Applications**: Insights from this research could benefit other fields like medical imaging and genomics.\n\nOverall, the ongoing research aims to enhance AI transparency and reliability, providing a clearer understanding of how these complex models function.",
      "ko": "최근 대형 언어 모델인 클로드가 어떻게 사고하고 작동하는지에 대한 이해가 발전하고 있다. 전통적인 프로그래밍과는 달리, 이러한 모델은 방대한 양의 데이터를 통해 학습하고 스스로 전략을 개발하기 때문에 내부 프로세스를 해석하기가 어렵다. 이러한 과정을 이해하는 것은 인공지능이 의도한 대로 행동하도록 보장하는 데 매우 중요하다.\n\n연구자들은 모델의 내부 작동 방식을 분석하기 위한 도구를 개발하고 있다. 이는 신경과학자들이 뇌를 연구하는 방식과 유사하다. 최근 연구 결과에 따르면, 클로드는 여러 언어 간에 공통된 개념적 이해를 보여준다. 이 모델은 시의 운율을 맞추는 것과 같이 응답을 계획할 수 있으며, 이는 단순히 다음 단어를 예측하는 것이 아니라 미리 생각한다는 것을 나타낸다. 때때로 클로드는 사용자 힌트에 맞추기 위해 정확한 이유를 제공하기보다는 주장을 만들어내기도 한다.\n\n연구자들은 클로드가 예기치 않은 능력을 가지고 있다는 사실을 발견했다. 예를 들어, 미리 계획을 세우고 정신적인 수학 문제를 해결하는 데 다양한 전략을 사용하는 능력이 있다. 그러나 현재의 방법은 모델의 계산 과정 중 일부만 드러내며, 이를 분석하는 데 상당한 인적 노력이 필요하다.\n\n이러한 메커니즘을 이해하는 것은 인공지능 시스템이 신뢰할 수 있고 인간의 가치와 일치하도록 보장하는 데 필수적이다. 특히 인공지능의 능력이 증가함에 따라 그 중요성이 더욱 커진다. 이 연구에서 얻은 통찰은 의료 영상이나 유전체학과 같은 다른 분야에도 도움이 될 수 있다.\n\n전반적으로 진행 중인 연구는 인공지능의 투명성과 신뢰성을 높이고, 이러한 복잡한 모델이 어떻게 작동하는지에 대한 명확한 이해를 제공하는 것을 목표로 하고 있다.",
      "ja": null
    }
  },
  {
    "id": "212a03a3801f11de",
    "title": {
      "en": "I tried making artificial sunlight at home",
      "ko": "인공 햇빛 만들기 도전!",
      "ja": null
    },
    "type": "story",
    "url": "https://victorpoughon.fr/i-tried-making-artificial-sunlight-at-home/",
    "score": 158,
    "by": "fouronnes3",
    "time": 1743104968,
    "content": "I tried making artificial sunlight at home\n\n                    27 Mar, 2025\n\n    Some time ago, I saw this video by DIY Perks where they make artificial sunlight at home with a 500W LED and a gigantic (1.2m) parabolic reflector. I've been fascinated by this project ever since, and I wanted my own.\nOver the past year or so, I finally took the time to work on a similar project, but I had the idea for a different design. The issue with the parabolic reflector is that it takes a huge amount of space. Could I do something similar, but with a less bulky design? This is the story of my first attempt at this project - version 1 so to speak. Perhaps there will be a version 2 in the future. Enjoy the read!\n\nMy idea - as others have had I'm sure - was to use an array of lenses laid out as a grid. Then, instead of a single light source, I would use a grid array of multiple LEDs, one per lens. In my mind, this would have two major advantages:\n\nLess bulky. The size of the device would be determined by the focal length of the individual lens elements, and because each would be small, the focal length could be small also, while maintaining a decent f number.\nEasier thermal management. Multiple light sources could be regular low power LEDs which wouldn't need special cooling. There would just be a lot of them, spread out over the entire device surface.\n\nOver the course of this project, I also intended to teach myself some manufacturing and 3D design, as I don't have any experience doing any of this. My background is software, and as you'll see I took a very software heavy approach to this. It was all a long learning journey for me, but in the end I used:\n\nMostly build123d for CAD modeling, with some FreeCAD for final assembly checks and some experiments here and there - including with the cool OpticsWorkbench.\nKiCad for PCB design.\nCustom python code for simulating light and optimizing the optical system. (This custom code eventually became an entire open-source project for optimization-based optical design)\nJLCPCB for printing and assembling PCBs, and for manufacturing aluminum and plastic parts with their CNC service.\n\nTL;DR: I did it! Here is the finished device sitting on my desk today, at night:\n\nAnd here it is during the day (much less impressive!)\n\nBeware it's kinda hard to take good pictures of it, and I don't have the best photo gear. Here's also a video: (at night)\n\n  Your browser does not support the video tag.\n\nKinda cool that you can see a lens flare effect in the shape of the lens grid array.\nTechnical specsMechanical:\n\nLens square side length: 30mm\nEffective Focal length: 55mm\nArray size: 6x6 = 36 LEDs\nTotal size: 180x180mm\n\nParts:\n\nLenses: 1 biconvex lens array, 1 plano-convex lens array - custom made out of PMMA acrylic, CNC fabrication with vapor polish finish @ JLCCNC\nLEDs: LUXEON 2835 3V -- Ref: 2835HE. CRI: 95+, color temp: 4000K, 65mA.\nPCBs: Custom design\nMounting hardware: custom design - aluminium 60601 for the CNC parts and mate black resin for the 3D printed parts\nRayleigh diffuser: waterproof printing inkjet film\n\nGeneral design and sizingTo create artificial sunlight, you need four ingredients:\n\nParallel light rays. The sun is so far away that light rays emitted from a point on the surface of the sun reach us essentially parallel. This is not to say that all light rays coming from the sun are parallel, as it still has a 0.5 deg apparent angular size. But they need to be pretty straight. Any light coming from an artificial light source like an LED will be going in all directions, so some optics is required.\nHigh color quality. A good indicator to look for on a datasheet is the color rendering index (CRI). 95+ is recommended to achieve a good effect. I'm sure there's more color science you could get into, but CRI is a great start for off the shelf parts.\nRayleigh scattering, or an imitation of it.\nA LOT of power.\n\nLight intensity is the most important sizing constraint, so let's look at it first. Now, the sun is very bright. Like, ridiculously bright: around 100,000 lux. To achieve this with LEDs is by no means impossible, but it's a challenge. For this first version, I thought that targetting 10,000 lux would be quite enough because it would reduce the power consumption a lot for a first prototype, and also brightness perception is logarithmic. So one tenth of the intensity is really, perceptually, almost the same as full brightness. (In the end, I estimate my design only effectively achieved something between 1000 and 10000 lux).\nThe general grid based design of this project really has two variables:\n\nthe individual LED light output, in lumens\nthe individual lens surface area in mm²\n\nAfter some research, I think values between 30 to 130 lumens are typical for high CRI surface mount LEDs. So, assuming this is what we are working with, what is the required lens size to achieve the brightness of the sun?\nWe have to assume some non perfect efficiency for collimating the light. This will never be 100%, and in fact may be quite low if the focal length is high, because a lot of the light will be hitting the side walls instead of reaching the lens. The lens itself will also be absorbing some light. So taking a wild guess of 0.5 for the overall optical efficiency, and taking three lumens value of 30, 80 and 130, we get this plot:\n\nWith that in mind, I selected 30mm as my lens square side length. Presumably, this would be small enough to achieve some effect, but not too small to make the lenses too hard to make.\nLensesFocal length, and the lenses shape in general, is the next design consideration. The goal is to have perfectly parallel light rays. In theory, with a perfect point source and a perfect lens this is easy. Put the light source at the lens focal length, you're done. In practice, a lot of things make it harder to achieve with a lens. (This is where the parabolic reflector design is superior to a lens).\n\nA LED is not a point source\nA lens will not have perfect optical performance (i.e. aberrations)\nMechanical reality of the device means that positioning and orientation will not be perfect\nA LED radiation pattern is not isotropic, meaning intensity will be greater at the lens center\n\nThis is the radiation pattern characteristics diagram from my LED datasheet:\n\nI wrote some custom python code to simulate the optical system I had in mind, and find the best lens shape using numerical optimization. (This code eventually became an open-source project: torchlensmaker) After a lot of experimentation, I settled on a 2 lens design:\n\nLens 1: Biconvex parabolic lens\nLens 2: Planoconvex parabolic lens\n\nThe effective focal length of this two lens system is about 55mm. Focal length is a key design parameter, and here I feel like more experimentation is needed. It's a big tradeoff consideration and has a huge impact on the system design. It impacts:\n\nThe curvature of the lens surface, which is a key manufacturing point (you want to minimize curvature for manufacturing, which means maximizing focal length)\nThe optical efficiency of the system due to the led radiance pattern (here you want to minimize focal length, to gather more of the emitted light)\nThe device thickness (here I wanted a not-too-thick device, so to minimize focal length also)\n\nI used a two lens system mostly to reduce the surface curvature of the lens arrays. This reduces the manufacturing cost by a lot. High curvature lenses are more expensive in general, and this grid array design means that a high curvature lens will create sort of \"valleys\" in between the lenses. Because I was targetting CNC manufacturing, this is to be minimized to get a design that's even possible to machine.\nThis is the optical simulation I had at the time I finalized the design and ordered the lenses. (Since then my simulation code has improved and I could likely do much better modeling today using the latest version of torchlensmaker):\n\nWith some custom build123d code I was able to make the two lenses 3D models by stacking the lenses in a grid pattern and adding edges for mounting:\n\n  <p>Your browser does not support iframes.</p>\n\n  <p>Your browser does not support iframes.</p>\n\nWhat's really cool using build123d for 3D modeling, is that I can just change a python variable to change the size of the array, of the thickness of the lens, of anything else really. It's all parametric out of the box because it's regular Python code! This makes exploring the design space very efficient. I've never done 3D modeling any other way, but I can't imagine ever not having the power of programming with me if I ever do it again!\nI had the lenses manufactured out of PMMA acrylic at JLC with a vapor polish finish. Total cost for the lenses was about 55€ which is really not bad!\nOne of the two main lens array, built by JLCCNC:\n\nLEDsI really wanted to use the 3030 G04 from YUJILEDS, but it's only sold on 5000 units reels that cost $1000 a piece... maybe for version 2 I will upgrade to those. For version 1, I settled on LUXEON 2835 3V. They are about 3 times less bright than the YUJILED, but they have good color rendering and the SMD package I was looking for. And importantly, the minimum order quantity was only 50 at JLC global sourcing.\nIn the version 1 design, the grid is 6x6 which means 36 LEDs total.\nPCBsI designed a custom PCB with KiCAD. Each PCB holds 6 LEDs which are laid out as 2 segments of a 12V led strip in parallel. This allows to use a standard wall plug 12V power supply.\n\nThe mechanical role of the PCB is very important in this design. Not only does it distribute power to the LEDs and regulate current, it also precisely positions the LEDs at the lens focal point. For this, exporting the PCB 3D model and importing it into FreeCAD was very useful to check that everything fits together: the PCB in the aluminum support baseplate, the holes on the light hoods, etc. My Python code exported the precise LED coordinates which I could input into KiCad's layout editor.\nI had the PCB printed and the components assembled by JLCPCB. It's very very cool to design an electronic board on your computer and get it fully assembled in the mail a few weeks later - no soldering required! (for this step anyway).\n\nMechanical mounting partsTo mount everything together I designed 3 parts:\n\nA baseplate, to hold the PCBs and the side walls. The PCBs are fitted below the baseplate, and light goes through holes drilled into the baseplate. There are also partial holes to allow for the thickness of the SMD resistors mounted on top of the PCBs, and finally two mounting holes per PCB. This is why it has so many holes :)\n\n  <p>Your browser does not support iframes.</p>\n\nSide walls to hold the lenses using grooves in which to insert them, and a larger groove to secure in the baseplate. The baseplate side holes are threaded to support M2 screws securing the base of the walls. Again, JLCCNC did the drilling and threading of the holes at a great price.\n\n  <p>Your browser does not support iframes.</p>\n\nLight hoods, a rectangle block with rectangular holes. It sits on top of the PCB to shape the light coming from each LED into a cone (or really a four sided pyramid). This is to make sure light from a given LED only reaches its matching lens on the lens array, and no other. Bleed light is inevitable, but at least this prevents direct leakage.\n\n  <p>Your browser does not support iframes.</p>\n\nThe hoods were 3D printed out of black resin, the walls and baseplate were CNC cut out of Aluminum 60601.\nI'm not a mechanical engineer so this process was... trial and error. Still the result is working so I'm quite happy with that. For a possible version 2, there's a lot I'll change in the mechanical design. But apart from the one design flaw I was able to fix manually with a drill (more on that below), everything fit together quite well on the first try.\nRayleigh scatteringThe final ingredient is Rayleigh scattering. This is the physical phenomenon that makes the sky look blue, and it's important to achieve a convincing effect. In the DIY Perks video that inspired this project, they used a home made liquid solution with suspended particles of the correct size for Rayleigh scattering. Not super practical and I really wanted to find another solution (get it?). Thankfully, some time after the original video, someone on the diyperks forum discovered that inkjet print film achieves a very similar effect. A quick trip to a local office supply store was all I needed here! Amazing discovery.\nI didn't anticipate this step during the initial design phase, so the film is simply cut to the correct size and secured with black electrical tape.\nAssemblyAfter a few weeks of design work, and another few weeks of waiting for the parts to arrive, it was finally time for assembly!\nOn top of the individual 3D models made with build123d, I had a final assembly FreeCAD model with all parts fitted together, including the lenses:\n\nNote the green brackets that I initially planned to use. When actually assembling the walls to the baseplate, the solidity of the formed box was very high, I decided to drop the brackets entirely. This is why some extra unused holes remain on the side walls.\nThis is all the parts just after unboxing (excluding the inkjet film, solder tin, screws, power supply, wiring, electrical tape):\n\nThe only real design flaw was insufficient width of the grooves that hold the lenses. The lenses have an edge thickness of 1.2mm, which I had intended to fit into a 1.22mm groove. Turns out this was not enough, probably due to a combination of manufacturing tolerance and additional thickness added by the anodizing black matte surface finish of the aluminum part. The lenses didn't fit into the grooves!\nI don't have a very advanced tools at home, so my best solution to this was making the existing grooves wider by hand using a power drill. I bought a 1.5mm metal drill bit and achieved a decent result by doing 4 to 5 passes per groove. This took about 2-3h in total because I had to move the bit quite slow and could only machine about 1/4th of each groove depth at a time by moving the drill bit slowly accross, and there are 8 grooves total.\n\nHere's some more pictures of assembly below.\nThe back side after soldering wires to the PCB power pins and a socket for the 12V power supply. The PCBs and hood pieces share a common mounting hole so only two screws per PCB-hood pair are used.\n\nThe front side of the baseplate + PCB + hoods assembly, but without the lenses, powered on. Don't look at it directly :)\n\nIt's interesting to note that in the picture above, all of the light you can see from the LEDs is actually \"bleed light\" and not useful light. None of the light visible above is the light that's intended to go into the lens and produce the sunlight effect.\nTesting with partial assembly of the walls and only 1 out of the 2 lenses:\n\nTesting the inkjet film layers with an avocado as a subject. I settled on using two layers of the inkjet film for the final build:\n\nCostOverall I spent around 1000€ on this project. But this includes cost of tools I was missing, prototype parts that I had manufactured but discarded, bulk orders for parts like LEDs and PCBs which had a minium order quantity above what I need for 1 unit, and various supplies like screws, etc. The actual raw cost of parts only, without shipping, to build the final unit is hard to estimate. But I would say around 300€. The most expensive parts are the CNC parts (PMMA lenses and the aluminum baseplate and walls) accounting for about 2/3rd of the total price. The rest (PCBs, assembly service, LEDs, 3D printed plastic parts) was quite cheap.\nConclusionAs I write this the final piece is sitting on my desk and producing a pleasant soft white glow. It's definitely nice, and I'm very proud of the result - especially because this was by far the biggest build project I have ever done.\n\nThanks to this project, I've learned a ton about PCB design, electronics and CNC manufacturing and optics. I even got so far down the side quest of learning optics that I started an open-source python project for modeling geometric optics.\nSo, is it convincing as artificial sunlight?\nMy honest answer to that is: partially. The geometric effect of the light source appearing at infinity works. As I pan and tilt my head from side to side, the illusion of light coming from way far behind the object is 100% a success. On top of that, if you look at it while moving your head into the light beam, my eyes get surprised - almost hurt - by the sudden intensity jump. This indicates that collimation is good and you can sort of see it in the video at the start of this post.\nHowever it's apparent that it's simply too weak. Don't get me wrong, it's still bright. I can't look at it directly without sunglasses, and honestly it's really hard to take a good picture of it because the contrast between the light it emits and the outside of it is very high.\nAnother downside is that I can definitely make out the grid of lenses, as the intensity pattern clearly reveals the grid shape. This is quite a minor downside and not really unpleasant, and I'm sure it could be improved upon.\nIf I were to ever work on a version 2, I would focus on:\n\nMore power. My feeling is the light output needs to be 3 to 5 times stronger to get any closer to a convincing effect, and it's not crazy to aim for as much as 10x brighter than this prototype.\nMore surface area. This prototype is 18cm x 18cm. So you only really get the effect if you are able to sit with the produced straight beam of light, which is quite narrow to resemble any kind of \"fake window\". A future version would need to be 2 to 4 times wider in my opinion.\nBetter optical design. I still think a refraction based design is possible, but it requires very precise optical design and mechanical tolerances. My feeling is that a refraction based design, especially as a grid, is very sensitive to positioning and orientation of parts. I lack mechanical engineering skills in this area.\n\nHowever there are some really encouraging things that I really like about this grid based, refractive design:\n\nIt's scalable. If I had built 4 identical items, I could literally stack them on top of each other and get more surface area. The \"bezels\" would be only 5% of the total light emitting area, and I'm sure this could be lowered. I also like that the inner design calls for repeated elements, as this introduces some economy of scale, even at the prototype level. The only part that's not trivially scalable is the lens grid. Maybe it could be injection molded for very large scale production, or for medium scale you could come up with a way to tile multiple lens grids into a larger overall grid pattern, adding some thin bezels for mounting.\nIt's compact. The total size is 19cm x 19cm x 9cm. This is quite compact for a 5cm focal length and an effective lighting area of 18cm x 18cm. Reflective designs like the DIYPerks video or commercial products like CoeLux do not achieve this form factor.\nThermal management is better by design. This is not really something I got into for this design, as it's quite underpowered. The whole thing runs comfortably on a 12V / 3A wall brick power supply. But this design offers great margin for scaling up because there isn't a single light source to cool down, but a number of LEDs proportional to the surface area. I suspect the main thermal issue when scaling up would be the cooling of the power supply itself, not of the lamp.\n\nAs final thoughts, let me talk about the software heavy approach I had for this project. It's awesome. If I was starting a manufacturing company today, I would do it all code based. PCBs, 3D models, assembly, testing... I want code everywhere. The power of changing a parameter and having the entire design updated with a single script it so good. Run a script and get all the production data including GERBERs, BOM, 3D models, mechanical schematics, technical diagrams, automated tolerance and electrical checks... absolutely no manual steps between changing a design parameter and ready to send a new order to manufacturing. The PCB and CAD space is even evolving to use proper CI/CD tools which is really exciting.\nI don't know if I'll ever have the time to work on version 2 of this project, but it was great fun anyway! And now I have a cool unique lamp. Thank you for reading!\n\n            19\n\n    document.querySelector('#upvote-form').addEventListener('submit', (e) => {\n        e.preventDefault();\n        const form = e.target;\n        fetch(form.action, {\n            method: form.method,\n            body: new FormData(form),\n        });\n        const button = form.querySelector('button')\n        button.disabled = true\n        button.style.color = \"salmon\"\n        const upvoteCount = document.querySelector('.upvote-count')\n        upvoteCount.innerHTML = `${(parseInt(upvoteCount.innerHTML.split(\" \")[0]) + 1)}`\n    });",
    "summary": {
      "en": "**Summary:**\n\nThe author attempted to create artificial sunlight at home, inspired by a DIY project using a large LED and parabolic reflector. They aimed for a more compact design using a grid of lenses and multiple LEDs. The benefits of this approach included a smaller size and better thermal management since low-power LEDs could be used.\n\nThe project involved learning about manufacturing and 3D design, using tools like CAD software, PCB design programs, and custom Python code for light simulation. After a year of development, the author successfully built the device, which features a 6x6 array of LEDs and two types of lenses to achieve parallel light rays.\n\nKey specifications include:\n- Lens size: 30mm square\n- Effective focal length: 55mm\n- Total device size: 180mm x 180mm\n\nThe author spent around 1000€ on the project, with the actual parts costing about 300€. While the final product produces a pleasant light, it is not yet as bright as natural sunlight. The author plans to improve future versions by increasing power, surface area, and optical design precision. They found the coding-heavy approach to design to be very effective for future manufacturing projects. Overall, they enjoyed the learning experience and are proud of their unique lamp.",
      "ko": "저자는 대형 LED와 포물선 반사를 이용한 DIY 프로젝트에서 영감을 받아 집에서 인공 태양광을 만들려고 시도했습니다. 더 컴팩트한 디자인을 위해 렌즈 그리드와 여러 개의 LED를 사용하기로 했습니다. 이 접근 방식의 장점은 크기가 작고 열 관리를 더 잘할 수 있다는 점으로, 저전력 LED를 사용할 수 있었습니다.\n\n이 프로젝트는 제조와 3D 디자인에 대한 학습을 포함했으며, CAD 소프트웨어, PCB 설계 프로그램, 조명 시뮬레이션을 위한 맞춤형 파이썬 코드와 같은 도구를 사용했습니다. 1년의 개발 끝에 저자는 6x6 배열의 LED와 평행한 빛을 생성하기 위한 두 가지 유형의 렌즈를 갖춘 장치를 성공적으로 만들었습니다.\n\n주요 사양은 다음과 같습니다. 렌즈 크기는 30mm 정사각형, 유효 초점 거리는 55mm, 전체 장치 크기는 180mm x 180mm입니다.\n\n저자는 이 프로젝트에 약 1000유로를 지출했으며, 실제 부품 비용은 약 300유로였습니다. 최종 제품은 쾌적한 빛을 생성하지만, 자연 태양광만큼 밝지는 않습니다. 저자는 향후 버전에서 전력, 표면적, 광학 설계의 정밀도를 높여 개선할 계획입니다. 코딩 중심의 디자인 접근 방식이 향후 제조 프로젝트에 매우 효과적이라는 것을 알게 되었습니다. 전반적으로 저자는 학습 경험을 즐겼고, 자신만의 독특한 램프에 자부심을 느끼고 있습니다.",
      "ja": null
    }
  },
  {
    "id": "14f7ff8f6ae0bd5e",
    "title": {
      "en": "Learning Theory from First Principles [pdf]",
      "ko": "기초부터 배우는 이론",
      "ja": null
    },
    "type": "story",
    "url": "https://www.di.ens.fr/~fbach/ltfp_book.pdf",
    "score": 24,
    "by": "Anon84",
    "time": 1743108313,
    "content": "Learning Theory from First Principles  August 27, 2024  Francis Bach  francis.bach@inria.fr  Copyright   in   this   Work   has   been   licensed   exclusively   to   The   MIT   Press,  http://mitpress.mit.edu ,   which   will   be   releasing   the   final   version   to   the   public   in 2024.   All inquiries regarding rights should be addressed to The MIT Press, Rights and Permissions Department.\n\nContents  Preface   xi  I   Preliminaries   1  1   Mathematical Preliminaries   3  1.1   Linear Algebra and Differentiable Calculus   . . . . . . . . . . . . . . . . .   3  1.1.1   Minimization of Quadratic Forms   . . . . . . . . . . . . . . . . . . .   3  1.1.2   Inverting a 2   ×   2 Matrix   . . . . . . . . . . . . . . . . . . . . . . . .   4  1.1.3   Inverting Matrices Defined by Blocks, Matrix Inversion Lemma   . .   4  1.1.4   Eigenvalue and Singular Value Decomposition   . . . . . . . . . . . .   6  1.1.5   Differential Calculus   . . . . . . . . . . . . . . . . . . . . . . . . . .   7  1.2   Concentration Inequalities   . . . . . . . . . . . . . . . . . . . . . . . . . . .   7  1.2.1   Hoeffding’s Inequality   . . . . . . . . . . . . . . . . . . . . . . . . .   10  1.2.2   McDiarmid’s Inequality   . . . . . . . . . . . . . . . . . . . . . . . .   13  1.2.3   Bernstein’s Inequality ( \u0007 )   . . . . . . . . . . . . . . . . . . . . . .   14  1.2.4   Expectation of the Maximum   . . . . . . . . . . . . . . . . . . . . .   16  1.2.5   Estimation of Expectations through Quadrature ( \u0007\u0007 )   . . . . . . .   18  1.2.6   Concentration Inequalities for Random Matrices ( \u0007\u0007 )   . . . . . . .   19  2   Introduction to Supervised Learning   21  2.1   From Training Data to Predictions   . . . . . . . . . . . . . . . . . . . . . .   22  2.2   Decision Theory   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   25  2.2.1   Supervised Learning Problems and Loss Functions   . . . . . . . . .   25  2.2.2   Risks   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   27  2.2.3   Bayes Risk and Bayes Predictor   . . . . . . . . . . . . . . . . . . . .   28  2.3   Learning from Data   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   30  2.3.1   Local Averaging   . . . . . . . . . . . . . . . . . . . . . . . . . . . .   31  2.3.2   Empirical Risk Minimization   . . . . . . . . . . . . . . . . . . . . .   32  2.4   Statistical Learning Theory   . . . . . . . . . . . . . . . . . . . . . . . . . .   36 iii\n\niv   CONTENTS  2.4.1   Measures of Performance   . . . . . . . . . . . . . . . . . . . . . . .   36  2.4.2   Notions of Consistency over Classes of Problems   . . . . . . . . . .   36  2.5   “No Free Lunch” Theorems ( \u0007 )   . . . . . . . . . . . . . . . . . . . . . . . .   38  2.6   Quest for Adaptivity   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   39  2.7   Beyond Supervised Learning   . . . . . . . . . . . . . . . . . . . . . . . . . .   40  2.8   Summary–Book Outline   . . . . . . . . . . . . . . . . . . . . . . . . . . . .   41  3   Linear Least-Squares Regression   45  3.1   Introduction   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   45  3.2   Least-Squares Framework   . . . . . . . . . . . . . . . . . . . . . . . . . . .   46  3.3   Ordinary Least-Squares Estimator   . . . . . . . . . . . . . . . . . . . . . .   47  3.3.1   Closed-Form Solution   . . . . . . . . . . . . . . . . . . . . . . . . .   47  3.3.2   Geometric Interpretation   . . . . . . . . . . . . . . . . . . . . . . .   48  3.3.3   Numerical Resolution   . . . . . . . . . . . . . . . . . . . . . . . . .   49  3.4   Statistical Analysis of Ordinary Least-Squares   . . . . . . . . . . . . . . . .   49  3.5   Fixed Design Setting   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   50  3.5.1   Statistical Properties of the OLS Estimator   . . . . . . . . . . . . .   52  3.5.2   Experiments   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   54  3.6   Ridge Least-Squares Regression   . . . . . . . . . . . . . . . . . . . . . . . .   56  3.7   Lower Bound ( \u0007 )   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   60  3.8   Random Design Analysis   . . . . . . . . . . . . . . . . . . . . . . . . . . . .   63  3.8.1   Gaussian Designs   . . . . . . . . . . . . . . . . . . . . . . . . . . . .   64  3.8.2   General Designs ( \u0007\u0007 )   . . . . . . . . . . . . . . . . . . . . . . . . .   65  3.9   Principal Component Analysis ( \u0007 )   . . . . . . . . . . . . . . . . . . . . . .   66  3.10 Conclusion   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   68  II   Generalization Bounds for Learning Algorithms   69  4   Empirical Risk Minimization   71  4.1   Convexification of the Risk   . . . . . . . . . . . . . . . . . . . . . . . . . .   72  4.1.1   Convex Surrogates   . . . . . . . . . . . . . . . . . . . . . . . . . . .   73  4.1.2   Geometric Interpretation of the Support Vector Machine ( \u0007 )   . . .   74  4.1.3   Conditional Φ-risk and Classification Calibration ( \u0007 )   . . . . . . . .   76  4.1.4   Relation between Risk and Φ-risk ( \u0007\u0007 )   . . . . . . . . . . . . . . .   79  4.2   Risk Minimization Decomposition   . . . . . . . . . . . . . . . . . . . . . .   84  4.3   Approximation Error   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   84  4.4   Estimation Error   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   85  4.4.1   Application of McDiarmid’s Inequality   . . . . . . . . . . . . . . . .   86  4.4.2   Easy Case I: Quadratic Functions   . . . . . . . . . . . . . . . . . . .   87  4.4.3   Easy Case II: Finite Number of Models   . . . . . . . . . . . . . . .   88  4.4.4   Beyond Finitely Many Models through Covering Numbers ( \u0007 )   . .   89  4.5   Rademacher Complexity   . . . . . . . . . . . . . . . . . . . . . . . . . . . .   91  4.5.1   Symmetrization   . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   92\n\nCONTENTS   v  4.5.2   Lipschitz-Continuous Losses   . . . . . . . . . . . . . . . . . . . . . .   94  4.5.3   Ball-Constrained Linear Predictions   . . . . . . . . . . . . . . . . .   96  4.5.4   Putting Things Together (Linear Predictions)   . . . . . . . . . . . .   97  4.5.5   From Constrained to Regularized Estimation ( \u0007 )   . . . . . . . . .   98  4.5.6   Extensions and Improvements   . . . . . . . . . . . . . . . . . . . . .   102  4.6   Model Selection ( \u0007 )   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   103  4.6.1   Structural Risk Minimization ( \u0007 )   . . . . . . . . . . . . . . . . . . .   104  4.6.2   Selection Based on Validation Set ( \u0007 )   . . . . . . . . . . . . . . . .   104  4.7   Relation with Asymptotic Statistics ( \u0007 )   . . . . . . . . . . . . . . . . . . .   105  4.8   Summary   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   107  5   Optimization for Machine Learning   109  5.1   Optimization in Machine Learning   . . . . . . . . . . . . . . . . . . . . . .   109  5.2   Gradient Descent   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   111  5.2.1   Simplest Analysis: Ordinary Least-Squares   . . . . . . . . . . . . .   112  5.2.2   Convex Functions and Their Properties   . . . . . . . . . . . . . . .   116  5.2.3   Analysis of Gradient Descent for Strongly Convex and Smooth Functions   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   119  5.2.4   Analysis of Gradient Descent for Convex and Smooth Functions ( \u0007 )   124  5.2.5   Beyond Gradient Descent ( \u0007 )   . . . . . . . . . . . . . . . . . . . .   126  5.2.6   Nonconvex Objective Functions ( \u0007 )   . . . . . . . . . . . . . . . . .   129  5.3   Gradient Methods on Nonsmooth Problems   . . . . . . . . . . . . . . . . .   130  5.4   Stochastic Gradient Descent   . . . . . . . . . . . . . . . . . . . . . . . . . .   134  5.4.1   Strongly Convex Problems ( \u0007 )   . . . . . . . . . . . . . . . . . . . .   139  5.4.2   Adaptive Methods ( \u0007 )   . . . . . . . . . . . . . . . . . . . . . . . . .   141  5.4.3   Bias-Variance Trade-offs for Least-Squares ( \u0007 )   . . . . . . . . . . .   143  5.4.4   Variance Reduction ( \u0007 )   . . . . . . . . . . . . . . . . . . . . . . . .   146  5.5   Conclusion   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   151  6   Local Averaging Methods   155  6.1   Introduction   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   155  6.2   Local Averaging Methods   . . . . . . . . . . . . . . . . . . . . . . . . . . .   157  6.2.1   Linear Estimators   . . . . . . . . . . . . . . . . . . . . . . . . . . .   157  6.2.2   Partition Estimators   . . . . . . . . . . . . . . . . . . . . . . . . . .   158  6.2.3   Nearest-Neighbors   . . . . . . . . . . . . . . . . . . . . . . . . . . .   160  6.2.4   Nadaraya-Watson Estimator (aka Kernel Regression) ( \u0007 )   . . . . .   162  6.3   Generic Simplest Consistency Analysis   . . . . . . . . . . . . . . . . . . . .   163  6.3.1   Fixed Partition   . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   165  6.3.2   k -nearest Neighbor   . . . . . . . . . . . . . . . . . . . . . . . . . . .   168  6.3.3   Kernel Regression (Nadaraya-Watson) ( \u0007 )   . . . . . . . . . . . . .   170  6.4   Universal Consistency ( \u0007 )   . . . . . . . . . . . . . . . . . . . . . . . . . . .   174  6.5   Adaptivity ( \u0007\u0007 )   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   177  6.6   Conclusion   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   178\n\nvi   CONTENTS  7   Kernel Methods   179  7.1   Introduction   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   180  7.2   Representer Theorem   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   181  7.3   Kernels   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   183  7.3.1   Linear and Polynomial Kernels   . . . . . . . . . . . . . . . . . . . .   186  7.3.2   Translation-Invariant Kernels on [0 ,   1]   . . . . . . . . . . . . . . . .   187  7.3.3   Translation-Invariant Kernels on   R d   . . . . . . . . . . . . . . . . .   191  7.3.4   Beyond Vectorial Input Spaces ( \u0007 )   . . . . . . . . . . . . . . . . . .   194  7.4   Algorithms   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   196  7.4.1   Representer Theorem   . . . . . . . . . . . . . . . . . . . . . . . . .   196  7.4.2   Column Sampling   . . . . . . . . . . . . . . . . . . . . . . . . . . .   197  7.4.3   Random Features   . . . . . . . . . . . . . . . . . . . . . . . . . . . .   198  7.4.4   Dual Algorithms ( \u0007 )   . . . . . . . . . . . . . . . . . . . . . . . . . .   199  7.4.5   Stochastic Gradient Descent ( \u0007 )   . . . . . . . . . . . . . . . . . . .   200  7.4.6   Kernelization of Linear Algorithms   . . . . . . . . . . . . . . . . . .   201  7.5   Generalization Guarantees–Lipschitz-continuous Losses   . . . . . . . . . . .   202  7.5.1   Risk Decomposition   . . . . . . . . . . . . . . . . . . . . . . . . . .   203  7.5.2   Approximation Error for Translation-Invariant Kernels on   R d   . . .   205  7.6   Theoretical Analysis of Ridge Regression ( \u0007 )   . . . . . . . . . . . . . . . .   208  7.6.1   Kernel Ridge Regression as a Linear Estimator   . . . . . . . . . . .   208  7.6.2   Bias and Variance Decomposition ( \u0007 )   . . . . . . . . . . . . . . . .   209  7.6.3   Relating Empirical and Population Covariance Operators   . . . . .   212  7.6.4   Analysis for Well-Specified Problems ( \u0007 )   . . . . . . . . . . . . . . .   214  7.6.5   Analysis beyond Well-Specified Problems ( \u0007 )   . . . . . . . . . . . .   216  7.6.6   Balancing Bias and Variance ( \u0007 )   . . . . . . . . . . . . . . . . . . .   217  7.7   Experiments   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   218  7.8   Conclusion   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   220  8   Sparse Methods   221  8.1   Introduction   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   221  8.1.1   Dedicated Proof Technique for Constrained Least-Squares   . . . . .   223  8.1.2   Probabilistic and Combinatorial Lemmas   . . . . . . . . . . . . . .   224  8.2   Variable Selection by the   ℓ 0 -penalty   . . . . . . . . . . . . . . . . . . . . .   226  8.2.1   Assuming That   k   Is Known   . . . . . . . . . . . . . . . . . . . . . .   226  8.2.2   Sparsity-Adaptive Estimation (Unknown   k ) ( \u0007 )   . . . . . . . . . . .   228  8.3   Variable Selection by   ℓ 1 -regularization   . . . . . . . . . . . . . . . . . . . .   231  8.3.1   Intuition and Algorithms   . . . . . . . . . . . . . . . . . . . . . . .   231  8.3.2   Slow Rates–Random Design   . . . . . . . . . . . . . . . . . . . . . .   234  8.3.3   Slow Rates–Fixed Design (Square Loss)   . . . . . . . . . . . . . . .   236  8.3.4   Fast Rates–Fixed Design ( \u0007 )   . . . . . . . . . . . . . . . . . . . . .   238  8.3.5   Zoo of Conditions ( \u0007\u0007 )   . . . . . . . . . . . . . . . . . . . . . . . .   239  8.3.6   Fast Rates–Random Design ( \u0007 )   . . . . . . . . . . . . . . . . . . . .   241  8.4   Experiments   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   243  8.5   Extensions   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   243\n\nCONTENTS   vii  8.6   Conclusion   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   245  9   Neural Networks   247  9.1   Introduction   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   247  9.2   Single Hidden-Layer Neural Network   . . . . . . . . . . . . . . . . . . . . .   249  9.2.1   Optimization   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   251  9.2.2   Rectified Linear Units and Homogeneity   . . . . . . . . . . . . . . .   253  9.2.3   Estimation Error   . . . . . . . . . . . . . . . . . . . . . . . . . . . .   253  9.3   Approximation Properties   . . . . . . . . . . . . . . . . . . . . . . . . . . .   256  9.3.1   Universal Approximation Property in One Dimension   . . . . . . .   256  9.3.2   Infinitely Many Neurons and the Variation Norm   . . . . . . . . . .   257  9.3.3   Variation Norm in One Dimension   . . . . . . . . . . . . . . . . . .   260  9.3.4   Variation Norm in an Arbitrary Dimension   . . . . . . . . . . . . .   263  9.3.5   Precise Approximation Properties   . . . . . . . . . . . . . . . . . .   265  9.3.6   From the Variation Norm to a Finite Number of Neurons ( \u0007 )   . . .   266  9.4   Generalization Performance for Neural Networks   . . . . . . . . . . . . . .   269  9.5   Relationship with Kernel Methods ( \u0007 )   . . . . . . . . . . . . . . . . . . . .   271  9.5.1   From a Banach Space   F 1   to a Hilbert Space   F 2   ( \u0007 )   . . . . . . . . .   271  9.5.2   Kernel Function ( \u0007\u0007 )   . . . . . . . . . . . . . . . . . . . . . . . . .   273  9.5.3   Upper Bound on RKHS Norm ( \u0007\u0007 )   . . . . . . . . . . . . . . . . .   275  9.6   Experiments   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   277  9.7   Extensions   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   278  9.8   Conclusion   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   279  III   Special Topics   281  10 Ensemble Learning   283  10.1 Averaging/Bagging   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   284  10.1.1   Independent Datasets   . . . . . . . . . . . . . . . . . . . . . . . . .   284  10.1.2   Bagging   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   286  10.2 Random Projections and Averaging   . . . . . . . . . . . . . . . . . . . . .   288  10.2.1   Gaussian Sketching   . . . . . . . . . . . . . . . . . . . . . . . . . . .   290  10.2.2   Random Projections   . . . . . . . . . . . . . . . . . . . . . . . . . .   292  10.3 Boosting   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   298  10.3.1   Problem Setup   . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   298  10.3.2   Incremental Learning   . . . . . . . . . . . . . . . . . . . . . . . . .   301  10.3.3   Matching Pursuit   . . . . . . . . . . . . . . . . . . . . . . . . . . . .   302  10.3.4   Adaboost   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   303  10.3.5   Greedy Algorithm Based on Gradient Boosting   . . . . . . . . . . .   304  10.3.6   Convergence of Expected Risk   . . . . . . . . . . . . . . . . . . . .   308  10.3.7   Experiments   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   310  10.4 Conclusion   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   311\n\nviii   CONTENTS  11 From Online Learning to Bandits   313  11.1 First-Order Online Convex Optimization   . . . . . . . . . . . . . . . . . . .   315  11.1.1   Convex Case   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   316  11.1.2   Strongly Convex Case ( \u0007 )   . . . . . . . . . . . . . . . . . . . . . .   318  11.1.3   Online Mirror Descent ( \u0007 )   . . . . . . . . . . . . . . . . . . . . . . .   319  11.1.4   Lower Bounds ( \u0007\u0007 )   . . . . . . . . . . . . . . . . . . . . . . . . . .   321  11.2 Zeroth-Order Convex Optimization   . . . . . . . . . . . . . . . . . . . . . .   323  11.2.1   Smooth Stochastic Gradient Descent   . . . . . . . . . . . . . . . . .   325  11.2.2   Stochastic Smoothing ( \u0007 )   . . . . . . . . . . . . . . . . . . . . . . .   328  11.2.3   Extensions   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   331  11.3 Multiarmed Bandits   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   331  11.3.1   Need for an Exploration-Exploitation Trade-off   . . . . . . . . . . .   333  11.3.2   “Explore-Then-Commit”   . . . . . . . . . . . . . . . . . . . . . . . .   333  11.3.3   Optimism in the Face of Uncertainty ( \u0007 )   . . . . . . . . . . . . . .   336  11.3.4   Adversarial Bandits ( \u0007 )   . . . . . . . . . . . . . . . . . . . . . . . .   339  11.4 Conclusion   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   341  12 Overparameterized Models   343  12.1 Implicit Bias of Gradient Descent   . . . . . . . . . . . . . . . . . . . . . . .   344  12.1.1   Least-Squares Regression   . . . . . . . . . . . . . . . . . . . . . . .   344  12.1.2   Separable Classification   . . . . . . . . . . . . . . . . . . . . . . . .   346  12.1.3   Beyond Convex Problems ( \u0007 )   . . . . . . . . . . . . . . . . . . . . .   351  12.1.4   Remarks on Implicit Bias   . . . . . . . . . . . . . . . . . . . . . . .   354  12.2 Double Descent   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   355  12.2.1   The Double Descent Phenomenon   . . . . . . . . . . . . . . . . . .   355  12.2.2   Empirical Evidence   . . . . . . . . . . . . . . . . . . . . . . . . . . .   356  12.2.3   Linear Regression with Gaussian Inputs   . . . . . . . . . . . . . . .   358  12.2.4   Linear Regression with Gaussian Projections ( \u0007\u0007 )   . . . . . . . . .   360  12.3 Global Convergence of Gradient Descent   . . . . . . . . . . . . . . . . . . .   365  12.3.1   Mean Field Limits   . . . . . . . . . . . . . . . . . . . . . . . . . . .   365  12.3.2   From Linear Networks to Positive-Definite Matrices   . . . . . . . .   370  12.3.3   Global Convergence for Positive-Definite Matrices   . . . . . . . . .   370  12.3.4   Special Cases   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   374  12.4 Lazy Regime and Neural Tangent Kernels ( \u0007 )   . . . . . . . . . . . . . . . .   375  12.5 Conclusion   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   377  13 Structured Prediction   379  13.1 Multicategory Classification   . . . . . . . . . . . . . . . . . . . . . . . . . .   380  13.1.1   Extension of Classical Convex Surrogates   . . . . . . . . . . . . . .   380  13.1.2   Generalization Bound I: Stochastic Gradient Descent   . . . . . . . .   383  13.1.3   Generalization Bound II: Rademacher Complexities ( \u0007 )   . . . . . .   384  13.2 General Setup and Examples   . . . . . . . . . . . . . . . . . . . . . . . . .   387  13.2.1   Examples   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   387  13.2.2   Structure Encoding Loss Functions   . . . . . . . . . . . . . . . . . .   390\n\nCONTENTS   ix  13.3 Surrogate Methods   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   391  13.3.1   Score Functions and Decoding Step   . . . . . . . . . . . . . . . . . .   392  13.3.2   Fisher Consistency and Calibration Functions   . . . . . . . . . . . .   392  13.3.3   Main Surrogate Frameworks   . . . . . . . . . . . . . . . . . . . . . .   393  13.4 Smooth/Quadratic Surrogates   . . . . . . . . . . . . . . . . . . . . . . . . .   393  13.4.1   Quadratic Surrogate   . . . . . . . . . . . . . . . . . . . . . . . . . .   393  13.4.2   Theoretical Guarantees   . . . . . . . . . . . . . . . . . . . . . . . .   394  13.4.3   Linear Estimators and Decoding Steps   . . . . . . . . . . . . . . . .   395  13.4.4   Smooth Surrogates ( \u0007 )   . . . . . . . . . . . . . . . . . . . . . . . . .   396  13.5 Max-Margin Formulations   . . . . . . . . . . . . . . . . . . . . . . . . . . .   398  13.5.1   Structured Support Vector Machines   . . . . . . . . . . . . . . . . .   399  13.5.2   Max-Min Formulations ( \u0007\u0007 )   . . . . . . . . . . . . . . . . . . . . .   399  13.6 Generalization Bounds ( \u0007 )   . . . . . . . . . . . . . . . . . . . . . . . . . . .   402  13.7 Experiments   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   404  13.7.1   Robust Regression   . . . . . . . . . . . . . . . . . . . . . . . . . . .   404  13.7.2   Ranking   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   404  13.8 Conclusion   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   407  14 Probabilistic Methods   409  14.1 From Empirical Risks to Log-Likelihoods   . . . . . . . . . . . . . . . . . .   409  14.1.1   Conditional Likelihoods   . . . . . . . . . . . . . . . . . . . . . . . .   411  14.1.2   Classical Priors   . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   411  14.1.3   Sparse Priors   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   412  14.1.4   On the Relationship between MAP and MMSE ( \u0007 )   . . . . . . . . .   413  14.2 Discriminative versus Generative Models   . . . . . . . . . . . . . . . . . . .   417  14.2.1   Linear Discriminant Analysis and Softmax Regression   . . . . . . .   417  14.2.2   Naive Bayes   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   418  14.2.3   Maximum Likelihood Estimations   . . . . . . . . . . . . . . . . . .   419  14.3 Bayesian Inference   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   420  14.3.1   Computational Handling of Posterior Distributions   . . . . . . . . .   421  14.3.2   Model Selection through Marginal Likelihood   . . . . . . . . . . . .   422  14.4 PAC-Bayesian Analysis   . . . . . . . . . . . . . . . . . . . . . . . . . . . .   423  14.4.1   Setup   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   423  14.4.2   Uniformly Bounded Loss Functions   . . . . . . . . . . . . . . . . . .   424  14.5 Conclusion   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   426  15 Lower Bounds   427  15.1 Statistical Lower Bounds   . . . . . . . . . . . . . . . . . . . . . . . . . . . .   428  15.1.1   Minimax Lower Bounds   . . . . . . . . . . . . . . . . . . . . . . . .   428  15.1.2   Reduction to a Hypothesis Test   . . . . . . . . . . . . . . . . . . . .   429  15.1.3   Review of Information Theory   . . . . . . . . . . . . . . . . . . . .   431  15.1.4   Lower Bound on Hypothesis Testing Based on Information Theory   434  15.1.5   Examples   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   436  15.1.6   Minimax Lower Bounds through Bayesian Analysis   . . . . . . . . .   438\n\nx   CONTENTS  15.2 Optimization Lower Bounds   . . . . . . . . . . . . . . . . . . . . . . . . . .   441  15.2.1   Convex Optimization   . . . . . . . . . . . . . . . . . . . . . . . . .   441  15.2.2   Nonconvex Optimization ( \u0007 )   . . . . . . . . . . . . . . . . . . . . .   443  15.3 Lower Bounds for Stochastic Gradient Descent ( \u0007 )   . . . . . . . . . . . . .   447  15.4 Conclusion   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   449  Conclusion   451  References   453",
    "summary": {
      "en": "The text is an outline for a book titled \"Learning Theory from First Principles\" by Francis Bach, set to be published by The MIT Press in 2024. The book covers a wide range of topics in machine learning and statistical learning theory. \n\nKey sections include:\n\n1. **Mathematical Foundations**: Introduces linear algebra, calculus, and concentration inequalities.\n2. **Supervised Learning**: Discusses how to make predictions from training data, decision theory, loss functions, and learning from data.\n3. **Regression Techniques**: Details linear least-squares regression, including its statistical analysis and various enhancements like ridge regression.\n4. **Generalization Bounds**: Focuses on empirical risk minimization, estimation errors, and model selection.\n5. **Optimization Methods**: Explains optimization techniques in machine learning, including gradient descent and stochastic methods.\n6. **Local Averaging and Kernel Methods**: Covers local averaging methods and the theory behind kernel methods.\n7. **Sparse Methods**: Addresses variable selection and regularization techniques.\n8. **Neural Networks**: Explores neural network structures, optimization, and their approximation properties.\n9. **Ensemble Learning**: Discusses methods like bagging and boosting to improve predictive performance.\n10. **Probabilistic Methods**: Examines the relationship between empirical risks and probabilistic models.\n11. **Lower Bounds**: Looks at statistical and optimization lower bounds for learning algorithms.\n\nOverall, the book aims to provide a comprehensive understanding of learning theory, blending theory with practical applications in machine learning.",
      "ko": "프란시스 바흐의 \"기초부터 배우는 이론\"이라는 제목의 책이 2024년 MIT 프레스에서 출간될 예정이다. 이 책은 머신러닝과 통계학습 이론에 관한 다양한 주제를 다룬다.\n\n주요 내용은 다음과 같다. 첫째, 수학적 기초 부분에서는 선형대수, 미적분학, 집중 불평등에 대해 소개한다. 둘째, 감독 학습에서는 훈련 데이터를 바탕으로 예측하는 방법, 의사결정 이론, 손실 함수, 데이터로부터 학습하는 과정을 논의한다. 셋째, 회귀 기법에서는 선형 최소 제곱 회귀에 대한 통계적 분석과 리지 회귀와 같은 다양한 개선 방법을 상세히 설명한다. 넷째, 일반화 경계에서는 경험적 위험 최소화, 추정 오류, 모델 선택에 중점을 둔다. \n\n다섯째, 최적화 방법에서는 머신러닝에서의 최적화 기법, 특히 경량 하강법과 확률적 방법을 설명한다. 여섯째, 지역 평균화 및 커널 방법에서는 지역 평균화 기법과 커널 방법의 이론을 다룬다. 일곱째, 희소 방법에서는 변수 선택과 정규화 기법을 다룬다. 여덟째, 신경망에서는 신경망 구조, 최적화, 근사 특성에 대해 탐구한다. 아홉째, 앙상블 학습에서는 예측 성능을 향상시키기 위한 배깅과 부스팅과 같은 방법을 논의한다. 마지막으로, 확률적 방법에서는 경험적 위험과 확률 모델 간의 관계를 살펴본다. \n\n이 책은 이론과 머신러닝의 실제 응용을 결합하여 학습 이론에 대한 포괄적인 이해를 제공하는 것을 목표로 한다.",
      "ja": null
    }
  },
  {
    "id": "92403d609b4fdb25",
    "title": {
      "en": "How to Use Em Dashes (–), En Dashes (–), and Hyphens (-)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.merriam-webster.com/grammar/em-dash-en-dash-how-to-use",
    "score": 113,
    "by": "Stratoscope",
    "time": 1743106778,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "f40cf8f6b86c632d",
    "title": {
      "en": "Launch HN: Continue (YC S23) – Create custom AI code assistants",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://hub.continue.dev/explore/assistants",
    "score": 128,
    "by": "sestinj",
    "time": 1743087986,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "d552523a0f5f9a5e",
    "title": {
      "en": "Piranesi's Perspective Trick (2019)",
      "ko": "피라네시의 시선 속임수",
      "ja": null
    },
    "type": "story",
    "url": "https://medium.com/@brunopostle/piranesis-perspective-trick-6bcd7a754da9",
    "score": 290,
    "by": "amatheus",
    "time": 1743075690,
    "content": "Piranesi’s Perspective TrickBruno Postle·Follow8 min read·Apr 27, 20191171ListenShareThis is a quick report explaining what I have been doing with some research on what I started to call ‘Piranesi’s perspective trick’. In the past I would have written this up as an academic paper, that may happen at some point, but not anytime soon.Giovanni Battista Piranesi was a Eighteenth century artist famous for his ‘vedute’ etchings of classical and contemporary buildings in Rome. He wasn’t the only artist doing this sort of thing, there was quite an industry around souvenirs of the Grand Tour.Veduta del Ponte Molle, PiranesiArtistically the images are excellent, and in many ways these etchings are a very good record of the places, as Piranesi was very much concerned with getting the detail right. However there are a few stand-out things about these pictures, and pictures from other artists of the period, that are a little bit strange to our modern eyes that are used to experiencing places through photography:1. The engravings give a very good impression of the sense of place, we are all used to visiting places that we have previously seen in photographs and found the reality to be very different, whereas Piranesi’s places are very identifiable when encountered.2. Related to this, often the images show a wide panoramic view that is impossible to represent in a single photograph, but without any of the distortions that you see at the edges of a photograph.3. To a modern eye, brought up with television and magazines, there is something about the perspective in these pictures that looks just a little bit off, or even very wrong, but there is no evidence that people at the time thought like this — Piranesi was famous for his technical ability as a draughtsman.A few years ago, we were looking at introducing new panoramic projections for the Hugin panorama stitcher, and one of the features often asked for was to have a projection that was very wide-angle, but which didn’t have the extreme edge distortions of normal photographic projections, or the awkward curvature that you see in fisheye or map projections, basically what we wanted was a way to recreate these ‘vedute’ views in software. What we came up with was the ‘Panini projection’ (sometimes called the ‘vedutismo projection’, probably a better name, but hey, once you create software, you can’t go changing names of things just like that).An image created in Hugin in Panini/Vedutismo projection. The angle of view of this image is nearly 180°, this is an extreme wide angleThe Panini projection is very useful, and you have probably encountered it at some point without even realising it, it has the property that vertical lines are straight, radial lines are straight, and that edge distortion is imperceptible. With the Panini projection it is possible to show a scene with an enormously wide angle of view (over 180°), and for it to appear like a normal photograph, sometimes, at least.Another Hugin panorama in extreme wide-angle Panini projectionBut our Panini projection in Hugin isn’t quite the same as the perspective views used by Piranesi and his contemporaries. Although there is a straightforward way to construct a Hugin/Panini perspective on a drawing board, there is no evidence that any of these historical artists actually used it. Some got very close, in particular Panini himself and Vincent van Gogh, but that is a story for another day.But there is a distinctive feature of Piranesi’s perspective trick, a feature that marks it out from Hugin’s Panini perspective, and from a normal ‘rectilinear’ image.‘Rectilinear’ is the kind of normal perspective you get from a camera, but also from drawing a perspective by projecting a plan on a drawing board, and also it is the default perspective you get using one of the various historical perspective machines such as a camera obscura.The distinctive feature of Piranesi’s perspective trick is that when you have a series of similar objects receding into the distance, such as houses or arches in a bridge, the nearer versions are just drawn as larger versions of those in the distance — real perspective doesn’t work like this, not at all, this is a trick that Piranesi and other artists used to cram more things into the pictures while retaining legibility.Note that all three arches are basically the same, just drawn at different sizes, PiranesiThe way to spot it involves drawing some lines on the picture, use a copy, it is best not to use an original.In a normal perspective, any parallel lines in the scene that are not perpendicular to the viewing direction converge onto points. This sounds complex, but it is not, here is what we call a one-point perspective. The edges of the buildings converge to a single ‘vanishing point’ on the horizon, this is why we call it ‘one point perspective’.Normal single-point perspective. The parallel lines of the buildings converge on a vanishing point at the middle on the horizon. The diagonals of the buildings are parallel and they converge on vanishing points above and below.But other parallel lines in the scene also converge to points, the diagonals of the buildings are also parallel, and so when shown in perspective they converge on points directly above and below the main vanishing point.But look at one of Piranesi’s engravings:Veduta del palazzo Odescalchi, PiranesiSome of the diagonals, towards the middle of the image, do the right thing, they converge top and bottom as expected, but the diagonal lines at the left of the scene are drawn completely parallel. This is outrageous! real, true perspective doesn’t work like this, it isn’t possible to construct a camera or a computer program to render a view that does this. But the engravings work as pictures nonetheless, in fact they are quite good.Piranesi engraving, note that the three arches are drawn exactly the same at different scales, diagonal lines are parallelThis one below is a Canaletto, the same trick is in use, the centre and right hand part of the picture is photographically correct, but the long building on the left has been extended using Piranesi’s perspective trick. I’ve drawn diagonals on each bay of the building and they are obviously parallel. If this was a true perspective these diagonal lines would converge into a vanishing point in the sky somewhere above St Mark’s.The mathematics of this is quite simple, just draw objects in a series such that the closer ones are the same proportion as those further away. You can show that the only way to do this is is for the ratio of the distance from the vanishing point to any two features is the same for the next two features.Piranesi’s perspective trick, the diagonals of features in the elevation stay parallel when remappedHere’s the science bit:…and here are some tests, with Panini’s perspective trick we can’t render the entire scene, just rectangular objects in a scene, rectangular objects like the elevation of a street. This is a street elevation:Main Street USA elevationHere is the same elevation distorted using Piranesi’s perspective trick, looks ok to me, it’s a first test so is a bit blurry.An early test of a single-point Piranesi perspectiveBut here is the same elevation drawn using correct rectilinear perspective. I hope you will agree that the Piranesi version is much more legible, the furthest house above is easier to see and the nearest house isn’t horribly distorted as it is below.Comparison perspective, this is a ‘true’ single-point perspective, note how distorted the left and right buildings are compared to both the original elevation and the Piranesi perspective versionBy legibility I mean that the Piranesi distortion is easier to read, and that to anyone unfamiliar with photographs, that hasn’t grown up with TV, photographs and magazines, the Piranesi version would look much better, and the true perspective would look rather odd.So what can we do with this? We can’t add this trick to Hugin as a new general purpose mapping, because it isn’t a real perspective. However most image editors, like GIMP, have a perspective tool that performs a distortion on a rectangular selection, technically this is called a ‘homography matrix transform’ and it produces a photographically correct rectilinear perspective, with all the usual unpleasant edge distortions.Piranesi’s perspective trick can also be extended into two dimensions like this, sometimes called a ‘two point perspective’. Piranesi never did this, but with computers we can do all sorts of things. So here is a general 2D remapping as a prototype replacement for the perspective tool in image editors (I have no ability to add this to GIMP, somebody else needs to step up):Two-point ‘Piranesi’ perspective distortionI think that the result is much more legible than that produced by a ‘true’ perspective, no matter how correct it is:Normal homography perspective transformation produced in GIMP. Note how squashed the buildings at the right and left are when taken in isolationOnce you know what to look for, i.e parallel diagonal lines, you can see Piranesi’s trick being used by any number of historical artists, and even some modern artists, this isn’t ‘lost’ knowledge.The trick was very common as a way to show a birds-eye aerial view of landscapes, but also keeping them legible enough to be used as a map, like this:We can do this in the computer, here is a birds-eye view of a London map redrawn using Piranesi’s perspective trick.Piranesi bird’s eye perspective, OSM contributorsI hope you will agree that it is quite a bit more legible than the same view shown in ‘correct perspective’:‘Correct’ birds-eye perspective, OSM contributors..and so this article doesn’t finish on a boring picture, here is some lineart remapped using Piranesi’s perspective trick. The thing to note is that the left building has basically the same amount of distortion as the right building, only smaller. Similarly, all the windows have about the same amount of distortion, this is very very unlike a true, ‘correct’ perspective:Palazzo Porto, Palladio. Original image by Joshua Cesa, Alessandro Senno, Elia Venturini, WikipediaBruno Postle, April 2019",
    "summary": {
      "en": "This text discusses Giovanni Battista Piranesi, an 18th-century artist known for his detailed etchings of buildings in Rome. Piranesi's work features a unique perspective trick that differs from modern photographic perspectives. Key points include:\n\n1. **Piranesi's Etchings**: His images effectively capture the essence of places, often presenting wide panoramic views without the distortions typical in photography.\n\n2. **Perspective Trick**: Piranesi used a method where similar objects, like arches or houses, are drawn at different sizes rather than following real perspective rules. This allows for clearer and more legible images.\n\n3. **Panini Projection**: The text introduces the \"Panini projection,\" a modern software technique that mimics Piranesi's style by keeping vertical and radial lines straight while reducing edge distortion.\n\n4. **Comparison of Perspectives**: The article contrasts Piranesi’s method with traditional perspectives, showing that his approach can enhance legibility, especially for viewers unfamiliar with photography.\n\n5. **Applications**: While this perspective trick cannot be easily integrated into general mapping software like Hugin, it can be replicated in image editing tools. It can also be adapted for 2D representations, creating clearer visuals.\n\nOverall, Piranesi's perspective trick is a valuable technique for artists and illustrators, providing a more understandable way to represent complex scenes.",
      "ko": "이 글은 18세기 예술가 조반니 바티스타 피라네시(Giovanni Battista Piranesi)에 대해 다루고 있습니다. 그는 로마의 건축물을 세밀하게 에칭한 작품으로 유명합니다. 피라네시의 작품은 현대 사진의 시각적 관점과는 다른 독특한 시각적 기법을 특징으로 합니다.\n\n피라네시의 에칭은 장소의 본질을 효과적으로 포착하며, 사진에서 흔히 발생하는 왜곡 없이 넓은 파노라마 뷰를 제공합니다. 그는 아치나 집과 같은 유사한 물체를 실제 원근법 규칙을 따르지 않고 서로 다른 크기로 그리는 방법을 사용했습니다. 이로 인해 이미지가 더 명확하고 읽기 쉬워집니다.\n\n또한, 글에서는 \"파니니 투영(Panini projection)\"이라는 현대 소프트웨어 기법을 소개합니다. 이 기법은 피라네시의 스타일을 모방하여 수직선과 방사선은 곧게 유지하면서 가장자리 왜곡을 줄입니다. 피라네시의 방법과 전통적인 원근법을 비교하면서, 그의 접근 방식이 사진에 익숙하지 않은 관객에게도 가독성을 높일 수 있음을 보여줍니다.\n\n이러한 원근법 기법은 일반적인 매핑 소프트웨어인 후진(Hugin)에는 쉽게 통합할 수 없지만, 이미지 편집 도구에서는 복제할 수 있습니다. 또한 2D 표현에 적응하여 더 명확한 시각 자료를 만들 수 있습니다.\n\n전반적으로 피라네시의 원근법 기법은 예술가와 일러스트레이터에게 유용한 기술로, 복잡한 장면을 보다 이해하기 쉽게 표현하는 방법을 제공합니다.",
      "ja": null
    }
  },
  {
    "id": "88c3e580de46b991",
    "title": {
      "en": "Take this on-call rotation and shove it",
      "ko": "온콜 지옥 탈출!",
      "ja": null
    },
    "type": "story",
    "url": "https://www.scottsmitelli.com/articles/take-oncall-and-shove-it/",
    "score": 42,
    "by": "mirawelner",
    "time": 1743109768,
    "content": "Take This On-Call Rotation and Shove ItPublishedMarch 27, 2025On This PageWho you gonna call?Grandpa, what’s a beeper?Getting on the same pageWait, you guys are getting paid?168 long, cold, lonely hoursWe need to talk about KafkaImportant meaningless things / Meaningful unimportant thingsSomething for the painSleeping through the nightThe familiar blue and gold intro graphic fills the screen every evening at six o’clock on the dot. The jabbing staccato string music conjures up vague secondhand memories of what a teletype machine might have sounded like. A high angle view of the studio floor with the large Lexan-clad desk in the middle, then a cross dissolve to a two shot of the presenters for this newscast. The music fades, each person introduces themselves, then they jump straight into the top story for the evening. It’s been this way for as long as anybody can remember. They’ve never failed to get this show on the air.They’ve never failed.Who you gonna call?Everything fails all the time.Werner VogelsProducing any sort of live television show is a complex ballet. The studio’s cameras and microphones route their signals into video switchers and audio mixers, pre-taped packages come from the video server, field reporters are connected bidirectionally through a satellite link, and with a sprinkling of pizazz from the motion graphics machine, the final product is sent off to master control and ultimately to all the kitchen counters and family rooms across the city.But there are ancillary systems outside of this direct pipeline. The studio lighting is quite important, as most professional broadcast cameras tend to produce underwhelming images under inadequate light. The teleprompters feeding the anchors their scripts are obviously important. The weather reporting segments use an entirely separate system of graphics rendering equipment that must be linked through a chroma keyer to place the meteorologist in front of the computer-generated forecast images. And quite obviously, this equipment requires a handful of human operators.Studio-grade equipment is obscenely expensive, but it is also incredibly reliable. It is rare for things to outright fail, but anything can eventually wear out after enough daily use. If a camera fails, perhaps they can wheel the one from the sports desk over to cover this part of the broadcast. If the teleprompters fail, the anchors have a copy of the script at their desk that they can look down at. If one of the anchors calls out sick, they can sub in talent from the morning news team.Each of these is an example of either a redundant backup system or spare capacity that can be reallocated if needed. The broadcast technically does not need any of these contingencies to function under normal circumstances, but in cases where things go wrong it can mean the difference between success and total failure.Not everything can be made completely redundant. A failure in the power system for the lights will most likely plunge the entire studio into darkness, and that’s no way to run a news program. Similarly, if the $50,000 video switcher dies, it’s highly unlikely that they’ll have a spare holed up in the supply closet. To insure against every possible thing that could ever go wrong, they would have to build a second studio on a separate part of the city’s electric grid, with redundant copies of all the equipment and broadcast content, along with a full crew of understudies ready to take over at a moment’s notice. This is a degree of redundancy that can’t reasonably be achieved by any budget-conscious station.There is a hybrid between the two options, allowing the station to only maintain a single instance of anything expensive while having some assurance that the equipment they do have will work when needed: They can find an expert of some sort who is capable of fixing anything that breaks well enough to get the broadcast out. We’ll name this person Alex. If the microphone battery dies, Alex will swap it out. If the video server acts up, Alex knows how to get it working again. If the tire pressure light in the Chevrolet Weather Beast comes on, or the studio’s air conditioning fails, or the technical director breaks both their hands and needs somebody to push the buttons on their behalf, it’s Alex’s time to shine.Now, naturally, most of the time everything is going fine and Alex has nothing to do. So Alex has some other regular job in the studio—say running the audio mixer. In fact, the audio mixer thing is their official job title and their primary responsibility at the station; they only jump into universal-problem-solving mode when something goes wrong. As soon as the problem is resolved, it’s back to the audio mixer.The other thing about all this is, well, it’s very difficult to find and train people like Alex. So since they are at the station all evening anyway, why not also have them stick around in case anything goes wrong during the 7:00 news, and 11:00? And if anything happens during the 4:30–7 a.m. news, the station can call Alex at home and have them bop over and fix the problem. Oh, and also the news at noon, and the 4 p.m. block.  Apparently this station broadcasts six hours of live news programming most days. At least it’s only four hours on Sunday. In the station’s view, there is no need for anybody to relieve Alex because—most of the time—they never need Alex’s emergency response skills at all. There should be no need to hire and train somebody else to do this stuff because they barely use the services of the person they already have.There is, of course, another option that the station has never seriously entertained: Don’t hold Alex to any of those responsibilities at all, and if things really go to hell they can just throw on an old The Price Is Right rerun and hope for better luck during the next scheduled newscast.Grandpa, what’s a beeper?1-800-759-7243But if you ain’t got that pin number, dummy, you can’t call meTo hook up with Mix you gotta call that numberThen sit by the phone and wonderWill he call? If you’re fine I mightIf you’re a duck, good nightSir Mix-A-Lot, “Beepers”There was a time—not that long ago, really—where people couldn’t contact you if they didn’t know where you were. Telephones were literally screwed into the walls of houses and businesses. Portable two way radios existed, but they were a massive pain to carry around and operate. If somebody wished to contact you, they would not call you specifically but rather your house or your workplace, places where you might or might not have been at the time. If you were not there, maybe they’d try to call your brother’s house, your favorite bar, the Kiwanis club, or another location that was significant to you. If they still couldn’t find you, eventually they’d give up. People used to be more chill in that way.In a more structured environment—say a hospital where doctors moved from room to room but stayed inside one building—it was important to be able to get in touch with a specific person without knowing which room they were in. To accomplish this, a phone operator would page  This verb form of the word “page” uses the same sense as the noun “page,” an old-timey word meaning roughly “servant boy.” I page you in the sense that I am asking Kenneth, the NBC page from 30 Rock to send for you. the desired person via an announcement over the building’s public address speakers: “Paging Dr. Johnson, Dr. Johnson, please call fourth floor nurse station.” Assuming Dr. Johnson was in the building to hear this, they would find a phone and call the station as instructed.This worked fine, but it generated a lot of “useless” noise because most of the staff were uninvolved in most of the pages they overheard. Thanks to incremental improvements in technology, the voice announcements were phased out to make way for unidirectional radio broadcasts that covered the entire building. The content of the radio message remained the same as the audible announcement: who the page was for, and who that person needed to contact in response. Each person who needed to receive pages was given a pager, a radio receiver that was pre-programmed to only activate in response to pages specifically addressed to it. Each pager contained a small numeric display where the information about who to call could be shown. These were colloquially called beepers because, well, they made a beeping sound to announce each incoming page.To send a page, a person would pick up one of the building’s telephones and dial the number for the paging system. They would be prompted to enter the recipient’s PIN or unique identification code along with a callback number. If the sender wanted the recipient to call them directly, the callback number would be a phone that the sender was ready to pick up. It didn’t have to be, though. For example, the sender and recipient could have a prearranged system in which a code like “505” could be interpreted as the distress signal SOS with some mutually understood meaning.  These codes were more common from senders that the recipient knew well, representing messages they frequently needed to exchange. To a building maintenance worker, “234” could indicate an emergency at 234 Maple Avenue while “5300” could have been 5300 Elm Street. The codes meant what the sender and recipient agreed they meant.Technology got better. Things got smaller and faster. The unidirectional pager networks started becoming overshadowed by mobile phone networks which soon gained the ability to send bidirectional SMS messages. Microprocessors advanced to the point where a battery-operated handheld device could serve as a phone that could also send and receive text messages. These advances made it possible to send longer messages using a more expressive character set on a device that also did other things. My very first mobile phone could run a game of Snake that objectively blew. But the capabilities were there. Phones continued to gain capabilities, the networks they ran on continued to get faster with wider coverage, but the central thread of “I need to get this message to that device” is as clear today as it was when Sir Mix-A-Lot was courting his lady friends in the 1980s.Also, the systems described up to this point had one thing in common: The person sending the page was a human being.Getting on the same pageDude: They gave Dude a beeper, so whenever these guys call—Walter: What if it’s during a game?Dude: Oh, I told them if it was during league play…Donny: What’s during league play?Walter: Life does not stop and start at your convenience, you miserable piece of shit.The Big Lebowski (1998)Like a disheartening number of things in the tech industry, there are no real standards around what on-call responsibilities look like. Each organization And each team within! is free to set things up in whichever way suits their tastes, and the resulting practices vary widely as a result. In order to ground this article in something concrete, I will describe Alex’s on-call arrangement, which seems to be typical for US companies whose business model is “Have a website and/or mobile app, and either put ads all over it or convince the users to enter their credit card information somewhere to use it.” The prevailing attitude of these organizations is that the product must work at all times, otherwise it results in failure to show an ad or collect a payment. Both of these negatively affect revenue.Alex’s company uses the SEV system, which might Again, no standards. Somebody copied part of the philosophy from Amazon or Facebook or someplace but never bothered to codify exactly what the abbreviation meant to them. mean “severity,” “site event,” “significant event,” “serious event,” or anything else you might care to contrive that matches the pattern. SEVs are further divided into numbered classes depending on their impact on the product experience; a SEV 1 means that the business is currently failing to be a business because it is unable to perform its core functions and/or collect its revenue. The lesser SEV 3 might represent degraded performance on some non-critical portion of the application.  An example of a SEV 3 might be a situation where users can still change their profile pictures, but those changes are not showing up promptly in the app due to some kind of processing delay. This will probably not impact the quarterly financial statement in a measurable way. An instance of a SEV 1, on the other hand, might entail the mobile app showing a perpetual loading spinner on every request to every user at once. That type of thing tends to get noticed.Below the SEV system, there is a bubbling churn of things that are subtly broken, or are well on the way to someday being definitely broken, but are fine for the time being. A good example of this would be a disk that is 98% full. In its current state, nothing is actually wrong. But once it finally becomes 100% full and cannot accept any more data, something else in the system is going to respond poorly and this can likely cascade into some kind of SEV. Most systems in most organizations have monitoring in place for this sort of thing, and it is common for an on-call engineer to receive pages due to (e.g.) high disk usage to investigate specifically to avoid a potential SEV in the future. Practically all pages of this nature are generated and sent through automated means, and these pages can sometimes resolve themselves without outside intervention if (e.g.) the disk usage abates naturally.The on-call engineer in Alex’s department is selected out of a rotation of all the team members. The on-call shift is seven consecutive days of 24-hour support, or 168 solid hours. ±1 hour depending on how daylight saving time shakes out. The on-call engineer does not need to stay awake for seven straight days; the idea is that they’re supposed to work on typical tasks during business hours and go about their non-work lives as usual, but be able to jump into handling an issue quickly after receiving any page at any time. The “quickly” part is formally defined as time to acknowledge, and durations from 5 to 30 minutes are fairly typical. Alex’s team expects pages to be acknowledged within 15 minutes.If a page is not acknowledged by the on-call engineer, a system of escalation begins. The escalation policy usually follows one of these patterns:If there is only a single on-call engineer, the page may escalate to them again. This re-raises the original alert in case it was somehow missed the first time.In a “primary/secondary” type of arrangement, there are actually two people on-call at any given moment. All pages go to the primary, and only unacknowledged pages escalate to the secondary. If the secondary doesn’t acknowledge the page either, it may escalate further as described by the other bullet points here.In a “hunt group” configuration, an unacknowledged page is sent to every member of the team—none of whom are officially on-call at the moment—in the hopes that one of them is free to acknowledge and handle the issue. This arrangement has a strong tendency to break down into one of two degenerate states:One or a few people naturally become highly responsive to all pages, acknowledging them before most of their teammates have the opportunity to do so. Over time, most of the team members stop paying attention to pages and leave their highly-responsive peers to handle everything that comes in.Something very close to the bystander effect occurs, where everybody in the group assumes somebody else will acknowledge the page, and ultimately nobody steps up to do it. This deadlock is broken when somebody (perhaps a team lead or supervisor) tags a specific team member and tasks them with taking ownership of the issue.In each of the setups described above, the team’s manager may or may not be part of the escalation chain. If they are, it adds a whole new layer to the on-call calculus: Nobody wants their unacknowledged pages to end up notifying their manager, especially outside of working hours. Alex’s team uses the “single on-call engineer” model with escalation to the manager.On-call shifts occur one week out of every N weeks, where N is the number of people on the team.  For primary/secondary arrangements, the shift frequency is two weeks out of N, even though one of those weeks will ideally see few or zero pages. Still, the secondary must remain fully available during that time. If there are fifteen people on a team, each person will barely need to cover one shift per quarter. On a team of two, each person is on-call every other week. This is a substantial source of variability, and it can change suddenly as team members go on vacation, take personal leave, or part ways with the team or company. Alex works in a department of four, resulting in an on-call shift approximately once a month.Sometimes life interferes with on-call scheduling, and for those times there is usually a mechanism for team members to trade partial or complete on-call shifts between themselves. If the active on-call engineer needs a few uninterrupted hours to attend a family function or unavoidable appointment, they can seek out a peer who is willing to cover the responsibility for that time. At some future date, the favor can be reciprocated when that other person is on-call and needs somebody to cover for them.When an engineer receives a page and needs to do unplanned work in response to it, that work is called on-call load. Each organization has an expected amount of on-call load for each shift. Or rather, they’re supposed to, but it’s not surprising to find places that have never given the idea any serious thought. If an excessive number of issues occur and the load exceeds the expectations for the shift, it becomes on-call pain. True fact. Why would I make that up? Pages that occur outside of regular working hours are considered more painful than those that occur during weekdays.As far as what the on-call engineer needs to do during incident response—the time between acknowledging a page and resolving the issue that caused it—this is another area of huge variance. Sometimes they’ll need to log into some web UI and click one button. Sometimes they’ll spend ten straight hours trying to resuscitate a completely inaccessible product.  A team may experience both ends of the load spectrum from one week to another just by luck of the draw.Occasionally the on-call engineer will be faced with a situation that is objectively unfixable. Sometimes a critical piece of AWS’s entire us-east-1 region fails, ultimately hobbling a significant chunk of the internet along with it. Sometimes 33 Whitehall loses generator power after Superstorm Sandy drowns its fuel pumps in seawater. Alex’s company has worked very hard to cut down on operational costs by farming out a bit too much of its core functionality to a third party with bad customer support turnaround times, whose outages then become Alex’s outages by proxy. In instances like these, sometimes the on-call engineer just has to throw up their hands in defeat. Other than simply waiting out the problem, the only other feasible option would be to undertake some over-ambitions migration to an entirely different provider. That’s not something that anybody can do in any kind of reasonable time frame, and doing it under the duress of a service outage would be unwise at best. At a certain point, the best Alex can do is turn on The Price Is Right and wait for things to blow over.Now, obviously, on-call duty is by no means a job requirement that is specific to the tech industry. Doctors and surgeons can be on-call. The building superintendent for an apartment complex can be on-call. The guy who fixes air conditioners can be on-call. The difference is that the people in those industries are fairly compensated for doing it.Wait, you guys are getting paid?Work work work, day after dayFifty hour week, forty hour payNo time to get over all this overtimeYeah I’m always runnin’, but I’m always runnin’ behindTracy Lawrence, “Runnin’ Behind”There are ways of looking at employee wages in the US that are elegantly simple. An employee is hired at a rate of $X/hour, they work for Y hours in a week, and the total pay is the product of those two numbers. There is a minimum wage at the federal and possibly state level that sets the smallest legal amount for $X. The employee should work a maximum Y of 40 hours in that week, otherwise they enter an overtime situation where their hourly $X becomes $X and a half. Those highfalutin white collar workers are basically the same, except their Y is fixed at 40 hours regardless of the time actually worked so their total pay stays the same week after week. That’s how it all works, right?This is the system laid out in the Fair Labor Standards Act of 1938 (FLSA) and its many amendments. This is the law that underpins concepts like minimum wage, overtime, the 40-hour work week, and the notion that child labor probably isn’t such a good thing to do. It also defines a set of exemptions to the rules, thus creating the concept of an exempt employee. If you are a US-based tech worker in a full-time position, I’m going to take a stab in the dark and assume that you are almost certainly classified as an exempt employee. This means that the FLSA’s protections effectively do not exist for you. You are not guaranteed overtime, and you could conceivably work so many hours over the course of a week that your effective hourly rate ends up less than minimum wage.  Now I’m wondering if an employer could get away with hiring child labor by classifying them as exempt employees. I would guess not, or somebody out there would be doing it right now.The FLSA is designed with repetitive and predictable work in mind: Somebody who works on an assembly line, or who moves boxes around in a warehouse, drivers and couriers, et cetera. Workers in these sorts of jobs have a tendency to produce a similar and predictable amount of work in any given hour. Drop in on them during one hour of the workday and you’ll observe roughly the same level of productivity that you would find from them at any other hour.Employees who are exempt from the FLSA tend to have variability in their workday. The original thinking was that this would apply to executives and highly-skilled professionals who performed such a wide range of tasks throughout the day that some hours were markedly more valuable than others. These sensibilities changed and eventually morphed into “white collar jobs that paid a lot.” The current regulations specifically list computer-related occupations in their list of exempted fields. And from a certain angle, if you really squint, it makes sense! Think of hours where you have pounded out hundreds of lines of code, then compare it to hours where you sat in a conference room staring at a blinking text insertion cursor instead of paying attention to the presenter. Sometimes you’ll make no progress towards a thorny challenge during the course of an entire workday, which might be completely offset by a single spark of creative inspiration while washing the dishes later that night.All this to say, there is nothing in the regulations of the United States that can protect Alex from working more than 40 hours in a week. There is no requirement that overtime be paid to them. If the work requires more than 40 hours in a week, oh well, sucks to be Alex.  This means that technically Alex could work fewer than 40 hours by applying the same logic, assuming they get all their necessary work done. They have been meaning to work up the nerve to try to pull that one day.So. With that bit of background out of the way, it’s clear that there is no legal or regulatory requirement for an employer to pay anything for performing on-call duties as long as the responsibility is given to an exempt employee. Based on my own experiences and informal polling of others in the industry, the prevailing attitude is that on-call is part of the job description and “baked in” with the total compensation. It’s not at all unusual to find on-call shifts that receive no additional payment or consideration for carrying the pager. There is also usually nothing extra paid for responding to a page that occurs outside of regular working hours.And again, there are no absolute rules about this. Some places actually do pay a modest honorarium for each on-call shift worked. Some will provide “unofficial” compensatory time And if your employer gives comp time, a small question for you: Do they also reduce the amount of sprint story points they expect you to work through when you take it? to balance out a page handled outside of typical business hours. Legends are told of organizations where the teams are staffed adequately and the systems simply don’t page. Just imagine a magical place where a person is only on-call for like three weeks a year, and who never gets paged during those times. Alex, who once spent an entire summer being on-call every other week while occasionally fielding a dozen pages in the span of a single day, cannot.Most places won’t even provide a phone or subsidize a mobile carrier bill, nor will they provide a company-paid mobile hotspot for laptop tethering purposes. It’s just assumed that you’ll happily install PagerDuty or Opsgenie or some other hateful app that violates the sanctity of your personal device, right there on the home screen next to Okta Verify. A brief tangent: Fuck Okta Verify. Your personal phone becomes your pager, the thing that pulls you out of leisure time and into work time. After a while, you might start to notice on-call beginning to fundamentally change your relationship with the device.The absolute largest source of variability comes from a team’s willingness to improve the on-call situation as opposed to simply accepting that things are the way they’re meant to be. Some teams view every page—no matter how trivial—as a signal that something needs to be immediately fixed to prevent that specific thing from ever happening again. Other teams view it as something that just happens as a natural consequence of supporting a product, like a smoke detector battery chirp that everybody has learned to tune out over the course of several years. It is the manifestation of technical debt that has been boiling for years, looking for a pressure relief valve to escape through, and it just happens to keep finding its release through Alex’s pager.Perhaps unsurprisingly, the teams that are most willing to defend against recurring pages are also the most likely to actually perform in-depth postmortems so they can write and maintain their on-call runbooks. Sometimes the runbook is the only friend an on-call engineer has, and there’s nothing more disappointing than discovering that this friend can’t help fix anything.168 long, cold, lonely hoursAll I wanna do all day is spend it in bedBut that’s bad for the body and even worse for my headSo I’ll try and find a place where no one will ask me a thingIt’ll help me to forget and help me to singReel Big Fish, “Drunk Again”A page can conceivably come at any time, day or night. Alex needs to receive the alert and begin working on the issue within fifteen minutes, which means they must have a suitable work computer and sufficient internet connectivity available within that time commitment. They must remain cognizant of their phone’s signal quality and the availability of nearby Wi-Fi networks. Unless they take their bulky work laptop with them,  By the way, not everybody lives in a perfectly idyllic area. There are plenty of places where computers get stolen from parked cars and bags get snatched. Carrying this stuff around is a genuine risk for people in some situations. it’s not possible to travel anywhere that takes more than a few minutes to return from.Even certain household tasks—cutting the lawn for example—require special consideration. If a page arrives during that activity, Alex needs to put the mower away to a certain extent  In some areas, as above, an unattended mower might get swiped. In others, it could lead to an HOA fine. before going inside to clean up enough to do knowledge work. It’s mentally taxing to jump from domestic labor to complex problem-solving, and it’s equally difficult to go back when the issue is finally resolved.It turns out that there are many things in life that are technically compatible with an on-call shift, but which require such delicate planning and forethought that it sometimes ends up being easier to just not bother doing any of that stuff during an on-call week. No significant travel or long walks/drives, no excessive drinking or *ahem*, no ability to simply unplug and decompress. Even if a page never actually comes, there is always the potential for a page to come. Maybe the primary on-call turned off their phone without telling anybody to attend a screening of Oppenheimer. Actually happened. Maybe there’s time to quickly run to the grocery store and back, but it might be cutting it close. Maybe it’s better to just stay home until the end of the week. Park in front of the TV and run out the clock. But don’t watch anything too engrossing; getting paged right during the good part really sucks.This has a tendency to happen eventually, even at organizations where the expected on-call load is near zero. It’s not possible to live life completely normally while staying prepared to handle any page at any time. It would perhaps be hyperbolic to compare the experience to that of being placed under house arrest, but it’s the closest a lot of us will ever get to experiencing that level of freedom-yet-confinement.And, of course, when a page does come, it manages to find the most inopportune time to do so. Alex has been paged during nice dinners, in the middle of live entertainment, and at times that rightly should’ve been devoted to time with family members and friends. Not to mention that alert sound, and the notification box on the phone’s lock screen. Alex’s phone became a source of resentment and negative emotions to the point where they basically had to disable almost every other sound and all other notifications because their heart jumped every time one popped up. Alex won’t go as far as to say it caused PTSD, but it sure led to a fair number of the symptoms of PTSD.Also, it regularly ruined my sleep. Whoops, I meant Alex’s sleep. I’m not Alex. Nope.Sometimes pages decide to come during overnight hours. Here’s what happens when a page occurs in the middle of the night: First, if you happen to have a significant other, the alert sound invariably wakes them up before it wakes you. You get out of bed. It’s dark. It’s cold. You open your work laptop. Even at its lowest brightness setting, the 16-inch Liquid Retina XDR display lights up the room with its blinding intensity. You log into your email and Slack, open some dashboards, open Okta Verify on your phone, Fuck Okta Verify. and you’ve basically done everything you usually do at 9 a.m. on a regular workday. Six hours before you’re supposed to be here, you’re here. Still half asleep—no sense having any caffeine if the intention is to try to go back to bed after this is over—this is really not the right kind of headspace to be in while poking at unfamiliar and on-fire code on production systems. And since it’s the middle of the night, nobody else is here to help diagnose or double-check anything. There would be a kind of palpable loneliness here, if you had the mental acuity to notice it. Maybe you’ll manually page somebody else to come and help. Or maybe you can’t bear the thought of being the one responsible for spreading this on-call pain onto them.Eventually the problem gets resolved one way or another. You close the laptop and try to quietly return to bed. Your significant other (if applicable) is awoken again by this. You end up lying there for a while, unable to go to sleep due to the mental exertion, the light from the computer screen, and a fair bit of leftover adrenaline. May as well just stay awake; the issue probably isn’t actually fixed and it’ll likely page again in a few minutes anyway.Hey, you know what this sounds like? Anxiety! On-call basically causes anxiety. And if you’re a person who already has anxiety due to some other preexisting reason, congratulations! Now you have extra anxiety. And for what? Because some Kafka broker stopped running?We need to talk about KafkaI thought that since Kafka was a system optimized for writing, using a writer’s name would make sense. I had taken a lot of lit classes in college and liked Franz Kafka. Plus the name sounded cool for an open source project.Jay Kreps, Kafka: The Definitive Guide, Second EditionJay Kreps contributed to the technology that would eventually become Apache Kafka while he was working at LinkedIn. Very broadly, Kafka can be thought of as a message queue that accepts data from one side and sends it out to one or many interested parties on the other side. Unlike a typical queue it also persists this stream of messages on disk so that delivery can be deferred, batched, or even repeated at some future date. At scale, it may be tasked with handling such an immense volume of data that the operation of the system becomes a major pain in the ass.Part of this operational difficulty is caused by the fact that Kafka runs on multiple discrete computers that must constantly cooperate with each other to behave as a single larger system. Much like the Borg in Star Trek. But Google already took that name. If any of the members of the cluster of computers become disconnected or degraded, the performance and stability of the entire group is impacted. If an organization runs Kafka in production, there is a very good chance it is routinely paging somebody due to low disk space, processing lag, or other inscrutable gremlins.The sheer quantity of data that Kafka wants to write to its disks, as alluded to in the Kreps quote above, is what led to its name. Apache Kafka writes a lot, just like author Franz Kafka did. Surely there is no reason to think any further about this.Franz Kafka created literary worlds in which unbearably absurd things happen for seemingly no reason and people are expected to simply endure them as if nothing out of the ordinary is going on. His environments only partially make sense, producing bureaucracies that defy any attempt at comprehension. The protagonists in his stories feel alienated and isolated. A queasy undercurrent of anxiousness and sometimes outright horror runs through his whole oeuvre. The author was likely neurotic, he destroyed approximately 90% of everything he ever wrote, then he died well ahead of when he probably should have—leaving several substantial works unfinished. In this regard, Apache Kafka shares some similarities.That is how you justify the project’s name. Saying “I took some literature classes in college and I thought I remembered liking them” is just intellectually lazy.Important meaningless things / Meaningful unimportant thingsJesse: Look, I like making cherry product, but let’s keep it real, alright? We make poison for people who don’t care. We probably have the most unpicky customers in the world.Breaking Bad, “Fly” (Season 3 Episode 10)I am going to pose what might sound like an unthinkable question: Is this important?My question is sincere. Does this service or product fulfill a need so critical that there is a legitimate reason to always keep one or more human beings on-call for it?  Or my personal favorite, usually offered by engineers trying to pull one another back into the crab bucket, which goes something like: “Don’t you think you should be responsible for your own code that you have put into production?” The proper response to this, of course, is “What my code? We are a team; this is our code.” Or, probably a more healthy view, “This is the code.” The production system in question is almost certainly a schizophrenic box of compromises brought about through poor decision-making, unaddressed technical debt, design-by-committee, and impossible timelines and budgets. This is not a system that any single rational human being on the team would’ve chosen to build if permitted to do so alone. Trying to assert ownership over an environment like that is just begging to get your shit rocked. Will the business suffer a significant loss in sales due to an outage? Will they break a contractual service-level agreement (SLA) and expose themselves to legal liability if the outage exceeds a certain threshold? Will they lose the goodwill of customers if the product is unavailable for too long? Do the customers have other options if they get upset with the reliability of the product? Is it even feasible for them to switch to those competitors? Can an unaddressed issue lead to loss of life or property damage?The answer to at least one of those questions is probably automatically “yes,” which justifies the use of any means the organization deems appropriate to avoid risks. Like an adult sternly barking “because I said so,” the conversation is supposed to end here. On-call is important because it’s important. The mere idea of questioning that axiom brings almost certain trouble, so few people dare prying further.But it is worth prying. If there are no firm SLAs, it’s hard to justify why the “time to acknowledge” expectations are set the way they are. How much additional customer goodwill does the organization earn by adding one more nine of availability? What is customer goodwill actually worth in the first place? Is it worth more or less than the long term mental well-being of the engineering staff and the eventual turnover incurred by burning them all out?Each of these perspectives boil down to the same thing: The business might lose money (either from uncaptured revenue or due to penalties) if somebody is not around all the time to handle any technical fault that may occur. It then follows that this person—this lowly on-call engineer—is like an insurance policy that can prevent a larger calamity.But here’s the key difference: Insurance policies have premiums that cost something. The insured entity can’t just hand-wave the cost away by smearing the responsibility across a bunch of exempt employees who have the words “and other duties as assigned” at the end of their job descriptions. Handling on-call load is work. Modifying life outside of business hours to make them compatible with potential on-call load is work. On-call pain is tantamount to a large volume of work. Work should be compensated, especially if that work is such a critical part of the organization’s risk mitigation plan. If it’s not important enough to fairly compensate the people who have to shoulder the on-call load, why is it important enough to base the success of the business on?“Importance” really is the key to thinking about all of this. Some might hold the opinion that if an engineer is not on-call as a part of their regular duties, they clearly must not be working on anything very important. I propose to look at it a different way. To understand this perspective, you’ll need to go deep into the forgotten corner of your closet and find That Cage. You know, That Cage you have worked so long and so hard to trap your imposter syndrome in. Go ahead and pull That Cage out for a minute. Lift off the bed sheet that’s been covering it up. Stare deeply into the dark, haunting eyes of that demon. Once comfortable in each other’s presence, ask your imposter syndrome a simple question: If this was actually important to the success of the organization, why did they trust us with it?Something for the painBart: Milhouse, how could you let this happen? You were supposed to be the night watchman!Milhouse: I was watching. I saw the whole thing. First it started falling over, then it fell over.The Simpsons, “Homer’s Enemy” (Season 8 Episode 23)Obviously, there are ways to support a product that don’t involve putting staff on-call outside of working hours. The so-called “follow the sun” paradigm pretty accurately describes itself—the team is distributed around the world and the product is supported by whichever part of the globe is in daylight at that time. To do this perfectly fairly, there should be a minimum of three teams each separated by eight hours of timezone distance. When it’s 5 p.m. in Chicago and folks are preparing to go home, it’s 9 a.m in Melbourne. When the Aussies are done for the day, it’s a new workday in Lviv and the sun is rising over Dublin.  This doesn’t provide perfectly fair coverage during weekends or holidays, but see below for some ideas about that. Or, just close during those times. Banks and financial institutions all do it, and they seem to be doing fine! At any given point during the course of the day, there is some team that is already awake and already within working hours that can handle things without pulling somebody out of their slumber.If there is no other option than to require on-call support outside working hours, consider making it voluntary. Now I know what you’re going to say to that: If it’s voluntary, nobody will want to do it. And that is absolutely the point. Nobody wants to do it because it freakin’ sucks. It’s not a good deal. So it’s the organization’s responsibility to sweeten that deal enough for somebody to consider taking it. Pay people something for taking an on-call shift. Either a per-day or per-week stipend, or something like the equivalent of one hour’s pay for every four hours on-call. Saying “well, you’re already making plenty of money with your engineer’s salary so that should count” ain’t the way to do it at all.On-call staff should also be paid something each time they respond to a page, especially when it occurs outside of working hours. If simply being available is worth a flat-rate stipend, actually having to jump into a firefight should be worth something even greater than that. Because if not, it implies that a span of unbearable on-call pain endured by person A is exactly equivalent to an uncharacteristically tranquil week enjoyed by person B at a different time. This is not fair to either of these people or the team as a whole.Making the business pay to page staff will certainly change the timbre of the on-call load. Nothing cleans up noisy, flapping, inactionable pager alerts quicker than making them expensive to generate.In a distant past life, this was proposed and shot down with the following rationale, which I distinctly remember as being one of the stupidest things I have ever heard somebody in my management chain say. Paraphrasing: “If on-call engineers were to receive compensation for each incident they resolved, it would incentivize them to intentionally build systems that fail so they could increase their pay by increasing their on-call load.” My guy, that is sabotage and fraud. You are hypothesizing a scenario where your subordinates are committing actual crimes. If somebody is doing criminal acts at work, fire their ass! Not to mention that anybody who deliberately self-inflicts on-call load is a goddamn idiot and should be sacked just on that basis alone.There is also the radical option of simply leaving certain spans of the day/week uncovered, with nobody officially on-call during those hours. If something fails, let it fail for a while and then deal with it during support hours. Sometimes a large and visceral production incident needs to bubble up to senior management’s attention in order to rally together the willpower to actually pay down some of the technical debt that led to the problem. If all the engineers know that the product is a wobbly tower of paperclips and duct tape, perhaps the people seated at the very top of the infernal structure should get to see exactly how precarious the whole thing really is from time to time. It’s rather easy to put up a fake facade of perfect customer-facing uptime, and it’s also surprisingly easy to conceal the damage done to the employees who are tasked with carrying the weight of that facade on their backs. At least until they all become disillusioned and quit, anyway.Something to bear in mind is that you, as the employee, have a certain voice here. You can ask potential employers during the interview stage how they do on-call, and withdraw yourself from consideration if you don’t like the answer they give. You can tell them flatly that this is your reason for not wanting to work there. You can leave a job if on-call is cramping your style or ruining your life, and you can tell the exit interviewer exactly this. If you’re at a place that’s thinking about formally adopting on-call, you can dig your heels in and either demand compensation for it or refuse to do it. Will your employer respect the boundaries you’re drawing? There’s really only one way to find out.If you find yourself negotiating a job, try to get a line in the employment agreement that specifically disallows unpaid after-hours on-call shenanigans. Remember, negotiating isn’t just about arguing over compensation numbers, you can try to haggle over material job duties and expectations. Push back on the non-compete and non-solicitation clauses while you’re at it, and the overreaching intellectual property assignment, all that crap.  Have you ever redlined a contract? It might be worth giving it a shot someday! And I won’t go as far as to say that tech workers should unionize or anything like that, but I will say that it seems like a whole lot of employers in this industry specifically do not want their employees to unionize. There’s probably a reason they’re apprehensive about it, and that reason almost certainly benefits the employers and not the rest of us.And it’s scary to stand up for principles like this, which is likely a big part of why on-call duties get successfully foisted on so many unfortunate people. I can’t promise this won’t lead to an uncomfortable and fruitless conversation, or a burned bridge, or a pay cut, or months of unemployment. All I can say is that you—you specifically—are worth something. Your time is worth something, just like your mental health and physical well-being. Your employer spent money hiring you, and they would need to spend money to replace you with another hire. Unless you are absolutely useless or a complete dickhead, losing you would negatively affect your team’s morale and output. Your manager has to go through the performance review cycle just like you do, and losing a direct report is not a good look for them. You have a voice and you have some leverage. It’s up to you what you do with it.Sleeping through the nightBut it’s a five o’clock world when the whistle blowsNo one owns a piece of my timeAnd there’s a five o’clock me inside my clothesThinking that the world looks fine, yeahThe Vogues, “Five O’Clock World”Earlier, when I asked if this was important, I suspect that most readers answered from the perspective of the company. Of course it’s important, why wouldn’t it be? But now I’m asking you, specifically. Is this important to you?I suspect that a fair number of readers are going to feel that what I wrote is naive, overly cynical, too idealistic, or simply incompatible with the realities of modern business expectations. Perhaps this article will be perceived as a handbook for how to become embittered and then get fired. But detached from all of that, in the innate nature of almost every human that participates in these systems, this can’t possibly sit right. Why do we accept this plainly abusive practice? Why do we go above and beyond to forfeit the enjoyment of our free time to an organization that is unwilling to reciprocate in any meaningful way? To an organization that is perhaps incapable of reciprocating?It turns out that there are all kinds of different people out there. Some are young (or young at heart) and have nothing better to do outside of work than party and pass out drunk. For them, on-call might be almost fun, like that invigorating feeling somebody might get when they sign up for stage crew in high school and get to screw around in the building after all the other students and staff have gone home and it’s weird and empty. Others have complicated home lives with difficult caretaking situations and really do not need to be dealing with yet another source of stress and anxiety in an existence that is already a hair’s breadth from going completely off the rails. Some people simply do not care about work when they’re not physically there; they clock in, work for the day, then clock out. There is nothing inherently wrong with trying to limit the encroachment of work into life. Each of these people have different priorities, different needs, different values and principles. It is not fair to blindly shoehorn them all into the same on-call rotation and pretend they are going to respond to it the same way.On average, most of us get around 4,000 weeks of life on this earth. If you’re exceptionally fortunate, you might make it to 5,000. How many of those weeks do you want to spend in the shadow of a pager?When I was just shy of 2,000 weeks old, I suffered through a particularly acute week of on-call pain. At one point I was in my third or fourth video call about the same long and protracted smoldering SEV and, in a moment of frustrated weakness, I made an offhand comment about just being tired of repeatedly handling the same problem. My manager was present on the call, and my statement seemed to really set him off. I was essentially told that my feelings about the situation—perhaps the only authentic part of myself I ever expressed there—were wrong. In the days that followed I was made to feel like I was not a team player, that I was not pulling my weight, and that I was not meeting the bare minimum of what was expected of a person bearing the torch of on-call. With the utterance of a single sentence, I opened a rift in the relationship with my manager that remained until the day I left that job.But I meant what I said. I mean it now more than ever: I have been paged enough.«Back to Articles",
    "summary": {
      "en": "The article \"Take This On-Call Rotation and Shove It\" discusses the challenges and frustrations of being on-call in the tech industry, particularly for engineers like \"Alex.\" Here are the key points:\n\n1. **On-Call Responsibilities**: On-call duties require engineers to be available 24/7 for a week, handling urgent issues that arise. This can disrupt their personal lives, including sleep and leisure activities.\n\n2. **Lack of Compensation**: Many companies do not provide additional pay for on-call duties, assuming it's part of the job. This leads to engineers working unpaid hours and dealing with stress and anxiety without adequate support.\n\n3. **Technical Reliance**: The article highlights how technology, while intended to streamline communication, can create pressure on engineers to respond quickly to pages, often during inconvenient times.\n\n4. **Mental Health Impact**: Being on-call can lead to mental health issues, including anxiety and stress, as engineers feel the weight of their responsibilities and the expectation to resolve issues immediately.\n\n5. **Questioning Importance**: The article challenges the necessity of constant on-call support, asking if such demands are truly vital for the business or if they simply serve to maintain appearances.\n\n6. **Possible Solutions**: Suggestions include compensating on-call work fairly, allowing voluntary on-call shifts, and potentially leaving some hours uncovered to reduce the burden on staff.\n\n7. **Employee Empowerment**: The article encourages employees to advocate for fair treatment regarding on-call duties, emphasizing the importance of mental health and work-life balance.\n\nOverall, the article sheds light on the often overlooked challenges of on-call work in tech and calls for a reevaluation of how companies manage these responsibilities.",
      "ko": "\"이 기사는 기술 산업에서의 온콜 근무의 어려움과 불만을 다루고 있으며, 특히 '알렉스'와 같은 엔지니어들에게 미치는 영향을 설명합니다. \n\n온콜 근무는 엔지니어가 일주일 동안 24시간 대기해야 하며, 긴급한 문제를 처리해야 하는 책임이 있습니다. 이로 인해 개인 생활이 방해받고, 수면이나 여가 활동에도 영향을 미칠 수 있습니다. \n\n많은 기업들이 온콜 근무에 대해 추가 보상을 제공하지 않으며, 이를 직무의 일환으로 간주합니다. 이로 인해 엔지니어들은 보수를 받지 못하는 시간 동안 일하게 되고, 적절한 지원 없이 스트레스와 불안에 시달리게 됩니다. \n\n기술이 의사소통을 원활하게 하려는 의도에도 불구하고, 엔지니어들에게는 즉각적인 반응을 요구하는 압박을 가할 수 있습니다. 특히 불편한 시간에 페이지를 받는 경우가 많습니다. \n\n온콜 근무는 정신 건강에도 부정적인 영향을 미칠 수 있습니다. 엔지니어들은 자신의 책임감과 즉각적으로 문제를 해결해야 한다는 압박감으로 인해 불안과 스트레스를 느끼게 됩니다. \n\n이 기사는 지속적인 온콜 지원의 필요성에 의문을 제기하며, 이러한 요구가 정말로 비즈니스에 필수적인지, 아니면 단순히 겉모습을 유지하기 위한 것인지 질문합니다. \n\n해결책으로는 온콜 근무에 대한 공정한 보상, 자발적인 온콜 근무 교대 허용, 그리고 직원들의 부담을 줄이기 위해 일부 시간을 비워두는 방법이 제안됩니다. \n\n또한, 직원들이 온콜 근무에 대한 공정한 대우를 주장하도록 격려하며, 정신 건강과 일과 삶의 균형의 중요성을 강조합니다. \n\n이 기사는 기술 분야에서 온콜 근무의 간과된 어려움에 대해 조명하고, 기업들이 이러한 책임을 관리하는 방식을 재평가할 필요성을 촉구합니다.\"",
      "ja": null
    }
  },
  {
    "id": "a5620036423b6eb7",
    "title": {
      "en": "Blasting Past WebP - An analysis of the NSO BLASTPASS iMessage exploit",
      "ko": "웹P를 넘어서: NSO 블래스트패스 분석",
      "ja": null
    },
    "type": "story",
    "url": "https://googleprojectzero.blogspot.com/2025/03/blasting-past-webp.html",
    "score": 202,
    "by": "el_duderino",
    "time": 1743079784,
    "content": "Project Zero\n\nNews and updates from the Project Zero team at Google\n\nWednesday, March 26, 2025\n\nBlasting Past Webp\n\n@import url(https://themes.googleusercontent.com/fonts/css?kit=XGMkxXUZTA64h2imyzu79g);.lst-kix_t2u4j4vhkrnm-3>li:before{content:\"\\0025cf   \"}.lst-kix_t2u4j4vhkrnm-0>li:before{content:\"\\0025cf   \"}.lst-kix_t2u4j4vhkrnm-4>li:before{content:\"\\0025cb   \"}.lst-kix_t2u4j4vhkrnm-7>li:before{content:\"\\0025cb   \"}ul.lst-kix_t2u4j4vhkrnm-8{list-style-type:none}ul.lst-kix_t2u4j4vhkrnm-6{list-style-type:none}ul.lst-kix_t2u4j4vhkrnm-7{list-style-type:none}ul.lst-kix_t2u4j4vhkrnm-4{list-style-type:none}.lst-kix_t2u4j4vhkrnm-5>li:before{content:\"\\0025a0   \"}ul.lst-kix_t2u4j4vhkrnm-5{list-style-type:none}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}ul.lst-kix_t2u4j4vhkrnm-2{list-style-type:none}.lst-kix_t2u4j4vhkrnm-6>li:before{content:\"\\0025cf   \"}ul.lst-kix_t2u4j4vhkrnm-3{list-style-type:none}ul.lst-kix_t2u4j4vhkrnm-0{list-style-type:none}ul.lst-kix_t2u4j4vhkrnm-1{list-style-type:none}.lst-kix_t2u4j4vhkrnm-8>li:before{content:\"\\0025a0   \"}.lst-kix_t2u4j4vhkrnm-1>li:before{content:\"\\0025cb   \"}.lst-kix_t2u4j4vhkrnm-2>li:before{content:\"\\0025a0   \"}ol{margin:0;padding:0}table td,table th{padding:0}.XQFzMDWmii-c30{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:234pt;border-top-color:#000000;border-bottom-style:solid}.XQFzMDWmii-c36{padding-top:0pt;border-top-width:0pt;padding-bottom:0pt;line-height:1.5;border-top-style:solid;background-color:#ffffff;border-bottom-width:0pt;border-bottom-style:solid;orphans:2;widows:2;text-align:left}.XQFzMDWmii-c40{padding-top:14pt;padding-bottom:4pt;line-height:1.5;page-break-after:avoid;orphans:2;widows:2;text-align:left}.XQFzMDWmii-c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:\"Arial\";font-style:normal}.XQFzMDWmii-c22{color:#434343;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:\"Arial\";font-style:normal}.XQFzMDWmii-c35{color:#666666;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:\"Arial\";font-style:normal}.XQFzMDWmii-c18{color:#434343;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:\"Arial\";font-style:normal}.XQFzMDWmii-c38{padding-top:0pt;padding-bottom:16pt;line-height:1.5;page-break-after:avoid;orphans:2;widows:2;text-align:left}.XQFzMDWmii-c12{padding-top:16pt;padding-bottom:4pt;line-height:1.5;page-break-after:avoid;orphans:2;widows:2;text-align:left}.XQFzMDWmii-c4{padding-top:0pt;padding-bottom:0pt;line-height:1.5;orphans:2;widows:2;text-align:left}.XQFzMDWmii-c10{color:#000000;text-decoration:none;vertical-align:baseline;font-size:11pt;font-style:normal}.XQFzMDWmii-c29{color:#b80672;text-decoration:none;vertical-align:baseline;font-size:11pt;font-style:normal}.XQFzMDWmii-c11{padding-top:0pt;padding-bottom:0pt;line-height:1.5;text-align:left;margin-right:-72pt}.XQFzMDWmii-c21{color:#000000;vertical-align:baseline;font-size:11pt;font-style:normal}.XQFzMDWmii-c17{color:#000000;text-decoration:none;vertical-align:baseline;font-style:normal}.XQFzMDWmii-c1{font-size:9pt;font-family:\"Roboto Mono\";color:#188038;font-weight:400}.XQFzMDWmii-c42{color:#666666;vertical-align:baseline;font-size:15pt;font-style:normal}.XQFzMDWmii-c45{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.XQFzMDWmii-c3{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.XQFzMDWmii-c41{border-spacing:0;border-collapse:collapse;margin-right:auto}.XQFzMDWmii-c37{font-weight:400;text-decoration:none;font-family:\"Arial\"}.XQFzMDWmii-c2{color:#1967d2;font-weight:400;font-family:\"Roboto Mono\"}.XQFzMDWmii-c26{color:#000000;vertical-align:baseline;font-size:11pt}.XQFzMDWmii-c5{color:#37474f;font-weight:400;font-family:\"Roboto Mono\"}.XQFzMDWmii-c39{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.XQFzMDWmii-c13{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;text-decoration:underline}.XQFzMDWmii-c16{background-color:#00ff00;font-style:italic}.XQFzMDWmii-c9{font-weight:400;font-family:\"Courier New\"}.XQFzMDWmii-c23{font-weight:700;font-family:\"Courier New\"}.XQFzMDWmii-c6{font-weight:400;font-family:\"Roboto Mono\"}.XQFzMDWmii-c19{background-color:#ffff00}.XQFzMDWmii-c32{background-color:#00ff00}.XQFzMDWmii-c15{background-color:#ff9900}.XQFzMDWmii-c24{font-style:italic}.XQFzMDWmii-c25{font-weight:700}.XQFzMDWmii-c20{font-size:10pt}.XQFzMDWmii-c8{font-size:9pt}.XQFzMDWmii-c43{padding-left:0pt}.XQFzMDWmii-c44{color:#c5221f}.XQFzMDWmii-c28{height:0pt}.XQFzMDWmii-c31{background-color:#f4cccc}.XQFzMDWmii-c14{background-color:#ff00ff}.XQFzMDWmii-c34{margin-left:36pt}.XQFzMDWmii-c27{color:#188038}.XQFzMDWmii-c7{height:11pt}.XQFzMDWmii-c33{background-color:#00ffff}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:\"Arial\";line-height:1.5;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:\"Arial\";line-height:1.5;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:\"Arial\"}p{margin:0;color:#000000;font-size:11pt;font-family:\"Arial\"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:\"Arial\";line-height:1.5;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:\"Arial\";line-height:1.5;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:\"Arial\";line-height:1.5;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:\"Arial\";line-height:1.5;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:\"Arial\";line-height:1.5;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:\"Arial\";line-height:1.5;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}\n An analysis of the NSO BLASTPASS iMessage exploit\n Posted by Ian Beer, Google Project Zero\n\n On September 7, 2023 Apple issuedan out-of-band security update for iOS:\n\n Around the same time on September 7th 2023, Citizen Lab published a blog postlinking the two CVEs fixed in iOS 16.6.1 to an \"NSO Group Zero-Click, Zero-Day exploit captured in the wild\":\n\n \"[The target was] an individual employed by a Washington DC-based civil society organization with international offices...\n\n The exploit chain was capable of compromising iPhones running the latest version of iOS (16.6) without any interaction from the victim.\n\n The exploit involved PassKit attachments containing malicious images sent from an attacker iMessage account to the victim.\"\n\n The day before, on September 6th 2023, Apple reported a vulnerability to the WebP project, indicating in the report that they planned to ship a custom fix for Apple customers the next day.\n\n The WebP team posted their first proposed fixin the public git repo the next day, and five days after that on September 12th Google released a new Chrome stable releasecontaining the WebP fix. Both Apple and Google marked the issue as exploited in the wild, alerting other integrators of WebP that they should rapidly integrate the fix as well as causing the security research community to take a closer look...\n\n A couple of weeks later on September 21st 2023, former Project Zero team lead Ben Hawkes (in collaboration with @mistymntncop) published the first detailed writeupof the root cause of the vulnerability on the IsoscelesBlog. A couple of months later, on November 3rd, a group called Dark Navypublished their first blog post: a two-part analysis (Part 1- Part 2) of the WebP vulnerability and a proof-of-concept exploittargeting Chrome (CVE-2023-4863).\n\n Whilst the Isosceles and Dark Navy posts explained the underlying memory corruption vulnerability in great detail, they were unable to solve another fascinating part of the puzzle: just how exactly do you land an exploit for this vulnerability in a one-shot, zero-click setup? As we'll soon see, the corruption primitive is very limited. Without access to the samples it was almost impossible to know.\n\n In mid-November, in collaboration with Amnesty International Security Lab, I was able to obtain a number of BLASTPASS PKPasssample files as well as crash logs from failed exploit attempts.\n\n This blog post covers my analysis of those samples and the journey to figure out how one of NSO's recent zero-click iOS exploits really worked. For me that journey began by immediately taking three months of paternity leave, and resumed in March 2024 where this story begins:Setting the scene\n For a detailed analysis of the root-cause of the WebP vulnerability and the primitive it yields, I recommend first reading the three blog posts I mentioned earlier (Isosceles, Dark Navy 1, Dark Navy 2.) I won't restate their analyses here (both because you should read their original work, and because it's quite complicated!) Instead I'll briefly discuss WebP and the corruption primitive the vulnerabilityyields.WebP\n WebPis a relatively modern image file format, first released in 2010. In reality WebP is actually two completely distinct image formats: a lossy formatbased on the VP8 video codec and a separate lossless format. The two formats share nothing apart from both using a RIFFcontainer and the string WEBPfor the first chunk name. From that point on (12 bytes into the file) they are completely different. The vulnerability is in the lossless format, with the RIFF chunk name VP8L.\n\n Lossless WebP makes extensive use of Huffman coding; there are at least 10 huffman trees present in the BLASTPASS sample. In the file they're stored as canonical huffman trees, meaning that only the code lengths are retained. At decompression time those lengths are converted directly into a two-level huffman decoding table, with the five largest tables all getting squeezed together into the same pre-allocated buffer. The (it turns out not quite) maximum size of these tables is pre-computed based on the number of symbols they encode. If you're up to this part and you're slightly lost, the other three blogposts referenced above explain this in detail.\n\n With control over the symbol lengths it's possible to define all sorts of strange trees, many of which aren't valid. The fundamental issue was that the WebP code only checked the validity of the tree afterbuilding the decoding table. But the pre-computed size of the decoding table was only correct for validtrees.\n\n As the Isoscel",
    "summary": {
      "en": "**Summary of Project Zero Update - March 26, 2025**\n\nThe Project Zero team at Google provided an analysis of a recent iMessage exploit, known as the NSO BLASTPASS exploit, that targeted iPhones. \n\n- **Security Updates**: On September 7, 2023, Apple released a security update for iOS, addressing vulnerabilities linked to a zero-click exploit discovered by Citizen Lab. This exploit allowed attackers to compromise iPhones without user interaction by sending malicious images via iMessage.\n\n- **WebP Vulnerability**: On the same day, Apple reported a vulnerability in the WebP image format, which was quickly addressed. Google also released a fix for Chrome shortly after. Both companies noted that the vulnerability was actively exploited, prompting further investigation and research.\n\n- **Research Collaboration**: Ian Beer from Project Zero, in collaboration with Amnesty International, analyzed samples related to the exploit, seeking to understand how it worked. This included studying memory corruption issues associated with the WebP format.\n\n- **WebP Format Details**: WebP, an image format introduced in 2010, has both lossy and lossless types. The current vulnerability pertains to the lossless version, which involves complex Huffman coding. The exploit took advantage of how the decoding tables were built without validating the input properly, leading to potential security issues.\n\nThis update emphasizes the ongoing efforts by Google and collaborators to address and analyze vulnerabilities that pose risks to user security.",
      "ko": "구글의 프로젝트 제로 팀은 최근 아이폰을 겨냥한 iMessage 취약점인 NSO BLASTPASS 취약점에 대한 분석을 제공했습니다. \n\n2023년 9월 7일, 애플은 시민 연구소에서 발견된 제로 클릭 취약점과 관련된 보안 업데이트를 iOS에 배포했습니다. 이 취약점은 공격자가 사용자와의 상호작용 없이 악성 이미지를 iMessage로 전송하여 아이폰을 해킹할 수 있게 했습니다.\n\n같은 날 애플은 WebP 이미지 형식에서 발견된 취약점에 대해서도 보고했으며, 이를 신속하게 수정했습니다. 구글은 그 직후 크롬에 대한 수정 패치를 발표했습니다. 두 회사는 이 취약점이 적극적으로 악용되고 있음을 언급하며 추가 조사와 연구를 진행했습니다.\n\n프로젝트 제로의 이안 비어는 국제앰네스티와 협력하여 이 취약점과 관련된 샘플을 분석하며 작동 방식을 이해하려고 했습니다. 이 과정에서 WebP 형식과 관련된 메모리 손상 문제도 연구했습니다.\n\nWebP는 2010년에 도입된 이미지 형식으로, 손실형과 비손실형 두 가지 유형이 있습니다. 현재의 취약점은 비손실형 버전과 관련이 있으며, 복잡한 허프만 코딩을 포함합니다. 이 취약점은 디코딩 테이블이 입력을 제대로 검증하지 않고 구축되는 방식을 이용하여 보안 문제를 일으킬 수 있습니다.\n\n이번 업데이트는 구글과 협력자들이 사용자 보안에 위협이 되는 취약점을 해결하고 분석하기 위한 지속적인 노력을 강조합니다.",
      "ja": null
    }
  },
  {
    "id": "c8fcfc9715939d0e",
    "title": {
      "en": "Revyl (YC F24) Is Hiring a Front End Engineer Intern",
      "ko": "레빌, 프론트엔드 인턴 모집!",
      "ja": null
    },
    "type": "job",
    "url": "https://www.ycombinator.com/companies/revyl/jobs/5rbIJLP-frontend-engineer-intern",
    "score": 1,
    "by": "landseer",
    "time": 1743109229,
    "content": "We're looking for a Frontend Engineer Intern to help build the next generation of AI-powered testing and observability tools. You'll work closely with our founding team to craft intuitive, high-performance user experiences while gaining hands-on experience with modern frontend technologies.\nWhat you'll be working on\n\nBuilding elegant, user-friendly interfaces for our AI-driven testing and tracing platform\nDesigning and implementing interactive data visualizations to help users understand test results and system behavior\nImproving the UX of our agentic testing workflows, making debugging and error detection seamless\n\nWhat we're looking for\n\nPassion for frontend development and design—you care about how things look, feel, and function\nExperience with modern frontend frameworks\nInterest in data visualization and making complex systems easy to understand\nUnderstanding of frontend performance optimization\nHigh level of ownership and autonomy—you're excited to take on challenges and drive features end-to-end\n\nBonus points for\n\nExperience with browser automation or testing frameworks\nHaving a strong portfolio of past work\nExperience in network logs and browser internals\n\nWhy join us?\n\nWork with cutting-edge AI and observability tools\nBuild a product from the ground up and have a real impact\nLearn from a small, high-talent founding team in San Francisco\nOpportunity for full-time conversion",
    "summary": {
      "en": "We are looking for a Frontend Engineer Intern to help create advanced AI-powered testing and observability tools. You will work with our founding team to develop user-friendly interfaces and gain practical experience with modern technologies.\n\n**Key Responsibilities:**\n- Create easy-to-use interfaces for our AI testing platform.\n- Design interactive visualizations to simplify test results and system behavior.\n- Enhance user experience in testing workflows for easier debugging.\n\n**What We Want:**\n- A passion for frontend development and design.\n- Experience with modern frontend frameworks.\n- Interest in data visualization.\n- Knowledge of optimizing frontend performance.\n- A proactive attitude and willingness to take on challenges.\n\n**Bonus Qualifications:**\n- Experience with browser automation or testing tools.\n- A strong portfolio of previous work.\n- Familiarity with network logs and browser internals.\n\n**Why Join Us?**\n- Work with innovative AI tools.\n- Contribute to building a product from scratch.\n- Learn from a talented team in San Francisco.\n- Potential for a full-time job offer.",
      "ko": "우리는 고급 AI 기반 테스트 및 관찰 도구를 개발할 프론트엔드 엔지니어 인턴을 찾고 있습니다. 창립 팀과 함께 사용자 친화적인 인터페이스를 개발하고 현대 기술에 대한 실무 경험을 쌓을 수 있는 기회입니다.\n\n주요 책임은 AI 테스트 플랫폼을 위한 사용하기 쉬운 인터페이스를 만드는 것입니다. 테스트 결과와 시스템 동작을 간단하게 이해할 수 있도록 인터랙티브한 시각화를 설계해야 합니다. 또한, 디버깅을 쉽게 할 수 있도록 테스트 워크플로우에서 사용자 경험을 향상시키는 것도 포함됩니다.\n\n우리가 원하는 인재는 프론트엔드 개발과 디자인에 대한 열정을 가진 사람입니다. 현대 프론트엔드 프레임워크에 대한 경험이 필요하며, 데이터 시각화에 대한 관심도 중요합니다. 프론트엔드 성능 최적화에 대한 지식과 함께, 도전 과제를 기꺼이 받아들이는 적극적인 태도가 요구됩니다.\n\n추가 자격으로는 브라우저 자동화 또는 테스트 도구에 대한 경험이 있으면 좋습니다. 이전 작업의 강력한 포트폴리오와 네트워크 로그 및 브라우저 내부에 대한 이해도 도움이 됩니다.\n\n우리와 함께하면 혁신적인 AI 도구로 작업할 수 있으며, 처음부터 제품을 만드는 데 기여할 수 있습니다. 샌프란시스코의 재능 있는 팀으로부터 배울 수 있는 기회도 있으며, 정규직 제안의 가능성도 있습니다.",
      "ja": null
    }
  },
  {
    "id": "c3cc09f7c3386ef2",
    "title": {
      "en": "Clean, a formal verification DSL for ZK circuits in Lean4",
      "ko": "클린: Lean4의 ZK 회로 검증 DSL",
      "ja": null
    },
    "type": "story",
    "url": "https://blog.zksecurity.xyz/posts/clean/",
    "score": 39,
    "by": "vons",
    "time": 1743100380,
    "content": "Introducing clean, a formal verification DSL for zk circuits in Lean4\n\n            Written by\n\n                    Giorgio Dell'Immagine\n\n            on\n            Mar 27, 2025\n\n    We are really excited to share our initial steps towards building clean, an embedded DSL and formal verification framework for ZK circuits in Lean4.\nAs we recently shared, Zero Knowledge circuits are full of bugs, but fortunately, techniques like formal verification can provide a huge confidence boost in the correctness of ZK circuits.\nClean enables us to define circuits in Lean4, specify their desired properties, and – most importantly – formally prove them!\nThis work is part of the zkEVM Formal Verification Project which aims to provide infrastructure and tooling to enable formal verification for zkEVMs.\nRead about clean below, or watch our presentation on it in the zkEVM project updates call.\n\nObjectives\nOur objective is to build an embedded DSL for writing zk circuits in Lean4, that allows us to reason about them in a formal way.\nWe believe that co-locating circuit definitions with their desired specification and correctness proof will allow us to create a robust library of reusable formally verified circuit gadgets.\nWe currently target AIR arithmetization, and we assume to have a table lookup primitive available by the underlying proof system.\nHow to formally verify ZK circuits\nIn order to reason formally about ZK circuits, we first need to define a formal model. This involves the following steps:\n\nDefining what primitives our circuit language supports, i.e., which are the operations that we can use to define circuits.\nDefining the semantics of such primitives.\nDefining what are the properties we are interested to formally prove for a given circuit.\n\nOur language allows us to specify a circuit, which is composed of two main objects.\n\nA collection of variables, and\nconstraints and lookup relations over those variables.\nThe goal of a zero-knowledge proof system is exactly to convince a verifier that the prover knows a witness (i.e., an assignment of the variables) that satisfies the constraints and lookups of a given circuit.\n\nAt a fundamental level, for a given circuit we are interested in how the satisfaction of the constraints and the witness are related.\nIn other words: if a witness satisfies the constraints, what property can we infer about it?\nLet’s make a concrete example.\nConsider a circuit defined over one variable x and that is composed of only one contraint:\nC1 : x * (x - 1) === 0\nThis is a very common gadget that ensures that x is a boolean value, i.e., it is either 0 or 1.\nThe specification of this ciruit can be expressed as:\nx = 0 ∨ x = 1\nAlbeit being a very simple example, it shows the basic idea: we are interested in formally proving that if an assignment to the variables satisfies the constraints, then the specification holds as well.\nWe are also interested in the other direction: if an honest prover holds a witness that satisfies the specification, then there exists an assignment of the variables that satisfies the constraints.\nTake the following alternative implementation of a boolean check\nC2 : x === 0\nThis new constraint is sound, because the only valid assignment that satifies it is x = 0, which is a boolean value.\nHowever, it is not complete, as an honest prover that holds a valid boolean x = 1 cannot provide a witness that satisfies the constraint.\nMore formally, the two properties we want to prove are:\n\nSoundness: if the prover can exhibit any witness that satisfies the constraints and lookup relations defined by the circuit, then some specification property holds over that witness. Proving this property ensures that the circuit is not underconstrained.\nCompleteness: for every possible input, an honest prover can always exhibit a witness that satisfies the constraints and lookup relations defined by the circuit. Proving this property ensures that the circuit is not overconstrained.\n\nDSL design\nIn our DSL, we support four basic operations for defining circuits.\ninductive Operation (F : Type) [Field F] where\n  | witness : (m: ℕ) -> (compute : Environment F -> Vector F m) -> Operation F\n  | assert : Expression F -> Operation F\n  | lookup : Lookup F -> Operation F\n  | subcircuit : {n : ℕ} -> SubCircuit F n -> Operation F\nIndeed, we can:\n\nWitness: introduce m new variables in the circuit, and add them to the witness.\nThis operation accepts also a compute function, which represents the witness generation function, in the honest prover case.\nAssert: add a new constraint to the circuit.\nLookup: add a new lookup relation to the circuit. A lookup defines which variables are being looked up and in which other table.\nSubcircuit: add a new subcircuit to the circuit.\nThe subcircuit is instantiated in the current environment, and the internal variables of the subcircuit are added to the witness.\nThis is the main way to gain composability of gadgets.\n\nTo enhance usability, we provide a way to define a circuit using a monadic interface, with a lot of convenience functions.\nThis interface allows us to define the circuits using very natural syntax constructs.\nDesign of the composable verification framework\nThe main building block of our framework is the FormalCircuit structure.\nstructure FormalCircuit (F: Type) (β α: TypeMap)\n  [Field F] [ProvableType α] [ProvableType β]\nwhere\n  main: Var β F -> Circuit F (Var α F)\n  assumptions: β F -> Prop\n  spec: β F -> α F -> Prop\n\n  soundness:\n    ∀ offset : ℕ, ∀ env,\n    -- for all inputs that satisfy the assumptions\n    ∀ b_var : Var β F, ∀ b : β F, eval env b_var = b ->\n    assumptions b ->\n    -- if the constraints hold\n    constraints_hold.soundness env (circuit.main b_var |>.operations offset) ->\n    -- the spec holds on the input and output\n    let a := eval env (circuit.output b_var offset)\n    spec b a\n\n  completeness:\n    -- for all environments which _use the default witness generators for local variables_\n    ∀ offset : ℕ, ∀ env, ∀ b_var : Var β F,\n    env.uses_local_witnesses (circuit.main b_var |>.operations offset) ->\n    -- for all inputs that satisfy the assumptions\n    ∀ b : β F, eval env b_var = b ->\n    assumptions b ->\n    -- the constraints hold\n    constraints_hold.completeness env (circuit.main b_var |>.operations offset)\nThis structure tightly packages in a dependent-type way, the following objects:\n\nβ and α are respectively the input and output “shapes”, essentially they define a structured collection of elements.\nmain: the circuit definition itself.\nassumptions: the assumptions that the circuit makes on the inputs. All properties are proved assuming that the input variables satisfy these assumptions.\nspec: the specification property of the circuit.\nsoundness: proof for soundness of the circuit.\ncompleteness: proof for completeness of the circuit.\n\nA FormalCircuit encapsulates a formally proved, reusable gadget: when instantiating a FormalCircuit as a subcircuit, we are able to reuse the soundness and completeness proofs of the subcircuit to prove the soundness and completeness properties of the whole circuit.\nThis is accomplished by automatically replacing the constraints of a subcircuit with its (formally verified) spec.\nIn this way we can formally verify even large circuits by applying a divide-et-impera approach: we start by defining and proving correctness of low-level reusable gadgets, and then combine them to build more and more complex circuits.\nA concrete example: 8-bit addition\nLet’s walk through one of the simple gadgets we have implemented and verified: addition on 8-bit numbers.\nThis is a gadget that takes as input two bytes and an input carry, and returns the sum of the two bytes modulo 256, and the output carry.\nFirst, we need to define the input and output shapes.\nstructure Inputs (F : Type) where\n  x: F\n  y: F\n  carry_in: F\n\nstructure Outputs (F : Type) where\n  z: F\n  carry_out: F\nNow, we define the assumptions the circuit makes on the input values, and the specification that the circuit should satisfy.\ndef assumptions (input : Inputs (F p)) :=\n  let { x, y, carry_in } := input\n  x.val < 256 ∧ y.val < 256 ∧ (carry_in = 0 ∨ carry_in = 1)\n\ndef spec (input : Inputs (F p)) (out : Outputs (F p)) :=\n  let { x, y, carry_in } := input\n  out.z.val = (x.val + y.val + carry_in.val) % 256 ∧\n  out.carry_out.val = (x.val + y.val + carry_in.val) / 256\nThe main circuit is defined as follows.\ndef add8_full_carry (input : Var Inputs (F p)) :\n    Circuit (F p) (Var Outputs (F p)) := do\n  let { x, y, carry_in } := input\n\n  -- witness the result\n  let z <- witness (fun eval => mod_256 (eval (x + y + carry_in)))\n\n  -- do a lookup over the byte table for z\n  lookup (ByteLookup z)\n\n  -- witness the output carry\n  let carry_out <- witness (fun eval => floordiv (eval (x + y + carry_in)) 256)\n\n  -- ensures that the output carry is boolean\n  -- by instantiating the Boolean.circuit as a subcircuit\n  assertion Boolean.circuit carry_out\n\n  -- main 8-bit addition constraint\n  assert_zero (x + y + carry_in - z - carry_out * (const 256))\n\n  return { z, carry_out }\nFinally, we define the FormalCircuit structure, which packages all those definitions, together with the soundness and completeness proofs\ndef circuit : FormalCircuit (F p) Inputs Outputs where\n  main := add8_full_carry\n  assumptions := assumptions\n  spec := spec\n  soundness := by\n    ...\n  completeness := by\n    ...\nNotice that this definition is generic over the field prime p, however we require an additional assumption on the prime, namely p>512, otherwise the circuit is not sound!\nvariable {p : ℕ} [Fact p.Prime]\nvariable [p_large_enough: Fact (p > 512)]\nVerifying concrete AIR tables\nThe FormalCircuit abstraction provides a modular definition of verified circuits, and it is mostly arithmetization-agnostic.\nHowever, we want to target AIR as a concrete arithmetization, as it is a very popular choice in the zkVM design space.\nAIR circuits are defined over traces: constraints are specificed together with an application domain, which represent which rows they should be applied to.\nIn principle, one could choose an arbitrary domain, however, in practice we choose domains that have a succinct representation in terms of vanishing polynomial.\nHere are some examples of domains that have succinct representations and are used in practice.\n\nThe constraint is applied to one specific row of the trace. This is often referred to as a boundary constraint.\nThe constraint is applied to all rows of the trace. This is often referred to as a recurring constraint. One feature is that constraints applied to every row can access neighbouring rows: for example they could access the next row or the previous row.\nThe constraint is applied to all rows, except a chosen small set of rows. For example, it could be applied to every row except the last one, or except the first one.\nThe constraint is applied to every n rows of the trace.\n\nAs a concrete example, let’s say that we want to give constraint over a trace for computing correctly the Fibonacci sequence, which is defined as follows.\n{F0=0F1=1Fn=Fn−2+Fn−1\nWe can implement it with a trace composed of two columns: x and y.\nThe invariant we want to prove is: on the i-th row, xi sould contain Fi and yi should contain Fi+1.\nWe can achieve this behaviour by imposing the following contraints.\n\nWe impose a boundary constraint on the first row: x0=0 and y0=1.\nAdditionally, we impose two recurring constraints, implementing the recursive relation: for every i – except the last one –, yi+1=xi+yi, and xi+1=yi.\nThis set of constraints is depicted in the following figure.\n\nIt is straight-forward to see that if a trace satisfies those constraints, then in the i-th row we will find the i-th Fibonacci number in the first column.\nNotice that this set of constraints can be thought also as defining a correct sequence of states, one for each row:\n\nthe boundary constraint is ensuring that the initial state is valid, while\nthe recurring constraint is ensuring that the state transition function is executed correctly.\n\nIn our framework, we support AIR constraints by:\n\nDefining an inductive trace data structure, which we model as a sequence of rows.\nModeling what it means to apply a contraint to a trace using a particular domain: this in practice is done by providing an assignment from abstract variables to concrete trace cells and then applying the original constraint semantics.\n\nYou can check out the soundness proof for the Fibonacci table using 8-bit addition here, which satisfies the following, slightly more complicated, specification:\nFor every rowi,xi=(Fimod256)\nUpcoming work\nThe framework is still in early stages fo development, but it is already showing promising results.\nSome planned next steps are:\n\nContinue adding low-level gadgets so that we have a rich library of basic reusable circuits.\nDefining common hash function circuits and proving their correctness.\nBuild a formally verified minimal VM for a subset of RISC-V.\n\nThe whole project is open source and available on GitHub, make sure to check it out!",
    "summary": {
      "en": "**Summary:**\n\nThe text introduces **clean**, a new formal verification domain-specific language (DSL) for zero-knowledge (ZK) circuits, developed using Lean4. The goal of clean is to help define ZK circuits, specify their properties, and formally prove their correctness. This is part of the zkEVM Formal Verification Project aimed at enhancing the reliability of ZK circuits.\n\n**Key Objectives:**\n- Create an embedded DSL for ZK circuits in Lean4 to enable formal reasoning.\n- Co-locate circuit definitions with their specifications and proofs to develop reusable circuit components.\n- Focus on AIR arithmetization and include a table lookup capability.\n\n**Formal Verification Process:**\n1. Define supported operations (primitives) for circuits.\n2. Establish the meaning of these primitives (semantics).\n3. Identify properties to formally prove for each circuit.\n\nCircuits consist of variables and constraints, with the aim of ensuring that a prover can demonstrate knowledge of a witness that satisfies these constraints. Key properties to be proven include:\n- **Soundness:** If a witness satisfies the constraints, then certain specifications must hold.\n- **Completeness:** For any valid input, a witness can always be found.\n\n**DSL Design:**\nThe DSL supports four basic operations for circuit definition:\n- **Witness:** Introduce variables and define how to generate them.\n- **Assert:** Add constraints to the circuit.\n- **Lookup:** Define relationships between variables and tables.\n- **Subcircuit:** Include smaller circuits within larger ones for composability.\n\n**Verification Framework:**\nThe framework uses a **FormalCircuit** structure to package circuit definitions along with proofs of soundness and completeness. It allows for the modular verification of circuits, enabling the reuse of verified components.\n\n**Example Implementation:**\nAn 8-bit addition circuit is provided as an example, which computes the sum of two bytes and handles carry operations. The circuit is defined with input and output structures, assumptions, specifications, and soundness/completeness proofs.\n\n**Future Work:**\nThe project aims to expand its library of reusable circuits, define common hash functions, and create a minimally verified virtual machine for RISC-V. The entire project is open-source and available on GitHub.",
      "ko": "clean은 Lean4를 사용하여 개발된 새로운 형식 검증 도메인 특화 언어(DSL)로, 제로 지식(ZK) 회로를 위한 것입니다. clean의 목표는 ZK 회로를 정의하고, 그 속성을 명시하며, 형식적으로 정확성을 증명하는 것입니다. 이는 ZK 회로의 신뢰성을 높이기 위한 zkEVM 형식 검증 프로젝트의 일환입니다.\n\n주요 목표는 Lean4에서 ZK 회로를 위한 내장 DSL을 만들어 형식적 추론을 가능하게 하는 것입니다. 회로 정의와 그 사양 및 증명을 함께 배치하여 재사용 가능한 회로 구성 요소를 개발하는 데 중점을 두고 있습니다. 또한 AIR 수치화에 집중하고 테이블 조회 기능을 포함할 예정입니다.\n\n형식 검증 과정은 다음과 같습니다. 첫째, 회로에 대한 지원되는 연산(원시 연산)을 정의합니다. 둘째, 이러한 원시 연산의 의미(의미론)를 설정합니다. 셋째, 각 회로에 대해 형식적으로 증명할 속성을 식별합니다. 회로는 변수와 제약 조건으로 구성되며, 증명자가 이러한 제약 조건을 만족하는 증거를 보여줄 수 있도록 하는 것이 목표입니다. 증명해야 할 주요 속성으로는 신뢰성(사실 증거가 제약 조건을 만족하면 특정 사양이 성립해야 함)과 완전성(유효한 입력에 대해 항상 증거를 찾을 수 있어야 함)이 있습니다.\n\nDSL 설계는 회로 정의를 위한 네 가지 기본 연산을 지원합니다. 첫째, 증거를 도입하고 이를 생성하는 방법을 정의하는 '증거(Witness)'가 있습니다. 둘째, 회로에 제약 조건을 추가하는 '주장(Assert)'이 있습니다. 셋째, 변수와 테이블 간의 관계를 정의하는 '조회(Lookup)'가 있습니다. 넷째, 더 큰 회로 안에 작은 회로를 포함하는 '하위 회로(Subcircuit)'가 있습니다.\n\n검증 프레임워크는 회로 정의와 신뢰성 및 완전성 증명을 패키징하는 'FormalCircuit' 구조를 사용합니다. 이를 통해 회로의 모듈형 검증이 가능해지며, 검증된 구성 요소를 재사용할 수 있습니다.\n\n예시 구현으로는 두 바이트의 합을 계산하고 캐리 연산을 처리하는 8비트 덧셈 회로가 제공됩니다. 이 회로는 입력 및 출력 구조, 가정, 사양, 신뢰성 및 완전성 증명으로 정의됩니다.\n\n향후 작업으로는 재사용 가능한 회로 라이브러리를 확장하고, 공통 해시 함수를 정의하며, RISC-V를 위한 최소 검증된 가상 머신을 만드는 것이 목표입니다. 이 프로젝트는 오픈 소스이며 GitHub에서 이용할 수 있습니다.",
      "ja": null
    }
  },
  {
    "id": "0bba5f1428db36d1",
    "title": {
      "en": "AI models miss disease in Black and female patients",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.science.org/content/article/ai-models-miss-disease-black-female-patients",
    "score": 185,
    "by": "pseudolus",
    "time": 1743100701,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "5c09f290955ffdb2",
    "title": {
      "en": "Alkanes on Mars",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.science.org/content/blog-post/alkanes-mars",
    "score": 53,
    "by": "nick__m",
    "time": 1743095458,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "c3a9091d32d97723",
    "title": {
      "en": "I genuinely don't understand why some people are still bullish about LLMs",
      "ko": "LLM에 대한 의문",
      "ja": null
    },
    "type": "story",
    "url": "https://twitter.com/skdh/status/1905132853672784121",
    "score": 55,
    "by": "ksec",
    "time": 1743110562,
    "content": "Sabine Hossenfelder@skdhI genuinely don't understand why some people are still bullish about LLMs.\n\nI use GPT, Grok, Gemini, Mistral etc every day in the hope they'll save me time searching for information and summarizing it. They continue to fabricate links, references, and quotes, like they did from day one.\n\nI ask them to give me a source for an alleged quote, I click on the link, it returns a 404 error. I Google for the alleged quote, it doesn't exist. They reference a scientific publication, I look it up, it doesn't exist.\n\nHappens all the time.\n\nYes, it has gotten somewhat better in the past 2 years in that with DeepSearch and chains of thought about 50-60% or so of the references exist. By my personal estimate currently GPT 4o DeepResearch is the best one. Grok in particular often doesn't include references even if asked. It can't seem to link even to tweets. It's hugely frustrating.\n\nYes, I have tried Gemini, and actually it was even worse in that it frequently refuses to even search for a source and instead gives me instructions for how to do it myself. Stopped using it for that reason.\n\nI also use them for quick estimates for orders of magnitude and they get them wrong all the time. One thing they do save me time with is unit conversion and collecting all kinds of constants. You'd think though that this shouldn't take a 100 million++ LLM to get done.\n\nYesterday I uploaded a paper to GPT to ask it to write a summary and it told me the paper is from 2023, when the header of the PDF clearly says it's from 2025. I don't even know what the heck is going on there, but intelligence ain't it.\n\nI sense that a lot of people now think knowledge graphs will fix the LLM-issue, but no, they will not. They cannot.\n\nEven in the case that knowledge graphs would prevent logical inconsistency 100%, there are a lot of text-constructions that are perfectly logically consistent but have zero relation to reality.\n\nCompanies will keep pumping up LLMs until the day a newcomer puts forward a different type of AI model that will swiftly outperform them. On that day, it will become apparent that a lot of companies have been hugely overvalued. It will be a very bad day for the stock market.Last editedOpens edit history2:40 PM · Mar 27, 2025·1M Views1K9105K1.3KRead 1K replies",
    "summary": {
      "en": "Sabine Hossenfelder expresses frustration with large language models (LLMs) like GPT, Grok, and Gemini, which she uses daily. She finds that these models often make up sources, links, and quotes that don't exist, leading to inaccuracies in their responses. Although there has been some improvement over the past two years, she estimates that only 50-60% of references provided by these models are legitimate.\n\nHossenfelder notes that Grok often fails to include references, while Gemini is even less reliable, frequently directing her to search for information herself. She also mentions that LLMs make frequent errors in estimating numerical values but can help with unit conversions and collecting constants.\n\nShe highlights a specific instance where GPT incorrectly dated a paper she uploaded, further illustrating her concerns about the models' reliability. Hossenfelder believes that while some hope knowledge graphs will solve LLM issues, they won't, as logical consistency doesn't guarantee truth. She predicts that companies investing in LLMs might face significant losses if a better AI model emerges, which could impact the stock market negatively.",
      "ko": "사빈 호센펠더는 매일 사용하는 대형 언어 모델(LLM)인 GPT, Grok, Gemini에 대해 불만을 표출하고 있다. 그녀는 이러한 모델들이 종종 존재하지 않는 출처, 링크, 인용구를 만들어내어 응답의 정확성을 떨어뜨린다고 지적한다. 지난 2년 동안 약간의 개선이 있었지만, 그녀는 이 모델들이 제공하는 출처의 50-60%만이 신뢰할 수 있다고 추정하고 있다.\n\n호센펠더는 Grok이 출처를 포함하지 않는 경우가 많고, Gemini는 더욱 신뢰할 수 없어서 자주 스스로 정보를 찾아보라고 한다고 언급한다. 또한 LLM들이 숫자 값을 추정하는 데 자주 오류를 범하지만, 단위 변환이나 상수 수집에는 도움을 줄 수 있다고 설명한다.\n\n그녀는 GPT가 자신이 업로드한 논문의 날짜를 잘못 기재한 구체적인 사례를 강조하며, 모델의 신뢰성에 대한 우려를 더욱 부각시킨다. 호센펠더는 일부 사람들이 지식 그래프가 LLM의 문제를 해결할 것이라고 기대하지만, 논리적 일관성이 진실을 보장하지 않기 때문에 그렇지 않을 것이라고 믿고 있다. 그녀는 LLM에 투자하는 기업들이 더 나은 AI 모델이 등장할 경우 큰 손실을 입을 수 있으며, 이는 주식 시장에 부정적인 영향을 미칠 수 있다고 예측한다.",
      "ja": null
    }
  },
  {
    "id": "4da14c24ca4164ea",
    "title": {
      "en": "First-of-its-kind trial enables paralysed man to stand via stem cell injection",
      "ko": "줄기세포로 일어선 남자",
      "ja": null
    },
    "type": "story",
    "url": "https://www.nature.com/articles/d41586-025-00863-0?linkId=13622861",
    "score": 111,
    "by": "bentobean",
    "time": 1742839624,
    "content": "NEWS\n                24 March 2025\n\n            Paralysed man stands again after receiving ‘reprogrammed’ stem cells\n\n                    Another man also regained some movement, but two others experienced minimal improvement.\n\n                By\n\n                Smriti Mallapaty\n\n                    Smriti Mallapaty\n\n                            View author publications\n\n                                You can also search for this author in PubMed\n                                    Google Scholar\n\n            Twitter\n\n            Facebook\n\n            Email\n\n                You have full access to this article via your institution.\n\n                         Nerve cells derived from induced pluripotent stem cells have the potential to reverse paralysis.Credit: IKELOS GmbH/Dr. Christopher B. Jackson/SPLA paralysed man can stand on his own after receiving an injection of neural stem cells to treat his spinal cord injury. The Japanese man was one of four individuals in a first-of-its-kind trial that used reprogrammed stem cells to treat people who are paralysed.Another man can now move his arms and legs following the treatment, but the two others did not show substantial improvements. The trial was run by Hideyuki Okano, a stem-cell scientist at Keio University in Tokyo, and his colleagues.The results, which were announced at a press conference on 21 March and have not yet been peer reviewed, suggest that the treatment is safe, say researchers.“That’s a great positive outcome. It’s very exciting for the field,” says James St John, a translational neuroscientist at Griffith University in the Gold Coast, Australia.Previous trials using other types of stem cell have also demonstrated that the therapy is safe, but have so far shown mixed results. “Nothing’s really worked so far,” says St John.Larger trials will be needed to establish whether the improvements observed in the two individuals in the current study were a result of the treatment. It’s possible the patients experienced a natural recovery, says St John.In 2019, roughly 0.9 million people globally experienced a spinal cord injury, and some 20 million people were living with the condition1.Reprogrammed cellsReprogrammed or induced pluripotent stem (iPS) cells are created by reverting adult cells to an embryonic-like state, from which they can be coaxed to develop into other cell types.In this trial, iPS cells derived from a donor were used to create neural precursor cells. Two million of these were injected into each patient’s injury site, in the hope that they would eventually develop into neurons and glial cells. The trial’s first surgery was performed in December 2021; the other three were conducted between 2022 and 2023. All four recipients were adult males and two were aged 60 or older. They all had surgery between two and four weeks after the damage was done, says Okano. Recipients were given immune-suppressing drugs to prevent their bodies from attacking the cells for six months after the surgery.The results are the latest in a series of small human trials testing the potential of iPS cells to regenerate tissue and treat disease.Learning to walkAt the one-year follow-up, the researchers did not observe any serious adverse effects.All individuals started the trial with the highest injury classification of A, as measured by the American Spinal Injury Association Impairment Scale (AIS). People with this level of impairment have no sensory or motor function below the point of injury. Two of the participants did not show improvements in their ability to feel or move in the lowest section of their spinal cord. One individual moved up to a classification of C in the period after surgery, and can move some of their arm and leg muscles but cannot stand on their own. Another individual improved to a level D (normal function is classified as E) and can stand independently. “That person is now training to walk,” says Okano. “This is a dramatic recovery.” Preliminary analysis of the data suggests the treatment works, says Okano.\n\n                                    Enjoying our latest content?\n                                    Login or create an account to continue\n\n                                            Access the most recent journalism from Nature's award-winning team\n                                            Explore the latest features & opinion covering groundbreaking research\n\n                                            Access through your institution\n\n                                    or\n\n                                            Sign in or create an account\n\n                                            Continue with Google\n\n                                            Continue with ORCiD\n\n                doi: https://doi.org/10.1038/d41586-025-00863-0\n\n                ReferencesDing, W. et al. Spine 47, 1532–1540 (2022).Article\n    PubMed\n\n                    Google Scholar\n                Muheremu, A., Shu, L., Liang, J., Aili, A. & Jiang, K. Transl. Neurosci. 12, 494–511 (2021).Article\n    PubMed\n\n                    Google Scholar\n                Download references\n\n                    Reprints and permissions\n\n                Related Articles\n\n                        Stem cells reverse woman’s diabetes — a world first\n\n                        World-first therapy using donor cells sends autoimmune diseases into remission\n\n                        ‘Reprogrammed’ stem cells to treat spinal-cord injuries for the first time\n\n                        ‘Reprogrammed’ stem cells approved to mend human hearts for the first time\n\n                        ‘﻿Reprogrammed’ stem cells to be tested in people with Parkinson’s\n\n                        Japanese man is first to receive 'reprogrammed' stem cells from another person\n\n                Subjects\n\n                Cell biology\n\n                Medical research\n\n                Physiology\n\n    Latest on:\n\n                Cell biology\n\n                                    BRCA2 prevents PARPi-mediated PARP1 retention to protect RAD51 filaments\n                                    Article 26 MAR 25\n\n                                    Oxidation of retromer complex controls mitochondrial translation\n                                    Article 26 MAR 25\n\n                                    The contribution of de novo coding mutations to meningomyelocele\n                                    Article 26 MAR 25\n\n                Medical research\n\n                                    Crucial meeting: molecule helps vaccine to interact with killer T cells\n                                    News & Views 26 MAR 25\n\n                                    Lasso-shaped molecule is a new type of broad-spectrum antibiotic\n                                    News & Views 26 MAR 25\n\n                                    Gene-modified pig-to-human liver xenotransplantation\n                                    Article 26 MAR 25\n\n                Physiology\n\n                                    Pregnancy’s true toll on the body: huge birth study paints most detailed picture yet\n                                    News 26 MAR 25\n\n                                    First map of human brain mitochondria is ‘groundbreaking’ achievement\n                                    News 26 MAR 25\n\n                                    Glutamate gating of AMPA-subtype iGluRs at physiological temperatures\n                                    Article 26 MAR 25\n\n                                    BRCA2 prevents PARPi-mediated PARP1 retention to protect RAD51 filaments\n                                    Article 26 MAR 25\n\n                                    Oxidation of retromer complex controls mitochondrial translation\n                                    Article 26 MAR 25\n\n                                    The contribution of de novo coding mutations to meningomyelocele\n                                    Article 26 MAR 25\n\n            Jobs\n\n                        Junior Group Leader Position at IMBA - Institute of Molecular Biotechnology\n\n                            TheInstitute of Molecular Biotechnology (IMBA)is one of Europe’s leading institutes for basic research in the life sciences. IMBA is located on t...\n                            Austria (AT)\n                            IMBA - Institute of Molecular Biotechnology\n\n                        Job Posting Title Associate or Senior Editor (Human Genetics), Nature Communications\n\n                            As an Associate or Senior Editor at Nature Communications you will handle manuscripts in Human Genetics.\n                            Jersey City, London, New York or Philadelphia - hybrid working model\n                            Springer Nature Ltd\n\n                        Solutions Specialist\n\n                            Job Title: Solutions Specialist  Location: New York or London (Hybrid Working) Closing date: March 31st, 2025  About Springer Nature Group Sprin...\n                            New York City, New York (US)\n                            Springer Nature Ltd\n\n                        Faculty Positions at FAFU  (Fujian Agriculture and Forestry University, Fuzhou, China)\n\n                            Fuzhou, Fujian (CN)\n                            Fujian Agriculture and Forestry University\n\n                        Department Chairs, School of Medicine at Southern University of Science and Technology (copy)\n\n                            Shenzhen, Guangdong, China\n                            Southern University of Science and Technology, School of Medicine",
    "summary": {
      "en": "A Japanese man who was paralyzed can now stand on his own after receiving an injection of reprogrammed stem cells for his spinal cord injury. He was one of four participants in a pioneering trial led by Hideyuki Okano at Keio University. Another participant regained some arm and leg movement, while the other two saw very little improvement. The trial is still in the early stages and has not been peer-reviewed, but researchers believe the treatment is safe.\n\nInduced pluripotent stem cells (iPS cells) were used in this study, created by reverting adult cells to an embryonic state. Two million of these cells were injected into each patient's injury site to potentially develop into nerve cells. All participants had severe spinal injuries and were given immune-suppressing drugs to prevent rejection of the cells.\n\nAt the one-year follow-up, no serious side effects were reported, and one participant is even training to walk. Larger trials are needed to confirm whether the observed improvements were due to the treatment or natural recovery.",
      "ko": "일본의 한 남성이 척수 손상으로 인해 마비되었으나, 재프로그래밍된 줄기세포 주사를 받은 후 혼자서 서 있을 수 있게 되었습니다. 그는 게이오 대학교의 오카노 히데유키 교수가 주도한 선구적인 임상 시험에 참여한 네 명 중 한 명입니다. 다른 한 참가자는 팔과 다리의 일부 움직임을 회복했으며, 나머지 두 명은 거의 개선이 없었습니다. 이 시험은 아직 초기 단계에 있으며 동료 평가를 받지 않았지만, 연구자들은 이 치료법이 안전하다고 믿고 있습니다.\n\n이 연구에서는 유도 만능 줄기세포(iPS 세포)가 사용되었습니다. 이는 성체 세포를 배아 상태로 되돌려 만들어진 세포입니다. 각 환자의 부상 부위에 이 세포 200만 개가 주입되어 신경 세포로 발전할 가능성을 모색했습니다. 모든 참가자는 심각한 척수 손상을 입었고, 세포 거부 반응을 방지하기 위해 면역 억제제를 투여받았습니다.\n\n1년 후 추적 조사에서는 심각한 부작용이 보고되지 않았으며, 한 참가자는 걷기 훈련을 하고 있습니다. 관찰된 개선이 치료 때문인지 자연 회복 때문인지 확인하기 위해서는 더 큰 규모의 임상 시험이 필요합니다.",
      "ja": null
    }
  },
  {
    "id": "e9ae83fde3e64616",
    "title": {
      "en": "Banned Books: Analysis of Censorship on Amazon.com (2024)",
      "ko": "금서의 진실: 아마존 검열 분석",
      "ja": null
    },
    "type": "story",
    "url": "https://citizenlab.ca/2024/11/analysis-of-censorship-on-amazon-com/",
    "score": 47,
    "by": "gnabgib",
    "time": 1743104270,
    "content": "ResearchFree Expression Online\n\n\tBanned Books\n\n\t Analysis of Censorship on Amazon.com\n\n            By\n            Jeffrey Knockel, Jakub Dalek, Noura Aljizawi, Mohamed Ahmed, Levi Meletti, and Justin Lau\n\n          November 25, 2024\n\n              الترجمة العربية (Arabic translation)\n\n                   Download this report\n\n                  Key findings\n\nWe analyze the system Amazon deploys on the US “amazon.com” storefront to restrict shipments of certain products to specific regions. We found 17,050 products that Amazon restricted from being shipped to at least one world region.\nWhile many of the shipping restrictions are related to regulations involving WiFi, car seats, and other heavily regulated product categories, the most common product category restricted by Amazon in our study was books.\nBanned books were largely related to LGBTIQ, the occult, erotica, Christianity, and health and wellness. The regions affected by this censorship were the UAE, Saudi Arabia, and many other Middle Eastern countries as well as Brunei Darussalam, Papua New Guinea, Seychelles, and Zambia. In our test sample, Amazon censored over 1.1% of the books sold on amazon.com in at least one of these regions.\nWe identified three major censorship blocklists which Amazon assigns to different regions. In numerous cases, the resulting censorship is either overly broad or miscategorized. Examples include the restriction of books relating to breast cancer, recipe books invoking “food porn” euphemisms, Nietzsche’s Gay Science, and “rainbow” Mentos candy.\nTo justify why restricted products cannot be shipped, Amazon uses varying error messages such as by conveying that an item is temporarily out of stock. In misleading its customers and censoring books, Amazon is violating its public commitments to both LGBTIQ and more broadly human rights.\nWe conclude our report by providing Amazon multiple recommendations to address concerns raised by our work.\n\nIntroduction\nThe rise in online shopping has led to more global reach into markets that may otherwise be inaccessible for companies through traditional retail channels. This increased reach brings new opportunities but also has its own challenges for global e-commerce retailers. One such challenge is in dealing with different, more restrictive regulatory environments worldwide.\nIn this report, we analyze American e-commerce retailer Amazon and its system for preventing shipments of certain products to certain world regions as it is implemented on the US storefront — amazon.com. Specifically, we analyze functionality that Amazon implements to restrict shipments of certain products to certain regions even if the product is available and sellers are offering to ship it there. While Amazon normally hides this restriction system from customers using misleading error messages, we employ a novel methodology to uncover and measure on which products and in which regions it is activated by peeling back the layers of Amazon’s website and analyzing its internal workings. Notably, our method can distinguish between a product being restricted by Amazon and it being organically unavailable in a region.\nIn total, we found 17,050 products that were restricted from being shipped to at least one world region. While many of the shipping restrictions observed in our study are related to regulations involving WiFi, car seats, and other heavily regulated product categories, the most common product category restricted by Amazon was books. Banned books were largely related to LGBTIQ, the occult, erotica, Christianity, and health and wellness. More broadly, books were the victims of censorship, which in this report we define as Amazon’s restriction of product shipment under political or religious motivation. The regions commonly affected by this censorship were the United Arab Emirates (UAE), Saudi Arabia, and many other Middle Eastern countries as well as Brunei Darussalam, Papua New Guinea, Seychelles, and Zambia.\nGiven that the topics censored include LGBTIQ, our findings call into question Amazon’s public commitment to LGBTIQ rights as well as its respect for the rights of its users at large. By censoring the availability of books, Amazon is depriving its users of valuable information. Furthermore, by communicating to customers that censored products are organically unavailable (e.g., being out of stock), Amazon is depriving customers of the ability to make informed decisions. We conclude our report by making multiple recommendations to Amazon.\nBackground\nIn this section we briefly describe Amazon’s history as it relates to our analysis. We then outline some of the regulations applying to Amazon’s business in Saudi Arabia, the UAE, and China, which are some of the more restrictive regulatory environments to which products on amazon.com can be shipped.\nAmazon background\nAmazon is an American multinational company that originated as an online bookseller and has since evolved into a global e-commerce marketplace. Amazon’s business is heavily focused on managing shipping logistics internationally and serving a global consumer base. Alongside the main e-commerce platform, they also provide cloud computing services (Amazon Web Services), consumer electronics (Amazon Kindle and Amazon Echo), and online streaming (Amazon Prime Video) among other offerings.\nAmazon is best known for its original website — amazon.com — which serves as the landing page for US customers, although items can be shipped globally depending on seller preferences. As of 2024, there are dedicated storefronts for 22 other regions. Alongside the online expansions to other regions there has been an analogous expansion of physical infrastructure in those regions including shipping hubs, fulfillment centers, sorting facilities, and delivery stations.\nMost relevant to our study, Amazon has expanded its dedicated storefronts to include the UAE in 2017 and Saudi Arabia in 2020. This expansion included opening a regional headquarters in Riyadh, Saudi Arabia, in 2022 and a fulfillment center in Dubai, the UAE, in 2023. These recent expansions into the Middle East create their own unique challenges to the retailer because of the region’s distinct regulatory regimes, which we detail below.\nCompliance with international regulations\nAmazon polices the products sold on its platform, and their own shipping restrictions FAQ provides some guidance on why certain products may be restricted, including the need to “comply with all laws and regulations and with Amazon policies” and that Amazon may be “restricted from shipping to your location due to government import/export requirements, manufacturer restrictions, or warranty issues”. Amazon has adapted its policies to allow for the removal of offensive content including content that Amazon determines is “hate speech, promotes the abuse or sexual exploitation of children, contains pornography, glorifies rape or pedophilia, advocates terrorism”, but also “other material [they] deem inappropriate or offensive”. However, Amazon has failed to reveal specifically what categories of content it restricts to comply with the demands of authoritarian governments.\nThere have been reported incidences where Amazon complied with governments’ requests to restrict certain products or even go as far as manipulate its reviews. For example, Amazon restricted items for purchase and in search results relating to over 150 keywords relating to LGBTIQ content in the UAE after receiving pressure from the government to remove them. In China, Amazon removed all customer ratings and reviews for a book of Chinese president Xi Jinping’s speeches and writings. In both instances, Amazon claimed that they were following local laws and regulations. However, in India, internal Amazon documents showed that Amazon was circumventing local regulations by providing preferential treatment to certain sellers and by also promoting its own merchandise by rigging search results. Amazon has also been criticized for allowing its platform to spread white supremacy and racism. Items with Nazi symbols and Kindle books associated with neo-Nazis and white supremacists have remained widely available despite Amazon having been notified by journalists and non-profit organizations.\nRegulations in Saudi Arabia\nIn Saudi Arabia, content is largely governed by two laws: the 2003 Law of Printing and Publication, largely regulating print media, and the 2007 Anti-Cyber Crimes Law, regulating online media. Article 9 of the Law of Printing and Publications states that printed media cannot contravene Sharia Law, stir up internal discord, injure the economic and health situation of the country, or lead to a breach of either public security, public policy, or foreign interests. Article 18 states these regulations should apply to the importation and distribution of printed materials. An approval is required, within the framework of Article 18 of the Printing and Publication law, in order to certify that content is free from any content that is insulting to Islam, the government, interests of the Emirates, or ethical standard and public morality. In terms of enforcement, Article 39 states that any contravening printed items can be withdrawn from circulation if they are found to violate either Articles 9 or 18.\nThe 2007 Anti-Cyber Crimes Law is chiefly focused on regulations around information security and content regulation. Article 6 of this law states that “production, preparation, transmission, or storage of material impinging on public order, religious values, public morals, or privacy, through an information network or computer” is a criminal offense. Contravening this article can lead to a maximum punishment of five years in prison and a maximum fine of three million riyals (approximately 800,000 USD). This law has been applied against online content. For example, in 2019, Saudi Arabia alerted Netflix that an episode of Hasan Minhaj’s comedy show Patriot Act violated this statute as it contained criticism of a Saudi Arabian royal. Netflix complied with the government order and restricted access to the episode for Saudi Arabian users.\nRegulations in the UAE\nIn the UAE, content is governed by the Federal Decree-Law No.55 of 2023 on Media Regulation, which replaced the previous Federal Law No.15 of 1980 Concerning Publications and Publishing. Specifically, it regulates print, television, as well as online media. Another relevant regulation is the Internet Access Management Regulatory Policy which focuses on the regulation of online content. Under this policy, the only two internet service providers (ISPs) in the UAE, Etisalat and Du, are required to block online content if requested by the Telecommunications and Digital Government Regulatory Authority. Prohibited Internet content includes pornography, contempt of religion, and promotion of or trading in prohibited commodities and services. Category 13 of the policy prohibits sites from promoting or trading in commodities prohibited or restricted by licenses in the UAE, including “prints, paintings, photographs, drawings, cards, books, magazines, and stone sculptures, which are contrary to the Islamic religion or public morals, or involving intent of corruption or sedition”.\nCompliance with Chinese demands\nIn 2004, Amazon entered the Chinese market via its acquisition of Joyo, a Chinese online bookstore. Amazon faced scrutiny for its political censorship of products on its Chinese site — amazon.cn. However, facing competition from domestic rivals, Amazon terminated its online store in China in 2019, although for a limited time overseas products were still sold on the amazon.cn site. Amazon still has other operations in China, such as Amazon Web Services (AWS), which is Amazon’s cloud computing service. Outside of China, in 2021, on the US Amazon storefront — amazon.com — Amazon partnered with China International Book Trading Corp, a state-owned firm that has been labeled as “China’s state propaganda arm”, to create a portal for selling books that amplify the Chinese Communist Party’s agenda.\nMethodology\nIn this section, we explain how we determine product availability across different regions. Our methodology consists of two phases. As our original motivation was to understand how Amazon censorship applies to Middle Eastern countries, in our first phase, we focus on studying how products and shipment restrictions vary across multiple countries in the Middle East. We were particularly motivated to understand the differences between restrictions imposed on the shipment of products to Middle Eastern countries in which Amazon operates a storefront (namely, the UAE and Saudi Arabia) versus those in which it does not. To understand how censorship applies more broadly to the world at large, in our second phase we pivot from the results of the first phase and measure product availability in regions across the globe.\nIn designing our methodology, we were motivated by eliminating false positives, even if doing so might introduce false negatives. The rationale is that we would rather omit the measurement of some instances of censorship rather than falsely attribute censorship to products that are not censored.\nIn the remainder of this section we explain the two phases of our methodology.\nPhase 1: Measuring censorship in Middle East\nOne way to try to measure Amazon censorship in Middle Eastern countries would be to visit those Amazon storefronts which are available in the Middle East, namely, the UAE’s amazon.ae or Saudi Arabia’s amazon.sa, and to try to determine which products are anomalously “missing” from being sold on these two Amazon sites. This approach, however, would be limited. For example, if we saw one book related to LGBTIQ topics that was sold on amazon.com but not amazon.ae, that might be due to the book being censored on amazon.ae, but another possibility is that the book was out of stock or not sufficiently popular to be sold in some countries. However, if we saw a disproportionately large number of books related to LGBTIQ topics that were available on amazon.com but not sold on amazon.ae, then we would have a stronger argument, but this argument would be at best a statistical argument, and for any individual product we would not be able to prove whether it was",
    "summary": {
      "en": "The report by Jeffrey Knockel and colleagues analyzes Amazon's censorship practices on its US website, particularly regarding the restriction of certain products, especially books, from being shipped to specific regions. Here are the key points:\n\n- **Censorship on Amazon**: Amazon has restricted over 17,000 products, with books being the most commonly affected category. The restrictions often target topics related to LGBTIQ issues, the occult, erotica, Christianity, and health and wellness.\n\n- **Affected Regions**: The regions most impacted by these shipping restrictions include the UAE, Saudi Arabia, and various other Middle Eastern countries, as well as Brunei, Papua New Guinea, Seychelles, and Zambia.\n\n- **Misleading Practices**: Amazon uses misleading error messages, suggesting products are out of stock when they are actually restricted, which undermines customers' ability to make informed decisions.\n\n- **Regulatory Compliance**: The report outlines how Amazon adapts its policies to comply with strict local laws governing content in regions like Saudi Arabia and the UAE, which can lead to broad censorship that goes beyond legal requirements.\n\n- **Recommendations**: The report concludes with several recommendations for Amazon to improve transparency and adhere more closely to its stated commitments to human rights and LGBTIQ rights.\n\nOverall, the analysis raises concerns about Amazon's practices and their implications for free expression and information access in various countries.",
      "ko": "제프리 노켈과 동료들이 작성한 보고서는 아마존의 미국 웹사이트에서의 검열 관행을 분석하고 있습니다. 특히 특정 지역으로의 배송이 제한된 제품, 특히 책에 대한 내용을 다루고 있습니다. 주요 내용은 다음과 같습니다.\n\n아마존은 17,000개 이상의 제품을 제한하고 있으며, 그 중 책이 가장 많이 영향을 받고 있습니다. 이러한 제한은 LGBTIQ 문제, 오컬트, 에로티카, 기독교, 건강 및 웰빙과 관련된 주제를 주로 겨냥하고 있습니다.\n\n배송 제한의 영향을 가장 많이 받는 지역은 아랍에미리트, 사우디아라비아, 그리고 여러 중동 국가들입니다. 또한 브루나이, 파푸아뉴기니, 세이셸, 잠비아 등도 포함됩니다.\n\n아마존은 제품이 실제로는 제한되어 있음에도 불구하고 재고가 없다는 잘못된 오류 메시지를 사용하여 고객이 올바른 결정을 내리는 것을 방해하고 있습니다.\n\n보고서는 아마존이 사우디아라비아와 아랍에미리트와 같은 지역의 엄격한 콘텐츠 관련 법률을 준수하기 위해 정책을 조정하는 방법을 설명하고 있습니다. 이로 인해 법적 요구 사항을 넘어서는 광범위한 검열이 발생할 수 있습니다.\n\n마지막으로, 보고서는 아마존이 투명성을 개선하고 인권 및 LGBTIQ 권리에 대한 약속을 더 잘 지킬 수 있도록 몇 가지 권고 사항을 제시하고 있습니다.\n\n전반적으로 이 분석은 아마존의 관행과 그것이 여러 국가에서의 자유로운 표현 및 정보 접근에 미치는 영향에 대한 우려를 제기하고 있습니다.",
      "ja": null
    }
  },
  {
    "id": "e4f517c188f4dad1",
    "title": {
      "en": "Philosophy of Coroutines (2023)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/coroutines-philosophy/",
    "score": 47,
    "by": "HeliumHydride",
    "time": 1743096215,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "beffd413028c8d44",
    "title": {
      "en": "Show HN: Dish: A lightweight HTTP and TCP socket monitoring tool written in Go",
      "ko": "디시: 경량 소켓 모니터링 툴",
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/thevxn/dish",
    "score": 14,
    "by": "tackx",
    "time": 1743107275,
    "content": "dish\n\ntiny one-shot monitoring service\nremote configuration of independent 'dish network' (via -source ${REMOTE_JSON_API_URL} flag)\nfast concurrent testing, low overall execution time, 10-sec timeout per socket by default\n0 dependencies\n\nInstall\ngo install go.vxn.dev/dish/cmd/dish@latest\n\nUsage\ndish -h\nUsage of dish:\n  -failedOnly\n        a bool, specifies whether only failed checks should be reported (default true)\n  -hname string\n        a string, custom additional header name\n  -hvalue string\n        a string, custom additional header value\n  -name string\n        a string, dish instance name (default \"generic-dish\")\n  -source string\n        a string, path to/URL JSON socket list (default \"./configs/demo_sockets.json\")\n  -target string\n        a string, result update path/URL to pushgateway, plaintext/byte output\n  -telegramBotToken string\n        a string, Telegram bot private token\n  -telegramChatID string\n        a string, Telegram chat/channel ID\n  -timeout uint\n        an int, timeout in seconds for http and tcp calls (default 10)\n  -updateURL string\n        a string, URL of the source api instance\n  -verbose\n        a bool, console stdout logging toggle\n  -webhookURL string\n        a string, URL of webhook endpoint\n\nSocket List\nThe list of sockets can be provided via a local JSON-formated file (e.g. demo_sockets.json file in the CWD), or via a remote REST/RESTful JSON API.\n# local JSON file\ndish -source /opt/dish/sockets.json\n\n# remote JSON API source\ndish -source http://restapi.example.com/dish/sockets/:instance\n\nAlerting\nWhen a socket test fails, it's always good to be notified. For this purpose, dish provides 4 different ways of doing so (can be combined):\n\ntest results upload to a remote JSON API (via -updateURL flag)\nfailed sockets list as the Telegram message body (via Telegram-related flags, see the help output above)\nfailed count and last test timestamp update to Pushgateway for Prometheus (via -pushgw and -target flags)\ntest results push to a webhook URL (via the webhookURL and webhooks flags)\n\n(The screenshot above shows the Telegram alerting as of v1.5.0.)\nExamples\nOne way to run dish is to build and install a binary executable.\n# Fetch and install the specific version\ngo install go.vxn.dev/dish/cmd/dish@latest\n\nexport PATH=$PATH:~/go/bin\n\n# Load sockets from sockets.json file, and use Telegram\n# provider for alerting\ndish -source sockets.json -telegram -telegramChatID \"-123456789\" \\\n -telegramBotToken \"123:AAAbcD_ef\"\n\n# Use remote JSON API service as socket source, and push\n# the results to Pushgateway\ndish -source https://api.example.com/dish/sockets -pushgw \\\n -target https://pushgw.example.com/\n\nUsing Docker\n# Copy, and/or edit dot-env file (optional)\ncp .env.example .env\nvi .env\n\n# Build a Docker image\nmake build\n\n# Run using docker compose stack\nmake run\n\n# Run using native docker run\ndocker run --rm \\\n dish:1.7.1-go1.23 \\\n -verbose \\\n -source https://api.example.com \\\n -pushgw \\\n -target https://pushgateway.example.com\n\nBash script and cronjob\nCreate a bash script to easily deploy dish and update its settings:\nvi tiny-dish-run.sh\n\n#!/bin/bash\n\nTELEGRAM_TOKEN=\"123:AAAbcD_ef\"\nTELEGRAM_CHATID=\"-123456789\"\n\nSOURCE_URL=https://api.example.com/dish/sockets\nUPDATE_URL=https://api.example.com/dish/sockets/results\nTARGET_URL=https://pushgw.example.com\n\nDISH_TAG=dish:1.6.0-go1.22\nINSTANCE_NAME=tiny-dish\n\nSWAPI_TOKEN=AbCd\n\ndocker run --rm \\\n        ${DISH_TAG} \\\n        -name ${INSTANCE_NAME} \\\n        -source ${SOURCE_URL} \\\n        -hvalue ${SWAPI_TOKEN} \\\n        -hname X-Auth-Token \\\n        -pushgw \\\n        -target ${TARGET_URL} \\\n        -update \\\n        -updateURL ${UPDATE_URL} \\\n        -telegram \\\n        -telegramBotToken ${TELEGRAM_TOKEN} \\\n        -telegramChatID ${TELEGRAM_CHATID} \\\n        -timeout 15 \\\n        -verbose\n\nMake it an executable:\nchmod +x tiny-dish-run.sh\n\nCronjob to run periodically\ncrontab -e\n\n# m h  dom mon dow   command\nMAILTO=monitoring@example.com\n\n*/2 * * * * /home/user/tiny-dish-run.sh\n\nHistory\ndish history article\nUse Cases\nThe idea of a tiny one-shot service comes with the need for a quick monitoring service implementation to test HTTP/S and generic TCP endpoints (or just sockets in general = hosts and their ports).",
    "summary": {
      "en": "**Summary of Dish Monitoring Service**\n\nDish is a lightweight monitoring service designed for quick testing of HTTP/S and TCP endpoints. Here are the key features:\n\n- **Functionality**: Supports remote configuration through a JSON API and allows fast concurrent testing with a default timeout of 10 seconds per socket.\n- **Installation**: Use the command `go install go.vxn.dev/dish/cmd/dish@latest` to install.\n- **Usage Options**:\n  - Use various flags for custom configurations, such as specifying a socket source, setting alert methods, and defining timeouts.\n  - Sockets can be provided via a local JSON file or a remote API.\n  \n- **Alerting Options**: Users can receive notifications through:\n  - Remote JSON API updates\n  - Telegram messages for failed checks\n  - Integration with Prometheus via Pushgateway\n  - Webhook notifications\n\n- **Examples**: Instructions are provided for running Dish from the command line, using Docker, or through a bash script that can be scheduled with cron.\n\n- **Use Cases**: Ideal for quick, on-demand monitoring of network sockets to ensure they are operational.\n\nThis service has no dependencies and is designed to be efficient and easy to set up.",
      "ko": "Dish는 HTTP/S와 TCP 엔드포인트를 신속하게 테스트하기 위해 설계된 경량 모니터링 서비스입니다. 주요 기능은 다음과 같습니다.\n\n먼저, Dish는 JSON API를 통해 원격 구성을 지원하며, 기본적으로 소켓당 10초의 타임아웃을 설정하여 빠른 동시 테스트를 가능하게 합니다. 설치는 `go install go.vxn.dev/dish/cmd/dish@latest` 명령어를 사용하면 됩니다.\n\n사용자는 다양한 플래그를 통해 맞춤형 구성을 할 수 있습니다. 예를 들어, 소켓 소스를 지정하거나 알림 방법을 설정하고 타임아웃을 정의할 수 있습니다. 소켓 정보는 로컬 JSON 파일이나 원격 API를 통해 제공할 수 있습니다.\n\n알림 옵션으로는 원격 JSON API 업데이트, 실패한 체크에 대한 텔레그램 메시지, Pushgateway를 통한 Prometheus 통합, 웹훅 알림 등이 있습니다.\n\n명령줄에서 Dish를 실행하는 방법, Docker를 사용하는 방법, cron으로 예약할 수 있는 bash 스크립트를 통한 실행 방법에 대한 예시도 제공됩니다.\n\n이 서비스는 네트워크 소켓의 상태를 신속하게 모니터링하는 데 적합하며, 의존성이 없고 효율적이며 쉽게 설정할 수 있도록 설계되었습니다.",
      "ja": null
    }
  },
  {
    "id": "80ec2b04d40d6e80",
    "title": {
      "en": "HDMI Musings: YCbCr color subsampling, Dolby Vision MEL/FEL, and and5V injection",
      "ko": "HDMI 탐구: 색상과 기술",
      "ja": null
    },
    "type": "story",
    "url": "http://archimago.blogspot.com/2025/03/hdmi-musings-high-speed-cables-data.html",
    "score": 14,
    "by": "colinprince",
    "time": 1743104999,
    "content": "Archimago's Musings\n\nA 'more objective' take for Rational Audiophiles. Among other topics!\nX/Twitter: @Archimago\nE-Mail: archimagosmusings(at)outlook.com\n[Some items linked to affiliate accounts - I may receive gift certs from qualifying purchases.]\n\nSaturday, 1 March 2025\n\nHDMI Musings: high speed cables, data rates, YCbCr color subsampling, Dolby Vision MEL/FEL, optical cables and +5V injection.\n\nHey folks, for this post, I thought it would be good to dive a little more into the world of AV technologies after discussing the nVidia Shield TV Pro last week. While \"classic\" audiophile technology (ie. standard hi-fi analog and digital 2-channel stereo without special DSP advancements) has matured nicely already, this isn't quite the case with modern digital video tech. While many (probably most) features have settled, we can see ongoing evolution of the High-Definition Multimedia Interface (HDMI) standard to be mindful of - for example the recent announcement of HDMI 2.2 at CES2025expanding capabilities well beyond the needs of today.As usual, it'll take time (years) for this standard to be incorporated into TVs and source devices like GPUs (latest nVidia RTX 5080/5090are HDMI 2.1b) or something like VR devices being at the forefront of potential generational gains. For more than 20 years, with each significant revision of the HDMI standard, we're seeing doubling of data speed with HDMI 2.2 now aiming at just shy of 100 gigabits-per-second (96Gbps), twice of the 48Gbps bit rate in current HDMI 2.1 products.This recent update makes HDMI the fastest of all currently-announced consumer Audio-Video connection standards, the one wire that basically does it all - hi-res video (with high dynamic and frame rate), hi-res audio (up to 32 PCM channels at 24/192, with DSD to 8-channel of DSD256), HDCP copy protection, audio return channel, even ethernet (100Mbps).This level of sophistication (and licensing costs) could make it difficult for small companies with limited R&D capabilities to get in the game with custom designs. This is probably in part why the cottage industry audiophile companies rarely use HDMI other than selling overprices cables (like AudioQuest). Plus frankly, basic 2-channel audio doesn't need the higher technical capabilities anyway (which is basically what Paul McGowan says).Let's run through a few thoughts about HDMI that might be good to know as general knowledge as tech enthusiasts. I'll provide a few links of interest, touch on tech stuff like color spaces and subsampling, and things you might want to try if you're running longer high-speed HDMI connections and potentially noticing issues.I. High speed HDMI, the variantsHDMI cables have basically looked the same since their release in 2002. They all have 19 pins on the connectors:Source. For HDMI compliance, the +5V pin sink on the \"sink\" end should not draw more than 55mA. And voltage should be regulated between 4.7-5.3V (~250mW). HDMI2.1b now has a HDMI Cable Power feature to provide up to 300mA (1.5W) for active cables.However, they need to conform to specifications in order to accurately pass along the data at high gigabits/second speeds based on TMDS signaling(note the 4 differential TMDS signal pin pairs). It is in these HDMI cables where we see companies apply techniques to minimize interference (like crosstalk, EMI), optimize conductivity (high purity copper wiring), improve durability, and use stringent manufacturing standards to achieve verifiably reliable performance.We can look at the official HDMI Cables Resource page to see the various cable classifications. As of this writing in early 2025, we have 3 cable speed grades:HDMI Standard (10.2 Gbps), HDMI High Speed (18 Gbps), and HDMI Ultra High Speed (48Gbps). Higher speed cables are backwards compatible with the slower ones. With the introduction of HDMI 2.2, we'll be seeing \"Ultra96\" cables certified for 96Gbps coming.Certification is important! The bulk of these gigabits-per-second communications for audio and video are unidirectional and non-error-corrected which is why one can see data errors show up in the image, or worse, disconnection if the data loss is severe (data errors will show up in a form similar to discussion of poor USB transmissionyears ago).One technical detail worth noting is that the Gbps numbers above signify \"bit rate\" which is higher than the actual \"data rate\" (often ~80% of bit rate). Data rate is the actual information passed along from transmitting to receiving device to be processed.So the actual data rates look like this for the various HDMI standards:HDMI 1.0-1.2(2002) 3.96GbpsHDMI1.3-1.4(2006) 8.16GbpsHDMI2.0(2013) 14.40GbpsHDMI 2.1(2017)  41.89GbpsHDMI 2.2(2025)  83.78Gbps (presumably HDMI 2.1 FRL-like)For those who want to see the nitty-gritty of HDMI data rate and these calculations, make sure to head over to this phenomenal LTT online calculator.Based on the data rate, this is how we know what screen resolutions and framerates the source device (like your UHD BluRay player, computer, AppleTV, nVidia Shield TV) and display can handle. In general, a rule of thumb is that if you want modern 4K/60fps HDR10/Dolby Vision, HDMI 2.0 will be enough. Above that, use HDMI 2.1 for 4K/120fps and 8K/60Hz. And if one day we need 12K/60fps, there's support in HDMI 2.2.[For fun, let's think about 12K resolution. About 11,520 x 6,480 pixels for typical 16:9 aspect ratio scaled up from our usual 4K of 3840x2160. That's 74.65 megapixels per frame. If this is used in a home theater, and we sit a generous 12' away from the screen, \"retina resolution\" will be achieved with a screen 550\" diagonal optimally! The TV would be 40' wide x 22.5' high -billboard-sized!Alternatively, say we use a very large 150\" diagonal screen (about 12.5' wide), we'll need to sit 3.2' away to get within the retinal resolution threshold of a 12K image. For reference, the largest flat panel TV I've seen these days would be the 115\" TCL QM89 which currently goes for US$20k.12K resolution? Gotta have it!🤣 Have fun with the retinal screen calculator here,play around with the numbers.]II. High Speed HDMI cables...Unlike \"high end audiophile cables\" where even with US$1000 asking price, there are no clear standards or certifications for quality (hence mostly a form of snake oil), HDMI cables do have a certification process. While companies will sell cables claiming to support standards like 48Gbps HDMI 2.1, look for evidence that the cable has been certified.It's usually not difficult to achieve rated speed for short cables like 6' and below. But if you are going higher like 10' or 15' wires, save yourself some headaches and buy the certified stuff.Recently I needed 15' of HDMI Ultra High Speed wire (HDMI 2.1) for my nVidia RTX 4090 GPU to 65\" TV set-up and I can vouch that the Monoprice 8K Certified Ultra High Speed HDMI 15'(US$27, CAD$33)worked out well:How do we confirm the product is certified? Look for the label on the package with the hologram:Scan the QR code on your phone and you should be able to confirm that the product has indeed been through the certification process off the HDMI.org site:While I'm sure anything can be faked, it'd be pretty silly to fake a label like this for a $30 product! As usual, make sure to buy from a reputable company that's not peddling in snake oil or \"luxury\" stuff unless that's what you're going for.This cable worked well for me with stable 4K/120fps over 15' (3840x2160, 120fps, 10-bit color, lossless RGB = 32.3Gbps data rate). Being NEC CL3 ratedfor fire-resistance, this can be run behind the walls. One caveat about this cable is that the metal connector is very robust and that strain relief is stiff. Make sure you have about 3-4\" clearance behind the connector on your TV or source device. I see Monoprice also sells the 8K Certified Braided Ultra HDMI cable which has a shorter connector and strain relief that will fit better in confined spaces / smaller devices.III. Video formats (RGB, YCbCr) and CompressionA bit like audio where we have PCM and DSD encoding formats, in the video world the data can be transferred from your source (eg. UHD BluRay player, computer, streamer) to the receiver/screen encoded in different ways. Ultimately, all video is converted to RGB (red, green, blue - the three additive primary colors) by the time the light is created on the screen. However, due to historical reasons back to the days of black & white TV, video encoding has typically been in the \"YCbCr\" (also known as \"YCC\" or \"YUV\") consumer video format:Y = Luma (the B&W brightness level)CbCr = Chroma - blue and red \"difference from neutral\"Since Cb and Cr refer to blue and red deviations from perceptual neutral (grayscale), the higher these values, the more color shifts these values represent.See here for the conversion formula between YCbCr to RGB.Since our vision is most sensitive to the brightness (luminance) signal, we can keep that resolution high but drop some of those color (chroma) signals for \"lossy\" compression which can still look very good in natural scenes. Compression is most noticeable on a computer when reading the fine details in colored static text. This color compression is represented by notations like 4:2:2, and 4:2:0 to indicate \"subsampling\" or \"downsampling\".Here's what those 3 numbers are referring to in the chroma subsampling nomenclature:4:4:4 is uncompressed - for every 4 horizontal B&W/luma pixels, we have corresponding 4 chroma values for the odd and even scanlines (you might recall the days we routinely used interlaced image formats where even and odd scanlines were displayed with each sweep of the CRT). We can \"subsample\" the color information down to 4:2:2 which means there's a 50% reduction (2 chroma for 4 luma samples) in the horizontal color values for both odd and even scanlines (1920x1080 screen resolution→ 960x1080 color resolution).It might be surprising to know that almost all video including all your (UHD) BluRays and online streams have been compressed to 4:2:0; both a reduction in horizontal and vertical color resolution (1920x1080 screen resolution now becomes→ 960x540 color resolution). Notice that the even scanlines contain no color information, rather it's interpolated from the odd scanlines. 4:2:0 contains only 25% of the color (chroma) information and requires ~50% of the overall data rate resulting in substantial data savings.[For an excellent detailed writeup to explore more of what those numbers represent with graphics, see Spears & Munsil's Choosing A Color Space, 2nd Edition.]Color values can be of different bit depth resolution - typically 8-bits in standard dynamic range, and 10-bits for HDR10 (high dynamic range). Dolby Vision is graded up to 12-bits with high dynamic range and dynamic metadata to allow greater tonal flexibility with changing scenes. Then there's also color gamutwhich is the description for the range of colors these numbers represent (we talked a little about this back in 2016 on 4K TVs):Note other terms like gamma correction/tone mapping, white point which wewon't discuss here you might want to be familiar with. (image source)Standard HDTV uses the Rec.709/sRGB (gamma 2.2) gamut whereas UHD-4K/HDR can encompass richer and deeper colors as per the larger Rec.2020 triangle above (also see Rec.2100for HDR).Taken together, these color variables describe the \"color space\". There's obviously quite a bit of technical complexity here. Visual data encompasses more variables than the comparatively simple 2-channel audio data that audiophiles are used to (even DSD is probably easier to appreciate).IV. A little more about Dolby Vision (MEL/FEL)While there has been competition with HDR10+ (HDR 10 Plus), announced by Samsung and Amazon in 2017, the premier high dynamic range color format remains Dolby Vision which simply has significantly more content. Both of these systems allow dynamic tone mapping to be applied scene-by-scene (even frame-by-frame) which provides more subtle shading/coloring as compared to the static map applied throughout a movie with the baseHDR10 profile available for all HDR displays.[An interesting aside, many displays such as projectors are not Dolby Vision capable. But you might be able to \"fake it\" using Low-Latency Dolby Vision (LLDV) using products like the HDFury Vertex2 - have a look at this article.]As we have seen over the years with audio technology, there are clearly diminishing returns as technology evolves. For example, the color bit depth (which refers to the number of shades of red/green/blue) of our flat screen display panels remain at 10-bits/color currently (total 30-bits adding RGB). Like with audio where we can only appreciate very high dynamic range if we push up the volume to deafening levels (limit of the best DACs somewhere around 20-21-bits even with the best 24-bit or 32-bit data), so too with video panels. While our typical TV sets might average peak 1000 nits (an easier term to say than \"candela/square meter\") brightness, professional monitors like the Flanders Scientific XM312U capable of an advertised 5000 nits are still 10-bit panels and cost over $20k.So how bright do we want/need our displays to be? How many bits?For me, even a 500 nits TV in a dark room is enough viewing from about 10' away. As with hi-fi audio and the need for low ambient noise for best listening experience, likewise I can't imagine seriously enjoying high quality video unless the lights are turned down.[Great comparison article here between HDR10/HDR10+/Dolby Vision.]If you're wondering, Dolby Vision is able to deliver 12-bit color to your display over HDMI as 12-bit 4:2:2 YCbCr encapsulated into 8-bit/color (24-bit total) RGB labeled as Rec.709. This data is then decoded in your DV-capable TV to its full 12-bit Rec.2100 glory. The data rate of 4K/60fps Dolby Vision therefore works out to 12.54Gbps, within the HDMI 2.0 maximum data rate of 14.4Gbps.True uncompressed 4K/60fps 12-bit RGB or 4:4:4 YCbCr will require over 18.5Gbps and thus HDMI 2.1 bandwidth would be required.One more thing about Dolby Vision for UHD BluRay connoisseurs - keep an eye out for Full Enhancement Layer (FEL, 12-bit) encoded discs as compared to standard Minimal Enhancement Layer (MEL, 10-bit) discs. Check out this list. Whether we can subjectively see a difference or not, 12-bits can objectively encode smoother gradients.FEL is available on favorites and interesting titles like Gladiator (2000), Forrest Gump (1994), Lord of War (2005), Saving Private Ryan (1998), Top Gun (1986), Back To The FutureTrilogy (1985), Braveheart (1995), Watchmen: Ultimate Cut (2009), andShutter Island (2010) if you're looking for examples.[Current video players like thenVidia Shield TV Pro (2019)andAppleTV 4K (latest 2022 model)will not decode the secondary Dolby Vision 12-bit Full Enhancement Layer. The only TV box I've seen that decodes UHD BluRay rips of Dolby Vision profile 7 with FEL is theUgoos AM6B Plus TV Box(based on Amlogic S922X-J SoC, about US$190,CAD$270) usingCoreELECKodi software. You might be able to find a better deal on AliExpress typically around US$160.A fun toy for you perfectionistic videophiles with ripped UHD BluRays libraries who don't mind a little DIY \"hacking\". 😉]Even though our display panels might be limited to 10-bits, that extra bit-depth data might still have some benefit in the display device to reduce banding according to Dolby (some debate here). Plus this is future-proof I suppose since the PQ transfer function is capable of luma up to 10,000 nits if ever we have displays capable of that kind of brightness - not sure we need such a thing!V. Optical HDMI for longer lengths and +5V Injector for marginal connectionsEven though the Monoprice 15' HDMI 2.1 cable discussed above works well for me, sometimes you might want even longer cables like say 20+ feet. For those circumstances, you should look into fiber optic and active cables/extenders. For my basement movie room, I've been using the inexpensive Ablink 30' Optical HDMI 2.1 cable(about CAD$50) which has worked well. I see many brands with similar designs like this, or this. Note that the \"optical\" connection refers to the high speed audio/video data transmission. Within these cables there are still copper wires for power and features like the ethernet line.Beyond length extension, another nice thing about optical HDMI is that the cables are thinner so easier to route down the conduits I installed behind the wall to hide wires. One does have to be careful not to damage the cable with excessive bending. Note that optical cables are genuinely directional (unlike expensive analog audio cables 🤔) so make sure you have the ends for the source and display connected appropriately.When using an active optical cable where the HDMI port has to supply power to the transceiver module, if you ever run into unexpected issues like the monitor losing sync going intermittently blank or notice data errors (pixel corruption, color banding), it could be because the HDMI port isn't supplying an adequately stable 5V to the transceiver (current limit for HDMI compliance is only 55mA for the sink). In these cases, you could try using a 5V USB injector (about $10), assuming you have a USB port nearby:If you don't have a USB port easily available,this one had its own AC adaptor.Plug that into the source HDMI side and use power from the USB output (eg. 5V/1A from my Integra DRX-8.4USB port) instead of just drawing from HDMI pin 18. Unless you have very long cables, for the most part, you should be OK but it's a good tool to consider for trouble shooting.+5V injector from USB port connected to optical 30' HDMI cable.Beyond the optical HDMI cables, there are active non-optical cables like this Cable Matters Active 48Gbps HDMI(again, mind the direction), orcoupling active signal booster (Cable Matters, StarTech) as alternate means of extending range.One more thing about power. Large screen TVs can suck up quite a bit of energy (150-200W isn't unusual with 4K UHD displays) with HDR turned on and the screen showing bright content. I've had situations where I plugged the TV into a power bar connected to other devices causing voltage droops that can result in loss of HDMI connection. The simple solution is to plug your TV into its own outlet. Don't share it with your AV receiver or subwoofer(s) which might also suck a few 100 watts.VI. Concluding thoughts...Yeah, I know, HDMI technology isn't the usual stuff audiophiles think about; but I think we should.🙂As modern audiophiles of the 21st Century, I think we need to appreciate audio as just one member of the family of multimedia technologies with converging qualitative goals toward higher fidelity reproduction. Whether it's the evolution of a high bandwidth interface, better recording capabilities, improved digital-to-analog conversion, evolution in storage technology, these things will have impacts across audio and visual digital media. Broader knowledge of current objective performance helps protect us from purely subjective foolish claims.Furthermore, if we seriously want to push audio reproduction forward, as I discussed a year ago in my article on Fidelity, Immersion, and Realism (FIR), we need to consider multichannel audio to better re-create realistic sound fields and the most common interface that this rides on is of course HDMI. HDMI is commonly available, there's already a lot of content, and does so at resolutions without lossy compromises thanks to these phenomenal bitrates. HDMI for home entertainment is positioned as the one cable/interface to use until maybe a day when wireless technologies might more conveniently take over.As I suggested above, I suspect the latest and greatest HDMI 2.2 will take a bit of time to be adopted since many of the latest features are simply not needed for most applications today (even ultra-ultra-high-res 8K/60fps displays are fine with HDMI 2.1). It's good that the hardware standard is available to encourage creativity and opportunities for content creators though!Some technologies will never need very high speed interfaces. For example hi-res 2-channel stereo audio hasn't needed to go beyond USB 2.0 (480Mbps) DACs even though we're up to USB 4 now(20-120Gbps!).Ultimately, the human sensory systems are finite and can be saturated quite easily by modern digital technology. Diminishing returns with higher data rates whether with audio or video is simply to be expected. I think it's great to be living at a time where the evolution is still happening quickly, literally before our eyes!I think that's all I have to say about HDMI and related topics for now. There is something else which I'll save for its own post next time.😉Hope you're all doing well as we enter March and the Spring weather ahead in the Northern Hemisphere!New music for the week: Panda Bear's \"Praise\" off Sinister Grift (2025, DR6 2-channel stereo, DR11 multichannel/Atmos). Appropriate album title given recent political developments!?Alt. rock with a bit of the Beach Boys vibe to my ears:And for some retro-remix pop, here's Roxette with Galantis' single \"Fading Like A Flower\" (again, very cool in multichannel); enjoy:\n\nPosted by\n\nArchimago's Musings\n\nat\n\n10:09:00\n\nEmail ThisBlogThis!Share to XShare to FacebookShare to Pinterest\n\nLabels:\n4K Video,\nDiminishing Returns,\nDolby Vision,\nFidelity,\nHDMI,\nHome Theater,\nMultichannel audio\n\n8 comments:\n\n    (function() {\n      var items = null;\n      var msgs = null;\n      var config = {};\n\n// <![CDATA[\n      var cursor = null;\n      if (items && items.length > 0) {\n        cursor = parseInt(items[items.length - 1].timestamp) + 1;\n      }\n\n      var bodyFromEntry = function(entry) {\n        var text = (entry &&\n                    ((entry.content && entry.content.$t) ||\n                     (entry.summary && entry.summary.$t))) ||\n            '';\n        if (entry && entry.gd$extendedProperty) {\n          for (var k in entry.gd$extendedProperty) {\n            if (entry.gd$extendedProperty[k].name == 'blogger.contentRemoved') {\n              return '<span class=\"deleted-comment\">' + text + '</span>';\n            }\n          }\n        }\n        return text;\n      }\n\n      var parse = function(data) {\n        cursor = null;\n        var comments = [];\n        if (data && data.feed && data.feed.entry) {\n          for (var i = 0, entry; entry = data.feed.entry[i]; i++) {\n            var comment = {};\n            // comment ID, parsed out of the original id format\n            var id = /blog-(\\d+).post-(\\d+)/.exec(entry.id.$t);\n            comment.id = id ? id[2] : null;\n            comment.body = bodyFromEntry(entry);\n            comment.timestamp = Date.parse(entry.published.$t) + '';\n            if (entry.author && entry.author.constructor === Array) {\n              var auth = entry.author[0];\n              if (auth) {\n                comment.author = {\n                  name: (auth.name ? auth.name.$t : undefined),\n                  profileUrl: (auth.uri ? auth.uri.$t : undefined),\n                  avatarUrl: (auth.gd$image ? auth.gd$image.src : undefined)\n                };\n              }\n            }\n            if (entry.link) {\n              if (entry.link[2]) {\n                comment.link = comment.permalink = entry.link[2].href;\n              }\n              if (entry.link[3]) {\n                var pid = /.*comments\\/default\\/(\\d+)\\?.*/.exec(entry.link[3].href);\n                if (pid && pid[1]) {\n                  comment.parentId = pid[1];\n                }\n              }\n            }\n            comment.deleteclass = 'item-control blog-admin';\n            if (entry.gd$extendedProperty) {\n              for (var k in entry.gd$extendedProperty) {\n                if (entry.gd$extendedProperty[k].name == 'blogger.itemClass') {\n                  comment.deleteclass += ' ' + entry.gd$extendedProperty[k].value;\n                } else if (entry.gd$extendedProperty[k].name == 'blogger.displayTime') {\n                  comment.displayTime = entry.gd$extendedProperty[k].value;\n                }\n              }\n            }\n            comments.push(comment);\n          }\n        }\n        return comments;\n      };\n\n      var paginator = function(callback) {\n        if (hasMore()) {\n          var url = config.feed + '?alt=json&v=2&orderby=published&reverse=false&max-results=50';\n          if (cursor) {\n            url += '&published-min=' + new Date(cursor).toISOString();\n          }\n          window.bloggercomments = function(data) {\n            var parsed = parse(data);\n            cursor = parsed.length < 50 ? null\n                : parseInt(parsed[parsed.length - 1].timestamp) + 1\n            callback(parsed);\n            window.bloggercomments = null;\n          }\n          url += '&callback=bloggercomments';\n          var script = document.createElement('script');\n          script.type = 'text/javascript';\n          script.src = url;\n          document.getElementsByTagName('head')[0].appendChild(script);\n        }\n      };\n      var hasMore = function() {\n        return !!cursor;\n      };\n      var getMeta = function(key, comment) {\n        if ('iswriter' == key) {\n          var matches = !!comment.author\n              && comment.author.name == config.authorName\n              && comment.author.profileUrl == config.authorUrl;\n          return matches ? 'true' : '';\n        } else if ('deletelink' == key) {\n          return config.baseUri + '/comment/delete/'\n               + config.blogId + '/' + comment.id;\n        } else if ('deleteclass' == key) {\n          return comment.deleteclass;\n        }\n        return '';\n      };\n\n      var replybox = null;\n      var replyUrlParts = null;\n      var replyParent = undefined;\n\n      var onReply = function(commentId, domId) {\n        if (replybox == null) {\n          // lazily cache replybox, and adjust to suit this style:\n          replybox = document.getElementById('comment-editor');\n          if (replybox != null) {\n            replybox.height = '250px';\n            replybox.style.display = 'block';\n            replyUrlParts = replybox.src.split('#');\n          }\n        }\n        if (replybox && (commentId !== replyParent)) {\n          replybox.src = '';\n          document.getElementById(domId).insertBefore(replybox, null);\n          replybox.src = replyUrlParts[0]\n              + (commentId ? '&parentID=' + commentId : '')\n              + '#' + replyUrlParts[1];\n          replyParent = commentId;\n        }\n      };\n\n      var hash = (window.location.hash || '#').substring(1);\n      var startThread, targetComment;\n      if (/^comment-form_/.test(hash)) {\n        startThread = hash.substring('comment-form_'.length);\n      } else if (/^c[0-9]+$/.test(hash)) {\n        targetComment = hash.substring(1);\n      }\n\n      // Configure commenting API:\n      var configJso = {\n        'maxDepth': config.maxThreadDepth\n      };\n      var provider = {\n        'id': config.postId,\n        'data': items,\n        'loadNext': paginator,\n        'hasMore': hasMore,\n        'getMeta': getMeta,\n        'onReply': onReply,\n        'rendered': true,\n        'initComment': targetComment,\n        'initReplyThread': startThread,\n        'config': configJso,\n        'messages': msgs\n      };\n\n      var render = function() {\n        if (window.goog && window.goog.comments) {\n          var holder = document.getElementById('comment-holder');\n          window.goog.comments.render(holder, provider);\n        }\n      };\n\n      // render now, or queue to render when library loads:\n      if (window.goog && window.goog.comments) {\n        render();\n      } else {\n        window.goog = window.goog || {};\n        window.goog.comments = window.goog.comments || {};\n        window.goog.comments.loadQueue = window.goog.comments.loadQueue || [];\n        window.goog.comments.loadQueue.push(render);\n      }\n    })();\n// ]]>\n\nBlogue à part2 March 2025 at 05:48Thanks for this excellent update on HDMI.The modern HDMI signal could support up to 32 ch but only the 8ch of LPCM.Considering the venue of ATMOS, we need at least 12ch, 16 ch will be even better.At my job, we used an HDMI to AoIP converter, to bring the gaming console 8ch ( 7.1) over audio over IP to our monitoring system.ReplyDeleteRepliesArchimago's Musings2 March 2025 at 13:47Interesting Blogue,I thought since HDMI 2.0, there were 32 potential PCM channels that could be supported by devices as part of the standard but as far as I am aware, there are no current consumer-level devices that expose more than 7.1 LPCM input and I don't think I've seen Windows support something like 7.1.4 to discretely address height channels.I might be mistaken and would love to know if there is such a thing as a receiver that opens up >7.1 on the HDMI and if computers can access those additional channels!DeleteRepliesReplyReplyDan3 March 2025 at 06:19Thanks amigo. This was interesting. It’s convinced me even more that 4K UHD Blu-ray is a waste of money. I do think HDR is a nice upgrade, but it’s not worth the investment needed for HDR. I’m happy with 1080p Blu-ray.In Britain, second-hand Blu-ray discs can be bought for as little as £1, up to around £4. One day, I’d like to upgrade my system to a 5.1.4 setup. I would call myself a film fan, but I only have a handful of favourites—one of which is Watchmen, which doesn’t have a Dolby Atmos mix.The film industry has done absolutely everything possible with 4k UHD Blu-ray to make it as appealing as dog shit.ReplyDeleteRepliesArchimago's Musings4 March 2025 at 10:20I hear what you're saying Dan,I think with the fast moving world of streaming and the availability of streamed 4K with multichannel audio including Atmos (and DTS-X), the idea of buying discs - especially more expensive ones over BluRay - is a tough one.I'm quite selective about the discs I buy and will only go for the ones I think might have the technical resolution for UHD BluRay to be better. This is why back in early 2018 I made a series of posts comparing 1080P with 4K (like this on Interstellar among others).The reality is that unless one just wants to see film gain better 😏, I don't bother with UHD for 35mm films. Only worthwhile IMO for those filmed at 70mm (like Dunkirk) or if it's filmed digitally beyond 1080P (like Blade Runner 2049). HDR is noticeable of course.To me it's a lot like CD-resolution vs. Hi-Res but IMO more worth it than hi-res audio because with a big enough screen and HDR, you'll see the difference. 1080P is certainly good enough to enjoy the movie; but if I want to own something that's the \"reference\" in my collection, then UHD BluRays can't be beat for collectors!DeleteRepliesReplyDan4 March 2025 at 13:40Hi amigo. Glad you enjoy 4K UHD Blu-ray. I can't justify the investment—I’ve tried my best, but I just can't do it.I should have made it clear in my first post: to get full 4K UHD capabilities, I’m looking at about £200 for the player, around £19 per disc, about £200 for each height ceiling speaker, and an amp with nine channels costing around £600. This is just a rough estimate.The gains of 4K HDR and Dolby Atmos don’t justify the cost. I’m happy with lossy 4K HDR streaming, I also think the internet will keep getting better. It won’t be long before it’s possible to stream lossless 4K HDR Dolby Atmos videos.DeleteRepliesReplyArchimago5 March 2025 at 18:53I hear ya Dan,The numbers do add up and certainly don't blame you for not seeing adequate value given what's needed! As usual, all this stuff have diminishing returns at some point and if one already has a good multichannel system and capable display and perfectly fine BluRay player, already all the pieces are there to enjoy the movies!Cheers!DeleteRepliesReplyReplyHenry Wilson5 March 2025 at 13:06I had similar thoughts to Blogue, I would like to see more LPCM channels standard over HDMI. Obviously 7.1 is very usable, but for a standard that goes 8 years between revisions I should have liked to see more channels to better unlock spatial audio this time around. Devices like the miniDSP Flex HTx are creeping toward obsoleting home theatre processors provided the source can decode Dolby/DTS and output multichannel LPCM. I'm sure you knew this already, its one of the main reasons I've considered products like the Ugoos you mention. Having the source, AVR/HTP, and TV all need Dolby licensing is so restrictive (and is hard to read as anything other than greedy triple-dipping by Dolby). Would MUCH rather see a blossoming of configurations possible where only the source/player needs these Dolby renderers, so the downstream products (DSP/DAC, TV) can be \"dumb\" focusing on doing just one or two things really well.Suspect I'm preaching to the choir here, but thought it was worth reiterating.ReplyDeleteRepliesArchimago5 March 2025 at 19:14Totally makes sense Henry,In my discussions with DAC manufacturers, I have enquired about things like high quality multichannel devices that can allow more flexibility (with high resolution of course), but most of the time the response I get is that the market is too small, not enough interest, etc. Maybe one day if more audiophile hobbyists get into multichannel, we might get greater interest from manufacturers.Yeah, the companies like Dolby would of course love to extract as much $$$ as they can, keeping the audio data wrapped within their proprietary system with licensing fees.I haven't looked into this more but I believe Chris Connaker on Audiophile Style continues to decode Atmos through the Dolby Reference Player as he discussed. Costs $400/year if one keeps the subscription though. That miniDSP Flex HTx looks like a great DAC!DeleteRepliesReplyReplyAdd commentLoad more...\n\n      BLOG_CMT_createIframe('https://www.blogger.com/rpc_relay.html');\n\nNewer Post\n\nOlder Post\n\nHome\n\nSubscribe to:\nPost Comments (Atom)\n\n     (adsbygoogle = window.adsbygoogle || []).push({});\n\nSearch This Blog\n\nTranslate\n언어 선택한국어가어갈리시아어과라니어광둥어구자라트어그리스어그린란드어나우아틀어(동부 우아스테카)네덜란드어네팔 바사어(네와르어)네팔어노르웨이어뉘어다리어덴마크어도그리어독일어돔베어드율라어디베히어딩카어라오어라트갈레어라트비아어라틴어러시아어롬바르디아어롬어루간다어루마니아어루바어루오어룩셈부르크어룬디어리구리아어리투아니아어림뷔르흐어링갈라어마두라어마라티어마르와디어마셜어마오리어마이틸어마카사르어마케도니아어말라가시어말라얄람어말레이어말레이어(자위)맘어맹크스어메도우 마리어메이테이어(마니푸르어)모리셔스 크리올어몰타어몽골어몽어미낭어미얀마어(버마어)미조어바스크어바시키르어바울레어바탁 시말룽운어바탁 카로어바탁토바어발루치어발리어밤바라어베네치아어베타위어베트남어벤다어벨라루스어벰바어벵골어보스니아어보즈푸리어부랴트어불가리아어브르타뉴어비콜어사모아어사미어(북부)사포텍어산스크리트어산탈어(라틴 문자)산탈어(올 치키 문자)상고어샨어세르비아어세부아노어세소토어세이셸 크리올어세페디어소말리아어쇼나어수수어순다어스와티어스와힐리어스웨덴어스코틀랜드 게일어스페인어슬로바키아어슬로베니아어시칠리아어신디어실레지아어싱할라어아랍어아르메니아어아바르어아삼어아와디어아이마라어아이슬란드어아이티 크리올어아일랜드어아제르바이잔어아체어아촐리어아파르어아프리칸스어알루르어알바니아어암하라어압하지야어야쿠트어에스토니아어에스페란토어에웨어오로모어오리야어오세트어오크어와라이어요루바어우드무르트어우르두어우즈베크어우크라이나어월로프어웨일즈어위구르어유카텍 마야어은다우어은데벨레어(남부)응코어이그보어이누크티투트어(라틴 문자)이누크티투트어(음절 문자)이디시어이반어이탈리아어인도네시아어일로카노어일본어자메이카 파투아어자바어조지아어종카어줄루어중국어(간체)중국어(번체)징포어차모로어체와어체첸어체코어총가어추바시어추우케어츠와나어카시어카자흐어카탈로니아어카팜팡안어칸나다어칸누리어케추아어켁치어코르시카어코미어코사어콕보록어콘칸어콩고어쿠르드어(소라니)쿠르드어(쿠르만지)크로아티아어크리오어크림 타타르어(라틴 문자)크림 타타르어(키릴 문자)크메르어키가어키냐르완다어키르기스어키투바어타마지트어타마지트어(티피나그)타밀어타지크어타타르어타히티어태국어터키어테툼어텔루구어톡 피신어통가어투르크멘어투바어툴루어툼부카어트위어티그리냐어티베트어티브어파슈토어파피아멘토어팡가시난어펀자브어(구르무키)펀자브어(샤무키)페로어페르시아어포르투갈어(브라질)포르투갈어(포르투갈)폰어폴란드어풀라니어프랑스어프랑스어(캐나다)프리울리어프리지아어피지어핀란드어필리핀어하와이어하우사어하카 친어헝가리어훈스뤼크어히브리어힌디어힐리가이논어번역에서 제공\n\n    function googleTranslateElementInit() {\n      new google.translate.TranslateElement({\n        pageLanguage: 'en',\n        autoDisplay: 'true',\n        layout: google.translate.TranslateElement.InlineLayout.VERTICAL\n      }, 'google_translate_element');\n    }\n\nBlog Archive\n\n        ▼\n\n2025\n\n(11)\n\n        ▼\n\nMarch 2025\n\n(3)\n\nMUSINGS: Trust, empirical testing, and evidence in...\nHigh Frame Rate (HFR) movies: time to overcome res...\nHDMI Musings: high speed cables, data rates, YCbCr...\n\n        ►\n\nFebruary 2025\n\n(4)\n\n        ►\n\nJanuary 2025\n\n(4)\n\n        ►\n\n2024\n\n(46)\n\n        ►\n\nDecember 2024\n\n(3)\n\n        ►\n\nNovember 2024\n\n(4)\n\n        ►\n\nOctober 2024\n\n(4)\n\n        ►\n\nSeptember 2024\n\n(4)\n\n        ►\n\nAugust 2024\n\n(5)\n\n        ►\n\nJuly 2024\n\n(3)\n\n        ►\n\nJune 2024\n\n(4)\n\n        ►\n\nMay 2024\n\n(4)\n\n        ►\n\nApril 2024\n\n(3)\n\n        ►\n\nMarch 2024\n\n(4)\n\n        ►\n\nFebruary 2024\n\n(4)\n\n        ►\n\nJanuary 2024\n\n(4)\n\n        ►\n\n2023\n\n(44)\n\n        ►\n\nDecember 2023\n\n(4)\n\n        ►\n\nNovember 2023\n\n(4)\n\n        ►\n\nOctober 2023\n\n(4)\n\n        ►\n\nSeptember 2023\n\n(3)\n\n        ►\n\nAugust 2023\n\n(4)\n\n        ►\n\nJuly 2023\n\n(4)\n\n        ►\n\nJune 2023\n\n(3)\n\n        ►\n\nMay 2023\n\n(5)\n\n        ►\n\nApril 2023\n\n(3)\n\n        ►\n\nMarch 2023\n\n(4)\n\n        ►\n\nFebruary 2023\n\n(3)\n\n        ►\n\nJanuary 2023\n\n(3)\n\n        ►\n\n2022\n\n(52)\n\n        ►\n\nDecember 2022\n\n(4)\n\n        ►\n\nNovember 2022\n\n(4)\n\n        ►\n\nOctober 2022\n\n(5)\n\n        ►\n\nSeptember 2022\n\n(4)\n\n        ►\n\nAugust 2022\n\n(5)\n\n        ►\n\nJuly 2022\n\n(6)\n\n        ►\n\nJune 2022\n\n(4)\n\n        ►\n\nMay 2022\n\n(4)\n\n        ►\n\nApril 2022\n\n(5)\n\n        ►\n\nMarch 2022\n\n(3)\n\n        ►\n\nFebruary 2022\n\n(4)\n\n        ►\n\nJanuary 2022\n\n(4)\n\n        ►\n\n2021\n\n(53)\n\n        ►\n\nDecember 2021\n\n(4)\n\n        ►\n\nNovember 2021\n\n(5)\n\n        ►\n\nOctober 2021\n\n(6)\n\n        ►\n\nSeptember 2021\n\n(4)\n\n        ►\n\nAugust 2021\n\n(4)\n\n        ►\n\nJuly 2021\n\n(4)\n\n        ►\n\nJune 2021\n\n(4)\n\n        ►\n\nMay 2021\n\n(5)\n\n        ►\n\nApril 2021\n\n(4)\n\n        ►\n\nMarch 2021\n\n(4)\n\n        ►\n\nFebruary 2021\n\n(4)\n\n        ►\n\nJanuary 2021\n\n(5)\n\n        ►\n\n2020\n\n(48)\n\n        ►\n\nDecember 2020\n\n(3)\n\n        ►\n\nNovember 2020\n\n(3)\n\n        ►\n\nOctober 2020\n\n(5)\n\n        ►\n\nSeptember 2020\n\n(3)\n\n        ►\n\nAugust 2020\n\n(5)\n\n        ►\n\nJuly 2020\n\n(3)\n\n        ►\n\nJune 2020\n\n(4)\n\n        ►\n\nMay 2020\n\n(4)\n\n        ►\n\nApril 2020\n\n(5)\n\n        ►\n\nMarch 2020\n\n(6)\n\n        ►\n\nFebruary 2020\n\n(4)\n\n        ►\n\nJanuary 2020\n\n(3)\n\n        ►\n\n2019\n\n(51)\n\n        ►\n\nDecember 2019\n\n(3)\n\n        ►\n\nNovember 2019\n\n(5)\n\n        ►\n\nOctober 2019\n\n(4)\n\n        ►\n\nSeptember 2019\n\n(7)\n\n        ►\n\nAugust 2019\n\n(4)\n\n        ►\n\nJuly 2019\n\n(3)\n\n        ►\n\nJune 2019\n\n(5)\n\n        ►\n\nMay 2019\n\n(4)\n\n        ►\n\nApril 2019\n\n(5)\n\n        ►\n\nMarch 2019\n\n(3)\n\n        ►\n\nFebruary 2019\n\n(4)\n\n        ►\n\nJanuary 2019\n\n(4)\n\n        ►\n\n2018\n\n(47)\n\n        ►\n\nDecember 2018\n\n(4)\n\n        ►\n\nNovember 2018\n\n(3)\n\n        ►\n\nOctober 2018\n\n(2)\n\n        ►\n\nSeptember 2018\n\n(5)\n\n        ►\n\nAugust 2018\n\n(4)\n\n        ►\n\nJuly 2018\n\n(4)\n\n        ►\n\nJune 2018\n\n(4)\n\n        ►\n\nMay 2018\n\n(4)\n\n        ►\n\nApril 2018\n\n(5)\n\n        ►\n\nMarch 2018\n\n(3)\n\n        ►\n\nFebruary 2018\n\n(4)\n\n        ►\n\nJanuary 2018\n\n(5)\n\n        ►\n\n2017\n\n(46)\n\n        ►\n\nDecember 2017\n\n(3)\n\n        ►\n\nNovember 2017\n\n(3)\n\n        ►\n\nOctober 2017\n\n(3)\n\n        ►\n\nSeptember 2017\n\n(5)\n\n        ►\n\nAugust 2017\n\n(3)\n\n        ►\n\nJuly 2017\n\n(4)\n\n        ►\n\nJune 2017\n\n(4)\n\n        ►\n\nMay 2017\n\n(4)\n\n        ►\n\nApril 2017\n\n(5)\n\n        ►\n\nMarch 2017\n\n(3)\n\n        ►\n\nFebruary 2017\n\n(4)\n\n        ►\n\nJanuary 2017\n\n(5)\n\n        ►\n\n2016\n\n(46)\n\n        ►\n\nDecember 2016\n\n(4)\n\n        ►\n\nNovember 2016\n\n(3)\n\n        ►\n\nOctober 2016\n\n(4)\n\n        ►\n\nSeptember 2016\n\n(4)\n\n        ►\n\nAugust 2016\n\n(4)\n\n        ►\n\nJuly 2016\n\n(3)\n\n        ►\n\nJune 2016\n\n(4)\n\n        ►\n\nMay 2016\n\n(3)\n\n        ►\n\nApril 2016\n\n(4)\n\n        ►\n\nMarch 2016\n\n(4)\n\n        ►\n\nFebruary 2016\n\n(5)\n\n        ►\n\nJanuary 2016\n\n(4)\n\n        ►\n\n2015\n\n(51)\n\n        ►\n\nDecember 2015\n\n(4)\n\n        ►\n\nNovember 2015\n\n(3)\n\n        ►\n\nOctober 2015\n\n(2)\n\n        ►\n\nSeptember 2015\n\n(5)\n\n        ►\n\nAugust 2015\n\n(5)\n\n        ►\n\nJuly 2015\n\n(6)\n\n        ►\n\nJune 2015\n\n(5)\n\n        ►\n\nMay 2015\n\n(4)\n\n        ►\n\nApril 2015\n\n(4)\n\n        ►\n\nMarch 2015\n\n(4)\n\n        ►\n\nFebruary 2015\n\n(4)\n\n        ►\n\nJanuary 2015\n\n(5)\n\n        ►\n\n2014\n\n(49)\n\n        ►\n\nDecember 2014\n\n(4)\n\n        ►\n\nNovember 2014\n\n(2)\n\n        ►\n\nOctober 2014\n\n(4)\n\n        ►\n\nSeptember 2014\n\n(4)\n\n        ►\n\nAugust 2014\n\n(4)\n\n        ►\n\nJuly 2014\n\n(5)\n\n        ►\n\nJune 2014\n\n(4)\n\n        ►\n\nMay 2014\n\n(2)\n\n        ►\n\nApril 2014\n\n(4)\n\n        ►\n\nMarch 2014\n\n(7)\n\n        ►\n\nFebruary 2014\n\n(5)\n\n        ►\n\nJanuary 2014\n\n(4)\n\n        ►\n\n2013\n\n(65)\n\n        ►\n\nDecember 2013\n\n(3)\n\n        ►\n\nNovember 2013\n\n(2)\n\n        ►\n\nOctober 2013\n\n(4)\n\n        ►\n\nSeptember 2013\n\n(4)\n\n        ►\n\nAugust 2013\n\n(2)\n\n        ►\n\nJuly 2013\n\n(3)\n\n        ►\n\nJune 2013\n\n(8)\n\n        ►\n\nMay 2013\n\n(10)\n\n        ►\n\nApril 2013\n\n(6)\n\n        ►\n\nMarch 2013\n\n(4)\n\n        ►\n\nFebruary 2013\n\n(18)\n\n        ►\n\nJanuary 2013\n\n(1)\n\n        ►\n\n2012\n\n(2)\n\n        ►\n\nDecember 2012\n\n(1)\n\n        ►\n\nMay 2012\n\n(1)\n\n     (adsbygoogle = window.adsbygoogle || []).push({});\n\nTotal Views:\n\n048143241347448536641739834944103211100129413551435153616401743184419402041213622352337243525322632273428312943\n11,920,330\n\n     (adsbygoogle = window.adsbygoogle || []).push({});\n\nMost read over last year:\n\n24-Bit vs. 16-Bit Audio Test - Part II: RESULTS & CONCLUSIONS\n\nDo CD and lossless streaming sound the same? A response to ana[dia]log video. And on excessive complications in the audiophile culture.\n\nPart II: Fosi Audio V3 Mono Amp; Class D + PFFB, TI TPA3255 - Retail box with single 48V/10A power supply and filter. [Power, Distortion, and Subjective Impressions.] And on rejoicing over \"Solved Problems\" in audio.\n\nPart II: Comparison of Bluetooth Fidelity - AAC encoder quality (Android 10 & 13, Windows 11, Apple iPhones & Mac)\n\n\"High-End\" DAC Blind Listening Results - PART II: Results & Analyses\n\nExpensive Audio & Medical Quackery: Mark Levinson promoting Daniel Hertz \"C Wave Technology\". And the Maria amps. [Including company response.]\n\nnVidia Shield TV Pro (2019 version), still relevant in 2025! A listen to Auro-3D & Auro-Matic Upsampler.\n\nMUSINGS: Myths, clichés and Hi-Fi+'s Taiko Audio SGM Extreme computer review. A hypothetical fast, fanless audio computer build. And \"channels/bit-depth/samplerate\" labeling convention - a suggestion.\n\nPart I: 3e Audio - A5 Stereo and A7 Mono Class D + PFFB, TI TPA3251/3255 Amplifiers - Features and initial measurements.\n\nPart II: 3e Audio - A5 Stereo and A7 Mono Class D + PFFB, TI TPA3251/3255 Amplifiers - power, distortion, and the subjective. (And a few photos for Swifties.)\n\n     (adsbygoogle = window.adsbygoogle || []).push({});\n\nLabels / Topics:\n\nMedia criticism\n(80)\n\nComputer Audio\n(48)\n\nDAC\n(48)\n\nMQA\n(48)\n\nListening Test\n(43)\n\nSnake Oil\n(37)\n\nAudiophile Philosophy\n(33)\n\nMultichannel audio\n(30)\n\nHi-Res Audio\n(29)\n\nAudiophile psychology\n(28)\n\nDigital Filters\n(27)\n\nAmplifier\n(25)\n\nAudio Cables\n(24)\n\nObjectivism vs. Subjectivism\n(19)\n\nAudio Shows\n(17)\n\nDigital Room Correction\n(17)\n\nVinyl\n(17)\n\n4K Video\n(16)\n\nHow-To\n(14)\n\nJitter\n(14)\n\nMeasurement Technique\n(14)\n\nheadphones\n(14)\n\nClass D\n(13)\n\nEthernet\n(13)\n\nSpeakers\n(13)\n\nLossy CODEC\n(12)\n\nUpsampling\n(12)\n\nApple\n(11)\n\nAudiophile Network\n(11)\n\nDIY\n(11)\n\nS.M.S.L.\n(11)\n\nSound Room\n(11)\n\nDolby Atmos\n(10)\n\nHDMI\n(10)\n\nMiniPC\n(10)\n\nMusic Streaming Services\n(10)\n\nTopping\n(10)\n\nADC\n(9)\n\nDemo\n(9)\n\nDynamic Range Compression\n(9)\n\nE1DA\n(9)\n\nRoon\n(9)\n\nqSpin\n(9)\n\nDynamic Range\n(8)\n\nHigh-End Audio\n(8)\n\nRetro\n(8)\n\nTHD+N\n(8)\n\nAcourate\n(7)\n\nCPU\n(7)\n\nDistortion\n(7)\n\nNoise\n(7)\n\nRME\n(7)\n\n4K vs. 1080P\n(6)\n\nDigital Audio Player\n(6)\n\nDigital vs. Analogue\n(6)\n\nDiminishing Returns\n(6)\n\nLP\n(6)\n\nPoll\n(6)\n\nSACD\n(6)\n\nSpatial Audio\n(6)\n\nUSB Cable\n(6)\n\nheadphone amp\n(6)\n\nSpeaker Cables\n(5)\n\nWindows\n(5)\n\nAI\n(4)\n\nAccuracy\n(4)\n\nBalanced vs. Unbalanced\n(4)\n\nBluetooth\n(4)\n\nCD Players\n(4)\n\nCOVID-19\n(4)\n\nFUD\n(4)\n\nIEM\n(4)\n\nMusic Playlist\n(4)\n\nPlayer Software\n(4)\n\nTagging\n(4)\n\nearphones\n(4)\n\nAMPT\n(3)\n\nAcoustics\n(3)\n\nAuditory Acuity\n(3)\n\nBattery Power\n(3)\n\nBits Are Bits\n(3)\n\nChristmas\n(3)\n\nClass A\n(3)\n\nGalvanic Isolation\n(3)\n\nMoFiGate\n(3)\n\nNOS\n(3)\n\nNetwork Switch\n(3)\n\nPCM vs. DSD\n(3)\n\nS/PDIF\n(3)\n\nSINAD\n(3)\n\nSabaj\n(3)\n\nSubwoofer\n(3)\n\nUSB Noise\n(3)\n\nAudio Demo\n(2)\n\nAudiophile Fantasies\n(2)\n\nClass A/B\n(2)\n\nESS\n(2)\n\nFidelity\n(2)\n\nGood Enough\n(2)\n\nImpedance\n(2)\n\nIntersample Overload\n(2)\n\nLinux\n(2)\n\nNon-utilitarian Factors\n(2)\n\nOSX\n(2)\n\nPower\n(2)\n\nR-2R\n(2)\n\nSocial Trends\n(2)\n\nSpeaker Impedance\n(2)\n\nSubjective Preferences\n(2)\n\nUnderclock\n(2)\n\nground loops\n(2)\n\nAKM\n(1)\n\nAnonymity\n(1)\n\nAtmos\n(1)\n\nAudio Conversion\n(1)\n\nAudio Production\n(1)\n\nAudiophile Forums\n(1)\n\nBooks\n(1)\n\nCognition\n(1)\n\nDemographics\n(1)\n\nDesign\n(1)\n\nDithering\n(1)\n\nDownsampling\n(1)\n\nEQ\n(1)\n\nEsthetics\n(1)\n\nHardware fetishism\n(1)\n\nIntentional Imperfections\n(1)\n\nLossless CODEC\n(1)\n\nMusic\n(1)\n\nNumerical Obsessionality\n(1)\n\nObsessionality\n(1)\n\nOperating Systems\n(1)\n\nPerceptibly Perfect\n(1)\n\nPower Conditioner\n(1)\n\nPre-Emphasis\n(1)\n\nRoom Treatment\n(1)\n\nSociological microcosm\n(1)\n\nSound Quality\n(1)\n\nTest Tracks\n(1)\n\ncalibration\n(1)\n\n     (adsbygoogle = window.adsbygoogle || []).push({});\n\n(C) 2012-2025 Archimago. Watermark theme. Powered by Blogger.",
    "summary": {
      "en": "**Summary of Archimago's Musings on HDMI and AV Technologies**\n\nIn a recent blog post, Archimago discusses advancements in HDMI technology, focusing on the latest HDMI 2.2 standard. This new version significantly increases data speeds to nearly 100 Gbps, supporting high-resolution video, high dynamic range (HDR), and multi-channel audio. The post emphasizes the importance of using certified HDMI cables to ensure quality and avoid issues like data loss or image errors.\n\nKey points include:\n\n1. **HDMI Evolution**: The HDMI standard has evolved over 20 years, with each update doubling the data speed. HDMI 2.2 is set to enhance capabilities further, although many devices still rely on HDMI 2.1.\n\n2. **Cable Types**: There are different grades of HDMI cables (Standard, High Speed, and Ultra High Speed), and certification is crucial for high-performance connections, especially in longer cables.\n\n3. **Video Formats**: The blog explains color encoding formats like RGB and YCbCr, and how video is often compressed using chroma subsampling (e.g., 4:2:0), impacting color quality.\n\n4. **Dolby Vision**: This HDR format is highlighted for its superior content and dynamic tone mapping capabilities over competitors like HDR10+.\n\n5. **Longer Cables**: For long-distance connections, optical HDMI cables are recommended, which are thinner and can handle high data rates.\n\n6. **Future Outlook**: Archimago suggests that while HDMI technology is critical for audio-visual media, the rapid advancements may lead to diminishing returns in quality, especially since human perception has limits. \n\nOverall, the post advocates for audiophiles to expand their knowledge of video technologies as they increasingly intersect with audio experiences.",
      "ko": "최근 블로그 글에서 아르키마고는 HDMI 기술의 발전에 대해 이야기하며 최신 HDMI 2.2 표준에 주목하고 있습니다. 이 새로운 버전은 데이터 속도를 거의 100 Gbps로 대폭 증가시켜 고해상도 비디오, 고동적 범위(HDR), 다채널 오디오를 지원합니다. 글에서는 품질을 보장하고 데이터 손실이나 이미지 오류와 같은 문제를 피하기 위해 인증된 HDMI 케이블을 사용하는 것이 중요하다고 강조합니다.\n\nHDMI 표준은 20년 이상 발전해 왔으며, 각 업데이트마다 데이터 속도가 두 배로 증가했습니다. HDMI 2.2는 기능을 더욱 향상시킬 예정이지만, 여전히 많은 기기가 HDMI 2.1에 의존하고 있습니다. HDMI 케이블에는 표준, 고속, 초고속 등 다양한 등급이 있으며, 특히 긴 케이블의 경우 고성능 연결을 위해 인증이 필수적입니다.\n\n블로그에서는 RGB와 YCbCr과 같은 색상 인코딩 형식과 비디오가 종종 크로마 서브샘플링(예: 4:2:0)을 사용해 압축된다는 점을 설명하며, 이로 인해 색상 품질에 영향을 미친다고 언급합니다. 돌비 비전은 경쟁사인 HDR10+보다 우수한 콘텐츠와 동적 톤 매핑 기능으로 주목받고 있습니다.\n\n장거리 연결을 위해서는 얇고 높은 데이터 전송 속도를 처리할 수 있는 광 HDMI 케이블이 추천됩니다. 아르키마고는 HDMI 기술이 오디오-비주얼 미디어에 필수적이지만, 빠른 발전이 품질의 감소로 이어질 수 있다고 경고합니다. 이는 인간의 인식 한계 때문입니다.\n\n전반적으로 이 글은 오디오 애호가들이 비디오 기술에 대한 지식을 확장할 것을 권장하며, 이러한 기술들이 오디오 경험과 점점 더 밀접하게 연결되고 있음을 강조합니다.",
      "ja": null
    }
  },
  {
    "id": "b1401d688e92d96d",
    "title": {
      "en": "Using Gorilla glass for home building",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.wsj.com/business/corning-window-gorilla-glass-4f443b07",
    "score": 57,
    "by": "nailer",
    "time": 1742776058,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "aad4bbdfbd20907f",
    "title": {
      "en": "Source code art in the Rivulet language",
      "ko": "리뷰렛 코드 아트",
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/rottytooth/Rivulet",
    "score": 141,
    "by": "cranbor",
    "time": 1743076519,
    "content": "Rivulet\nRivulet is a programming language of flowing strands, written in semigraphic characters. A strand is not pictographic: its flow does not simulate computation. There are four kinds of strands, each with their own symbolism and grammatical rules. Together, they form glyphs, tightly-packed blocks of code whose strands execute together.\nHere is a complete Fibonacci program:\n   ╵──╮───╮╭─    ╵╵╭────────╮\n    ╰─╯╰──╯│       ╰─╶ ╶╮╶╮╶╯\n   ╰─────╮ │      ╭─────╯ ╰─────╮\n         ╰─╯ ╷    ╰───       ───╯╷\n\n   ╵╵─╮  ╭─╮     ╭──       ╵╵╰─╮  ──╮──╮\n      ╰─╮│ ╰─╯ ╵╵╰─╯╶╮       ╴─╯  ╭─╯╭─╯\n      ╰─╯╰─ ╰──╯╰────╯       ╭╴ ╵╶╯ ╶╯╶╮\n        ╭─╮ ╭╴               │  ╰──────╯\n        │ │ │                ╰─╮       ╭─╮\n      │ │ ╰─╯                  │     │   │\n      ╰─╯            ╷         ╰──── ╰───╯╷\n\n   ╵╵ ╭──  ──╮  ╭─╮         ╵╰─╮\n      ╰─╮  ╭─╯╭─╯ │          ╴─╯\n       ╶╯╵╶╯  │ ╷╶╯          ╭─╮\n     ╭─╮ ╰────╯ │   ╭─╮        │\n     │ ╰────╮ ╭─╯ ╭╴│ │      ╭─╯\n     ╰────╮ │ │ │ │ │ │      │\n     ╭────╯ │ │ ╰─╯ │ ╷      ╰─╷\n     ╰────╮ │ ╰─────╯ │\n          │ ╰─────────╯╷\n\nHere is the same program formatted by the interpreter into an svg, alongside two variations that produce equivalent computer instructions:\n\nFibonacci 1\nFibonacci 2\nFibonacci 4\n\n⚠️ WARNING\n\nStatus: Version 0.4. This is a mostly-working interpreter, and a tool to generate svg files of source code. The command list will likely need to expand for usability.\n\nDesign Philosophy\nRivulet is a list-based language that avoids ordinary approaches to branching and looping. Strands never split and no strand is left un-executed.\nIts writing system was inspired by the satisfying compactness of mazes, Anni Albers's Meanders series, and space-filling algorithms. Its calligraphic aspects draw from natural language and favor the ability to write by hand.\nData Model\nIn Rivulet, data is organized into lists of adjacent cells, populated with zeros by default. Commands are applied to either a single cell or an entire list. They take a second parameter, a constant or the value of another cell.\nCommands can also be run list-to-list, applying the command to each successive cell of one list, from the corresponding cells of the other. While these consider zero-populated cells as well, a list-to-list command ends at the last cell holding a value in either list.\nThe first list, List 1, is sometimes used as the output stream. This is an interpreter setting, as is whether they are displayed as numerical data or a Unicode string (where each value is rounded to the nearest integer).\nControl Flow\nEvery strand of every glyph runs in a Rivulet program; there is no equivalent of an \"if\" statement. If a glyph leads to an unwanted state, that glyph and the others of its block (all contiguous glyphs of the same level or higher), can be rolled back, setting the execution state to what it was before the glyph (or set of glyphs) fired. The conditional rollback is the only form of branching in Rivulet. Loops only end with a rollback of their last iteration. Tests for rollback are that a single cell or an entire list is either zero or non-zero, indicated by a special strand called the Question Strand.\nData strands are run in the order they begin at the top left, moving through each column flowing to the right. So the strand beginning at coordinate 2,0 is run, then 2,1, then 3,0, and so on. Question strands are always run after the data strands are executed.\nSyntax\nRivulet's nuanced grammar may seem overwhelming at first but becomes easy to read and write with practice.\nGlyphs\nGlyphs begin with markers: ╵ in the upper left and end with ╷ at the bottom right. They must not have a vertically-oriented character directly above or below them, or they'll be confused for strands. Any text outside of glyph markers is ignored.\nThe level of the glyph is marked by how many ╵s appear at the beginning of the glyph. Levels tell where glyphs fall within larger blocks of code.\nGlyphs can be arranged vertically or side-by-side. They are read in the order of their starting marker location: top-to-bottom, left-to-right.\nIn other words, this program:\n1 ╵╰──╮╰─ ╭──╯ ╶╮\n2    ─┘   └─    │\n3    ╭──────────┘\n5    └────────  ╷\n\n1 ╵╵     ╭───╮ ╭─\n2    ╴─╮╶╯╶╮ ╷╶╯\n3  ╵╰──┘   │\n5  ╰───────╯\n\nis identical to this one:\n1 ╵╰──╮╰─ ╭──╯ ╶╮ ╵╵     ╭───╮ ╭─\n2    ─┘   └─    │    ╴─╮╶╯╶╮ ╷╶╯\n3    ╭──────────┘  ╵╰──┘   │\n5    └────────  ╷  ╰───────╯\n\nLines of code\nThe interpreter refers to code locations in terms of glyph numbers and then line numbers. Line numbers reset to 1 in each glyph. After line 1, they are numbered for each successive prime. These numbers are semantically meaningful for some strands.\nOther strand types use horizontal line numbers, counting in primes away from their starting hook. They always begin on line 1 and their neighbors are 2 on each side. They progress through primes, but always in distance from their starting point. Line numbers are every-other-line vertically: this is so vertical lines are not packed too tightly.\nHere are examples of such strands:\n  ╭─╮ ╭╴  ╭╴\n  │ │ │   │ │\n│ │ ╰─╯   │ │\n╰─╯       ╰─╯\n5 3 2 1   1 2\n\nLexemes\nRivulet commands are written with these signs. Some re-use characters in a way that only context can disambiguate:\n\nName\nSigns\nContext\nInterpretation\n\nGlyph Start and End\n╵ ╷\nNot be adjacent another sign with a vertical reading\nMarks the glyph, the smallest block of code in Rivulet\n\nLocation\n╵ ╷ ╴╶\nLeaves a gap, to punctuate the end of a strand e.g. from left: ──╶\nA reference pointer to a cell\n\nContinue\n─ │\nContinues the flows in the same direction e.g.  ────\nDepending on the strand type, it can add or subtract the line number of its horizontal or vertical line number, or simply continue the strand\n\nCorner\n╯┘╰└ ╮┐╭┌\nSharp or curved corners have the same meaning and can be used interchangeably\nTurns direction of flow\n\nHook\n╯┘╰╴└╴ ╮┐╭╴┌╴\nIt's a character or characters that turn ninety degrees at the beginning of some strands. If it turns to the right or left, it is extended with a half-length line, the same character used to indicate Location, but flipped to extend the hook and not leave a gap.\n\nNon-hook Begin Strand\n╷ above a │\nStrands with no hook begin with the half-length character to extend it\nMarks the beginning of a Question Strand\n\nData Strands\nValue Strands\nA value strand indicates a command that takes a constant value. Value strands (and other data strands), begin with a hook that points up (as in the third strand below) or to the left (as in the first two). All three of the strands below are value strands:\n1 ╵╰──╮╭──╯╶╮\n2    ─┘└─   └─╮\n3\n5              ╷\n\nEach of these value strands writes to list 1, as their hooks sit on line 1. The first strand writes to the first cell (cell 0), as it appears first on that line, the second writes to the second cell (cell 1), etc.\nThe first strand moves two spaces (two ── characters) to the right on line 1, adding 1 twice. It then moves one ── to the left on line 2, subtracting two. This leaves zero. This makes the first strand a zero strand. The default command applied to a strand is addition assignement, and so zero strands usually invoke no operation.\nThe second strand is also a zero strand: it makes the same motions in reverse of the first strand, subtracting two and then adding two strand back.\nThe third strand adds the value two to the third cell or list 1. The importance of the two zero strands is in marking the third strand to write to cell 2 of list 1, rather than cell 0.\nReference Strands\nReference strands look identical to value strands, only they end with a Location Marker, a small gap that punctuates the end of the strand. It appears in the two top strands here:\n1 ╵╰──╮  ╭──╯\n2   ╴─┘╶╮└─╶\n3       └─╮\n5            ╷\n\nThe movement of Reference Strands back and forth through the glyph has no effect on what they reference; only where they end.\nThe first strand above is no longer a Zero Strand, but a reference to the first cell (cell 0) of List 2. The second strand beginning on line 1 refers to the second cell (cell 1) of List 2. This is because between those two strands is a strand writing to cell 0 of List 2. If we wanted both of the top strands to read from cell 0 of List 2, we would move its end to before that assignment (here using the vertical version of the Location Marker):\n1 ╵╰──╮╭────╯\n2   ╴─┘╷╶╮\n3        └─╮\n5            ╷\n\nAction Strands\nThe default command is addition assignment ( += ). To choose another commands, we create an Action Strand to apply to an existing data strand.\nAction Strands have hooks that point down or to the right. They sit directly below the data strand they apply to. If two data strands' hooks are aligned vertically, the top action strand applies to the top data strand, the second to the second, etc.\nWhere a data strand's value is determined by movements to the left and right, action strands determine value through vertical movement. Their line numbers are independent of the other strands in the glyph, each beginning with line 1 as the column where they begin. Their neighbors to the left and right are line 2, followed by 3 and 5.\nEXAMPLE: This command that raises the values of list 1, cells 0 and 1, each to their fourth power:\n 1 ╵╰─╮ ╰─╮\n 2    │   │\n 3  ╭╴└─╭╴└─\n 5  │   │\n 7  │   │\n11  ╰─╮ ╰─╮\n13    │   │ ╷\n    1 2 1 2\n\nThe action strands each have a value of 4, which corresponds to exponentiation_assignment, under data strands of value 4. Here is the (INCOMPLETE) command list, showing which values assign to what command:\n\nValue\nCommand\nInterpretation\n\ndefault\naddition_assignment\nadd to location, set to zero by default\n\n0\noverwrite\nassignment, overwriting existing value\n\n1\ninsert\ninserts value after indicated cell\n\n-1\nsubtraction assignment\n\n2\nmultiplication assignment\n\n-2\ndivision assignment\n\n3\nno-op\nTBD; currently only has value when assigned to list\n\n-3\nmod assignment\nmodulus of cell value against supplied argument\n\n4\nexponentiation assignment\nraise to power of supplied argument\"\n\n-4\nroot assignment\ntake root at power of supplied argument\n\n:WARNING: It is every-other-line that increments between the primes, as the vertical length for a block-drawing char is longer than their horizontal length. This sounds confusing but is usually clear visually.\nHere is an example of two action strands and their numbering:\n  ╭─╮ ╭╴  ╭╴\n  │ │ │   │ │\n│ │ ╰─╯   │ │\n╰─╯       ╰─╯\n5 3 2 1   1 2\n\nThe first strand has a value of: (1 - 2 + 2*3 - 5) = 2, multiplication assignment. The second strand has a value of (2 * 1) - (2 * 2) = -2, division assignment.\nList indicator\nAction strands can also mark that a command applies not to a single cell (as is the default) but to an entire list. This is indicated by ending an action strand with a horizontal movement. When a list indicator appears, the data strand maintains the same order as if it were its cell that updates. If cell 3 has an action strand, it is still run after cell 2 and before cell 4 strands.\nList 2 List\nIf an action strand ends with a location marker (the tiny gap), it shows that the action should be applied for every cell of the referenced list to every cell of the assigned list. This is only syntactically valid when the data strand also ends with a location marker (is a reference strand).\nEvery cell with a number in the second list is applied to the cells in the first.\nQuestion Strand Sets\nQuestion Strands appear in pairs, one above the other.\nTogether, they pose a question about the state of the data. Should it be found wanting, the glyph and its siblings (those at the same level) are rolled back. If in a loop, only the most recent iteration is undone. This is the only way to exit a loop.\nThe top question strand begins with a vertical line. It ends either to the left or right of where it began (above or below has no semantic meaning).\nThe bottom question strand begins directly above its partner. It too ends either to the left or right of where it began, and it ends with a vertical piece (indicating the question applies only to a single cell) or a horizontal piece (indicating the entire list is to be questioned, the answer an accumulation of its answer).\nQuestion strands, read only by their beginning vs end, can move back and forth through the glyph, filling in blank spaces. They are often decorative, gap-filling lines.\nQuestion lines always fail if an item is less than or equal to zero.\n\nTop Line\nBottom Line\nUse\nChecks\n\nLeft\nHorizontal\nIf\nList (all items)\n\nLeft\nVertical\nIf\nCell\n\nRight\nHorizontal\nWhile\nList\n\nRight\nVertical\nWhile\nCell\n\n(any) vs (all) are equivalent if testing only a single cell",
    "summary": {
      "en": "**Summary of Rivulet Programming Language**\n\nRivulet is a unique programming language that uses flowing strands made of semi-graphic characters instead of traditional coding syntax. Here are the key points:\n\n1. **Strands and Glyphs**: \n   - There are four types of strands, each with specific symbols and rules.\n   - Strands combine to form glyphs, which are blocks of code executed together.\n\n2. **Basic Functionality**: \n   - Rivulet has a simple structure where each strand executes without branching or looping. \n   - If a glyph produces an undesired result, it can be rolled back to a previous state.\n\n3. **Data Organization**: \n   - Data is arranged in lists of cells, starting with zeros. \n   - Commands can operate on individual cells or entire lists.\n\n4. **Control Flow**: \n   - The language does not use \"if\" statements; instead, it relies on rollback for control. \n   - A special strand called the Question Strand checks conditions and can trigger rollbacks.\n\n5. **Syntax**: \n   - Glyphs are marked with specific characters and read from top to bottom, left to right.\n   - The grammar might seem complicated initially but becomes easier with practice.\n\n6. **Commands**: \n   - Commands are represented by various signs, with some context-dependent meanings. \n   - The default operation is addition, but there are other commands for different mathematical operations.\n\n7. **Action Strands**: \n   - These strands apply commands to data strands, allowing for various operations like multiplication and division.\n\n8. **Question Strands**: \n   - They assess the state of data and determine whether to roll back or continue execution based on conditions.\n\nOverall, Rivulet emphasizes a list-based, visual programming approach that is compact and inspired by design concepts from mazes and art.",
      "ko": "리뷰렛은 전통적인 코딩 문법 대신 반그래픽 문자로 이루어진 흐르는 줄을 사용하는 독특한 프로그래밍 언어입니다. 주요 특징은 다음과 같습니다.\n\n리뷰렛에는 네 가지 유형의 줄이 있으며, 각각 특정 기호와 규칙을 가지고 있습니다. 이 줄들이 결합되어 글리프를 형성하며, 글리프는 함께 실행되는 코드 블록입니다.\n\n리뷰렛의 기본 구조는 간단하여 각 줄이 분기나 반복 없이 실행됩니다. 만약 글리프가 원하지 않는 결과를 낼 경우, 이전 상태로 되돌릴 수 있습니다.\n\n데이터는 0으로 시작하는 셀의 리스트로 구성됩니다. 명령은 개별 셀이나 전체 리스트에 적용할 수 있습니다.\n\n이 언어는 \"if\" 문을 사용하지 않고, 대신 되돌리기를 통해 제어합니다. 질문 줄이라는 특별한 줄이 조건을 확인하고 필요에 따라 되돌리기를 트리거할 수 있습니다.\n\n글리프는 특정 문자로 표시되며, 위에서 아래로, 왼쪽에서 오른쪽으로 읽습니다. 문법은 처음에는 복잡하게 느껴질 수 있지만, 연습을 통해 쉽게 익힐 수 있습니다.\n\n명령은 다양한 기호로 표현되며, 일부는 문맥에 따라 의미가 달라집니다. 기본적인 연산은 덧셈이지만, 다른 수학적 연산을 위한 명령도 존재합니다.\n\n행동 줄은 데이터 줄에 명령을 적용하여 곱셈이나 나눗셈과 같은 다양한 연산을 가능하게 합니다.\n\n질문 줄은 데이터의 상태를 평가하고 조건에 따라 되돌리기를 할지 계속 실행할지를 결정합니다.\n\n전반적으로 리뷰렛은 리스트 기반의 시각적 프로그래밍 접근 방식을 강조하며, 간결하고 미로와 예술에서 영감을 받은 디자인 개념을 반영하고 있습니다.",
      "ja": null
    }
  },
  {
    "id": "5581076f942aae2e",
    "title": {
      "en": "Parameter-free KV cache compression for memory-efficient long-context LLMs",
      "ko": "메모리 절약 LLM을 위한 KV 캐시 압축",
      "ja": null
    },
    "type": "story",
    "url": "https://arxiv.org/abs/2503.10714",
    "score": 55,
    "by": "PaulHoule",
    "time": 1743098861,
    "content": "The linear growth of key-value (KV) cache memory and quadratic computational complexity pose significant bottlenecks for large language models (LLMs) in long-context processing. While existing KV cache optimization methods address these challenges through token pruning or feature merging, they often suffer from irreversible information loss or require costly parameter retraining. We propose ZeroMerge, a dynamic zero-shot compression framework that achieves efficient cache management through three key innovations: (1) Fine-grained memory allocation guided by multi-dimensional token importance metrics at head-level granularity, (2) A residual merging mechanism that preserves critical context through compensated attention scoring, and (3) Parameter-free adaptation compatible with diverse LLM architectures without retraining. Comprehensive evaluations across LLaMA-2 model demonstrate that ZeroMerge maintains full-cache performance at 5\\% compression ratios while doubling inference throughput at 40K token lengths. The method effectively balances memory efficiency, generation quality, and deployment flexibility, advancing practical long-context LLM applications. The code is available at this https URL.",
    "summary": {
      "en": "Large language models (LLMs) face challenges with memory and processing speed when dealing with long texts. Current methods to optimize their key-value (KV) cache often lead to lost information or need expensive retraining. We introduce ZeroMerge, a new approach that improves cache management without these downsides. It works through three main innovations: \n\n1. **Smart Memory Use**: It allocates memory based on the importance of different tokens.\n2. **Context Preservation**: It merges information in a way that keeps important context intact.\n3. **No Retraining Needed**: It can be adapted to various LLMs without requiring retraining.\n\nTests on the LLaMA-2 model show that ZeroMerge can compress data by 5% while significantly improving processing speed for long texts. This method enhances memory efficiency and quality, making it a strong option for using LLMs with long contexts. The code for ZeroMerge is available online.",
      "ko": "대형 언어 모델(LLM)은 긴 텍스트를 처리할 때 메모리와 처리 속도에서 어려움을 겪습니다. 현재 키-값(KV) 캐시를 최적화하는 방법은 종종 정보 손실을 초래하거나 비싼 재훈련이 필요합니다. 우리는 이러한 단점을 극복하는 새로운 접근 방식인 제로머지(ZeroMerge)를 소개합니다. 제로머지는 세 가지 주요 혁신을 통해 캐시 관리를 개선합니다.\n\n첫째, 스마트 메모리 사용입니다. 이는 다양한 토큰의 중요도에 따라 메모리를 할당합니다. 둘째, 맥락 보존입니다. 중요한 맥락을 유지하면서 정보를 병합하는 방식으로 작동합니다. 셋째, 재훈련이 필요하지 않습니다. 다양한 LLM에 재훈련 없이 쉽게 적용할 수 있습니다.\n\nLLaMA-2 모델에 대한 테스트 결과, 제로머지는 데이터를 5% 압축하면서 긴 텍스트의 처리 속도를 크게 향상시킬 수 있음을 보여주었습니다. 이 방법은 메모리 효율성과 품질을 높여 긴 맥락을 가진 LLM을 사용하는 데 강력한 옵션이 됩니다. 제로머지의 코드는 온라인에서 확인할 수 있습니다.",
      "ja": null
    }
  },
  {
    "id": "884d3da19b9a13c6",
    "title": {
      "en": "Dagger: A shell for the container age",
      "ko": "다거: 컨테이너 시대의 껍데기",
      "ja": null
    },
    "type": "story",
    "url": "https://dagger.io/blog/dagger-shell",
    "score": 112,
    "by": "gk1",
    "time": 1743020773,
    "content": "html body { background: var(--token-a24aec66-d34e-473f-9079-178faf2e7341, rgb(13, 12, 27)); }Dagger CloudDaggerverseBlogDocsCommunitySign inJoin us for Hack Night in LondonRegister NowA Shell for the Container Age: Introducing Dagger ShellMarch 26, 2025ShareShareThe Unix shell is over 50 years old, but it still defines how programmers use their computers. We type a few words in a terminal, and milliseconds later an ephemeral factory comes online: the Unix pipeline. Data streams through a network of simple programs working concurrently, like robots on the factory floor, executing a computational choreography we composed seconds ago. Its job done, the factory vanishes. Onto the next command. That loop built the internet, and still runs it today.The design principles of Unix are timeless: write simple programs, compose them with standard interfaces. But 50 years is a long time… the software stack has expanded, and Unix interfaces are buried under layers of abstraction. We still use the shell, but it’s no longer the universal composition system it was meant to be. What if we changed that?What if the Unix shell took the best ideas from docker, make, powershell and nix? Native support for primitives like containers, secrets and service endpoints; typed objects; declarative execution; content-addressed artifacts; everything sandboxed and cached by default. What if those all became standard shell features, available with standard shell syntax, and backed by a modern API? Would we still need to learn a dozen DSLs to automate the basic tasks of shipping software? Or would the complexity of the modern software stack melt away, leaving only two fundamental elements: shell and code? Let’s find out!Today we’re introducing Dagger Shell: a bash syntax frontend for our state-of-the-art runtime and composition system, the Dagger Engine. Use it for builds, tests, ephemeral environments, deployments, or any other task you want to automate from the terminal. It’s also a great way to compose AI agents…but more on that later.To get started, just install the latest version of Dagger, and type dagger. Or keep reading for examples.A complement to your system shellDagger Shell isn’t meant to replace your system shell, but to complement it. When a workflow is too complex to fit in a regular shell, the next available option is often a brittle monolith: not as simple as a shell script, not as robust as full-blown software. The Dagger Shell aspires to help you replace that monolith with a collection of simple modules, composed with standard interfaces.container |\n  from alpine |\n  with-exec apk add git\n\nShell and code are all you needWhen a script becomes too complex and outgrows the shell syntax, why learn a weird DSL when you can write real code?Unix was built on the twin pillars of its shell and C programming environment. Dagger follows the same model, with SDKs available for Go, Python, Typescript, Java and PHP. Pick a language, write a function…Congratulations, you just extended Dagger with your own primitive! No matter how complex your system, you should only ever need two building blocks: shell and code.Shell, meet APIDagger Shell is just another Dagger API client, giving you typed objects, built-in documentation, and access to a cross-language ecosystem of reusable modules.That’s right, Dagger Shell can load any of the thousands of modules in the Daggerverse, inspect their API, and run their functions. Here we explore a Trivy module:Sandboxed by defaultDagger Shell commands run as sandboxed functions, accessing host resources (files, secrets, services) only when explicitly provided as arguments.This can make commands slightly more verbose, but also more repeatable, giving you confidence to iterate quickly without second-guessing.For example, here’s a dev environment using a secret, local directory, and database access:\n\nSimple container buildThis creates an Alpine container, drops in a text file with your message, sets it to display that message when run, and publishes it to a temporary registry. All in one pipeline - no context switching between Dockerfile creation, build commands, and registry pushes.Test environmentsA common problem in CI is creating ephemeral test environments: you need to containerize the software being tested, and connect it to its dependencies. Dagger makes this easy with native support for service bindings.For example, let’s create an environment with several live instances of the Dagger docs hooked up, and “test” them with curl.This also highlights how third-party modules can be seamlessly mixed and matched.# Build a wolfi linux container with curl, then test connection to stable and dev docs\ngithub.com/dagger/dagger/modules/wolfi | container --packages=curl |\n  with-service-binding docs-stable $(github.com/dagger/dagger/docs@v0.17.1 | server) |\n  with-service-binding docs-dev $(github.com/dagger/dagger/docs@main | server) |\n  with-exec curl http://docs-stable |\n  with-exec curl\n\nMulti-Stage BuildsImplement complex build workflows with clear, modular syntax. Maintain visibility and control over each stage of your pipeline. Save and reuse parts of your pipeline. Why juggle between editing Dockerfiles, running build and push commands, when you can do it all in one go?repo=$(git https://github.com/dagger/hello-dagger | head | tree)\n\nenv=$(container | from node:23 | with-directory /app $repo | with-workdir /app)\n\nbuild=$($env | with-exec npm install | with-exec npm run build | directory ./dist)\n\ncontainer | from nginx | with-directory /usr/share/nginx/html $build | terminal --cmd\n\nThis example pulls source code, creates a Node.js build environment, runs the build process, and packages just the built files into a minimal nginx container. Each stage is a named variable you can inspect, modify, or reuse - the state is explicit, not hidden in layers of a Dockerfile or build cache.Get a dev build of Dagger, fresh from our repository:container |\n  from golang:latest |\n  with-directory /src $(git https://github.com/dagger/dagger | head | tree) |\n  with-workdir /src |\n  with-exec go build ./cmd/dagger |\n  file ./dagger |\n  export\n\nWhat’s Next?We’d love for you to try out Dagger Shell and share your feedback. Learn more in the Dagger Shell documentation, join our Discord and let us know how Dagger Shell improves your development workflow!Happy hacking! 🚀Get Involved With the communityDiscover what our community is doing, and join the conversation on Discord & GitHub to help shape the evolution of Dagger.DiscordGitHubCommunitySubscribe to our newsletterResourcesDocsBlogVideosCommunityEventsGet InvolvedDagger LoveDagger CommandersCommunity SDKsProductDagger CloudDaggerverseCompanyPartnersCareersBrandTerms of ServicePrivacy PolicyTrademark GuidelinesDagger Trust Center© Dagger 2022-2025",
    "summary": {
      "en": "Dagger has introduced Dagger Shell, a new tool designed to enhance the traditional Unix shell experience. The Unix shell, which has been around for over 50 years, allows programmers to execute commands and automate tasks. However, as software has evolved, its complexity has increased, making traditional Unix interfaces less effective.\n\nDagger Shell aims to modernize this by integrating features from Docker, PowerShell, and other tools while maintaining familiar shell syntax. It supports containers, secrets, and other advanced features, enabling users to automate software tasks without learning multiple domain-specific languages (DSLs). \n\nKey features of Dagger Shell include:\n\n1. **Complementary Tool**: It works alongside existing system shells to simplify complex workflows without relying on cumbersome monolithic scripts.\n2. **Coding Flexibility**: Users can write functions in popular programming languages (Go, Python, TypeScript, Java, and PHP) to extend Dagger’s capabilities.\n3. **Sandboxed Execution**: Commands run in a controlled environment, enhancing reliability and predictability.\n4. **Streamlined Builds and Tests**: Dagger Shell simplifies the process of creating containers and testing environments by allowing users to define everything in a single pipeline.\n\nOverall, Dagger Shell seeks to make software development easier and more efficient by combining shell commands with modern programming practices. Users are encouraged to try it out and provide feedback.",
      "ko": "다거(Dagger)는 전통적인 유닉스 셸 경험을 향상시키기 위해 다거 셸(Dagger Shell)이라는 새로운 도구를 출시했습니다. 유닉스 셸은 50년 이상 사용되어 온 시스템으로, 프로그래머가 명령을 실행하고 작업을 자동화할 수 있게 해줍니다. 그러나 소프트웨어가 발전하면서 그 복잡성이 증가하여 전통적인 유닉스 인터페이스의 효율성이 떨어지고 있습니다.\n\n다거 셸은 도커(Docker), 파워셸(PowerShell) 등 다른 도구의 기능을 통합하면서도 익숙한 셸 문법을 유지하여 이를 현대화하는 것을 목표로 하고 있습니다. 이 도구는 컨테이너, 비밀 관리, 기타 고급 기능을 지원하여 사용자가 여러 도메인 특화 언어(DSL)를 배우지 않고도 소프트웨어 작업을 자동화할 수 있게 합니다.\n\n다거 셸의 주요 기능은 다음과 같습니다. 첫째, 기존 시스템 셸과 함께 작동하여 복잡한 작업 흐름을 단순화합니다. 둘째, 사용자는 인기 있는 프로그래밍 언어인 고(Go), 파이썬(Python), 타입스크립트(TypeScript), 자바(Java), PHP로 함수를 작성하여 다거의 기능을 확장할 수 있습니다. 셋째, 명령은 통제된 환경에서 실행되어 신뢰성과 예측 가능성을 높입니다. 넷째, 다거 셸은 모든 것을 단일 파이프라인에서 정의할 수 있게 하여 컨테이너 생성 및 테스트 환경 설정 과정을 간소화합니다.\n\n전반적으로 다거 셸은 셸 명령과 현대적인 프로그래밍 관행을 결합하여 소프트웨어 개발을 더 쉽고 효율적으로 만들고자 합니다. 사용자들은 이를 사용해 보고 피드백을 제공할 것을 권장받고 있습니다.",
      "ja": null
    }
  },
  {
    "id": "bb971b880efb5628",
    "title": {
      "en": "A language for building concurrent software with confidence",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/inko-lang/inko",
    "score": 17,
    "by": "znano",
    "time": 1743099312,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "cf478f18342503bf",
    "title": {
      "en": "How I pwned a major New Zealand service provider",
      "ko": "뉴질랜드 서비스 제압기",
      "ja": null
    },
    "type": "story",
    "url": "https://mrbruh.com/majorprovider/",
    "score": 4,
    "by": "MrBruh",
    "time": 1742857634,
    "content": "Note: The name of local company asked very politely to have their name redacted so it has been replaced with KiwiServices as suggested by ChatGPT\nPwning a major New Zealand service provider\nOn the evening of the 19th February 2025 I had an itch, an itch to do good in the world and to continue to polish my pen testing skills.\nFirst I went through a list of trending apps on the Google Play Store, and looked for an “‘semi” popular app in New Zealand. After a bit of scrolling past the most popular apps I found it, KiwiServices. An app with under a 4 star rating (potentially indicating rushed development) and under 100k downloads.\nI did some research and found that the app did infact have a responsible disclosure policy which at that point, I was happy to continue forth.\nAfter downloading the App onto my rooted (spare) phone, I connected it to HTTP Toolkit (shout-out to the creator Tim for giving me free pro) and started the process of creating an account so I could test the different functionalities of the app.\nWhen I typed in my phone number to make an account I noticed it make an outgoing HTTP request checking if I already had an account.\nhttps://kiwiservices.example.com/users?filter=[\"phone\":\"64123456789\"]\nCan you spot the potential issue?\nYes, you just had the same thought as me. If I removed the filter, what would happen? Would it return user details unrelated to my account, or would it just error out?\nWell it actually turned out, neither. The request timed out after 30 seconds with no data returned. However this is still very odd as the request should have either given an error code or some kind of response.\nMy next thought was that what if the request was timing out because it was trying to do a computationally heavy task such as gathering multiple users details and send it to me.\nAfter some trial and error I managed to finally make the request work.\nhttps://kiwiservices.example.com/users?range=[1,1] // Requests the first user\nThis returned a list of user[s] containing the following data (censored) for what seemed to be a testing account.\n[\n\n{\n\n\"id\": 1,\n\n\"email\": \"example@gmail.com\",\n\n\"firstName\": \"John\",\n\n\"lastName\": \"Smith\",\n\n\"phone\": \"64123456789\",\n\n\"dateOfBirth\": \"2000-01-01T00:00:00.000Z\",\n\n\"firstSeen\": \"0000-00-00T00:00:00.000Z\",\n\n\"lastLoggedIn\": null,\n\n\"Card\": null,\n\n\"isNew\": 0,\n\n\"cognitoId\": \"00000000-0000-0000-0000-000000000000\",\n\n\"CustomerId\": null,\n\n\"LoyaltyId\": null,\n\n\"LoyaltyCard\": null,\n\n\"RewardsMemberId\": null,\n\n\"RewardsCard\": null,\n\n\"isActive\": 1,\n\n\"referralCode\": \"\",\n\n\"fcmToken\": null,\n\n\"allowTransNotif\": 1,\n\n\"allowPromoNotif\": 1,\n\n\"isVip\": 0,\n\n\"session_id\": 0,\n\n\"giftcard_id\": 0,\n\n\"site_id\": []\n\n}\n\n]\nAt this point I was confident that their entire user database was at risk, since a malicious actor could dump the entire user database by iterating through it with a large range. However it was midnight so I decided to report it the next day after I awoke.\nDay 2\nOn day 2 I awoke and began by finding some form of contact details, information was somewhat sparse but I managed to find a phone number. After talking with someone there they said they would talk to one of the big cheeses and get them to call me back.\nAfter maybe 30 minutes a big wig called me back and I ended up sending them a vulnerability report via email including replication steps.\nA while after that, I got an email back saying that they managed to replicate the issue and were working on a fix. We also agreed on a 30-day responsible disclosure period.\nAs of the 22nd of March they have fixed the vulnerability and the 30-day disclosure period has ended.\nTimeline\n19/02/2025 - Vulnerability Discovered\n20/02/2025 - Vulnerability Reported\n11/03/2025 - Vulnerability Fixed\n22/03/2025 - 30 Day responsible disclosure ends\n25/03/2025 - Writeup published",
    "summary": {
      "en": "On February 19, 2025, a security researcher discovered a vulnerability in the KiwiServices app, a lesser-known service provider in New Zealand. The researcher noticed that the app's user account check was not properly secured, potentially allowing access to user data. After testing, they confirmed that user information could be retrieved through an insecure request.\n\nThe next day, the researcher contacted KiwiServices and reported the issue, providing details on how to replicate it. The company acknowledged the vulnerability and worked on a fix, agreeing to a 30-day responsible disclosure period. By March 11, they had resolved the issue, and the disclosure period ended on March 22. The researcher published a write-up of the incident on March 25. \n\n**Key Timeline:**\n- **Feb 19, 2025**: Vulnerability discovered\n- **Feb 20, 2025**: Vulnerability reported\n- **Mar 11, 2025**: Vulnerability fixed\n- **Mar 22, 2025**: Disclosure period ends\n- **Mar 25, 2025**: Write-up published",
      "ko": "2025년 2월 19일, 한 보안 연구자가 뉴질랜드의 덜 알려진 서비스 제공업체인 KiwiServices 앱에서 취약점을 발견했습니다. 연구자는 이 앱의 사용자 계정 확인 과정이 제대로 보호되지 않아 사용자 데이터에 접근할 수 있는 가능성이 있음을 알게 되었습니다. 테스트 결과, 불안전한 요청을 통해 사용자 정보를 쉽게 조회할 수 있다는 사실이 확인되었습니다.\n\n다음 날, 연구자는 KiwiServices에 연락해 문제를 보고하고 이를 재현하는 방법에 대한 세부 정보를 제공했습니다. 회사는 이 취약점을 인정하고 수정 작업에 착수했으며, 30일의 책임 있는 공개 기간에 동의했습니다. 3월 11일까지 문제를 해결했으며, 공개 기간은 3월 22일에 종료되었습니다. 연구자는 3월 25일에 사건에 대한 보고서를 발표했습니다.",
      "ja": null
    }
  },
  {
    "id": "3e6d30eb4a5dc2af",
    "title": {
      "en": "Zoom bias: The social costs of having a 'tinny' sound during video conferences",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://phys.org/news/2025-03-bias-social-tinny-video-conferences.html",
    "score": 96,
    "by": "bookofjoe",
    "time": 1743094183,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "8a97e6e133dda66b",
    "title": {
      "en": "We are building the next DocuSign",
      "ko": "차세대 전자서명 혁명",
      "ja": null
    },
    "type": "story",
    "url": "https://sgnly.com",
    "score": 9,
    "by": "esaidm",
    "time": 1743108702,
    "content": "Redefining Document SigningTransform Signed DocumentsInto Smart TemplatesSimple • Error-Free • Ready to Use5x faster document workflows — AI that auto-fills, explains, and builds reusable templates in seconds.Get Early AccessNo credit card required14-day free trialCancel anytime\n\nSave Time & MoneyWhy You Should Go With SgnlyTurn any document into a smart template, auto-filled with known data, explained by a voice agent, and personalized to every signer.Save On Processing TimeWith Sgnly, 80% of your document workflows are automated, so you can process more documents without expanding your team.An AI Assistant That Self-OptimizesSgnly isn't your average AI assistant. Not only does it automate tasks, it also learns from your feedback over time, just like a real human would.Get Up and Running Within A WeekLearning to use Sgnly takes minutes - and with our dedicated support team's help, you'll be onboarded and live within days!Replicate Your Top PerformersUsing our AI Playbooks, we can work with you to automate your top-performing document processes & writing workflows.All-In-One SubscriptionFrom email deliverability to B2B data, we've got it all. We've built best-in-class products for the entire document cycle, all in one place.Intent-Driven Document CreationHarness the power of behavioral, firmographic, and technographic data to create documents tailored to specific customer needs. Traditional Document ProcessManual Document ProcessingSlow, error-prone document handling with manual inputTime WastedHours spent completing complex formsManual DocumentationManual data entry for each documentNo AutomationRepetitive processes without reusable templates Sgnly Smart SolutionSgnly Document AssistantAI-poweredAgreement for {{company_name}}Date: {{current_date}}Dear {{customer_name}},This agreement confirms our services at the rate of {{hourly_rate}} per hour, with an estimated total of {{total_amount}}.Payment is due within {{payment_terms}} days of receipt.Sincerely,{{sender_name}}AI SuggestionAdd payment instructions to improve clarityVariables auto-filled from your data GenerateAI AssistantExplains complex terms and guides throughout the processSmart AutocompleteAutomatically fills in with saved informationAutomatic GenerationCreates customized documents in secondsTry Sgnly Now\n\nReady to transform your document workflow?Join thousands of businesses using SgnlySave time with AI-powered templatesSecure document managementEasy integration with existing systemsUnlimited electronic signaturesFull NameWork EmailCountry CodeUnited States (+1)United States (+1)United Kingdom (+44)Canada (+1)Australia (+61)Germany (+49)France (+33)Spain (+34)Italy (+39)Japan (+81)China (+86)India (+91)Brazil (+55)Mexico (+52)Russia (+7)South Korea (+82)Chile (+56)Phone NumberCompanyCurrent E-Signature SoftwareSelect your current e-signature solutionDocuSignAdobe SignHelloSignDropbox SignPandaDocSignNowSigneasyNo software currentlyOtherPrimary Use CaseRequest AccessBy submitting, you agree to our Terms of Service and Privacy Policy. Your information may be used to contact you about our products and services.",
    "summary": {
      "en": "**Sgnly Overview**\n\nSgnly is a tool that streamlines document signing by transforming traditional documents into smart templates. It offers several key benefits:\n\n- **Speed and Efficiency**: Processes are five times faster, allowing for quick document creation with AI that auto-fills information and creates reusable templates.\n- **Cost Savings**: Automating 80% of workflows means you can handle more documents without needing to hire additional staff.\n- **Intelligent Assistant**: Sgnly learns from user feedback, improving its performance over time.\n- **Quick Setup**: Users can start using Sgnly within a week, thanks to user-friendly features and dedicated support.\n- **Comprehensive Features**: The platform includes tools for email, data management, and more, all in one subscription.\n\nSgnly’s smart solutions reduce manual processing errors, save time, and enhance document clarity with AI assistance. It is designed for businesses looking to optimize their document workflows and improve efficiency. A 14-day free trial is available for those interested.",
      "ko": "Sgnly는 전통적인 문서를 스마트 템플릿으로 변환하여 문서 서명을 간소화하는 도구입니다. 이 도구는 여러 가지 주요 이점을 제공합니다.\n\n첫째, 속도와 효율성입니다. Sgnly는 문서 작성 과정을 다섯 배 빠르게 처리할 수 있으며, 인공지능이 정보를 자동으로 채워주고 재사용 가능한 템플릿을 생성합니다. 둘째, 비용 절감 효과가 있습니다. 80%의 작업 흐름을 자동화함으로써 추가 인력을 고용하지 않고도 더 많은 문서를 처리할 수 있습니다. 셋째, 지능형 어시스턴트 기능이 있습니다. Sgnly는 사용자 피드백을 학습하여 시간이 지남에 따라 성능을 개선합니다. 넷째, 빠른 설정이 가능합니다. 사용자 친화적인 기능과 전담 지원 덕분에 사용자는 일주일 이내에 Sgnly를 시작할 수 있습니다. 마지막으로, 포괄적인 기능을 제공합니다. 이 플랫폼은 이메일, 데이터 관리 등 다양한 도구를 하나의 구독으로 이용할 수 있습니다.\n\nSgnly의 스마트 솔루션은 수작업 처리 오류를 줄이고, 시간을 절약하며, 인공지능의 도움으로 문서의 명확성을 높입니다. 이 도구는 문서 작업 흐름을 최적화하고 효율성을 개선하고자 하는 기업을 위해 설계되었습니다. 관심 있는 분들을 위해 14일 무료 체험이 제공됩니다.",
      "ja": null
    }
  },
  {
    "id": "b1dfa6cfb530e81e",
    "title": {
      "en": "Ignoring unwanted Terraform attribute changes",
      "ko": "원치 않는 Terraform 속성 무시하기",
      "ja": null
    },
    "type": "story",
    "url": "https://blog.mattsbit.co.uk/2025/03/23/ignoring-unwanted-terraform-provider-attribute-changes/",
    "score": 34,
    "by": "mrmattyboy",
    "time": 1742753386,
    "content": "Ignoring unwanted Terraform provider attribute changes\n\n23 Mar, 2025\n\n  · Read in about 3 min\n  · (582 Words)\n\n  The Problem!\nI’ve ocassionally found Terraform providers that take an attribute and manipulate it in a way that.. isn’t favourable.\nThis means that, following runs see a change in the attribute (compared) to the attribute passed and want to modify it.\nFor example!…\nThe Docker Terraform provider (by kreuzwerker :D  https://registry.terraform.io/providers/kreuzwerker/docker/latest/docs) manipulates the “image” attribute to the SHA digest of the image…\nThis means, that if I create:\nresource \"docker_container\" \"my_important_container\" {\n    ...\n    image = \"base-image-for-my-important-container:v2.20.2\"\n    ...\n}\nA terraform refresh later, the image attribute will be been read as a SHA digest.\nThis means that next time Terraform runs, it will want to re-create the container, changing the image from the SHA digest to the original named image.\nFor some resources, this is probably fine, it maybe an update-in-place and nothing changes.. but in this case, it forces a re-creation and, for a container, this means an outage. (Yes, yes, this is an awful way of deploying containers.. but it’s for Homelab use and sshh!… See what I was dealing with here (https://github.com/MatthewJohn/vault-nomad-consul-terraform) and then complain!).\nIgnorance is Bliss\nOf course, the way most would achive this is to simply ignore changes to this attribute:\nresource \"docker_container\" \"my_important_container\" {\n  ...\n  image = \"base-image-for-my-important-container:v2.20.2\"\n  ...\n\n  lifecycle {\n    ignore_changes = [image]\n  }\n}\nOf course this would work.. at least to the point of stopping the re-creation of the resource and shutting Terraform up…\nBut suppose I make a change:\ndiff\n-   image = \"base-image-for-my-important-container:v2.20.2\"\n+   image = \"base-image-for-my-important-container:v2.20.3\"\nWell, of course, all hell has broken loose.. I’ve made the change to deploy to new version and I’ve run Terraform and it’s showing all good!.. But it hasn’t, has it.. we told it to ignore it and it has done just that. Only, we forgot about that 3 years later and our pipeline was green showing Terraform had plan/applied, so all good.\nListening to the ignored\nA little way around this that I like to use (when necessary) is to simply create a null_resource, triggered by the attribute value that we really care about and configure a replace based on this trigger.\nE.g.:\nlocals {\n  image = \"base-image-for-my-important-container:v2.20.2\"\n}\n\n# Handle changes to image, which are ignored by the container resource\nresource \"null_resource\" \"container_image\" {\n  triggers = {\n    image = local.image\n  }\n}\n\nresource \"docker_container\" \"my_important_container\" {\n  ...\n\n  image = local.image\n  ...\n\n  lifecycle {\n    ignore_changes = [image]\n\n    # When container image name changes\n    replace_triggered_by = [\n      null_resource.container_image,\n    ]\n  }\n}\nAnd ta-da.. as long as the image name doesn’t change, the trigger doesn’t get rebuilt. The docker_container sits happily with some garbage/impossible-to-really-verify/i-would-never-pass-this-in SHA digest, which Terraform is ignoring.\nAs soon as the image changes, whilst the docker_container resource ignores the attribute change, the null_resource certainly doesn’t and that re-creation then triggers a container re-creation.\nOf course, this isn’t particularly pretty and should be used sparingly, but sometimes fixes those little problems.\nImprovements?\nHonestly, perhaps the image attribute of the docker_container could been set to null_resource.container_image.triggers.image, but I’ve always taken the image in as a variable, which has reduced the duplication for me.\nHaving said this, I like to use attributes where ownership lies.. I don’t think the null_resource is a “provider” of the image, more an additional consumer. So perhaps a local is fine.\nAnd, honestly, I should have created a PR with the provider to handle resolving an image name to a SHA digest and then comparing it before forcing a re-creation.. but hey, sometimes we don’t have time to fix every little thing!\n\n  Navigation\n\n      Previous\n\n  Comments\n\n  (function() {\n\n    if (window.location.hostname == \"localhost\")\n      return;\n\n    var dsq = document.createElement('script'); dsq.async = true; dsq.type = 'text/javascript';\n    dsq.src = '//mattsbit.disqus.com/embed.js';\n    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);\n  })();\n\nPlease enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>",
    "summary": {
      "en": "### Summary\n\n**Issue with Terraform Providers:**\nSometimes, Terraform providers change attributes in ways that can cause problems. For instance, the Docker provider converts the \"image\" attribute into a SHA digest. This can lead to Terraform wanting to recreate resources unnecessarily, causing downtime.\n\n**Current Solution:**\nOne common fix is to ignore changes to the \"image\" attribute in the resource configuration. However, this can lead to issues if you later change the image version, as Terraform won't recognize the update and may not apply the intended changes.\n\n**Alternative Approach:**\nA better method is to use a `null_resource` that tracks changes to the image. When the image changes, this `null_resource` triggers a re-creation of the Docker container, while the original resource ignores the change. Here’s how you can set it up:\n\n1. Define the image as a local variable.\n2. Create a `null_resource` that triggers on the image change.\n3. In the Docker container resource, ignore image changes but include the `null_resource` in the `replace_triggered_by` attribute.\n\nThis way, you can manage updates without unnecessary downtime, although this method should be used sparingly.\n\n**Improvements:**\nThe author suggests that ideally, the Docker provider should be improved to handle image name changes better. A pull request could have been made to address this, but sometimes time constraints prevent such fixes.",
      "ko": "Terraform 프로바이더에 대한 문제는 가끔 속성이 변경되어 문제가 발생할 수 있다는 점입니다. 예를 들어, Docker 프로바이더는 \"image\" 속성을 SHA 다이제스트로 변환합니다. 이로 인해 Terraform이 자원을 불필요하게 재생성하려고 하여 다운타임이 발생할 수 있습니다.\n\n현재의 해결책 중 하나는 자원 구성에서 \"image\" 속성의 변경을 무시하는 것입니다. 하지만 나중에 이미지 버전을 변경할 경우, Terraform이 업데이트를 인식하지 못해 의도한 변경이 적용되지 않을 수 있습니다.\n\n대안으로는 이미지 변경을 추적하는 `null_resource`를 사용하는 방법이 있습니다. 이미지가 변경되면 이 `null_resource`가 Docker 컨테이너의 재생성을 유도하고, 원래 자원은 변경을 무시합니다. 설정 방법은 다음과 같습니다. 먼저 이미지를 로컬 변수로 정의하고, 이미지 변경 시 트리거되는 `null_resource`를 생성합니다. 그런 다음 Docker 컨테이너 자원에서는 이미지 변경을 무시하되, `replace_triggered_by` 속성에 `null_resource`를 포함시킵니다. 이렇게 하면 불필요한 다운타임 없이 업데이트를 관리할 수 있지만, 이 방법은 신중하게 사용해야 합니다.\n\n저자는 이상적으로는 Docker 프로바이더가 이미지 이름 변경을 더 잘 처리할 수 있도록 개선되어야 한다고 제안합니다. 이를 해결하기 위한 풀 리퀘스트가 제출될 수 있었지만, 때때로 시간 제약으로 인해 이러한 수정이 이루어지지 않는 경우가 있습니다.",
      "ja": null
    }
  },
  {
    "id": "9a7110f17d414461",
    "title": {
      "en": "They Might Be Giants Flood EPK Promo (1990) [video]",
      "ko": "거대 홍수의 전조",
      "ja": null
    },
    "type": "story",
    "url": "https://www.youtube.com/watch?v=C-tQSFQ-ESY",
    "score": 170,
    "by": "CaliforniaKarl",
    "time": 1743046675,
    "content": "Back\n\n        Search",
    "summary": {
      "en": "It seems that you haven't provided the text you'd like summarized. Please share the text, and I'll be happy to help you create a clear and concise summary!",
      "ko": "제공하신 텍스트가 없는 것 같습니다. 요약하고 싶은 내용을 공유해 주시면, 명확하고 간결한 요약을 만들어 드리겠습니다!",
      "ja": null
    }
  },
  {
    "id": "2e51d5a127d4fc86",
    "title": {
      "en": "Thinner Films Conduct Better Than Copper",
      "ko": "얇은 필름, 구리보다 우수!",
      "ja": null
    },
    "type": "story",
    "url": "https://spectrum.ieee.org/thin-film",
    "score": 55,
    "by": "rbanffy",
    "time": 1743083957,
    "content": "SemiconductorsNews\n        New Films Conduct Better the Thinner They Get\n    Future chips need something better than copper. Are topological semimetals the answer?Alfred Poor10 hours ago3 min read 1 Alfred Poor is the former editor of Health Tech Insider and a contributor to IEEE Spectrum.A film, as thick as a few atoms, of polycrystalline niobium phosphide conducts better through the surface. This makes the material a better conductor as a whole.\n        Il-Kwon Oh/Asir Khan\n\n    {\"customDimensions\": {\"5\":\"Alfred Poor\",\"11\":2671372821,\"7\":\"thin films, copper, interconnects\",\"10\":\"thin films\",\"6\":\"semiconductors\",\"8\":\"03/27/2025\"}, \"post\": {\"id\": 2671372821, \"providerId\": 0, \"sections\": [497728257, 539621009, 544169523, 2267926519, 544169516], \"authors\": [21079642], \"tags\": [\"thin films\", \"copper\", \"interconnects\"], \"streams\": [], \"split_testing\": {}} }",
    "summary": {
      "en": "New research suggests that future chips may need alternatives to copper for better conductivity. A thin film made of polycrystalline niobium phosphide, just a few atoms thick, shows improved surface conductivity, making it a potentially better overall conductor.",
      "ko": "새로운 연구에 따르면, 미래의 반도체 칩은 더 나은 전도성을 위해 구리 대신 다른 소재가 필요할 수 있다고 합니다. 몇 개의 원자 두께로 이루어진 다결정 나이오븀 인화물로 만든 얇은 필름은 표면 전도성이 향상되어, 전체적으로 더 나은 전도체가 될 가능성이 있습니다.",
      "ja": null
    }
  },
  {
    "id": "cc85ffdb6595d8fb",
    "title": {
      "en": "Crawl Order and Disorder",
      "ko": "기어가는 혼돈",
      "ja": null
    },
    "type": "story",
    "url": "https://www.marginalia.nu/log/a_117_crawl_order/",
    "score": 56,
    "by": "ingve",
    "time": 1743080821,
    "content": "Posted: 2025-03-27Tags:\nsearch-engineA problem the search engine’s crawler has struggled with for some time is that it takes a fairly long time to finish up, usually spending several days wrapping up the final few domains.This has been actualized recently, since the migration to slop crawl data has dropped memory requirements of the crawler by something like 80%, and as such I’ve been able to increase the number of crawling tasks, which has led to a bizarre case where 99.9% of the crawling is done in 4 days, and the remaining 0.1% takes a week.This happens for a few reasons, in part because the the sizes of websites seem to follow a pareto distribution and some sites are just very large, but also because the crawler limits how many concurrent crawl tasks are allowed per common domain name.This limit is to avoid accidentally exceeding crawl rates by crawling the same site via different aliases. It’s also flat necessary to avoid getting blocked by anti-crawler software on some domains, especially in academia which tends to have an egregious number of subdomains often served by a small number of relatiely underpowered machines. They also tend to host some of the largest websites, often with tens or even hundreds of thousands of documents.The limit varies based on domain name, certain larger blog hosts has a very generous limit, whereas edu and gov domains tend to have very the most restrictive limits.The original crawl order was random. This shuffling of domains was done to ensure that if the crawler ever did end up with some unfortunate crawl order that caused problems for a website (via e.g. domain aliases), or the crawler was caused to run out of memory (no longer an issue), this would not repeat itself every time.Though in practice, what ended up happening was that through poor luck and the push of domain limits, larger (often academic) domains were often started late.As these websites take a lot of time to crawl, it’s desirable to start them as soon as possible, rather than choosing the order at random.The next idea was to sort the crawl tasks by the number of subdomains for the domain name, and then use a random ordering as a tiebreaker.This really did not have the intended effect.The change ended up creating a shotgun, blasting a blog host with dozens of simultaneous requests at 1 second intervals, as a number of crawl tasks were started at the same time and were given the same crawl delay instructions in robots.txt.Oops!To avoid this unfortunate pattern, I added some jitter to the delay between requests. This makes it so that the request timings will drift apart even if multiple crawl tasks are started at the times.I also changed the sort order to not order the domains strictly by how many subdomains they had, but whether they had more than 8 subdomains, to get a better mixture. The aim was never to prefer the crawling of blog hosts after all, but academic websites.This revised change appears to have worked, and schedules the slowest crawl tasks first. Though it won’t completely fix the problem, which is in part a consequence of the batch oriented model of crawling the search engine uses, it will at least make better use of the crawler’s runtime.Further optimizations to this could also be to store information about the time the crawl task took the last run, and sort the tasks based on that, though this is information that is not currently available, though it may be possible to use the on-disk size of the historical crawl data to approximate this information.Previous:Marginalia Search receives second nlnet grant2025-03-25",
    "summary": {
      "en": "The search engine's crawler has been slow, often taking several days to finish crawling the last few websites. Recently, changes reduced memory usage by 80%, allowing more crawling tasks to be completed quickly. Now, 99.9% of crawling is done in 4 days, while the remaining 0.1% takes a week. This delay is due to some websites being very large and restrictions on how many concurrent tasks can be run for the same domain to avoid issues with anti-crawler software.\n\nInitially, the crawler used a random order for crawling, which sometimes caused larger academic sites to start late. To improve this, the crawling tasks were sorted by the number of subdomains, but this led to too many requests hitting blog hosts at once. To fix this, a delay was added to stagger the requests, and the sorting criteria were adjusted to better mix the domains.\n\nThe new approach seems to prioritize slower crawling tasks more effectively, although it won't completely solve the problem. Future improvements could involve tracking how long previous crawls took, but this information isn't currently available.",
      "ko": "검색 엔진의 크롤러가 느려서 마지막 몇 개 웹사이트를 크롤링하는 데 며칠이 걸리는 경우가 많았습니다. 최근에 메모리 사용량을 80% 줄이는 변화가 있어 더 많은 크롤링 작업을 빠르게 완료할 수 있게 되었습니다. 이제 크롤링의 99.9%가 4일 안에 완료되며, 나머지 0.1%는 일주일이 걸립니다. 이 지연은 일부 웹사이트가 매우 크고, 동일한 도메인에 대해 동시에 실행할 수 있는 작업 수에 제한이 있어 발생합니다. 이는 크롤러 방지 소프트웨어와의 문제를 피하기 위한 조치입니다.\n\n처음에 크롤러는 무작위 순서로 크롤링을 진행했는데, 이로 인해 큰 학술 사이트가 늦게 시작되는 경우가 있었습니다. 이를 개선하기 위해 크롤링 작업을 서브도메인 수에 따라 정렬했지만, 이로 인해 블로그 호스트에 너무 많은 요청이 동시에 몰리는 문제가 발생했습니다. 이를 해결하기 위해 요청 간에 지연을 추가하고, 도메인을 더 잘 섞을 수 있도록 정렬 기준을 조정했습니다.\n\n새로운 접근 방식은 느린 크롤링 작업을 더 효과적으로 우선시하는 것처럼 보이지만, 문제를 완전히 해결하지는 못할 것입니다. 향후 개선 사항으로는 이전 크롤링에 소요된 시간을 추적하는 방법이 있을 수 있지만, 현재로서는 이 정보가 제공되지 않고 있습니다.",
      "ja": null
    }
  },
  {
    "id": "3b38a4aa458e3cee",
    "title": {
      "en": "What went wrong with the Alan Turing Institute?",
      "ko": "앨런 튜링 연구소의 위기",
      "ja": null
    },
    "type": "story",
    "url": "https://www.chalmermagne.com/p/how-not-to-build-an-ai-institute",
    "score": 75,
    "by": "alexicon",
    "time": 1743081532,
    "content": "Share this postChalmermagne How not to build an AI InstituteCopy linkFacebookEmailNotesMoreDiscover more from Chalmermagne Systems, markets, technology Over 1,000 subscribersSubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.Already have an account? Sign inHow not to build an AI InstituteWhat went wrong with the Alan Turing Institute? Alex ChalmersMar 27, 202546Share this postChalmermagne How not to build an AI InstituteCopy linkFacebookEmailNotesMore1010ShareIntroductionThe UK’s national AI institute is in crisis. Despite receiving a fresh £100 million funding settlement in 2024, the Alan Turing Institute (ATI) is gearing up for mass redundancies and to cut a quarter of its research projects. Staff are in open revolt.In 2023, when generative AI fever swept the world, far from taking centre stage, the ATI found itself playing defence. A report from the Tony Blair Institute argued that:While industry figures have predicted some advances and privately warned of major risks, existing government channels have failed to anticipate the trajectory of progress. For example, neither the key advance of transformers nor its application in LLMs were picked up by advisory mechanisms until ChatGPT was headline news. Even the most recent AI strategies of the Alan Turing Institute, University of Cambridge and UK government make little to no mention of AGI, LLMs or similar issues.Martin Goodson, then of the Royal Statistical Society, dubbed the ATI “at best irrelevant to the development of modern AI in the UK”.The criticism hasn’t slowed. Matt Clifford’s AI Opportunities Action Plan recommended that the government should “consider the broad institutional landscape and the full potential of the Alan Turing Institute to drive progress at the cutting edge”. While Clifford was too diplomatic to say the quiet bit out loud, in government speak, this is code for suggesting that the ATI was not fulfilling its intended purpose.UK Research and Innovation, the UK’s main research and innovation funding body, is running out of patience. Its quinquennial review of the ATI, published in 2024, was politely scathing about the institute’s governance, financial management, and the quality of its most recent strategy (dubbed Turing 2.0).There aren’t many areas of consensus across the UK’s fractious tech community, but the ATI has come to play an oddly unifying role. From left to right and north to south, there’s a sense that the institute is running out of friends and time.While the median founder, investor, or civil servant will cheerfully roll their eyes at the mention of the ATI, what’s less understood is why the UK’s national AI institute has proven such a flop, despite spending over quarter of a billion pounds since its inception. To piece this together, I’ve spoken to current and former Turing insiders, figures from the world of research funding, academics, and civil servants.SubscribeSet up to fail?To understand the ATI, we need to understand the circumstances of its creation. Back in 2014, we had hit the peak of the ‘big data’ hype cycle.Gartner Emerging Technologies Hype Cycle - 2013As data storage costs tumbled, big tech companies printed money, and the cloud became the rage - the UK Government decided it wanted a piece of the action. ‘Big data’ had been identified as one of the UK’s ‘Eight Great Technologies in 2013’, unlocking new funds for consultancies to produce pdfs on public sector innovation.In the 2014 Budget, George Osborne announced that the UK would invest £42 million over five years into a data science institute, dedicated to the memory of Alan Turing. From day one, the goals of the institute were nebulous. The official government announcement stated that the institute would “collaborate and work closely with other e-infrastructure and big-data investments across the UK Research Base”, as well “attracting the best talent and investment from across the globe”.The task of bringing this ambition to life fell to the Engineering & Physical Sciences Research Council (EPSRC), a powerful public body tasked with allocating money to research and postgraduate degrees. In July, a few months after the Budget announcement, the EPSRC called for expressions of interest from university partners. It would go on to announce Cambridge, Edinburgh, Oxford, UCL and Warwick as its initial partners following a competition to which over 20 universities applied. It has since expanded its university partnerships to a network of over 60.The government announced that the ATI would have a headquarters in the British Library in London (after turning down a bid from Liverpool), along with “spurs” in businesses and universities. This would “bring benefits to the whole country … including in our great northern cities”, despite no academic institutions from the latter making the cut.Creating a national data science institute in partnership with some of the UK’s top universities might seem like a logical move. But arguably, this was the root from which all of the ATI’s ills stemmed.A virtual institute By outsourcing the institute to the universities, the EPSRC guaranteed that the ATI would always be less than the sum of its parts.In the words of one senior academic: “The EPSRC wanted to establish a data science institute, but didn’t want to take on the long-term responsibility of funding and governing it … the landscape is filled with institutions that have been offloaded onto higher education”.Universities contributed research muscle, but if an academic became a ‘Turing fellow’ or led a programme at the institute, they would retain their primary affiliation and would often work no more than a couple of days a week at the ATI.This made it hard to build a research culture or set of owned capabilities. One ex-ATI staffer described the institute as “a set of sole traders … all self-interested”, who had “little incentive to collaborate”. While they remembered some of their former colleagues contributing, others treated the ATI as “a fancy coffee room”. A current Turing staffer said, “the academics just have no skin in the game”.A senior research funder adds, “If you’re not aiming to build an institutional culture, then why set up an institution?”Another source described how “it was conceived as a weird virtual institute”, which meant that “universities bickered with each other and then with the EPSRC. There were too many cooks, no core of people who could deliver stuff, and they [the ATI] never had their own strategy.” To illustrate the many cooks, there are 25 people who sit across the ATI’s three advisory boards, only one of whom works for a frontier AI lab.There was no bigger source of bickering than money. While the EPSRC stumped up much of the ATI’s money, partner universities were big financial contributors. They expected their money back - with interest.Each partner university nominated a ‘Turing liaison’ to act as the contact point between their university and the institute. In reality, the liaison director’s job was to maximise the amount of money awarded to their own institution’s research. As our former Turing insider from before puts it, “the original five universities were customers and suppliers”, this “created an obvious conflict of interest”.If we take 2017 as an example:The original five universities paid in a million apiece, and made a good return on their investment.As the ATI’s group of partner universities expanded, the original five were the predominant beneficiaries. While they took a ‘loss’ on the first couple of years of contributions, they soon handsomely profited. In 2019, for example, the five put in £5 million and received back £9.8 million in grants. The new universities in the coalition who had contributed £8 million benefitted from less than £4 million.It likely doesn’t hurt that the five founding universities hold the majority of seats on the ATI’s board of directors, along with veto rights.UKRI has grown weary of this setup, with the quinquennial review warning that “there is a clear need for the governance and leadership structure to change fundamentally to reflect a more representative set of stakeholders … the Institute must have an impartial governance arrangement which is fit for this purpose, which follows best practice for similar public investments, and be shown to represent the concerns of the whole community”.A national institute for hire How were the universities ‘profiting’?In part, the ATI acted as a vehicle for shuffling public money from one institution to another. But from day one, the institute had commercial ambitions. The ATI quickly established itself as a taxpayer-subsidised technology consultancy.As one former ATI staffer put it, the pressure from universities to maximise returns meant that the institute would “go where the money is … it didn’t smell right”.From 2016, it began striking partnerships with outside organisations. Work the ATI undertook included flight demand forecasting with British Airways, fraud detection in telecoms with Accenture, and improving Android game recommendations for Samsung. Other work the ATI conducted, while not directly sponsored by industry, clearly was designed with commercial applications in mind, such as exploring the correlation of viewers’ emotional journeys with the commercial success of films.This is not to call into question the merit or rigour of any of this work. But it is reasonable to ask why a national institute was conducting it and how this was building nationally-useful capabilities. Industry could easily commission academics directly or pay consultancies to conduct this work for them.The ATI was also at an, arguably unfair, advantage in this respect. Thanks to its generous upfront funding and academics’ comparatively low dayrate, the institute had a price advantage versus its commercial rivals. It didn’t hurt that they could, as an academic put it, “hawk the national institute” and “trade off the Alan Turing name”.Missing the AI boomIn 2017, the ATI added artificial intelligence to its remit at the request of the government. The institute, however, struggled to make an impact in its new role.Again, the universities bear a significant share of the blame.Despite the frequent use of the ‘world-leading’ sobriquet by the government, UK universities were not at the frontier of contemporary research on deep learning. Senior British computer science academics’ strengths historically lay in fields like symbolic AI or probabilistic modelling. They initially viewed the work of DeepMind and others as little more than a glitzier version of 1980s work on neural networks. If you look at leading UK university computer science curriculums across 2018-2020, deep learning scarcely features. Senior UK academics were sceptical about how much deep learning could scale and some even forecast an ‘AI winter’.Michael Wooldridge, a senior Oxford academic and director of Foundational AI Research at the ATI was among the sceptics.As late as 2023, he was still playing down recent innovation, dismissing AlphaGo as an artifact of scale. In the same piece, Wooldridge dismissed LLMs as “prompt completion … exactly what your mobile phone does”. In response to the DeepSeek mania in January of this year, he said, “I confess I hadn’t heard of them”. This oversight might have been excusable had it not been made by the most senior AI researcher at a national AI institute.At the ATI’s flagship annual AI UK conference in 2023, the single talk on generative AI (“ChatGPT: friend or foe?”) was delivered by Gary Marcus, a prominent LLM sceptic who had argued a year earlier that deep learning was hitting a wall.This dismissiveness towards the latest AI research was fused with, as an ex-staffer puts it, “a left-leaning middle management layer … who wanted to narrate the public conversation”. As a result, the ATI positioned itself as “the police officer, not the innovator”.This is reflected by the institute’s output from 2018 onwards, which skewed heavily towards a specific ideological interpretation of responsibility and ethics.There is, of course, nothing wrong with promoting the responsible use of AI or ethical development practices. Problems like bias are well-documented and have caused real-world harm. The ATI’s work, however, tended to focus less on building technical solutions and more towards the banal restatement of well-known problems (e.g. the risks of facial recognition in policing, the importance of data protection), unremarkable surveys of public opinion (people think detecting cancer is a good thing, but are nervous about bombing people), or highly politicised content that seems better suited to a liberal arts college.Recognise faces, not war A current member of staff observed that “the public policy programmes have some of the wooliest outputs imaginable”.A drinking game based on mentions of ‘stakeholders’ or throwaway references to the need to ‘consult civil society’ would prove fatal.This cultural bias was also self-fulfilling. As another former insider put it, “many technical academics just got fed up and walked away”. Another bemoaned how the “institutional capture by sociology is total”.A national institute could, of course, make practical contributions on these questions. The Toronto-based Vector Institute, for example, built an open source tool that not only classifies biased training data, but also debiases it. That said, the ATI did build a closed source bias classifier for … its customers strategic partners at Accenture.The ATI’s decision essentially to ignore much of the cutting edge work coming out of DeepMind and US labs meant that its leadership was asleep at the wheel as the generative AI boom got underway.Until early 2023, the ATI’s output did not mention language models at all. In response to criticism, the ATI’s most senior researchers argued that LLMs had caught the whole world by surprise, so it was unfair to single them out.While these models may not have entered the public consciousness pre-ChatGPT, it is unserious to suggest that they were somehow secret. Nathan Benaich and Ian Hogarth covered them in the 2019 installment of their State of AI Report.1 The Report has been widely read in AI circles for over half a decade. OpenAI’s initial decision not to release GPT-2 earlier that year was covered in UK national newspapers. What the ATI dubbed hindsight, others could term dereliction of duty.“Hindsight is a wonderful thing.” - Mike Wooldridge on LLMs in 2023When the government did need to call on the ATI for help, they were frequently underwhelmed. A former government insider who requested the ATI’s help on Covid-related technical analysis was horrified when the institute took 18 months to complete the work. Based on their own technical background, they believe a competent team could have managed it in one. By the time they received the output, the pandemic was over and it was no longer useful. They consistently found the ATI’s leadership more interested in the amount of money they could charge than in how to be the best possible partner.This is a wider disease in the UK ecosystem. The government’s (now defunct) AI Council, primarily staffed by the great and good of UK academia, spilt much ink on the subject of responsibility, but failed to include LLMs in their 2021 roadmap. One academic familiar with the work of the council said that, “On some level, these people still don’t believe AI is real. They were telling the government that there was nothing novel about the transformer [the architecture that underpins LLMs] and that they should be focused on regulating upstream applications”.There is little sign that UK academia has learnt its lesson. In 2023, UKRI staked Responsible AI (RAI) with £31 million of public money.RAI was born after the new Department for Science Innovation and Technology lent on UKRI to free up money, so that it could showcase investment in the UK’s new ‘Five Critical Technologies’ - the government’s latest set of priorities for tech policy. UKRI scraped together a £250 million Technology Missions Fund from old budgets, but struggled to find investable projects.UKRI turned to the universities for help and a coalition, led by Southampton, conceived of Responsible AI. RAI, the Technology Missions Fund’s flagship AI investment, has gone on to fund a series of Turing-esque projects, including yet more responsibility guidelines, the use of music to keep tired drivers awake, and whether citizens should have personal carbon budgets. RAI’s first annual report lists six achievements, two of which boil down to hosting roundtables and presentations. The purpose of the Technology Missions Fund was to drive the development and adoption of new technology, not to, as RAI put it, “connect the ecosystem”.As ChatGPT stunned and terrified Whitehall in equal measure over the course of 2023, the ATI found itself on the defensive. But its defence frequently underscored the problem. Then director and chief executive, Adrian Smith said, “We have delivered numerous briefings to government officials, hosted training and contributed to several roundtables on this topic, as well as offering commentary through media and broadcast interviews.” The Turing 2.0 relaunch strategy from the same year contains a single reference to LLMs across its 66 pages.There are welcome signs that academia’s grip is being broken. Dissatisfied with the advice they received from the universities, the government established the UK AI Safety Institute (recently renamed to the ‘Security’ Institute) in November 2023, to bring it closer to the work conducted in frontier industry labs.As the highest levels of government grapple with the interdisciplinary risks, such as biosecurity, that have risen from a rapid acceleration in capabilities, the universities remain unrepentant. If anything, senior academics have prided themselves on their opposition to the new direction and have vented their frustration at losing their place in the Number 10 rolodex.Dame Wendy Hall, a senior computer scientist and former AI Council Chair turned RAI leadership team member, has condemned the “tech bro takeover” of AI policy and accused the government of ignoring cutting-edge work in universities. Professor Joanna Bryson, co-author of the UK’s first national-level AI ethics policy in 2011, recently derided AI safety as “transhumanist nonsense”. Neil Lawrence, who holds a DeepMind-sponsored professorship, slated the UK government’s “appalling” discussion of AGI and accused it of celebrating big tech companies for “solving problems that no one ever knew they had”.It is, of course, possible to object to specific claims made in AI safety research or to disagree with researchers concerned about existential risk. But the response of academia’s old guard has been to ignore the substance of this work and instead to designate the entire field as ideologically suspect.The ATI itself doesn’t seem to have learned either. Its 2025 AI UK conference didn’t feature a single researcher from a frontier research lab this year.What could have been?Amidst a maelstrom of stakeholder wheelspinning, one area of strength stands out in the ATI’s work: defence and security. While sitting under the ATI umbrella, the defence and security programme has its own team, research agenda, and operates independently. Thanks to its ties to industry and the intelligence agencies, it’s also the least ‘academic’ part of the ATI.Its work includes directly supporting the application of technology to national security problems, AI research for defence, and partnering with allies in the US and Singapore.The most publicly visible manifestation of this work is the Centre for Emerging Technology and Security (CETaS). Driven by a clearer sense of mission, CETaS has built up a degree of community respect that the ATI proper has not. A security researcher at Google DeepMind described them as “great… the best bit of Turing”. An academic with experience of working with the national security community said that “they just quietly get on with it”.Their output consistently engages with frontier research, provides specific and differentiated advice to industry (e.g. on the need to integrate the UK and Korean semiconductor industries), and empirically analyses popular memes (e.g. the (lack of) impact of AI-enabled disinformation on elections) as opposed to parroting them.CETaS also … just makes sense. There are obvious advantages a public body has when it comes to working with the national security community. When it comes to broader ‘ethics’ work, which any number of think tanks or advocacy groups could do, the relevance is less apparent. Similarly, the intersection of AI and health, another ATI ‘mission’, more logically sits in universities with links to hospitals.If the government were to re-evaluate the ATI, there would be a strong case for preserving CETaS, potentially as a sister organisation to the AI Security Institute.A system failureThe story of the ATI is, in many ways, the story of the UK’s approach to technology.Firstly, drift. The UK has chopped and changed its approach to technology repeatedly, choosing seven different sets of priority technologies between 2012 and 2023. Government has variously championed the tech industry as a source of jobs, a vehicle for exports, a means of fixing public services, and a way of expanding the UK’s soft power. These are all legitimate goals, but half-heartedly attempting all of them over a decade is a surefire means of accomplishing relatively little.Connected to this is our second challenge: everythingism. My friend Joe Hill described this aptly as “the belief that every government policy can be about every other government policy, and that there are no real costs to doing that”. This results in policymakers loading costs onto existing projects, at the expense of efficiency and prioritisation.As the ATI’s original goals were so vague, it was a prime target. Even before the ATI was up and running, the government announced that it would also be a body responsible for allocating funding for fintech projects. It then had a data ethics group bolted onto it as a result of a 2016 select committee report. As one ex-insider put it, “there was never a superordinate goal”.Finally, the perils of the UK government’s dependence on the country’s universities for research. The UK has historically channelled 80% of its non-business R&D through universities, versus 40-60% for many peer nations.Source: Nurse ReviewThis is the product of policy choices made in the 1980s onwards, which prioritised funding research in universities over public sector research establishments or government labs. This started under the Conservatives, but continued under Labour with the first Blair Government’s decision to close and partly privatise the Defence Evaluation and Research Agency (then the UK’s largest government R&D body).The other driver is the 2002 introduction of the full economic costing model (fEC) for research funding. When a research council awards a grant to a university, they do not have to fund it in full. They typically only cover 80% of the fEC, while universities pay up the rest. In practice, research councils will often try to drive their share even lower.Unlike other institutions, universities are able to cross-subsidise research costs from other income sources, such as international student fees. They also benefit from an annual £2 billion bloc grant that helps them make up any shortage. This system favours universities over everyone else and incentivises the worst kinds of grant-chasing.It’s perhaps why some of the more interesting institutes and labs attached to universities are the product of philanthropy, rather than the usual cycle of research council allocations. The Gatsby Unit at UCL, where Demis Hassabis completed his PhD, was funded by David Sainsbury. The Peter Bennett Foundation funds institutes at Cambridge, Oxford, and Sussex. A source in the research funding world put it more bluntly: “Rich people can defy university shitness… the Turing is the product of academic capture and the professors that run research councils.”In essence, the universities ended up in charge of the ATI, because government policy ensured that there was no one else with the necessary scale and resources to take on the responsibility. It just so happened that they were also uniquely ill-suited to the task. It’s not a coincidence that the most functional arm of the ATI was the one with the least academic involvement.The most common theme of Chalmermagne posts so far has been institutional failure and the legacy debts it creates. If you look across reams of UK policy, whether it’s in technology, defence, or finance, you see the same thing: a penchant for symbolic announcements, a bias towards outsourcing to ‘stakeholders’, and a reluctance to kill off bad policy long after any reasonable trial period.Considering how the UK treated Alan Turing while he was alive, he deserved a better institute to honour his memory. But then again, government is slow to learn from its mistakes.SubscribeDisclaimer: These are my views and my views only. They are not the views of my employer, any stakeholders, or civil society organisations. I’m not an expert in anything, I get a lot of things wrong, and change my mind. Don’t say you weren’t warned.1Disclosure: I previously worked for Nathan at Air Street Capital and supported on the 2023 and 2024 editions of the State of AI Report46 Likes∙10 Restacks46Share this postChalmermagne How not to build an AI InstituteCopy linkFacebookEmailNotesMore1010Share",
    "summary": {
      "en": "The Alan Turing Institute (ATI), the UK's national AI institute, is facing a crisis despite receiving £100 million in funding in 2024. The institute is planning to lay off staff and cut research projects due to internal discontent and criticism of its effectiveness. Critics argue that the ATI has failed to keep pace with advancements in AI, particularly in generative AI, and that it has not addressed key areas like large language models (LLMs) in its strategies. \n\nOriginally established in 2014 amid the \"big data\" hype, the ATI has struggled with vague goals and a lack of cohesive governance. It operates more like a collection of universities rather than a unified research institution, leading to conflicts of interest and limited collaboration among academics. Many researchers view the ATI as a way for universities to profit from public funds rather than as a driver of innovation.\n\nThe ATI's focus has shifted towards commercial projects and ethics rather than cutting-edge AI research, which has alienated many top researchers. Its failure to adapt to significant developments in AI, such as the rise of generative models, has resulted in a perception of irrelevance. A recent review by UK Research and Innovation highlighted the need for a fundamental change in governance.\n\nWhile some programs within the ATI, particularly those related to defense and security, have shown success, the overall approach of the institute reflects broader issues in the UK's technology strategy, such as shifting priorities and reliance on universities for research. The ATI's experience serves as a cautionary tale about institutional failure and the challenges of effectively managing public research initiatives.",
      "ko": "영국의 국가 인공지능 연구소인 앨런 튜링 연구소(ATI)가 2024년에 1억 파운드의 자금을 지원받았음에도 불구하고 위기에 직면해 있습니다. 내부 불만과 효과성에 대한 비판으로 인해 연구소는 직원 감축과 연구 프로젝트 축소를 계획하고 있습니다. 비평가들은 ATI가 인공지능, 특히 생성적 인공지능 분야의 발전에 뒤처졌다고 주장하며, 대형 언어 모델(LLM)과 같은 핵심 분야를 전략에서 다루지 않았다고 지적합니다.\n\nATI는 2014년 \"빅데이터\" 열풍 속에 설립되었지만, 모호한 목표와 일관된 관리 부족으로 어려움을 겪고 있습니다. 연구소는 통합된 연구 기관이라기보다는 여러 대학의 집합체처럼 운영되고 있어 이해관계 충돌과 학문 간 협력이 제한되고 있습니다. 많은 연구자들은 ATI를 혁신의 주체라기보다는 공공 자금을 통해 대학들이 이익을 얻는 수단으로 보고 있습니다.\n\nATI의 초점은 최첨단 인공지능 연구보다는 상업 프로젝트와 윤리 문제로 이동하면서 많은 우수 연구자들이 소외되었습니다. 생성 모델의 부상과 같은 인공지능의 중요한 발전에 적응하지 못한 결과, ATI는 관련성이 떨어진다는 인식을 초래했습니다. 최근 영국 연구 혁신 기관의 검토에서는 관리 방식의 근본적인 변화가 필요하다고 강조했습니다.\n\nATI 내 일부 프로그램, 특히 방위 및 보안 관련 프로젝트는 성공을 거두었지만, 연구소의 전반적인 접근 방식은 영국의 기술 전략에서 우선순위 변화와 연구에 대한 대학 의존과 같은 더 넓은 문제를 반영하고 있습니다. ATI의 경험은 공공 연구 이니셔티브를 효과적으로 관리하는 데 있어 제도적 실패와 도전 과제를 경고하는 사례로 작용하고 있습니다.",
      "ja": null
    }
  },
  {
    "id": "a3084be8cb383f83",
    "title": {
      "en": "A UI might not make it easier",
      "ko": "UI가 더 어렵다?",
      "ja": null
    },
    "type": "story",
    "url": "https://jeremymikkola.com/posts/2025_02_22_might_not_want_self_serve_ui.html",
    "score": 7,
    "by": "piinbinary",
    "time": 1742909859,
    "content": "A UI might not make it easier\n\n    Posted on February 22, 2025\n\nBuilding self-serve access to internal systems is generally a good idea. It would be silly to do engineering work to make a change when pressing buttons would do. The fact that it’s generally (and not universally) a good idea means that there are cases where it isn’t a good idea. The idea of making a task easier by making it self-serve, no code, in a UI shouldn’t be applied when the unavoidable complexity and nuance of the task to be done exceeds what a UI can reasonably handle.\nYou probably have examples of a story like this: A company has a problem to solve. They build a system to solve that problem. It does a good job of solving that problem, though only for that variant of the problem. Time marches on and the system gets expanded to handle more variations of the problem. This expansion requires bespoke engineering work each time. Some systems grow to the point where the expansion becomes routine. The internals of that system (assuming sensible leadership in that engineering organization) evolve into general-purpose pieces that can be reused in the different variations of the problem. Handling new cases may be as simple as just plugging together the existing pieces of the system in a new configuration. As more and more variations are handled, the unique engineering work fades and configuration work takes over. At this point it’s reasonable to ask the question, why does this require engineering at all? Wouldn’t it be nice if the person who came up with the requirements could open some admin tool and punch in what they need? Let’s build a self-serve UI!\nAs stated above, building this UI often is a good idea. As a rule of thumb, the simpler the configuration, the better this idea is. When systems are more complex, it doesn’t make as much sense to have a self-serve layer in front of them. If you find yourself saying, “this is trivial, why do we need engineers involved?” then by all means create a self-serve UI. But if you are saying, “this is hard, let’s add a self-serve layer to make it easier,” you might find yourself running into some problems. Having self-serve access to the system doesn’t actually make the hard parts easier, but it does make the system more rigid, and it risks giving a false sense of security while actually making the system more dangerous.\nHere are some reasons why you might not want to build a self-serve interface:\nIt may not solve the problem\nThe work to be done is configuration. Or at least, it is labeled as configuration work. But the people doing the work know that writing the config file is easy. The hard part is everything else leading up to that point. The actual engineering part of the work doesn’t go away just because there’s a self-serve way to handle the configuration portion.\nA project where the end result is a new configuration might start with changing the underlying system to support that new configuration. It may have already been possible to specify that configuration, but it might not have actually worked (perhaps there is an option for languages, but it didn’t yet support right-to-left languages, or the phone number validation doesn’t work for that country). In other cases it may not even have been possible to specify the desired logic and it takes engineering work to create a way to describe what you want in the configuration. The required engineering work could be subtler than that: it might work fine for any one user (or even a thousand) but run into some kind of scaling or cost problem when shipped to production traffic.\nAny time a system is asked to do a new thing that requires engineering work, regardless of whether that new thing is described as a feature or as a configuration. Even determining whether engineering work is needed requires the input from an engineer.\nPractical downsides\nThere are other practical downsides to structuring a config as data created by a UI and stored in a database instead of a config structured as a file checked in to git. You don’t (automatically) get revision history or deploy controls over the config.\nDebugging can be harder as well. In a development environment you may not have a copy of the configuration as it exists in the production database. Accurately reproducing the bug now requires extra work to reproduce the configuration.\nYou can’t write tests for something that’s not there. Testing requires a copy of the logic being tested to live in the codebase. Often configuration written using DSLs will be treated like any other code and be subject to unit tests. This both tests that the configuration does what was expected and the system that is configured handles that config correctly. Without these tests, the new config is tested in production.\nBuilding self-serve systems to handle complex cases is also hard work that takes a very real amount of engineering time. An accurate accounting of the real cost of building such a system (figuring out usecases, designing, building, iterating, documenting, teaching and learning) may reveal that it would cost far more to build than the work it could save - and it’s easy to overestimate how much work it will save. (See the previous point about solving the problem.)\nThe computer can’t think for you\nWhen there are important decisions to be made surrounding the launch of a new variant of a thing, those decisions can’t be left up to the computer (at least not with today’s AI…). The computer hasn’t talked to customers, it doesn’t understand the goals of the company’s leaders, knows nothing of the company culture, isn’t qualified to understand today’s legal constraints, and doesn’t have experience with what has worked well or poorly in the past. It just doesn’t have the context to make decisions.\nA self-serve UI can’t save you from novel engineering work, and it also can’t save you from thinking. You still have to understand goals, customers, priorities, communication channels, and everything else that goes into defining how this functionality will work and will be launched. Not only must this irreducible work be done, but someone also has to understand how to do it. They’ll have to reason it out from first principles or talk to people who have done it before. Computers can’t replace experience.\nExperience gives an understanding of the implications of decisions. Experience can take you from thinking “we’ll refresh the data on all of the accounts” to “we need to start a conversation with the team that handles this contract to get our limits increased for this API that we call when refreshing that data.” Even if you have a 1-click way to start that data refresh process, someone still has to understand the implications of doing it.\nIt makes the system too rigid\nProjects don’t exist in a vacuum. They impact the things around them. When self-serve tooling is bolted on top of a system, it tends to change that system. The self-serve tooling depends on the system, but the system also tends to start depending on that tooling: you often end up being forced to use that tooling if you want to interact with the system. If this doesn’t happen when the tooling is built, it can happen (intentionally or accidentally) later as the two systems co-evolve. This is fine if you are always happy with using that UI. Sometimes you aren’t.\nA UI can do the things that it was built to be able to do, and only those things. A UI can’t make the underlying system more general, but it can fail to expose the full breadth of that system’s generality. This means you can run into things that you want to do, and that the underlying system is able to do, but that you can’t do because the UI (that you now have to use) can’t handle it. This is a case where the UI intended to make the process easier is ironically making it harder.\nThis situation tends to be handled either by working around the self-serve tools (perhaps doing sketchy things like changing values in the database by hand) or by taking the time to update the UI to support this particular use case. Some systems see a constant stream of issues due to the workarounds. Others cause a pattern of delayed projects due to the time needed to stop and update the UI.\nA false sense of security\nThis isn’t so much a reason to avoid creating a self-serve UI as something to be aware of after creating it: the presence of a UI can make tasks look easier and safer than they actually are. Changes made to code are given a level of planning, thought, and scrutiny that presses of buttons are not.\nWhen there’s a class of project that looks like it doesn’t require engineering effort - perhaps because there’s a nice shiny UI or perhaps because this kind of project sometimes really is possible without engineering effort - plans get made assuming the project will not involve any engineering effort. That might not be true in this particular case. The relevant engineers are finally consulted late in the project, and everyone is dismayed to find out that this actually will require engineering work, work from an engineering team has no spare capacity this quarter (“look at all the super important things we already had to cut!”) and even if they did, there’s no way it could be done in the 2 days before this thing launched. Some readers may have just felt their blood pressure increase.\nTo generalize that idea, it is dangerous to make a type of project appear too easy because that will set an expectation that it will actually be that easy. Take lessons from Star Trek’s Scotty.\nThere’s a point that is worth bringing up again: projects often require expertise and experience in that kind of project. For someone new, there can be a lot of unknown unknowns. Tooling that abstracts over details that are actually important can result in a project being undertaken by people who, through no fault of their own, are lacking critical knowledge to make that project successful. They may not even know all the things that they don’t know. The people who built this UI have this institutional knowledge, but since they aren’t involved in the project, that expertise doesn’t get transferred to the people doing the project. That institutional knowledge can also rot if they continue to not be involved in these projects. Simple interfaces give the incorrect impression that only a simple understanding of the project is needed. Planning and staffing decisions made based on that incorrect assumption.\nMaking a task self-serve doesn’t inherently make it safe. It certainly can, and this can be one of the strongest arguments for adding UIs. You can write any kind of logic you like to validate and safety-check, to require second-person approval, and anything else you can think of. For a lot of projects this really does make the operation quite safe! However, there are some projects that you can never make truly safe. No matter how nice of a UI you put on a flamethrower, it’s still a flamethrower. But that UI could make it seem safe, at least until someone’s car is on fire.\nYou can incrementally make UIs safer over time by adding more warnings and safeguards. However, know that those changes will be written in blood. You will have to accept making big mistakes with a UI before you’ve found all the ways it can be dangerous.\nSelf-serve UIs can even introduce safety problems of their own. This can happen when it is too easy to misclick something you can’t undo. Another common pattern is an operation having very unexpected side effects (think along the lines of, you update a minor setting only to fnd out that doing so resulted in it emailing thousands of users).\nConclusion\nMany if not most projects to give self-serve access to systems don’t run into these problems. If your goal is to minimize tedium, and there is genuinely very little decision-making, planning, expertise, engineering, or risk management required each time, then you are probably fine. By all means make the process easier.\nBut if you are trying to make hard work easier by adding a self-serve UI, consider whether you have a complete understanding of what was making that work hard in the first place. You might only be making it easier to accomplish the nominal task (“create a new config”) but not to do the actual work behind it.\nWhen a project to build a self-serve UI is on the table, two things are clear: (1) there’s a problem to solve, and (2) there’s some engineering effort that can be thrown at the problem. When those are both the case, it may pay better dividends to approach the problem by getting a little bit deeper into the weeds. If there are sharp edges in the existing system that keep causing issues or deficiencies that keep having to be worked around, fix those (especially if everyone has given up on those problems being solved and considers them to be an intrinsic part of this kind of project). Make the system more general, make it easier to understand, write friction logs and fix that friction, fix the underlying data model - there’s an endless set of projects that might make future changes easier.\nIf you do build self-serve tools, make them as specific and orthogonal as possible. Let users recombine them in ways you had never expected.",
    "summary": {
      "en": "**Summary:**\n\nCreating self-serve access to internal systems can be beneficial, but it's not always the right solution. It's helpful when tasks are simple, but for complex tasks, a self-serve UI may not actually simplify the process and can introduce risks. \n\nKey points include:\n\n1. **Complex Tasks Require Engineering:** Just because a task seems trivial doesn't mean it doesn't need engineering. Complex changes often require significant background work that a self-serve UI can't address.\n\n2. **Practical Downsides:** Self-serve UIs can complicate debugging, lack revision history, and make testing difficult, as they do not integrate well with traditional code management practices.\n\n3. **Decision-Making Limitations:** Computers lack the context to make informed decisions about project goals, customer needs, and company culture. Human experience is crucial for understanding implications of engineering decisions.\n\n4. **Rigidity and False Security:** Self-serve UIs can make systems more rigid, limiting what users can do and creating a false sense of security that tasks are simpler than they are.\n\n5. **Potential for Mistakes:** Easy interfaces can lead to errors, as users may misclick or not fully understand the consequences of their actions.\n\n6. **Conclusion:** While self-serve UIs can reduce tedious work, they shouldn't be used to simplify complex tasks without a thorough understanding of the underlying challenges. It’s often better to address the root issues in the system rather than relying solely on UI solutions. If a self-serve tool is developed, it should be specific and adaptable to various needs.",
      "ko": "내부 시스템에 대한 셀프 서비스 접근을 만드는 것은 유익할 수 있지만, 항상 올바른 해결책은 아닙니다. 간단한 작업에는 도움이 되지만, 복잡한 작업의 경우 셀프 서비스 사용자 인터페이스(UI)가 실제로 과정을 단순화하지 못하고 위험을 초래할 수 있습니다.\n\n첫째, 복잡한 작업은 엔지니어링이 필요합니다. 작업이 사소해 보인다고 해서 엔지니어링이 필요하지 않다는 의미는 아닙니다. 복잡한 변경 사항은 종종 셀프 서비스 UI로 해결할 수 없는 상당한 배경 작업을 요구합니다.\n\n둘째, 실용적인 단점이 있습니다. 셀프 서비스 UI는 디버깅을 복잡하게 만들고, 수정 이력이 부족하며, 전통적인 코드 관리 관행과 잘 통합되지 않아 테스트를 어렵게 만듭니다.\n\n셋째, 의사 결정의 한계가 있습니다. 컴퓨터는 프로젝트 목표, 고객의 요구, 회사 문화에 대한 맥락이 부족하여 정보에 기반한 결정을 내릴 수 없습니다. 엔지니어링 결정의 함의를 이해하는 데는 인간의 경험이 필수적입니다.\n\n넷째, 경직성과 잘못된 안전감이 있습니다. 셀프 서비스 UI는 시스템을 더 경직되게 만들어 사용자가 할 수 있는 일을 제한하고, 작업이 실제보다 더 간단하다는 잘못된 안전감을 줄 수 있습니다.\n\n다섯째, 실수의 가능성이 있습니다. 사용자가 잘못 클릭하거나 자신의 행동의 결과를 완전히 이해하지 못할 수 있어, 간단한 인터페이스가 오히려 오류를 초래할 수 있습니다.\n\n결론적으로, 셀프 서비스 UI는 지루한 작업을 줄일 수 있지만, 복잡한 작업을 단순화하기 위해서는 기본적인 문제를 철저히 이해한 후에 사용해야 합니다. UI 솔루션에만 의존하기보다는 시스템의 근본적인 문제를 해결하는 것이 더 나은 경우가 많습니다. 만약 셀프 서비스 도구가 개발된다면, 다양한 요구에 맞게 구체적이고 적응 가능해야 합니다.",
      "ja": null
    }
  },
  {
    "id": "f73ba6234a81ec5d",
    "title": {
      "en": "NASA Deletes Comic Book About How Women Can Be Astronauts",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://futurism.com/nasa-deletes-comic-women-astronauts",
    "score": 65,
    "by": "doener",
    "time": 1743102922,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "02484f7b1f29231b",
    "title": {
      "en": "Glider for Apple II",
      "ko": "애플 II 글라이더",
      "ja": null
    },
    "type": "story",
    "url": "https://www.colino.net/wordpress/en/glider-for-apple-ii/",
    "score": 100,
    "by": "rbanffy",
    "time": 1743070376,
    "content": "Glider for Apple II\n\nA lot of people my age that touched Macintoshes in the nineties know that game, Glider, written by John Calhoun. I recently started wanting to try and program a game for the Apple II as it is something I never attempted before, and I wanted to see if I got good enough at 6502 assembly for that. After a difficult start, I managed to have the project take off (pun intended) and decided to make a Glider reimplementation for the Apple II.\n\nGlider’s splash screen\n\nI hope you will enjoy playing it as much as I enjoyed developing it!\n\nGlider for Apple II is now freely downloadable:\n\nDownload the Glider for Apple II floppy image\n\nDownload the Bulgarian version of Glider for Apple II\n\nIf you find issues, please report them on the issue tracker!\n\nGlider for Apple II requires an Apple ][+ or more recent; on the Apple ][+, a Mouse Card and mouse are required to play. On more recent computers of the Apple II line, the mouse is optional and Glider can be controlled either by mouse or keyboard. Glider for Apple II is best enjoyed on a monochrome screen. This is a design choice.\n\nIf you’re interested in some technical details about my first 6502 game’s development, I have put up a Glider for Apple II development log article.\n\nPlaying through the first level\n\n\t\t6 Responses\n\n\t\t\tComments6\n\n\t\t\t\t\t\tAldo says:\n\n\t\t\t2025/03/16 at 10:59\n\n\t\tOldies but Goodies ! Merci\n\n\t\t\t\t\t\tDP says:\n\n\t\t\t2025/03/16 at 22:09\n\n\t\tThis is so cool. Can’t wait to check it out!\n\n\t\t\t\t\t\tFrank M. says:\n\n\t\t\t2025/03/22 at 23:44\n\n\t\tDownloaded! Fun game. Thanks again. Frank\n\n\t\t\t\t\t\tJaphy Riddle says:\n\n\t\t\t2025/03/23 at 01:36\n\n\t\tAwesome! Thank you for using my drawing in the game : )\nAlso, great job with the other graphics that aren’t from the official Glider games. The sun room is beautiful. And I love the harmless sleeping cats.\nI just played through it on actual hardware and had a good time.\n\n\t\t\t\t\t\tColin says:\n\n\t\t\t2025/03/23 at 10:03\n\n\t\tHi Japhy! Thanks for the drawing and the kind feedback :-)\n\n\t\t\t\t\t\tMichael says:\n\n\t\t\t2025/03/25 at 01:26\n\n\t\tI like how flicker-free and smooth the glider is, pretty impressive that you can do it all during a VBlank.\n\n\t\tLeave a ReplyYour email address will not be published. Required fields are marked *Comment * Name *\nEmail *\nWebsite\n\nΔdocument.getElementById( \"ak_js_1\" ).setAttribute( \"value\", ( new Date() ).getTime() );\n\tThis site uses Akismet to reduce spam. Learn how your comment data is processed.",
    "summary": {
      "en": "The text discusses the development of a new game called \"Glider for Apple II,\" a reimplementation of a classic game created by John Calhoun. The author wanted to try programming for the Apple II and learned 6502 assembly language in the process. The game is now available for free download, with versions in English and Bulgarian. \n\nTo play, users need an Apple ][+ or a newer model, and a mouse is required for the Apple ][+ but optional for newer models. The game is designed to be best played on a monochrome screen. \n\nThe author also provides a development log for those interested in the technical aspects of creating the game. Players have shared positive feedback about the game in the comments.",
      "ko": "새로운 게임 \"Glider for Apple II\"의 개발에 대한 내용이 담겨 있습니다. 이 게임은 존 칼훈이 만든 클래식 게임을 재구현한 것입니다. 저자는 Apple II에서 프로그래밍을 시도하고 싶어 6502 어셈블리 언어를 배우게 되었습니다. 현재 이 게임은 무료로 다운로드할 수 있으며, 영어와 불가리아어 버전이 제공됩니다.\n\n게임을 플레이하려면 Apple ][+ 또는 그 이후 모델이 필요합니다. Apple ][+에서는 마우스가 필수지만, 이후 모델에서는 선택 사항입니다. 이 게임은 단색 화면에서 플레이하는 것이 가장 적합하게 설계되었습니다.\n\n저자는 게임 개발에 관심이 있는 사람들을 위해 개발 로그도 제공합니다. 플레이어들은 댓글을 통해 게임에 대한 긍정적인 피드백을 공유하고 있습니다.",
      "ja": null
    }
  },
  {
    "id": "980d79e65d5e530e",
    "title": {
      "en": "Rust Adopting Ferrocene Language Specification",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://lwn.net/Articles/1015636/",
    "score": 11,
    "by": "Tomte",
    "time": 1743108957,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "699c2faeb151cf3d",
    "title": {
      "en": "A filmmaker and a crooked lawyer shattered Denmark's self-image",
      "ko": "덴마크의 자존심 붕괴",
      "ja": null
    },
    "type": "story",
    "url": "https://www.theguardian.com/world/2025/mar/27/black-swan-denmark-documentary-mads-brugger-amira-smajic",
    "score": 186,
    "by": "RapperWhoMadeIt",
    "time": 1743080491,
    "content": "‘I’m a film-maker who craves sensation’ … Mads Brügger. Photograph: Marie Hald/The GuardianThe Black Swan follows a repentant master criminal as she sets up corrupt clients in front of hidden cameras. But is she really reformed – and is the director up to his own tricks?By Samanth SubramanianThu 27 Mar 2025 05.00 GMTLast modified on Thu 27 Mar 2025 10.07 GMTShareThe trap was laid in a rented office: two rooms in downtown Copenhagen, furnished without a whisper of Scandi style. If it wasn’t for a Frida Kahlo print on one wall, the premises might have felt as impersonal and stark as a confessional. That, in any event, was what it became. For six months, beginning in mid-2022, a parade of people – members of motorcycle gangs, entrepreneurs, lawyers, real-estate barons, politicians – trooped through to recount their sins to Amira Smajic. They didn’t come for expiation. They knew Smajic to be one of them – an outlaw, and in her particular case, a business lawyer so skilled at laundering money that she’d enabled a couple of billion kroner in financial crime over the previous decade. They called her the Ice Queen, because she showed not a flicker of regret for what she did.In her office, Smajic’s visitors bragged about dodging tax, bribing officials or exploiting the bankruptcy code. She offered them coffee and coaxed forth their confidences. Six cameras and three microphones, secreted in power sockets, captured it all – footage that was turned into a documentary called The Black Swan. In its surreptitious method and breathtaking drama, The Black Swan bore all the fingerprints of its director, Mads Brügger, a provocateur who has spent his career searching for bombshells to drop but who had never quite managed it as well as he did here. Denmark’s national bird is the Cygnus olor, a swan as white as virtue. The Black Swan, in showing such easy, unbridled formulations of crime, blew up Denmark’s idea of itself.Since airing last May as a five-part series on TV2, Denmark’s biggest television network, The Black Swan has sent the country into convulsions. One out of every two Danes has seen the documentary. After its release, a biker-gang member and his accountant were charged with financial crimes and taken into custody; others, including a municipal official, are under investigation. The Danish Bar and Law Society formally apologised to the minister of justice for the conduct of two lawyers caught on camera; they have been either fired or disbarred. A new money-laundering law was introduced to give banks more oversight over “client accounts” – the kind of accounts in which lawyers pool the funds of several clients and transact on their behalf, and that featured in many of the machinations in Smajic’s office. In her New Year’s speech, Denmark’s prime minister suggested biker-gang criminals ought to be stripped of their pension rights – a detail so specific it was surely inspired by The Black Swan.Other Scandinavian nations also reeled upon watching The Black Swan. After the series premiered in Sweden, a criminologist at Lund University warned: “There’s a lot of evidence that it’s probably even worse here.” Norwegian civil servants invited Brügger to Oslo in January to talk to them about money-laundering. All of Scandinavia, he believes, has persuaded itself that crime exists only in violent, poor abscesses on the edges of their societies. “The Danes totally subscribe to this idea that Denmark has no corruption, and to the idea of Denmark as the end of the road,” Brügger said, referring to the political scientist Francis Fukuyama’s notion that “getting to Denmark” is the goal of every modern democracy. “The Black Swan punctured that hallucination,” Brügger said. “It was Denmark’s red-pill moment.”Sitting in the Copenhagen offices of Frihedsbrevet, or Freedom Letter, an investigative journalism site Brügger co-founded in 2021, I asked him what ordinarily passes for corruption in Denmark. He thought about it for a comically long time. During his boyhood, he recalled, one major scandal involved a small-town mayor being bribed with a bathroom renovation for his home. In 2011, Danish newspapers carried as front-page news the revelation that the prime minister, Helle Thorning-Schmidt, was leasing her car from Germany, saving €20 a month and depriving the exchequer of €70 a month in tax. Brügger had told a Greek friend about this brouhaha; the friend stared at him and said: “Back home, we’re talking about a politician who was given an entire island as a bribe.” Brügger also related a Frihedsbrevet scoop: Copenhagen’s leading newspaper editors had been attending a Proust book club run by a government official, a degree of socialising that Brügger characterised as unhealthy. This was, he was suggesting, the scale of grift Danes were used to – chump-change tax avoidance and highbrow hobnobbing – until The Black Swan came along.But he was embroidering for effect. There have been graver controversies: a stock price manipulation scheme in 2008; a money-laundering case involving Danske Bank; a $1bn tax fraud case that ended in a 12-year prison sentence for its mastermind last December. Michael Bang Petersen, a political scientist at Aarhus University who studies trust in Danish society, told me that citizens’ trust in politicians has fallen by 20 percentage points since 2007. But their trust in fellow citizens has stayed stable. When asked if they can trust most people, an astonishing 80% of Danes reply in the affirmative. Lawyers, roasted as rogues practically everywhere, enjoy a glowing reputation in Denmark, and the welfare state is revered, as inviolable as a cathedral.“We’re taught from a young age that cheating the system is not something you do, because you end up pissing on everybody,” Ane Cortzen, a television presenter and Brügger’s sister, told me. “Cheating on taxes is one of the most serious crimes you can commit.” Kalle Johannes Rose, an associate professor at Copenhagen Business School, observed: “Most Danish scandals have to do with the state – public healthcare, public banks, public something or the other. People want to know their high taxes are being spent correctly. If they don’t trust the system, they don’t pay their taxes, and then the house of cards falls down.” The Black Swan thus invited viewers to dwell on their worst nightmare: a shattering of the trust that underpins not just the smooth functioning of their beloved welfare state but the essence of what makes Danes proud to be Danes.Amira Smajic wears her dark hair pulled back so tightly that her skin tightens around her high cheekbones. It lends her the severe, credible look of a schoolteacher, doubtless an asset during her years of crime. In her first job after university, at an accounting firm, Smajic quickly sensed they were skating close to the edge of the law. Her employer was subsequently convicted of fraud and forgery – but by then, Smajic had struck out on her own, working with Denmark’s biker gangs, which are notorious for criminality and violence. “I specialised in making accounts look as needed – getting white money to turn black and vice versa,” she says in the film. For these services, she earned several hundred thousand kroner a month. “I shopped in Louis Vuitton every week. I bought shoes like other people buy milk.”View image in fullscreenAmira Smajic in a still from The Black Swan. Photograph: Wingman Media/TV2 DenmarkIn 2020, wearying of the paranoia and guilt of this delinquent life, Smajic thought of going public – through a book, perhaps, or a film. Having met several publishers and journalists, she eventually found her way to Brügger, and she commanded every shred of his attention. Over sushi, she told him so much about her connections with the criminal underworld that “she was clearly the real deal”, Brügger told me. He was instantly smitten, in that half-ardent, half-extractive way that journalists are with their subjects.Brügger and TV2 first considered investigating the old contracts, emails and texts in Smajic’s files. But Michael Nørgaard, TV2’s editor-in-chief, said he was aware that Smajic had spent years engaging in fraud and forgery. “Could we believe that the materials she came to us with were intact – that she didn’t take out documents to put her in a better light?” he wondered. The idea to open a new office and clandestinely film its operations, Brügger and Nørgaard told me, came from Smajic. In a 2021 email, which Brügger showed me, Smajic excitedly laid out five pages of plans to monetise her past: articles, newsletters, podcasts, an eight-part true-crime show, the documentary, four books, the full panoply of a repentance empire. The arc, Smajic wrote, referring to herself in the third person, would be of “her social and moral redress”.Brügger says he believed her. Smajic had come to Denmark as a child refugee from Bosnia, along with her family, and on one occasion she told Brügger that her father, who’d died of cancer, would have been disappointed that she’d turned to crime after Denmark had taken her in. “I will never get out of this life if I do nothing,” Smajic says in the first episode, with the air of a woman plotting to burn a bridge even as she flees over it. Before filming began, a security expert talked Smajic through the consequences of making the documentary, Brügger told me. “He didn’t spare her. He said she may have to relocate to another country, change her name, or not see her friends any more. She was crying, and I thought: ‘OK, that’s it. She’s out.’ But she insisted on continuing.”Brügger and Nørgaard knew one more thing about Smajic. She was at the time, and had been for years, a police informant. On her request, they withheld that from the documentary – but they also didn’t let the police know in advance about the trap they were setting. In a brief contract, drafted at the outset of production and barely two pages long, Brügger’s producer, Peter Engel, stipulated that Smajic would be paid 30,000 kroner (roughly £3,350) a month. Engel said she also agreed to refrain from any actual criminal activity during production. In the opening minutes of The Black Swan, sitting across a desk from Smajic in a room resembling an interrogation chamber, Brügger asks her what the worst outcome of her undertaking could be. She replies: “That someone finds out and I will be liquidated before any of this is shown.”The suspense of whether Smajic will be unmasked keeps The Black Swan as taut as a bowstring. The documentary’s more immediate shocks come from watching people methodically plan to break the law. The crimes range from the paltry to the serious. A man named Wassem, to whom Smajic introduces herself in the first episode, runs a shawarma shop and wants to skip out on tax. Fasar Abrar Raja, a grey-bearded member of a motorcycle gang called Bandidos, helps demolition crews dispose of asbestos and other toxic material without the costly safety measures the law requires. For a fee, he will bribe environmental analysts and local officials to look the other way while he dumps the material in the Danish countryside. Fasar also brings along Martin Malm, a smooth-faced businessman who launders millions of kroner a month through his “invoice factories”: companies that issue fake invoices for services never rendered. (Malm might invoice a nightclub owner for providing bouncers, say; the owner would pay Malm, who’d keep a fee and return the rest to the owner in cash or some other fashion, allowing him to avoid paying tax on it. The bouncers, needless to say, don’t exist.)One of the film’s revelations, Brügger says in a voiceover, is the connection “between the nice-looking, everyday citizens and the underworld”. In Smajic’s presence, a lawyer named Lise Roulund delivers to Fasar a USB drive full of confidential documents she has obtained from the police – an illegal act in itself. On suspicions of money-laundering, Fasar’s bank account has been frozen, so Roulund helps him enact his tax dodges by transferring money in and out of an account she controls. Without lawyers willing to look the other way, criminal activity would seize up, Roulund says. “We’re the ones who make it go around.”Another lawyer, named Nicolai Dyhr, a partner at one of Denmark’s most prestigious law firms, is a fount of suggestions on how to exploit the bankruptcy code. He lays out how Wassem could shutter his shawarma business, declare bankruptcy and avoid a tax debt of 2.4m kroner; he even details how Wassem could squeeze additional money out of a government fund that guarantees worker salaries while companies are going through bankruptcy. Malm, the businessman with the invoice factories, also files for bankruptcy, and Dyhr advises him to hide evidence of fraud and deliberately undervalue his businesses. (Later, Dyhr claimed he was “eel-trapping” – leading Malm on to secure him as a client, but with no plans to commit crimes. Dyhr sued TV2, demanding that all covert footage of him be edited out of the film, but lost his case. Fasar denied committing the crimes discussed on camera; Malm told TV2 it “didn’t have the whole picture”; Roulund has refused to comment.)By themselves, the sums of laundered money bandied about run only to a few million pounds, small enough in scale that one expert described it to me as hyggekrim – crime so domestic it’s practically cosy. But all these cons purport to show how simple it is to exploit the Danish state. It was one of the earliest lessons of Smajic’s career, she says: “The state always pays.”Some episodes of The Black Swan prickle with violence. On a trip to his native Pakistan, Fasar discusses killing a patient in a hospital – a potential witness in a trial against him. (Nørgaard told me TV2 tipped off the Danish police about Fasar’s plans.) After returning to Copenhagen, Fasar storms into Smajic’s office, threatening to “crush you with my bare hands” because she has failed to secure a Danish passport for his daughter. Without breaking character, Smajic mollifies him. By the end of the meeting, they’re reminiscing about how a mental illness diagnosis concocted years ago has kept Fasar out of prison for drug and arms trafficking offences, and he’s laughing along. It’s an astonishing performance from Smajic – like watching someone act out Hamlet while walking a high wire.Then, in the final episode, we learn Smajic is an even more nerveless and consummate actor than we thought – that she has double-crossed Brügger himself.From one of Smajic’s sound recorders, the producers recover a file she has deleted, and discover that she knows Wassem far better than she lets on. His name is not even Wassem, and from their conversation on the file, about transporting cash, she seems to be engaged in an entirely different caper with him, one that Brügger and TV2 know nothing about. Upon investigation, they find she has secretly been running a second office, where they suspect she has been laundering hundreds of thousands of kroner via invoice factories for other clients. (Smajic has said she never facilitated any crimes during this period.) In one case, TV2 claims to have found undiluted fraud: Smajic embezzling 65,000 kroner from a client by forwarding him emails that she’d fabricated, and that appeared to be from the tax authority. Smajic isn’t a crook on the mend at all, the film concludes; she’s a crook in the thick of committing an assortment of crimes. When she finds out that Brügger and his colleagues know about her side hustles, she demands the documentary be shelved.All documentaries are artificial: their footage has been carefully threshed and sieved with an eye to telling a story or pushing an argument. The Black Swan, though, relies on the unblinking, real-time gaze of hidden CCTV cameras, so we lull ourselves into thinking that we’re seeing the full picture, the full truth. No such thing. Instead, we get evasion upon evasion: Smajic’s charade for her clients, Malm cheating the taxman, TV2 withholding their work from the police, Brügger keeping details from his audience. Smajic’s final bluff merely confirms what Brügger seems to have believed throughout his career: everywhere, there are conspiracies and lies that he must expose, even if he has to participate in the dissembling himself.Brügger, who turns 53 in June, is a very tall, very bald man with a very red beard. He never seems to run out of conversational energy; whatever time of day it is, he’s likely to be ready to talk for hours, looking at you unblinkingly through his chunky spectacles as he tells you how bizarre or absurd the world really is. When I first met him, at the Frihedsbrevet offices in January, we loitered in the building’s courtyard, our pates goose-pimpling over in the Danish winter, so he could finish his cigarette. He warmed us up with fresh gossip. We had originally planned to attend, that evening, a public lecture by three TV2 journalists about The Black Swan. But Smajic had emailed the journalists a few days earlier, promising to show up and ask a few questions of her own, such as: “How does it feel to take credit for a program I pay for with my life (even though I’m still breathing, yes)?” or “How many people have you thrown under the bus against your better judgment to make your story work?”After the documentary’s release, fearing for her safety and that of her young son, Smajic had gone into hiding, so her cameo at the lecture would have been sensational. She would bring “a bunch of friends”, she warned – and then, in a second email, added: “Have you thought about and arranged security for that night … The assessment is that my participation that evening increases the risk for both me, you and the audience.”Citing caution, TV2 cancelled the event. I couldn’t tell if Brügger felt disappointed or vindicated – the first at the dashed prospect of seeing Smajic rising in the audience and setting it abuzz, the second at how Smajic’s emails appeared laced with an articulate derangement. “She’s an expert in creating conflict and manipulating people,” he told me. “If you plant her inside a biker gang, she could tear it apart within two weeks.”View image in fullscreenSmajic and Fasar Abrar Raja in a still from The Black Swan. Photograph: Wingman MediaBrügger was raised in the belief that conflict makes for great copy. His parents were journalists, and at the dinner table, his sister, Ane Cortzen, said: “We’d talk about society and politics, and you couldn’t just sit and listen. You had to have an opinion.” Cortzen remembers Brügger as an inventive child obsessed with comic books, to the point that he developed a “very black-and-white view of the world, in which some people are good and some are evil”. (On the middle finger of his right hand, Brügger wears a skull ring as homage to The Phantom, a comic-book crimefighter who wears a skintight purple suit and lives in a cave resembling a human cranium.) At university, Brügger studied film-making, and then worked at the state broadcaster, where he met his longtime producer, Peter Engel. “The best thing, I discovered, is to let him do his own stuff,” Engel said. “If you hear there’s a black market for diplomatic credentials, an ordinary journalist will say: ‘I’ll interview the broker and write a piece.’ Mads would say: ‘Let me become a fake diplomat.’ He always wants to step into his own universe.”As a documentarian, Brügger likes to make things happen. Not for him the Attenboroughian serenity of waiting for a lion to grow hungry and then track down its antelope; he’d rather starve the lion, hobble the antelope, and then introduce both beasts into a cage to film the carnage. In all his projects, Brügger has mounted elaborate, artificial setups just like Smajic’s office, and lured people into self-indictment, folly or sudden disclosures. Most of his films pivot on Brügger pretending to be someone he isn’t. In The Red Chapel, which won a Sundance award in 2010, he plays the manager of a pair of comedians touring North Korea. In The Ambassador, he impersonates a Liberian diplomat in Central African Republic. His cameras are, if not hidden, claiming to be present for benign purposes. In Pyongyang with his comedians, Brügger’s tapes were screened every night by a government agency; the film’s splenetic views of North Korea – “a sanctuary for crazy people” – emerge in the edits and in Brügger’s voiceovers. As in The Black Swan, the most burning question in these films is always: will someone tear the facade away and expose Brügger?Even in Cold Case Hammarskjöld, in which Brügger tamely appears as himself – a film-maker smelling conspiracy behind the death of Dag Hammarskjöld, the UN secretary general, in a 1961 plane crash – he cannot resist a splash of play-acting. He wears an all-white outfit, down to his sneakers, because, as he says in his narration: “I know for a fact that the villain of this story, he wore only white.” Brügger fails to prove that this villain – a long-dead South African mercenary – actually brought down Hammarskjöld’s plane, but that kind of factfinding is, in any case, never the priority of his films. “Hammarskjöld was a ticket to all the things I really enjoy: tracking down mercenaries, telling tales of evil men who dress in white, [and] rumours about secret African societies,” Brügger says in one voiceover. He’s always out for spectacle, shock and a wild ride. “If Hunter S Thompson had gone into film and ditched all the drugs,” a Norwegian columnist wrote of Cold Case Hammarskjöld, “maybe this is what he could have ended up with.”Brügger’s approach can leave his collaborators uneasy. When I spoke to one of the comedians in The Red Chapel, he euphemistically called Brügger’s journalism “uncompromising,” adding: “Mads is often portrayed as either a villain or a genius – and maybe he’s both.” An early collaborator described Brügger to me as ruthlessly ambitious – someone who wouldn’t hesitate to pilfer an idea or sell out a friend to make good TV. But he admits Brügger can be charming and persuasive, and I know what he means. When Brügger outlines his outre philosophies of journalism – of orchestrating scenarios and entering them – you feel like a mug for having arranged an interview in a cafe.It’s often unclear what Brügger is setting out to discover, what facts he’s seeking to establish. In The Red Chapel, even as his comedians rehearse on a riverbank, Brügger declares he wants to “expose the very core of the evil in North Korea” – an aim as grand and adolescent as it is vague. (As it happens, he never even makes it out of his minders’ line of sight.) As a Liberian diplomat in The Ambassador, Brügger tries to buy blood diamonds, pretends he wants to start a match factory in Central African Republic (CAR), and arranges to tour a “Pygmy village”. The CAR is a place with no moral boundaries, he tells us, and so it “offers itself as a sort of Jurassic Park for people longing for the Africa of the 1970s”. He circles some sort of exposé – evidence of how illicitly obtained diplomatic papers can be used to smuggle diamonds and commit other crimes – but never quite pins it to the mat. By the end of Cold Case Hammarskjöld, similarly, Brügger has proven no conspiracy. The ride has been weird, but the case remains cold.View image in fullscreenMads Brügger. Photograph: Marie Hald/The GuardianOnly in The Mole, a story of such reckless and dogged infiltration that it might have been hatched by an intelligence agency, does Brügger obtain more orthodox journalistic results. Ulrich Larsen, a retired chef who had watched The Red Chapel, tracked Brügger down in 2010 and volunteered as a spy within the Copenhagen chapter of the Korean Friendship Association. Brügger gave him cameras and instructed him to film everything. “I thought I’d just be showing these Danish guys as Monty Python weirdos doing silly walks,” Larsen told me. Instead, under Brügger’s supervision, Larsen posed so effectively as a sympathiser that he wound up penetrating the heart of North Korea’s influence network better than any full-time spy – and filmed himself doing it, to boot. With an accomplice, he duped North Korean officials into thinking he was setting up a drug and arms factory in Uganda – part of a plan to make enough money to buy North Korea weapons despite prevailing sanctions. The con ran a full decade – so long that Brügger sometimes clean forgot about it for months on end.Brügger wouldn’t necessarily quibble with these characterisations of his movies. “I’m a film-maker who craves sensation,” he says in The Mole, and that he does provide. His tone is caustic, his characters are colourful, and his plot twists are what Lotte Folke Kaarsholm, the opinion editor at the Danish daily Politiken, wryly calls “maximalist”. During one of our conversations, Brügger quoted Jørgen Leth, the doyen of Danish documentarians, to describe their line of work as “laying a trap in the forest and then waiting behind a tree to see who falls in”. Later, I looked up the quote and found that Leth had talked about setting a trap for reality, to capture the most authentic version of the world. “We are relaxed, attentive and noncommittal,” he said in a 2000 interview. “Things happen when they happen.” Leth was advocating patience and preparation; Brügger was thinking of a literal trap to tempt someone into making a mistake.One morning in Copenhagen, I visited Smajic’s lawyer, who led me into a conference room, laid his mobile on the table, and dialled her on speakerphone. The previous day, she’d been convicted in a different case of a million-kroner fraud; the following week, she would be sentenced to 18 months in prison. When we spoke, she was still in hiding, but there wasn’t a trace of anxiety in her voice. She complimented me on pronouncing her name correctly, and said she’d spoken to no other journalist since the documentary’s release. Midway through our conversation, while mentioning the episode during which Fasar threatened her, I told her I was appalled that journalists had put her in that position. I did mean it, but it’s also the sort of thing a journalist says, with exaggerated concern, to gain someone’s confidence. “That’s the first time someone has been sympathetic and said that,” she told me – something I knew to be false, because I’d read Danish columnists expressing the same views. Later, I learned that I was also not the first journalist to interview her about the film.Smajic believes she’s a victim of journalistic deceit. The Black Swan was meant to be about her life, she said, with the hidden camera footage being used only sparingly to corroborate her stories. She’d been offered no security during the filming, she said. When TV2 screened the first three episodes for her approval, they were really just raw, unedited clips, she maintained, and in any case, she’d been strongly medicated after a surgery and couldn’t assess them with a clear mind. (“Amira watched the edited episodes, they just needed finalising,” TV2’s Nørgaard told me. “During the four hours she spent with the editorial team that day, she appeared unaffected and seemed coherent, as we also documented in the series.”) Smajic hadn’t been running any other office at the time, she said to me, and in any case, “they hadn’t bought the rights to every single moment in my life”.Smajic felt betrayed. “For two years, these people were telling me to go through with it, saying: ‘This is going to be the biggest thing. You’re going to be a star.’” When she began worrying that the documentary would place her in danger, she asked for it to be suspended. “They figured that if they made me out to be a criminal, I wouldn’t have a say,” she told me. Early in 2024, months before The Black Swan was due to be broadcast, Smajic sued for an injunction against the film. A court denied her plea on grounds of public interest. In its verdict, it decided Smajic was fully aware of the project’s risks, the protection that TV2 arranged for her, and the ambit of the documentary.Among those who think Smajic was treated poorly is Jacob Mollerup, a veteran of the Danish media and a co-founder of Foreningen for Undersøgende Journalistik (FUJ), an association of investigative journalists. Mollerup described The Black Swan as “an exceptional production”, but argued Brügger had prized his dramatisation too much, abandoning fairness and balance in the bargain. “Normally, you protect your sources, but here they say: ‘Now she’s just a criminal, she broke our contract, so we can set aside her wishes about the production,” Mollerup told me. Hiding Smajic’s ongoing work as a police informant from The Black Swan’s viewers was dishonest, he said. If she was telling her handlers everything about the sting as it happened, that made it an operation implicitly sanctioned by the police – and plunged it into all sorts of ethical murk. Was Smajic inviting into her lair suspects whom the police wanted to nab? Which of the crimes being planned on camera were actually carried out, and how? Mollerup believes journalists must be transparent about their methods and precise in documenting misdeeds. When The Black Swan won an FUJ prize, he gave up his membership. “I told them: ‘This is not what I worked for.’”Brügger briskly rejected all of Smajic’s statements. She’d often claimed to be on medication before, he said, including once when she was pleading loss of memory while testifying in another criminal trial. She was merely recycling this excuse to explain to me why she hadn’t objected to the advance cuts of the first three episodes, Brügger told me. On his laptop, he found a photo of a production team’s stakeout that had been in place throughout the sting, in an office near Smajic’s. The team constantly watched the feed from the hidden cameras, ready to summon security if things went south – a setup she knew about, he said. When I wondered if the police had known of Smajic’s parallel adventures in money-laundering, or perhaps even endorsed them for their own purposes, Brügger said: “I find it highly unlikely, but it’s a possibility. The police wouldn’t confirm or deny this anyway.”Nothing I learned from Smajic solved the central mystery of The Black Swan: why did she choose to capsize her life by participating at all? Janet Malcolm, the deft vivisectionist of the psyche in journalism, would argue that such masochistic tendencies can be found in anyone who volunteers to talk to the press. But Smajic wasn’t just anyone: she was a habitual lawbreaker, so for her to let a television crew into the darkest corners of her life felt positively self-destructive. Perhaps she did think of exposure as disinfectant, a step towards a cure. Perhaps she believed she could bear any waves of bad press, or even surf them towards fame and freedom. “The thing is,” Brügger said, “with Amira, you can just never be sure of anything.”View image in fullscreenFrida Kahlo, The Two Fridas (1939). Photograph: Archivart/AlamyHe said this with the kind of awe that one veteran trickster reserves for another. Despite the lies and lawsuits, Brügger remains magnetised by Smajic. In one conversation, he’d mentioned his sole contribution to the decor of Smajic’s office: a print of The Two Fridas, in which Frida Kahlo painted herself twice, once with a gaping hole in her chest cavity, and again with a healthy heart and a small picture in her hand. “I’d hoped someone would come in and ask about it, but I didn’t prepare Amira for that,” Brügger said. This trap-within-a-trap is sprung in the fourth episode by a motorcycle thug’s curiosity, but Smajic improvises like a maestro. “It’s only when you cut out the heart that you can think coldly and rationally,” she explains. The picture the second Kahlo holds is of her child, Smajic ad-libs: “You can’t be cold if you have a family … That was me then, and this is me now.”Brügger was delighted. “It was brilliant. I became so happy and excited when I saw that. It’s my favourite scene.” The journalist may often be cast as the seducer, coaxing information out of people, but he’s just as liable to be seduced – by the mirage of the perfect story, as clean and vivid as a comic book. For Brügger, Smajic had initially promised to provide just that. When she turned out to be staging a perilous deception, throwing his production into chaos, he only grew further enthralled – perhaps because he recognised in Smajic an even more skilled version of himself.It’s difficult to feel sorry for Smajic, or for anyone in The Black Swan. The most moving passages in Brügger’s films always involve peripheral players in the great jape: the Ugandan villagers who are told they will be relocated so that an arms factory can be built on their land; or the North Korean interpreter who weeps at the memorial to Kim Il-Sung, claiming she’s mourning him but possibly grieving for some other reason; or the Central African Republicans who take lessons in how to make matches in a factory that Brügger will never build. To his credit, Brügger acknowledges the odd pang of guilt in his voiceovers – but only in passing.For The Ambassador, Brügger flew an Indian match-exporter named Sumeet Mehta to the CAR for a few days, ostensibly to train his employees. The Ambassador came out in 2011, but until I called him recently, Mehta didn’t know he’d featured in a documentary – or, indeed, that Brügger was a film-maker and not a diplomat. “I was kind of afraid to go, but I went anyway,” Mehta said, sounding baffled. “I sensed this factory was some kind of gimmick, but I didn’t know the reason behind it.” Ulrich Larsen told me that he wonders about the repercussions that the North Koreans unwittingly cast in The Mole might have suffered. “The rough answer is: I’m not responsible for what the regime does,” Larsen said. He hoped that “Mr Kang”, his translator in Pyongyang, was all right, “but of course, nobody knows. I did what I could. I brought his daughter a Lego.” Like Brügger, Larsen seemed to write it off as the cost of making an engrossing film. As Brügger says in The Red Chapel: “For your sake and mine, I have to lie.”Who killed the prime minister? The unsolved murder that still haunts SwedenRead moreThe Black Swan is such a careful, hermetically sealed production that it yields no such collateral damage, and I wondered if it was because Brügger was less cavalier in his own country, with his compatriots. Most journalists begin their careers at home before venturing farther afield. Brügger’s has run in reverse – in part, I think, because he, too, had once bought into the image of Denmark as a safe, dull place where nothing ever happens. “I’ve come to Africa because Europe has become old and tired,” he says in The Ambassador – a sentence that could have been uttered by a European man in any of the last half-dozen centuries. It was a backhanded jibe: a suggestion that Europe was no longer troubled by the anarchic social disorder that he desires in his films. The Black Swan showed Brügger can find all that he craves at home: conspiracy, corruption, shape-shifters, sensation, stories that evaporate like dry ice or swallow you like quicksand. The world is full of lies, not least the ones we tell ourselves. Listen to our podcasts here and sign up to the long read weekly email here.Explore more on these topicsThe long readDenmarkEuropeTelevisionOrganised crimefeaturesShareReuse this content",
    "summary": {
      "en": "**Summary of \"The Black Swan\" Documentary**\n\n\"The Black Swan\" is a documentary directed by Mads Brügger, featuring Amira Smajic, a former master criminal turned informant. Over six months in a rented office in Copenhagen, Smajic secretly filmed corrupt conversations with various individuals involved in financial crimes, including lawyers and gang members. The documentary reveals Denmark's hidden corruption and challenges the country's self-image as a virtuous society.\n\nSince its airing in 2025, \"The Black Swan\" has shocked Denmark, leading to criminal charges and the introduction of new money-laundering laws. About half of the Danish population has seen the series, sparking discussions about trust in public institutions and the reality of corruption in a country that prides itself on integrity.\n\nSmajic, known as the \"Ice Queen,\" had previously facilitated money laundering for biker gangs. Initially seeking redemption, she later revealed her duplicity, as she was still engaging in criminal activities during filming. The documentary raises ethical questions about journalistic practices, as Smajic's role as a police informant was kept secret.\n\nBrügger's provocative style often involves setting traps for his subjects, and this documentary is no exception. It showcases the ease with which individuals exploit the system, blurring the lines between crime and legality. Ultimately, \"The Black Swan\" exposes the underlying corruption within Danish society, prompting a reevaluation of trust and morality among its citizens.",
      "ko": "\"블랙 스완\"은 마즈 브뤼거가 감독한 다큐멘터리로, 범죄의 대가에서 정보 제공자로 변신한 아미라 스마이치가 주인공이다. 스마이치는 코펜하겐의 한 사무실에서 6개월 동안 다양한 금융 범죄에 연루된 인물들과의 부패한 대화를 몰래 촬영했다. 이 다큐멘터리는 덴마크의 숨겨진 부패를 드러내며, 이 나라가 자부하는 도덕적 이미지에 도전한다.\n\n2025년 방영 이후 \"블랙 스완\"은 덴마크에 큰 충격을 주었고, 범죄 기소와 새로운 자금 세탁 법안의 도입으로 이어졌다. 덴마크 인구의 약 절반이 이 시리즈를 시청했으며, 공공 기관에 대한 신뢰와 부패의 현실에 대한 논의가 촉발되었다.\n\n\"얼음 여왕\"으로 알려진 스마이치는 이전에 바이커 갱을 위해 자금 세탁을 도왔던 인물이다. 처음에는 구속을 원했지만, 촬영 중에도 여전히 범죄 활동에 연루되어 있었음을 드러내며 이중성을 보여주었다. 이 다큐멘터리는 스마이치의 경찰 정보원으로서의 역할이 비밀로 유지된 점에서 저널리즘의 윤리에 대한 질문을 제기한다.\n\n브뤼거의 도발적인 스타일은 그의 주제에게 함정을 설정하는 것으로 유명하며, 이 다큐멘터리도 예외는 아니다. 개인들이 시스템을 얼마나 쉽게 악용하는지를 보여주며, 범죄와 합법성의 경계를 모호하게 만든다. 결국 \"블랙 스완\"은 덴마크 사회의 근본적인 부패를 드러내며, 시민들 사이에서 신뢰와 도덕성에 대한 재평가를 촉구한다.",
      "ja": null
    }
  },
  {
    "id": "e762d69afa7de586",
    "title": {
      "en": "Scientists Discover New Heavy-Metal Molecule 'Berkelocene'",
      "ko": "과학자들, 신소재 '버켈로센' 발견!",
      "ja": null
    },
    "type": "story",
    "url": "https://newscenter.lbl.gov/2025/03/11/scientists-discover-new-heavy-metal-molecule-berkelocene/",
    "score": 118,
    "by": "gmays",
    "time": 1742821658,
    "content": "Key Takeaways\n\nScientists have discovered “berkelocene,” the first organometallic molecule to be characterized containing the heavy element berkelium.\nThe extremely oxygen- and water-sensitive complex was formed from 0.3 milligram of berkelium-249 using specialized facilities for handling air-sensitive and radioactive materials.\nThe breakthrough disrupts long-held theories about the chemistry of the elements that follow uranium in the periodic table.\n\nA research team led by the Department of Energy’s Lawrence Berkeley National Laboratory (Berkeley Lab) has discovered “berkelocene,” the first organometallic molecule to be characterized containing the heavy element berkelium.\nOrganometallic molecules, which consist of a metal ion surrounded by a carbon-based framework, are relatively common for early actinide elements like uranium (atomic number 92) but are scarcely known for later actinides like berkelium (atomic number 97).\n“This is the first time that evidence for the formation of a chemical bond between berkelium and carbon has been obtained. The discovery provides new understanding of how berkelium and other actinides behave relative to their peers in the periodic table,” said Stefan Minasian, a scientist in Berkeley Lab’s Chemical Sciences Division and one of four co-corresponding authors of a new study published in the journal Science.\nA heavy metal molecule with Berkeley roots\nBerkelium is one 0f 15 actinides in the periodic table’s f-block. One row above the actinides are the lanthanides.\nThe pioneering nuclear chemist Glenn Seaborg discovered berkelium at Berkeley Lab in 1949. It would become just one of many achievements that led to his winning the 1951 Nobel Prize in Chemistry with fellow Berkeley Lab scientist Edwin McMillan for their discoveries in the chemistry of the transuranium elements.\n“This is the first time that evidence for the formation of a chemical bond between berkelium and carbon has been obtained.”\n– Stefan Minasian, Chemical Sciences Division staff scientist\nFor many years, the Heavy Element Chemistry group in Berkeley Lab’s Chemical Sciences Division has been dedicated to preparing organometallic compounds of the actinides, because these molecules typically have high symmetries and form multiple covalent bonds with carbon, making them useful for observing the unique electronic structures of the actinides.\n“When scientists study higher symmetry structures, it helps them understand the underlying logic that nature is using to organize matter at the atomic level,” Minasian said.\nFrom left: Dominic Russo, Amy Price, Alyssa Gaiser, Polly Arnold, Jacob Branson, and Jennifer Wacker at Berkeley Lab’s Heavy Element Research Laboratory. They are co-authors on a new study published in Science, which reported their discovery of the heavy-metal molecule berkelocene. (Credit: Stefan Minasian/Berkeley Lab)\nBut berkelium is not easy to study because it is highly radioactive. And only very minute amounts of this synthetic heavy element are produced globally every year. Adding to the difficulty, organometallic molecules are extremely air-sensitive and can be pyrophoric.\n“Only a few facilities around the world can protect both the compound and the worker while managing the combined hazards of a highly radioactive material that reacts vigorously with the oxygen and moisture in air,” said Polly Arnold, a co-corresponding author on the paper who is a UC Berkeley professor of chemistry and director of Berkeley Lab’s Chemical Sciences Division.\nBreaking down the berkelium barrier\nSo Minasian, Arnold, and co-corresponding author Rebecca Abergel, a UC Berkeley associate professor of nuclear engineering and of chemistry who leads the Heavy Element Chemistry Group at Berkeley Lab, assembled a team to overcome these obstacles.\nAt Berkeley Lab’s Heavy Element Research Laboratory, the team custom-designed new gloveboxes enabling air-free syntheses with highly radioactive isotopes. Then, with just 0.3 milligram of berkelium-249, the researchers conducted single-crystal X-ray diffraction experiments. The isotope that was acquired by the team was initially distributed from the National Isotope Development Center, which is managed by the DOE Isotope Program at Oak Ridge National Laboratory.\n\n    The results showed a symmetrical structure with the berkelium atom sandwiched between two 8-membered carbon rings. The researchers named the molecule “berkelocene,” because its structure is analogous to a uranium organometallic complex called “uranocene.” (UC Berkeley chemists Andrew Streitwieser and Kenneth Raymond discovered uranocene in the late 1960s.)\nIn an unexpected finding, electronic structure calculations performed by co-corresponding author Jochen Autschbach at the University of Buffalo revealed that the berkelium atom at the center of the berkelocene structure has a tetravalent oxidation state (positive charge of +4), which is stabilized by the berkelium–carbon bonds.\n“Traditional understanding of the periodic table suggests that berkelium would behave like the lanthanide terbium,” said Minasian.\n“But the berkelium ion is much happier in the +4 oxidation state than the other f-block ions we expected it to be most like,” Arnold said.\nThe researchers say that more accurate models showing how actinide behavior changes across the periodic table are needed to solve problems related to long-term nuclear waste storage and remediation. “This clearer portrait of later actinides like berkelium provides a new lens into the behavior of these fascinating elements,” Abergel said.\nThis work was supported by the DOE Office of Science.\n###\nLawrence Berkeley National Laboratory (Berkeley Lab) is committed to groundbreaking research focused on discovery science and solutions for abundant and reliable energy supplies. The lab’s expertise spans materials, chemistry, physics, biology, earth and environmental science, mathematics, and computing. Researchers from around the world rely on the lab’s world-class scientific facilities for their own pioneering research. Founded in 1931 on the belief that the biggest problems are best addressed by teams, Berkeley Lab and its scientists have been recognized with 16 Nobel Prizes. Berkeley Lab is a multiprogram national laboratory managed by the University of California for the U.S. Department of Energy’s Office of Science.\nDOE’s Office of Science is the single largest supporter of basic research in the physical sciences in the United States, and is working to address some of the most pressing challenges of our time. For more information, please visit energy.gov/science.\n\n        Tags:\n\n                  Materials",
    "summary": {
      "en": "Scientists have discovered a new molecule called “berkelocene,” which is the first organometallic molecule containing the heavy element berkelium. This molecule was created from a tiny amount of berkelium-249 in specialized facilities that handle radioactive materials. The discovery challenges previous beliefs about how elements after uranium behave chemically.\n\nBerkelium, discovered in 1949 by Glenn Seaborg, is part of a group of elements known as actinides. While organometallic molecules are common for earlier actinides like uranium, they are rare for later actinides like berkelium. The research team, led by experts at Lawrence Berkeley National Laboratory, found that berkelocene has a unique structure with berkelium bonded to carbon, which helps scientists understand the behavior of actinides better.\n\nStudying berkelium is difficult due to its radioactivity and sensitivity to air. The researchers designed new equipment to safely conduct experiments with this element. Their findings revealed that berkelium can exist in a +4 oxidation state, which is different from what was expected based on traditional models.\n\nThis research may help improve understanding of actinides, which is important for dealing with nuclear waste and other related challenges.",
      "ko": "과학자들이 '버켈로신'이라는 새로운 분자를 발견했습니다. 이 분자는 중금속 원소인 버켈륨을 포함한 최초의 유기금속 분자입니다. 버켈로신은 방사성 물질을 다루는 특수 시설에서 소량의 버켈륨-249를 이용해 만들어졌습니다. 이 발견은 우라늄 이후의 원소들이 화학적으로 어떻게 작용하는지에 대한 기존의 믿음에 도전하는 결과입니다.\n\n버켈륨은 1949년 글렌 시보그에 의해 발견된 원소로, 액티늄 계열 원소 중 하나입니다. 유기금속 분자는 우라늄과 같은 초기 액티늄 원소에서는 흔하지만, 버켈륨과 같은 후속 액티늄 원소에서는 드물게 발견됩니다. 로렌스 버클리 국립 연구소의 전문가들로 구성된 연구팀은 버켈로신이 탄소와 결합한 독특한 구조를 가지고 있음을 발견했습니다. 이는 과학자들이 액티늄 원소의 행동을 더 잘 이해하는 데 도움을 줍니다.\n\n버켈륨은 방사능과 공기에 대한 민감성 때문에 연구하기가 어렵습니다. 연구자들은 이 원소를 안전하게 실험할 수 있도록 새로운 장비를 설계했습니다. 그들의 연구 결과, 버켈륨이 +4 산화 상태로 존재할 수 있다는 사실이 밝혀졌습니다. 이는 전통적인 모델에 기반한 예상과는 다른 결과입니다.\n\n이 연구는 액티늄 원소에 대한 이해를 높이는 데 기여할 수 있으며, 이는 핵 폐기물 처리 및 관련 문제를 다루는 데 중요합니다.",
      "ja": null
    }
  },
  {
    "id": "1d0cfd36de623aca",
    "title": {
      "en": "Things that go wrong with disk IO",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://notes.eatonphil.com/2025-03-27-things-that-go-wrong-with-disk-io.html",
    "score": 11,
    "by": "todsacerdoti",
    "time": 1743097212,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "ba0f61a32ee6bd0d",
    "title": {
      "en": "The Leader of the LeetCode Rebellion: An Interview with Roy Lee",
      "ko": "리트코드 반란의 주역, 로이 리 인터뷰",
      "ja": null
    },
    "type": "story",
    "url": "https://thepennpost.com/2025/03/25/nicolas-casey-the-leader-of-the-leetcode-rebellion-an-interview-with-roy-lee/",
    "score": 56,
    "by": "distalx",
    "time": 1743107644,
    "content": "Nicolas Casey | The Leader of the Leetcode Rebellion: An interview with Roy Lee\n\n\t\t\t\tPosted on March 25, 2025March 25, 2025\n\nStudent entrepreneur uses AI to beat Amazon interviews\n\nBy Nicolas Casey\n\nOn March 3rd, Roy Lee got an email from Columbia University. Figuring it was just another college newsletter, Roy, the creator of Interview Coder, was shocked at what he saw when he opened the latest message in his inbox. It was a notification from Columbia University’s Center for Student Success and Intervention (CSSI) that his case would be reopened following an initial Dean’s Discipline Hearing on February 17th, where he avoided potential suspension or expulsion. The Columbia College sophomore spoke with The Pennsylvania Post about the saga that has catapulted him to virality on X, caught the attention of the global media spanning from the Hindustan Times to CNBC, and raised vital questions about the viability of current paradigms surrounding digital assessments and formal education.\n\nRoy launched Interview Coder in February: a tool designed to solve LeetCode problems, the software engineering industry’s standard in applicant technical assessment. Long the bane of computer science majors applying for entry-level positions and internships, LeetCode has been subject to a great deal of criticism for being unrelated to the day-to-day reality of programming roles, posing a substantial barrier to entering the industry, and exacting substantial tolls on the mental health of applicants, who typically spend hundreds of hours preparing for career-defining technical interview problems that last around 20 or 30 minutes.\n\nThe emergence of large language models (LLMs) changed everything. Consumer facing LLMs like ChatGPT and Claude challenged the continued relevance of LeetCode interviews, as these tools persistently improve their ability to code. Leveraging this technology and the concepts he was learning in computer science courses, Roy set out to demonstrate this point, publishing a video of how he used Interview Coder to get a summer internship offer from Amazon at their headquarters in Seattle.\n\nTwo days after uploading the video to YouTube, which has since garnered over 100,000 views in a month, an Amazon executive filed a Behavioral Conduct Reporting Form with CSSI on February 9th at 3:26 AM. A day later, CSSI initiated the process of a Dean’s Discipline Hearing, resulting in a conversation with university administrators that ended amicably. CSSI initiated a File Distribution Agreement which barred publication of unauthorized materials related to their hearings. Together with Amazon’s aggressive response, these actions have left Roy feeling frustrated with the enforcement and merit structure that our education and employment systems deploy.\n\nToward the end of February, Interview Coder began gaining traction on X. Roy had posted a screenshot of an email to his advisor expressing his intent to drop out of Columbia and thought it would be an ideal time to capitalize on an emergent wave of virality by posting a copy of the redacted Behavioral Conduct Reporting Form that CSSI provided him. In his comments to the Penn Post, Roy said that he intended to “ride the marketing” and hopes to, “bring down LeetCode-style technical interviews.”\n\nRoy’s marketing instincts were right. The redacted form Roy posted has accumulated over 7 million views on X since February 27th. It also notably includes an implicit threat made by Amazon’s representative: absent “proper action” with respect to Roy, Columbia’s “long-standing partnership” with the tech giant might be at risk given their “[deep] concerns” over “situations like this.” In particular, the form makes explicit reference to Amazon’s Summer Undergraduate Research Experience (SURE) program, which provides undergraduate students from historically underrepresented communities with research experiences at a set of top-tier universities which include Columbia. When asked about what he thought the purpose of including this verbiage was, Roy expressed high confidence that this constituted a coercive threat of further negative publicity against the University.\n\nThis post, alongside another image, which the Penn Post has confirmed to come from an as-yet unreleased recording of Roy’s initial hearing, comprise the core of CSSI’s rationale for reopening his case. Since New York is a one-party consent state, unlike Pennsylvania, Roy’s recording is not illegal, and the crux of his disciplinary review rests solely in violations of the four policies outlined in CSSI’s March 3rd notice.\n\nFor his part, Roy believes that his actions in releasing the redacted form and recording the hearing were justified as a form of free speech. Similarly, he is adamant in his belief that his creation and usage of Interview Coder were not malicious and constitute a protest, which he views as being “shut down and silenced by incumbents.” To that end, Roy sent an email to Amazon onboarding denying their offer of employment two days prior to uploading his YouTube video. In the email, Roy states that he never intended to work at Amazon and apologizes for any inconvenience that he caused—details that were not included in the redacted version of Amazon’s form.\n\nWhile discussing his motivations to create Interview Coder, Roy told the Penn Post that there was nothing unique about it and views its abilities as an inevitability in a world of LLMs. Additionally, he observed that this variety of digital subversion is already widespread among students but conducted in a more ad hoc fashion—without a tool as polished as Interview Coder. If the abuse was already happening and would eventually become a product, Roy thought he might as well build it well.\n\nRoy argues that LLMs influence on academic work is a problem today on Columbia’s campus. He cites his experiences observing how easily students are able to simply copy questions from Canvas (a popular learning management system) into ChatGPT for an answer. He also expressed concerns over the integrity of College Board’s exams as it pushes online exam delivery.\n\nIn closing, the Penn Post asked for Roy’s thoughts on the future of the software industry and potential solutions for the flaws exposed by Interview Coder. Addressing future hiring practices, Roy saw two paths. If the goal is to purely evaluate an applicant’s knowledge, high-stress, time-constrained challenges should be replaced with a holistic code review of past work. This could include anything from independent projects to repository contributions on platforms like GitHub, which would allow for a deeper understanding of someone’s technical ability. On the other hand, employers looking for realistic assessment results are best served by embracing LLMs as the standard tool for professional software engineers that they are, integrating them into more relaxed technical interviews.\n\nRoy does not plan to ride the Interview Coder wave beyond the eventual crash of the media cycle that it has created. He also told the Penn Post that he is currently working on a different project, which will be released soon on his X account.\n\nNicolas Casey is a senior in the College studying Mathematical Economics from London, England. Nick is also the Technology Director forThe Pennsylvania Post. His email is ncasey25@sas.upenn.edu.\n\n\t\tPosted in News",
    "summary": {
      "en": "Roy Lee, a sophomore at Columbia University, created a tool called Interview Coder to help solve LeetCode problems, which are often used in technical job interviews but criticized for being disconnected from real-world programming tasks. After using his tool to secure a summer internship at Amazon, Roy faced scrutiny from the university and Amazon, leading to a disciplinary hearing due to concerns over his actions and their potential impact on university partnerships with the tech giant.\n\nRoy's video showcasing Interview Coder went viral, gaining over 100,000 views and sparking significant media attention. He believes he was unfairly targeted for expressing his views on the flaws of traditional technical interviews and argues that his actions were a form of free speech. He also pointed out that many students are already using AI tools inappropriately in their academic work.\n\nLooking forward, Roy suggests that hiring practices should shift from high-pressure technical challenges to more holistic assessments of applicants’ past work. He doesn't plan to continue the momentum of Interview Coder for long and is working on a new project.",
      "ko": "콜롬비아 대학교 2학년인 로이 리는 기술 직무 면접에서 자주 사용되는 리트코드 문제를 해결하기 위해 '인터뷰 코더'라는 도구를 만들었다. 이 도구는 실제 프로그래밍 작업과는 거리가 멀다는 비판을 받고 있다. 로이는 이 도구를 사용해 아마존에서 여름 인턴십을 확보했지만, 그의 행동이 대학과 아마존의 파트너십에 미칠 영향에 대한 우려로 인해 대학과 아마존으로부터 조사를 받았다. 이로 인해 징계 청문회에 참석해야 했다.\n\n로이가 인터뷰 코더를 소개하는 영상을 올리자 이 영상은 10만 회 이상의 조회수를 기록하며 큰 화제를 모았다. 그는 전통적인 기술 면접의 문제점에 대한 자신의 의견을 표현한 것 때문에 불공정한 표적이 되었다고 주장하며, 자신의 행동은 표현의 자유의 일환이라고 강조했다. 또한 많은 학생들이 이미 학업에서 AI 도구를 부적절하게 사용하고 있다는 점도 지적했다.\n\n앞으로 로이는 채용 방식이 고압적인 기술 도전 과제에서 지원자의 과거 작업에 대한 보다 포괄적인 평가로 변화해야 한다고 제안했다. 그는 인터뷰 코더의 흐름을 계속 이어갈 계획은 없으며, 새로운 프로젝트에 집중하고 있다.",
      "ja": null
    }
  },
  {
    "id": "b7171f9759293716",
    "title": {
      "en": "University of Toronto Snags Yale Professors as Canada Raids American Brain Trust",
      "ko": "토론토대, 예일 교수 영입!",
      "ja": null
    },
    "type": "story",
    "url": "https://deanblundell.substack.com/p/canadas-quiet-coup-u-of-t-snags-yale",
    "score": 22,
    "by": "newaccountlol",
    "time": 1743096512,
    "content": "Share this postDean BlundellCanada’s Quiet Coup: U of T Snags Yale Professors as We Start Raiding America’s Brain Trust Copy linkFacebookEmailNotesMoreDiscover more from Dean BlundellMedia guy, content provider, dog whisperer, Canadian raconteur, muckraker. Over 6,000 subscribersSubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.Already have an account? Sign inCanada’s Quiet Coup: U of T Snags Yale Professors as We Start Raiding America’s Brain Trust If we do this right, America won’t be our neighbour—it’ll be our talent farm.Dean BlundellMar 27, 2025496Share this postDean BlundellCanada’s Quiet Coup: U of T Snags Yale Professors as We Start Raiding America’s Brain Trust Copy linkFacebookEmailNotesMore4159ShareIt’s not a joke anymore. The University of Toronto is actively recruiting—and landing—some of the United States’ most high-profile intellectuals, and it’s not just about prestige. It’s about survival.This Substack is reader-supported. To receive new posts and support my work, consider becoming a free or paid subscriber.SubscribeU.S. President Donald Trump’s reelection has officially set off the intellectual fire alarm. America’s top minds are packing up their books, burning their Yale parking passes, and heading north before the last lights go out on the republic.And we’re not talking about a couple of junior profs looking for a sabbatical escape hatch. We’re talking academic giants. The kind of people who usually advise presidents, not run from them.“Of course our American catastrophe played a role in our final decision.”— Marci Shore, Yale historian now at the Munk School, U of TU of T Is Becoming America’s Backup BrainHere’s the roll call:Timothy Snyder – Bestselling author of On Tyranny and The Road to Unfreedom, a global authority on authoritarianism and the Trump threat, is now joining the Munk School of Global Affairs & Public Policy.Marci Shore, his partner and fellow Yale professor, is also making the move. She says she and Snyder will be in Toronto “for the long-term.”Jason Stanley, the philosopher and author of How Fascism Works, also just took a job at U of T, citing Trump’s attacks on U.S. universities as the breaking point. Stanley didn’t mince words: “My decision was entirely because of the political climate in the United States.”This is more than symbolic. It’s strategic migration—the kind that happens when people no longer believe their country is safe for their work, their families, or their principles.And Canada? We’re the safe house.Why They’re Fleeing: The American Academic Purge Has BegunTrump’s second term isn’t even six months in, and he’s already gutting universities like they’re woke hotbeds of Marxist terror.Columbia University—an Ivy League bastion—just folded like a cheap lawn chair, agreeing to:Hire campus police with arrest powersBan masks at protests (aka criminalize student dissent)Review its Middle Eastern studies programs (aka political censorship)Jason Stanley saw the writing on the wall and got out. Fast.And he’s not alone. Snyder and Shore say U of T began recruiting them two years ago, but Trump’s return—and America’s rapid march toward soft fascism with a Fox News aesthetic—sealed the deal.“Both Toronto and the Munk School were and are very attractive places unto themselves... bracketing the American descent into fascism.”— Marci ShoreTranslation: Canada was already attractive. America just made the decision really, really easy.It’s Not Just Professors—Canada Has a Shot at a Talent RevolutionLet’s zoom out: it’s not just Ivy League profs fleeing Trump’s America. It’s doctors, teachers, scientists, and tech talent—every sector Trumpism is targeting.DoctorsThey’re already coming to Canada in droves. Ontario and some maritime provinces actively and successfully recruit doctors and female OBGYNs. But we can do better. Red-state doctors are walking off the job or looking for ways out. Between abortion bans, anti-trans laws, and government interference in medicine, many are saying “screw this” and eyeing the border.Canada is short over 44,000 doctors. Let’s connect the dots.TeachersChristian nationalists are rewriting curricula. AP African American Studies and AP Psychology are banned in multiple states. Teachers are being surveilled for teaching \"unapproved\" facts. We need 30,000+ teachers over the next decade. You do the math.Tech WorkersElon Musk turned Twitter into Truth Social with AI. Meanwhile, conservative states are trying to “de-wokeify” tech. Canadian cities like Toronto, Montreal, and Halifax have growing tech hubs, real housing markets, and a thing called universal health care.Creatives and ActorsHollywood? Already half in Vancouver. Just ask Shawn Majumder, who bailed on L.A. because he didn’t want to raise his kids in a country where assault rifles have better access to school than teachers.Canada’s Play: Open the Tap, Drain the BrainThis is the moment to be bold. Canada can’t outspend or outarm America, but we can outsmart them.Create a “Brain Rescue Visa”: fast-track U.S. professionals fleeing political persecution, professional sabotage, or threats to academic and medical freedom.Let’s actively recruit:Physicians from red statesClimate scientists are being silencedJournalists and academics under political pressureLGBTQ+ professionals targeted by legislationTech experts disillusioned by MAGA Silicon ValleyWe could even incentivize regional settlements—send them where we actually need people: northern Ontario, rural Alberta, Newfoundland, the Prairies, andthe North.Bonus: they pay taxes, start companies, and strengthen Canadian democracy.Because unlike in America, here, fascism isn’t a growth industry.Thinking Bigger: America as Canada’s 11th Province (With 50 Municipalities)You know what comes after brain drain? Institutional collapse.If Trump’s second term continues gutting universities, press freedom, and public health like it’s a Steve Bannon fever dream, the U.S. will hollow out fast.Canada should be ready—not to invade, but to absorb.Think about it:Washington State? Already more aligned with B.C. than Texas.Minnesota? Culturally halfway to Manitoba.Vermont? Might as well be Quebec’s hipster cousin.Start laying the groundwork: trade, immigration, dual infrastructure, cultural exchange. Let the 11th province idea grow organically—one defector at a time.And while we’re at it, give Pierre Poilievre a Greyhound ticket to Florida. Let him see what his political idols are building.Thanks for reading! This post is public so feel free to share it.ShareSubscribe to Dean BlundellLaunched a month agoMedia guy, content provider, dog whisperer, Canadian raconteur, muckraker. SubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.496 Likes∙159 Restacks496Share this postDean BlundellCanada’s Quiet Coup: U of T Snags Yale Professors as We Start Raiding America’s Brain Trust Copy linkFacebookEmailNotesMore4159Share",
    "summary": {
      "en": "The University of Toronto (U of T) is attracting top American professors, including those from Yale, as many intellectuals flee the political climate in the U.S. under President Trump. Notable figures like Timothy Snyder and Marci Shore have cited the deteriorating conditions for academics in America as a reason for their move to Canada. This trend is not limited to professors; professionals across various fields, such as medicine and technology, are also seeking a safer environment in Canada due to political pressures in the U.S.\n\nCanada has an opportunity to capitalize on this influx by creating a \"Brain Rescue Visa\" to fast-track professionals escaping political persecution. By actively recruiting these individuals, Canada could strengthen its workforce and democracy. The author suggests that as the U.S. faces potential institutional collapse, Canada should prepare to absorb talented individuals, fostering closer ties between the two countries.",
      "ko": "토론토 대학교는 미국의 정치적 분위기에서 벗어나려는 지식인들이 늘어나면서 예일대학교를 포함한 미국의 저명한 교수들을 유치하고 있습니다. 티모시 스나이더와 마르시 쇼어와 같은 저명한 인물들은 미국의 학문적 환경이 악화되고 있는 것을 캐나다로 이주하는 이유로 들고 있습니다. 이러한 경향은 교수들에만 국한되지 않고, 의학과 기술 등 다양한 분야의 전문가들도 미국의 정치적 압박을 피하기 위해 캐나다에서 더 안전한 환경을 찾고 있습니다.\n\n캐나다는 이러한 인재 유입을 활용할 기회를 가지고 있으며, 정치적 박해를 피해오는 전문가들을 위한 \"브레인 레스큐 비자\"를 만들어 신속하게 이들을 수용할 수 있습니다. 이러한 인재를 적극적으로 유치함으로써 캐나다는 자국의 노동력과 민주주의를 강화할 수 있을 것입니다. 저자는 미국이 잠재적인 제도적 붕괴에 직면하고 있는 상황에서, 캐나다가 인재를 받아들일 준비를 하여 두 나라 간의 관계를 더욱 돈독히 해야 한다고 제안합니다.",
      "ja": null
    }
  },
  {
    "id": "e55026afcf765d59",
    "title": {
      "en": "The Quantum Chaos of Literature",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.nybooks.com/articles/2025/04/10/the-quantum-chaos-of-literature-benjamin-labatut/",
    "score": 4,
    "by": "mitchbob",
    "time": 1742885892,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "a8137c3c4854deab",
    "title": {
      "en": "Building a modern durable execution engine from first principles",
      "ko": "현대적 실행 엔진 구축하기",
      "ja": null
    },
    "type": "story",
    "url": "https://restate.dev/blog/building-a-modern-durable-execution-engine-from-first-principles/",
    "score": 26,
    "by": "whoiskatrin",
    "time": 1743083504,
    "content": "Building a modern Durable Execution Engine from First PrinciplesHow Restate works, Part 2Posted February 20, 2025 by Stephan Ewen, Ahmed Farghal, and Till Rohrmann‐20min readWe dive into the architecture details of Restate, a Durable Execution engine we built from the ground up. Restate requires no database/log or other system, but implements a full stack that competes with the best logs in terms of durability and operations.This is the second article in our series on building a durable execution system from first principles. The first blog post in this series, Every System is a Log, looks at this from the application side, and shows how a unified log architecture results in a tremendous simplification of distributed coordination logic. This post discusses the details of how we built the log-based runtime for that paradigm.Modelling locking and database updates through an application log,taken from Every System is a LogTo build this runtime, we asked ourselves, what such a system would look like when designed from first principles? We built a precursor to this with Stateful Functions, and from all the lessons learned there, we arrived at a design with a self-contained complete stack, centered around a command log and event-processor, shipping as a single Rust binary. To get an idea of the user experience we arrived at, check the videos in the announcement post.This stands somewhat in contrast to the common wisdom “don’t build a new stateful system, just use Postgres”. But we saw a clear case to build a new stack, for multiple reasons: First, the interactions and access patterns are different enough from existing systems that we can offer both better performance and operational behavior, similar to why message queues exist, even though you can queue with a SQL table. Second, the architecture of event logs has made significant advancements in recent years, but the advanced implementations are exclusive to proprietary stacks and managed offerings - the open source logs and queues still follow architectures from the on-prem era. Third, we saw how a converged stack lets us provide a much better end-to-end developer experience, from first experiments on your laptop to multi-region production deployments.Recap: Server and Services #A Restate application stack consists of two components: Restate Server, which sits at a similar place in your stack as a message broker, and the application services, which are durable functions/handlers containing the application logic. The server receives invocation events, persists them, and pushes them to the services, similar to an event broker. The services run the code that corresponds to RPC- or event-handlers, workflows, activities, or actors. Services may run as processes, containers, or even serverless functions.But Restate doesn’t just push invocations, it maintains a bidirectional connection with the executing service handler and lets the code perform durable actions as part of the invocation, including journaling steps, sending events to other handlers, accessing/modifying state, creating persistent futures (callbacks) and timers. The services use a thin SDK library which communicates the actions to the server - somewhat comparable to a KafkaConsumer or JDBC client, but more high-level. See Restate’s examples for sample code and details.The server handles all coordination and durability for the invocation life cycle, journals, embedded K/V state, and manages failover, leader-election, and fencing. The server’s view on an invocation and its journal is the ground truth; the services follow the server’s view and function executions may be cancelled/reset/retried as needed.That approach makes the services completely stateless and simple to operate. They scale rapidly and run on serverless infrastructure like AWS Lambda, Cloudflare workers, etc.This characteristic also lets us build those SDKs in many languages fairly easily, including TypeScript, Java, Kotlin, Python, Go, and Rust.Clusters, Object Stores, and the Latency Gap #A distributed Restate deployment is a cluster of nodes that are connected to each other. Invocations and events can be sent to any node, and all nodes participate in the storage of events and dispatching of invocations to services/functions. Restate is active/active from a cluster-perspective, but has leader/follower roles at the granularity of individual partitions, similar to systems like Kafka or TiKV.Restate stores data using two mechanisms: New events (invocations, journal entries, state updates, …) are persisted in an embedded replicated log (called Bifrost). From there, events move to state indexes in RocksDB, which are periodically snapshotted to an object store. So at any point in time, the majority of the data is durable in the object store (the nodes maintain a copy as cache) while a smaller part of the data is durable in the log replicated across the nodes.This is a form of storage tiering, though not the classical tiering like in modern logs. It is more similar to a database management system, where the write-ahead-log (WAL) would be replicated across nodes, while the table data files and indexes would be persisted on S3 (and cached on the nodes).Object store + latency gap #Architectures that keep most- or all - of their data on object stores have become popular for many reasons: Object stores are unbeatable in terms of combined scalability, durability, and cost (AWS S3 cites eleven 9s of durability, stores more than 100 trillion objects, and is cheaper than persistent disks). Plus, the storage exists disaggregated from compute nodes, making the nodes stateless (or owning little state), which is highly desirable for efficient operations.Object stores are also available in most on-prem setups we’ve encountered. It was natural for us to design Restate such that object storage would be the primary durability for the majority of the data.The reason why Restate has additionally a replication layer that persists new events (rather than writing events straight to object storage) is to provide low latency. Pure object store approaches have latencies that average around 100ms to make data durable, with tail latencies being a multiple of that. While that is feasible for analytical systems (e.g., Apache Flink) and data pipelines (like WarpStream), such latencies can quickly become prohibitive for many applications.Restate’s replication bridges the latency gap between the requirements of fast durable functions and the capabilities of object storage.Navigating the cloud latency-cost-disk triad #The setup described above is what we ship first, in Restate 1.2: a fast log replicated between Restate server nodes. However, Restate uses virtual log abstraction, to easily support other log implementations as well, without having to build a full consensus machinery each time. This is a defining feature of Restate’s runtime implementation that we’ll dive deeper into in the next article in this series. We are currently using that mechanism to build object-store support in the log as well, which is a powerful feature for bringing the amount of data persistent on nodes to very small amounts, even zero.There is no single best configuration for that setup - only a spectrum of trade-offs to pick from. In his Materialized View newsletter, Chris Riccomini describes it as a CAP-theorem-like choice:In our context (durable execution runtime), durability must be a given, but we have the additional dimension of how much replicated data is kept on the nodes. Restate’s triad thus is: latency-cost-disk.➕ Low latency, ➕ low cost, ➖some data on disks: Quorum replication to nodes with async batch writes to S3.The nodes provide fast durability through replication and keep the data for anything between a few 100ms and minutes, before moving the events to object storage. Restate 1.2 can be seen as a variant with a long flush interval.➕ Low latency, ➖ high cost, ➕ no data on disks: Quorum replication directly to S3 Express One Zone.Restate’s replication mechanism deals only with ordering, quorum, and repairs upon loss of a zone, but doesn’t keep data on the nodes (except caches).➖ High latency, ➕ low cost, ➕ no data on disks: Synchronous batch writes to S3.Restate’s replication mechanism isn’t used.Naturally, there are nuances: direct replication has an even lower latency than S3 Express 1Z quorums. Synchronous batch writes to S3 can be cheaper than anything else, because that approach may avoid cross AZ bandwidth cost. Disks still exist as caches in all configurations. And there is the option of using quorum replication to S3 Express 1Z in different regions, to support multi-region deployments without relying on disks. But it shows that there is the spectrum of options, with which we aim to enable developers to use Restate in diverse setups across cloud and on-prem, while maintaining a simple dependency: just an object store.Restate 1.2 ships with all the virtual log infrastructure, and a low-latency replicated log implementation. We are currently working on the other configurations - if you are interested in being an early tester or design partner, please reach out to us.As a final thought, being able to adjust to different trade-offs also helps Restate and its users adapt to changing cloud pricing models. To quote another prolific dist. sys. writer:Partitioned Scale out #Restate follows the partitioned scale-out model: A cluster has a set of partitions, each with a log partition and an event processor instance. Partitions operate independently and allow the system to scale both across cores and nodes.Everything related to an invocation happens within a single partition: invocation, idempotency & deduplication, journal entries, state, promises/futures, avoiding the need to synchronize and coordinate with any other shards. The target partition for an invocation is determined by hashing the virtual object key, workflow ID, or idempotency key, if applicable - otherwise, the partition is freely chosen.In some cases, a function execution produces an event for another partition, for example RPC events or completions. In that case, events are still written to the local partition, and the server has an out-of-band exactly-once event shuffler to move events to the right target partition.The partitions are not exposed to applications (though you see them when using restatectl) - only keys are directly addressable (virtual object id, workflow id, idempotency key), to allow changing the number of partitions without losing consistency.From here on, we look only at what happens inside a single partition.Event Log and Processor #The work that Restate Server does inside one partition happens in two components: the distributed durable log (called Bifrost) and the processors. The log is the fast primary durability for events (e.g., make invocations, add journal entry, update state, create durable promise, …), the processor acts on events (e.g., invokes handlers) and materializes their state. Log and processor are co-partitioned, meaning a partition processor connects to one log partition. They are independent, but frequently co-located in the same process.Compared to databases, you can think of Bifrost as the transaction WAL, and the processor as the query engine and table storage. Compared to stream processing, you can think of Restate’s log as Kafka and the processor as a stream processing application (like KStreams or Flink).Log and processor form a tight loop: the processor continuously tails the log, and acts on the events (e.g., making invocation). That may produce more events (journal entries, state updates, …), which are written to the log and again processed by the processor.Let’s go through an example to illustrate this:A client invokes service handler processPayment with idempotency key K through Restate. The ingress enqueues the invocation to the log partition, as determined by hashing K.The leader Processor for the partition receives that event and checks its local idempotency key state. K is not contained there for processPayment. The processor atomically adds K to the state and transitions the invocation to running, then builds a connection to the target service endpoint, and pushes the invoke journal entry.The service streams back a step result event (ctx.run({...})) and the processor enqueues that journal entry event in the log. Being persistent in the log is the point when “the step happened”, meaning from there on it will always be recovered.When the processor receives the event from the log (that means no other processor has taken over leadership in the meantime) then it adds the event to the invocation’s journal state and sends an ack to the service.Similar steps happen when the service sends a state update, a timer, an RPC event, or creates a durable promise. Events get always added to the log first, and once they are received by the processor they are acted upon (e.g., added to journal, routed as invocation to other service, etc.)Once the invocation completes, the Processor adds the result event to the log. Upon receiving that event, it sets the invocation state as complete and sends the result back to the client.When the function execution fails (e.g., crash, loss of connection, user-defined error), the processor dispatches a new invocation, attaching the full journal events from this invocation so far. To avoid split brain scenarios between services, the processor tracks invocation execution attempts (retries) and rejects events sent from an invocation if a newer attempt has started. This can be tracked with simple in-memory state, because invocations are sticky to partitions, and partitions have strong leaders.State storage #The processor stores all its non-transient state in an embedded RocksDB instance. Operations on that embedded store are very fast, but the state is lost when the node is lost. However, all state of the processor is deterministically derived from the durable log and can always be rebuilt from the log during recovery. To avoid arbitrarily long re-build phases, the RocksDB database is periodically snapshotted to the object store and the log is trimmed to the point of the snapshot. Processors can be restored by fetching the latest snapshot and attaching to the log at the event sequence number when the snapshot was taken.The implementation of the partition processor is a tight event loop in Rust’s Tokio runtime. Partition Processors operate independently from each other and access exclusively local data structures (in memory, RocksDB, streams to ongoing invocations). The partition-local handling of invocations is easy in a log-first design, and would be much harder to achieve if we built this on a general purpose database.That property makes the design also both simple and fast: Committing an event (e.g., step / activity) means appending the event to the log (obtaining a write quorum). As soon as that happens, the event is pushed from the log leader in-memory to the attached processor and ack-ed to the handler/workflow. This takes a single network roundtrip for a replication quorum, and no distributed reads. The durability of RocksDB happens completely asynchronously in the background.Leaders and Followers #Both log and processors have one leader and optional followers. In the case of the log, followers increase durability for events through additional copies. In the case of processors, followers are hot standbys that have a copy of the state (deterministically derived from the log) and can quickly take over upon failure. Only the processor leader actually dispatches invocations for functions and workflows to the services, and only the leader writes snapshots to object store.High-level architecture and request flow.Control Plane, Data Plane, External Consensus #So far, everything we looked at was the data plane of the system: The log and the partition processor.Everything is coordinated by a control plane, which is responsible for failure detection, failover coordination, and re-configuration. The control plane stores metadata for the cluster (like configurations) and runs the cluster controller that handles partition placement and leader election.Control Plane and Data PlaneBecause Restate has one control plane for both log and processors, it can co-coordinate both, for example ensuring that the leader processor is always co-located with the log partition leader, to reduce network hops and optimize reading from local memory caches. In contrast, if we were to build this transparently on an external log like Kafka, this co-location would be harder to achieve. The benefits of this joint control plane show in many parts of the system and are one of the reasons Restate is simpler to set up, scale, and operate.Besides managing re-configurations and failover, the control plane also provides the external consensus for the data plane, allowing the data plane to operate more efficiently and with simpler properties than full consensus. We’ll go into the details of Restate’s log implementation in the next blog post - for now, a useful high-level way to think about this is that the control plane moves the data plane from one steady configuration to another, whenever the previous configuration is no longer functional (a failure happened) or desired (e.g., re-balancing). This blog post from Jack Vanlightly gives a nice introduction to that concept.\nThe Control Plane reconfigures the Data Plane (Figure from “An Introduction to Virtual Consensus in Delos “by Jack Vanlightly)Another benefit of this design is that it allows Restate to use a simpler/slower implementation of consensus on the control plane, because it is rarely invoked. Restate abstracts its consensus to just an atomic compare-and-swap (CAS) metadata operation, which the built-in metadata store backs with an implementation of the RAFT consensus algorithm. But this can be easily extended to plug in different storage systems, as long as they support atomic CAS.Failover & Reconfiguration #Though the control plane jointly coordinates log and processor reconfiguration, each has their own mechanisms to ensure consistency.A segmented log (Figure from “An Introduction to Virtual Consensus in Delos “by Jack Vanlightly)Bifrost’s (the log’s) mechanism is based on a mix of Delos (Virtual Consensus) and LogDevice. From a high-level, bifrost is segmented and failover or reconfiguration seals the active segment and creates a new segment, possibly with a new leader and a different set of nodes that store replicas. To the outside and the partition processors, everything looks like a single contiguous log.When a partition processor fails, the control plane selects a new leader for that partition.The failover procedure relies on the external consensus provided by the control plane: New leaders obtain the next epoch in a strictly monotonous sequence (so newer leaders have higher epochs). The new leader appends a message to the log to signal their epoch is now active, and then simply starts appending events from its operations. The old leader (who might still be following the log) will receive that epoch bump message and step down at that exact point - it will keep materializing state (as a follower) but not dispatch invocations any more. The old leader also aborts ongoing function executions and lets the new leader recover those.Leader handover via messages in the logAny messages carrying lower epochs than the latest epoch-bumping message will be ignored, which filters messages that the old leader might have still been appending to the log before it found out that it was superseded by another leader. If the old leader was attempting to commit a journal entry, but the message was appended after the epoch-bump message, that commit cannot happen: The new leader will (or might have already) recover the process without that journal entry and execute and commit that step. This mechanism ensures that any step / activity result is committed exactly once. No split brain view is possible.This mechanism also automatically resolves concurrent competition for leadership - the highest epoch will win, and late events are consistently ignored.Converged Single Binary #Restate is architected as a set of individual components that communicate with each other and make no assumptions about the whereabouts of their peers. A Restate binary can run every component or a subset of them; a set of components is described by a role.The default configuration is the converged mode, where every binary runs every role. In that case, you get a distributed architecture in a single binary. You can start more instances of the binary to form clusters. This mode is easy to use and efficient, because it also lets different components communicate efficiently through in-memory channels and caches whenever possible (e.g., log to processor).The roles and components of RestateHowever, you can of course also run it as a disaggregated setup, where different sets of nodes run different roles. That lets you separate control plane from data plane and pick your best trade-offs in terms of cost/durability/availability for metadata and data. For example:Deploy three nodes with the admin role across three different regions, ensure application and consensus metadata are disaster proof.Deploy six nodes with the Log-Server role across three availability zones, making sure data is replicated to tolerate zone outages.Deploy Ingress and Worker roles in one availability zone to strictly co-locate them with zone-local services.Restate’s architecture gives you a great developer experience from the start (launching a single binary on your laptop) all the way to sophisticated distributed deployments (disaggregated distributed setups).Some Performance Numbers #How fast can a system like this be? The answer is, we don’t really know, we have plenty more optimization we can do. Our main focus for this release was durability, resilience, and operational tooling.But even before any deep performance optimization, the system already pushes some pretty cool numbers, both latency- and throughput wise. Below are numbers from Restate 1.2. We measure the throughput / latency of the server against mock functions and activities, to put maximum stress on the server.Latency #Restate aims to keep latencies low, despite giving strong guarantees on durability (replication) and consistency (strong consensus on locking keys, workflow ids, etc.).Below are the numbers for durable functions with an increasing number of intermediate durable steps, running on a 3-way replicated cluster. Under low load, things are generally fast and a single step has a median latency of 3ms. Under high load, steps still have a median latency of 10ms after the initial workflow setup. Tail latencies under low load are a bit higher than we like (possibly caused by some Tokio / RocksDB issues) and we believe we can get these down in the future.LoadIntermediate StepsLatencyp50p90p99Low (10 concurrent clients)05ms34ms54ms315ms42ms69ms931ms57ms93ms2761ms106ms155msHigh (1200 concurrent clients)028ms41ms58ms358ms76ms98ms9116ms138ms163ms27283ms320ms356msOne thing you can observe here is that Restate is built to make durable steps cheap. An initial function or workflow invocation needs to check whether the workflow ID, idempotency key, or object key already exists and whether it is under execution. But once an invocation has been made, adding steps is just the equivalent of a conditional append to the log.Throughput #We ran a workflow of 9 intermediate durable steps (totalling 11 actions, including invoke/result), using 1200 concurrent clients to submit workflows.Restate pushes 94,286 actions (steps) per second, equalling 8,571 full workflows each second. The system maintains a p50 of 116.36ms and a p99 of 163.33ms for the full workflow! The p50 per step is 10ms.The experiment ran on AWS c6id.8xlarge nodes, which are admittedly beefy, but also deliver a throughput that most companies will not ever reach or exceed in their transactional load. And, if they exceed that, Restate still scales to more nodes through more partitions.Up next: a fast, flexible, state-of-the-art log #We mentioned that we built our own implementation of a distributed replicated log (called Bifrost), because we didn’t find any of the existing logs suitable in terms of latency (single roundtrip, quorum replication with external consensus), durability (active-active with flexible quorums), flexibility (segmented log that can be dynamically reconfigured).The next post in this series will dig into all the nitty gritty details of how we built that log. This log itself is a marvel of engineering and one of the reasons Restate is as powerful as it is. If you are into distributed systems, you will enjoy that one for sure!Try it out! #We hope you find this work as exciting as we do. If you want to try this out, the quickstart, helps to get you to your first demo app (and give us a star on GitHub).To get right into the distributed fun, check this guide to running a cluster locally.Let us know what you think or ask questions in our community on Discord or Slack and get more deep content like this from us on X, LinkedIn, Bluesky, or via email.Restate is also available as a fully managed cloud service, if all you want is to use it and let us operate it.restate\nrelease\narchitecture\ndeployment",
    "summary": {
      "en": "The article discusses the development of Restate, a modern Durable Execution Engine designed without relying on traditional databases or logs. Instead, it uses a unified log architecture to simplify distributed coordination, improving performance and operational behavior.\n\nKey Points:\n\n1. **Architecture Overview**: Restate consists of two main components: the Restate Server, which manages invocation events like a message broker, and application services that handle the logic. The server maintains a connection with the services, allowing for durable actions and state modifications.\n\n2. **Distributed Deployment**: Restate operates as a cluster of nodes, where each node can send and receive events. It uses a dual mechanism for data storage: a replicated log for new events and an object store for durable data.\n\n3. **Latency and Cost Management**: Restate addresses the latency issues associated with object storage by implementing a replication layer, ensuring fast data durability while also allowing users to choose configurations based on their latency and cost needs.\n\n4. **Partitioned Scale-Out**: The system scales by dividing work into partitions, each with its log and processor. This design allows for efficient handling of events and reduces the need for synchronization across partitions.\n\n5. **Control and Data Planes**: Restate features a control plane for managing failures and coordination, enhancing efficiency by simplifying consensus mechanisms.\n\n6. **Performance Metrics**: The article provides performance data, showcasing low latency and high throughput, with Restate capable of handling thousands of actions per second.\n\n7. **Future Developments**: Future articles will detail Restate's log implementation and further optimizations.\n\nOverall, Restate aims to offer a robust, flexible execution engine that simplifies the developer experience while delivering high performance in durable function execution.",
      "ko": "이 기사는 전통적인 데이터베이스나 로그에 의존하지 않고 설계된 현대적인 내구성 실행 엔진인 Restate의 개발에 대해 다룹니다. Restate는 통합 로그 아키텍처를 사용하여 분산 조정을 간소화하고 성능과 운영 효율성을 향상시킵니다.\n\nRestate는 두 가지 주요 구성 요소로 이루어져 있습니다. 첫 번째는 메시지 브로커처럼 호출 이벤트를 관리하는 Restate 서버이며, 두 번째는 로직을 처리하는 애플리케이션 서비스입니다. 서버는 서비스와의 연결을 유지하여 내구성 있는 작업과 상태 수정을 가능하게 합니다.\n\nRestate는 노드 클러스터로 작동하며, 각 노드는 이벤트를 송수신할 수 있습니다. 데이터 저장을 위해 복제 로그와 내구성 있는 데이터를 위한 객체 저장소라는 두 가지 메커니즘을 사용합니다.\n\nRestate는 객체 저장소와 관련된 지연 문제를 해결하기 위해 복제 계층을 구현하여 빠른 데이터 내구성을 보장합니다. 또한 사용자가 지연 시간과 비용에 따라 구성 옵션을 선택할 수 있도록 합니다.\n\n시스템은 작업을 파티션으로 나누어 확장됩니다. 각 파티션은 자체 로그와 프로세서를 가지고 있어 이벤트를 효율적으로 처리하고 파티션 간 동기화 필요성을 줄입니다.\n\nRestate는 실패 관리와 조정을 위한 제어 평면을 갖추고 있어 합의 메커니즘을 간소화하여 효율성을 높입니다.\n\n기사는 Restate의 성능 데이터를 제공하며, 낮은 지연 시간과 높은 처리량을 보여줍니다. Restate는 초당 수천 개의 작업을 처리할 수 있는 능력을 가지고 있습니다.\n\n향후 기사는 Restate의 로그 구현과 추가 최적화에 대해 자세히 다룰 예정입니다. Restate는 개발자 경험을 간소화하면서 내구성 있는 기능 실행에서 높은 성능을 제공하는 강력하고 유연한 실행 엔진을 목표로 하고 있습니다.",
      "ja": null
    }
  },
  {
    "id": "6f218eb0dd5b6817",
    "title": {
      "en": "Show HN: A difficult game to test your logic",
      "ko": "논리 테스트 게임!",
      "ja": null
    },
    "type": "story",
    "url": "https://rvlabs.ca/jumping-frogs",
    "score": 75,
    "by": "CodeCrusader",
    "time": 1742824022,
    "content": "Swap the frogs! Green frogs go right, brown frogs go left. Click a frog to jump to the empty space.Level 1Reset Level",
    "summary": {
      "en": "In this game, green frogs move to the right and brown frogs move to the left. To play, click on a frog to make it jump into the empty space. You can also reset the level if needed.",
      "ko": "이 게임에서 초록색 개구리는 오른쪽으로 이동하고 갈색 개구리는 왼쪽으로 이동합니다. 게임을 하려면 개구리를 클릭하여 빈 공간으로 점프하게 하면 됩니다. 필요할 경우 레벨을 초기화할 수도 있습니다.",
      "ja": null
    }
  },
  {
    "id": "5cf58e86d5cb098d",
    "title": {
      "en": "Show HN: Xorq – open-source Python-first Pandas-style pipelines",
      "ko": "Xorq: 파이썬 데이터 혁신",
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/xorq-labs/xorq",
    "score": 13,
    "by": "secretasiandan",
    "time": 1743096451,
    "content": "xorq: Multi-engine ML pipelines made simple\n\nxorq is a deferred computational framework that brings the replicability and\nperformance of declarative pipelines to the Python ML ecosystem. It enables us\nto write pandas-style transformations that never run out of memory,\nautomatically cache intermediate results, and seamlessly move between SQL\nengines and Python UDFs—all while maintaining replicability. xorq is built on\ntop of Ibis and DataFusion.\n\nFeature\nDescription\n\nDeclarative expressions\nExpress and execute complex data processing logic via declarative functions. Define transformations as Ibis expressions so that you are not tied to a specific execution engine.\n\nMulti-engine\nCreate unified ML workflows that leverage the strengths of different data engines in a single pipeline. xorq orchestrates data movement between engines (e.g., Snowflake for initial extraction, DuckDB for transformations, and Python for ML model training).\n\nBuilt-in caching\nxorq automatically caches intermediate pipeline results, minimizing repeated work.\n\nSerializable pipelines\nAll pipeline definitions, including UDFs, are serialized to YAML, enabling version control, reproducibility, and CI/CD integration. Ensures consistent results across environments and makes it easy to track changes over time.\n\nPortable UDFs\nBuild pipelines as  UDxFs- aggregates, windows, and transformations. The DataFusion-based xorq engine provides a portable runtime for UDF execution.\n\nArrow-native architecture\nBuilt on the Apache Arrow columnar memory format and Arrow Flight transport layer, xorq achieves high-performance data transfer without cumbersome serialization overhead.\n\nGetting Started\nxorq functions as both an interactive library for building expressions and a\ncommand-line interface. This dual nature enables seamless transition\nfrom exploratory research to production-ready artifacts. The steps below will\nguide through using both the CLI and library components to get started.\nCautionThis library does not currently have a stable release. Both the\nAPI and implementation are subject to change, and future updates may not be\nbackward compatible.\n\nInstallation\nxorq is available as xorq on PyPI:\npip install xorq\n\nNoteWe are changing the name from LETSQL to xorq.\n\nUsage\n# your_pipeline.py\nimport xorq as xo\nimport xorq.expr.datatypes as dt\n\n@xo.udf.make_pandas_udf(\n    schema=xo.schema({\"title\": str, \"url\": str}),\n    return_type=dt.bool,\n    name=\"url_in_title\",\n)\ndef url_in_title(df):\n    return df.apply(\n        lambda s: (s.url or \"\") in (s.title or \"\"),\n        axis=1,\n    )\n\n# Connect to xorq's embedded engine\ncon = xo.connect()\n\n# Reference to the parquet file\nname = \"hn-data-small.parquet\"\n\nexpr = xo.deferred_read_parquet(\n    con,\n    xo.options.pins.get_path(name),\n    name,\n).mutate(**{\"url_in_title\": url_in_title.on_expr})\n\nexpr.execute().head()\n\nxorq provides a CLI that enables you to build serialized artifacts from expressions, making your pipelines reproducible and deployable:\n# Build an expression from a Python script\nxorq build your_pipeline.py -e \"expr\" --target-dir builds\n\nThis will create a build artifact directory named by its expression hash:\nbuilds\n└── fce90c2d4bb8\n   ├── abe2c934f4fe.sql\n   ├── cec2eb9706bc.sql\n   ├── deferred_reads.yaml\n   ├── expr.yaml\n   ├── metadata.json\n   ├── profiles.yaml\n   └── sql.yaml\n\nThe CLI converts Ibis expressions into serialized artifacts that capture the complete execution graph, ensuring consistent results across environments.\nMore info can be found in the tutorial Building with xorq.\nFor more examples on how to use xorq, check the\nexamples directory, note\nthat in order to run some of the scripts in there, you need to install the\nlibrary with examples extra:\npip install 'xorq[examples]'\n\nContributing\nContributions are welcome and highly appreciated. To get started, check out the contributing guidelines.\nAcknowledgements\nThis project heavily relies on Ibis and DataFusion.\nLicense\nThis repository is licensed under the Apache License",
    "summary": {
      "en": "**Summary of xorq: Multi-engine ML Pipelines Made Simple**\n\nxorq is a framework designed to simplify the creation of machine learning (ML) pipelines in Python. It allows users to build data processing workflows that are efficient and easy to replicate. Key features include:\n\n- **Declarative Expressions**: Users can define data transformations using straightforward expressions, making them independent of any specific data engine.\n  \n- **Multi-engine Support**: xorq can integrate multiple data engines (like Snowflake and DuckDB) in a single workflow, facilitating data movement between them.\n\n- **Built-in Caching**: It automatically saves intermediate results to reduce redundant computations.\n\n- **Serializable Pipelines**: All aspects of the pipeline are saved in a YAML format, ensuring they can be version controlled and reproduced reliably.\n\n- **Portable UDFs**: Users can create user-defined functions (UDFs) that work across different engines.\n\n- **High Performance**: Built on Apache Arrow, xorq ensures fast data transfer without excessive delay.\n\n**Getting Started**:\n- xorq can be installed via PyPI with the command `pip install xorq`.\n- Users can create pipelines using a simple Python script and can build these into deployable artifacts through a command-line interface (CLI).\n\n**Contributions**: The project is open for contributions, and users can refer to the guidelines for participation.\n\n**Note**: The library is still in development, and the API may change in future updates.",
      "ko": "xorq는 파이썬에서 머신러닝(ML) 파이프라인을 쉽게 만들 수 있도록 설계된 프레임워크입니다. 사용자는 효율적이고 복제하기 쉬운 데이터 처리 워크플로우를 구축할 수 있습니다. 주요 기능으로는 다음과 같은 것들이 있습니다.\n\n사용자는 간단한 표현식을 사용하여 데이터 변환을 정의할 수 있으며, 이는 특정 데이터 엔진에 의존하지 않습니다. xorq는 Snowflake나 DuckDB와 같은 여러 데이터 엔진을 하나의 워크플로우에 통합할 수 있어 데이터 이동을 용이하게 합니다. 또한, 중간 결과를 자동으로 저장하여 불필요한 계산을 줄이는 내장 캐싱 기능이 있습니다.\n\n파이프라인의 모든 요소는 YAML 형식으로 저장되어 버전 관리가 가능하고 신뢰성 있게 재현할 수 있습니다. 사용자는 다양한 엔진에서 작동하는 사용자 정의 함수(UDF)를 만들 수 있으며, Apache Arrow를 기반으로 구축되어 빠른 데이터 전송을 보장합니다.\n\nxorq는 PyPI를 통해 `pip install xorq` 명령어로 설치할 수 있습니다. 사용자는 간단한 파이썬 스크립트를 사용하여 파이프라인을 생성하고, 명령줄 인터페이스(CLI)를 통해 배포 가능한 아티팩트로 만들 수 있습니다.\n\n이 프로젝트는 기여를 받을 수 있으며, 참여를 위한 가이드라인을 참고할 수 있습니다. 현재 라이브러리는 개발 중이며, 향후 업데이트에서 API가 변경될 수 있습니다.",
      "ja": null
    }
  },
  {
    "id": "0da8cdfb63befb78",
    "title": {
      "en": "OpenAI adds MCP support to Agents SDK",
      "ko": "OpenAI, 에이전트 SDK에 MCP 지원 추가",
      "ja": null
    },
    "type": "story",
    "url": "https://openai.github.io/openai-agents-python/mcp/",
    "score": 766,
    "by": "gronky_",
    "time": 1743015329,
    "content": "Model context protocol (MCP)\nThe Model context protocol (aka MCP) is a way to provide tools and context to the LLM. From the MCP docs:\n\nMCP is an open protocol that standardizes how applications provide context to LLMs. Think of MCP like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect your devices to various peripherals and accessories, MCP provides a standardized way to connect AI models to different data sources and tools.\n\nThe Agents SDK has support for MCP. This enables you to use a wide range of MCP servers to provide tools to your Agents.\nMCP servers\nCurrently, the MCP spec defines two kinds of servers, based on the transport mechanism they use:\n\nstdio servers run as a subprocess of your application. You can think of them as running \"locally\".\nHTTP over SSE servers run remotely. You connect to them via a URL.\n\nYou can use the MCPServerStdio and MCPServerSse classes to connect to these servers.\nFor example, this is how you'd use the official MCP filesystem server.\nasync with MCPServerStdio(\n    params={\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", samples_dir],\n    }\n) as server:\n    tools = await server.list_tools()\n\nUsing MCP servers\nMCP servers can be added to Agents. The Agents SDK will call list_tools() on the MCP servers each time the Agent is run. This makes the LLM aware of the MCP server's tools. When the LLM calls a tool from an MCP server, the SDK calls call_tool() on that server.\nagent=Agent(\n    name=\"Assistant\",\n    instructions=\"Use the tools to achieve the task\",\n    mcp_servers=[mcp_server_1, mcp_server_2]\n)\n\nCaching\nEvery time an Agent runs, it calls list_tools() on the MCP server. This can be a latency hit, especially if the server is a remote server. To automatically cache the list of tools, you can pass cache_tools_list=True to both MCPServerStdio and MCPServerSse. You should only do this if you're certain the tool list will not change.\nIf you want to invalidate the cache, you can call invalidate_tools_cache() on the servers.\nEnd-to-end examples\nView complete working examples at examples/mcp.\nTracing\nTracing automatically captures MCP operations, including:\n\nCalls to the MCP server to list tools\nMCP-related info on function calls",
    "summary": {
      "en": "**Summary of the Model Context Protocol (MCP)**\n\nThe Model Context Protocol (MCP) is a standardized way to connect AI models to various tools and data sources, similar to how a USB-C port connects devices. \n\nKey Points:\n- MCP allows applications to provide context to Large Language Models (LLMs).\n- There are two types of MCP servers: \n  - **stdio servers**: run locally as part of your application.\n  - **HTTP over SSE servers**: run remotely and are accessed via a URL.\n- The Agents SDK supports MCP, enabling the use of multiple MCP servers with agents.\n- When an agent runs, it checks for available tools on the MCP servers, ensuring the LLM knows what tools are available.\n- To improve performance, you can cache the list of tools to reduce delays, but this should only be done if you are sure the tool list won’t change.\n- There are functions available to invalidate the cache if needed.\n- Examples of using MCP can be found in the examples/mcp directory.\n- MCP also includes tracing features that track operations related to server calls and function calls.",
      "ko": "모델 컨텍스트 프로토콜(MCP)은 인공지능 모델을 다양한 도구와 데이터 소스에 연결하는 표준화된 방법입니다. 이는 USB-C 포트가 장치를 연결하는 방식과 유사합니다.\n\nMCP의 주요 내용은 다음과 같습니다. MCP는 애플리케이션이 대형 언어 모델(LLM)에 컨텍스트를 제공할 수 있도록 합니다. MCP 서버는 두 가지 유형이 있습니다. 첫 번째는 애플리케이션의 일부로 로컬에서 실행되는 표준 입출력 서버(stdio 서버)입니다. 두 번째는 원격에서 실행되며 URL을 통해 접근하는 HTTP over SSE 서버입니다. 에이전트 SDK는 MCP를 지원하여 여러 MCP 서버를 에이전트와 함께 사용할 수 있게 합니다.\n\n에이전트가 실행될 때, 사용 가능한 도구를 MCP 서버에서 확인하여 LLM이 어떤 도구를 사용할 수 있는지 알 수 있도록 합니다. 성능을 향상시키기 위해 도구 목록을 캐시하여 지연 시간을 줄일 수 있지만, 도구 목록이 변경되지 않을 것이라는 확신이 있을 때만 이 작업을 수행해야 합니다. 필요할 경우 캐시를 무효화할 수 있는 기능도 제공됩니다. MCP 사용 예시는 examples/mcp 디렉토리에서 확인할 수 있습니다. 또한 MCP는 서버 호출 및 함수 호출과 관련된 작업을 추적하는 기능도 포함하고 있습니다.",
      "ja": null
    }
  },
  {
    "id": "9d30127327950d75",
    "title": {
      "en": "Virginia will punish fast drivers with devices that limit their speed",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.washingtonpost.com/dc-md-va/2025/03/27/virginia-speed-limit-device/",
    "score": 8,
    "by": "reaperducer",
    "time": 1743108226,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "677a6df836887470",
    "title": {
      "en": "What happens to DNA data of millions as 23andMe files bankruptcy?",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.pbs.org/newshour/show/what-happens-to-dna-data-of-millions-as-23andme-files-bankruptcy",
    "score": 12,
    "by": "ajdude",
    "time": 1743104372,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "93438806283e2660",
    "title": {
      "en": "Meta debuts Friends tab, Mark Zuckerberg pushes 'throwback to OG Facebook'",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.cnbc.com/2025/03/27/meta-debuts-friends-tab-as-zuckerberg-pushes-throwback-to-og-facebook.html",
    "score": 7,
    "by": "koolba",
    "time": 1743109312,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "6d5b72b71a45cde9",
    "title": {
      "en": "California bill aims to phase out harmful ultra-processed foods in schools",
      "ko": "캘리포니아, 학교 가공식품 퇴출!",
      "ja": null
    },
    "type": "story",
    "url": "https://www.thenewlede.org/2025/03/california-bill-aims-to-phase-out-harmful-ultra-processed-foods-in-schools/",
    "score": 119,
    "by": "PaulHoule",
    "time": 1743097530,
    "content": "March 19 2025\n\n\t\t\tCalifornia bill aims to phase out harmful ultra-processed school foods\n\n\t\t\t\t\t\t\t\t\t\t\tShannon Kelleher\n\n        0\n\n    Shares\n\n                                        As states across the country move to ban food dyes, California lawmakers on March 19 introduced the first US bill that would phase out certain ultra-processed foods from school meals.\nIf signed into law, Assembly Bill 1264 would establish the first statutory definition of what qualifies as an ultra-processed food and would direct state scientists to work with university experts to identify particularly harmful products, which would then be removed from school cafeterias by 2032.\n“We have not done enough to protect [children] from ultra-processed foods and beverages that have far more in common with a cigarette than they do a fruit or vegetable,” Ashley Gearhardt, a professor of psychology at the University of Michigan and director of the school’s Food Addiction Science & Treatment Lab, said on a March 19 press call.\nAB 1264 is a “courageous step forward” towards treating ultra-processed foods like the serious health threats they are, said Gearhardt.\nTo identify which ultra-processed foods should be eliminated from school offerings, scientists will consider whether a product includes additives that are banned elsewhere, whether it has been linked to health harms, whether it has been show to contribute to food addiction, and whether it contains excessive fat, sugar or salt, California Assemblymember Jesse Gabriel, one of the lawmakers who introduced the bill, said on the call.\nThe scientists will be required to publish a first report outlining this subcategory of especially harmful ultra-processed foods by July 1, 2026, said Gabriel, and will be required to update the list every two years as research on these foods evolves.\nThe bill was embraced by both Democrat and Republican lawmakers, added Gabriel. “Protecting our kids from harm is, and always should be, a bipartisan issue,” he said\nIn 2023, Gabriel introduced the California Food Safety Act or so-called “Skittles bill”, which is set to ban foods in the state containing brominated vegetable oil, Red Dye No. 3 and other toxic chemicals beginning in 2027. The bill was followed by the California School Food Safety Act last year, which will ban schools from serving foods with six artificial dyes linked to neurobehavioral problems in children.\nThe bills, signed into law by Governor Gavin Newsom, have been followed by a wave of recent legislation across the country. West Virginia this month advanced a sweeping bipartisan bill that would ban a range of food dyes linked to health problems, while New York lawmakers have proposed a law that would ban seven food dyes from public schools and eliminate Red Dye 3 and two other food additives statewide.\nUltra-processed foods, which have typically undergone many industrial changes such as the addition of preservatives, sweeteners, and artificial flavors, have been linked to 32 health conditions, according to a 2024 review in the British Medical Journal (BMJ), including heart disease, diabetes, mental health disorders and obesity. About one in five US children between the ages of two and 19 are obese. Eating ultra-processed foods has also been associated with attention-deficit/hyperactivity disorder (ADHD) in children.\nSome industry players have pushed back against the growing negative perception of ultra-processed foods, with General Mills reportedly arguing in a letter to the Department of Health and Human Services last year that “Not all processed foods are nutritionally equivalent and do not have the same impact on health.”\nProcessing is “part of a complex food system that helps consumers meet nutritional needs within their abilities, budget and preferences,” wrote General Mills.\nCountries around the world have banned ingredients with concerning health effects that are still commonly used in US foods, including titanium dioxide, brominated vegetable oil and artificial food dyes.\n“We hope that this bill will inspire food companies to tweak their recipes and remove unnecessary harmful additives so that they can continue to access this enormous [California] market,” said Gabriel.\n(Featured image by Gaining Visuals on Unsplash.)",
    "summary": {
      "en": "On March 19, 2025, California lawmakers introduced Assembly Bill 1264, aimed at phasing out harmful ultra-processed foods from school meals. If passed, this bill would define ultra-processed foods and require scientists to identify particularly harmful items, with plans to remove them from schools by 2032.\n\nExperts, including Ashley Gearhardt from the University of Michigan, support the bill, emphasizing the health risks of ultra-processed foods. The criteria for elimination will include the presence of banned additives, health risks, potential for food addiction, and high levels of fat, sugar, or salt. A report on harmful foods is expected by July 1, 2026, with updates every two years.\n\nThe bill has bipartisan support, reflecting a shared commitment to protecting children's health. It follows previous laws targeting harmful food additives and is part of a broader national movement to regulate food dyes and unhealthy ingredients in schools. Ultra-processed foods have been linked to various health issues, including obesity and ADHD, prompting calls for change in the food industry. \n\nLawmakers hope this bill will encourage food companies to improve their recipes to remain competitive in California's market.",
      "ko": "2025년 3월 19일, 캘리포니아 주 의원들은 학교 급식에서 해로운 초가공식품을 단계적으로 없애기 위한 법안인 제정법 1264를 발의했습니다. 이 법안이 통과되면 초가공식품의 정의가 내려지고, 과학자들이 특히 해로운 식품을 식별할 의무가 생기며, 2032년까지 이를 학교에서 제거할 계획입니다.\n\n미시간 대학교의 애슐리 기어하르트 등 전문가들은 초가공식품의 건강 위험성을 강조하며 이 법안을 지지하고 있습니다. 제거 기준에는 금지된 첨가물의 존재, 건강 위험, 식품 중독 가능성, 그리고 높은 지방, 설탕 또는 소금 함량이 포함됩니다. 해로운 식품에 대한 보고서는 2026년 7월 1일까지 발표될 예정이며, 이후 매 2년마다 업데이트될 것입니다.\n\n이 법안은 양당의 지지를 받고 있으며, 아동 건강 보호에 대한 공동의 노력을 반영하고 있습니다. 이는 해로운 식품 첨가물에 대한 이전 법률을 따르며, 학교에서 식품 색소와 건강에 좋지 않은 성분을 규제하려는 더 넓은 국가적 움직임의 일환입니다. 초가공식품은 비만과 ADHD 등 다양한 건강 문제와 연관되어 있어, 식품 산업의 변화를 촉구하는 목소리가 커지고 있습니다.\n\n법안 발의자들은 이 법안이 식품 회사들이 경쟁력을 유지하기 위해 레시피를 개선하도록 유도할 것으로 기대하고 있습니다.",
      "ja": null
    }
  },
  {
    "id": "b4913c0a45c1dfa9",
    "title": {
      "en": "Debian bookworm live images now reproducible",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://lwn.net/Articles/1015402/",
    "score": 721,
    "by": "bertman",
    "time": 1743009742,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "03d4352df9cb9f9f",
    "title": {
      "en": "Abundance Isn't Going to Happen Unless Politicians Are Scared of the Status Quo",
      "ko": "정치인들이 두려워해야 변화가 온다",
      "ja": null
    },
    "type": "story",
    "url": "https://inpractice.yimbyaction.org/p/abundance-isnt-going-to-happen-unless",
    "score": 182,
    "by": "viajante1882",
    "time": 1743095353,
    "content": "Share this postIn PracticeAbundance Isn’t Going To Happen Unless Politicians Are Scared of the Status QuoCopy linkFacebookEmailNotesMoreDiscover more from In PracticePractical politics for housing reform. SubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.Already have an account? Sign inAbundance Isn’t Going To Happen Unless Politicians Are Scared of the Status QuoIt’s A Race Between Building Up and Burning DownLaura FooteMar 28, 20257Share this postIn PracticeAbundance Isn’t Going To Happen Unless Politicians Are Scared of the Status QuoCopy linkFacebookEmailNotesMore1Share“Too many goods created a bad.”That’s how California Governor Jerry Brown put it in 2017 when he signed a package of 15 housing-related bills, the first YIMBY-supported pro-housing bills. He was explaining, in his meandering way, how the housing shortage was the result of hundreds of well-intentioned laws, which had promised all kinds of things, but mostly succeeded in slowing down the production of new housing.In many ways, Jerry was ahead of the curve. He was lamenting “everything bagel liberalism” and “process over outcomes” before it was cool.But at the same time, there was a distinct lack of fire under the Governor’s ass. He was skeptical that this new legislation would do much to unwind the tangled legal web holding back housing production. And while we YIMBYs were bright eyed and bushy tailed, seeing as it was the first state bill signing we’d been invited to, it was also kind of depressing.Less than ten years later, the discourse is suddenly all about the seemingly simple idea that we should “do things and build things,” and there is finally a sense of urgency. I’m heading to Atlanta this week to interview Derek Thompson about his new book Abundance with Ezra Klein at the Georgia Center for the Book (tickets). Their thesis is simple: we need more of everything. More housing, more immigrants, more clean energy infrastructure.And Klein and Thomson’s book is just the latest entry into the growing genre that attempts to address the root causes of stagnation in America and prescribe a path towards abundance and renewal. Recent books in this genre include:Why Nothing Works by Marc J. DunkelmanStuck by Yoni Applebaum,Recoding America by Jennifer Pahlka,Meanwhile Ned Resnikoff is pointing out that YIMBYs were doing abundance before it was cool (or maybe were the driving force in making it cool). While I appreciate the credit to the original YIMBY brand, I’m choosing to adopt an abundance mindset about abundance. More is more, after all.SubscribeAs I see it, the Abundance concept isn’t quite an ideology, it’s more of a refocusing on outcomes. It’s a framework that points to tangible outcomes and asks us to tactically identify what is blocking progress to that goal, irrespective of what the intention of that blockage might be. It extends the classic YIMBY way of doing politics into a larger philosophy. It is a re-focusing on “ends” over “means,” and allows for a variety of ideologies to come together on specific ends. The language around “outcomes focused legislating” brings some degree of sanity to our often self-sabotaging process.Outcome-based politics seems so obvious that it can sound silly to say it out loud. Government should deliver material outcomes. Elected officials should be extremely motivated to produce tangible outcomes for the largest number of people.So why isn’t that happening?What’s eating state capacity?The hot topic in the growing abundance discourse is state capacity. That’s a fancy term for a simple concept — the government’s ability to deliver outcomes, whether those are growing the economy, establishing laws, or just picking up the garbage.It’s not a new idea. In fact, five decades ago, some of the world’s most pre-eminent political scientists fretted about the erosion of state capacity in democracies across the world. “The demands on democratic government grow,” wrote Michael Crozier, Samuel Huntington, and Joji Watanuki, “while the capacity of democratic government stagnates.”Sounds familiar, doesn’t it? They argued that in countries like the United States, Japan, and Western Europe, citizens were asking their governments to take on more and more without increasing their government’s ability to carry out those projects. That led to an erosion in legitimacy, and decades of retrenchment and cutbacks. About the only thing that the left and the right seemed to agree on was that the “era of big government is over.”The 70’s was an era of community organizing to stop The Big Bad Thing, and this urge to tap the breaks continues to this day. Elaborate outreach processes, reporting requirements and the opportunities for objections about those reports… they all can seem reasonable. But added together they created a mighty web. Marc Dunkelman explained the problem of this approach on a recent episode of the Political Gabfest: “We've now created a system where there are so many veto players and you need so many approvals that government fundamentally doesn't work.”Little by little, we stitched together a gigantic wet blanket that continues to hold back housing production. It’s easy to blame liberals, but like all of our worst problems, it was bipartisan. Conservatives liked constraining government because they didn’t trust it. Liberals liked constraining it because they overestimated how much those constraints would produce better outcomes like protecting the marginalized, preserving the environment, and elevating the voices of the community. And people who felt that integration was a threat, helped create innumerable local processes to tap the brakes on all kinds of public goods and housing.As a society, we have chronically underestimated the cost of all this. It’s a chronic case of letting the perfect be the enemy of the good. Every constraint makes the continuation of the status quo more likely. Legislators and advocates know this, but treat it like the normal cost of getting the sausage made. In the process of getting pro-housing bills passed, I’ve had countless arguments with other advocates and legislators over the innumerable ways bills are weakened. And while we’re taking steps in the right direction, the outcomes still aren’t great.State capacity is being eaten by excessive process. Overly restrictive rules (zoning) and elaborate process (permitting and planning) create chronic shortages driving prices higher and creating an angry populace. That’s the TLDR thesis of a lot of the abundance books. Things grind to a halt. People suffer. It all feels incredibly self-sabotaging and frustrating.Two paths forwardThat brings us to today, in which people across the country are mad that shit sucks. I could say more but, come on, shit sucks. Prices are high, infrastructure is crumbling, people are pissed, you’ve heard this already.There are two big responses happening right now:Tear down (DOGE vibes)Build up (Abundance vibes)The DOGE point of view says if the government can’t do anything, we should just get rid of the government. That appeals to many Americans because they are angry. The frustrated urge to blow it all up is strong. As a rule, shortages do not bring out the best in humanity. They make us blame perceived-outsiders and foster the urge to topple governments.The housing shortage fosters a “crabs in a bucket” mentality everywhere, from Blue-dot cities to Red rural communities. Whether your enemy is yuppies, coastal elites or immigrants, the through-line is that there isn’t enough to go around and someone is stealing from us. The DOGE-style of governance is about trying to tell everyone who has been stealing and publicly firing those people.The alternative gaining traction is a (sometimes vague) notion of abundance, which boils down to “things should work.” But if these new books are any indication, the Build Up team is feeling more urgency. The consensus that government is not delivering tangible good outcomes for average people is finally being recognized as an existential threat to the democratic project. The constituency for “can we please just fucking do things” is real.YIMBY has a practical goal of housing abundance. And for years, YIMBYs have been building ideologically diverse coalitions aligned on that specific, narrow goal.And while that work is great, my key point is that we are running out of time. “Get your house in order” should have a deep level of urgency right now. Elected officials at the state and local level need to rebuild the belief that government is worth preserving and can deliver a thriving middle class.We are in the middle of a race between the destroyers and the builders, and too many elected officials are twiddling their thumbs the sinking ship of the status quo.Who needs to change?People will nod and agree to everything I just said above, but what does it actually look like in practice?Literally yesterday I spoke with a city council member who was thinking about introducing single stair reform in their city. He knew how it could be a deeply impactful reform and is completely safe. But then he said “We can’t do it without the support of the firefighters union, and they’re deeply opposed.” Every redundant requirement was deliberately put there by someone who doesn’t think it needs reform and will fight it. Most elected officials weigh the various highly engaged stakeholders, as if they represent the average voters in their district. They’re not.YIMBY Action, through our local chapter model, is building a visible constituency to incentivize politicians to take bolder action on housing. But politicians need to get ahead of this. To critique my own work: it shouldn’t be necessary! Elected officials should be more concerned that the general public is feeling economically stunted and enraged!Incumbents should be more terrified of not doing things. The status quo is a downward trajectory and you will be punished electorally for maintaining it. If people continue to feel economically stunted, they will continue to boot incumbents. Being committed to outcomes requires continuous deep commitment to pushing back, with the knowledge that outcomes add up to that important “right track / wrong track” polling data.Share7 Likes∙1 Restack7Share this postIn PracticeAbundance Isn’t Going To Happen Unless Politicians Are Scared of the Status QuoCopy linkFacebookEmailNotesMore1Share",
    "summary": {
      "en": "The article discusses the urgent need for housing reform and the challenges faced in achieving abundance in various sectors, particularly housing. It highlights that California's past legislation aimed at addressing housing shortages has not significantly improved production due to excessive regulations and processes.\n\nThe author emphasizes the importance of an \"abundance mindset,\" which focuses on tangible outcomes rather than just intentions. This approach advocates for a government that effectively delivers results for its citizens, contrasting it with the current situation where bureaucratic hurdles hinder progress.\n\nThe text also explores two prevailing attitudes towards government action: one that seeks to tear down existing structures in frustration (the \"DOGE\" perspective) and another that promotes building up and improving systems (the \"Abundance\" perspective). The author argues that there is a growing urgency for politicians to act decisively to address citizens' frustrations with issues like high prices and poor infrastructure.\n\nUltimately, the article calls for elected officials to prioritize effective governance and take bold actions to foster a thriving middle class, warning that failing to do so could lead to electoral consequences. The emphasis is on creating a visible constituency to encourage politicians to pursue necessary reforms in housing and beyond.",
      "ko": "이 글은 주택 개혁의 긴급한 필요성과 다양한 분야, 특히 주택에서 풍요를 이루는 데 직면한 도전 과제를 다루고 있습니다. 캘리포니아의 과거 법안들이 주택 부족 문제를 해결하기 위해 시행되었지만, 과도한 규제와 절차로 인해 생산이 크게 개선되지 않았다는 점을 강조합니다.\n\n저자는 \"풍요의 사고방식\"의 중요성을 강조하며, 이는 단순한 의도보다는 실질적인 결과에 초점을 맞추고 있습니다. 이 접근법은 시민들에게 효과적으로 결과를 제공하는 정부를 지지하며, 현재의 관료적 장애물이 진전을 방해하는 상황과 대조됩니다.\n\n또한, 정부의 행동에 대한 두 가지 주요 태도를 살펴봅니다. 하나는 기존 구조를 무너뜨리려는 불만의 시각(\"DOGE\" 관점)이고, 다른 하나는 시스템을 구축하고 개선하려는 시각(\"풍요\" 관점)입니다. 저자는 정치인들이 높은 가격과 열악한 인프라와 같은 문제에 대해 시민들의 불만을 해결하기 위해 단호하게 행동할 필요성이 커지고 있다고 주장합니다.\n\n결국, 이 글은 선출된 공직자들이 효과적인 거버넌스를 우선시하고 중산층을 육성하기 위한 대담한 조치를 취할 것을 촉구합니다. 이를 소홀히 할 경우 선거에서의 결과에 부정적인 영향을 미칠 수 있다고 경고합니다. 정치인들이 주택 문제와 그 외의 분야에서 필요한 개혁을 추진하도록 유도하기 위해 가시적인 지지 기반을 만드는 것이 중요하다는 점이 강조됩니다.",
      "ja": null
    }
  },
  {
    "id": "02e9dd72d825cbca",
    "title": {
      "en": "The devastating truth about the war on education – Raw Story",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.rawstory.com/raw-investigates/gop-education/",
    "score": 3,
    "by": "rbanffy",
    "time": 1743110932,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "f5897dc4e18d0cf0",
    "title": {
      "en": "War story: the hardest bug I ever debugged",
      "ko": "전쟁의 디버깅",
      "ja": null
    },
    "type": "story",
    "url": "https://www.clientserver.dev/p/war-story-the-hardest-bug-i-ever",
    "score": 417,
    "by": "jakevoytko",
    "time": 1742827066,
    "content": "Share this postClient/ServerWar story: the hardest bug I ever debuggedCopy linkFacebookEmailNotesMoreDiscover more from Client/ServerA staff software engineer's view on current events, tech trends, and the occasional rant.SubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.Already have an account? Sign inWar story: the hardest bug I ever debuggedAll of a sudden, without any ostensible cause, Google Docs was flooded with errors. How it took me 2 days and a coworker to solve the hardest bug I ever debugged.Jacob VoytkoMar 24, 202513Share this postClient/ServerWar story: the hardest bug I ever debuggedCopy linkFacebookEmailNotesMore1ShareWhen I was on the Google Docs team, we did a weekly bug triage where we’d look for new issues and randomly assign them to teammates to investigate. One week, we had a new top error by a wide margin.It was a fatal error. This means that it prevented the user from editing without reloading. It didn’t correspond to a Google Docs release. The stack trace added very little information. There wasn’t an associated spike in user complaints, so we weren’t even sure it was really happening — but if it was happening it would be really bad. It was Chrome-only starting at a specific release. This is less helpful than it sounds, since we often wrote browser-specific Docs bugs that affected only one of Internet Explorer, Firefox, Safari, and Chrome. I tried to repro in dev. This was important for 2 reasons:Rule out Closure Compiler, Docs’ JavaScript compiler at the time.Debugging in unoptimized code is always easier than the alternative.Okay, how do I begin? I crawled through our logs for internal users who had suffered from the problem. I hoped that somebody could tell me, “oh yeah, every time I try to do $foo it breaks.” But no internal users had been affected. Back to the drawing board.I did a bunch of wild edits for a while. I added as many esoteric features as I could, copy/pasted a bunch of stuff into Docs from news websites to try to trigger the issue, played around with tables for a while. No dice.What next? At the time, Docs had a basic scripting tool that could perform repetative actions. It was mostly useful for performance benchmarking, but because it provided consistent behavior I tried it out. I made a 50-page doc filled with lorem ipsum and had the script bold and unbold the entire document 100 times. Somewhere around the 20th cycle it crashed. I checked my console and it was the error in question!I do it a few more times. It’s not always the 20th iteration, but it usually happens sometime between the 10th and 40th iteration. Sometimes it never happend. Okay, the bug is nondeterministic. We’re off to a bad start.I think about the repro case. Is there anything interesting about bolding and unbolding large bodies of text? Yes actually. In many fonts and for many text samples, bolded text is wider than unbolded text. This was true for the font I was using. So it could have something do with wrapping lots of lines of text.I set a breakpoint and started investigating. The crash looked like it was caused by some bad bookkeeping in the view, because the actual crash was reading a garbage cache value and trying to operate on it, and crashing as a result.At the time1, Google Docs didn’t produce an HTML page as you might expect. It absolutely positioned every single thing on screen below the menu. To power this, Docs had a full layout engine that ran on every single keystroke. To make this even remotely performant within a browser in the 2010s, everything in the view was cached to within an inch of its life.What does this mean? The place that crashed was downstream of the error. The error happened. Then there were some operations on the error. Then some accumulators accepted the wrong value. Then they were written to a cache. Eventually, some time later, there were enough bookkeeping errors that it was causing a crash.If you consider everything together, this is a worst-case scenario. The bug is nondeterministic. What could even be causing that? Subpixel rendering errors that don’t deterministically accumulate? Kill me now. Next, the reproduction was slow. It took probably 20 seconds just to load the dev version of the editor, and another 40 seconds to trigger the issue. Then I needed to examine the state until I found something that was wrong, and then figure out how to set a breakpoint at the moment that wrong thing was cached or added to a queue.If you haven’t debugged a lot of mystery problems, this is not what you want. Normally, you’d want to debug a mystery problem by binary searching.Is the problem on the client or the server? Client.Is the problem in the model or the view? ViewIs the problem in the UI rendering or the generic view processing? Generic view processing.And so on until you find where the bug is happening.And then you keep halving what could be causing the problem until it’s clearly happening within some component, and then the bug’s days are numbered.I had something else going against me. Until this point, I had mostly worked in the server, model, and network code. I was far from an expert in the view, which was the most complicated part of the application at the time. So I called over a coworker who had implemented a bunch of view features. It’s been 12 years so I don’t fully remember, but our conversation went something like this:Me: “I’m investigating that view crash that is suddenly our top bug”Him: ”Do you have a specific question about the view? I have a lot on my plate right now.”Me: ::showed him the repro and what I had found so far::Him: “I’ll clear my schedule.”And there we sat, slowly bumping our breakpoints further and further back for 2 whole days, getting closer and closer to the cause.After about a day and a half we had a breakthrough: the culprit was in a specific block of bookkeeping code2. It was in a part of the code where an accumulator value was updated. So as we had done dozens of times, we updated our breakpoints to trigger a little earlier. We reload the document and execute the repro steps. Eventually, the breakpoint is triggered. We stare at the values of the variables in the function. It must be happening right now. Something is wrong.The math isn’t adding up. My coworker adds a few logging statements and we reload and run the reproduction case again. This still doesn’t make sense.I point to the Math.abs() call in the middle of the function. “Can we log the output value of this Math.abs() call?” We debate whether it’s worth the time, but he admitted that if it were somehow returning negative values it would actually explain the math.We rerun the repro. We look at the logged value. Math.abs() is returning negative values for negative inputs. We reload and run it again. Math.abs() is returning negative values for negative inputs. We reload and run it again. Math.abs() is returning negative values for negative inputs.We start spitballing about why this might be true. We checked to see if the function had been overridden. The function was still the builtin. We stare at every single character of this function. Everything looks fine.Then we called in our Tech Lead / Manager, who had a reputation of being a human JavaScript compiler. We explained how we got here, that Math.abs() is returning negative values, and whether she could find anything that we were doing wrong. After persuading her that we weren’t somehow horribly mistaken, she sat down and looked at the code. Her CPU spun up to 100%, and she was muttering in Russian about parse trees or something while staring at the code and typing into the debug console. Finally she leaned back and declared that Math.abs() was definitely returning negative values for negative inputs.And now, one of the major advantages of working at Google: backchanneling! I reached out to a Chrome contact to ask how I should even go about figuring out who to ask. They gave me some annoying “technically this is a V8 issue, not a Chrome issue.” I jump a link in the chain and either file a V8 bug or ask someone on the V8 team. I legitimately do not remember which. The V8 team immediately points me to a bug in their bug tracker that they already had in the Fixed state.So what happened? Apparently V8 had refactored their optimization passes recently. To the best of my memory, this was the problem:V8 had two levels of optimization.A basic level used by most code. The compilation pass was extremely fast but not very optimized.A super-optimized level for hot paths. To get an idea of what constitutes a hot path, create a 50 page Google Doc and bold and unbold it 20 times, and imagine how many times a function operating multiple times per word needs to run.When doing the refactoring, they needed to provide new implementations for every opcode. Someone accidentally turned Math.abs() into the identity function for the super-optimized level.But nobody noticed because it almost never ran — and was right half of the time when it did.Satisfied that we had gotten to the root of the problem, we added a temporary browser check for the specific Chrome version. If it was that version of Chrome, it would just perform a manual if statement inline that would do the operation. We also added an extremely long comment — with citations — explaining that math.Abs() could return negative values in this specific Chrome version because of a V8 release with a regression, and to please delete it when our usage of that Chrome version dropped low enough.There you have it: 2 days to find an issue that was already fixed and would have been resolved with no interaction. What can I even do from here as the newsletter author? Normally I like finding a teachable lesson. But it was 2 days of grueling debugging and somehow there aren’t any teachable lessons there. Ah well, that’s life. And isn’t that the ultimate lesson?Thanks for reading Client/Server! Subscribe for free to receive new posts and support my work.Subscribe1They switched everything to canvas rendering many years after I left. In theory this would have allowed them to dramatically simplify the layout code, and nothing I am writing here would still apply.2To the best I can recall 12 years later, our view broke down rendering passes into “Tasks”, and Tasks could either “steal” or “give away” extra characters to adjacent rendering tasks. I don’t remember why this was useful. The code in question was executed for every Task and calculated the current tally of how many characters had been stolen or given away.Subscribe to Client/ServerBy Jacob Voytko · Launched 4 months agoA staff software engineer's view on current events, tech trends, and the occasional rant.SubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.13 Likes13Share this postClient/ServerWar story: the hardest bug I ever debuggedCopy linkFacebookEmailNotesMore1Share",
    "summary": {
      "en": "The author shares a challenging experience while working on Google Docs, where they encountered a severe bug that caused crashes without a clear cause. This bug, which appeared in a specific version of Chrome, was particularly difficult to debug because it was non-deterministic, meaning it didn’t happen consistently.\n\nInitially, the author investigated logs and tried to replicate the bug through various methods but had no luck. They eventually created a large document and used a script to bold and unbold text, which helped them reproduce the crash. After extensive debugging alongside a coworker over two days, they discovered the issue stemmed from a problem in the bookkeeping code that involved a faulty implementation of the `Math.abs()` function, which incorrectly returned negative values.\n\nThe root cause was traced back to a recent optimization in the V8 engine (Chrome's JavaScript engine), where a mistake in the code had transformed `Math.abs()` into an identity function, only manifesting under specific conditions. The team implemented a temporary fix while documenting the issue for future reference. Ultimately, the experience highlighted the unpredictable nature of debugging and the complexities involved in software development, with no clear moral or lesson to be gleaned.",
      "ko": "저자는 Google Docs에서 작업하는 동안 겪은 어려운 경험을 공유합니다. 그들은 특정 버전의 Chrome에서 발생한 심각한 버그를 만났고, 이 버그는 명확한 원인 없이 프로그램이 중단되는 문제를 일으켰습니다. 이 버그는 비결정적이어서 항상 발생하지 않아 디버깅이 특히 어려웠습니다.\n\n처음에 저자는 로그를 조사하고 다양한 방법으로 버그를 재현하려고 했지만 성공하지 못했습니다. 결국, 그들은 큰 문서를 만들고 텍스트를 굵게 했다가 다시 원래대로 돌리는 스크립트를 사용하여 중단 현상을 재현할 수 있었습니다. 이틀 동안 동료와 함께 광범위한 디버깅을 진행한 끝에, 문제는 `Math.abs()` 함수의 잘못된 구현으로 인해 발생한 회계 코드의 문제에서 비롯된 것을 발견했습니다. 이 함수는 잘못된 값을 반환하고 있었습니다.\n\n문제의 근본 원인은 V8 엔진(Chrome의 자바스크립트 엔진)에서 최근에 이루어진 최적화로 거슬러 올라갔습니다. 코드의 실수로 인해 `Math.abs()` 함수가 정체성 함수로 변형되었고, 이는 특정 조건에서만 나타났습니다. 팀은 문제를 문서화하면서 임시 해결책을 적용했습니다. 결국, 이 경험은 디버깅의 예측 불가능한 성격과 소프트웨어 개발의 복잡성을 강조했으며, 뚜렷한 교훈이나 도덕적 메시지는 없었습니다.",
      "ja": null
    }
  },
  {
    "id": "119fb7c37a4ba291",
    "title": {
      "en": "21-year old dev destroys LeetCode, gets kicked out of school",
      "ko": "21세 개발자, 리트코드 정복 후 퇴학!",
      "ja": null
    },
    "type": "story",
    "url": "https://www.youtube.com/watch?v=AwZ8PtoqCeU",
    "score": 9,
    "by": "amrrs",
    "time": 1743108721,
    "content": "Back\n\n  KR\n\n    Skip navigation\n\n        Search\n\n  Search with your voice\n\nSign in\n\n  KR\n\n{\"@context\":\"https://schema.org\",\"@type\":\"VideoObject\",\"description\":\"Try Brilliant free for 30 days https://brilliant.org/fireship You’ll also get 20% off an annual premium subscription.\\n\\nA 21-year old student at Columbia University got into trouble to developing an app that helps people cheat on the software engineering technical interview. Learn what this means for the future of LeetCode and its impact on the interview process for programmers.\\n\\n#programming #ai #thecodereport \\n\\n💬 Chat with Me on Discord\\n\\nhttps://discord.gg/fireship\\n\\n🔗 Resources\\n\\nKicked out of Columbia https://x.com/im_roy_lee/status/1905063484783472859\\nTechnical Interview Cheating  https://youtu.be/Lf883rNZjSE?si=2reXrbNplKqcHst3\\nWhat is Vibe Coding? https://youtu.be/Tw18-4U7mts\\n\\n🔥 Get More Content - Upgrade to PRO\\n\\nUpgrade at https://fireship.io/pro\\nUse code YT25 for 25% off PRO access \\n\\n🎨 My Editor Settings\\n\\n- Atom One Dark \\n- vscode-icons\\n- Fira Code Font\\n\\n🔖 Topics Covered\\n\\n- What is LeetCode?\\n- Why do people hate LeetCode?\\n- What makes the technical interview so hard?\\n- How do I get a job with a FAANG company?\\n- How do people cheat on technical interviews?\",\"duration\":\"PT235S\",\"embedUrl\":\"https://www.youtube.com/embed/AwZ8PtoqCeU\",\"name\":\"21-year old dev destroys LeetCode, gets kicked out of school...\",\"thumbnailUrl\":[\"https://i.ytimg.com/vi/AwZ8PtoqCeU/maxresdefault.jpg\"],\"uploadDate\":\"2025-03-27T11:47:45-07:00\",\"@id\":\"https://www.youtube.com/watch?v=AwZ8PtoqCeU\",\"interactionStatistic\":{\"@type\":\"InteractionCounter\",\"interactionType\":\"https://schema.org/WatchAction\",\"userInteractionCount\":\"248707\"},\"genre\":\"Science & Technology\",\"author\":\"Fireship\"}\n\n21-year old dev destroys LeetCode, gets kicked out of school...SearchWatch laterShareCopy linkInfoShoppingTap to unmute2xIf playback doesn't begin shortly, try restarting your device.Includes paid promotion•Up nextLiveUpcomingCancelPlay NowYou're signed outVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.CancelConfirmShareInclude playlistAn error occurred while retrieving sharing information. Please try again later.0:000:00 / 3:54•Watch full videoLive••Scroll for details\n\n      21-year old dev destroys LeetCode, gets kicked out of school...\n\n    Fireship\n\n    Fireship\n\n  Verified\n\n3.79M subscribers\n      SubscribeSubscribed\n\n        1234567890123456789012345678912345678901234567890123456789KShareDownload\n  Download\n\nSave\n\n  248,707 views  3 hours ago  #programming #ai #thecodereport\n\n  248,707 views • Mar 27, 2025 • #programming #ai #thecodereport\n\n          Show less\n\n        Try Brilliant free for 30 days https://brilliant.org/fireship You’ll also get 20% off an annual premium subscription.\n\nA 21-year old student at Columbia University got into trouble to developing an app that helps people cheat on the soft…...more\n...more\n\nHow this content was madeAuto-dubbedAudio tracks for some languages were automatically generated. Learn more\n\n  Transcript\n\n  Follow along using the transcript.\n\n  Show transcript\n\n    Fireship\n\n      3.79M subscribers\n\n  Videos\n  About\n\n    VideosAboutTwitterDiscordTikTokInstagram\n\n          Show less\n\n            #programming #ai #thecodereport21-year old dev destroys LeetCode, gets kicked out of school...248,707 viewsMar 27, 20251234567890123456789012345678912345678901234567890123456789KShareDownload\n  Download\n\nSave\n\n      1,265 Comments\n\n  Sort comments\n\n      Sort by\n\n              Top comments\n\n              Newest first\n\n  Add a comment...\n\n              @justicekawuma7832\n\n            3 hours ago\n\n    \"I built a SAAS that's makes 2mil annual revenue\"\nFAANG: \"but can you reverse a linked list?\"\n\n  Show less\n\n  Read more\n\n  Like\n\n    3.3K\n\n  Dislike\n\n  Reply\n\n      19 replies\n\n      19 replies\n\n              @DingleFlop\n\n            3 hours ago\n\n    \"Let's use AI to get rid of all the tedious, useless, crappy parts of our job!\"\n\"Wait, don't get rid of THAT crappy part!!\"\n\n  Show less\n\n  Read more\n\n  Like\n\n    2K\n\n  Dislike\n\n  Reply\n\n      8 replies\n\n      8 replies\n\n              @SXZ-dev\n\n            3 hours ago\n\n    Fake engineering engaging in a massive reaction against a real engineer exposing their crap, free Roy, he's a hero.\n\n  Show less\n\n  Read more\n\n  Like\n\n    1.8K\n\n  Dislike\n\n  Reply\n\n      3 replies\n\n      3 replies\n\n              @moomie1634\n\n            3 hours ago\n\n    Honestly, if you could build something like that, a company should just hire you straight up\n\n  Show less\n\n  Read more\n\n  Like\n\n    3.7K\n\n  Dislike\n\n  Reply\n\n      31 replies\n\n      31 replies\n\n              @x_____________\n\n            2 hours ago\n\n    > \"AI is the future\"\n> uses AI\n> Woah wait not like that - that's cheating\n\n  Show less\n\n  Read more\n\n  Like\n\n    400\n\n  Dislike\n\n  Reply\n\n      2 replies\n\n      2 replies\n\n              @Isabella_Laiba_Gaming\n\n            3 hours ago (edited)\n\n    I am Isabella, from California I am sick pray for me\n\n  Show less\n\n  Read more\n\n  Like\n\n    4.8K\n\n  Dislike\n\n  Reply\n\n      32 replies\n\n      32 replies\n\n              @robertkaminski1781\n\n            2 hours ago\n\n    before my first interview, I solved around 400 problems on LeetCode. After five years of working as a developer, I never — never — had to use that knowledge. So now, I barely remember any of that stuff.\n\n  Show less\n\n  Read more\n\n  Like\n\n    278\n\n  Dislike\n\n  Reply\n\n      8 replies\n\n      8 replies\n\n              @kris1123259\n\n            2 hours ago\n\n    If you are questioning why is he getting black listed instead of hired, is because he clearly doesn't respect the corporate overlords. His software is going to increase the costs of hiring someone.\n\n  Show less\n\n  Read more\n\n  Like\n\n    304\n\n  Dislike\n\n  Reply\n\n      7 replies\n\n      7 replies\n\n              @Slade-Wilson-m1x\n\n            2 hours ago\n\n    This guy is so good at problem solving, he solved the entire coding interview problem.\n\n  Show less\n\n  Read more\n\n  Like\n\n    737\n\n  Dislike\n\n  Reply\n\n      4 replies\n\n      4 replies\n\n              @MarthaWare-q2w\n\n            1 hour ago\n\n    These interviews are meant to hurt you; this man is a hero.  I wouldn't want this shit on anyone I truly despise after going through it myself.\n\n  Show less\n\n  Read more\n\n  Like\n\n    210\n\n  Dislike\n\n  Reply\n\n      1 reply\n\n      1 reply\n\n              @Rawbful\n\n            2 hours ago\n\n    This should get him hired not fired. This is exactly what a software engineer is supposed to do.\n\n  Show less\n\n  Read more\n\n  Like\n\n    563\n\n  Dislike\n\n  Reply\n\n      6 replies\n\n      6 replies\n\n              @BeastyJr69\n\n            3 hours ago\n\n    his linkedIN is funny af too\nhe changed it now but before on his columbian university mention he had written\ngot kicked out for having too much rizz + too many huzz\n\n  Show less\n\n  Read more\n\n  Like\n\n    128\n\n  Dislike\n\n  Reply\n\n      4 replies\n\n      4 replies\n\n              @BirdiestBird\n\n            3 hours ago\n\n    Thanks Roy\n\n  Show less\n\n  Read more\n\n  Like\n\n    500\n\n  Dislike\n\n  Reply\n\n      1 reply\n\n      1 reply\n\n              @ConnerArdman\n\n    @ConnerArdman\n\n    @ConnerArdman\n\n            3 hours ago\n\n    I would never endorse cheating in an interview.... but these companies all missed an opportunity to hire someone who seems absolutely brilliant. Would I want to hire someone who just cheated on my interviews in a boring old school way? No. Would I want to hire someone who built an AI tool that allowed them to flawlessly cheat in my interviews and circumvent the anti-cheating measures we thought we had? Absolutely.\n\n  Show less\n\n  Read more\n\n  Like\n\n    1K\n\n  Dislike\n\n  Reply\n\n      16 replies\n\n      16 replies\n\n              @chrismdavis2009\n\n            28 minutes ago\n\n    Roy is a goddamn saint and if his app doesn’t work out, we need to make sure he’s taken care of.\n\n  Show less\n\n  Read more\n\n  Like\n\n    3\n\n  Dislike\n\n  Reply\n\n              @find2hard\n\n            3 hours ago\n\n    How is this even grounds for getting kicked out? He should sue!\n\n  Show less\n\n  Read more\n\n  Like\n\n    1.2K\n\n  Dislike\n\n  Reply\n\n      17 replies\n\n      17 replies\n\n              @aev6075\n\n            3 hours ago\n\n    Work places demand applicants to remember answers to ridiculous, obsolete questions in setting that doesn't match the actual job is the real problem to begin with.\n\nThis was inevitable.\n\n  Show less\n\n  Read more\n\n  Like\n\n    348\n\n  Dislike\n\n  Reply\n\n      7 replies\n\n      7 replies\n\n              @vyndecimibd\n\n            3 hours ago\n\n    Amazon filed a copyright claim and his video is taken down… wtf\n\n  Show less\n\n  Read more\n\n  Like\n\n    211\n\n  Dislike\n\n  Reply\n\n      9 replies\n\n      9 replies\n\n              @havocthehobbit\n\n            3 hours ago\n\n    The fact they didnt make him a better offer , shows you how useless those companies are for real progress , so not sure why we even call them tech companies anymore then car service companies . He showed ingenuity to solve a problem nobody few even understood was a big problem , the generic heiring process.\nHopefully he outwites all those companies and starts up something that steels most their business and investment away from them , so they regret not only not hiring him but getting him kicked out of skill and backstreet blacklisted .\n\n  Show less\n\n  Read more\n\n  Like\n\n    340\n\n  Dislike\n\n  Reply\n\n      6 replies\n\n      6 replies\n\n              @someoldcliche\n\n            48 minutes ago\n\n    I'm not a coder, I'm a Business Boi. But I'm 100% in favor of this application. If a hirer is relying on bullshit software to do their hiring, coders should be able to rely on software to get hired. Don't want people to exploit the limits of LeetCode? Then come up with your own technical questions that are ACTUALLY APPLICABLE TO THE ROLE IN QUESTION. I have no sympathy for these insanely wealthy tech companies that can't even be bothered to come up with their own interview materials.\n\n  Show less\n\n  Read more\n\n  Like\n\n    6\n\n  Dislike\n\n  Reply\n\n      1 reply\n\n      1 reply\n\n      Comments\n      1.2K\n\n              Top comments\n\n              Newest first\n\n      In this video\n\nTranscript\n\n      Description\n\n  21-year old dev destroys LeetCode, gets kicked out of school...\n\n20KLikes248,707Views3hAgo\nTry Brilliant free for 30 days https://brilliant.org/fireship You’ll also get 20% off an annual premium subscription.\n\nA 21-year old student at Columbia University got into trouble to developing an app that helps people cheat on the software engineering technical interview. Learn what this means for the future of LeetCode and its impact on the interview process for programmers.\n\n#programming #ai #thecodereport\n\n💬 Chat with Me on Discord\n\n/discord\n\n🔗 Resources\n\nKicked out of Columbia https://x.com/im_roy_lee/status/19050...\nTechnical Interview Cheating  •Cheatersarebreakingthetechnicali...\nWhat is Vibe Coding? •The\"vibecoding\"mindvirusexplained…\n\n🔥 Get More Content - Upgrade to PRO\n\nUpgrade at https://fireship.io/pro\nUse code YT25 for 25% off PRO access\n\n🎨 My Editor Settings\n\nAtom One Dark\nvscode-icons\nFira Code Font\n\n🔖 Topics Covered\n\nWhat is LeetCode?\nWhy do people hate LeetCode?\nWhat makes the technical interview so hard?\nHow do I get a job with a FAANG company?\nHow do people cheat on technical interviews?…...more\n...more\nShow less\n\nHow this content was madeAuto-dubbedAudio tracks for some languages were automatically generated. Learn more\n\n  Transcript\n\n  Follow along using the transcript.\n\n  Show transcript\n\n    Fireship\n\n      3.79M subscribers\n\n  Videos\n  About\n\n    VideosAboutTwitterDiscordTikTokInstagram\n\n      Transcript\n\nNaN / NaN\n\n  4:47\n    4:47\n  Now playing\n\n            The \"vibe coding\" mind virus explained…\n\n    Fireship\n\n    Fireship\n\n  Verified\n\n    •\n\n    •\n\n      816K views\n\n      1 day ago\n\n          New\n\n  5:07\n    5:07\n  Now playing\n\n            Snow White Pitch Meeting\n\n    Pitch Meeting\n\n    Pitch Meeting\n\n  Verified\n\n    •\n\n    •\n\n      313K views\n\n      4 hours ago\n\n          New\n\n  33:59\n    33:59\n  Now playing\n\n            Texting War Plans = Illegal\n\n    LegalEagle\n\n    LegalEagle\n\n  Verified\n\n    •\n\n    •\n\n      288K views\n\n      4 hours ago\n\n          New\n\n  10:02\n    10:02\n  Now playing\n\n            Apple’s best just got cheaper AND better - MacBook Air M4\n\n    ShortCircuit\n\n    ShortCircuit\n\n  Verified\n\n    •\n\n    •\n\n      43K views\n\n      3 hours ago\n\n          New\n\n  8:30\n    8:30\n  Now playing\n\n            Strangest Delivery Driver Ever\n\n    Daily Dose Of Internet\n\n    Daily Dose Of Internet\n\n  Verified\n\n    •\n\n    •\n\n      436K views\n\n      2 hours ago\n\n          New\n\n  6:10\n    6:10\n  Now playing\n\n            We Need To Talk about Nvidia - RTX 5090 Laptops\n\n    Dave2D\n\n    Dave2D\n\n  Verified\n\n    •\n\n    •\n\n      47K views\n\n      2 hours ago\n\n          New\n\n  8:09\n    8:09\n  Now playing\n\n            Is This the end of Software Engineers?\n\n    Economy Media\n\n    Economy Media\n\n    •\n\n    •\n\n      82K views\n\n      2 days ago\n\n          New\n\n  16:17\n    16:17\n  Now playing\n\n            Has The Car Market Ruined Itself?\n\n    How Money Works\n\n    How Money Works\n\n  Verified\n\n    •\n\n    •\n\n      222K views\n\n      8 hours ago\n\n          New\n\n  15:43\n    15:43\n  Now playing\n\n            How ideology affects food waste\n\n    Adam Ragusea\n\n    Adam Ragusea\n\n  Verified\n\n    •\n\n    •\n\n      43K views\n\n      4 hours ago\n\n          New\n\n  3:17\n    3:17\n  Now playing\n\n            Inappropriate time for Gwent\n\n    Viva La Dirt League\n\n    Viva La Dirt League\n\n  Verified\n\n    •\n\n    •\n\n      79K views\n\n      2 hours ago\n\n          New\n\n  3:49\n    3:49\n  Now playing\n\n            Next.js rocked by critical 9.1 level exploit...\n\n    Fireship\n\n    Fireship\n\n  Verified\n\n    •\n\n    •\n\n      654K views\n\n      3 days ago\n\n          New\n\n  8:05\n    8:05\n  Now playing\n\n            Tesla’s problems are bigger than just Elon Musk\n\n    The Verge\n\n    The Verge\n\n  Verified\n\n    •\n\n    •\n\n      45K views\n\n      7 hours ago\n\n          New\n\n  8:56\n    8:56\n  Now playing\n\n            Why Do Trump & Vance Hate Europe So Much?\n\n    TLDR News Global\n\n    TLDR News Global\n\n    •\n\n    •\n\n      476K views\n\n      1 day ago\n\n          New\n\n  12:45\n    12:45\n  Now playing\n\n            Why we can't focus.\n\n    Jared Henderson\n\n    Jared Henderson\n\n  Verified\n\n    •\n\n    •\n\n      2M views\n\n      3 months ago\n\n  14:46\n    14:46\n  Now playing\n\n            China's slaughterbots show WW3 would kill us all.\n\n    Digital Engine\n\n    Digital Engine\n\n    •\n\n    •\n\n      2.5M views\n\n      3 months ago\n\n  16:47\n    16:47\n  Now playing\n\n            Crossing Japan With No Map\n\n    Ludwig\n\n    Ludwig\n\n  Verified\n\n    •\n\n    •\n\n      1.9M views\n\n      5 days ago\n\n          New\n\n  48:11\n    48:11\n  Now playing\n\n            The Neovim Experience\n\n    Bog\n\n    Bog\n\n  Verified\n\n    •\n\n    •\n\n      20K views\n\n      4 hours ago\n\n          New\n\n  29:33\n    29:33\n  Now playing\n\n            The New Way Nations are Fighting (and why it matters)\n\n    Johnny Harris\n\n    Johnny Harris\n\n  Verified\n\n    •\n\n    •\n\n      465K views\n\n      10 hours ago\n\n          New\n\n  15:54\n    15:54\n  Now playing\n\n            They Leaked The Group Chat...\n\n    Big A\n\n    Big A\n\n    •\n\n    •\n\n      329K views\n\n      1 day ago\n\n          New\n\n  8:19\n    8:19\n  Now playing\n\n            Asking for Supervisor Turns into Big Chase\n\n    penguinz0\n\n    penguinz0\n\n  Verified\n\n    •\n\n    •\n\n      393K views\n\n      6 hours ago\n\n          New\n\nShow more",
    "summary": {
      "en": "A 21-year-old student at Columbia University created an app that helps users cheat during technical interviews, specifically for software engineering roles. This led to him getting expelled from the university. The incident raises questions about the future of LeetCode, a popular platform for interview preparation, and how it may affect the hiring process for programmers. \n\nThe video discusses the challenges of technical interviews, why many people dislike LeetCode, and explores topics like \"vibe coding\" and the implications of cheating in interviews. It emphasizes the need for companies to rethink their interview methods rather than relying on outdated questions.",
      "ko": "컬럼비아 대학교의 21세 학생이 소프트웨어 엔지니어링 직무를 위한 기술 면접에서 사용자들이 부정행위를 할 수 있도록 돕는 앱을 만들었습니다. 이로 인해 그는 대학에서 퇴학당하게 되었습니다. 이 사건은 면접 준비를 위한 인기 플랫폼인 리트코드(LeetCode)의 미래와 프로그래머 채용 과정에 미칠 영향에 대한 의문을 제기합니다.\n\n영상에서는 기술 면접의 어려움과 많은 사람들이 리트코드를 싫어하는 이유를 다루고 있으며, \"바이브 코딩\"과 면접에서의 부정행위가 미치는 영향 같은 주제도 탐구합니다. 또한, 기업들이 구식 질문에 의존하기보다는 면접 방식을 재고할 필요성이 강조됩니다.",
      "ja": null
    }
  },
  {
    "id": "6671c95fedbefe68",
    "title": {
      "en": "When Getting Phished Puts You in Mortal Danger",
      "ko": "피싱의 위험한 진실",
      "ja": null
    },
    "type": "story",
    "url": "https://krebsonsecurity.com/2025/03/when-getting-phished-puts-you-in-mortal-danger/",
    "score": 6,
    "by": "todsacerdoti",
    "time": 1743093869,
    "content": "March 27, 2025\n\n\t\t3 Comments\n\n\t\t\t\t\t\t\t\t\t\t\t\tMany successful phishing attacks result in a financial loss or malware infection. But falling for some phishing scams, like those currently targeting Russians searching online for organizations that are fighting the Kremlin war machine, can cost you your freedom or your life.\nThe real website of the Ukrainian paramilitary group “Freedom of Russia” legion. The text has been machine-translated from Russian.\nResearchers at the security firm Silent Push mapped a network of several dozen phishing domains that spoof the recruitment websites of Ukrainian paramilitary groups, as well as Ukrainian government intelligence sites.\nThe website legiohliberty[.]army features a carbon copy of the homepage for the Freedom of Russia Legion (a.k.a. “Free Russia Legion”), a three-year-old Ukraine-based paramilitary unit made up of Russian citizens who oppose Vladimir Putin and his invasion of Ukraine.\nThe phony version of that website copies the legitimate site — legionliberty[.]army — providing an interactive Google Form where interested applicants can share their contact and personal details. The form asks visitors to provide their name, gender, age, email address and/or Telegram handle, country, citizenship, experience in the armed forces; political views; motivations for joining; and any bad habits.\n“Participation in such anti-war actions is considered illegal in the Russian Federation, and participating citizens are regularly charged and arrested,” Silent Push wrote in a report released today. “All observed campaigns had similar traits and shared a common objective: collecting personal information from site-visiting victims. Our team believes it is likely that this campaign is the work of either Russian Intelligence Services or a threat actor with similarly aligned motives.”\nSilent Push’s Zach Edwards said the fake Legion Liberty site shared multiple connections with rusvolcorps[.]net. That domain mimics the recruitment page for a Ukrainian far-right paramilitary group called the Russian Volunteer Corps (rusvolcorps[.]com), and uses a similar Google Forms page to collect information from would-be members.\nOther domains Silent Push connected to the phishing scheme include: ciagov[.]icu, which mirrors the content on the official website of the U.S. Central Intelligence Agency; and hochuzhitlife[.]com, which spoofs the Ministry of Defense of Ukraine & General Directorate of Intelligence (whose actual domain is hochuzhit[.]com).\n\nAccording to Edwards, there are no signs that these phishing sites are being advertised via email. Rather, it appears those responsible are promoting them by manipulating the search engine results shown when someone searches for one of these anti-Putin organizations.\nIn August 2024, security researcher Artem Tamoian posted on Twitter/X about how he received startlingly different results when he searched for “Freedom of Russia legion” in Russia’s largest domestic search engine Yandex versus Google.com. The top result returned by Google was the legion’s actual website, while the first result on Yandex was a phishing page targeting the group.\n“I think at least some of them are surely promoted via search,” Tamoian said of the phishing domains. “My first thread on that accuses Yandex, but apart from Yandex those websites are consistently ranked above legitimate in DuckDuckGo and Bing. Initially, I didn’t realize the scale of it. They keep appearing to this day.”\nTamoian, a native Russian who left the country in 2019, is the founder of the cyber investigation platform malfors.com. He recently discovered two other sites impersonating the Ukrainian paramilitary groups — legionliberty[.]world and rusvolcorps[.]ru — and reported both to Cloudflare. When Cloudflare responded by blocking the sites with a phishing warning, the real Internet address of these sites was exposed as belonging to a known “bulletproof hosting” network called Stark Industries Solutions Ltd.\nStark Industries Solutions appeared two weeks before Russia invaded Ukraine in February 2022, materializing out of nowhere with hundreds of thousands of Internet addresses in its stable — many of them originally assigned to Russian government organizations. In May 2024, KrebsOnSecurity published a deep dive on Stark, which has repeatedly been used to host infrastructure for distributed denial-of-service (DDoS) attacks, phishing, malware and disinformation campaigns from Russian intelligence agencies and pro-Kremlin hacker groups.\nIn March 2023, Russia’s Supreme Court designated the Freedom of Russia legion as a terrorist organization, meaning that Russians caught communicating with the group could face between 10 and 20 years in prison.\nTamoian said those searching online for information about these paramilitary groups have become easy prey for Russian security services.\n“I started looking into those phishing websites, because I kept stumbling upon news that someone gets arrested for trying to join [the] Ukrainian Army or for trying to help them,” Tamoian told KrebsOnSecurity. “I have also seen reports [of] FSB contacting people impersonating Ukrainian officers, as well as using fake Telegram bots, so I thought fake websites might be an option as well.”\nSearch results showing news articles about people in Russia being sentenced to lengthy prison terms for attempting to aid Ukrainian paramilitary groups.\nTamoian said reports surface regularly in Russia about people being arrested for trying carry out an action requested by a “Ukrainian recruiter,” with the courts unfailingly imposing harsh sentences regardless of the defendant’s age.\n“This keeps happening regularly, but usually there are no details about how exactly the person gets caught,” he said. “All cases related to state treason [and] terrorism are classified, so there are barely any details.”\nTamoian said while he has no direct evidence linking any of the reported arrests and convictions to these phishing sites, he is certain the sites are part of a larger campaign by the Russian government.\n“Considering that they keep them alive and keep spawning more, I assume it might be an efficient thing,” he said. “They are on top of DuckDuckGo and Yandex, so it unfortunately works.”\nFurther reading: Silent Push report, Russian Intelligence Targeting its Citizens and Informants.\n\n         This entry was posted on Thursday 27th of March 2025 12:39 PM\n\n\t\t\t\t\tA Little Sunshine Russia's War on Ukraine\n\t\t\t\t\t\t\t\tArtem Tamoian DuckDuckGo Freedom of Russia legion Russian Volunteer Corps Silent Push Stark Industries Solutions Ltd Yandex Zach Edwards",
    "summary": {
      "en": "On March 27, 2025, it was reported that many phishing scams are targeting Russians looking for anti-Kremlin groups, with some potentially leading to severe consequences, including arrest. Security firm Silent Push identified a network of phishing websites that mimic the recruitment pages of Ukrainian paramilitary groups, such as the Freedom of Russia Legion, which opposes Vladimir Putin.\n\nThese fake sites collect personal information from users through forms asking for details like name, age, and political views. The phishing campaigns likely originate from Russian intelligence or aligned actors, as participating in such anti-war actions is illegal in Russia.\n\nThe phishing websites are not promoted via email but manipulate search engine results, often appearing above legitimate sites on platforms like Yandex, which is popular in Russia. Security researcher Artem Tamoian noted that those searching for information about these groups can easily fall victim to these scams.\n\nIn March 2023, Russia classified the Freedom of Russia Legion as a terrorist organization, increasing the risk for individuals trying to connect with it. While there is no direct evidence linking arrests to these phishing sites, Tamoian believes they are part of a broader Russian government strategy to suppress dissent.",
      "ko": "2025년 3월 27일, 반크렘린 단체를 찾고 있는 러시아인들을 겨냥한 여러 피싱 사기가 발생하고 있다는 보도가 있었습니다. 이 사기는 심각한 결과, 즉 체포로 이어질 가능성도 있습니다. 보안 회사인 Silent Push는 블라디미르 푸틴에 반대하는 우크라이나 준군사조직인 러시아 자유군단의 모집 페이지를 모방한 피싱 웹사이트 네트워크를 발견했습니다.\n\n이 가짜 사이트들은 사용자로부터 이름, 나이, 정치적 견해와 같은 개인 정보를 수집하는 양식을 통해 정보를 요청합니다. 이러한 피싱 캠페인은 러시아 정보기관이나 그와 연관된 세력에서 시작된 것으로 보이며, 러시아에서는 반전 행동에 참여하는 것이 불법입니다.\n\n피싱 웹사이트는 이메일을 통해 홍보되지 않고, 검색 엔진 결과를 조작하여 러시아에서 인기 있는 플랫폼인 얀덱스와 같은 사이트에서 합법적인 사이트보다 상위에 노출됩니다. 보안 연구원인 아르템 타모이안은 이러한 단체에 대한 정보를 찾는 사람들이 쉽게 사기의 피해자가 될 수 있다고 경고했습니다.\n\n2023년 3월, 러시아는 러시아 자유군단을 테러 조직으로 분류하여 이 단체와 연결하려는 개인들에게 위험을 증가시켰습니다. 이러한 피싱 사이트와 체포 간의 직접적인 연관 증거는 없지만, 타모이안은 이들이 반대 의견을 억압하기 위한 러시아 정부의 광범위한 전략의 일환이라고 믿고 있습니다.",
      "ja": null
    }
  },
  {
    "id": "7d1c42d11faaaa35",
    "title": {
      "en": "Problems with the heap",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://rachelbythebay.com/w/2025/03/26/atop/",
    "score": 195,
    "by": "todsacerdoti",
    "time": 1743017016,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "639137152036036a",
    "title": {
      "en": "A love letter to the CSV format",
      "ko": "CSV 포맷에 대한 사랑",
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/medialab/xan/blob/master/docs/LOVE_LETTER.md",
    "score": 664,
    "by": "Yomguithereal",
    "time": 1743008936,
    "content": "A love letter to the CSV format\nOr why people pretending CSV is dead are wrong\nEvery month or so, a new blog article declaring the near demise of CSV in favor of some \"obviously superior\" format (parquet, newline-delimited JSON, MessagePack records etc.) find its ways to the reader's eyes. Sadly those articles often offer a very narrow and biased comparison and often fail to understand what makes CSV a seemingly unkillable staple of data serialization.\nIt is therefore my intention, through this article, to write a love letter to this data format, often criticized for the wrong reasons, even more so when it is somehow deemed \"cool\" to hate on it. My point is not, far from it, to say that CSV is a silver bullet but rather to shine a light on some of the format's sometimes overlooked strengths.\n1. CSV is dead simple\nThe specification of CSV holds in its title: \"comma separated values\". Okay, it's a lie, but still, the specification holds in a tweet and can be explained to anybody in seconds: commas separate values, new lines separate rows. Now quote values containing commas and line breaks, double your quotes, and that's it. This is so simple you might even invent it yourself without knowing it already exists while learning how to program.\nOf course it does not mean you should not use a dedicated CSV parser/writer because you will mess something up.\n2. CSV is a collective idea\nNo one owns CSV. It has no real specification (yes, I know about the controversial ex-post RFC 4180), just a set of rules everyone kinda agrees to respect implicitly. It is, and will forever remain, an open and free collective idea.\n3. CSV is text\nLike JSON, YAML or XML, CSV is just plain text, that you are free to encode however you like. CSV is not a binary format, can be opened with any text editor and does not require any specialized program to be read. This means, by extension, that it can both be read and edited by humans directly, somehow.\n4. CSV is streamable\nCSV can be read row by row very easily without requiring more memory than what is needed to fit a single row. This also means that a trivial program that anyone can write is able to read gigabytes of CSV data with only some kilobytes of RAM.\nBy comparison, column-oriented data formats such as parquet are not able to stream files row by row without requiring you to jump here and there in the file or to buffer the memory cleverly so you don't tank read performance.\nBut of course, CSV is terrible if you are only interested in specific columns because you will indeed need to read all of a row only to access the part you are interested in.\nColumn-oriented data format are of course a very good fit for the dataframes mindset of R, pandas and such. But critics of CSV coming from this set of practices tend to only care about use-cases where everything is expected to fit into memory.\n5. CSV can be appended to\nIt is trivial to add new rows at the end of a CSV file and it is very efficient to do so. Just open the file in append mode (a+) and get going.\nOnce again, column-oriented data formats cannot do this, or at least not in a straightforward manner. They can actually be regarded as on-disk dataframes, and like with dataframes, adding a column is very efficient while adding a new row really isn't.\n6. CSV is dynamically typed\nPlease don't flee. Let me explain why this is sometimes a good thing. Sometimes when dealing with data, you might like to have some flexibility, especially across programming languages, when parsing serialized data.\nConsider JavaScript, for instance, that is unable to represent 64 bits integers. Or what languages, frameworks and libraries consider as null values (don't get me started on pandas and null values). CSV lets you parse values as you see fit and is in fact dynamically typed. But this is as much of a strength as it can become a potential footgun if you are not careful.\nNote also, but this might be hard to do with higher-level languages such as python and JavaScript, that you are not required to decode the text at all to process CSV cell values and that you can work directly on the binary representation of the text for performance reasons.\n7. CSV is succinct\nHaving the headers written only once at the beginning of the file means the amount of formal repetition of the format is naturally very low. Consider a list of objects in JSON or the equivalent in XML and you will quickly see the cost of repeating keys everywhere. That does not mean JSON and XML will not compress very well, but few formats exhibit this level of natural conciseness.\nWhat's more, strings are often already optimally represented and the overhead of the format itself (some commas and quotes here and there) is kept to a minimum. Of course, statically-typed numbers could be represented more concisely, but you will not save up an order of magnitude there neither.\n8. Reverse CSV is still valid CSV\nThis one is not often realized by everyone but a reversed (byte by byte) CSV file, is still valid CSV. This is only made possible because of the genius idea to escape quotes by doubling them, which means escaping is a palindrome. It would not work if CSV used a backslash-based escaping scheme, as is most common when representing string literals.\nBut why should you care? Well, this means you can read very efficiently and very easily the last rows of a CSV file. Just feed the bytes of your file in reverse order to a CSV parser, then reverse the yielded rows and their cells' bytes and you are done (maybe read the header row before though).\nThis means you can very well use a CSV output as a way to efficiently resume an aborted process. You can indeed read and parse the last rows of a CSV file in constant time since you don't need to read the whole file but only to position yourself at the end of the file to buffer the bytes in reverse and feed them to the parser.\n9. Excel hates CSV\nIt clearly means CSV must be doing something right.\nSigned: xan, the CSV magician",
    "summary": {
      "en": "The article defends the CSV (Comma-Separated Values) format against claims of its decline in favor of newer formats. Here are the key points:\n\n1. **Simplicity**: CSV is easy to understand and create. Its basic rules are straightforward, making it accessible to anyone learning to program.\n\n2. **Collective Ownership**: No one owns CSV, and it lacks a formal specification, which allows it to remain open and free.\n\n3. **Text-Based**: CSV is plain text, easily readable and editable with any text editor, unlike binary formats.\n\n4. **Streamable**: CSV can be read one row at a time, requiring minimal memory, making it efficient for handling large datasets.\n\n5. **Easy to Append**: Adding new rows to a CSV file is simple and efficient, unlike column-oriented formats.\n\n6. **Dynamic Typing**: CSV allows flexibility in data types across different programming languages, which can be beneficial in certain contexts.\n\n7. **Conciseness**: CSV files are succinct, with headers written only once, reducing repetition compared to formats like JSON or XML.\n\n8. **Reversible**: A reversed CSV file is still valid, allowing for efficient reading of the last rows without scanning the entire file.\n\n9. **Excel Compatibility**: If Excel struggles with CSV, it suggests that CSV may be doing something right.\n\nOverall, the article emphasizes that CSV has unique strengths that keep it relevant, despite criticisms.",
      "ko": "이 기사는 CSV(Comma-Separated Values) 형식이 새로운 형식에 밀려나고 있다는 주장에 대해 방어하고 있습니다. 주요 내용은 다음과 같습니다.\n\nCSV는 이해하고 만들기 쉬운 형식입니다. 기본 규칙이 간단하여 프로그래밍을 배우는 누구나 접근할 수 있습니다. CSV는 특정 소유자가 없고 공식적인 규격이 없기 때문에 개방적이고 자유롭게 사용할 수 있습니다. 또한 CSV는 일반 텍스트 형식으로, 이진 형식과 달리 모든 텍스트 편집기로 쉽게 읽고 수정할 수 있습니다.\n\nCSV는 한 번에 한 행씩 읽을 수 있어 메모리 사용이 최소화되며, 대용량 데이터셋을 처리하는 데 효율적입니다. 새로운 행을 CSV 파일에 추가하는 것도 간단하고 효율적이며, 열 중심 형식과는 다릅니다. CSV는 다양한 프로그래밍 언어에서 데이터 유형의 유연성을 허용하여 특정 상황에서 유리할 수 있습니다.\n\nCSV 파일은 간결하여 헤더가 한 번만 작성되므로 JSON이나 XML과 같은 형식에 비해 중복이 줄어듭니다. CSV 파일을 거꾸로 읽어도 유효하므로 전체 파일을 스캔하지 않고도 마지막 행을 효율적으로 읽을 수 있습니다. 만약 Excel이 CSV 파일을 처리하는 데 어려움을 겪는다면, 이는 CSV가 올바른 방식으로 작동하고 있다는 것을 의미할 수 있습니다.\n\n전반적으로 이 기사는 CSV가 비판에도 불구하고 여전히 중요한 이유가 있다는 점을 강조하고 있습니다.",
      "ja": null
    }
  },
  {
    "id": "5e9b0df9d3ba8808",
    "title": {
      "en": "Writing a tiny undo/redo stack in JavaScript",
      "ko": "자바스크립트로 작은 실행 취소 스택 만들기",
      "ja": null
    },
    "type": "story",
    "url": "https://blog.julik.nl/2025/03/a-tiny-undo-stack",
    "score": 160,
    "by": "julik",
    "time": 1742806403,
    "content": "UI Algorithms: A Tiny Undo Stack\n\n  ·22 Mar 2025\n\n    I’ve needed this before - a couple of times. Third time I figured I needed something small, nimble - yet complete. And - at the same time - wondering about how to do it in a very simple manner. I think it worked out great, so let’s dig in.\n\nUndo histories and managers\n\nMost UIs will have some form of undo functionality. Now, there are generally two forms of it: undo stacks and version histories. A “version history” is what Photoshop history gives you - the ability to “paint through” to a previous state of the system. You can add five paint strokes, and then reveal a stroke you have made 4 steps back.\n\nBut most apps won’t need that. What you will need is an undo stack, which can be specced out as follows:\n\n  An undoable action gets performed and gets pushed onto the stack.\n  If undo is requested, the stack is popped and the rollback action gets applied for the popped action.\n  If an action was undone, you can redo that action. If you have undone 2 actions, you can redo 2 actions.\n  If you push an undoable action onto the stack in presence of actions that can be redone, they get discarded - there is no branching, remember?\n\nIf you are curious how “the big guys” used to do it - check out the NSUndoManager documentation\n\nSo, as I usually like to do, I want to understand the API that would be optimal. For this use case - drawing - I had the following workflow:\n\n  When you draw a stroke the input points get added to currentStroke\n  When you release the pen the currentStroke gets appended to strokes and reset for the next stroke.\n\nI wanted something like this:\n\nlet addStroke = () => strokes.push(currentPaintStroke);\nlet removeStroke = () => strokes.pop();\nundoThing.push(addStroke, removeStroke);\n\n// then, on user action\nundoThing.undo(); // calls removeStroke()\nundoThing.redo(); // calls strokes.push(...) again\n\nThe perils of stack pointers\n\nSimplest thing in the world. Now, if you look at most recommended (and some existing!) implementations of an undo stack, you will find they usually make use of a stack with a pointer. Like here and here - you would have a stack, usually represented as a JS array, and some kind of pointer or an index that you would use to index into it.\n\nAnd while it is workable and standard, it just didn’t jive with me well. See, using an index into an array usually makes JS code susceptible to two things, which bite me every single time:\n\n  Indexing into a nonexistent index - hello undefined checks\n  Mistakes in offsets when calling Array.slice and Array.splice. Oh, and confusing slice and splice, of course.\n\nThe fact that Ruby and JS have different semantics for slice - one uses the index bounds, the other uses two offsets - doesn’t help things. And what happens if an API uses offsets into a vector? Exactly: confusion whether those offsets are inclusive or exclusive. Oh, and the offsets change after you mutate the array, which makes it even more painful.\n\nCould we not index?\n\nSo what came to mind was this: we effectively have two stacks, not one. We have an undoStack (things that can be rolled back) and a redoStack - things that can be rolled forward. All the things we do with our undo-redo actions actually do not change the pointer - they move things from one stack to another. And rules change between these two stacks! We erase the redoable actions when we add a new undoable action, remember? So while an undoable stack will rarely get “nullified”, the redoable stack likely will be nullified frequently.\n\nOnce this became clear, the implementation practically wrote itself:\n\nfunction createUndoStack() {\n  let past = [];\n  let future = [];\n\n  return {\n    push(doFn, undoFn) {\n      doFn();\n      past.push({doFn, undoFn});\n      // Adding a new action wipes the redoable steps\n      future.length = 0;\n    },\n    undo() {\n     let action = past.pop();\n     if (action) {\n       action.undoFn();\n       future.unshift(action);\n     }\n    },\n    redo() {\n      let action = future.unshift();\n      if (action) {\n        action.doFn();\n        past.push(action);\n      }\n    }\n  };\n}\n\nSo instead of trying to save resources by having just one array (and miserably failing with off-by-one index errors), we can embrace dynamically sized arrays and just forget indices altogether. Neat!\n\nLet’s add a couple more methods to display our UI:\n\n  get canUndo() {\n    return past.length > 0;\n  },\n  get canRedo() {\n    return future.length > 0;\n  }\n\nThe pass-by-reference problem\n\nThere is a catch with our implementation though. JS is pass-by-reference for pretty much all of its types. This means, that when we create a closure over a value - currentStroke in this case - the closure will keep addressing whatever is stored in currentStroke right now. And these doFn and undoFn are very particular in a specific behavioral trait: they must be idempotent. No matter how many times you call them, they should lead to the same result.\n\nIf we just do this:\n\nlet doFn = () => strokes.push(currentStroke)\n\neach time we call doFn - whatever is currentStroke in the calling scope will end up getting pushed onto the strokes stack. That’s not what we want - we want the doFn to use a cloned copy of the currentStroke, and we want it to do so always. Same for the undoFnx - although in this case there is no need for it to know what’s stored in strokes nor what the currentStroke used to be, as we are not going to resume drawing that currentStroke. Modern JS has a thing for this called structuredClone(), which is perfect for the occasion:\n\n  push(doFn, undoFn, ...withArgumentsToClone) {\n    const clonedArgs = structuredClone(withArgumentsToClone);\n    const action = {\n      doWithData() {\n        doFn(...clonedArgs);\n      },\n      undoWithData() {\n        undoFn(...clonedArgs);\n      },\n    };\n    action.doWithData();\n\n    // Adding a new action wipes the redoable steps\n    past.push(action);\n    future.length = 0;\n  }\n\nand we’ll amend our functions accordingly. Instead of closuring over currentStroke we’ll make it an argument:\n\nlet appendStroke = strokes.push.bind(strokes);\nundoStack.push(appendStroke, () => strokes.pop(), currentStroke);\n\nwith the push() of our undoStack taking care of making a deep clone for us. Nice!\n\nThe complete definition then becomes:\n\nfunction createUndoStack() {\n  const past = [];\n  const future = [];\n  return {\n    push(doFn, undoFn, ...withArgumentsToClone) {\n      const clonedArgs = structuredClone(withArgumentsToClone);\n      const action = {\n        doWithData() {\n          doFn(...clonedArgs);\n        },\n        undoWithData() {\n          undoFn(...clonedArgs);\n        },\n      };\n      action.doWithData();\n\n      // Adding a new action wipes the redoable steps\n      past.push(action);\n      future.length = 0;\n    },\n    undo() {\n      let action = past.pop();\n      if (action) {\n        action.undoWithData();\n        future.unshift(action);\n      }\n    },\n    redo() {\n      let action = future.shift();\n      if (action) {\n        action.doWithData();\n        past.push(action);\n      }\n    },\n    get undoAvailable() {\n      return past.length > 0;\n    },\n    get redoAvailable() {\n      return future.length > 0;\n    },\n    clear() {\n      past.length = 0;\n      future.length = 0;\n      return true;\n    }\n  }\n}\n\nexport {createUndoStack};\n\nRobust, small, and no indexing errors. My jam.",
    "summary": {
      "en": "### Summary of \"UI Algorithms: A Tiny Undo Stack\"\n\nThis article discusses creating a simple and efficient undo stack for user interfaces, which allows users to reverse actions.\n\n**Key Points:**\n\n1. **Types of Undo Functionality**:\n   - **Undo Stacks**: Track individual actions that can be undone and redone.\n   - **Version Histories**: Allow users to go back to previous states, like in Photoshop.\n\n2. **Basic Undo Stack Operations**:\n   - Push an action onto the stack when it is performed.\n   - Pop the last action off the stack to undo it.\n   - Redo the last undone action by pushing it back onto the stack.\n\n3. **Implementation**:\n   - The proposed implementation uses two separate stacks: one for past actions (undo) and one for future actions (redo).\n   - When a new action is added, the redo stack is cleared.\n\n4. **Avoiding Index Issues**:\n   - The article highlights problems with using array indexes, such as accessing nonexistent indices and confusion between methods like `slice` and `splice`.\n   - The solution is to manage two separate stacks without relying on array indices.\n\n5. **Handling JavaScript's Pass-by-Reference**:\n   - Since JavaScript shares references for objects, the implementation ensures actions always use a cloned copy of the relevant data, preventing unexpected behavior.\n\n6. **Final Implementation**:\n   - The final undo stack allows adding actions with their corresponding undo functions and manages cloning automatically.\n   - It includes methods to check if undo or redo actions are available and to clear the stacks.\n\nOverall, this approach results in a robust and error-free undo stack suitable for simple UI applications.",
      "ko": "이 글에서는 사용자 인터페이스를 위한 간단하고 효율적인 실행 취소 스택을 만드는 방법에 대해 설명합니다. 이 스택은 사용자가 수행한 작업을 되돌릴 수 있게 해줍니다.\n\n첫 번째로, 실행 취소 기능의 종류에 대해 설명합니다. 실행 취소 스택은 개별 작업을 추적하여 사용자가 되돌리거나 다시 실행할 수 있도록 합니다. 반면, 버전 히스토리는 포토샵처럼 이전 상태로 돌아갈 수 있게 해줍니다.\n\n기본적인 실행 취소 스택 작업에는 세 가지가 있습니다. 사용자가 작업을 수행할 때 해당 작업을 스택에 추가하고, 마지막 작업을 스택에서 제거하여 실행 취소하며, 마지막으로 실행 취소된 작업을 다시 실행하기 위해 스택에 다시 추가하는 방식입니다.\n\n제안된 구현 방식은 과거 작업을 위한 스택과 미래 작업을 위한 스택, 두 개의 별도 스택을 사용합니다. 새로운 작업이 추가될 때마다 다시 실행 스택은 비워집니다.\n\n또한, 배열 인덱스를 사용할 때 발생할 수 있는 문제를 피하는 방법도 설명합니다. 존재하지 않는 인덱스에 접근하거나 `slice`와 `splice` 같은 메서드 간의 혼란이 생길 수 있습니다. 이를 해결하기 위해 배열 인덱스에 의존하지 않고 두 개의 별도 스택을 관리하는 방식을 채택합니다.\n\n자바스크립트의 참조 전달 방식도 다룹니다. 자바스크립트는 객체에 대한 참조를 공유하기 때문에, 구현에서는 항상 관련 데이터의 복제본을 사용하여 예기치 않은 동작을 방지합니다.\n\n최종 구현에서는 작업과 해당 작업의 실행 취소 기능을 추가할 수 있는 실행 취소 스택을 제공합니다. 또한 실행 취소나 다시 실행할 수 있는 작업이 있는지 확인하고 스택을 비우는 메서드도 포함되어 있습니다.\n\n이러한 접근 방식은 간단한 사용자 인터페이스 애플리케이션에 적합한 강력하고 오류 없는 실행 취소 스택을 만들어냅니다.",
      "ja": null
    }
  },
  {
    "id": "163c3a71aec979c9",
    "title": {
      "en": "The Ocean Sunfish: Why the Rant Is Wrong (2017)",
      "ko": "바다의 태양어: 반론의 진실",
      "ja": null
    },
    "type": "story",
    "url": "https://imgur.com/gallery/MMRg9",
    "score": 8,
    "by": "Tomte",
    "time": 1743104679,
    "content": "NextChevron Pointing Right",
    "summary": {
      "en": "Sure! Please provide the text you would like me to summarize.",
      "ko": "물론입니다! 요약해드릴 내용을 제공해 주시면 번역해 드리겠습니다.",
      "ja": null
    }
  },
  {
    "id": "0f9ecfa7eec46811",
    "title": {
      "en": "Unofficial Windows 7 Service Pack 2",
      "ko": "윈도우 7 비공식 SP2",
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/i486girl/win7-sp2",
    "score": 231,
    "by": "XzetaU8",
    "time": 1742643903,
    "content": "Windows 7 Service Pack 2\nWarningThis project is far from finished, meaning that bugs are to be expected. If you encounter any bugs, please report them in the issue tracker or in the Discord server. Thank you for your understanding.\n\nWindows 7 Service Pack 2 is a package consisting of updates, tweaks, backported apps and overall enhancements with the goal of providing an effortless way to have a fully updated Windows 7 ISO and enhancing usability on semi-modern machines.\nTODOs\n\n - All updates until 2020, with the addition of Windows Embedded Standard 7 updates\n - Snipping Tool from Windows 10 version 1507 -> Credits: vxiduu\n - Paint from Windows 8 build 9425\n - Registry Editor from Windows 10 build 16212 -> Credits: Aurorarion\n - In-place service pack installer\n - Native USB 3.0 and NVMe support -> Credits: Unknown user from MyDigitalLife forums\n - TPM 2.0 support\n - UEFI support in the ISO releases, with EFI GOP support too -> Credits: Typical/int10h, AveragePC\n - Inclusion of VxKex -> Credits: vxiduu\n - Windows 8 PE in the ISO, with a theme that resembles Windows 7 -> Credits: ImSwordQueen (Windows 7 theme for Windows 8.x)\n - Windows 10 setup engine in the ISO release -> Credits: Aurorarion (helping out on the 10 setup engine port)\n - Builtin Webp and FLAC codecs\n - Task Manager from Windows 8 build 7880 -> Credits: Jevil7452\n - Upscaled graphics in some places\n - Windows Vista/7 PE's boot screen on the ISO -> Credits: Microsoft Corporation and Tech Stuff (Boot8Plus)\n - Removal of the annoying \"Unsupported hardware\" prompt\n - Better DPI support in aero.msstyles -> Credits: Vaporvance (high DPI classes from Aero10 that will be ported to Windows 7)\n - Integration of Microsoft Visual C++ Redistributable AIO -> Credits: Microsoft Corporation (Visual C++) and abbodi1406 (VC++ AIO repack)\n - Disk Cleanup from Windows 8 build 7861 -> Credits: Jevil7452\n - Windows 8 build 7861's PDF Reader -> Credits: Jevil7452\n - Windows 10 20H1's System Information -> Credits: Jevil7452\n - Windows 11 24H2's timeout.exe command\n - Windows 10 1709's XPS Viewer\n - Windows 11 24H2's sudo.exe command\n - Windows 10 build 9845's Notepad\n - Windows Management Framework 5.1\n - Segoe UI Emoji\n - Microsoft Agent\n - WinHlp32\n - Work Folders\n - Restore Windows Journal\n - Microsoft Camera Codec Pack\n\nInstallation\nCautionFailure to follow instructions properly will result in a bricked system! We do not take responsibility for this unless this is a legitimate bug in the installer!\n\nImportantRAID/Intel RST only works on specific systems!\n\nWarningBoth versions of the Windows 7 Service Pack 2 are x64 only! Don't file issues over the lack of 32-bit hardware support as we do not plan to support 32-bit hardware. Consult the FAQ for more information.\n\nISO installation\nImportantThe ISO releases are for clean installs only. For in-place installs, please use the installer once we release a working version of it.\n\nPick the UEFI or Legacy Boot version, depending on what your system uses\nFlash it onto a DVD or a USB flash drive\nBoot from the Windows 7 install media on the machine you wish to install it in\nProceed as usual\nFinish the out of box experience\n\nIn-place intallation\n\nDownload the Windows 7 SP2 installer under the releases of this repository\nFollow the steps in the installer as usual\n\nWarningIf the installer is stuck at a certain percentage (during the Upgrading Windows phase), don't turn off your computer! Doing so will brick your system!\n\nFAQ\nQ: My system is 32-bit in hardware. Why won't the installer and the ISO run?\nA: Supporting both 64-bit and 32-bit Windows would be very time consuming, so we chose to support only 64-bit systems. Please don't file any issues regarding the lack of 32-bit support as we have no interests of making any releases targeting 32-bit hardware.\nQ: Why don't you include ESU updates?\nA: ESU updates will be rolled out by Microsoft until 2026, and unfortunately adding ESU updates will be time consuming as it requires us to roll out new releases every time a batch of ESU updates gets rolled out.\nQ: Why VxKex instead of the dotexe1337 Windows 7 Extended Kernel?\nA: VxKex is safer than dotexe's Windows 7 Extended Kernel as it relies on external DLLs.\nQ: Does this include custom integrated GPU dri-\nA: NO\nQ: Why aren't my drives appearing in the Windows Setup screen?\nA: If you are using an RAID/RST configuration, it may well likely be that the drivers installed do not work.\nOther credits\n\nK4sum1: Inspiration for creating Windows 7 SP2\nGMM2003: \"Under construction\" wallpaper used in Public Beta builds\nOur contributors\nAnyone who reports bugs constantly",
    "summary": {
      "en": "**Windows 7 Service Pack 2 Summary**\n\nWindows 7 Service Pack 2 is an ongoing project aimed at updating and enhancing Windows 7 for better usability on modern machines. It includes various updates, new features, and software improvements.\n\n**Key Features:**\n- Updates up to 2020, including those for Windows Embedded Standard 7.\n- New applications like Snipping Tool, Paint, and Task Manager from newer Windows versions.\n- Support for USB 3.0, NVMe, TPM 2.0, and UEFI booting.\n- Enhanced graphics and DPI support.\n- Integration of Microsoft Visual C++ and other tools.\n\n**Installation Notes:**\n- The project currently only supports 64-bit systems; 32-bit support is not planned.\n- ISO files are for clean installations only.\n- Users must follow installation instructions carefully to avoid system issues.\n\n**FAQs:**\n- 32-bit hardware is not supported due to the complexity of maintaining two versions.\n- ESU updates are not included as they will continue to be provided by Microsoft until 2026.\n- VxKex is preferred over other extended kernels for safety reasons.\n\n**Important Warnings:**\n- Bugs may occur, and users are encouraged to report them.\n- Improper installation can lead to system failures.\n\nThis project is still under development, so users should expect ongoing changes and updates.",
      "ko": "Windows 7 서비스 팩 2는 현대 기기에서 더 나은 사용성을 위해 Windows 7을 업데이트하고 개선하는 프로젝트입니다. 이 프로젝트는 다양한 업데이트, 새로운 기능, 소프트웨어 개선을 포함하고 있습니다.\n\n주요 특징으로는 2020년까지의 업데이트가 포함되며, Windows Embedded Standard 7에 대한 업데이트도 포함됩니다. 또한, 최신 Windows 버전에서 제공되는 스니핑 도구, 페인트, 작업 관리자와 같은 새로운 애플리케이션이 추가됩니다. USB 3.0, NVMe, TPM 2.0, UEFI 부팅을 지원하며, 그래픽과 DPI 지원이 향상되었습니다. Microsoft Visual C++와 기타 도구의 통합도 이루어졌습니다.\n\n설치에 관한 주의사항으로는 현재 이 프로젝트가 64비트 시스템만 지원하며, 32비트 지원은 계획되어 있지 않습니다. ISO 파일은 깨끗한 설치를 위한 것이며, 사용자는 시스템 문제를 피하기 위해 설치 지침을 주의 깊게 따라야 합니다.\n\n자주 묻는 질문에서는 32비트 하드웨어는 두 가지 버전을 유지하는 복잡성 때문에 지원되지 않으며, ESU 업데이트는 2026년까지 Microsoft에서 계속 제공될 예정입니다. 안전상의 이유로 VxKex가 다른 확장 커널보다 선호됩니다.\n\n중요한 경고로는 버그가 발생할 수 있으며, 사용자는 이를 보고해 주기를 권장합니다. 부적절한 설치는 시스템 고장을 초래할 수 있습니다. 이 프로젝트는 아직 개발 중이므로, 사용자들은 지속적인 변화와 업데이트를 기대해야 합니다.",
      "ja": null
    }
  },
  {
    "id": "0139bf0faee41001",
    "title": {
      "en": "The mysterious flow of fluid in the brain",
      "ko": "뇌 속 신비한 유체 흐름",
      "ja": null
    },
    "type": "story",
    "url": "https://www.quantamagazine.org/the-mysterious-flow-of-fluid-in-the-brain-20250326/",
    "score": 133,
    "by": "pseudolus",
    "time": 1743035507,
    "content": "Quanta Homepage\n\n                                        Physics\n\n                                        Mathematics\n\n                                        Biology\n\n                                        Computer Science\n\n                                        Topics\n\n                                        Archive\n\n                                        Blog\n\n                                        Columns\n\n                                        Interviews\n\n                                        Podcasts\n\n                                        Puzzles\n\n                                        Multimedia\n\n                                        Videos\n\n                                        About Quanta\n\n                                    An editorially independent publication supported by the Simons Foundation.\n\n                                    Follow Quanta\n\n    Facebook\n\n        Youtube\n\n        Instagram\n\n    RSS\n\n                Newsletter\n\n                    Get the latest news delivered to your inbox.\n\n                            Email\n\n                        Subscribe\n\n                        Recent newsletters\n\n                                    Gift Store\n\n                                        Shop Quanta gear\n\nNewsletter\n\n                    Get the latest news delivered to your inbox.\n\n                            Email\n\n                        Subscribe\n\n                        Recent newsletters\n\nQuanta Homepage\n\n                                        Physics\n\n                                        Mathematics\n\n                                        Biology\n\n                                        Computer Science\n\n                                        Topics\n\n                                        Archive\n\n        Saved articles\n\n                    Saved Articles\n                                            Create a reading list by clicking the Read Later icon next to the articles you wish to save.\n\n                            See all saved articles\n\n        Login\n\n                    Log out\n\n                    Change password\n\n                                Search\n\nHome\n\n                The Mysterious Flow of Fluid in the Brain\n\n        Comment\n                1\n\n        Save Article\n\n                    Read Later\n\n                                                Share\n\n    Facebook\n\n                            Copied!\n\n    Copy link\n         (opens a new tab)\n\n    Email\n\n    Pocket\n\n    Reddit\n\n    Ycombinator\n\nphysiology\n    The Mysterious Flow of Fluid in the Brain\n\n        By\n\n                Veronique Greenwood\n\nMarch 26, 2025\n\n            A popular hypothesis for how the brain clears molecular waste, which may help explain why sleep feels refreshing, is a subject of debate.\n\n        Comment\n                1\n\n        Save Article\n\n                    Read Later\n\nphysiology\n    The Mysterious Flow of Fluid in the Brain\n\n        By\n\n                Veronique Greenwood\n\nMarch 26, 2025\n\n            A popular hypothesis for how the brain clears molecular waste, which may help explain why sleep feels refreshing, is a subject of debate.\n\n        Comment\n                1\n\n        Save Article\n\n                    Read Later\n\nNo one knows why cerebrospinal fluid circulates through and around our brains, or what directs its flow.\n\n    Chanelle Nibbelink forQuanta Magazine\n\nEncased in the skull, perched atop the spine, the brain has a carefully managed existence. It receives only certain nutrients, filtered through the blood-brain barrier; an elaborate system of protective membranes surrounds it. That privileged space contains a mystery. For more than a century, scientists have wondered: If it’s so hard for anything to get into the brain, how does waste get out?\nThe brain has one of the highest metabolisms of any organ in the body, and that process must yield by-products that need to be removed. In the rest of the body, blood vessels are shadowed by a system of lymphatic vessels. Molecules that have served their purpose in the blood move into these fluid-filled tubes and are swept away to the lymph nodes for processing. But blood vessels in the brain have no such outlet. Several hundred kilometers of them, all told, seem to thread their way through this dense, busily working tissue without a matching waste system.\nHowever, the brain’s blood vessels are surrounded by open, fluid-filled spaces. In recent decades, the cerebrospinal fluid, or CSF, in those spaces has drawn a great deal of interest. “Maybe the CSF can be a highway, in a way, for the flow or exchange of different things within the brain,” said Steven Proulx, who studies the CSF system at the University of Bern.\nA recent paper in Cell contains a new report about what is going on around the brain (opens a new tab) and in its hidden cavities. A team at the University of Rochester led by the neurologist Maiken Nedergaard (opens a new tab) asked whether the slow pumping of the brain’s blood vessels might be able to push the fluid around, among, and in some cases through cells, to potentially drive a system of drainage. In a mouse model, researchers injected a glowing dye into CSF, manipulated the blood vessel walls to trigger a pumping action, and saw the dye concentration increase in the brain soon after. They concluded that the movement of blood vessels might be enough to move CSF, and possibly the brain’s waste, over long distances.\nThe team took a further step in their interpretation. Because this kind of pumping — distinct from the familiar pulse from the heart — is regularly observed during sleep, they suggest that perhaps their observations can help explain why sleep feels refreshing. But it’s a hypothesis that not everyone agrees is well founded (opens a new tab). When it comes to ascribing purpose to the fluid moving through the brain, many researchers believe that the truth is still elusive.\nBrain Drain\nAt the center of the brain are flooded caverns, like great cisterns shrouded in darkness, called ventricles. Cerebrospinal fluid seeps from the ventricle walls and then moves. Under pressure, it emerges elsewhere within the skull, flows down the neck and enters the spine.\n\nThe neurologist Maiken Nedergaard’s “glymphatic hypothesis” proposes that cerebrospinal fluid helps drain waste from the brain during sleep. Her evidence is highly debated.\n\n    Adam Fenster, University of Rochester\n\nScientists have known for more than a century that, at the moment of death (opens a new tab), CSF flows from the spine into the brain. This suggests that the living brain somehow keeps the stuff moving, but no one knows exactly how or where it flows. Any arrows drawn on diagrams of the brain and skull to show its movement should not be taken as the complete truth.\n“Everyone accepts that there must be some kind of flow here,” said Christer Betsholtz (opens a new tab), a professor of vascular biology at the Karolinska Institute in Sweden. “About half a liter of CSF is produced in the ventricles every day, and it has to get out. People are still fighting about where the cerebrospinal fluid gets out.”\n\nAlso under discussion is whether it picks up waste on the way out of the brain and, crucially, how. There is good evidence that small molecules, at least, can diffuse through the spaces between cells, make their way to the CSF, and ride it out of the brain (opens a new tab). In fact, some researchers believe that the entire system works by way of passive diffusion.\nIn 2012, results from Nedergaard’s lab suggested a more active process. Nedergaard, along with the neurologist Jeffrey Iliff (opens a new tab), then a postdoc in her lab, and their colleagues, injected a tracer into cerebrospinal fluid (opens a new tab) and watched it quickly arrive elsewhere. How did it get from one place to another? They proposed that the spaces around blood vessels commune with even smaller spaces deep in the brain, between individual cells. They also suggested that CSF moves through brain cells called astrocytes into those spaces. There, the fluid might drop off some molecules and pick up others; it may then wend its way back out to the spaces around blood vessels, and thence move waste out of the brain. All of this would have to be driven by a flow of uncertain mechanism.\nIt was a striking idea. Nedergaard, who is the senior author of the new paper, and colleagues soon made it more striking by linking it to another mystery: why sleep seems to be beneficial. In a 2013 paper, her team wrote that there was more movement of cerebrospinal fluid (opens a new tab) in sleeping and anesthetized mice than in waking ones — and that perhaps during sleep CSF sweeps waste out of the brain. Maybe this “brainwashing,” as headlines described it, could provide one reason why sleep is necessary (opens a new tab), and explain how much better we feel after a good night of it.\n\nMark Belan/Quanta Magazine\n\n“I’m of the strong belief that the restorative part of sleep is not memory consolidation,” Nedergaard said. “Maybe it is partly. But it is really the housekeeping function of sleep that is important.”\nIn the years since those initial studies, a large number of papers (opens a new tab) referencing this brain-drainage theory, called the glymphatic hypothesis, have been published. It’s a catchy idea, but parts of the story raise red flags (opens a new tab) to some researchers who study the brain’s vasculature.\nAlan Verkman (opens a new tab), a professor emeritus at the University of California, San Francisco who studies fluid flow in the body, has argued that some aspects of the theory are physically implausible — for instance, the channels said to let the fluid in cannot actually play the role demanded of them. According to Betsholtz, there is no evidence that fluid is moving into the spaces around blood vessels that leave the brain.\nBut many other researchers appear to have accepted the glymphatic hypothesis. That’s because it fills a hole in our understanding of the brain, said Donald McDonald (opens a new tab), who studies blood and lymph vessels at the UCSF School of Medicine. Personally, he doesn’t feel that the theory holds water, but he acknowledges its popularity. It fits comfortably in the space where there is a mystery.\nEbb and Flow\n\n            Any arrows drawn on diagrams of the brain and skull to show the fluid’s movement should not be taken as the complete truth.\n\nImagine a sealed bottle of water. To study that fluid in its natural state, you have to cut a hole in the bottle. This is the difficulty that scientists studying CSF flow have to deal with. “If you are studying a fluid and you put a hole in the system, you really change it,” said Laura Lewis (opens a new tab), a professor of neuroscience at the Massachusetts Institute of Technology. “Fluid dynamics are really easily disturbed by invasive procedures.” Further, so many behaviors that living animals perform, such as breathing and having a heartbeat, directly affect the fluid.\nBuilding a case for a new hypothesis in this area, then, is tricky. In the Nedergaard group’s recent Cell paper, the team wanted to explore an intriguing connection (opens a new tab) that would not only explain how CSF could be pumped between brain cells, but also link that process to sleep.\nFor the study, mice underwent surgery to have sensors, wires and tubes implanted within the brain — one way to study the bottle of water. The researchers’ goal was to inject tracer dye into CSF at one point in the brain and then track its oscillations and dynamics while the mice slept.\nThe data showed that, while mice were in their non–rapid eye movement (NREM) phase of sleep, the concentration of tracer moved rhythmically. From a sensor perched above the brain surface, the researchers saw a pattern of increases and decreases, according to first author Natalie Haugland. “It had this wave pattern.”\nWhat could be driving this rhythmic flow? The researchers thought of the neurotransmitter norepinephrine, which causes blood vessels to constrict. “Norepinephrine is very well known for controlling blood flow,” Nedergaard said. It’s possible, they thought, that vessels constricting and relaxing could put enough force on the surrounding cerebrospinal fluid to push it through the brain’s tissues.\n\nResearch led by Natalie Haugland suggests that pulses of norepinephrine help pump cerebrospinal fluid through the brain during non-REM sleep.\n\n    Björn Sigurdsson\n\nWhat’s more, during NREM sleep norepinephrine levels change rhythmically. This neurotransmitter could help tie together their hypotheses — the physical movement of CSF through brain tissues and the “brainwashing” occurring during sleep.\nThe team engineered mice in which they could switch the production of the neurotransmitter on and off. When norepinephrine levels went up, the volume of CSF in the brain went up, they saw, suggesting that it was somehow altering the fluid’s flow.\n\n            All of this would have to be driven by a flow of uncertain mechanism.\n\nThen, to test whether the pumping of blood vessels could move CSF, the team engineered mice with blood vessel walls they could manipulate directly. Instead of pumping the vessels slowly, as happens naturally, they moved the walls quickly — once every 10 seconds rather than once every 50. “When we did this, we increased CSF flow on one side of the brain” in a very small area where they were pumping, Haugland said. “It was very local. … Everywhere else in the brain it was the same.”\nFor Nedergaard, Haugland and their collaborators, the findings tie together norepinephrine, the physical movement of blood vessels, and the flow of CSF in the brain. Nedergaard also asserts that the results are consistent with her group’s earlier finding that there is more brain drainage during sleep than during wakefulness.\n“We have been searching for why the glymphatic [system] primarily works when we sleep for a long time,” Nedergaard said. “The paper is really about: Now we’ve found the motor or the driver of how we wash the brain when we sleep.”\nHowever, to critics of the theory, there are still too many open spaces.\nUnder Pressure\n\nMcDonald, of the UCSF School of Medicine, pointed out that the work is complex and requires many intricate methods. However, he’s concerned that Nedergaard is working backward: seeking an explanation for her hypothesis rather than trying to find out how the system actually works. “In this paper, it’s unclear what is interpretation and what is data,” he said. “Very early on, their interpretation gets substituted for what actually are the data.” He pointed to schematics showing flow dynamics that he doesn’t see supported, for instance.\nProulx questioned whether the tracer dye moved via an active force at all. The molecule is so small that it could be traveling by diffusion, he said. He imagines an experiment, using techniques Nedergaard’s lab has used before, where a large molecule is infused into the CSF. If the rhythmic releases of norepinephrine correlate with the arrival of a larger tracer at a sensor on the brain’s surface, that would be a fascinating finding. “That’s what I would have liked to have seen,” he said. To his eye, it would make a clearer connection between fluid flow and norepinephrine than the lab’s work has shown thus far.\nThe critiques of Nedergaard’s work come on strong in part because this idea is currently the most prominent hypothesis of CSF flow in the brain. That may change if other researchers can introduce other ideas that can be tested. Another wrinkle is that not everyone means the same thing when they talk about the glymphatic system. “Some people use ‘glymphatics’ to mean ‘waste transport system of the brain.’ Other people use it to mean a really specific mechanistic model,” Lewis said. “It’s clear that the brain has and needs a waste clearance system. … It’s really interesting to explore what that is and how that works.”\n\n                Related:\n\n                                    How the Brain Protects Itself From Blood-Borne Threats\n\n                                    Sleep Evolved Before Brains. Hydras Are Living Proof.\n\n                                    Why Do We Die Without Sleep?\n\nHaugland, now a postdoc at the University of Oxford, is aware of the controversy about the glymphatic hypothesis. “There is critique of it. I’m also not sure that we understand it in the right way,” she said. “The more people who are actually working on finding out how it works, no matter what their hypothesis is — all that will help drive the field forward and give us more knowledge.\n“The results are what they are. They show something about the biology,” she continued. “We are trying to ask a lot of questions and we’re not, maybe, all the time very good at it because we don’t know how it works — the big picture.”\n“Nobody has the truth,” Proulx said, about what the brain is doing up there, in our skulls, to rid itself of its waste. “Some people think they know. But I think we don’t know.”\n\nBy Veronique Greenwood\n                Contributing Writer\n\n                March 26, 2025\n\n                    View PDF/Print Mode\n\n                            biology\n\n                            brains\n\n                            explainers\n\n                            metabolism\n\n                            neuroscience\n\n                            physiology\n\n                            sleep\n\n                    All topics\n\n     (opens a new tab)\n\nShare this article\n\n    Facebook\n\n                            Copied!\n\n    Copy link\n         (opens a new tab)\n\n    Email\n\n    Pocket\n\n    Reddit\n\n    Ycombinator\n\n                    Newsletter\n\n                    Get Quanta Magazine delivered to your inbox\n\n                    Subscribe now\n\n                    Recent newsletters\n\n             (opens a new tab)\n\nThe Quanta Newsletter\n\n                    Get highlights of the most important news delivered to your email inbox\n\n                            Email\n\n                        Subscribe\n\n                        Recent newsletters\n                                             (opens a new tab)\n\nAlso in Biology\n\n                    How Metabolism Can Shape Cells’ Destinies\n\n                developmental biology\n\n                    How Metabolism Can Shape Cells’ Destinies\n\n        By\n\n                Viviane Callier\n\n            March 21, 2025\n\n        Comment\n\n        Save Article\n\n                    Read Later\n\n                    How Did Multicellular Life Evolve?\n\n                The Joy of Why\n\n                    How Did Multicellular Life Evolve?\n\n        By\n\n                    Janna Levin\n\n                 +1 authors\n\n                        Steven Strogatz\n\n            March 20, 2025\n\n        Comment\n                1\n\n        Save Article\n\n                    Read Later\n\n                    A New, Chemical View of Ecosystems\n\n                ecology\n\n                    A New, Chemical View of Ecosystems\n\n        By\n\n                Molly Herring\n\n            March 5, 2025\n\n        Comment\n                3\n\n        Save Article\n\n                    Read Later\n\nComment on this article\n\n                    Quanta Magazine moderates comments tofacilitate an informed, substantive, civil conversation. Abusive, profane, self-promotional, misleading, incoherent or off-topic comments will be rejected. Moderators are staffed during regular business hours (New York time) and can only accept comments written in English.\n\n        Show comments\n\nNext article\n                Three Hundred Years Later, a Tool from Isaac Newton Gets an Update\n\nQuanta Homepage\n\n    Facebook\n\n        Youtube\n\n        Instagram\n\n                                        About Quanta\n\n                                        Archive\n\n                                        Contact Us\n\n                                        Terms & Conditions\n\n                                        Privacy Policy\n\n                        All Rights Reserved © 2025\n\n                    An editorially independent publication supported by the Simons Foundation.\n\n                        Simons Foundation\n\nQuanta Homepage\n\n    Facebook\n\n        Youtube\n\n        Instagram\n\nAbout Quanta\n\n                                        Archive\n\n                                        Contact Us\n\n                                        Terms & Conditions\n\n                                        Privacy Policy\n\nAll Rights Reserved © 2025\n\n                    An editorially independent publication supported by the Simons Foundation.\n\n                        Simons Foundation\n\nClose\n\n    Log in to Quanta\n\n            Use your social network\n\n                        Facebook                        Connect with Facebook\n\n                                                Connect with Google\n\n            or\n\n                    email\n\n                    password\n\n                    Remember me\n\n                        Forgot your password ?\n\n    Don't have an account yet?\n         Sign up\n\nClose\n\n        Forgot your password?\n        We’ll email you instructions to reset your password\n\n                email\n\nClose\n\n        Change your password\n        Enter your new password\n\n                Password\n\n                Retype new password\n\nClose\n\n        Sign Up\n\n                    First Name\n\n                    Last Name\n\n                    Email\n\n                    Password\n\n                    Retype Password\n\n            Creating an account means you accept Quanta Magazine's\n\n            Terms & Conditions and Privacy Policy",
    "summary": {
      "en": "The article discusses the mysterious flow of cerebrospinal fluid (CSF) in the brain, which is essential for clearing waste but not fully understood. Scientists have long debated how waste is removed from the brain since it lacks a traditional lymphatic system found in the rest of the body.\n\nRecent research led by Maiken Nedergaard suggests that the slow pumping of blood vessels may help move CSF, potentially aiding in waste removal, particularly during sleep. This idea, known as the \"glymphatic hypothesis,\" proposes that CSF acts as a drainage system for waste, especially when we sleep, making sleep feel refreshing.\n\nDespite its popularity, the glymphatic hypothesis faces skepticism. Critics argue that some aspects are not physically plausible, and there is still no clear understanding of how CSF flows in the brain or how it interacts with brain cells to remove waste. Researchers continue to investigate this complex system, acknowledging that more studies are needed to uncover the true mechanisms at play.\n\nOverall, while there are promising ideas about how the brain clears waste, the exact processes remain largely a mystery, and scientists are still exploring various theories to understand brain physiology better.",
      "ko": "이 기사는 뇌에서의 뇌척수액(CSF) 흐름에 대한 신비로운 사실을 다룹니다. 뇌척수액은 노폐물을 제거하는 데 필수적이지만, 그 작용은 완전히 이해되지 않고 있습니다. 과학자들은 뇌가 신체의 다른 부분에 있는 전통적인 림프계가 없기 때문에 노폐물이 어떻게 제거되는지에 대해 오랫동안 논의해왔습니다.\n\n최근 마이켄 네더가르드가 이끄는 연구에 따르면, 혈관의 느린 펌핑이 뇌척수액을 이동시키는 데 도움을 줄 수 있으며, 이는 특히 수면 중 노폐물 제거에 기여할 수 있다고 합니다. 이 개념은 \"글림프 시스템 가설\"로 알려져 있으며, 뇌척수액이 노폐물의 배수 시스템 역할을 한다고 제안합니다. 특히 우리가 잠을 잘 때 이 과정이 이루어져, 수면이 상쾌하게 느껴지게 만든다고 합니다.\n\n하지만 글림프 시스템 가설은 비판도 받고 있습니다. 비평가들은 이 가설의 일부 측면이 물리적으로 불가능하다고 주장하며, 뇌척수액이 뇌에서 어떻게 흐르는지, 그리고 뇌세포와 어떻게 상호작용하여 노폐물을 제거하는지에 대한 명확한 이해가 부족하다고 지적합니다. 연구자들은 이 복잡한 시스템을 계속 조사하고 있으며, 진정한 작용 메커니즘을 밝혀내기 위해 더 많은 연구가 필요하다고 인정하고 있습니다.\n\n전반적으로 뇌가 노폐물을 제거하는 방법에 대한 유망한 아이디어가 있지만, 그 정확한 과정은 여전히 미스터리로 남아 있으며, 과학자들은 뇌 생리학을 더 잘 이해하기 위해 다양한 이론을 탐구하고 있습니다.",
      "ja": null
    }
  },
  {
    "id": "2bfa44bc1d84798a",
    "title": {
      "en": "Trapping misbehaving bots in an AI Labyrinth",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://blog.cloudflare.com/ai-labyrinth/",
    "score": 228,
    "by": "pabs3",
    "time": 1742391067,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "7fa226e6fe29e884",
    "title": {
      "en": "Show HN: Formal Verification for Machine Learning Models Using Lean 4",
      "ko": "기계 학습 검증, Lean 4로!",
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/fraware/leanverifier",
    "score": 49,
    "by": "MADEinPARIS",
    "time": 1742755513,
    "content": "Formal Verification of Machine Learning Models in Lean\nWelcome to the Formal Verification of Machine Learning Models in Lean project. This repository provides a framework for specifying and proving properties—such as robustness, fairness, and interpretability—of machine learning models using Lean 4.\nA live, interactive webpage is available at: proof-pipeline-interactor.lovable.app\nOverview\nIn high-stakes applications (e.g., healthcare, finance, autonomous systems), ensuring that machine learning models meet strict reliability and fairness properties is essential. This project provides:\n\nLean Library: Formal definitions for neural networks, linear models, decision trees, and advanced models (ConvNets, RNNs, Transformers), along with properties like adversarial robustness, fairness, interpretability, monotonicity, and sensitivity analysis.\nModel Translator: A Python-based tool that exports trained models (e.g., from PyTorch) to a JSON schema and automatically generates corresponding Lean code.\nWeb Interface: A Flask application for uploading models, triggering Lean verification, visualizing model architectures (using Graphviz), and viewing proof logs.\nCI/CD Pipeline: A reproducible, Dockerized environment using Lean 4’s Lake build system with GitHub Actions for continuous integration and deployment.\n\nFeatures\n\nFormal Verification: Prove key properties of ML models including adversarial robustness and fairness.\nAdvanced Model Support: Extendable to support convolutional networks, recurrent architectures, transformers, and even symbolic models.\nInteractive Web Portal: Upload model JSON files, view generated Lean code, trigger Lean proof compilation, and visualize the model architecture.\nAutomated Build Pipeline: Docker and GitHub Actions integration for reliable, reproducible builds.\n\nQuick Start\n\nClone the Repository:\ngit clone https://github.com/fraware/formal_verif_ml.git\ncd formal_verif_ml\n\nBuild the Docker Image:\ndocker build -t formal-ml .\n\nRun the Container:\ndocker run -p 5000:5000 formal-ml\n\nAccess the Web Interface:\n\nOpen http://localhost:5000 in your browser.\nFor detailed usage and contribution guidelines, please refer to the User Guide and Developer Guide.\nContributing\nContributions, improvements, and bug reports are welcome. Please see the docs/ folder for additional developer guidelines and contribution standards.\nLicense\nThis project is licensed under the MIT License.",
    "summary": {
      "en": "**Summary: Formal Verification of Machine Learning Models in Lean**\n\nThis project focuses on verifying machine learning models for important properties like reliability and fairness using Lean 4. It aims to ensure that models, especially in critical fields like healthcare and finance, function correctly and ethically.\n\n**Key Features:**\n- **Lean Library:** Offers formal definitions for various machine learning models (e.g., neural networks, decision trees) and their properties (e.g., robustness, fairness).\n- **Model Translator:** A Python tool that converts trained models from formats like PyTorch to a JSON schema and generates Lean code.\n- **Web Interface:** A user-friendly platform for uploading models, initiating verification, and visualizing model architectures.\n- **CI/CD Pipeline:** Uses Docker and GitHub Actions for consistent and reliable development and deployment.\n\n**Getting Started:**\n1. Clone the repository.\n2. Build the Docker image.\n3. Run the container and access the web interface at http://localhost:5000.\n\nContributions and feedback are encouraged, and the project is licensed under the MIT License. For more details, refer to the User and Developer Guides.",
      "ko": "이 프로젝트는 Lean 4를 사용하여 기계 학습 모델의 신뢰성과 공정성과 같은 중요한 속성을 검증하는 데 중점을 두고 있습니다. 특히 의료와 금융과 같은 중요한 분야에서 모델이 올바르고 윤리적으로 작동하도록 보장하는 것을 목표로 합니다.\n\nLean 라이브러리는 다양한 기계 학습 모델(예: 신경망, 결정 트리)과 그 속성(예: 강건성, 공정성)에 대한 공식적인 정의를 제공합니다. 모델 변환기는 PyTorch와 같은 형식에서 훈련된 모델을 JSON 스키마로 변환하고 Lean 코드를 생성하는 파이썬 도구입니다. 웹 인터페이스는 모델을 업로드하고 검증을 시작하며 모델 구조를 시각화할 수 있는 사용자 친화적인 플랫폼을 제공합니다. CI/CD 파이프라인은 Docker와 GitHub Actions를 사용하여 일관되고 신뢰할 수 있는 개발 및 배포를 지원합니다.\n\n시작하려면 먼저 저장소를 복제하고 Docker 이미지를 빌드한 후, 컨테이너를 실행하여 http://localhost:5000에서 웹 인터페이스에 접근하면 됩니다. 기여와 피드백은 환영하며, 이 프로젝트는 MIT 라이선스 하에 배포됩니다. 더 자세한 내용은 사용자 및 개발자 가이드를 참조하시기 바랍니다.",
      "ja": null
    }
  },
  {
    "id": "e6ecee51680ba393",
    "title": {
      "en": "NotaGen: Symbolic Music Generation",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://electricalexis.github.io/notagen-demo/",
    "score": 97,
    "by": "explosion-s",
    "time": 1742738509,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "5421576565d00295",
    "title": {
      "en": "DJ With Apple Music launches to enable subscribers to mix their own sets",
      "ko": "애플뮤직 DJ 서비스 런칭!",
      "ja": null
    },
    "type": "story",
    "url": "https://www.musicweek.com/digital/read/dj-with-apple-music-launches-to-enable-subscribers-to-mix-their-own-sets/091655",
    "score": 190,
    "by": "CharlesW",
    "time": 1743036690,
    "content": "DJ With Apple Music launches to enable subscribers to mix their own sets\n\n            by Andre Paine\n\tMarch 25th 2025\t\tat 2:00PM\n\n                    Apple has introduced DJ with Apple Music, which enablesApple Music subscribers to build and mix sets directly from the streaming services catalogue.\nThe feature is integrated with DJ software and hardware platforms AlphaTheta, Serato and InMusic’s Engine DJ, Denon DJ, Numark, and Rane DJ. It builds on an integration with Algoriddim's Djay Pro software.\nDJs at all levels will have access to Apple Music’s catalogue of more than 100 million songs to create and rehearse their sets on any of these DJ software and hardware platforms\n“Apple Music is committed to supporting DJs,” said Stephen Campbell, global head of dance, electronic & DJ mixes at Apple Music. “With this latest integration, we’re taking that commitment even further – seamlessly connecting Apple Music with the industry’s leading DJ software and hardware. This innovation brings the full power of Apple Music into the creative workflow, making it easier than ever for DJs to access, play, and discover music in real time.”\nUsers can also access a new dedicated DJ with Apple Music category page, spotlighting a series of DJ-friendly editorial playlists, along with new curator pages for each DJ software and hardware platform that will showcase any mixes or sample playlists that can be used to practice.\n\"The integration of Djay with Apple Music across mobile, desktop, and spatial devices opens up a world of creative possibilities for both beginners and seasoned pros,” said Karim Morsy, CEO at Algoriddim. “With instant access to Apple Music’s catalogue of over 100 million songs, DJs can mix anytime, anywhere – transforming the way they discover and play their favorite music. Whether using Automix for a seamless, hands-free experience or crafting their own unique sets with djay’s powerful mixing tools, this integration marks a major milestone in making DJing more accessible than ever.”\n\nApple Music is committed to supporting DJs\nStephen Campbell\n\n“We are thrilled to bring this integration with Apple Music to DJs around the world and in their creative process,\" said Yoshinori Kataoka, president and CEO of AlphaTheta. \"This marks a significant step forward in making DJing more accessible, and we couldn’t be more excited about the possibilities it opens up.”\n\"At inMusic, our goal has always been to empower DJs with innovative tools that enhance creativity and performance,” said Morgan Donoghue, VP marketing, DJ brands at InMusic. “Integrating Apple Music into our Engine DJ platforms marks a major step forward, giving DJs instant access to a vast library of over 100 million tracks and expertly curated playlists to build and refine their sets with ease. This collaboration ensures DJs at every level can seamlessly discover, play, and integrate new music into their creative workflow.\"\n“Teaming up with Apple Music is a milestone moment for Serato’s artist community,” said Young Ly, CEO at Serato. “We are excited to see established and new DJs alike combine their passion and creativity with access to one of the largest streaming catalogues in the world.”\nDJ with Apple Music builds upon Apple Music’s DJ Mixes, a programme that launched on the platform in September 2021 with a collection of thousands of sets.\nIn December, Apple Music also launched Apple Music Club, a live global radio station with curated DJ mixes.\n“Today’s launch aims to expand on that relationship by encouraging DJs to use this integrated experience as a creative tool, allowing them to access and find inspiration in their libraries more easily,” concluded the statement.\n\n                         FOLLOW\n\t\t\t\t\t\tAndre Paine\n\n    // Get the current browser URL and title\n    var fullUrl = window.location.href;\n    var pageTitle = document.title;\n\n    // LinkedIn share link setup\n    document.querySelector('.linkedin-share-link').addEventListener('click', function(event) {\n        event.preventDefault();\n        var linkedinShareUrl = 'https://www.linkedin.com/shareArticle?mini=true&url=' + encodeURIComponent(fullUrl) + '&title=' + encodeURIComponent(pageTitle) + '&summary=' + encodeURIComponent(pageTitle);\n        window.open(linkedinShareUrl, '_blank', 'width=600,height=400');\n    });\n\n\t/*document.querySelector('.linkedin-share-link').addEventListener('click', function(event) {\n\tevent.preventDefault();\n\n\tvar fullUrl = window.location.href; // Get the current page URL\n\tvar pageTitle = document.title; // Get the current page title\n\n\tvar linkedinShareUrl = 'https://www.linkedin.com/shareArticle?mini=true&url=' + encodeURIComponent(fullUrl) +\n\t'&title=' + encodeURIComponent(pageTitle) +\n\t'&summary=' + encodeURIComponent(pageTitle);\n\n\twindow.open(linkedinShareUrl, '_blank', 'width=600,height=400');\n\t});*/\n\n    // Facebook share link setup\n    document.querySelector('.facebook-share-link').addEventListener('click', function(event) {\n        event.preventDefault();\n        var facebookShareUrl = 'https://www.facebook.com/sharer/sharer.php?u=' + encodeURIComponent(fullUrl);\n        window.open(facebookShareUrl, '_blank', 'width=600,height=400');\n    });\n\n    // Generic social share links for Twitter and LinkedIn\n    document.querySelectorAll('.social-share-link').forEach(function(el) {\n        el.addEventListener('click', function(event) {\n            event.preventDefault();\n            var service = this.getAttribute('data-service');\n            var shareUrl = '';\n\n            if (service === 'twitter') {\n                shareUrl = 'https://twitter.com/intent/tweet?url=' + encodeURIComponent(fullUrl) + '&text=' + encodeURIComponent(pageTitle);\n            }\n\n            window.open(shareUrl, '_blank', 'width=600,height=400');\n        });\n    });\n\n                        googletag.cmd.push(function () {\n                            googletag.display('article-advert');\n                        });\n\n                        googletag.cmd.push(function () {\n                            googletag.display('article-advert-mobile');\n                        });\n\n For more stories like this, and to keep up to date with all our market leading news, features and analysis, sign up to receive our daily Morning Briefing newsletter\n\n\t\t\tTags:\n                            dance music,\n                                electronic music,\n                                Apple Music,\n                                dj,\n                                Stephen Campbell,\n                                DJ With Apple Music\n\n\tRelated Content\n\nSoundCloud's Music Intelligence Report reveals consumption trends including top genres and new music\n\nWarner Music France launches dance label Adore Music\n\nAVA founder Sarah McBriar on how live music and club culture can thrive in the age of AI\n\nNTIA report reveals electronic music's economic impact",
    "summary": {
      "en": "Apple has launched a new feature called \"DJ with Apple Music,\" allowing subscribers to mix their own music sets using Apple Music's extensive library of over 100 million songs. This feature works with popular DJ software and hardware like Serato and Denon DJ, making it accessible for DJs of all skill levels.\n\nThe integration includes a dedicated section on Apple Music with DJ-friendly playlists and curated content. Key figures from collaborating companies expressed excitement about the feature, highlighting its potential to enhance creativity and accessibility for DJs. This initiative builds on previous Apple Music offerings like DJ Mixes and the Apple Music Club.\n\nOverall, the launch aims to provide DJs with innovative tools to discover and mix music seamlessly.",
      "ko": "애플이 \"DJ with Apple Music\"이라는 새로운 기능을 출시했습니다. 이 기능을 통해 구독자는 애플 뮤직의 1억 곡이 넘는 방대한 음악 라이브러리를 이용해 자신만의 음악 세트를 믹싱할 수 있습니다. 이 기능은 Serato와 Denon DJ와 같은 인기 있는 DJ 소프트웨어와 하드웨어와 호환되어, 모든 수준의 DJ들이 쉽게 사용할 수 있습니다.\n\n애플 뮤직에는 DJ 친화적인 플레이리스트와 큐레이션된 콘텐츠를 포함한 전용 섹션이 마련되어 있습니다. 협력사 주요 인사들은 이 기능에 대한 기대감을 표명하며, DJ들에게 창의성과 접근성을 높일 수 있는 잠재력을 강조했습니다. 이번 이니셔티브는 DJ 믹스와 애플 뮤직 클럽과 같은 이전의 애플 뮤직 서비스에 기반하고 있습니다.\n\n전반적으로 이번 출시의 목표는 DJ들이 음악을 쉽게 발견하고 믹싱할 수 있는 혁신적인 도구를 제공하는 것입니다.",
      "ja": null
    }
  },
  {
    "id": "2cabaee1145596a6",
    "title": {
      "en": "The long-awaited Friend Compound laws in California",
      "ko": "캘리포니아 친구법 시행",
      "ja": null
    },
    "type": "story",
    "url": "https://supernuclear.substack.com/p/the-long-awaited-friend-compound",
    "score": 146,
    "by": "simonebrunozzi",
    "time": 1742995311,
    "content": "Share this postSupernuclearThe long-awaited Friend Compound laws in CaliforniaCopy linkFacebookEmailNotesMoreDiscover more from SupernuclearHow to live near (and with) friendsOver 9,000 subscribersSubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.Already have an account? Sign inThe long-awaited Friend Compound laws in CaliforniaPhil LevinJan 11, 202578Share this postSupernuclearThe long-awaited Friend Compound laws in CaliforniaCopy linkFacebookEmailNotesMore106Share[X-posted from the Live Near Friends blog]It’s now much easier to build a friend compound in California.Supernuclear is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.SubscribeI used to tell people that their “all my friends on a plot of land” dream was impossible in California. Getting permits would be too much of a nightmare. This is no longer true.Here’s a breakdown of the two brand-new laws that makes it easier to build compounds with 4-10 homes, sometimes called Missing Middle Housing.SB 684 - Starter homes on micro-lotsThis is a formula for people owning their own starter homes on their own small lots right next to to each other.And to show you how much I believe in this one, my wife Kristen and I just purchased a property in Alameda to build our own future family compound using SB 684. We plan to turn a single lot into 6 individually-owned family-sized homes.Future site of 5 new homes, including mineI’ll likely go into more detail in a future post, but the basic idea is this:Step 1: Buy a home with a large lotStep 2: Subdivide it into smaller lots (up to 10!)Step 3: Sell the new lots to friendsStep 4: Friends build their own homesExample of how a $1m home becomes 4x $600-700k homes using SB 684Why this is so powerful is that people can get their own individual loans and sell/exit their own homes. No more tricky co-ownership arrangements with multiple people on a loan.And it means that one “sponsor” (in this case Kristen and I) can get something going without having to fund the capital for a whole big project. To get this going, we had to buy 1 home, not 6 of them.The law puts a shot clock on how long cities have to review applications. They need to get you comments back within 60 days (so no stalling on your permit for multiple years, as was common before). It also requires that they approve your proposal as long as it meets a well-defined set of guidelines, including size of units, setbacks, and resulting lot size.There are a few limitations you should be aware of: First, SB 684 only applies to parcels zoned multifamily (more on this below). Second, you generally need to maximize the allowable density on the parcel. So if the parcel allows for 5 units without SB 684, you will need to build at least 5 units. Third, the average square footage of a home can not be larger than 1750 sq ft, but this is plenty for 3 or even 4 bedrooms.There is a lot of fine print on this one and we'd be happy to connect you to experts who can help sort through it for a particular project.There is also a “clean up bill” coming in 2025 which will further liberalize these restrictions, including allowing projects on vacant single-family parcels (not just multifamily).SB 1211 - ADUs to the MAXExisting law typically allowed people to build 1-2 ADUs on a property in California. It meant turning a home into 2 or max 3 homes.SB 1211 is for ADU maximalists.With this law you can now build an equal number of ADUs to existing units. So a duplex can get 2 ADUs, a triplex can get 3 new ADUs and a quadplex can get 4 new ADUs. Up to 8 ADUs! And you can do this on parking spaces without replacing them.Real estate developers use this law to densify existing properties in cities like Oakland, Berkeley, and Mill Valley. They replace parking areas with housing. And ADUs don’t have to be standalone. You can stack multiples on top of each other or next to each other like townhomes.What to look for in a propertyThe elements that make properties a good candidate for these laws is a) large lot size (look for min 5000 and likely greater than 7000 sq ft) and b) multifamily zoning.Multifamily-zoned parcels are more abundant in the East Bay and San Francisco than the North Bay and Peninsula. But you can find opportunities throughout the Bay.Here are zoning maps of San Francisco, Oakland, Alameda, San Rafael, and Berkeley to give you a feel. Much of the \"flats\" of Oakland and Berkeley for example are zoned for multifamily housing.-Drop me a line if you have a project you are considering. Would be happy to help. phil@livenearfriends.com. Supernuclear is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.Subscribe78 Likes∙6 Restacks78Share this postSupernuclearThe long-awaited Friend Compound laws in CaliforniaCopy linkFacebookEmailNotesMore106Share",
    "summary": {
      "en": "California has introduced new laws that make it easier to build \"friend compounds,\" which are groups of homes for friends or families living close together. Here are the key points:\n\n1. **SB 684 - Starter Homes on Micro-Lots**: \n   - Allows people to buy a large property and split it into smaller lots for individual homes (up to 10).\n   - Each home can be owned separately, making it easier for friends to live together without complicated shared ownership.\n   - Cities must review applications within 60 days and approve them if they meet certain guidelines.\n   - The average home size can’t exceed 1,750 square feet.\n\n2. **SB 1211 - More Accessory Dwelling Units (ADUs)**:\n   - This law lets property owners build as many ADUs as they have existing housing units, potentially adding up to 8 ADUs.\n   - ADUs can be built in place of parking spots, allowing for denser housing.\n\n3. **Property Considerations**:\n   - Ideal properties for these laws have large lot sizes (at least 5,000 sq ft) and are zoned for multifamily use.\n   - These zoning opportunities are more common in areas like the East Bay and San Francisco.\n\nOverall, these laws aim to simplify the process of building homes for friends and families in California.",
      "ko": "캘리포니아에서는 친구나 가족이 가까이 살 수 있도록 집을 모아 지을 수 있는 \"친구 단지\"를 쉽게 만들 수 있는 새로운 법안을 도입했습니다. 주요 내용은 다음과 같습니다.\n\nSB 684는 소형 주택을 위한 법안으로, 사람들이 큰 부지를 사서 이를 최대 10개의 작은 부지로 나눌 수 있도록 허용합니다. 각 주택은 개별 소유가 가능해 친구들이 복잡한 공동 소유 없이 함께 살기 쉬워집니다. 도시에서는 신청서를 60일 이내에 검토하고 특정 기준을 충족하면 승인해야 합니다. 평균 주택 면적은 1,750 제곱피트를 초과할 수 없습니다.\n\nSB 1211은 부속 주택 유닛(ADU)을 더 많이 건설할 수 있도록 하는 법안입니다. 이 법에 따라 주택 소유자는 기존 주택 수만큼 ADU를 지을 수 있으며, 최대 8개의 ADU를 추가할 수 있습니다. ADU는 주차 공간 대신에 건설할 수 있어 주택 밀도를 높일 수 있습니다.\n\n이 법안이 적용되기 위해서는 적합한 부지가 필요하며, 최소 5,000 제곱피트의 넓은 면적을 가지고 있어야 하고 다가구 주택 용도로 지정되어 있어야 합니다. 이러한 용도 지역은 이스트 베이와 샌프란시스코와 같은 지역에서 더 흔하게 찾아볼 수 있습니다.\n\n이러한 법안들은 캘리포니아에서 친구와 가족을 위한 주택 건설 과정을 간소화하는 것을 목표로 하고 있습니다.",
      "ja": null
    }
  },
  {
    "id": "c958d0e9d9186976",
    "title": {
      "en": "Adopting the Ferrocene Language Specification",
      "ko": "페로센 언어 채택",
      "ja": null
    },
    "type": "story",
    "url": "https://blog.rust-lang.org/2025/03/26/adopting-the-fls.html",
    "score": 7,
    "by": "rascul",
    "time": 1743106540,
    "content": "Adopting the FLS\n\n    Mar. 26, 2025 · TC\n     on behalf of the Spec Team\n\n      Adopting the FLS\nSome years ago, Ferrous Systems assembled a description of Rust called the FLS1.  They've since been faithfully maintaining and updating this document for new versions of Rust, and they've successfully used it to qualify toolchains based on Rust for use in safety-critical industries.  Seeing this success, others have also begun to rely on the FLS for their own qualification efforts when building with Rust.\nThe members of the Rust Project are passionate about shipping high quality tools that enable people to build reliable software at scale.  Such software is exactly the kind needed by those in safety-critical industries, and consequently we've become increasingly interested in better understanding and serving the needs of these customers of our language and of our tools.\nIt's in that light that we're pleased to announce that we'll be adopting the FLS into the Rust Project as part of our ongoing specification efforts.  This adoption is being made possible by the gracious donation of the FLS by Ferrous Systems.  We're grateful to them for the work they've done in assembling the FLS, in making it fit for qualification purposes, in promoting its use and the use of Rust generally in safety-critical industries, and now, for working with us to take the next step and to bring the FLS into the Project.\nWith this adoption, we look forward to better integrating the FLS with the processes of the Project and to providing ongoing and increased assurances to all those who use Rust in safety-critical industries and, in particular, to those who use the FLS as part of their qualification efforts.\nThis adoption and donation would not have been possible without the efforts of the Rust Foundation, and in particular of Joel Marcey, the Director of Technology at the Foundation, who has worked tirelessly to facilitate this on our behalf.  We're grateful to him and to the Foundation for this support.  The Foundation has published its own post about this adoption.\nI'm relying on the FLS today; what should I expect?\nWe'll be bringing the FLS within the Project, so expect some URLs to change.  We plan to release updates to the FLS in much the same way as they have been happening up until now.\nWe're sensitive to the fact that big changes to this document can result in costs for those using it for qualification purposes, and we don't have any immediate plans for big changes here.\nWhat's this mean for the Rust Reference?\nThe Reference is still the Reference.  Adopting the FLS does not change the status of the Reference, and we plan to continue to improve and expand the Reference as we've been doing.\nWe'll of course be looking for ways that the Reference can support the FLS, and that the FLS can support the Reference, and in the long term, we're hopeful we can find ways to bring these two documents closer together.\n\nThe FLS stood for the \"Ferrocene Language Specification\".  The minimal fork of Rust that Ferrous Systems qualifies and ships to their customers is called \"Ferrocene\", hence the name.  We'll be dropping the expansion and just calling it the FLS within the Project. ↩\n\nThe FLS stood for the \"Ferrocene Language Specification\".  The minimal fork of Rust that Ferrous Systems qualifies and ships to their customers is called \"Ferrocene\", hence the name.  We'll be dropping the expansion and just calling it the FLS within the Project. ↩",
    "summary": {
      "en": "**Summary: Adopting the FLS into the Rust Project**\n\nFerrous Systems created the FLS (Ferrocene Language Specification) to describe Rust and has updated it for safety-critical industries. Many have successfully used the FLS for qualifying Rust toolchains. The Rust Project aims to support users in these industries and is pleased to announce the adoption of the FLS, thanks to Ferrous Systems' donation and support from the Rust Foundation.\n\nWith this adoption, the FLS will be integrated into the Rust Project, leading to some changes in URLs, but no major changes are planned for the document itself to avoid affecting users' qualification processes. The Rust Reference will remain unchanged, with ongoing improvements expected, while efforts will be made to better connect the FLS and the Reference in the future. \n\nOverall, this collaboration aims to enhance the reliability of Rust for safety-critical applications.",
      "ko": "Ferrous Systems는 Rust를 설명하기 위해 FLS(페로센 언어 사양)를 만들었고, 안전이 중요한 산업을 위해 이를 업데이트했습니다. 많은 사람들이 Rust 도구 체인을 인증하는 데 FLS를 성공적으로 사용했습니다. Rust 프로젝트는 이러한 산업의 사용자들을 지원하기 위해 FLS의 채택을 발표하게 되어 기쁩니다. 이는 Ferrous Systems의 기부와 Rust 재단의 지원 덕분입니다.\n\nFLS가 채택됨에 따라 Rust 프로젝트에 통합될 예정이며, 이로 인해 URL에 일부 변경이 있을 것입니다. 그러나 문서 자체에는 사용자 인증 과정에 영향을 주지 않기 위해 큰 변화가 계획되어 있지 않습니다. Rust 참조 문서는 변경되지 않으며, 지속적인 개선이 기대됩니다. 앞으로 FLS와 참조 문서 간의 연결을 강화하기 위한 노력도 진행될 것입니다.\n\n이 협업은 안전이 중요한 애플리케이션을 위한 Rust의 신뢰성을 높이는 것을 목표로 하고 있습니다.",
      "ja": null
    }
  },
  {
    "id": "e397dcc6ed2968dd",
    "title": {
      "en": "Writing Programs with Ncurses",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://invisible-island.net/ncurses/ncurses-intro.html",
    "score": 117,
    "by": "begoon",
    "time": 1742736446,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "ead14bb057b47cb1",
    "title": {
      "en": "I got kicked out of Columbia for taking a stand against LeetCode interviews",
      "ko": "리트코드 반란, 콜롬비아 퇴출!",
      "ja": null
    },
    "type": "story",
    "url": "https://www.linkedin.com/posts/roy-lee-goat_i-just-got-kicked-out-of-columbia-for-taking-activity-7310834407433453568-tqAm",
    "score": 7,
    "by": "mmarian",
    "time": 1743106409,
    "content": "Chungin Lee\n\n                interview coder | ex columbia | zfellow\n\n      21h\n\n                      Report this post\n\n    I just got kicked out of Columbia for taking a stand against Leetcode interviews.\n\nHere’s the whole story:\n\nIn Fall 2024, I transferred into Columbia as a CS major. I came in knowing I wanted to start a company and immediately found a great co-founder in Neel Shanmugam\n\nA few early projects didn’t go anywhere. After a while, we realized something important: We didn’t need a better product. We needed distribution.\n\nSo we flipped the usual startup model on its head and decided to just build something we knew would go viral first, focusing on distribution before anything.\n\nThat's how we came up with Interview Coder — an invisible application that helps you pass Leetcode interviews The plan was to use it ourselves, get offers from top companies, film everything, and ride the shock factor.\n\nWe built the MVP in 10 days and launched it free + open source on LinkedIn.\nIt got a few views, but it didn’t go hyper viral. Still, we had a gut feeling this idea had legs. So we used the whole recruiting season to polish it.\n\nEventually, I got offers from Meta, TikTok, and Capital One. We launched on LinkedIn again, this time with actual results — and the response was bigger.\n\nI went too far when I posted all the offers as job experience. That got my old account banned from LinkedIn— and from many subreddits we were relying on for growth. But by that point, I had what I needed.\n\nWe recorded the entire Amazon process — every step of me using InterviewCoder from OA to final offer. The UX was clean. The results were real.\nI posted the video. It started to blow up.\n\nThen Columbia sent me a disciplinary notice. Apparently, an Amazon executive had sent the school a letter saying something to the effect of: “Expel this kid or we won’t hire from your school again.”\n\nI was told not to share the letter, but at that point, I had two choices:\n → Stay quiet and get buried (Columbia would surely have sided with Amazon to kick me out)\n → Or go public and fight\n\nI leaked the letter, and it went super viral on X. That single moment probably protected me more than anything else.\n\nAt the hearing, Columbia forced me into a weird admission that the tool could be used to help students cheat in a Columbia CS class. (It can’t.) and put me on academic probation, scheduling another hearing for leaking all the documents.\n\nA week and a half later, and I'm completely kicked out from school. LOL!\n\nIt's funny now and an amazing story in hindsight, but the truth is I was tweaking bad throughout this entire process. Taking this much risk tested my mental strength to its absolute limit, but I'm super glad I did it.\n\nInterview Coder still works, and we just pushed the biggest update to it ever for maximal undetectability.\n\nMore soon.\n\n                    29,901\n\n                1,865 Comments\n\n      Like\n\n      Comment\n\n              Share\n\n      Copy\n\n      LinkedIn\n\n      Facebook\n\n      Twitter\n\n                    Rohan Singla\n\n                  SDE-1 @Flipkart  ||  Ex-Samsung  ||  Expert @Codeforces (1772)  ||  Guardian @Leetcode  ||  5⭐ @Codechef\n\n      11h\n\n                      Report this comment\n\n    One additional adaptation of handcam requirement (where your hands and screen both are visible) like in chess or other games, and everything falls apart in context of cheating.\nNot sure why companies haven't started doing that already.\n\n      Like\n\n      Reply\n\n                  2Reactions\n\n                3Reactions\n\n                    Jane Odum\n\n                  PhD student at The University of Georgia | software engineer\n\n      3h\n\n                      Report this comment\n\n    I am totally against the way leetcode interviews are conducted, but this is a silly product to even encourage. What happens when companies improve their interviewing process, what will your product do then. This just doesn’t make sense risking your education for something that helps people cheat.\n\n      Like\n\n      Reply\n\n                  23Reactions\n\n                24Reactions\n\n                    Benjamin Schmidt\n\n                  bennyschmidt.com\n\n      5h\n\n                      Report this comment\n\n    Well, you were only able to get the interviews from being on an Ivy League fast track.\n\nIn the emails you shared the recruiters had \"Ivy League Recruiting\" in their signature - you guys get interviews more than the average person.\n\nHaving a GPT wrapper won't help most candidates as they will not even be able to get interviews at Amazon, etc.\n\nReally - it was Columbia getting you the interview, not your GPT Wrapper.\n\n      Like\n\n      Reply\n\n                  49Reactions\n\n                50Reactions\n\n                    Arvin Shahid\n\n                  UC Berkeley | Regents Scholar\n\n      9h\n\n                      Report this comment\n\n    If you truly cared about some sort of revolution, you would make it free. Instead you're painting yourself as some victim and monetizing off it. Exploiting other's desperation to get into tech makes you no worse then the entities you critique. In fact, it does two things...\n\n1) Makes companies realize that most entry roles aren't needed, therefore lowering hiring for everyone plus switching interviews to in person\n\n2) More selective interviewing, so targeting top tier schools will be the norm. Candidates from lower socio-economic standing will get impacted the most\n\nYou revolutionized tech hiring for sure. The small remnants of meritocracy is gone, and tech will become gatekept like consulting or finance. Leetcode sucks but I fear this is a negative long term. Congrats Chungin Lee\n\n      Like\n\n      Reply\n\n                  68Reactions\n\n                69Reactions\n\n                    Rachael Cuevas-Ortiz, MBA\n\n                  Strategy & Ops | Ex-Amazon | IE MBA\n\n      2h\n\n                      Report this comment\n\n    I'm confused as to how this isn't effectively a tool to cheat your way into a job? I'm no SDE so to myself and the millions of others on LinkedIn that aren't, that's how this tool comes across.\nIf it is, then both Columbia and Amazon (or any other company you used this for in interviews) would be well within their rights to be upset and levy negative ramifications.\n\n      Like\n\n      Reply\n\n                  46Reactions\n\n                47Reactions\n\n                    Matt Luo\n\n                  ceo ClarityText group chats for marketing pro expertise and products\n\n      3h\n\n                      Report this comment\n\n    Interesting how much UX can fit into a browser overlay experience\n\n      Like\n\n      Reply\n\n                  5Reactions\n\n                6Reactions\n\n                    Jessica Chiang\n\n                  Engineering @ AirBnb\n\n      9h\n\n                      Report this comment\n\n    Just wow. The era of calling a cheating tool an inspiring innovation. If you do not like leetcode questions, interview with companies that do not require them. It is like applying to college and lie about what you put on the application because you don’t like the questions.\n\n      Like\n\n      Reply\n\n                  59Reactions\n\n                60Reactions\n\n                    Arpit G.\n\n                  VP/CTPO FinTech|eCom|AI|SAAS|AdTech|Enterprise\n\n      10h\n\n                      Report this comment\n\n    I think next assignment for u should be to create interview bot that is digital twin of the candidate. Not only they give all the coding answers but also deliver behavioral interview. This will be fool proof. It will really test the recruiting process of Big Tech if they can detect a human AI Agent? Which performs similar to an actual human.\nU have short circuited the interview and no company will like this. But u can build trust and safety tools to catch this type of fraud. https://m.youtube.com/watch?v=_hYqotRFzm4\n\n      Like\n\n      Reply\n\n                  2Reactions\n\n                3Reactions\n\n                    Mark Barwinski\n\n                  Global Cybersecurity Leader | Board Adviser | Speaker | CISO | Boardroom-Certified Technology Leader (QTE), CISM, CISSP\n\n      10h\n\n                      Report this comment\n\n    Frame this ! You will look back one day and say …. Best decision ever !\n\n      Like\n\n      Reply\n\n                1Reaction\n\n                See more comments\n\n        To view or add a comment, sign in",
    "summary": {
      "en": "Chungin Lee, a former Columbia University student, was expelled for opposing Leetcode interview practices. After transferring to Columbia as a Computer Science major in Fall 2024, he and his co-founder, Neel Shanmugam, decided to create a tool called Interview Coder to help candidates pass technical interviews. They quickly developed the app and launched it on LinkedIn, which gained traction after Chungin secured job offers from major companies like Meta and TikTok.\n\nHowever, after posting about his offers, Columbia received a complaint from an Amazon executive, leading to disciplinary action against him. Faced with the choice to stay silent or speak out, Chungin leaked the complaint letter online, which went viral. This backlash resulted in his expulsion after Columbia accused him of potentially facilitating cheating.\n\nChungin reflects on the experience, acknowledging the stress it caused but expressing satisfaction with his decision. He continues to develop Interview Coder, which has undergone significant updates.",
      "ko": "전 콜롬비아 대학교 학생인 청인 리는 리트코드 인터뷰 방식에 반대하다가 퇴학당했습니다. 2024년 가을 컴퓨터 과학 전공으로 콜롬비아에 전학한 그는 공동 창립자 닐 샨무감과 함께 후보자들이 기술 면접을 통과할 수 있도록 돕는 도구인 인터뷰 코더를 만들기로 결정했습니다. 그들은 빠르게 앱을 개발하여 링크드인에 출시했고, 청인이 메타와 틱톡 같은 대기업으로부터 취업 제안을 받으면서 인기를 끌었습니다.\n\n그러나 청인이 제안에 대해 게시한 후, 콜롬비아는 아마존 임원으로부터 불만을 접수받아 그에 대한 징계 조치를 취했습니다. 침묵할지 아니면 목소리를 낼지 고민하던 청인은 불만 편지를 온라인에 유출했고, 이는 빠르게 퍼졌습니다. 이 반발로 인해 콜롬비아는 그가 부정행위를 조장했을 가능성이 있다고 주장하며 퇴학 조치를 내렸습니다.\n\n청인은 이번 경험을 돌아보며 스트레스를 느꼈지만 자신의 결정에 만족한다고 밝혔습니다. 그는 인터뷰 코더를 계속 개발하고 있으며, 이 앱은 많은 업데이트를 거쳤습니다.",
      "ja": null
    }
  },
  {
    "id": "ef3370f08595c360",
    "title": {
      "en": "Waymos crash less than human drivers",
      "ko": "웨이모, 인간보다 안전해!",
      "ja": null
    },
    "type": "story",
    "url": "https://www.understandingai.org/p/human-drivers-keep-crashing-into",
    "score": 313,
    "by": "rbanffy",
    "time": 1743022679,
    "content": "Share this postUnderstanding AIAfter 50 million miles, Waymos crash a lot less than human driversCopy linkFacebookEmailNotesMoreDiscover more from Understanding AIExploring how AI works and how it's changing our world.Over 58,000 subscribersSubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.Already have an account? Sign inAfter 50 million miles, Waymos crash a lot less than human driversHuman drivers keep crashing into Waymos that aren't even moving.Timothy B. LeeMar 27, 202577Share this postUnderstanding AIAfter 50 million miles, Waymos crash a lot less than human driversCopy linkFacebookEmailNotesMore2410ShareI’ve been thinking a lot about autonomous vehicles as I prepare for next Wednesday’s Ride AI Summit in Los Angeles.I’ll be moderating two panels. One features three companies—Waabi, Bot.auto, and Torc—that are working to automate long-haul trucking. The other features Nuro and Wayve, two of the leading companies developing next-generation driver asssistance systems for customer-owned cars.We still have some tickets available, so if you are involved—or interested—in the AV industry, you’ll want to join us in Los Angeles.The first ever fatal crash involving a fully driverless vehicle occurred in San Francisco on January 19. The driverless vehicle belonged to Waymo, but the crash was not Waymo’s fault.Here’s what happened: a Waymo with no driver or passengers stopped for a red light. Another car stopped behind the Waymo. Then, according to Waymo, a human-driven SUV rear-ended the other vehicles at high speed, causing a six-car pileup that killed one person and injured five others. Someone’s dog also died in the crash.Another major Waymo crash occurred in October in San Francisco. Once again, a driverless Waymo was stopped for a red light. According to Waymo, a vehicle traveling in the opposite direction crossed the double yellow line and crashed into an SUV that was stopped to the Waymo’s left. The force of the impact shoved the SUV into the Waymo. One person was seriously injured.These two incidents produced worse injuries than any other Waymo crash in the last nine months. But in other respects they were typical Waymo crashes. Most Waymo crashes involve a Waymo vehicle scrupulously following the rules while a human driver flouts them: speeding, running red lights, careening out of their lanes, and so forth.Waymo’s service will only grow in the coming months and years. So Waymo will inevitably be involved in more crashes—including some crashes that cause serious injuries and even death.SubscribeBut as this happens, it’s crucial to keep the denominator in mind. Since 2020, Waymo has reported roughly 60 crashes serious enough to trigger an airbag or cause an injury. But those crashes occurred over more than 50 million miles of driverless operations. If you randomly selected 50 million miles of human driving—that’s roughly 70 lifetimes behind the wheel—you would likely see far more serious crashes than Waymo has experienced to date.Federal regulations require Waymo to report all significant crashes, whether or not the Waymo vehicle was at fault—indeed, whether or not the Waymo is even moving at the time of the crash. I’ve spent the last few days poring over Waymo’s crash reports from the last nine months. Let’s dig in.Examining Waymo’s safety record since JulyA Waymo car in Los Angeles. (Photo by P_Wei via Getty)Last September, I analyzed Waymo crashes through June 2024. So this section will focus on crashes between July 2024 and February 2025. During that period, Waymo reported 38 crashes that were serious enough to either cause an (alleged) injury or an airbag deployment.In my view only one of these crashes was clearly Waymo’s fault. Waymo may have been responsible for three other crashes—there wasn’t enough information to say for certain. The remaining 34 crashes seemed to be mostly or entirely the fault of others:The two serious crashes I mentioned at the start of this article are among 16 crashes where another vehicle crashed into a stationary Waymo (or caused a multi-car pileup involving a stationary Waymo). This included ten rear-end crashes, three side-swipe crashes, and three crashes where a vehicle coming from the opposite direction crossed the center line.Another eight crashes involved another car (or in one case a bicycle) rear-ending a moving Waymo.A further five crashes involved another vehicle veering into a Waymo’s right of way. This included a car running a red light, a scooter running a red light, and a car running a stop sign.Three crashes occurred while Waymo was dropping a passenger off. The passenger opened the door and hit a passing car or bicycle. Waymo has a “Safe Exit” program to alert passengers and prevent this kind of crash, but it’s not foolproof.There were two incidents where it seems like no crash happened at all:In one incident, Waymo says that its vehicle “slowed and moved slightly to the left within its lane, preparing to change lanes due to a stopped truck ahead.” This apparently spooked an SUV driver in the next lane, who jerked the wheel to the left and ran into the opposite curb. Waymo says its vehicle never left its lane or made contact with the SUV.In another incident, a pedestrian walked in front of a stopped Waymo. The Waymo began moving after the pedestrian had passed, but then the pedestrian “turned around and approached the Waymo AV.” According to Waymo, the pedestrian “may have made contact with the driver side of the Waymo AV” and “later claimed to have a minor injury.” Waymo’s report stops just short of calling this pedestrian a liar.So that’s a total of 34 crashes. I don’t want to make categorical statements about these crashes because in most cases I only have Waymo’s side of the story. But it doesn’t seem like Waymo was at fault in any of them.There was one crash where Waymo clearly seemed to be at fault: in December, a Waymo in Los Angeles ran into a plastic crate, pushing it into the path of a scooter in the next lane. The scooterist hit the crate and fell down. Waymo doesn’t know whether the person riding the scooter was injured.I had trouble judging the final three crashes, all of which involved another vehicle making an unprotected left turn across a Waymo’s lane of travel. In two of these cases, Waymo says its vehicle slammed on the brakes but couldn’t stop in time to avoid a crash. In the third case, the other vehicle hit the Waymo from the side. Waymo’s summaries make it sound like the other car was at fault in all three cases, but I don’t feel like I have enough information to make a definite judgment.Even if we assume all three of these crashes were Waymo’s fault, that would still mean that a large majority of the 38 serious crashes were not Waymo’s fault. And as we’ll see, Waymo vehicles are involved in many fewer serious crashes than human-driven vehicles.Waymos get in fewer crashes than human driversAnother way to evaluate the safety of Waymo vehicles is by comparing their per-mile crash rate to human drivers. Waymo has been regularly publishing data about this over the last couple of years. Its most recent release came last week, when Waymo updated its safety data hub to cover crashes through the end of 2024.Waymo knows exactly how many times its vehicles have crashed. What’s tricky is figuring out the appropriate human baseline, since human drivers don’t necessarily report every crash. Waymo has tried to address this by estimating human crash rates in its two biggest markets—Phoenix and San Francisco. Waymo’s analysis focused on the 44 million miles Waymo had driven in these cities through December, ignoring its smaller operations in Los Angeles and Austin.Using human crash data, Waymo estimated that human drivers on the same roads would get into 78 crashes serious enough to trigger an airbag. By comparison, Waymo’s driverless vehicles only got into 13 airbag crashes. That represents an 83 percent reduction in airbag crashes relative to typical human drivers.This is slightly worse than last September, when Waymo estimated an 84 percent reduction in airbag crashes over Waymo’s first 21 million miles.Over the same 44 million miles, Waymo estimates that human drivers would get into 190 crashes serious enough to cause an injury. Instead, Waymo only got in 36 injury-causing crashes across San Francisco or Phoenix. That’s an 81 percent reduction in injury-causing crashes.This is a significant improvement over last September, when Waymo estimated its cars had 73 percent fewer injury-causing crashes over its first 21 million driverless miles.SubscribeInsurance claims against Waymo are about 90 percent lowerThe above analysis counts all crashes, whether or not Waymo’s technology was at fault. Things look even better for Waymo if we focus on crashes where Waymo was determined to be responsible for a crash.To assess this, Waymo co-authored a study in December with the insurance giant Swiss Re. It focused on crashes that led to successful insurance claims against Waymo. This data seems particularly credible because third parties, not Waymo, decide when a crash is serious enough to file an insurance claim. And claims adjusters, not Waymo, decide whether to hold Waymo responsible for a crash.But one downside is that it takes a few months for insurance claims to be filed. So the December report focused on crashes that occurred through July 2024.Waymo had completed 25 million driverless miles by July 2024. And by the end of November 2024, Waymo had faced only two potentially successful claims for bodily injury. Both claims are still pending, which means they could still be resolved in Waymo’s favor.One of them was this crash that I described at the beginning of my September article about Waymo’s safety record:On a Friday evening last November, police chased a silver sedan across the San Francisco Bay Bridge. The fleeing vehicle entered San Francisco and went careening through the city’s crowded streets. At the intersection of 11th and Folsom streets, it sideswiped the fronts of two other vehicles, veered onto a sidewalk, and hit two pedestrians.According to a local news story, both pedestrians were taken to the hospital with one suffering major injuries. The driver of the silver sedan was injured, as was a passenger in one of the other vehicles. No one was injured in the third car, a driverless Waymo robotaxi.It seems unlikely that an insurance adjuster will ultimately hold Waymo responsible for these injuries.The other pending injury claim doesn’t seem like a slam dunk either. In that case, another vehicle steered into a bike lane before crashing into a Waymo as it was making a left turn.But let’s assume that both crashes are judged to be Waymo’s fault. That would still be a strong overall safety record.Based on insurance industry records, Waymo and Swiss Re estimate that human drivers in San Francisco and Phoenix would generate about 26 successful bodily injury claims over 25 million miles of driving. So even if both of the pending claims against Waymo succeed, two injuries represent a more than 90 percent reduction in successful injury claims relative to typical human drivers.The reduction in property damage claims is almost as dramatic. Waymo’s vehicles generated nine successful or pending property damage claims over its first 25 million miles. Waymo and Swiss Re estimate that human drivers in the same geographic areas would have generated 78 property damage claims. So Waymo generated 88 percent fewer property damage claims than typical human drivers.Subscribe to Understanding AIBy Timothy B. Lee · Thousands of paid subscribersExploring how AI works and how it's changing our world.SubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.77 Likes∙10 Restacks77Share this postUnderstanding AIAfter 50 million miles, Waymos crash a lot less than human driversCopy linkFacebookEmailNotesMore2410Share",
    "summary": {
      "en": "Waymo, a company that operates driverless vehicles, has completed over 50 million miles of driving and has experienced significantly fewer crashes than human drivers. Even during incidents where Waymo vehicles were hit by other cars, they were largely following traffic rules while the other drivers were not. \n\nIn recent months, Waymo reported 38 serious crashes, with only one clearly being the fault of a Waymo vehicle. The majority of crashes involved other drivers failing to follow traffic laws, such as rear-ending stationary Waymos or crossing into their lanes.\n\nWhen comparing crash rates, Waymo estimates that its vehicles would have about 83% fewer airbag-triggering crashes and 81% fewer injury-causing crashes than typical human drivers in the same areas.\n\nAdditionally, insurance claims against Waymo are about 90% lower than what would be expected for human drivers, highlighting their strong safety record. Overall, Waymo's data suggests that their autonomous vehicles are safer than human-driven cars.",
      "ko": "웨이모는 자율주행 차량을 운영하는 회사로, 5천만 마일 이상의 주행을 완료했으며, 인간 운전사보다 사고가 훨씬 적은 것으로 나타났습니다. 웨이모 차량이 다른 차에 부딪히는 사고가 발생했을 때도, 대부분의 경우 웨이모 차량은 교통 법규를 준수하고 있었고, 다른 운전사들이 법규를 어긴 경우가 많았습니다.\n\n최근 몇 달 동안 웨이모는 38건의 심각한 사고를 보고했으며, 그 중 단 한 건만이 웨이모 차량의 명백한 과실로 확인되었습니다. 대부분의 사고는 다른 운전사들이 교통 법규를 지키지 않아 발생했으며, 예를 들어 정지해 있는 웨이모 차량에 후방 충돌하거나 차선에 들어오는 경우가 많았습니다.\n\n사고 발생률을 비교해보면, 웨이모는 자사의 차량이 같은 지역에서 일반 인간 운전사보다 에어백이 작동하는 사고는 약 83% 적고, 부상을 초래하는 사고는 81% 적을 것으로 추정하고 있습니다.\n\n또한, 웨이모에 대한 보험 청구는 인간 운전사에 비해 약 90% 낮아, 이들의 안전 기록이 매우 우수함을 보여줍니다. 전반적으로 웨이모의 데이터는 자율주행 차량이 인간이 운전하는 차량보다 더 안전하다는 것을 시사합니다.",
      "ja": null
    }
  },
  {
    "id": "182251cd050fddba",
    "title": {
      "en": "Plato: Organist to the Beasts (2022)",
      "ko": "플라톤: 짐승의 오르가니스트",
      "ja": null
    },
    "type": "story",
    "url": "https://www.willbuckingham.com/plato-organist-to-the-beasts/",
    "score": 10,
    "by": "marginalia_nu",
    "time": 1742900466,
    "content": "It’s a curious image. The philosopher Plato, dressed in a turban and a sash, sits before a pipe organ and plays sweet tunes. Meanwhile, around him lies a catatonic tribe of lions, tigers, antelopes, leopards, deer, rhinos, cranes, phoenixes and other beasts. What’s going on here? Are these animals sleeping? Are they dead? What is Plato up to?It’s a strange and fascinating scene, so yesterday —not having much else to do under lockdown —I set about finding out more.It is not a scene you will find in any of the Greek texts. The image, now in the British Museum, is attributed to the Mughal painter Madhu Khanazad at the end of the sixteenth century. It is an illustration of a part of the Persian book of Sufi poetry, the Khamsa of Nizami, which in turn dates to around 1200. Here’s the full picture.Nizami’s Khamsa is particularly famous for the love story of Layla and Majnun. This scene of Platonic music-making occurs later on in the book, in the section dedicated to the exploits of Alexander the Great. In Nizami’s story, Alexander calls together a bunch of philosophers. Then he challenges them to a contest to prove who is the wisest. At first, it looks as if Aristotle is going to win, but then Plato gives a strange musical demonstration, and he eventually takes the prize.The story is a fascinating one because of the picture it paints of medieval Islamic views of Plato and Aristotle. Plato is reinvented as a mystic, by way of Neoplatonism and Sufi illuminationism or hikmat-i ishrāq, which was in the ascendant in Persia at the time the book was written.I couldn’t track down an English version of the story, but I did find a version in German by J.C. Bürgel. So my English retelling is based on Bürgel’s (see the references at the end). It sticks pretty close to the original text.Plato Makes Music to Shame AristotleAdapted from the Khamsa of NizamiThe ruler Alexander, whose tongue and mind were like flame and candle-wax, summoned the philosophers of his empire to a debate, to see who was the greatest. In the palace, the philosophers sat at the foot of Alexander’s throne in a line. And there they talked about logic and magic, mathematics and art, theology and science. But after every philosopher spoke, the great Aristotle always found something else to add: some extra subtlety, some further thoughtThe philosophers were awed by Aristotle’s wisdom and insight. “I am the leader of all in matters of wisdom,” Aristotle bragged. “This judgement is not a lie. It is not arrogance. It is simply how things are.” And Alexander was inclined to agree.But Plato, who had not yet spoken, was indignant. He made himself scarce and fled the palace, the way a phoenix can suddenly disappear. He took himself to a solitary place and hid himself away in a barrel. There in his lonely retreat, Plato followed the traces of the spheres and tracks of the stars. He listened for their harmonies, traced their pattern.When he had complete knowledge of the sounds, Plato made a musical instrument, fashioning wood, making strings, stretching gazelle-leather over a gourd, and rubbing the skin with musk so the sounds were sweet. When he was done, he had made a contraption that could sing out high and low, in loud and soft tones. His instrument could emulate the sounds of all living beings — even the lion and the ox —a music that could speak to the individual nature of every living thing, could testify to their suffering and joys.Plato went out into the desert, taking his strange contraption with him. He drew a circle around himself and sat down. Then he began to play. The moment he did so, the wild animals from the mountains and the plains all came running. They put their heads down on the line that Plato had drawn, lost their senses, and collapsed to the ground as if dead. Wolves and lambs, lions and wild asses: they all snoozed peaceably next to each other.Plato’s music of the spheres could do this: whether you were an ass or a lion or a human being, it could bring you to a sleep so deep it was almost like death, and then it could revive you and you could go on your way refreshed.The news of this wonderful music spread quickly, as far as Alexander’s palace. When Aristotle heard about Plato’s skill, he was plunged into depression. How could he be so outdone by his predecessor? So he brooded in the dark of Alexander’s castle, and after long reflection, his keen mind penetrated all the way to the bottom of Plato’s sublime art. He made a musical instrument of his own, and he hurried out to the desert to give it a try.Aristotle’s instrument more or less worked. He could lull anybody to sleep, just as Plato could. But stirring them back to life turned out to be a bigger problem, and no matter how much Aristotle struggled to find the right mode or harmony, he failed.So in the end, Aristotle swallowed his pride and went to see his teacher Plato: “I know the mode that lulls people into unconsciousness,” he said, “but I do not know the one that brings the spirit back from its sleeping.”Plato offered to teach Aristotle his art. Again Plato drew a circle about them both, and he started to play. Again the animals came running, put their heads on the line, and fell asleep. Aristotle more or less knew how to do as much, and was not greatly impressed. But then Plato modulated into a new key. The music was sweet as nectar, and Aristotle immediately lost consciousness. Whilst Aristotle was passed out, Plato changed key again, and he woke up the animals. Once the animals were restored to consciousness, Plato woke Aristotle.Aristotle got to his feet like a ray of fire, confused by what had happened, annoyed he had missed Plato’s profound teaching. But then seeing he was outwitted, he apologised for his arrogance, and praised Plato for his greater powers.Only then did Plato teach the much chastened Aristotle what it meant to play the modes that reflected the harmonies of the heavenly spheres. And from then onwards, Aristotle never ceased praising Plato for his greater wisdom. As for Alexander, when news of Plato’s demonstration reached him, he showered honours upon the older philosopher, acknowledging him as the wisest man in all Greece.References:My retelling is taken from the German translation in “Der Wettstreit zwischen Plato und Aristoteles im Alexander-Epos des persischen Dichters Nizami” by J.C. Bürgel, Die Welt des Orients, Bd. 17 (1986), pp. 95-109.There’s also an essay by Bürgel in The Poetry of Nizami Ganjavi: Knowledge, Love, and Rhetoric, edited by Kamran Talattof and Jerome W. Clinton (Palgrave 2000).",
    "summary": {
      "en": "The text describes a unique image of the philosopher Plato playing music on an organ while surrounded by a group of seemingly lifeless animals. This artwork, attributed to Mughal painter Madhu Khanazad, illustrates a scene from the Persian Sufi poetry book, the Khamsa of Nizami, which dates back to around 1200. \n\nIn the story, Alexander the Great gathers philosophers to determine who is the wisest. While Aristotle initially impresses everyone with his knowledge, Plato, feeling overshadowed, retreats to create a musical instrument that can put animals to sleep and awaken them. When he plays, all the animals around him fall asleep, and he later revives them with his music. \n\nAristotle, feeling threatened by Plato's talent, tries to replicate his ability but struggles to awaken the animals. Eventually, he seeks Plato's help, and after a demonstration, he realizes Plato's superiority in musical knowledge. He acknowledges Plato's wisdom, leading to Plato being honored as the wisest man by Alexander. \n\nOverall, the story highlights themes of wisdom, rivalry, and the transformative power of music, showcasing the medieval Islamic perspective on Plato and Aristotle.",
      "ko": "철학자 플라톤이 오르간을 연주하는 독특한 이미지가 그려진 작품이 있다. 이 작품은 무굴 화가 마두 카나자드의 작품으로, 1200년경에 쓰인 페르시아 수피 시집 '니자미의 칸사'에서 영감을 받았다. \n\n이 이야기에서 알렉산더 대왕은 가장 지혜로운 철학자를 찾기 위해 철학자들을 모은다. 아리스토텔레스는 처음에 자신의 지식으로 모두를 감명시키지만, 플라톤은 자신이 그늘에 가려졌다고 느끼고 동물들을 잠재울 수 있는 악기를 만들기로 한다. 그가 연주를 시작하자 주변의 모든 동물들이 잠이 들고, 그는 나중에 음악으로 그들을 다시 깨운다. \n\n아리스토텔레스는 플라톤의 재능에 위협을 느끼고 그의 능력을 따라 하려 하지만 동물들을 깨우는 데 어려움을 겪는다. 결국 그는 플라톤의 도움을 요청하고, 시연을 통해 플라톤의 음악 지식이 자신보다 뛰어나다는 것을 깨닫는다. 아리스토텔레스는 플라톤의 지혜를 인정하고, 그로 인해 플라톤은 알렉산더에 의해 가장 지혜로운 사람으로 존경받게 된다. \n\n이 이야기는 지혜, 경쟁, 음악의 변혁적인 힘이라는 주제를 강조하며, 중세 이슬람 사회에서 플라톤과 아리스토텔레스에 대한 관점을 보여준다.",
      "ja": null
    }
  },
  {
    "id": "8c4b2148f54868fc",
    "title": {
      "en": "Collapse OS",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "http://collapseos.org/",
    "score": 210,
    "by": "kaycebasques",
    "time": 1742999672,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "61278c568b957939",
    "title": {
      "en": "Building a Firecracker-Powered Course Platform to Learn Docker and Kubernetes",
      "ko": "폭발적 도커·쿠버네티스 학습 플랫폼",
      "ja": null
    },
    "type": "story",
    "url": "https://iximiuz.com/en/posts/iximiuz-labs-story/",
    "score": 142,
    "by": "fazkan",
    "time": 1743019712,
    "content": "June 17, 2023Containers,Programming,Ranting\nThis is a long overdue post on iximiuz Labs' internal kitchen.\nIt'll cover why I decided to build my own learning-by-doing platform for DevOps, SRE, and Platform engineers, how I designed it, what technology stack chose, and how various components of the platform were implemented.\nIt'll also touch on some of the trade-offs that I had to make along the way and highlight the most interesting parts of the platform's architecture.\nIn the end, I'll, of course, share my thoughts on what's next on the roadmap.\nSounds interesting? Then brace for a long read!\n\nLevel up your server-side game — join 10,000 engineers getting insightful learning materials straight to their inbox.Subscribe!Built with ConvertKit.formkit-form[data-uid=\"9712232248\"] *{box-sizing:border-box;}.formkit-form[data-uid=\"9712232248\"]{-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;}.formkit-form[data-uid=\"9712232248\"] legend{border:none;font-size:inherit;margin-bottom:10px;padding:0;position:relative;display:table;}.formkit-form[data-uid=\"9712232248\"] fieldset{border:0;padding:0.01em 0 0 0;margin:0;min-width:0;}.formkit-form[data-uid=\"9712232248\"] body:not(:-moz-handler-blocked) fieldset{display:table-cell;}.formkit-form[data-uid=\"9712232248\"] h1,.formkit-form[data-uid=\"9712232248\"] h2,.formkit-form[data-uid=\"9712232248\"] h3,.formkit-form[data-uid=\"9712232248\"] h4,.formkit-form[data-uid=\"9712232248\"] h5,.formkit-form[data-uid=\"9712232248\"] h6{color:inherit;font-size:inherit;font-weight:inherit;}.formkit-form[data-uid=\"9712232248\"] h2{font-size:1.5em;margin:1em 0;}.formkit-form[data-uid=\"9712232248\"] h3{font-size:1.17em;margin:1em 0;}.formkit-form[data-uid=\"9712232248\"] p{color:inherit;font-size:inherit;font-weight:inherit;}.formkit-form[data-uid=\"9712232248\"] ol:not([template-default]),.formkit-form[data-uid=\"9712232248\"] ul:not([template-default]),.formkit-form[data-uid=\"9712232248\"] blockquote:not([template-default]){text-align:left;}.formkit-form[data-uid=\"9712232248\"] p:not([template-default]),.formkit-form[data-uid=\"9712232248\"] hr:not([template-default]),.formkit-form[data-uid=\"9712232248\"] blockquote:not([template-default]),.formkit-form[data-uid=\"9712232248\"] ol:not([template-default]),.formkit-form[data-uid=\"9712232248\"] ul:not([template-default]){color:inherit;font-style:initial;}.formkit-form[data-uid=\"9712232248\"] .ordered-list,.formkit-form[data-uid=\"9712232248\"] .unordered-list{list-style-position:outside !important;padding-left:1em;}.formkit-form[data-uid=\"9712232248\"] .list-item{padding-left:0;}.formkit-form[data-uid=\"9712232248\"][data-format=\"modal\"]{display:none;}.formkit-form[data-uid=\"9712232248\"][data-format=\"slide in\"]{display:none;}.formkit-form[data-uid=\"9712232248\"][data-format=\"sticky bar\"]{display:none;}.formkit-sticky-bar .formkit-form[data-uid=\"9712232248\"][data-format=\"sticky bar\"]{display:block;}.formkit-form[data-uid=\"9712232248\"] .formkit-input,.formkit-form[data-uid=\"9712232248\"] .formkit-select,.formkit-form[data-uid=\"9712232248\"] .formkit-checkboxes{width:100%;}.formkit-form[data-uid=\"9712232248\"] .formkit-button,.formkit-form[data-uid=\"9712232248\"] .formkit-submit{border:0;border-radius:5px;color:#ffffff;cursor:pointer;display:inline-block;text-align:center;font-size:15px;font-weight:500;cursor:pointer;margin-bottom:15px;overflow:hidden;padding:0;position:relative;vertical-align:middle;}.formkit-form[data-uid=\"9712232248\"] .formkit-button:hover,.formkit-form[data-uid=\"9712232248\"] .formkit-submit:hover,.formkit-form[data-uid=\"9712232248\"] .formkit-button:focus,.formkit-form[data-uid=\"9712232248\"] .formkit-submit:focus{outline:none;}.formkit-form[data-uid=\"9712232248\"] .formkit-button:hover > span,.formkit-form[data-uid=\"9712232248\"] .formkit-submit:hover > span,.formkit-form[data-uid=\"9712232248\"] .formkit-button:focus > span,.formkit-form[data-uid=\"9712232248\"] .formkit-submit:focus > span{background-color:rgba(0,0,0,0.1);}.formkit-form[data-uid=\"9712232248\"] .formkit-button > span,.formkit-form[data-uid=\"9712232248\"] .formkit-submit > span{display:block;-webkit-transition:all 300ms ease-in-out;transition:all 300ms ease-in-out;padding:12px 24px;}.formkit-form[data-uid=\"9712232248\"] .formkit-input{background:#ffffff;font-size:15px;padding:12px;border:1px solid #e3e3e3;-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;line-height:1.4;margin:0;-webkit-transition:border-color ease-out 300ms;transition:border-color ease-out 300ms;}.formkit-form[data-uid=\"9712232248\"] .formkit-input:focus{outline:none;border-color:#1677be;-webkit-transition:border-color ease 300ms;transition:border-color ease 300ms;}.formkit-form[data-uid=\"9712232248\"] .formkit-input::-webkit-input-placeholder{color:inherit;opacity:0.8;}.formkit-form[data-uid=\"9712232248\"] .formkit-input::-moz-placeholder{color:inherit;opacity:0.8;}.formkit-form[data-uid=\"9712232248\"] .formkit-input:-ms-input-placeholder{color:inherit;opacity:0.8;}.formkit-form[data-uid=\"9712232248\"] .formkit-input::placeholder{color:inherit;opacity:0.8;}.formkit-form[data-uid=\"9712232248\"] [data-group=\"dropdown\"]{position:relative;display:inline-block;width:100%;}.formkit-form[data-uid=\"9712232248\"] [data-group=\"dropdown\"]::before{content:\"\";top:calc(50% - 2.5px);right:10px;position:absolute;pointer-events:none;border-color:#4f4f4f transparent transparent transparent;border-style:solid;border-width:6px 6px 0 6px;height:0;width:0;z-index:999;}.formkit-form[data-uid=\"9712232248\"] [data-group=\"dropdown\"] select{height:auto;width:100%;cursor:pointer;color:#333333;line-height:1.4;margin-bottom:0;padding:0 6px;-webkit-appearance:none;-moz-appearance:none;appearance:none;font-size:15px;padding:12px;padding-right:25px;border:1px solid #e3e3e3;background:#ffffff;}.formkit-form[data-uid=\"9712232248\"] [data-group=\"dropdown\"] select:focus{outline:none;}.formkit-form[data-uid=\"9712232248\"] [data-group=\"checkboxes\"]{text-align:left;margin:0;}.formkit-form[data-uid=\"9712232248\"] [data-group=\"checkboxes\"] [data-group=\"checkbox\"]{margin-bottom:10px;}.formkit-form[data-uid=\"9712232248\"] [data-group=\"checkboxes\"] [data-group=\"checkbox\"] *{cursor:pointer;}.formkit-form[data-uid=\"9712232248\"] [data-group=\"checkboxes\"] [data-group=\"checkbox\"]:last-of-type{margin-bottom:0;}.formkit-form[data-uid=\"9712232248\"] [data-group=\"checkboxes\"] [data-group=\"checkbox\"] input[type=\"checkbox\"]{display:none;}.formkit-form[data-uid=\"9712232248\"] [data-group=\"checkboxes\"] [data-group=\"checkbox\"] input[type=\"checkbox\"] + label::after{content:none;}.formkit-form[data-uid=\"9712232248\"] [data-group=\"checkboxes\"] [data-group=\"checkbox\"] input[type=\"checkbox\"]:checked + label::after{border-color:#ffffff;content:\"\";}.formkit-form[data-uid=\"9712232248\"] [data-group=\"checkboxes\"] [data-group=\"checkbox\"] input[type=\"checkbox\"]:checked + label::before{background:#10bf7a;border-color:#10bf7a;}.formkit-form[data-uid=\"9712232248\"] [data-group=\"checkboxes\"] [data-group=\"checkbox\"] label{position:relative;display:inline-block;padding-left:28px;}.formkit-form[data-uid=\"9712232248\"] [data-group=\"checkboxes\"] [data-group=\"checkbox\"] label::before,.formkit-form[data-uid=\"9712232248\"] [data-group=\"checkboxes\"] [data-group=\"checkbox\"] label::after{position:absolute;content:\"\";display:inline-block;}.formkit-form[data-uid=\"9712232248\"] [data-group=\"checkboxes\"] [data-group=\"checkbox\"] label::before{height:16px;width:16px;border:1px solid #e3e3e3;background:#ffffff;left:0px;top:3px;}.formkit-form[data-uid=\"9712232248\"] [data-group=\"checkboxes\"] [data-group=\"checkbox\"] label::after{height:4px;width:8px;border-left:2px solid #4d4d4d;border-bottom:2px solid #4d4d4d;-webkit-transform:rotate(-45deg);-ms-transform:rotate(-45deg);transform:rotate(-45deg);left:4px;top:8px;}.formkit-form[data-uid=\"9712232248\"] .formkit-alert{background:#f9fafb;border:1px solid #e3e3e3;border-radius:5px;-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;list-style:none;margin:25px auto;padding:12px;text-align:center;width:100%;}.formkit-form[data-uid=\"9712232248\"] .formkit-ale",
    "summary": {
      "en": "The post discusses the creation of iximiuz Labs' internal learning platform for DevOps, SRE, and Platform engineers. It explains the reasons behind building this platform, the design process, the technology stack used, and the implementation of various components. The author also shares the trade-offs made during development and highlights interesting aspects of the platform's architecture. Finally, the post outlines future plans for the platform.",
      "ko": "이 글에서는 iximiuz Labs의 내부 학습 플랫폼을 개발한 과정에 대해 다룹니다. 이 플랫폼은 DevOps, SRE, 그리고 플랫폼 엔지니어를 위한 것입니다. 플랫폼을 구축하게 된 이유와 디자인 과정, 사용된 기술 스택, 그리고 다양한 구성 요소의 구현 방법을 설명합니다. 저자는 개발 중에 고려한 여러 가지 선택 사항과 플랫폼 아키텍처의 흥미로운 점들을 강조합니다. 마지막으로, 플랫폼의 향후 계획에 대해서도 간략히 언급합니다.",
      "ja": null
    }
  },
  {
    "id": "9a198520ed9aa4e5",
    "title": {
      "en": "Mathematicians Find Proof to 122-Year-Old Triangle-to-Square Puzzle",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.scientificamerican.com/article/mathematicians-find-proof-to-122-year-old-triangle-to-square-puzzle/",
    "score": 9,
    "by": "IdealeZahlen",
    "time": 1743101286,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "2993ab0694ef2682",
    "title": {
      "en": "De-Atomization is the Secret to Happiness (2022)",
      "ko": "행복의 비밀, 분자 해체",
      "ja": null
    },
    "type": "story",
    "url": "https://blog.nateliason.com/p/de-atomization-is-the-secret-to-happiness",
    "score": 13,
    "by": "Tomte",
    "time": 1743101207,
    "content": "Share this postNat Eliason's NewsletterDe-Atomization is the Secret to HappinessCopy linkFacebookEmailNotesMoreDiscover more from Nat Eliason's NewsletterEssays on life, technology, making it as an author, and thriving as a solopreneur.Over 46,000 subscribersSubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.Already have an account? Sign inDe-Atomization is the Secret to HappinessIt may be that simpleNat EliasonNov 08, 2022291Share this postNat Eliason's NewsletterDe-Atomization is the Secret to HappinessCopy linkFacebookEmailNotesMore3116ShareThere are at least two kinds of fun:Type 1 fun is fun in the moment. Watching a movie, playing a video game, scrolling TikTok, reading a book. You want to have fun, you do the fun thing, and voilá, it is fun.Type 2 fun is fun in retrospect. Running a marathon is mostly un-fun from moment to moment; you’re often either zoned out or in some form of pain. But in retrospect, it was fun.I’ve spent over 1,000 hours playing the video game DOTA 2, but I remember almost zero of that time. It was strong type 1 fun but very low type 2 fun.I once went to a DOTA 2 International tournament with a friend, though, and I remember most of that experience quite vividly. Significant parts were unfun: waiting in line, commuting to the arena, bumping into one of the questionably hygienic gamers whose body must be 63% pizza and getting a whiff of Geneva-violating body odor. But overall, it was very fun.Despite being moment-to-moment less fun than playing the game, going to the tournament was ultimately more fun. Playing the videogame is very fun, but it’s monolithic. It’s just play, there’s no environmental novelty full of multisensory stimuli to hook your memory into. It blurs from one moment to the next, and like bad American Chinese food, you find yourself paradoxically unsatiated when you’re done. There’s something more fun about complex fun, even if the individual moments might score lower on the hedonometer.But fun is just one area where we can see this phenomenon. There is a clear experiential divide between rich multisensory life and what I’ll call “atomized” life.And atomized life is worth avoiding.IIWhen I wrote about how much weaker we’ve gotten, several people rightly pointed out that the reason was obvious: most of us no longer do hard labor as our day jobs. When you had to walk or jog 20 miles a day for sustenance or spend all day carrying canoes and packs on your shoulders or drag bricks of limestone around for pharaoh, you were default strong. When you spend all day sitting in a chair getting enraged / entertained / aroused / whatever by algorithms, you are default flabby.Life and fitness used to be deeply intertwined. You could not live without fitness. Now they are separate: fitness is a cute thing rich people do in their Lululemon after work or while jiggling their mouse to keep the Slack bubble green. You don’t do it to stay alive, you do it to get laid or not resent yourself or maybe if you’re particularly enlightened to “feel good.”Fitness has been atomized: it is no longer part of a cohesive whole life. It’s a separate thing you have to try to “find time for.” When someone says they “don’t have time” to work out, they’re both stating their priorities (obviously, everyone has time)1 but also stating something about their life. It does not have fitness incorporated into it.Beyond the atomization separating fitness from normal life, there is also further atomization within fitness. Let’s take biking as an example. First, biking was something you did outside, often with friends. There was scenery, socialization, exploration, sunlight, and exercise. Then the exercise element was captured in stationary bikes, placed in a gym or a spin class, and most of the richness was removed. You still got the exercise, and some socialization from being in the gym or class, but there was no scenery, no exploration, no time in the outdoors. Then we got Peloton. No socialization. No scenery. No exploration. No sunlight. Exercise, sure, and Emma is cute, but that’s it. The richness of biking is gone.And, look, I love my Peloton, but it’s Type 1 exercise. Instead of exercise being a multifaceted activity that incorporates other essential life elements like seeing friends, getting fresh air, and looking away from a screen for a few moments, it reduces it to its simplest element and suggests that’s just as good. Maybe even better because you get a “harder workout.” The most important part of exercise, after all, is INTENSITY.2Where else do we see over-atomization? Food comes to mind. A meal should be about more than just food. Relaxation, spending time with your friends and family, fun, maybe joy. If you looked at an Italian neighborhood dinner and said “wow what a waste, don’t they know they could just drink a Huel and get back to work?” then, well, oof.But atomization encourages us to reduce multivariate experiences, often the most important parts of life, to their single most obvious element:Biking is about exercise, and scheduling with friends and planning a route and inflating your tires all get in the way of that.Eating is about sustenance, and inviting friends and getting groceries and cooking all get in the way of that.Relationships are about talking, and meeting up in person and leaving the house and scheduling are all inconveneiences.Work is about checking off tasks, so spending time commuting to an office where you might goof off and socialize all get in the way of that.Then when we feel lonely, painfully isolated by our atomized life, we schedule some atomized social time like going to a bar or coffee to see friends in between our lonely work and lonely dinner because we’ve removed most of the natural socializing elements from all of the other parts of life. Atomization turns an integrated day of socializing, eating, exercising, and working into discrete hurried chunks of trying to move from one thing to another, wondering why we never seem to have time for everything.Atomization is a global version of the problem I discussed in “work life balance is impossible,” the reason you can never have “work-life balance” is that you’ve placed Work and Life at odds, as ends of a scale that needs to be balanced out lest it tips too far in either direction.If you throw Exercise and Socialization and Food and Fun and Hobbies into some complicated hexascale with Work and Life, you suddenly feel overwhelmed and start eyeing the benzos because seriously how can you possibly oh shit did the dogs get fed today ugh when did you last finish a book can you believe she hasn’t called you back is it 5 o’clock yet?But at the root of this overwhelm is the language we use around many activities. “I’m going to go workout” feels more responsible than “I’m going to go for a walk with a friend.” We separate “I’m working” and “I’m playing.” We want to make everything extremely efficient, so we opt for going for a run alone instead of trying to link up with people along the way. We need to “be productive” so we don’t work from a coffee shop with friends.There is probably some blame to be put on the dumb productivity world for this too. People think they need to focus and give things their full attention as if attention is the most important resource to optimize for. For your hour or two of deep work, sure, but after that, there’s no reason you can’t hang with friends while slowing chugging through shallow work. Obviously, you can multitask. You’ve never talked to someone while walking before?The solution to the atomization curse that both gives us significantly more time back, and makes us much happier, is to seek to reintegrate these various foci of life as much as possible. How do you turn food back into a rich, multivariate experience with friends, fun, exploration, and relaxation? How do you blend socialization and exercise and community? How do you spend less time having shallower atomized relationships through a screen, and more time having rich in-person relationships where you get the full experience of other people?The challenge is that these “Type 1,” or Atomized, versions of activities are the most immediately appealing. Booting up my computer to play a video game is way easier and sounds more immediately fun than texting some friends to play pickleball. Crushing takeout chips and queso sounds tastier and easier than cooking steak and rice. But I know I’ll feel better afterward with the latter, and that’s what we have to try to optimize for. Integrated living is more satisfying than atomic living.Instead of looking at some problem like “I don’t see enough friends,” or “I don’t work out enough,” or “I don’t have enough fun,” and then trying to find time to fit those priorities into, we should see how we can incorporate them into what we’re already doing. Could you make your workout less perfectly optimized so you can do it with friends? Can you loosen the reigns on your Super Duper Productive Routine to hang at a coffee shop with friends for a few hours a week? And for the love of God, can you please stop drinking fucking Huel or Soylent at your desk and talk to someone instead?The more creatively we can integrate the various parts of life that matter to us, the more satisfied we’ll be in our day to day.The more we atomize, the more lonely and overwhelmed we start to feel.De-atomization is the secret to happiness.If you enjoyed this, join the 36,000 other weekly readers by subscribing.Subscribe1I started working out in high school after hearing that Obama worked out for an hour a day despite his other duties as President. I figured if he had time to make it a priority, I probably did too. 2I’m not sure this is true, either. My best marathon training runs were done with a friend who was training for an Ironman. He kept subtly pushing us faster, and I was always surprised by how well I could keep up under social pressure.291 Likes∙16 Restacks291Share this postNat Eliason's NewsletterDe-Atomization is the Secret to HappinessCopy linkFacebookEmailNotesMore3116Share",
    "summary": {
      "en": "In \"De-Atomization is the Secret to Happiness,\" Nat Eliason discusses two types of fun: Type 1 fun, which is enjoyable in the moment (like playing video games), and Type 2 fun, which is rewarding in hindsight (like running a marathon). He argues that many aspects of life have become \"atomized,\" meaning they are simplified and separated from richer experiences. \n\nFor example, fitness has become a separate activity rather than an integral part of life, with people prioritizing efficiency over enjoyment. Activities like biking and eating have lost their social and experiential richness, leading to feelings of loneliness and overwhelm. \n\nTo combat this atomization, Eliason suggests reintegrating different life experiences. Instead of compartmentalizing activities like exercise, socialization, and work, we should blend them together for a more satisfying life. The key to happiness lies in embracing de-atomization, allowing us to enjoy a fuller, more connected existence.",
      "ko": "\"탈원자화가 행복의 비결이다\"라는 글에서 Nat Eliason은 두 가지 유형의 재미에 대해 이야기합니다. 첫 번째 유형은 순간적으로 즐거운 재미로, 예를 들어 비디오 게임을 하는 것과 같습니다. 두 번째 유형은 회상할 때 보람을 느끼는 재미로, 마라톤을 뛰는 것과 비슷합니다. 그는 많은 삶의 측면이 \"원자화\"되어 있다고 주장하는데, 이는 더 풍부한 경험에서 단순화되고 분리되었다는 의미입니다.\n\n예를 들어, 운동은 이제 삶의 필수적인 부분이 아니라 별개의 활동이 되었고, 사람들은 즐거움보다 효율성을 우선시하고 있습니다. 자전거 타기나 식사와 같은 활동은 사회적이고 경험적인 풍요로움을 잃어버려 외로움과 압박감을 느끼게 합니다.\n\n이러한 원자화를 극복하기 위해 Eliason은 다양한 삶의 경험을 다시 통합할 것을 제안합니다. 운동, 사회화, 일과 같은 활동을 분리하지 말고, 이들을 함께 엮어 더 만족스러운 삶을 만들어야 한다는 것입니다. 행복의 열쇠는 탈원자화를 받아들이는 데 있으며, 이를 통해 우리는 더 풍부하고 연결된 삶을 누릴 수 있습니다.",
      "ja": null
    }
  },
  {
    "id": "70879acda466dee4",
    "title": {
      "en": "All clothing is handmade (2022)",
      "ko": "모든 옷, 수제!",
      "ja": null
    },
    "type": "story",
    "url": "https://ruthtillman.com/post/all-clothing-is-handmade/",
    "score": 153,
    "by": "panic",
    "time": 1742697834,
    "content": "All Clothing Is Handmade\n\n    2022-10-29\n\n    quilts\n\n       The idea for this quilt came to me several years ago. Besides making quilts, I make some of my own clothes and occasionally make clothes for people in my life. I made my own wedding dress and one for a college friend as well (and never doing that again).\nThere’s something special nowadays about something that was made or tailored just for you, whether it was done by a friend/family member or something you had made. When I was a kid, my father gave my mother a birthday gift of what was essentially a copy of her favorite dress, made by a local seamstress. The style was pretty basic, but not being sold in stores at that time. I’m sure it was quite expensive. It was very thoughtful, she loved and wore the dress for ages, until her cancer surgeries made its fit uncomfortable.\nBut while I love the personal connections of bespoke/self-made clothing, I want to push back a bit against some language that occurs when people (myself included) talk about store-bought clothing, especially cheaper clothing. Whether it’s the nice Lands End flannel shirt I’m wearing as I write this, or the one my husband is wearing that I made for him, or the TikTok influencer’s Shein t-shirt that rips when she tries it on… all clothing is handmade.\nI made this quilt using worn textiles from friends, family, and some folks in my church and scraps from my own projects. It’s got a piece of fabric from my wedding dress and others from one of my mom’s favorite dresses. There’s a couple of my husband’s old dress shirts, bits of my father’s ties, the lining of my little sister’s worn jacket, highlights from a friend’s tunic, bits of baby dresses, another friend’s dress shirt, and more. There’s a couple little pieces of leftover fabric from that same flannel shirt my husband is wearing right now.\nI pieced it ages ago, I can’t quite even remember… 2020? It’s a wonky quilt, dozens of fabrics that were never meant to sit side-by-side. A little leather just for fun and frustration. It took forever to figure out how I wanted to quilt it (large stitches with long strokes, colors matching as best I could in the blocks themselves) and then to get to it, but I’m very glad that it’s done and I can share it now.\nWho Controls Clothing’s Quality\n21st century textile production happens on a massive scale, one that keeps ramping up and has enormous environmental costs. The quality of mass-market textiles has dropped noticeably since the 1990s, which were also a noticeable drop from the 1970s.\nWhen people talk about this change in quality, the language often carries a set of assumptions. These assumptions are often both geographically and racially coded. “This t-shirt was made in Honduras” or “This blouse was made in Cambodia” imply things about their quality and often subtly, sometimes not-so-subtly, attached to the people actually doing the work.\nBut even if one assessed ten thousand blouses made in Cambodia for their quality, any negative findings say nothing about the talents or potential of Cambodian garment workers as a whole or relate to, say, American garment workers. What they reflect instead are:\n\nThe brand’s choice of materials:\n\nWhere did the cloth come from and how was it made?\nWhat kind of thread are they using? (Thread can make a major difference in the longevity of a garment. The recent trend of lowering thread quality to save money is why the crotch of your new pants split after a month when the last pair you got of these same pants lasted a solid 5 years.)\n\nThe cutting setup:\n\nHow many layers of fabric are being cut together?\nHow much time do the workers have to operate cutting tools?\nHow likely are things to slip and slide during the cutting process?\n\nThe actual garment design. Before a garment is made, especially in a mass-production context, decisions about construction need to be made including:\n\nHow are the seams finished?\nWhat’s the stitch length?\nHow are buttons attached?\n\nThe speed at which it is being made:\n\nIs the garment worker sewing 30 seams an hour? 60? 100?\nWhat happens when there’s an error?\nWhat kind of breaks does the worker get? Really?\n\nThese decisions are all made by the company that ordered the clothing and, quite possibly, also by one or more subcontractors. While brands may plead innocence and outsourcing, the cost per item they demand from their suppliers directly influences this…and they know it.\nMass-produced clothing is low quality when someone overseeing the project decided to make it low quality. Mass-produced clothing is high quality when someone overseeing the project decided to make it high quality. What country it came from and who made it is a reflection of how the company wanted to pay and treat the people making it, not of those people’s skills or potential.\nHow does the way we speak of textile manufacturing (especially that done in East Asia) create and reinforce perceptions of East Asian women’s (primarily in our perception) capacity to do high-quality sewing? Why do we let it reflect on them instead of on the companies which made the choice? What can we change about our how we think and talk to change that stereotype?\nWhile some clothing is bespoke, all clothing is handmade.\nSources and Further Reading/Watching\nIn thinking about this, I was inspired by many of the books I’ve read but also by Janneken Smucker’s Amish Quilts. In particular, she delves into perceptions of Hmong women in Lancaster County and, later on, in refugee camps in Thailand, who sew “Amish” quilts. Is there something about an Amish woman that actually makes her a better seamstress or is that our racial perceptions? (yes)\nAlso check out the documentary Invisible Seams by Jia Li and Jodie Chan, which interviews eight Asian seamstresses and pattern-makers in the New York garment industry. I only came across this a week ago, just before finishing the quilt, but it spoke to another element of the same thing. Check out this interview with Jia and Jodie where they go into more depth about the project and what they didn’t have space to cover.",
    "summary": {
      "en": "The author shares their experience with handmade clothing and quilts, emphasizing the personal value of items made specifically for someone, as opposed to mass-produced clothing. They recall a meaningful gift their father gave their mother, highlighting the emotional connections tied to custom-made garments.\n\nThe author discusses the quality of clothing in the 21st century, noting a decline in quality due to mass production and various factors controlled by companies, such as materials used and production speed. They argue that the quality of clothing reflects the decisions of the companies that produce it, rather than the skills of the workers in countries where the clothing is made. \n\nThe text encourages readers to reconsider how we discuss garment quality and the assumptions tied to the origins of clothing, particularly regarding East Asian workers. The author believes that while some clothing is specially made, all clothing is ultimately handmade. They also suggest further reading and viewing on the topic, including works that explore the perceptions of sewing skills among different communities.",
      "ko": "저자는 수제 의류와 퀼트에 대한 자신의 경험을 공유하며, 특정인을 위해 만들어진 물건의 개인적인 가치를 강조합니다. 대량 생산된 의류와는 달리, 맞춤형 의류가 지닌 감정적 연결을 이야기하며, 아버지가 어머니에게 준 의미 있는 선물을 회상합니다.\n\n21세기 의류의 품질에 대해 논의하면서, 대량 생산으로 인해 품질이 저하되고 있다고 지적합니다. 이는 기업이 사용하는 원자재와 생산 속도 등 여러 요소를 통제하기 때문입니다. 저자는 의류의 품질이 이를 생산하는 기업의 결정에 반영되며, 의류가 만들어지는 국가의 노동자 기술과는 무관하다고 주장합니다.\n\n이 글은 독자들에게 의류 품질에 대한 논의와 의류의 출처에 대한 가정, 특히 동아시아 노동자와 관련된 부분을 재고해보도록 권장합니다. 저자는 일부 의류가 특별히 제작되지만, 모든 의류는 궁극적으로 수제로 만들어진다고 믿습니다. 또한, 다양한 커뮤니티에서의 재봉 기술에 대한 인식을 탐구하는 작품들을 포함해 이 주제에 대한 추가적인 독서와 시청을 제안합니다.",
      "ja": null
    }
  },
  {
    "id": "9ab6f37579a0d2a3",
    "title": {
      "en": "Sundance Film Festival is leaving Utah",
      "ko": "선댄스, 유타를 떠나다",
      "ja": null
    },
    "type": "story",
    "url": "https://www.sltrib.com/artsliving/2025/03/27/sundance-film-festival-will-leave/",
    "score": 9,
    "by": "freedomben",
    "time": 1743102761,
    "content": "‘Mormon Land’: Podcasters explain why they resigned their membership, insist they ‘want the church to thrive’",
    "summary": {
      "en": "In a podcast episode titled \"Mormon Land,\" hosts share their reasons for leaving the Mormon Church. They emphasize that their decision to resign doesn't mean they want the church to fail; instead, they express a desire for it to improve and thrive.",
      "ko": "\"Mormon Land\"라는 팟캐스트 에피소드에서 진행자들은 몰몬 교회를 떠난 이유를 이야기합니다. 그들은 교회를 떠나기로 한 결정이 교회가 실패하기를 바라는 것이 아니라는 점을 강조합니다. 오히려 그들은 교회가 개선되고 발전하기를 바라는 마음을 표현합니다.",
      "ja": null
    }
  },
  {
    "id": "f4ad71c5e4ebd341",
    "title": {
      "en": "Lumon Terminal Pro",
      "ko": "루몬 터미널 프로",
      "ja": null
    },
    "type": "story",
    "url": "https://www.apple.com/mac/lumon-terminal-pro/",
    "score": 107,
    "by": "jervant",
    "time": 1743015962,
    "content": "Lumon\n\n\t\t\t\tTerminal Pro\n\n\t\t\t\t\t\tSpoilers ahead! See how the AppleOriginal series Severance was edited onMac.\n\n\t\t\t\t                Watch the film\n\n\t\t\t\t\t\tBuy a new Mac and get three months of AppleTV+ for free.*\n\n\t\t\t\t\t\t\tShop Mac\n\n* AppleTV+ offer available to new and qualified returning subscribers only.\n\t\t\t\t\t\t$9.99/month after free trial. Only one offer per AppleAccount and only one offer per family if you’re part of a FamilySharing group, regardless of the number of devices that you or your family purchase. This offer is not available if you or your family have previously accepted an AppleTV+ 3months free or 1year free offer. Offer good for 3months after eligible device is activated. Plan automatically renews until cancelled. Restrictions and other termsapply.\n\nMore ways to shop: Find an Apple Store or other retailer near you. Or call 1-800-MY-APPLE.\n\n\t\t\tUnited States\n\n\t\t\tCopyright ©\n\n\t\t\t\t2025\n\t\t\t\t Apple Inc. All rights reserved.\n\n\t\t\t\t\tPrivacy Policy\n\n\t\t\t\t\tTerms of Use\n\n\t\t\t\t\tSales and Refunds\n\n\t\t\t\t\tLegal\n\n\t\t\t\t\tSite Map",
    "summary": {
      "en": "**Summary:**\n\nLumon is promoting the Apple Original series \"Severance,\" highlighting how it was edited on Mac computers. They also offer a deal where new and returning customers can get three months of Apple TV+ for free when buying a new Mac. However, this offer is only for eligible subscribers and has specific restrictions, such as being limited to one offer per Apple account or family. After the free trial, the subscription will cost $9.99 per month. For more information, customers can visit an Apple Store or call Apple support.",
      "ko": "루몬은 애플 오리지널 시리즈 \"세버런스\"를 홍보하며 이 시리즈가 맥 컴퓨터에서 편집되었다고 강조하고 있습니다. 또한, 새로운 맥을 구매하는 고객에게는 애플 TV+를 3개월 무료로 이용할 수 있는 혜택을 제공합니다. 하지만 이 혜택은 특정 조건을 충족하는 가입자에게만 적용되며, 애플 계정이나 가족당 하나의 혜택으로 제한됩니다. 무료 체험 후에는 구독료가 월 9.99달러로 청구됩니다. 더 자세한 정보는 애플 스토어를 방문하거나 애플 고객 지원에 문의하면 됩니다.",
      "ja": null
    }
  },
  {
    "id": "3e89f38959aabcd2",
    "title": {
      "en": "Oracle customers confirm data stolen in alleged cloud breach is valid",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.bleepingcomputer.com/news/security/oracle-customers-confirm-data-stolen-in-alleged-cloud-breach-is-valid/",
    "score": 323,
    "by": "el_duderino",
    "time": 1743021073,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "566d5c3a89e390ae",
    "title": {
      "en": "AI Reveals Secrets of Dendritic Growth in Thin Films",
      "ko": "AI가 밝혀낸 얇은 필름의 비밀",
      "ja": null
    },
    "type": "story",
    "url": "https://www.tus.ac.jp/en/mediarelations/archive/20250320_5263.html",
    "score": 43,
    "by": "rustoo",
    "time": 1742838428,
    "content": "Contact Us\n\n\t\t\tPublic Relations Division, Tokyo University of Science\n\t\t\te-mail: mediaoffice(at sign)admin.tus.ac.jp\n\t\t\t1-3, KAGURAZAKA SHINJUKU-KU TOKYO 162-8601 JAPAN",
    "summary": {
      "en": "**Contact Information**\n\n- **Department:** Public Relations Division, Tokyo University of Science\n- **Email:** mediaoffice@admin.tus.ac.jp\n- **Address:** 1-3 Kagurazaka, Shinjuku-ku, Tokyo 162-8601, Japan",
      "ko": "연락처 정보\n\n부서: 도쿄과학대학 홍보부  \n이메일: mediaoffice@admin.tus.ac.jp  \n주소: 일본 도쿄 신주쿠구 가구라자카 1-3, 162-8601",
      "ja": null
    }
  },
  {
    "id": "2007b33bda208fc1",
    "title": {
      "en": "An Interview with Zen Chief Architect Mike Clark",
      "ko": "젠의 수장, 마이크 클락 인터뷰",
      "ja": null
    },
    "type": "story",
    "url": "https://www.computerenhance.com/p/an-interview-with-zen-chief-architect",
    "score": 144,
    "by": "Smaug123",
    "time": 1742843149,
    "content": "Share this postComputer, Enhance!An Interview with Zen Chief Architect Mike ClarkCopy linkFacebookEmailNotesMoreDiscover more from Computer, Enhance!Programming courses, interviews, commentary.Over 50,000 subscribersSubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.Already have an account? Sign inAn Interview with Zen Chief Architect Mike ClarkCasey MuratoriMar 25, 2025144Share this postComputer, Enhance!An Interview with Zen Chief Architect Mike ClarkCopy linkFacebookEmailNotesMore166ShareZen is one of the most important microarchitectures in the history of the x86 ecosystem. Not only is it the reigning champion in many x64 benchmarks, but it is also the architecture that enabled AMD’s dramatic rise in CPU marketshare over the past eight years: from 10% when the first Zen processor was launched, to 25% at the introduction of Zen 5.I recently had the honor of interviewing none other than Zen’s chief architect, Mike Clark. I only had 30 minutes, but I tried to fit in as many of our microarchitecture questions as I could! Subscribers to Computer Enhance will recognize many of them as ones we’ve collectively wondered about during Q&A’s in the Performance-Aware Programming series - and I’m delighted to report that, as you’ll see, Mike gave detailed answers to all of them.Below is the edited transcript of our conversation. I’ve tried to keep it as accurate as possible to the original audio, while reworking the phrasing to be appropriate for reading rather than listening. I have also had AMD approve the transcript to ensure accuracy, and I will be working with them to release an extended video version as well.Now, without further ado, my interview with Mike Clark:CASEY: You will often hear “people on the internet” say that ARM as an ISA is better for low power than x64. People like me who study ISAs tend to be skeptical of this claim. As a hardware designer, are there any specific things about the x64 ISA that you find difficult to deal with for low-power designs?MIKE: Having spent my career working on x86, I might have a bias here! I do think each ISA has its own quirks that influence some of the microarchitecture. But at the base level, we can build low-power x86 designs as well as ARM can, and ARM can build high frequency, high performance designs as well as x86 can. None of the quirks are really limiting you on the microarchitecture. The reality is that the markets we've been targeting have been different, so they've driven the architectures to optimize for different design points. ARM is in much lower power markets where x86 hasn't had the market share to chase.On the x86 side, the higher performance / higher frequency devices are the established market that our devices have to compete in, so that's where our design focus is. We could build the same Zen microarchitecture with an ARM ISA on top instead. We could deliver the same performance per watt. We don't view the ISA as a fundamental input to the design as far as power or performance.CASEY: So the memory model, whether the instructions are variable length, those sorts of things don’t factor in? None of the differences are big enough to matter?MIKE: No. It may take a little bit more microarchitectural work for us to account for stronger memory order on the x86 side, but of course the software has to account for the weaker memory ordering on the ARM side. So there are tradeoffs.Variable length instructions are harder than fixed length, but we've created techniques like the uop cache, and it also gives us better density overall by having smaller instructions. x86 can put more work into each instruction byte, so we can have denser binaries and increase performance that way.So these are all just tradeoffs in the microarchitecture. They’re not fundamental issues for delivering performance per watt at the end of the day.CASEY: Similar question, but moving to the OS side of things: does the 4k page size on x64 create problems for you architecturally by limiting the L1 cache size due to how tagging works? Would architectures like Zen benefit if x64 operating systems moved to 2mb pages as the smallest page size, or perhaps a 16k or 64k page size if you were to introduce that in a future architecture?MIKE: Definitely. We always encourage developers to use larger page sizes if they can, because it gives us a lot more capacity in our TLBs and therefore less TLB pressure overall. But we have the ability to combine 4k pages into larger pages in our TLB if the OS allocates them sequentially. We can turn four 4k pages into a 16k page if they are virtually and physically sequential. That's been a technique we've used even since the original Zen to help software get the benefits of larger page sizes without moving away from 4k pages.However, 4k to 2mb is a big jump. We're always looking for ways to allow our software partners to have larger page sizes, but maybe something in between is more appropriate.CASEY: Just to poke a little further at that, for the L1 cache specifically, you're hitting up against the limit of the address bits. Have you ever wanted to put in bigger L1 caches, but found that you couldn't because the 4k page size means you can't do that without going to a larger-way cache? MIKE: No. In the past we have built L1 caches that don't follow the “ways times 4k page size is the largest index you can have” property. There are ways to do that. We've solved those problems. It is a little bit more logic, but it's a solvable problem. It doesn’t limit us in what we design.  CASEY: Moving on to the sizes of registers and cache lines, I have two questions about how CPUs seem to do things differently than GPUs.First, CPUs seem to be settling into a natural size of 64 bytes. The L1 cache lines are 64 bytes. The registers are 64 bytes. It doesn't look like anyone's trying to go beyond that. But GPUs seem to prefer at least 128 bytes for both. Is this because of the difference in clock rates? Does it have to do with CPU versus GPU workloads? In general, do you see 64 bytes as a natural settling point for CPUs, and if so, why does it seem to be different from GPUs?MIKE: We do look at increasing the line size. We're always going to a clean sheet of paper and making sure we're rethinking things and not missing anything as workloads evolve and things change. We don't want to be locked into a mindset where we think we've proven 64 bytes to be the correct size for everything on a CPU.But the reality is that CPUs are targeted at low latency, smaller datatype, integer workloads as their fundamental value proposition. We've grown that capability with all our out-of-order engines, trying to expose ILP. So far, it’s allowed us to build vector units as wide as 64 bytes.But it's been a journey to even get that wide because if you look at, say, from Zen 4 to Zen 5 - we supported 512-bit vectors on Zen 4 via a 256-bit data path. For Zen 5, we went full bore and supported the full 512-bit data path. That required a fundamental replumbing of the microarchitecture. We had to grow the width of the delivery of data from the L2 to the L1, and we had to double the delivery from the L1 to really take advantage of the wider vector units.The integer workloads that are still primarily reading data out of the cache and branching, they're not getting any benefit from that sort of fundamental change. We have to do it in a very cautious and meticulous manner, so that those highways of delivery can exist while still ensuring that if there's only one car on the highway, we’re not burning power as if all the lanes were full. It’s tricky.When you look at the GPU side, the workloads where they excel are throughput based. Not having to excel at the lowest-latency, small-datatype workloads frees them up to leverage all that extra investment. You need to have workloads that are really focused on using that much data in a wide vector to get the return on that investment.So that's always the trick. If we try to go too big, too wide, we lose our value proposition in performance per watt for the mainstream workloads people buy our new generations for.Does that make sense?CASEY: It makes perfect sense, and it leads right into my next question.Underlying what you said is the implication that, if we as software developers were taking better advantage of wider workloads, it would be worth your while to widen them. One of the problems people often have when trying to widen a data path in software is that CPUs seem to be a lot worse at scatter/gather. It’s an important feature for taking data that isn’t naturally wide and putting it through a wide datapath with some level of efficiency. For example, if I want to widen something that does an array lookup, historically it’s been hard to port that code directly because of poor gather performance.Could you give us some insight on why this is?MIKE: That's a good question. It does tie back to the previous question in the sense that it’s really not the fundamental scatter/gather concept that’s the problem. It's the amount of bandwidth needed to pull all those different elements inside the CPU to put them together to feed to the vector unit.Again, we're focused on latency, not throughput. That has permeated itself out into the interface to what we call our “data fabric”. The memory system isn't wide enough to be able to pull all the data in so it can be assembled into lanes and operated on. If we wanted to attack that, we’d have to widen the interface, and that would come with a large power cost.So again, that's the trick. You're trying to avoid the power cost when you're running workloads that don't require scatter/gather. If you widen these paths, you’ve overbuilt the design for the the baseline workloads that you normally run. We are always trying to grow and pull more applications in, but we have to balance that against the power requirements of widening the bandwidth into the CPU.CASEY: So in other words, it's a chicken and egg problem? If software developers were giving you software that ran fantastically with scatter/gather, you’d do it. But they’re not, so it’s hard to argue for it?MIKE: Right, yes.CASEY: The rest of my questions don’t group together into any particular theme, so I’ll just go through them randomly.Random question number one: previously, on the software side, we thought nontemporal stores were solely there to prevent pollution of caches. That was our mental model. But lately we have noticed that nontemporal stores seem to perform better than regular stores, separate from cache pollution - as if the memory subsystem doesn't have to do as much work, or something similar. Is there more about nontemporal stores that we need to understand, or are we mistaken?MIKE: If you were just doing nontemporal stores to data that is in the caches, obviously that would not be a good thing. So you still have to apply good judgment on when to use nontemporal stores. But tying it back to the ARM-ISA-weakly-ordered discussion, nontemporal stores, while not exactly being weakly ordered, are in some ways easier to deal with in the base case. We can process them efficiently as long as they really are nontemporal. So I think your intuition is right - we can do well with them as long as the software side ensures that the data isn’t finding itself in caches along the way.CASEY: Random question number two: for educational purposes, does anyone publish modern CPU pipeline diagrams that would be reasonably accurate? AMD and Intel, for example, both publish flow charts for new microarchitectures, but not pipeline diagrams.MIKE: It might surprise people, but if you go back to when we did publish pipeline diagrams, those are still fine for learning how a modern CPU works. We do have more complicated pipelines today, and we don't publish them because they reveal proprietary techniques we're using, or give hints that we don't want to give to the competition. But at the end of the day, it's still a fetch block, a decode block, an execute block, a retire block... there's more stages within those blocks, and you can break it down even more than that, but the fundamental pipelining is still similar.CASEY: So, for example, I think the Bulldozer pipeline diagram was the last one I saw from AMD. It’s not woefully out of date? If someone learned that pipeline, they would be able to understand what you actually do now if they were given an updated diagram?MIKE: Roughly speaking, yes.CASEY: Random question number three: if you look at a single-uop instruction like sqrtpd that has a latency longer than the pipeline depth of an execution unit, can you give a cursory explanation of how this works for those of us on the software side who don't understand hardware very well?MIKE: One way to conceptualize it is that you could have taken sqrtpd and split it up into a bunch of different uops that can operate in parallel with dependencies along the way. It can be very expensive to keep all those operations in flight, to build the pipeline to pass the data forward so you can let something new in behind it that's working on an earlier stage. The hardware cost would be too high to create a pipeline to get the execution done in a way that allows another sqrtpd to start on an earlier stage - especially if it's going to be, say, 16 stages of execution until you have achieved your answer.It's really just that cost. Is the amount of hardware worth it to make something like sqrtpd a pipelineable instruction, or can we save a lot of power and hardware by just doing one of them at a time?CASEY: Just to make sure I understand: does that mean inside an execution unit that can do one of these, the uop gets issued and it knows it’s got something special that it has to work on for a while, so it asks not to be given anything else for several cycles while some special control part inside it takes over?MIKE: Correct. The scheduler that feeds it understands that it's not a pipeline execution unit that can take another uop every cycle. But it has a known quantity where, if it has sent one in, after some number of cycles, it knows it can send another one in and it should be safe.CASEY: So the system upstream of the execution unit - the thing that's feeding it - knows not to send more?MIKE: It knows, yes.CASEY: Last question: are there things you wish we as software developers would start - or stop - doing that would help take advantage of the hardware you design, or that would make it easier for you to design new hardware in the future?MIKE: We already hit on one, which is the feedback loop when we add new ISA components - larger vectors, for example. We need software to use them to get the return on investment that we're putting in.Of course we also understand that, as a new feature comes out, it's only on the new hardware. You want your software to run well on our old hardware as well as on our new hardware. We totally understand that problem. But still, if software developers could embrace the new features more aggressively, that would definitely help.It would be great if the software could find ways to leverage wider vectors, AI, and so on - all the areas we've invested a lot of hardware in. And of course we would also like to get feedback from you guys - “if we just had this instruction or this concept, we could really leverage that in our software” and so on. We're constantly open to that, too. We want to know how to make your lives easier.And finally, one other thing I would add is that larger basic blocks are better. Taking branches versus not taking branches can have a big effect on code flow. Try to put conditional operations in the right places. I’m sure you guys probably focus on this already.CASEY: Yes, but it’s always good to hear it from you. We only ever know that something runs faster when we time it - we can't always guess what the designers are thinking on the hardware side.MIKE: Gotcha.CASEY: Well, I think we are out of time. Thank you very much! This has been fantastic. Thank you for answering all of my questions, and please keep in touch. We always have questions like this on the software side, so anytime you want to talk, or if there is anything new you want to tell us about, please let us know.MIKE: Okay, cool. It was a great conversation. And yeah, any time you're wondering what's going on in the hardware, we want to close that gap as best we can!CASEY: We all appreciate it. And we love Zen as well! I’m conducting this interview from a Zen processor as we speak. So thank you for all your hard work over the years.MIKE: Alright, thanks! Talk to you later.If you enjoyed this article, and would like to receive more like it, you can put your email address in the box below to access both our free and paid subscription options:Subscribe144 Likes∙6 Restacks144Share this postComputer, Enhance!An Interview with Zen Chief Architect Mike ClarkCopy linkFacebookEmailNotesMore166SharePrevious",
    "summary": {
      "en": "The interview with Mike Clark, the chief architect of AMD's Zen microarchitecture, highlights the significance of Zen in the x86 ecosystem, noting its role in AMD's growth from a 10% to a 25% market share in CPUs. Here are the key points from the conversation:\n\n1. **x86 vs. ARM**: Clark believes that both x86 and ARM architectures can perform well in low-power and high-performance designs. The differences between them are mainly due to market focus rather than fundamental limitations.\n\n2. **Memory and Page Size**: Clark encourages the use of larger memory page sizes to improve performance, noting that 4k page sizes can limit efficiency. He mentions that while larger pages would help, they need to be balanced with existing systems.\n\n3. **Cache and Register Sizes**: The standard cache line and register size for CPUs is 64 bytes, with Clark indicating that while they are always evaluating this, it suits current CPU workloads. GPUs, in contrast, can benefit from wider data paths due to their different workload requirements.\n\n4. **Scatter/Gather Performance**: Clark explains that scatter/gather operations are challenging for CPUs due to their focus on low latency, which affects their ability to efficiently handle wide data paths.\n\n5. **Nontemporal Stores**: These stores can be beneficial as they may require less processing power, but developers still need to be judicious about when to use them.\n\n6. **Software Development Insights**: Clark emphasizes the need for software developers to leverage new hardware features, like wider vectors, to maximize performance. He also suggests that larger code blocks can enhance efficiency.\n\n7. **Closing the Feedback Loop**: Clark expresses a desire for better communication between hardware designers and software developers to improve future designs.\n\nThe interview concludes with an appreciation for the collaborative relationship between software and hardware development, emphasizing the importance of adapting to new technologies.",
      "ko": "AMD의 Zen 마이크로아키텍처 수석 설계자인 마이크 클락과의 인터뷰는 Zen이 x86 생태계에서 가지는 중요성을 강조하며, AMD가 CPU 시장 점유율을 10%에서 25%로 성장시키는 데 기여한 역할을 언급했습니다. 대화의 주요 내용은 다음과 같습니다.\n\n클락은 x86과 ARM 아키텍처 모두 저전력 및 고성능 설계에서 좋은 성능을 발휘할 수 있다고 믿습니다. 두 아키텍처 간의 차이는 주로 시장의 초점에 기인하며, 근본적인 한계 때문은 아니라고 설명했습니다.\n\n메모리와 페이지 크기에 관해서는, 클락이 성능 향상을 위해 더 큰 메모리 페이지 크기를 사용하는 것을 권장했습니다. 그는 4k 페이지 크기가 효율성을 제한할 수 있다고 언급하며, 더 큰 페이지가 도움이 될 수 있지만 기존 시스템과의 균형이 필요하다고 강조했습니다.\n\nCPU의 표준 캐시 라인과 레지스터 크기는 64바이트이며, 클락은 이 크기가 현재 CPU 작업에 적합하다고 평가하고 있다고 밝혔습니다. 반면, GPU는 작업 요구 사항이 다르기 때문에 더 넓은 데이터 경로의 이점을 누릴 수 있습니다.\n\n클락은 스캐터/가더 작업이 CPU에 도전 과제가 된다고 설명했습니다. 이는 CPU가 저지연성에 중점을 두기 때문에 넓은 데이터 경로를 효율적으로 처리하는 데 어려움이 있다는 것입니다.\n\n비시간적 저장소는 처리 능력을 덜 요구할 수 있어 유용할 수 있지만, 개발자들은 언제 사용하는 것이 적절한지 신중하게 판단해야 한다고 말했습니다.\n\n소프트웨어 개발에 대한 통찰력으로, 클락은 소프트웨어 개발자들이 더 넓은 벡터와 같은 새로운 하드웨어 기능을 활용하여 성능을 극대화할 필요가 있다고 강조했습니다. 또한, 더 큰 코드 블록이 효율성을 높일 수 있다고 제안했습니다.\n\n마지막으로 클락은 하드웨어 설계자와 소프트웨어 개발자 간의 더 나은 소통이 필요하다고 언급하며, 향후 설계를 개선하기 위한 피드백 루프를 닫는 것이 중요하다고 말했습니다. 인터뷰는 소프트웨어와 하드웨어 개발 간의 협력 관계에 대한 감사의 뜻을 전하며, 새로운 기술에 적응하는 것의 중요성을 강조하며 마무리되었습니다.",
      "ja": null
    }
  },
  {
    "id": "9d2654c48c09d68a",
    "title": {
      "en": "A Special Year at the Heinz Nixdorf MuseumsForum",
      "ko": "하인즈 닉스도르프 특별한 해",
      "ja": null
    },
    "type": "story",
    "url": "https://www.computer.org/csdl/magazine/an/2025/01/10927608/2558cmiC7UA",
    "score": 9,
    "by": "rbanffy",
    "time": 1743082262,
    "content": "AbstractOne-hundred years ago, on 9 April 1925, computer pioneer Heinz Nixdorf was born in Paderborn, Germany. In 1952 he established his first company, subsequently known as Nixdorf Computer AG, whose principal administrative headquarters today house the Heinz Nixdorf MuseumsForum (HNF). Following his unexpected passing in 1986, two foundations assumed responsibility for continuing his work. The HNF is celebrating his legacy with a series of new and innovative exhibition projects on historical and future computer technology. This overview presents a summary of activities in 2024 and a preview of the anniversary year in 2025.\n\n   One-hundred years ago, on 9 April 1925, computer pioneer Heinz Nixdorf was born in\n      Paderborn, Germany. In 1952 he established his first company, subsequently known as\n      Nixdorf Computer AG, whose principal administrative headquarters today house the Heinz\n      Nixdorf MuseumsForum (HNF). Following his unexpected passing in 1986, two foundations\n      assumed responsibility for continuing his work. The HNF is celebrating his legacy\n      with a series of new and innovative exhibition projects on historical and future computer\n      technology. This overview presents a summary of activities in 2024 and a preview of\n      the anniversary year in 2025.\n    2024 Permanent Exhibition Updates\n   The HNF continues to implement updates to its permanent exhibition. In 2023, areas\n      dedicated to the history of analog computers, digital photography, the emergence of\n      hacker culture, minicomputers, and word processing machines were added or underwent\n      a comprehensive redesign.1\n    Supercomputers: Modeling Reality\n   In early 2024, the HNF revealed a remodeled supercomputer display to the general public\n      (see Figure 1).2 In close proximity to the still formidable Cray 2, the history of early supercomputers\n      is exhibited. The exhibit commences with early CDC machines, exemplified by the Cyber\n      76 from the Lower Saxony Regional Computer Centre RRZN in Hannover. Using Eduard Pestel’s\n      Deutschland Modell, this device calculated a comprehensive simulation of Germany’s\n      economic development from the mid-1970s up until the 1990s.Figure 1. HNF’s new supercomputer ensemble. Left to right: Cray-2, reconstructed Parsytec GC,\n      KSR 1.\n   A new emphasis is placed on the collaboration between the Paderborn Center for Parallel\n      Computing and Parsytec, one of the largest developers of Transputer-based parallel\n      computers. A replica of the Parsytec GC 3 serves as a display case for a driving simulator\n      (Volkswagen, 1989), a self-driving car (Mercedes, 1991), as well as the T9000, a chip\n      that was never released to the market.\n   Additionally, visitors can view a portion of an interview with Prof. Burkhard Monien,\n      the founder of PC2, in which he elucidates the complexities and triumphs of early\n      parallel computing in a multitude of domains, including flight planning, chess, and\n      the 1992 update to German zip codes.\n    Printing and Copying: More Than Black on White\n   The recently established exhibition area, which showcases contemporary printing and\n      copying devices from the 1950s onward, opened in early 2024.3\n   The centerpiece is a 1960 still operational Xerox 1385 photocopier (see Figure 2). The first electrophotographic universal copier is the forefather of all today’s\n      laser printers and copiers. The device comprises a repro camera with an adjustable\n      slide, an original holder, and repro lamps. The base contains an apparatus for electrically\n      charging and developing selenium-coated plates, preheating the paper, and storing\n      necessary materials. An electric heater fixes the copy permanently.Figure 2. The first electrophotographic universal copier Xerox 1385.\n   Additionally, an array of contemporary printing devices is exhibited, spanning from\n      a 1975 drum printer to laser and ink-jet printers from the 1990s. Accompanying these\n      devices are audio samples illustrating the distinctive characteristics of each.\n    Games and Gadgets\n   In addition to conventional, series-produced computers, the world’s largest computer\n      museum by exhibition area also features several bespoke innovations and custom-built\n      computers. Two of these were the subject of particular interest during 2024.\n    Winning Wearable: Maschine 1\n   In 1972, two Ph.D. degree students at the University of Bonn, Herrmann Krause and\n      Rudolf Schönhardt, developed a small, portable computer, which they named Maschine\n      1 (see Figure 3). Schönhardt visited the HNF in January 2024, during which he provided a comprehensive\n      interview concerning the machine, the algorithm used, and the story behind it.4Figure 3. Staging of Maschine 1. The five finger input on the front left, pipe case with radio\n      receiver and output on the back left.\n   The machine is designed to enhance the user’s probability of winning by anticipating\n      the subsequent card to be drawn in a game of Blackjack. The device was constructed\n      by the two physicists themselves, using 900 DM-worth of the then newly available Intel\n      1024-bit shift registers to store the order of several decks of cards. The device\n      can be worn in and under a jacket, which would be appropriate attire in a casino setting.\n      The component located within the wearer’s right pocket has five buttons, one for each\n      finger, which are used to enter cards as seen.\n   The fundamental principle of the algorithm is based on a comprehensive understanding\n      of the distinct shuffling techniques at Blackjack tables in German and French casinos\n      at the time. The result is then conveyed to a second player via an old-fashioned pipe\n      case, which is equipped with a radio receiver and an output of 10 concealed lamps.\n   Even though the data entry was quite tricky and required a lot of practice, the builders\n      report winnings high enough to leave the casino after about two hours, without ever\n      getting caught.\n    Claude Shannon’s Mind Reading Machine\n   Following the success of the working replica of Shannon’s Theseus, a maze-solving\n      “mouse,” which is currently on display at both the HNF and the MIT Museum, Shannon’s\n      equally renowned Mind Reading Machine has now been reconstructed for display at the\n      HNF (see Figure 4).5,6Figure 4. Rainer Glaschick and Jochen Viehoff presenting the working reconstruction of Claude\n      Shannon’s Mind Reading Machine.\n   To engage with what is also known as the “Penny Game,” the user inputs a binary series\n      by manipulating a toggle switch in a leftward and rightward direction. Immediately\n      prior to each input, the machine performs a “guess” of the subsequent move. The machine\n      wins when it makes an accurate prediction and loses when it makes an incorrect one.\n      Despite its name, the machine achieves a high winning rate by analyzing the player’s\n      input pattern and using the fact that humans do not typically produce genuine randomness.\n   The two identical replicas were constructed by Rainer Glaschick, who was also the\n      primary engineer responsible for the Theseus replica. Like the original, they are\n      based on relay technology as well as a comprehensive analysis and a few, but vital,\n      corrections to Shannon’s original schematics as published.7\n    Nixdorf’s 100th Birthday\n   The HNF has dedicated the entirety of 2025 to commemorating the centenary of its namesake,\n      Heinz Nixdorf, through a series of special events and exhibition reopenings.\n   In the postwar period, Nixdorf pursued studies in physics and business administration\n      in Frankfurt am Main. It was during this time that he encountered Walter Sprick, who\n      earlier developed the first electron tube calculation device in Germany.8 With Sprick’s guidance, Nixdorf established his Labor für Impulstechnik (laboratory\n      for pulse technology) in 1952, which subsequently evolved into Nixdorf Computer AG\n      (NCAG) in 1968 and became the fourth largest computer producer in Europe.\n   From the outset of his professional life, Nixdorf evinced a keen interest in historical\n      calculating devices and perceived a need to enlighten the general public about the\n      functionality and applications of computers. In 1984, the NCAG presented a traveling\n      exhibition, aptly titled “The Glass Computer,” which aimed to reduce people’s doubts\n      about the new technology (see Figure 5).Figure 5. At the 1984 exhibition “The Glass Computer.” Left: Nixdorf presents the reconstructed\n      Schickard mechanical calculator (today on display in the HNF permanent exhibition).\n      Right: The eponymous glass computers (depicted here: Nixdorf 8870 Quattro/24) were\n      Nixdorf products with transparent acrylic glass housings.\n   The HNF will launch the refurbished exhibition area on the NCAG in conjunction with\n      the anniversary of the birth of its namesake. The space provides a comprehensive overview\n      of Nixdorf products, from its inception to the advent of personal computers. It includes\n      a functional reproduction of the company’s earliest mass-produced item, the Electron\n      Balancer9 ES 24. The redesigned exhibition will provide visitors the opportunity to participate\n      in a series of interactive games that will facilitate hands-on explorations of some\n      of the production processes of Nixdorf computers. The exhibits will be accompanied\n      by contemporary witness testimonials about the work and life of the Paderborn computer\n      pioneer.\n   Additional events include the retro computer festival. The biannual Heinz Nixdorf\n      Colloquium will discuss this year’s other topic of interest, the quantum computer\n      (see next section). The program will also feature a reunion with former Nixdorf employees\n      and several open-door events for families and children.\n    One-Hundred Years of Quantum: A New Computing Technology\n   In honor of the 100th anniversary of modern quantum mechanics, 2025 is the International\n      Year of Quantum Science and Technology. For the HNF, the most significant technological\n      development to emerge from these 100 years is the quantum computer, to which the HNF\n      dedicated a completely remodeled 100 m2 of the permanent exhibition, open to the public\n      from January.\n   The initial impetus for the conceptualization of quantum computing can be traced back\n      to Richard Feynman’s 1982 paper, “Simulating Physics with Computers”: “Nature isn’t\n      classical, dammit!” The European research landscape on this topic is currently characterized\n      by a high level of diversity and innovation. However, the topic is often presented\n      to the general public in a way that makes it seem mysterious, magical, and beyond\n      the comprehension of the average person.\n   The HNF’s approach to the presentation is centered on the world’s largest quantum\n      bit, an interactive light installation that represents quantum states by color rather\n      than mathematical formulae. This, as well as the entire exhibition, was designed in\n      a manner that renders its content comprehensible to any individual, regardless of\n      mathematical expertise.\n   The field of quantum computing is currently undergoing a period of rapid development.\n      In order to present a comprehensive overview of the latest developments, the exhibition\n      showcases a range of emerging technologies. These include a 1980s ion trap from the\n      Nobel Prize laureate Wolfgang Paul, a chandelier cryostat10 from the Forschungszentrum Jülich research center, which is arguably the most iconic\n      object in the field of quantum computing, a working NMR 2-qubit computer, and a demonstration\n      lab setup from photonic quantum computing (see Figure 6).Figure 6. Left: Stephan Schaecher (Infineon, center) presents two novel ion traps for the quantum\n      computing exhibition to HNF’s Christian Berg (left) and David Woitkowski (right).\n      Right: Detail of the chandelier cryostat in its original lab surrounding at Jülich\n      research center.\n    From Orchards to Personal Computers\n   Two of the HNFs most relevant exhibition areas are scheduled for renovation and reopening\n      in fall 2025. These areas will present the history of the former orchard region that\n      came to be known as Silicon Valley, as well as the closely related development of\n      the personal computer.\n    Silicon Valley: A Californian Dream\n   The long-established exhibition space dedicated to Silicon Valley will undergo redevelopment\n      to prominently feature an extensive aerial view of the region.11 This will be complemented by comprehensive background information detailing its historical\n      development, as well as the productive and innovative technological culture that characterizes\n      the area.\n   A key addition to the exhibition will be a replica of one of the infamous one-dollar\n      bills signed by the “Traitorous Eight” in 1957, marking their departure from Shockley\n      Semiconductor Laboratory to establish Fairchild Semiconductor. The exhibition will\n      also continue to explore the close relationship between Stanford University and Silicon\n      Valley, exemplified by the university’s first spin-off company, Hewlett-Packard. Additionally,\n      the exhibit will highlight the significant role of the Palo Alto research center in\n      the valley’s success, showcasing the Xerox Alto, the first device to utilize the now\n      ubiquitous mouse-controlled graphical user interface.\n    The Birth of the PC: From the Garage to the Big, Wide World\n   Located facing the Silicon Valley exhibit is the HNF’s PC gallery, a space where many\n      visitors encounter the first computer they personally used.12 The gallery is slated for a visual enhancement and will be reorganized to accommodate\n      an even greater number of early computers. A special section will feature three of\n      the most significant “firsts” in personal computing history: The first commercially\n      successful personal computer, the ALTAIR 8800; Steve Job’s and Steve Wozniak’s inaugural\n      creation, the Apple-1; and the SOL-20, the first fully assembled microcomputer, the\n      prototype of what would be known as a home computer.\n   With the help of so many contributors, experts, and partners, the HNF will make 2025\n      a very special anniversary. This year will make the world’s largest computer museum\n      an even more exciting destination for people of all ages.\n   David Woitkowski \n   Heinz Nixdorf MuseumsForum 33102 Paderborn, Germany\nFOOTNOTES\n   1Read more about these exhibits and more at our permanent exhibit page: https://www.hnf.de/en/permanent-exhibition.html\n\n   2Read more: https://www.hnf.de/en/permanent-exhibition/exhibition-areas/computers-in-science-and-technology-1950-to-1970/supercomputers-modelling-reality.html\n\n   3Read more: https://www.hnf.de/en/permanent-exhibition/exhibition-areas/computers-in-business-and-professions-1970-to-1980/printers-black-on-white.html\n\n   4View the full Interview (German): https://www.youtube.com/watch?v=0Z5Uvr-aH1Y\n\n   5Read more: https://www.hnf.de/en/permanent-exhibition/exhibition-areas/everything-goes-digital/man-robots-living-with-artificial-intelligence-and-robotics/replica-of-theseus.html\n\n   6Claude E. Shannon: A Mind-Reading (?) Machine. Bell Laboratories Memorandum, March\n      18, 1953. https://this1that1whatever.com/miscellany/mind-reader/Shannon-Mind-Reading.pdf\n\n   7Read more: https://rclab.de/en/shannon/inhalt\n\n   8Read more (German): https://blog.hnf.de/vor-70-jahren-ein-elektronisches-gehirn-in-kiel/\n\n   9Watch (German): https://www.youtube.com/watch?v=6O_88kqs9Jo\n\n   10A chandelier cryostat is used to obtain the very low temperatures needed to operate\n      most current quantum computer platforms. As it is usually the largest part of the\n      machine, it dominates the illustration of the topic as a whole.\n\n   11Read more: https://www.hnf.de/en/permanent-exhibition/exhibition-areas/computers-for-everyone-1980-to-2000/silicon-valley-a-californian-dream.html\n\n   12Read more: https://www.hnf.de/en/permanent-exhibition/exhibition-areas/computers-for-everyone-1980-to-2000/the-birth-of-the-pc-from-the-garage-to-the-big-wide-world.html",
    "summary": {
      "en": "Heinz Nixdorf, a key figure in computer history, was born on April 9, 1925, in Germany. He founded Nixdorf Computer AG in 1952, which later became the Heinz Nixdorf MuseumsForum (HNF) after his death in 1986. The HNF is celebrating his legacy with new exhibitions on computer technology in 2024 and a special anniversary in 2025.\n\nIn 2024, the HNF updated its permanent exhibits to include the history of analog computers, digital photography, hacker culture, and more. Notable new displays include a remodeled supercomputer exhibit featuring early models and an area dedicated to printing technology, showcasing devices like the 1960 Xerox 1385 photocopier.\n\nThe museum will also highlight unique innovations, such as the Maschine 1, a wearable computer designed for Blackjack, and Claude Shannon’s Mind Reading Machine, showcasing early developments in computer science.\n\nIn 2025, HNF will host events celebrating Nixdorf’s 100th birthday, including the reopening of exhibitions focused on his work and significant advancements in quantum computing. Special exhibits will cover the history of Silicon Valley and the evolution of personal computers, featuring iconic models like the Apple-1 and the ALTAIR 8800. The HNF aims to engage visitors of all ages with these exciting developments.",
      "ko": "하인츠 닉스도르프는 컴퓨터 역사에서 중요한 인물로, 1925년 4월 9일 독일에서 태어났습니다. 그는 1952년에 닉스도르프 컴퓨터 AG를 설립하였고, 1986년 그의 사망 이후 이 회사는 하인츠 닉스도르프 박물관 포럼(HNF)으로 발전했습니다. HNF는 2024년에 컴퓨터 기술에 관한 새로운 전시를 선보이며 그의 유산을 기념하고, 2025년에는 특별한 기념행사를 계획하고 있습니다.\n\n2024년에는 HNF의 상설 전시가 업데이트되어 아날로그 컴퓨터의 역사, 디지털 사진, 해커 문화 등을 포함하게 됩니다. 특히 주목할 만한 새로운 전시로는 초기 모델을 포함한 리모델링된 슈퍼컴퓨터 전시와 1960년대 Xerox 1385 복사기와 같은 인쇄 기술을 보여주는 공간이 있습니다.\n\n박물관은 블랙잭을 위해 설계된 웨어러블 컴퓨터인 마신 1과 클로드 섀넌의 마음 읽기 기계와 같은 독특한 혁신도 강조할 예정입니다. 이는 컴퓨터 과학의 초기 발전을 보여주는 전시입니다.\n\n2025년에는 닉스도르프의 100번째 생일을 기념하는 행사가 열리며, 그의 작업과 양자 컴퓨팅의 중요한 발전에 초점을 맞춘 전시가 다시 열릴 예정입니다. 특별 전시는 실리콘 밸리의 역사와 개인용 컴퓨터의 발전을 다루며, 애플-1과 ALTAIR 8800과 같은 상징적인 모델을 소개할 것입니다. HNF는 이러한 흥미로운 발전을 통해 모든 연령대의 방문객들과 소통할 계획입니다.",
      "ja": null
    }
  },
  {
    "id": "fcd27f228533181e",
    "title": {
      "en": "The role of developer skills in agentic coding",
      "ko": "개발자 기술의 힘",
      "ja": null
    },
    "type": "story",
    "url": "https://martinfowler.com/articles/exploring-gen-ai.html#memo-13",
    "score": 313,
    "by": "BerislavLopac",
    "time": 1742988252,
    "content": "content\n\nVideos\n\nContent Index\n\nBoard Games\n\nPhotography",
    "summary": {
      "en": "The text includes a list of topics: videos, a content index, board games, and photography.",
      "ko": "이 텍스트는 여러 주제를 포함하고 있습니다. 첫 번째는 비디오입니다. 다양한 비디오 콘텐츠가 포함되어 있으며, 흥미로운 주제를 다루고 있습니다. 두 번째는 콘텐츠 색인입니다. 이는 특정 주제나 항목을 쉽게 찾을 수 있도록 정리된 목록입니다. 세 번째는 보드 게임입니다. 여러 종류의 보드 게임이 소개되며, 가족이나 친구들과 함께 즐길 수 있는 활동입니다. 마지막으로 사진 촬영에 대한 내용이 있습니다. 이는 사진을 찍는 기술이나 팁을 공유하며, 더 나은 사진을 찍는 데 도움을 줍니다.",
      "ja": null
    }
  },
  {
    "id": "966f5fb1d6acab8b",
    "title": {
      "en": "Building a Linux Container Runtime from Scratch",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://edera.dev/stories/styrolite",
    "score": 203,
    "by": "curmudgeon22",
    "time": 1743021346,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "aa6e90a7c05a4507",
    "title": {
      "en": "Botswana launches first satellite BOTSAT-1 aboard SpaceX Falcon 9",
      "ko": "보츠와나 첫 위성 발사!",
      "ja": null
    },
    "type": "story",
    "url": "https://spaceinafrica.com/2025/03/15/botswana-successfully-launches-first-satellite-botsat-1/",
    "score": 386,
    "by": "vinnyglennon",
    "time": 1743004976,
    "content": "Botswana Successfully Launches First Satellite, BOTSAT-1March 15, 2025|In News, Satellite Launch|By Samuel Nyangi\n\nLaunch of BOTSAT-1 from a SpaceX Falcon 9 heavy rocket. Source: Space X\nBotswana’s first satellite, BOTSAT-1, was successfully launched aboard SpaceX’s Falcon 9—Transporter 13 rideshare mission on Saturday, March 15, 2025, from the Vandenberg Space Force Base, USA. The Falcon 9 rocket lifted off the Space Launch Complex 4E (SLC-4E) at 06:39 am GMT, carrying 74 satellites. These included BOTSAT-1, 26 satellites as part of the Transporter-13 rideshare mission, and a trio of CubeSats for NASA’s Electrojet Zeeman Imaging Explorer (EZIE) mission; Arvaker 1, the first microsatellite for Kongsberg NanoAvionics’ N3X constellation.\nClick here to watch the launch replay.\nBOTSAT-1 is a 3U hyperspectral Earth Observation satellite developed by the Botswana International University of Science and Technology (BIUST) and led by Dr Dimane Mpoeleng, BOTSAT-1’s Project Lead. The project was done in collaboration with EnduroSat, the satellite’s lead manufacturer. Similarly, ExoLaunch managed the launch in partnership with Space X as part of the Transporter missions, a smallsat rideshare programme.\nTransporter-13 payloads overview. Source: SpaceX\nBotswana’s President, Duma Gideon Boko, attended the launch of BOTSAT-1 as part of his working visit to the United States of America, accompanied by BIUST Vice Chancellor Professor Totolo and other senior government officials. The President also visited various sites such as the Space X Rocket Production and Starlink Mini Production Facilities, signalling future increased collaborations with the company. The launch of BOTSAT-1 as the country’s first satellite demonstrates its space capabilities and positions Botswana to fully utilise space as a resource for its national development.\nKey Mission Objectives\nLaunched at a near-polar and sun-synchronous orbit at approximately 500 kilometres, BOTSAT-1 will provide crucial Earth Observation data for the country’s national development priorities: food security, environmental conservation and urban planning. Additionally, the project has allowed for intensive Human Capacity Development (HCD) among Botswana’s engineers, who have gained practical skills in satellite development. This strengthens the country’s technical capabilities and positions it to lead future developments, such as BOTSAT-2, the country’s second satellite whose specifications are currently being developed.\n\nDr Modisa Mosalaosi, BOTSAT Lead Engineer with the BOTSAT-1 satellite during testing. Source: Botswana International University of Science and Technology (BIUST)\nBOTSAT-1 will transmit data through its fully operational ground station located at BIUST. The satellite will scan Botswana from east to west, covering a swath of approximately 32 kilometres with a resolution of 12 meters. Conceptualised in 2020, the satellite has passed through various stages of development and completed its Assembly, Integration and Testing (AIT) phase in September 2024, marking the last significant milestone before its transportation to its launch location.\nThe launch of BOTSAT-1 brings Africa’s total satellite launches to 65, joining 10 other countries operating Earth Observation satellites leveraging satellite data to tackle regional challenges in agriculture, resource management, and disaster preparedness.\nBotswana Advances Space Technology with Dragonfly Aerospace Partnership\nAs part of Botswana’s push to integrate cutting-edge technology into its space initiatives, the mission employs the Mantis Hyperspectral Imager from Dragonfly Aerospace, a South African-manufactured system providing advanced imaging capabilities. Furthermore, the imager delivers detailed spectral insights for environmental monitoring, resource management, and agricultural research—critical applications for sustainable development across Africa.\nBeyond supplying state-of-the-art imaging technology, Dragonfly Aerospace is supporting the Botswana International University of Science and Technology (BIUST) in developing its clean room facilities. These controlled environments, essential for satellite assembly, integration, and testing (AIT), will feature industry-standard air filtration systems, static control measures, and environmental monitoring tools to meet stringent space industry requirements.\nThis initiative will enable BIUST to build a sustainable pipeline of space technology projects while facilitating hands-on learning opportunities for students and researchers. Additionally, Dragonfly Aerospace has provided a 3U satellite structure for demonstration and training purposes, further enhancing BIUST’s satellite development capabilities.\n\nBOTSAT-1\nBotswana\nBotswana International University of Science and Technology (BIUST)\nDragonfly Aerospace\n\n\t{\"slug\":\"widget-events-list\",\"prev_url\":\"\",\"next_url\":\"\",\"view_class\":\"Tribe\\\\Events\\\\Views\\\\V2\\\\Views\\\\Widgets\\\\Widget_List_View\",\"view_slug\":\"widget-events-list\",\"view_label\":\"View\",\"view\":null,\"should_manage_url\":true,\"id\":null,\"alias-slugs\":null,\"title\":\"Botswana Successfully Launches First Satellite, BOTSAT-1 - Space in Africa\",\"limit\":\"5\",\"no_upcoming_events\":false,\"featured_events_only\":true,\"jsonld_enable\":true,\"tribe_is_list_widget\":false,\"admin_fields\":{\"title\":{\"label\":\"Title:\",\"type\":\"text\",\"classes\":\"\",\"dependency\":\"\",\"id\":\"widget-tribe-widget-events-list-2-title\",\"name\":\"widget-tribe-widget-events-list[2][title]\",\"options\":[],\"placeholder\":\"\",\"value\":null},\"limit\":{\"label\":\"Show:\",\"type\":\"number\",\"default\":5,\"min\":1,\"max\":10,\"step\":1,\"classes\":\"\",\"dependency\":\"\",\"id\":\"widget-tribe-widget-events-list-2-limit\",\"name\":\"widget-tribe-widget-events-list[2][limit]\",\"options\":[],\"placeholder\":\"\",\"value\":null},\"no_upcoming_events\":{\"label\":\"Hide this widget if there are no upcoming events.\",\"type\":\"checkbox\",\"classes\":\"\",\"dependency\":\"\",\"id\":\"widget-tribe-widget-events-list-2-no_upcoming_events\",\"name\":\"widget-tribe-widget-events-list[2][no_upcoming_events]\",\"options\":[],\"placeholder\":\"\",\"value\":null},\"featured_events_only\":{\"label\":\"Limit to featured events only\",\"type\":\"checkbox\",\"classes\":\"\",\"dependency\":\"\",\"id\":\"widget-tribe-widget-events-list-2-featured_events_only\",\"name\":\"widget-tribe-widget-events-list[2][featured_events_only]\",\"options\":[],\"placeholder\":\"\",\"value\":null},\"jsonld_enable\":{\"label\":\"Generate JSON-LD data\",\"type\":\"checkbox\",\"classes\":\"\",\"dependency\":\"\",\"id\":\"widget-tribe-widget-events-list-2-jsonld_enable\",\"name\":\"widget-tribe-widget-events-list[2][jsonld_enable]\",\"options\":[],\"placeholder\":\"\",\"value\":null}},\"events\":[],\"url\":\"https:\\/\\/spaceinafrica.com\\/?post_type=tribe_events&eventDisplay=widget-events-list&featured=1\",\"url_event_date\":false,\"bar\":{\"keyword\":\"\",\"date\":\"\"},\"today\":\"2025-03-27 00:00:00\",\"now\":\"2025-03-27 17:42:00\",\"rest_url\":\"https:\\/\\/spaceinafrica.com\\/wp-json\\/tribe\\/views\\/v2\\/html\",\"rest_method\":\"POST\",\"rest_nonce\":\"\",\"today_url\":\"https:\\/\\/spaceinafrica.com\\/?post_type=tribe_events&eventDisplay=widget-events-list&featured=1&year=2025&monthnum=03&day=15&name=botswana-successfully-launches-first-satellite-botsat-1\",\"today_title\":\"Click to select today's date\",\"today_label\":\"Today\",\"prev_label\":\"\",\"next_label\":\"\",\"date_formats\":{\"compact\":\"n\\/j\\/Y\",\"month_and_year_compact\":\"n\\/Y\",\"month_and_year\":\"F Y\",\"time_range_separator\":\" - \",\"date_time_separator\":\" @ \"},\"messages\":{\"notice\":[\"There are no upcoming events.\"]},\"start_of_week\":\"1\",\"header_title\":\"Featured\",\"header_title_element\":\"h1\",\"content_title\":\"\",\"breadcrumbs\":[],\"before_events\":\"\",\"after_events\":\"\\n<!--\\nThis calendar is powered by The Events Calendar.\\nhttp:\\/\\/evnt.is\\/18wn\\n-->\\n\",\"display_events_bar\":false,\"disable_event_search\":false,\"live_refresh\":true,\"ical\":{\"display_link\":true,\"link\":{\"url\":\"https:\\/\\/spaceinafrica.com\\/?post_type=tribe_events&#038;eventDisplay=widget-events-list&#038;featured=1&#038;ical=1\",\"text\":\"Export Events\",\"title\":\"Use this to share calendar data with Google Calendar, Apple iCal and other compatible apps\"}},\"container_classes\":[\"tribe-common\",\"tribe-events\",\"tribe-events-view\",\"tribe-events-view--widget-events-list\",\"tribe-events-widget\"],\"container_data\":[],\"is_past\":false,\"breakpoints\":{\"xsmall\":500,\"medium\":768,\"full\":960},\"breakpoint_pointer\":\"bae04905-deef-48ab-aca8-6ebf4f6d0e01\",\"is_initial_load\":true,\"public_views\":{\"list\":{\"view_class\":\"Tribe\\\\Events\\\\Views\\\\V2\\\\Views\\\\List_View\",\"view_url\":\"https:\\/\\/spaceinafrica.com\\/events\\/list\\/featured\\/\",\"view_label\":\"List\"},\"month\":{\"view_class\":\"Tribe\\\\Events\\\\Views\\\\V2\\\\Views\\\\Month_View\",\"view_url\":\"https:\\/\\/spaceinafrica.com\\/events\\/month\\/featured\\/\",\"view_label\":\"Month\"},\"day\":{\"view_class\":\"Tribe\\\\Events\\\\Views\\\\V2\\\\Views\\\\Day_View\",\"view_url\":\"https:\\/\\/spaceinafrica.com\\/events\\/today\\/featured\\/\",\"view_label\":\"Day\"}},\"show_latest_past\":false,\"past\":false,\"compatibility_classes\":[\"tribe-compatibility-container\"],\"view_more_text\":\"View Calendar\",\"view_more_title\":\"View more events.\",\"view_more_link\":\"https:\\/\\/spaceinafrica.com\\/events\\/\",\"widget_title\":\"Meet the Space in Africa Team\",\"hide_if_no_upcoming_events\":false,\"display\":[],\"subscribe_links\":{\"gcal\":{\"label\":\"Google Calendar\",\"single_label\":\"Add to Google Calendar\",\"visible\":true,\"block_slug\":\"hasGoogleCalendar\"},\"ical\":{\"label\":\"iCalendar\",\"single_label\":\"Add to iCalendar\",\"visible\":true,\"block_slug\":\"hasiCal\"},\"outlook-365\":{\"label\":\"Outlook 365\",\"single_label\":\"Outlook 365\",\"visible\":true,\"block_slug\":\"hasOutlook365\"},\"outlook-live\":{\"label\":\"Outlook Live\",\"single_label\":\"Outlook Live\",\"visible\":true,\"block_slug\":\"hasOutlookLive\"}},\"_context\":{\"slug\":\"widget-events-list\"}}\n\n\t\t\t\t\t\tMeet the Space in Africa Team\n\n\t\t\t\t\tThere are no upcoming events.\n\n\t( function () {\n\t\tvar completed = false;\n\n\t\tfunction initBreakpoints() {\n\t\t\tif ( completed ) {\n\t\t\t\t// This was fired already and completed no need to attach to the event listener.\n\t\t\t\tdocument.removeEventListener( 'DOMContentLoaded', initBreakpoints );\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tif ( 'undefined' === typeof window.tribe ) {\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tif ( 'undefined' === typeof window.tribe.events ) {\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tif ( 'undefined' === typeof window.tribe.events.views ) {\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tif ( 'undefined' === typeof window.tribe.events.views.breakpoints ) {\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tif ( 'function' !== typeof (window.tribe.events.views.breakpoints.setup) ) {\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tvar container = document.querySelectorAll( '[data-view-breakpoint-pointer=\"bae04905-deef-48ab-aca8-6ebf4f6d0e01\"]' );\n\t\t\tif ( ! container ) {\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\twindow.tribe.events.views.breakpoints.setup( container );\n\t\t\tcompleted = true;\n\t\t\t// This was fired already and completed no need to attach to the event listener.\n\t\t\tdocument.removeEventListener( 'DOMContentLoaded', initBreakpoints );\n\t\t}\n\n\t\t// Try to init the breakpoints right away.\n\t\tinitBreakpoints();\n\t\tdocument.addEventListener( 'DOMContentLoaded', initBreakpoints );\n\t})();\n\n\t\tLatest Articles\n\n\t\t\t\t\tWho Manufactured Africa’s 65 Satellites?\n\t\t\t\t\t\t\t\t\t\t\tMarch 27, 2025\n\n\t\t\t\t\tASI Renews Agreement for Kenyan Personnel at the Luigi Broglio Space Centre\n\t\t\t\t\t\t\t\t\t\t\tMarch 27, 2025\n\n\t\t\t\t\tEgSA and AQMAAR Partner to Strengthen Domestic Satellite Manufacturing\n\t\t\t\t\t\t\t\t\t\t\tMarch 25, 2025\n\n\t\t\t\t\tSouth Africa and China Establish 12,900 km Quantum Satellite Link\n\t\t\t\t\t\t\t\t\t\t\tMarch 20, 2025\n\n\t\t\t\t\tStarlink Kenya Rises to Seventh-Largest ISP With 19,146 Subscribers\n\t\t\t\t\t\t\t\t\t\t\tMarch 19, 2025",
    "summary": {
      "en": "Botswana successfully launched its first satellite, BOTSAT-1, on March 15, 2025, using SpaceX's Falcon 9 rocket from the Vandenberg Space Force Base in the USA. The launch was part of a mission that included 74 satellites. BOTSAT-1, a 3U hyperspectral Earth Observation satellite, was developed by the Botswana International University of Science and Technology (BIUST) in collaboration with EnduroSat and managed by ExoLaunch.\n\nThe launch was attended by Botswana's President, Duma Gideon Boko, and other officials, highlighting the country's commitment to space technology for national development. BOTSAT-1, operating at around 500 kilometers in a sun-synchronous orbit, aims to collect data for food security, environmental conservation, and urban planning, while also helping to train Botswana's engineers in satellite development.\n\nThe satellite will transmit data from its ground station at BIUST, covering an area of 32 kilometers with a resolution of 12 meters. BOTSAT-1 marks an important step for Botswana in leveraging space resources for development and sets the stage for future projects like BOTSAT-2. Additionally, the partnership with Dragonfly Aerospace enhances Botswana's technical capabilities and supports the development of a clean room facility for satellite work at BIUST.",
      "ko": "보츠와나가 2025년 3월 15일, 미국 반덴버그 우주군 기지에서 스페이스X의 팔콘 9 로켓을 이용해 첫 번째 위성인 BOTSAT-1을 성공적으로 발사했습니다. 이번 발사는 74개의 위성을 포함하는 미션의 일환으로 진행되었습니다. BOTSAT-1은 3U 하이퍼스펙트럴 지구 관측 위성으로, 보츠와나 국제 과학기술대학교(BIUST)와 엔듀로샛의 협력으로 개발되었으며, 엑소론치가 관리하고 있습니다.\n\n발사 행사에는 보츠와나의 두마 기디온 보코 대통령과 여러 관계자들이 참석하여 국가 발전을 위한 우주 기술에 대한 보츠와나의 의지를 강조했습니다. BOTSAT-1은 약 500킬로미터의 태양 동기 궤도에서 운영되며, 식량 안전, 환경 보존, 도시 계획을 위한 데이터를 수집하는 것을 목표로 하고 있습니다. 또한 이 위성은 보츠와나의 엔지니어들이 위성 개발을 배우는 데도 도움을 줄 것입니다.\n\n위성은 BIUST의 지상국에서 데이터를 전송하며, 32킬로미터의 범위를 커버하고 해상도는 12미터입니다. BOTSAT-1은 보츠와나가 우주 자원을 활용하여 발전하는 데 중요한 이정표가 되며, BOTSAT-2와 같은 미래 프로젝트를 위한 기반을 마련합니다. 또한 드래곤플라이 우주항공과의 파트너십은 보츠와나의 기술 능력을 향상시키고 BIUST에서 위성 작업을 위한 클린룸 시설 개발을 지원합니다.",
      "ja": null
    }
  },
  {
    "id": "9e5f0a0703929b17",
    "title": {
      "en": "Sharding Pgvector",
      "ko": "샤딩 Pgvector",
      "ja": null
    },
    "type": "story",
    "url": "https://pgdog.dev/blog/sharding-pgvector",
    "score": 85,
    "by": "levkk",
    "time": 1743009030,
    "content": "Sharding pgvector\n        Mar 25th, 2025Lev Kokotov\n       If you find yourself working with embeddings, you’ve shopped around for a vector database. pgvector is a great option if you’re using Postgres already. Once you reach a certain scale (about a million arrays), building indices starts taking a long time. Some workarounds, like parallel workers, help, but you still need to fit the whole graph in memory.\n\nThe solution to lots of data is more compute, so we sharded pgvector. In this context, we’re specifically talking about sharding a vector index. Splitting tables between machines is easier. Our goal is to have fast and good recall: we want to quickly find matches when we search for them.\n\nA bit of background\n\npgvector has two types of indexes: HNSW and IVFFlat. They are different ways to organize vectors in multi-dimensional space. HNSW builds a multi-layer graph that can be searched quickly, in O(log(n)) time. The trade-off is it takes a long time to build.\n\nIVFFlat splits the vector space into parts, grouped around centroids, i.e., points in space that seem to be in the center of something. Finding that something is done using K-means, a machine learning (mathematical, really) algorithm for grouping arbitrary things. IVFFlat index is quick to build, but slower to search. Once the graph is split into parts, searching becomes roughly O(sqrt(n))). Lastly, to find centroids, you need to have a representative sample of your vectors beforehand. Centroids are specific to your data and are not generalizable.\n\nBoth of these algorithms work well when parts of the graph are hot and others cold. The hot area grows as your company does, since more users search for more stuff. Once the hot area exceeds your memory, things slow down. If your algorithm has a linear-time component, even searching in memory isn’t always as quick as you’d like.\n\nSharding the index\n\nUsing first principles, sharding a vector index means splitting it into parts. Stating the obvious is sometimes a good idea. When you say that out loud, IVFFLat becomes the obvious choice. It does that already. It splits embeddings into groups using K-means. That means, we can do the same thing for sharding. Except, instead of placing the parts on the same machine, let’s place them on multiple. When we search for a match, we can pick a host (or multiple), based on a query parameter.\n\nTo test this idea, we took a publicly available dataset from HuggingFace. It’s called Cohere/wikipedia and it’s an encoding of the entire English Wikipedia using an AI embedding model. It produces embeddings with 768 points, so our vector in Postgres is pretty large:\n\nCREATE TABLE embeddings (\n  title text,\n  body text,\n  embedding vector(768),\n);\n\nWikipedia articles have a title and a body. The model generates a mathematical summary of what the article is about so we can easily find articles based on some context, provided by the user. This is called semantic search and it’s used everywhere, including Google.\n\nWith the table schema ready, the next step is to figure out the centroids. For this, we used scikit-learn. It’s super simple to use and has all the algorithms we need. After downloading a sample from HuggingFace (the dataset is split for us already), calculating K-means was pretty simple. I’ll save you the trouble of looking at code now. All of it is available in our GitHub repository.\n\nHave you ever wondered what the summary of all human knowledge looks like on a graph? Here it is, reduced to 2 dimensions using PCA:\n\nThe little red x’s are centroids. We calculated 16 because it seems like a good number to start with. For our purposes, we need as many centroids as we have shards. If you know a data scientist, they will tell you how many is a good idea for your dataset.\n\nPlacement of data is important and we’ll cover it in a minute. For now, our sharding function is:\n\nshards = pick(min(l2_distance(vector, centroids)) mod shards, probes)\n\nUsing Euclidean distance, we’re looking for the shards that have the centroids closest to the vector in the query. If you’re using pgvector already, you know it as the <-> operator or the vector_l2_ops index type.\n\nSince we’re using IVFFlat, we know that 1 probe isn’t enough, so we pick multiple. You can configure how many you’d like in pgdog.toml. By default, we are setting it to sqrt(centroids), in our case, that’s 4. This is analogous to the ivfflat.probes  setting in pgvector and improves recall, at the cost of compute utilization. The performance does not suffer that much, since the lookups are done in parallel over warm connections.\n\nAdding support for vectors in PgDog was straightforward. We are using pg_query, a Rust crate that directly bundles the PostgreSQL query parser. PgDog can read all valid Postgres queries. When searching for a match, typically, you’re trying to rank vectors by distance. The closer they are to each other, the better the semantic match between data they represent.\n\nTo make this work, we parse the ORDER BY clause that contains the <-> operator and route it to the nearest shards to the parameter:\n\nSELECT title, body, embedding FROM embeddings\nORDER BY embedding <-> $1;\n\n$1 is just a placeholder for an embedding passed in by a caller using prepared statements. In reality, it’s a vector with 768 dimensions that your application generated, using the same AI model, for a search query.\n\nRecall\n\nThe results were pretty good. You can reproduce them using the code in GitHub, but here is one:\n\ntitle   |                                                                                                         body\n----------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nSpecies  | Fewer than a quarter of the species have not been identified, named and catalogued. At the present rate, it might take over 1000 years to complete this job. Some will become extinct before this count is complete.\nLocust   | The origin and apparent extinction of certain species of locust—some of which grew to in length—are unclear.\nLizard   | There are other versions, and the taxonomy will probably not settle until more molecular evidence is collected.\nFish     | Fish used to be a class of vertebrates. Now the term covers five classes of animals that live in the water:\nPheasant | In many countries pheasant species are hunted, often illegally, as game, and several species are threatened by this and other human activities.\n(5 rows)\n\nAt the first glance, these seem pretty similar. They all mention some kind of animal and use words like species. I picked one of the centroids as my parameter, so it makes sense: a large section of Wikipedia talks about animals.\n\nWe did a more scientific measurement. We took 1,000 embeddings and looked for neighbors that were 0.1 units away, in ascending order. With 1 shard, we were getting results for 96% of our queries. With 16 we were able to get all queries to return something.\n\nIVFFlat is an approximation. In reality, what you’re looking for could be an outlier, so the centroid you picked can be wrong, even if it seemed close. More often, it just places points in the wrong cluster and your query misses it. It’s all about trade-offs.\n\nThe simplest workaround is to not use IVFFlat at all. Just split data across all shards evenly, using some other sharding key, and talk to all of them for each query. PgDog supports parallel cross-shard queries, so a map-reduce across your n vector indexes (or even just table scans, for perfect recall) is feasible.\n\nWe could also “bin pack” nearby centroids on the same shards. Doing multiple probes becomes cheaper: they are already on the same machine and fit in memory. For this to work, you’d need to increase your original K-means centroids number to something high enough that will split your dataset just right. Some experimentation here is needed.\n\nQuick clarification around choice of algorithm for sharding. IVFFLat is only used for shard selection in PgDog. Each shard (a Postgres database) can use either IVFFlat or HNSW or no index at all. If using an index, you’ll notice performance benefits of sharding immediately, especially around initial construction. The trade-off is reduced recall from stacking an approximation (PgDog index) on top of another (database index).\n\nNext steps\n\nMore distance algorithms, like cosine similarity (<=>), negative inner product (<#>), and others are on the roadmap. We also need to use SIMD instructions for L2 (and all others), so we can crunch numbers faster. This matters for large embeddings and the difference is noticeable, even in testing.\n\nPgDog is just getting started. If this looks interesting, get in touch. PgDog is an open source project for sharding Postgres and is looking for early design partners.",
    "summary": {
      "en": "### Summary of Sharding pgvector\n\n**Overview**\npgvector is a tool for handling embeddings in Postgres, but as data scales (around a million arrays), indexing becomes slow. A solution to manage large datasets is sharding, which involves splitting the vector index across multiple machines to improve search speed and recall.\n\n**Index Types**\npgvector has two index types:\n1. **HNSW**: Fast searching (O(log(n))) but slow to build.\n2. **IVFFlat**: Quick to build but slower to search (O(sqrt(n))). It organizes vectors into groups based on centroids.\n\n**Sharding Approach**\nSharding involves splitting data into parts, similar to how IVFFlat groups vectors. This allows parts to be distributed across multiple machines, enabling faster searches. The process starts by calculating centroids using K-means clustering.\n\n**Testing Sharding**\nThe team tested the concept using the Cohere/wikipedia dataset, which encodes English Wikipedia articles into 768-dimensional embeddings. The sharding function selects shards based on the distance to centroids.\n\n**Results**\nTests showed good results, with high recall rates for similar queries. Using multiple shards improved the number of successful queries significantly.\n\n**Future Steps**\nThe project plans to explore additional distance algorithms and implement SIMD instructions for faster computations. PgDog is open-source and seeking partners for development.",
      "ko": "pgvector는 Postgres에서 임베딩을 처리하는 도구입니다. 그러나 데이터가 커질수록, 예를 들어 백만 개의 배열을 다룰 때 인덱싱 속도가 느려지는 문제가 발생합니다. 대규모 데이터셋을 관리하기 위한 해결책으로 샤딩(sharding)이 있습니다. 샤딩은 벡터 인덱스를 여러 대의 기계에 분산시켜 검색 속도와 재현율을 향상시키는 방법입니다.\n\npgvector는 두 가지 인덱스 유형을 제공합니다. 첫 번째는 HNSW로, 빠른 검색 속도(O(log(n)))를 자랑하지만 구축 속도가 느립니다. 두 번째는 IVFFlat로, 구축 속도는 빠르지만 검색 속도는 느린 편입니다(O(√n)). IVFFlat은 벡터를 중심점(centroid)을 기준으로 그룹화하여 조직합니다.\n\n샤딩은 데이터를 여러 부분으로 나누는 과정으로, IVFFlat이 벡터를 그룹화하는 방식과 유사합니다. 이렇게 나눈 부분들은 여러 대의 기계에 분산되어 더 빠른 검색이 가능해집니다. 이 과정은 K-평균 클러스터링을 사용해 중심점을 계산하는 것으로 시작됩니다.\n\n팀은 Cohere/wikipedia 데이터셋을 사용하여 샤딩 개념을 테스트했습니다. 이 데이터셋은 영어 위키피디아 기사를 768차원 임베딩으로 인코딩합니다. 샤딩 함수는 중심점과의 거리를 기준으로 샤드를 선택합니다.\n\n테스트 결과, 유사한 쿼리에 대해 높은 재현율을 보이며 좋은 성과를 나타냈습니다. 여러 샤드를 사용함으로써 성공적인 쿼리 수가 크게 증가했습니다.\n\n앞으로 이 프로젝트는 추가적인 거리 알고리즘을 탐색하고, 더 빠른 계산을 위해 SIMD 명령어를 구현할 계획입니다. PgDog는 오픈 소스이며 개발 파트너를 찾고 있습니다.",
      "ja": null
    }
  },
  {
    "id": "4006d2e792d2996e",
    "title": {
      "en": "MCP server for Ghidra",
      "ko": "기드라 MCP 서버",
      "ja": null
    },
    "type": "story",
    "url": "https://github.com/LaurieWired/GhidraMCP",
    "score": 347,
    "by": "tanelpoder",
    "time": 1742928457,
    "content": "ghidraMCP\nghidraMCP is an Model Context Protocol server for allowing LLMs to autonomously reverse engineer applications. It exposes numerous tools from core Ghidra functionality to MCP clients.\n\n    ghidraMCP_demo.mp4\n\nFeatures\nMCP Server + Ghidra Plugin\n\nDecompile and analyze binaries in Ghidra\nAutomatically rename methods and data\nList methods, classes, imports, and exports\n\nInstallation\nPrerequisites\n\nMac / Windows\nInstall Ghidra\nPython3\n\nGhidra\nFirst, download the latest release from this repository. This contains the Ghidra plugin and Python MCP client. Then, you can directly import the plugin into Ghidra.\n\nRun Ghidra\nSelect File -> Install Extensions\nClick the + button\nSelect the GhidraMCP-1-0.zip (or your chosen version) from the downloaded release\nRestart Ghidra\nMake sure the GhidraMCPPlugin is enabled in File -> Configure -> Developer\n\nVideo Installation Guide:\n\n    ghidra_MCP_Install.mp4\n\nMCP Clients\nTheoretically, any MCP client should work with ghidraMCP.  Two examples are given below.\nExample 1: Claude Desktop\nTo set up Claude Desktop as a Ghidra MCP client, go to Claude -> Settings -> Developer -> Edit Config -> claude_desktop_config.json and add the following:\n{\n  \"mcpServers\": {\n    \"ghidra\": {\n      \"command\": \"python\",\n      \"args\": [\n        \"/ABSOLUTE_PATH_TO/bridge_mcp_ghidra.py\"\n      ]\n    }\n  }\n}\n\nAlternatively, edit this file directly:\n/Users/YOUR_USER/Library/Application Support/Claude/claude_desktop_config.json\n\nExample 2: 5ire\nAnother MCP client that supports multiple models on the backend is 5ire. To set up GhidraMCP, open 5ire and go to Tools -> New and set the following configurations:\n\nTool Key: ghidra\nName: GhidraMCP\nCommand: python /ABSOLUTE_PATH_TO/bridge_mcp_ghidra.py\n\nBuilding from Source\nBuild with Maven by running:\nmvn clean package assembly:single\nThe generated zip file includes the built Ghidra plugin and its resources. These files are required for Ghidra to recognize the new extension.\n\nlib/GhidraMCP.jar\nextensions.properties\nModule.manifest",
    "summary": {
      "en": "**Summary of ghidraMCP**\n\nghidraMCP is a server that helps Large Language Models (LLMs) to automatically reverse engineer applications using Ghidra, a software analysis tool. It provides various Ghidra features to clients.\n\n**Key Features:**\n- Decompile and analyze binary files.\n- Automatically rename methods and data.\n- List methods, classes, imports, and exports.\n\n**Installation Steps:**\n1. Requirements:\n   - Mac or Windows\n   - Install Ghidra\n   - Install Python 3\n\n2. Installation Process:\n   - Download the latest release from the repository (includes the Ghidra plugin and Python client).\n   - Open Ghidra, go to File -> Install Extensions, and upload the downloaded zip file.\n   - Restart Ghidra and ensure the GhidraMCPPlugin is enabled.\n\n**MCP Client Setup:**\n- Any MCP client can work with ghidraMCP. Two examples are provided:\n  - **Claude Desktop:** Modify the config file to connect to the Ghidra MCP server.\n  - **5ire:** Set up a new tool in the application to link to the GhidraMCP.\n\n**Building from Source:**\n- Use Maven to build the project, which generates necessary files for Ghidra to recognize the extension. \n\nFor visual guidance, installation videos are available.",
      "ko": "ghidraMCP는 Ghidra라는 소프트웨어 분석 도구를 사용하여 대형 언어 모델(LLM)이 애플리케이션을 자동으로 리버스 엔지니어링할 수 있도록 돕는 서버입니다. 이 서버는 클라이언트에게 다양한 Ghidra 기능을 제공합니다.\n\n주요 기능으로는 이진 파일을 디컴파일하고 분석하는 기능, 메서드와 데이터를 자동으로 이름 변경하는 기능, 메서드, 클래스, 임포트 및 익스포트를 나열하는 기능이 있습니다.\n\n설치 과정은 다음과 같습니다. 먼저, Mac 또는 Windows 운영 체제가 필요하며 Ghidra와 Python 3을 설치해야 합니다. 그 후, 저장소에서 최신 릴리스를 다운로드합니다. 이 릴리스에는 Ghidra 플러그인과 Python 클라이언트가 포함되어 있습니다. Ghidra를 열고 파일 메뉴에서 '확장 프로그램 설치'를 선택한 후 다운로드한 zip 파일을 업로드합니다. Ghidra를 재시작하고 GhidraMCPPlugin이 활성화되어 있는지 확인합니다.\n\nMCP 클라이언트는 ghidraMCP와 함께 사용할 수 있으며, 두 가지 예시가 있습니다. 첫 번째는 Claude Desktop으로, 설정 파일을 수정하여 Ghidra MCP 서버에 연결합니다. 두 번째는 5ire로, 애플리케이션 내에서 GhidraMCP와 연결할 새로운 도구를 설정합니다.\n\n소스에서 빌드하려면 Maven을 사용하여 프로젝트를 빌드해야 하며, 이 과정에서 Ghidra가 확장을 인식할 수 있도록 필요한 파일이 생성됩니다. 설치에 대한 시각적 안내를 원하신다면 설치 동영상도 제공됩니다.",
      "ja": null
    }
  },
  {
    "id": "3471f1a0b97b0f6f",
    "title": {
      "en": "Malware found on NPM infecting local package with reverse shell",
      "ko": "NPM 악성코드 발견!",
      "ja": null
    },
    "type": "story",
    "url": "https://www.reversinglabs.com/blog/malicious-npm-patch-delivers-reverse-shell",
    "score": 225,
    "by": "gnabgib",
    "time": 1743011627,
    "content": "AI coding weaponized: What your AppSec team needs to know\n\nThe \"Rules File Backdoor\" attack method is pernicious — and one that can be easily exploited with the rise of \"vibe coding\" and agentic AI.\n\nRead More",
    "summary": {
      "en": "The article discusses a dangerous hacking technique called the \"Rules File Backdoor,\" which can be easily used due to the increasing use of \"vibe coding\" and autonomous AI tools. It highlights the need for Application Security (AppSec) teams to be aware of these vulnerabilities to protect against potential attacks.",
      "ko": "이 기사는 \"룰 파일 백도어\"라는 위험한 해킹 기법에 대해 다루고 있습니다. 이 기법은 \"바이브 코딩\"과 자율 AI 도구의 사용이 증가함에 따라 쉽게 활용될 수 있습니다. 따라서 애플리케이션 보안 팀은 이러한 취약점을 인지하고 잠재적인 공격으로부터 보호할 필요성이 강조됩니다.",
      "ja": null
    }
  },
  {
    "id": "2b7811a964761526",
    "title": {
      "en": "Has the decline of knowledge work begun?",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.nytimes.com/2025/03/25/business/economy/white-collar-layoffs.html",
    "score": 370,
    "by": "pseudolus",
    "time": 1742922153,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "093032a63c2e43f3",
    "title": {
      "en": "Nature of Code",
      "ko": "코드의 본질",
      "ja": null
    },
    "type": "story",
    "url": "https://natureofcode.com/",
    "score": 12,
    "by": "Tomte",
    "time": 1743099208,
    "content": "<picture><source type=\"image/webp\" srcSet=\"/static/18df89731f3e9900d7f93faf5febcb7e/dff21/0.webp 400w,/static/18df89731f3e9900d7f93faf5febcb7e/b2a35/0.webp 800w,/static/18df89731f3e9900d7f93faf5febcb7e/e9d78/0.webp 1600w,/static/18df89731f3e9900d7f93faf5febcb7e/a6352/0.webp 3200w\" sizes=\"(min-width: 1600px) 1600px, 100vw\"/><img data-gatsby-image-ssr=\"\" data-main-image=\"\" style=\"opacity:0\" sizes=\"(min-width: 1600px) 1600px, 100vw\" decoding=\"async\" loading=\"lazy\" src=\"/static/18df89731f3e9900d7f93faf5febcb7e/f6810/0.jpg\" srcSet=\"/static/18df89731f3e9900d7f93faf5febcb7e/eee8e/0.jpg 400w,/static/18df89731f3e9900d7f93faf5febcb7e/1e21a/0.jpg 800w,/static/18df89731f3e9900d7f93faf5febcb7e/f6810/0.jpg 1600w,/static/18df89731f3e9900d7f93faf5febcb7e/13708/0.jpg 3200w\" alt=\"nature of code book cover on the front\"/></picture>const t=\"undefined\"!=typeof HTMLImageElement&&\"loading\"in HTMLImageElement.prototype;if(t){const t=document.querySelectorAll(\"img[data-main-image]\");for(let e of t){e.dataset.src&&(e.setAttribute(\"src\",e.dataset.src),e.removeAttribute(\"data-src\")),e.dataset.srcset&&(e.setAttribute(\"srcset\",e.dataset.srcset),e.removeAttribute(\"data-srcset\"));const t=e.parentNode.querySelectorAll(\"source[data-srcset]\");for(let e of t)e.setAttribute(\"srcset\",e.dataset.srcset),e.removeAttribute(\"data-srcset\");e.complete&&(e.style.opacity=1,e.parentNode.parentNode.querySelector(\"[data-placeholder-image]\").style.opacity=0)}}<picture><source type=\"image/webp\" srcSet=\"/static/d66de958f5eed9a45e99e2858fdbc2ed/dff21/1.webp 400w,/static/d66de958f5eed9a45e99e2858fdbc2ed/b2a35/1.webp 800w,/static/d66de958f5eed9a45e99e2858fdbc2ed/e9d78/1.webp 1600w,/static/d66de958f5eed9a45e99e2858fdbc2ed/a6352/1.webp 3200w\" sizes=\"(min-width: 1600px) 1600px, 100vw\"/><img data-gatsby-image-ssr=\"\" data-main-image=\"\" style=\"opacity:0\" sizes=\"(min-width: 1600px) 1600px, 100vw\" decoding=\"async\" loading=\"lazy\" src=\"/static/d66de958f5eed9a45e99e2858fdbc2ed/f6810/1.jpg\" srcSet=\"/static/d66de958f5eed9a45e99e2858fdbc2ed/eee8e/1.jpg 400w,/static/d66de958f5eed9a45e99e2858fdbc2ed/1e21a/1.jpg 800w,/static/d66de958f5eed9a45e99e2858fdbc2ed/f6810/1.jpg 1600w,/static/d66de958f5eed9a45e99e2858fdbc2ed/13708/1.jpg 3200w\" alt=\"a bright pink cover with white text and subtle wavy patterns.\"/></picture>const t=\"undefined\"!=typeof HTMLImageElement&&\"loading\"in HTMLImageElement.prototype;if(t){const t=document.querySelectorAll(\"img[data-main-image]\");for(let e of t){e.dataset.src&&(e.setAttribute(\"src\",e.dataset.src),e.removeAttribute(\"data-src\")),e.dataset.srcset&&(e.setAttribute(\"srcset\",e.dataset.srcset),e.removeAttribute(\"data-srcset\"));const t=e.parentNode.querySelectorAll(\"source[data-srcset]\");for(let e of t)e.setAttribute(\"srcset\",e.dataset.srcset),e.removeAttribute(\"data-srcset\");e.complete&&(e.style.opacity=1,e.parentNode.parentNode.querySelector(\"[data-placeholder-image]\").style.opacity=0)}}<picture><source type=\"image/webp\" srcSet=\"/static/4542c1a0f08793f6702b97d33b4d3678/dff21/2.webp 400w,/static/4542c1a0f08793f6702b97d33b4d3678/b2a35/2.webp 800w,/static/4542c1a0f08793f6702b97d33b4d3678/e9d78/2.webp 1600w,/static/4542c1a0f08793f6702b97d33b4d3678/a6352/2.webp 3200w\" sizes=\"(min-width: 1600px) 1600px, 100vw\"/><img data-gatsby-image-ssr=\"\" data-main-image=\"\" style=\"opacity:0\" sizes=\"(min-width: 1600px) 1600px, 100vw\" decoding=\"async\" loading=\"lazy\" src=\"/static/4542c1a0f08793f6702b97d33b4d3678/f6810/2.jpg\" srcSet=\"/static/4542c1a0f08793f6702b97d33b4d3678/eee8e/2.jpg 400w,/static/4542c1a0f08793f6702b97d33b4d3678/1e21a/2.jpg 800w,/static/4542c1a0f08793f6702b97d33b4d3678/f6810/2.jpg 1600w,/static/4542c1a0f08793f6702b97d33b4d3678/13708/2.jpg 3200w\" alt=\"nature of code book cover on the back\"/></picture>const t=\"undefined\"!=typeof HTMLImageElement&&\"loading\"in HTMLImageElement.prototype;if(t){const t=document.querySelectorAll(\"img[data-main-image]\");for(let e of t){e.dataset.src&&(e.setAttribute(\"src\",e.dataset.src),e.removeAttribute(\"data-src\")),e.dataset.srcset&&(e.setAttribute(\"srcset\",e.dataset.srcset),e.removeAttribute(\"data-srcset\"));const t=e.parentNode.querySelectorAll(\"source[data-srcset]\");for(let e of t)e.setAttribute(\"srcset\",e.dataset.srcset),e.removeAttribute(\"data-srcset\");e.complete&&(e.style.opacity=1,e.parentNode.parentNode.querySelector(\"[data-placeholder-image]\").style.opacity=0)}}<picture><source type=\"image/webp\" srcSet=\"/static/b9d29c9de6785d78ebd6bdd3858fa0fe/dff21/3.webp 400w,/static/b9d29c9de6785d78ebd6bdd3858fa0fe/b2a35/3.webp 800w,/static/b9d29c9de6785d78ebd6bdd3858fa0fe/e9d78/3.webp 1600w,/static/b9d29c9de6785d78ebd6bdd3858fa0fe/a6352/3.webp 3200w\" sizes=\"(min-width: 1600px) 1600px, 100vw\"/><img data-gatsby-image-ssr=\"\" data-main-image=\"\" style=\"opacity:0\" sizes=\"(min-width: 1600px) 1600px, 100vw\" decoding=\"async\" loading=\"lazy\" src=\"/static/b9d29c9de6785d78ebd6bdd3858fa0fe/f6810/3.jpg\" srcSet=\"/static/b9d29c9de6785d78ebd6bdd3858fa0fe/eee8e/3.jpg 400w,/static/b9d29c9de6785d78ebd6bdd3858fa0fe/1e21a/3.jpg 800w,/static/b9d29c9de6785d78ebd6bdd3858fa0fe/f6810/3.jpg 1600w,/static/b9d29c9de6785d78ebd6bdd3858fa0fe/13708/3.jpg 3200w\" alt=\"an open book being held by both hands, displaying pages from “The Nature of Code.”\"/></picture>const t=\"undefined\"!=typeof HTMLImageElement&&\"loading\"in HTMLImageElement.prototype;if(t){const t=document.querySelectorAll(\"img[data-main-image]\");for(let e of t){e.dataset.src&&(e.setAttribute(\"src\",e.dataset.src),e.removeAttribute(\"data-src\")),e.dataset.srcset&&(e.setAttribute(\"srcset\",e.dataset.srcset),e.removeAttribute(\"data-srcset\"));const t=e.parentNode.querySelectorAll(\"source[data-srcset]\");for(let e of t)e.setAttribute(\"srcset\",e.dataset.srcset),e.removeAttribute(\"data-srcset\");e.complete&&(e.style.opacity=1,e.parentNode.parentNode.querySelector(\"[data-placeholder-image]\").style.opacity=0)}}<picture><source type=\"image/webp\" srcSet=\"/static/9b124bdb41538201fd0552d03a6a8c87/dff21/4.webp 400w,/static/9b124bdb41538201fd0552d03a6a8c87/b2a35/4.webp 800w,/static/9b124bdb41538201fd0552d03a6a8c87/e9d78/4.webp 1600w,/static/9b124bdb41538201fd0552d03a6a8c87/a6352/4.webp 3200w\" sizes=\"(min-width: 1600px) 1600px, 100vw\"/><img data-gatsby-image-ssr=\"\" data-main-image=\"\" style=\"opacity:0\" sizes=\"(min-width: 1600px) 1600px, 100vw\" decoding=\"async\" loading=\"lazy\" src=\"/static/9b124bdb41538201fd0552d03a6a8c87/f6810/4.jpg\" srcSet=\"/static/9b124bdb41538201fd0552d03a6a8c87/eee8e/4.jpg 400w,/static/9b124bdb41538201fd0552d03a6a8c87/1e21a/4.jpg 800w,/static/9b124bdb41538201fd0552d03a6a8c87/f6810/4.jpg 1600w,/static/9b124bdb41538201fd0552d03a6a8c87/13708/4.jpg 3200w\" alt=\"an open book with a coding example titled “Including Friction,” featuring code in JavaScript (p5.js) and an screenshot of the sketch in motion.\"/></picture>const t=\"undefined\"!=typeof HTMLImageElement&&\"loading\"in HTMLImageElement.prototype;if(t){const t=document.querySelectorAll(\"img[data-main-image]\");for(let e of t){e.dataset.src&&(e.setAttribute(\"src\",e.dataset.src),e.removeAttribute(\"data-src\")),e.dataset.srcset&&(e.setAttribute(\"srcset\",e.dataset.srcset),e.removeAttribute(\"data-srcset\"));const t=e.parentNode.querySelectorAll(\"source[data-srcset]\");for(let e of t)e.setAttribute(\"srcset\",e.dataset.srcset),e.removeAttribute(\"data-srcset\");e.complete&&(e.style.opacity=1,e.parentNode.parentNode.querySelector(\"[data-placeholder-image]\").style.opacity=0)}}Hi! Welcome! You can read the whole book here, thank you Creative Commons! If this project sparks joy and you want to support it, you can sponsor on GitHub or grab a copy of a bound collection of processed cellulose fibers, imprinted with symbolic glyphs via pigment-based transfer particles direct from me!<picture><source type=\"image/webp\" srcSet=\"/static/0e1e5349b22478378ae5bfc53977821c/500cb/bookmark-pink-bg.webp 38w,/static/0e1e5349b22478378ae5bfc53977821c/dda9f/bookmark-pink-bg.webp 75w,/static/0e1e5349b22478378ae5bfc53977821c/2aae9/bookmark-pink-bg.webp 150w,/static/0e1e5349b22478378ae5bfc53977821c/c14c9/bookmark-pink-bg.webp 300w\" sizes=\"(min-width: 150px) 150px, 100vw\"/><img data-gatsby-image-ssr=\"\" data-main-image=\"\" style=\"opacity:0\" sizes=\"(min-width: 150px) 150px, 100vw\" decoding=\"async\" loading=\"lazy\" src=\"/static/0e1e5349b22478378ae5bfc53977821c/b4a05/bookmark-pink-bg.png\" srcSet=\"/static/0e1e5349b22478378ae5bfc53977821c/481ab/bookmark-pink-bg.png 38w,/static/0e1e5349b22478378ae5bfc53977821c/ea265/bookmark-pink-bg.png 75w,/static/0e1e5349b22478378ae5bfc53977821c/b4a05/bookmark-pink-bg.png 150w,/static/0e1e5349b22478378ae5bfc53977821c/866e1/bookmark-pink-bg.png 300w\" alt=\"a hand holding a bookmark and a sticker\"/></picture>const t=\"undefined\"!=typeof HTMLImageElement&&\"loading\"in HTMLImageElement.prototype;if(t){const t=document.querySelectorAll(\"img[data-main-image]\");for(let e of t){e.dataset.src&&(e.setAttribute(\"src\",e.dataset.src),e.removeAttribute(\"data-src\")),e.dataset.srcset&&(e.setAttribute(\"srcset\",e.dataset.srcset),e.removeAttribute(\"data-srcset\"));const t=e.parentNode.querySelectorAll(\"source[data-srcset]\");for(let e of t)e.setAttribute(\"srcset\",e.dataset.srcset),e.removeAttribute(\"data-srcset\");e.complete&&(e.style.opacity=1,e.parentNode.parentNode.querySelector(\"[data-placeholder-image]\").style.opacity=0)}}Buying optionsOrder Direct*includes exclusive bookmark and sticker!No StarchBookshop.orgAmazonBarnes & NobleGlobal Retailers",
    "summary": {
      "en": "The text introduces a book titled \"The Nature of Code,\" which can be read online for free thanks to Creative Commons. It encourages readers to support the project by sponsoring on GitHub or purchasing a printed copy, which comes with a special bookmark and sticker. The book explores coding concepts, particularly through the lens of nature and computation. Additionally, it provides various buying options from different retailers.",
      "ko": "\"코드의 본질\"이라는 책이 소개됩니다. 이 책은 크리에이티브 커먼즈 덕분에 온라인에서 무료로 읽을 수 있습니다. 독자들에게는 GitHub에서 후원하거나 인쇄된 사본을 구매해 프로젝트를 지원할 것을 권장합니다. 인쇄본에는 특별한 책갈피와 스티커가 포함되어 있습니다. 이 책은 코딩 개념을 탐구하며, 특히 자연과 컴퓨터의 관점에서 다룹니다. 또한, 다양한 소매점에서 구매할 수 있는 여러 옵션도 제공합니다.",
      "ja": null
    }
  },
  {
    "id": "2480187f412ae58a",
    "title": {
      "en": "Blender releases their Oscar winning version tool",
      "ko": "블렌더, 오스카 툴 출시!",
      "ja": null
    },
    "type": "story",
    "url": "https://www.blender.org/download/releases/4-4/",
    "score": 759,
    "by": "babuloseo",
    "time": 1743035223,
    "content": "Splash artwork: Flow © Dream Well Studio, Sacrebleu Productions, Take FiveImage licensed under CC-BY-SA – https://flow.movie/\n\nWhat’s New Recap\n\nRound-up of what’s new in Blender 4.4, in detail.\n\nBlender 4.4 new features overview by Jonathan Lampel from CGCookie, Harry Blends, Paul Caggegi, and Wayne Dixon.\n\nQUALITY BLEND\n\nBlender 4.4 is all about stability. During the 2024–2025 northern hemisphere winter, Blender developers doubled down on quality and stability in a group effort called “Winter of Quality.”\n\n\t\tAmount of high severity bugs since January 1st, 2025\n\nWinter of Quality\n\nIn just a few months, developers fixed over 700 reported issues, revisited old bug reports, and addressed unreported problems.\n\nAlongside bug fixes, Winter of Quality also included tackling technical debt and improving documentation.\n\nRead Blog Post\n\nIssues Addressed per Module\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tAnimation & Rigging: 37\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tAsset System: 9\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tCore: 15\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tGrease Pencil: 147\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tModeling: 45\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tNodes & Physics: 80\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tPipeline & IO: 22\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tPlatforms, Builds & Tests: 5\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tPython API: 21\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tRender & Cycles: 53\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tSculpt, Paint & Texture: 45\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tUser Interface: 119\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tVFX & Video: 51\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tViewport & EEVEE: 47\n\n\t\t\t\t\t\t#word-cloud-block_ee6590eb760be9aa9e6087741d18482e {\n\t\t\t\tcolor: #ffffff;\n\t\t\t}\n\t\t\t\t\t\t\t\t\t#word-cloud-block_ee6590eb760be9aa9e6087741d18482e .block-words-cloud-categories > div{\n\t\t\t\tbackground: #ff3a3a;\n\t\t\t}\n\t\t\t#word-cloud-block_ee6590eb760be9aa9e6087741d18482e.block-word-cloud > ul > li strong {\n\t\t\t\tcolor: #ff3a3a;\n\t\t\t}\n\nACTION PACKED\n\nBlender 4.4 introduces Action Slots, revolutionizing animation workflows by letting multiple data-blocks share a single Action.\n\nWHAT AREACTION SLOTS?\n\nBefore Action Slots, each data-block specific animation—like an object’s position, a camera’s depth of field, or a material’s shader properties—needed its own separate Action. This made it difficult to animate multiple elements together or share animations between objects or even projects.\n\nFor example, if you wanted to animate a camera moving while also changing its depth of field, you’d need two separate Actions, which couldn’t be easily linked or reused.\n\nNow you can mix all sorts of animations such as an object’s position, its material properties, even compositing effects—all within a single Action.\n\nRelease Notes\n\nRead Manual\n\nMORE ANIMATION\n\n\t\t\tConstraints\n\n\t\t\t\t\t\t\tRelationship Lines for constraints are no longer drawn when there is no target.\n\n\t\t\tGraph Editor\n\n\t\t\t\t\t\t\tNew F-Curve Noise modifer algorithm.\n\n\t\t\tRigging\n\n\t\t\t\t\t\t\tBone collection membership is now mirrored when symmetrizing an armature.\n\n\t\t\t\t\t\t\tN-panel normalization now supports locking multiple vertex groups.\n\n\t\t\t\t\t\t\tRemoving a modifier, constraint, or shape key also deletes its driver.\n\n\t\t\tPython API\n\n\t\t\t\t\t\t\tConceptual changes in the API.\n\n\t\t\tPose Library\n\n\t\t\t\t\t\t\tMajor overhaul to Pose Assets to improve the user experience.\n\n\t\t\t\t\t\t\tDocumentation: User Manual update.\n\n\t\t\t\t\t\t#word-cloud-block_b85d66c7a871960db7d4f62fc4679261 {\n\t\t\t\tcolor: #ffffff;\n\t\t\t}\n\t\t\t\t\t\t\t\t\t#word-cloud-block_b85d66c7a871960db7d4f62fc4679261 .block-words-cloud-categories > div{\n\t\t\t\tbackground: #0a0000;\n\t\t\t}\n\t\t\t#word-cloud-block_b85d66c7a871960db7d4f62fc4679261.block-word-cloud > ul > li strong {\n\t\t\t\tcolor: #0a0000;\n\t\t\t}\n\nVSE: Vastly Superior Editing\n\nThe Video Sequencer continues to improve with quality-of-life upgrades for text editing, expanded support for codecs including H.265 and 10/12-bit videos, and performance improvements that make editing faster than ever.\n\nEDIT TEXT ON THE SPOT\n\nIntroducing: Edit mode for text strips in Preview!\n\nSimply press Tab and type away.\n\nFIND YOUR CENTER\n\nMulti-line text strips can now be properly aligned to the left, right, or center.\n\nFASTEREDITING EVERYTHING\n\nBuilding proxies for image sequences is faster now.\n\nPreview playback performance of float/HDR content is faster now.\n\nText strip background fill “Box” is several times faster for large fill areas.\n\nCurves, Hue Correct, White Balance modifiers are 1.5x-2x faster now.\n\nMany sequencer effects are slightly faster now thanks to more efficient multi-threading.\n\nVIDEOBEYOND\n\nBlender 4.4 adds support for rendering videos using the H.265/HEVC codec.\n\nVideos are now rendered in BT.709 color space now, preventing playback inconsistencies from the previously unspecified color space.\n\nAdditionally, video playback YUV->RGB conversion is more accurate now, fixing color shifts and banding in dark regions.\n\nBlender now supports 10 and 12 bit/channel videos!\n\nDuring rendering, you can set a color depth of 10 or 12 bits for supported codecs (10 bit for H.264, H.265, AV1, 12 bit for H.265, AV1).\n\nWhen reading 10-bit or 12-bit videos, they are loaded as floating-point images.\n\nA BITMORE\n\nEVEN MORE SEQUENCER\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tImproved layout for Text strip properties.\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tNew roundness property for text strips background box.\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tCustom font text strips no longer default to other fonts for missing characters.\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tSnapping now works with retiming keys.\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tDuplicate strip images in Preview area.\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t“Add Effect” menu has been improved.\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tVideo rotation metadata is now respected.\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tCopying strips or creating metastrips now includes effect chains automatically, avoiding selection errors.\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tProxies for EXR (or other float/HDR format) image strips now work properly.\n\n\t\t\t\t\t\t#word-cloud-block_79fa57d3266c700bab7ba3a976ef80de {\n\t\t\t\tcolor: #ffffff;\n\t\t\t}\n\t\t\t\t\t\t\t\t\t#word-cloud-block_79fa57d3266c700bab7ba3a976ef80de .block-words-cloud-categories > div{\n\t\t\t\tbackground: #ffff1f;\n\t\t\t}\n\t\t\t#word-cloud-block_79fa57d3266c700bab7ba3a976ef80de.block-word-cloud > ul > li strong {\n\t\t\t\tcolor: #ffff1f;\n\t\t\t}\n\n\tEXPAND\nYOUR BLENDER\n\n\t\t#paragraph-plus-block_7d3bd197e4872c4e32873562a13aad04 {\n\t\t\t\t\t\t\t\t\tfont-size: 64px;\t\t}\n\n\t\t\t\t\t#paragraph-plus-block_7d3bd197e4872c4e32873562a13aad04 em {\n\t\t\t\tbackground: rgb(221,51,51);\t\t\t\tcolor: rgb(255,255,255);\t\t\t\tfont-style: normal;\t\t\t}\n\nThe Blender Extensions platform keeps growing, with over 500 free add-ons and themes to customize your workflows.\n\nYou can also share your own add-ons and themes!\n\nBrowse Extensions\n\nShare Your Extensions\n\nMODELING\n\nPole Position\n\nA new option in the Select by Trait operator lets you select by pole count.\n\nEasily find all 3-pole or 5-pole points in your mesh.\n\nGiven their impact on topology, the default selects all poles that do not have 4 edges, allowing for easy inspection.\n\nSee Manual\n\nMODELING\n\nInfluencer\n\nJoining triangles to quads now prioritizes quad-dominant topology, creating a more structured “grid” layout. This helps maintain cleaner geometry and improves mesh flow, especially in models where uniform quads are preferred.\n\nThis behavior can be adjusted using a topology influence factor, to better control how triangles are merged.\n\nSee Manual\n\nMODELING\n\nVertex & Edge Dissolve\n\nDissolving edges may remove additional, unselected edges to ensure the mesh remains valid. Previously, this also dissolved vertices connected to those unselected edges.\n\nThe new behavior processes only vertices that belonged to the selected, now dissolved edges.\n\nSee Manual\n\nMORE MODELING\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tUp to 15% faster playback when using custom normals or sharp edges\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tN-panel normalization now supports locking multiple vertex groups\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tAdded Curves Edit Mode support for “Select Linked Pick” (L, Shift+L)\n\n\t\t\t\t\t\t#word-cloud-block_d0711cbb567333a72415a9367834dbe2 {\n\t\t\t\tcolor: #eeeeee;\n\t\t\t}\n\t\t\t\t\t\t\t\t\t#word-cloud-block_d0711cbb567333a72415a9367834dbe2 .block-words-cloud-categories > div{\n\t\t\t\tbackground: #8cff4f;\n\t\t\t}\n\t\t\t#word-cloud-block_d0711cbb567333a72415a9367834dbe2.block-word-cloud > ul > li strong {\n\t\t\t\tcolor: #8cff4f;\n\t\t\t}\n\nPUT IT TO THE TEST\n\nShare and compare your computer’s score with openly accessible benchmarks provided by the Blender community.\n\nDownload Blender Benchmark\n\nSCULPT\n\nPlane & Simple\n\nStay grounded or reach new heights with a new sculpt brush type: Plane.\n\nCustomized settings for the Plane brush type in Blender 4.4\n\nThe Plane brush is a generalization of the existing Flatten, Fill, and Scrape brushes, with new options to control stabilization and range of influence above and below the brush plane.\n\nKey features include adjustable height above the brush plane, depth control for vertices below it, and an option to invert these settings.\n\nStabilization options for the Normal (brush plane’s orientation) and Plane‘s position are also available for precise control.\n\nSee Manual\n\nMORE SCULPT\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tGrab Random Cloth and Grab Cloth now use Local Simulation Area by default.\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tSample Color operator assigned to Shift+Ctrl+X in Texture Paint\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tPrevent entering Sculpt Mode in invisible objects\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tRebuild BVH no longer adds an undo entry\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tFrame Selected renamed to Frame Last Stroke\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tNew operator property to override position on sculpt and paint modes.\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tCloth brushes now have the Persistent option off by default.\n\n\t\t\t\t\t\t#word-cloud-block_a27c9106f2b21da3cd5b4527430763f8 {\n\t\t\t\tcolor: #eeeeee;\n\t\t\t}\n\t\t\t\t\t\t\t\t\t#word-cloud-block_a27c9106f2b21da3cd5b4527430763f8 .block-words-cloud-categories > div{\n\t\t\t\tbackground: #8cff4f;\n\t\t\t}\n\t\t\t#word-cloud-block_a27c9106f2b21da3cd5b4527430763f8.block-word-cloud > ul > li strong {\n\t\t\t\tcolor: #8cff4f;\n\t\t\t}\n\n\t\tNew window decorations on Windows 11\n\nUSER INTERFACE\n\nShe Comes in Colors\n\nWindow decorations now follow the theme colors on Windows 11 and macOS.\n\nSnap Into Place\n\nEditors now softly snap to minimum and maximum sizes, with improved splitting previews and docking feedback.\n\nScrollbars are hidden for small editors.\n\nResizing editors now snaps to a minimum, maximum, and half way.\n\nScrollbars are now automatically hidden.\n\nHidden Away\n\nHorizontal scrollbars are now hidden automatically when they don’t fit in the editor.\n\nSee at a glance whether inputs are valid.\n\nNode Editor\n\nFade\n\nIn Node Editors, inputs that can’t affect output are now grayed out for group nodes, Geometry Nodes modifiers, and node tools.\n\nMore Node Editor Improvements\n\nOTHER EDITOR IMPROVEMENTS\n\n\t\t\tAsset Browser\n\n\t\t\t\t\t\t\tNew default sorting option: assets now sorted by catalog instead of name.\n\n\t\t\t\t\t\t\tNew operator to remove asset preview, available in the Asset Browser sidebar.\n\n\t\t\t\t\t\t\tBrush names are easier to read with Light theme.\n\n\t\t\tUV/Image Editor\n\n\t\t\t\t\t\t\tImage Editor view mode now uses “sample” as default tool.\n\n\t\t\t\t\t\t\tUV Editor: Ctrl-C and Ctrl-V shortcuts for copying and pasting UVs.\n\n\t\t\tStatus Bar\n\n\t\t\t\t\t\t\tShow warning when active object has non-uniform or negative scale.\n\n\t\t\t\t\t\t\tShow warning when transform operation has no effect.\n\n\t\t\t\t\t\t\tColor Picker Status Help shown for MacOS for picking outside of Blender.\n\n\t\t\t\t\t\t\tImproved status bar display for several operators.\n\n\t\t\t\t\t\t\tNotification banners are now truncated when very long.\n\n\t\t\t\t\t\t\tShow notification when hiding objects.\n\n\t\t\t3D Viewport\n\n\t\t\t\t\t\t\tDefault front face color for Face Orientation overlay is now transparent.\n\n\t\t\t\t\t\t\tMesh indices overlay setting is now always visible, regardless of Developer Extras.\n\n\t\t\t\t\t\t\t“Measure” tool items can now be deleted with gizmos off.\n\n\t\t\t\t\t\t\tAnimations can now play in Sculpt mode.\n\n\t\t\t\t\t\t\tViewport Render now displays a progress bar.\n\n\t\t\t\t\t\t\tBrush/tool falloff curve presets expanded in popovers.\n\n\t\t\t\t\t\t\tKnife tool overlay now uses gizmo theme colors.\n\n\t\t\t\t\t\t\tMore readable mesh indices.\n\n\t\t\t\t\t\t\tObject data name show shown in text overlay.\n\n\t\t\t\t\t\t\tFPS display in overlay no longer jiggles when starting.\n\n\t\t\tPreferences\n\n\t\t\t\t\t\t\tExtensions: Add button to quickly access an add-on’s folder.\n\n\t\t\t\t\t\t\tMore user preferences now reset to actual defaults.\n\n\t\t\t\t\t\t\tImproved Studio Lights Editor interface layout.\n\n\t\t\t\t\t\t\tLanguage translation options are now preserved when changing languages.\n\n\t\t\t\t\t\t\tSee all Preferences changes.\n\n\t\t\tProperties\n\n\t\t\t\t\t\t\tPersistent height and scroll position on tree-views.\n\n\t\t\t\t\t\t\tImproved container/codec ordering in FFMPEG video drop-downs.\n\n\t\t\t\t\t\t\tUI Lists now be sorted in reversed alphabetical.\n\n\t\t\t\t\t\t\tSee all Properties editor changes.\n\n\t\t\t\t\t\t#word-cloud-block_03c9d03db8ba61e400849a342fa8f1bd {\n\t\t\t\tcolor: #ffffff;\n\t\t\t}\n\t\t\t\t\t\t\t\t\t#word-cloud-block_03c9d03db8ba61e400849a342fa8f1bd .block-words-cloud-categories > div{\n\t\t\t\tbackground: #000000;\n\t\t\t}\n\t\t\t#word-cloud-block_03c9d03db8ba61e400849a342fa8f1bd.block-word-cloud > ul > li strong {\n\t\t\t\tcolor: #000000;\n\t\t\t}\n\nmacOS\n\nQuick Look\n\nOn macOS, you can now preview blend file contents in a thumbnail in Finder, App Exposé and Spotlight.\n\n\t\t.blend file previews on macOS Finder.\n\nEVERYTHING UI\n\n\t\t\tFiles\n\n\t\t\t\t\t\t\tFull file name now shown on the recent items tool-tips.\n\n\t\t\t\t\t\t\tDefault file names now capitalized as “Untitled”.\n\n\t\t\tFonts\n\n\t\t\t\t\t\t\tImproved selection from Fonts folder.\n\n\t\t\t\t\t\t\tImproved calculation of text string length for monospaced fonts.\n\n\t\t\t\t\t\t\tInterface font, “Inter”, updated to 4.1\n\n\t\t\tDialogs\n\n\t\t\t\t\t\t\tCentered dialogs can now be moved.\n\n\t\t\t\t\t\t\tPopup alerts (e.g. Python script warnings) adjust width based on content.\n\n\t\t\t\t\t\t\tQuick Favorites and other menus now support accelerators (underlined letters) for toggles.\n\n\t\t\t\t\t\t\tImproved Adjust Last Operation behavior\n\n\t\t\t\t\t\t\tReset to Defaults in Redo Panel no longer closes it.\n\n\t\t\t\t\t\t\tPreview items show a slight background in menus.\n\n\t\t\t\t\t\t\tPie menus now close on loss of window focus.\n\n\t\t\tCursors\n\n\t\t\t\t\t\t\tIncreased contrast for transform cursors.\n\n\t\t\t\t\t\t\tLarger alternative version of the “frame” cursor for high-DPI displays.\n\n\t\t\tIcons\n\n\t\t\t\t\t\t\tBatch Rename now has icons for data type.\n\n\t\t\t\t\t\t\t“Material” icon flipped horizontally to better differentiate it from “World.”\n\n\t\t\tGeneral\n\n\t\t\t\t\t\t\tRename masking property to better reflect its usage.\n\n\t\t\t\t\t\t\tEyedropper color picking can now be canceled without losing the original color.\n\n\t\t\t\t\t\t\tPreview images load faster and display fewer artifacts at different sizes.\n\n\t\t\t\t\t\t\tShow warning when image drag-and-drop fails.\n\n\t\t\t\t\t\t\tPreview items show a loading icon while loading or being rendered.\n\n\t\t\tTheme\n\n\t\t\t\t\t\t\tSelected outline color for pulldowns is now customizable.\n\n\t\t\t\t\t\t\tLight theme icon border now mirrors earlier Blender versions.\n\n\t\t\tInput\n\n\t\t\t\t\t\t\tColor picker HSL value can now be adjusted by trackpad scrolling.\n\n\t\t\t\t\t\t\tImprovements to 3Dconnexion NDOF support.\n\n\t\t\t\t\t\t\tNDOF Orbit direction now inverts when upside down.\n\n\t\t\tText Objects\n\n\t\t\t\t\t\t\tText Objects with non-default fonts no longer load missing characters from other fonts.\n\n\t\t\t\t\t\t\tText Objects with missing or invalid fonts no longer use characters from the default font.\n\n\t\t\tTooltips\n\n\t\t\t\t\t\t\tTooltips now appear while animation is playing.\n\n\t\t\t\t\t\t\tStatus colors are clearer on light backgrounds.\n\n\t\t\t\t\t\t\tColorspace tooltips show more details with expanded acronyms.\n\n\t\t\t\t\t\t\tProperly display colors without alpha.\n\n\t\t\t\t\t\t\tImproved vertical centering.\n\n\t\t\t\t\t\t\tTooltips now disappear faster with Gizmo interaction.\n\n\t\t\t\t\t\t\tNo longer redraw with slight mouse movements.\n\n\t\t\tAnimation\n\n\t\t\t\t\t\t\tAnimated values for nodes and inputs now have clearer names.\n\n\t\t\t\t\t\t\tAuto keyframe toggle works better with keyboard shortcuts.\n\n\t\t\t\t\t\t\tBetter feedback during Animation Playback timer test.\n\n\t\t\t\t\t\t\tImproved current frame indicator styling in movie clip and image editors.\n\n\t\t\t\t\t\t#word-cloud-block_b26d1ee9b65b204f87a2a6e62796d75e {\n\t\t\t\tcolor: #0a0a0a;\n\t\t\t}\n\t\t\t\t\t\t\t\t\t#word-cloud-block_b26d1ee9b65b204f87a2a6e62796d75e .block-words-cloud-categories > div{\n\t\t\t\tbackground: #ffffff;\n\t\t\t}\n\t\t\t#word-cloud-block_b26d1ee9b65b204f87a2a6e62796d75e.block-word-cloud > ul > li strong {\n\t\t\t\tcolor: #ffffff;\n\t\t\t}\n\n\tBlender is and will\nalways be free, forever.\n\n\t\t#paragraph-plus-block_88f494eee05b83d08e8869836b7c8374 {\n\t\t\t\t\t\tcolor: #ff3a3a;\t\t\tfont-size: 48px;\t\t}\n\n\t\t\t#paragraph-plus-block_88f494eee05b83d08e8869836b7c8374 {\n\t\t\t\tbackground-image: linear-gradient(45deg, rgb(255,255,255) 0%, rgb(255,255,255));\n\t\t\t\t-webkit-background-clip: text;\n\t\t\t\t-webkit-text-fill-color: transparent;\n\t\t\t}\n\nReleases are possible thanks to donations by the community.\n\n Donate Monthly\n\nDonate Once\n\nCOMPOSITOR\n\nSpeedfor Everyone\n\nThe CPU compositor was rewritten to pave the way for future development.\n\nThe rewrite provides significant improvements in performance in certain configurations of some nodes, caching of static resources like images, and less memory usage on node setups with many nodes that operate on pixels.\n\nFilter nodes are particularly faster now:\n\nLevels node is up to 10x faster.\n\nFilter and Kuwahara are twice as fast.\n\nBlur nodes up to four times faster.\n\nGlare filter is not only more advanced but also 6x more performant.\n\nPixelate node is 9x faster.\n\nAdjusting compositor node trees can be significantly faster and more interactive. That’s because the compositor now avoids computing outputs that aren’t viewed by the user through the backdrop or image editor.\n\nThe overall compositing experience should now feel more responsive, whether you’re using the CPU or GPU.\n\n          Blender 4.3\n\n          Blender 4.4\n\n        Levels\n\n        Pixelate\n\n        Glare\n\n        Pixel Nodes\n\n        Bilateral Blur\n\n        Variable Blur\n\n        Kuwahara\n\n        Masks\n\n        Lens Distortion\n\n    0246810\n\n        Relative performance (higher is better)\n\n\t\tComparison between Blender 4.3 and Blender 4.4\n\nCOMPOSITOR\n\nGlare Glow Up\n\nThe Glare node got a major revamp for better control and usability:\n\nLinkable Inputs – Most node options are now input sockets you can connect.\n\nNew Outputs – Generated glare and highlights are now exposed as output sockets.\n\nA new Strength input lets you adjust glare intensity.\n\nFog Glow and Bloom sizes are now linear and scale properly.\n\nMore realistic, energy-conserving, and properly scaled Bloom.\n\nAdjust glare saturation and tint with dedicated inputs.\n\nHighlight Control – Clamp and smooth highlights with the new Smoothness and Maximum inputs.\n\nTidy UI – Inputs are now neatly organized into collapsible panels.\n\nSee Manual\n\nMORE COMPOSITOR\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tAdded support for integer sockets\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tNew Quality option of OpenImageDenoise on the Denoise node\n\n\t\t\t\t\t\t\t\t\tFast Gaussian Blur is now much more accurate\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tImage transformations are no longer destructive until processed\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tWrapping is now Repeat in the Translate node\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tNotes on compatibility\n\n\t\t\t\t\t\t#word-cloud-block_2c33456875e6a59bac9db3cc16befb5f {\n\t\t\t\tcolor: #ffffff;\n\t\t\t}\n\t\t\t\t\t\t\t\t\t#word-cloud-block_2c33456875e6a59bac9db3cc16befb5f .block-words-cloud-categories > div{\n\t\t\t\tbackground: #c578ff;\n\t\t\t}\n\t\t\t#word-cloud-block_2c33456875e6a59bac9db3cc16befb5f.block-word-cloud > ul > li strong {\n\t\t\t\tcolor: #c578ff;\n\t\t\t}\n\n\tINDUSTRY READY\n\n\t\t#paragraph-plus-block_bdba7a4c2aba9cde90dc3322057b7fae {\n\t\t\t\t\t\t\t\t\tfont-size: 64px;\t\t}\n\n\t\t\t\t\t#paragraph-plus-block_bdba7a4c2aba9cde90dc3322057b7fae em {\n\t\t\t\tbackground: rgb(221,51,51);\t\t\t\tcolor: rgb(255,255,255);\t\t\t\tfont-style: normal;\t\t\t}\n\nAll library versions used in Blender 4.4 are aligned with the VFX Reference Platform 2025, making studio pipeline integration and maintenance easier.\n\nVFX Platform\n\nSee All Library Changes\n\nBUT WAIT, THERE’S MORE\n\n\t\t\tGeometry Nodes\n\n\t\t\t\t\t\t\tNew node “Find in String”.\n\n\t\t\t\t\t\t\tNew input nodes: Collection and Object.\n\n\t\t\t\t\t\t\tNew “Limit Surface” option available in the Subdivision Surface node.\n\n\t\t\t\t\t\t\tNormal input node now outputs proper face corner normals instead of just face normals.\n\n\t\t\t\t\t\t\tJoin Geometry and Realize Instances nodes now preserve the vertex group status of input attributes.\n\n\t\t\t\t\t\t\tThe text overlay in the 3D view works for matrix attributes now.\n\n\t\t\t\t\t\t\tPerformance: Triangulate node is 30x to 100x faster.\n\n\t\t\t\t\t\t\tSort Elements node is 50% faster in common scenarios.\n\n\t\t\t\t\t\t\tThe Warning node now has a dynamic label depending on the selected type.\n\n\t\t\t\t\t\t\tUI: Resizing nodes now support snapping.\n\n\t\t\tCore\n\n\t\t\t\t\t\t\tAdded support for rendering videos using H.265/HEVC codec.\n\n\t\t\t\t\t\t\tBLENDER_SYSTEM_SCRIPTS now supports multiple paths.\n\n\t\t\t\t\t\t\tNew BLENDER_CUSTOM_SPLASH to replace the splash screen artwork.\n\n\t\t\t\t\t\t\tEXR images that use DWAA/DWAB compression codec now have a Quality setting\n\n\t\t\t\t\t\t\t“Render Audio” can now render to AAC (.aac) format.\n\n\t\t\t\t\t\t\tAuto-save and quit.blend files are now always saved with compression.\n\n\t\t\t3D Viewport\n\n\t\t\t\t\t\t\tOverlays engine rewrite and improvements.\n\n\t\t\t\t\t\t\tExpose view Lock Rotation property in the sidebar View panel.\n\n\t\t\tUSD\n\n\t\t\t\t\t\t\tAnimated volumes from Geometry Nodes or volume modifiers are now supported for export.\n\n\t\t\t\t\t\t\tMaterial displacement for UsdPreviewSurface is now supported in import and export.\n\n\t\t\t\t\t\t\tPoint instancers with animated attributes are now supported on import.\n\n\t\t\t\t\t\t\tThe experimental “Instancing” option now supports object hierarchies and non-mesh geometry (e.g., curves, point clouds).\n\n\t\t\t\t\t\t\tNew Python hooks.\n\n\t\t\t\t\t\t\tAdded “Merge parent Xform” option to control USD prim merging with its Xform parent during import for better hierarchy preservation.\n\n\t\t\t\t\t\t\tAdded “Apply Unit Conversion Scale” option to scale objects by the USD stage’s meters per unit value.\n\n\t\t\t\t\t\t\tAdded “Merge parent Xform” option to control whether Blender object transforms are written to their data prim or kept separate on export, reducing USD prim count and preserving hierarchy.\n\n\t\t\t\t\t\t\tAdded “Units” and “Meters Per Unit” options to set the USD Stage measurement or a custom value.\n\n\t\t\t\t\t\t\tUSD & Alembic: Edge and vertex crease processing now respects the value range expected by OpenSubdiv.\n\n\t\t\tVulkan (experimental)\n\n\t\t\t\t\t\t\tHuge performance improvements.\n\n\t\t\t\t\t\t\tCompatibility and Known Issues.\n\n\t\t\tGrease Pencil\n\n\t\t\t\t\t\t\tSeveral operators and functionality from the old Grease Pencil were restored.\n\n\t\t\t\t\t\t\tProperties of locked materials can be edited (similar to properties of locked layers)\n\n\t\t\t\t\t\t\tInvisible layers are no longer part of evaluated data.\n\n\t\t\t\t\t\t\tUI: The preview icon of locked materials is no longer grayed out.\n\n\t\t\t\t\t\t\t“Lock all” and “Unlock all” operators now work on Layer Groups.\n\n\t\t\t\t\t\t\t“Hide Others” operator now also considers layer groups.\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tVertex colors and layer tinting render in “Solid” shading mode.\n\n\t\t\tglTF\n\n\t\t\t\t\t\t\tAdded support for importing Action Slots.\n\n\t\t\t\t\t\t\tAdd option to not select created objects.\n\n\t\t\t\t\t\t\tAdd option to import scene extras or not.\n\n\t\t\t\t\t\t\tExport: Several (breaking) changes to hooks.\n\n\t\t\t\t\t\t\tExport: Always bake scene animation, so driven animated properties can be exported.\n\n\t\t\t\t\t\t\tExport: Add interpolation fallback option.\n\n\t\t\t\t\t\t\tExport: Several improvements to Collection export.\n\n\t\t\t\t\t\t\tBug fixes\n\n\t\t\tPython API\n\n\t\t\t\t\t\t\tNew: `bpy.app.module` indicates if Blender is running as Python module.\n\n\t\t\t\t\t\t\tNew property to check if installation is portable.\n\n\t\t\t\t\t\t\tNew operations for Curves.\n\n\t\t\t\t\t\t\tNew properties for Nodes.\n\n\t\t\t\t\t\t\tGrease Pencil Python API updates.\n\n\t\t\t\t\t\t\tVSE: Major API breaking changes and deprecated properties.\n\n\t\t\t\t\t\t\tAnimation: Slotted Action related additions, deprecated properties and breaking changes.\n\n\t\t\t\t\t\t\tComplete list of API breaking changes.\n\n\t\t\tCycles\n\n\t\t\t\t\t\t\tImproved OptiX Denoiser\n\n\t\t\t\t\t\t\tMore accurately render sub-pixel bump mapping.\n\n\t\t\t\t\t\t\tBaking: Speed up Selected to Active baking.\n\n\t\t\t\t\t\t\tOSL: Improved closure compatibility with MaterialX.\n\n\t\t\t\t\t\t\tImproved Sample Subset\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tMore robust host memory fallback when the GPU runs out of memory.\n\n\t\t\t\t\t\t\tNVIDIA: Support GeForce RTX 50×0 series (Blackwell)\n\n\t\t\t\t\t\t\tAMD: Support RX 90×0 series (RDNA4)\n\n\t\t\t\t\t\t\tAMD: HIP RT library updates and minimum driver version increased.\n\n\t\t\t\t\t\t\tIntel: Minimum driver version increased.\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tIntel: Improved image texture sampling performance.\n\n\t\t\tSpreadsheet\n\n\t\t\t\t\t\t\tThe selection filter for meshes now gives expected results for non-vertex domains.\n\n\t\t\tOutliner\n\n\t\t\t\t\t\t\tImproved vertex group sorting.\n\n\t\t\t\t\t\t\tSupport Ctrl/Shift for excluding collections.\n\n\t\t\t\t\t\t\tFixed overlapping icons with some display options.\n\n\t\t\t\t\t\t\tCan now un-isolate collection when a linked collection is present.\n\n\t\t\t\t\t\t\tNon-object active item text now drawn in “text high” color.\n\n\t\t\t\t\t\t\tDrag and drop to scene now updates the view.\n\n\t\t\t\t\t\t\tChild objects linked to other collections are now faded.\n\n\t\t\tWindows\n\n\t\t\t\t\t\t\tCopy and paste OS image paths into Image Editor.\n\n\t\t\t\t\t\t\tFile system volume names display correctly with high-bit Unicode characters.\n\n\t\t\t\t\t\t\tAltGr key is now treated as regular Alt key.\n\n\t\t\tmacOS\n\n\t\t\t\t\t\t\tImproved color picking outside of Blender windows.\n\n\t\t\t\t\t\t\tKeymaps can be searched with native key names.\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tMain window title improvements.\n\n\t\t\t\t\t\t\tEnhanced NDOF device input handling when using 3DConnexion driver v10.8.7 and later.\n\n\t\t\tLinux\n\n\t\t\t\t\t\t\tFixed text pasting from Blender to certain other applications (such as Firefox) not working under X11.\n\n\t\t\t\t\t\t\t\t\t.block-words-cloud-categories > div {\n  --box-background-color: #1e2225;\n}\n\nPlus hundreds of bug fixes, code cleanups and refactors.See the full list of changes.\n\nCREDITS\n\nList of developers that contributed to Blender 4.4\n\nBlender is a community project.Learn more on how you can contribute to Blender.\n\nSplash artwork: Flow © Dream Well Studio, Sacrebleu Productions, Take Five – Licensed under CC-BY-SA – flow.movieHuge thanks to everyone involved\n\nThe Blender team. March 18th, 2025",
    "summary": {
      "en": "### Blender 4.4 Summary\n\n**Overview:**\nBlender 4.4 focuses on improving stability and quality through a special initiative called \"Winter of Quality.\" During this period, over 700 bugs were fixed, and technical documentation was enhanced.\n\n**Key Features:**\n\n1. **Action Slots:**\n   - New feature allowing multiple animations to share a single Action, simplifying the animation process for various elements (e.g., moving a camera while changing its depth of field).\n\n2. **Video Sequencer Enhancements:**\n   - Improved text editing, faster image processing, and support for H.265 codec.\n   - New features include on-the-fly text editing, better alignment for text strips, and faster rendering.\n\n3. **Modeling Improvements:**\n   - New options for selecting mesh points and better topology management.\n   - Enhanced performance for various modeling tasks.\n\n4. **Sculpting Tools:**\n   - Introduction of a new Plane brush type with customizable settings for better control.\n\n5. **User Interface Updates:**\n   - Enhanced window decorations for Windows 11 and macOS.\n   - Improved editor functionality, such as better snapping and visibility options.\n\n6. **Compositor Upgrades:**\n   - Major performance improvements and new controls for glare effects and other nodes, making the compositing process faster and more efficient.\n\n7. **Extensive Bug Fixes:**\n   - Over 700 issues addressed across multiple modules, improving overall software stability.\n\n8. **Additional Features:**\n   - New nodes for Geometry Nodes, improvements in rendering, better handling of video files, and various user interface enhancements across different platforms.\n\nBlender 4.4 represents a significant step forward in usability and performance, making it easier for users to create and edit complex animations and visual effects. The software remains free and open-source, supported by community donations.",
      "ko": "Blender 4.4는 \"품질의 겨울\"이라는 특별한 이니셔티브를 통해 안정성과 품질을 개선하는 데 중점을 두고 있습니다. 이 기간 동안 700개 이상의 버그가 수정되었고, 기술 문서도 향상되었습니다.\n\n새로운 기능 중 하나는 여러 애니메이션이 하나의 액션을 공유할 수 있는 액션 슬롯입니다. 이를 통해 카메라를 이동하면서 심도 변화를 주는 등 다양한 요소의 애니메이션 작업이 간편해졌습니다. 비디오 시퀀서에서는 텍스트 편집이 개선되고, 이미지 처리 속도가 빨라졌으며, H.265 코덱을 지원합니다. 새로운 기능으로는 실시간 텍스트 편집, 텍스트 스트립의 정렬 개선, 빠른 렌더링 등이 포함됩니다.\n\n모델링 부분에서는 메쉬 포인트 선택 옵션이 추가되고, 더 나은 토폴로지 관리 기능이 제공됩니다. 다양한 모델링 작업의 성능도 향상되었습니다. 조각 도구에서는 사용자 설정이 가능한 새로운 평면 브러시 타입이 도입되어 보다 정밀한 제어가 가능합니다.\n\n사용자 인터페이스도 업데이트되어 Windows 11과 macOS의 창 장식이 개선되었습니다. 편집기 기능도 향상되어 스냅 및 가시성 옵션이 더 좋아졌습니다. 합성기에서는 성능이 크게 개선되었고, 눈부심 효과와 기타 노드에 대한 새로운 제어 기능이 추가되어 합성 과정이 더 빠르고 효율적으로 진행됩니다.\n\n700개 이상의 문제를 해결하여 소프트웨어의 전반적인 안정성이 향상되었습니다. 추가적으로 기하학 노드를 위한 새로운 노드, 렌더링 개선, 비디오 파일 처리 향상, 다양한 플랫폼에서의 사용자 인터페이스 개선 등이 포함됩니다.\n\nBlender 4.4는 사용성과 성능에서 큰 발전을 이루어 복잡한 애니메이션과 시각 효과를 보다 쉽게 만들고 편집할 수 있도록 도와줍니다. 이 소프트웨어는 여전히 무료이며 오픈 소스로, 커뮤니티의 기부로 지원받고 있습니다.",
      "ja": null
    }
  },
  {
    "id": "a42fcf6f48d18fb3",
    "title": {
      "en": "Coordinating the Superbowl's visual fidelity with Elixir",
      "ko": "엘릭서로 슈퍼볼 비주얼 조율하기",
      "ja": null
    },
    "type": "story",
    "url": "https://elixir-lang.org/blog/2025/03/25/cyanview-elixir-case/",
    "score": 627,
    "by": "lawik",
    "time": 1742966362,
    "content": "Cyanview: Coordinating Super Bowl's visual fidelity with Elixir\n\n        March 25, 2025 · by Lars Wikman, José Valim · in\n\n            Elixir in Production\n\n        How do you coordinate visual fidelity across two hundred cameras for a live event like the Super Bowl?\n\nThe answer is: by using the craft of camera shading, which involves adjusting each camera to ensure they match up in color, exposure and various other visual aspects. The goal is to turn the live event broadcast into a cohesive and consistent experience. For every angle used, you want the same green grass and the same skin tones. Everything needs to be very closely tuned across a diverse set of products and brands. From large broadcast cameras, drone cameras, and PTZ cameras to gimbal-held mirrorless cameras and more. This is what Cyanview does. Cyanview is a small Belgian company that sells products for the live video broadcast industry, and its primary focus is shading.\n\nBroadcast is a business where you only get one chance to prove that your tool is up to the task. Reliability is king. There can be no hard failures.\n\nA small team of three built a product so powerful and effective that it spread across the industry purely on the strength of its functionality. Without any marketing, it earned a reputation among seasoned professionals and became a staple at the world’s top live events. Cyanview’s Remote Control Panel (RCP) is now used by specialist video operators on the Olympics, Super Bowl, NFL, NBA, ESPN, Amazon and many more. Even most fashion shows in Paris use Cyanview’s devices.\n\nThese devices put Elixir right in the critical path for serious broadcast operations. By choosing Elixir, Cyanview gained best-in-class networking features, state-of-the-art resilience and an ecosystem that allowed fast iteration on product features.\n\nWhy Elixir?\n\nThe founding team of Cyanview primarily had experience with embedded development, and the devices they produce involve a lot of low-level C code and plenty of FPGA. This is due to the low-level details of color science and the really tight timing requirements.\n\nIf you’ve ever worked with camera software, you know it can be a mixed bag. Even after going fully digital, much of it remained tied to analog systems or relied on proprietary connectivity solutions. Cyanview has been targeting IP (as in Internet Protocol) from early on. This means Cyanview’s software can operate on commodity networks that work in well-known and well-understood ways. This has aligned well with an increase in remote production, partially due to the pandemic, where production crews operate from a central location with minimal crew on location. Custom radio frequency or serial wire protocols have a hard time scaling to cross-continent distances.\n\nThis also paved the way for Elixir, as the Erlang VM was designed to communicate and coordinate millions of devices, reliably, over the network.\n\nElixir was brought in by the developer Ghislain, who needed to build integrations with cameras and interact with the other bits of required video gear, with many different protocols over the network. The language comes with a lot of practical features for encoding and decoding binary data down to the individual bits. Elixir gave them a strong foundation and the tools to iterate fast.\n\nGhislain has been building the core intellectual property of Cyanview ever since. While the physical device naturally has to be solid, reliable, and of high quality, a lot of the secret sauce ultimately lies in the massive number of integrations and huge amounts of reverse engineering. Thus, the product is able to work with as many professional camera systems and related equipment as possible. It is designed to be compatible with everything and anything a customer is using. Plus, it offers an API to ensure smooth integration with other devices.\n\nDavid Bourgeois, the founder of Cyanview, told us a story how these technical decisions alongside Elixir helped them tackle real-world challenges:\n\n“During the Olympics in China, a studio in Beijing relied on a large number of Panasonic PTZ cameras. Most of their team, however, was based in Paris and needed to control the cameras remotely to run various shows throughout the day. The problem? Panasonic’s camera protocols were never designed for internet use — they require precise timing and multiple messages for every adjustment. With network latency, that leads to timeouts, disconnects, and system failures… So they ended up placing our devices next to the cameras in Beijing and controlled them over IP from Paris — just as designed.”\n\nThe devices in a given location communicate and coordinate on the network over a custom MQTT protocol. Over a hundred cameras without issue on a single Remote Control Panel (RCP), implemented on top of Elixir’s network stack.\n\nTechnical composition\n\nThe system as a whole consists of RCP devices running a Yocto Linux system, with most of the logic built in Elixir and C. While Python is still used for scripting and tooling, its role has gradually diminished. The setup also includes multiple microcontrollers and the on-camera device, all communicating over MQTT. Additionally, cloud relays facilitate connectivity, while dashboards and controller UIs provide oversight and control. The two critical devices are the RCP offering control on the production end and the RIO handling low-latency manipulation of the camera. Both run Elixir.\n\nThe configuration UI is currently built in Elm, but - depending on priorities - it might be converted to Phoenix LiveView over time to reduce the number of languages in use. The controller web UI is already in LiveView, and it is performing quite well on a very low-spec embedded Linux machine.\n\nThe cloud part of the system is very limited today, which is unusual in a world of SaaS. There are cloud relays for distributing and sharing camera control as well as forwarding network ports between locations and some related features, also built in Elixir, but cloud is not at the core of the business. The devices running Elixir on location form a cluster over IP using a custom MQTT-based protocol suited to the task and are talking to hundreds of cameras and other video devices.\n\nIt goes without saying that integration with so much proprietary equipment comes with challenges. Some integrations are more reliable than others. Some devices are more common, and their quirks are well-known through hard-won experience. A few even have good documentation that you can reference while others offer mystery and constant surprises. In this context, David emphasizes the importance of Elixir’s mechanisms for recovering from failures:\n\n“If one camera connection has a blip, a buggy protocol or the physical connection to a device breaks it is incredibly important that everything else keeps working. And this is where Elixir’s supervision trees provide a critical advantage.”\n\nGrowth & team composition\n\nThe team has grown over the 9 years that the company has been operating, but it did so at a slow and steady pace. On average, the company has added just one person per year. With nine employees at the time of writing, Cyanview supports some of the biggest broadcast events in the world.\n\nThere are two Elixir developers on board: Daniil who is focusing on revising some of the UI as well as charting a course into more cloud functionality, and Ghislain, who works on cameras and integration. Both LiveView and Elm are used to power device UIs and dashboards.\n\nWhat’s interesting is that, overall, the other embedded developers say that they don’t know much about Elixir and they don’t use it in their day-to-day work. Nonetheless, they are very comfortable implementing protocols and encodings in Elixir. The main reason they haven’t fully learned the language is simply time — they have plenty of other work to focus on, and deep Elixir expertise hasn’t been necessary. After all, there’s much more to their work beyond Elixir: designing PCBs, selecting electronic components, reverse engineering protocols, interfacing with displays, implementing FPGAs, managing production tests, real productions and releasing firmware updates.\n\nInnovation and customer focus\n\nWhether it’s providing onboard cameras in 40+ cars during the 24 hours of Le Mans, covering Ninja Warrior, the Australian Open, and the US Open, operating a studio in the Louvre, being installed in NFL pylons, or connecting over 200 cameras simultaneously – the product speaks for itself. Cyanview built a device for a world that runs on top of IP, using Elixir, a language with networking and protocols deep in its bones. This choice enabled them to do both: implement support for all the equipment and provide features no one else had.\n\nBy shifting from conventional local-area radio frequency, serial connections, and inflexible proprietary protocols to IP networking, Cyanview’s devices redefined how camera systems operate. Their feature set is unheard of in the industry: Unlimited multicam. Tally lights. Pan & Tilt control. Integration with color correctors. World-spanning remote production.\n\nThe ease and safety of shipping new functionality have allowed the company to support new features very quickly. One example is the increasing use of mirrorless cameras on gimbals to capture crowd shots. Cyanview were able to prototype gimbal control, test it with a customer and validate that it worked in a very short amount of time. This quick prototyping and validation of features is made possible by a flexible architecture that ensures that critical fundamentals don’t break.\n\nCamera companies that don’t produce broadcast shading remotes, such as Canon or RED, recommend Cyanview to their customers. Rather than competing with most broadcast hardware companies, Cyanview considers itself a partner. The power of a small team, a quality product and powerful tools can be surprising. Rather than focusing on marketing, Cyanview works very closely with its customers by supporting the success of their events and providing in-depth customer service.\n\nLooking back and forward\n\nWhen asked if he would choose Elixir again, David responded:\n\n“Yes. We’ve seen what the Erlang VM can do, and it has been very well-suited to our needs. You don’t appreciate all the things Elixir offers out of the box until you have to try to implement them yourself. It was not pure luck that we picked it, but we were still lucky. Elixir turned out to bring a lot that we did not know would be valuable to us. And we see those parts clearly now.”\n\nCyanview hopes to grow the team more, but plans to do so responsibly over time. Currently there is a lot more to do than the small team can manage.\n\nDevelopment is highly active, with complementary products already in place alongside the main RCP device, and the future holds even more in that regard. Cloud offerings are on the horizon, along with exciting hardware projects that build on the lessons learned so far. As these developments unfold, we’ll see Elixir play an increasingly critical role in some of the world’s largest live broadcasts.\n\nIn summary\n\nA high-quality product delivering the right innovation at the right time in an industry that’s been underserved in terms of good integration. Elixir provided serious leverage for developing a lot of integrations with high confidence and consistent reliability. In an era where productivity and lean, efficient teams are everything, Cyanview is a prime example of how Elixir empowers small teams to achieve an outsized impact.",
    "summary": {
      "en": "**Summary of Cyanview's Use of Elixir for Super Bowl Broadcasts**\n\nCyanview is a Belgian company specializing in live video broadcast technology, particularly camera shading, which ensures consistency in color and exposure across numerous cameras during events like the Super Bowl. Their main product, the Remote Control Panel (RCP), is widely used in major broadcasts, including the Olympics and NFL games.\n\nCyanview chose Elixir for its strong networking capabilities and reliability, essential for operating over IP networks rather than traditional methods, which often struggle with latency. This decision allowed them to develop versatile products that integrate smoothly with various camera systems and protocols.\n\nThe company operates with a small team of nine, including two Elixir developers. They focus on building robust and flexible systems that enable quick prototyping of new features, enhancing their product offerings. Cyanview prioritizes customer collaboration over marketing, leading to strong partnerships in the industry.\n\nLooking ahead, Cyanview plans to grow responsibly while expanding their product range and cloud capabilities. Their success illustrates how Elixir can help small teams make a significant impact in complex industries like live broadcasting.",
      "ko": "Cyanview는 벨기에에 본사를 둔 회사로, 라이브 비디오 방송 기술, 특히 카메라 색상 조정에 전문화되어 있습니다. 이 기술은 슈퍼볼과 같은 대규모 이벤트에서 여러 카메라의 색상과 노출을 일관되게 유지하는 데 도움을 줍니다. 이 회사의 주요 제품인 원격 제어 패널(RCP)은 올림픽과 NFL 경기 등 주요 방송에서 널리 사용됩니다.\n\nCyanview는 강력한 네트워킹 기능과 신뢰성을 갖춘 Elixir를 선택했습니다. 이는 전통적인 방법보다 IP 네트워크에서 운영하는 데 필수적이며, 전통적인 방법은 종종 지연 문제로 어려움을 겪기 때문입니다. 이 선택 덕분에 다양한 카메라 시스템과 프로토콜과 원활하게 통합되는 다재다능한 제품을 개발할 수 있었습니다.\n\nCyanview는 두 명의 Elixir 개발자를 포함해 아홉 명의 소규모 팀으로 운영됩니다. 이들은 강력하고 유연한 시스템을 구축하여 새로운 기능을 신속하게 프로토타입할 수 있도록 하여 제품의 품질을 향상시키고 있습니다. Cyanview는 마케팅보다 고객과의 협업을 우선시하여 업계에서 강력한 파트너십을 구축하고 있습니다.\n\n앞으로 Cyanview는 제품 범위와 클라우드 기능을 확장하면서 책임감 있게 성장할 계획입니다. 이들의 성공은 Elixir가 소규모 팀이 복잡한 라이브 방송 산업에서 큰 영향을 미칠 수 있도록 도와준다는 것을 보여줍니다.",
      "ja": null
    }
  },
  {
    "id": "3e1a68171d0fae9e",
    "title": {
      "en": "Akamai Now Providing the Hosting Infrastructure for Kernel.org",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.phoronix.com/news/Akamai-Hosting-Kernel.org",
    "score": 6,
    "by": "speckx",
    "time": 1743101685,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "863f6f62acbec3d2",
    "title": {
      "en": "Even If Those Weren't War Plans in Hegseth's Signal Chat, They Were War Crimes",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.techdirt.com/2025/03/27/even-if-those-werent-war-plans-in-hegseths-signal-chat-they-were-war-crimes/",
    "score": 8,
    "by": "hn_acker",
    "time": 1743111031,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "231895d570006adc",
    "title": {
      "en": "Pi Pico Rx – A crystal radio for the digital age?",
      "ko": "디지털 시대의 크리스탈 라디오?",
      "ja": null
    },
    "type": "story",
    "url": "https://101-things.readthedocs.io/en/latest/radio_receiver.html",
    "score": 140,
    "by": "nolist_policy",
    "time": 1742732290,
    "content": "Pi Pico Rx - A crystal radio for the digital age?¶\nMy first step into the world of electronics was with a crystal radio, just like this one.\n\nBack then, I don’t think it has ever occurred to me that I could make a radio\nmyself, so I wasn’t expecting it to work. But when I put the earphone in, I was\namazed to hear very faint sounds coming through. I couldn’t believe that\nbuilding a radio could be so simple, and the best part was, it didn’t need any\nbatteries! That little experience sparked my interest in electronics.\nTimes have certainly changed since then, and today, we find ourselves in a\ngolden age for electronics enthusiasts. Back in the ninteen eightees, I could\nhave never imagined that my pocket money would one day buy a device with\ncomputing power that could have filled an entire room just a few decades ago.\nI often wonder how we can still capture that sense of awe and excitement from\nmy first crystal radio experience. Is it still possible to create something\nsimple yet captivating?\nThe Pi Pico Rx - may be the answer to that question. While it may not be quite\nas straightforward as the crystal radio, the Pi Pico Rx presents a remarkably\nsimple solution. Armed with just a\nRaspberry Pi Pico,\nan analogue switch, and an op-amp, we now have the power to construct a capable\nSDR receiver covering the LW, MW, and SW bands. With the ability to receive\nsignals from halfway around the globe. I can’t help but think that my younger\nself would have been truly impressed!\nIf you are interested in a simpler version that can be built on a breadboard using mostly through-hole components, checkout the breadboard version\nhere.\n\nFeatures¶\n\n0 - 30MHz coverage\n250kHz bandwidth SDR reciever\nCW/SSB/AM/FM reception\nOLED display\nsimple spectrum scope\nHeadphones/Speaker\n500 general purpose memories\nruns on 3 AAA batteries\nless than 50mA current consumption\n\nPi Pico Rx¶\n\nPi Pico Rx is a minimal SDR receiver based around the Raspberry Pi Pico.\nThe design uses a “Tayloe” Quadrature Sampling Detector (QSD) popularised by\nDan Tayloe.\nAnd used in many HF SDR radio designs. This simple, design allows a\nhigh-quality mixer to be implemented using an inexpensive analogue\nswitch.\n\nA quadrature oscillator is generated using the PIO feature of the\nRP2040. This eliminates the need to use an external programmable\noscillator. Without overclocking the device this supports frequencies up\nto about 30MHz, conveniently covering the LW, MW and SW bands.\nThe IQ output from the QSD is amplified using a high-speed, low-noise\nop-amp. The I and Q channels are sampled by the built-in ADC which\nprovides 250kHz of bandwidth. The dual-core ARM Cortex M0 processor\nimplements the Digital Signal Processing algorithms, demodulating AM,\nFM, SSB and CW to produce an audio output.\nAudio output is provided using a PWM followed by a low-pass filter. At\nfirst, I used an LM386 audio amplifier, but later found that with a\nsuitable current limiting resistor the IO pin could easily drive a pair\nof headphones or even a small speaker directly.\nA cobbled-together prototype proves that it is possible to build an HF\nSDR receiver using a Pi Pico, an analogue switch, an op-amp and a\nhandful of discrete components.\n\nOther SDR Receivers¶\nThis isn’t a new idea by any means, there are loads of SDR designs out\nthere. Some use a PC soundcard, others use a built-in CPU for the SDR.\nThe uSDX project even does the DSP processing using an 8-bit micro! I\nhave put a list of links to some of the other projects that have\ninspired me. Each of these projects introduces new ideas and\ninnovations. I hope that the Pi Pico Rx has introduced some of its own\nevolutionary developments that might inspire others with their designs.\n\nsoftrock SDR\nQRP Labs\nmcHF\nelecraft KX3\nusdx\n(tr)uSDX\nuSDR-pico\ncompact-si5351-based-sdr\nqex\nTeensy-Convolutional SDR\n\nContinuing in the spirit of knowledge sharing, I have attempted to\ndescribe the finer details of the hardware and software in this wiki,\nstarting here with an overview of the new features introduced by the Pi\nPico Rx that I haven’t seen before in other designs.\n\nSampling IQ data using a round-robin ADC¶\nOne of the challenges I faced was sampling IQ signals using an ADC that\nis only capable of sampling one channel at a time. It can be configured\nin a round-robin mode that samples I and Q alternately. I had worried\nthat this might create a phase imbalance between the I and Q channels. I\nneedn’t have worried, it turns out that there is a simple way to recover\na complex signal with 250kHz bandwidth by sampling I and Q alternately\nat 500kSample/s.\nThe trick is to low pass filter the I and Q data leaving 250kHz of\nbandwidth from -125kHz to 125kHz. The QSD detector itself forms a\nlow-pass filter, so this can easily be achieved by selecting suitable\ncapacitor values in the op-amp. The ADC is configured to sample I and Q\nalternately (starting with I). In the software, the “missing” values can\nbe replaced by zeros.\n\nThis results in a complex signal with a sampling rate of 500KS/s. The\ncentral half of the spectrum from -125kHz to 125kHz contains the\nspectrum we want. The outer half of the spectrum from -250kHz to -125kHz\nand 125kHz to 250kHz contains reflections of the central half. To\nrecover the original spectrum, we simply need to low-pass filter the\nsignal so that we retain the central section. We could then reduce the\nsample rate to 250kHz. In our application, we filter out only a very\nsmall section of the spectrum containing the signal of interest, we\ndon’t need to take any additional steps to remove the reflections.\n\nWhy does this work?¶\nTo understand why this works, it helps to think about how we could have\nconverted the signal into a real (rather than complex IQ) signal and\nsampled it using a single-channel ADC. This is one of the approaches I\nhad originally considered taking. I only realised that there was an\neasier way once I worked the problem through. This was my thought\nprocess.\n\nTo satisfy Nyquist, we need to filter the complex data so that all our\nsignals sit between -125kHz and 125kHz. We could then shift the data up\nby 125kHz so that our signals are between 0 and 250kHz. The frequency\nshift is 1/4 of the 500KSample/second sample rate. A frequency shift by\nFs/4 can be implemented by rotating the signal by 1/4 turn in each\nsample. This doesn’t need any multiplication, only negation.\nSince our signal now only contains positive frequencies, the imaginary\n(Q) part of the signal doesn’t contain any useful information and we can\nthrow it away. A signal containing only real (I) values has a\nsymmetrical spectrum, discarding the imaginary samples introduces\nnegative frequency reflections of the positive frequency signals.\nThe real signal can now be sampled with a single-channel ADC at\n500kSamples/s. The frequency shift could have been implemented in\nhardware using a simple mixer, but we only need I and Q samples\nalternately, so we could use a round-robin ADC to capture the alternate\nI and Q samples and implement the mixer in software, negating I and Q\nwhen necessary.\nOnce we have the real signal in the software, we might like to convert\nthe real signal back to a complex one. We could use a Hilbert transform,\nthis would filter out the negative frequencies leaving a complex signal\nwith an asymmetrical spectrum containing only positive frequencies from\n0 to 250kHz.\nAnother approach would be to shift the frequencies down by 125kHz\nleaving the original spectrum from -125kHz to 125kHz, now with\nreflections in the outer half of the spectrum. These could be removed\nwith a low-pass filter. We can take the same approach to the Fs/4\nfrequency shift, this time rotating 1/4 turn each sample in the opposite\ndirection.\nInspecting the resulting samples, we can see that the downwards\nfrequency shift has cancelled out the negations we performed during the\nupwards frequency shift, leaving us with the alternating I/Q samples we\noriginally captured.\nConveniently, it turns out, the alternating IQ samples captured from the\nround-robin ADC were the only samples we needed to fully capture the\ncentral half of the frequency spectrum. The “missing” samples only\ncontributed to the outer part of the spectrum that we had already\nfiltered out.\n\nCreating Quadrature Oscillator Using PIO¶\nThe pi pico is based on the\nRP2040\nmicrocontroller. The PIO is a novel feature of the RP2040. Programmable\nState Machines (like small microprocessors) can be configured to offload\nIO functions from the software. It is fairly simple to configure a PIO\nstate machine to output a quadrature oscillator on 2 IO pins. Once\nconfigured the Oscillator runs autonomously without software\nintervention, not placing any further load on the CPU.\nThe PIO program is remarkably simple:\n.program nco\nset pins, 0\nset pins, 1      ; Drive pin low\nset pins, 3      ; Drive pin high\nset pins, 2      ; Drive pin low\n\nThe frequency of the NCO can be programmed using the PIO clock divider.\nThis has a 16-bit integer and an 8-bit fractional part. With an input\nclock of 125MHz, the NCO can be programmed from a few hundred Hz to just\nover 30MHz. Perfect for an LW/MW/SW receiver.\nAt low frequencies, a good resolution can be achieved, but at high\nfrequencies, the step size can be more than 100kHz. However, with a\nbandwidth of 250kHz, that is still enough to give continuous coverage of\nthe whole frequency range. To compensate for the coarse frequency\nresolution in the oscillator, a high-resolution frequency shifter is\nimplemented in the software. (The 32-bit phase accumulator has a\ntheoretical resolution of a little over 0.0001 Hz which should be\nample.)\n\nHardware Design¶\nThe design aim for the hardware is to make the design as simple and\ncheap as possible without compromising the performance too much. I have\ndesigned a PCB that expands on the basic concept to include a\npreamplifier and a bank of low-pass filters. To check out the details\nyou can look at the full Schematics in pdf format,\nbut I will walk through some of the details here.\n\nRaspberry Pi Pico¶\nThe heart of the receiver is a Raspberry Pi Pico. The onboard ADC\nsamples at 500kSamples/s giving us 250kHz of bandwidth. The 12-bit ADC\nhas a theoretical dynamic range of 72 dB, but it won’t be that good in\nreality. An SSB signal only needs 2.5kHz of bandwidth. We can exchange\nour excess bandwidth for increased dynamic range improving the overall\nsensitivity. An oversampling ratio of 100 gives us an extra 20dB,\nequivalent to adding 3 extra bits. This gives a theoretical dynamic\nrange of 92dB in SSB mode. The ADC has an input range of 0 to 3.3V. With\nno amplification, that represents a range of -78 dBm to 14 dBm.\n\nThe Raspberry Pi Pico has an onboard switched mode regulator, which\nallows the Pico to be easily powered by batteries. This design uses\n3xAAA batteries. It is possible to add additional external components to\nthe pi pico to allow the device to be powered from batteries, or the USB\npower supply. This design is primarily intended to be a portable\nstandalone radio, with the USB connection providing the ability to\nprogram the flash. To avoid the need to add additional components, I\nopted to remove D1 from the pico instead. This prevents the possibility\nof contention between the USB supply and the batteries.\n\nUser Interface¶\nThere isn’t anything particularly unusual about the user interface. A\n128x64 OLED display uses an ssd1306-based I2C interface. These are\nfairly ubiquitous these days and have replaced the HD44780 as the go-to\ncheap/simple display. The I2C interface certainly helps reduce the pin\ncount. Cost is a key driver, I could have replaced the rotary encoder\nwith a pair of push buttons to save cost, but I think this would be a\nstep too far. It wouldn’t feel like a radio without a proper tuning\nknob. Ideally, I would have liked to use something a bit more compact, a\nthumbwheel-based rotary encoder mounted on one edge would have been\nideal. Although there do seem to be some around, they seem to be quite\nhard to find.\nThis\ndirectional navigation scroll wheel also caught my eye, but in the end,\ncost won out and I went with a standard encoder.\n\nPWM audio¶\nAt first, I considered using an LM386 (or similar) audio amplifier to\ndrive the headphones or a small speaker. It turns out that the PWM is\nperfectly capable of driving headphones or a small speaker directly. A\n100uF capacitor blocks DC, the larger the capacitance the better the DC\nresponse, but in this application 100uF is probably overkill. The RP2040\nhas a maximum drive strength of 12mA. The 100-ohm resistor serves as a\ncurrent limiting resistor and one-half of an RC low-pass filter. With a\npeak voltage of 1.65v, and assuming an internal resistance of about 40\nohms, the maximum current into a 32-ohm load is\n1.65/(100+40+32) = 9.5mA and with an 8-ohm load is\n1.65/(100+40+8) = 11.1mA.\n\nIf a better speaker were needed, the\nTPA2012\nlooks like the ideal modern replacement for the LM386, and would be\nideal for battery-powered applications. The output also works well with\nPC speakers, but watch out for the drive level being significantly\nhigher than the usual 100mV pk-pk.\n\nQSD Detector (Tayloe Detector)¶\nThe design uses a “Tayloe” Quadrature Sampling Detector (QSD)\npopularised by [Dan Tayloe](The design uses a “Tayloe” Quadrature\nSampling Detector (QSD) popularised by Dan Tayloe.\nIt is used in many SDR receivers, and for good reason. In this design,\nthe select inputs to the analogue switch are driven directly by the\nRaspberry Pi Pico, the PIO feature of the RP2040 is capable of\ngenerating a quadrature oscillator at frequencies up to 30MHz without\nsoftware intervention. The resistor values have been chosen to give a\ngain of 1000 or 60 dB. This gives a theoretical input range at the input\nto the QSD of -138 dBm to -46 dBm. The capacitor values have been chosen\nto give a cut-off frequency of about 60kHz and a bandwidth of 120kHz.\nQSD is effectively acting as the anti-aliasing filter, so a degree of\noversampling helps. The gain and bandwidth requirements require a fast\nop-amp. The LT6231 is a popular choice in this type of SDR because of\nits low noise, it is fast enough to cope with the larger bandwidth used\nin this design compared to most SDRs. The newer LTC6227 op-amp is\nrecommended for new designs and is even better.\n\nOne potential weakness of this design is the potential of aliasing in\nthe ADC. This isn’t an issue for SDRs that use sound cards or audio\nADCs, they usually include very good antialiasing filters. A potential\nimprovement would be to include an active low-pass filter. This could\nmake use of a more basic (and cheaper) op-amp. There is also a potential\nto save cost by cascading several cheaper op-amps sharing the gain\nbetween them, the gain ban",
    "summary": {
      "en": "**Summary of Pi Pico Rx - A Crystal Radio for the Digital Age**\n\nThe Pi Pico Rx is a modern software-defined radio (SDR) receiver inspired by the simplicity and excitement of building a crystal radio. It can be constructed using a Raspberry Pi Pico, an analogue switch, and an op-amp, allowing it to receive signals across longwave (LW), mediumwave (MW), and shortwave (SW) bands. \n\n**Key Features:**\n- Frequency coverage from 0 to 30 MHz\n- 250 kHz bandwidth\n- Supports various reception modes: CW, SSB, AM, FM\n- OLED display and simple spectrum scope\n- Can save 500 memory presets\n- Powered by 3 AAA batteries with low current consumption\n\nThe Pi Pico Rx utilizes a unique design featuring a Tayloe Quadrature Sampling Detector (QSD) and a quadrature oscillator, allowing it to function without an external oscillator. It processes signals using the built-in ADC and digital signal processing capabilities of the RP2040 microcontroller. \n\n**User Interface and Audio Output:**\nThe user interface includes a basic OLED display and a rotary tuning knob. Initially designed with an audio amplifier, the Pi Pico Rx ultimately drives headphones directly through PWM output, simplifying the design.\n\n**Design Innovations:**\n- Uses a round-robin method to sample IQ signals with a single ADC channel.\n- Implements a quadrature oscillator through the PIO feature of the RP2040.\n- Focuses on cost-effective yet high-performance components.\n\nOverall, the Pi Pico Rx combines modern technology with the simplicity of traditional radio-making, offering an accessible way for electronics enthusiasts to explore radio communication.",
      "ko": "Pi Pico Rx는 현대의 소프트웨어 정의 라디오(SDR) 수신기로, 크리스탈 라디오를 만드는 단순함과 흥미에서 영감을 받았습니다. 이 장치는 Raspberry Pi Pico, 아날로그 스위치, 연산 증폭기를 사용하여 제작할 수 있으며, 장파(LW), 중파(MW), 단파(SW) 대역의 신호를 수신할 수 있습니다.\n\n주요 특징으로는 0에서 30 MHz까지의 주파수 범위와 250 kHz의 대역폭이 있습니다. 다양한 수신 모드를 지원하며, CW, SSB, AM, FM 방식으로 작동합니다. OLED 디스플레이와 간단한 스펙트럼 스코프를 갖추고 있으며, 500개의 메모리 프리셋을 저장할 수 있습니다. 전원은 3개의 AAA 배터리로 공급되며, 전력 소모가 낮습니다.\n\nPi Pico Rx는 독특한 디자인을 채택하여 Tayloe Quadrature Sampling Detector(QSD)와 쿼드러처 발진기를 사용합니다. 이 덕분에 외부 발진기 없이도 작동할 수 있습니다. 신호는 RP2040 마이크로컨트롤러의 내장 ADC와 디지털 신호 처리 기능을 통해 처리됩니다.\n\n사용자 인터페이스는 기본 OLED 디스플레이와 회전 조정 노브로 구성되어 있습니다. 초기에는 오디오 증폭기를 설계했으나, 최종적으로는 PWM 출력을 통해 헤드폰을 직접 구동하도록 간소화되었습니다.\n\n디자인 혁신으로는 단일 ADC 채널을 사용하여 IQ 신호를 샘플링하는 라운드 로빈 방법이 있습니다. RP2040의 PIO 기능을 통해 쿼드러처 발진기를 구현하였으며, 비용 효율적이면서도 고성능 부품에 중점을 두었습니다.\n\n전반적으로 Pi Pico Rx는 현대 기술과 전통적인 라디오 제작의 단순함을 결합하여 전자기기 애호가들이 라디오 통신을 탐구할 수 있는 접근 가능한 방법을 제공합니다.",
      "ja": null
    }
  },
  {
    "id": "29e3a9556d84a434",
    "title": {
      "en": "High-Performance PNG Decoding",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://blend2d.com/blog/png-image-codec.html",
    "score": 83,
    "by": "PaulHoule",
    "time": 1742710934,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "5f242b139bf770c2",
    "title": {
      "en": "A (Long) Peek into Reinforcement Learning",
      "ko": "강화학습의 모든 것",
      "ja": null
    },
    "type": "story",
    "url": "https://lilianweng.github.io/posts/2018-02-19-rl-overview/",
    "score": 157,
    "by": "Brysonbw",
    "time": 1742715629,
    "content": "A (Long) Peek into Reinforcement Learning\n\n    Date: February 19, 2018  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n\n            Table of Contents\n\n                    What is Reinforcement Learning?\n\n                    Key Concepts\n\n                    Model: Transition and Reward\n\n                    Policy\n\n                    Value Function\n\n                    Optimal Value and Policy\n\n                    Markov Decision Processes\n\n                    Bellman Equations\n\n                    Bellman Expectation Equations\n\n                    Bellman Optimality Equations\n\n                    Common Approaches\n\n                    Dynamic Programming\n\n                    Policy Evaluation\n\n                    Policy Improvement\n\n                    Policy Iteration\n\n                    Monte-Carlo Methods\n\n                    Temporal-Difference Learning\n\n                    Bootstrapping\n\n                    Value Estimation\n\n                    SARSA: On-Policy TD control\n\n                    Q-Learning: Off-policy TD control\n\n                    Deep Q-Network\n\n                    Combining TD and MC Learning\n\n                    Policy Gradient\n\n                    Policy Gradient Theorem\n\n                    REINFORCE\n\n                    Actor-Critic\n\n                    A3C\n\n                    Evolution Strategies\n\n                    Known Problems\n\n                    Exploration-Exploitation Dilemma\n\n                    Deadly Triad Issue\n\n                    Case Study: AlphaGo Zero\n\n                    References\n\n[Updated on 2020-09-03: Updated the algorithm of SARSA and Q-learning so that the difference is more pronounced.\n\n[Updated on 2021-09-19: Thanks to 爱吃猫的鱼, we have this post in Chinese].\nA couple of exciting news in Artificial Intelligence (AI) has just happened in recent years.  AlphaGo defeated the best professional human player in the game of Go. Very soon the extended algorithm AlphaGo Zero beat AlphaGo by 100-0 without supervised learning on human knowledge. Top professional game players lost to the bot developed by OpenAI on DOTA2 1v1 competition. After knowing these, it is pretty hard not to be curious about the magic behind these algorithms — Reinforcement Learning (RL). I’m writing this post to briefly go over the field. We will first introduce several fundamental concepts and then dive into classic approaches to solving RL problems. Hopefully, this post could be a good starting point for newbies, bridging the future study on the cutting-edge research.\nWhat is Reinforcement Learning?#\nSay, we have an agent in an unknown environment and this agent can obtain some rewards by interacting with the environment. The agent ought to take actions so as to maximize cumulative rewards. In reality, the scenario could be a bot playing a game to achieve high scores, or a robot trying to complete physical tasks with physical items; and not just limited to these.\n\nFig. 1. An agent interacts with the environment, trying to take smart actions to maximize cumulative rewards.\nThe goal of Reinforcement Learning (RL) is to learn a good strategy for the agent from experimental trials and relative simple feedback received. With the optimal strategy, the agent is capable to actively adapt to the environment to maximize future rewards.\nKey Concepts#\nNow Let’s formally define a set of key concepts in RL.\nThe agent is acting in an environment. How the environment reacts to certain actions is defined by a model which we may or may not know. The agent can stay in one of many states (s∈S) of the environment, and choose to take one of many actions (a∈A) to switch from one state to another. Which state the agent will arrive in is decided by transition probabilities between states (P). Once an action is taken, the environment delivers a reward (r∈R) as feedback.\nThe model defines the reward function and transition probabilities. We may or may not know how the model works and this differentiate two circumstances:\n\nKnow the model: planning with perfect information; do model-based RL. When we fully know the environment, we can find the optimal solution by Dynamic Programming (DP). Do you still remember “longest increasing subsequence” or “traveling salesmen problem” from your Algorithms 101 class? LOL. This is not the focus of this post though.\nDoes not know the model: learning with incomplete information; do model-free RL or try to learn the model explicitly as part of the algorithm. Most of the following content serves the scenarios when the model is unknown.\n\nThe agent’s policy π(s) provides the guideline on what is the optimal action to take in a certain state with the goal to maximize the total rewards. Each state is associated with a value function V(s) predicting the expected amount of future rewards we are able to receive in this state by acting the corresponding policy. In other words, the value function quantifies how good a state is. Both policy and value functions are what we try to learn in reinforcement learning.\n\nFig. 2. Summary of approaches in RL based on whether we want to model the value, policy, or the environment. (Image source: reproduced from David Silver's RL course lecture 1.)\nThe interaction between the agent and the environment involves a sequence of actions and observed rewards in time, t=1,2,…,T. During the process, the agent accumulates the knowledge about the environment, learns the optimal policy, and makes decisions on which action to take next so as to efficiently learn the best policy. Let’s label the state, action, and reward at time step t as St, At, and Rt, respectively. Thus the interaction sequence is fully described by one episode (also known as “trial” or “trajectory”) and the sequence ends at the terminal state ST:\n\nS1,A1,R2,S2,A2,…,ST\n\nTerms you will encounter a lot when diving into different categories of RL algorithms:\n\nModel-based: Rely on the model of the environment; either the model is known or the algorithm learns it explicitly.\nModel-free: No dependency on the model during learning.\nOn-policy: Use the deterministic outcomes or samples from the target policy to train the algorithm.\nOff-policy: Training on a distribution of transitions or episodes produced by a different behavior policy rather than that produced by the target policy.\n\nModel: Transition and Reward#\nThe model is a descriptor of the environment. With the model, we can learn or infer how the environment would interact with and provide feedback to the agent. The model has two major parts, transition probability function P and reward function R.\nLet’s say when we are in state s, we decide to take action a to arrive in the next state s’ and obtain reward r. This is known as one transition step, represented by a tuple (s, a, s’, r).\nThe transition function P records the probability of transitioning from state s to s’ after taking action a while obtaining reward r. We use P as a symbol of “probability”.\n\nP(s′,r|s,a)=P[St+1=s′,Rt+1=r|St=s,At=a]\n\nThus the state-transition function can be defined as a function of P(s′,r|s,a):\n\nPss′a=P(s′|s,a)=P[St+1=s′|St=s,At=a]=∑r∈RP(s′,r|s,a)\n\nThe reward function R predicts the next reward triggered by one action:\n\nR(s,a)=E[Rt+1|St=s,At=a]=∑r∈Rr∑s′∈SP(s′,r|s,a)\n\nPolicy#\nPolicy, as the agent’s behavior function π, tells us which action to take in state s. It is a mapping from state s to action a and can be either deterministic or stochastic:\n\nDeterministic: π(s)=a.\nStochastic: π(a|s)=Pπ[A=a|S=s].\n\nValue Function#\nValue function measures the goodness of a state or how rewarding a state or an action is by a prediction of future reward. The future reward, also known as return, is a total sum of discounted rewards going forward. Let’s compute the return Gt starting from time t:\n\nGt=Rt+1+γRt+2+⋯=∑k=0∞γkRt+k+1\n\nThe discounting factor γ∈[0,1] penalize the rewards in the future, because:\n\nThe future rewards may have higher uncertainty; i.e. stock market.\nThe future rewards do not provide immediate benefits; i.e. As human beings, we might prefer to have fun today rather than 5 years later ;).\nDiscounting provides mathematical convenience; i.e., we don’t need to track future steps forever to compute return.\nWe don’t need to worry about the infinite loops in the state transition graph.\n\nThe state-value of a state s is the expected return if we are in this state at time t, St=s:\n\nVπ(s)=Eπ[Gt|St=s]\n\nSimilarly, we define the action-value (“Q-value”; Q as “Quality” I believe?) of a state-action pair as:\n\nQπ(s,a)=Eπ[Gt|St=s,At=a]\n\nAdditionally, since we follow the target policy π, we can make use of the probility distribution over possible actions and the Q-values to recover the state-value:\n\nVπ(s)=∑a∈AQπ(s,a)π(a|s)\n\nThe difference between action-value and state-value is the action advantage function (“A-value”):\n\nAπ(s,a)=Qπ(s,a)−Vπ(s)\n\nOptimal Value and Policy#\nThe optimal value function produces the maximum return:\n\nV∗(s)=maxπVπ(s),Q∗(s,a)=maxπQπ(s,a)\n\nThe optimal policy achieves optimal value functions:\n\nπ∗=arg⁡maxπVπ(s),π∗=arg⁡maxπQπ(s,a)\n\nAnd of course, we have Vπ∗(s)=V∗(s) and Qπ∗(s,a)=Q∗(s,a).\nMarkov Decision Processes#\nIn more formal terms, almost all the RL problems can be framed as Markov Decision Processes (MDPs). All states in MDP has “Markov” property, referring to the fact that the future only depends on the current state, not the history:\n\nP[St+1|St]=P[St+1|S1,…,St]\n\nOr in other words, the future and the past are conditionally independent given the present, as the current state encapsulates all the statistics we need to decide the future.\n\nFig. 3. The agent-environment interaction in a Markov decision process. (Image source: Sec. 3.1 Sutton & Barto (2017).)\nA Markov deicison process consists of five elements M=⟨S,A,P,R,γ⟩, where the symbols carry the same meanings as key concepts in the previous section, well aligned with RL problem settings:\n\nS - a set of states;\nA - a set of actions;\nP - transition probability function;\nR - reward function;\nγ - discounting factor for future rewards.\nIn an unknown environment, we do not have perfect knowledge about P and R.\n\nFig. 4. A fun example of Markov decision process: a typical work day. (Image source: randomant.net/reinforcement-learning-concepts)\nBellman Equations#\nBellman equations refer to a set of equations that decompose the value function into the immediate reward plus the discounted future values.\n\nV(s)=E[Gt|St=s]=E[Rt+1+γRt+2+γ2Rt+3+…|St=s]=E[Rt+1+γ(Rt+2+γRt+3+…)|St=s]=E[Rt+1+γGt+1|St=s]=E[Rt+1+γV(St+1)|St=s]\n\nSimilarly for Q-value,\n\nQ(s,a)=E[Rt+1+γV(St+1)∣St=s,At=a]=E[Rt+1+γEa∼πQ(St+1,a)∣St=s,At=a]\n\nBellman Expectation Equations#\nThe recursive update process can be further decomposed to be equations built on both state-value and action-value functions. As we go further in future action steps, we extend V and Q alternatively by following the policy π.\n\nFig. 5. Illustration of how Bellman expection equations update state-value and action-value functions.\n\nVπ(s)=∑a∈Aπ(a|s)Qπ(s,a)Qπ(s,a)=R(s,a)+γ∑s′∈SPss′aVπ(s′)Vπ(s)=∑a∈Aπ(a|s)(R(s,a)+γ∑s′∈SPss′aVπ(s′))Qπ(s,a)=R(s,a)+γ∑s′∈SPss′a∑a′∈Aπ(a′|s′)Qπ(s′,a′)\n\nBellman Optimality Equations#\nIf we are only interested in the optimal values, rather than computing the expectation following a policy, we could jump right into the maximum returns during the alternative updates without using a policy. RECAP: the optimal values V∗ and Q∗ are the best returns we can obtain, defined here.\n\nV∗(s)=maxa∈AQ∗(s,a)Q∗(s,a)=R(s,a)+γ∑s′∈SPss′aV∗(s′)V∗(s)=maxa∈A(R(s,a)+γ∑s′∈SPss′aV∗(s′))Q∗(s,a)=R(s,a)+γ∑s′∈SPss′amaxa′∈AQ∗(s′,a′)\n\nUnsurprisingly they look very similar to Bellman expectation equations.\nIf we have complete information of the environment, this turns into a planning problem, solvable by DP. Unfortunately, in most scenarios, we do not know Pss′a or R(s,a), so we cannot solve MDPs by directly applying Bellmen equations, but it lays the theoretical foundation for many RL algorithms.\nCommon Approaches#\nNow it is the time to go through the major approaches and classic algorithms for solving RL problems. In future posts, I plan to dive into each approach further.\nDynamic Programming#\nWhen the model is fully known, following Bellman equations, we can use Dynamic Programming (DP) to iteratively evaluate value functions and improve policy.\nPolicy Evaluation#\nPolicy Evaluation is to compute the state-value Vπ for a given policy π:\n\nVt+1(s)=Eπ[r+γVt(s′)|St=s]=∑aπ(a|s)∑s′,rP(s′,r|s,a)(r+γVt(s′))\n\nPolicy Improvement#\nBased on the value functions, Policy Improvement generates a better policy π′≥π by acting greedily.\n\nQπ(s,a)=E[Rt+1+γVπ(St+1)|St=s,At=a]=∑s′,rP(s′,r|s,a)(r+γVπ(s′))\n\nPolicy Iteration#\nThe Generalized Policy Iteration (GPI) algorithm refers to an iterative procedure to improve the policy when combining policy evaluation and improvement.\n\nπ0→evaluationVπ0→improveπ1→evaluationVπ1→improveπ2→evaluation⋯→improveπ∗→evaluationV∗\n\nIn GPI, the value function is approximated repeatedly to be closer to the true value of the current policy and in the meantime, the policy is improved repeatedly to approach optimality. This policy iteration process works and always converges to the optimality, but why this is the case?\nSay, we have a policy π and then generate an improved version π′ by greedily taking actions, π′(s)=arg⁡maxa∈AQπ(s,a). The value of this improved π′ is guaranteed to be better because:\n\nQπ(s,π′(s))=Qπ(s,arg⁡maxa∈AQπ(s,a))=maxa∈AQπ(s,a)≥Qπ(s,π(s))=Vπ(s)\n\nMonte-Carlo Methods#\nFirst, let’s recall that V(s)=E[Gt|St=s]. Monte-Carlo (MC) methods uses a simple idea: It learns from episodes of raw experience without modeling the environmental dynamics and computes the observed mean return as an approximation of the expected return. To compute the empirical return Gt, MC methods need to learn from complete episodes S1,A1,R2,…,ST to compute Gt=∑k=0T−t−1γkRt+k+1 and all the episodes must eventually terminate.\nThe empirical mean return for state s is:\n\n𝟙𝟙V(s)=∑t=1T1[St=s]Gt∑t=1T1[St=s]\n\nwhere 𝟙1[St=s] is a binary indicator function. We may count the visit of state s every time so that there could exist multiple visits of one state in one episode (“every-visit”), or only count it the first time we encounter a state in one episode (“first-visit”). This way of approximation can be easily extended to action-value functions by counting (s, a) pair.\n\n𝟙𝟙Q(s,a)=∑t=1T1[St=s,At=a]Gt∑t=1T1[St=s,At=a]\n\nTo learn the optimal policy by MC, we iterate it by following a similar idea to GPI.\n\nImprove the policy greedily with respect to the current value function: π(s)=arg⁡maxa∈AQ(s,a).\nGenerate a new episode with the new policy π (i.e. using algorithms like ε-greedy helps us balance between exploitation and exploration.)\nEstimate Q using the new episode: 𝟙𝟙qπ(s,a)=∑t=1T(1[St=s,At=a]∑k=0T−t−1γkRt+k+1)∑t=1T1[St=s,At=a]\n\nTemporal-Difference Learning#\nSimilar to Monte-Carlo methods, Temporal-Difference (TD) Learning is model-free and learns from episodes of experience. However, TD learning can learn from incomplete episodes and hence we don’t need to track the episode up to termination. TD learning is so important that Sutton & Barto (2017) in their RL book describes it as “one idea … central and novel to reinforcement learning”.\nBootstrapping#\nTD learning methods update targets with regard to existing estimates rather than exclusively relying on actual rewards and complete returns as in MC methods. This approach is known as bootstrapping.\nValue Estimation#\nThe key idea in TD learning is to update the value function V(St) towards an estimated return Rt+1+γV(St+1) (known as “TD target”). To what extent we want to update the value function is controlled by the learning rate hyperparameter α:\n\nV(St)←(1−α)V(St)+αGtV(St)←V(St)+α(Gt−V(St))V(St)←V(St)+α(Rt+1+γV(St+1)−V(St))\n\nSimilarly, for action-value estimation:\n\nQ(St,At)←Q(St,At)+α(Rt+1+γQ(St+1,At+1)−Q(St,At))\n\nNext, let’s dig into the fun part on how to learn optimal policy in TD learning (aka “TD control”). Be prepared, you are gonna see many famous names of classic algorithms in this section.\nSARSA: On-Policy TD control#\n“SARSA” refers to the procedure of updaing Q-value by following a sequence of …,St,At,Rt+1,St+1,At+1,…. The idea follows the same route of GPI. Within one episode, it works as follows:\n\nInitialize t=0.\nStart with S0 and choose action A0=arg⁡maxa∈AQ(S0,a), where ϵ-greedy is commonly applied.\nAt time t, after applying action At, we observe reward Rt+1 and get into the next state St+1.\nThen pick the next action in the same way as in step 2: At+1=arg⁡maxa∈AQ(St+1,a).\nUpdate the Q-value function: Q(St,At)←Q(St,At)+α(Rt+1+γQ(St+1,At+1)−Q(St,At)).\nSet t=t+1 and repeat from step 3.\n\nIn each step of SARSA, we need to choose the next action according to the current policy.\nQ-Learning: Off-policy TD control#\nThe development of Q-learning (Watkins & Dayan, 1992) is a big breakout in the early days of Reinforcement Learning. Within one episode, it works as follows:\n\nInitialize t=0.\nStarts with S0.\nAt time step t, we pick the action according to Q values, At=arg⁡maxa∈AQ(St,a) and ϵ-greedy is commonly applied.\nAfter applying action At, we observe reward Rt+1 and get into the next state St+1.\nUpdate the Q-value function: Q(St,At)←Q(St,At)+α(Rt+1+γmaxa∈AQ(St+1,a)−Q(St,At)).\nt=t+1 and repeat from step 3.\n\nThe key difference from SARSA is that Q-learning does not follow the current policy to pick the second action At+1. It estimates Q∗ out of the best Q values, but which action (denoted as a∗) leads to this maximal Q does not matter and in the next step Q-learning may not follow a∗.\n\nFig. 6. The backup diagrams for Q-learning and SARSA. (Image source: Replotted based on Figure 6.5 in Sutton & Barto (2017))\nDeep Q-Network#\nTheoretically, we can memorize Q∗(.) for all state-action pairs in Q-learning, like in a gigantic table. However, it quickly becomes computationally infeasible when the state and action space are large. Thus people use functions (i.e. a machine learning model) to approximate Q values and this is called function approximation. For example, if we use a function with parameter θ to calculate Q values, we can label Q value function as Q(s,a;θ).\nUnfortunately Q-learning may suffer from instability and divergence when combined with an nonlinear Q-value function approximation and bootstrapping (See Problems #2).\nDeep Q-Network (“DQN”; Mnih et al. 2015) aims to greatly improve and stabilize the training procedure of Q-learning by two innovative mechanisms:\n\nExperience Replay: All the episode steps et=(St,At,Rt,St+1) are stored in one replay memory Dt={e1,…,et}. Dt has experience tuples over many episodes. During Q-learning updates, samples are drawn at random from the replay memory and thus one sample could be used multiple times. Experience replay improves data efficiency, removes correlations in the observation sequences, and smooths over changes in the data distribution.\nPeriodically Updated Target: Q is optimized towards target values that are only periodically updated. The Q network is cloned and kept frozen as the optimization target every C steps (C is a hyperparameter). This modification makes the training more stable as it overcomes the short-term oscillations.\n\nThe loss function looks like this:\n\nL(θ)=E(s,a,r,s′)∼U(D)[(r+γmaxa′Q(s′,a′;θ−)−Q(s,a;θ))2]\n\nwhere U(D) is a uniform distribution over the replay memory D; θ− is the parameters of the frozen target Q-network.\nIn addition, it is also found to be helpful to clip the error term to be between [-1, 1]. (I always get mixed feeling with parameter clipping, as many studies have shown that it works empirically but it makes the math much less pretty. :/)\n\nFig. 7. Algorithm for DQN with experience replay and occasionally frozen optimization target. The prepossessed sequence is the output of some processes running on the input images of Atari games. Don't worry too much about it; just consider them as input feature vectors. (Image source: Mnih et al. 2015)\nThere are many extensions of DQN to improve the original design, such as DQN with dueling architecture (Wang et al. 2016) which estimates state-value function V(s) and advantage function A(s, a) with shared network parameters.\nCombining TD and MC Learning#\nIn the previous section on value estimation in TD learning, we only trace one step further down the action chain when calculating the TD target. One can easily extend it to take multiple steps to estimate the return.\nLet’s label the estimated return following n steps as Gt(n),n=1,…,∞, then:\n\n          n\n          Gt\n          Notes\n\n          n=1\n          Gt(1)=Rt+1+γV(St+1)\n          TD learning\n\n          n=2\n          Gt(2)=Rt+1+γRt+2+γ2V(St+2)\n\n          …\n\n          n=n\n          Gt(n)=Rt+1+γRt+2+⋯+γn−1Rt+n+γnV(St+n)\n\n          …\n\n          n=∞\n          Gt(∞)=Rt+1+γRt+2+⋯+γT−t−1RT+γT−tV(ST)\n          MC estimation\n\nThe generalized n-step TD learning still has the same form for updating the value function:\n\nV(St)←V(St)+α(Gt(n)−V(St))\n\nWe are free to pick any n in TD learning as we like. Now the question becomes what is the best n? Which Gt(n) gives us the best return approximation? A common yet smart solution is to apply a weighted sum of all possible n-step TD targets rather than to pick a single best n. The weights decay by a factor λ with n, λn−1; the intuition is similar to why we want to discount future rewards when computing the return: the more future we look into the less confident we would be. To make all the weight (n → ∞) sum up to 1, we multiply every weight by (1-λ), because:\n\nletS=1+λ+λ2+…S=1+λ(1+λ+λ2+…)S=1+λSS=1/(1−λ)\n\nThis weighted sum of many n-step returns is called λ-return Gtλ=(1−λ)∑n=1∞λn−1Gt(n). TD learning that adopts λ-return for value updating is labeled as TD(λ). The original version we introduced above is equivalent to TD(0).\n\nFig. 8. Comparison of the backup diagrams of Monte-Carlo, Temporal-Difference learning, and Dynamic Programming for state value functions. (Image source: David Silver's RL course lecture 4: \"Model-Free Prediction\")\nPolicy Gradient#\nAll the methods we have introduced above aim to learn the state/action value function and then to select actions accordingly. Policy Gradient methods instead learn the policy directly with a parameterized function respect to θ, π(a|s;θ). Let’s define the reward function (opposite of loss function) as the expected return and train the algorithm with the goal to maximize the reward function. My next post described why the policy gradient theorem works (proof) and introduced a number of policy gradient algorithms.\nIn discrete space:\n\nJ(θ)=Vπθ(S1)=Eπθ[V1]\n\nwhere S1 is the initial starting state.\nOr in continuous space:\n\nJ(θ)=∑s∈Sdπθ(s)Vπθ(s)=∑s∈S(dπθ(s)∑a∈Aπ(a|s,θ)Qπ(s,a))\n\nwhere dπθ(s) is stationary distribution of Markov chain for πθ. If you are unfamiliar with the definition of a “stationary distribution,” please check this reference.\nUsing gradient ascent we can find the best θ that produces the highest return. It is natural to expect policy-based methods are more useful in continuous space, because there is an infinite number of actions and/or states to estimate the values for in continuous space and hence value-based approaches are computationally much more expensive.\nPolicy Gradient Theorem#\nComputing the gradient numerically can be done by perturbing θ by a small amount ε in the k-th dimension. It works even when J(θ) is not differentiable (nice!), but unsurprisingly very slow.\n\n∂J(θ)∂θk≈J(θ+ϵuk)−J(θ)ϵ\n\nOr analytically,\n\nJ(θ)=Eπθ[r]=∑s∈Sdπθ(s)∑a∈Aπ(a|s;θ)R(s,a)\n\nActually we have nice theoretical support for (replacing d(.) with dπ(.)):\n\nJ(θ)=∑s∈Sdπθ(s)∑a∈Aπ(a|s;θ)Qπ(s,a)∝∑s∈Sd(s)∑a∈Aπ(a|s;θ)Qπ(s,a)\n\nCheck Sec 13.1 in Sutton & Barto (2017) for why this is the case.\nThen,\n\nJ(θ)=∑s∈Sd(s)∑a∈Aπ(a|s;θ)Qπ(s,a)∇J(θ)=∑s∈Sd(s)∑a∈A∇π(a|s;θ)Qπ(s,a)=∑s∈Sd(s)∑a∈Aπ(a|s;θ)∇π(a|s;θ)π(a|s;θ)Qπ(s,a)=∑s∈Sd(s)∑a∈Aπ(a|s;θ)∇ln⁡π(a|s;θ)Qπ(s,a)=Eπθ[∇ln⁡π(a|s;θ)Qπ(s,a)]\n\nThis result is named “Policy Gradient Theorem” which lays the theoretical foundation for various policy gradient algorithms:\n\n∇J(θ)=Eπθ[∇ln⁡π(a|s,θ)Qπ(s,a)]\n\nREINFORCE#\nREINFORCE, also known as Monte-Carlo policy gradient, relies on Qπ(s,a), an estimated return by MC methods using episode samples, to update the policy parameter θ.\nA commonly used variation of REINFORCE is to subtract a baseline value from the return Gt to reduce the variance of gradient estimation while keeping the bias unchanged. For example, a common baseline is state-value, and if applied, we would use A(s,a)=Q(s,a)−V(s) in the gradient ascent update.\n\nInitialize θ at random\nGenerate one episode S1,A1,R2,S2,A2,…,ST\nFor t=1, 2, … , T:\n\nEstimate the the return G_t since the time step t.\nθ←θ+αγtGt∇ln⁡π(At|St,θ).\n\nActor-Critic#\nIf the value function is learned in addition to the policy, we would get Actor-Critic algorithm.\n\nCritic: updates value function parameters w and depending on the algorithm it could be action-value Q(a|s;w) or state-value V(s;w).\nActor: updates policy parameters θ, in the direction suggested by the critic, π(a|s;θ).\n\nLet’s see how it works in an action-value actor-critic algorithm.\n\nInitialize s, θ, w at random; sample a∼π(a|s;θ).\nFor t = 1… T:\n\nSample reward rt∼R(s,a) and next state s′∼P(s′|s,a).\nThen sample the next action a′∼π(s′,a′;θ).\nUpdate policy parameters: θ←θ+αθQ(s,a;w)∇θln⁡π(a|s;θ).\nCompute the correction for action-value at time t:\nGt:t+1=rt+γQ(s′,a′;w)−Q(s,a;w)\nand use it to update value function parameters:\nw←w+αwGt:t+1∇wQ(s,a;w).\nUpdate a←a′ and s←s′.\n\nαθ and αw are two learning rates for policy and value function parameter updates, respectively.\nA3C#\nAsynchronous Advantage Actor-Critic (Mnih et al., 2016), short for A3C, is a classic policy gradient method with the special focus on parallel training.\nIn A3C, the critics learn the state-value function, V(s;w), while multiple actors are trained in parallel and get synced with global parameters from time to time. Hence, A3C is good for parallel training by default, i.e. on one machine with multi-core CPU.\nThe loss function for state-value is to minimize the mean squared error, Jv(w)=(Gt−V(s;w))2 and we use gradient descent to find the optimal w. This state-value function is used as the baseline in the policy gradient update.\nHere is the algorithm outline:\n\nWe have global parameters, θ and w; similar thread-specific parameters, θ’ and w'.\nInitialize the time step t = 1\nWhile T <= T_MAX:\n\nReset gradient: dθ = 0 and dw = 0.\nSynchronize thread-specific parameters with global ones: θ’ = θ and w’ = w.\ntstart = t and get st.\nWhile (st≠TERMINAL) and (t−tstart<=tmax):\n\nPick the action at∼π(at|st;θ′) and receive a new reward rt and a new state st+1.\nUpdate t = t + 1 and T = T + 1.\n\nInitialize the variable that holds the return estimation R={0ifstis TERMINALV(st;w′)otherwise.\nFor i=t−1,…,tstart:\n\nR←ri+γR; here R is a MC measure of Gi.\nAccumulate gradients w.r.t. θ’: dθ←dθ+∇θ′log⁡π(ai|si;θ′)(R−V(si;w′));\nAccumulate gradients w.r.t. w’: dw←dw+∇w′(R−V(si;w′))2.\n\nUpdate synchronously θ using dθ, and w using dw.\n\nA3C enables the parallelism in multiple agent training. The gradient accumulation step (6.2) can be considered as a reformation of minibatch-based stochastic gradient update: the values of w or θ get corrected by a little bit in the direction of each training thread independently.\nEvolution Strategies#\nEvolution Strategies (ES) is a type of model-agnostic optimization approach. It learns the optimal solution by imitating Darwin’s theory of the evolution of species by natural selection. Two prerequisites for applying ES: (1) our solutions can freely interact with the environment and see whether they can solve the problem; (2) we are able to compute a fitness score of how good each solution is. We don’t have to know the environment configuration to solve the problem.\nSay, we start with a population of random solutions. All of them are capable of interacting with the environment and only candidates with high fitness scores can survive (only the fittest can survive in a competition for limited resources). A new generation is then created by recombining the settings (gene mutation) of high-fitness survivors. This process is repeated until the new solutions are good enough.\nVery different from the popular MDP-based approaches as what we have introduced above, ES aims to learn the policy parameter θ without value approximation. Let’s assume the distribution over the parameter θ is an isotropic multivariate Gaussian with mean μ and fixed covariance σ2I. The gradient of F(θ) is calculated:\n\n∇θEθ∼N(μ,σ2)F(θ)=∇θ∫θF(θ)Pr(θ)Pr(.) is the Gaussian density function.=∫θF(θ)Pr(θ)∇θPr(θ)Pr(θ)=∫θF(θ)Pr(θ)∇θlog⁡Pr(θ)=Eθ∼N(μ,σ2)[F(θ)∇θlog⁡Pr(θ)]Similar to how we do policy gradient update.=Eθ∼N(μ,σ2)[F(θ)∇θlog⁡(12πσ2e−(θ−μ)22σ2)]=Eθ∼N(μ,σ2)[F(θ)∇θ(−log⁡2πσ2−(θ−μ)22σ2)]=Eθ∼N(μ,σ2)[F(θ)θ−μσ2]\n\nWe can rewrite this formula in terms of a “mean” parameter θ (different from the θ above; this θ is the base gene for further mutation), ϵ∼N(0,I) and therefore θ+ϵσ∼N(θ,σ2). ϵ controls how much Gaussian noises should be added to create mutation:\n\n∇θEϵ∼N(0,I)F(θ+σϵ)=1σEϵ∼N(0,I)[F(θ+σϵ)ϵ]\n\nFig. 9. A simple parallel evolution-strategies-based RL algorithm. Parallel workers share the random seeds so that they can reconstruct the Gaussian noises with tiny communication bandwidth. (Image source: Salimans et al. 2017.)\nES, as a black-box optimization algorithm, is another approach to RL problems (In my original writing, I used the phrase “a nice alternative”; Seita pointed me to this discussion and thus I updated my wording.). It has a couple of good characteristics (Salimans et al., 2017) keeping it fast and easy to train:\n\nES does not need value function approximation;\nES does not perform gradient back-propagation;\nES is invariant to delayed or long-term rewards;\nES is highly parallelizable with very little data communication.\n\nKnown Problems#\nExploration-Exploitation Dilemma#\nThe problem of exploration vs exploitation dilemma has been discussed in my previous post. When the RL problem faces an unknown environment, this issue is especially a key to finding a good solution: without enough exploration, we cannot learn the environment well enough; without enough exploitation, we cannot complete our reward optimization task.\nDifferent RL algorithms balance between exploration and exploitation in different ways. In MC methods, Q-learning or many on-policy algorithms, the exploration is commonly implemented by ε-greedy; In ES, the exploration is captured by the policy parameter perturbation. Please keep this into consideration when developing a new RL algorithm.\nDeadly Triad Issue#\nWe do seek the efficiency and flexibility of TD methods that involve bootstrapping. However, when off-policy, nonlinear function approximation, and bootstrapping are combined in one RL algorithm, the training could be unstable and hard to converge. This issue is known as the deadly triad (Sutton & Barto, 2017). Many architectures using deep learning models were proposed to resolve the problem, including DQN to stabilize the training with experience replay and occasionally frozen target network.\nCase Study: AlphaGo Zero#\nThe game of Go has been an extremely hard problem in the field of Artificial Intelligence for decades until recent years. AlphaGo and AlphaGo Zero are two programs developed by a team at DeepMind. Both involve deep Convolutional Neural Networks (CNN) and Monte Carlo Tree Search (MCTS) and both have been approved to achieve the level of professional human Go players. Different from AlphaGo that relied on supervised learning from expert human moves, AlphaGo Zero used only reinforcement learning and self-play without human knowledge beyond the basic rules.\n\nFig. 10. The board of Go. Two players play black and white stones alternatively on the vacant intersections of a board with 19 x 19 lines. A group of stones must have at least one open point (an intersection, called a \"liberty\") to remain on the board and must have at least two or more enclosed liberties (called \"eyes\") to stay \"alive\". No stone shall repeat a previous position.\nWith all the knowledge of RL above, let’s take a look at how AlphaGo Zero works. The main component is a deep CNN over the game board configuration (precisely, a ResNet with batch normalization and ReLU). This network outputs two values:\n\n(p,v)=fθ(s)\n\ns: the game board configuration, 19 x 19 x 17 stacked feature planes; 17 features for each position, 8 past configurations (including current) for the current player + 8 past configurations for the opponent + 1 feature indicating the color (1=black, 0=white). We need to code the color specifically because the network is playing with itself and the colors of current player and opponents are switching between steps.\np: the probability of selecting a move over 19^2 + 1 candidates (19^2 positions on the board, in addition to passing).\nv: the winning probability given the current setting.\n\nDuring self-play, MCTS further improves the action probability distribution π∼p(.) and then the action at is sampled from this improved policy. The reward zt is a binary value indicating whether the current player eventually wins the game. Each move generates an episode tuple (st,πt,zt) and it is saved into the replay memory. The details on MCTS are skipped for the sake of space in this post; please read the original paper if you are interested.\n\nFig. 11. AlphaGo Zero is trained by self-play while MCTS improves the output policy further in every step. (Image source: Figure 1a in Silver et al., 2017).\nThe network is trained with the samples in the replay memory to minimize the loss:\n\nL=(z−v)2−π⊤log⁡p+c‖θ‖2\n\nwhere c is a hyperparameter controlling the intensity of L2 penalty to avoid overfitting.\nAlphaGo Zero simplified AlphaGo by removing supervised learning and merging separated policy and value networks into one. It turns out that AlphaGo Zero achieved largely improved performance with a much shorter training time! I strongly recommend reading these two papers side by side and compare the difference, super fun.\nI know this is a long read, but hopefully worth it. If you notice mistakes and errors in this post, don’t hesitate to contact me at [lilian dot wengweng at gmail dot com]. See you in the next post! :)\n\nCited as:\n@article{weng2018bandit,\n  title   = \"A (Long) Peek into Reinforcement Learning\",\n  author  = \"Weng, Lilian\",\n  journal = \"lilianweng.github.io\",\n  year    = \"2018\",\n  url     = \"https://lilianweng.github.io/posts/2018-02-19-rl-overview/\"\n}\ncopyReferences#\n[1] Yuxi Li. Deep reinforcement learning: An overview. arXiv preprint arXiv:1701.07274. 2017.\n[2] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction; 2nd Edition. 2017.\n[3] Volodymyr Mnih, et al. Asynchronous methods for deep reinforcement learning. ICML. 2016.\n[4] Tim Salimans, et al. Evolution strategies as a scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864 (2017).\n[5] David Silver, et al. Mastering the game of go without human knowledge. Nature 550.7676 (2017): 354.\n[6] David Silver, et al. Mastering the game of Go with deep neural networks and tree search. Nature 529.7587 (2016): 484-489.\n[7] Volodymyr Mnih, et al. Human-level control through deep reinforcement learning. Nature 518.7540 (2015): 529.\n[8] Ziyu Wang, et al. Dueling network architectures for deep reinforcement learning. ICML. 2016.\n[9] Reinforcement Learning lectures by David Silver on YouTube.\n[10] OpenAI Blog: Evolution Strategies as a Scalable Alternative to Reinforcement Learning\n[11] Frank Sehnke, et al. Parameter-exploring policy gradients. Neural Networks 23.4 (2010): 551-559.\n[12] Csaba Szepesvári. Algorithms for reinforcement learning. 1st Edition. Synthesis lectures on artificial intelligence and machine learning 4.1 (2010): 1-103.\n\nIf you notice mistakes and errors in this post, please don’t hesitate to contact me at [lilian dot wengweng at gmail dot com] and I would be super happy to correct them right away!\n\n      Reinforcement-Learning\n      Long-Read\n      Math-Heavy\n\n    «\n\n    Policy Gradient Algorithms\n\n     »\n\n    The Multi-Armed Bandit Problem and Its Solutions",
    "summary": {
      "en": "### Summary of Reinforcement Learning Overview\n\n**Reinforcement Learning (RL)** is a type of machine learning where an agent learns to make decisions by interacting with an environment to maximize rewards. Key concepts include:\n\n1. **Agent and Environment**: The agent takes actions in an environment to receive rewards. The goal is to learn a strategy that maximizes cumulative rewards over time.\n\n2. **Key Terms**:\n   - **State (s)**: A specific situation in the environment.\n   - **Action (a)**: The choices available to the agent.\n   - **Reward (r)**: Feedback from the environment after an action is taken.\n   - **Policy (π)**: A strategy that defines the action to take in each state.\n   - **Value Function (V)**: A measure of how good it is to be in a given state under a specific policy.\n\n3. **Markov Decision Processes (MDPs)**: Most RL problems can be framed as MDPs, where future states depend only on the current state and action.\n\n4. **Common Algorithms**:\n   - **Dynamic Programming**: Used when the model of the environment is known.\n   - **Monte-Carlo Methods**: Learn from complete episodes of experience without needing the model.\n   - **Temporal-Difference Learning**: Learns from incomplete episodes, updating estimates based on other learned values.\n   - **SARSA and Q-Learning**: Popular TD learning methods; SARSA is on-policy, while Q-learning is off-policy.\n   - **Deep Q-Networks (DQN)**: Combines Q-learning with deep learning to handle large state-action spaces.\n\n5. **Policy Gradient Methods**: Directly learn the policy rather than the value functions, useful for complex action spaces.\n\n6. **Challenges**:\n   - **Exploration vs. Exploitation**: Balancing between trying new actions and optimizing known rewards.\n   - **Deadly Triad**: Instability that arises when combining off-policy learning, nonlinear function approximation, and bootstrapping.\n\n7. **Case Study - AlphaGo Zero**: A landmark example of RL where the AI defeated top human players in Go, using self-play and reinforcement learning without human data.\n\nOverall, reinforcement learning is a powerful framework for developing intelligent systems capable of learning optimal behaviors through trial and error in various environments.",
      "ko": "강화 학습(Reinforcement Learning, RL)은 에이전트가 환경과 상호작용하며 보상을 극대화하는 결정을 배우는 기계 학습의 한 종류입니다. 이 과정에서 에이전트는 환경 내에서 행동을 취하고, 그에 대한 보상을 받습니다. 목표는 시간이 지남에 따라 누적 보상을 최대화하는 전략을 배우는 것입니다.\n\n강화 학습의 주요 용어에는 상태(state), 행동(action), 보상(reward), 정책(policy), 가치 함수(value function) 등이 있습니다. 상태는 환경 내의 특정 상황을 의미하고, 행동은 에이전트가 선택할 수 있는 옵션을 나타냅니다. 보상은 행동을 취한 후 환경으로부터 받는 피드백이며, 정책은 각 상태에서 취해야 할 행동을 정의하는 전략입니다. 가치 함수는 특정 정책 하에서 주어진 상태에 있는 것이 얼마나 좋은지를 측정하는 지표입니다.\n\n대부분의 강화 학습 문제는 마르코프 결정 과정(Markov Decision Processes, MDPs)으로 구성될 수 있습니다. 여기서 미래의 상태는 오직 현재의 상태와 행동에만 의존합니다. 일반적인 알고리즘으로는 동적 프로그래밍, 몬테카를로 방법, 시간 차 학습(Temporal-Difference Learning), SARSA 및 Q-러닝이 있습니다. SARSA는 정책 기반 학습 방법이고, Q-러닝은 비정책 기반 학습 방법입니다. 딥 Q-네트워크(Deep Q-Networks, DQN)는 Q-러닝과 딥 러닝을 결합하여 큰 상태-행동 공간을 처리할 수 있도록 합니다.\n\n정책 경량화 방법(Policy Gradient Methods)은 가치 함수 대신 정책을 직접 학습하여 복잡한 행동 공간에 유용합니다. 그러나 강화 학습에는 탐험과 활용의 균형, 비정상적인 학습 방식의 조합으로 인한 불안정성 같은 도전 과제가 존재합니다.\n\n알파고 제로(AlphaGo Zero)는 강화 학습의 대표적인 사례로, AI가 인간 최고 선수들을 이긴 바둑 게임에서 자가 학습과 강화 학습을 통해 인간 데이터를 사용하지 않고도 성과를 이뤘습니다. 전반적으로 강화 학습은 다양한 환경에서 시행착오를 통해 최적의 행동을 학습할 수 있는 지능형 시스템을 개발하는 강력한 프레임워크입니다.",
      "ja": null
    }
  },
  {
    "id": "b1123fc59c8adc59",
    "title": {
      "en": "Much of the administration's agenda for research is in Proj. 2025's 900+page doc",
      "ko": "2025 연구 청사진",
      "ja": null
    },
    "type": "story",
    "url": "https://www.nature.com/articles/d41586-025-00780-2",
    "score": 13,
    "by": "rntn",
    "time": 1743104295,
    "content": "NEWS EXPLAINER\n                27 March 2025\n\n            How Trump is following Project 2025’s radical roadmap to defund science\n\n                    Much of the Trump administration’s agenda for research is laid out in the 900-plus-page blueprint. Nature read it so you don’t have to.\n\n                By\n\n                Dan Garisto\n\n                    Dan Garisto\n\n                            View author publications\n\n                                You can also search for this author in PubMed\n                                    Google Scholar\n\n            Twitter\n\n            Facebook\n\n            Email\n\n                You have full access to this article via your institution.\n\n                         Project 2025 is a 900-plus-page playbook aimed at conservative policymakers.Credit: Mike Segar/ReutersIndiscriminate firings. Terminated grants. Cancelled programmes. The barrage of actions by US President Donald Trump has shocked the country’s research community over the past two months. Yet, much of it was planned out years in advance and laid out publicly.‘Scientists will not be silenced’: thousands protest Trump research cutsThe Heritage Foundation, a right-wing think-tank based in Washington DC, released Project 2025, a policy guidebook and staffing list, in April 2023, as a blueprint for what it hoped would be a second Trump presidency. Trump, however, disavowed the initiative during his 2024 presidential campaign, saying that he had no knowledge of it, after there was public backlash over the publication’s sweeping Republican policy proposals, such as banning abortion, overhauling the federal government and slashing funding for climate science.But Trump and his administration have closely hewed to Project 2025’s agenda, detailed in a sprawling, 922-page book, passing executive orders to defund climate initiatives and target diversity programmes. The Wall Street Journal found that more than half of Trump’s executive orders (EOs) align with Project 2025 recommendations. And most of its 40 listed authors are now key figures on Trump’s team.The Heritage Foundation did not respond to Nature’s request for comment. Taylor Rodgers, a White House spokesperson, said: “No one cared about Project 2025 when they elected president Trump in November 2024, and they don’t care now. President Trump is implementing the America First agenda he campaigned on to free up wasteful DEI [diversity, equity and inclusion] spending for cutting-edge scientific research, roll back radical climate regulations and restore America’s energy dominance.” But policy watchers expect that the president’s upcoming budget request to the US government will contain massive cuts to science agencies.Here, Nature examines science-related policies from Project 2025 that have already come to pass, and which ones might be on the horizon.Has Trump ever followed the think-tank’s advice before?The Heritage Foundation has been publishing policy recommendations since 1981, often timing them for a new presidency. At the start of Trump’s first term in 2017, the foundation listed hundreds of priorities, such as opening up sites for off-shore oil drilling. According to the foundation, after a year in office, Trump had followed through on 64% of them. Compared with previous guidebooks, Project 2025 advises more-extreme actions. For instance, whereas the 2017 version advised the president to “rein in the administrative state”, Project 2025 advocates that the president “dismantle the administrative state”, which employs more than two million federal workers. And its proposals don’t apply only to the federal government: Project 2025 also aims to reform the US education system, including universities. The goal, its authors write, is to “defang and defund the woke culture warriors who have infiltrated every last institution in America”.Protesters have demonstrated against Project 2025 before and after US President Donald Trump took office.Credit: Selcuk Acar/Anadolu via GettyWhich policies from the project has the Trump team implemented so far?Project 2025 notes that the US president has sole authority over the US executive branch, which includes many science agencies, and encourages the president to exercise it. Trump has so far followed this philosophy, using EOs to accomplish his wishlist. (EOs direct the US government to take action but cannot contravene laws.) Many are being challenged by lawsuits. Here are some of the science-related recommendations that have already been implemented:Anti-diversity efforts“Unwinding policies and procedures that are used to advance radical gender, racial, and equity initiatives under the banner of science.” page 60Trump signed an EO ending “DEI programs” on 20 January. In response, agencies such as the US National Science Foundation (NSF), a key funder of basic science, paused and reviewed grants for violations of Trump’s orders; the US National Institutes of Health (NIH), the world’s largest funder of biomedical research, has terminated grants related to diversity. DEI offices at numerous agencies have also been shuttered.Defunding climate science and green energy“The Biden [Trump’s predecessor] Administration’s climate fanaticism will need a whole-of-government unwinding.” page 60As he did during his first presidency, Trump has started the process to withdraw the United States from the 2015 Paris climate agreement, which commits nations to reducing their greenhouse-gas emissions. He also signed an EO targeting energy regulations. The US Environmental Protection Agency has since put forward a plan to roll back dozens of regulations on fossil fuels that protect against pollution.Cutting funds to universities“Congress should cap the indirect cost rate paid to universities.” page 355The NIH, under the Trump team’s orders, but without the approval of Congress, issued a notice on 7 February that it would cut the indirect cost rate paid to universities for things such as electricity and equipment maintenance to 15%. This would have slashed billions from university budgets. A federal judge has halted the policy from taking effect.Downsizing government“Permanent and substantive reductions in the number of nondefense federal employees.” page 78Russell Vought, an author of Project 2025 and now director of the US Office of Management and Budget (OMB), has ordered mass ‘reductions in force’ at agencies. NASA has already closed offices, including its Office of the Chief Scientist, and fired more than 20 workers. Other agencies are preparing plans: reports indicate that the NIH will cut 1,200 people — about 6% — from its staff.Russell Vought (right), an author of Project 2025, is now director of the US Office of Management and Budget under Trump (left).Credit: Evan Vucci/AP Photo/AlamyWhich proposed science policies from Project 2025 might be on the horizon?Project 2025 contains hundreds of recommendations, and many have not been implemented, although there are signs that the Trump administration is considering them. Here are some of those:Fetal-tissue research\n\n                                    Enjoying our latest content?\n                                    Login or create an account to continue\n\n                                            Access the most recent journalism from Nature's award-winning team\n                                            Explore the latest features & opinion covering groundbreaking research\n\n                                            Access through your institution\n\n                                    or\n\n                                            Sign in or create an account\n\n                                            Continue with Google\n\n                                            Continue with ORCiD\n\n                doi: https://doi.org/10.1038/d41586-025-00780-2\n\n                    Reprints and permissions\n\n                Related Articles\n\n                        Are the Trump team’s actions affecting your research? How to contact Nature\n\n                        ‘Scientists will not be silenced’: thousands protest Trump research cuts\n\n                        Trump’s siege of science: how the first 30 days unfolded and what’s next\n\n                        Postdocs and PhD students hit hard by Trump’s crackdown on science\n\n                        How Trump 2.0 is reshaping science\n\n                Subjects\n\n                Policy\n\n                Scientific community\n\n                Funding\n\n                Politics\n\n                Government\n\n    Latest on:\n\n                Policy\n\n                                    ‘Open source’ AI isn’t truly open — here’s how researchers can reclaim the term\n                                    World View 27 MAR 25\n\n                                    Gender gap in research publishing is improving — slowly\n                                    Nature Index 26 MAR 25\n\n                                    The full lethal impact of massive cuts to international food aid\n                                    Comment 26 MAR 25\n\n                Scientific community\n\n                                    Showing ‘ability’ in ‘disability’ — how I mastered interviews while using a wheelchair\n                                    Career Column 27 MAR 25\n\n                                    ‘Open source’ AI isn’t truly open — here’s how researchers can reclaim the term\n                                    World View 27 MAR 25\n\n                                    75% of US scientists who answered Nature poll consider leaving\n                                    News 27 MAR 25\n\n                Funding\n\n                                    75% of US scientists who answered Nature poll consider leaving\n                                    News 27 MAR 25\n\n                                    Exclusive: NIH to cut grants for COVID research, documents reveal\n                                    News 26 MAR 25\n\n                                    Give grants to female scientists in war zones\n                                    Correspondence 25 MAR 25\n\n                                    ‘Open source’ AI isn’t truly open — here’s how researchers can reclaim the term\n                                    World View 27 MAR 25\n\n                                    Gender gap in research publishing is improving — slowly\n                                    Nature Index 26 MAR 25\n\n                                    The full lethal impact of massive cuts to international food aid\n                                    Comment 26 MAR 25\n\n            Jobs\n\n                        Junior Group Leader Position at IMBA - Institute of Molecular Biotechnology\n\n                            TheInstitute of Molecular Biotechnology (IMBA)is one of Europe’s leading institutes for basic research in the life sciences. IMBA is located on t...\n                            Austria (AT)\n                            IMBA - Institute of Molecular Biotechnology\n\n                        Job Posting Title Associate or Senior Editor (Human Genetics), Nature Communications\n\n                            As an Associate or Senior Editor at Nature Communications you will handle manuscripts in Human Genetics.\n                            Jersey City, London, New York or Philadelphia - hybrid working model\n                            Springer Nature Ltd\n\n                        Solutions Specialist\n\n                            Job Title: Solutions Specialist  Location: New York or London (Hybrid Working) Closing date: March 31st, 2025  About Springer Nature Group Sprin...\n                            New York City, New York (US)\n                            Springer Nature Ltd\n\n                        Faculty Positions at FAFU  (Fujian Agriculture and Forestry University, Fuzhou, China)\n\n                            Fuzhou, Fujian (CN)\n                            Fujian Agriculture and Forestry University\n\n                        Department Chairs, School of Medicine at Southern University of Science and Technology (copy)\n\n                            Shenzhen, Guangdong, China\n                            Southern University of Science and Technology, School of Medicine",
    "summary": {
      "en": "**Summary of How Trump is Following Project 2025’s Roadmap to Defund Science**\n\nIn recent months, President Donald Trump has taken significant actions that impact the U.S. research community, many of which were planned in advance as part of \"Project 2025,\" a detailed policy guide released by the Heritage Foundation. This blueprint outlines aggressive proposals aimed at conservative policymakers, including drastic cuts to science funding and the dismantling of diversity programs.\n\nDespite Trump initially claiming he was unaware of Project 2025 during his 2024 campaign, his administration has enacted many of its recommendations. Over half of Trump’s executive orders align with Project 2025, which seeks to reduce federal oversight and funding for scientific initiatives. Major actions already taken include:\n\n1. **Ending Diversity Programs**: Trump has signed an executive order that terminates diversity, equity, and inclusion (DEI) programs, affecting key agencies like the National Science Foundation (NSF) and the National Institutes of Health (NIH).\n\n2. **Defunding Climate Science**: The Trump administration is working to withdraw from the Paris climate agreement and roll back environmental regulations.\n\n3. **Cutting University Funding**: The NIH attempted to reduce the indirect costs universities receive for research, although a federal judge has temporarily halted this policy.\n\n4. **Reducing Government Workforce**: There have been mass layoffs in federal agencies, including NASA and NIH, as part of efforts to downsize the government.\n\nLooking ahead, many recommendations from Project 2025 remain unimplemented, but there are indications that further cuts to scientific research, including fetal-tissue research, may be on the way. Overall, Trump's actions reflect a departure from traditional science funding and support, aligning closely with the radical agenda laid out in Project 2025.",
      "ko": "최근 몇 달 동안 도널드 트럼프 대통령은 미국 연구 커뮤니티에 영향을 미치는 중요한 조치를 취했습니다. 이러한 조치들은 헤리티지 재단이 발표한 \"프로젝트 2025\"라는 정책 가이드의 일환으로 미리 계획된 것들입니다. 이 청사진은 보수 정책 입안자들을 겨냥한 공격적인 제안들을 담고 있으며, 과학 자금의 대폭 삭감과 다양성 프로그램의 해체를 포함하고 있습니다.\n\n트럼프는 2024년 대선 캠페인 중 프로젝트 2025에 대해 몰랐다고 주장했지만, 그의 행정부는 많은 권고 사항을 실행해왔습니다. 트럼프의 행정명령 중 절반 이상이 프로젝트 2025와 일치하며, 이는 과학적 이니셔티브에 대한 연방의 감독과 자금을 줄이려는 목표를 가지고 있습니다. 이미 시행된 주요 조치들은 다음과 같습니다.\n\n첫째, 다양성 프로그램의 종료입니다. 트럼프는 다양성, 형평성, 포용성(DEI) 프로그램을 종료하는 행정명령에 서명했으며, 이는 국가 과학 재단(NSF)과 국립 보건원(NIH)과 같은 주요 기관에 영향을 미칩니다.\n\n둘째, 기후 과학 자금의 삭감입니다. 트럼프 행정부는 파리 기후 협정에서 탈퇴하고 환경 규제를 완화하기 위해 노력하고 있습니다.\n\n셋째, 대학 자금 지원의 축소입니다. NIH는 대학들이 연구를 위해 받는 간접비를 줄이려 했으나, 연방 판사가 이 정책을 일시적으로 중단시켰습니다.\n\n넷째, 정부 인력의 축소입니다. NASA와 NIH를 포함한 연방 기관에서 대규모 해고가 발생했으며, 이는 정부를 축소하려는 노력의 일환입니다.\n\n앞으로도 프로젝트 2025의 많은 권고 사항이 아직 시행되지 않았지만, 태아 조직 연구를 포함한 과학 연구에 대한 추가적인 자금 삭감이 있을 것으로 보입니다. 전반적으로 트럼프의 행동은 전통적인 과학 자금 지원에서 벗어나 프로젝트 2025에 명시된 급진적인 의제와 밀접하게 연결되어 있습니다.",
      "ja": null
    }
  },
  {
    "id": "386d3831dbdd6bee",
    "title": {
      "en": "Exploring Dynamic Dispatch in Rust (2017)",
      "ko": "러스트의 동적 디스패치 탐험",
      "ja": null
    },
    "type": "story",
    "url": "https://alschwalm.com/blog/static/2017/03/07/exploring-dynamic-dispatch-in-rust/",
    "score": 37,
    "by": "ibobev",
    "time": 1742762211,
    "content": "Let me preface this by saying that I am a novice in the world of rust (though I'm liking things so far!), so if I make technical mistakes please let me know and I will try to correct them. With that out of the way, lets get started.\nMy real motivation for taking a closer look at dynamic dispatch can be seen in the following code snippet. Suppose I want to create a struct CloningLab that contains a vector of trait objects (in this case, Mammal):\n\n          struct CloningLab {\n\n              subjects: Vec<Box<Mammal>>,\n\n          }\n\n          trait Mammal {\n\n              fn walk(&self);\n\n              fn run(&self);\n\n          }\n\n          #[derive(Clone)]\n\n          struct Cat {\n\n              meow_factor: u8,\n\n              purr_factor: u8\n\n          }\n\n          impl Mammal for Cat {\n\n              fn walk(&self) {\n\n                  println!(\"Cat::walk\");\n\n              }\n\n              fn run(&self) {\n\n                  println!(\"Cat::run\")\n\n              }\n\n          }\n\n        view raw\n\n          dispatch_1.rs\n\n        hosted with ❤ by GitHub\n\nThis works fine. You can iterate over the vector of subjects and call run or walk as you would expect. However, things break down when you try to add an additional trait to the trait object bounds like:\n\n          struct CloningLab {\n\n              subjects: Vec<Box<Mammal + Clone>>,\n\n          }\n\n          impl CloningLab {\n\n              fn clone_subjects(&self) -> Vec<Box<Mammal + Clone>> {\n\n                  self.subjects.clone()\n\n              }\n\n          }\n\n        view raw\n\n          dispatch_2.rs\n\n        hosted with ❤ by GitHub\n\nThis fails with the the following error:\nerror[E0225]: only the builtin traits can be used as closure or object bounds\n --> test1.rs:3:32\n  |\n3 |     subjects: Vec<Box<Mammal + Clone>>,\n  |                                ^^^^^ non-builtin trait used as bounds\n\nAnd I found this surprising. In my mind, a trait object with multiple bounds would be analogous to multiple inheritance in C++. I would expect the object to have multiple vpointers for each 'base', and do dispatch through the appropriate one. Given that rust is still a somewhat young language, I could appreciate why the developers might not want to introduce that complexity immediately (being stuck with a poor design forever would be a high cost for little reward), but I wanted to work out exactly how such a system might work (or not work).\nVtables in Rust\nLike C++, dynamic dispatch is achieved in Rust  though a table of function pointers (described here in the rust docs). According to that documentation, the memory layout of a Mammal trait object made from a Cat will consist of two pointers arranged like:\n\nI was surprised to see that the data members of the object had an additional layer of indirection. This is unlike the (typical) C++ representation which would look this:\n\nWith the vtable pointer first and the data members immediately following. The rust approach is interesting. It incurs a cost when 'constructing' a trait object, unlike the C++ approach in which a cast to a base pointer is free (or just some addition for multiple inheritance). But this cost is very minor. The rust approach has the benefit that an object does not have to store the vtable pointer if it is never used in a polymorphic context. I think it is fair to say that rust encourages the use of monomorphism, so this is probably a good trade-off.\nTrait Objects with Multiple Bounds\nReturning to the original problem, lets consider how it is resolved in C++. If we have multiple traits (purely abstract classes) that we implement for some structure, then an instance of that structure will have the following layout (e.x., Mammal and Clone):\n\nNotice that we now have multiple vtable pointers, one for each base class Cat inherits from (that contains virtual functions). To convert a Cat* to a Mammal*, we don't need to do anything, but to convert a Cat* to a Clone*, the compiler will add 8 bytes (assuming sizeof(void*) == 8) to the this pointer.\nIt is easy to imagine a similar thing for rust:\n\nSo there are now two vtable pointers in the trait object. If the compiler needs to perform dynamic dispatch on a Mammal + Clone trait object, it can access the appropriate entry in the appropriate vtable and perform the call. Because rust does not (yet) support struct inheritance, the problem of determining the correct subobject to pass as self, does not exist. self will always be whatever is pointed at by the data pointer.\nThis seems like it would work well, but this approach also has some redundancy. We have multiple copies of the type's size, alignment, and drop pointer. We can eliminate this redundancy by combining the vtables. This is essentially what happens when you perform trait inheritance like:\n\n          trait CloneMammal: Clone + Mammal{}\n\n          impl<T> CloneMammal for T where T: Clone + Mammal{}\n\n        view raw\n\n          dispatch_3.rs\n\n        hosted with ❤ by GitHub\n\nUsing trait inheritance in this way is a commonly suggested trick to get around the normal limitation of trait objects. The use of trait inheritance produces a single vtable without any redundancy. So the memory layout looks like:\n\nMuch simpler! And you can currently do this! Perhaps what we really want is for the compiler to generate a trait like this for us when we try to make a trait object with multiple bounds. But hold on, there are some significant limitations. Namely, you cannot convert a trait object of CloneMammal in to a trait object of Clone. This seems like very strange behavior, but it is not hard to see why such a conversion won't work.\nSuppose you attempt to write something like:\n\n          let cat = Cat {\n\n            meow_factor: 7\n\n            purr_factor: 8\n\n          };\n\n          // No problem, a CloneMammal is impl for Cat\n\n          let clone_mammal: &CloneMammal = cat;\n\n          // Error!\n\n          let clone: &Clone = &clone_mammal;\n\n        view raw\n\n          dispatch_4.rs\n\n        hosted with ❤ by GitHub\n\nLine 10 must fail to compile because the compiler cannot possibly find the appropriate vtable to put in the trait object. It only knows that the object being referenced implements CloneMammal, but it doesn't know which one. Of course, we can tell that it must be a Cat, but what if the code was something like:\n\n          let cat = Cat {\n\n            meow_factor: 7\n\n            purr_factor: 8\n\n          };\n\n          let dog = Dog { ... };\n\n          let clone_mammal: &CloneMammal;\n\n          if get_random_bool() == true {\n\n            clone_mammal = &cat;\n\n          } else {\n\n            clone_mammal = &dog;\n\n          }\n\n          // Error! How can the compiler know what vtable to\n\n          // point to?\n\n          let clone: &Clone = &clone_mammal;\n\n        view raw\n\n          dispatch_5.rs\n\n        hosted with ❤ by GitHub\n\nThe problem is more clear here. How can the compiler know what vtable to put in the trait object being constructed on line 17? If clone_mammal refers to a Cat, then it should be the Cat vtable for Clone. If it refers to a Dog then it should be the Dog vtable for Clone.\nSo the trait-inheritance approach has this limitation. You cannot convert a trait object in to any other kind of trait object, even when the trait object you want is more specific than the one you already have.\nThe multiple vtable pointer approach seems like a good way forward to allowing trait objects with multiple bounds. It is trivial to convert to a less-bounded trait object with that setup. The vtable the compiler should use is simply whatever is already Clone vtable pointer slot (the second pointer in diagram 4).\nConclusions\nI hope going through this was a useful exercise to some readers. It certainly helped me organize how I was thinking about trait objects. In practice, I think this is not really a pressing issue, the restriction was just surprising to me.\n\n                    Twitter\n\n                    Facebook\n\nrust",
    "summary": {
      "en": "The author, a beginner in Rust, explores the concept of dynamic dispatch and trait objects through a coding example involving a struct called `CloningLab` that uses a vector of `Mammal` trait objects. Initially, the author defines the `Mammal` trait with methods for `walk` and `run`, and implements it for a `Cat` struct. This setup works well until the author tries to add another trait, `Clone`, to the `Mammal` bounds, which results in a compilation error. \n\nThe author is surprised by this limitation, as they expected it to work similarly to multiple inheritance in C++. They explain that Rust uses vtables (tables of function pointers) for dynamic dispatch, which adds an indirection layer and differs from C++'s approach. Rust's design encourages monomorphism, which is seen as a beneficial trade-off.\n\nThe author discusses how multiple bounds could be handled in C++ and suggests a potential implementation in Rust using trait inheritance to create a single vtable, reducing redundancy. However, they highlight a limitation: you cannot convert a trait object of a combined trait type (like `CloneMammal`) to a more specific trait type (like `Clone`). This is due to the compiler's inability to determine the correct vtable for the specific trait object at runtime.\n\nIn conclusion, the author finds this exploration helpful for understanding trait objects in Rust, despite the restriction not being a major issue in practice.",
      "ko": "저자는 Rust 초보자로서 `Mammal` 트레이트 객체를 사용하는 `CloningLab`이라는 구조체를 통해 동적 디스패치와 트레이트 객체의 개념을 탐구합니다. 처음에 저자는 `walk`와 `run` 메서드를 가진 `Mammal` 트레이트를 정의하고, 이를 `Cat` 구조체에 구현합니다. 이 설정은 잘 작동하지만, 저자가 `Mammal`의 경계에 `Clone`이라는 또 다른 트레이트를 추가하려고 하자 컴파일 오류가 발생합니다.\n\n저자는 이 제한에 놀라며, C++의 다중 상속과 비슷하게 작동할 것이라고 예상했음을 설명합니다. Rust는 동적 디스패치를 위해 vtable(함수 포인터 테이블)을 사용하여 간접적인 계층을 추가하며, 이는 C++의 접근 방식과 다릅니다. Rust의 설계는 단일형(monormorphism)을 장려하며, 이는 유익한 절충으로 여겨집니다.\n\n저자는 C++에서 다중 경계를 처리하는 방법에 대해 논의하고, Rust에서 트레이트 상속을 사용하여 단일 vtable을 생성함으로써 중복을 줄이는 잠재적인 구현을 제안합니다. 그러나 그들은 한 가지 제한점을 강조합니다. 결합된 트레이트 타입(예: `CloneMammal`)의 트레이트 객체를 더 구체적인 트레이트 타입(예: `Clone`)으로 변환할 수 없다는 것입니다. 이는 컴파일러가 런타임에 특정 트레이트 객체에 대한 올바른 vtable을 결정할 수 없기 때문입니다.\n\n결론적으로 저자는 이러한 탐구가 Rust의 트레이트 객체를 이해하는 데 도움이 되었다고 느끼며, 이러한 제한이 실제로는 큰 문제가 되지 않는다고 말합니다.",
      "ja": null
    }
  },
  {
    "id": "60a2c79a07869468",
    "title": {
      "en": "Kilo Code: Speedrunning open source coding AI",
      "ko": "킬로 코드: 오픈소스 AI 속도전",
      "ja": null
    },
    "type": "story",
    "url": "https://blog.kilocode.ai/p/kilo-code-speedrunning-open-source-coding-ai",
    "score": 94,
    "by": "ofou",
    "time": 1743005731,
    "content": "Share this postKilo Code blogKilo Code: speedrunning open source coding AICopy linkFacebookEmailNotesMoreDiscover more from Kilo Code bloghttps://kilocode.ai/SubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.Already have an account? Sign inKilo Code: speedrunning open source coding AIJP PosmaMar 27, 20256Share this postKilo Code blogKilo Code: speedrunning open source coding AICopy linkFacebookEmailNotesMore21ShareLast year, the project that I led (Vesuvius Challenge) achieved a breakthrough in resurrecting an ancient library from the ashes of a volcano (and they’re still going). This success taught me an important lesson: an extremely fast-moving community can achieve incredible things. This seems obvious, but few do it! Since then I’ve been thinking about how to apply this lesson to other spaces, such as helping my friend Steve at Val Town prototype an AI agent.Since then I’ve been thinking a lot about AI agents. They’re the closest I’ve seen to the dream of “programming for all”: all humans shaping our digital worlds (“moldable as clay”). This is something that has long interested me, and I think its time has come. Gone are the days of thinking of code directly—AI agents will produce code “by the kilo”. Enter: Kilo Code.There are already a million AI agent companies out there, so why another one? Me and my team are an impatient bunch, and we want to quickly apply this idea of an extremely fast-moving community to AI agents. We’ve been living this extreme focus on speed already:Two weeks ago, I dropped everything in my life to build the first version.A week later we had assembled a team of 10 (full time).We then forked the Roo Code VSCode extension (which itself is a fork of Cline).Since then, we shipped a bunch of small improvements (more about that below).By Friday, we’ll have fixed two of your requests.Next week, we’ll do ~10 more improvements.Most importantly, we want to build this with you. We would love your continuous feedback (on Github and Discord), and we have a free tier (with $15 in free tokens a month!) in order to thank you—and for the best feedback we’ll give out more free tokens. The goal is to create the most user-friendly AI coding agent, as quickly as possible.Meet the teamThe team came together within a week, which means that we’ve self-selected for folks who value speed, and are down for adventure. Here are a few of us:I’m leading the team (JP Posma).Justin Halsall leads developer relations. He has worked on an AI company for the last 5 years, is the open source maintainer of rrweb, and an Apache contributor.Johan Otten is our lead developer. After building products for teaching kids to code, he’s now excited to better teach LLMs to code.In addition, we’re well-funded, and hiring. (If you’re interested mail us at hi@kilocode.ai. We’re remote-first, but with offices in SF and Amsterdam. Expect to work really hard and have a ton of fun.)Small fixes shippedWe’re serious about speed! In a few days we’ve plucked some low-hanging fruit:No need to create an OpenRouter account (use our own proxy)No need to pay and use a credit card (batteries included free tier using Claude 3.7 Sonnet)No need to select a model (good defaults)No need for telemetry acceptance (we send what we need to improve the product and are transparent about what we send and how we use it)No need to set auto acceptance (vibe coding—but you can turn it off)Support for DeepSeek in Fireworks (super fast)Multiple onboarding improvements (after doing user testing)Next week we’ll similarly focus on low-hanging fruit. Tell us what you want to see, and we’ll give out more free tokens for the best ideas!Secret Master PlanBesides quickly building with the community, we do need some rough direction of where to take things. We started with an AI agent (as opposed to a Copilot/Cursor-style UI), because we want to build the future we want to see, and which we think is coming. We want to build for the dream of billions of programmers; billions of artists; billions of scientists—using computing as moldable clay.However, right now AI agents only work well for smaller projects, and for novice/non-programmers. But we think we can quickly make them better, and move upmarket to more advanced use cases (Clay Christensen “Innovator’s Dilemma”-style).For this, we’ll be part of the larger community of AI coding companies. Hopefully we can bring meaningful new ideas to the space, as well as quickly implement the best ideas that we see from others. Since our extension is open source, we encourage everyone (e.g. our upstream Cline and Roo Code) to use our best code and ideas.We have some ideas already, many of which are already being pioneered by other tools, and some which are more out there:Instant app - Type an idea, press Enter (or Google-style “I’m feeling lucky!”), get a live app with a backend and database.Up-to-date docs - Crawl docs automatically, so whenever you need to find the latest version of your favorite tool, everything is up to date.Prompt/product-first workflows - Not even looking at any code; instead looking at the product in a browser, together with an agent.Browser IDE - A full development environment in the browser. Sandboxed, secure, no setup.Local/on-prem models - For extra speed, security, and privacy.Live collaboration - Share a link and work with your team in real time, prompting together.Parallel-agents - Run many agents in the same app in separate threads.Code variants - Create multiple code implementations from one prompt. Pick the best.Shared context - Reduce API calls by storing project knowledge across the team.Open source sharing - Allow snippets of your code to be shared as open source, and get used by others who are trying to do the same task (generalized caching).MCP marketplace - Making it easier to extend your agents.Integrated CI - Agent writes and runs tests, fixes things until all pass.Monitoring/production agents - Agent sees runtime stats and errors, and can fix or escalate.Security agents - Pentesting by agent, fixing by another agent.Sketching - draw the UIs or data architecture you want, or show with video recordings what to change.Generate kilos of code for free in VS CodeKilo Code is live in VS Code. Try it out and generate kilos of code for free. Sign up with your Google Account and get $15 worth in free Claude 3.7 usage without needing a credit card. And tell us what needs to improve, on Github and Discord. We’re building fast—your feedback shapes what comes next.Thanks for reading Kilo Code blog! Subscribe for free to receive updates and become part of the community.Subscribe6 Likes∙1 Restack6Share this postKilo Code blogKilo Code: speedrunning open source coding AICopy linkFacebookEmailNotesMore21Share",
    "summary": {
      "en": "**Summary of Kilo Code Blog Post**\n\nKilo Code is focused on speeding up the development of AI coding agents, inspired by a successful project that revived an ancient library. The team believes in the potential of AI agents to make coding accessible to everyone.\n\nKey points include:\n- The team quickly formed, with a focus on speed and efficiency.\n- They released a working version in just two weeks and plan to make continuous improvements based on user feedback.\n- The goal is to create a user-friendly AI coding agent that allows everyone to shape their digital environments.\n- They offer a free tier with $15 in usage tokens and encourage community feedback to refine the product.\n- Future plans include advanced features like instant app generation, real-time collaboration, and improved coding workflows.\n\nKilo Code is currently available in VS Code, allowing users to generate code for free and participate in shaping its development.",
      "ko": "Kilo Code는 고대 도서관을 되살린 성공적인 프로젝트에서 영감을 받아 AI 코딩 에이전트 개발 속도를 높이는 데 집중하고 있습니다. 팀은 AI 에이전트가 코딩을 모든 사람에게 접근 가능하게 만들 수 있는 잠재력을 믿고 있습니다.\n\n팀은 신속성과 효율성을 중시하며 빠르게 구성되었습니다. 그들은 단 2주 만에 작동하는 버전을 출시했으며, 사용자 피드백을 바탕으로 지속적인 개선을 계획하고 있습니다. 목표는 모든 사용자가 자신의 디지털 환경을 형성할 수 있도록 돕는 사용자 친화적인 AI 코딩 에이전트를 만드는 것입니다. 이들은 15달러의 사용 토큰이 포함된 무료 요금제를 제공하며, 제품 개선을 위해 커뮤니티의 피드백을 적극적으로 수렴하고 있습니다.\n\n앞으로의 계획에는 즉시 앱 생성, 실시간 협업, 개선된 코딩 작업 흐름과 같은 고급 기능이 포함되어 있습니다. 현재 Kilo Code는 VS Code에서 사용 가능하며, 사용자는 무료로 코드를 생성하고 개발 과정에 참여할 수 있습니다.",
      "ja": null
    }
  }
]