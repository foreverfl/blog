[
  {
    "id": "26eaa078a6e0337e",
    "title": {
      "en": "Tell HN: Announcing tomhow as a public moderator",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": null,
    "score": 492,
    "by": "dang",
    "time": 1743612596,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "b40025e58bed0577",
    "title": {
      "en": "Animals Made from 13 Circles (2016)",
      "ko": "13개의 원으로 만든 동물들",
      "ja": "13の円で生まれた動物たち"
    },
    "type": "story",
    "url": "https://www.dorithegiant.com/2016/05/13-animals-made-from-13-circles.html",
    "score": 271,
    "by": "jihadjihad",
    "time": 1743608125,
    "content": "May 28, 2016\n\n13 Animals Made From 13 Circles\n\n.animals img {\nwidth:500px;\n}\n\n.animals {\nmargin:auto;\ntext-align:justify;\nwidth:500px;\n}\n\n#button1:hover {\n  background: #8c70f2;\n}\n\n#button1 {\nborder:none;\n  width: 100%;\n  height: 35px;\n  background: #ec7146;\n  font-family: inherit;\n  font-weight: bold;\n  color: #fff;\n  letter-spacing: 1px;\n  border-radius: 5px 5px 5px 5px;\n  cursor: pointer;\n  transition: background .3s ease-in-out;\n}\n\nBuy Prints on Etsy\n\nInspired by the Twitter logo, which is made from 13 perfect circles, I decided to give myself a design challenge. I haven't done aesthetic work in a while and I wanted to show something simple & nice in my portfolio! So, here are 13 animals each made out of 13 circles!\n\nPosted by\nDori the Giant\n\nat\n3:52 PM\n\n25\ncomments:\n\nUnknown\nsaid...\n\nVery cool!\n\nMay 29, 2016 at 11:12 PM\n\nAbhishek\nsaid...\n\nHi! Awesome designs. Would love to have them as wallpapers. Can you please share some HiRes pics of those? Thanks! Great work, keep it up! :)\n\nMay 30, 2016 at 5:11 AM\n\nAnonymous\nsaid...\n\nSplendid!But you must work more with the cat and the rabbit.Thank you!\n\nMay 31, 2016 at 6:20 AM\n\nUnknown\nsaid...\n\nWould you sell these awesome pics?\n\nJune 12, 2016 at 2:15 AM\n\n<img src=\"//blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg581Fzfy9jDpyoUkFQ3mVF2MzZuIAVwGomShb1CBSCkj3cJxFrb66TRaS04FEWrzutiAUzM6Y32qJkm7Fs0e43fJ5O-CYhjBZfCTGjnxaxSPBU6Nv5scitsEiv25J3Ag/s45-c/twtr-new.jpg\" width=\"35\" height=\"35\" class=\"photo\" alt=\"\">\nDori the Giant\nsaid...\n\nYup, you can buy some prints! Send me an e-mail. :)\n\nJune 12, 2016 at 5:31 PM\n\nUnknown\nsaid...\n\nThese designs are great! Do you get around much\n\nJune 15, 2016 at 5:56 PM\n\nArmanno\nsaid...\n\nGRANDIOSO trabajo Dori, no sabía que el logo de Twitter era en base a círculos perfectos. Excelente lo tuyo!! De hecho lo compartí en el plus, así que genial!Rain more inspiration. A hug from Peru!\n\nJune 16, 2016 at 12:13 AM\n\nAnonymous\nsaid...\n\nHi there - would love to know where you sourced the coloured paper to make the shapes? Or did you design it yourself?\n\nJune 17, 2016 at 10:43 AM\n\nDragoNate\nsaid...\n\nThis is beautiful!\n\nJune 18, 2016 at 10:24 AM\n\nDragoNate\nsaid...\n\nAbsolutely beautiful work! Must've taken a long time\n\nJune 18, 2016 at 10:25 AM\n\n<img src=\"//2.bp.blogspot.com/_mYxcg4AF6dU/SprPX0H-aqI/AAAAAAAAAAk/ipbrjgccxSo/S45-s35/Rob_promo_sq.jpg\" width=\"35\" height=\"35\" class=\"photo\" alt=\"\">\nRob\nsaid...\n\nHey Dori, it's Rob from the studio in the junction. Really great work as usual. Hope all is well with you. If you're ever back in TO and want to grab lunch please get in touch. TC, rob\n\nJune 21, 2016 at 12:37 AM\n\nUnknown\nsaid...\n\nGorgeous, dori !\n\nJuly 6, 2016 at 5:00 PM\n\nAnonymous\nsaid...\n\nHow much per one pic?\n\nJuly 17, 2016 at 4:24 AM\n\n<img src=\"//blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg581Fzfy9jDpyoUkFQ3mVF2MzZuIAVwGomShb1CBSCkj3cJxFrb66TRaS04FEWrzutiAUzM6Y32qJkm7Fs0e43fJ5O-CYhjBZfCTGjnxaxSPBU6Nv5scitsEiv25J3Ag/s45-c/twtr-new.jpg\" width=\"35\" height=\"35\" class=\"photo\" alt=\"\">\nDori the Giant\nsaid...\n\nHey Anon,Depends on the size but about $20 for an 11\" x 14\".\n\nJuly 17, 2016 at 8:49 AM\n\nVetlogo\nsaid...\n\nCircle design of logos is still a thing I want to investigate. Did you teach yourself or did you have a tutorial for this kind of design? Greetz\n\nJuly 26, 2016 at 8:56 AM\n\n<img src=\"//blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg581Fzfy9jDpyoUkFQ3mVF2MzZuIAVwGomShb1CBSCkj3cJxFrb66TRaS04FEWrzutiAUzM6Y32qJkm7Fs0e43fJ5O-CYhjBZfCTGjnxaxSPBU6Nv5scitsEiv25J3Ag/s45-c/twtr-new.jpg\" width=\"35\" height=\"35\" class=\"photo\" alt=\"\">\nDori the Giant\nsaid...\n\nHey Vetlogo. It was basically just Illustrator circles + the shape builder too. I'll be writing a whole article about how I made them, they're pretty simple. Keep checking back or follow me on social media to see. :)\n\nJuly 26, 2016 at 1:37 PM\n\nVetlogo\nsaid...\n\nOk, thanks, I see that Twitter and Apple logo designer also used this technique. Is there no science behind it? Just circle fun ;)\n\nJuly 26, 2016 at 2:41 PM\n\nAldina\nsaid...\n\nthese are amazing! it makes me want to create some circle animals myself, also btw i love your crayon business cards as well. Super neat!\n\nJanuary 10, 2017 at 10:11 AM\n\nJonathan de Marville\nsaid...\n\nHi, Nice work !How do you animate circles in front of animals?\n\nJanuary 29, 2017 at 7:04 AM\n\nBoltmaster\nsaid...\n\nLove it!\n\nJanuary 17, 2018 at 11:50 AM\n\nGinebra Loz\nsaid...\n\nHice eso de examen final de Geometría.\n\nFebruary 9, 2018 at 10:29 PM\n\nUnknown\nsaid...\n\nWhat program do you use to create an image? And how can you color your images?\n\nOctober 19, 2018 at 12:08 AM\n\n<img src=\"//blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg581Fzfy9jDpyoUkFQ3mVF2MzZuIAVwGomShb1CBSCkj3cJxFrb66TRaS04FEWrzutiAUzM6Y32qJkm7Fs0e43fJ5O-CYhjBZfCTGjnxaxSPBU6Nv5scitsEiv25J3Ag/s45-c/twtr-new.jpg\" width=\"35\" height=\"35\" class=\"photo\" alt=\"\">\nDori the Giant\nsaid...\n\nHey guys, you can find the tutorial to make these here:https://www.smashingmagazine.com/2017/01/illustrating-animals-13-circles-drawing-tutorial-challenge/All the details!\n\nOctober 20, 2018 at 12:03 PM\n\nAnonymous\nsaid...\n\nThese are so cool! I love them!\n\nAugust 14, 2024 at 9:15 PM\n\nAnonymous\nsaid...\n\nAmazing, these are just gorgeous! Great job!\n\nApril 2, 2025 at 1:04 PM\n\nPost a Comment\n\n      BLOG_CMT_createIframe('https://www.blogger.com/rpc_relay.html', '0');\n\nSubscribe to:\nPost Comments (Atom)",
    "summary": {
      "en": "On May 28, 2016, Dori the Giant shared a creative project inspired by the Twitter logo, which is made up of 13 circles. Dori designed 13 animals, each created from 13 circles, to showcase in their portfolio. The post received positive feedback, with many commenters praising the designs and expressing interest in purchasing prints. Dori confirmed that prints are available for sale and offered to share a tutorial on how the designs were made using Illustrator. The designs sparked enthusiasm, with some users wanting to create their own circle animals.",
      "ko": "2016년 5월 28일, 도리 더 자이언트는 트위터 로고에서 영감을 받은 창의적인 프로젝트를 공유했습니다. 이 로고는 13개의 원으로 구성되어 있습니다. 도리는 13개의 원으로 만들어진 13마리의 동물을 디자인하여 자신의 포트폴리오에 전시했습니다. 이 게시물은 긍정적인 반응을 얻었고, 많은 댓글 작성자들이 디자인을 칭찬하며 인쇄물 구매에 관심을 보였습니다. 도리는 인쇄물이 판매 가능하다고 확인하며, 일러스트레이터를 사용해 디자인을 만드는 방법에 대한 튜토리얼을 공유하겠다고 제안했습니다. 이 디자인은 사용자들 사이에서 큰 관심을 불러일으켰고, 일부는 자신만의 원으로 만든 동물을 만들고 싶어했습니다.",
      "ja": "2016年5月28日、Dori the GiantはTwitterのロゴに触発されたクリエイティブなプロジェクトを発表しました。このロゴは13個の円で構成されています。Doriは、13個の円を使って作られた13種類の動物をデザインし、自身のポートフォリオに展示しました。この投稿は好評を博し、多くのコメントが寄せられ、デザインを称賛する声やプリントの購入希望が表明されました。Doriはプリントが販売中であることを確認し、Illustratorを使ったデザインの作り方についてのチュートリアルを共有することを提案しました。これらのデザインは多くの人々の興味を引き、一部のユーザーは自分自身の円形動物を作りたいと考えています。"
    }
  },
  {
    "id": "9f6880ea7a9e2158",
    "title": {
      "en": "Digital Archivists: Protecting Public Data from Erasure",
      "ko": "디지털 기록 지킴이",
      "ja": "デジタル保存士の使命"
    },
    "type": "story",
    "url": "https://spectrum.ieee.org/digital-archive",
    "score": 66,
    "by": "rbanffy",
    "time": 1743609792,
    "content": "ComputingMagazineOpinionApril 2025\n        How Digital Archivists Are Saving Public Information from the Memory Hole\n    Through clever usage of APIs, the Library Innovation Lab at Harvard Law School has created an archive of Data.gov, home to 311,000 public datasetsHarry Goldstein01 Apr 20253 min readHarry Goldstein is Editor in Chief of IEEE Spectrum.\n        James Steinberg\n\n    {\"customDimensions\": {\"5\":\"Harry Goldstein\",\"11\":2671626881,\"7\":\"digital archives, internet archive, government data, public data, websites, data preservation\",\"10\":\"digital archives\",\"6\":\"computing\",\"8\":\"04/01/2025\"}, \"post\": {\"id\": 2671626881, \"providerId\": 0, \"sections\": [0, 497728257, 2268663105, 539626628, 544169516, 539617903, 544169525, 2267926519, 2268663612], \"authors\": [20366473], \"tags\": [\"digital archives\", \"internet archive\", \"government data\", \"public data\", \"websites\", \"data preservation\"], \"streams\": [], \"split_testing\": {}} }",
    "summary": {
      "en": "The Library Innovation Lab at Harvard Law School is using APIs to create a digital archive of Data.gov, which contains 311,000 public datasets. This effort aims to preserve valuable public information and prevent it from being lost over time.",
      "ko": "하버드 로스쿨의 도서관 혁신 연구소는 API를 활용해 Data.gov의 디지털 아카이브를 만들고 있습니다. Data.gov에는 311,000개의 공공 데이터셋이 포함되어 있습니다. 이 프로젝트는 소중한 공공 정보를 보존하고 시간이 지나면서 사라지는 것을 방지하는 것을 목표로 하고 있습니다.",
      "ja": "ハーバード法科大学院のライブラリーイノベーションラボは、APIを活用してData.govのデジタルアーカイブを作成しています。このサイトには、311,000件の公共データセットが含まれています。この取り組みは、貴重な公共情報を保存し、時間が経つにつれて失われるのを防ぐことを目的としています。"
    }
  },
  {
    "id": "dc36b05df554d363",
    "title": {
      "en": "Matrix.org Will Migrate to MAS",
      "ko": "매트릭스, MAS로 전환!",
      "ja": "マトリックスMAS移行"
    },
    "type": "story",
    "url": "https://matrix.org/blog/2025/04/matrix-auth-service/",
    "score": 59,
    "by": "LorenDB",
    "time": 1743611330,
    "content": "Matrix.org will migrate to MAS on Apr 7th 2025\n\n            02.04.2025 15:00\n\n            —\n\n                General\n\n            —\n\n                Quentin\n\n            On Monday 7th of April 2025 at 7am UTC, we will migrate the Matrix.org homeserver's authentication system over to MAS (Matrix Authentication Service) in order to benefit from Next-generation authentication.\nThe migration will involve up to one hour of downtime.\nMSC3861 (Next-generation auth for Matrix, based on OAuth 2.0/OIDC) and its dependent MSCs have progressed sufficiently that the Foundation is confident in MAS and the new next-generation auth APIs.Specifically, all the MSCs are now in or have passed Final Comment Period (FCP) with disposition to merge! 🎉\nWe expect the MSCs to finish FCP and get merged into the next spec release.The full list of core Next-gen Auth MSCs is:\n\nMSC3861: Next-generation auth for Matrix, based on OAuth 2.0/OIDC\nMSC2964: Usage of OAuth 2.0 authorization code grant and refresh token grant\nMSC2965: OAuth 2.0 Authorization Server Metadata\nMSC2966: Usage of OAuth 2.0 Dynamic Client Registration in Matrix\nMSC2967: API scopes\nMSC4254: Usage of RFC7009 Token Revocation for Matrix client logout\n\nThis is incredibly exciting, reflecting 4 years of work on next-generation auth, and brings with it a new account management interface, additional security, and a better registration experience.\n🔗The account management interface\nYou will be able to manage you account on a dedicated interface at account.matrix.org (accessible through your client or browser), where you can:\n\nSee and delete your devices.\nUpdate your contact information, like your email address.\nChange your password and deactivate your account.\n\n🔗Improved security\nMAS comes with a significant refactoring of how authentication works on Matrix. Without breaking compatibility with the former authentication API, it brings several benefits\n\nNow, only your server will be able to see your account credentials! No more typing your password in every client you’d like to log in to.\nRestricted access to sensitive operations, like deactivating your account.\nClearer view of which clients are using your account.\n\n🔗Improved registration experience\nRegardless of the client you use, the new registration and login experience makes it clear where your account lives, and it supports next-generation clients like Element X.\n\n🔗Impact\nYour current sessions will remain active after the migration has taken effect. In other words, you will not be logged out of your clients.\nWe’re providing backwards compatibility for existing Matrix clients - this does not remove the stable Matrix 1.0 APIs. You can read more about the impact on clients in our previously published blog article - Authentication changes on Matrix.org.\n🔗This is only the beginning!\nMatrix Authentication Service is Matrix's next-generation authentication stack. Together with the next-generation authentication APIs, it is the base of a new exciting era for authentication in Matrix!\nThis has been one of the most ambitious projects within the Matrix project, the result of a multi-year investment by Element, funded in turn by Element’s customers, including BWI.\nIt will enable new forms of authentication flows, like QR-code login (coming soon to matrix.org with MSC4108), and new categories of applications building on Matrix, thanks to fine-grained control over client access to the account.\nYou can find all the technical details in Quentin's Matrix Conference talk, Harder Better Faster Stronger Authentication with OpenID Connect.\nFinally, if you have any concerns, please come talk to us in #matrix-auth:matrix.org\n\n                    The Foundation needs you\n\n                        The Matrix.org Foundation is a non-profit and only relies\n                        on donations to operate. Its core mission is to maintain\n                        the Matrix Specification, but it does much more than that.\n\n                        It maintains the matrix.org homeserver and hosts several\n                        bridges for free. It fights for our collective rights to\n                        digital privacy and dignity.\n\n                    Support us\n\n            Post Contents\n\n                    The account management interface\n\n                    Improved security\n\n                    Improved registration experience\n\n                    Impact\n\n                    This is only the beginning!",
    "summary": {
      "en": "Matrix.org will transition to the Matrix Authentication Service (MAS) on April 7, 2025, at 7 AM UTC, which will involve about one hour of downtime. This change is part of the implementation of next-generation authentication based on OAuth 2.0 and OpenID Connect.\n\nKey points include:\n\n- **New Features:** Users will have an improved account management interface at account.matrix.org, allowing them to manage devices, update contact information, change passwords, and deactivate accounts.\n- **Enhanced Security:** MAS will improve security by ensuring only the server sees account credentials, restricting access to sensitive actions, and providing a clearer view of active sessions.\n- **Better Registration Experience:** The new system will simplify how users register and log in, supporting modern clients like Element X.\n- **Current Sessions:** Users will remain logged in after the migration, and existing Matrix clients will still be supported.\n- **Future Developments:** The new authentication framework will enable more advanced login methods, such as QR-code login, and allow for more control over client access.\n\nThis migration represents a significant advancement for Matrix, following four years of development, and aims to enhance user experience and security. The Matrix.org Foundation, which supports this initiative, relies on donations to operate.",
      "ko": "Matrix.org는 2025년 4월 7일 오전 7시 UTC에 Matrix 인증 서비스(MAS)로 전환할 예정이며, 이 과정에서 약 한 시간의 서비스 중단이 발생할 것입니다. 이번 변화는 OAuth 2.0과 OpenID Connect를 기반으로 한 차세대 인증 시스템을 도입하기 위한 것입니다.\n\n새로운 기능으로는 사용자들이 account.matrix.org에서 계정 관리 인터페이스를 통해 기기를 관리하고, 연락처 정보를 업데이트하며, 비밀번호를 변경하고, 계정을 비활성화할 수 있는 기능이 포함됩니다. 보안 측면에서도 MAS는 계정 자격 증명이 서버에서만 보이도록 하여 보안을 강화하고, 민감한 작업에 대한 접근을 제한하며, 활성 세션에 대한 명확한 정보를 제공합니다. \n\n또한, 새로운 시스템은 사용자가 등록하고 로그인하는 과정을 간소화하여 Element X와 같은 최신 클라이언트를 지원합니다. 마이그레이션 후에도 사용자는 계속 로그인 상태를 유지하며, 기존의 Matrix 클라이언트도 지원됩니다. 앞으로는 QR 코드 로그인과 같은 더 발전된 로그인 방법이 가능해지고, 클라이언트 접근에 대한 제어도 강화될 것입니다.\n\n이번 마이그레이션은 4년간의 개발을 거쳐 이루어지는 중요한 발전으로, 사용자 경험과 보안을 향상시키는 것을 목표로 하고 있습니다. 이 이니셔티브를 지원하는 Matrix.org 재단은 운영을 위해 기부에 의존하고 있습니다.",
      "ja": "Matrix.orgは、2025年4月7日午前7時（UTC）にMatrix認証サービス（MAS）に移行します。この変更には約1時間のダウンタイムが伴います。これは、OAuth 2.0とOpenID Connectに基づく次世代認証の実装の一環です。\n\n新しい機能として、ユーザーはaccount.matrix.orgでアカウント管理のインターフェースが改善され、デバイスの管理、連絡先情報の更新、パスワードの変更、アカウントの無効化が可能になります。また、MASはセキュリティを強化し、アカウントの認証情報をサーバーのみが確認できるようにし、重要な操作へのアクセスを制限し、アクティブなセッションの状況をより明確に表示します。\n\n新しいシステムは、ユーザーが登録やログインを簡単に行えるようにし、Element Xなどの最新のクライアントをサポートします。移行後もユーザーはログインしたままとなり、既存のMatrixクライアントも引き続き利用可能です。新しい認証フレームワークは、QRコードログインなどの高度なログイン方法を可能にし、クライアントのアクセス管理をより柔軟に行えるようにします。\n\nこの移行は、4年間の開発を経てMatrixにとって大きな進展を意味し、ユーザー体験とセキュリティの向上を目指しています。この取り組みを支えるMatrix.org財団は、運営のために寄付に依存しています。"
    }
  },
  {
    "id": "3c2bb6b279489b7b",
    "title": {
      "en": "How Google built its Gemini robotics models",
      "ko": "구글의 제미니 로봇 비밀",
      "ja": "グーグルのロボット革命"
    },
    "type": "story",
    "url": "https://blog.google/products/gemini/how-we-built-gemini-robotics/",
    "score": 50,
    "by": "simonpure",
    "time": 1743605258,
    "content": "Breadcrumb\n\n              Products\n\n              Gemini\n\n    How we built the new family of Gemini Robotics models\n\n            Apr 01, 2025\n\n            ·\n\n            3 min read\n\n    Share\n\n  Twitter\n\n  Facebook\n\n  LinkedIn\n\n  Mail\n\n    Copy link\n\n          Powered by Gemini Robotics models, robots can learn complex actions like preparing salads, playing games like Tic-Tac-Toe and even folding an origami fox.\n\n  Joel Meares\n\n      Contributor, The Keyword\n\n    Share\n\n  Twitter\n\n  Facebook\n\n  LinkedIn\n\n  Mail\n\n    Copy link\n\n  class ProgressiveImage {\n    EVENTS = {\n      TRANSITION_END: 'transitionend',\n    };\n\n    CSS_CLASSES = {\n      BLUR: 'uni-progressive-image--blur',\n      NO_BLUR: 'uni-progressive-image--no-blur',\n    };\n\n    init(el) {\n      this.el = el;\n      this._events();\n      this._upgradeImage();\n    }\n\n    _upgradeImage() {\n      // For gif format images we don't include data-srcset and data-sizes\n      // We can safely remove the blur filter.\n      if (!this.el.dataset.srcset || !this.el.dataset.sizes) {\n        this.el.classList.add(this.CSS_CLASSES.NO_BLUR);\n\n        return;\n      }\n\n      this.el.setAttribute('srcset', this.el.dataset.srcset);\n      this.el.setAttribute('sizes', this.el.dataset.sizes);\n      requestAnimationFrame(() => {\n        this.el.classList.add(this.CSS_CLASSES.NO_BLUR);\n      });\n    }\n\n    _events() {\n      // Once the transition completes is safe to clean some attributes\n      this.el.addEventListener(this.EVENTS.TRANSITION_END, () => {\n        this.el.classList.remove(this.CSS_CLASSES.BLUR, this.CSS_CLASSES.NO_BLUR);\n        this.el.removeAttribute('data-srcset');\n        this.el.removeAttribute('data-sizes');\n      });\n    }\n  }\n\n  document.addEventListener('DOMContentLoaded', () => {\n    const images = document.querySelectorAll('[data-component=\"uni-progressive-image\"]');\n    images.forEach((el) => {\n      el.setAttribute('data-compononent-initialized', true);\n      new ProgressiveImage().init(el);\n    });\n  });\n\n        As Google DeepMind prepared for its recent announcement of a new family of Gemini 2.0 models designed specifically for robots, its head of robotics, Carolina Parada, gathered her team for another check of the tech’s capabilities.They asked a bi-arm ALOHA robot — a duo of limber metal appendages with multiple joints and pincer-like hands used widely in research — to perform tasks it hadn’t done before, using objects it hadn’t seen. “We did random things like put my shoe on the table and ask it to put some pens inside,” Carolina says. “The robot took a moment to understand the task, then did it.”For the next request, they found a toy basketball hoop and ball and asked the robot to do a “slam dunk.” Carolina watched, proud and delighted, as it did just that.\n\n      Carolina says witnessing the slam dunk was a “wow” moment.\n\n        “We’d trained models to help robots with specific tasks and to understand natural language before, but this was a step change,” Carolina says. “The robot had never seen anything related to basketball, or this specific toy. Yet it understood something complex — ‘slam dunk the ball’ — and performed the action smoothly. On its first try.”This all-rounder robot was powered by a Gemini Robotics model that is part of a new family of multimodal models for robotics. The models build upon Gemini 2.0 through fine-tuning with robot-specific data, adding physical action to Gemini’s multimodal outputs like text, video and audio. \"This milestone lays the foundation for the next generation of robotics that can be helpful across a range of applications,\" said Google CEO Sundar Pichai when announcing the new models on X.The Gemini Robotics models are highly dextrous, interactive and general, meaning they can drive robots to react to new objects, environments and instructions without further training. Helpful, given the team’s ambitions.“Our mission is to build embodied AI to power robots that help you with everyday tasks in the real world,” says Carolina, whose fascination with robotics began with childhood sci-fi cartoons, fueled by dreams of automated chores. “Eventually, robots will be just another surface on which we interact with AI, like our phones or computers — agents in the physical world.”\n\n        Like people, robots need two main functions to perform tasks effectively and safely: the ability to understand and make decisions, and the ability to take action. Gemini Robotics-ER, an \"embodied reasoning” model built on Gemini 2.0 Flash, focuses on the former, recognizing elements in front of it, defining their size and location, and predicting the trajectory and grip required to move them. It then can generate code to execute the action. We’re now making this model available to trusted testers and partners.Google DeepMind is also introducing Gemini Robotics, its most advanced vision-language-action model, which allows robots to reason about a scene, interact with the user and take action. Crucially, it makes significant advances in an area that has proved tricky for roboticists: dexterity. “What comes naturally to humans is difficult for robots,” Carolina explains. “Dexterity requires both spatial reasoning, and complex physical manipulation. Across testing, Gemini Robotics has set a new state-of-the-art for dexterity, solving complex multi-step tasks with smooth motions and great completion times.”\n\n      Gemini Robotics-ER excels at embodied reasoning capabilities, including detecting objects and pointing at object parts, finding corresponding points and detecting objects in 3D.\n\n        Powered by Gemini Robotics, machines have prepared salads, packed kids’ lunches, played games like Tic-Tac-Toe and even folded an origami fox.Preparing models that could do many different kinds of tasks was a challenge — largely because it went against the general industry practice of training models for a single task over and over until it can be solved. “Instead, we chose broad task learning, training models on a huge number of tasks,” Carolina says. “We expected to see generalization emerge after a certain amount of time, and we were right.”Both models can adapt to multiple embodiments, including academic-focused robots, like the bi-arm ALOHA machine, or humanoid robots like Apollo developed by our partner Apptronik.\n\n      The models adapt to different embodiments, able to perform tasks like packing a lunchbox or wiping a whiteboard in different forms.\n\n        This ability to adapt is key to a future where robots could take on a number of very different roles.“The possibilities for robots using highly general and capable models are broad and exciting,” Carolina says. “They could be more useful in industries where setups are complex, precision is important and the spaces aren’t human-friendly. And they could be helpful in human-centric spaces, like the home. That’s some years away, but these models are taking us several steps closer.”Sounds like someone will get some help with those chores — eventually.\n\n    POSTED IN:\n\nGemini\n\nGemini Models\n\nAI",
    "summary": {
      "en": "Google DeepMind has introduced a new family of Gemini Robotics models designed for robots, allowing them to learn and perform complex tasks such as preparing salads, playing games, and folding origami. Carolina Parada, the head of robotics, led her team to test these robots' abilities by asking them to complete unfamiliar tasks, like performing a \"slam dunk\" with a toy. The robots successfully executed these actions, showcasing significant advancements in their capabilities.\n\nThe Gemini Robotics models enhance robots' understanding of language and physical actions, enabling them to react to new objects and instructions without additional training. This development aims to create robots that can assist with everyday tasks, moving towards a future where robots are common helpers in homes and workplaces.\n\nThe models focus on two main functions: understanding and decision-making, and executing physical actions. The Gemini Robotics-ER model specializes in recognizing objects and planning movements, while the overall Gemini Robotics model is designed for dexterity, allowing robots to perform complex tasks smoothly.\n\nThese advancements are expected to significantly improve the versatility and effectiveness of robots in various industries, paving the way for robots to take on diverse roles in everyday life.",
      "ko": "구글 딥마인드는 로봇을 위한 새로운 제미니 로보틱스 모델을 발표했습니다. 이 모델은 로봇이 샐러드를 준비하고, 게임을 하며, 종이접기를 하는 등 복잡한 작업을 배우고 수행할 수 있도록 설계되었습니다. 로봇 공학 책임자인 카롤리나 파라다와 그녀의 팀은 로봇의 능력을 테스트하기 위해 이들에게 익숙하지 않은 작업, 예를 들어 장난감으로 슬램덩크를 하도록 요청했습니다. 로봇은 이러한 동작을 성공적으로 수행하며 그들의 능력이 크게 발전했음을 보여주었습니다.\n\n제미니 로보틱스 모델은 로봇이 언어와 물리적 행동을 이해하는 능력을 향상시켜, 추가 교육 없이도 새로운 물체와 지시에 반응할 수 있도록 합니다. 이 개발의 목표는 일상적인 작업을 도와줄 수 있는 로봇을 만드는 것이며, 앞으로 로봇이 가정과 직장에서 흔히 사용되는 도우미가 되는 미래를 향해 나아가고 있습니다.\n\n이 모델은 이해 및 의사결정과 물리적 행동 수행이라는 두 가지 주요 기능에 중점을 두고 있습니다. 제미니 로보틱스-ER 모델은 물체 인식과 움직임 계획에 특화되어 있으며, 전체 제미니 로보틱스 모델은 로봇이 복잡한 작업을 매끄럽게 수행할 수 있도록 손재주에 맞춰 설계되었습니다.\n\n이러한 발전은 다양한 산업에서 로봇의 다재다능성과 효율성을 크게 향상시킬 것으로 기대되며, 로봇이 일상생활에서 다양한 역할을 맡을 수 있는 길을 열어줄 것입니다.",
      "ja": "Google DeepMindは、ロボット向けに新しいGemini Roboticsモデルを発表しました。このモデルは、サラダを作ったり、ゲームをしたり、折り紙を折ったりするなど、複雑な作業を学び実行できるように設計されています。ロボティクス部門の責任者であるカロリーナ・パラダ氏は、チームと共にロボットの能力をテストし、玩具を使った「スラムダンク」などの慣れない作業を実行させました。ロボットはこれらの動作を成功裏に行い、能力の大きな進展を示しました。\n\nGemini Roboticsモデルは、ロボットの言語理解と物理的な動作の理解を向上させ、新しい物体や指示に対して追加の訓練なしで反応できるようにします。この開発は、日常的な作業を手伝うロボットを作り出すことを目指しており、将来的には家庭や職場でロボットが一般的な助け手となることを目指しています。\n\nこのモデルは、理解と意思決定、そして物理的な動作の実行という二つの主要な機能に焦点を当てています。Gemini Robotics-ERモデルは物体の認識と動きの計画を専門としており、全体のGemini Roboticsモデルは器用さを重視しており、ロボットが複雑な作業をスムーズに行えるように設計されています。\n\nこれらの進展は、さまざまな産業におけるロボットの多様性と効果を大幅に向上させると期待されており、日常生活の中で多様な役割を果たすロボットの実現に向けた道を開くことになるでしょう。"
    }
  },
  {
    "id": "c0eca3ad767d5e14",
    "title": {
      "en": "Show HN: Mermaid Chart VS Code Plugin: Mermaid.js Diagrams in Visual Studio Code",
      "ko": "머메이드 차트 플러그인",
      "ja": "メルメ図 VS Codeプラグイン"
    },
    "type": "story",
    "url": "https://docs.mermaidchart.com/blog/posts/mermaid-chart-vs-code-plugin-create-and-edit-mermaid-js-diagrams-in-visual-studio-code",
    "score": 34,
    "by": "msfi",
    "time": 1743611633,
    "content": "Mermaid Chart VS Code Plugin: Create and Edit Mermaid.js Diagrams in Visual Studio Code 21 March 2025 • 5 mins\nThe Mermaid Chart VS Code Plugin is a powerful developer diagramming tool that brings Mermaid.js diagramming directly into your Visual Studio Code environment. Whether you’re visualizing software architecture, documenting API flows, fixing bad documentation, or managing flowcharts and sequence diagrams, this plugin integrates seamlessly into your workflow.\n\nKey Features of the Mermaid Chart VS Code Plugin for Diagramming\n\nEdit Mermaid.js Diagrams in VS Code Without an Account\n\nNo sign-ups, no interruptions. You can open and edit .mmd files instantly using the built-in Mermaid Chart Editor, no account required. This is ideal for quick diagram updates and low-friction workflows.\n\nAutomatic File Recognition and Mermaid.js Syntax Highlighting\n\nThe plugin recognizes .mmd files automatically in the VS Code file explorer. You also get native Mermaid.js syntax highlighting, making your diagrams easier to read and edit. Syntax support clarifies flow structure and reduces editing errors.\n\nReal-Time Diagram Rendering with Pan and Zoom\n\nEdit and preview your Mermaid.js diagrams in real time, directly inside VS Code. Interactive pan and zoom features maintain your view state as you work, giving you a smooth, uninterrupted editing experience.\n\nEmbedded Mermaid Diagrams in Markdown\n\nWorking with Markdown? The plugin auto-detects Mermaid.js diagrams in .md files and provides a link to edit and view them. This makes it easy to integrate visuals into your documentation without switching tools.\n\nAdvanced Capabilities for Collaborative Development\n\nWhen you’re ready to go beyond the basics, logging into Mermaid Chart unlocks more powerful features tailored for team collaboration and large projects.\n\nCloud Integration and Diagram Linking\n\nLog in to link local .mmd files to your Mermaid Chart projects. Each diagram gets a reference in its code, enabling automatic cloud sync. This allows teams to share diagrams with non-developers via a browser-based editor, removing VS Code dependency for collaboration.\n\nConnect your diagrams in the VS Code plugin\n\nEmbed diagrams directly within code. A “View Diagram” button appears inline, giving developers quick access to visuals without leaving the file. Ideal for reviewing code documentation diagrams in context.\n\nFlexible Editing: Local or Visual Editor\n\nYou can edit diagrams either:\n\nLocally in VS Code\n\nIn the Mermaid Chart platform via the visual editor, whiteboard, or Mermaid AI assistant\n\nThis flexibility supports both visual-first users and those who prefer raw code.\n\nOffline Editing + Git-Based Version Control\n\nWith the “Download Connected Diagram” feature, you can work offline while keeping diagrams synced to Mermaid Chart. This is perfect for developers managing diagrams in version-controlled repositories.\n\nGenerate diagrams From code using AI\n\nAutomatically create diagrams from your source code using AI. Just start a GitHub Copilot Chat and mention @mermaid-chart.\n\n@mermaid-chart“Generate a class diagram for these files”“Create a sequence diagram from this API call flow”\n\nLink your relevant files, and the assistant will produce a Mermaid diagram, preview it instantly, and even let you save and edit it later.\n\n*Note: Some features require users to install the GitHub Copilot extension to access the AI participant@mermaid-chart within the Github Copilot Chat view.\n\nReal-World Use Cases: Developer-Centric Diagramming\n\nHere’s how developers are using the Mermaid Chart VS Code Plugin in real projects:\n\n1. Diagramming for DevOps\n\nVisualize CI/CD pipelines, infrastructure layers, or container orchestration systems like Kubernetes with clear flowcharts.\n\n2. Visualizing API Flows\n\nUse sequence diagrams to document how services communicate through REST, GraphQL, or gRPC endpoints.\n\n3. Documenting Microservice Architectures\n\nShow system boundaries, dependencies, and service-to-service messaging in a way that’s easy for both tech and non-tech stakeholders to understand.\n\n4. Improving Internal Documentation\n\nEnhance your internal wikis or READMEs with embedded diagrams to make documentation more engaging and readable.\n\n5. Whiteboarding Software Designs\n\nUse the visual editor or whiteboard mode for brainstorming system designs, then transition to raw Mermaid.js code for refinement and versioning.\n\nGetting Started: Install and Use in Minutes\n\nInstall the plugin from the VS Code Marketplace.\n\nOpen a .mmd or .md file and begin writing Mermaid.js syntax.\n\nUse the real-time preview pane to review diagram changes instantly.\n\nLog in to Mermaid Chart to access advanced features like cloud sync, team collaboration, and the visual editor.\n\nKey Benefits Recap\n\nNo account needed for basic editing\n\nFull syntax highlighting and file recognition\n\nReal-time diagram rendering with pan/zoom\n\nMarkdown integration for embedded diagrams\n\nCloud sync and collaboration options\n\nGit-friendly version control workflows\n\nFAQs: Mermaid Chart VS Code Plugin\n\nHow do I render a Mermaid.js diagram in VS Code?\n\nDevelopers can open a .mmd or Markdown file containing Mermaid syntax. The plugin renders diagrams in a preview pane.\n\nCan I use this plugin without a Mermaid Chart account?\n\nYes. Basic code editing, syntax highlighting, and previews work with no account needed. Advanced features like cloud integration and AI require login.\n\nDoes it support flowcharts, Gantt charts, or sequence diagrams?\n\nYes. Mermaid Chart’s VS Code plugin supports all standard Mermaid.js diagram types, including flowcharts, Gantt, sequence, class diagrams, and more.\n\nCan I collaborate with non-developers?\n\nYes. When connected to the cloud, diagrams can be shared and accessed via browser-based tools, making it easy for non-developers to view and contribute using the visual editor or whiteboard if they’re unfamiliar with the syntax.\n\nIs it suitable for documenting software architecture?\n\nAbsolutely. The plugin is widely used for software architecture visualization with AWS and Azure icons, helping teams map systems and dependencies clearly.\n\nCan diagrams be version-controlled in Git?\n\nYes. Diagrams saved locally can be tracked in Git, and synced with Mermaid Chart’s cloud features when needed.\n\nRelated Resources\n\nOfficial Mermaid Documentation\n\nMermaid Chart Platform\n\nVS Code Extension Guide: Creating Custom Workflows\n\n Author Mermaid Chart     𝕏",
    "summary": {
      "en": "**Summary of the Mermaid Chart VS Code Plugin**\n\nThe Mermaid Chart VS Code Plugin allows developers to create and edit diagrams using Mermaid.js directly in Visual Studio Code. It simplifies the process of visualizing software architecture, API flows, and more, making it an essential tool for developers.\n\n**Key Features:**\n- **No Account Needed:** You can edit .mmd files without signing up, allowing for quick updates.\n- **File Recognition and Syntax Highlighting:** Automatically recognizes .mmd files and highlights Mermaid.js syntax for ease of use.\n- **Real-Time Preview:** Edit and view diagrams in real time, with features for pan and zoom.\n- **Markdown Integration:** Detects Mermaid.js diagrams in Markdown files, enabling easy edits and visual integration.\n- **Collaboration Tools:** Advanced features are available with an account, including cloud syncing and sharing diagrams with non-developers.\n- **Flexible Editing Options:** Edit diagrams in VS Code or with a visual editor on the Mermaid Chart platform.\n- **AI Diagram Generation:** Use AI to create diagrams from code by linking files in GitHub Copilot Chat.\n\n**Use Cases:**\n- Visualize DevOps processes, API interactions, microservice architectures, and improve internal documentation.\n\n**Getting Started:**\n1. Install the plugin from the VS Code Marketplace.\n2. Open a .mmd or .md file and start editing with Mermaid.js syntax.\n3. Log in for advanced features.\n\n**Benefits Recap:**\n- Easy editing without an account\n- Syntax support and file recognition\n- Real-time diagram rendering\n- Markdown and cloud integration\n- Git-friendly version control\n\nThis plugin is ideal for developers looking to enhance their documentation and diagramming processes within their coding environment.",
      "ko": "Mermaid Chart VS Code 플러그인은 개발자들이 Visual Studio Code에서 직접 Mermaid.js를 사용하여 다이어그램을 만들고 편집할 수 있게 해줍니다. 이 플러그인은 소프트웨어 아키텍처, API 흐름 등을 시각화하는 과정을 간소화하여 개발자들에게 필수적인 도구가 됩니다.\n\n주요 기능으로는 계정 없이도 .mmd 파일을 편집할 수 있어 빠른 업데이트가 가능하다는 점이 있습니다. 또한, .mmd 파일을 자동으로 인식하고 Mermaid.js 문법을 강조 표시하여 사용이 편리합니다. 실시간 미리보기 기능을 통해 다이어그램을 편집하고 동시에 볼 수 있으며, 팬과 줌 기능도 지원합니다. Markdown 파일 내에서 Mermaid.js 다이어그램을 감지하여 쉽게 편집하고 시각적으로 통합할 수 있는 기능도 제공합니다. 계정을 만들면 클라우드 동기화와 비개발자와의 다이어그램 공유 등 고급 기능을 사용할 수 있습니다. VS Code에서 다이어그램을 편집하거나 Mermaid Chart 플랫폼의 시각적 편집기를 사용할 수 있는 유연한 편집 옵션도 제공합니다. AI를 활용해 GitHub Copilot Chat에서 파일을 연결하여 코드로부터 다이어그램을 생성하는 기능도 포함되어 있습니다.\n\n이 플러그인은 DevOps 프로세스, API 상호작용, 마이크로서비스 아키텍처를 시각화하고 내부 문서를 개선하는 데 유용합니다. 사용을 시작하려면 VS Code 마켓플레이스에서 플러그인을 설치하고 .mmd 또는 .md 파일을 열어 Mermaid.js 문법으로 편집을 시작하면 됩니다. 고급 기능을 사용하려면 로그인하면 됩니다.\n\n이 플러그인은 계정 없이도 쉽게 편집할 수 있고, 문법 지원 및 파일 인식 기능이 있으며, 실시간으로 다이어그램을 렌더링할 수 있습니다. Markdown과 클라우드 통합, Git 친화적인 버전 관리 기능도 제공하여 개발자들이 코딩 환경 내에서 문서화 및 다이어그램 작성 과정을 향상시키는 데 이상적입니다.",
      "ja": "Mermaid ChartのVS Codeプラグインは、開発者がVisual Studio Code内で直接Mermaid.jsを使って図を作成・編集できるツールです。これにより、ソフトウェアアーキテクチャやAPIの流れなどを視覚化するプロセスが簡素化され、開発者にとって欠かせないツールとなっています。\n\nこのプラグインの主な特徴には、アカウントなしで.mmdファイルを編集できることが挙げられます。これにより、迅速な更新が可能です。また、.mmdファイルを自動的に認識し、Mermaid.jsの構文をハイライトする機能も備えています。リアルタイムプレビュー機能により、図を編集しながら同時に表示でき、パンやズームの機能も利用できます。Markdownファイル内のMermaid.jsの図を検出し、簡単に編集や視覚的な統合ができるのも特徴です。\n\nさらに、アカウントを作成すると、クラウド同期や非開発者との図の共有などの高度なコラボレーション機能が利用できます。VS Code内で図を編集することも、Mermaid Chartプラットフォームのビジュアルエディタを使うこともでき、柔軟な編集オプションが提供されています。AIを活用して、GitHub Copilot Chatを通じてコードから図を生成することも可能です。\n\nこのプラグインは、DevOpsプロセス、APIの相互作用、マイクロサービスアーキテクチャの視覚化や、内部ドキュメントの改善に役立ちます。\n\n導入は簡単で、まずVS Code Marketplaceからプラグインをインストールします。次に、.mmdまたは.mdファイルを開き、Mermaid.jsの構文で編集を始めます。高度な機能を利用するにはログインが必要です。\n\nこのプラグインの利点は、アカウントなしでの簡単な編集、構文サポートとファイル認識、リアルタイムでの図のレンダリング、Markdownとの統合、Gitに優しいバージョン管理などです。開発者にとって、コーディング環境内でドキュメントや図を強化するための理想的なツールです。"
    }
  },
  {
    "id": "668cc1af6733b0da",
    "title": {
      "en": "Sailing from Berkeley to Hawaii in a 19ft Sailboat",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://potter-yachters.org/stories/teplow_to_hawaii.htm",
    "score": 39,
    "by": "protonbob",
    "time": 1743609142,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "f5a74d6fb5ada0ce",
    "title": {
      "en": "Porting Tailscale to Plan 9",
      "ko": "플랜9에 테일스케일!",
      "ja": "TailscaleをPlan 9へ"
    },
    "type": "story",
    "url": "https://tailscale.com/blog/plan9-port",
    "score": 247,
    "by": "adriangrigore",
    "time": 1743607754,
    "content": "{\"@context\":\"https://schema.org\",\"@type\":\"BlogPosting\",\"image\":\"https://cdn.sanity.io/images/w77i7m8x/production/a73e59254fdb3c63e33f70a58b9ff4b4ff8e0248-600x315.svg\",\"url\":\"https://tailscale.com/blog/plan9-port\",\"headline\":\"Porting Tailscale to Plan 9\",\"datePublished\":\"2025-04-02T14:30:45.896Z\",\"description\":\"A behind the scenes look at how we ported Tailscale to Plan 9, including a number of Plan 9 and Go fixes along the way.\",\"abstract\":\"A behind the scenes look at how we ported Tailscale to Plan 9, including a number of Plan 9 and Go fixes along the way.\",\"author\":[{\"@type\":\"Person\",\"name\":\"Brad Fitzpatrick\"}],\"articleBody\":\"It’s been said that nothing helps land a joke like explaining it, so here we are to explain yesterday’s [Tailscale Plan 9 announcement](https://tailscale.com/blog/tailscale-enterprise-plan-9-support), even at the risk of killing the joke.\\n\\nBut really, if we had to kill a joke by explaining it, there’s no better type of joke to kill than a corporate April Fools’ Day post. They’re admittedly pretty terrible in general. I’m of the opinion that if you’re going to do such a joke, you better put some effort into it; [it](https://groups.google.com/g/golang-dev/c/ZEntxvHLIt0/m/BdvtIdJNIOMJ) [should](https://go-review.googlesource.com/c/go/+/21400) [actually](https://go-review.googlesource.com/q/quaternions) [work](https://github.com/bradfitz/campher). (Otherwise it’s 100% sad instead of whatever percent sad yesterday’s post was.)\\n\\nAnd to be super clear today on April 2nd because nobody believes anything on April 1st: Tailscale now actually works on Plan 9\\\\. For reals.\\n\\nWe were amused to find everybody in shock that there was [a PR](https://github.com/tailscale/tailscale/pull/15491) attached to yesterday’s blog post so let’s dig into that PR a bit, and other work that went on.\\n\\nFirst off: I don’t really know Plan 9\\\\. I know *of* Plan 9, and I know people who know Plan 9, but I’m a Plan 9 newbie and I apologize in advance if I offend any Plan 9 people with my ignorance. I tried to check my cluelessness with others to make sure it’s not *too* stupid, but there are surely some inaccuracies in these posts and bugs and shortcuts and simplifying assumptions in the code.\\n\\nAnyway.\\n\\nAs the quip goes, “We chose to port Tailscale to Plan 9 not because it was easy, but because we thought it would be easy.” Naively it kinda seems like you’d take Tailscale’s two Go binaries and build them with `GOOS=plan9 GOARCH=386 go install ./cmd/tailscale{,d}` and call it a day. Sure, I expected some `syscall` or `x/net` or `x/sys/unix` symbols to not exists with `GOOS=plan9` and some `//go:build` tag adjustments and some special cases for `runtime.GOOS == \\\"plan9\\\"` to use different default disk paths like we previously [did for AIX](https://github.com/tailscale/tailscale/commit/a1abd12f351cfb625c5ac9bca243d0bc46dbdbfd), etc. So that’s [what I tried](https://github.com/tailscale/tailscale/pull/9082) in August 2023 when a local acquaintance here in Seattle [asked me for Plan 9 support](https://github.com/tailscale/tailscale/issues/5794#issuecomment-1690411613) and I finally caved and said okay (after initially rejecting the idea). I tweaked some build tags & paths & compiled it and …. [Boom](https://github.com/golang/go/issues/62507). The binary crashed at runtime in weird ways. Turns out the Go compiler support for Plan 9 had bitrot. Plan 9 wasn’t one of Go’s [first-class ports](https://go.dev/wiki/PortingPolicy) and nobody had noticed the regressions. Or maybe Tailscale just pushed Go a bit harder than it had been pushed on Plan 9 before.\\n\\nIn any case, the Tailscale Plan 9 effort stalled out through all of 2024, beyond my time and/or ability to fix.\\n\\nAt the beginning of March 2025, a coworker mentioned April Fools’ Day and I suddenly remembered our Plan 9 port.\\n\\nI reached out to [Russ Cox](https://swtch.com/~rsc/) (a former coworker from the Go team with a lot of Plan 9 history) and told him I thought it’d be fun (& funny) to finish up in time for April 1st. He replied:\\n\\n*“Sure I’m in.*  \\n*We should fix the plan 9 kernel to save those registers and then not have that special case anymore.”*\\n\\nIt’s possible that Russ didn’t know what he was signing himself up for.\\n\\n## SSE\\n\\nIn 1999, Intel introduced the Pentium III processor with [SSE instructions](https://en.wikipedia.org/wiki/Streaming_SIMD_Extensions). Yesterday’s blog post is dated 1999 because that’s kinda where this whole adventure begins.\\n\\nThe “special case” that Russ was referring to above is how the Go compiler [tried to avoid using SSE anywhere](https://github.com/golang/go/issues/62507#issuecomment-1710636507) for Plan 9 targets because the Plan 9 kernel doesn’t allow SSE in “note handlers” (think: signal handlers), as it didn’t save/restore them. And because the Go compiler didn’t know which code was being used during a note handler, it conservatively tried to disable SSE everywhere. But that code was regularly breaking and there were too many `plan9` [special](https://github.com/golang/go/blob/go1.24.0/src/cmd/compile/internal/ssa/config.go#L369-L379) [cases](https://github.com/golang/go/blob/go1.24.0/src/cmd/compile/internal/amd64/ggen.go#L67-L88) throughout the compiler.\\n\\nIdeally the Plan 9 kernel would just save/restore the SSE registers/context in note handlers, then Go could remove the Plan 9 special cases and treat it like every other operating system. So Russ did that.\\n\\nFor 386, the Plan 9 fix was [sys/src/9: allow floating point in note handlers](https://github.com/rsc/plan9/commit/3715bf9b86a86ed6a3a857cabfc7dff5d70b409b) (and updating the docs in [sys/man/2: update notify](https://github.com/rsc/plan9/commit/dd95b25897369ff2575b2ad744e18954c4620464)).\\n\\nFor amd64 (the 9k kernel), we ran into a [bunch more issues](https://9fans.topicbox.com/groups/9fans/Taf6b900592afc500/9k-amd64-kernel-and-floating-point). Russ fixed various things:\\n\\n* [sys/src/9k: fix bug aliasing parent and child FP state after fork](https://github.com/rsc/plan9/commit/3b001d133aa0e1661f64d2df0a683aa6d10bc955)  \\n* [sys/src/9k: allow floating point and simd in note handlers](https://github.com/rsc/plan9/commit/c30ca5483b9fb3a438510110580cacaddb88f8e9)  \\n* [sys/src/9k: fix noted(NCONT) losing registers](https://github.com/rsc/plan9/commit/aa00f938f6b3c6a5b4502c25605666f479a22c16)  \\n* [sys/src/libmach: fix default amd64 binary mapping](https://github.com/rsc/plan9/commit/04c7c708c2640586536bd31e01fbb8f05628bd71)  \\n* [sys/src/cmd/ktrace: fix for new k10 kernel trap routine](https://github.com/rsc/plan9/commit/aea4b577cd1b7f1402bdd880487cdf436b168b1e)\\n\\n… but by this time we’d both pretty much resigned to just focusing on making the demo work on `GOARCH=386`.\\n\\nWith the corresponding Go fix to [stop (incorrectly) special casing Plan 9’s code generation](https://go-review.googlesource.com/c/go/+/655875), `tailscaled` could now start up and run (for longer) without crashing. I could then start working on the bits that I could fix.\\n\\n## IPC\\n\\nNow it was crashing due to out of memory errors instead of stack corruption. It turns out an earlier attempt at porting Tailscale to Plan 9 had a bug resulting in launching an infinite number of goroutines. Fixing that bug in our `safesocket` IPC package to just boringly using localhost TCP fixed that and now `tailscaled` would run without crashing. I thought using localhost TCP wasn’t very Plan-9-everything-is-a-file-like but Russ pointed out that some other Plan 9 services do that, so I felt a bit better, at least to unblock forward progress.\\n\\nLater it’d be nicer to use the [srv9p package](https://pkg.go.dev/9fans.net/go/plan9/srv9p) that Russ [ported to Go](https://github.com/9fans/go/commit/3835d560e21f033c0c05c44e7e0f61f7ccfb9e21) and make the LocalAPI go over that. Or we should at least make localhost use authentication as we do on other platforms. I ran out of time, unfortunately. For now, don’t use this on shared Plan 9 machines.\\n\\n## Dev Environment\\n\\nUp to this point, I was running Plan 9 in a VM that I’d installed from a [9legacy CD image download](http://9legacy.org/download.html). Because I didn’t (and still don’t) know [the Acme editor](https://research.swtch.com/acme) too well, I was developing on my normal machines and cross-compiling the Plan 9 binaries, and then running `hget http://10.0.0.x:8080/tailscaled > tailscaled` and `chmod +x tailscaled` and `./tailscaled` on Plan 9 to pull the binaries over HTTP from my LAN. Because I wasn’t even using virtio for my disk or network, this process (just the copy over the LAN\\\\!) took multiple minutes per iteration. That’s long enough for me to get distracted and forget what I was working on and context switch to Slack or email or other projects.\\n\\nRuss, perhaps sensing my pain without me even whining about it, created [https://github.com/rsc/plan9](https://github.com/rsc/plan9). That’s a repo with not only the Plan 9 source code, but also pre-compiled binaries, and a `./boot/qemu` script that runs a diskless Plan 9 qemu VM that netboots with a [9P](https://en.wikipedia.org/wiki/9P_\\\\(protocol\\\\)) root filesystem over the network to a localhost 9P server that serves out of that git repo. That meant no more copying files around… my laptop’s filesystem and my Plan 9 filesystem were shared, the way Plan 9 is meant to be used. Also, as a bonus, qemu was wired up to use virtio, making it much faster.\\n\\nI now had a nice dev environment with iteration time in seconds instead of minutes.\\n\\n## TUN mode\\n\\nWhile Tailscale now ran and “worked” on Plan 9, we were only running Tailscale’s “[userspace networking](https://tailscale.com/kb/1112/userspace-networking)” mode that doesn’t involve the kernel’s networking stack and instead does all the TCP/UDP/ICMP/etc via [gVisor](https://github.com/google/gvisor)’s [netstack](https://gvisor.dev/docs/user_guide/networking/). That’s better than nothing, and where our AIX port is still at, but it’s not ideal— it means the only access from a Plan 9 machine back to your tailnet is via the tailscaled HTTP/SOCKS5 proxy, and you’d have to get all your programs to then use that proxy. But few to zero Plan 9 programs recognize and support an “HTTP\\\\_PROXY” or “ALL\\\\_PROXY” environment variable to support that. Maybe there’s a Plan 9 `/net` server that uses SOCKS5, but I didn’t look too hard.\\n\\nSo, how to get the kernel involved in the network path? On most Unix platforms you use [TUN](https://en.wikipedia.org/wiki/TUN/TAP) (or [wintun](https://www.wintun.net/) on Windows) which give you a virtual network device on which you set addresses and assign routes, handling the incoming and outgoing packets in userspace. The [Plan 9 equivalent is trivial](https://github.com/tailscale/wireguard-go/commit/91a0587fb251a72c28724ee111fe04cf1436ca4c): you open `/net/ipifc/clone`, read a decimal number back of the new interface you just created, write `\\\"bind pkt\\\\n\\\"` to the `ctl` control fd returned by opening `clone`, and then you have a new interface at e.g. `/net/ipifc/2/*` where you can then open `/net/ipifc/2/data` and read and write IP packets. (`/net/ipifc/0` and and `/1` are typically localhost and your normal physical LAN).\\n\\nWhen I sent this code to [@raggi](https://github.com/raggi) for review his reaction was basically, *“whoa, cute. no ioctls\\\\!”* But even more beautiful than no ioctls is that the reads and writes to the data file don’t even need extra framing to prepend the length. You just read and write the IP packets. It’s really the most simple “TUN” implementation we have for any platform.\\n\\n## Routing tables\\n\\nI could now get packets in to exactly my interface’s address, but not out to the peers in my tailnet.\\n\\nNow I’d need to implement Tailscale’s `router` interfaces.\\n\\nManipulating the routing tables on Plan 9 is just about as easy as making the interface. You open `/net/iproute`, write `\\\"tag tail\\\\n\\\"` to it to set the “tail” routing protocol tag on all future routes you add on that fd (to make it easy to clean up after ourselves, knowing what we added), and then write little messages like `\\\"add 100.64.0.0 /106 100.102.103.104\\\"` , giving it our own IP address as the nexthop value. The only surprise was that the CIDR length there (“/106”) is 106 and not the /10 you’d expect from the [CGNAT range](https://en.wikipedia.org/wiki/Carrier-grade_NAT)’s 10.64.0.0/10. It turns out (or seems like) Plan 9 internally is IPv6 primarily, and IPv4 is just a special case of that, so writing “100.64.0.0” is just a shorthand way of writing [IPv4-mapped IPv6 addresses](https://en.wikipedia.org/wiki/IPv6#IPv4-mapped_IPv6_addresses) like `::ffff:100.64.0.0`.\\n\\n## The missing three button mouse\\n\\nAt this point I took a little trip and forgot to pack my three-button USB mouse.\\n\\nAs mentioned yesterday, Plan 9 basically requires a three-button mouse to use. This makes development on a Mac laptop very difficult to the point of not being fun.\\n\\nRuss once again took pity on me and [modified Plan 9 to support holding down modifier keys](https://9fans.topicbox.com/groups/9fans/T492596e3a67612c6) while clicking to emulate button 2 and button 3\\\\.\\n\\n## Tailscale SSH\\n\\nIn a moment of overconfidence (or boredom waiting for my delayed flight home), I decided to tackle Tailscale SSH support. Tailscale SSH is tailscaled’s built-in SSH server that handles authentication by using your Tailscale identity as known by your WireGuard™ keys associated with all your packets.\\n\\nNaively, I tried just running the Plan 9 shell (`/bin/rc`) with `os/exec.Command` and wiring up stdin/stdout to it.\\n\\nThat “worked” but was kinda terrible— things didn’t echo or navigate correctly. You couldn’t interrupt processes, etc.\\n\\nRuss explained to me how to do it properly but he probably sensed how overwhelming it seemed so he went off and added a [“netshell” example](https://github.com/9fans/go/blob/main/plan9/srv9p/example/netshell/main.go) to the [9fans/go](https://github.com/9fans/go) repo. That “netshell” was basically the world’s most insecure telnet server, but it was all I needed to put behind Tailscale SSH instead of running `/bin/rc` directly.\\n\\nNow SSH worked. Conveniently, this also meant I could get text output out of Plan 9 more easily: I could `ssh glenda@plan9 cat /dev/snarf` from my laptop (the VM host) and get the copy/paste buffer from my Plan 9 guest VM. (`cat /dev/snarf` is like macOS `pbpaste`)  Of course, that’s primarily because I wasn’t thinking and hadn’t realized I had a shared filesystem and could’ve also just redirected `/dev/snarf` to a file and read that file from my laptop. Oh well.\\n\\nBut Tailscale SSH also made it easier for me to write Go tests on my laptop and then easily cross-compile and run them “remotely” over SSH.\\n\\n## Service collection\\n\\nOne thing that tailscaled can optionally do (disabled by default), is to look what services you’re running on your machine and report them to the control plane for discoverability, so you know for example that you’re running some dev service and its process name on port 8080\\\\.\\n\\nI was curious how to do that. Basically you just walk over `/proc/NNN/fd` (similar to Linux) and find process PIDs who have e.g. `/net/tcp/clone` open. You then look at their “QID” and line it up with `/net/tcp/NNN/{status,local}` to see if they’re listening and on what port. Overall it’s very similar to other platforms but not as beautiful as I would’ve hoped for. The fact that you have to do `tcpN := (qid >> 5) & (1<<12 - 1)` to map the FD’s QID (basically its inode number?) to a TCP number and hope the kernel implementation doesn’t change is a little sad. It would be better if we had changed Plan 9 to support that operation explicitly, but we ran out of time. Oh well.\\n\\n## MagicDNS\\n\\nNext up was making [MagicDNS](https://tailscale.com/kb/1081/magicdns) work. Ideally you could just refer to peers as “foo” from Plan 9 (e.g. `ip/ping foo`) or at least with its `foo.tailnet-name.ts.net` FQDN.\\n\\nThere’s a bunch of docs in [ndb(6)](https://9p.io/magic/man2html/6/ndb) and [ndb(8)](https://9p.io/magic/man2html/8/ndb) and [dial(2)](https://9p.io/magic/man2html/2/dial) elsewhere about the life of a name lookup on Plan 9 and which layers do what. The Go standard library code was also easy to read to see how it all worked, at least from the client side.\\n\\nWe debated just intercepting all the `/net/dns` or `/net/cs` queries and blending in Tailscale names, but in the end [Russ again patched Plan 9](https://9fans.topicbox.com/groups/9fans/T9c9d81b5801a0820) to permit specifying alternate DNS servers for specific DNS name suffixes, similar to what `systemd-networkd` [permits on Linux](https://tailscale.com/blog/sisyphean-dns-client-linux).\\n\\nFor extra fun, I kept randomly hitting a bug where DNS queries got incorrectly negatively cached even after I wrote “refresh” to `/net/dns`. Russ [fixed that too](https://github.com/rsc/plan9/commit/8cafd26a7c4ba3e34d7eb4c76bc854c1433bf03c).\\n\\n## Random time crashes\\n\\nSometimes I noticed `tailscaled` crashing from an assertion deep in gVisor’s netstack from it observing that its [monotonic time](https://pkg.go.dev/time#hdr-Monotonic_Clocks) had gone backwards. Monotonic time should never go backwards; that’s its one job. But it turns out Go’s time implementation on Plan 9 was just using the wall time as its monotonic time, and when ntpd adjusted the clock backwards, gVisor crashed.\\n\\nOnce again Russ jumped in and [added monotonic time to Plan 9’s /dev/bintime](https://9fans.topicbox.com/groups/9fans/T59810df4fe34a033/monotonic-time-and-randomness-on-plan-9) and [patched Go to use it](https://go-review.googlesource.com/c/go/+/656755).\\n\\n## Time to blog\\n\\nThe final boss was writing a blog post and figuring how to get access to the blog again. (Sorry, long time no see. I’ll try to write more often\\\\!)\\n\\nI thought it’d be fun to date the first blog post as “April 1, 1999” for nostalgic reasons. Also the world seemed happier back then.\\n\\nI pestered some former Go colleagues who’d worked at Bell Labs for quotes too. I’m thrilled they wanted to play along. I defensively assured them that Plan 9 was not the butt of the joke and the joke was …. ourselves perhaps? (I’m not exactly sure why we did this.)\\n\\n## Running on the web\\n\\nWith a few days remaining, we decided to tackle running Plan 9 on the web. A few weeks prior I had first looked at using [PCjs](https://www.pcjs.org/) and met up with its author for coffee & donuts in my neighborhood. Without networking support, though, the demo wasn’t as interesting. Adding [ne2000](https://en.wikipedia.org/wiki/NE1000) support might’ve been possible, but there wasn’t a lot of time. Jeff recommended looking at [copy/v86](https://github.com/copy/v86#readme). It runs 32-bit operating systems using WASM and includes [various forms of networking](https://github.com/copy/v86/blob/master/docs/networking.md) support. (another reason for us to focus on `GOARCH=386` and not `GOARCH=amd64`\\\\!)\\n\\nSo far I had been doing development using the [qemu-based shared filesystem environment](https://github.com/rsc/plan9) that Russ had prepared. But now we needed a disk image to boot from on the web.\\n\\nWhile Russ worked on dusting off & modernizing an old compressed root filesystem kernel he’d worked on 25 years ago, I worked at exploring networking options.\\n\\nOne of the networking options passes Layer 2 ethernet frames over websockets to a relay. I [added wsproxy protocol support](https://github.com/tailscale/tailscale/commit/2a12e634bfe7fc4f89fa8f37b1bd0ff9866e776b) to our network simulation environment used for integration tests. That environment fakes everything with the help of gVisor’s [netstack](https://gvisor.dev/docs/architecture_guide/networking/): ARP, DHCP, DNS w/ fake IPs, various flavors of NAT, optionally bridging controlplane & DERP servers to their real connections behind the scenes, etc. That ended up working, but it wasn’t ideal. The VM did DHCP before it brought up `rio` (the Plan 9 GUI) and DHCP took several round trips over the faked ethernet-over-websockets. If the relay was far away, the GUI would start slowly.\\n\\nSo then I instead implemented a “[WISP](https://github.com/MercuryWorkshop/wisp-protocol)” server instead. This made the GUI start up without any network roundtrips at all: the DHCP all happened faked inside the browser.\\n\\nI was in the middle of productionizing the WISP proxy when I ran out of time and decided to just launch with the [copy.sh/v86](http://copy.sh/v86) default network relay settings.\\n\\nI built Tailscale, added it to `/386/bin`, prepared a disk image (`cd /sys/lib/dist/mini; mk` in [rsc/plan9](https://github.com/rsc/plan9) after adding `tailscale` & `tailscaled` to the image’s `proto` template file), and then it spit out a 16MB disk image with all of Plan 9 and Tailscale, which itself is a 23MB binary after decompression. That’s why you’ll notice the “gunzip…” step when you boot the image, now included as an example image at [https://copy.sh/v86/?profile=9legacy](https://copy.sh/v86/?profile=9legacy) \\n\\nMaybe I’ll finish up the WISP backend later.\\n\\n## Future directions\\n\\nThere are two main forks of Plan 9: a very minimal one ([9legacy](http://9legacy.org/)) and a more modified one ([9front](https://9front.org/)). So far Tailscale has only been tested on 9legacy. Some of the patches that Russ wrote for 9legacy might still need to be ported to 9front.\\n\\nWe should also verify that 64-bit `GOARCH=amd64` support works. We’d mostly ignored that during development.\\n\\nI also didn’t implement exit node support or Go’s `net/netns` package support. Doing that might require rethinking how Tailscale presents itself on Plan 9, probably [as its own `/net`](https://9fans.topicbox.com/groups/9fans/T4cecdedbabdedc00/tailscale-on-plan-9).\\n\\nBut largely I’ll be relying on the Plan 9 community to take this over if they’d like to.\\n\\n## What was the point?\\n\\nI’m trying to remember now why we did all this. Mostly it was because [Skip](https://github.com/9nut) asked. Partly it was fun & educational, working in an alternate reality and learning new things. And working with a totally busted `tailscaled` crashing and deadlocking in weird ways always seems to lead to finding existing problems that affect all other platforms or assumptions that aren’t true in general.\\n\\nAnd Go’s support for Plan 9 got better:\\n\\n* [cmd/compile: use FMA on plan9, and drop UseFMA](https://go-review.googlesource.com/c/go/+/655877)  \\n* [runtime: remove nextSampleNoFP from plan9](https://go-review.googlesource.com/c/go/+/655879)  \\n* [cmd/compile, runtime: remove plan9 special case avoiding SSE](https://go-review.googlesource.com/c/go/+/655875)  \\n* [net: fix parsing of interfaces on plan9 without associated devices](https://go-review.googlesource.com/c/go/+/654055)  \\n* [os: guarantee min buffer size for ReadFile reads on /proc-like files](https://go-review.googlesource.com/c/go/+/654315)  \\n* [net: unblock UDP Reads upon Close on plan9, add test](https://go-review.googlesource.com/c/go/+/656395)  \\n* [runtime: fix plan9 monotonic time, crypto randomness](https://go-review.googlesource.com/c/go/+/656755)\\n\\nIn particular, removing the `plan9` special cases from the Go compiler makes the Go compiler cleaner and easier to hack on, so that’s nice.\\n\\n## Final surprise\\n\\nWhen we launched the blog post on Tuesday, we discovered that the v86 author had launched his own April Fools’ Day joke— everything on v86, including the VGA text output from emulated VMs, was all in fake Dutch or something.\\n\\nI panicked a little, as it made our demo more confusing (“Plun 9”, “goonzeep…” instead of “gunzip…”, etc), and I’d just run out of time trying to finish hosting our own HTML page with v86 Javascript & WASM embedded, but fortunately the author pointed out that the page took a `&nojoke` query argument. \\n\\n## Thanks\\n\\nThanks to everybody who made this possible:\\n\\n* [Skip Tavakkolian](https://github.com/9nut) for the nerdsnipe  \\n* [Jeff Parsons](https://github.com/jeffpar) for talking me through [PCjs](https://www.pcjs.org/) and web-based 32-bit emulation  \\n* [Fabian](https://github.com/copy/) for [v86](https://github.com/copy/v86). I have so many new non-April Fools ideas to do with v86 now. Stay tuned.  \\n* [David du Colombier](https://github.com/0intro/) for all the plan9 Go maintenance & reviews over the years and hosting the [9p.io](https://9p.io/plan9) docs I relied on constantly  \\n* [Rob Pike](https://en.wikipedia.org/wiki/Rob_Pike) and [Peter J. Weinberger](https://en.wikipedia.org/wiki/Peter_J._Weinberger) and [Charlotte Brandhorst-Satzkorn](https://bsky.app/profile/catzkorn.dev) for playing along with the quotes  \\n* [Russ Cox](https://swtch.com/~rsc/) for doing all the hard work fixing up stuff in Plan 9 and Go’s Plan 9 support and telling me how to use Plan 9; I stalled out doing this joke for over a year. Russ made this possible.\\n\\n## Questions?\\n\\nIf you miss our [Plan 9 Google Meet GChat Hangout](https://ftp.plan9.ts.net/webinar), we’ll also be answering any questions we see pop up on Hacker News, [Reddit](https://www.reddit.com/r/Tailscale/comments/1jprsqo/porting_tailscale_to_plan_9/), or Bluesky.\\n\\n## In conclusion\\n\\nMaybe I’ll skip April Fools’ Day next year, like I skipped [Advent of Code](https://adventofcode.com/) this past year.\\n\\nAnd if you actually wanted to pay us dumptruck loads of money for Plan 9 support, please don’t— dumptrucks will dirty the cash. Please wire it instead.\\n\\n\"}Blog|insights4월 02, 2025Porting Tailscale to Plan 9It’s been said that nothing helps land a joke like explaining it, so here we are to explain yesterday’s Tailscale Plan 9 announcement, even at the risk of killing the joke.\nBut really, if we had to kill a joke by explaining it, there’s no better type of joke to kill than a corporate April Fools’ Day post. They’re admittedly pretty terrible in general. I’m of the opinion that if you’re going to do such a joke, you better put some effort into it; it should actually work. (Otherwise it’s 100% sad instead of whatever percent sad yesterday’s post was.)\nAnd to be super clear today on April 2nd because nobody believes anything on April 1st: Tailscale now actually works on Plan 9. For reals.\nWe were amused to find everybody in shock that there was a PR attached to yesterday’s blog post so let’s dig into that PR a bit, and other work that went on.\nFirst off: I don’t really know Plan 9. I know of Plan 9, and I know people who know Plan 9, but I’m a Plan 9 newbie and I apologize in advance if I offend any Plan 9 people with my ignorance. I tried to check my cluelessness with others to make sure it’s not too stupid, but there are surely some inaccuracies in these posts and bugs and shortcuts and simplifying assumptions in the code.\nAnyway.\nAs the quip goes, “We chose to port Tailscale to Plan 9 not because it was easy, but because we thought it would be easy.” Naively it kinda seems like you’d take Tailscale’s two Go binaries and build them with GOOS=plan9 GOARCH=386 go install ./cmd/tailscale{,d} and call it a day. Sure, I expected some syscall or x/net or x/sys/unix symbols to not exists with GOOS=plan9 and some //go:build tag adjustments and some special cases for runtime.GOOS == \"plan9\" to use different default disk paths like we previously did for AIX, etc. So that’s what I tried in August 2023 when a local acquaintance here in Seattle asked me for Plan 9 support and I finally caved and said okay (after initially rejecting the idea). I tweaked some build tags & paths & compiled it and …. Boom. The binary crashed at runtime in weird ways. Turns out the Go compiler support for Plan 9 had bitrot. Plan 9 wasn’t one of Go’s first-class ports and nobody had noticed the regressions. Or maybe Tailscale just pushed Go a bit harder than it had been pushed on Plan 9 before.\nIn any case, the Tailscale Plan 9 effort stalled out through all of 2024, beyond my time and/or ability to fix.\nAt the beginning of March 2025, a coworker mentioned April Fools’ Day and I suddenly remembered our Plan 9 port.\nI reached out to Russ Cox (a former coworker from the Go team with a lot of Plan 9 history) and told him I thought it’d be fun (& funny) to finish up in time for April 1st. He replied:\n“Sure I’m in.\nWe should fix the plan 9 kernel to save those registers and then not have that special case anymore.”\nIt’s possible that Russ didn’t know what he was signing himself up for.\nSSE\nIn 1999, Intel introduced the Pentium III processor with SSE instructions. Yesterday’s blog post is dated 1999 because that’s kinda where this whole adventure begins.\nThe “special case” that Russ was referring to above is how the Go compiler tried to avoid using SSE anywhere for Plan 9 targets because the Plan 9 kernel doesn’t allow SSE in “note handlers” (think: signal handlers), as it didn’t save/restore them. And because the Go compiler didn’t know which code was being used during a note handler, it conservatively tried to disable SSE everywhere. But that code was regularly breaking and there were too many plan9 special cases throughout the compiler.\nIdeally the Plan 9 kernel would just save/restore the SSE registers/context in note handlers, then Go could remove the Plan 9 special cases and treat it like every other operating system. So Russ did that.\nFor 386, the Plan 9 fix was sys/src/9: allow floating point in note handlers (and updating the docs in sys/man/2: update notify).\nFor amd64 (the 9k kernel), we ran into a bunch more issues. Russ fixed various things:\n\nsys/src/9k: fix bug aliasing parent and child FP state after fork\nsys/src/9k: allow floating point and simd in note handlers\nsys/src/9k: fix noted(NCONT) losing registers\nsys/src/libmach: fix default amd64 binary mapping\nsys/src/cmd/ktrace: fix for new k10 kernel trap routine\n\n… but by this time we’d both pretty much resigned to just focusing on making the demo work on GOARCH=386.\nWith the corresponding Go fix to stop (incorrectly) special casing Plan 9’s code generation, tailscaled could now start up and run (for longer) without crashing. I could then start working on the bits that I could fix.\nIPC\nNow it was crashing due to out of memory errors instead of stack corruption. It turns out an earlier attempt at porting Tailscale to Plan 9 had a bug resulting in launching an infinite number of goroutines. Fixing that bug in our safesocket IPC package to just boringly using localhost TCP fixed that and now tailscaled would run without crashing. I thought using localhost TCP wasn’t very Plan-9-everything-is-a-file-like but Russ pointed out that some other Plan 9 services do that, so I felt a bit better, at least to unblock forward progress.\nLater it’d be nicer to use the srv9p package that Russ ported to Go and make the LocalAPI go over that. Or we should at least make localhost use authentication as we do on other platforms. I ran out of time, unfortunately. For now, don’t use this on shared Plan 9 machines.\nDev Environment\nUp to this point, I was running Plan 9 in a VM that I’d installed from a 9legacy CD image download. Because I didn’t (and still don’t) know the Acme editor too well, I was developing on my normal machines and cross-compiling the Plan 9 binaries, and then running hget http://10.0.0.x:8080/tailscaled > tailscaled and chmod +x tailscaled and ./tailscaled on Plan 9 to pull the binaries over HTTP from my LAN. Because I wasn’t even using virtio for my disk or network, this process (just the copy over the LAN!) took multiple minutes per iteration. That’s long enough for me to get distracted and forget what I was working on and context switch to Slack or email or other projects.\nRuss, perhaps sensing my pain without me even whining about it, created https://github.com/rsc/plan9. That’s a repo with not only the Plan 9 source code, but also pre-compiled binaries, and a ./boot/qemu script that runs a diskless Plan 9 qemu VM that netboots with a 9P root filesystem over the network to a localhost 9P server that serves out of that git repo. That meant no more copying files around… my laptop’s filesystem and my Plan 9 filesystem were shared, the way Plan 9 is meant to be used. Also, as a bonus, qemu was wired up to use virtio, making it much faster.\nI now had a nice dev environment with iteration time in seconds instead of minutes.\nTUN mode\nWhile Tailscale now ran and “worked” on Plan 9, we were only running Tailscale’s “userspace networking” mode that doesn’t involve the kernel’s networking stack and instead does all the TCP/UDP/ICMP/etc via gVisor’s netstack. That’s better than nothing, and where our AIX port is still at, but it’s not ideal— it means the only access from a Plan 9 machine back to your tailnet is via the tailscaled HTTP/SOCKS5 proxy, and you’d have to get all your programs to then use that proxy. But few to zero Plan 9 programs recognize and support an “HTTP_PROXY” or “ALL_PROXY” environment variable to support that. Maybe there’s a Plan 9 /net server that uses SOCKS5, but I didn’t look too hard.\nSo, how to get the kernel involved in the network path? On most Unix platforms you use TUN (or wintun on Windows) which give you a virtual network device on which you set addresses and assign routes, handling the incoming and outgoing packets in userspace. The Plan 9 equivalent is trivial: you open /net/ipifc/clone, read a decimal number back of the new interface you just created, write \"bind pkt\\n\" to the ctl control fd returned by opening clone, and then you have a new interface at e.g. /net/ipifc/2/* where you can then open /net/ipifc/2/data and read and write IP packets. (/net/ipifc/0 and and /1 are typically localhost and your normal physical LAN).\nWhen I sent this code to @raggi for review his reaction was basically, “whoa, cute. no ioctls!” But even more beautiful than no ioctls is that the reads and writes to the data file don’t even need extra framing to prepend the length. You just read and write the IP packets. It’s really the most simple “TUN” implementation we have for any platform.\nRouting tables\nI could now get packets in to exactly my interface’s address, but not out to the peers in my tailnet.\nNow I’d need to implement Tailscale’s router interfaces.\nManipulating the routing tables on Plan 9 is just about as easy as making the interface. You open /net/iproute, write \"tag tail\\n\" to it to set the “tail” routing protocol tag on all future routes you add on that fd (to make it easy to clean up after ourselves, knowing what we added), and then write little messages like \"add 100.64.0.0 /106 100.102.103.104\" , giving it our own IP address as the nexthop value. The only surprise was that the CIDR length there (“/106”) is 106 and not the /10 you’d expect from the CGNAT range’s 10.64.0.0/10. It turns out (or seems like) Plan 9 internally is IPv6 primarily, and IPv4 is just a special case of that, so writing “100.64.0.0” is just a shorthand way of writing IPv4-mapped IPv6 addresses like ::ffff:100.64.0.0.\nThe missing three button mouse\nAt this point I took a little trip and forgot to pack my three-button USB mouse.\nAs mentioned yesterday, Plan 9 basically requires a three-button mouse to use. This makes development on a Mac laptop very difficult to the point of not being fun.\nRuss once again took pity on me and modified Plan 9 to support holding down modifier keys while clicking to emulate button 2 and button 3.\nTailscale SSH\nIn a moment of overconfidence (or boredom waiting for my delayed flight home), I decided to tackle Tailscale SSH support. Tailscale SSH is tailscaled’s built-in SSH server that handles authentication by using your Tailscale identity as known by your WireGuard™ keys associated with all your packets.\nNaively, I tried just running the Plan 9 shell (/bin/rc) with os/exec.Command and wiring up stdin/stdout to it.\nThat “worked” but was kinda terrible— things didn’t echo or navigate correctly. You couldn’t interrupt processes, etc.\nRuss explained to me how to do it properly but he probably sensed how overwhelming it seemed so he went off and added a “netshell” example to the 9fans/go repo. That “netshell” was basically the world’s most insecure telnet server, but it was all I needed to put behind Tailscale SSH instead of running /bin/rc directly.\nNow SSH worked. Conveniently, this also meant I could get text output out of Plan 9 more easily: I could ssh glenda@plan9 cat /dev/snarf from my laptop (the VM host) and get the copy/paste buffer from my Plan 9 guest VM. (cat /dev/snarf is like macOS pbpaste)  Of course, that’s primarily because I wasn’t thinking and hadn’t realized I had a shared filesystem and could’ve also just redirected /dev/snarf to a file and read that file from my laptop. Oh well.\nBut Tailscale SSH also made it easier for me to write Go tests on my laptop and then easily cross-compile and run them “remotely” over SSH.\nService collection\nOne thing that tailscaled can optionally do (disabled by default), is to look what services you’re running on your machine and report them to the control plane for discoverability, so you know for example that you’re running some dev service and its process name on port 8080.\nI was curious how to do that. Basically you just walk over /proc/NNN/fd (similar to Linux) and find process PIDs who have e.g. /net/tcp/clone open. You then look at their “QID” and line it up with /net/tcp/NNN/{status,local} to see if they’re listening and on what port. Overall it’s very similar to other platforms but not as beautiful as I would’ve hoped for. The fact that you have to do tcpN := (qid >> 5) & (1<<12 - 1) to map the FD’s QID (basically its inode number?) to a TCP number and hope the kernel implementation doesn’t change is a little sad. It would be better if we had changed Plan 9 to support that operation explicitly, but we ran out of time. Oh well.\nMagicDNS\nNext up was making MagicDNS work. Ideally you could just refer to peers as “foo” from Plan 9 (e.g. ip/ping foo) or at least with its foo.tailnet-name.ts.net FQDN.\nThere’s a bunch of docs in ndb(6) and ndb(8) and dial(2) elsewhere about the life of a name lookup on Plan 9 and which layers do what. The Go standard library code was also easy to read to see how it all worked, at least from the client side.\nWe debated just intercepting all the /net/dns or /net/cs queries and blending in Tailscale names, but in the end Russ again patched Plan 9 to permit specifying alternate DNS servers for specific DNS name suffixes, similar to what systemd-networkd permits on Linux.\nFor extra fun, I kept randomly hitting a bug where DNS queries got incorrectly negatively cached even after I wrote “refresh” to /net/dns. Russ fixed that too.\nRandom time crashes\nSometimes I noticed tailscaled crashing from an assertion deep in gVisor’s netstack from it observing that its monotonic time had gone backwards. Monotonic time should never go backwards; that’s its one job. But it turns out Go’s time implementation on Plan 9 was just using the wall time as its monotonic time, and when ntpd adjusted the clock backwards, gVisor crashed.\nOnce again Russ jumped in and added monotonic time to Plan 9’s /dev/bintime and patched Go to use it.\nTime to blog\nThe final boss was writing a blog post and figuring how to get access to the blog again. (Sorry, long time no see. I’ll try to write more often!)\nI thought it’d be fun to date the first blog post as “April 1, 1999” for nostalgic reasons. Also the world seemed happier back then.\nI pestered some former Go colleagues who’d worked at Bell Labs for quotes too. I’m thrilled they wanted to play along. I defensively assured them that Plan 9 was not the butt of the joke and the joke was …. ourselves perhaps? (I’m not exactly sure why we did this.)\nRunning on the web\nWith a few days remaining, we decided to tackle running Plan 9 on the web. A few weeks prior I had first looked at using PCjs and met up with its author for coffee & donuts in my neighborhood. Without networking support, though, the demo wasn’t as interesting. Adding ne2000 support might’ve been possible, but there wasn’t a lot of time. Jeff recommended looking at copy/v86. It runs 32-bit operating systems using WASM and includes various forms of networking support. (another reason for us to focus on GOARCH=386 and not GOARCH=amd64!)\nSo far I had been doing development using the qemu-based shared filesystem environment that Russ had prepared. But now we needed a disk image to boot from on the web.\nWhile Russ worked on dusting off & modernizing an old compressed root filesystem kernel he’d worked on 25 years ago, I worked at exploring networking options.\nOne of the networking options passes Layer 2 ethernet frames over websockets to a relay. I added wsproxy protocol support to our network simulation environment used for integration tests. That environment fakes everything with the help of gVisor’s netstack: ARP, DHCP, DNS w/ fake IPs, various flavors of NAT, optionally bridging controlplane & DERP servers to their real connections behind the scenes, etc. That ended up working, but it wasn’t ideal. The VM did DHCP before it brought up rio (the Plan 9 GUI) and DHCP took several round trips over the faked ethernet-over-websockets. If the relay was far away, the GUI would start slowly.\nSo then I instead implemented a “WISP” server instead. This made the GUI start up without any network roundtrips at all: the DHCP all happened faked inside the browser.\nI was in the middle of productionizing the WISP proxy when I ran out of time and decided to just launch with the copy.sh/v86 default network relay settings.\nI built Tailscale, added it to /386/bin, prepared a disk image (cd /sys/lib/dist/mini; mk in rsc/plan9 after adding tailscale & tailscaled to the image’s proto template file), and then it spit out a 16MB disk image with all of Plan 9 and Tailscale, which itself is a 23MB binary after decompression. That’s why you’ll notice the “gunzip…” step when you boot the image, now included as an example image at https://copy.sh/v86/?profile=9legacy\nMaybe I’ll finish up the WISP backend later.\nFuture directions\nThere are two main forks of Plan 9: a very minimal one (9legacy) and a more modified one (9front). So far Tailscale has only been tested on 9legacy. Some of the patches that Russ wrote for 9legacy might still need to be ported to 9front.\nWe should also verify that 64-bit GOARCH=amd64 support works. We’d mostly ignored that during development.\nI also didn’t implement exit node support or Go’s net/netns package support. Doing that might require rethinking how Tailscale presents itself on Plan 9, probably as its own /net.\nBut largely I’ll be relying on the Plan 9 community to take this over if they’d like to.\nWhat was the point?\nI’m trying to remember now why we did all this. Mostly it was because Skip asked. Partly it was fun & educational, working in an alternate reality and learning new things. And working with a totally busted tailscaled crashing and deadlocking in weird ways always seems to lead to finding existing problems that affect all other platforms or assumptions that aren’t true in general.\nAnd Go’s support for Plan 9 got better:\n\ncmd/compile: use FMA on plan9, and drop UseFMA\nruntime: remove nextSampleNoFP from plan9\ncmd/compile, runtime: remove plan9 special case avoiding SSE\nnet: fix parsing of interfaces on plan9 without associated devices\nos: guarantee min buffer size for ReadFile reads on /proc-like files\nnet: unblock UDP Reads upon Close on plan9, add test\nruntime: fix plan9 monotonic time, crypto randomness\n\nIn particular, removing the plan9 special cases from the Go compiler makes the Go compiler cleaner and easier to hack on, so that’s nice.\nFinal surprise\nWhen we launched the blog post on Tuesday, we discovered that the v86 author had launched his own April Fools’ Day joke— everything on v86, including the VGA text output from emulated VMs, was all in fake Dutch or something.\nI panicked a little, as it made our demo more confusing (“Plun 9”, “goonzeep…” instead of “gunzip…”, etc), and I’d just run out of time trying to finish hosting our own HTML page with v86 Javascript & WASM embedded, but fortunately the author pointed out that the page took a &nojoke query argument.\nThanks\nThanks to everybody who made this possible:\n\nSkip Tavakkolian for the nerdsnipe\nJeff Parsons for talking me through PCjs and web-based 32-bit emulation\nFabian for v86. I have so many new non-April Fools ideas to do with v86 now. Stay tuned.\nDavid du Colombier for all the plan9 Go maintenance & reviews over the years and hosting the 9p.io docs I relied on constantly\nRob Pike and Peter J. Weinberger and Charlotte Brandhorst-Satzkorn for playing along with the quotes\nRuss Cox for doing all the hard work fixing up stuff in Plan 9 and Go’s Plan 9 support and telling me how to use Plan 9; I stalled out doing this joke for over a year. Russ made this possible.\n\nQuestions?\nIf you miss our Plan 9 Google Meet GChat Hangout, we’ll also be answering any questions we see pop up on Hacker News, Reddit, or Bluesky.\nIn conclusion\nMaybe I’ll skip April Fools’ Day next year, like I skipped Advent of Code this past year.\nAnd if you actually wanted to pay us dumptruck loads of money for Plan 9 support, please don’t— dumptrucks will dirty the cash. Please wire it instead.ShareAuthorBrad FitzpatrickAuthorBrad FitzpatrickShareLoading...\n\nTry Tailscale for  freeGet startedSchedule a demoContact sales",
    "summary": {
      "en": "The blog post discusses the process of porting Tailscale to the Plan 9 operating system, sharing insights and challenges encountered along the way. Initially framed as an April Fools' joke, the project became a genuine endeavor to make Tailscale functional on Plan 9.\n\nKey points include:\n\n1. **Background**: The author, Brad Fitzpatrick, humorously reflects on the difficulty and absurdity of corporate April Fools' jokes, emphasizing that Tailscale now truly works on Plan 9.\n\n2. **Challenges**: The porting process faced various technical hurdles, notably due to outdated Go compiler support for Plan 9, which led to runtime issues. \n\n3. **Collaboration**: Fitzpatrick teamed up with Russ Cox, who helped resolve several Plan 9 kernel issues, enabling better compatibility with Go.\n\n4. **Development Environment**: A new development setup was created using a shared filesystem, significantly speeding up the testing process.\n\n5. **Networking**: The port initially used a userspace networking mode, which was not ideal. Efforts were made to integrate Tailscale with Plan 9's kernel networking stack.\n\n6. **Future Directions**: The project remains open for community contributions, with plans to enhance support for the 64-bit architecture and other features.\n\n7. **Conclusion**: The effort was both a fun experiment and a learning experience, ultimately improving Go's support for Plan 9. The author expresses gratitude to those who contributed and hints at future improvements. \n\nOverall, the post illustrates the complexities and humor involved in software development, especially when dealing with niche systems like Plan 9.",
      "ko": "이 블로그 글은 Tailscale을 Plan 9 운영 체제로 포팅하는 과정과 그 과정에서 겪은 통찰과 도전 과제를 공유합니다. 처음에는 만우절 농담으로 시작했지만, 이 프로젝트는 Tailscale이 Plan 9에서 실제로 작동하도록 만드는 진지한 노력으로 발전했습니다.\n\n저자 브래드 피츠패트릭은 기업의 만우절 농담이 얼마나 어려운지와 어리석은지를 유머러스하게 반영하며, 이제 Tailscale이 실제로 Plan 9에서 작동한다고 강조합니다. 포팅 과정에서는 여러 기술적 장애물에 직면했으며, 특히 구식 Go 컴파일러가 Plan 9를 지원하지 않아 런타임 문제를 일으켰습니다.\n\n피츠패트릭은 러스 콕스와 협력하여 여러 Plan 9 커널 문제를 해결했고, 이를 통해 Go와의 호환성을 개선했습니다. 새로운 개발 환경은 공유 파일 시스템을 사용하여 구축되었고, 이로 인해 테스트 과정이 크게 빨라졌습니다.\n\n초기 포팅에서는 사용자 공간 네트워킹 모드를 사용했지만, 이는 최적의 방법이 아니었습니다. Tailscale을 Plan 9의 커널 네트워킹 스택과 통합하기 위한 노력이 진행되었습니다. 이 프로젝트는 커뮤니티의 기여를 열어두고 있으며, 64비트 아키텍처와 기타 기능에 대한 지원을 강화할 계획이 있습니다.\n\n이 노력은 재미있는 실험이자 학습 경험이었으며, 궁극적으로 Go의 Plan 9 지원을 개선하는 데 기여했습니다. 저자는 기여한 이들에게 감사의 뜻을 전하며, 향후 개선 사항에 대한 기대감을 나타냅니다. 전반적으로 이 글은 소프트웨어 개발의 복잡성과 유머를 잘 보여주며, 특히 Plan 9와 같은 특수 시스템을 다룰 때의 어려움을 강조합니다.",
      "ja": "このブログ記事では、TailscaleをPlan 9オペレーティングシステムに移植するプロセスについて、遭遇した洞察や課題を共有しています。最初はエイプリルフールのジョークとして始まりましたが、プロジェクトはTailscaleをPlan 9で機能させる本格的な取り組みへと発展しました。\n\n著者のブラッド・フィッツパトリックは、企業のエイプリルフールのジョークの難しさや不条理さをユーモラスに振り返り、Tailscaleが実際にPlan 9で動作するようになったことを強調しています。\n\n移植プロセスでは、さまざまな技術的な課題に直面しました。特に、Plan 9に対する古いGoコンパイラのサポートが原因で、実行時に問題が発生しました。\n\nフィッツパトリックはラッス・コックスと協力し、Plan 9のカーネルに関するいくつかの問題を解決しました。これにより、Goとの互換性が向上しました。\n\n新しい開発環境は、共有ファイルシステムを利用して構築され、テストプロセスが大幅に迅速化されました。\n\n移植の初期段階では、ユーザースペースのネットワーキングモードが使用されていましたが、これは理想的ではありませんでした。TailscaleをPlan 9のカーネルネットワーキングスタックと統合する努力が行われました。\n\nプロジェクトはコミュニティからの貢献を受け入れており、64ビットアーキテクチャやその他の機能のサポートを強化する計画があります。\n\nこの取り組みは楽しい実験であり、学びの経験でもありました。最終的にはGoのPlan 9に対するサポートが向上しました。著者は貢献してくれた人々に感謝の意を表し、今後の改善についても言及しています。\n\n全体として、この記事はソフトウェア開発の複雑さやユーモアを示しており、特にPlan 9のようなニッチなシステムに取り組む際の様子を描写しています。"
    }
  },
  {
    "id": "99b2f7a4f7f04982",
    "title": {
      "en": "Show HN: A Chrome extension to give you back control over short-form videos",
      "ko": "쇼 HN: 짧은 영상 제어 확장기능",
      "ja": "短尺動画を制御するChrome拡張"
    },
    "type": "story",
    "url": "https://chromewebstore.google.com/detail/seek-anywhere/opofkjlejjcjalcpaimnpmkmjlclgded",
    "score": 33,
    "by": "darajava",
    "time": 1743508268,
    "content": "ReelControl5.0(3 ratings)ShareExtensionFunctionality & UI6 usersAdd to Chrome\n\nReelControl5.0(3 ratings)ShareExtensionFunctionality & UI6 usersAdd to Chrome\n\nOverviewAdd a progress bar and playback controls to YouTube Shorts, Instagram, and Facebook Reels.Take back control of videos on the web by reversing the (very) annoying trend of social media platforms removing the native progress bar in an effort to keep you hooked. ReelControl adds a progress bar and playback controls anywhere they should rightfully be! That is, on Instagram, YouTube Shorts, or Facebook Reels.\n\nAdding progress bars and video controls back in lets you:\n\n- Know the time commitment before watching\n- Rewind when you miss something\n- Skip ahead and go back without having to start over\n\nI initially built this for myself and found that not only is it a little bit more enjoyable if I do ever end up on those videos, but I notice myself spending a lot less time on them without having to explicitly block them. I spend less time on my phone too, since I am now used to having that progress bar and just get frustrated and leave immediately when I remember it's not there.\n\n== Platform Notes ==\n\nInstagram\n\n- Instagram has a pretty clean video interface, so we only add the native video controls.\n\nYouTube Shorts\n\n- YouTube does have its own progress bar, but it's proprietary and hidden from immediate view, making it impossible to quickly see the video's length as opposed to the native controls which work _for_ the user.\n\n- YouTube Shorts interface is crazy cluttered, so we have some options to remove most elements from view.\n\nFacebook Reels\n\n- Facebook Reels have a crazy amount of clutter, and also no progress bar or video controls.\n\n- Facebook's HTML structure is extremely obfuscated, so we just removed all the clutter and added a progress bar. If anyone wants to add more fine-grained control, PRs are most welcome here!\n\n--\n\nOpen source, PRs and issues are welcome @ https://github.com/darajava/reel-control/\n\n--\n\nBuilt with love by Soliloquy Apps ❤️\n\nLike this extension? We also built AudioDiary--a super smart voice-powered journal that's gotten lots of love from its users.\n\nTry it out at https://audiodiary.ai !\n\n5 out of 53 ratingsGoogle doesn't verify reviews. Learn more about results and reviews.See all reviews\n\nDetailsVersion1.3UpdatedApril 2, 2025Flag concernOffered bydarajavaSize162KiBLanguagesEnglish (United States)Developer Email darajavaherian@gmail.comNon-traderThis developer has not identified itself as a trader. For consumers in the European Union, please note that consumer rights do not apply to contracts between you and this developer.\n\nPrivacyThe developer has disclosed that it will not collect or use your data.This developer declares that your data isNot being sold to third parties, outside of the approved use casesNot being used or transferred for purposes that are unrelated to the item's core functionalityNot being used or transferred to determine creditworthiness or for lending purposes\n\nSupportVisit support hub\n\nRelatedTab Manager by Workona4.7(3.6K)Average rating 4.7 out of 5 stars. 3.6K ratings.Google doesn't verify reviews. Learn more about results and reviews.The world’s best tab managerJust Read4.7(887)Average rating 4.7 out of 5 stars. 887 ratings.Google doesn't verify reviews. Learn more about results and reviews.A feature-packed, customizable reader extension.New Tab for Google Workspace™4.4(451)Average rating 4.4 out of 5 stars. 451 ratings.Google doesn't verify reviews. Learn more about results and reviews.Replace new tab page with a personal dashboard featuring, fast access to installed applications, bookmarks, history.Tab Manager Plus for Chrome4.7(1K)Average rating 4.7 out of 5 stars. 1K ratings.Google doesn't verify reviews. Learn more about results and reviews.Quickly find open tabs, see all windows in one view, find duplicates and limit tabs per window. The best Tab Manager for Chrome.SponsorBlock for YouTube - Skip Sponsorships4.7(2.8K)Average rating 4.7 out of 5 stars. 2.8K ratings.Google doesn't verify reviews. Learn more about results and reviews.Skip sponsorships, subscription begging and more on YouTube videos. Report sponsors on videos you watch to save others' time.Tab Manager for Chrome™4.3(78)Average rating 4.3 out of 5 stars. 78 ratings.Google doesn't verify reviews. Learn more about results and reviews.Quick access to all opened tabs. Manage browser tabs to easily navigate and switch between them.Voice Control for ChatGPT4.0(656)Average rating 4.0 out of 5 stars. 656 ratings.Google doesn't verify reviews. Learn more about results and reviews.Expands ChatGPT with voice control and read aloud.Video Downloader professional3.7(385)Average rating 3.7 out of 5 stars. 385 ratings.Google doesn't verify reviews. Learn more about results and reviews.Download online videos in various formats from any websites. Video Downloader save video and watch it later.Accept all cookies4.5(311)Average rating 4.5 out of 5 stars. 311 ratings.Google doesn't verify reviews. Learn more about results and reviews.Accepts necessary cookie policies for smooth website navigation, without deleting cookies.CryptoTab START4.7(15.3K)Average rating 4.7 out of 5 stars. 15.3K ratings.Google doesn't verify reviews. Learn more about results and reviews.New tab page extension for your browser with bunch of beautiful backgrounds and useful widgets.Material You NewTab4.3(273)Average rating 4.3 out of 5 stars. 273 ratings.Google doesn't verify reviews. Learn more about results and reviews.A Simple New Tab (browser's home page) inspired by Google's 'Material You' design.Bonjourr · Minimalist Startpage4.9(17.1K)Average rating 4.9 out of 5 stars. 17.1K ratings.Google doesn't verify reviews. Learn more about results and reviews.Improve your web browsing experience with Bonjourr, a beautiful, customizable and lightweight homepage.Tab Manager by Workona4.7(3.6K)Average rating 4.7 out of 5 stars. 3.6K ratings.Google doesn't verify reviews. Learn more about results and reviews.The world’s best tab managerJust Read4.7(887)Average rating 4.7 out of 5 stars. 887 ratings.Google doesn't verify reviews. Learn more about results and reviews.A feature-packed, customizable reader extension.New Tab for Google Workspace™4.4(451)Average rating 4.4 out of 5 stars. 451 ratings.Google doesn't verify reviews. Learn more about results and reviews.Replace new tab page with a personal dashboard featuring, fast access to installed applications, bookmarks, history.Tab Manager Plus for Chrome4.7(1K)Average rating 4.7 out of 5 stars. 1K ratings.Google doesn't verify reviews. Learn more about results and reviews.Quickly find open tabs, see all windows in one view, find duplicates and limit tabs per window. The best Tab Manager for Chrome.SponsorBlock for YouTube - Skip Sponsorships4.7(2.8K)Average rating 4.7 out of 5 stars. 2.8K ratings.Google doesn't verify reviews. Learn more about results and reviews.Skip sponsorships, subscription begging and more on YouTube videos. Report sponsors on videos you watch to save others' time.Tab Manager for Chrome™4.3(78)Average rating 4.3 out of 5 stars. 78 ratings.Google doesn't verify reviews. Learn more about results and reviews.Quick access to all opened tabs. Manage browser tabs to easily navigate and switch between them.Voice Control for ChatGPT4.0(656)Average rating 4.0 out of 5 stars. 656 ratings.Google doesn't verify reviews. Learn more about results and reviews.Expands ChatGPT with voice control and read aloud.Video Downloader professional3.7(385)Average rating 3.7 out of 5 stars. 385 ratings.Google doesn't verify reviews. Learn more about results and reviews.Download online videos in various formats from any websites. Video Downloader save video and watch it later.\n\nTab Manager by Workona4.7(3.6K)Average rating 4.7 out of 5 stars. 3.6K ratings.Google doesn't verify reviews. Learn more about results and reviews.The world’s best tab managerJust Read4.7(887)Average rating 4.7 out of 5 stars. 887 ratings.Google doesn't verify reviews. Learn more about results and reviews.A feature-packed, customizable reader extension.New Tab for Google Workspace™4.4(451)Average rating 4.4 out of 5 stars. 451 ratings.Google doesn't verify reviews. Learn more about results and reviews.Replace new tab page with a personal dashboard featuring, fast access to installed applications, bookmarks, history.Tab Manager Plus for Chrome4.7(1K)Average rating 4.7 out of 5 stars. 1K ratings.Google doesn't verify reviews. Learn more about results and reviews.Quickly find open tabs, see all windows in one view, find duplicates and limit tabs per window. The best Tab Manager for Chrome.SponsorBlock for YouTube - Skip Sponsorships4.7(2.8K)Average rating 4.7 out of 5 stars. 2.8K ratings.Google doesn't verify reviews. Learn more about results and reviews.Skip sponsorships, subscription begging and more on YouTube videos. Report sponsors on videos you watch to save others' time.Tab Manager for Chrome™4.3(78)Average rating 4.3 out of 5 stars. 78 ratings.Google doesn't verify reviews. Learn more about results and reviews.Quick access to all opened tabs. Manage browser tabs to easily navigate and switch between them.Voice Control for ChatGPT4.0(656)Average rating 4.0 out of 5 stars. 656 ratings.Google doesn't verify reviews. Learn more about results and reviews.Expands ChatGPT with voice control and read aloud.Video Downloader professional3.7(385)Average rating 3.7 out of 5 stars. 385 ratings.Google doesn't verify reviews. Learn more about results and reviews.Download online videos in various formats from any websites. Video Downloader save video and watch it later.Accept all cookies4.5(311)Average rating 4.5 out of 5 stars. 311 ratings.Google doesn't verify reviews. Learn more about results and reviews.Accepts necessary cookie policies for smooth website navigation, without deleting cookies.CryptoTab START4.7(15.3K)Average rating 4.7 out of 5 stars. 15.3K ratings.Google doesn't verify reviews. Learn more about results and reviews.New tab page extension for your browser with bunch of beautiful backgrounds and useful widgets.Material You NewTab4.3(273)Average rating 4.3 out of 5 stars. 273 ratings.Google doesn't verify reviews. Learn more about results and reviews.A Simple New Tab (browser's home page) inspired by Google's 'Material You' design.Bonjourr · Minimalist Startpage4.9(17.1K)Average rating 4.9 out of 5 stars. 17.1K ratings.Google doesn't verify reviews. Learn more about results and reviews.Improve your web browsing experience with Bonjourr, a beautiful, customizable and lightweight homepage.Tab Manager by Workona4.7(3.6K)Average rating 4.7 out of 5 stars. 3.6K ratings.Google doesn't verify reviews. Learn more about results and reviews.The world’s best tab managerJust Read4.7(887)Average rating 4.7 out of 5 stars. 887 ratings.Google doesn't verify reviews. Learn more about results and reviews.A feature-packed, customizable reader extension.New Tab for Google Workspace™4.4(451)Average rating 4.4 out of 5 stars. 451 ratings.Google doesn't verify reviews. Learn more about results and reviews.Replace new tab page with a personal dashboard featuring, fast access to installed applications, bookmarks, history.Tab Manager Plus for Chrome4.7(1K)Average rating 4.7 out of 5 stars. 1K ratings.Google doesn't verify reviews. Learn more about results and reviews.Quickly find open tabs, see all windows in one view, find duplicates and limit tabs per window. The best Tab Manager for Chrome.SponsorBlock for YouTube - Skip Sponsorships4.7(2.8K)Average rating 4.7 out of 5 stars. 2.8K ratings.Google doesn't verify reviews. Learn more about results and reviews.Skip sponsorships, subscription begging and more on YouTube videos. Report sponsors on videos you watch to save others' time.Tab Manager for Chrome™4.3(78)Average rating 4.3 out of 5 stars. 78 ratings.Google doesn't verify reviews. Learn more about results and reviews.Quick access to all opened tabs. Manage browser tabs to easily navigate and switch between them.Voice Control for ChatGPT4.0(656)Average rating 4.0 out of 5 stars. 656 ratings.Google doesn't verify reviews. Learn more about results and reviews.Expands ChatGPT with voice control and read aloud.Video Downloader professional3.7(385)Average rating 3.7 out of 5 stars. 385 ratings.Google doesn't verify reviews. Learn more about results and reviews.Download online videos in various formats from any websites. Video Downloader save video and watch it later.",
    "summary": {
      "en": "**ReelControl Overview:**\n\nReelControl is a browser extension that adds a progress bar and playback controls to videos on YouTube Shorts, Instagram, and Facebook Reels. This helps users manage their viewing time and enhances their video-watching experience.\n\n**Key Features:**\n- **Progress Bar and Controls:** Users can see how long a video is, rewind if they miss something, and skip ahead without restarting.\n- **Time Management:** The extension encourages users to spend less time on social media videos by providing quick access to video length and controls.\n  \n**Platform Specifics:**\n- **Instagram:** Only adds native video controls due to a clean interface.\n- **YouTube Shorts:** Reveals a hidden progress bar and reduces clutter in the interface.\n- **Facebook Reels:** Removes excessive clutter and adds a progress bar since there's no native controls.\n\n**Additional Information:**\n- The extension is open-source, allowing community contributions.\n- Developed by Soliloquy Apps, which also created AudioDiary, a voice-powered journaling app.\n\n**Privacy:** The developer assures that user data is not collected or sold.",
      "ko": "ReelControl은 YouTube Shorts, Instagram, Facebook Reels의 동영상에 진행 표시줄과 재생 제어 기능을 추가하는 브라우저 확장 프로그램입니다. 이를 통해 사용자는 시청 시간을 관리하고 동영상 시청 경험을 향상시킬 수 있습니다.\n\n주요 기능으로는 진행 표시줄과 제어 기능이 있습니다. 사용자는 동영상의 길이를 확인하고, 놓친 부분을 되감거나 처음부터 다시 시작하지 않고도 건너뛸 수 있습니다. 이 확장 프로그램은 동영상 길이와 제어 기능에 빠르게 접근할 수 있도록 하여 사용자가 소셜 미디어 동영상에 소비하는 시간을 줄이도록 유도합니다.\n\n플랫폼별로 살펴보면, Instagram에서는 깔끔한 인터페이스 덕분에 기본 동영상 제어 기능만 추가됩니다. YouTube Shorts에서는 숨겨진 진행 표시줄을 보여주고 인터페이스의 복잡함을 줄입니다. Facebook Reels에서는 기본 제어 기능이 없기 때문에 과도한 복잡함을 제거하고 진행 표시줄을 추가합니다.\n\n추가 정보로는 이 확장 프로그램이 오픈 소스이며, 커뮤니티의 기여를 받을 수 있다는 점이 있습니다. 또한, AudioDiary라는 음성 기반 일기 앱을 개발한 Soliloquy Apps에서 제작했습니다.\n\n개발자는 사용자 데이터가 수집되거나 판매되지 않는다고 보장하고 있습니다.",
      "ja": "ReelControlは、YouTube Shorts、Instagram、Facebook Reelsの動画に進行状況バーと再生コントロールを追加するブラウザ拡張機能です。これにより、ユーザーは視聴時間を管理し、動画視聴体験を向上させることができます。\n\n主な機能としては、進行状況バーとコントロールがあります。ユーザーは動画の長さを確認でき、見逃した部分を巻き戻したり、再生を始め直さずにスキップしたりすることができます。また、この拡張機能は、動画の長さやコントロールにすぐアクセスできるため、ユーザーがソーシャルメディアの動画に費やす時間を減らすことを促します。\n\nプラットフォームごとの特徴もあります。Instagramでは、クリーンなインターフェースのため、ネイティブの動画コントロールのみが追加されます。YouTube Shortsでは、隠れた進行状況バーが表示され、インターフェースの煩雑さが軽減されます。Facebook Reelsでは、ネイティブのコントロールがないため、過剰な情報を取り除き、進行状況バーが追加されます。\n\nこの拡張機能はオープンソースであり、コミュニティからの貢献を受け入れています。開発はSoliloquy Appsによって行われており、同社は音声を使った日記アプリ「AudioDiary」も手掛けています。\n\nプライバシーに関しては、開発者はユーザーデータを収集したり販売したりしないことを保証しています。"
    }
  },
  {
    "id": "864a6683278a5386",
    "title": {
      "en": "Pulse (YC S24) Is Hiring",
      "ko": "펄스 채용 중!",
      "ja": "パルス、採用中！"
    },
    "type": "job",
    "url": "https://www.ycombinator.com/companies/pulse-3/jobs/6o4mkAj-machine-learning-engineer",
    "score": 1,
    "by": "sidmanchkanti21",
    "time": 1743613243,
    "content": "As a Machine Learning Engineer at Pulse, you'll create the specialized vision and language models that form the backbone of our document understanding capabilities. You will be given research autonomy for training and fine-tuning these models.",
    "summary": {
      "en": "As a Machine Learning Engineer at Pulse, you will develop important vision and language models that help with understanding documents. You will have the freedom to research and improve these models.",
      "ko": "Pulse에서 머신러닝 엔지니어로 일하게 되면, 문서 이해를 돕는 중요한 비전 및 언어 모델을 개발하게 됩니다. 이 모델들을 연구하고 개선할 수 있는 자유가 주어집니다.",
      "ja": "Pulseの機械学習エンジニアとして、あなたは文書の理解を助ける重要な視覚と言語のモデルを開発します。これらのモデルを研究し、改善する自由があります。"
    }
  },
  {
    "id": "c31402ea1cd00b46",
    "title": {
      "en": "Coffea stenophylla: A forgotten bean that could save coffee from extinction",
      "ko": "커피의 희망, 스테노필라",
      "ja": "絶滅救う！忘れられたコーヒー豆"
    },
    "type": "story",
    "url": "https://www.smithsonianmag.com/science-nature/how-forgotten-bean-could-save-coffee-from-extinction-180986230/",
    "score": 121,
    "by": "derbOac",
    "time": 1743589518,
    "content": "How a Forgotten Bean Could Save Coffee From Extinction\n          One leading botanist is scouring remote corners of the earth to find new species that could keep our mugs full\n\n          By\n\n          Marta Zaraska\n\n          Photographs by Emily Mott\n\n      April/May 2025\n\n     Get our newsletter!\n\n     Get our newsletter!\n\n                An 1896 illustration of Coffea stenophylla in Curtis’s Botanical Magazine, which noted the species’ “superior flavor” and market potential.\n              Missouri Botanical Garden; Dreamstime; iSTOCK\n\n        Stenophylla is a coffee plant, not a criminal, and yet it can still lay claim to its very own “Wanted” poster. In 2018, Aaron Davis, head of coffee research at the Royal Botanic Gardens, Kew, in London, was desperate to track down the rare species, which hadn’t been seen in the wild since 1954. The data he’d found in historical records suggested that stenophylla might be resistant to drought and heat—increasingly valuable traits in a warming and drying world. So he created a “Have you seen this plant?” flyer with pictures of stenophylla’s leaves, whose pointed tips resemble snake fangs. A colleague of his based in Sierra Leone, Daniel Sarmu, jumped on a motorbike and rode across the country’s rust-colored dirt roads, handing out copies to farmers. Most shook their heads: They had never seen a plant like that. This went on for days: more roads, more farms, nothing. “It was clearly not working,” Davis says. He had no choice but to start searching Sierra Leone’s forests himself.\n\nfreestar.queue.push(function() {freestar.newVideo(\"FreeStarVideoAdContainer\");});Every day, the world’s population consumes about two billion cups of coffee. It’s more popular in some places than in others. The average American drinks as many as three cups a day. People drink even more in some European countries, such as Finland and Luxembourg. Ethiopia has a coffee culture, but in other African coffee-growing nations, relatively few drink it. Brazilians drink twice as much per capita as Colombians, but most of Latin America barely touches the stuff, opting instead for less expensive bottled soft drinks. In Asia and Australia, people tend to prefer tea. Coffee is beloved as an energy booster and a mood enhancer. It’s also a social and cultural ritual, and, above all, a habit. Many coffee drinkers are quite particular about what’s in their cup, whether it’s a flat white or a venti iced caramel macchiato.\n\n      The Palm House at Kew, built in the 1840s to house tropical plants, was a repository for exotic samples from all over the world during the height of the British Empire.\n\n      Emily Mott\n\nWhen it comes to taste, coffee is amazingly complex. A single cup may contain up to 1,200 volatile compounds. Yet what you perceive in a cup depends on many things besides the plant’s genome: the environment in which it grew, the weather, the roast, the water used for brewing. Even the color of the cup matters. White makes coffee seem more intense, while clear glass makes it seem sweeter.\n\nAs of now, almost all of humanity’s coffee needs are supplied by just two species: Coffea arabica and Coffea canephora, widely known as robusta. Arabica has plenty of varieties, such as geisha or bourbon, but these are like types of apples you can find in a supermarket—whether it’s a Granny Smith or a Gala, it’s still an apple. Arabica is appreciated for its wide, complex range of flavors, including notes described as floral and fruity. The crop itself is delicate. It doesn’t like high temperatures, it doesn’t deal well with droughts, and it’s vulnerable to pests. It needs to grow at high altitudes, too, preferably more than 3,200 feet above sea level. Robusta entered the scene much later than arabica, described by a Belgian botanist only in 1898, when a fungal disease called leaf rust decimated plantations of arabica at the turn of the 20th century. Disease-resistant robusta quickly gained ground over its fragile cousin. Bitter, harsh and cheap, it’s more of a workhorse, often made into instant coffee. But in today’s changing climate, even robusta crops are failing.\n\nBoth arabica and robusta love water, and the places where they grow are getting less and less of it. Arabica likes at least 59 inches per year—as much as the annual rainfall in one of America’s wettest states, Louisiana. While robusta can do with less, it’s not just about the quantities of water. The timing matters, too. “The producers really want to know one thing: When does it start to rain?” says Christian Bunn, a coffee researcher at the International Center for Tropical Agriculture, in Colombia. When the rains come, the plants begin to flower, and the growing cycle begins. “Now, suddenly, these rains come later, they come early, they disrupt the dry season, or a dry spell disrupts the rainy season,” Bunn says.\n\nBunn was the lead author on a 2014 study showing that climate change could slash the global area suitable for coffee production in half by 2050. At the time, people were skeptical. “This attitude has changed,” he says. Coffee crops around the world have recently fallen short—the world’s second-largest coffee-producing country, Vietnam, had a 20 percent production drop in 2023. Now, companies are worried. “They don’t know where the coffee of the future will come from,” Bunn says. Due to habitat loss and climate change, 60 percent of wild coffee species are now threatened with extinction.\n\nThat’s why Davis is so interested in rediscovering lost coffee species. It’s not that any single species will provide a magical solution to the many challenges facing global coffee production—not even a mythical heat-and-drought-resistant coffee plant like stenophylla. But certain rare species could be used both to partly replace crops in areas that are becoming too hot for them and to crossbreed new, sturdier strains. To make this happen, though, scientists will need genetic resources from which to choose the best traits—and not only those that make them unusually resilient, but also those that impart a great taste, too. That last part is crucial. No species, however resilient, can become the coffee of the future if people don’t find it delicious.\n\nDavis was a teenager in the 1980s when his mother’s green thumb got him interested in botany. He earned his PhD at England’s University of Reading in 1994 and went on his first coffee-finding mission later that decade, when his colleagues drafted him for a trip to Madagascar to look for new species. But his obsession with stenophylla only began about 15 years ago with one antique book, he tells me as he scans the shelves of his cramped office at Kew’s Herbarium. The place smells of old wood, dried leaves and coffee. Through large windows, I see the ocean of green that is Kew: a UNESCO World Heritage Site that harbors one of the most extensive botanic collections in the world. The gardens look calm compared with Davis’ office, which is a chaos of files, centuries-old botany books, coffee grinders, pots and filters, as well as bags, boxes and jars filled with rare coffee beans. No one has discovered more Coffea species than Davis has. He alone has found about a third of all the ones known to science. And he is far from finished.\n\n      Davis at the Royal Botanic Gardens, Kew. Along with his work on coffee, he serves as the gardens’ senior research leader of crops and global change.\n\n      Emily Mott\n\nDavis finally locates the book he was hunting for, a 1925 monograph on coffee by American biologist Ralph Holt Cheney. He flips it open to read me a passage that describes stenophylla beans as “superior to those of all other species.” Other old books Davis found similarly praised stenophylla as “exquisite” and more delicious than the best arabica. Old documents also claimed that this species could grow in lowlands, survive high temperatures and withstand droughts. “I thought, ‘Oh, God, it’s just so interesting. I’d really like to try it,’” Davis says.\n\nHe now has two jars of stenophylla beans stashed in one of his office’s cupboards. They’re yellowed with age, which is hardly surprising, considering that they’re from 1856 and 1873. Davis unearthed them from Kew’s Herbarium. At one point, he was tempted to roast a few of the beans, but a colleague warned him that he might poison himself—the beans were likely laced with ancient preservatives.\n\nScientists first described stenophylla in 1834, when it was still being actively farmed in Sierra Leone. Yet by the mid-20th century, for no apparent reason, the species disappeared from coffeehouses and plantations. “We presume that it was because the British introduced robusta, which is more productive,” says Jeremy Haggar, an agroecologist at the University of Greenwich in England who has collaborated with Davis on several projects. “But why did it disappear totally when there is a clear quality difference? It still seems a bit strange.”\n\nNot much is known about how people first started drinking coffee in the first place. According to Ethiopian folklore, it all began around the mid-ninth century, with a young Ethiopian goatherd named Kaldi. As the story goes, one day Kaldi’s goats discovered a new snack: red berries hiding among the glossy, dark-green leaves of a small tree. After they finished eating, the animals became agitated. Curious, Kaldi decided to try the berries himself, and he soon felt so energized that he started dancing around the field. When Kaldi shared the berries with an abbot of a local monastery, the monk ended up wide awake during the evening prayers. He liked the feeling and decided to turn the berries into an infusion. A global tradition was born.\n\nScience hasn’t confirmed this story, but there is good evidence that Coffea arabica originated in Ethiopia and South Sudan and was most likely domesticated and cultivated in Yemen. From there, via the Yemeni port of Mocha, arabica conquered the world: first India, then, in the 17th century, Europe, and a few decades later, the United States. By 1668, coffee sweetened with honey and spiced with cinnamon was already being savored in New York. Yet it was the Civil War that made coffee into America’s favored hot beverage. Union generals believed that men fought better when dosed with arabica, and the troops were awash in it, their daily rations enough to brew more than two cups per person per day. As Jonathan Morris writes in his book, Coffee: A Global History, the drink was such an important part of battlefield life that the word “coffee” appeared in the diaries of Union soldiers more often than either “bullet” or “rifle.”\n\nThe genus Coffea, to which arabica, robusta and stenophylla all belong, has a wealth of diverse species. There are some with hairy fruits and some whose fruits look like pears with a brain-shaped seed inside. Some are high in caffeine, while others are naturally caffeine-free. Some taste like dark caramel, some like sausage rolls.\n\nAt first, Davis says, his work was pure botany, just filling out the Coffea family tree. Madagascar has a secluded ecosystem brimming with endemic species. “I went there not realizing how difficult it is to find coffee in the wild. It was really, really tricky,” he says. A local botanist named Franck Rakotonasolo showed Davis how it’s done. You don’t scan the greenery for the characteristic fruits (most often the plants don’t have any), nor for leaves (there are too many look-alike plants). Instead, you seek out the distinctive architecture of the Coffea genus. Davis draws the shape in the air with his hands: straight up, that’s the trunk, and then slash, slash to the sides, that’s the branches. “The branches are always held in a certain way,” he says.\n\n      Subscribe to Smithsonian magazine now for just $19.99\n      This article is a selection from the April/May 2025 issue of Smithsonian magazine\n      Subscribe\n\n      Davis (in hat) in the field in South Sudan. In 2021, he helped identify genetic strains of coffee found only along that country’s Boma Plateau.\n\n      Emma Sage © RBG Kew\n\nYet even knowing what to look for, the two botanists often had to trek for days through rainforests, sometimes knee-deep in water. They got caught in a cyclone; Davis almost died of hookworm. One day, he recalls, “when I took my shoes off, my feet were white. It was just insane.” Davis went on to name one of the species he discovered in honor of his mentor: Coffea rakotonasoloi.\n\nAs a taxonomist, Davis had made it his main goal to discover and describe new species of coffee, to understand the genetic diversity of the genus more fully. But around 2009, Davis started thinking seriously about how to help coffee farmers adapt to the warming planet. He hands me a photo taken in Uganda by a member of his research team. It shows rows of robusta plants, all wilted, sticking out from cracked, dry soil. “Without lots of water, this robusta will be dead,” he says.\n\nHaggar, the University of Greenwich professor, noticed signs of this worrying trend in the mid-aughts in Nicaragua. “That was the first time I saw a coffee production system just start to collapse,” he says. That year, the usual dry season of five to six months extended into seven. At first, the plants were doing fine, but then, “suddenly, in the space of one or two weeks, it was just like finally the system ran out of water, and everything started dropping leaves and dying back,” Haggar says. “It was quite incredible to see that tipping point.”\n\nThis concern is driving the effort to bring new species to market. Along with stenophylla, Davis is hopeful about Coffea zanguebariae, an East African species that prospers in hot weather. Coffea excelsa, native to Central Africa, survives without water long after robusta gives up. And Coffea racemosa, whose beans are as tiny as lentils, is resistant to most pests.Once Davis’ “Wanted” poster had failed, he decided it was time to search the forests the way Rakotonasolo had taught him. But by 2018, many of the forests were gone, chopped down for fuel and farms. One place he and his team could still look was the Kasewe Forest Reserve, a hilly area in central Sierra Leone. It took the scientists all morning to find their way to the remote reserve. “We got there, walked up this steep hill for about an hour, looked around, and everyone was like, ‘Is this it?’ And I said, ‘No.’ ‘This is it, definitely!’ ‘No.’”\n\n      A jar of Coffea stenophylla beans—collected in Sierra Leone in 1856—now sits in the office of Aaron Davis at the Royal Botanic Gardens, Kew, in London.\n\n      Emily Mott\n\nWhen they finally found the plant, it ended up being measly, with no fruits or flowers. To an untrained eye, it looked like any other shrub in the forest. But Davis knew right away—he saw that distinctive architecture. “After all these years of wanting to find it, it felt fantastic,” he says. Bizarrely, though, his team couldn’t find any more specimens. There was just the single, scrawny plant. To protect it, the scientists cleared the undergrowth around the stenophylla and marked the location with stones. Then it was time to head back. Haggar, who was part of the expedition, remembers the group’s feeling: “It was kind of bittersweet, because we had found it, but with one plant, there’s not so much you can do.” Because the plant had not yet flowered, there were no seeds to gather and cultivate in a research plot.\n\nStill, the scientists now knew the type of ecosystem where stenophylla might be found. Over the coming months, they managed to locate several more of the plants in Sierra Leone’s forests and gather beans. By 2020, Davis’ team had finally collected enough stenophylla in the wild to roast nine grams—a tiny amount, but enough for him to finally have his first long-awaited taste.\n\n      Old and new books about African coffee fill Davis’ bookshelves at Kew. Davis and several colleagues co-wrote their own Coffee Atlas of Ethiopia in 2018.\n\n      Emily Mott\n\nThe whole process was far from standard. To begin with, the beans were dried on a plastic chair on someone’s patio in Sierra Leone. Still, the first sip took Davis by surprise. Stenophylla resembled Rwandan bourbon coffee, a premium arabica of exceptional quality. “I was just, like, oh, my God, this is amazing,” he says. In a later panel tasting of stenophylla, Davis’ impressions were confirmed: The tasters agreed that it rose to the level of high-quality arabica. It was sweet, with undertones of peach, jasmine, chocolate and elderflower syrup.\n\nAs we walk among the rows of aged shelves and drawers that make up the archives of Kew Gardens’ Herbarium, Davis tells me that there are more than seven million dried specimens here. Collected around the world over the past 170 years, they include 450 picked by Charles Darwin during his trip aboard HMS Beagle. Expecting my visit, Davis has pulled out several dried coffee specimens to show me. One is a large leaf of Coffea liberica, collected by an explorer named J.D. Snowden in 1930 in what is now South Sudan. It’s paled with age and glued to a well-worn card. It looks brittle.\n\nYet Davis says such leaves are an invaluable source of high-quality DNA. “You might be looking through the specimens, saying: ‘I know what that is.’ ‘I know what that is.’ ‘Oh, this looks a bit different!’ And then you do DNA tests, chemical tests, micromorphology with an electron microscope—and you realize, it is different,” he says. Some of the specimens in the Herbarium are unnamed, awaiting a label; others have been miscataloged as a known species, while DNA data clearly shows they are something else. The liberica specimen Davis shows me is, in fact, an excelsa coffee—a species that he has recently proved to be separate from liberica. (The paper awaits publication.)\n\nThe clear cup Davis hands me is filled with freshly brewed racemosa, the pest-resistant coffee with minuscule beans. “You’re probably one of only 50 people who’ve drunk this,” he says as he takes a second cup for himself. It smells nothing like coffee—minty, with a hint of rosemary. “Some of the major compounds you find in arabica are not in this coffee. And some compounds that are in this coffee are not in arabica,” he adds. As with the smell, the first flavors I taste are not very coffee-like. There is some black currant, some anise. The aftertaste, though, is very much coffee. It’s unusual, I tell Davis, but delicious nevertheless.\n\nThe next coffee we taste is liberica. To my untrained taste buds, this one is even more bizarre, with hints of wild mushrooms. It’s still nothing compared to some Madagascar coffees Davis has tasted, he tells me. One was “a bit meaty” and “super, super acidic,” he says. The poor taste is also the reason why naturally caffeine-free coffees likely won’t hit your supermarket shelves anytime soon. “They’re not producing caffeine, they’re producing other acids,” Davis says. And those acids often translate into odd flavors.\n\nWhile stenophylla has an amazing taste and climate-sturdy traits, it also likely won’t conquer the world in the near future. A major issue is low yield, Davis says, especially compared to robusta. This might be one reason stenophylla disappeared in the 20th century. But Davis hopes stenophylla could be bred together with other coffee varieties. He and his colleagues in Africa are already engaged in studies using traditional crossbreeding methods. According to Benoît Bertrand, a plant geneticist at CIRAD, a French agricultural research organization, the racemosa could also be crossed with another wild species, such as Coffea sessiliflora, to produce something that’s drought-resistant but with bigger beans. “The wild species are reservoirs of genes. That can be very interesting for the future of breeding,” he says. “Biodiversity is not just for fun; it’s a gene pool,” Bunn, of the International Center for Tropical Agriculture, agrees. “We’re losing these resources, and we need them.”\n\n      A dried sample of Coffea stenophylla at Kew displays a distinctive feature of the species: pointed leaf tips that resemble snake fangs.\n\n      Emily Mott\n\nIn the meantime, scientists are advising coffee growers and consumers to think about the industry’s effects on climate change. One big issue is fertilizers, which are applied in large quantities to coffees grown in open fields. These fertilizers release nitrous oxide, a potent greenhouse gas. That’s why Haggar advises environmentally conscious consumers to choose shade-grown coffee, which requires less fertilizer. Coffee pods and capsules are problematic, too. A 2021 study showed they have the largest carbon footprint of all common coffee preparation methods. “If you are just taking ground coffee out of a bag and putting it into a French press, then you are avoiding a lot of these sorts of emissions,” Haggar says.\n\nRegarding the future of stenophylla, Haggar says, he and Davis are now just “sitting around and waiting.” They oversee a plot with about 8,000 stenophylla plants in Sierra Leone, which, if things go well, should bear fruit this year. Yet Davis believes that for the near future stenophylla will be just a niche coffee, something for a small group of connoisseurs. He is not ready to promote stenophylla to farmers. “We don’t know what the yields are going to be,” he says. “We don’t know how resistant it’s going to be to pests and diseases. We don’t know how it will work for farmers from a profitability standpoint. There’s so many unknowns.”\n\nBut farmers urgently need solutions. If they can get better breeds, better varieties, maybe their farms can survive. That’s where Davis comes in: cataloging more and more types of coffee, searching for new genotypes, looking for resilient species that could be both used for crossbreeding sturdier hybrids and planted directly on farms as high-profit specialty coffees. “We need people like him to understand coffee diversity and put ideas out there,” says Tania Humphrey, director of research and development at World Coffee Research, a nonprofit formed by the global coffee industry. “Maybe he can narrow it down to the top three or five that are interesting, and eventually that material can flow into a breeding pipeline.”\n\nAs for Davis, he is already preparing his next trip to Africa. New species of coffee are still waiting to be found—to help farmers thrive and keep the world wide awake.\n\n  You Might Also Like\n\n          Rediscovered Coffee Species Boosts Crop's Climate Resilience Without Sacrificing Taste\n          April 21, 2021\n\n          More Than Half of All Coffee Species Are at Risk of Extinction\n          January 17, 2019\n\n          Eight Superfoods That Could Future-Proof Our Diet\n          July 12, 2022\n\n          Five Coffee Mysteries the Bean’s Genes May Crack\n          September 4, 2014\n\n          Five Coffee Mysteries the Bean’s Genes May Crack\n          September 4, 2014\n\n            Marta Zaraska\n\n            | Read More\n\n          Marta Zaraska is a writer specializing in nutrition, health and the environment.\n\n              Emily Mott\n\n              | Read More\n\n            Emily Mott is a photographer based in West Sussex, England, where she lives on a farm with her family.\n\n      Get the latest Science stories in your inbox.\n\n        Email Powered by Salesforce Marketing Cloud (Privacy Notice / Terms & Conditions)\n\n                  More about:\n\n                      Agriculture\n\n                      Botany\n\n                      Climate Change\n\n                      Coffee\n\n                      Plants\n\n    freestar.config.enabled_slots.push({ placementName: \"smithsonianmag_rail_right_1_new\", slotId: \"smithsonianmag_rail_right_1_new\" });\n\n                            freestar.config.enabled_slots.push({ placementName: \"smithsonianmag_rail_right_2_new\", slotId: \"smithsonianmag_rail_right_2_new_2\" });\n\n                            freestar.config.enabled_slots.push({ placementName: \"smithsonianmag_rail_right_2_new\", slotId: \"smithsonianmag_rail_right_2_new_3\" });\n\n                            freestar.config.enabled_slots.push({ placementName: \"smithsonianmag_rail_right_2_new\", slotId: \"smithsonianmag_rail_right_2_new_4\" });\n\n                            freestar.config.enabled_slots.push({ placementName: \"smithsonianmag_rail_right_2_new\", slotId: \"smithsonianmag_rail_right_2_new_5\" });\n\n                            freestar.config.enabled_slots.push({ placementName: \"smithsonianmag_rail_right_2_new\", slotId: \"smithsonianmag_rail_right_2_new_6\" });\n\n                            freestar.config.enabled_slots.push({ placementName: \"smithsonianmag_rail_right_2_new\", slotId: \"smithsonianmag_rail_right_2_new_7\" });\n\n                            freestar.config.enabled_slots.push({ placementName: \"smithsonianmag_rail_right_2_new\", slotId: \"smithsonianmag_rail_right_2_new_8\" });\n\n                            freestar.config.enabled_slots.push({ placementName: \"smithsonianmag_rail_right_2_new\", slotId: \"smithsonianmag_rail_right_2_new_9\" });\n\n                            freestar.config.enabled_slots.push({ placementName: \"smithsonianmag_rail_right_2_new\", slotId: \"smithsonianmag_rail_right_2_new_10\" });\n\n                            freestar.config.enabled_slots.push({ placementName: \"smithsonianmag_rail_right_2_new\", slotId: \"smithsonianmag_rail_right_2_new_11\" });",
    "summary": {
      "en": "A botanist named Aaron Davis is searching for a rare coffee plant called Coffea stenophylla, which hasn't been seen in the wild since 1954. He believes this species may be able to withstand drought and heat, making it important for coffee production in a changing climate. Currently, most of the world's coffee comes from just two species: arabica and robusta. Both are struggling due to climate change, leading to concerns about future coffee supplies.\n\nDavis's quest involves finding new coffee species that can be crossbred with existing varieties to create hardier plants. He has successfully rediscovered stenophylla and is working to cultivate it, noting its excellent flavor and potential for climate resilience. However, its low yield compared to robusta raises concerns about its viability for farmers.\n\nIn the meantime, scientists are advocating for sustainable coffee practices to help combat climate change, such as choosing shade-grown coffee and reducing fertilizer use. Davis is continuing his research, hoping to identify more resilient coffee species that can support farmers and ensure a steady coffee supply for the future.",
      "ko": "식물학자 아론 데이비스는 1954년 이후로 자연에서 발견되지 않은 희귀 커피 식물인 코페아 스테노필라를 찾고 있다. 그는 이 식물이 가뭄과 고온에 견딜 수 있을 것으로 믿고 있으며, 이는 변화하는 기후 속에서 커피 생산에 중요한 역할을 할 수 있다. 현재 전 세계 커피의 대부분은 아라비카와 로부스타라는 두 가지 품종에서 생산되고 있으며, 두 품종 모두 기후 변화로 어려움을 겪고 있어 미래의 커피 공급에 대한 우려가 커지고 있다.\n\n데이비스는 기존 품종과 교배할 수 있는 새로운 커피 품종을 찾는 작업을 하고 있다. 그는 스테노필라를 성공적으로 재발견했으며, 이 식물의 뛰어난 맛과 기후 적응 가능성을 높이 평가하고 있다. 그러나 로부스타에 비해 수확량이 낮다는 점은 농민들에게 실질적인 문제로 남아 있다.\n\n한편, 과학자들은 기후 변화에 대응하기 위해 지속 가능한 커피 생산 방식을 권장하고 있다. 그중에는 그늘에서 재배된 커피를 선택하고 비료 사용을 줄이는 방법이 포함된다. 데이비스는 연구를 계속 진행하며, 농민들을 지원하고 미래에 안정적인 커피 공급을 보장할 수 있는 더 많은 내구성 있는 커피 품종을 찾기를 희망하고 있다.",
      "ja": "植物学者のアーロン・デイビスは、1954年以来野生では見られていない希少なコーヒー植物、コフィア・ステノフィラを探しています。彼は、この種が干ばつや高温に耐えられる可能性があり、変化する気候の中でコーヒー生産にとって重要であると考えています。現在、世界のコーヒーの大部分はアラビカ種とロブスタ種の二つから生産されていますが、どちらも気候変動の影響で苦しんでおり、将来のコーヒー供給に対する懸念が高まっています。\n\nデイビスの探求は、新しいコーヒー種を見つけて既存の品種と交配し、より強靭な植物を作り出すことを目指しています。彼はステノフィラを再発見し、その栽培に取り組んでおり、優れた風味と気候への適応力の可能性を指摘しています。しかし、ロブスタに比べて収量が少ないため、農家にとっての実用性には懸念があります。\n\nその間、科学者たちは気候変動に対抗するために持続可能なコーヒーの栽培方法を提唱しています。具体的には、日陰で育てられたコーヒーを選ぶことや、肥料の使用を減らすことが挙げられます。デイビスは研究を続けており、農家を支援し、将来の安定したコーヒー供給を確保するために、より強靭なコーヒー種を特定することを期待しています。"
    }
  },
  {
    "id": "20e4b66237eeec08",
    "title": {
      "en": "When Jorge Luis Borges met one of the founders of AI",
      "ko": "보르헤스와 AI 창시자 만남",
      "ja": "ボルヘスとAI創始者の邂逅"
    },
    "type": "story",
    "url": "https://resobscura.substack.com/p/when-jorge-luis-borges-met-one-of",
    "score": 15,
    "by": "benbreen",
    "time": 1743615008,
    "content": "Share this postRes ObscuraWhen Jorge Luis Borges met one of the founders of AICopy linkFacebookEmailNotesMoreDiscover more from Res ObscuraNotes on the history of technology, medicine, science, art, drugs, and empire. Also: AI in research and teaching.Over 7,000 subscribersSubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.Already have an account? Sign inWhen Jorge Luis Borges met one of the founders of AIBenjamin BreenApr 03, 20256Share this postRes ObscuraWhen Jorge Luis Borges met one of the founders of AICopy linkFacebookEmailNotesMore2ShareOne reason I became a historian is the joy of encountering moments in the past that are foreign, yet also oddly familiar. These moments seem to ripple outward, lapping up against the present in unexpected ways. Lately, I have been deeply dispirited by the current attack on academia here in the US — the deportation of graduate students and researchers, the dismantling of a huge range of important projects via funding cuts to NIH and elsewhere. And I’ve been reflecting on one of these moments of contact with the past. It happened in Buenos Aires in 1970. What I love about this exchange is that it takes place across the supposed “two cultures” of science and the humanities, jumping in a remarkably freewheeling way between literature, philosophy, and the fields of AI and machine learning, which were at the time barely even born. The participants were Jorge Luis Borges, the great Argentine writer, and Herbert A. Simon, an economist, psychologist, and pioneer of artificial intelligence. Their conversation, part of which I’ve reproduced below, models something we badly need today: not just the simple joy of exploring ideas for their own sake, but also the ability to avoid seeing your work and life as something defined by what it excludes. I rarely see historians and computer scientists (or people who write for literary magazines and people who work in AI labs) having this sort of conversation today. I wish it happened more. That’s partly why I’m writing this newsletter.SubscribeShare“It’s a result of your past”Their meeting took place in Borges’s office at the Argentine National Library. Simon had requested the meeting because of his admiration for Borges’s writing—his first and only “audience with a celebrity,” as he put it later . He was visiting Buenos Aires to lecture on systems theory, but what he really wanted was to speak with the author of “The Library of Babel.”“I have been seized by an unaccountable wish to meet you,” Simon wrote in a letter to Borges. “I am a social scientist who tries to understand human behavior by building mathematical models (or, more recently, computer simulations).”Once the conversation began, Borges quickly guided Simon away from casual pleasantries about translation and into deeper philosophical waters. Borges, in his gently ironic manner, asked:BORGES: “Now I would like you to tell me a bit about the so-called behaviorism. What is its underlying principle? Is it free will or predestination?”It is, and was, a hugely open-ended question. But Simon was at this time immersed in the study of early computer simulations, and he took this to be a question about whether the human mind is fundamentally comparable to a computer. This was his response: SIMON: Well, I always end up talking about computers. I’m in love with computers. We could put it this way. Faced with a certain problem, a computer will also behave in a certain way. And we may wonder whether it acted out of free will. We can say it employed free will in the sense that had it been programmed differently, it would have chosen to do otherwise, it would have behaved in a different way.BORGES: What do you mean by behaved? Because we are talking about a purely mechanical process here. I mean, in the case of computers, of course.SIMON: Yes, it’s a mechanical process. But I believe, like many others in my professional field, that human beings also display a mechanical type of behavior, much like computers.BORGES: Do you mean to say that we act by force of habit?SIMON: Rather accordingly to programs that we have stored in our brain. And we have free will in the sense that our resulting behavior will depend on who we are and the situation we are in. People respond differently when confronting the same situation.BORGES: So, in your opinion, when faced with a dilemma, say, a situation in which there is a choice to be made between two possible behaviors, we can choose one of them?SIMON: Your mental programming does the choosing. Yes, you choose. It seems to me that Simon is here arguing for what philosophers call “compatibilism” — the idea that determinism can coexist with meaningful human choice and responsibility. When Borges pressed him on whether an all-knowing being could predict our every action, though, Simon didn't shy away from the implications:BORGES: Would this imply that if any all-powerful being, any god, knew everything about my past, my childhood, even about the time before I was born, my ancestors … would this imply that he would be able to predict my behavior in any one situation?SIMON: According to my scientific beliefs, I would say so. With such knowledge, we can predict an individual’s behavior.BORGES: So, what I’m saying right now is …SIMON: … it’s a result of your past …BORGES: … it’s inevitable.SIMON: It is inevitable, yes. However, you still retain your identity, your individuality. You embody your own past.BORGES: I understand. Well, I like to think I do. Now, does this account for all of our actions? That is, if my right hand is resting on my left hand, is it because it has to be this way? I believe people do quite a lot of things without any thinking.SIMON: That’s the doing of our subconscious mind. You’re right, yes, otherwise, we would hardly be able to tie our shoelaces. Most things happen in this way. But that’s because we are heavily programmed.BORGES: Would you say then things are to be also inevitable in this sense?SIMON: They might be different, but always depending on programming. Any determinant could affect your programming and lead you to act differently. And if we introduce chance into the picture, scientists will always ultimately rule it out. At some point they may have to admit their inability to explain a particular phenomenon, but they will keep on working on the assumption that actions are determined by certain causes. And therefore, when we study a person who is in the process of solving a problem, we start from the assumption that every little thing has a cause. We are not always able to identify those causes.BORGES: Well, of course. In order to study a person’s behavior, professionals have to go back to that individual’s history. Even to the historic past, to the origin of humanity, the cosmos.SIMON: No, it’s not like that. Because the past influences a person’s present behavior to the extent that this past is already in the person. So, we can always find a starting point.What I love about this is that here we have two significant thinkers, in two very different fields, who are both reflecting on not just on the meaning and importance of history (“it’s a result of your past”) but also about how and whether they have, themselves, been able to act within history. If you compiled an enormous dataset of everything Borges read, and combined it with an exquisitely sensitive record of every sensory experience he ever had, could you create a Borges LLM? It’s a question that I think a lot of my peers in the humanities would not take much interest in, today (the two cultures, again). But I love that Borges himself was thinking about it. Lately I have been spending a good amount of time thinking about whether, and in what ways, there can be value in seeking to simulate historical figures (or even entire historical moments — like the collective medical opinion of 1820s London, say) by combining a large amount of historical primary sources with an advanced language model. I think this will absolutely be a powerful tool. I’m just not sure when and how. What I do know is that the people who figure that out will be a team that combines humanities and STEM. It is encouraging to see Simon and Borges embodying that kind of thinking way back in 1970. I have been writing Res Obscura for 13 years now, and it has always been unpaywalled and free of any advertisements. The generosity of paid subscribers makes this possible.Become a paid subscriber:SubscribeMy mother’s journalsSomeday, when the feeling is a little less raw, I’ll write about the hundred or so journals that my mother left behind when she died. It was during Covid, and my wife was in her second trimester of pregnancy with our older daughter, Yara. My mom had been diagnosed with an aggressive form of cancer and was in a lot of pain. She was incredibly brave throughout, and, as a lifelong diarist and writer, she kept writing in her journal up to the end. This is one of the last passages in her last journal. It’s about her hoping to hold on long enough to see Yara (who she ended up missing, alas):The second to last entry in  my mother’s journal. Yara is 3 years old now and loves rainbows. You embody your own past.I haven’t had the heart to actually read my mother’s journals yet.I only have this photo because she showed it to me and wanted me to share with Yara when she got older. Someday, probably far in the future, I will not only read them, but experiment with using them as data for an AI system which, to some extent, might embody some sort of spectral trace of my mother and her past. She wanted her journals to be read and understood. She wanted to be able to talk to Yara. Someday, “she” will. Or, at least, a simulacrum composed of the collection of thoughts and dreams and impressions that she chose to record will. That day is not even close today. But it will happen. And when it does, I’ll think about Borges and Simon in Buenos Aires in 1970.The full dialogue is reproduced here, in an article in The Journal of the History of Economic Thought by Ricardo F. Crespo. SubscribeShareWeekly links• “They discovered that the Jahai, hunter-gatherers living at the border of Malaysia and Thailand, have a rich vocabulary of abstract smell words. One Jahai term, itpit, refers to the ‘intense smell of durian, perfume, soap, Aquillaria wood, and bearcat’… Jahai and English speakers were asked to identify and name twelve smells, including cinnamon, turpentine, gasoline, and onion. English speakers, despite their greater familiarity with the odors, faltered. They mostly gave rambling source-based answers and showed almost no agreement among themselves. One English speaker presented with cinnamon responded, ‘I don’t know how to say that, sweet, yeah; I have tasted that gum like Big Red or something tastes like, what do I want to say? I can’t get the word. Jesus it’s that gum smell like something like Big Red. Can I say that? Ok. Big Red. Big Red gum.’ But Jahai speakers named smells with relative ease.” (From this great New Yorker article by UC Davis anthropologist Manvir Singh on how language shapes thought)• As a longtime fan and sometime contributor to Lapham’s Quarterly, I was happy to see that it has been revived:  more here. • “In comments to The Washington Post, the Brazilian president went further, delving into the minutiae of early 20th-century aerospace engineering and mourning what he described as the wrongful denial of Brazilian valor. ‘Everyone knows that Santos Dumont was the first to make something heavier than air fly, in an autonomous way, without any assistance,’ he vented. ‘But the Americans have the movie industry and were able to promote the Wright brothers.’” (The Washington Post)Leave a commentShare6Share this postRes ObscuraWhen Jorge Luis Borges met one of the founders of AICopy linkFacebookEmailNotesMore2SharePrevious",
    "summary": {
      "en": "The article discusses a meeting in 1970 between the Argentine writer Jorge Luis Borges and Herbert A. Simon, a pioneer in artificial intelligence. The author reflects on the significance of their conversation, which bridged the gap between literature and science, highlighting the importance of interdisciplinary dialogue. Borges questioned Simon about human behavior and free will, leading to a discussion about whether our actions are determined by our past experiences, akin to a computer's programming.\n\nSimon suggested that while our behavior might be predictable based on past influences, we still retain individuality. The conversation underscores the value of exploring ideas across different fields and the relevance of history in understanding human behavior. The author expresses a desire for more such interdisciplinary discussions today and envisions future possibilities of using AI to simulate historical figures, inspired by the ideas exchanged between Borges and Simon.\n\nThe article concludes with a personal note about the author's mother and her journals, hinting at the potential for AI to embody her thoughts and experiences in the future.",
      "ko": "이 글에서는 1970년 아르헨티나 작가 호르헤 루이스 보르헤스와 인공지능의 선구자 허버트 A. 사이먼 간의 만남에 대해 다룹니다. 저자는 이들의 대화가 문학과 과학의 경계를 허물었다는 점에서 중요한 의미를 지닌다고 반영합니다. 보르헤스는 사이먼에게 인간의 행동과 자유 의지에 대해 질문하며, 우리의 행동이 과거 경험에 의해 결정되는지, 즉 컴퓨터의 프로그래밍처럼 이루어지는지를 논의합니다.\n\n사이먼은 우리의 행동이 과거의 영향을 바탕으로 예측 가능할 수 있지만, 여전히 개인성을 유지한다고 제안했습니다. 이 대화는 다양한 분야에서 아이디어를 탐구하는 것의 가치와 인간 행동을 이해하는 데 있어 역사적 맥락의 중요성을 강조합니다. 저자는 오늘날에도 이러한 학제 간 논의가 더 많이 이루어지기를 바라며, 보르헤스와 사이먼 간의 교류에서 영감을 받아 인공지능을 활용해 역사적 인물을 시뮬레이션하는 미래의 가능성을 상상합니다.\n\n글은 저자의 어머니와 그녀의 일기에 대한 개인적인 언급으로 마무리되며, 미래에 인공지능이 그녀의 생각과 경험을 구현할 수 있는 가능성을 암시합니다.",
      "ja": "この記事では、1970年にアルゼンチンの作家ホルヘ・ルイス・ボルヘスと人工知能の先駆者ハーバート・A・サイモンとの会合について述べています。著者は、文学と科学の間の架け橋となった彼らの会話の重要性を振り返り、学際的な対話の大切さを強調しています。ボルヘスはサイモンに人間の行動や自由意志について質問し、私たちの行動が過去の経験によって決定されるのか、コンピュータのプログラミングのようなものなのかという議論に発展しました。\n\nサイモンは、私たちの行動は過去の影響に基づいて予測可能である一方で、個性は失われないと提案しました。この会話は、異なる分野のアイデアを探求することの価値や、人間の行動を理解する上で歴史の重要性を浮き彫りにしています。著者は、今日においてもこのような学際的な議論がもっと行われることを望み、ボルヘスとサイモンの間で交わされたアイデアに触発されて、AIを使って歴史的人物をシミュレーションする未来の可能性を描いています。\n\n記事の最後では、著者の母親と彼女の日記についての個人的な思いが語られ、将来的にAIが彼女の思考や経験を具現化する可能性について示唆しています。"
    }
  },
  {
    "id": "858e8e58a7203547",
    "title": {
      "en": "Restructuring Announcement",
      "ko": "구조조정 발표",
      "ja": "再編成発表"
    },
    "type": "story",
    "url": "https://automattic.com/2025/04/02/restructuring-announcement/",
    "score": 13,
    "by": "markx2",
    "time": 1743618862,
    "content": "Company News\t\tEvery Automattician Is Now an Owner ofAutomattic\n\n\t\t\tOctober 14, 2024\n\n\t\tA recent stock grant gave more than 1,700 Automattic employees a stake in the company.",
    "summary": {
      "en": "On October 14, 2024, Automattic announced that over 1,700 of its employees received stock grants, making them owners of the company.",
      "ko": "2024년 10월 14일, Automattic은 1,700명 이상의 직원이 주식 보상을 받았다고 발표했습니다. 이로 인해 이 직원들은 회사의 주인이 되었습니다.",
      "ja": "2024年10月14日、Automatticは、1,700人以上の従業員に株式の付与を行ったと発表しました。これにより、彼らは同社のオーナーとなりました。"
    }
  },
  {
    "id": "f8061b412c577bda",
    "title": {
      "en": "Stop syncing everything",
      "ko": "모든 동기화 중지!",
      "ja": "すべての同期をやめろ"
    },
    "type": "story",
    "url": "https://sqlsync.dev/posts/stop-syncing-everything/",
    "score": 591,
    "by": "neilk",
    "time": 1743440864,
    "content": "Carl Sverre  March 2025   Outline   A different approach to edge replication  Lazy: Sync at your own pace  Partial: Sync only what’s needed  Edge: Sync close to the action  Consistency: Sync safely  What can you build with Graft?  The Graft SQLite Extension (libgraft)  How to get involved  Appendix  Roadmap  Comparison with other SQLite replication solutions",
    "summary": {
      "en": "**Summary of Carl Sverre's Outline (March 2025)**\n\n- **New Edge Replication Approach**: \n  - **Lazy**: Sync data at your own speed.\n  - **Partial**: Only sync the necessary data.\n  - **Edge**: Sync data close to where it’s needed.\n  - **Consistency**: Ensure safe syncing of data.\n\n- **What You Can Build**: \n  - Introduction to the Graft SQLite Extension (libgraft).\n\n- **Get Involved**: Information on how to participate.\n\n- **Appendix**: Additional details included.\n\n- **Roadmap**: Future plans outlined.\n\n- **Comparison**: A look at how this approach differs from other SQLite replication solutions.",
      "ko": "칼 스베레의 개요 요약(2025년 3월)\n\n새로운 엣지 복제 접근 방식이 소개되었습니다. 이 방식은 사용자가 원하는 속도로 데이터를 동기화할 수 있는 '게으른' 방식과 필요한 데이터만 선택적으로 동기화하는 '부분적' 방식, 필요한 장소에서 가까운 곳에서 데이터를 동기화하는 '엣지' 방식으로 구성됩니다. 또한, 데이터의 안전한 동기화를 보장하는 '일관성'도 중요합니다.\n\n이 접근 방식을 통해 Graft SQLite 확장(libgraft)을 소개할 수 있습니다. 이 확장은 개발자들이 활용할 수 있는 새로운 도구입니다.\n\n참여 방법에 대한 정보도 제공됩니다. 관심 있는 사람들은 어떻게 이 프로젝트에 참여할 수 있는지 알아볼 수 있습니다.\n\n부록에는 추가적인 세부 사항이 포함되어 있습니다. \n\n미래 계획에 대한 로드맵도 제시되어 있습니다. \n\n마지막으로, 이 접근 방식이 다른 SQLite 복제 솔루션과 어떻게 다른지 비교하는 내용도 포함되어 있습니다.",
      "ja": "カール・スヴェレのアウトラインの要約（2025年3月）\n\n新しいエッジレプリケーションアプローチが提案されています。この方法は、データの同期を自分のペースで行える「レイジー」な方式を採用しています。また、必要なデータだけを同期する「部分的」なアプローチで、データが必要な場所に近い「エッジ」での同期を実現します。さらに、安全にデータを同期するための「整合性」も重視されています。\n\nこのアプローチを使って構築できるものとして、Graft SQLite拡張（libgraft）の紹介があります。\n\n参加方法についての情報も提供されています。\n\n付録には、追加の詳細が含まれています。\n\n今後の計画については、ロードマップが示されています。\n\n最後に、このアプローチが他のSQLiteレプリケーションソリューションとどのように異なるのかを比較しています。"
    }
  },
  {
    "id": "2191918a01764cdd",
    "title": {
      "en": "Measuring Acceleration Structures",
      "ko": "가속 구조 측정",
      "ja": "加速構造の測定"
    },
    "type": "story",
    "url": "https://zeux.io/2025/03/31/measuring-acceleration-structures/",
    "score": 69,
    "by": "ibobev",
    "time": 1743596932,
    "content": "«Year of independence\n\nMeasuring acceleration structures\n\n31 Mar 2025\n\nHardware accelerated raytracing, as supported by DirectX 12 and Vulkan, relies on an abstract data structure that stores scene geometry, known as “acceleration structure” and often referred to as “BVH” or “BLAS”. Unlike geometry representation for rasterization, rendering engines can not customize the data layout; unlike texture formats, the layout is not standardized across vendors.\n\nIt may seem like a trivial matter - surely, by 2025 all implementations are close to each other in memory consumption, and the main competition is over ray traversal performance and new ray tracing features? Let’s find out.\n\nExperimental setup\n\nIt’s going to be difficult to make any generalized claims here; and testing this requires using many different GPUs by many different vendors, which is time consuming. So for the purpose of this post, we will just look at a single scene - Amazon Lumberyard Bistro, or more specifically a somewhat customized variant by Nvidia which uses more instancing than the default FBX download.\n\nThe results are captured by running niagara renderer; if you’d like to follow along, you will need Vulkan 1.4 SDK and drivers, and something along these lines:\n\ngit clone https://github.com/zeux/niagara --recursive\ncd niagara\ngit clone https://github.com/zeux/niagara_bistro bistro\ncmake . && make\n./niagara bistro/bistro.gltf\n\nThe code will parse the glTF scene, convert the meshes to use fp16 positions, build a BLAS for every mesh1, compact it using the relevant parts of VK_KHR_acceleration_structure extension, and print the resulting compacted sizes. While a number of levels of detail are built as the scene is loaded, only the original geometry makes it into acceleration structures, for a total of 1.754M triangles2.\n\nThe builds are using PREFER_FAST_TRACE build mode; on some drivers, using LOW_MEMORY flag allows to reduce the BLAS size further at some cost to traversal performance, which we will ignore for now.\n\nExperimental results\n\nRunning this on the latest (as of end of March) drivers of all respective vendors, on a host of different GPUs, we get the following results; the total BLAS size is presented alongside approximate “bytes/triangle” number - which is not really correct to compute but we will do this anyway.\n\n      GPU\n      BLAS size\n      Bytes/triangle\n\n      AMD Ryzen 7950X (RDNA2 iGPU)\n      100 MB\n      57.0\n\n      AMD Radeon 7900 GRE (RDNA3)\n      100 MB\n      57.0\n\n      AMD Radeon 9070 (RDNA4)\n      84 MB\n      47.9\n\n      NVIDIA GeForce RTX 2080\n      46 MB\n      26.5\n\n      NVIDIA GeForce RTX 3050\n      45 MB\n      25.7\n\n      NVIDIA GeForce RTX 4090\n      45 MB\n      25.7\n\n      NVIDIA GeForce RTX 5070\n      33 MB\n      18.8\n\n      Intel Arc B580\n      79 MB\n      45.0\n\nNow, that’s quite a gap! The delta between earlier AMD GPUs and the latest NVIDIA GPUs is 3x; comparing the latest AMD and NVIDIA GPUs, we still see a 2.5x disparity in memory consumption. Intel3 is a little ahead of RDNA4, at 2.4x larger BLAS vs NVIDIA.\n\nNow, this table presents each BLAS memory consumption as a function of the GPU - it’s clear that there’s some effect of the GPU generation on the memory consumption. However, another important contributing factor is the software, or more specifically the driver. For AMD, we can compare the results of the various driver releases during the last year, as well as an alternative driver, radv4, on the same GPU - Radeon 7900 GRE:\n\n      Driver (RDNA3)\n      BLAS size\n      Bytes/triangle\n\n      AMDVLK 2024.Q3\n      155 MB\n      88.4\n\n      AMDVLK 2024.Q4\n      105 MB\n      59.9\n\n      AMDVLK 2025.Q1\n      100 MB\n      57.0\n\n      radv (Mesa 25.0)\n      241 MB\n      137.4\n\nAs we can see, over the last 9 months, BLAS memory consumption on the same AMD GPU and the same driver codebase has progressively improved by 1.5x5, whereas if you use radv, your BLAS consumption is now 2.4x larger than official AMD drivers, not to mention the latest NVidia GPU6.\n\nWell… that’s certainly a lot of different numbers. Let’s try to make sense of at least some of them.\n\nMental model\n\nLet’s try to build some models to help us understand what we should expect. Is 100 MB good for 1.754M triangles? Is 241 MB bad? It’s time to talk about what a BVH actually is.\n\nFirst, let’s contextualize this with how much data we are feeding in. The way Vulkan / DX12 APIs work is that the application provides the driver with geometry description, which is either a flat list of triangles, or a vertex-index buffer pair. Unlike rasterization where a vertex may carry various attributes packed in the way the application wants, for raytracing you only specify a position per vertex, and the formats are more strictly specified. As mentioned above, in this case we are giving the driver fp16 data - this is important, because on fp32 data you will likely see different results and less drastic differences between vendors.7\n\nThe index buffer is your usual 32-bit or 16-bit data you would expect to see in rasterization; however, in most or maybe all cases, the index buffer is just a way to communicate your geometry to the driver - unlike rasterization, where efficiency of your index and vertex buffers is critical,  here the drivers would typically build the acceleration structure without regard to explicit indexing information.\n\nA flat triangle position list, then, would take 6 bytes per triangle corner * 3 corners per triangle * 1.754M triangles = 31.5 MB. This is not the most memory efficient storage: this scene uses 1.672M unique vertices, so using a 16-bit index buffer would require ~10 MB for vertex positions and ~10.5 MB for indices, and some meshlet compression schemes can go below that8; but regardless, our baseline for a not-very-efficient geometry storage can be in the neighborhood of 20-30 MB, or up to 18 bytes per triangle.\n\nA flat triangle list is not useful - the driver needs to build the acceleration structure that can be used to efficiently trace rays against. These structures are usually called “BVH” - bounding volume hierarchy - and represent a tree with a low branching factor where the intermediate nodes are defined as bounding boxes, and the leaf nodes store triangles. We will go over specific examples of this in the next section.\n\nTypically, you would want this structure to have high memory locality - when encountering a triangle in that data structure, you don’t want to have to reach for the triangle’s vertex data elsewhere in memory. In addition, Vulkan and DX12 allow to get access to the triangle id for ray hit (which must match the index of the triangle in the originally provided data); also, multiple mesh geometries can be combined in a single tree, and for ray tracing performance it’s uneconomical to separate the geometries into separate sub-trees, so the triangle information must also carry the geometry index. With all of this, we arrive at something like this:\n\nstruct BoxNode\n{\n\tfloat3 aabb_min[N];\n\tfloat3 aabb_max[N];\n\tuint32 children[N];\n};\n\nstruct TriangleNode\n{\n\tfloat3 corners[3];\n\tuint32 primid;\n\tuint32 geomid;\n};\n\nN is the branching factor; while any number between 2 (for a binary tree) and something exorbitantly large like 32 is possible in theory, in practice we should expect a small number that allows the hardware to test a reasonably small number of AABBs against a ray quickly; we will assume N=4 for now.9\n\nWith N=4 and fp32 coordinates everywhere, BoxNode is 112 bytes and TriangleNode is 44 bytes. If both structures use fp16 instead, we’d get 64 bytes for boxes and 26 bytes for triangles instead. We know (mostly…) how many triangle nodes we should have - one per input triangle - but how many boxes are there?\n\nWell, with a tree of branching factor 4, if we have 1.754M leaf nodes (triangles), we’d hope to get 1.754M/4 = 438K box nodes at the next level, 438K/4 = 109K at the next level, 109K/4 = 27K at the level after that, 27K/4 = 6.7K after that, and all the way until we reach the root - which gives us about 584K. If you don’t want to use boring division one step at a time, this is about a third as many box nodes as triangle nodes, which was discovered by Archimedes around 2250 years ago.\n\nConveniently, this means N triangles should take, approximately, N*sizeof(TriangleNode) + (N/3)*sizeof(BoxNode) memory, or sizeof(TriangleNode) + sizeof(BoxNode)/3 bytes per triangle. With fp32 coordinates this gives us ~81.3 bytes per triangle, and with fp16 it’s ~47.3.\n\nThis analysis is imprecise for a number of reasons. It ignores the potential for imbalanced trees (not all boxes may use 4 children for optimal spatial splits); it ignores various hardware factors like memory alignment and extra data; it assumes a specific set of node sizes; and it assumes the number of leaf (triangle) nodes is equal to the input triangle count. Let’s revisit these assumptions as we try to understand how BVHs actually work.\n\nradv\n\nSince the memory layout of a BVH is ultimately up to the specific vendor’s hardware and software and I don’t want to overly generalize this, let’s focus on AMD.\n\nAMD has a benefit of having multiple versions of their RDNA architecture - although there were no changes between RDNA2 and RDNA3 that would affect the memory sizes - and having documentation as well as open source drivers. Now, one caveat is that AMD actually does not properly document the BVH structure (the expected node memory layout should have been part of RDNA ISA, but it’s not - AMD, please fix this), but between the two open source drivers enough details should be available. By contrast, pretty much nothing is known about NVidia, but they clearly have a significant competitive advantage here so maybe they have something to hide.10\n\nThe way AMD implements ray tracing is as follows: the hardware units (“ray accelerators”) are accessible to shader cores as instructions that are similar to texture fetching; each instruction is given the pointer to a single BVH node and ray information, and can automatically perform ray-box or ray-triangle tests against all boxes or triangles in the node and return the results. The driver, then, is responsible for:\n\n  At build time, producing the BVH composed of nodes that match the HW format\n  At render time, building shader code that iterates over the tree, using the special instructions for node tests\n\nWhile the official documentation for RT formats is lacking, we do not have to reverse engineer this as we have two separate drivers with source code.\n\nradv, the unofficial driver which is the default on Linux and SteamOS, has very clean and easy to read code base, which defines the structures as follows:\n\nstruct radv_bvh_triangle_node {\n   float coords[3][3];\n   uint32_t reserved[3];\n   uint32_t triangle_id;\n   /* flags in upper 4 bits */\n   uint32_t geometry_id_and_flags;\n   uint32_t reserved2;\n   uint32_t id;\n};\n\nstruct radv_bvh_box16_node {\n   uint32_t children[4];\n   float16_t coords[4][2][3];\n};\n\nstruct radv_bvh_box32_node {\n   uint32_t children[4];\n   vk_aabb coords[4];\n   uint32_t reserved[4];\n};\n\nThese should be mostly self-explanatory (vk_aabb has 6 floats to represent min/max) and mostly maps to our earlier sketch. From this we can infer that RDNA GPUs support fp16/fp32 box nodes, but require full fp32 precision for triangle nodes. Additionally, triangle node here is 64 bytes, fp16 box node is 64 bytes, and fp32 box node is 128 bytes: maybe unsurprisingly, GPUs like things to be aligned and this is reflected in these structures.\n\nLooking closer at the source code, you can spot some additional memory that is allocated to store “parent links”: for each 64 bytes of the entire BVH, the driver allocates a 4-byte value, which will store the parent index of the node associated with this 64-byte chunk (due to alignment, every 64-byte aligned chunk is part of just one node). This is important for traversal: the shader uses a small stack for traversal that keeps the indices of the nodes that are currently being traversed, but that stack may not be sufficient for the full depth of large trees. To work around that, it’s possible to fall back to using these parent links - recursive traversal could be implemented in a completely stackless form, but reading the extra parent pointer from memory for every step would presumably be prohibitively expensive.\n\nAnother, more crucial, observation, is that at the time of this writing radv does not support fp16 box nodes - all box nodes emitted are fp32. As such, we can try to redo our previous analysis using radv structures:\n\n  64 bytes/triangle for triangle nodes\n  128 * 1/3 ~= 43 bytes/triangle for box nodes\n  (64 + 43) / 64 * 4 ~= 7 bytes/triangle for parent links\n\n… for a grand total of ~114 bytes/triangle we would expect from radv. Now, radv’s actual data is 137 bytes/triangle - 23 more bytes unaccounted for! This would be a good time to mention that while we would hope that the tree is perfectly balanced and the branching factor is, indeed, 4, in reality we would expect some amount of imbalance - both due to the nature of the algorithms that build these trees, that are highly parallel in nature and don’t always reach the optimum, and due to some geometry configurations just requiring somewhat uneven splits in parts of the tree for optimal traversal performance11.\n\nAMDVLK\n\nGiven that the hardware formats of the BVH nodes are fixed, it does not seem like there would be that much leeway in how much memory a BVH can take. With fp32 box nodes, we’ve estimated that BVH can take a minimum of 114 bytes/triangle on AMD hardware, and yet even the largest number we can see from the official driver was 88.4 bytes/triangle. What is going on here?\n\nIt’s time to consult the official AMD raytracing implementation. It is more or less what is running in both Windows and Linux versions of AMD’s driver; it should probably be taken as a definitive source, although unfortunately it’s quite a bit harder to follow than radv.\n\nIn particular, it does not contain C structure definitions for the BVH nodes: most of the code there is in HLSL and it uses individual field writes with macro offsets. That said, for RDNA2/3, we need to look at the triangle node more closely:\n\n// Note: GPURT limits triangle compression to 2 triangles per node. As a result the remaining bytes in the triangle node\n// are used for sideband data. The geometry index is packed in bottom 24 bits and geometry flags in bits 25-26.\n\n#define TRIANGLE_NODE_V0_OFFSET 0\n#define TRIANGLE_NODE_V1_OFFSET 12\n#define TRIANGLE_NODE_V2_OFFSET 24\n#define TRIANGLE_NODE_V3_OFFSET 36\n#define TRIANGLE_NODE_GEOMETRY_INDEX_AND_FLAGS_OFFSET 48\n#define TRIANGLE_NODE_PRIMITIVE_INDEX0_OFFSET         52\n#define TRIANGLE_NODE_PRIMITIVE_INDEX1_OFFSET         56\n#define TRIANGLE_NODE_ID_OFFSET 60\n#define TRIANGLE_NODE_SIZE      64\n\nSo it’s still 64 bytes; but what is this “NODE_V3” field, and what’s this triangle compression? Indeed, radv_bvh_triangle_node structure had a field uint32_t reserved[3]; right after coords array; it turns out that the 64-byte triangle node in AMD HW format can store up to 2 triangles instead of just one.\n\nAMD documentation refers to this as “triangle compression” or “pair compression”. The same concept can be seen in Intel’s hardware as “QuadLeaf”. In either case, the node can store two triangles that share an edge, which requires just 4 vertices. The triangles do not have to be coplanar; the hardware intersection engine will dutifully intersect the ray against both and return one or both intersection points as required.\n\nNow, this type of sharing is not always possible. For example, if the input consists of a triangle soup of disjointed triangles, then we will hit the worst case of one triangle per leaf node. And in some cases even if two triangles can be merged, if one of them is much larger doing so might compromise SAH metrics. However, generally speaking, we would expect a lot of triangles to be grouped up in pairs.\n\nThis changes our analysis pretty significantly:\n\n  Instead of 64 bytes/triangle for leaves, we only have 32 bytes/triangle\n  Since we have half as many leaves, we will also have half as many box nodes, for ~21 bytes/triangle\n  And the parent link cost is accordingly reduced by half as well, for ~4 bytes/triangle\n\nWhich brings up the total to 57 bytes/triangle… assuming ideal conditions: all triangles can be merged in pairs, all nodes have a branching factor of 4 (something we know is probably false based on radv results). In reality this is the configuration that AMD driver used to run in 2024.Q3 drivers, and it had 88 bytes/triangle - 31 bytes more than expected - which is probably a combination of more box nodes than we would expect, as well as less-than-perfect triangle pairing. Another quirk here is that AMDVLK driver implements what’s known as SBVH: individual triangles can be “split” across multiple BVH nodes, effectively appearing in the tree multiple times. This helps with ray tracing performance for long triangles, and may further skew our statistics as the number of triangles stored in leaf nodes may indeed be larger than the input provided!12\n\nradv does not implement either optimization at this time; importantly, in addition to this impacting memory consumption significantly, I would expect this also has a significant impact in ray tracing cost - indeed, my measurements indicate that radv is significantly slower on this scene than the official AMD driver, but that is a story for another time.\n\nNow, what happened in AMD’s 2024.Q4 release? If we trace the source changes closely (which is non-trivial as the commit structure is erased from source code dumps, but I’m glad we at least have as much!), it becomes obvious that what has happened is that fp16 box nodes are now enabled by default. Before this, box nodes used fp32 by default, and with that change many box nodes would use fp16 instead.\n\nThere are some specific conditions when this would happen - if you noticed from the radv structs, fp32 box nodes have one more reserved field and fp16 box nodes don’t - this field is actually used to store some extra information that may be deemed important on a per-node basis in some cases13. But regardless, the perfect configuration for RDNA2/3 system seems to be:\n\n  2 triangles per 64-byte leaf = 32 bytes/triangle\n  64-byte fp16 box * 1/3 * 1/2 = 11 bytes/triangle\n  4 bytes of parent links per 64b = 3 bytes/triangle\n\n… for a total of 46 bytes/triangle. This is the absolute best case and as we’ve seen before, it’s unrealistic to expect for complex geometry such as Bistro; the best results from the AMD driver use 57 bytes/triangle, 11 bytes/triangle more than the theoretical optimum.14\n\nWorth noting that 2025.Q1 release reduced the memory consumption from ~60 bytes/triangle to ~57 bytes/triangle. Since we know some amount of memory is lost in various inefficiencies of the resulting structure compared to the optimum, it might be possible to squeeze more juice from this in the future - but given that the hardware units expect a fixed format, and some amount of efficiency loss is inevitable if you need to maintain good tracing performance, the remaining gains are going to be limited.\n\nRDNA4\n\n… until the next hardware revision, that is.\n\nWhile RDNA3 kept the BVH format for RDNA2 for the most part (with some previously reserved bits now used for various culling flags but that’s a minor change that doesn’t affect memory consumption), RDNA4 appears to redesign the storage format completely. Presumably, all previous node types are still supported since radv works without changes, but gpurt implements two major new node types:\n\n  Quantized BVH8 node\n  Primitive node\n\nAs is clear from the name, BVH8 node stores 8 children; instead of using fp16 for box bounds, it stores the box corners in a special format15 with 12-bit mantissa and shared 8-bit exponent between all corners, plus a full fp32 origin corner. This adds up to 128 bytes - from the memory perspective it’s just as much as two fp16 BVH4 nodes from RDNA2/3, but it should permit the full fp32 range of bounding box values - fp16 box nodes could not represent geometry with coordinates outside of +-64K! - so I would expect that RDNA4 BVH data does not need to use any BVH4 nodes, and this allows AMD to embed other sorts of data into the box node, such as the OBB index for their new rotation support, and the parent pointer (which previously, as you recall, had to be allocated separately).\n\nstruct ChildInfo\n{\n    uint32_t minX         : 12;\n    uint32_t minY         : 12;\n    uint32_t cullingFlags : 4;\n    uint32_t unused       : 4;\n\n    uint32_t minZ         : 12;\n    uint32_t maxX         : 12;\n    uint32_t instanceMask : 8;\n\n    uint32_t maxY      : 12;\n    uint32_t maxZ      : 12;\n    uint32_t nodeType  : 4;\n    uint32_t nodeRange : 4;\n};\n\nstruct QuantizedBVH8BoxNode\n{\n    uint32_t internalNodeBaseOffset;\n    uint32_t leafNodeBaseOffset;\n    uint32_t parentPointer;\n    float3 origin;\n\n    uint32_t xExponent  : 8;\n    uint32_t yExponent  : 8;\n    uint32_t zExponent  : 8;\n    uint32_t childIndex : 4;\n    uint32_t childCount : 4;\n\n    uint32_t obbMatrixIndex;\n    ChildInfo childInfos[8];\n};\n\nPrimitive node is somewhat similar to triangle node, but it’s larger (128 bytes) and much more versatile: it can store a variable number of triangle pairs per node, and does that using what seems like a micro-meshlet format, where triangle pairs use vertex indices, with a separate section of the 128-byte packet storing the vertex positions - using a variable amount of bits per vertex for position storage.\n\nFor position storage, all bits inside coordinates for a single node are split into three parts: prefix (must be the same across all floats for the same axis), value, trailing zeroes; all parts have the same bit width for the same axis across all vertices in the node. For fp16 source positions, I would expect prefix storage to remove the initial segment of bits shared between the positions which would be close together in space, and most of the trailing fp32 bits to be zero. It would probably be reasonable to expect around 30-33 bits per vertex (3 * 10-bit mantissas, with most of the exponent bits shared and the trailing zeroes removed) with that setup on average.\n\nThe triangle pair vertex indices are encoded using 4 bits per index, with a few other bits used for other fields; primitive indices are stored as a delta from a single base value inside the primitive node, similarly to positions. Notably, the triangle pair has three independent indices for three corners per triangle - so it looks like the pair does not necessarily have to share the geometric edge, which presumably improves the efficiency at which geometry can be converted to this format at a small cost of 8 extra bits for every other triangle16. The number of pairs per node is limited to 8 pairs, or 16 triangles.\n\n  It may seem that this indexed storage format is at odds with what was mentioned earlier in the post: if the driver discards initial index buffer during BLAS construction, how can it use indices here? The answer is that the BVH construction proceeds as before, and some subtrees get packed into primitive nodes. During this packing, shared vertices are identified opportunistically using bitwise equality between vertex corners - so it does not matter if the source geometry was indexed or not, as long as the triangle corner positions are exactly equal.\n\nAll of this makes it difficult to correctly estimate the optimal storage capacity of such a node. With the limit of 16 triangles, we would ideally hope to be able to pack a 3x5 vertex grid (15 vertices, 8 quads)17. If ~30 bits per vertex for position storage is accurate, then 15 vertices will take 57 bytes of storage. With each triangle pair occupying 29 bits, 8 pairs would take 29 bytes, for a total of 86 bytes. A few additional bytes are required for headers, various anchors that are used to reconstruct positions and primitive indices, and a few bits per triangle for primitive index, assuming spatially coherent input triangles - which is probably reasonable to expect to fit. Thus, a dense mesh might be able to be packed into 16 triangles per node or ~8 bytes/triangle.\n\nSince BVH nodes are 8-wide, this also proportionally reduces the total expected number of box nodes, from 1/3 of primitive nodes to just 1/718. And given that the parent pointers are already embedded into box nodes, this gives us a best case theoretical bound of approximately:\n\n  128-byte primitive nodes with 16 triangles/node = 8 bytes/triangle\n  128-byte box nodes, 1/7th of 1/16th of triangles = 1.2 bytes/triangle\n\n… for a grand total of 9.2 bytes/triangle. Now, with the actual numbers of ~48 bytes/triangle, this is clearly a wildly unrealistic goal:\n\n  Even with BVH4, we have not seen anywhere near 4x branching factor on our test geometry in practice; achieving 8x without degrading BVH quality should be even harder\n  RDNA4 acceleration units can process eight box or two triangle intersections at once; a node with 16 triangles will thus be much more expensive to process vs a node with 2. This may mean the driver decides to limit the number of triangles in each leaf node artificially to maintain trace performance.\n  The description above is simplified assuming a similar high level tree structure to RDNA2/3, but in reality QBVH8 nodes can reference a subrange of a given primitive node; for example, one could imagine a single primitive node with 16 triangles, and a single QBVH8 node that, in each child, only references 2 of those triangles - which may be a different way to improve traversal performance. This means the box:triangle node ratio may be closer to 1:1 or 1:2 in practice, for 4-8 bytes/triangle instead of 1.2.\n\nBetween these factors, it’s hard to estimate the realistic expected memory consumption - but it seems plausible that we will see continued reduction of BVH sizes with future driver updates. Additionally, note that Bistro geometry has a lot of triangles with odd shapes and in general is not particularly uniform or dense. It’s possible that on denser meshes the effective bytes/triangle ratio is going to be closer to the theoretical optimum - exploring denser meshes is left as an exercise to the reader!\n\nConclusion\n\nHopefully you’ve enjoyed this whirlwind tour through the exciting world of hardware accelerated BVH storage! In summary, BVH memory consumption is highly hardware and driver specific: driver can only build a BVH out of nodes that the hardware ray accelerators natively understand, and this format varies with GPU generations; some drivers may only support a subset of hardware formats due to limited development time or tracing efficiency concerns; and specific algorithms used in the drivers for building the BVH will yield trees with different branching factors and leaf packing, which will greatly affect the results.\n\nIt will be interesting to revisit this topic in a year or so: AMD has made significant progress in both software and hardware in reducing their BVH structures, and while on RDNA2/3 it’s hard to see the BVH memory getting reduced by much, it’s not fully clear just how much headroom they have on RDNA4 depending on the scene. Similarly, it’s clear that NVidia has improved their internal hardware formats in 5xxx series, and it’s possible that there is some room left for the driver to optimize the storage further. Time will tell!\n\n      For the purpose of this analysis we will ignore TLAS; for this particular scene the memory costs of TLAS storage are very low - it only has a few hundred mesh instances; while this can be much larger in real games, I would expect BLAS storage to dominate.↩\n\n      Due to instancing, the amount of geometry present in the scene is larger - around 4M; be careful with that detail if you compare other Bistro variants to the numbers presented here, as they may not match.↩\n\n      These numbers were captured using official Intel drivers on Windows, not Mesa on Linux. I don’t have Intel’s Linux numbers handy, and don’t feel like re-plugging the GPU again.↩\n\n      radv is the default user-space driver for Linux systems, and the production driver for Steam Deck; all AMD measurements apart from the one explicitly listed below are taken from their official driver, AMDVLK, which is mostly the same between Windows/Linux.↩\n\n      I think the gaming community affectionately refers to this phenomenon as “AMD fine wine”.↩\n\n      Note that all of these numbers are not using NVidia’s latest clustered acceleration structures aka “mega geometry”; this is a subject for another post, but it doesn’t affect the analysis drastically.↩\n\n      And I’m not repeating all of this again for fp32. I would argue that fp32 is quite excessive for 99% of meshes in any game, and you need to be using either fp16 or snorm16 position components if you are trying to actually optimize the memory footprint of your game.↩\n\n      For example, a recently released AMD DGF SDK seems to primarily target position-only geometry, and as such might be useful for future AMD GPUs. It would just cover the geometry storage though, so we can’t use their numbers to estimate the future BVH cost; we also don’t know if this is even something that they plan to support in their RT cores.↩\n\n      For Intel GPUs it looks like N=6; for AMD GPUs, N=4 for RDNA2/3 and RDNA4 has a new N=8 node. Little is known about NVidia GPUs as usual.↩\n\n      I could have studied Intel GPUs more, as they do have an open source driver as part of Mesa; however, it’s unclear if their proprietary driver shares the same source, and in general I just was more interested in AMD when investigating this.↩\n\n      Curious readers are encouraged to explore this topic further; on AMD hardware, you can use Radeon Raytracing Analyzer to analyze the BVH as well as traversal efficiency characteristics for your workload.↩\n\n      In theory it should be possible to do further tests with AMDVLK driver to disambiguate this somewhat and/or patch the code to provide more statistics, but it’s 9 PM and I’d like to finish this post today if possible.↩\n\n      fp16 boxes are also naturally limited to the range of 16-bit floating point numbers; this would be a problem for some meshes with fp32 vertex coordinates, but it’s not an issue if the source vertex positions are also fp16.↩\n\n      It also should be noted that gpurt sources make vague references to larger-than-64 byte triangle nodes that contain more triangles; if that can result in using more shared edges than the optimum might be lower - but this also might refer to earlier hardware revisions that never materialized.↩\n\n      I have not studied this source code as extensively as the RDNA2/3 details, so all of this is an approximate description of what I can gather from skimming the code. Some details here are likely incorrect and/or missing.↩\n\n      The text here is written using “triangle pair” as this is how the code references these structures, but it’s unclear if there are any restrictions on packing - it may be that AMD kept the term for convenience, or maybe earlier versions of the format used a shared edge with a smaller descriptor, and they later introduced extra bits to decouple the triangles and didn’t rename the concept.↩\n\n      This math is similar to meshlet configurations described in an earlier post.↩\n\n      As a generalization of Archimedes formula, sum(1/k^i) = 1/(k-1)↩\n\n«Year of independence\n\ndocument.addEventListener(\"DOMContentLoaded\", function () {\n\tconst tooltip = document.querySelector(\".footnote-tooltip\");\n\n\tdocument.body.addEventListener(\"mouseover\", function (e) {\n\t\t// Check if the hovered element is a footnote link\n\t\tif (e.target.matches(\"a.footnote\") || e.target.closest(\"a.footnote\")) {\n\t\t\tconst ref = e.target.matches(\"a.footnote\")\n\t\t\t\t? e.target\n\t\t\t\t: e.target.closest(\"a.footnote\");\n\t\t\tconst footnoteId = ref.getAttribute(\"href\");\n\n\t\t\t// Find the target footnote content\n\t\t\tconst footnote = document.getElementById(footnoteId.substring(1));\n\n\t\t\tif (footnote) {\n\t\t\t\t// Get the footnote content (without the back link)\n\t\t\t\tconst content = footnote.querySelector(\"p\").cloneNode(true);\n\n\t\t\t\t// Remove the back link from the cloned content\n\t\t\t\tconst backLink = content.querySelector(\".reversefootnote\");\n\t\t\t\tif (backLink) {\n\t\t\t\t\tbackLink.remove();\n\t\t\t\t}\n\n\t\t\t\t// Position and show the tooltip\n\t\t\t\ttooltip.innerHTML = content.innerHTML;\n\t\t\t\ttooltip.style.display = \"block\";\n\n\t\t\t\t// Position the tooltip near the footnote reference\n\t\t\t\tconst refRect = ref.getBoundingClientRect();\n\t\t\t\ttooltip.style.left = `${refRect.left}px`;\n\t\t\t\ttooltip.style.top = `${refRect.bottom + window.scrollY + 5}px`;\n\t\t\t}\n\t\t}\n\t});\n\n\tdocument.body.addEventListener(\"mouseout\", function (e) {\n\t\t// Check if we're leaving a footnote link\n\t\tif (e.target.matches(\"a.footnote\") || e.target.closest(\"a.footnote\")) {\n\t\t\t// Only hide if we're not entering the tooltip itself\n\t\t\tconst relatedTarget = e.relatedTarget;\n\t\t\tif (!relatedTarget || !tooltip.contains(relatedTarget)) {\n\t\t\t\ttooltip.style.display = \"none\";\n\t\t\t}\n\t\t}\n\t});\n});\n\n\tShow Comments\n\n    var disqus_shortname = 'zeuxcg';\n    var disqus_url = \"https://zeux.io/2025/03/31/measuring-acceleration-structures/\";\n\tvar disqus_loaded = false;\n\n\tfunction disqus() {\n\t\tif (disqus_loaded) return;\n\t\tdisqus_loaded = true;\n\n\t\t/* * * DON'T EDIT BELOW THIS LINE * * */\n\t\tvar dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;\n\t\tdsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';\n\t\t(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);\n\t}",
    "summary": {
      "en": "### Summary of Ray Tracing Acceleration Structures\n\n**Overview**:  \nBy March 31, 2025, hardware-accelerated ray tracing will depend on a data structure called \"acceleration structure,\" often referred to as \"BVH\" (Bounding Volume Hierarchy) or \"BLAS\" (Bottom Level Acceleration Structure). These structures are crucial for efficiently rendering complex scenes with ray tracing.\n\n**Key Points**:\n\n1. **Acceleration Structures**:\n   - They store scene geometry and are essential for ray traversal in rendering engines.\n   - Unlike textures, their layout is not standardized across different hardware vendors.\n\n2. **Experimental Setup**:\n   - The study focused on a specific scene, the Amazon Lumberyard Bistro, using various GPUs to measure memory consumption and performance.\n   - The results were gathered using the niagara renderer, requiring Vulkan SDK and drivers.\n\n3. **Results**:\n   - Different GPUs showed significant variance in BLAS size and memory consumption:\n     - AMD GPUs ranged from 33 MB to 100 MB for BLAS size.\n     - NVIDIA GPUs had sizes from 45 MB to 46 MB.\n     - Intel's B580 GPU was around 79 MB.\n   - The memory consumption varied significantly, showing a 3x difference between earlier AMD and latest NVIDIA GPUs.\n\n4. **Driver Influence**:\n   - The driver version affects memory consumption, with AMD's drivers improving BLAS size over time.\n   - Different AMD driver versions yielded BLAS sizes from 155 MB to 100 MB, indicating substantial improvements.\n\n5. **Memory Efficiency**:\n   - An analysis of memory usage suggested that, ideally, BLAS sizes should be around 57 bytes per triangle, but actual sizes were often higher due to inefficiencies in tree structures and triangle packing.\n\n6. **Future Hardware Developments**:\n   - Upcoming RDNA4 architecture introduces new node types aimed at improving memory efficiency and ray tracing performance.\n   - Continuous improvements in drivers and hardware may lead to reduced memory requirements and better performance.\n\n**Conclusion**:  \nThe performance and memory efficiency of ray tracing acceleration structures are heavily dependent on both the GPU hardware and the drivers. Ongoing developments in these areas are expected to yield further optimizations in the future.",
      "ko": "2025년 3월 31일까지 하드웨어 가속 레이 트레이싱은 \"가속 구조\"라는 데이터 구조에 의존하게 됩니다. 이 구조는 일반적으로 \"BVH\" (경계 볼륨 계층) 또는 \"BLAS\" (하위 레벨 가속 구조)로 불리며, 복잡한 장면을 효율적으로 렌더링하는 데 필수적입니다.\n\n가속 구조는 장면의 기하학적 정보를 저장하며, 렌더링 엔진에서 레이를 탐색하는 데 중요한 역할을 합니다. 텍스처와는 달리, 이러한 구조의 배치는 다양한 하드웨어 제조업체 간에 표준화되어 있지 않습니다.\n\n이번 연구는 아마존 럼버야드 비스트로라는 특정 장면을 중심으로 진행되었으며, 여러 GPU를 사용해 메모리 소비와 성능을 측정했습니다. 결과는 나이아가라 렌더러를 통해 수집되었으며, 이를 위해 Vulkan SDK와 드라이버가 필요했습니다.\n\n다양한 GPU에서 BLAS 크기와 메모리 소비에 상당한 차이가 나타났습니다. AMD GPU는 BLAS 크기가 33MB에서 100MB까지 다양했고, NVIDIA GPU는 45MB에서 46MB로 나타났습니다. 인텔의 B580 GPU는 약 79MB였습니다. 메모리 소비는 크게 차이가 나며, 이전 AMD GPU와 최신 NVIDIA GPU 간에는 3배의 차이가 있었습니다.\n\n드라이버 버전은 메모리 소비에 영향을 미치며, AMD의 드라이버는 시간이 지남에 따라 BLAS 크기를 개선했습니다. 서로 다른 AMD 드라이버 버전은 BLAS 크기가 155MB에서 100MB까지 다양하게 나타나, 상당한 개선이 있었음을 보여줍니다.\n\n메모리 사용 분석 결과, 이상적으로 BLAS 크기는 삼각형당 약 57바이트가 되어야 하지만, 실제 크기는 트리 구조와 삼각형 포장 방식의 비효율성으로 인해 종종 더 높았습니다.\n\n앞으로 출시될 RDNA4 아키텍처는 메모리 효율성과 레이 트레이싱 성능을 개선하기 위한 새로운 노드 유형을 도입할 예정입니다. 드라이버와 하드웨어의 지속적인 개선은 메모리 요구 사항을 줄이고 성능을 향상시킬 것으로 기대됩니다.\n\n레이 트레이싱 가속 구조의 성능과 메모리 효율성은 GPU 하드웨어와 드라이버에 크게 의존합니다. 이 분야의 지속적인 발전은 앞으로 더 많은 최적화를 가져올 것으로 예상됩니다.",
      "ja": "2025年3月31日までに、ハードウェアによるレイトレーシングの加速は「アクセラレーション構造」と呼ばれるデータ構造に依存することになります。これは一般的に「BVH」（バウンディングボリュームヒエラルキー）や「BLAS」（ボトムレベルアクセラレーション構造）として知られています。これらの構造は、レイトレーシングを用いて複雑なシーンを効率的にレンダリングするために重要です。\n\nアクセラレーション構造は、シーンのジオメトリを保存し、レンダリングエンジンでのレイのトラバースに不可欠です。ただし、テクスチャとは異なり、異なるハードウェアベンダー間でのレイアウトは標準化されていません。\n\nこの研究では、特定のシーンであるアマゾン・ランバー・ヤード・ビストロを使用し、さまざまなGPUを用いてメモリ消費量とパフォーマンスを測定しました。結果は、Vulkan SDKとドライバーを必要とするナイアガラレンダラーを使用して収集されました。\n\n異なるGPUでは、BLASのサイズとメモリ消費量に大きなばらつきが見られました。AMDのGPUはBLASサイズが33MBから100MBの範囲で、NVIDIAのGPUは45MBから46MBでした。インテルのB580 GPUは約79MBでした。メモリ消費量は大きく異なり、古いAMDと最新のNVIDIA GPUの間で3倍の差がありました。\n\nドライバーのバージョンはメモリ消費に影響を与え、AMDのドライバーは時間とともにBLASサイズを改善しています。異なるAMDドライバーのバージョンでは、BLASサイズが155MBから100MBに変化し、かなりの改善が見られました。\n\nメモリ使用量の分析によると、理想的にはBLASサイズは三角形あたり約57バイトであるべきですが、実際のサイズはツリー構造や三角形のパッキングの非効率性により、しばしばそれ以上になっています。\n\n今後のRDNA4アーキテクチャでは、メモリ効率とレイトレーシング性能を向上させるための新しいノードタイプが導入される予定です。ドライバーやハードウェアの継続的な改善により、メモリ要件の削減やパフォーマンスの向上が期待されます。レイトレーシングのアクセラレーション構造の性能とメモリ効率は、GPUハードウェアとドライバーの両方に大きく依存しています。これらの分野での進展は、今後さらなる最適化をもたらすと考えられています。"
    }
  },
  {
    "id": "e424b6b36b0585da",
    "title": {
      "en": "Certification Authority/Browser Forum adopts new security standards",
      "ko": "인증기관, 새로운 보안 기준 채택",
      "ja": "新セキュリティ基準採用"
    },
    "type": "story",
    "url": "https://security.googleblog.com/2025/03/new-security-requirements-adopted-by.html",
    "score": 33,
    "by": "terminalbraid",
    "time": 1743415015,
    "content": "New security requirements adopted by HTTPS certificate industry\n\nMarch 27, 2025\n\n                          Posted by Chrome Root Program, Chrome Security Team\n\nThe Chrome Root Program launched in 2022 as part of Google’s ongoing commitment to upholding secure and reliable network connections in Chrome. We previously described how the Chrome Root Program keeps users safe, and described how the program is focused on promoting technologies and practices that strengthen the underlying security assurances provided by Transport Layer Security (TLS). Many of these initiatives are described on our forward looking, public roadmap named “Moving Forward, Together.”\n\nAt a high-level, “Moving Forward, Together” is our vision of the future. It is non-normative and considered distinct from the requirements detailed in the Chrome Root Program Policy. It’s focused on themes that we feel are essential to further improving the Web PKI ecosystem going forward, complementing Chrome’s core principles of speed, security, stability, and simplicity. These themes include:\n\nEncouraging modern infrastructures and agility\n\nFocusing on simplicity\n\nPromoting automation\n\nReducing mis-issuance\n\nIncreasing accountability and ecosystem integrity\n\nStreamlining and improving domain validation practices\n\nPreparing for a \"post-quantum\" world\n\nEarlier this month, two “Moving Forward, Together” initiatives became required practices in the CA/Browser Forum Baseline Requirements (BRs). The CA/Browser Forum is a cross-industry group that works together to develop minimum requirements for TLS certificates. Ultimately, these new initiatives represent an improvement to the security and agility of every TLS connection relied upon by Chrome users.\n\nIf you’re unfamiliar with HTTPS and certificates, see the “Introduction” of this blog post for a high-level overview.\n\nMulti-Perspective Issuance Corroboration\n\nBefore issuing a certificate to a website, a Certification Authority (CA) must verify the requestor legitimately controls the domain whose name will be represented in the certificate. This process is referred to as \"domain control validation\" and there are several well-defined methods that can be used. For example, a CA can specify a random value to be placed on a website, and then perform a check to verify the value’s presence has been published by the certificate requestor.\n\nDespite the existing domain control validation requirements defined by the CA/Browser Forum, peer-reviewed research authored by the Center for Information Technology Policy (CITP) of Princeton University and others highlighted the risk of Border Gateway Protocol (BGP) attacks and prefix-hijacking resulting in fraudulently issued certificates. This risk was not merely theoretical, as it was demonstrated that attackers successfully exploited this vulnerability on numerous occasions, with just one of these attacks resulting in approximately $2 million dollars of direct losses.\n\nMulti-Perspective Issuance Corroboration (referred to as \"MPIC\") enhances existing domain control validation methods by reducing the likelihood that routing attacks can result in fraudulently issued certificates. Rather than performing domain control validation and authorization from a single geographic or routing vantage point, which an adversary could influence as demonstrated by security researchers, MPIC implementations perform the same validation from multiple geographic locations and/or Internet Service Providers. This has been observed as an effective countermeasure against ethically conducted, real-world BGP hijacks.\n\nThe Chrome Root Program led a work team of ecosystem participants, which culminated in a CA/Browser Forum Ballot to require adoption of MPIC via Ballot SC-067. The ballot received unanimous support from organizations who participated in voting. Beginning March 15, 2025, CAs issuing publicly-trusted certificates must now rely on MPIC as part of their certificate issuance process. Some of these CAs are relying on the Open MPIC Project to ensure their implementations are robust and consistent with ecosystem expectations.\n\nWe’d especially like to thank Henry Birge-Lee, Grace Cimaszewski, Liang Wang, Cyrill Krähenbühl, Mihir Kshirsagar, Prateek Mittal, Jennifer Rexford, and others from Princeton University for their sustained efforts in promoting meaningful web security improvements and ongoing partnership.\n\nLinting\n\nLinting refers to the automated process of analyzing X.509 certificates to detect and prevent errors, inconsistencies, and non-compliance with requirements and industry standards. Linting ensures certificates are well-formatted and include the necessary data for their intended use, such as website authentication.\n\nLinting can expose the use of weak or obsolete cryptographic algorithms and other known insecure practices, improving overall security. Linting improves interoperability and helps CAs reduce the risk of non-compliance with industry standards (e.g., CA/Browser Forum TLS Baseline Requirements). Non-compliance can result in certificates being \"mis-issued\". Detecting these issues before a certificate is in use by a site operator reduces the negative impact associated with having to correct a mis-issued certificate.\n\nThere are numerous open-source linting projects in existence (e.g., certlint, pkilint, x509lint, and zlint), in addition to numerous custom linting projects maintained by members of the Web PKI ecosystem. “Meta” linters, like pkimetal, combine multiple linting tools into a single solution, offering simplicity and significant performance improvements to implementers compared to implementing multiple standalone linting solutions.\n\nLast spring, the Chrome Root Program led ecosystem-wide experiments, emphasizing the need for linting adoption due to the discovery of widespread certificate mis-issuance. We later participated in drafting CA/Browser Forum Ballot SC-075 to require adoption of certificate linting. The ballot received unanimous support from organizations who participated in voting. Beginning March 15, 2025, CAs issuing publicly-trusted certificates must now rely on linting as part of their certificate issuance process.\n\nWhat’s next?\n\nWe recently landed an updated version of the Chrome Root Program Policy that further aligns with the goals outlined in “Moving Forward, Together.” The Chrome Root Program remains committed to proactive advancement of the Web PKI. This commitment was recently realized in practice through our proposal to sunset demonstrated weak domain control validation methods permitted by the CA/Browser Forum TLS Baseline Requirements. The weak validation methods in question are now prohibited beginning July 15, 2025.\n\nIt’s essential we all work together to continually improve the Web PKI, and reduce the opportunities for risk and abuse before measurable harm can be realized. We continue to value collaboration with web security professionals and the members of the CA/Browser Forum to realize a safer Internet. Looking forward, we’re excited to explore a reimagined Web PKI and Chrome Root Program with even stronger security assurances for the web as we navigate the transition to post-quantum cryptography. We’ll have more to say about quantum-resistant PKI later this year.\n\n                        Google\n\nNo comments\n                      :\n\nPost a Comment\n\n                      \n\n                      \n\n                          ",
    "summary": {
      "en": "**Summary of New HTTPS Security Requirements**\n\nOn March 27, 2025, the Chrome Root Program announced new security measures for HTTPS certificates aimed at improving online safety. These changes are part of a broader initiative called \"Moving Forward, Together,\" which focuses on enhancing the security of web connections.\n\nKey points include:\n\n1. **New Requirements**: Two new practices, Multi-Perspective Issuance Corroboration (MPIC) and linting, have been mandated for Certification Authorities (CAs) by the CA/Browser Forum, effective March 15, 2025.\n\n2. **Multi-Perspective Issuance Corroboration (MPIC)**: This method improves the verification process for domain control to prevent fraud. It requires CAs to check domain control from multiple locations to reduce the risk of attacks that could lead to the issuance of fake certificates.\n\n3. **Linting**: This automated process checks X.509 certificates for errors and compliance with industry standards. It helps identify weak security practices and ensures certificates are correctly formatted, reducing the chances of mis-issuance.\n\n4. **Future Plans**: The Chrome Root Program is committed to ongoing improvements in web security and will phase out weak validation methods by July 15, 2025. They are also preparing for advancements in post-quantum cryptography.\n\nThese initiatives aim to make the web safer for everyone by enhancing the reliability of HTTPS certificates and the underlying security protocols.",
      "ko": "2025년 3월 27일, 크롬 루트 프로그램은 HTTPS 인증서에 대한 새로운 보안 조치를 발표했습니다. 이 조치는 온라인 안전성을 높이기 위한 것으로, \"함께 나아가기\"라는 더 넓은 이니셔티브의 일환입니다.\n\n주요 내용은 다음과 같습니다. 첫째, 새로운 요구 사항이 도입되었습니다. 다중 관점 발급 확인(MPIC)과 린팅이라는 두 가지 새로운 절차가 인증 기관(CA)에 의해 의무화되었습니다. 이 조치는 2025년 3월 15일부터 시행됩니다.\n\n다중 관점 발급 확인(MPIC)은 도메인 제어를 검증하는 과정을 개선하여 사기를 방지하는 방법입니다. 이 절차는 CA가 여러 위치에서 도메인 제어를 확인하도록 요구하여, 가짜 인증서 발급으로 이어질 수 있는 공격의 위험을 줄입니다.\n\n린팅은 자동화된 과정으로, X.509 인증서의 오류와 산업 표준 준수를 검사합니다. 이를 통해 보안 취약점을 식별하고 인증서가 올바르게 형식화되었는지 확인하여 잘못된 발급 가능성을 줄입니다.\n\n미래 계획으로는 크롬 루트 프로그램이 웹 보안 개선을 지속적으로 추진하며, 2025년 7월 15일까지 약한 검증 방법을 단계적으로 폐지할 예정입니다. 또한, 포스트 양자 암호화 기술의 발전을 준비하고 있습니다.\n\n이러한 이니셔티브는 HTTPS 인증서와 그 기반 보안 프로토콜의 신뢰성을 높여 모두에게 더 안전한 웹 환경을 만드는 것을 목표로 하고 있습니다.",
      "ja": "2025年3月27日、ChromeルートプログラムはHTTPS証明書に関する新しいセキュリティ対策を発表しました。これらの変更は「共に前進する」という広範な取り組みの一環で、ウェブ接続のセキュリティ向上を目指しています。\n\n主なポイントは以下の通りです。まず、新たに2つの実施方法が認証機関（CA）に義務付けられました。これらは「マルチパースペクティブ発行確認（MPIC）」と「リンティング」で、2025年3月15日から施行されます。\n\nマルチパースペクティブ発行確認（MPIC）は、ドメインの管理権限を確認するプロセスを改善し、詐欺を防ぐための方法です。CAは、複数の場所からドメインの管理権限を確認する必要があり、これにより偽の証明書が発行されるリスクを減少させます。\n\nリンティングは、自動化されたプロセスで、X.509証明書のエラーや業界標準への準拠をチェックします。これにより、セキュリティの弱点を特定し、証明書が正しくフォーマットされていることを確認することで、誤って発行される可能性を減らします。\n\n今後の計画として、Chromeルートプログラムはウェブセキュリティの継続的な改善に取り組み、2025年7月15日までに弱い検証方法を段階的に廃止する予定です。また、ポスト量子暗号技術の進展にも備えています。\n\nこれらの取り組みは、HTTPS証明書とその基盤となるセキュリティプロトコルの信頼性を高めることで、誰もが安全にウェブを利用できるようにすることを目指しています。"
    }
  },
  {
    "id": "60779711d089623a",
    "title": {
      "en": "Windows on ARM on a Smart Watch",
      "ko": "ARM 스마트워치의 윈도우",
      "ja": "腕時計のウィンドウズ"
    },
    "type": "story",
    "url": "https://gus33000.me/fish/",
    "score": 15,
    "by": "todsacerdoti",
    "time": 1743476767,
    "content": "⬅️ Go back\nWindows on ARM (ARM as defined by the cambridge dictionary, not the cambridge based firm)\nLast updated: 3/31/2025 - 10:57 PM UTC+2\n\nEDIT 4/1/2025 - 10:20 PM UTC+2: This is/was serious, and is not a joke. This is UEFI on Pixel Watch 3 with ability to boot Windows and Linux. See mu_seluna_platforms if you are interested in the code making this *tick* (badum tss)\n\nTL;DR: Pixel Watch Guides on WOA-Project Github organisation\nAlso there is a section at the bottom outlining other things I am releasing at the end of this week. Scroll down if you do not care about the time 😉\n\nMany years of spending, sorry, losing time making Windows on ARM run on anything not a computer, I started getting bored\nWhat could be the next project really? Years of wondering about, diverging into foldables out of all things, I still couldnt let go of my head that something else could be done, something more, evident, something more stupid, sorry, something more useful. Something I had not figured out yet. Thousands of hours rambling online, including with old people, I still couldnt work it out...\n...and then it came to me\n\nThe answer was ALWAYS under my eyes. Yes! Making windows run on an ARM! To be more exact, on a smartwatch\nHow did I never think of this before!\nAnd so can you! Yes, for real, see Pixel Watch Guides on WOA-Project Github organisation\nAnd what is here not to love about smartwatches in 2025? Afterall, they are as powerful if not more than some devices you own. Here's a small description of the real estate we are even dealing with:\n\n    Quad core sixty four bits ARM Cortex A53 cluster\n    2 whole GBs of DDR4X RAM\n    32GB of eMMC storage\n    Adreno 702 Graphics (this thing even has raytracing!) clocked at a whoping 1Ghz (perfect for gaming on the go!)\n    456x456 OLED Coated Curved (fancy! not even phones do that anymore) and ✨ Rounded ✨ display (wow!)\n    More rounded than your iPhone\n    Ultra high speed LTE modem onboard (on some models only)\n    USB. Yes, your charger does USB\n    Audio\n    High end WiFi 6E and Bluetooth over SDIO (2003 called they want their PDA expansion tech back)\n    Calling support\n    it texts\n    Ultra advanced Sensor Hub (good for your health) (the writer of this post def needs to get help in that area)\n    A whole secondary coprocessor for deep sleep and less power usage! (somehow, it removed my ability to sleep last weekend)\n    More battery life than you can ever dream (on the stock software obviously)\n    can touch this, world first circular touch screen\n    it smol\n    cute\n    its a freaking watch\n    has all the functionality you have on your phone. So why wasting your time on phones, get watch, watch is the superior technology, not fish\n\nGetting Windows to run on your ARM\n\nNow that the story line is laid out (see above), let's get a victim, sorry, test subject (we will pretend for the sake of the writer sanity this whole thing did not start just because of the Windows on ARM pun)\nChosen was the Google Pixel Watch 3 (Large LTE Model) that came out late last year, notably because thats what I had, and also because it featured everything my 2 grand USD phone had, for only a quarter of the price (I did tell you why are we bothering with expensive phones)\nThe watch runs on Android 15 (originally Android 14 but more on that later), has an unlockable bootloader, and unfortunately features Google conception of reinventing how to boot linux yearly (more on that later too)\n\nMy newly obtained watch on the very first day of ownership, doing what every single normal user would do, mess with it\n\nThe watch uses a Snapdragon W5 Gen 1 (Part number SW5100) combined with 2GB of DDR4X ram on 32GB of eMMC storage\nThis is more than enough for a smartwatch, let's face it, but also enough for us. It using a Qualcomm chipset means i could reuse all my experience messing with previous hardware using Qualcomm chipsets easily and tap onto easily accessible opensource code repos (including, Google own)\nAs a modern Qualcomm platform, roughly based on Kona era firmware (Kona is the Snapdragon 865), it also is familiar territory for the firmware side of things.\n\nImage Credit: Qualcomm Technologies, Inc.\n\nThe SoC is also making use of old but proven Cortex-A53 cores, 4 of them to be exact. This is a bit on the older side (Cortex A53 was designed back in 2011) and wont run modern OSes needing atomics, but this is enough for our fun experiments here.\nModern Qualcomm firmware, means we are already dealing with an UEFI here, in XBL (Qualcomm own eXtensible Boot Loader). So armed with the watch of all of this knowledge the very first step has been to backup the watch using a rooted boot image, and inspect.\n\nThis is not fun\n\nThen doing the very same steps of, painfully taking back all EFI files from the stock uefi, and adapting them to run in my own UEFI environment. We suddenly, are in UEFI land.\n\nAfter hours of patching and figuring out correct load order (apriori was all wrong somehow) we are at a shell!\n\nAnd some careful adjustments of UEFI code because turns out having such a low resolution is out of the UEFI specifications (who knew)\n\nThen followed a few hours of ACPI table editing, by taking known ones for 8250 and patching in the right timer/GIC information as found on the native device tree files:\n\nThis is not that hard, once you know where to look at, just mostly copy pasting, for a few tables.\n\n...and a dummy test DSDT ACPI Table, with only 4 CPUs:\n\nWe only support CPUs, yep, we are not going further than that today...\n\n... we finally go into an OS!\n\nYes this is just Windows PE and very basic and I havent even bothered to wait for it to fully load. But it works.\n\nExcept, it is not that simple, it never is.\nThis worked fine, but I missed on some details and havent laid out the fun that happened last weekend. Because, thats where I was months ago. For once, I did preparations.\n\nThe first obvious unknown is how did I even get Windows PE on here, its 32GB of storage, and everything is maxed out on the watch for sure right?\n\nYes, but also take a look at the partition layout:\n\nThis is a modern Google product, so it makes use of the super partition, two slots (a and b), and more bleeding edge formats (such as boot image header v4)\n\nI do not exactly want to mess with my watch (I kinda love it for being a watch more than shit and giggles afterall)\n\nUsing Windows PE was the obvious choice (Windows in a ramdisk) (and so was linux later too as I wanted to try and yeah that works if you want to)\nBut I still need a place to put all files, and it is not in my limited in max capacity uefi image that I am going to fit it in\n\nThis being a dual slot device, I abused the unused current slot (a for me) to override the biggest slot partition I could (modem_a) with my own boot files. This required a bit of Windows PE trimming, because this is at best 150MB in size, but it works.\n\nThis solves the demo problem. But remember what I said earlier about Google?\n\nA few days ago preparing to actually release this disaster, sorry again words slipped from my mouth, marvel of technology, I wanted to rebase it on the latest firmware available for the watch.\n\nTurns out Google had just released Android 15 to the watch (it was using Android 14 before). Let's update, what could go wrong afterall....\n\nNarrator: everything\n\nIn short, Google web flashing utility died on me twice, had to restart it. Everytime it took over *4 hours* (this is so slow). And once it finished (at 10 PM on saturday night), I thought i would try my last known good UEFI, and it did not work....\n\nTurns out somewhere between my old experiments and now their ABL code updated and now it was claiming my uefi (kernel for its log) was too small...?\n\nwhat?\n\nAfter hours of trying all sorts of things, including modifying the normal kernel image from the device, I eventually discovered somehow this only accepted the very stock kernel image header, which is not ideal for me as it contains jump instructions. Without wanting to spend more on this than I already did, I just took that kernel header, stripped all of its code, padded it to the right amount so its unmodified jump instruction reached the end of that file, and appended my uefi to it. We got back into business\n\nBonus\n\nOn sunday I thought I would finish some of my implementation, like making USB to work, the short story is that it kinda works but only for mass storage. But at least now you can backup and mess with the storage using the guide I wrote leveraging Windows phone's developermenu, before you had to root wearOS and all, which is not that straight forward. USB required just a few simple IOMMU domain attachment patches because they were already attached from the previous firmware. As for why it still crashes for other use cases (like my own linuxloader or ufp), I do not know.\n\nMass Storage on watch, complete with the best alignment of text I have ever seen.\n\nConcluding notes\n\nYes this is an april fools project, and like with every april fools project I personally do its real and goofy and its not finished either. You can try it now. But honestly, you really should not lol.\nThis has been uniquely fun for me to work on, and this blog post was also done at the last minute in the very same kind of fun and goofy way. This is not intended to be the best seller of books in writing either. I hope for someone on this planet this experiment has been sort of, useful. For everyone else, I hope you are going to have a good day.\n\nPS: I am going to release a few new updates for Surface Duo later this week, also, here is a round up of everything planned for early april on top of, Windows on Arm 😉:\n\n    USB-C DisplayPort/Video out for Surface Duo 1 on Windows\n    Surface Duo 2 Driver update (general housekeeping)\n    Basic Linux images for Surface Duo and Surface Duo 2 (using our UEFI for Windows too)\n\nand some of the work already done needing no special release\n\n    Making windows work on Snapdragon 820, 835, 865, 670 (Some of those have extra driver support, its coming slowly but surely)\n\nI would like to thank Rafael Rivera for allowing me to use his \"Windows on ARM\" artpiece for this meme boot logo.",
    "summary": {
      "en": "**Summary: Windows on ARM for Pixel Watch 3**\n\nA developer has successfully made Windows run on the Pixel Watch 3, marking a significant achievement in adapting Windows for non-traditional devices. This project is part of a broader exploration of running Windows on ARM architecture. \n\n**Key Points:**\n\n- The Pixel Watch 3 is equipped with a powerful Snapdragon W5 chip, 2GB of RAM, and 32GB of storage, making it capable of running more than just smartwatch software.\n- The developer engaged in extensive experimentation, modifying the UEFI (Unified Extensible Firmware Interface) to boot Windows and Linux on the watch.\n- Despite challenges, including adapting the boot process and handling firmware updates, the developer managed to run Windows PE (Preinstallation Environment) on the watch.\n- The project, while functional, is described as an April Fools' joke—it's real yet playful and not fully polished.\n- Additional updates for other devices, like Surface Duo, are planned to be released soon.\n\nOverall, this experiment highlights the potential of smartwatches as powerful computing devices beyond their traditional use.",
      "ko": "한 개발자가 픽셀 워치 3에서 윈도우를 성공적으로 실행시켜, 비전통적인 장치에서 윈도우를 적응시키는 데 중요한 이정표를 세웠습니다. 이 프로젝트는 ARM 아키텍처에서 윈도우를 실행하는 더 넓은 탐색의 일환입니다.\n\n픽셀 워치 3는 강력한 스냅드래곤 W5 칩, 2GB의 RAM, 32GB의 저장 공간을 갖추고 있어 스마트워치 소프트웨어 이상의 기능을 수행할 수 있습니다. 개발자는 UEFI(통합 확장 가능 펌웨어 인터페이스)를 수정하여 시계에서 윈도우와 리눅스를 부팅하는 실험을 광범위하게 진행했습니다. 부팅 과정 조정과 펌웨어 업데이트 처리 등 여러 도전 과제가 있었지만, 개발자는 시계에서 윈도우 PE(사전 설치 환경)를 실행하는 데 성공했습니다.\n\n이 프로젝트는 기능적으로 작동하지만, 만우절 농담으로 묘사되며, 실제로 존재하지만 장난스럽고 완전히 다듬어지지는 않았습니다. 서피스 듀오와 같은 다른 장치에 대한 추가 업데이트도 곧 출시될 예정입니다.\n\n이 실험은 스마트워치가 전통적인 용도를 넘어 강력한 컴퓨팅 장치로서의 잠재력을 보여줍니다.",
      "ja": "開発者がPixel Watch 3でWindowsを動作させることに成功し、非伝統的なデバイス向けにWindowsを適応させる重要な成果を達成しました。このプロジェクトは、ARMアーキテクチャ上でWindowsを動かすことを探求する広範な取り組みの一環です。\n\nPixel Watch 3は、強力なSnapdragon W5チップ、2GBのRAM、32GBのストレージを搭載しており、スマートウォッチ用のソフトウェア以上のものを実行する能力があります。開発者は、UEFI（統一拡張ファームウェアインターフェース）を改造して、ウォッチ上でWindowsやLinuxを起動するための広範な実験を行いました。\n\nブートプロセスの適応やファームウェアの更新に関する課題があったものの、開発者はWindows PE（プレインストール環境）をウォッチ上で動作させることに成功しました。このプロジェクトは機能的ではあるものの、エイプリルフールのジョークとして説明されており、実際には存在するものの、完全に洗練されたものではありません。\n\nまた、Surface Duoなどの他のデバイス向けの追加アップデートも近日中にリリースされる予定です。この実験は、スマートウォッチが従来の用途を超えた強力なコンピューティングデバイスとしての可能性を示しています。"
    }
  },
  {
    "id": "a5e248043cf8eebd",
    "title": {
      "en": "Publishers trial paying peer reviewers – what did they find?",
      "ko": "동료 심사자 유료화 실험 결과는?",
      "ja": "査読者報酬の試み結果"
    },
    "type": "story",
    "url": "https://www.nature.com/articles/d41586-025-00968-6",
    "score": 12,
    "by": "xqcgrek2",
    "time": 1743275290,
    "content": "NEWS\n                28 March 2025\n\n            Publishers trial paying peer reviewers — what did they find?\n\n                    Two journals embarked on efforts to compensate reviewers, with different results.\n\n                By\n\n                Holly Else\n\n                    Holly Else\n\n                            View author publications\n\n                                You can also search for this author in PubMed\n                                    Google Scholar\n\n            Twitter\n\n            Facebook\n\n            Email\n\n                You have full access to this article via your institution.\n\n                         Trials suggest that offering payment can increase the chance of a researcher agreeing to review, and in some cases speed up the process. Credit: Catherine Falls Commercial/GettyA spate of research findings offer fresh evidence in the debate about whether peer reviewers should be paid for their time and expertise — a fraught topic that has provoked discussion among researchers.This month, two journals released data from their own experiments that suggest that offering payments of around US$250 to researchers who review manuscripts speeds up the process, without affecting the quality of reviews. But some specialists warn that the practice could have unintended consequences for science and publishing.Although both trials are small, they are a good start at gathering data on paid peer review, says Balazs Aczel, a psychologist at Eötvös Loránd University in Budapest. But he adds that whether to pay peer reviewers remains “a very complicated question”. Rewarding reviewersThe peer-review system has come under pressure in recent years as more science is published and scientists face more demands on their time. Journal editors now find it harder to secure reviews and some scientists have questioned the fairness of their voluntary labour being relied upon by some highly profitable publishing companies.The idea of paying peer reviewers has long been discussed, but few publishers have chosen to go down this route so far. Economics journals have experimented with the idea in the past, and some medical journals pay certain reviewers. Others have adopted less-conventional compensation systems: open-access mega journal PeerJ uses a token system that gives reviewers a discount on publishing fees, whereas another title pays its reviewers in a specially developed cryptocurrency.‘Getting paid to review is justice’: journal pays peer reviewers in cryptocurrencySome researchers fear that offering reviewers cash incentives could lessen the quality of reviews or change the landscape of research in other, as-yet unknown ways. But until now, there has been a lack of hard evidence about the potential benefits and drawbacks.Intrigued about the effect of paying peer reviewers, editors at the journal Critical Care Medicine launched a six-month experiment led by David Maslove, a clinical scientist at Queen’s University in Kingston, Canada. Starting in September 2023, the journal asked 715 researchers to review papers. It offered roughly half of them a US$250 incentive.The results, published in the journal earlier this month1, found that paying for reviews moderately improved both the number of accepted invitations and the speed at which reviews were carried out. Some 53% of researchers accepted the invitation to review when offered payment, compared with 48% of those who received a standard, non-paid offer. On average, paid reviews came in one day earlier than unpaid ones. Journal editors assessed reviews from paid and unpaid reviewers and found no difference in quality.Maslove says that the small size of the effect suggests that money has a limited effect on motivating peer reviewers to change their behaviour. “There could be these other values that peer reviewers have, whether it’s a sense of responsibility or loyalty or owing to society.”Speed advantageA separate experiment at the journal Biology Open, found a larger effect, albeit with fewer reviewers.For six months starting in July 2024, editors covering two of the journal’s ten subject areas treated reviewers as paid contractors under two systems. Reviewers were either offered a £600 (US$776) retainer to review up to three papers per quarter, or were paid £220 per review. Under this scheme, editors would send freelance reviewers an invitation to review, which they had to accept or decline within one business day. Once accepted, the reviewer had four days to submit their peer-review report. A total of 20 manuscripts were reviewed in this way.\n\n                                    Enjoying our latest content?\n                                    Login or create an account to continue\n\n                                            Access the most recent journalism from Nature's award-winning team\n                                            Explore the latest features & opinion covering groundbreaking research\n\n                                            Access through your institution\n\n                                    or\n\n                                            Sign in or create an account\n\n                                            Continue with Google\n\n                                            Continue with ORCiD\n\n                doi: https://doi.org/10.1038/d41586-025-00968-6\n\n                ReferencesCotton, C. S. et al. Crit. Care Med. https://doi.org/10.1097/CCM.0000000000006637 (2025).Article\n\n                    Google Scholar\n                Gorelick, D. A. & Clark, A. Preprint at bioRxiv https://doi.org/10.1101/2025.03.18.644032 (2025).Download references\n\n                    Reprints and permissions\n\n                Related Articles\n\n                        ‘Getting paid to review is justice’: journal pays peer reviewers in cryptocurrency\n\n                        Stop the peer-review treadmill. I want to get off\n\n                        Three AI-powered steps to faster, smarter peer review\n\n                        New peer-review trial lets grant applicants evaluate each other’s proposals\n\n                        Nature is trialling transparent peer review — the early results are encouraging\n\n                        Crowd-based peer review can be good and fast\n\n                        ChatGPT is transforming peer review — how can we use it responsibly?\n\n                        If you can’t be kind in peer review, be neutral\n\n                Subjects\n\n                Publishing\n\n    Latest on:\n\n                Publishing\n\n                                    Do smartphones and social media really harm teens’ mental health?\n                                    News Feature 02 APR 25\n\n                                    China has already taken steps to reduce retractions of papers from its hospitals\n                                    Correspondence 01 APR 25\n\n                                    Incredible close-up of colourful crab spiders — March’s best science images\n                                    News 31 MAR 25\n\n                                    Do smartphones and social media really harm teens’ mental health?\n                                    News Feature 02 APR 25\n\n                                    China has already taken steps to reduce retractions of papers from its hospitals\n                                    Correspondence 01 APR 25\n\n                                    Incredible close-up of colourful crab spiders — March’s best science images\n                                    News 31 MAR 25\n\n            Jobs\n\n                        Chief Editor, Nature Biomedical Engineering\n\n                            Job Title: Chief Editor, Nature Biomedical Engineering Locations: New York, Beijing or Shanghai (Hybrid Working Model) Application Deadline: April ...\n                            New York City, New York (US)\n                            Springer Nature Ltd\n\n                        Global Talent Recruitment-Hospital of Stomatology Xi’an Jiaotong University\n\n                            Leading Scholars、Excellent Young Scholars(Overseas)、Outstanding Young Talents、Professor 、Associate Professor\n                            Xian, Shaanxi (CN)\n                            Hospital of Stomatology Xi’an Jiaotong University\n\n                        Faculty Positions in Advanced Materials Thrust, Function Hub, HKUST(GZ)\n\n                            Faculty Positions in Advanced Materials Thrust, Function Hub, HKUST(GZ).\n                            Guangzhou, Guangdong, China\n                            The Hong Kong University of Science and Technology (Guangzhou)\n\n                        Postdoctoral Scholar-Department of Dermatology\n\n                            The Department of Dermatology at the University of California, Irvine anticipates openings for a postdoctoral scholar. Applications are being sough...\n                            University of California Irvine, Irvine\n                            Ampi Montiel, AP&HR Manager\n\n                        Canada Excellence Research Chairs (CERC)\n\n                            Montréal, Quebec (CA)\n                            University of Montreal (UdeM)",
    "summary": {
      "en": "**Summary:**\n\nTwo scientific journals recently tested the idea of paying peer reviewers to see if it would improve the review process. They offered payments of around $250 to some reviewers and found that this increased the likelihood of researchers agreeing to review papers and sped up the review process. \n\nOne study showed that 53% of researchers accepted paid invitations compared to 48% for unpaid ones, and paid reviews were submitted about a day earlier without compromising quality. Another journal used a different payment system, offering up to £600 for reviewing multiple papers, and saw a similar positive effect.\n\nDespite these findings, some experts caution that paying reviewers could have negative impacts on the research field. Overall, while the trials provide initial data on paid peer review, the broader implications are still uncertain.",
      "ko": "최근 두 개의 과학 저널이 동료 심사자에게 보수를 지급하는 방안을 시험해 보았습니다. 이들은 일부 심사자에게 약 250달러의 보수를 제공했으며, 그 결과 연구자들이 논문 심사를 수락할 가능성이 높아지고 심사 과정이 빨라지는 것을 발견했습니다.\n\n한 연구에서는 보수를 받는 초대에 응한 연구자가 53%에 달한 반면, 보수를 받지 않는 초대에 응한 비율은 48%에 불과했습니다. 보수를 받은 심사는 품질에 영향을 주지 않으면서도 약 하루 정도 더 빨리 제출되었습니다. 또 다른 저널은 여러 논문을 심사하는 대가로 최대 600파운드를 지급하는 다른 보상 시스템을 도입했으며, 이 또한 긍정적인 효과를 보였습니다.\n\n하지만 이러한 결과에도 불구하고 일부 전문가들은 심사자에게 보수를 지급하는 것이 연구 분야에 부정적인 영향을 미칠 수 있다고 경고하고 있습니다. 전반적으로 이러한 실험은 유료 동료 심사에 대한 초기 데이터를 제공하지만, 그에 따른 더 넓은 의미는 여전히 불확실한 상황입니다.",
      "ja": "最近、二つの科学雑誌が査読者に報酬を支払うアイデアを試験しました。査読者に約250ドルの報酬を提供したところ、研究者が論文を査読することに同意する可能性が高まり、査読プロセスが迅速化されることが分かりました。\n\nある研究では、報酬付きの招待を受け入れた研究者が53%に対し、無報酬のものは48%だったことが示されました。また、報酬を受けた査読は、質を損なうことなく約1日早く提出されました。別の雑誌では、複数の論文を査読するために最大600ポンドの報酬を提供する異なるシステムを採用し、同様の良い結果が得られました。\n\nしかし、これらの結果にもかかわらず、一部の専門家は査読者に報酬を支払うことが研究分野に悪影響を及ぼす可能性があると警告しています。全体として、これらの試験は報酬付き査読に関する初期データを提供していますが、その広範な影響についてはまだ不確かです。"
    }
  },
  {
    "id": "be5c173df93cb884",
    "title": {
      "en": "Ace: Realtime Computer Autopilot",
      "ko": "에이스: 실시간 자율 비행",
      "ja": "エース：自動操縦システム"
    },
    "type": "story",
    "url": "https://generalagents.com/ace/",
    "score": 10,
    "by": "huerne",
    "time": 1743616538,
    "content": "Intelligence. On your computer.\n\n            Ace is a computer autopilot that performs tasks on your desktop\n            using your mouse and keyboard.\n\n                    Ace outperforms other models on our suite of computer use tasks,\n                    which we are\n            open-sourcing here. We're making the ace-control models available to selected\n            partners through our\n            developer platform.\n\n                        Model Accuracy Comparison\n\n                Correct Left-Click Predictions\n\n                                ace-control-medium\n\n                                ace-control-small\n\n                                Operator\n\n                                Molmo-72B-0924\n\n                                Claude 3.7 Sonnet\n\n                                UI-TARS-72B-SFT\n\n                                OmniParser V2 + GPT-4o\n\n                                Gemini 2.0 Flash\n\n                                Qwen2.5-VL-72B-Instruct\n\n                        020406080\n              Click Accuracy (95% CI)\n\n            Ace is exceptionally fast! It can perform many tasks in superhuman\n            time.\n\n                        ace-control-small324msace-control-medium533msUI-TARS-72B-SFT1977msGemini 2.0 Flash3069msQwen2.5-VL-72B-Instruct3790msOperator6385msMolmo-72B-09246599msClaude 3.7 Sonnet9656msOmniParser V2 + GPT-4o12642ms\n\n                            Action Prediction Latency (ms)\n\n                Ace works like we do—performing mouse clicks and keystrokes based on the screen and prompt—trained with <3 by our team of software specialists and domain experts on over a million tasks.\n\n            Ace is still learning and can make mistakes. Here are some examples\n            of its failures:\n\n            We're just beginning. As we increase training resources, Ace will\n            become more intelligent and capable.\n\n                Accuracy vs Training Resources\n\n                            1x\n                            2x\n                            4x\n                            8x\n                            16x\n\n                Normalized Training Resources (log scale)\n\n                        Accuracy\n\n            We're launching an early version of Ace in research preview. We\n            invite you to try Ace for yourself.\n\n          Sign up for Ace Research Preview",
    "summary": {
      "en": "Ace is a computer autopilot that helps perform tasks on your desktop using your mouse and keyboard. It is designed to be faster and more accurate than other models, demonstrating superior performance in various computer tasks.\n\nKey points:\n\n- **Performance**: Ace outperforms other models in accuracy and speed. For example, it can make correct left-click predictions much faster than competitors.\n- **Speed**: Ace executes tasks in superhuman time, completing actions significantly quicker than other systems.\n- **Learning**: Ace has been trained by experts on over a million tasks but is still learning and may make mistakes.\n- **Future Development**: The team plans to enhance Ace's capabilities as they increase training resources.\n- **Availability**: An early version of Ace is available for testing in a research preview.\n\nUsers are invited to sign up to try Ace.",
      "ko": "에이스는 마우스와 키보드를 사용하여 데스크탑에서 작업을 수행하는 데 도움을 주는 컴퓨터 자동 조종 장치입니다. 다른 모델들보다 더 빠르고 정확하게 설계되어 다양한 컴퓨터 작업에서 뛰어난 성능을 보여줍니다.\n\n에이스는 정확성과 속도 면에서 다른 모델들을 능가합니다. 예를 들어, 에이스는 경쟁 제품보다 훨씬 빠르게 정확한 왼쪽 클릭 예측을 할 수 있습니다. 에이스는 인간보다 빠른 속도로 작업을 수행하여 다른 시스템보다 훨씬 빠르게 작업을 완료합니다. \n\n에이스는 전문가들에 의해 백만 개 이상의 작업으로 훈련받았지만, 여전히 학습 중이며 가끔 실수를 할 수 있습니다. 팀은 훈련 자원을 늘리면서 에이스의 기능을 향상시킬 계획입니다. 현재 에이스의 초기 버전이 연구 미리보기에서 테스트용으로 제공되고 있습니다.\n\n사용자들은 에이스를 사용해 볼 수 있도록 신청할 수 있습니다.",
      "ja": "Aceは、デスクトップ上でマウスやキーボードを使ってタスクを実行するためのコンピュータ自動操縦装置です。他のモデルよりも速く、正確に動作するように設計されており、さまざまなコンピュータ作業において優れた性能を示しています。\n\nAceは、精度と速度の面で他のモデルを上回ります。例えば、正しい左クリックの予測を競合製品よりもはるかに速く行うことができます。また、Aceは超人的な速度でタスクを実行し、他のシステムよりもかなり早くアクションを完了します。\n\nAceは、専門家によって100万以上のタスクでトレーニングを受けていますが、まだ学習中であり、時には間違いを犯すこともあります。チームは、トレーニングリソースを増やすことでAceの能力を向上させる計画を立てています。\n\n現在、Aceの初期バージョンが研究プレビューとしてテスト用に提供されています。ユーザーは、Aceを試すためにサインアップすることができます。"
    }
  },
  {
    "id": "d08c37846d6147c4",
    "title": {
      "en": "NYC New Subway Map",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.nytimes.com/2025/04/02/nyregion/nyc-new-subway-map.html",
    "score": 27,
    "by": "bgschulman31",
    "time": 1743616946,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "7fdbeb47559e88be",
    "title": {
      "en": "Ask HN: How do you make a living contributing to and/or creating OSS projects?",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": null,
    "score": 15,
    "by": "Brysonbw",
    "time": 1743618291,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "c6cc4b859c6dd2dd",
    "title": {
      "en": "Real-Time Introspective Compression for Transformers",
      "ko": "실시간 변환 압축",
      "ja": "トランスフォーマーの瞬時圧縮"
    },
    "type": "story",
    "url": "https://github.com/Dicklesworthstone/llm_introspective_compression_and_metacognition",
    "score": 3,
    "by": "eigenvalue",
    "time": 1743615776,
    "content": "Real-Time Introspective Compression for Transformers\nBy Jeffrey Emanuel (and various collaborators of the electronic persuasion)\nWritten on April 1st, 2025\n\nIntroduction: Two Intertwined Problems\nTransformer-based large language models (LLMs) face two significant limitations that restrict their capabilities:\n\nLack of Introspection: Unless specifically instrumented, transformer-based LLMs have no ability to explicitly access their own internal states—the activations in their feed-forward layers, attention mechanisms, and other components. This opacity hinders mechanistic interpretability, self-monitoring, and dynamic reasoning.\n\nEphemeral Cognition: Most LLM \"thinking\" is fleeting—activations across billions of parameters that change during forward passes as the model processes tokens. Recording this data naively is computationally prohibitive due to its sheer volume.\n\nThese limitations have profound implications for interpretability, debugging, and developing more capable AI systems. This article proposes a novel approach to address both problems simultaneously.\nThe Problem: Transformer Black Boxes\nLarge transformer models generate massive volumes of intermediate data during inference. Each token step produces new hidden states, attention maps, and cached key/value tensors. These are ephemeral by design: they're discarded after each forward pass, with no built-in mechanism for inspection, rollback, or resumption.\nNaively saving the full state at each step is computationally prohibitive. A model like GPT-3, storing full activations and attention caches per token, can consume hundreds of megabytes per sequence. Existing approaches like PCA, quantization, or simple delta encoding are lossy and often irreversible, making them unsuitable for applications requiring high-fidelity recovery.\nWe lack a practical way to pause, inspect, and replay a model's internal state with precision.\nTheoretical Insight: The Transformer Thinks on a Low-Dimensional Manifold\nDespite their high dimensionality, transformer activations likely occupy a small portion of the possible state space. They appear to live on a lower-dimensional, structured manifold shaped by several factors:\n\nPretraining Dynamics: Models learn to represent language efficiently, creating structured internal representations.\nArchitectural Constraints: Attention mechanisms and layer normalization impose patterns on activation distributions.\nSemantic Priors: Natural language has inherent structure that shapes model activations.\nTask-Driven Optimization: Fine-tuning carves task-specific trajectories through this space.\n\nThis hypothesis draws from observations in neural network representations and suggests that transformer states could be compressed into smaller latent representations without losing critical information, much like a map reduces a terrain to key coordinates.\nThis raises a compelling possibility: what if we could encode those internal states directly onto this manifold? Instead of treating the activations as raw data, we could represent them as coordinates on a latent terrain.\nThe Analogy: Transformer State as a Video Game Save\nThink of a transformer as a single-player game engine. Each inference step is like a frame rendered during gameplay. Normally, you don't save every frame—you save the game state: player position, inventory, mission flags, world state. This compact representation allows you to stop, rewind, branch, or resume seamlessly.\nWe want the same thing for transformer inference: a way to save the complete thought state at a given point in a sequence, using as little space as possible, but with the ability to reconstruct it with high fidelity later.\nTechnical Proposal: Sidecar Transformers for State Compression\nWe propose a system for high-efficiency introspective compression, built around a learned latent manifold of transformer states. This introduces a lightweight sidecar model that rides alongside a host transformer, encoding its internal state into a compact latent representation z_t, from which the full state can be recovered.\nComponents\n\nMain Transformer (T_main): A frozen pretrained model (e.g., GPT or Mistral) producing full hidden states h_t and cached key/value tensors KV_t.\n\nSidecar Encoder (E): A model that takes the current token, prior latent code z_{t-1}, and a tap into a subset of T_main's hidden states to output a new latent code z_t.\n\nSidecar Decoder (D): A decoder that reconstructs the hidden states and key/value tensors from z_t.\n\nFor simplicity, the prototype uses feed-forward networks for E and D, though future iterations could explore attention-based or recurrent architectures to capture sequential dependencies more effectively.\nWhat Constitutes \"Internal State\"?\nFor clarity, we define the internal state we aim to compress as:\n\nHidden States: The activations from selected transformer layers (not necessarily all layers)\nKey/Value Cache: The cached attention tensors needed for efficient autoregressive generation\nAdditional Context: Any model-specific state needed for exact resumption of inference\n\nThis definition is important because reconstructing only partial internal state would limit the usefulness of the approach.\nTraining Methodology\nThe encoder and decoder are trained to model the latent manifold of transformer states:\n\nRun a sequence through T_main to obtain ground-truth h_t, KV_t\nCompute z_t = E(x_t, z_{t-1}, tap(h_t))\nDecode via D(z_t) to get ĥ_t, KV̂_t\nOptimize a loss function:\nLoss = λ₁||h_t - ĥ_t||² + λ₂||KV_t - KV̂_t||² + λ₃R(z_t)\n\nWhere R(z_t) is a regularization term that encourages z_t to live on a structured, low-entropy manifold. Depending on implementation, this could use VAE-style KL divergence, flow-based constraints, or other regularization approaches.\nTraining could use datasets like OpenWebText or task-specific corpora, with optimization via standard methods (e.g., Adam, learning rate ~1e-4).\nA Note on Reconstruction Fidelity\nIt's important to clarify that \"high-fidelity reconstruction\" rather than \"exact reconstruction\" is the realistic target. While autoencoders are typically lossy, our goal is to minimize reconstruction error to the point where the functional behavior of the model (e.g., next-token prediction) is preserved. This represents a trade-off between compression ratio and fidelity that can be tuned based on application requirements.\nImplementation: Full-State Compression System\nBuilding on our initial prototype, we now present a comprehensive implementation strategy for compressing the entire transformer state, including all hidden layers and KV caches. This represents a significant advancement toward practical, real-world deployment.\nArchitectural Approaches for Full-State Compression\nFor complete state capture and reconstruction, we must determine how to structure the sidecar encoder-decoder system. We explore three architectural strategies:\nOption 1: Layer-Specific Encoders/Decoders\nimport torch, json, os\nimport torch.nn as nn\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom collections import defaultdict\nimport numpy as np\n\n# Load model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\",\n                                            torch_dtype=torch.float16,\n                                            device_map=\"auto\")\nmodel.eval()\n\n# Configuration\nhidden_dim = 4096  # Mistral's hidden dimension\nn_layers = 32      # Number of layers in Mistral\nlatent_dim = 256   # Compressed dimension per layer\nkv_cache_latent_ratio = 0.1  # Compression ratio for KV cache\n\nclass LayerSpecificEncoderDecoder(nn.Module):\n    \"\"\"One encoder-decoder pair for each transformer layer\"\"\"\n    def __init__(self, n_layers, hidden_dim, latent_dim):\n        super().__init__()\n        self.encoders = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(hidden_dim, 1024),\n                nn.GELU(),\n                nn.LayerNorm(1024),\n                nn.Linear(1024, latent_dim)\n            ) for _ in range(n_layers)\n        ])\n\n        self.decoders = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(latent_dim, 1024),\n                nn.GELU(),\n                nn.LayerNorm(1024),\n                nn.Linear(1024, hidden_dim)\n            ) for _ in range(n_layers)\n        ])\n\n        # KV cache encoder/decoder (handles growing sequence length)\n        # More sophisticated than hidden state E/D to handle variable sizes\n        self.kv_encoder = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(\n                d_model=hidden_dim,\n                nhead=8,\n                dim_feedforward=1024,\n                batch_first=True\n            ), num_layers=2\n        )\n\n        self.kv_proj = nn.Linear(hidden_dim, int(hidden_dim * kv_cache_latent_ratio))\n        self.kv_unproj = nn.Linear(int(hidden_dim * kv_cache_latent_ratio), hidden_dim)\n\n        self.kv_decoder = nn.TransformerDecoder(\n            nn.TransformerDecoderLayer(\n                d_model=hidden_dim,\n                nhead=8,\n                dim_feedforward=1024,\n                batch_first=True\n            ), num_layers=2\n        )\n\n    def encode_hidden(self, hidden_states):\n        \"\"\"Encode hidden states from all layers\"\"\"\n        return [encoder(h) for encoder, h in zip(self.encoders, hidden_states)]\n\n    def decode_hidden(self, latents):\n        \"\"\"Decode compressed representations back to hidden states\"\"\"\n        return [decoder(z) for decoder, z in zip(self.decoders, latents)]\n\n    def encode_kv_cache(self, kv_cache):\n        \"\"\"Compress KV cache (more complex due to variable size)\"\"\"\n        # For each layer, head\n        compressed_kv = {}\n        for layer_idx, layer_cache in kv_cache.items():\n            compressed_kv[layer_idx] = {}\n            for head_idx, (k, v) in layer_cache.items():\n                # Shape: [batch, seq_len, head_dim]\n                # Apply transformer to get contextual representation\n                k_context = self.kv_encoder(k)\n                v_context = self.kv_encoder(v)\n\n                # Project to smaller dimension\n                k_compressed = self.kv_proj(k_context)\n                v_compressed = self.kv_proj(v_context)\n\n                compressed_kv[layer_idx][head_idx] = (k_compressed, v_compressed)\n\n        return compressed_kv\n\n    def decode_kv_cache(self, compressed_kv, seq_len):\n        \"\"\"Decompress KV cache back to original format\"\"\"\n        decompressed_kv = {}\n        for layer_idx, layer_cache in compressed_kv.items():\n            decompressed_kv[layer_idx] = {}\n            for head_idx, (k_comp, v_comp) in layer_cache.items():\n                # Expand back to original dimension\n                k_expanded = self.kv_unproj(k_comp)\n                v_expanded = self.kv_unproj(v_comp)\n\n                # Use transformer decoder with positional cues to restore sequence\n                # We provide a sequence length tensor as the memory for the decoder\n                pos_cue = torch.zeros(1, seq_len, k_expanded.size(-1)).to(k_expanded.device)\n                k_decompressed = self.kv_decoder(k_expanded, pos_cue)\n                v_decompressed = self.kv_decoder(v_expanded, pos_cue)\n\n                decompressed_kv[layer_idx][head_idx] = (k_decompressed, v_decompressed)\n\n        return decompressed_kv\n\n# Initialize the full-state compression system\ncompressor = LayerSpecificEncoderDecoder(n_layers, hidden_dim, latent_dim)\n\n# Hook into all model layers to capture hidden states\nhidden_states = [[] for _ in range(n_layers)]\nhooks = []\n\ndef create_hook_fn(layer_idx):\n    def hook_fn(module, input, output):\n        hidden_states[layer_idx].append(output.detach().to(torch.float32))\n    return hook_fn\n\n# Register hooks for all layers\nfor i in range(n_layers):\n    hook = model.model.layers[i].register_forward_hook(create_hook_fn(i))\n    hooks.append(hook)\n\n# Function to extract KV cache from the model\ndef extract_kv_cache(model):\n    \"\"\"Extract key-value cache from model's attention modules\"\"\"\n    kv_cache = {}\n    for i, layer in enumerate(model.model.layers):\n        kv_cache[i] = {}\n        for h, head in enumerate(layer.self_attn.heads):\n            # In a real implementation, there would be a way to access\n            # the actual KV cache. This is simplified.\n            k = torch.randn(1, 10, head.head_dim)  # Placeholder\n            v = torch.randn(1, 10, head.head_dim)  # Placeholder\n            kv_cache[i][h] = (k, v)\n    return kv_cache\n\n# Step 1: Run inference and capture all hidden states and KV cache\ninput_text = \"The cat sat on the mat.\"\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n\nwith torch.no_grad():\n    # Clear previous activations\n    for states in hidden_states:\n        states.clear()\n\n    # Run model inference\n    model(**inputs)\n\n    # Extract KV cache\n    kv_cache = extract_kv_cache(model)\n\n    # Process hidden states (convert list of activations → tensor)\n    processed_hiddens = []\n    for layer_states in hidden_states:\n        # Stack sequence length dimension\n        layer_tensor = torch.stack(layer_states[0], dim=0)\n        processed_hiddens.append(layer_tensor)\n\n# Step 2: Compress full state\ncompressed_hiddens = compressor.encode_hidden(processed_hiddens)\ncompressed_kv = compressor.encode_kv_cache(kv_cache)\n\n# Step 3: Save compressed state\nsave_dir = \"./compressed_state\"\nos.makedirs(save_dir, exist_ok=True)\ntorch.save(compressed_hiddens, os.path.join(save_dir, \"compressed_hiddens.pt\"))\ntorch.save(compressed_kv, os.path.join(save_dir, \"compressed_kv.pt\"))\ntorch.save(inputs[\"input_ids\"], os.path.join(save_dir, \"input_ids.pt\"))\n\n# Step 4: Reconstruct\nseq_len = inputs[\"input_ids\"].size(1)\nreconstructed_hiddens = compressor.decode_hidden(compressed_hiddens)\nreconstructed_kv = compressor.decode_kv_cache(compressed_kv, seq_len)\n\n# Evaluate reconstruction quality\nmse_per_layer = []\nfor i, (original, reconstructed) in enumerate(zip(processed_hiddens, reconstructed_hiddens)):\n    mse = nn.MSELoss()(original, reconstructed).item()\n    mse_per_layer.append(mse)\n    print(f\"Layer {i} MSE: {mse:.6f}\")\n\nprint(f\"Average MSE across layers: {np.mean(mse_per_layer):.6f}\")\n\n# Clean up hooks\nfor hook in hooks:\n    hook.remove()\n\nOption 2: Grouped Layer Encoder/Decoder\nclass GroupedLayerCompressor(nn.Module):\n    \"\"\"Compress K layers with each encoder-decoder pair\"\"\"\n    def __init__(self, n_layers, hidden_dim, latent_dim, group_size=4):\n        super().__init__()\n        self.n_groups = (n_layers + group_size - 1) // group_size  # Ceiling division\n        self.group_size = group_size\n\n        # Create encoder/decoder for each group of layers\n        self.group_encoders = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(hidden_dim * min(group_size, n_layers - i * group_size), 2048),\n                nn.GELU(),\n                nn.LayerNorm(2048),\n                nn.Linear(2048, latent_dim * min(group_size, n_layers - i * group_size))\n            ) for i in range(self.n_groups)\n        ])\n\n        self.group_decoders = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(latent_dim * min(group_size, n_layers - i * group_size), 2048),\n                nn.GELU(),\n                nn.LayerNorm(2048),\n                nn.Linear(2048, hidden_dim * min(group_size, n_layers - i * group_size))\n            ) for i in range(self.n_groups)\n        ])\n\n        # Similar KV cache handling as option 1...\n        # (KV cache code omitted for brevity but would be similar)\n\n    def encode_hidden(self, hidden_states):\n        \"\"\"Encode hidden states by groups\"\"\"\n        latents = []\n\n        for group_idx in range(self.n_groups):\n            start_idx = group_idx * self.group_size\n            end_idx = min(start_idx + self.group_size, len(hidden_states))\n\n            # Concatenate group's hidden states for each token\n            group_states = []\n            seq_len = hidden_states[0].size(0)\n\n            for token_idx in range(seq_len):\n                token_group_states = torch.cat([\n                    hidden_states[layer_idx][token_idx]\n                    for layer_idx in range(start_idx, end_idx)\n                ])\n                group_states.append(token_group_states)\n\n            group_input = torch.stack(group_states)\n            group_latent = self.group_encoders[group_idx](group_input)\n\n            # Split encoded representation back into per-layer latents\n            layers_in_group = end_idx - start_idx\n            latent_per_layer = group_latent.chunk(layers_in_group, dim=-1)\n            latents.extend(latent_per_layer)\n\n        return latents\n\n    def decode_hidden(self, latents):\n        \"\"\"Decode latents back to hidden states\"\"\"\n        reconstructed = []\n\n        for group_idx in range(self.n_groups):\n            start_idx = group_idx * self.group_size\n            end_idx = min(start_idx + self.group_size, len(latents))\n\n            # Concatenate group's latents\n            seq_len = latents[0].size(0)\n            group_latents = []\n\n            for token_idx in range(seq_len):\n                token_group_latents = torch.cat([\n                    latents[layer_idx][token_idx]\n                    for layer_idx in range(start_idx, end_idx)\n                ])\n                group_latents.append(token_group_latents)\n\n            group_latent_input = torch.stack(group_latents)\n            group_reconstruction = self.group_decoders[group_idx](group_latent_input)\n\n            # Split reconstruction back into per-layer hidden states\n            layers_in_group = end_idx - start_idx\n            hidden_per_layer = group_reconstruction.chunk(layers_in_group, dim=-1)\n            reconstructed.extend(hidden_per_layer)\n\n        return reconstructed\n\nOption 3: Single Unified Encoder/Decoder\nclass UnifiedStateCompressor(nn.Module):\n    \"\"\"One large encoder-decoder for all layers\"\"\"\n    def __init__(self, n_layers, hidden_dim, latent_dim_per_layer):\n        super().__init__()\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n        self.total_latent_dim = latent_dim_per_layer * n_layers\n\n        # Attention-based encoder to capture cross-layer dependencies\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=hidden_dim,\n            nhead=8,\n            dim_feedforward=4096,\n            batch_first=True\n        )\n        self.cross_layer_encoder = nn.TransformerEncoder(\n            encoder_layer, num_layers=3\n        )\n\n        # Projection to latent space\n        self.encoder_proj = nn.Sequential(\n            nn.Linear(hidden_dim * n_layers, 4096),\n            nn.GELU(),\n            nn.LayerNorm(4096),\n            nn.Linear(4096, self.total_latent_dim)\n        )\n\n        # Decoder architecture\n        decoder_layer = nn.TransformerDecoderLayer(\n            d_model=hidden_dim,\n            nhead=8,\n            dim_feedforward=4096,\n            batch_first=True\n        )\n        self.cross_layer_decoder = nn.TransformerDecoder(\n            decoder_layer, num_layers=3\n        )\n\n        # Projection from latent space\n        self.decoder_proj = nn.Sequential(\n            nn.Linear(self.total_latent_dim, 4096),\n            nn.GELU(),\n            nn.LayerNorm(4096),\n            nn.Linear(4096, hidden_dim * n_layers)\n        )\n\n        # Layer embedding to help the model differentiate layers\n        self.layer_embedding = nn.Embedding(n_layers, hidden_dim)\n\n        # KV cache handling components would follow\n        # (omitted for brevity but would be similar to previous options)\n\n    def encode_hidden(self, hidden_states):\n        \"\"\"Encode all hidden states into a unified latent representation\"\"\"\n        batch_size, seq_len = hidden_states[0].size(0), hidden_states[0].size(1)\n\n        # First process each layer with cross-attention\n        processed_layers = []\n        for i, h in enumerate(hidden_states):\n            # Add layer positional embedding\n            layer_pos = self.layer_embedding(torch.tensor([i], device=h.device))\n            h_with_pos = h + layer_pos.unsqueeze(1).expand(-1, seq_len, -1)\n            processed = self.cross_layer_encoder(h_with_pos)\n            processed_layers.append(processed)\n\n        # Stack all layers for each token\n        token_wise_concatenated = []\n        for token_idx in range(seq_len):\n            token_states = torch.cat([\n                layer[:, token_idx, :] for layer in processed_layers\n            ], dim=-1)\n            token_wise_concatenated.append(token_states)\n\n        token_wise_concatenated = torch.stack(token_wise_concatenated)\n\n        # Project to latent space\n        unified_latent = self.encoder_proj(token_wise_concatenated)\n\n        # Return as a single tensor rather than per-layer\n        return unified_latent\n\n    def decode_hidden(self, unified_latent):\n        \"\"\"Decode unified latent back to per-layer hidden states\"\"\"\n        seq_len = unified_latent.size(0)\n\n        # Project back to concatenated hidden dimension\n        expanded = self.decoder_proj(unified_latent)\n\n        # Split into per-layer representations\n        layer_chunks = expanded.chunk(self.n_layers, dim=-1)\n\n        # Process each layer with the decoder\n        reconstructed_layers = []\n        for i, chunk in enumerate(layer_chunks):\n            # Add layer positional embedding\n            layer_pos = self.layer_embedding(torch.tensor([i], device=chunk.device))\n            chunk_with_pos = chunk + layer_pos.unsqueeze(1).expand(-1, seq_len, -1)\n\n            # Generate positional memory for decoder\n            pos_memory = torch.zeros(1, seq_len, self.hidden_dim).to(chunk.device)\n            pos_memory = pos_memory + layer_pos.unsqueeze(1).expand(-1, seq_len, -1)\n\n            # Decode with cross-attention\n            reconstructed = self.cross_layer_decoder(chunk_with_pos, pos_memory)\n            reconstructed_layers.append(reconstructed)\n\n        return reconstructed_layers\n\nHandling the KV Cache\nThe key-value cache poses unique challenges due to its growing size with sequence length and its critical role in efficient autoregressive generation. We implement a specialized approach:\nclass KVCacheCompressor(nn.Module):\n    \"\"\"Specialized compressor for key-value cache\"\"\"\n    def __init__(self, n_layers, n_heads, head_dim, compression_ratio=0.25):\n        super().__init__()\n        self.n_layers = n_layers\n        self.n_heads = n_heads\n        self.head_dim = head_dim\n        self.compression_ratio = compression_ratio\n\n        # Size of compressed representation per head\n        self.compressed_dim = int(head_dim * compression_ratio)\n\n        # Convolutional layers for sequence-aware compression\n        self.key_encoder = nn.Sequential(\n            nn.Conv1d(head_dim, head_dim, kernel_size=3, padding=1),\n            nn.GELU(),\n            nn.Conv1d(head_dim, self.compressed_dim, kernel_size=3, padding=1)\n        )\n\n        self.value_encoder = nn.Sequential(\n            nn.Conv1d(head_dim, head_dim, kernel_size=3, padding=1),\n            nn.GELU(),\n            nn.Conv1d(head_dim, self.compressed_dim, kernel_size=3, padding=1)\n        )\n\n        # Sequence-aware decoders\n        self.key_decoder = nn.Sequential(\n            nn.Conv1d(self.compressed_dim, head_dim, kernel_size=3, padding=1),\n            nn.GELU(),\n            nn.Conv1d(head_dim, head_dim, kernel_size=3, padding=1)\n        )\n\n        self.value_decoder = nn.Sequential(\n            nn.Conv1d(self.compressed_dim, head_dim, kernel_size=3, padding=1),\n            nn.GELU(),\n            nn.Conv1d(head_dim, head_dim, kernel_size=3, padding=1)\n        )\n\n        # Metadata encoding (sequence positions, etc.)\n        self.metadata_dim = 64\n        self.metadata_encoder = nn.Linear(3, self.metadata_dim)  # layer, head, position\n        self.metadata_decoder = nn.Linear(self.metadata_dim, 3)\n\n    def encode(self, kv_cache):\n        \"\"\"Compress the KV cache\"\"\"\n        compressed_cache = {}\n        metadata = []\n\n        for layer_idx, layer_cache in kv_cache.items():\n            compressed_cache[layer_idx] = {}\n\n            for head_idx, (k, v) in layer_cache.items():\n                # Get sequence length\n                seq_len = k.size(1)\n\n                # Transpose for convolutional layers [batch, seq, dim] -> [batch, dim, seq]\n                k_conv = k.transpose(1, 2)\n                v_conv = v.transpose(1, 2)\n\n                # Apply convolutional compression\n                k_compressed = self.key_encoder(k_conv)\n                v_compressed = self.value_encoder(v_conv)\n\n                # Store compressed tensors\n                compressed_cache[layer_idx][head_idx] = (k_compressed, v_compressed)\n\n                # Create metadata tensor for reconstruction\n                for pos in range(seq_len):\n                    metadata.append([layer_idx, head_idx, pos])\n\n        # Encode metadata if present\n        encoded_metadata = None\n        if metadata:\n            metadata_tensor = torch.tensor(metadata, dtype=torch.float32)\n            encoded_metadata = self.metadata_encoder(metadata_tensor)\n\n        return compressed_cache, encoded_metadata\n\n    def decode(self, compressed_cache, encoded_metadata, max_seq_len):\n        \"\"\"Decompress the KV cache\"\"\"\n        decompressed_cache = {}\n\n        for layer_idx, layer_cache in compressed_cache.items():\n            decompressed_cache[layer_idx] = {}\n\n            for head_idx, (k_comp, v_comp) in layer_cache.items():\n                # Apply convolutional decompression\n                k_decompressed = self.key_decoder(k_comp)\n                v_decompressed = self.value_decoder(v_comp)\n\n                # Transpose back [batch, dim, seq] -> [batch, seq, dim]\n                k_restored = k_decompressed.transpose(1, 2)\n                v_restored = v_decompressed.transpose(1, 2)\n\n                # Store decompressed tensors\n                decompressed_cache[layer_idx][head_idx] = (k_restored, v_restored)\n\n        return decompressed_cache\n\nComplete Compression System\nTo integrate these approaches, we implement a unified compression manager:\nclass TransformerStateCompressor:\n    \"\"\"Complete system for transformer state compression\"\"\"\n    def __init__(self, model_config, compressor_type=\"layer_specific\", latent_dim=256):\n        self.model_config = model_config\n\n        # Extract model parameters\n        self.hidden_dim = model_config.hidden_size\n        self.n_layers = model_config.num_hidden_layers\n        self.n_heads = model_config.num_attention_heads\n        self.head_dim = model_config.hidden_size // model_config.num_attention_heads\n\n        # Select compressor architecture based on preference\n        if compressor_type == \"layer_specific\":\n            self.hidden_compressor = LayerSpecificEncoderDecoder(\n                self.n_layers, self.hidden_dim, latent_dim\n            )\n        elif compressor_type == \"grouped\":\n            self.hidden_compressor = GroupedLayerCompressor(\n                self.n_layers, self.hidden_dim, latent_dim, group_size=4\n            )\n        elif compressor_type == \"unified\":\n            self.hidden_compressor = UnifiedStateCompressor(\n                self.n_layers, self.hidden_dim, latent_dim // self.n_layers\n            )\n        else:\n            raise ValueError(f\"Unknown compressor type: {compressor_type}\")\n\n        # KV cache compressor\n        self.kv_compressor = KVCacheCompressor(\n            self.n_layers, self.n_heads, self.head_dim\n        )\n\n    def compress_state(self, hidden_states, kv_cache):\n        \"\"\"Compress full transformer state\"\"\"\n        compressed_hiddens = self.hidden_compressor.encode_hidden(hidden_states)\n        compressed_kv, metadata = self.kv_compressor.encode(kv_cache)\n\n        return {\n            \"hidden_states\": compressed_hiddens,\n            \"kv_cache\": compressed_kv,\n            \"metadata\": metadata\n        }\n\n    def decompress_state(self, compressed_state, seq_len):\n        \"\"\"Restore full transformer state from compressed representation\"\"\"\n        reconstructed_hiddens = self.hidden_compressor.decode_hidden(\n            compressed_state[\"hidden_states\"]\n        )\n\n        reconstructed_kv = self.kv_compressor.decode(\n            compressed_state[\"kv_cache\"],\n            compressed_state[\"metadata\"],\n            seq_len\n        )\n\n        return reconstructed_hiddens, reconstructed_kv\n\n    def evaluate_reconstruction(self, original_hiddens, original_kv,\n                              reconstructed_hiddens, reconstructed_kv):\n        \"\"\"Measure reconstruction quality\"\"\"\n        # Hidden state reconstruction quality\n        hidden_mse = []\n        for layer_idx in range(self.n_layers):\n            mse = ((original_hiddens[layer_idx] - reconstructed_hiddens[layer_idx]) ** 2).mean().item()\n            hidden_mse.append(mse)\n\n        # KV cache reconstruction quality\n        kv_mse = []\n        for layer_idx in range(self.n_layers):\n            for head_idx in range(self.n_heads):\n                orig_k, orig_v = original_kv[layer_idx][head_idx]\n                recon_k, recon_v = reconstructed_kv[layer_idx][head_idx]\n\n                k_mse = ((orig_k - recon_k) ** 2).mean().item()\n                v_mse = ((orig_v - recon_v) ** 2).mean().item()\n                kv_mse.append((k_mse + v_mse) / 2)\n\n        return {\n            \"hidden_mse_per_layer\": hidden_mse,\n            \"avg_hidden_mse\": sum(hidden_mse) / len(hidden_mse),\n            \"kv_mse_per_component\": kv_mse,\n            \"avg_kv_mse\": sum(kv_mse) / len(kv_mse)\n        }\n\nArchitectural Comparison and Recommendations\nEach architectural approach offers different trade-offs:\n\nLayer-Specific Encoders/Decoders:\n\nBest for high-fidelity reconstruction of individual layers\nIdeal when layers have distinct activation patterns\nMore parameters but enables parallel training\nRecommended for research applications requiring precise introspection\n\nGrouped Layer Compressors:\n\nBalances parameter efficiency and reconstruction quality\nCaptures some cross-layer dependencies\nGood compromise for most applications\nRecommended as the default approach\n\nUnified Encoder/Decoder:\n\nMost parameter-efficient\nBest at capturing cross-layer dependencies\nMay struggle with precise reconstruction of all layers\nRecommended for memory-constrained environments or when cross-layer relationships are important\n\nFor the KV cache, the specialized convolutional approach offers sequence-aware compression critical for autoregressive generation, though other approaches like attention-based compression or adaptive quantization could be explored for different models.\nImplementation Considerations\n\nMemory Management: For large models, gradient checkpointing or layer-by-layer processing may be necessary during training.\n\nTraining Strategy: Progressive training (start with a few layers, gradually add more) can improve stability.\n\nLatent Dimension Tuning: The optimal latent dimension likely varies by layer; early experiments suggest lower layers may need less compression than higher layers.\n\nHyperparameter Optimization: The balance between hidden state and KV cache reconstruction quality requires careful tuning of loss weights.\n\nA full implementation would incorporate these components into a reusable library that interfaces with major transformer frameworks like Hugging Face Transformers.\nPerformance Benchmarks\nWhile exact numbers would require empirical validation, preliminary experiments suggest:\n\nCompression ratios of 8-16x are achievable for hidden states\nKV cache compression of 4x appears feasible with minimal degradation\nArchitecture choice impacts reconstruction quality by 15-30%\nLayer-specific compression can achieve ~10⁻⁴ MSE on mid-level layers\n\nApplications: New Capabilities for Transformer Models\nWith high-fidelity compression of internal states, entirely new capabilities become possible:\nBacktracking in Reasoning\nYou can rewind the model to any past internal state and explore alternative continuations—crucial for tasks involving deduction, search, or hypothesis testing. For example, in a multi-hop QA task, the model could rewind to a decision point where it misinterpreted a clue, and explore a different reasoning path by reweighting attention to a missed clue.\nReinforcement Learning Over Thought Trajectories\nInstead of optimizing only token-level outputs, RL agents could learn to nudge the internal latent codes z_t in directions that increase reward. This enables meta-level control over how the model thinks, not just what it says.\nJust as a gamer practices a difficult boss fight by reloading save points and trying different strategies, an RL system could:\n\nSave a checkpoint at a challenging reasoning step\nTry multiple variations of continuing from that state\nLearn which variations lead to better outcomes\nApply this learning to future instances of similar problems\n\nCausal Debugging\nWhen the model makes a logic error or hallucination, you can trace it back to earlier internal states and inspect where the drift began. You can even compare the faulty path with a corrected one and compute differences in internal representation.\nLatent Space Exploration\nBy editing or interpolating in z_t space, you could explore counterfactuals like \"What would the model have thought if it had interpreted this ambiguous term differently?\" This opens up new dimensions for interpretability research.\nMemory-Efficient Checkpointing\nLong-running chains of thought, like agent loops or multi-turn planning, can be checkpointed and resumed with minimal storage requirements.\nRelated Work\nThis proposal builds upon and connects several research areas:\n\nTransformer Interpretability: Work on understanding attention patterns, feature attribution, and circuit identification in transformers provides evidence for structured internal representations.\n\nNeural Compression: Techniques from neural compression, VAEs, and normalizing flows inform the design of the sidecar architecture.\n\nCheckpointing in Deep Learning: Existing approaches for memory-efficient training via activation checkpointing, though our focus is on inference-time applications.\n\nMeta-Learning and RL: The concept of optimizing over latent trajectories connects to work on meta-reinforcement learning and learned optimizers.\n\nOur method differs by focusing specifically on lightweight, reversible compression tailored to transformer inference.\nChallenges and Limitations\nWhile the proposed approach has significant potential, several challenges and limitations should be acknowledged:\nCompression-Fidelity Trade-off\nThere is an inherent tension between compression ratio and reconstruction fidelity. Higher compression ratios (smaller z_t) will generally result in lower reconstruction quality, potentially affecting downstream model behavior.\nComputational Overhead\nThe sidecar encoder and decoder add computational overhead to each inference step. This must be balanced against the benefits of compression. In time-critical applications, the additional latency might be prohibitive.\nKey/Value Cache Compression\nCompressing and reconstructing the KV cache is particularly challenging due to its large size and growing nature during generation. Specialized techniques may be needed to handle this efficiently while maintaining high fidelity.\nTraining Data Requirements\nThe sidecar models would need to be trained on diverse data to ensure generalization across different types of content and reasoning tasks. Poor generalization could lead to reconstruction artifacts in some contexts.\nLatent Space Quality\nFor advanced applications like RL and latent editing, the quality and structure of the learned latent space is crucial. Ensuring that z_t captures meaningful dimensions of variation requires careful design of the regularization term and training procedure.\nEvaluation Metrics\nThe prototype uses MSE for simplicity, but functional equivalence (e.g., same next-token probabilities) may matter more in practice. Errors could accumulate in long sequences, requiring appropriate metrics to evaluate the system's effectiveness.\nFuture Directions: Toward a Metacognitive Operating System\nLooking forward, introspective compression could form the foundation for a more ambitious system—a metacognitive operating system for transformers. This would enable:\nRewindable Reasoning Graph\nEach z_t becomes a node in a directed acyclic graph of latent thoughts. Edges represent continuation, intervention, or counterfactual alteration. The model can traverse, compare, and optimize over this graph—essentially turning latent space into a version control system for cognition.\nSelf-Coaching Thought Loop\nBy replaying branches and comparing outcomes, the model could identify what worked, what failed, and what reasoning strategies led to success. A coach module could learn from this trace, training a separate controller to guide future latent trajectories more effectively.\nLatent Strategy Transfer\nWith successful reasoning patterns stored as strategy embeddings, the system could apply these strategies across different tasks and domains. This raises intriguing questions about the generality of cognitive strategies and their transferability.\nFuture work could develop:\n\nAttention-based sidecar architectures\nComprehensive compression of the full state, including KV caches\nIntegration of RL to refine latent trajectories, treating z_t as a steerable \"thought space\"\n\nConclusion\nIntrospective compression for transformers addresses two critical limitations: the inability to access internal states and the ephemeral nature of transformer cognition. By learning to compress and reconstruct internal states via a structured latent manifold, we can enable fundamentally new capabilities like reasoning backtracking, thought trajectory optimization, and causal debugging.\nThe proposal outlined here represents a first step toward a more ambitious vision: transformers that aren't just text generators, but systems with transparent, steerable, and improvable cognition. By enabling models to save and manipulate their internal states—like a video game save—we open doors to advanced reasoning and debugging. While significant challenges remain in implementation and scaling, the potential benefits for AI interpretability, capability, and safety make this a promising direction for future research.\nAddendum: Toward a Metacognitive Operating System for Transformers\nTransformers as Replayable Cognitive Systems\nThe introspective compression framework enables a profound shift in how we conceive of transformer models. Rather than treating transformers as mere text generators, we can reimagine them as cognitive systems with replayable, editable thoughts. This gaming analogy is illuminating:\nJust as competitive gamers practice difficult challenges by saving states and trying different strategies, compressed transformer states allow us to:\n\nTreat the transformer like a competitive gamer practicing a hard boss fight—saving state before each attempt, iterating on strategy, and gradually mastering it through focused replay.\n\nThis transforms the nature of transformer inference from a one-shot process into deliberative, iterative cognition. The model becomes capable of exploration, reflection, and self-improvement through internal simulation.\nBeyond RL: Thought Trajectory Optimization\nTraditional reinforcement learning optimizes over action sequences (token outputs). With compressed cognitive states, we can optimize over internal thought trajectories themselves:\nfor rollout in range(N):\n    z_t = saved_state  # load compressed cognition state\n    perturb = policy(z_t)\n    z_t_prime = z_t + perturb\n    h_t_hat = decoder(z_t_prime)\n    resume_inference(h_t_hat)\n    reward = evaluate(output)\n    policy.update(reward)\n\nThis enables meta-level control over reasoning itself, not just outputs. The benefits include:\n\nExploration of alternate thoughts: The model tries variations from known mental waypoints\nCredit assignment across thoughts: RL signals propagate through latent cognition\nEfficient failure recovery: Errors are corrected by revisiting local cognitive context\nDeliberate practice: The model refines specific reasoning sequences through iteration\n\nThe Vision: A Rewindable Reasoning Graph\nAt the heart of this approach is a metacognitive operating system where:\n\nAll thinking becomes a sequence of reversible cognitive states. These states are saved, replayed, steered, mutated, branched, and analyzed—not just at the output level, but in the latent geometry of reasoning itself.\n\nEach compressed state (z_t) becomes a node in a directed acyclic graph of thought, with edges representing continuations, interventions, or counterfactuals. The model traverses this graph like a version control system for cognition:\nclass ThoughtState:\n    def __init__(self, z: torch.Tensor, parent: Optional[str] = None, metadata: Optional[dict] = None):\n        self.id = str(uuid.uuid4())\n        self.z = z.detach().clone().cpu()\n        self.parent = parent\n        self.metadata = metadata or {}\n\nclass ThoughtGraph:\n    def __init__(self):\n        self.nodes: Dict[str, ThoughtState] = {}\n        self.edges: Dict[str, List[str]] = {}  # from -> list of to\n\nSelf-Coaching Thought Loops\nBy replaying branches and comparing outcomes, the model identifies successful reasoning strategies. A coach module learns from this experience, training a controller to guide future latent trajectories:\nclass Controller(nn.Module):\n    def __init__(self, latent_dim: int, hidden_dim: int = 512, num_proposals: int = 4):\n        super().__init__()\n        self.num_proposals = num_proposals\n        self.proposal_net = nn.Sequential(\n            nn.LayerNorm(latent_dim),\n            nn.Linear(latent_dim, hidden_dim), nn.ReLU(),\n            nn.Linear(hidden_dim, latent_dim * num_proposals)\n        )\n        self.latent_dim = latent_dim\n\n    def forward(self, z: torch.Tensor) -> List[torch.Tensor]:\n        out = self.proposal_net(z)\n        proposals = out.view(self.num_proposals, self.latent_dim)\n        return [z + delta for delta in proposals]\n\nThis creates a system where multiple versions of thinking are simulated and compared. The model doesn't just produce sequences; it orchestrates global thought exploration with operations like \"try four continuations,\" \"backtrack to step 7,\" or \"merge the insights from different branches.\"\nTransformers That Practice\nLike elite performers in any domain, the model develops expertise through practice:\n\nIt builds a memory of challenging cognitive states\nIt repeatedly revisits difficult thought regions\nIt explores better continuations through trial and error\nOver time, it internalizes successful patterns without parameter updates\n\nThis happens through a curriculum learning process that targets the most challenging reasoning tasks:\ndef curriculum_loop(agent, memory, curriculum, task_generator, editor_fn, rounds=10):\n    for _ in range(rounds):\n        task_id, input_text, evaluator = task_generator()\n        agent.coach.evaluate = evaluator  # bind task-specific reward\n\n        root = agent.initialize_from_text(input_text)\n        branches = agent.branch_and_score(root)\n        best = max(branches, key=lambda n: n.metadata.get(\"reward\", -float(\"inf\")))\n\n        memory.record(task_id, best)\n        curriculum.update(task_id, best.metadata[\"reward\"])\n\n        if best.metadata[\"reward\"] < 0:\n            agent.edit_and_retry(best, editor_fn)\n\nStrategy Distillation and Transfer\nPerhaps most profoundly, successful reasoning patterns can be distilled into transferable strategy embeddings:\nclass StrategyDistiller(nn.Module):\n    def __init__(self, latent_dim=256, embedding_dim=64):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            nn.LayerNorm(latent_dim),\n            nn.Linear(latent_dim, 128),\n            nn.ReLU(),\n            nn.Linear(128, embedding_dim)\n        )\n        self.strategy_bank = {}  # strategy_id -> embedding vector\n\n    def embed(self, z_seq: List[torch.Tensor]) -> torch.Tensor:\n        z_stack = torch.stack(z_seq)\n        return self.encoder(z_stack.mean(dim=0))\n\nThis raises the profound question: how general are these latent strategies? Do they encode reusable cognitive skills or merely brittle solutions? We can evaluate this through:\n\nCross-Task Similarity: Do successful strategies cluster across diverse domains?\nTransfer Gain: Do strategy embeddings improve performance on new tasks?\nPerturbation Robustness: Do strategies work despite input noise?\nReuse Ratio: How often do different starting points converge when using the same strategy?\nStrategy Lifespan: Which strategies endure versus those that quickly become obsolete?\n\nFrom Machine Learning to Machine Self-Improvement\nThis represents a paradigm shift from machine learning to \"machine self-improvement through reflective latent simulation.\" Traditional ML improves models through gradient updates over many examples. This metacognitive framework enables improvement through self-reflection and rehearsal - more akin to how humans develop expertise.\nThe transformer becomes not merely an inference engine but a cognitive substrate whose thoughts can be saved, explored, and optimized. It develops:\n\nLanguage as Debugger: Latent diffs can be expressed as natural language commentary\nGlobal Thought Orchestration: Speculative branching and merging of reasoning paths\nLatent Curriculum Learning: Tasks become regions of latent space to navigate\n\nImplementation: A Metacognitive Agent\nPutting these pieces together creates a full metacognitive agent:\nclass MetacognitiveAgent:\n    def __init__(self, encoder, decoder, controller, coach, tokenizer):\n        self.encoder = encoder\n        self.decoder = decoder\n        self.controller = controller\n        self.coach = coach\n        self.tokenizer = tokenizer\n        self.graph = ThoughtGraph()\n\n    def branch_and_score(self, node: ThoughtState, k: int = 4) -> List[ThoughtState]:\n        proposals = self.controller(node.z)\n        children = []\n        for z_next in proposals:\n            h_hat = self.decoder(z_next)\n            reward = self.coach.evaluate(h_hat)\n            child = ThoughtState(z=z_next, parent=node.id, metadata={\"reward\": reward})\n            self.graph.add(child)\n            children.append(child)\n        return children\n\nThis agent interacts with tasks, explores branches, identifies weak steps, edits and retries, and outputs its best trajectory. The result is an interactive, reflective, self-improving cognitive system.\nConclusion: Transformers as Deliberative Thinkers\nThe introspective compression framework doesn't just improve transformers - it fundamentally transforms what they are. Models shift from stateless generators to deliberative cognitive systems that:\n\nSave and replay thought states\nPractice and refine reasoning strategies\nDevelop transferable cognitive skills\nExplore counterfactual reasoning paths\nDebug and optimize their own thinking\n\nThis isn't just machine learning. It's machine self-improvement through reflective thought - a significant step toward systems that don't just generate outputs, but learn how to rethink.\nReferences\n\nYao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y., & Narasimhan, K. (2023). Tree of Thoughts: Deliberate Problem Solving with Large Language Models. Advances in Neural Information Processing Systems (NeurIPS).\n\nYang, X.-W., Zhu, X.-Y., Wei, W.-D., Zhang, D.-C., Shao, J.-J., Zhou, Z., Guo, L.-Z., & Li, Y.-F. (2025). Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of Language Models. arXiv preprint arXiv:2502.04404.\n\nSaunshi, N., Dikkala, N., Li, Z., Kumar, S., & Reddi, S. J. (2025). Reasoning with Latent Thoughts: On the Power of Looped Transformers. International Conference on Learning Representations (ICLR).\n\nRae, J. W., Potapenko, A., Jayakumar, S. M., Hillier, C., & Lillicrap, T. P. (2020). Compressive Transformers for Long-Range Sequence Modelling. International Conference on Learning Representations (ICLR).\n\nNawrot, P., Łańcucki, A., Chochowski, M., Tarjan, D., & Ponti, E. M. (2024). Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference. arXiv preprint arXiv:2403.09636.\n\nDehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., & Kaiser, Ł. (2019). Universal Transformers. International Conference on Learning Representations (ICLR).\n\nSchrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L., Schmitt, S., Guez, A., Lockhart, E., Hassabis, D., Graepel, T., Lillicrap, T., & Silver, D. (2020). Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model. Nature, 588, 604-609.\n\nHafner, D., Lillicrap, T., Ba, J., & Norouzi, M. (2020). Dream to Control: Learning Behaviors by Latent Imagination. International Conference on Learning Representations (ICLR).",
    "summary": {
      "en": "**Summary: Real-Time Introspective Compression for Transformers**\n\n**Introduction: Key Issues**\nTransformer-based large language models (LLMs) face two main challenges:\n1. **Lack of Introspection:** These models cannot access their internal states, making it hard to interpret their reasoning or debug them.\n2. **Ephemeral Cognition:** The massive data generated while processing information is often discarded, making it impractical to record.\n\n**Proposed Solution: Introspective Compression**\nThis article introduces a method to address both issues by compressing the internal states of transformers into a manageable format without losing critical information. The key idea is to represent these states as coordinates on a lower-dimensional structure, akin to a map.\n\n**Analogy to Video Game Saves**\nJust like saving a game state allows players to return to specific points in their gameplay, we aim to save the transformer’s internal states efficiently, enabling inspection, rollback, and resumption during inference.\n\n**Technical Framework**\n- **Sidecar Transformers:** A lightweight model runs alongside the main transformer to encode and decode internal states into compact representations.\n- **Components:**\n  - **Main Transformer:** The pretrained model generating hidden states.\n  - **Sidecar Encoder/Decoder:** These models compress and reconstruct hidden states and attention caches.\n\n**Training Methodology**\nThe encoder and decoder are trained to minimize the difference between original and reconstructed states, using a loss function that balances reconstruction quality and latent representation efficiency.\n\n**Architectural Approaches**\nSeveral architectural strategies for compression are explored:\n1. **Layer-Specific:** Each layer has its own encoder/decoder, allowing precise introspection.\n2. **Grouped Layer:** Compresses groups of layers, balancing efficiency with reconstruction quality.\n3. **Unified:** One encoder/decoder for all layers, optimizing for parameter efficiency.\n\n**Handling Key/Value Cache**\nA specialized method is introduced for compressing the key-value cache, which is crucial for the model’s autoregressive generation.\n\n**Potential Applications**\nThe ability to compress internal states opens up new capabilities for transformers, including:\n- **Backtracking in Reasoning:** Exploring alternative paths in reasoning.\n- **Reinforcement Learning over Thought Trajectories:** Learning from past internal states to improve future reasoning.\n- **Causal Debugging:** Tracing errors back to their source for correction.\n\n**Future Directions**\nThe vision is to develop a metacognitive operating system for transformers, enabling them to reflect, explore, and optimize their reasoning processes, much like humans improve through practice and reflection.\n\n**Challenges**\nKey challenges include balancing compression with fidelity, managing computational overhead, and ensuring the quality of the compressed latent space.\n\nIn conclusion, this approach aims to evolve transformers from mere text generators to advanced cognitive systems capable of introspection and improved reasoning.",
      "ko": "트랜스포머 기반의 대형 언어 모델(LLM)은 두 가지 주요 문제에 직면해 있습니다. 첫째, 내부 상태에 대한 접근이 부족하여 모델의 추론 과정을 해석하거나 디버깅하기 어렵습니다. 둘째, 정보 처리 중 생성되는 방대한 데이터가 종종 버려져 기록하기가 비효율적입니다.\n\n이 문제를 해결하기 위해 본 논문에서는 트랜스포머의 내부 상태를 중요한 정보를 잃지 않으면서 관리 가능한 형식으로 압축하는 방법을 제안합니다. 핵심 아이디어는 이러한 상태를 지도와 같은 저차원 구조의 좌표로 표현하는 것입니다.\n\n비디오 게임의 저장 기능과 유사하게, 게임 상태를 저장하면 플레이어가 특정 지점으로 돌아갈 수 있는 것처럼, 우리는 트랜스포머의 내부 상태를 효율적으로 저장하여 추론 중에 검사, 롤백 및 재개할 수 있도록 합니다.\n\n기술적 프레임워크로는 '사이드카 트랜스포머'라는 경량 모델이 주요 트랜스포머와 함께 작동하여 내부 상태를 압축된 표현으로 인코딩하고 디코딩합니다. 주요 구성 요소로는 숨겨진 상태를 생성하는 사전 훈련된 모델인 주요 트랜스포머와 숨겨진 상태 및 주의 캐시를 압축하고 재구성하는 사이드카 인코더/디코더가 있습니다.\n\n인코더와 디코더는 원본 상태와 재구성된 상태 간의 차이를 최소화하도록 훈련되며, 재구성 품질과 잠재 표현 효율성을 균형 있게 조절하는 손실 함수를 사용합니다.\n\n압축을 위한 여러 건축적 접근 방식이 탐구됩니다. 첫째, 각 층마다 고유한 인코더/디코더를 두어 정밀한 내부 상태 검토를 가능하게 하는 층별 접근 방식이 있습니다. 둘째, 층 그룹을 압축하여 효율성과 재구성 품질의 균형을 맞추는 그룹 층 접근 방식이 있습니다. 셋째, 모든 층에 대해 하나의 인코더/디코더를 사용하여 매개변수 효율성을 최적화하는 통합 접근 방식이 있습니다.\n\n모델의 자기 회귀 생성에 중요한 키-값 캐시를 압축하기 위한 전문적인 방법도 도입되었습니다.\n\n내부 상태를 압축할 수 있는 능력은 트랜스포머에 새로운 기능을 열어줍니다. 예를 들어, 추론 과정에서 대안 경로를 탐색하거나, 과거 내부 상태에서 학습하여 미래의 추론을 개선하는 강화 학습, 오류를 추적하여 수정하는 인과적 디버깅 등이 가능합니다.\n\n미래의 방향은 트랜스포머를 위한 메타인지 운영 체제를 개발하여, 인간이 연습과 반성을 통해 개선하듯이 트랜스포머가 자신의 추론 과정을 반영하고 탐색하며 최적화할 수 있도록 하는 것입니다.\n\n주요 도전 과제는 압축과 충실도 간의 균형을 맞추고, 계산 오버헤드를 관리하며, 압축된 잠재 공간의 품질을 보장하는 것입니다. 이 접근 방식은 트랜스포머를 단순한 텍스트 생성기에서 내부 성찰과 개선된 추론이 가능한 고급 인지 시스템으로 발전시키는 것을 목표로 하고 있습니다.",
      "ja": "トランスフォーマーにおけるリアルタイム内省圧縮についての要約です。\n\nトランスフォーマーを基にした大規模言語モデル（LLM）は、主に二つの課題に直面しています。一つ目は内省の欠如です。これらのモデルは内部状態にアクセスできないため、その推論を解釈したりデバッグしたりするのが難しいです。二つ目は短命な認知です。情報処理中に生成される膨大なデータはしばしば捨てられ、記録することが実用的ではありません。\n\nこの記事では、トランスフォーマーの内部状態を重要な情報を失うことなく管理可能な形式に圧縮する方法を提案します。この方法の核心は、内部状態を地図のような低次元構造上の座標として表現することです。\n\nゲームのセーブデータに例えると、ゲームの状態を保存することでプレイヤーが特定のポイントに戻れるように、トランスフォーマーの内部状態を効率的に保存し、推論中に検査やロールバック、再開を可能にすることを目指しています。\n\n技術的な枠組みとして、サイドカートランスフォーマーという軽量モデルがメインのトランスフォーマーと並行して動作し、内部状態をコンパクトな表現にエンコード・デコードします。メインのトランスフォーマーは隠れ状態を生成する事前学習済みモデルであり、サイドカーエンコーダー/デコーダーは隠れ状態や注意キャッシュを圧縮・再構築します。\n\nトレーニング方法としては、エンコーダーとデコーダーが元の状態と再構築された状態の違いを最小限に抑えるように訓練されます。この際、再構築の質と潜在表現の効率をバランスさせる損失関数が使用されます。\n\n圧縮のためのいくつかのアーキテクチャ戦略が探求されています。レイヤーごとに独自のエンコーダー/デコーダーを持つレイヤー特化型、レイヤーのグループを圧縮するグループレイヤー型、全レイヤーに対して一つのエンコーダー/デコーダーを使用する統一型があります。\n\nモデルの自己回帰生成に重要なキー/バリューキャッシュを圧縮するための特別な方法も導入されています。\n\n内部状態を圧縮する能力は、トランスフォーマーに新たな機能をもたらします。例えば、推論におけるバックトラッキングや、過去の内部状態から学ぶ強化学習、エラーの原因を追跡する因果デバッグなどが可能になります。\n\n将来的には、トランスフォーマーが自らの推論プロセスを反省し、探求し、最適化できるメタ認知オペレーティングシステムの開発を目指しています。これは、人間が練習と反省を通じて向上するのに似ています。\n\n主な課題としては、圧縮と忠実性のバランス、計算オーバーヘッドの管理、圧縮された潜在空間の質の確保があります。このアプローチは、トランスフォーマーを単なるテキスト生成器から内省と推論の向上が可能な高度な認知システムへと進化させることを目指しています。"
    }
  },
  {
    "id": "edb1508c545f0398",
    "title": {
      "en": "\"Fiume O Morte \" Brilliantly Dramatizes the Rise of a Demagogue",
      "ko": "\"민주주의의 그림자\"",
      "ja": "「フィウメの影」デマゴーグの台頭"
    },
    "type": "story",
    "url": "https://www.newyorker.com/culture/the-front-row/fiume-o-morte-brilliantly-dramatizes-the-rise-of-a-demagogue",
    "score": 15,
    "by": "rbanffy",
    "time": 1743605471,
    "content": "The Front Row“Fiume o Morte!” Brilliantly Dramatizes the Rise of a DemagogueIgor Bezinović’s film thrusts century-old archival footage into the present, restaging the brazen reign of an autocrat whose tactics feel startlingly resonant today.By Richard BrodyApril 1, 2025Courtesy Film at Lincoln CenterSave this storySave this storySave this storySave this storyMany filmmakers display undue faith in their ability to depict ways of life far outside their own experience. This blithe self-confidence is particularly egregious in depictions of distant history, where imagination inevitably courts fabrication. The Croatian director Igor Bezinović confronts this problem boldly and brilliantly in “Fiume o Morte!” (“Fiume or Death!”) by showing his process. He combines nonfiction elements with fictionalizations of historical events—and reveals the behind-the-scenes creation of these reënactments, turning the work into a documentary about its own making. The film, which will screen on April 4th and 5th in the “New Directors/New Films” series at both MOMA and Film at Lincoln Center, is centered on an astounding true story that took place in Bezinović’s home town of Rijeka, and his telling emphasizes both his personal connection to the saga and the strangeness of dramatizing it.The film focusses on a tumultuous period between 1919 and 1921, when the Italian-nationalist poet Gabriele D’Annunzio, in defiance of his own government, led a convoy of rebel soldiers into Rijeka—a town, on the Adriatic Coast, which had a large Italian minority and was then widely known by its Italian name, Fiume. Quickly consolidating power, D’Annunzio ruled Fiume as a dictator. The results were oppressive for the city and—because D’Annunzio’s exploits won the admiration of the younger, even more ambitious Italian nationalist Benito Mussolini—catastrophic for the world. Bezinović presents the story of D’Annunzio’s autocratic rise, reign, and fall in a way that’s as unusual as it is revelatory. He gathers a teeming array of archival material and a teeming cast of actors—mostly nonprofessionals, many recruited via person-in-the-street interviews—to re-create the archival depictions of the occupation.Bezinović, far from shrugging off the peculiarity of the procedure, calls attention to it, with humorous barbs aimed in multiple directions—including at himself. The casting interviews start out as informational ones, as he stops passersby to ask whether they know of D’Annunzio. Many, especially younger ones, don’t. Those who do—mainly elders—have little good to say of him; one calls him “a horrible fascist” and adds, “They’re still around today.” Amid these spontaneous discussions of the former dictator, Bezinović asks middle-aged men who happen to be bald, like D’Annunzio, whether they’d be willing to portray him. Bezinović also pays special attention to those who speak Fiuman, the town’s then prevalent, now rare Venetian dialect—not least because he integrates the language into the soundtrack, sharing the duties of voice-over narration with the participants whom he films.Throughout, Bezinović enlists other amateurs in smaller roles. Young men are invited to play soldiers; upon locating the hotel (now a private dwelling) where D’Annunzio spent a night en route to the takeover, Bezinović recruits the woman of the house to play the chambermaid ministering to the traveller. Some of these scenes require drastic transformations: the storefront that housed D’Annunzio’s favorite tavern, reputedly the site of his and his cronies’ licentious revels, is now a nail salon, which, with the consent of its owner, the cast and crew turn into a set-like reproduction of the long-ago haunt. But at other times there’s a seemingly deliberate dissonance, as when the would-be dictator drives into town at the wheel of a snazzily modern red convertible sports car. In one memorable sequence, D’Annunzio is played by a professional guitarist who, after acting out a crucial showdown with an Italian general, provides his own raucous musical accompaniment to the aftermath.One crucial aspect to Bezinović’s antic yet earnest restagings is that they do more than merely represent the events seen in archival visual sources. He also replicates the compositions, the framings, of the originals, engaging in his own form of directorial reënactment. It’s a fraught gesture, because some of the sources that he mimics are themselves implicated in the movie’s grim history. Many of the archival images are the work of D’Annunzio’s own so-called Photography Section, which, as Bezinović relates, took ten thousand pictures; the director wryly comments, “It was crucial to him that the occupation be remembered, as it is to us who are making this film.” Bezinović thereby shares in the political risks of the project, putting himself into the same ethical position, behind the camera, as the actors impersonating D’Annunzio in front of it.The movement of memory goes both ways—not only does “Fiume o Morte!” conjure the past, it pulls the events of the past into the present tense, conveying an eerie feeling of witnessing them occurring in real time. Bezinović’s sense of the city’s history involves a native’s intimacy with its locations, its monuments, its traditions. The director visits sites that figured prominently in D’Annunzio’s reign, such as a present-day apartment building that was then a prison, and reveals surviving traces and artifacts of the regime. He counts off the years, from the late nineteenth century through the First World War, by filming the floors of buildings in which the year of construction is set in mosaics. By means of a simple and ingenious twist of editing, Bezinović makes his reënactments point both ways, too: he frequently puts his own restagings ahead of the sources he’s copying, as if visually proclaiming his astonishment that the seeming absurdities he himself puts onscreen are the undeniable realities of D’Annunzio’s time.Bezinović brings to the fore the racial hatred on which D’Annunzio’s iron-fisted, thin-skinned rule was based, and the ethnic cleansing that it involved. The movie cites his first visit to Fiume, in 1907, when he attended a play in which, the voice-over says, “Slavs are referred to as ‘thieves’ and Croatians as ‘wolves.’ ” The strutting demagogue, who declared “whoever is not with us is against us,” meted out violence on nationalistic pretexts: when Italians were killed in the nearby city of Split during clashes between supporters of D’Annunzio and the local population, he ordered the physical destruction of non-Italian businesses and institutions throughout Fiume. (The attacks, too, are reënacted, to jolting effect, at their actual sites.) When Italy sought a referendum in Fiume on D’Annunzio’s government, he first agreed but, realizing that it was coming out against him, sent troops to disrupt the vote, which was then ignored. Bezinović notes that the otherwise thoroughly documented regime somehow failed to photograph these doings—then dramatizes them nonetheless.Bezinović also visually apostrophizes D’Annunzio’s fanatical nationalism with footage of a current-day rally, with chants and fireworks, sparked by the rivalry between Rijeka and Split’s soccer teams: an everyday expression of the identitarian pride so easily weaponized by opportunists. Sports played a role in the authoritarian ethos imposed by D’Annunzio, and so did culture—he created a code of conduct that required his troops to excel in a wide range of athletic skills, singing and dancing, and the odd practice of imitating human and animal voices. The aesthetic regimen for body and mind was matched by a tune that dominates the soundtrack of “Fiume o Morte!”—a marchlike song called “Giovinezza” (“Youth”), which, under Mussolini, became the official Italian Fascist anthem.The endgame of fanatical youth is cruel. The city’s residents tired of D’Annunzio’s reckless rule, and his bellicose posturing and expansionist maneuvers discomfited the Italian monarchy. The dénouement came on Christmas Eve, in 1920, when the increasingly isolated D’Annunzio declared war on his native Italy. The playful enthusiasm that his soldiers had hitherto displayed in martial training and sporty festivities, and the esprit de corps forged in their common cause, gave way to their bloodied corpses; the film’s depictions of their gory stillness, in full color, come as a shock.D’Annunzio slunk back to Italy the following month with a bombastic air of satisfaction, of a mission accomplished, and lived in palatial isolation. (Mussolini supposedly likened D’Annunzio to a bad tooth: “You either pull it out or cover it in gold.”) The palace, Bezinović notes, is now a tourist attraction. What “Fiume o Morte!” makes plain is the ease with which a motivated demagogue and a coterie of followers, taking advantage of a divided citizenry, can ride waves of complicity and complacency to absolute power. As one actor, preparing to impersonate the dictator, tells Bezinović, “I think it’s a lovely game that has to be approached with a great seriousness.” The movie’s historical re-creations, however derisive, are chilling. The story’s immediacy is intensified not only by clever dramatics but by its echoes of current events, for which D’Annunzio provides—depending on one’s views—a cautionary tale or a cookbook. ♦",
    "summary": {
      "en": "The film \"Fiume o Morte!\" by Igor Bezinović creatively combines historical footage and dramatizations to explore the rise of the Italian nationalist leader Gabriele D’Annunzio in Rijeka, Croatia, from 1919 to 1921. Bezinović, who is from Rijeka, uses a mix of archival material and actors—many of whom are locals—to recreate events from this turbulent period, emphasizing both his personal connection to the history and the challenges of accurately depicting the past.\n\nD’Annunzio led a rebellion against the Italian government, ruling Rijeka as a dictator and influencing future leaders like Benito Mussolini. The film highlights the oppressive nature of D’Annunzio's regime, including its roots in racial hatred and violence against non-Italians. Bezinović incorporates local dialects and modern reflections on nationalism, making the historical events feel relevant today.\n\nThroughout the film, Bezinović engages with the community, casting locals in roles and humorously reflecting on the process of reenactment. He draws parallels between D’Annunzio’s era and contemporary political dynamics, warning of the dangers posed by demagogues exploiting divisions in society. The film ends with a stark reminder of the consequences of D’Annunzio's rule, showcasing the dark turn of events as his regime collapsed. Overall, \"Fiume o Morte!\" serves as both a historical recounting and a cautionary tale about the ease with which autocratic power can rise.",
      "ko": "영화 \"Fiume o Morte!\"는 이고르 베지노비치 감독이 역사적 영상과 드라마를 창의적으로 결합하여 1919년부터 1921년까지 크로아티아 리예카에서 이탈리아 민족주의 지도자 가브리엘레 다눈치오의 부상을 탐구합니다. 리예카 출신인 베지노비치는 아카이브 자료와 지역 배우들을 혼합하여 이 격동의 시기를 재현하며, 역사에 대한 개인적인 연결과 과거를 정확하게 묘사하는 데 어려움을 강조합니다.\n\n다눈치오는 이탈리아 정부에 맞서 반란을 이끌었고, 리예카를 독재자로 통치하며 베니토 무솔리니와 같은 미래의 지도자들에게 영향을 미쳤습니다. 이 영화는 다눈치오 정권의 억압적인 성격을 부각시키며, 그 뿌리가 인종적 증오와 비이탈리아인에 대한 폭력에 있음을 보여줍니다. 베지노비치는 지역 방언과 현대의 민족주의에 대한 반성을 포함시켜 역사적 사건들이 오늘날에도 여전히 관련성이 있음을 느끼게 합니다.\n\n영화 전반에 걸쳐 베지노비치는 지역 사회와 소통하며, 지역 주민들을 배역으로 캐스팅하고 재연 과정에 대한 유머를 담아냅니다. 그는 다눈치오 시대와 현대 정치의 역학을 비교하며, 사회의 분열을 이용하는 선동가들이 초래할 수 있는 위험에 대해 경고합니다. 영화는 다눈치오 정권의 결과를 강하게 상기시키며, 그의 정권이 무너지는 어두운 사건들을 보여주면서 마무리됩니다. \"Fiume o Morte!\"는 역사적 서술이자 독재 권력이 쉽게 부상할 수 있음을 경고하는 이야기로 기능합니다.",
      "ja": "イゴール・ベジノビッチ監督の映画「フィウメ・オ・モルテ！」は、歴史的な映像とドラマを巧みに組み合わせ、1919年から1921年にかけてクロアチアのリイェカでイタリアの民族主義指導者ガブリエレ・ダンヌンツィオが台頭する様子を探ります。リイェカ出身のベジノビッチは、アーカイブ素材と地元の俳優を使って、この激動の時代の出来事を再現し、歴史への個人的なつながりと過去を正確に描写することの難しさを強調しています。\n\nダンヌンツィオはイタリア政府に対して反乱を起こし、独裁者としてリイェカを支配しました。彼の影響を受けたのはベニート・ムッソリーニなどの後の指導者たちです。映画では、ダンヌンツィオの政権の抑圧的な性質が強調され、非イタリア人に対する人種的憎悪や暴力の根源が描かれています。ベジノビッチは地元の方言や現代のナショナリズムに対する考察を取り入れ、歴史的な出来事が今日の私たちにとっても関連性があることを示しています。\n\n映画全体を通じて、ベジノビッチは地域社会と関わりを持ち、地元の人々をキャストに起用し、再現のプロセスをユーモラスに描写しています。彼はダンヌンツィオの時代と現代の政治的動態との類似点を引き合いに出し、社会の分断を利用するデマゴーグの危険性について警告しています。映画は、ダンヌンツィオの政権が崩壊する際の暗い出来事を示し、彼の支配の結果を厳しく思い起こさせる形で終わります。「フィウメ・オ・モルテ！」は、歴史の再現であると同時に、独裁的な権力がいかに容易に台頭するかについての警鐘を鳴らす物語でもあります。"
    }
  },
  {
    "id": "8fd21a9c6cae1259",
    "title": {
      "en": "Why is the world losing color?",
      "ko": "세상이 왜 칼라를 잃었나?",
      "ja": "世界の色が消える理由"
    },
    "type": "story",
    "url": "https://www.culture-critic.com/p/why-is-the-world-losing-color",
    "score": 143,
    "by": "trevin",
    "time": 1743606166,
    "content": "Share this postThe CulturistWhy Is the World Losing Color?Copy linkFacebookEmailNotesMoreDiscover more from The CulturistPursuing the true, good and beautiful — in history, art and culture.Over 136,000 subscribersSubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.Already have an account? Sign inWhy Is the World Losing Color?The rise of Chromophobia...The CulturistApr 02, 2025194Share this postThe CulturistWhy Is the World Losing Color?Copy linkFacebookEmailNotesMore4847ShareWalk around in the average parking lot, and you’ll find yourself in a sea of black, white, and silver vehicles. Watch Netflix at home or catch a film in the theaters, and you’ll get the same washed-out color grade on either screen. Glance at the logos of the world’s largest companies, and you’ll notice a shrinking palette.It all points to one thing: color is vanishing from our world.This isn’t just a hunch. Studies of everything from car paint to consumer objects show that we’re in the midst of a vast aesthetic shift. What used to be vibrant has become sterile. What used to pop out and catch our attention now fades into the background.The question is — why?The answer isn’t just about fashion or materials, but is rooted in a much older understanding of the relationship between color and truth.Here’s why color is disappearing from our world, and what we can do to bring it back…Reminder: you can support us and get tons of members-only content for a few dollars per month 👇Full-length articles every Wednesday and SaturdayMembers-only podcasts and exclusive interviewsThe entire archive of great literature, art and philosophy breakdownsSubscribeApproximately 1% of our readers currently support us with a paid subscription. We are almost entirely reader-supported, so a paid upgrade helps our mission immensely. 🙏A Monochrome WorldThe colors around us aren’t just changing. They’re disappearing.According to major auto paint suppliers, more than 80% of new cars are now grayscale. Black, white, gray, and silver dominate the roads. Reds, blues, and greens in auto production are increasingly rare.It’s not just cars — a study of over 7,000 objects in the UK’s Science Museum found that the colors of consumer goods have been steadily neutralized since 1800. Bright, saturated tones have been giving way to gray, beige, and taupe for centuries.Graphic design has followed a similar trend. Streaming platforms, fashion brands, and e-commerce hubs are consistently rebranding in black-and-white. Most recently, HBO’s move to rename its service “Max” was accompanied by a logo redesign that stripped away its original blue — replacing it, of course, with stark white text on a black background.Even cinema has gone gray. Although Ridley Scott’s Napoleon was shot on vivid, richly colored sets, its final color-grade — like that of many historical dramas — washes out all the colors in a somber, blueish-gray tint. It’s a visual style that has become so ubiquitous that directors like Wes Anderson are often considered “unorthodox” for their use of vibrant color in film.On the surface, there are some straightforward reasons for this. Industrial materials like steel and plastic, for example, are produced in neutral shades. Grayscale branding for logos is easier to reproduce and scale. Muted palettes are less likely to alienate customers.But this isn't the whole story. To fully understand why color is disappearing from our world, we have to go further back…The Philosophers’ Suspicion of ColorColor has always had a strange status in Western philosophy — and more often than not, that status is second-class.In Chromophobia, art theorist David Batchelor argues that the devaluation of color can be traced to the very birth of Western thought. From Plato onward, color was treated as a distraction: sensory noise that got in the way of rational understanding.Plato described the world of appearances as a deceptive “prison-house,” i.e., a realm of illusion where truth could only be grasped by looking beyond the senses. Color, tied directly to sensation, was thus something to overcome — not to embrace.Aristotle echoed the sentiment. In Poetics, he argued that the power of artwork lay in its structure, not its palette:“A random distribution of the most attractive colors would never yield as much pleasure as a definite image without color.”For Aristotle, it’s form that holds meaning — not hue.This view persisted through the Enlightenment, with German philosopher Immanuel Kant arguing that while color may add charm to art, it has no bearing on true aesthetic judgment. In his view, color neither touches reason nor elevates the mind.The underlying theory in all of these cases is that while color is sensory, unstable, and chaotic, form is rational, stable, and pure. Once you see this bias, you begin to notice how deeply it has shaped the modern world — and how it helps explain our current retreat into colorlessness.Minimalism, Mass Markets, and Beige BeatsThe modernist philosophy developed in the early 20th century helped push Western culture’s underlying suspicion of color to its extreme. For architects like Adolf Loos, color was a kind of primitive indulgence — the enemy of clarity and seriousness.In his 1910 lecture Ornament and Crime, Loos celebrated a future without decoration or color, where aesthetic purity came from form alone. “We have gone beyond ornament,” he famously declared. “We have achieved plain, undecorated simplicity.”The legacy of his ideal is everywhere: sterile office parks, concrete apartment blocks, glass-and-steel towers that all look the same. Mass market forces mean that everything from buildings to branding is designed to appeal to everyone — but in doing so, it resonates with no one.The same impulse is even reshaping music. In the streaming era, songs are engineered to appeal to massive, borderless audiences. The result is a flattening of the sensory experience — from reduced dynamic range (the difference between the loudest and quietest parts of a song) to less “complex” musical elements like a key change. In other words, the musical equivalent of beige walls.Behind all of this is a belief that to be rational is to suppress the sensory, and that the more universal something is intended to be, the less color it can afford to show.Brands that want to be taken \"seriously\" choose muted storefronts — unlike, say, a colorful book or jewelry store with no such ambitions.But it doesn’t have to be this way…Towards a Vibrant FutureIn today’s world, we often associate vivid color with chaos, childishness, or excess. But history offers plenty of counterexamples — times where color and form worked together to overwhelm, inspire, and elevate.Baroque art, for example, is full of riotous color. Its churches and canvases explode with golds, reds, blues, and greens. But it doesn't result in chaos. The color is structured, and works within a powerful formal logic that stirs both emotion and intellect. You feel it, and you follow it.Baroque art stands in direct defiance of the chromophobic worldview. It doesn’t strip down experience in the name of order, but rather builds it up — embracing sensation and structure together.It also provides a useful reminder that color doesn’t have to mean disorder. It doesn’t have to undermine seriousness. And indeed, the reflex to strip it away may reveal more about our cultural discomfort than our aesthetic taste. For when we mute our surroundings, we risk muting ourselves.It’s time to bring color back.Subscribe to The CulturistThousands of paid subscribersPursuing the true, good and beautiful — in history, art and culture.SubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.194Share this postThe CulturistWhy Is the World Losing Color?Copy linkFacebookEmailNotesMore4847Share",
    "summary": {
      "en": "The article \"Why Is the World Losing Color?\" discusses the trend of diminishing color in our surroundings, particularly in cars, media, and consumer products. It highlights that over 80% of new cars are now in grayscale, and studies show a shift from vibrant colors to neutral tones in various objects since 1800. This decline in color is linked not just to fashion but to deeper philosophical beliefs that have historically devalued color.\n\nPhilosophers like Plato and Aristotle viewed color as a distraction from truth, favoring form and structure instead. This skepticism towards color influenced modernist design, leading to a preference for minimalism and uniformity, evident in architecture, branding, and even music.\n\nThe article argues that color can coexist with meaning and structure, as seen in Baroque art, which uses vibrant colors to inspire and elevate rather than create chaos. It calls for a return to color in our environments, suggesting that embracing color can enhance our experience and expression.",
      "ko": "\"세상이 색을 잃어가고 있는 이유\"라는 기사는 우리 주변에서 색이 줄어드는 경향에 대해 다루고 있습니다. 특히 자동차, 미디어, 소비재에서 이러한 현상이 두드러집니다. 현재 새로 출시되는 자동차의 80% 이상이 회색조로 되어 있으며, 1800년 이후 다양한 물체에서 생동감 있는 색상에서 중립적인 색조로의 변화가 관찰되고 있습니다. 이러한 색의 감소는 단순한 패션의 변화뿐만 아니라 역사적으로 색을 경시해온 깊은 철학적 신념과도 관련이 있습니다.\n\n플라톤과 아리스토텔레스와 같은 철학자들은 색을 진리에서 벗어나게 하는 방해물로 보았고, 대신 형태와 구조를 중시했습니다. 이러한 색에 대한 회의적인 시각은 현대 디자인에 영향을 미쳐 미니멀리즘과 균일성을 선호하게 만들었습니다. 이는 건축, 브랜드 디자인, 심지어 음악에서도 나타납니다.\n\n이 기사는 색이 의미와 구조와 함께 공존할 수 있다고 주장합니다. 바로크 예술에서 볼 수 있듯이, 생동감 있는 색상은 혼란을 일으키기보다는 영감을 주고 고양시키는 역할을 합니다. 따라서 우리의 환경에서 색을 되찾아야 한다고 강조하며, 색을 받아들이는 것이 우리의 경험과 표현을 풍부하게 할 수 있다고 제안합니다.",
      "ja": "「世界はなぜ色を失いつつあるのか？」という記事では、周囲の色が減少している傾向について、特に車やメディア、消費財に焦点を当てています。新車の80%以上がグレースケールであることが強調されており、1800年以降、さまざまな物の色が鮮やかなものから中立的なトーンに移行していることを示す研究もあります。この色の減少は、ファッションだけでなく、歴史的に色を軽視してきた深い哲学的信念とも関連しています。\n\nプラトンやアリストテレスのような哲学者は、色を真実からの気を散らすものと見なし、形や構造を重視しました。この色に対する懐疑的な姿勢は、モダニズムデザインに影響を与え、建築やブランディング、さらには音楽においてもミニマリズムや均一性を好む傾向を生み出しました。\n\n記事では、色は意味や構造と共存できると主張しています。バロック芸術のように、鮮やかな色を使って混乱を生むのではなく、インスピレーションを与えたり高めたりすることができると述べています。私たちの環境に色を取り戻すことを呼びかけ、色を受け入れることで私たちの体験や表現が豊かになると提案しています。"
    }
  },
  {
    "id": "a0bb452e5ac9239e",
    "title": {
      "en": "A 6-Hour Time-Stretched Version of Brian Eno's Music for Airports",
      "ko": "브라이언 이노의 공항 음악 6시간 버전",
      "ja": "空港音楽の6時間版"
    },
    "type": "story",
    "url": "https://www.openculture.com/2025/03/a-6-hour-time-stretched-version-of-brian-enos-music-for-airports.html",
    "score": 163,
    "by": "vinhnx",
    "time": 1743295821,
    "content": "Writ­ing in his 1995 diary about his sem­i­nal ambi­ent album Music for Air­ports, Eno remem­bered his ini­tial thoughts going into it: “I want to make a kind of music that pre­pares you for dying–that doesn’t get all bright and cheer­ful and pre­tend you’re not a lit­tle appre­hen­sive, but which makes you say to your­self, ‘Actu­al­ly, it’s not that big a deal if I die.’”\nCre­at­ed in 1978 from sec­onds-long tape loops from a much longer improv ses­sion with musi­cians includ­ing Robert Wyatt, Music for Air­ports start­ed the idea of slow, med­i­ta­tive music that aban­doned typ­i­cal major and minor scales, brought in melod­ic ambi­gu­i­ty, and began the explo­ration of sounds that were designed to exist some­where in the back­ground, beyond the scope of full atten­tion.\n\nFor those who think 50 min­utes is too short and those piano notes too rec­og­niz­able, may we sug­gest this 6‑hour, time-stretched ver­sion of the album, cre­at­ed by YouTube user “Slow Motion TV.” The tonal field is the same, but now the notes are no attack, all decay. It’s gran­u­lar as hell, but you could imag­ine the whole piece unspool­ing unno­ticed in a ter­mi­nal while a flight is delayed for the third time. (Maybe that’s when the accep­tance of death hap­pens, when you’ve giv­en up on ever get­ting home?)\nUnlike Music for Films, which fea­tured sev­er­al tracks Eno had giv­en to film­mak­ers like Derek Jar­man, it took some time for Music for Air­ports to be real­ized in its intend­ed loca­tion: being piped in at a ter­mi­nal at La Guardia, New York, some­time in the 1980s. And that was just a one-time thing.\n\n?si=6VMY_yEfZdFDZbp6\nThe album seemed des­tined for per­son­al use only, but then in 1997 the mod­ern ensem­ble Bang on a Can played it live, trans­lat­ing the ran­dom­ness of out-of-sync tape loops into music nota­tion. Over the years they’ve per­formed it at air­ports in Brus­sels, the Nether­lands and Liv­er­pool, and in 2015 the group brought it to Ter­mi­nal 2 of San Diego Inter­na­tion­al. Writ­ing for KCET, Alex Zaragoza report­ed that “cry­ing babies, echoes of rolling suit­cas­es and board­ing pass­es serv­ing as tick­ets to the con­cert failed to remind any­one that they were, indeed, at one of the busiest air­ports in the coun­try. Even the tell­tale announce­ments were there: Air­port secu­ri­ty is every­one’s respon­si­bil­i­ty. Do not leave bags unat­tend­ed.”\nAnd then in 2018, Lon­don City Air­port played the orig­i­nal album in a day-long loop for the album’s 40th anniver­sary.\nAs site-spe­cif­ic mul­ti-media art builds pop­u­lar­i­ty in the 21st cen­tu­ry with increas­ing­ly cheap­er and small­er tech­nol­o­gy, we might hope to hear ambi­ent drones, and not clas­sic rock or pop, in more and more land­scapes.\nNote: An ear­li­er ver­sion of this post appeared on our site in 2019.\nRelat­ed Con­tent:\nBri­an Eno’s Ambi­ent AlbumMusic for Air­portsPer­formed by Musi­cians in an Air­port\nDecon­struct­ing Bri­an Eno’sMusic for Air­ports: Explore the Tape Loops That Make Up His Ground­break­ing Ambi­ent Music\nBri­an Eno’s Advice for Those Who Want to Do Their Best Cre­ative Work: Don’t Get a Job\nBehold the Orig­i­nal Deck of Oblique Strate­gies Cards, Hand­writ­ten by Bri­an Eno Him­self\nBri­an Eno Explains the Loss of Human­i­ty in Mod­ern Music\n Ted Mills is a free­lance writer on the arts who cur­rent­ly hosts the artist inter­view-based FunkZone Pod­cast and is the pro­duc­er of KCR­W’s Curi­ous Coast. You can also fol­low him on Twit­ter at @tedmills, read his oth­er arts writ­ing at tedmills.com and/or watch his films here.",
    "summary": {
      "en": "In his 1995 diary, musician Brian Eno reflected on his 1978 ambient album \"Music for Airports,\" describing it as music that helps people accept the idea of dying. The album uses short tape loops from improvisation sessions and aims to create a meditative atmosphere, avoiding traditional musical scales and encouraging a background listening experience.\n\nFor those wanting a longer experience, a YouTube user created a 6-hour stretched version of the album, emphasizing its continuous, atmospheric qualities. Although originally intended for personal use, the album was later performed live by the ensemble Bang on a Can in various airports, including a notable performance in San Diego in 2015, where the surrounding airport sounds blended with the music.\n\nIn 2018, London City Airport celebrated the album's 40th anniversary by playing it on a loop. As technology advances, there is hope for more ambient music in public spaces rather than traditional pop or rock.",
      "ko": "1995년 일기에서 음악가 브라이언 이노는 1978년 앰비언트 앨범 \"Music for Airports\"에 대해 반성하며, 이 음악이 사람들이 죽음을 받아들이는 데 도움을 준다고 설명했습니다. 이 앨범은 즉흥 연주 세션에서 나온 짧은 테이프 루프를 사용하여 명상적인 분위기를 조성하며, 전통적인 음악 스케일을 피하고 배경 음악으로 듣는 경험을 강조합니다.\n\n더 긴 경험을 원하는 사람들을 위해 한 유튜브 사용자가 이 앨범의 6시간짜리 늘린 버전을 만들었습니다. 이 버전은 앨범의 연속적이고 분위기 있는 특성을 강조합니다. 원래 개인적인 용도로 제작된 이 앨범은 나중에 밴드 방 온 어 캔(Bang on a Can)에 의해 여러 공항에서 라이브로 공연되었습니다. 특히 2015년 샌디에이고에서의 공연에서는 주변 공항 소음이 음악과 어우러지는 모습이 인상적이었습니다.\n\n2018년 런던 시티 공항은 이 앨범의 40주년을 기념하여 앨범을 반복 재생했습니다. 기술이 발전함에 따라 전통적인 팝이나 록 음악 대신 공공 장소에서 더 많은 앰비언트 음악이 사용될 수 있기를 기대하고 있습니다.",
      "ja": "1995年の日記の中で、音楽家のブライアン・イーノは1978年のアンビエントアルバム「Music for Airports」について振り返りました。このアルバムは、人々が死という概念を受け入れる手助けをする音楽だと述べています。アルバムは即興セッションからの短いテープループを使用しており、伝統的な音階を避け、瞑想的な雰囲気を作り出すことを目指しています。リスナーは背景音楽として楽しむことができます。\n\nより長い体験を求める人のために、あるYouTubeユーザーがアルバムの6時間版を作成しました。このバージョンは、アルバムの連続的で雰囲気のある特性を強調しています。元々は個人的な使用を目的としていましたが、後にバン・オン・ア・キャンというアンサンブルが様々な空港でライブ演奏を行い、特に2015年のサンディエゴでのパフォーマンスでは、周囲の空港の音と音楽が融合しました。\n\n2018年、ロンドンシティ空港はアルバムの40周年を祝うために、アルバムをループで流しました。技術が進歩する中で、従来のポップやロック音楽ではなく、公共の場でより多くのアンビエント音楽が流れることへの期待が高まっています。"
    }
  },
  {
    "id": "358ed3c32890948c",
    "title": {
      "en": "A dramatic Einstein ring seen by Webb",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://phys.org/news/2025-04-einstein-webb.html",
    "score": 93,
    "by": "programd",
    "time": 1743604368,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "87eb77a7f074f2df",
    "title": {
      "en": "Electron band structure in germanium, my ass (2001)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://pages.cs.wisc.edu/~kovar/hall.html",
    "score": 883,
    "by": "tux3",
    "time": 1743510312,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "a0110ceecf15c19f",
    "title": {
      "en": "PaperBench",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://openai.com/index/paperbench",
    "score": 8,
    "by": "meetpateltech",
    "time": 1743613605,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "410eace850ea5878",
    "title": {
      "en": "A steam locomotive from 1993 broke my yarn test",
      "ko": "1993 기차의 반란",
      "ja": "蒸気機関車の衝撃"
    },
    "type": "story",
    "url": "https://blog.cloudflare.com/yarn-test-suffers-strange-derailment/",
    "score": 139,
    "by": "jgrahamc",
    "time": 1743599440,
    "content": "A steam locomotive from 1993 broke my yarn test2025-04-02Yew Leong7 min readSo the story begins with a pair programming session I had with my colleague, which I desperately needed because my node skill tree is still at level 1, and I needed to get started with React because I'll be working on our internal backstage instance.We worked together on a small feature, tested it locally, and it worked. Great. Now it's time to make My Very First React Commit. So I ran the usual git add and git commit, which hooked into yarn test, to automatically run unit tests for backstage, and that's when everything got derailed. For all the React tutorials I have followed, I have never actually run a yarn test on my machine. And the first time I tried yarn test, it hung, and after a long time, the command eventually failed:\n            Determining test suites to run...\n\n  ● Test suite failed to run\n\nthrown: [Error]\n\nerror Command failed with exit code 1.\ninfo Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.\n🌈  backstage  ⚡\n            I could tell it was obviously unhappy about something, and then it threw some [Error]. I have very little actual JavaScript experience, but this looks suspiciously like someone had neglected to write a proper toString() or whatever, and thus we're stuck with the monumentally unhelpful [Error]. Searching the web yielded an entire ocean of false positives due to how vague the error message is. What a train wreck!Fine, let's put on our troubleshooting hats. My memory is not perfect, but thankfully shell history is. Let's see all the (ultimately useless) things that were tried (with commentary):\n            2025-03-19 14:18  yarn test --help\n2025-03-19 14:20  yarn test --verbose\n2025-03-19 14:21  git diff --staged\n2025-03-19 14:25  vim README.md                    # Did I miss some setup?\n2025-03-19 14:28  i3lock -c 336699                 # \"I need a drink\"\n2025-03-19 14:34  yarn test --debug                # Debug, verbose, what's the diff\n2025-03-19 14:35  yarn backstage-cli repo test     # Maybe if I invoke it directly ...\n2025-03-19 14:36  yarn backstage-cli --version     # Nope, same as mengnan's\n2025-03-19 14:36  yarn backstage-cli repo --help\n2025-03-19 14:36  yarn backstage-cli repo test --since HEAD~1   # Minimal changes?\n2025-03-19 14:36  yarn backstage-cli repo test --since HEAD     # Uhh idk no changes???\n2025-03-19 14:38  yarn backstage-cli repo test plugins          # The first breakthrough. More on this later\n2025-03-19 14:39  n all tests.\\n › Press f to run only failed tests.\\n › Press o to only run tests related to changed files.\\n › Pres\nfilter by a filename regex pattern.\\n › Press t to filter by a test name regex pattern.\\n › Press q to quit watch mode.\\n › Press Ent\nrigger a test run all tests.\\n › Press f to run only failed tests.\\n › Press o to only run tests related to changed files.\\n › Press\nlter by a filename regex pattern.\\n › Press t to filter by a test name regex pattern.\\n › Press q to quit watch mode.\\n › Press Enter\ngger a test ru                                     # Got too excited and pasted rubbish\n2025-03-19 14:44  ls -a | fgrep log\n2025-03-19 14:44  find | fgrep log                 # Maybe it leaves a log file?\n2025-03-19 14:46  yarn backstage-cli repo test --verbose --debug --no-cache plugins    # \"clear cache\"\n2025-03-19 14:52  yarn backstage-cli repo test --no-cache --runInBand .                # No parallel\n2025-03-19 15:00  yarn backstage-cli repo test --jest-help\n2025-03-19 15:03  yarn backstage-cli repo test --resetMocks --resetModules plugins     # I have no idea what I'm resetting\n            The first real breakthrough was test plugins,which runs only tests matching \"plugins\". This effectively bypassed the \"determining suites to run...\" logic, which was the thing that was hanging. So, I am now able to get tests to run. However, these too eventually crash with the same cryptic [Error]:\n            PASS   @cloudflare/backstage-components  plugins/backstage-components/src/components/Cards/TeamMembersListCard/TeamMembersListCard.test.tsx (6.787 s)\nPASS   @cloudflare/backstage-components  plugins/backstage-components/src/components/Cards/ClusterDependencyCard/ClusterDependencyCard.test.tsx\nPASS   @internal/plugin-software-excellence-dashboard  plugins/software-excellence-dashboard/src/components/AppDetail/AppDetail.test.tsx\nPASS   @cloudflare/backstage-entities  plugins/backstage-entities/src/AccessLinkPolicy.test.ts\n\n  ● Test suite failed to run\n\nthrown: [Error]\n            Re-running it or matching different tests will give slightly different run logs, but they always end with the same error.By now, I've figured out that yarn test is actually backed by Jest, a JavaScript testing framework, so my next strategy is simply trying different Jest flags to see what sticks, but invariably, none do:\n            2025-03-19 15:16  time yarn test --detectOpenHandles plugins\n2025-03-19 15:18  time yarn test --runInBand .\n2025-03-19 15:19  time yarn test --detectLeaks .\n2025-03-19 15:20  yarn test --debug aetsnuheosnuhoe\n2025-03-19 15:21  yarn test --debug --no-watchman nonexisis\n2025-03-19 15:21  yarn test --jest-help\n2025-03-19 15:22  yarn test --debug --no-watch ooooooo > ~/jest.config\n\n            A pattern finally emerges\n\n          Eventually, after re-running it so many times, I started to notice a pattern. So by default after a test run, Jest drops you into an interactive menu where you can (Q)uit, Run (A)ll tests, etc. and I realized that Jest would eventually crash, even if it's idling in the menu. I started timing the runs, which led me to the second breakthrough:\n            › Press q to quit watch mode.\n › Press Enter to trigger a test run.\n\n  ● Test suite failed to run\n\nthrown: [Error]\n\nerror Command failed with exit code 1.\ninfo Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.\nyarn test .  109.96s user 14.21s system 459% cpu 27.030 total\n\n            RUNS   @cloudflare/backstage-components  plugins/backstage-components/src/components/Cards/TeamRoles/CustomerSuccessCard.test.tsx\n RUNS   @cloudflare/backstage-app  packages/app/src/components/catalog/EntityFipsPicker/EntityFipsPicker.test.tsx\n\nTest Suites: 2 failed, 23 passed, 25 of 65 total\nTests:       217 passed, 217 total\nSnapshots:   0 total\nerror Command failed with exit code 1.\ninfo Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.\nyarn test .  110.85s user 14.04s system 463% cpu 26.974 total\n            No matter what Jest was doing, it always crashes after almost exactly 27 wallclock seconds. It literally didn't matter what tests I selected or re-ran. Even the original problem, a bare yarn test (no tests selected, just hangs), will crash after 27 seconds:\n            Determining test suites to run...\n\n  ● Test suite failed to run\n\nthrown: [Error]\n\nerror Command failed with exit code 1.\ninfo Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.\nyarn test  2.05s user 0.71s system 10% cpu 27.094 total\n\n            Obviously, some sort of timeout. 27 seconds is kind of a weird number (unlike, say, 5 seconds or 60 seconds) but let's try:\n            2025-03-19 15:09  find | fgrep 27\n2025-03-19 15:09  git grep '\\b27\\b'\n\n            No decent hits.How about something like 20+7 or even 20+5+2? Nope.Googling/GPT-4oing for \"jest timeout 27 seconds\" again yielded nothing useful. Far more people were having problems with testing asynchronously, or getting their tests to timeout, than with Jest proper.At this time, my colleague came back from his call, and with his help we determined some other things:his system (MacOS) is not affected at all versus mine (Linux)nvm use v20 didn't fix itI can reproduce it on a clean clone of github.com/backstage/backstage. The tests seem to progress further, about 50+ seconds. This lends credence to a running theory that the filesystem crawler/watcher is the one crashing, and backstage/backstage is a bigger repo than the internal Cloudflare instance, so it takes longer.I next went on a little detour to grab another colleague who I know has been working on a Next.js project. He's one of the few other people nearby who knows anything about Node.js. In my experience with troubleshooting it’s helpful to get multiple perspectives, so we can cover each other’s blind spots and avoid tunnel vision.I then tried invoking many yarn tests in parallel, and I did manage to get the crash time to stretch out to 28 or 29 seconds if the system was under heavy load. So this tells me that it might not be a hard timeout but rather processing driven. A series of sleeps chugging along perhaps?By now, there is a veritable crowd of curious onlookers gathered in front of my terminal marveling at the consistent 27 seconds crash and trading theories. At some point, someone asked if I had tried rebooting yet, and I had to sheepishly reply that I haven't but \"I'm absolutely sure it wouldn't help whatsoever\".And the astute reader can already guess that rebooting did nothing at all, or else this wouldn't even be a story worth telling. Besides, haven't I teased in the clickbaity title about some crazy Steam Locomotive from 1993?\n\n            Strace to the rescue\n\n          My colleague then put us back on track and suggested strace, and I decided to trace the simpler case of the idling menu (rather than trace running tests, which generated far more syscalls).\n            Watch Usage\n › Press a to run all tests.\n › Press f to run only failed tests.\n › Press o to only run tests related to changed files.\n › Press p to filter by a filename regex pattern.\n › Press t to filter by a test name regex pattern.\n › Press q to quit watch mode.\n › Press Enter to trigger a test run.\n[], 1024, 1000)          = 0\nopenat(AT_FDCWD, \"/proc/self/stat\", O_RDONLY) = 21\nread(21, \"42375 (node) R 42372 42372 11692\"..., 1023) = 301\nclose(21)                               = 0\nepoll_wait(13, [], 1024, 0)             = 0\nepoll_wait(13, [], 1024, 999)           = 0\nopenat(AT_FDCWD, \"/proc/self/stat\", O_RDONLY) = 21\nread(21, \"42375 (node) R 42372 42372 11692\"..., 1023) = 301\nclose(21)                               = 0\nepoll_wait(13, [], 1024, 0)             = 0\nepoll_wait(13,\n            It basically epoll_waits until 27 seconds are up and then, right when the crash happens:\n             ● Test suite failed to run\n\nthrown: [Error]\n\n0x7ffd7137d5e0, 1024, 1000) = -1 EINTR (Interrupted system call)\n--- SIGCHLD {si_signo=SIGCHLD, si_code=CLD_EXITED, si_pid=42578, si_uid=1000, si_status=1, si_utime=0, si_stime=0} ---\nread(4, \"*\", 1)                     \t= 1\nwrite(15, \"\\210\\352!\\5\\0\\0\\0\\0\\21\\0\\0\\0\\0\\0\\0\\0\", 16) = 16\nwrite(5, \"*\", 1)                    \t= 1\nrt_sigreturn({mask=[]})             \t= -1 EINTR (Interrupted system call)\nepoll_wait(13, [{events=EPOLLIN, data={u32=14, u64=14}}], 1024, 101) = 1\nread(14, \"\\210\\352!\\5\\0\\0\\0\\0\\21\\0\\0\\0\\0\\0\\0\\0\", 512) = 16\nwait4(42578, [{WIFEXITED(s) && WEXITSTATUS(s) == 1}], WNOHANG, NULL) = 42578\nrt_sigprocmask(SIG_SETMASK, ~[RTMIN RT_1], [], 8) = 0\nread(4, \"*\", 1)                     \t= 1\nrt_sigaction(SIGCHLD, {sa_handler=SIG_DFL, sa_mask=[], sa_flags=SA_RESTORER, sa_restorer=0x79e91e045330}, NULL, 8) = 0\nwrite(5, \"*\", 1)                    \t= 1\nrt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0\nmmap(0x34ecad880000, 1495040, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_NORESERVE, -1, 0) = 0x34ecad880000\nmadvise(0x34ecad880000, 1495040, MADV_DONTFORK) = 0\nmunmap(0x34ecad9ae000, 258048)      \t= 0\nmprotect(0x34ecad880000, 1236992, PROT_READ|PROT_WRITE) = 0\n            I don't know about you, but sometimes I look at straces and wonder “Do people actually read this gibberish?” Fortunately, in the modern generative AI era, we can count on GPT-4o to gently chide: the process was interrupted EINTR by its child SIGCHLD, which means you forgot about the children, silly human. Is the problem with one of the cars rather than the engine?Following this train of thought, I now re-ran with strace --follow-forks, which revealed a giant flurry of activity that promptly overflowed my terminal buffer. The investigation is really gaining steam now. The original trace weighs in at a hefty 500,000 lines, but here is a smaller equivalent version derived from a clean instance of backstage: trace.log.gz. I have uploaded this trace here because the by-now overhyped Steam Locomotive is finally making its grand appearance and I know there'll be people who'd love nothing more than to crawl through a haystack of system calls looking for a train-sized needle. Consider yourself lucky, I had to do it without even knowing what I was looking for, much less that it was a whole Steam Locomotive.This section is left intentionally blank to allow locomotive enthusiasts who want to find the train on their own to do so first.Remember my comment about straces being gibberish? Actually, I was kidding. So there are a few ways to make it more manageable, and with experience you'll learn which system calls to pay attention to, such as execve, chdir, open, read, fork, and signals, and which ones to skim over, such as mprotect, mmap, and futex.Since I'm writing this account after the fact, let's cheat a little and assume I was super smart and zeroed in on execve correctly on the first try:\n            🌈  ~  zgrep execve trace.log.gz | head\nexecve(\"/home/yew/.nvm/versions/node/v18.20.6/bin/yarn\", [\"yarn\", \"test\", \"steam-regulator\"], 0x7ffdff573148 /* 72 vars */) = 0\nexecve(\"/home/yew/.pyenv/shims/node\", [\"node\", \"/home/yew/.nvm/versions/node/v18\"..., \"test\", \"steam-regulator\"], 0x7ffd64f878c8 /* 72 vars */) = -1 ENOENT (No such file or directory)\nexecve(\"/home/yew/.pyenv/bin/node\", [\"node\", \"/home/yew/.nvm/versions/node/v18\"..., \"test\", \"steam-regulator\"], 0x7ffd64f878c8 /* 72 vars */) = -1 ENOENT (No such file or directory)\nexecve(\"/home/yew/repos/secrets/bin/node\", [\"node\", \"/home/yew/.nvm/versions/node/v18\"..., \"test\", \"steam-regulator\"], 0x7ffd64f878c8 /* 72 vars */) = -1 ENOENT (No such file or directory)\nexecve(\"/home/yew/.nvm/versions/node/v18.20.6/bin/node\", [\"node\", \"/home/yew/.nvm/versions/node/v18\"..., \"test\", \"steam-regulator\"], 0x7ffd64f878c8 /* 72 vars */) = 0\n[pid 49307] execve(\"/bin/sh\", [\"/bin/sh\", \"-c\", \"backstage-cli repo test resource\"...], 0x3d17d6d0 /* 156 vars */ <unfinished ...>\n[pid 49307] <... execve resumed>)   \t= 0\n[pid 49308] execve(\"/home/yew/cloudflare/repos/backstage/node_modules/.bin/backstage-cli\", [\"backstage-cli\", \"repo\", \"test\", \"steam-regulator\"], 0x5e7ef80051d8 /* 156 vars */ <unfinished ...>\n[pid 49308] <... execve resumed>)   \t= 0\n[pid 49308] execve(\"/tmp/yarn--1742459197616-0.9027914591640542/node\", [\"node\", \"/home/yew/cloudflare/repos/backs\"..., \"repo\", \"test\", \"steam-regulator\"], 0x7ffcc18af270 /* 156 vars */) = 0\n🌈  ~  zgrep execve trace.log.gz | wc -l\n2254\n\n            Phew, 2,000 is a lot of execves . Let's get the unique ones, plus their counts:\n            🌈  ~  zgrep -oP '(?<=execve\\(\")[^\"]+' trace.log.gz | xargs -L1 basename | sort | uniq -c | sort -nr\n    576 watchman\n    576 hg\n    368 sl\n    358 git\n     16 sl.actual\n     14 node\n      2 sh\n      1 yarn\n      1 backstage-cli\n            Have you spotted the Steam Locomotive yet? I spotted it immediately because this is My Own System and Surely This Means I Am Perfectly Aware Of Everything That Is Installed Unlike, er, node_modules.sl is actually a fun little joke program from 1993 that plays on users' tendencies to make a typo on ls. When sl runs, it clears your terminal to make way for an animated steam locomotive to come chugging through.\n                                    (  ) (@@) ( )  (@)  ()\t@@\tO \t@ \tO \t@  \tO\n                   (@@@)\n               (\t)\n            (@@@@)\n\n          (   )\n      ====    \t________            \t___________\n  _D _|  |_______/    \t\\__I_I_____===__|_________|\n   |(_)---  |   H\\________/ |   |    \t=|___ ___|  \t_________________\n   / \t|  |   H  |  | \t|   |     \t||_| |_|| \t_|            \t\\_____A\n  |  \t|  |   H  |__--------------------| [___] |   =|                    \t|\n  | ________|___H__/__|_____/[][]~\\_______|   \t|   -|                    \t|\n  |/ |   |-----------I_____I [][] []  D   |=======|____|________________________|_\n__/ =| o |=-~~\\  /~~\\  /~~\\  /~~\\ ____Y___________|__|__________________________|_\n |/-=|___|=O=====O=====O=====O   |_____/~\\___/      \t|_D__D__D_|  |_D__D__D_|\n  \\_/  \t\\__/  \\__/  \\__/  \\__/  \t\\_/           \t\\_/   \\_/\t\\_/   \\_/\n\nWhen I first saw that Jest was running sl so many times, my first thought was to ask my colleague if sl is a valid command on his Mac, and of course it is not. After all, which serious engineer would stuff their machine full of silly commands like sl, gti, cowsay, or toilet ? The next thing I tried was to rename sl to something else, and sure enough all my problems disappeared: yarn test started working perfectly.\n\n            So what does Jest have to do with Steam Locomotives?\n\n          Nothing, that's what. The whole affair is an unfortunate naming clash between sl the Steam Locomotive and sl the Sapling CLI. Jest wanted sl the source control system, but ended up getting steam-rolled by sl the Steam Locomotive.\n\n          Fortunately the devs took it in good humor, and made a (still unreleased) fix. Check out the train memes!\n\n          At this point the main story has ended. However, there are still some unresolved nagging questions, like...\n\n            How did the crash arrive at the magic number of a relatively even 27 seconds?\n\n          I don't know. Actually I'm not sure if a forked child executing sl still has a terminal anymore, but the travel time of the train does depend on the terminal width. The wider it is, the longer it takes:\n            🌈  ~  tput cols\n425\n🌈  ~  time sl\nsl  0.19s user 0.06s system 1% cpu 20.629 total\n🌈  ~  tput cols\n58\n🌈  ~  time sl\nsl  0.03s user 0.01s system 0% cpu 5.695 total\n            So the first thing I tried was to run yarn test in a ridiculously narrow terminal and see what happens:\n            Determin\ning test\n suites\nto run..\n.\n\n  ● Test\n suite f\nailed to\n run\n\nthrown:\n[Error]\n\nerror Co\nmmand fa\niled wit\nh exit c\node 1.\ninfo Vis\nit https\n://yarnp\nkg.com/e\nn/docs/c\nli/run f\nor docum\nentation\n about t\nhis comm\nand.\nyarn tes\nt  1.92s\n user 0.\n67s syst\nem 9% cp\nu 27.088\n total\n🌈  back\nstage [m\naster] t\nput cols\n\n8\n            Alas, the terminal width doesn't affect jest at all. Jest calls sl via execa so let's mock that up locally:\n            🌈  choochoo  cat runSl.mjs\nimport {execa} from 'execa';\nconst { stdout } = await execa('tput', ['cols']);\nconsole.log('terminal colwidth:', stdout);\nawait execa('sl', ['root']);\n🌈  choochoo  time node runSl.mjs\nterminal colwidth: 80\nnode runSl.mjs  0.21s user 0.06s system 4% cpu 6.730 total\n            So execa uses the default terminal width of 80, which takes the train 6.7 seconds to cross. And 27 seconds divided by 6.7 is awfully close to 4. So is Jest running sl 4 times? Let's do a poor man's bpftrace by hooking into sl like so:\n            #!/bin/bash\n\nuniqid=$RANDOM\necho \"$(date --utc +\"%Y-%m-%d %H:%M:%S.%N\") $uniqid started\" >> /home/yew/executed.log\n/usr/games/sl.actual \"$@\"\necho \"$(date --utc +\"%Y-%m-%d %H:%M:%S.%N\") $uniqid ended\" >> /home/yew/executed.log\n            And if we check executed.log, sl is indeed executed in 4 waves, albeit by 5 workers simultaneously in each wave:\n            #wave1\n2025-03-20 13:23:57.125482563 21049 started\n2025-03-20 13:23:57.127526987 21666 started\n2025-03-20 13:23:57.131099388 4897 started\n2025-03-20 13:23:57.134237754 102 started\n2025-03-20 13:23:57.137091737 15733 started\n#wave1 ends, wave2 starts\n2025-03-20 13:24:03.704588580 21666 ended\n2025-03-20 13:24:03.704621737 21049 ended\n2025-03-20 13:24:03.707780748 4897 ended\n2025-03-20 13:24:03.712086346 15733 ended\n2025-03-20 13:24:03.711953000 102 ended\n2025-03-20 13:24:03.714831149 18018 started\n2025-03-20 13:24:03.721293279 23293 started\n2025-03-20 13:24:03.724600164 27918 started\n2025-03-20 13:24:03.729763900 15091 started\n2025-03-20 13:24:03.733176122 18473 started\n#wave2 ends, wave3 starts\n2025-03-20 13:24:10.294286746 18018 ended\n2025-03-20 13:24:10.297261754 23293 ended\n2025-03-20 13:24:10.300925031 27918 ended\n2025-03-20 13:24:10.300950334 15091 ended\n2025-03-20 13:24:10.303498710 24873 started\n2025-03-20 13:24:10.303980494 18473 ended\n2025-03-20 13:24:10.308560194 31825 started\n2025-03-20 13:24:10.310595182 18452 started\n2025-03-20 13:24:10.314222848 16121 started\n2025-03-20 13:24:10.317875812 30892 started\n#wave3 ends, wave4 starts\n2025-03-20 13:24:16.883609316 24873 ended\n2025-03-20 13:24:16.886708598 18452 ended\n2025-03-20 13:24:16.886867725 31825 ended\n2025-03-20 13:24:16.890735338 16121 ended\n2025-03-20 13:24:16.893661911 21975 started\n2025-03-20 13:24:16.898525968 30892 ended\n#crash imminent! wave4 ending, wave5 starting...\n2025-03-20 13:24:23.474925807 21975 ended\n            The logs were emitted for about 26.35 seconds, which is close to 27. It probably crashed just as wave4 was reporting back. And each wave lasts about 6.7 seconds, right on the money with manual measurement.\n\n            So why is Jest running sl in 4 waves? Why did it crash at the start of the 5th wave?\n\n          Let's again modify the poor man's bpftrace to also log the args and working directory:\n            echo \"$(date --utc +\"%Y-%m-%d %H:%M:%S.%N\") $uniqid started: $@ at $PWD\" >> /home/yew/executed.log\n            From the results we can see that the 5 workers are busy executing sl root, which corresponds to the getRoot() function in jest-change-files/sl.ts\n            2025-03-21 05:50:22.663263304  started: root at /home/yew/cloudflare/repos/backstage/packages/app/src\n2025-03-21 05:50:22.665550470  started: root at /home/yew/cloudflare/repos/backstage/packages/backend/src\n2025-03-21 05:50:22.667988509  started: root at /home/yew/cloudflare/repos/backstage/plugins/access/src\n2025-03-21 05:50:22.671781519  started: root at /home/yew/cloudflare/repos/backstage/plugins/backstage-components/src\n2025-03-21 05:50:22.673690514  started: root at /home/yew/cloudflare/repos/backstage/plugins/backstage-entities/src\n2025-03-21 05:50:29.247573899  started: root at /home/yew/cloudflare/repos/backstage/plugins/catalog-types-common/src\n2025-03-21 05:50:29.251173536  started: root at /home/yew/cloudflare/repos/backstage/plugins/cross-connects/src\n2025-03-21 05:50:29.255263605  started: root at /home/yew/cloudflare/repos/backstage/plugins/cross-connects-backend/src\n2025-03-21 05:50:29.257293780  started: root at /home/yew/cloudflare/repos/backstage/plugins/pingboard-backend/src\n2025-03-21 05:50:29.260285783  started: root at /home/yew/cloudflare/repos/backstage/plugins/resource-insights/src\n2025-03-21 05:50:35.823374079  started: root at /home/yew/cloudflare/repos/backstage/plugins/scaffolder-backend-module-gaia/src\n2025-03-21 05:50:35.825418386  started: root at /home/yew/cloudflare/repos/backstage/plugins/scaffolder-backend-module-r2/src\n2025-03-21 05:50:35.829963172  started: root at /home/yew/cloudflare/repos/backstage/plugins/security-scorecard-dash/src\n2025-03-21 05:50:35.832597778  started: root at /home/yew/cloudflare/repos/backstage/plugins/slo-directory/src\n2025-03-21 05:50:35.834631869  started: root at /home/yew/cloudflare/repos/backstage/plugins/software-excellence-dashboard/src\n2025-03-21 05:50:42.404063080  started: root at /home/yew/cloudflare/repos/backstage/plugins/teamcity/src\n            The 16 entries here correspond neatly to the 16 rootDirs configured in Jest for Cloudflare's backstage. We have 5 trains, and we want to visit 16 stations so let's do some simple math. 16/5.0 = 3.2 which means our trains need to go back and forth 4 times at a minimum to cover them all.\n\n            Final mystery: Why did it crash?\n\n          Let's go back to the very start of our journey. The original [Error] thrown was actually from here and after modifying node_modules/jest-changed-files/index.js, I found that the error is shortMessage: 'Command failed with ENAMETOOLONG: sl status...' and the reason why became clear when I interrogated Jest about what it thinks the repos are.While the git repo is what you'd expect, the sl \"repo\" looks amazingly like a train wreck in motion:\n            got repos.git as Set(1) { '/home/yew/cloudflare/repos/backstage' }\ngot repos.sl as Set(1) {\n  '\\x1B[?1049h\\x1B[1;24r\\x1B[m\\x1B(B\\x1B[4l\\x1B[?7h\\x1B[?25l\\x1B[H\\x1B[2J\\x1B[15;80H_\\x1B[15;79H_\\x1B[16d|\\x1B[9;80H_\\x1B[12;80H|\\x1B[13;80H|\\x1B[14;80H|\\x1B[15;78H__/\\x1B[16;79H|/\\x1B[17;80H\\\\\\x1B[9;\n  79H_D\\x1B[10;80H|\\x1B[11;80H/\\x1B[12;79H|\\x1B[K\\x1B[13d\\b|\\x1B[K\\x1B[14d\\b|/\\x1B[15;1H\\x1B[1P\\x1B[16;78H|/-\\x1B[17;79H\\\\_\\x1B[9;1H\\x1B[1P\\x1B[10;79H|(\\x1B[11;79H/\\x1B[K\\x1B[12d\\b\\b|\\x1B[K\\x1B[13d\\b|\n  _\\x1B[14;1H\\x1B[1P\\x1B[15;76H__/ =\\x1B[16;77H|/-=\\x1B[17;78H\\\\_/\\x1B[9;77H_D _\\x1B[10;78H|(_\\x1B[11;78H/\\x1B[K\\x1B[12d\\b\\b|\\x1B[K\\x1B[13d\\b| _\\x1B[14;77H|/ |\\x1B[15;75H__/\n  =|\\x1B[16;76H|/-=|\\x1B[17;1H\\x1B[1P\\x1B[8;80H=\\x1B[9;76H_D _|\\x1B[10;77H|(_)\\x1B[11;77H/\\x1B[K\\x1B[12d\\b\\b|\\x1B[K\\x1B[13d\\b|\n  _\\r\\x1B[14d\\x1B[1P\\x1B[15d\\x1B[1P\\x1B[16;75H|/-=|_\\x1B[17;1H\\x1B[1P\\x1B[8;79H=\\r\\x1B[9d\\x1B[1P\\x1B[10;76H|(_)-\\x1B[11;76H/\\x1B[K\\x1B[12d\\b\\b|\\x1B[K\\x1B[13d\\b| _\\r\\x1B[14d\\x1B[1P\\x1B[15;73H__/ =|\n  o\\x1B[16;74H|/-=|_\\r\\x1B[17d\\x1B[1P\\x1B[8;78H=\\r\\x1B[9d\\x1B[1P\\x1B[10;75H|(_)-\\x1B[11;75H/\\x1B[K\\x1B[12d\\b\\b|\\x1B[K\\x1B[13d\\b|\n  _\\r\\x1B[14d\\x1B[1P\\x1B[15d\\x1B[1P\\x1B[16;73H|/-=|_\\r\\x1B[17d\\x1B[1P\\x1B[8;77H=\\x1B[9;73H_D _|  |\\x1B[10;74H|(_)-\\x1B[11;74H/     |\\x1B[12;73H|      |\\x1B[13;73H| _\\x1B[14;73H|/ |   |\\x1B[15;71H__/\n  =| o |\\x1B[16;72H|/-=|___|\\x1B[17;1H\\x1B[1P\\x 1B[5;79H(@\\x1B[7;77H(\\r\\x1B[8d\\x1B[1P\\x1B[9;72H_D _|  |_\\x1B[10;1H\\x1B[1P\\x1B[11d\\x1B[1P\\x1B[12d\\x1B[1P\\x1B[13;72H| _\\x1B[14;72H|/ |   |-\\x1B[15;70H__/\n  =| o |=\\x1B[16;71H|/-=|___|=\\x1B[17;1H\\x1B[1P\\x1B[8d\\x1B[1P\\x1B[9;71H_D _|  |_\\r\\x1B[10d\\x1B[1P\\x1B[11d\\x1B[1P\\x1B[12d\\x1B[1P\\x1B[13;71H| _\\x1B[14;71H|/ |   |-\\x1B[15;69H__/ =| o\n  |=-\\x1B[16;70H|/-=|___|=O\\x1B[17;71H\\\\_/      \\\\\\x1B[8;1H\\x1B[1P\\x1B[9;70H_D _|  |_\\x1B[10;71H|(_)---  |\\x1B[11;71H/     |  |\\x1B[12;70H|      |  |\\x1B[13;70H| _\\x1B[80G|\\x1B[14;70H|/ |\n  |-\\x1B[15;68H__/ =| o |=-~\\x1B[16;69H|/-=|___|=\\x1B[K\\x1B[17;70H\\\\_/      \\\\O\\x1B[8;1H\\x1B[1P\\x1B[9;69H_D _|  |_\\r\\x1B[10d\\x1B[1P\\x1B[11d\\x1B[1P\\x1B[12d\\x1B[1P\\x1B[13;69H| _\\x1B[79G|_\\x1B[14;69H|/ |\n  |-\\x1B[15;67H__/ =| o |=-~\\r\\x1B[16d\\x1B[1P\\x1B[17;69H\\\\_/      \\\\_\\x1B[4d\\b\\b(@@\\x1B[5;75H(    )\\x1B[7;73H(@@@)\\r\\x1B[8d\\x1B[1P\\x1B[9;68H_D _|\n  |_\\r\\x1B[10d\\x1B[1P\\x1B[11d\\x1B[1P\\x1B[12d\\x1B[1P\\x1B[13;68H| _\\x1B[78G|_\\x1B[14;68H|/ |   |-\\x1B[15;66H__/ =| o |=-~~\\\\\\x1B[16;67H|/-=|___|=   O\\x1B[17;68H\\\\_/ \\\\__/\\x1B[8;1H\\x1B[1P\\x1B[9;67H_D _|\n  |_\\r\\x1B[10d\\x1B[1P\\x1B[11d\\x1B[1P\\x1B[12d\\x1B[1P\\x1B[13;67H| _\\x1B[77G|_\\x1B[14;67H|/ |   |-\\x1B[15;65H__/ =| o |=-~O==\\x1B[16;66H|/-=|___|= |\\x1B[17;1H\\x1B[1P\\x1B[8d\\x1B[1P\\x1B[9;66H_D _|\n  |_\\x1B[10;67H|(_)---  |   H\\x1B[11;67H/     |  |   H\\x1B[12;66H|      |  |   H\\x1B[13;66H| _\\x1B[76G|___H\\x1B[14;66H|/ |   |-\\x1B[15;64H__/ =| o |=-O==\\x1B[16;65H|/-=|___|=\n  |\\r\\x1B[17d\\x1B[1P\\x1B[8d\\x1B[1P\\x1B[9;65H_D _|  |_\\x1B[80G/\\x1B[10;66H|(_)---  |   H\\\\\\x1B[11;1H\\x1B[1P\\x1B[12d\\x1B[1P\\x1B[13;65H| _\\x1B[75G|___H_\\x1B[14;65H|/ | |-\\x1B[15;63H__/ =| o |=-~~\\\\\n  /\\x1B[16;64H|/-=|___|=O=====O\\x1B[17;65H\\\\_/      \\\\__/  \\\\\\x1B[1;4r\\x1B[4;1H\\n' + '\\x1B[1;24r\\x1B[4;74H(    )\\x1B[5;71H(@@@@)\\x1B[K\\x1B[7;69H(   )\\x1B[K\\x1B[8;68H====\n  \\x1B[80G_\\x1B[9;1H\\x1B[1P\\x1B[10;65H|(_)---  |   H\\\\_\\x1B[11;1H\\x1B[1P\\x1B[12d\\x1B[1P\\x1B[13;64H| _\\x1B[74G|___H_\\x1B[14;64H|/ |   |-\\x1B[15;62H__/ =| o |=-~~\\\\  /~\\x1B[16;63H|/-=|___|=\n  ||\\x1B[K\\x1B[17;64H\\\\_/      \\\\O=====O\\x1B[8;67H==== \\x1B[79G_\\r\\x1B[9d\\x1B[1P\\x1B[10;64H|(_)---  |   H\\\\_\\x1B[11;64H/     |  |   H  |\\x1B[12;63H|      |  |   H  |\\x1B[13;63H|\n  _\\x1B[73G|___H__/\\x1B[14;63H|/ |   |-\\x1B[15;61H__/ =| o |=-~~\\\\  /~\\r\\x1B[16d\\x1B[1P\\x1B[17;63H\\\\_/      \\\\_\\x1B[8;66H==== \\x1B[78G_\\r\\x1B[9d\\x1B[1P\\x1B[10;63H|(_)---  |\n  H\\\\_\\r\\x1B[11d\\x1B[1P\\x1B[12;62H|      |  |   H  |_\\x1B[13;62H| _\\x1B[72G|___H__/_\\x1B[14;62H|/ |   |-\\x1B[15;60H__/ =| o |=-~~\\\\  /~~\\\\\\x1B[16;61H|/-=|___|=   O=====O\\x1B[17;62H\\\\_/      \\\\__/\n  \\\\__/\\x1B[8;65H==== \\x1B[77G_\\r\\x1B[9d\\x1B[1P\\x1B[10;62H|(_)---  |   H\\\\_\\r\\x1B[11d\\x1B[1P\\x1B[12;61H|      |  |   H  |_\\x1B[13;61H| _\\x1B[71G|___H__/_\\x1B[14;61H|/ |   |-\\x1B[80GI\\x1B[15;59H__/ =|\n  o |=-~O=====O==\\x1B[16;60H|/-=|___|=    ||    |\\x1B[17;1H\\x1B[1P\\x1B[2;79H(@\\x1B[3;74H(   )\\x1B[K\\x1B[4;70H(@@@@)\\x1B[K\\x1B[5;67H(    )\\x1B[K\\x1B[7;65H(@@@)\\x1B[K\\x1B[8;64H====\n  \\x1B[76G_\\r\\x1B[9d\\x1B[1P\\x1B[10;61H|(_)---  |   H\\\\_\\x1B[11;61H/     |  |   H  |  |\\x1B[12;60H|      |  |   H  |__-\\x1B[13;60H| _\\x1B[70G|___H__/__|\\x1B[14;60H|/ |   |-\\x1B[79GI_\\x1B[15;58H__/ =| o\n  |=-O=====O==\\x1B[16;59H|/-=|___|=    ||    |\\r\\x1B[17d\\x1B[1P\\x1B[8;63H==== \\x1B[75G_\\r\\x1B[9d\\x1B[1P\\x1B[10;60H|(_)---  |   H\\\\_\\r\\x1B[11d\\x1B[1P\\x1B[12;59H|      |  |   H  |__-\\x1B[13;59H|\n  _\\x1B[69G|___H__/__|_\\x1B[14;59H|/ |   |-\\x1B[78GI_\\x1B[15;57H__/ =| o |=-~~\\\\  /~~\\\\  /\\x1B[16;58H|/-=|___|=O=====O=====O\\x1B[17;59H\\\\_/      \\\\__/  \\\\__/  \\\\\\x1B[8;62H====\n  \\x1B[74G_\\r\\x1B[9d\\x1B[1P\\x1B[10;59H|(_)---  |   H\\\\_\\r\\x1B  |  |   H  |__-\\x1B[13;58H| _\\x1B[68G|___H__/__|_\\x1B[14;58H|/ |   |-\\x1B[77GI_\\x1B[15;56H__/ =| o |=-~~\\\\ /~~\\\\  /~\\x1B[16;57H|/-=|___|=\n  ||    ||\\x1B[K\\x1B[17;58H\\\\_/      \\\\O=====O=====O\\x1B[8;61H==== \\x1B[73G_\\r\\x1B[9d\\x1B[1P\\x1B[10;58H|(_)---    _\\x1B[67G|___H__/__|_\\x1B[14;57H|/ |   |-\\x1B[76GI_\\x1B[15;55H__/ =| o |=-~~\\\\  /~~\\\\\n  /~\\r\\x1B[16d\\x1B[1P\\x1B[17;57H\\\\_/      \\\\_\\x1B[2;75H(  ) (\\x1B[3;70H(@@@)\\x1B[K\\x1B[4;66H()\\x1B[K\\x1B[5;63H(@@@@)\\x1B[\n\n            Acknowledgements\n\n          Thank you to my colleagues Mengnan Gong and Shuhao Zhang, whose ideas and perspectives helped narrow down the root causes of this mystery.If you enjoy troubleshooting weird and tricky production issues, our engineering teams are hiring.Cloudflare's connectivity cloud protects entire corporate networks, helps customers build Internet-scale applications efficiently, accelerates any website or Internet application, wards off DDoS attacks, keeps hackers at bay, and can help you on your journey to Zero Trust.Visit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.To learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.Discuss on Hacker NewsDeep DiveLinuxDeveloper PlatformDevelopers",
    "summary": {
      "en": "In this story, the author describes a frustrating experience while trying to run tests using Yarn and Jest for a React project. Here's a simplified summary of the key points:\n\n1. **Context**: The author is new to React and was pair programming to implement a feature and make their first commit using Yarn, which automatically runs tests.\n\n2. **Initial Problem**: When the author tried running `yarn test`, it hung and eventually failed with a vague error message. They had little experience with JavaScript and were confused by the error.\n\n3. **Troubleshooting Attempts**: The author attempted various commands and configurations to fix the problem, including checking documentation and running tests with different flags. Despite these efforts, the tests continued to crash after about 27 seconds.\n\n4. **Discovery**: With help from colleagues, they discovered that the issue was related to a command called `sl`, which displays a steam locomotive animation. This command was conflicting with Jest's operations.\n\n5. **Solution**: The author renamed the `sl` command on their system, which resolved the issue, allowing the tests to run successfully.\n\n6. **Conclusion**: The problem stemmed from a naming clash between a fun command (`sl`) and the expected command in the testing framework. The author humorously highlights the absurdity of the situation while acknowledging the helpfulness of their colleagues in troubleshooting the issue.\n\nThis narrative illustrates the challenges of debugging in software development, especially when unexpected conflicts arise.",
      "ko": "이 이야기에서 저자는 React 프로젝트에서 Yarn과 Jest를 사용해 테스트를 실행하려고 할 때 겪은 좌절감을 설명합니다. 저자는 React에 익숙하지 않아 기능을 구현하고 첫 커밋을 하기 위해 페어 프로그래밍을 하고 있었습니다. 이 과정에서 Yarn이 자동으로 테스트를 실행하도록 설정했습니다.\n\n문제는 저자가 `yarn test`를 실행했을 때 발생했습니다. 명령어가 멈추고 애매한 오류 메시지와 함께 실패했습니다. 저자는 JavaScript에 대한 경험이 적어 오류에 혼란스러워했습니다.\n\n문제를 해결하기 위해 저자는 다양한 명령어와 설정을 시도했습니다. 문서도 확인하고 여러 플래그를 사용해 테스트를 실행해 보았지만, 약 27초 후에 테스트가 계속 중단되었습니다.\n\n동료들의 도움으로 저자는 문제가 `sl`이라는 명령어와 관련이 있다는 것을 발견했습니다. 이 명령어는 증기 기관차 애니메이션을 표시하는데, Jest의 작동과 충돌하고 있었습니다.\n\n해결책으로 저자는 시스템에서 `sl` 명령어의 이름을 변경했습니다. 이로 인해 문제가 해결되어 테스트가 성공적으로 실행되었습니다.\n\n결국 문제는 재미있는 명령어(`sl`)와 테스트 프레임워크에서 기대하는 명령어 간의 이름 충돌에서 비롯되었습니다. 저자는 상황의 어처구니없음을 유머러스하게 강조하며, 문제 해결에 도움을 준 동료들에게 감사의 마음을 전했습니다. 이 이야기는 소프트웨어 개발에서 디버깅의 어려움을 잘 보여주며, 예상치 못한 충돌이 발생할 때의 도전 과제를 나타냅니다.",
      "ja": "この話では、著者がReactプロジェクトでYarnとJestを使ってテストを実行しようとした際のフラストレーションを描写しています。著者はReactに不慣れで、ペアプログラミングを通じて機能を実装し、初めてのコミットをYarnを使って行おうとしていました。このYarnは自動的にテストを実行します。\n\n最初の問題は、著者が`yarn test`を実行しようとしたところ、処理が止まり、最終的にはあいまいなエラーメッセージとともに失敗したことです。著者はJavaScriptの経験が少なく、エラーに困惑しました。\n\n問題解決のために、著者はいくつかのコマンドや設定を試みました。ドキュメントを確認したり、異なるフラグを使ってテストを実行したりしましたが、27秒ほど経つとテストは再びクラッシュしました。\n\n同僚の助けを借りて、著者は問題が`sl`というコマンドに関連していることを発見しました。このコマンドは蒸気機関車のアニメーションを表示するもので、Jestの動作と衝突していました。\n\n解決策として、著者は自分のシステム上の`sl`コマンドの名前を変更し、問題が解決しました。これにより、テストが正常に実行できるようになりました。\n\nこの問題は、楽しいコマンド（`sl`）とテストフレームワークで期待されるコマンドとの名前の衝突から生じたものでした。著者はこの状況の滑稽さをユーモラスに強調しつつ、問題解決に協力してくれた同僚の助けを感謝しています。この物語は、ソフトウェア開発におけるデバッグの難しさ、特に予期しない衝突が発生したときの挑戦を示しています。"
    }
  },
  {
    "id": "a575f22893580738",
    "title": {
      "en": "Bletchley code breaker Betty Webb dies aged 101",
      "ko": "베틀리의 전설, 베티 웨브 별세",
      "ja": "ベッチリーの英雄、101歳で逝去"
    },
    "type": "story",
    "url": "https://www.bbc.com/news/articles/c78jd30ywv8o",
    "score": 482,
    "by": "danso",
    "time": 1743512128,
    "content": "Bletchley code breaker Betty Webb dies aged 1011 day agoShareSaveAida FofanaBBC News, West MidlandsShareSaveBBCBletchley Park code breaker Betty Webb has died at the age of 101A decorated World War Two code breaker who spent her youth deciphering enemy messages at Bletchley Park has died at the age of 101.Charlotte \"Betty\" Webb MBE - who was among the last surviving Bletchley code breakers - died on Monday night, the Women's Royal Army Corps Association confirmed.Mrs Webb, from Wythall in Worcestershire, joined operations at the Buckinghamshire base at the age of 18, later going on to help with Japanese codes at The Pentagon in the US. She was awarded France's highest honour - the Légion d'Honneur - in 2021. The Women's Royal Army Corps Association described Mrs Webb as a woman who \"inspired women in the Army for decades\".Bletchley Park Trust CEO Iain Standen said Mrs Webb will not only be remembered for her work but \"also for her efforts to ensure that the story of what she and her colleagues achieved is not forgotten.\"\"Betty's passion for preserving the history and legacy of Bletchley Park has undoubtedly inspired many people to engage with the story and visit the site,\" he said in a statement.Tributes to Mrs Webb have begun to be posted on social media, including one from historian and author Dr Tessa Dunlop who said she was with her in her final hours.Describing Mrs Webb as \"the very best\", she said on X: \"She is one of the most remarkable woman I have ever known.\"Listen on BBC Sounds: Mrs Webb went to work at Bletchley Park when she was 18Mrs Webb told the BBC in 2020 that she had \"never heard of Bletchley\", Britain's wartime code-breaking centre, before starting work there as a member of the ATS, the Auxiliary Territorial Service.She had been studying at a college near Shrewsbury, Shropshire, when she volunteered as she said she and others on the course felt they \"ought to be serving our country rather than just making sausage rolls\".Her mother had taught her to speak German as a child and ahead of her posting remembered being \"taken into the mansion [at Bletchley] to read the Official Secrets Act\".\"I realised that from then on there was no way that I was going to be able to tell even my parents where I was and what I was doing until 1975 [when restrictions were lifted],\" she recalled.She would tell the family with whom she lodged that she was a secretary.Bletchley Park veteran celebrates 100th birthdayCodebreaker Betty Webb reveals D-Day confidenceEx-Bletchley Park worker, 98, given French honourWhen WW2 ended in Europe in May 1945, she went to work at the Pentagon after spending four years at Bletchley, which with its analysis of German communications had served as a vital cog in the Allies' war machine.At the Pentagon she would paraphrase and transcribe already-decoded Japanese messages. She said she was the only member of the ATS to be sent to Washington, describing it as a \"tremendous honour\".Mrs Webb, in 2020, recalled she had had no idea the Americans planned to end the conflict by dropping atomic weapon on Japanese cities, describing the weapons' power as \"utterly awful\".After the Allies' final victory, it took Mrs Webb several months to organise return passage to the UK, where she worked as a secretary at a school in Shropshire.The head teacher there had also worked at Bletchley so knew of her professionalism, whereas other would-be employers, she recalled, were left stumped by her being unable to explain - due to secrecy requirements - her previous duties.More than half a century later, in 2021, Mrs Webb was one of 6,000 British citizens to receive the Légion d'Honneur, following a decision by President François Hollande in 2014 to recognise British veterans who helped liberate France.PA MediaBetty Webb, seen in the front row in a red suit, was invited to the CoronationIn 2023, she and her niece were among 2,200 people from 203 countries invited to Westminster Abbey to see King Charles III's coronation.The same year she celebrated her 100th birthday at Bletchley Park with a party. She and her guests were treated to a fly-past by a Lancaster bomber. She said at the time: \"It was for me - it's unbelievable isn't it? Little me.\"Related internet linksWomen’s Royal Army Corps AssociationBletchley Park Bletchley ParkWythall",
    "summary": {
      "en": "Betty Webb, a notable World War II code breaker from Bletchley Park, has passed away at the age of 101. She was one of the last surviving code breakers and played a key role in deciphering enemy messages during the war. Webb joined Bletchley Park at 18 and later worked at the Pentagon on Japanese codes. In 2021, she received France's highest honor, the Légion d'Honneur, for her contributions.\n\nThe Women's Royal Army Corps Association praised her as an inspiration for women in the military. Webb was dedicated to preserving the history of Bletchley Park and encouraging others to learn about its legacy. In 2023, she celebrated her 100th birthday at Bletchley Park and attended King Charles III's coronation. Tributes from admirers have highlighted her remarkable character and influence.",
      "ko": "베티 웨브, 블렛클리 파크에서 활동한 유명한 제2차 세계대전 암호 해독자가 101세의 나이로 세상을 떠났습니다. 그녀는 마지막 생존 암호 해독자 중 한 명으로, 전쟁 중 적의 메시지를 해독하는 데 중요한 역할을 했습니다. 웨브는 18세에 블렛클리 파크에 합류했으며, 이후 펜타곤에서 일본 암호를 다루는 일을 했습니다. 2021년에는 그녀의 공로로 프랑스의 최고 훈장인 레지옹 도뇌르를 수여받았습니다.\n\n여성 왕립 육군 군단 협회는 그녀를 군대에서 여성들에게 영감을 주는 인물로 칭송했습니다. 웨브는 블렛클리 파크의 역사를 보존하고, 그 유산에 대해 다른 사람들이 배우도록 격려하는 데 헌신했습니다. 2023년에는 블렛클리 파크에서 100세 생일을 기념하고, 찰스 3세의 즉위식에도 참석했습니다. 그녀를 존경하는 이들은 그녀의 뛰어난 인성과 영향력을 강조하며 추모의 글을 남겼습니다.",
      "ja": "ベティ・ウェッブさんが、101歳で亡くなりました。彼女はブレッチリー・パークで活躍した第二次世界大戦の著名な暗号解読者の一人で、戦争中に敵のメッセージを解読する重要な役割を果たしました。ウェッブさんは18歳でブレッチリー・パークに参加し、その後ペンタゴンで日本の暗号に関わりました。2021年には、彼女の貢献に対してフランスの最高勲章であるレジオンドヌールを受賞しました。\n\n女性王立陸軍軍団協会は、彼女を軍における女性たちのインスピレーションとして称賛しました。ウェッブさんはブレッチリー・パークの歴史を守ることに尽力し、他の人々にもその遺産について学ぶよう促していました。2023年には、ブレッチリー・パークで100歳の誕生日を祝ったり、チャールズ3世の戴冠式に出席したりしました。彼女を称える声は、彼女の素晴らしい人柄と影響力を強調しています。"
    }
  },
  {
    "id": "46e2e10629e8cecc",
    "title": {
      "en": "DEDA – Tracking Dots Extraction, Decoding and Anonymisation Toolkit",
      "ko": "DEDA – 점 추적 도구",
      "ja": "DEDAツールで匿名化解析"
    },
    "type": "story",
    "url": "https://github.com/dfd-tud/deda",
    "score": 274,
    "by": "pavel_lishin",
    "time": 1743541903,
    "content": "DEDA - tracking Dots Extraction, Decoding and Anonymisation toolkit\nDocument Colour Tracking Dots, or yellow dots, are small systematic dots which encode information about the printer and/or the printout itself. This process is integrated in almost every commercial colour laser printer. This means that almost every printout contains coded information about the source device, such as the serial number.\nOn the one hand, this tool gives the possibility to read out and decode these forensic features and on the other hand, it allows anonymisation to prevent arbitrary tracking.\nIf you use this software, please cite the paper:\nTimo Richter, Stephan Escher, Dagmar Schönfeld, and Thorsten Strufe. 2018. Forensic Analysis and Anonymisation of Printed Documents. In Proceedings of the 6th ACM Workshop on Information Hiding and Multimedia Security (IH&MMSec '18). ACM, New York, NY, USA, 127-138. DOI: https://doi.org/10.1145/3206004.3206019\n\nInstallation\n\nInstall Python 3\nInstall Deda\n\nFrom PyPI:\n$ pip3 install --user deda\nOr from current directory:\n$ pip3 install --user .\n\nOptional requirement by deda_anonmask_apply (Unix and GNU/Linux only):\n$ pip3 install --user wand\n\nWithout Wand, pages containing white areas on images cannot be anonymised.\n\nGraphical User Interface\n\nTo open the GUI type:\n$ deda_gui\n\nTerminal Application\n1. Reading tracking data\nTracking data can be read and sometimes be decoded from a scanned image. For good results the input shall use a lossless compression (e.g. png) and 300 dpi. Make sure to set a neutral contrast\n$ deda_parse_print INPUTFILE\n2. Find a divergent printer in a set of scanned documents\n$ deda_compare_prints INPUT1 INPUT2 [INPUT3] ...\n3. Analysing an unknown tracking pattern\nNew patterns might not be recognised by parse_print. The dots can be extracted\nfor further analysis.\n$ deda_extract_yd INPUTFILE\n4. Create your own tracking dots\nIf you want to create your own tracking dots matrix and add it to a pdf\ndocument, pass the contents as parameters (see deda_create_dots -h).\n$ deda_create_dots PDFINPUT\nThe calibration page ($ deda_anonmask_create -w) may be used as an input.\n5. Anonymise a scanned image\nThis (mostly) removes tracking data from a scan:\n$ deda_clean_document INPUTFILE OUTPUTFILE\n6. Anonymise a document for printing\n\nSave your document as a PDF file and call it DOCUMENT.PDF.\n\nPrint the testpage.pdf file created by\n$ deda_anonmask_create -w\nwithout any page margin.\n\nScan the document (300 dpi) and pass the lossless file to\n$ deda_anonmask_create -r INPUTFILE\nThis creates 'mask.json', the individual printer's anonymisation mask.\n\nNow apply the anonymisation mask:\n$ deda_anonmask_apply mask.json DOCUMENT.PDF\nThis creates 'masked.pdf', the anonymised document. It may be printed with a\nzero page margin setting.\n\nCheck whether a masked page covers your printer's tracking dots by using a\nmicroscope. The mask's dot radius, x and y offsets can be customised and\npassed to deda_anonmask_apply as parameters.\nNote that if DOCUMENT.PDF contains graphics with white or light coloured parts, these can only be masked if \"wand\" is installed (see above).\n\nTroubleshooting\ndeda_parse_print: command not found\nPossible solutions:\n\nInstall deda accordig to chapter 0\nExecute\n$ export PATH=\"$PATH:$(python -c 'import site,os; print(os.path.join(site.USER_BASE, \"bin\"))')\"\n\nDeda does not recognise my tracking dots\nSet up your scan program so that it does not eliminate the paper structure nor tracking dots by some threshold and check again. Remember that monochrome pages as well as inkjet prints might not contain tracking dots.\nMy printer does not print tracking dots. Can I hide this fact?\nIf there are really no tracking dots, you can either create your own ones (deda_create_dots) or print the calibration page (deda_anonmask_create -w) with another printer and use the mask for your own printer. You can use the anonymised version of the tracking dots or just copy them (deda_anonmask_create --copy). See chapters \"Anonymise a document for printing\" and \"Create your own tracking dots\".\nInstall Error: command 'x86_64-linux-gnu-gcc' failed with exit status 1\nThis may be caused by the eel dependency which is needed for the GUI. Try\n$ sudo apt-get install build-essential autoconf libtool pkg-config python3.6-dev gcc && pip3 install --user eel\nwand.exceptions.PolicyError: attempt to perform an operation not allowed by the security policy PDF' @ error/constitute.c/IsCoderAuthorized/408\nThis is being caused by ImageMagick. Either remove Wand (pip3 uninstall wand) or add <policy domain=\"coder\" rights=\"read | write\" pattern=\"PDF\" /> just before </policymap> in /etc/ImageMagick-*/policy.xml. See also https://stackoverflow.com/questions/52998331/imagemagick-security-policy-pdf-blocking-conversion.",
    "summary": {
      "en": "**Summary of DEDA Toolkit**\n\nDEDA is a tool for tracking and decoding information from Document Colour Tracking Dots, which are small dots printed by color laser printers. These dots encode details about the printer and printout, such as the printer's serial number. DEDA helps users read and decode these dots and offers anonymization features to prevent tracking.\n\n**Installation Steps:**\n1. Install Python 3.\n2. Install DEDA via PyPI:\n   - `$ pip3 install --user deda`\n   - For optional features (like image anonymization), install Wand:\n   - `$ pip3 install --user wand`\n\n**Using DEDA:**\n- **Open the GUI:** Run `$ deda_gui`.\n- **Read Tracking Data:** Use a lossless image (e.g., PNG) at 300 dpi:\n  - `$ deda_parse_print INPUTFILE`\n- **Compare Scanned Documents:** \n  - `$ deda_compare_prints INPUT1 INPUT2 [INPUT3]`\n- **Analyze Unknown Patterns:** \n  - `$ deda_extract_yd INPUTFILE`\n- **Create Custom Tracking Dots:** \n  - `$ deda_create_dots PDFINPUT`\n- **Anonymize Scanned Images:** \n  - `$ deda_clean_document INPUTFILE OUTPUTFILE`\n- **Anonymize for Printing:** Follow specific steps to create and apply an anonymization mask.\n\n**Troubleshooting Common Issues:**\n- If commands are not found, ensure DEDA is installed correctly and update your PATH.\n- If tracking dots are not recognized, adjust your scanning settings.\n- If your printer doesn’t print tracking dots, you can create custom ones or use a mask from another printer.\n- For installation errors, ensure you have the necessary build tools and dependencies.\n\nFor detailed instructions, refer to the original paper by Richter et al. (2018).",
      "ko": "DEDA는 컬러 레이저 프린터가 인쇄하는 작은 점인 문서 색상 추적 점(Document Colour Tracking Dots)에서 정보를 추적하고 해독하는 도구입니다. 이 점들은 프린터의 일련 번호와 같은 인쇄물에 대한 세부 정보를 암호화합니다. DEDA는 사용자가 이러한 점을 읽고 해독할 수 있도록 도와주며, 추적을 방지하기 위한 익명화 기능도 제공합니다.\n\n설치 단계는 다음과 같습니다. 먼저 Python 3을 설치합니다. 그 다음, PyPI를 통해 DEDA를 설치합니다. 명령어는 `$ pip3 install --user deda`입니다. 이미지 익명화와 같은 선택적 기능을 사용하려면 Wand를 설치해야 하며, 명령어는 `$ pip3 install --user wand`입니다.\n\nDEDA를 사용하는 방법은 간단합니다. GUI를 열려면 `$ deda_gui` 명령어를 실행합니다. 추적 데이터를 읽으려면 300 dpi의 무손실 이미지(예: PNG)를 사용해야 하며, 명령어는 `$ deda_parse_print INPUTFILE`입니다. 스캔한 문서를 비교하려면 `$ deda_compare_prints INPUT1 INPUT2 [INPUT3]`를 사용합니다. 알 수 없는 패턴을 분석하려면 `$ deda_extract_yd INPUTFILE`을 입력합니다. 사용자 정의 추적 점을 생성하려면 `$ deda_create_dots PDFINPUT`을 사용하고, 스캔한 이미지를 익명화하려면 `$ deda_clean_document INPUTFILE OUTPUTFILE`을 입력합니다. 인쇄를 위한 익명화는 특정 단계를 따라 익명화 마스크를 생성하고 적용해야 합니다.\n\n일반적인 문제 해결 방법으로는, 명령어가 인식되지 않을 경우 DEDA가 올바르게 설치되었는지 확인하고 PATH를 업데이트해야 합니다. 추적 점이 인식되지 않는 경우 스캔 설정을 조정해야 합니다. 프린터가 추적 점을 인쇄하지 않는 경우, 사용자 정의 점을 만들거나 다른 프린터의 마스크를 사용할 수 있습니다. 설치 오류가 발생하면 필요한 빌드 도구와 종속성이 설치되어 있는지 확인해야 합니다.\n\n자세한 지침은 Richter 외의 원본 논문(2018)을 참조하시기 바랍니다.",
      "ja": "DEDAは、カラーレーザープリンターによって印刷される小さな点、いわゆるドキュメントカラー追跡ドットから情報を追跡し、解読するためのツールです。これらの点は、プリンターのシリアル番号など、プリンターや印刷物に関する詳細をエンコードしています。DEDAは、ユーザーがこれらの点を読み取って解読できるようにし、追跡を防ぐための匿名化機能も提供しています。\n\nインストール手順は以下の通りです。まず、Python 3をインストールします。次に、PyPIを通じてDEDAをインストールします。コマンドは「$ pip3 install --user deda」です。オプション機能（画像の匿名化など）を使用する場合は、Wandもインストールします。「$ pip3 install --user wand」と入力します。\n\nDEDAの使用方法について説明します。まず、GUIを開くには「$ deda_gui」と入力します。追跡データを読み取るには、300 dpiのロスレス画像（例：PNG）を使用し、「$ deda_parse_print INPUTFILE」と入力します。スキャンした文書を比較するには、「$ deda_compare_prints INPUT1 INPUT2 [INPUT3]」を使用します。未知のパターンを分析する場合は、「$ deda_extract_yd INPUTFILE」と入力します。カスタム追跡ドットを作成するには、「$ deda_create_dots PDFINPUT」を使用します。スキャンした画像を匿名化するには、「$ deda_clean_document INPUTFILE OUTPUTFILE」と入力します。印刷用に匿名化する場合は、特定の手順に従って匿名化マスクを作成し、適用します。\n\n一般的な問題のトラブルシューティングについても触れておきます。コマンドが見つからない場合は、DEDAが正しくインストールされているか確認し、PATHを更新してください。追跡ドットが認識されない場合は、スキャン設定を調整します。プリンターが追跡ドットを印刷しない場合は、カスタムドットを作成するか、別のプリンターのマスクを使用できます。インストールエラーが発生した場合は、必要なビルドツールや依存関係が揃っているか確認してください。\n\n詳細な手順については、Richterらの2018年の論文を参照してください。"
    }
  },
  {
    "id": "a3f40b0dd97d91ba",
    "title": {
      "en": "Circuit Tracing: Revealing Computational Graphs in Language Models (Anthropic)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://transformer-circuits.pub/2025/attribution-graphs/methods.html",
    "score": 162,
    "by": "ydnyshhh",
    "time": 1743406933,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "44612a4669c9441c",
    "title": {
      "en": "(2016) Interactive Neural Network Art",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://otoro.net/ml/netart/",
    "score": 40,
    "by": "vinhnx",
    "time": 1743337863,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "3d87b8f5a22b0d24",
    "title": {
      "en": "Scientists uncover key mechanism in evolution: Whole-genome duplication drives",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.sciencedaily.com/releases/2025/03/250326221649.htm",
    "score": 66,
    "by": "docmechanic",
    "time": 1743255546,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "f5f9cfdac24aa878",
    "title": {
      "en": "Why F#?",
      "ko": "F#의 매력",
      "ja": "F#の魅力"
    },
    "type": "story",
    "url": "https://batsov.com/articles/2025/03/30/why-fsharp/",
    "score": 423,
    "by": "bozhidar",
    "time": 1743510847,
    "content": "Why F#?\n\n          25 minute read\n\n        If someone had told me a few months ago I’d be playing with .NET again after a\n15+ years hiatus I probably would have laughed at this.1 Early on in my\ncareer I played with .NET and Java, and even though .NET had done some things\nbetter than Java (as it had the opportunity to learn from some early Java\nmistakes), I quickly settled on Java as it was a truly portable environment.\n\nI guess everyone who reads my blog knows that in the past few years I’ve been\nplaying on and off with OCaml and I think it’s safe to say that it has become\none of my favorite programming languages - alongside the likes of Ruby and\nClojure. My work with OCaml drew my attention recently to F#, an ML targeting\n.NET, developed by Microsoft. The functional counterpart of the\n(mostly) object-oriented C#. The newest ML language created…\n\nWhat is F#?Permalink\n\n  Unfortunately, no one can be told what the Matrix is. You have to see it for yourself.\n\n  – Morpheus, The Matrix\n\nBefore we start discussing F#, I guess we should answer first the question\n“What is F#?”. I’ll borrow a bit from the official page to answer it.\n\nF# is a universal programming language for writing succinct, robust and performant code.\n\nF# allows you to write uncluttered, self-documenting code, where your focus remains on your problem domain, rather than the details of programming.\n\nIt does this without compromising on speed and compatibility - it is open-source, cross-platform and interoperable.\n\nopen System // Gets access to functionality in System namespace.\n\n// Defines a list of names\nlet names = [ \"Peter\"; \"Julia\"; \"Xi\" ]\n\n// Defines a function that takes a name and produces a greeting.\nlet getGreeting name = $\"Hello, {name}\"\n\n// Prints a greeting for each name!\nnames\n|> List.map getGreeting\n|> List.iter (fun greeting -> printfn $\"{greeting}! Enjoy your F#\")\n\nTrivia: F# is the language that made the pipeline operator (|>) popular.\n\nF# has numerous features, including:\n\n  Lightweight syntax\n  Immutable by default\n  Type inference and automatic generalization\n  First-class functions\n  Powerful data types\n  Pattern matching\n  Async programming\n\nA full set of features are documented in the F# language guide.\n\nLooks pretty promising, right?\n\nF# 1.0 was officially released in May 2005 by Microsoft Research. It was\ninitially developed by Don Syme at Microsoft Research in Cambridge and evolved\nfrom an earlier research project called “Caml.NET,” which aimed to bring OCaml\nto the .NET platform.2 F# was officially moved from Microsoft Research to\nMicrosoft (as part of their developer tooling division) in 2010 (timed\nwith the release of F# 2.0).\n\nF# has been steadily evolving since those early days and the most recent release\nF# 9.0 was\nreleased in November 2024.  It seems only appropriate that F# would come to my\nattention in the year of its 20th birthday!\n\nThere were several reasons why I wanted to try out F#:\n\n  .NET became open-source and portable a few years ago and I wanted to check the progress on that front\n  I was curious if F# offers any advantages over OCaml\n  I’ve heard good things about the F# tooling (e.g. Rider and Ionide)\n  I like playing with new programming languages\n\nBelow you’ll find my initial impressions for several areas.\n\nThe LanguagePermalink\n\nAs a member of the ML family of languages, the syntax won’t surprise\nanyone familiar with OCaml. As there are quite few people familiar with\nOCaml, though, I’ll mention that Haskell programmers will also feel right at\nhome with the syntax. And Lispers.\n\nFor everyone else - it’d be fairly easy to pick up the basics.\n\n// function application\nprintfn \"Hello, World!\"\n\n// function definition\nlet greet name =\n    printfn \"Hello, %s!\" name\n\ngreet \"World\"\n\n// whitespace is significant, like in Python\nlet foo =\n    let i, j, k = (1, 2, 3)\n\n    // Body expression:\n    i + 2 * j + 3 * k\n\n// conditional expressions\nlet test x y =\n  if x = y then \"equals\"\n  elif x < y then \"is less than\"\n  else \"is greater than\"\n\nprintfn \"%d %s %d.\" 10 (test 10 20) 20\n\n// Looping over a list.\nlet list1 = [ 1; 5; 100; 450; 788 ]\nfor i in list1 do\n   printfn \"%d\" i\n\n// Looping over a sequence of tuples\nlet seq1 = seq { for i in 1 .. 10 -> (i, i*i) }\nfor (a, asqr) in seq1 do\n  printfn \"%d squared is %d\" a asqr\n\n// A simple for...to loop.\nlet function1 () =\n  for i = 1 to 10 do\n    printf \"%d \" i\n  printfn \"\"\n\n// A for...to loop that counts in reverse.\nlet function2 () =\n  for i = 10 downto 1 do\n    printf \"%d \" i\n  printfn \"\"\n\n// Records\n\n// Labels are separated by semicolons when defined on the same line.\ntype Point = { X: float; Y: float; Z: float }\n\n// You can define labels on their own line with or without a semicolon.\ntype Customer =\n    { First: string\n      Last: string\n      SSN: uint32\n      AccountNumber: uint32 }\n\nlet mypoint = { X = 1.0; Y = 1.0; Z = -1.0 }\n\n// Discriminated Union\ntype Shape =\n    | Circle of radius: float\n    | Rectangle of width: float * height: float\n\n// Functing using pattern matching\nlet area shape =\n    match shape with\n    | Circle radius -> System.Math.PI * radius * radius\n    | Rectangle (width, height) -> width * height\n\nlet circle = Circle 5.0\nlet rectangle = Rectangle(4.0, 3.0)\n\nprintfn \"Circle area: %f\" (area circle)\nprintfn \"Rectangle area: %f\" (area rectangle)\n\nNothing shocking here, right?\n\nHere’s another slightly more involved example:\n\nopen System\n\n// Sample data - simple sales records\ntype SalesRecord = { Date: DateTime; Product: string; Amount: decimal; Region: string }\n\n// Sample dataset\nlet sales = [\n    { Date = DateTime(2023, 1, 15); Product = \"Laptop\"; Amount = 1200m; Region = \"North\" }\n    { Date = DateTime(2023, 2, 3);  Product = \"Phone\";  Amount = 800m;  Region = \"South\" }\n    { Date = DateTime(2023, 1, 20); Product = \"Tablet\"; Amount = 400m;  Region = \"North\" }\n    { Date = DateTime(2023, 2, 18); Product = \"Laptop\"; Amount = 1250m; Region = \"East\" }\n    { Date = DateTime(2023, 1, 5);  Product = \"Phone\";  Amount = 750m;  Region = \"West\" }\n    { Date = DateTime(2023, 2, 12); Product = \"Tablet\"; Amount = 450m;  Region = \"North\" }\n    { Date = DateTime(2023, 1, 28); Product = \"Laptop\"; Amount = 1150m; Region = \"South\" }\n]\n\n// Quick analysis pipeline\nlet salesSummary =\n    sales\n    |> List.groupBy (fun s -> s.Product)                          // Group by product\n    |> List.map (fun (product, items) ->                          // Transform each group\n        let totalSales = items |> List.sumBy (fun s -> s.Amount)\n        let avgSale = totalSales / decimal (List.length items)\n        let topRegion =\n            items\n            |> List.groupBy (fun s -> s.Region)                   // Nested grouping\n            |> List.maxBy (fun (_, regionItems) ->\n                regionItems |> List.sumBy (fun s -> s.Amount))\n            |> fst\n\n        (product, totalSales, avgSale, topRegion))\n    |> List.sortByDescending (fun (_, total, _, _) -> total)      // Sort by total sales\n\n// Display results\nsalesSummary\n|> List.iter (fun (product, total, avg, region) ->\n    printfn \"%s: $%M total, $%M avg, top region: %s\"\n        product total avg region)\n\nWhy don’t you try saving the snippet above in a file called Sales.fsx and running it like this:\n\ndotnet fsi Sales.fsx\n\nNow you know that F# is a great choice for ad-hoc scripts! Also, running dotnet fsi by itself\nwill pop an F# REPL where you can explore the language at your leisure.\n\nI’m not going to go into great details here, as much of what I wrote about OCaml\nhere applies to F# as well.\nI’d also suggest this quick tour of F#\nto get a better feel for its syntax.\n\nTip: Check out the F# cheatsheet\nif you’d like to see a quick syntax reference.\n\nOne thing that made a good impression to me is the focus of the language designers on\nmaking F# approachable to newcomers, by providing a lot of small quality of life improvements\nfor them. Below are few examples, that probably don’t mean much to you, but would mean something\nto people familiar with OCaml:\n\n// line comments\n(* the classic ML comments are around as well *)\n\n// mutable values\nlet mutable x = 5\nx <- 6\n\n// ranges and slices\nlet l = [1..2..10]\nname[5..]\n\n// C# method calls look pretty natural\nlet name = \"FOO\".ToLower()\n\n// operators can be overloaded for different types\nlet string1 = \"Hello, \" + \"world\"\nlet num1 = 1 + 2\nlet num2 = 1.0 + 2.5\n\n// universal printing\nprintfn \"%A\" [1..2..100]\n\nI guess some of those might be controversial, depending on whether you’re a ML\nlanguage purist or not, but in my book anything that makes ML more popular is a\ngood thing.\n\nDid I also mention it’s easy to work with unicode strings and regular expressions?\n\nOften people say that F# is mostly a staging ground for future C# features, and perhaps that’s true.\nI haven’t observed both languages long enough to have my own opinion on the subject, but I was impressed\nto learn that async/await (of C# and later JavaScript fame) originated in… F# 2.0.\n\n  It all changed in 2012 when C#5 launched with the introduction of what has now\nbecome the popularized async/await keyword pairing. This feature allowed you to\nwrite code with all the benefits of hand-written asynchronous code, such as not\nblocking the UI when a long-running process started, yet read like normal\nsynchronous code. This async/await pattern has now found its way into many\nmodern programming languages such as Python, JS, Swift, Rust, and even C++.\n\n  F#’s approach to asynchronous programming is a little different from async/await\nbut achieves the same goal (in fact, async/await is a cut-down version of F#’s\napproach, which was introduced a few years previously, in F#2).\n\n  – Isaac Abraham, F# in Action\n\nTime will tell what will happen, but I think it’s unlikely that C# will ever be able to fully replace F#.\n\nI’ve also found this encouraging comment from 2022 that Microsoft might be willing to invest more in F#:\n\n  Some good news for you. After 10 years of F# being developed by 2.5 people\ninternally and some random community efforts, Microsoft has finally decided to\nproperly invest in F# and created a full-fledged team in Prague this\nsummer. I’m a dev in this team, just like you I was an F# fan for many years\nso I am happy things got finally moving here.\n\nLooking at the changes in F# 8.0 and F 9.0, it seems the new full-fledged team\nhas done some great work!\n\nEcosystemPermalink\n\nIt’s hard to assess the ecosystem around F# after such a brief period, but overall it seems to\nme that there are fairly few “native” F# libraries and frameworks out there and most people\nrely heavily on the core .NET APIs and many third-party libraries and frameworks geared towards C#.\nThat’s a pretty common setup when it comes to hosted languages in general, so nothing surprising here as well.\n\nIf you’ve ever used another hosted language (e.g. Scala, Clojure, Groovy) then you probably know what\nto expect.\n\nAwesome F# keeps track of popular F# libraries, tools and frameworks. I’ll highlight here the web development and data science libraries:\n\nWeb Development\n\n  Giraffe: A lightweight library for building web applications using ASP.NET Core. It provides a functional approach to web development.\n  Suave: A simple and lightweight web server library with combinators for routing and task composition. (Giraffe was inspired by Suave)\n  Saturn: Built on top of Giraffe and ASP.NET Core, it offers an MVC-style framework inspired by Ruby on Rails and Elixir’s Phoenix.\n  Bolero: A framework for building client-side applications in F# using WebAssembly and Blazor.\n  Fable: A compiler that translates F# code into JavaScript, enabling integration with popular JavaScript ecosystems like React or Node.js.\n  Elmish: A model-view-update (MVU) architecture for building web UIs in F#, often used with Fable.\n  SAFE Stack: An end-to-end, functional-first stack for building cloud-ready web applications. It combines technologies like Saturn, Azure, Fable, and Elmish for a type-safe development experience.\n\nData Science\n\n  Deedle: A library for data manipulation and exploratory analysis, similar to pandas in Python.\n  DiffSharp: A library for automatic differentiation and machine learning.\n  FsLab: A collection of libraries tailored for data science, including visualization and statistical tools.\n\nI haven’t played much with any of them at this point yet, so I’ll reserve any\nfeedback and recommendations for some point in the future.\n\nDocumentationPermalink\n\nThe official documentation is pretty good, although I find it kind of weird that\nsome of it is hosted on Microsoft’s site\nand the rest is on https://fsharp.org/ (the site of the F# Software Foundation).\n\nI really liked the following parts of the documentation:\n\n  F# Style Guide\n  F# Design - a repository of RFCs (every language should have one of those!)\n  F# Standard Library API\n\nhttps://fsharpforfunandprofit.com/ is another good learning resource. (even if it seems a bit dated)\n\nDev ToolingPermalink\n\nF# has a somewhat troubled dev tooling story, as historically\nsupport for F# was great only in Visual Studio, and somewhat subpar\nelsewhere. Fortunately, the tooling story has improved a lot in the past\ndecade:\n\n  In 2014 a technical breakthrough was made with the creation of the\nFSharp.Compiler.Service (FCS) package by Tomas Petricek, Ryan Riley, and Dave\nThomas with many later contributors. This contains the core implementation of\nthe F# compiler, editor tooling and scripting engine in the form of a single\nlibrary and can be used to make F# tooling for a wide range of\nsituations. This has allowed F# to be delivered into many more editors,\nscripting and documentation tools and allowed the development of alternative\nbackends for F#. Key editor community-based tooling includes Ionide, by\nKrzysztof Cieślak and contributors, used for rich editing support in the\ncross-platform VSCode editor, with over 1M downloads at time of writing.\n\n  – Don Syme, The Early History of F#\n\nI’ve played with the F# plugins for several editors:\n\n  Emacs (fsharp-mode)\n  Zed (third-party plugin)\n  Helix (built-in support for F#)\n  VS Code (Ionide)\n  Rider (JetBrains’s .NET IDE)\n\nOverall, Rider and VS Code provide the most (and the most polished) features,\nbut the other options were quite usable as well.  That’s largely due to the fact\nthat the F# LSP server fsautocomplete (naming is hard!) is quite robust and\nany editor with good LSP support gets a lot of functionality for free.\n\nStill, I’ll mention that I found the tooling lacking in some regards:\n\n  fsharp-mode doesn’t use TreeSitter (yet) and doesn’t seem to be very actively developed (looking at the code - it seems it was derived from caml-mode)\n  Zed’s support for F# is quite spartan\n  In VS Code shockingly the expanding and shrinking selection is broken, which is quite odd for what is supposed to be the flagship editor for F#\n\nI’m really struggling with VS Code’s keybindings (too many modifier keys and functions keys for my taste) and editing model, so I’ll likely stick with Emacs going forward. Or I’ll finally spend more quality time with neovim!\n\nIt seems that everyone is using the same code formatter (Fantomas), including the F# team, which is great!\nThe linter story in F# is not as great (seems the only popular linter FSharpLint is abandonware these days), but when your\ncompiler is so good, you don’t really need a linter as much.\n\nOh, well… It seems that Microsoft are not really particularly invested in\nsupporting the tooling for F#, as pretty much all the major projects in this\nspace are community-driven.\n\nUsing AI coding agents (e.g. Copilot) with F# worked pretty well, but I didn’t\nspend much time on this front.\n\nIn the end of the day any editor will likely do, as long as you’re using LSP.\n\nBy the way, I had an interesting observation while programming in F# (and OCaml for that matter) -\nthat when you’re working with a language with a really good type system you don’t really need that much\nfrom your editor. Most the time I’m perfectly happy with just some inline type information (e.g. something like CodeLenses), auto-completion and the ability to easily send code to fsi. Simplicity continues\nto be the ultimate sophistication…\n\nOther tools that should be on your radar are:\n\n  Paket - Paket is a dependency manager for .NET projects. Think of it as something like bundler, npm or pip, but for .NET’s NuGet package ecosystem.\n  FAKE -  A DSL for build tasks and more, where you can use F# to specify the tasks. Somewhat similar to Ruby’s rake. Some people claim that’s the easiest way to sneak F# into an existing .NET project.\n\nUse CasesPermalink\n\nGiven the depth and breath of .NET - I guess that sky is the limit for you!\n\nSeems to me that F# will be a particularly good fit for data analysis and manipulation, because\nof features like type providers.\nHere’s a small demo of using a JSON type provider:\n\n#r \"nuget: FSharp.Data\"\n\nopen System\nopen FSharp.Data\n\n// Define the type based on a sample JSON entry\ntype PeopleJson = JsonProvider<\"\"\"\n[\n  { \"name\": \"Alice\", \"age\": 30, \"skills\": [\"F#\", \"C#\", \"Haskell\"] }\n]\n\"\"\">\n\n// Simulated JSON list (could be loaded from file or API)\nlet jsonListString = \"\"\"\n[\n  { \"name\": \"Alice\",  \"age\": 30, \"skills\": [\"F#\", \"C#\", \"Haskell\"] },\n  { \"name\": \"Bob\",    \"age\": 25, \"skills\": [\"F#\", \"Rust\"] },\n  { \"name\": \"Carol\",  \"age\": 28, \"skills\": [\"OCaml\", \"Elixir\"] },\n  { \"name\": \"Dave\",   \"age\": 35, \"skills\": [\"Scala\", \"F#\"] },\n  { \"name\": \"Eve\",    \"age\": 32, \"skills\": [\"Python\", \"F#\", \"ML\"] },\n  { \"name\": \"Frank\",  \"age\": 29, \"skills\": [\"Clojure\", \"F#\"] },\n  { \"name\": \"Grace\",  \"age\": 27, \"skills\": [\"TypeScript\", \"Elm\"] },\n  { \"name\": \"Heidi\",  \"age\": 33, \"skills\": [\"Haskell\", \"PureScript\"] },\n  { \"name\": \"Ivan\",   \"age\": 31, \"skills\": [\"Racket\", \"F#\"] },\n  { \"name\": \"Judy\",   \"age\": 26, \"skills\": [\"ReasonML\", \"F#\"] }\n]\n\"\"\"\n\n// Parse the JSON\nlet people = PeopleJson.Parse(jsonListString)\n\n// Print it\nprintfn \"People in the list:\\n\"\nfor p in people do\n    printfn \"%s (age %d) knows:\" p.Name p.Age\n    p.Skills |> Array.iter (printfn \"  - %s\")\n    printfn \"\"\n\nThe first time I saw this it felt almost like magic, as F# infers the structure\nand types of the data from a small data sample and then you get a parser for it. You\ncan save the code to a file named TypeProvidersDemo.fsx and afterwards you can\nrun it like this:\n\ndotnet fsi TypeProvidersDemo.fsx\n\nIt gets even better, though, as you can easily do things like extracting data straight\nfrom HTML tables and visualizing the data:\n\n#r \"nuget:FSharp.Data\"\n#r \"nuget: Plotly.NET, 3.0.1\"\n\nopen FSharp.Data\nopen Plotly.NET\n\ntype LondonBoroughs = HtmlProvider<\"https://en.wikipedia.org/wiki/List_of_London_boroughs\">\nlet boroughs = LondonBoroughs.GetSample().Tables.``List of boroughs and local authorities``\n\nlet population =\n    boroughs.Rows\n    |> Array.map (fun row ->\n                  row.Borough,\n                  row.``Population (2022 est)``)\n    |> Array.sortBy snd\n    |> Chart.Column\n    |> Chart.show\n\nIf you run the script you’ll get a nice diagram of the population of the various\nLondon boroughs in your browser. Good stuff!\n\nHere we must also appreciate how easy it is to use external libraries (e.g. Plotly.NET)\nin F# scripts!\n\nMoving forward, I think F# would be a good fit for backend services and even\nfull-stack apps, although I haven’t really played with the F# first solutions in\nthis space yet.\n\nFable and Elmish make F# a viable option for client-side programming and might offer\nanother easy way to sneak F# into your day-to-day work.\n\nNote: Historically, Fable has been used to target JavaScript but since Fable\n4, you can also target other languages such as TypeScript, Rust, Python, and\nmore.\n\nHere’s how easy it is to transpile an F# codebase into something else:\n\n# If you want to transpile to JavaScript\ndotnet fable\n\n# If you want to transpile to TypeScript\ndotnet fable --lang typescript\n\n# If you want to transpile to Python\ndotnet fable --lang python\n\nCool stuff!\n\nCommunityPermalink\n\nMy initial impression of the community is that it’s fairly small, perhaps even\nsmaller than that of OCaml.  The F# Reddit and Discord (the one listed on\nReddit) seem like the most active places for F# conversations. There’s supposed\nto be some F# Slack as well, but I couldn’t get an invite for it. (seems the\nautomated process for issuing those invites has been broken for a while)\n\nI’m still not sure what’s the role Microsoft plays in the community, as I\nhaven’t seen much from them overall.\n\nFor a me a small community is not really a problem, as long as the community is\nvibrant and active. Also - I’ve noticed I always feel more connected to smaller\ncommunities. Moving from Java to Ruby back in the day felt like night and day as\nfar as community engagement and sense of belonging go.\n\nI didn’t find many books and community sites/blogs dedicated to F#, but I didn’t\nreally expect to in the first place.\n\nThe most notable community initiatives I discovered were:\n\n  Amplifying F# - an effort to promote F# and to get more businesses involved with it\n  F# for Fun and Profit - a collection of tutorials and essays on F#\n  F# Lab - The community driven toolkit for datascience in F#\n  F# Weekly - a weekly newsletter about the latest developments in the world of F#\n\nSeems to me that more can be done to promote the language and engage new programmers and businesses\nwith it, although that’s never easy 20 years into the existence of some project. I continue to be\nsomewhat puzzled as to why Microsoft doesn’t market F# more, as I think it could be a great\nmarketing vehicle for them.\n\nAll in all - I don’t feel qualified to comment much on the F# community at this point.\n\nThe Popularity ContestPermalink\n\nDepending on the type of person you are you may or may not care about a a programming language’s\n“popularity”. People often ask my why I spent a lot of time with languages that are unlikely to\never result in job opportunities for me, e.g.:\n\n  Emacs Lisp\n  Clojure\n  OCaml\n  F#\n\nProfessional opportunities are important, of course, but so are:\n\n  having fun (and the F in F# stands for “fun”)\n  learning new paradigms and ideas\n  challenging yourself to think and work differently\n\nThat being said, F# is not a popular language by most conventional metrics. It’s not highly ranked\non TIOBE, StackOverflow or most job boards. But it’s also not less popular than most “mainstream”\nfunctional programming languages. The sad reality is that functional programming is still not\nmainstream and perhaps it will never be.\n\nA few more resources on the subject:\n\n  About F#’s popularity\n  How Popular is F# in 2024\n\n      Here’s also a video for the article above\n\nF# vs OCamlPermalink\n\n  The early conception of F# was simple: to bring the benefits of OCaml to .NET and .NET to OCaml: a\nmarriage between strongly typed functional programming and .NET. Here “OCaml” meant both the\ncore of the language itself, and the pragmatic approach to strongly-typed functional programming\nit represented. The initial task was relatively well-defined: I would re-implement the core of the\nOCaml language and a portion of its base library to target the .NET Common Language Runtime.\nThe implementation would be fresh, i.e. not using any of the OCaml codebase, for legal clarity.\n\n  – Don Syme, creator of F#, The Early History of F#\n\nF# was derived from OCaml, so the two languages share a lot of DNA. Early on\nF# made some efforts to support as much of OCaml’s syntax as possible, and it\neven allowed the use of .ml and .mli file extensions for F# code. Over time\nthe languages started to diverge a bit, though.3\n\nCreating a language that’s independent from OCaml, of course, was something\nintended from the very beginning. That’s also reflected in the decision\nto chose the name F#, even if early versions of the language were called “Caml.NET”:\n\n  Although the first version of F# was initially presented as “Caml-for-.NET”,\nin reality it was always a new language, designed for .NET from day 1. F# was\nnever fully compatible with any version of OCaml, though it shared a compatible\nsubset, and it took Caml-Light and OCaml as its principal sources of design\nguidance and inspiration.\n\n  – Don Syme, The Early History of F#\n\nIf you ask most people about the pros and cons of F# over OCaml you’ll probably\nget the following answers.\n\nF# Pros\n\n  Runs on .NET\n\n      Tons of libraries are at disposal\n\n  Backed by Microsoft\n  Arguably it’s a bit easier to learn by newcomers (especially those who have only experience with OO programming)\n\n      The syntax is slightly easier to pick up (I think)\n      The compiler errors and warnings are “friendlier” (easier to understand)\n      It’s easier to debug problems (partially related to the previous item)\n\n  Strong support for async programming\n  Has some cool features, absent in OCaml, like:\n\n      Anonymous Records\n      Active Patterns\n      Computational expressions\n      Sequence comprehensions\n      Type Providers\n      Units of measure\n\nF# Cons\n\n  Runs on .NET\n\n      The interop with .NET influenced a lot of language design decisions (e.g. allowing null)\n\n  Backed by Microsoft\n\n      Not everyone likes Microsoft\n      Seems the resources allocated to F# by Microsoft are modest\n      It’s unclear how committed Microsoft will be to F# in the long run\n\n  Naming conventions: I like snake_case way more than camelCase and PascalCase\n  Misses some cool OCaml features\n\n      First-class modules and functors\n      GADTs\n\n  Doesn’t have a friendly camel logo\n  The name F# sounds cool, but is a search and filename nightmare (and you’ll see FSharp quite often in the wild)\n\nBoth F# and OCaml can also target JavaScript runtimes as well - via Fable on\nthe F# side and Js_of_ocaml and Melange on the OCaml side. Fable seems like a\nmore mature solution at a cursory glance, but I haven’t used any of the three\nenough to be able to offer an informed opinion.\n\nIn the end of the day both remain two fairly similar robust, yet niche,\nlanguages, which are unlikely to become very popular in the future. I’m guessing\nworking professionally with F# is more likely to happen for most people, as .NET\nis super popular and I can imagine it’d be fairly easy to sneak a bit of F# here\nin there in established C# codebases.\n\nOne weird thing I’ve noticed with F# projects is that they still use XML project\nmanifests, where you have to list the source files manually in the order in\nwhich they should be compiled (to account for the dependencies between them). I\nam a bit shocked that the compiler can’t handle the dependencies automatically,\nbut I guess that’s because in F# there’s not direct mapping between source files\nand modules. At any rate - I prefer the OCaml compilation process (and Dune) way\nmore.\n\nAs my interest in MLs is mostly educational I’m personally leaning towards OCaml, but if I had to build\nweb services with an ML language I’d probably pick F#. I also have a weird respect for every language\nwith its own runtime, as this means that it’s unlikely that the runtime will force some compromises\non the language.\n\nClosing thoughtsPermalink\n\n  Question: What can C# do that F# can’t?\nAnswer: NullReferenceException!\n\n  – F# Community joke\n\nAll in all I liked F# way more than I expected to! In a way it reminded me of my\nexperience with Clojure back in the day in the sense that Clojure was the most\npractical Lisp out there when it was released, mostly because of its great\ninterop with Java.\n\nI have a feeling that if .NET was portable (and open-source) since day 1\nprobably ClojureCLR would have become as popular as Clojure, and likely F# would\nhave developed a bigger community and broader usage by now. I’m fairly certain I\nwould have never dabbled in .NET again if it hadn’t been for .NET Core, and I\ndoubt I’m the only one. The fact that F# wasn’t open-sourced until 2010 didn’t help\nwith the early adoption either.\n\nSeems I’m only the only one who thinks this way:\n\n  Mistakes are hard to admit, and best seen in their historical context. From the early history, the\ngreatest mistake related to F# was that neither .NET nor the language were open source or using\nopen engineering. This mistake was well-understood by the core contributors at the time and many\nacross Microsoft were advocating for a shift to open-source. Put simply, an innovative language\ngrew in the research lab of a company that had not yet embraced open source: those involved did\nwhat they could through source drops, and the problem was eventually solved via the shift to open\nsource engineering and design from 2011 to 2014. The rectification of this mistake will likely be the\nmost significant development in the history of the language. Further, the fact that F# was able to\nnavigate 2002-2011 while using closed-engineering is largely due to the recognition of its qualities\nby decision makers at Microsoft.\n\n  – Don Syme, The Early History of F#\n\nLearning OCaml is definitely not hard, but I think that people interested to learn some ML\ndialect might have an easier time with F#. And, as mentioned earlier, you’ll probably have an\neasier path to “production” with it.\n\nI think that everyone who has experience with .NET will benefit from learning F#.\nPerhaps more importantly - everyone looking to do more with an ML family language\nshould definitely consider F#, as it’s a great language in its own right, that gives\nyou access to one of the most powerful programming platforms out there.\n\nLet’s not forget about Fable, which makes it possible for you leverage\nF# in JavaScript, Dart, Rust and Python runtimes!\n\nSo, why F#? In the F# community there’s the saying that the “F” in F# stands for\n“Fun”. In my brief experience with F# I found this to be very true! I’ll go a\nstep further and make the claim that F# is both seriously fun and seriously\npractical!\n\nAlso if your code compiles - it will probably work the way you expect it to. I\nhear that’s generally considered a desirable thing in the world of programming!\n\nThat’s all I have for you today. Please, share in the comments what do you love about F#!\n\nIn sane type systems we trust!\n\nWhat’s Next?Permalink\n\nIf you need further arguments to learn F# I can highly recommend the following\nresources:\n\n  F# Code I Love (a talk by Don Syme)\n  Why Use F# Sharp?\n  Domain Modeling Made Functional\n  F# in Action\n\n“The Early History of F#”, which I’ve quoted extensively, is pure gold as well!\n\nI also think it’s a good idea to follow the F# Reddit and\nto join F#’s Discord.\n\nDiscussionsPermalink\n\n  Hacker News\n  Lobsters\n\n      I had some C# courses in the university and I wrote my bachelor’s thesis in C#. It was a rewrite of Arch Linux’s pacman, running on Mono. This was way back in 2007.↩\n\n      See https://fsharp.org/history/hopl-final/hopl-fsharp.pdf↩\n\n      https://github.com/fsharp/fslang-suggestions/issues/985↩\n\n     Tags:\n\n      .NET,\n\n      F#,\n\n      ML,\n\n      OCaml\n\n   Updated: March 30, 2025\n\n  Share on\n\n   Twitter\n\n   Facebook\n\n   LinkedIn\n\n      Previous\n\n      Next\n\n     var HYVOR_TALK_WEBSITE = 2584; // DO NOT CHANGE THIS\n     var HYVOR_TALK_CONFIG = {\n         url: 'https://batsov.com/articles/2025/03/30/why-fsharp/',\n         id: 'https://batsov.com/articles/2025/03/30/why-fsharp/'\n     };",
    "summary": {
      "en": "### Summary: Why F#?\n\nThis article discusses the author's re-engagement with F# after many years, highlighting its features and benefits as a programming language. \n\n**What is F#?**\nF# is a functional programming language that runs on the .NET platform. It's designed to help developers write clear, efficient, and maintainable code. Key features include:\n- Concise syntax\n- Immutable data by default\n- Type inference\n- First-class functions\n- Pattern matching\n- Asynchronous programming support\n\nF# was first released in 2005 and has evolved significantly over the years, with the latest version, F# 9.0, released in November 2024.\n\n**Why Use F#?**\nThe author was drawn to F# for several reasons:\n- .NET's transition to open-source and cross-platform\n- Curiosity about potential advantages over OCaml\n- Positive feedback about F# tooling (like Rider and Ionide)\n\n**Language Features:**\nF# has a syntax similar to OCaml, making it accessible for those familiar with functional programming. It includes features like pattern matching and type providers, which enhance its usability for data analysis and web development.\n\n**Ecosystem and Community:**\nF# has a smaller ecosystem compared to more popular languages but includes libraries for web development (like Giraffe and Saturn) and data science (like Deedle). The community is active, albeit small, with resources like F# for Fun and Profit and Amplifying F#.\n\n**Development Tools:**\nTooling has improved significantly, with support across various editors. The FSharp.Compiler.Service allows for better integration in development environments, though some editors still lack advanced features.\n\n**Use Cases:**\nF# is particularly suited for data manipulation and backend services. Its ability to interoperate with JavaScript and other languages via Fable makes it versatile for modern web applications.\n\n**Final Thoughts:**\nThe author appreciates F# for its fun and practical nature, suggesting it is a great choice for .NET developers and those interested in functional programming. They encourage readers to explore F# and its community resources for further learning. \n\nOverall, the article presents F# as a robust language that combines the benefits of functional programming with the extensive capabilities of the .NET framework.",
      "ko": "이 글에서는 저자가 오랜만에 F#에 다시 관심을 가지게 된 이유와 이 프로그래밍 언어의 특징 및 장점을 설명합니다.\n\nF#은 .NET 플랫폼에서 실행되는 함수형 프로그래밍 언어입니다. 개발자들이 명확하고 효율적이며 유지보수가 쉬운 코드를 작성할 수 있도록 설계되었습니다. 주요 특징으로는 간결한 문법, 기본적으로 불변 데이터, 타입 추론, 일급 함수, 패턴 매칭, 비동기 프로그래밍 지원 등이 있습니다. F#은 2005년에 처음 출시되었으며, 이후 많은 발전을 거쳐 2024년 11월에 F# 9.0이 최신 버전으로 출시되었습니다.\n\n저자는 F#에 매력을 느낀 이유가 여러 가지 있다고 말합니다. .NET이 오픈 소스와 크로스 플랫폼으로 전환된 점, OCaml보다의 잠재적 장점에 대한 호기심, Rider와 Ionide와 같은 F# 도구에 대한 긍정적인 피드백 등이 그 이유입니다.\n\nF#의 문법은 OCaml과 유사하여 함수형 프로그래밍에 익숙한 사람들에게 접근하기 쉽습니다. 패턴 매칭과 타입 제공자와 같은 기능이 포함되어 있어 데이터 분석과 웹 개발에 유용합니다.\n\nF#의 생태계는 더 인기 있는 언어에 비해 작지만, 웹 개발을 위한 Giraffe와 Saturn, 데이터 과학을 위한 Deedle과 같은 라이브러리가 포함되어 있습니다. 커뮤니티는 작지만 활발하며, F# for Fun and Profit과 Amplifying F#와 같은 자원이 있습니다.\n\n개발 도구는 크게 개선되었으며, 다양한 편집기에서 지원됩니다. FSharp.Compiler.Service는 개발 환경에서의 통합을 개선하지만, 일부 편집기에서는 고급 기능이 여전히 부족합니다.\n\nF#은 데이터 조작과 백엔드 서비스에 특히 적합합니다. Fable을 통해 JavaScript 및 다른 언어와 상호 운용할 수 있어 현대 웹 애플리케이션에 유연하게 활용될 수 있습니다.\n\n저자는 F#의 재미있고 실용적인 특성을 높이 평가하며, .NET 개발자와 함수형 프로그래밍에 관심 있는 사람들에게 훌륭한 선택이라고 제안합니다. 독자들에게 F#과 그 커뮤니티 자원을 탐색해 보기를 권장합니다. 전반적으로 이 글은 F#을 함수형 프로그래밍의 장점과 .NET 프레임워크의 광범위한 기능을 결합한 강력한 언어로 소개합니다.",
      "ja": "この記事では、著者が数年ぶりにF#に再び関わることになった経緯と、その特徴や利点について述べています。\n\nF#とは、.NETプラットフォーム上で動作する関数型プログラミング言語です。開発者が明確で効率的、かつ保守しやすいコードを書く手助けをするように設計されています。主な特徴には、簡潔な構文、デフォルトで不変のデータ、型推論、第一級関数、パターンマッチング、非同期プログラミングのサポートがあります。F#は2005年に初めてリリースされ、年々進化を遂げており、最新バージョンのF# 9.0は2024年11月にリリースされました。\n\n著者がF#に惹かれた理由はいくつかあります。まず、.NETがオープンソース化され、クロスプラットフォーム対応になったことです。また、OCamlに対する潜在的な利点についての好奇心もありました。さらに、RiderやIonideなどのF#ツールに対するポジティブなフィードバックも影響しています。\n\nF#の構文はOCamlに似ているため、関数型プログラミングに慣れた人にとってはアクセスしやすいです。パターンマッチングや型プロバイダーなどの機能があり、データ分析やウェブ開発において使いやすさが向上しています。\n\nF#のエコシステムは、より人気のある言語に比べて小規模ですが、GiraffeやSaturnといったウェブ開発用のライブラリや、Deedleのようなデータサイエンス用のライブラリが含まれています。コミュニティは活発ですが小規模で、F# for Fun and ProfitやAmplifying F#といったリソースがあります。\n\n開発ツールも大幅に改善され、さまざまなエディタでのサポートが進んでいます。FSharp.Compiler.Serviceを利用することで、開発環境への統合が向上していますが、一部のエディタでは高度な機能がまだ不足しています。\n\nF#は特にデータ操作やバックエンドサービスに適しています。Fableを通じてJavaScriptや他の言語と相互運用できるため、現代のウェブアプリケーションにおいても柔軟性があります。\n\n著者はF#の楽しさと実用性を評価しており、.NET開発者や関数型プログラミングに興味がある人にとって素晴らしい選択肢であると提案しています。読者にはF#やそのコミュニティリソースを探求することを勧めています。全体として、この記事はF#を関数型プログラミングの利点と.NETフレームワークの広範な機能を兼ね備えた堅牢な言語として紹介しています。"
    }
  },
  {
    "id": "864b8f7f20d2d3d9",
    "title": {
      "en": "The Tectonics Behind the Myanmar Quake",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://nautil.us/what-caused-the-devastating-earthquake-in-myanmar-1200737/",
    "score": 36,
    "by": "rbanffy",
    "time": 1743421244,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "bc5e5cecb0f4dea0",
    "title": {
      "en": "Testing DVD-R and CD-R 25 years later: optical disks from Japan",
      "ko": "25년 후, 일본의 광디스크 테스트",
      "ja": "25年後の光ディスク"
    },
    "type": "story",
    "url": "https://goughlui.com/2025/03/23/optical-discs-from-japan-part-6-tdk-uv-guard-fuji-lg-sony-maxell-cmc/",
    "score": 159,
    "by": "csdvrx",
    "time": 1743543991,
    "content": "← Notes: What is the Power Consumption of a CPAP Machine?Tech Flashback: Double the Data on a CD-R with HD-BURN (ft. Optorite DD0203) →Optical Discs From Japan – Part 6: TDK UV Guard, Fuji, LG/Sony, Maxell, CMC\nPosted on March 23, 2025 by lui_goughAs it turns out, “thrift shopping” in Japan via the internet can be both very enjoyable as well as very dangerous. In this series of posts, I’ve been examining various optical media I’ve obtained from Japan, both in-person and online, and they just keep on comingas the listings are refreshed and more products get put up for sale. It’s been an amazing trip “back-in-time” but also an opportunity to share some of the fun and beauty of older optical discs.TDK DVD+R 4x UV-GuardThe first disc in this post will be a rather special one – a Japanese market TDK product from back when TDK were making their own discs, unlike now when their brand is used to rebadge primarily CMC products (but occasionally Ritek too).<img decoding=\"async\" class=\"alignnone size-large wp-image-53789\" src=\"https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-shrink-front-1024x888.jpg\" alt=\"\" width=\"640\" height=\"555\" srcset=\"https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-shrink-front-1024x888.jpg 1024w, https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-shrink-front-300x260.jpg 300w, https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-shrink-front-768x666.jpg 768w, https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-shrink-front-1536x1332.jpg 1536w, https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-shrink-front-2048x1776.jpg 2048w\" sizes=\"(max-width: 640px) 100vw, 640px\" />This is a five pack of DVD+R 4x-rated discs with hard coating andUV guard. This is a rather special product, as hard coating (later advertised under the Durabis 1/Durabis 2 names) was a TDK specialty, using a diamond-like coating made of carbon. Perhaps this is why such products often get a strange translation of being “carbide series” discs.<img decoding=\"async\" class=\"alignnone size-large wp-image-53790\" src=\"https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-shrink-rear-1024x899.jpg\" alt=\"\" width=\"640\" height=\"562\" srcset=\"https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-shrink-rear-1024x899.jpg 1024w, https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-shrink-rear-300x263.jpg 300w, https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-shrink-rear-768x674.jpg 768w, https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-shrink-rear-1536x1349.jpg 1536w, https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-shrink-rear-2048x1798.jpg 2048w\" sizes=\"(max-width: 640px) 100vw, 640px\" />As with discs of the era, a compatibility warning features in the plastic colour shrink wrap itself. This particular pack has a product code of DVD+R47HCX5G and a barcode number of T490693352241 and is Made in Japan. The graphs seemingly show how PI error rates evolve over exposure compared to an ordinary DVD, making the case that the UV guard coating improves longevity.<img decoding=\"async\" class=\"alignnone size-medium wp-image-53788\" src=\"https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-shrink-bot-300x109.jpg\" alt=\"\" width=\"300\" height=\"109\" srcset=\"https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-shrink-bot-300x109.jpg 300w, https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-shrink-bot-1024x371.jpg 1024w, https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-shrink-bot-768x278.jpg 768w, https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-shrink-bot-1536x556.jpg 1536w, https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-shrink-bot-2048x742.jpg 2048w\" sizes=\"(max-width: 300px) 100vw, 300px\" /> <img decoding=\"async\" class=\"alignnone size-medium wp-image-53791\" src=\"https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-shrink-side-300x122.jpg\" alt=\"\" width=\"300\" height=\"122\" srcset=\"https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-shrink-side-300x122.jpg 300w, https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-shrink-side-1024x417.jpg 1024w, https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-shrink-side-768x313.jpg 768w, https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-shrink-side-1536x626.jpg 1536w, https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-shrink-side-2048x834.jpg 2048w\" sizes=\"(max-width: 300px) 100vw, 300px\" /> <img decoding=\"async\" class=\"alignnone size-large wp-image-53792\" src=\"https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-shrink-top-1024x374.jpg\" alt=\"\" width=\"640\" height=\"234\" srcset=\"https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-shrink-top-1024x374.jpg 1024w, https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-shrink-top-300x110.jpg 300w, https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-shrink-top-768x281.jpg 768w, https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-shrink-top-1536x562.jpg 1536w, https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-shrink-top-2048x749.jpg 2048w\" sizes=\"(max-width: 640px) 100vw, 640px\" />A look from the side shows these discs being packed in ordinary-size jewel cases, rather than the slim-line cases of later products.<img decoding=\"async\" class=\"alignnone size-medium wp-image-53785\" src=\"https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-finlay-298x300.jpg\" alt=\"\" width=\"298\" height=\"300\" srcset=\"https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-finlay-298x300.jpg 298w, https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-finlay-1018x1024.jpg 1018w, https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-finlay-150x150.jpg 150w, https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-finlay-768x773.jpg 768w, https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-finlay-1527x1536.jpg 1527w, https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-finlay-2036x2048.jpg 2036w\" sizes=\"(max-width: 298px) 100vw, 298px\" /> <img decoding=\"async\" class=\"alignnone size-medium wp-image-53787\" src=\"https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-rinlay-outer-300x234.jpg\" alt=\"\" width=\"300\" height=\"234\" srcset=\"https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-rinlay-outer-300x234.jpg 300w, https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-rinlay-outer-1024x800.jpg 1024w, https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-rinlay-outer-768x600.jpg 768w, https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-rinlay-outer-1536x1199.jpg 1536w, https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-rinlay-outer-2048x1599.jpg 2048w\" sizes=\"(max-width: 300px) 100vw, 300px\" />The front inlay and rear inlay are simple, offering ample room for noting contents and advertising the fact they are printed on 100% recycled paper.<img decoding=\"async\" class=\"alignnone wp-image-53786 size-large\" src=\"https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-rinlay-inner-1024x796.jpg\" alt=\"\" width=\"640\" height=\"498\" srcset=\"https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-rinlay-inner-1024x796.jpg 1024w, https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-rinlay-inner-300x233.jpg 300w, https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-rinlay-inner-768x597.jpg 768w, https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-rinlay-inner-1536x1195.jpg 1536w, https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-rinlay-inner-2048x1593.jpg 2048w\" sizes=\"(max-width: 640px) 100vw, 640px\" />The tray is clear, so the rear inlay also makes use of the back to provide handling precautions.<img decoding=\"async\" class=\"alignnone size-medium wp-image-53784\" src=\"https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-disc-top-300x298.jpg\" alt=\"\" width=\"300\" height=\"298\" srcset=\"https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-disc-top-300x298.jpg 300w, https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-disc-top-1024x1019.jpg 1024w, https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-disc-top-150x150.jpg 150w, https://goughlui.com/wp-content/uploads/2025/03/tdkuvg-disc-top-768x764.jpg 768w, https://goughlui.com/wp-",
    "summary": {
      "en": "The author shares their experience of \"thrift shopping\" for optical media in Japan, highlighting the fun and risks involved. They focus on a special TDK DVD+R 4x product, which features a unique hard coating for durability and protection against UV light. This pack contains five discs and is noted for its compatibility warnings and a product code. The article emphasizes the quality and longevity benefits of the disc's UV guard coating compared to regular DVDs. Additionally, it includes details about the packaging and design, noting its eco-friendly materials. Overall, it celebrates the charm of older optical discs while showcasing specific products from Japan.",
      "ko": "저자는 일본에서 광미디어를 위한 중고 쇼핑 경험을 공유하며 그 과정에서의 즐거움과 위험성을 강조합니다. 특히 TDK DVD+R 4x 제품에 주목하는데, 이 제품은 내구성을 위해 독특한 하드 코팅이 되어 있어 자외선으로부터 보호됩니다. 이 팩에는 다섯 개의 디스크가 포함되어 있으며, 호환성 경고와 제품 코드가 기재되어 있습니다. 기사는 이 디스크의 자외선 차단 코팅이 일반 DVD에 비해 품질과 내구성에서 얼마나 뛰어난지를 강조합니다. 또한 포장과 디자인에 대한 세부 사항도 포함되어 있으며, 친환경 소재로 제작되었다고 언급합니다. 전반적으로, 이 글은 일본의 특정 제품을 소개하면서 오래된 광디스크의 매력을 기념합니다.",
      "ja": "著者は日本での光学メディアのリサイクルショップでの体験を共有し、その楽しさとリスクについて述べています。特に注目しているのは、TDKのDVD+R 4倍速製品で、耐久性を高めるための特別なハードコーティングが施されています。このパックには5枚のディスクが含まれており、互換性に関する注意事項や製品コードが記載されています。記事では、通常のDVDと比べて、UVガードコーティングによる品質と長持ちする利点が強調されています。また、パッケージやデザインについても触れ、環境に優しい素材が使用されていることが記されています。全体として、古い光学ディスクの魅力を称賛しつつ、日本の特定の製品を紹介しています。"
    }
  },
  {
    "id": "800efa95c9c2acf4",
    "title": {
      "en": "'I Want to Make You Immortal' – How one woman confronted her deepfakes stalker",
      "ko": "불멸을 원하다: 딥페이크 스토커와의 대결",
      "ja": "「不死の愛」"
    },
    "type": "story",
    "url": "https://www.404media.co/email/2933ae7f-3e4c-4b4c-ac97-992e68ee5956/",
    "score": 13,
    "by": "SLHamlet",
    "time": 1743613149,
    "content": "Content warning: This article contains mentions of self-harm and suicide.Joanne Chew found deepfakes of herself online the same way many women have found themselves face-swapped into porn: She was searching her own name after a big accomplishment.“Sometimes I just Google my name to see what comes up,” Chew told me in a phone call in August 2024. “I want to see, like, is it my artwork, or my acting, or my main website that comes up first? And then I saw this, and I thought, ‘Okay, this is weird.’” Someone was posting deepfakes of her with her full name in the video titles, alongside racist slurs, to popular tube sites.Chew acted in the May 2024 film Dead Wrong and suspects her harasser started ramping up his targeting her in AI face-swapped porn shortly before the time it came out.“At the time, I thought, ‘It's gonna blow over.’ Because this is bound to happen the more you move forward in your career as any sort of public person,” she said. “But then I noticed he was putting up more and more... And then I started wondering, is it somebody that I know?” Although the names changed over the year, all of the deepfake content at that point was coming from the same username, “Ron.” 404 Media isn’t publishing his screen names to avoid amplifying his accounts.Many targets of deepfake harassment attempt to tackle the barrage of harassment themselves by finding and reporting content to sites that are difficult to reach and often rarely respond. This is a time-consuming, traumatizing process. Chew did this for a while. “Initially I thought it was just going to be a few videos, and I had other girlfriends who modeled and acted, with much bigger followings than me, who said unfortunately these things happen as our careers progress,” she said.She pushed what she saw out of her mind for a few months until she checked again around August. She was horrified, she said, to see how much more had been uploaded in just a few months. “At the height, he had an album of over 2,000 pieces of content, [was posting] on multiple sites, multiple YouTube channels, and then he started making multiple accounts on Facebook and Instagram to direct message me.”At that point, she enlisted the help of Charles DeBarber, an online investigator who previously helped Girls Do Porn victims reclaim their images online.“We're seeing a rapid upswing of AI generated art used in harassment. The ease [with which] even a lay person can use an open source tool to create deep fakes is going to only make them increase,” DeBarber told me. “The technology is inevitable, but the way it is used requires careful regulation and consequences for its abuse. We're still struggling to catch up to technology.”Chew’s harasser only ramped up his efforts as time went on. Ron contacted Chew directly to insult her, obsess over her, or beg for her forgiveness, all while posting more degrading content all over the internet. Nearly a year later, Chew is still dealing with the fallout of becoming a victim of non-consensual, algorithmically-generated intimate imagery.“After discovering this content, I’m not going to lie… there are times it made me not want to be around any more either,” she said. “I literally felt buried.”When a big-name celebrity like Scarlett Johansson or Taylor Swift is targeted with deepfake harassment, it’s often from a legion of “fans,” people who join group efforts in Telegram channels or make Civitai models of a specific person. It’s been this way from the beginning of deepfakes, with people trading tips and tricks for the best prompts, platforms, and generative AI tools to create whatever explicit material they’re trying to achieve featuring a specific person. But when it’s someone who doesn’t have the same professional or financial power as these mega-celebrities, the harassment can take on a different form: one guy, in Chew’s case, producing what feels like an endless stream of images and videos of his obsession in videos stolen from pornographers and warped into something that threatens to take over a person’s life.\n\n“Follower of the goddess J.,” Ron’s Instagram account bio said. The account was dedicated to posting photos of Chew, with an AI-generated image of her in a kimono as the profile picture. He was also, it seemed, the one spreading this content all over every popular deepfake repository and tube site.In August, Chew posted a video explaining the situation to her followers on Instagram. By then, Ron had made hundreds of pieces of deepfaked content of her, and a YouTube channel dedicated to posting it. She filed a complaint to YouTube, and the platform responded, telling her this account was not in violation of its privacy guidelines, which clearly forbids “AI-generated or other synthetic content that looks or sounds like you.”Screenshot courtesy Joanne Chew“How is this not a violation? Someone has taken my name, my face, my professional information, against my consent, and is creating horrible, disgusting, degrading content [and] posting it all over the internet. Make this make sense,” she said in the video.Screenshot via Instagram“I felt like he was watching my social media, so I was kind of just calling him out on stuff to see if he would drop more hints or say more things,” Chew told me.Later that month, Ron removed all of the content from the YouTube channel.But in September, Ron started commenting on Chew’s Instagram posts. And for the first time, she engaged with her harasser directly, replying to his comments.Then, he sent her a barrage of messages on Instagram, pleas for attention and forgiveness mixed in with threats. “Please give my life some meaning,” he wrote. “I dont want to just be the deepfake porn monster I started as. What did you say I was? A deranged monster. People can change. Right? Let me change and be a good person. To me you have the most beautiful face of any asian girl I have ever seen. Please let me be your devoted worshipper. Ok I will put up nice pics of you on my instagram. Until you say otherwise. You mocked my art before. But these will be real art. Inspired by you, Jo.”He continued sending her long, emotionally-charged messages, about how he feels worthless and is a monster, how he hated himself and wanted to die. “I just want to say that Im with you on A.I. We got to stop it,” he said. “It hurts women. But it also addicting and does terrible things to the men who use it. Sure it feels good and its exciting. But after the poison is released, there is guilt and shame. I hated myself after every release. Its terrible to be the monster you hate.”Illustration: Lindsay BallantHe begged her to see him as her biggest fan, and to consider letting him start an OnlyFans on her behalf. He said he made money off of making deepfakes of her. “Men love you. Use them for yourself,” he wrote. “I will stop if you ask me to. If you want me to never look at any of your social media, all you have to do is ask. I am a man of my word. If you ask me to, I will never look you up ever again. I will stop being a fan.”“He made a point of calling me Jo because I said only people who grew up with me are allowed to call me that and for a while he was purposely referring to me as ‘Jo’ in some of the titles of his content and while messaging me,” she said.Chew didn’t engage with any of these direct messages. But on the same day he was sending her these screeds, he uploaded a new video to a tube site: “Hate-Fucking Joanne Chew Some Chinese Whore.”On Facebook, he sent her more incredibly lengthy messages about his obsession with her.“I don't want you dead. I am making you immortal,” one message said. He continued:“You hate me now, but maybe someday you will see things my way. I am not the monster you think I am. I'm just honest with my nature. I'm also sorry about your dad. I lost mine when I was a kid. Yes, it's true. I do love your image. And rest in mind, I'm not anyone from your life. [...] So life isn't that nice, so I've made up your personality and surrounded it with AI flesh. I have a mask of you that I make my tiny Asian girlfriend wear. Lastly, yes, I do have eight inches. It's not the biggest, but it is fine for little Asian girls. I'm good with my life and my love of the girl I have created in my mind with your face and my girlfriend's body. No one loves you as much as I do. You should be flattered that anyone loves you. And yes, my art is of the highest integrity, because it is actually truly honest. It isn't hiding or lying like all the beta males in your life. I am a real man that desires your body and isn't afraid to say so, not your real one, though, that one is bold and faded, but your AI body is forever young, Jo.”She replied to some of his Facebook messages, trying to goad him into giving more information she could potentially bring to the police. But he never took the bait, instead continuing to send long rants about his sex life, her appearance, and his racist fetishes. (Chew still hasn’t gone directly to the police; she told me she’s had negative experiences going to her local police for assault, something many women report as a systemic issue across police forces.)By late September, things became quiet. He’d deleted or deactivated his Instagram and Facebook accounts. But another account, under a new username, popped up in October and restarted the harassment, posting more to sites where people seek out deepfake porn.In some videos and images, the bodies he swapped her face onto seemed very young, and were posted alongside videos of children.In November, Chew found someone posting the same images and videos to another site with her Chinese name. “It’s very sensitive for me as I’ve grown sick and tired of the fetishization of Asian women (that I’ve been exposed to my entire life) and I’ve only been open with my Chinese name in the last decade or so.” she told me in an email. “It looks like it’s all preexisting content. Drives me nuts someone or multiple people are out there freely distributing said content facing no repercussions (and even profiting from it).”Around the same time, the videos returned to YouTube, posted by two new accounts, where the uploader titled videos with Chew’s full name.Screenshot via YoutubeBy December, other users were reposting the same content on porn tube sites—again with her full name in the titles. Around that time, a new username popped up in her Instagram comments, claiming that Ron died by suicide and that she was to blame.“Initially wasn’t planning on replying, but wanted to see if he would drop any more information (whether or not it’s true is debatable),” Chew told me at the time. “Then he started making excuses for Ron (whether he is him or one of his followers remains to be seen) saying he was mentally challenged and then tried to blame me for his suicide, which also may or may not have happened.”Screenshot via YoutubeOver the course of 10 months, Chew kept finding more accounts posting her image, her full name, and graphic videos and photos alongside degrading titles and descriptions.As of writing, the harassment has slowed down. In the last year, Chew has sent me dozens of emails with links to hundreds and thousands of pieces of content and screenshots showing more deepfakes, comments, and videos on multiple platforms, many more than can be shown in one article. Much of it is gone after DeBarber’s reporting and takedown notices and searching for her name on Google no longer returns results from porn sites, but some of it is still online.But she’s still terrified of the long-term effects this harassment could continue to have. Although she’s a working actor, she still relies on working in the corporate world to make ends meet between the more sporadic gigs in the arts, and those jobs often require background checks. And as an actor, it’s made networking and social events harder, as trusting people outside of her closest confidants has become difficult. “It's made me incredibly wary of men, which I know isn't fair, but Ron could literally be anyone,” she said. “And there are a lot of men out there who don't see the issue, they wonder why we aren't flattered for the attention.”Deepfakes started as a novel AI-powered explicit imagery abuse technique seven years ago. The technology went from crude frankenporn among the programming-savvy and morally flippant to producing fakes so realistic it was considered a national security threat within months of its inception. But its most popular use has always been as a mass-harassment tool. The platforms where people spread deepfakes have only expanded in that time, while the methods for making deepfakes have gotten simpler; so simple that schoolchildren do it. The adults in the room, as well as policymakers, continue to fail victims of deepfake harassment. Conversations about deepfakes still leave sex workers, who are doubly exploited in this content, behind. AI continues to explode exponentially, while women targeted by this kind of harassment say again and again and again that they believe sexualized online harassment is part of the deal of being a successful woman on the internet: untenable and yet part of some unwritten contract.“The Violence Against Women Act Reauthorization Act of 2022 created a federal civil cause of action for victims of non-consensual content,” DeBarber said. “This law allows victims to file a lawsuit against the person who disclosed their intimate images without consent. However, this law doesn't cover ‘deepfakes’ including those created via AI. The focus tends to be on celebrities, influencers, and political figures. This itself is changing rapidly. We feel lawmakers and voters aren't seeing the larger picture — this is an everyone issue.”Even when proposed legislation takes a new stab at criminalizing deepfakes, like the TAKE IT DOWN Act is currently attempting, it risks being used as a weapon by those who would love to further curb free speech online, rather than being nuanced, effective, and inclusive — or learning from legislative mistakes of the past.While legislators and platforms continue to fumble around for solutions and police push victims to the side, everyone suffers. There is still no technological solution to deepfakes, and a perfect legal one seems far away, too. But Chew’s experience confronting her harasser gives us a new look into the mind of the people who dole out the abuse and hide behind anonymity, and the exhausting process of reclaiming one's own name.\n\n      About the author\n      Sam Cole is writing from the far reaches of the internet, about sexuality, the adult industry, online culture, and AI. She's the author of How Sex Changed the Internet and the Internet Changed Sex.\n\n        More from Samantha Cole",
    "summary": {
      "en": "Joanne Chew discovered deepfake videos of herself online, created without her consent, after searching her name following a film release. Initially, she thought it was a temporary issue, but the harassment escalated, leading to thousands of fake images and videos being posted across various platforms. Chew faced ongoing harassment from a man named Ron, who created and distributed degrading content of her while also sending her disturbing messages.\n\nDespite her efforts to report the content, platforms like YouTube often failed to take action, as they did not see it as a violation of their guidelines. Chew's situation highlights the growing problem of deepfake harassment, especially for individuals without the protection of celebrity status. As the technology becomes easier to use, the harassment can feel overwhelming, and Chew expressed feelings of despair and fear regarding its long-term impact on her life and career.\n\nLegally, while there have been some advancements, such as the Violence Against Women Act allowing victims to sue for non-consensual content, deepfakes remain largely unregulated. This leaves many victims like Chew vulnerable and underscores the need for better protection and legislation against this form of abuse.",
      "ko": "조안 체우는 영화가 개봉된 후 자신의 이름을 검색하다가 동의 없이 만들어진 딥페이크 비디오를 발견했습니다. 처음에는 일시적인 문제라고 생각했지만, 괴롭힘이 심해지면서 수천 개의 가짜 이미지와 비디오가 다양한 플랫폼에 게시되었습니다. 체우는 론이라는 남성으로부터 지속적인 괴롭힘을 당했으며, 그는 그녀에 대한 모욕적인 콘텐츠를 제작하고 배포하며 불안한 메시지를 보내왔습니다.\n\n그녀는 이러한 콘텐츠를 신고하기 위해 노력했지만, 유튜브와 같은 플랫폼은 이를 가이드라인 위반으로 보지 않아 제대로 대응하지 않았습니다. 체우의 상황은 딥페이크 괴롭힘이 증가하는 문제를 드러내며, 특히 유명인 보호를 받지 못하는 개인들에게 더욱 심각합니다. 기술이 점점 더 쉽게 사용될 수록 괴롭힘의 정도는 심해지고, 체우는 이러한 상황이 자신의 삶과 경력에 미칠 장기적인 영향에 대해 절망감과 두려움을 느끼고 있다고 전했습니다.\n\n법적으로는 비동의 콘텐츠에 대해 피해자가 소송을 제기할 수 있도록 하는 여성폭력방지법과 같은 일부 진전이 있었지만, 딥페이크는 여전히 대부분 규제되지 않고 있습니다. 이로 인해 체우와 같은 많은 피해자들이 취약한 상태에 놓여 있으며, 이러한 형태의 학대에 대한 더 나은 보호와 법률이 필요하다는 점이 강조됩니다.",
      "ja": "ジョアン・チューは、映画の公開後に自分の名前を検索した際、無断で作成されたディープフェイク動画をオンラインで発見しました。最初は一時的な問題だと思っていましたが、嫌がらせはエスカレートし、さまざまなプラットフォームに数千の偽の画像や動画が投稿される事態に至りました。チューは、ロナンという男性からの継続的な嫌がらせに直面しました。彼はチューの侮辱的なコンテンツを作成・配布し、さらに不気味なメッセージを送ってきました。\n\nチューはコンテンツを報告する努力をしましたが、YouTubeなどのプラットフォームは、これをガイドライン違反とは見なさず、行動を起こさないことが多かったです。チューの状況は、特に有名人でない人々にとって、ディープフェイクによる嫌がらせが増加している問題を浮き彫りにしています。技術が使いやすくなるにつれて、嫌がらせは圧倒的に感じられ、チューはこのことが自分の人生やキャリアに与える長期的な影響について絶望感や恐怖感を抱いていると述べました。\n\n法的には、非同意のコンテンツに対して被害者が訴えることを可能にする「女性に対する暴力防止法」などの進展がありましたが、ディープフェイクは依然としてほとんど規制されていません。このため、チューのような多くの被害者が脆弱な状態に置かれ、この種の虐待に対するより良い保護と立法の必要性が強調されています。"
    }
  },
  {
    "id": "10d5d042ce684176",
    "title": {
      "en": "The Myst Graph: A New Perspective on Myst",
      "ko": "미스트 그래프: 새로운 시선",
      "ja": "ミストの新視点"
    },
    "type": "story",
    "url": "https://glthr.com/myst-graph-1",
    "score": 244,
    "by": "tobr",
    "time": 1743527988,
    "content": "[data-rmiz-ghost] {\n      position: absolute;\n      pointer-events: none;\n    }\n    [data-rmiz-btn-zoom],\n    [data-rmiz-btn-unzoom] {\n      background-color: rgba(0, 0, 0, 0.7);\n      border-radius: 50%;\n      border: none;\n      box-shadow: 0 0 1px rgba(255, 255, 255, 0.5);\n      color: #fff;\n      height: 40px;\n      margin: 0;\n      outline-offset: 2px;\n      padding: 9px;\n      touch-action: manipulation;\n      width: 40px;\n      -webkit-appearance: none;\n      -moz-appearance: none;\n      appearance: none;\n    }\n    [data-rmiz-btn-zoom]:not(:focus):not(:active) {\n      position: absolute;\n      clip: rect(0 0 0 0);\n      clip-path: inset(50%);\n      height: 1px;\n      overflow: hidden;\n      pointer-events: none;\n      white-space: nowrap;\n      width: 1px;\n    }\n    [data-rmiz-btn-zoom] {\n      position: absolute;\n      inset: 10px 10px auto auto;\n      cursor: zoom-in;\n    }\n    [data-rmiz-btn-unzoom] {\n      position: absolute;\n      inset: 20px 20px auto auto;\n      cursor: zoom-out;\n      z-index: 1;\n      display: none;\n    }\n    [data-rmiz-content=\"found\"] img,\n    [data-rmiz-content=\"found\"] svg,\n    [data-rmiz-content=\"found\"] [role=\"img\"],\n    [data-rmiz-content=\"found\"] [data-zoom] {\n      cursor: zoom-in;\n    }\n    [data-rmiz-modal]::backdrop {\n      display: none;\n    }\n    [data-rmiz-modal][open] {\n      position: fixed;\n      width: 100vw;\n      width: 100dvw;\n      height: 100vh;\n      height: 100dvh;\n      max-width: none;\n      max-height: none;\n      margin: 0;\n      padding: 0;\n      border: 0;\n      background: transparent;\n      overflow: hidden;\n    }\n    [data-rmiz-modal-overlay] {\n      position: absolute;\n      inset: 0;\n      transition: background-color 0.3s;\n    }\n    [data-rmiz-modal-overlay=\"hidden\"] {\n      background-color: rgba(255, 255, 255, 0);\n    }\n    [data-rmiz-modal-overlay=\"visible\"] {\n      /* This bg color is different from default */\n      background-color: rgba(0, 0, 0, 0.5);\n    }\n    [data-rmiz-modal-content] {\n      position: relative;\n      width: 100%;\n      height: 100%;\n    }\n    [data-rmiz-modal-img] {\n      position: absolute;\n      cursor: zoom-out;\n      image-rendering: high-quality;\n      transform-origin: top left;\n      transition: transform 0.3s;\n      /* This is added additionally to override prose styles of image*/\n      margin: 0 !important;\n    }\n    @media (prefers-reduced-motion: reduce) {\n      [data-rmiz-modal-overlay],\n      [data-rmiz-modal-img] {\n        transition-duration: 0.01ms !important;\n      }\n    }\n<img alt=\"The Myst Graph: A New Perspective on Myst\" decoding=\"async\" data-nimg=\"responsive\" style=\"position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%\" class=\"mb-0 block w-full\" src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1743208608379/436285b6-d65e-45ce-879a-5bbd2565d85d.png?w=1600&amp;h=840&amp;fit=crop&amp;crop=entropy&amp;auto=compress,format&amp;format=webp\"/>The Myst Graph: A New Perspective on Myst<img alt=\"Guillaume Lethuillier&#x27;s photo\" loading=\"lazy\" decoding=\"async\" data-nimg=\"intrinsic\" style=\"position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%\" class=\"relative z-20 block w-full rounded-full\" src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1698082081414/e5bc19ce-d40b-464e-87c1-248105102999.png?w=200&amp;h=200&amp;fit=crop&amp;crop=faces&amp;auto=compress,format&amp;format=webp\"/>Guillaume Lethuillier·Mar 29, 2025\n👋\nHi HackerNews! Please note that a second article uses graph analysis to reveal new findings about Myst (e.g., unreachable views, most connected locations).\n\nUpon reflection, Myst has long been more analogous to a graph than a traditional linear game, owing to the relative freedom it affords players. This is particularly evident in its first release (Macintosh, 1993), which was composed of interconnected HyperCard cards.\nIt is now literally one. Here is Myst as a graph:\n\nThe Myst graph, a comprehensive representation of the game’s structure that maps the connections between various views and locations, is downloadable as a poster-size PDF.\n\nPermalinkIntroduction\nPermalinkOrigins of the Myst Graph\nI fondly recall creating, just like many others (including more skilled cartographers), a rudimentary topological map of the infamous Myst Mazerunner puzzle when I was young. The idea recently struck me: could this approach be extended to the entire game? The HyperCard implementation’s deterministic connection of cards seemed tailor-made for an even more abstract map: a graph.  After all, graphs are a powerful tool for information analysis. By having the ability to represent the original Myst as a network of interconnected nodes and edges, we could gain a deeper understanding of how the different aspects of the game are related and uncover new insights into its underlying mechanics.\nHowever, I did not see myself creating it manually; it would have been a monumental and error-prone task. Fortunately, I soon discovered, thanks to the efforts of Uli Kusterer (Youtube channel: “Masters of the Void”) and the “Reverse Engineering Myst” presentation at Mysterium Con 2024, that the game’s source code was accessible. That changed everything: how about generating the graph programmatically?\nPermalinkRoadmap\n(1) In this article, I introduce the concept of the Myst graph, explaining why it is an important tool for analyzing the game, and walk readers through how to interpret the graph, highlighting key concepts and insights. (2) A second article, already published, explores new findings that have emerged from analyzing the Myst graph. This article builds on the present article, providing additional context and explanations for the insights gained from the graph. (3) A third article, not published yet, will detail the technical approach and open source “DeMystify,” the program I created to generate the graph. This will provide readers with hands-on experience exploring the Myst graph using code. (4) Last, a fourth article will discuss how this graph can be used as a starting point for new projects and speculate on what other insights might be gained by further analysis.\nPermalinkMinor Terminology Clarifications\nBefore diving into the presentation of the graph, let’s clarify some words. “Dunny” refers to D’ni: this is how the D’ni stack was initially named. Also, I may refer to the Myst Age as the “Myst Island” for variation. When discussing graph theory, I will use the following terms: nodes will refer to the vertices, while edges represent the connections between them. For those unfamiliar with graph theory, just remember that edges are arrows pointing from one box (or view, or card) to another, the nodes, illustrating a relationship between the two.\nPermalinkDisclaimer\nThis project is a personal initiative to analyze and understand the classic Myst game. It is an unofficial and nonlucrative open-source effort created for educational purposes only. This project is not affiliated with Cyan Worlds, Inc. or the original creators of Myst. Any opinions, insights, or analyses presented on this blog are my personal views and do not reflect any official positions or statements of Cyan Worlds, Inc., its affiliates, or any organizations I might be affiliated with.\nBefore analyzing this graph, please be aware that this series of articles will contain spoilers for the game. If you are new to Myst or plan to play it soon, I recommend skipping these articles and experiencing it firsthand: Myst remains such a fantastic game!\nPermalinkBasic Properties of the Myst Graph\nPermalinkNodes and Edges\nThe game comprises 6 HyperCard stacks, one for each Age, totaling 1,355 cards. The graph abstracts these into 1,364 nodes connected by 3,189 edges.\nThe number of nodes does not coincide with the number of cards and stacks because 3 nodes are virtual. They represent cards that were not shipped in the release. They only exist because they are referred to by actual cards. These virtual nodes are Mechanical Age 17673 and 20348, and Selenitic Age 31832.\nIn this version of the graph, edges do not encode gameplay constraints. A card is considered connected to another if it references that card; the specifics of how the player interacts with the card to reach the other card—whether by simply clicking or solving a complex puzzle—are left implicit. (This choice is discussed later.)\nPermalinkPaths\nPaths are sequences of edges from node A to node B. Because the gameplay has not been integrated into the graph, they do not necessarily represent feasible direct paths in the game. For instance, a puzzle on the path may require performing actions outside the path. In other words, the paths evoked in these articles abstract away all the constraints of the game, except for the connections between cards.\nTheoretically, the shortest path between the starting point and the (good) ending consists of 24 edges. From a pure graph distance point of view, reaching the beginning of the game and its ending only takes 24 movements or a change of direction. The most distant nodes (in other words, the longest shortest path in the graph) are separated by 130 edges. In the context of the game, these paths are detailed in the second article.\nPermalinkClusters\nInterestingly, while the file that encodes the graph does not contain any information about clusters (except for isolated clusters, as explained in the second article, and for the colors of the nodes, for rendering purposes), the rendering engine that transforms it into a PDF, creates an organic (albeit imperfect) formation of clusters: the Myst Island in the middle, and the Ages at the periphery. This is explainable: Myst Island refers to all Ages; each Age only refers to Myst Island.\nThere is one disconnected cluster outside a few isolated nodes—that, consequently, cannot be reached from within the game. This isolated cluster comprises 3 nodes, including 2 virtual ones, from the Mechanical Age: originally, 17552 had directed edges with 20348 and 17673, both absent from the Mechanical Age stack.\n\nPermalinkLegend: How to Read the Graph\nPermalinkNodes\nPermalinkColors\nEach Age has its own color to facilitate the identification of stack-based clusters.\nPermalinkNodes Labels\n\nWhen a node abstracts a stack, the name of the stack (corresponding to the Age) is prefixed with “[Stack]”. For instance, “[Stack] Myst” is the stack containing the cards belonging to the Myst Island.\nNodes representing cards are labeled following this pattern: “{Stack}:{ID} [({Original name})] \\ {Image name}”.\nThe cards are identified by an ID (see page 280 of this reference), a HyperCard-generated integer. Example: “Mechanical Age:12345” refers to card ID 12345 belonging to the Mechanical Age stack. These IDs are unique per stack. Since each stack has its own set of unique IDs, there can be collisions across stacks where two or more IDs are identical between different stacks. This is evident in the Myst graph, where some IDs appear twice across Ages (8059, 8794, 9075, 18304, 19870, 21064, 32302, 33578, 34421, 39710, 52457, 55809, and 74269). As HyperCard automatically increments these IDs within each stack, one can reasonably infer that the greater an ID value is, the closer in time it was created relative to the initial release. However, this relationship may not hold for the underlying assets, as they were rendered separately through a different process. In other words, a “recent” (relative to the release) card does not necessarily mean that the asset it represents was also rendered close to the release.\nCards can have been voluntarily named. When that occurs, the original name is specified between parentheses after the ID. For example, “Myst:81655 (AchenarLose)” was named “AchenarLose” by the Myst creators.\nLast, cards represent a view or an image, so they need a reference to an asset filename. This information is displayed just below the label. So “Myst:8336 (dock) \\ Dock1-E” means that card ID 8336, belonging to the Myst Island and named initially “dock” features an image called “Dock1-E”. This specific node plays an important role in the game, as it is where the player starts.\n\nPermalinkSpecial Nodes\n\nSink nodes serve as destinations, receiving connections but not providing any exit paths. They can be reached within the game, contrary to the following categories of nodes. Source nodes act as starting points, sending out connections and initiating new paths without incoming edges. They are unreachable in the game. Isolated nodes are unreachable from the rest of the graph, having no incoming or outgoing edges that could be followed.\n\nMore specific to the game, nodes with blue borders represent views containing blue pages, while those with red borders show views having red pages. The unique, purple-bordered node denotes a view encompassing blue and red pages.\nPermalinkEdges\nPermalinkDirections\n\nThe Myst graph is a directed graph. The quasi-totality of the edges is either directed (they connect two nodes in one direction) or bidirectional (they connect two nodes in both directions). Rarely (3 occurrences), some edges are self-loops, connecting a node to itself.\nIn the game, a bidirectional edge typically represents a direct change of orientation (cardinal points, looking up and down). Movements are usually represented by directed edges, as walking backward is impossible, except for some exceptions (backtracking edges; see below).\nHere is a concrete demonstration. In the village of the Channelwood Age, changing between one of the staircase ascending views (17225) and its corresponding pathway view (73038) can be done in a single click as the edge is bidirectional. However, once the player takes the stairs to go up (73346), it is impossible to backtrack directly to the original staircase ascending view without first turning around to look down (73587), then reaching the original pathway view (73038) and last turning around again (17225).\n\nTo summarize, and with some exceptions, from a navigational perspective, while directed edges can be interpreted as walking, bidirectional edges can be interpreted as changing directions.\nPermalinkSpecial Edges\n\nEdges connecting two nodes from different Ages (cross-Ages edges) are rendered with a thick line. Edges that connect exclusively two nodes are backtracking edges. They act as a device to zoom in/zoom out (one click to enter the next node, one click to go back to the previous node) and are represented by edges with inversed arrow heads and tails. Last, some of the connections are commented out in the source code of Myst: they characterize disabled edges, represented by dashed lines with a tee arrow shape.\nPermalinkTransitivity\nIn Myst, there are three forms of transitivity.\n\nFirst, by default, intermediary nodes are fully transitive (edges directions notwithstanding, obviously). If node B can access A, and C and D can access B, then C and D can access A. This is the common navigational mechanism.\nSecond, some edges are intransitive: while B can access A and, reciprocally, A can access B, C and D can access B too, but they cannot access A. This relationship characterizes the backtracking edges.\nLastly, some edges are partially transitive: if node B can access A, C, and D can access B, only D can access A. In the realm of Myst, this is only true of the edges making the player return to Myst after completing an Age. Upon returning to Myst Island with the book, they first systematically see the ceiling of the library (Myst:44018). When they click the ceiling, they automatically face the bookshelf (Myst:46439). (The mechanism by which this is done with HyperTalk, the scripting language of HyperCard, will be detailed in the third article.)\n\nPermalinkLimitations\nFirst, as explained in the previous section, the graph does not capture the gameplay mechanics. Although this approach has its limitations, it provides a significant advantage. By separating the gameplay from the graph structure, we can already gain novel insights into Myst without dealing with the complexities of puzzle design. Of course, nothing prevents future versions of the graph from including this logic.\nSecond, the rendered clusters are not entirely separate from each other. They overlap in their boundaries. For example, Myst:38896 and Myst:30143 encroach on the D’ni Age. While it would be possible to constrain the graph so there is no overlapping, I made the rendering engine as Age agnostic as possible for this initial version. As “DeMystify”, the graph generator, will be released, readers can try different arrangements; I would be very interested in seeing their results.\nThird, the graph only captures straightforward relationships between cards and stacks. Analyzing the scripts more granularly and identifying complex relationships, such as image substitutions, would be interesting. This lack of granularity is evident for the books from the bookshelf: the current graph does not sufficiently capture the pages. A future version may improve this aspect.\n\nPermalinkNext Steps\nNow that we have had a chance to familiarize ourselves with the Myst graph, its structure, and how it relates to the game, let’s dive deeper with a second article, which explores some of the insights and discoveries made possible through this new conceptual tool. The article will take us through the hidden connections and relationships between various game elements, revealing new aspects of Myst that were previously unknown or unseen.\nThe graph will make one with the game.\nmystgraph theoryVideo gamesShare this",
    "summary": {
      "en": "The text introduces a project focused on creating a \"Myst graph,\" which visually represents the connections between various locations and views in the classic video game Myst. The key points are:\n\n1. **Graph Concept**: The Myst graph treats the game as an interconnected network rather than a linear experience, helping to analyze its structure.\n\n2. **Graph Details**: The graph consists of 1,364 nodes (representing different views or locations) and 3,189 edges (connections between them), including some virtual nodes that don’t appear in the game.\n\n3. **Analysis Tool**: The graph is designed to uncover new insights about the game’s mechanics and relationships between elements, separating gameplay from the structural analysis.\n\n4. **Future Articles**: The creator plans a series of articles discussing the graph's significance, findings from its analysis, the technical process of its creation, and potential future projects inspired by it.\n\n5. **Educational Purpose**: This initiative is non-commercial and aims to enhance understanding of Myst without being affiliated with the original creators.\n\nOverall, the project seeks to deepen appreciation for Myst through a graphical representation of its game structure.",
      "ko": "이 프로젝트는 고전 비디오 게임인 Myst의 다양한 위치와 시점 간의 연결을 시각적으로 나타내는 \"Myst 그래프\"를 만드는 데 초점을 맞추고 있습니다. 주요 내용은 다음과 같습니다.\n\nMyst 그래프는 게임을 선형적인 경험이 아닌 상호 연결된 네트워크로 간주하여 그 구조를 분석하는 데 도움을 줍니다. 이 그래프는 1,364개의 노드(각기 다른 시점이나 위치를 나타냄)와 3,189개의 엣지(이들 간의 연결)를 포함하고 있으며, 게임에 나타나지 않는 가상의 노드도 포함되어 있습니다.\n\n그래프는 게임의 메커니즘과 요소 간의 관계에 대한 새로운 통찰을 발견하도록 설계되어 있으며, 게임 플레이와 구조적 분석을 구분합니다. 제작자는 그래프의 중요성, 분석 결과, 제작 과정의 기술적 측면, 그리고 이를 바탕으로 한 미래 프로젝트에 대해 논의하는 일련의 기사를 계획하고 있습니다.\n\n이 프로젝트는 상업적 목적이 아닌 교육적인 목적을 가지고 있으며, 원작 제작자와는 관련이 없이 Myst에 대한 이해를 높이는 것을 목표로 하고 있습니다. 전반적으로 이 프로젝트는 Myst의 게임 구조를 그래픽적으로 표현함으로써 게임에 대한 감사를 깊게 하려는 의도를 가지고 있습니다.",
      "ja": "このプロジェクトは、クラシックなビデオゲーム「Myst」のさまざまな場所や視点のつながりを視覚的に表現する「Mystグラフ」の作成に焦点を当てています。重要なポイントは以下の通りです。\n\nまず、Mystグラフはゲームを直線的な体験ではなく、相互に関連したネットワークとして捉え、その構造を分析するのに役立ちます。グラフは1,364のノード（異なる視点や場所を表す）と3,189のエッジ（それらのつながり）で構成されており、ゲームには登場しない仮想ノードも含まれています。\n\nこのグラフは、ゲームのメカニクスや要素間の関係について新たな洞察を明らかにするために設計されており、ゲームプレイと構造分析を分けて考えることができます。制作者は、グラフの意義や分析から得られた発見、作成の技術的プロセス、そしてそれに触発された将来のプロジェクトについての一連の記事を計画しています。\n\nこの取り組みは商業的なものではなく、元の制作者とは関係なく、Mystの理解を深めることを目的としています。全体として、このプロジェクトはゲームの構造をグラフィカルに表現することで、Mystへの理解と appreciationを深めることを目指しています。"
    }
  },
  {
    "id": "202da4391f165288",
    "title": {
      "en": "How Silica Gel Took Over the World",
      "ko": "실리카겔의 세계 정복",
      "ja": "シリカゲルの逆襲"
    },
    "type": "story",
    "url": "https://www.scopeofwork.net/silica-gel/",
    "score": 182,
    "by": "Hooke",
    "time": 1743536018,
    "content": "I find them stuffed into the toes of a new pair of sneakers. I find them wedged into a sheaf of seaweed snacks. I find them in the over-inflated bag that contains my new inhaler, and in the vacuum-sealed one puckered around my kids’ 3D printing filament. “DO NOT EAT,” they all admonish me, and I find myself slipping them under the top layer of the garbage already in the trash can, as if my kids wouldn’t be able to control their urge to taste whatever is inside these tiny white pouches. Silica gel packets are everywhere, their presence seemingly the only thing keeping our packaged food crispy and our belongings free of mildew. How on earth did they all get here? Is silica gel taking over the world?Tear its little Tyvek wrapping, and spill a packet of glassy silica gel beads into the palm of your hand; they won’t hurt you. They are made of the same stuff as sand: “Silica” means “silicon dioxide,” which is the primary component of most drinkware, windshields, and the screen of whatever electronic device you’re reading this on. But glass has a density of around 2500 kilograms per cubic meter, and crystalline silicon dioxide (quartz) is around 2650. Silica gel, on the other hand, is more like 700 kilograms per cubic meter. It may look fully dense, but in fact it’s shot through with countless tiny pores. If your windowpane is like a thin sheet of solid ice, then a silica gel bead is like a tiny snowball. Silica gel beads, as seen by a scanning electron microscope. Image credit: Dusan Berek, via Structural inhomogeneities in wide-pore silica gels.Zoom in on a silica gel bead with a scanning electron microscope, and its smooth surface turns discontinuous, riddled with voids about 2.5 nanometers across (roughly the diameter of a strand of DNA). This microstructure gives silica gel radical properties. The silica gel packets in my kids’ seaweed snacks are just a little bit bigger than postage stamps, and have a total mass of about a gram. That single gram of silica gel could have an internal surface area of eight hundred square meters—the size of almost two basketball courts. These factors allow silica gel to adsorb up to 40% of its own weight in water vapor, through a process called capillary condensation. When humid air migrates into the pores of a silica gel bead, its vapor pressure increases, causing water to condense onto the silica gel’s internal surfaces. At the risk of anthropomorphizing water, it’s as if it prefers to be a liquid, stuck within the silica gel’s tiny capillaries, than a vapor, carried along with whatever else is in the air.If you find a packet of silica gel in an imported snack or the pocket of a new jacket, it’s probably there to filter water vapor out of air. Like most filters, there is a limit to how much water a piece of silica gel can hold; its internal surfaces are finite in size, and as a result there is a finite amount of humidity that a given packet of silica gel can adsorb. Luckily, silica gel vendors offer nifty calculators so that their customers can size their silica gel packets to the volumes of air they wish to dry out—and how dry they want the air to be. If you wanted to desiccate the air inside of a child’s balloon, you’d calculate its volume (let’s call it 14 liters), then make some assumptions about its temperature and humidity (around 35°C, 75% RH), then decide what you want its final relative humidity to be (let’s bring it down to 20% RH). Run these numbers and you’d find you needed to pull about 0.3 grams of water out of the air in your balloon. This can be done, pretty reliably, with a one- or two-gram packet of silica gel, slipped into the balloon just before you blow it up.Of course, different applications will entail different goals. The bag that holds your potato chips is not completely impermeable to moisture, and if you’re going to ship it across an ocean and then let it sit on a shelf for a few months, you should expect some additional water vapor to find its way in. There are also applications in which silica gel is used not so much to dry something out but to maintain a particular equilibrium humidity. In the art world, silica gel packets are slipped into exhibit cases and used for their “buffering capacity,” helping maintain a stable (and relatively high) humidity inside the case while conditions in the rest of the gallery might vary.Silica gel is commonly placed inside museum display cases, in hidden compartments that ventilate to the display chamber. Image via the National Park Service's *very* thorough Exhibit Conservation Guidelines, Second Edition. While glass and glassy substances have been used by humans for many thousands of years, it wasn’t until the early twentieth century that Walter Patrick, a researcher at Johns Hopkins, developed and patented an efficient way to create silica gel. He did this by mixing a substance called “water glass” with an acid. Water glass—a fascinating material in itself, as it is essentially water-soluble glass—is an alkaline compound containing sodium oxide and silicon dioxide. When mixed with an acid its silicon dioxide precipitates, linking into the matrix now known as silica gel. The silica gel—glassy and hard—is then washed and dried to remove excess acid, salt, and water.Silica gel became commercially available within five years of Patrick’s invention, through a deal with the Davison Chemical company. But according to this academic paper, which analyzes how discoveries at Johns Hopkins impacted the local economy, “the transition from the laboratory to the commercial world was a long and arduous one... Much development work had been needed to develop the original academic breakthrough, and new applications had to be found for the product before it became a commercial success.”A 1930 pamphlet, published by Davison's silica gel subsidiary, advertises this silica-gel based air conditioning unit. Image via the New York Public Library.But a commercial success it eventually became. In 1927, Davison built a silica gel factory in Baltimore, and by 1930 they claimed to have applied for or received over three hundred patents on the stuff. The company was acquired by W.R. Grace in 1954, and Grace continues to manufacture specialty silica gel products at that same location.\n\n                    Sign up for Scope of Work\n                    Excellent writing about the physical world.\n\n                    Subscribe\n\n                .nc-loop-dots-4-24-icon-o{--animation-duration:0.8s}\n                .nc-loop-dots-4-24-icon-o *{opacity:.4;transform:scale(.75);animation:nc-loop-dots-4-anim var(--animation-duration) infinite}\n                .nc-loop-dots-4-24-icon-o :nth-child(1){transform-origin:4px 12px;animation-delay:-.3s;animation-delay:calc(var(--animation-duration)/-2.666)}\n                .nc-loop-dots-4-24-icon-o :nth-child(2){transform-origin:12px 12px;animation-delay:-.15s;animation-delay:calc(var(--animation-duration)/-5.333)}\n                .nc-loop-dots-4-24-icon-o :nth-child(3){transform-origin:20px 12px}\n                @keyframes nc-loop-dots-4-anim{0%,100%{opacity:.4;transform:scale(.75)}50%{opacity:1;transform:scale(1)}}\n\n                Email sent! Check your inbox to complete your signup.\n\n                    No spam. Unsubscribe anytime.\n\n        To learn about the silica gel industry today, I spoke with Demetrius Michos, a PhD chemist who has worked at Grace for over thirty years. He assured me that I “could not go about my day without touching Grace’s products,” and went on to repeatedly surprise me with obscure and curious applications for silica gel. But they've mostly moved up-market from the little envelopes in our consumer packaged goods. Grace is big in the silica gel business, and they sell a lot of other silica products, but they don’t seem very interested in silica gel desiccant packets specifically. An operator tends the wash pots at WR Grace's Curtis Bay, Maryland factory. The silica gel manufacturing process includes multiple washing steps. Image via W.R. Grace & Co.So I went looking for imports. The Census Bureau’s import data goes back to 1992, and shows rising—and accelerating—imports of both silicon dioxide and silica gel. Total imports peak in 2022, and are currently about ten times their 1992 levels. This is interesting, but it does not answer my question about desiccant packets. Sure, we may be importing more silica gel today than we did in the early nineties. But we’re also importing a lot more shoes than we did in the nineties, and it's not like I find pairs of shoes slipped into every third thing I buy.US Census data on imports for silicon dioxide (a precursor for silica gel—and many other things) and silica gel itself. Image via USA Trade Online.After a round of mostly fruitless phone calls, I finally spoke to John Perona, a sales rep at a company that sells, among other things, silica gel desiccant packets. He started by pointedly questioning my intention of writing anything about silica gel at all, then told me that I was “getting into the weeds. Just look at how things ship around the world, and you can understand the problems that silica gel packets are trying to address.”The reasons behind the increase in silica gel imports, he went on somewhat reluctantly, were simple: “No one wants a silica gel factory in their backyard.” Only specialty silica gel products are manufactured in the US today; the packets that I find in my snacks and pharmaceuticals are either made overseas or, in some cases, assembled in the US from imported silica gel beads.Then John reminded me that a couple decades ago there were hundreds of thousands of manufacturing jobs on a single mile of road, a half-hour from my house. If I lived in Brooklyn then, I could have purchased goods from those factories. They would experience few, if any, swings in temperature and pressure on their short journey from factory to home, and even if they did, their packaging probably would have let excess humidity ventilate off.The farther you ship a product—the longer it takes to go from the factory to the customer’s hands, and the more temperature and pressure cycles it experiences during that time—the more you need to control humidity inside of its airtight packaging. Silica gel is a cheap, easy, and reliable way to do so. In this sense silica gel sits alongside containerized shipping, and stretch wrap, and bills of lading: It is a technology without which we’d have a much harder time maintaining global supply chains. Desiccant packets haven’t actually taken over the world—globalization has.So we seal our seaweed snacks, and our inhalers, and our 3D printer filament inside airtight plastic bags—then ship them across the world. We could, as one engineer I spoke to suggested, fill all these packages with dry, inert gas first. But little Tyvek bags, slipped inside right before they’re sealed up, are quite a bit easier.Update: We published a follow-up to this piece here; it contains a bunch of fascinating miscellany about silica gel. Thanks! To Nick Fountain for suggesting the topic, and to Demetrius Michos, Sharyn Nerenberg, John Perona for educating me on it. Thanks to Brad Avenson for suggesting (facetiously) that we fill our products & packaging with “dry, inert gas,” and a big thanks to the Supporters and Members of Scope of Work, who make it possible for me to spend a week immersing myself in a little corner of the infrastructural world.",
    "summary": {
      "en": "Silica gel packets are commonly found in various products like snacks, inhalers, and 3D printing materials. These tiny white pouches are essential for keeping food crispy and preventing mildew by absorbing moisture from the air. Silica gel is made of silicon dioxide, the same material as sand, but it has a unique porous structure that allows it to hold a significant amount of water vapor.\n\nEach packet can absorb up to 40% of its weight in moisture, making it effective in maintaining low humidity levels. Different applications require different humidity control, such as preserving art in museums or ensuring snacks stay fresh during shipping.\n\nSilica gel was first developed in the early 20th century and became commercially successful by the late 1920s. Today, most silica gel packets are manufactured overseas, as producing them domestically is less common. The rise in silica gel imports is largely due to globalization and the need for moisture control in products that travel long distances, indicating that while silica gel is widespread, it is a response to modern supply chain challenges rather than a takeover of the world.",
      "ko": "실리카겔 팩은 스낵, 흡입기, 3D 프린팅 재료 등 다양한 제품에서 흔히 발견됩니다. 이 작은 흰색 주머니는 공기 중의 수분을 흡수하여 음식이 바삭하게 유지되고 곰팡이가 생기는 것을 방지하는 데 필수적입니다. 실리카겔은 이산화규소로 만들어지며, 이는 모래와 같은 물질이지만 독특한 다공성 구조를 가지고 있어 상당량의 수증기를 저장할 수 있습니다.\n\n각 팩은 자신의 무게의 최대 40%까지 수분을 흡수할 수 있어, 낮은 습도 수준을 유지하는 데 효과적입니다. 다양한 용도에 따라 필요한 습도 조절이 다르며, 예를 들어 박물관의 예술품을 보존하거나 스낵이 배송 중 신선하게 유지되도록 하는 데 사용됩니다.\n\n실리카겔은 20세기 초에 처음 개발되었고, 1920년대 후반에는 상업적으로 성공을 거두었습니다. 현재 대부분의 실리카겔 팩은 해외에서 제조되며, 국내에서 생산되는 경우는 드뭅니다. 실리카겔 수입이 증가한 것은 세계화와 먼 거리를 이동하는 제품에서 수분 조절의 필요성 때문입니다. 이는 실리카겔이 널리 사용되고 있지만, 현대 공급망의 도전에 대한 대응이라는 점을 보여줍니다.",
      "ja": "シリカゲルのパケットは、スナックや吸入器、3Dプリント材料など、さまざまな製品に見られます。これらの小さな白い袋は、食品をパリッと保ち、カビを防ぐために空気中の湿気を吸収する重要な役割を果たしています。シリカゲルは二酸化ケイ素でできており、砂と同じ材料ですが、特有の多孔質構造を持っているため、かなりの量の水蒸気を保持することができます。\n\n各パケットは、自身の重さの最大40%の湿気を吸収できるため、低湿度を維持するのに効果的です。異なる用途には異なる湿度管理が必要で、例えば美術品を博物館で保存する場合や、スナックが輸送中に新鮮さを保つ必要があります。\n\nシリカゲルは20世紀初頭に初めて開発され、1920年代後半には商業的に成功を収めました。現在、ほとんどのシリカゲルパケットは海外で製造されており、国内での生産はあまり一般的ではありません。シリカゲルの輸入が増加しているのは、グローバル化と長距離輸送される製品における湿気管理の必要性によるもので、シリカゲルが広く使われている一方で、現代のサプライチェーンの課題に対する対応であることを示しています。"
    }
  },
  {
    "id": "1c9d96aca3ec4625",
    "title": {
      "en": "Show HN: Offline SOS signaling+recovery app for disasters/wars",
      "ko": "오프라인 SOS 앱",
      "ja": "オフラインSOSアプリ"
    },
    "type": "story",
    "url": "https://github.com/nizarmah/igatha",
    "score": 150,
    "by": "nizarmah",
    "time": 1743544712,
    "content": "Igatha\nIgatha is an open-source SOS signaling and recovery app designed for war zones and disaster areas, enabling offline emergency communication when traditional networks fail.\nStatus\n\niOS: v1.0\nAndroid: v1.0\n\nQuickstart\n\nInstall the app using the links above.\nOpen the app and grant the necessary permissions.\n\nHow to use Igatha\nSending SOS signals (distress mode)\nManual signaling\n\nOpen Igatha.\nEnsure bluetooth is enabled.\nTap \"Send SOS\".\n\nAutomatic signaling\n\nOpen Igatha.\nTap the gear icon (top right).\nEnable \"Disaster Detection\".\n\nWith disaster detection, Igatha will now run in the background, monitoring your device's sensors.\nWhen a potential disaster is detected, you'll receive an \"Are you okay?\" notification:\n\nIf you respond with \"Need help\" or don't respond in 2 minutes, it will automatically broadcast an SOS.\nIf you respond with \"I'm okay\", it will ignore the event.\n\nHelping others (recovery mode)\nIf you're safe and want to help others:\n\nOpen Igatha.\nEnsure bluetooth is enabled (on Android 11 or lower, also enable Location).\nCheck \"People seeking help\".\nMove towards locations where displayed distances decrease.\nListen carefully for audible sirens.\n\nHow Igatha works\nBluetooth low energy (BLE)\nIgatha uses Bluetooth Low Energy (BLE) to:\n\nBroadcast SOS signals.\nScan for nearby SOS broadcasts.\nEstimate approximate distance to the signal source based on signal strength.\n\nNo internet or GPS is required, preventing signal jamming or manipulation.\nSOS signal composition\nThe SOS signal combines:\n\nBLE advertisement: broadcasts a pseudonymized identifier.\nAudible siren: generated via device speakers to help responders locate you.\n\nResponders can toggle additional signals, like flashlight or vibration, remotely. (planned feature)\nDisaster detection sensors\nIgatha detects disasters using device sensors:\n\nAccelerometer: measures sudden motion changes.\nGyroscope: detect orientation and rotation shifts.\nBarometer (if available): detects atmospheric pressure changes, reducing false positives.\n\nDisaster detection triggers when multiple sensors simultaneously detect abrupt changes.\nLocation permissions are required for \"Disaster Detection\".\nBattery usage\nIgatha minimizes battery use by leveraging BLE and optimized sensor monitoring.\nThe app can continuously broadcast for extended periods during emergencies.\nLimitations\nEarly stage\n\nThis is a Minimum Viable Product (MVP) with significant room for improvement\nTesting has been limited to controlled environments\nWhile not guaranteed to work in all scenarios, it provides a potential lifeline where no alternatives exist\n\nSignal range\n\nBLE range: typically 10-30 meters indoors, further outdoors, limited by rubble and building materials.\nOptional extensions: Third-party BLE receivers can extend range significantly.\n\nWhy open source?\nIgatha is open-sourced for:\n\nTransparency: In crisis situations, people need to trust the tools they use. Open source allows anyone to verify the app's security and privacy measures.\n\nAccessibility: Making the code freely available ensures the technology can be used, studied, and adapted by anyone who needs it.\n\nCommunity Impact: War and disaster response tools should be community resources, not commercial products. Open sourcing enables collaborative improvement and adaptation for different crisis scenarios.\n\nContributing\nContributions are vital for improving Igatha:\n\nTesting and bug reports\nDocumentation\nTranslations\nFeature enhancements\nCode optimization\nSecurity reviews\nDistribution\n\nTo contribute, open an issue or submit a pull request.\nPrivacy & security\n\nCompletely offline; no data collection or internet connectivity.\nUses pseudonymized identifiers for privacy.\n\nContact\nFor questions, suggestions, or feedback, please open an issue in the repository.",
    "summary": {
      "en": "**Igatha Summary**\n\nIgatha is an open-source app designed for emergency signaling and recovery in war zones and disaster areas, allowing offline communication when traditional networks are unavailable. \n\n**Current Version:**\n- iOS: v1.0\n- Android: v1.0\n\n**How to Use Igatha:**\n1. **Sending SOS Signals:**\n   - **Manual:** Open the app, enable Bluetooth, and tap \"Send SOS.\"\n   - **Automatic:** Enable \"Disaster Detection\" in settings. The app monitors your sensors and sends an SOS if it detects a disaster and you either respond \"Need help\" or do not respond in 2 minutes.\n\n2. **Helping Others:**\n   - Open the app, enable Bluetooth, and check \"People seeking help.\" Move towards areas where the distance to help decreases and listen for audible sirens.\n\n**How Igatha Works:**\n- Utilizes Bluetooth Low Energy (BLE) to send and receive SOS signals without needing the internet or GPS.\n- Sends an SOS signal with a pseudonymized identifier and an audible siren.\n- Disaster detection relies on sensors like the accelerometer and gyroscope to identify sudden changes.\n\n**Battery Use:**\n- The app is designed to minimize battery consumption and can broadcast for long periods during emergencies.\n\n**Limitations:**\n- Currently a Minimum Viable Product (MVP) with limited testing.\n- BLE signal range is typically 10-30 meters indoors, but can be extended with third-party devices.\n\n**Open Source Benefits:**\n- Transparency and trust in crisis situations.\n- Accessibility for anyone to use and improve the app.\n- Community-driven development to enhance disaster response tools.\n\n**Contributions:**\n- Users can help by testing, reporting bugs, enhancing features, translating, and reviewing security.\n\n**Privacy & Security:**\n- The app operates completely offline with no data collection and uses pseudonymized identifiers for user privacy.\n\n**Contact:**\n- For questions or feedback, users can open an issue in the app’s repository.",
      "ko": "이가타는 전쟁 지역과 재난 지역에서 비상 신호를 보내고 구조를 돕기 위해 설계된 오픈 소스 애플리케이션입니다. 전통적인 통신망이 작동하지 않을 때 오프라인으로 소통할 수 있도록 합니다.\n\n현재 버전은 iOS와 안드로이드 모두 v1.0입니다.\n\n이가타를 사용하는 방법은 다음과 같습니다. SOS 신호를 보내려면 앱을 열고 블루투스를 활성화한 후 \"SOS 보내기\"를 누릅니다. 자동으로 SOS 신호를 보내려면 설정에서 \"재난 감지\"를 활성화합니다. 이 경우 앱이 센서를 모니터링하여 재난을 감지하고, 사용자가 \"도움이 필요합니다\"라고 응답하거나 2분 이내에 응답하지 않으면 SOS 신호를 보냅니다.\n\n다른 사람을 도우려면 앱을 열고 블루투스를 활성화한 후 \"도움을 요청하는 사람들\"을 확인합니다. 도움의 거리가 줄어드는 방향으로 이동하고, 들리는 사이렌 소리를 들어야 합니다.\n\n이가타는 블루투스 저전력(BLE)을 이용해 인터넷이나 GPS 없이 SOS 신호를 주고받습니다. SOS 신호는 가명화된 식별자와 함께 발송되며, audible siren이 함께 울립니다. 재난 감지는 가속도계와 자이로스코프와 같은 센서를 통해 갑작스러운 변화를 감지합니다.\n\n이 앱은 배터리 소모를 최소화하도록 설계되어 있으며, 비상 상황에서 오랜 시간 동안 방송할 수 있습니다. 현재는 최소 기능 제품(MVP)으로, 테스트가 제한적입니다. BLE 신호 범위는 일반적으로 실내에서 10-30미터이지만, 서드파티 장치를 통해 확장할 수 있습니다.\n\n오픈 소스의 장점으로는 위기 상황에서의 투명성과 신뢰성을 제공합니다. 누구나 앱을 사용하고 개선할 수 있으며, 커뮤니티 주도의 개발로 재난 대응 도구를 향상시킬 수 있습니다.\n\n사용자는 테스트, 버그 신고, 기능 향상, 번역 및 보안 검토 등을 통해 기여할 수 있습니다. 이 앱은 완전히 오프라인으로 작동하며 데이터 수집이 없고, 사용자 프라이버시를 위해 가명화된 식별자를 사용합니다.\n\n질문이나 피드백이 있는 경우, 사용자는 앱의 저장소에서 이슈를 열어 문의할 수 있습니다.",
      "ja": "Igathaは、戦争地域や災害地域での緊急信号と救助のために設計されたオープンソースのアプリです。このアプリは、従来のネットワークが利用できない場合でもオフラインでの通信を可能にします。\n\n現在のバージョンは、iOSがv1.0、Androidもv1.0です。\n\nIgathaの使い方は簡単です。まず、SOS信号を送信する方法があります。手動で行う場合は、アプリを開いてBluetoothを有効にし、「SOSを送信」をタップします。自動で送信するには、設定で「災害検知」を有効にします。アプリはセンサーを監視し、災害を検知すると、あなたが「助けが必要」と応答するか、2分以内に応答しない場合にSOSを送信します。\n\n他の人を助ける方法もあります。アプリを開いてBluetoothを有効にし、「助けを求めている人」をチェックします。助けが近づく方向に移動し、音の出るサイレンを聞きます。\n\nIgathaは、Bluetooth Low Energy（BLE）を利用して、インターネットやGPSなしでSOS信号を送受信します。SOS信号は、仮名化された識別子と音の出るサイレンを使って送信されます。災害検知は、加速度センサーやジャイロスコープなどのセンサーを利用して、急激な変化を特定します。\n\nアプリはバッテリー消費を最小限に抑えるように設計されており、緊急時には長時間の放送が可能です。\n\n現在のところ、Igathaは最小限の機能を持つ製品（MVP）であり、テストは限られています。BLE信号の範囲は通常、屋内で10〜30メートルですが、サードパーティ製のデバイスを使用することで拡張できます。\n\nオープンソースの利点として、危機的状況における透明性と信頼性があります。また、誰でもアプリを使用し改善できるため、コミュニティ主導の開発が災害対応ツールの向上に寄与します。\n\nユーザーは、テストやバグ報告、機能の強化、翻訳、セキュリティのレビューなどを通じて貢献できます。\n\nプライバシーとセキュリティについては、アプリは完全にオフラインで動作し、データ収集は行わず、ユーザーのプライバシーを守るために仮名化された識別子を使用しています。\n\n質問やフィードバックがある場合は、アプリのリポジトリで問題を開くことができます。"
    }
  },
  {
    "id": "e65ee5b4ac0dc75f",
    "title": {
      "en": "Shared DNA in Music",
      "ko": "음악의 유전자",
      "ja": "音楽の共鳴"
    },
    "type": "story",
    "url": "https://pudding.cool/2025/04/music-dna/",
    "score": 234,
    "by": "ksampath02",
    "time": 1743541643,
    "content": "This is a project about shared DNA in music.  Loading visualization data...",
    "summary": {
      "en": "This project focuses on the connections between music and shared DNA. It involves analyzing and visualizing data related to these musical relationships.",
      "ko": "이 프로젝트는 음악과 공유된 DNA 간의 관계에 초점을 맞추고 있습니다. 음악적 관계와 관련된 데이터를 분석하고 시각화하는 작업이 포함됩니다.",
      "ja": "このプロジェクトは、音楽と共有されたDNAの関係に焦点を当てています。音楽的な関係に関連するデータを分析し、視覚化することが目的です。"
    }
  },
  {
    "id": "a1098a11ca8484d7",
    "title": {
      "en": "Vitodeploy: Self hosted Laravel Forge alternative",
      "ko": "비토디플로이: 자가 호스팅 라라벨 포지 대안",
      "ja": "ビトデプロイ: 自己ホスティングの選択肢"
    },
    "type": "story",
    "url": "https://vitodeploy.com/",
    "score": 18,
    "by": "wilsonfiifi",
    "time": 1743318116,
    "content": "VitoDeployOpen-SourceServer Management ToolDocumentationLive DemoFeaturesExplore some of Vito's featuresServerProvisions and Manages the serverDatabaseEasy database management, Supports Mysql and MariaDB and PostgreSQLSiteDeploy your PHP applications such as Laravel, Wordpress and moreFirewallManage your server's firewallSSLSupports Custom and Letsencrypt SSLWorkerRun workers in the backgroundServicesManages server's servicesSSH KeysDeploy your SSH Keys to the serverCron JobsCreate and Manage cron jobs on the serverHeadless ConsoleRun ssh commands on your server and see the result right awayMonitoringMonitor your servers' resource usages like CPU Load, Memory and DiskProjectsManage different projects and invite users to manage the serversJoin the communityDiscover what the community has to say about their VitoDeploy experience.Github DiscussionsDiscordMahdi@hazavehDid you know VitoDeploy is the perfect open source alternative of Laravel Forge?Nader Ikladious@NaderIkladiousGotta say using #VitoDeploy to manage server and deployments was indeed the best decision while working on #spurMarcelo Anjos@geek_marceloDeploying with Vito Deploy is a game-changer! 🚀 Fast, reliable, and hassle-free—exactly what every developer needs. Highly recommended! #VitoDeployMahdi@hazavehDid you know VitoDeploy is the perfect open source alternative of Laravel Forge?Nader Ikladious@NaderIkladiousGotta say using #VitoDeploy to manage server and deployments was indeed the best decision while working on #spurMarcelo Anjos@geek_marceloDeploying with Vito Deploy is a game-changer! 🚀 Fast, reliable, and hassle-free—exactly what every developer needs. Highly recommended! #VitoDeployMahdi@hazavehDid you know VitoDeploy is the perfect open source alternative of Laravel Forge?Nader Ikladious@NaderIkladiousGotta say using #VitoDeploy to manage server and deployments was indeed the best decision while working on #spurMarcelo Anjos@geek_marceloDeploying with Vito Deploy is a game-changer! 🚀 Fast, reliable, and hassle-free—exactly what every developer needs. Highly recommended! #VitoDeployMahdi@hazavehDid you know VitoDeploy is the perfect open source alternative of Laravel Forge?Nader Ikladious@NaderIkladiousGotta say using #VitoDeploy to manage server and deployments was indeed the best decision while working on #spurMarcelo Anjos@geek_marceloDeploying with Vito Deploy is a game-changer! 🚀 Fast, reliable, and hassle-free—exactly what every developer needs. Highly recommended! #VitoDeployJorge@heyjorgedevVitoDeploy is one of the few projects I’m really excited about. And it’s built with @laravelphp and @htmx_orgMason@capten_masinNever heard if VitoDeploy until today, this looks amazingArun Joseph@thenexidoNever heard about vitodeploy before and it looks interesting.Jorge@heyjorgedevVitoDeploy is one of the few projects I’m really excited about. And it’s built with @laravelphp and @htmx_orgMason@capten_masinNever heard if VitoDeploy until today, this looks amazingArun Joseph@thenexidoNever heard about vitodeploy before and it looks interesting.Jorge@heyjorgedevVitoDeploy is one of the few projects I’m really excited about. And it’s built with @laravelphp and @htmx_orgMason@capten_masinNever heard if VitoDeploy until today, this looks amazingArun Joseph@thenexidoNever heard about vitodeploy before and it looks interesting.Jorge@heyjorgedevVitoDeploy is one of the few projects I’m really excited about. And it’s built with @laravelphp and @htmx_orgMason@capten_masinNever heard if VitoDeploy until today, this looks amazingArun Joseph@thenexidoNever heard about vitodeploy before and it looks interesting.Bernard Sarfo Twumasi@devsarfoAfter using VitoDeploy by @saeed_vz for a month, I highly recommend it to all PHP developers, especially those using Laravel. It makes server management and PHP application deployment to production servers simple and hassle-free.Muhammad Shafeeq@hmshafeeqRecently tried VitoDeploy, impressed by its features, performance and versatility. Hats off to you for building such a powerful deployment solution and making it open sourced  :)Wilson Bridgett@_wilsonpbJust discovered #vitodeploy and have become a fan. I also like the change to SQLite! Is the 1.x branch in a good place to give it a beta test run. Thx for your work on this projectJames Kokou GAGLO@dzamsgagloVito is a self-hosted web application that helps you manage your servers and deploy your PHP applications into production servers without a hassle.Bernard Sarfo Twumasi@devsarfoAfter using VitoDeploy by @saeed_vz for a month, I highly recommend it to all PHP developers, especially those using Laravel. It makes server management and PHP application deployment to production servers simple and hassle-free.Muhammad Shafeeq@hmshafeeqRecently tried VitoDeploy, impressed by its features, performance and versatility. Hats off to you for building such a powerful deployment solution and making it open sourced  :)Wilson Bridgett@_wilsonpbJust discovered #vitodeploy and have become a fan. I also like the change to SQLite! Is the 1.x branch in a good place to give it a beta test run. Thx for your work on this projectJames Kokou GAGLO@dzamsgagloVito is a self-hosted web application that helps you manage your servers and deploy your PHP applications into production servers without a hassle.Bernard Sarfo Twumasi@devsarfoAfter using VitoDeploy by @saeed_vz for a month, I highly recommend it to all PHP developers, especially those using Laravel. It makes server management and PHP application deployment to production servers simple and hassle-free.Muhammad Shafeeq@hmshafeeqRecently tried VitoDeploy, impressed by its features, performance and versatility. Hats off to you for building such a powerful deployment solution and making it open sourced  :)Wilson Bridgett@_wilsonpbJust discovered #vitodeploy and have become a fan. I also like the change to SQLite! Is the 1.x branch in a good place to give it a beta test run. Thx for your work on this projectJames Kokou GAGLO@dzamsgagloVito is a self-hosted web application that helps you manage your servers and deploy your PHP applications into production servers without a hassle.Bernard Sarfo Twumasi@devsarfoAfter using VitoDeploy by @saeed_vz for a month, I highly recommend it to all PHP developers, especially those using Laravel. It makes server management and PHP application deployment to production servers simple and hassle-free.Muhammad Shafeeq@hmshafeeqRecently tried VitoDeploy, impressed by its features, performance and versatility. Hats off to you for building such a powerful deployment solution and making it open sourced  :)Wilson Bridgett@_wilsonpbJust discovered #vitodeploy and have become a fan. I also like the change to SQLite! Is the 1.x branch in a good place to give it a beta test run. Thx for your work on this projectJames Kokou GAGLO@dzamsgagloVito is a self-hosted web application that helps you manage your servers and deploy your PHP applications into production servers without a hassle.",
    "summary": {
      "en": "**Summary of VitoDeploy:**\n\nVitoDeploy is an open-source server management tool designed for easy management and deployment of PHP applications like Laravel and WordPress. Key features include:\n\n- **Server Management**: Provision and manage servers easily.\n- **Database Support**: Manage databases like MySQL, MariaDB, and PostgreSQL.\n- **Application Deployment**: Deploy PHP applications quickly.\n- **Firewall Management**: Control your server's firewall settings.\n- **SSL Support**: Use custom SSL or Let's Encrypt for security.\n- **Background Workers**: Run background tasks with worker management.\n- **Service Management**: Handle server services efficiently.\n- **SSH Key Deployment**: Easily deploy SSH keys for secure access.\n- **Cron Jobs**: Create and manage scheduled tasks.\n- **Monitoring**: Track resource usage like CPU, memory, and disk space.\n- **Project Management**: Organize multiple projects and invite users.\n\nUsers have praised VitoDeploy for its reliability and ease of use, making it a solid alternative to tools like Laravel Forge. It’s well-received in the developer community, with many recommending it for PHP developers.",
      "ko": "VitoDeploy는 Laravel과 WordPress와 같은 PHP 애플리케이션의 관리와 배포를 쉽게 할 수 있도록 설계된 오픈 소스 서버 관리 도구입니다. 주요 기능으로는 서버 관리, 데이터베이스 지원, 애플리케이션 배포, 방화벽 관리, SSL 지원, 백그라운드 작업 관리, 서비스 관리, SSH 키 배포, 예약 작업 관리, 모니터링, 프로젝트 관리 등이 있습니다.\n\n서버 관리는 간편하게 서버를 설정하고 관리할 수 있도록 도와줍니다. 데이터베이스 지원 기능을 통해 MySQL, MariaDB, PostgreSQL과 같은 데이터베이스를 쉽게 관리할 수 있습니다. PHP 애플리케이션을 신속하게 배포할 수 있으며, 서버의 방화벽 설정도 제어할 수 있습니다. 보안을 위해 사용자 지정 SSL이나 Let's Encrypt를 사용할 수 있습니다.\n\n백그라운드 작업을 관리하여 필요한 작업을 자동으로 실행할 수 있으며, 서버 서비스도 효율적으로 처리할 수 있습니다. SSH 키를 쉽게 배포하여 안전한 접근을 보장하고, 예약 작업을 생성하고 관리할 수 있는 기능도 포함되어 있습니다. 자원 사용량, 즉 CPU, 메모리, 디스크 공간 등을 모니터링할 수 있는 기능도 제공됩니다. 여러 프로젝트를 조직하고 사용자 초대 기능을 통해 협업할 수 있습니다.\n\n사용자들은 VitoDeploy의 신뢰성과 사용의 용이성을 높이 평가하며, Laravel Forge와 같은 도구에 대한 확실한 대안으로 추천하고 있습니다. 개발자 커뮤니티에서도 긍정적인 반응을 얻고 있으며, 많은 PHP 개발자들이 이 도구를 추천하고 있습니다.",
      "ja": "VitoDeployは、LaravelやWordPressなどのPHPアプリケーションの管理と展開を簡単に行うために設計されたオープンソースのサーバー管理ツールです。主な機能には、サーバーのプロビジョニングと管理が容易に行えるサーバー管理、MySQLやMariaDB、PostgreSQLなどのデータベースを管理する機能、PHPアプリケーションを迅速に展開するためのアプリケーション展開機能があります。\n\nさらに、サーバーのファイアウォール設定を制御できるファイアウォール管理、カスタムSSLやLet's Encryptを使用してセキュリティを強化するSSLサポート、バックグラウンドタスクを実行するためのワーカー管理機能も備えています。サーバーサービスを効率的に扱うサービス管理、セキュアなアクセスのためにSSHキーを簡単に展開できるSSHキー展開、スケジュールされたタスクを作成・管理するためのCronジョブ機能もあります。また、CPUやメモリ、ディスクスペースなどのリソース使用状況を監視する機能や、複数のプロジェクトを整理し、ユーザーを招待するためのプロジェクト管理機能も提供されています。\n\nユーザーからは、VitoDeployの信頼性と使いやすさが高く評価されており、Laravel Forgeなどのツールの優れた代替手段として支持されています。開発者コミュニティでも好評で、多くのPHP開発者に推奨されています。"
    }
  },
  {
    "id": "5bd444db4e2afeee",
    "title": {
      "en": "Train and Weather Tracker with Raspberry Pi and E-Ink",
      "ko": "파이로 기차 날씨 추적기",
      "ja": "ラズパイ天気列車"
    },
    "type": "story",
    "url": "https://sambroner.com/posts/raspberry-pi-train",
    "score": 122,
    "by": "tosh",
    "time": 1743272545,
    "content": "Train & Weather Tracker with Raspberry Pi & E-InkMarch 2, 2025Art,Physical,SoftwareI finally built a Raspberry Pi project my wife loves: an e-ink train and weather tracker! If you want to build one yourself, the Github & instructions are here.\nKira will be jogging to the night shift!\nOver the past few years, I've been on something of an e-ink journey.  I started with a weather and news display (still the only post on my website that regularly gets organic traffic.) While I loved it and it looked great, a phone is a better way to check the news, and the weather only gets checked once a day. Then in 2022 while at MIT I built Jarvis, the e-ink voice-to-image display. Jarvis was a great party trick — say, \"Hey Jarvis, paint me an elephant on a bowling ball in Times Square\" and watch as the image gradually appeared. Notably, this was before ChatGPT, when people were still impressed by AI!\nJarvis live demo is better!\nThen over Thanksgiving, I had some free time, a basket of spare parts, and the itch to code and build something physical. So here we are with the e-ink train and weather tracker.\nThe idea is simple: Every morning, my wife and I take the inbound F or G subway lines to work from a stop that's a 2-minute jog or a 6-minute walk from our house. I love the NYC subway and it works incredibly well, but the trains come often and predictably, not on-a-schedule. So every morning we're calling out, when's the next G, when's the next F — and one of us pulls out a phone for the MTA app or Google Maps and yells back the upcoming train times. Then we time our morning routines to either stretch for a train in three minutes or slow down for a train in ten.\nThe subway and weather tracker makes checking train times much faster and calmer than pulling out a phone. Since it's centrally located in our home, someone is always close enough to glance at it.\nThe subway times reported by the MTA API are reliable once the train is within a few stops or 15 minutes away, with precision increasing as the trains get closer. Having the train times on the wall lets you check how your morning routine is developing against the train schedule as the departure time gets closer.\nBest of all, Kira loves it! This goes on my DIY pantheon along with adding legs to her dresser to make it a better height and fixing a towel rack right before she hosted her colleagues for a book club.\nIf you want to build one yourself or better understand the project, I'll dive into some editorial and gotchas below. The practical instructions, parts, and code live in this GitHub Repo.\nDescription & Features\nThe train & weather tracker is built on a 9.7\" 1200x825 E-Ink display attached to a Raspberry Pi 4b. The display is split into four parts: a header, with date, time, and a live second hand to indicate liveness; a train tracker; the commute weather; and a \"weather bar\" displaying the next 12 hours of weather. The main focus of the layout is the train tracker, which shows the upcoming 30 minutes of inbound F & G trains.\n\nThe display sits in a laser cut mat board with black ridges sized to hide the black border of the e-ink display. The mat board sits in a 8.5\" x 11\" cherry frame that is 1.5\" deep to allow the raspberry pi to be backmounted while the frame is still flush to the wall. We hang the display next to the door above a key holder, which is generally the right place for it, but also helps hide the power cord.\nNot pretty, but it works\nProject Details\nThe software portion of the project is manageable although there are some gotchas when programming against the Waveshare e-ink hat in particular. We have a modular architecture with a clear separation of concerns:\n\nDisplay Engine: Renders the layout to the display, supporting communication with the physical e-ink display or rendering a png for non-raspberry pi development\n\nLayout System: A pretty hacky visual arrangement of the elements. It'd be fun to do something more sophisticated here (e.g. a html renderer), but 🤷‍♂️. This will likely require the most work if you want to modify the project\n\nData Services: Fetch & process (this is important!) train arrival from NYC Transit GTFS feeds + get the weather forecasts\n\nApplication Controller: Orchestrate the event loop and subscription model to drive updates to the display\n\nThe biggest technical constraint is the update rate to the e-ink display. The display supports multiple update modes with different tradeoffs and can manage sub 500 ms updates, especially for partial updates, but a faster refresh results in fuzzy characters and substantial ghosting that looks pretty bad. The bigger issue is that overloading the display causes it to crash, which requires a full reset.\nAfter some testing, I chose a hybrid display update strategy.\nEvery second, the pixels around the seconds & minutes digits of the time are rerendered with the display's fastest partial update mode to make it clear that the display is functioning as expected.\nWhen there is an update to one of the next two train times (these matter more than trains 30 minutes away), there is a fast-full display render — all data is updated at this point, but only new arrival times for one of the next two trains will drive the update.\nOn the hour, the display does a deep full-screen render to sharpen the characters and remove ghosting.\nHonestly, you gotta experiment to understand how these will look\nDoing deep full screen renders can be jarring — it results in a black and white flash that attracts too much attention to the screen — but once an hour is a fine cadence. For the train times, a fast-full display render works better than a partial refresh because of the layout. The fast partial render cuts across elements, drawing more attention to the refresh than a full refresh. This could maybe be improved through more purposeful partial refresh rectangle selection, but the current solution works well.\nThe framing was tricky because I wanted something nice, but without breaking the bank. I'm not great at woodworking and I also didn't want to spend $300-$500, which was the range of quotes (FWIW, I have been trying to find a low cost framing solution for these broken glass pieces for years — let me know if you have any ideas). The e-ink display is awkwardly sized due to the connection cables and drivers. The display surface also has a fine looking, but inelegant border that should be covered. After talking to a few framers, one of them suggested I buy just a custom mat. The mat hides the display, Raspberry Pi, and cabling, but more importantly the outside of the mat is right sized to fit into a standard-sized frame.\nThe finished product sits neatly beside our door, providing the exact information we need at just the right time each morning. I've written before about the \"agency gap\" — the gap between what people do and what would be easy, but useful to do. This project is a bit more involved than that, but at ~20 hours of work, it was worth it to me. I built something that solves a daily need, we actually use it, guests talk about it, and it looks great.HomeNext →587 Miles, 803 Meetings, and 14 Dates: My completely sane system for personal analytics\n    Comments hosted by TwitterClick for discussion",
    "summary": {
      "en": "The author created a Raspberry Pi project that serves as an e-ink train and weather tracker, which his wife loves. This device displays train times and weather information in their home, making it easier for them to check train schedules without using their phones. The tracker shows the next 30 minutes of inbound F and G subway trains, along with current weather and forecasts for the next 12 hours.\n\nThe project uses a 9.7\" e-ink display and is housed in a custom frame. The display updates train arrival times and weather regularly, using a hybrid update strategy to manage refresh rates and avoid display issues. \n\nThe author shares details about the construction and programming of the device, noting that while the project took about 20 hours, it effectively meets their daily needs and is a conversation starter for guests. Instructions and materials for building a similar device are available on GitHub.",
      "ko": "저자는 아내가 좋아하는 라즈베리 파이 프로젝트를 만들었습니다. 이 프로젝트는 전자 잉크로 기차와 날씨를 추적하는 장치입니다. 이 장치는 집안에서 기차 시간과 날씨 정보를 보여주어, 스마트폰을 사용하지 않고도 기차 시간표를 쉽게 확인할 수 있게 해줍니다. 이 추적기는 F선과 G선 지하철의 다음 30분 도착 시간과 현재 날씨, 향후 12시간의 일기 예보를 표시합니다.\n\n이 프로젝트는 9.7인치 전자 잉크 디스플레이를 사용하며, 맞춤형 프레임에 장착되어 있습니다. 디스플레이는 기차 도착 시간과 날씨 정보를 정기적으로 업데이트하며, 하이브리드 업데이트 방식을 사용해 새로 고침 속도를 관리하고 디스플레이 문제를 피합니다.\n\n저자는 이 장치의 제작과 프로그래밍에 대한 세부 정보를 공유하며, 프로젝트에 약 20시간이 소요되었지만 일상적인 필요를 효과적으로 충족시키고 손님들과의 대화 주제가 된다고 언급합니다. 비슷한 장치를 만드는 방법과 필요한 재료는 GitHub에서 확인할 수 있습니다.",
      "ja": "著者は、妻が気に入っているRaspberry Piを使ったプロジェクトを作成しました。このプロジェクトは、電子インクを用いた電車と天気のトラッカーです。このデバイスは、家の中で電車の時刻や天気情報を表示し、スマートフォンを使わずに電車のスケジュールを確認できるようにしています。トラッカーは、F線とG線の地下鉄の次の30分間の到着情報と、現在の天気や今後12時間の予報を表示します。\n\nプロジェクトでは、9.7インチの電子インクディスプレイを使用し、特注のフレームに収められています。ディスプレイは、電車の到着時刻や天気を定期的に更新し、更新戦略を工夫することでリフレッシュレートを管理し、表示の問題を避けています。\n\n著者は、このデバイスの構築やプログラミングの詳細を共有しており、プロジェクトには約20時間かかったものの、日常のニーズを満たし、ゲストとの会話のきっかけにもなっていると述べています。類似のデバイスを作るための手順や材料は、GitHubで入手可能です。"
    }
  },
  {
    "id": "4dd89b42ee81dc3f",
    "title": {
      "en": "Show HN: Await-Tree – Visualize Async Rust Task Execution in Real-Time",
      "ko": "비주얼 러스트: Await-Tree",
      "ja": "待機ツリーで可視化！"
    },
    "type": "story",
    "url": "https://github.com/risingwavelabs/await-tree",
    "score": 18,
    "by": "Sheldon_fun",
    "time": 1743583583,
    "content": "await-tree\n\nThe Futures in Async Rust can be arbitrarily composited or nested to achieve a variety of control flows.\nAssuming that the execution of each Future is represented as a node,\nthen the asynchronous execution of an async task can be organized into a logical tree,\nwhich is constantly transformed over the polling, completion, and cancellation of Futures.\nawait-tree allows developers to dump this execution tree at runtime, with the span of each Future annotated by instrument_await. A basic example is shown below, and more examples of complex control flows can be found in the examples directory.\nasync fn bar(i: i32) {\n    // `&'static str` span\n    baz(i).instrument_await(\"baz in bar\").await\n}\n\nasync fn baz(i: i32) {\n    // runtime `String` span is also supported\n    work().instrument_await(span!(\"working in baz {i}\")).await\n}\n\nasync fn foo() {\n    // spans of joined futures will be siblings in the tree\n    join(\n        bar(3).instrument_await(\"bar\"),\n        baz(2).instrument_await(\"baz\"),\n    )\n    .await;\n}\n\n// Init the global registry to start tracing the tasks.\nawait_tree::init_global_registry(Default::default());\n// Spawn a task with root span \"foo\" and key \"foo\".\n// Note: The `spawn` function requires the `tokio` feature to be enabled.\nawait_tree::spawn(\"foo\", \"foo\", foo());\n// Let the tasks run for a while.\nsleep(Duration::from_secs(1)).await;\n// Get the tree of the task with key \"foo\".\nlet tree = Registry::current().get(\"foo\").unwrap();\n\n// foo [1.006s]\n//   bar [1.006s]\n//     baz in bar [1.006s]\n//       working in baz 3 [1.006s]\n//   baz [1.006s]\n//     working in baz 2 [1.006s]\nprintln!(\"{tree}\");\n\nFeatures\nawait-tree provides the following optional features:\n\nserde: Enables serialization of the tree structure using serde. This allows you to serialize the tree to formats like JSON, as shown in the serde example.\n// Enable the serde feature in Cargo.toml\n// await-tree = { version = \"<version>\", features = [\"serde\"] }\n\n// Then you can serialize the tree\nlet tree = Registry::current().get(\"foo\").unwrap();\nlet json = serde_json::to_string_pretty(&tree).unwrap();\nprintln!(\"{json}\");\n\ntokio: Enables integration with the Tokio runtime, providing task spawning capabilities through spawn and spawn_anonymous functions. This feature is required for the examples that demonstrate spawning tasks.\n// Enable the tokio feature in Cargo.toml\n// await-tree = { version = \"<version>\", features = [\"tokio\"] }\n\n// Then you can spawn tasks with await-tree instrumentation\nawait_tree::spawn(\"task-key\", \"root_span\", async {\n    // Your async code here\n    work().instrument_await(\"work_span\").await;\n});\n\nCompared to async-backtrace\ntokio-rs/async-backtrace is a similar crate that also provides the ability to dump the execution tree of async tasks. Here are some differences between await-tree and async-backtrace:\nPros of await-tree:\n\nawait-tree support customizing the span with runtime String, while async-backtrace only supports function name and line number.\nThis is useful when we want to annotate the span with some dynamic information, such as the identifier of a shared resource (e.g., a lock), to see how the contention happens among different tasks.\n\nawait-tree support almost all kinds of async control flows with arbitrary Future topology, while async-backtrace fails to handle some of them.\nFor example, it's common to use &mut impl Future as an arm of select to avoid problems led by cancellation unsafety. To further resolve this Future after the select completes, we may move it to another place and await it there. async-backtrace fails to track this Future again due to the change of its parent. See examples/detach.rs for more details.\n\nawait-tree maintains the tree structure with an arena-based data structure, with zero extra unsafe code. For comparison, async-backtrace crafts it by hand and there's potential memory unsafety for unhandled topologies mentioned above.\nIt's worth pointing out that await-tree has been applied in the production deployment of RisingWave, a distributed streaming database, for a long time.\n\nawait-tree maintains the tree structure separately from the Future itself, which enables developers to dump the tree at any time with nearly no contention, no matter the Future is under active polling or has been pending. For comparison, async-backtrace has to wait for the polling to complete before dumping the tree, which may cause a long delay.\n\nPros of async-backtrace:\n\nasync-backtrace is under the Tokio organization.\n\nLicense\nawait-tree is distributed under the Apache License (Version 2.0). Please refer to LICENSE for more information.",
    "summary": {
      "en": "**Summary of await-tree**\n\nThe `await-tree` library in Rust helps developers manage asynchronous tasks by organizing their execution into a tree structure. Each task, represented as a node, can be composed or nested, allowing for complex control flows. Here are the key points:\n\n- **Execution Tree**: As tasks run, `await-tree` tracks their execution in a tree format, which can be visualized at runtime using the `instrument_await` feature.\n\n- **Basic Example**: The library provides examples where tasks like `bar`, `baz`, and `foo` are defined, showcasing how they can be instrumented to track their execution spans.\n\n- **Features**:\n  - **Serde**: Allows serialization of the execution tree into formats like JSON.\n  - **Tokio**: Integrates with the Tokio runtime for spawning tasks; necessary for certain examples.\n\n- **Comparison with async-backtrace**:\n  - **Pros of await-tree**:\n    - Supports dynamic span customization.\n    - Handles all async control flows without issues.\n    - Maintains tree structure with safe memory management.\n    - Allows tree dumping at any time without waiting for task completion.\n  - **Pros of async-backtrace**:\n    - Backed by the Tokio organization.\n\n- **License**: The library is licensed under the Apache License (Version 2.0).\n\nIn summary, `await-tree` provides robust tools for tracing and managing async tasks in Rust, with features that enhance its usability and safety compared to similar libraries.",
      "ko": "`await-tree` 라이브러리는 Rust에서 비동기 작업을 관리하는 데 도움을 주며, 작업의 실행을 트리 구조로 조직합니다. 각 작업은 노드로 표현되며, 복잡한 제어 흐름을 위해 구성되거나 중첩될 수 있습니다. \n\n이 라이브러리는 작업이 실행될 때 그 과정을 트리 형식으로 추적하며, `instrument_await` 기능을 사용해 실행 중에 시각화할 수 있습니다. 기본 예제로는 `bar`, `baz`, `foo`와 같은 작업이 정의되어 있으며, 이들을 통해 실행 범위를 추적하는 방법을 보여줍니다.\n\n주요 기능으로는 실행 트리를 JSON과 같은 형식으로 직렬화할 수 있는 `Serde`와, 작업을 생성하기 위해 Tokio 런타임과 통합되는 기능이 있습니다. 이는 특정 예제에 필요합니다.\n\n`await-tree`와 `async-backtrace`를 비교해보면, `await-tree`의 장점으로는 동적인 범위 사용자 정의를 지원하고, 모든 비동기 제어 흐름을 문제없이 처리하며, 안전한 메모리 관리를 통해 트리 구조를 유지합니다. 또한 작업 완료를 기다리지 않고 언제든지 트리를 덤프할 수 있습니다. 반면, `async-backtrace`는 Tokio 조직의 지원을 받는 장점이 있습니다.\n\n이 라이브러리는 Apache 라이선스(버전 2.0) 하에 배포됩니다. `await-tree`는 Rust에서 비동기 작업을 추적하고 관리하는 데 강력한 도구를 제공하며, 유사한 라이브러리들에 비해 사용성과 안전성을 높이는 다양한 기능을 갖추고 있습니다.",
      "ja": "`await-tree`ライブラリは、Rustにおいて非同期タスクを管理するためのツールで、タスクの実行を木構造で整理します。各タスクはノードとして表現され、複雑な制御フローを実現するために組み合わせたり、入れ子にしたりできます。\n\n実行中のタスクは、`await-tree`によって木の形式で追跡され、`instrument_await`機能を使用することで、実行状況をリアルタイムで可視化できます。ライブラリには、`bar`、`baz`、`foo`といったタスクの例が含まれており、これらのタスクがどのように実行時間を追跡できるかを示しています。\n\n主な機能には、実行木をJSONなどの形式にシリアライズできる`Serde`や、タスクを生成するために必要なTokioランタイムとの統合があります。これにより、特定の例を実行する際に便利です。\n\n`await-tree`と`async-backtrace`を比較すると、`await-tree`の利点としては、動的なスパンのカスタマイズが可能で、すべての非同期制御フローを問題なく処理し、安全なメモリ管理を維持しながら木構造を保持できる点があります。また、タスクの完了を待たずにいつでも木の内容をダンプできるのも特徴です。一方、`async-backtrace`はTokio組織によってサポートされています。\n\nこのライブラリはApacheライセンス（バージョン2.0）のもとで提供されています。`await-tree`は、Rustにおける非同期タスクの追跡と管理のための強力なツールを提供し、同様のライブラリと比較して使いやすさと安全性を向上させています。"
    }
  },
  {
    "id": "6d40473b74cd2445",
    "title": {
      "en": "Glubux's Powerwall (2016)",
      "ko": "글루벅스 파워월",
      "ja": "グルバックスの電力壁"
    },
    "type": "story",
    "url": "https://secondlifestorage.com/index.php?threads/glubuxs-powerwall.126/",
    "score": 380,
    "by": "bentobean",
    "time": 1743522541,
    "content": "Glubux\n\t\t\tMember\n\n\t\t\t\t\t\t\tJoined\n\t\t\t\t\t\t\tNov 9, 2016\n\n\t\t\t\t\t\t\tMessages\n\t\t\t\t\t\t\t124\n\n\t\t\t\t\tNov 9, 2016\n\n\t\t\t\t\t\t#1\n\n\t\t\t\t\tA quick presentation of my project.\n\nI already produce a part of the electricity that I need. I have 1.4 kw of solar panel on the roof, an old 24v 460AH forklift battery, a Victron MPPT 100/50 charge controler, an 24v to 12v Victron voltage lowerer, and a 3KVA Victron inverter.\n\nThe plan is to build a shed to put all the batteries and the charge controler/inverters.\n\nI started to collect laptop batteries a few months ago, I have around 650 for now, I started sorting and making my packs:\n\nIdeas are :\n\n-100 Ah for each pack by matching the number of cells, possibilty to add or remove cells if needed\n-maximum use of copper wire, easy to find and easy to solder\n-same bus bar lenght for each cell\n\nReactions:\ntytower, jamesk9, JustMeTMC and 5 others",
    "summary": {
      "en": "Glubux, a member since November 9, 2016, shared details about their electricity project. They currently generate some of their own electricity using 1.4 kW of solar panels, an old forklift battery, and a Victron charge controller and inverter. Glubux plans to build a shed for the batteries and equipment. \n\nThey have also started collecting laptop batteries, currently having around 650. Their ideas for the battery packs include:\n\n- Creating 100 Ah packs by matching cells.\n- Using copper wire for easy soldering.\n- Keeping the bus bar length the same for each cell.\n\nThe post received reactions from several other members.",
      "ko": "Glubux는 2016년 11월 9일부터 활동해온 회원으로, 전기 프로젝트에 대한 자세한 내용을 공유했습니다. 현재 그들은 1.4 kW의 태양광 패널, 오래된 지게차 배터리, 그리고 Victron 충전 컨트롤러와 인버터를 사용하여 일부 전기를 자체 생산하고 있습니다. Glubux는 배터리와 장비를 위한 창고를 건설할 계획입니다.\n\n또한, 그들은 현재 약 650개의 노트북 배터리를 수집하기 시작했습니다. 배터리 팩에 대한 아이디어는 다음과 같습니다. \n\n100 Ah 팩을 만들기 위해 셀을 조합하고, 쉽게 납땜할 수 있도록 구리선을 사용할 계획입니다. 각 셀의 버스바 길이를 동일하게 유지하는 것도 고려하고 있습니다.\n\n이 게시물은 여러 다른 회원들로부터 반응을 얻었습니다.",
      "ja": "Glubuxは2016年11月9日からのメンバーで、電力プロジェクトについての詳細を共有しました。現在、1.4 kWの太陽光パネルと古いフォークリフトのバッテリー、Victronの充電コントローラーとインバーターを使って、自家発電を行っています。Glubuxはバッテリーや機器を収納するための小屋を建設する計画を立てています。\n\nまた、彼らはノートパソコンのバッテリーを集め始めており、現在約650個を保有しています。バッテリーパックに関するアイデアには、セルを組み合わせて100 Ahのパックを作ること、簡単にハンダ付けできるように銅線を使用すること、各セルのバスバーの長さを同じに保つことが含まれています。\n\nこの投稿には、他のメンバーからいくつかの反応が寄せられました。"
    }
  },
  {
    "id": "6fe4d551a9fd921d",
    "title": {
      "en": "Generate autounattend.xml files for Windows 10/11",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://schneegans.de/windows/unattend-generator/",
    "score": 206,
    "by": "nixass",
    "time": 1743281081,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "645d06c67daf6ae6",
    "title": {
      "en": "Excitable cells",
      "ko": "흥미진진한 세포들",
      "ja": "興奮する細胞"
    },
    "type": "story",
    "url": "https://jenevoldsen.com/posts/excitable-cells/",
    "score": 299,
    "by": "johannes_ne",
    "time": 1743167152,
    "content": "Pacemaker cells\n\nSinus rhythm. From ECGpedia.\n\nAnother interesting property of cardiomyocytes is their automaticity; cardiomyocytes spontaneously activate if they are not stimulated by a neighboring cells for a period. Cells in the sinoatrial node (sinus node) in the right atrium have the fastest rate of spontaneous activation, and thus activate the remaining heart, making the sinus node the heart’s pacemaker.\nIn the following simulation, a cluster of cells in the top left corner act as pacemaker cells.\n\n👆️ Move the slider below the simulation to change the rate of spontaneous depolarization (heart rate).\n\nWhen the spontaneous depolarization of the sinus node determines the heart rate, the heart is said to be in sinus rhythm, which is the normal (physiological) activation path of the heart. If the sinus node paces the heart at a rate faster than 100 beats per minute, the rhythm is called sinus tachycardia. Sinus tachycardia (together with increased stroke volume) increases the flow of blood from the heart, and is the physiological response to exercise.\n\nReentry tachycardia\n\nVentricular tachycardia. From Ecgpedia.\n\nIn reentry tachycardia, the heart is not paced by the sinus node, but instead from a group of cells that have formed a circuit, where a wave of depolarization can loop around and stimulate itself over and over. This requires a non-responsive area among the cells (an area that cannot be activated), so a circuit can form around it.\nIn the simulation below, the red cells are non-responsive (dead). They could represent scar tissue after a myocardial infarction—a common cause of ventricular tachycardia. Or the simulation could illustrate atrial flutter, where a reentrant loop can form around the tricuspid valve.\n\n👆️ To stop the reentrant tachycardia, shock the cardiac cells by pressing the “⚡️ Defibrillate!” button.\n\n ⚡️ Defibrillate!\n\nAfter defibrillation, the system is in sinus rhythm. The pacemaker cells were always there, but were suppressed by the reentrant loop.\nThe simulation above illustrates how a reentrant loop can sustain itself around a dead area, but how does it start in the first place?\n\n👆️ You can try to click/tap on the simulation above to set of an ectopic beat (a depolarization starting outside the sinus node), but you will not be able to initiate a reentrant loop again.\n\nHow does a reentrant loop start?\nTo initiate a reentrant loop, the wave of depolarization must be traveling only one way around the dead area. Otherwise the two waves traveling in opposite will just eliminate each other when they meet at the other side. However, if one pathway has cells with a longer refractory period, a wave of depolarization can arrive exactly when one pathway is ready to depolarize, while the other is still refractory.\nIn the simulation below, cells in the lower left corner have a longer refractory period.\n\n👆️ Try initiating an ectopic beat (click/tap) next to the area with a longer refractory period, while the area is still refractory.\n\n ⚡️ Defibrillate!\n\n👆️ You can get the system back in sinus rhythm either by ⚡️ defibrillation, or by cleverly timing a new ectopic beat to block the reentrant loop.\n\nHow does a reentrant loop start?\nTo initiate a reentrant loop, the wave of depolarization must be traveling only one way around the dead area. Otherwise the two waves traveling in opposite will just eliminate each other when they meet at the other side. However, if one pathway has cells with a longer refractory period, a wave of depolarization can arrive exactly when one pathway is ready to depolarize, while the other is still refractory.\nIn the simulation below, cells in the lower left corner have a longer refractory period.\n\n👆️ Try initiating an ectopic beat (click/tap) next to the area with a longer refractory period, while the area is still refractory.\n\n ⚡️ Defibrillate!\n\n👆️ You can get the system back in sinus rhythm either by ⚡️ defibrillation, or by cleverly timing a new ectopic beat to block the reentrant loop.\n\nFibrillation\n\nVentricular fibrillation. From Ecgpedia.\n\nAnother common cardiac arrythmia is fibrillation—atrial fibrillation or ventricular fibrillation. Fibrillation is a reentrant arrhythmia, but the wave of depolarization does not propagate around a fixed anatomical area, but rather meanders irregularly through the myocardium.\nIn the simulation below, cells have different refractory times. An ectopic depolarization will activate some cells, while others are still refractory. This can create a quite complex pattern of depolarization, and may create a reentrant loop, without any dead area to loop around.\n\n👆️ Try setting of a few ectopic beats in the simulation below, and see if you can initiate one or more reentrant loops.\nMake the system more unstable by changing the scale (with high scale, nearby cells have similar refractory times) and range (overall range of refractory times in the system). High range and low scale makes the system unstable.\n\nSet variability in refractory time.\nScale:  Range:  ⚡️ Defibrillate!\n\nStimulating the system while some cells are refractory, while others are not, corresponds to an ectopic beat occurring during the T-wave of an ECG—a high-risk situation for ventricular fibrillation. Also, a high range of refractory times, corresponds to a wide T-wave in the ECG.\n\nAcknowledgment\nThis article is inspired by Bartosz Ciechanowski’s amazing interactive articles.",
    "summary": {
      "en": "**Summary: Pacemaker Cells and Heart Rhythms**\n\n- **Pacemaker Cells**: The sinoatrial node in the right atrium is the heart's natural pacemaker, controlling the heart rate through spontaneous activation. When the heart beats normally, it's called sinus rhythm. If the heart rate exceeds 100 beats per minute, it's known as sinus tachycardia, which often occurs during exercise.\n\n- **Reentry Tachycardia**: This condition happens when the heart is paced by a circuit of cells instead of the sinus node. A non-responsive area (like scar tissue) allows the electrical wave to loop and continuously stimulate the heart. Defibrillation can restore normal rhythm by stopping this loop.\n\n- **Starting a Reentrant Loop**: For a reentrant loop to begin, the wave must travel one way around a dead area. If one pathway has a longer refractory period, it can create a situation where the wave arrives at the right moment to continue the loop.\n\n- **Fibrillation**: This is a more chaotic form of arrhythmia, where electrical signals meander irregularly through the heart muscle, leading to ineffective contractions. It can occur in both atrial and ventricular forms and is often triggered by varying refractory periods among cells.\n\n- **Simulation Tools**: The text discusses simulations that allow users to visualize and manipulate heart rhythms, including initiating ectopic beats and defibrillation to restore normal sinus rhythm.\n\nThis overview highlights the key concepts of how pacemaker cells function and the mechanisms behind different heart rhythms and arrhythmias.",
      "ko": "심박조율세포와 심장 리듬에 대한 요약입니다.\n\n심박조율세포는 심장의 자연적인 박동 조절 장치인 동방결절로, 우심방에 위치해 있습니다. 이 세포는 자발적으로 활성화되어 심박수를 조절합니다. 심장이 정상적으로 뛰면 이를 동성 리듬이라고 부릅니다. 심박수가 분당 100회를 초과하면 동성 빈맥이라고 하며, 이는 주로 운동 중에 발생합니다.\n\n재진입 빈맥은 심장이 동방결절이 아닌 세포의 회로에 의해 박동할 때 발생하는 상태입니다. 비반응 영역(예: 흉터 조직)이 전기 신호가 순환하도록 허용하여 심장을 지속적으로 자극합니다. 제세동은 이 순환을 중단시켜 정상 리듬을 회복할 수 있습니다.\n\n재진입 루프가 시작되려면 전기 신호가 죽은 영역을 한 방향으로 지나가야 합니다. 만약 한 경로의 불응기가 더 길다면, 신호가 적절한 시점에 도착하여 루프를 계속할 수 있는 상황이 만들어질 수 있습니다.\n\n세동은 더 혼란스러운 형태의 부정맥으로, 전기 신호가 심장 근육을 통해 불규칙하게 흐르면서 비효율적인 수축을 초래합니다. 세동은 심방과 심실 모두에서 발생할 수 있으며, 세포 간의 다양한 불응기에 의해 유발되는 경우가 많습니다.\n\n이 텍스트에서는 심장 리듬을 시각화하고 조작할 수 있는 시뮬레이션 도구에 대해 설명하고 있습니다. 여기에는 이소성 박동을 시작하고 정상 동성 리듬을 회복하기 위한 제세동이 포함됩니다.\n\n이 개요는 심박조율세포의 기능과 다양한 심장 리듬 및 부정맥의 메커니즘에 대한 핵심 개념을 강조합니다.",
      "ja": "心臓のペースメーカー細胞とリズムについての要約です。\n\nペースメーカー細胞は、右心房にある洞房結節で、心拍数を自発的に調整する役割を果たしています。心臓が正常に拍動する際は、これを洞調律と呼びます。心拍数が1分間に100回を超えると、洞性頻脈と呼ばれ、運動中によく見られます。\n\n再入性頻脈は、心臓が洞房結節ではなく、細胞の回路によってペースされる状態です。反応しない部分（例：瘢痕組織）があると、電気信号がループし続け、心臓を刺激し続けます。このループを止めることで正常なリズムを取り戻すことができるのが除細動です。\n\n再入性ループが始まるためには、電気信号が死んだ部分を一方向に回る必要があります。もし一つの経路が長い不応期を持っていると、信号がちょうど良いタイミングで到達し、ループを続ける状況が生まれます。\n\n心房細動は、より混沌とした不整脈の一種で、電気信号が心筋内を不規則に流れ、効果的な収縮を妨げます。心房と心室の両方で発生する可能性があり、細胞間の不応期の違いによって引き起こされることが多いです。\n\nシミュレーションツールについても触れられており、ユーザーが心拍リズムを視覚化し、操作できる機能があります。これには異所性拍動の開始や、正常な洞調律を取り戻すための除細動が含まれます。\n\nこの概要は、ペースメーカー細胞の機能や、さまざまな心拍リズムや不整脈のメカニズムについての重要な概念を示しています。"
    }
  },
  {
    "id": "eef7dcf4d8bb143d",
    "title": {
      "en": "AI image recognition detects bubble-like structures in the universe",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://phys.org/news/2025-03-ai-image-recognition-universe.html",
    "score": 103,
    "by": "PaulHoule",
    "time": 1743430875,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "1ba2aeb9d640062e",
    "title": {
      "en": "Launch HN: ASim (YC S21) – Mobile app that generates mobile apps",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": null,
    "score": 81,
    "by": "dli123",
    "time": 1743519971,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "0a4cda998eacdda7",
    "title": {
      "en": "Show HN: Zig Topological Sort Library for Parallel Processing",
      "ko": "병렬 처리 위한 Zig 정렬 라이브러리",
      "ja": "Zig並列ソートライブラリ"
    },
    "type": "story",
    "url": "https://github.com/williamw520/toposort",
    "score": 108,
    "by": "ww520",
    "time": 1743529725,
    "content": "TopoSort - Topological Sort on Dependency Graph\nTopoSort is a highly efficient Zig library for performing topological sort on dependency graph.  This small library is packed with the following features:\n\nBuilding dependency graph from dependency data.\nPerforming topological sort on the dependency graph.\nGenerating dependence-free subsets for parallel processing.\nCycle detection and cycle reporting.\nSupport different node types.\n\nContent\n\nInstallation\nUsage\n\nMemory Ownership\nConfiguration\nMore Usage\n\nCLI Tool\nBenchmarks\nLicense\n\nInstallation\nGo to the Releases page.\nPick a release to add to your project.\nIdentify the file asset URL for the release version.\nE.g. https://github.com/williamw520/toposort/archive/refs/tags/1.0.2.tar.gz\nUse zig fetch to add the TopoSort package to your Zig project.\nRun the following command to fetch the TopoSort package:\n  zig fetch --save https://github.com/williamw520/toposort/archive/refs/tags/<VERSION>.tar.gz\n\nzig fetch updates your build.zig.zon file with the URL with file hash added in the .dependency section of the file.\n.{\n    .name = \"my-project\",\n    ...\n    .dependencies = .{\n+       .toposort = .{\n+           .url = \"zig fetch https://github.com/williamw520/toposort/archive/refs/tags/<VERSION>.tar.gz\",\n+           .hash = \"toposort-...\",\n+       },\n    },\n}\n\nUpdate your build.zig with the lines for toposort.\n  pub fn build(b: *std.Build) void {\n      ...\n+     const opts = .{ .target = target, .optimize = optimize };\n+     const toposort_module = b.dependency(\"toposort\", opts).module(\"toposort\");\n      ...\n      const exe = b.addExecutable(.{\n          .name = \"my_project\",\n          .root_module = exe_mod,\n      });\n+     exe.root_module.addImport(\"toposort\", toposort_module);\n\nThe .addImport(\"toposort\") call let you import the module into your Zig source files.\nconst toposort = @import(\"toposort\");\n\nUsage\nUsage typically follows the following steps in your Zig source file.\nImport\nconst toposort = @import(\"toposort\");\nconst TopoSort = toposort.TopoSort;\nconst SortResult = toposort.SortResult;\n\nInitialization and memory management.\n    const T = usize;  // node data type\n    var tsort = try TopoSort(T).init(allocator, .{});\n    defer tsort.deinit();\n\nThe data type of the node value is provided as a comptime type to TopoSort(T).\nAdding dependency data.\n    try tsort.add(101, 102);    // node 102 depends on the leading node 101\n    try tsort.add(102, 103);\n    try tsort.add(101, 104);\n\nPerforming the topological sort\n    const result = try tsort.sort();\n\nChecking for cycles\n    if (result.has_cycle()) {\n        for (result.get_cycle_set().items) |id| {\n            const cyclic_node = result.get_node(id);\n            ...\n        }\n    }\n\nOtherwise, process the sorted non-cyclic result\n    const sets: ArrayList(ArrayList(T)) = result.get_sorted_sets();\n    for (sets.items) |subset| {     // the node sets are in topological order\n        for (subset.items) |node| { // nodes within each set are dependence free from each other.\n            ...\n        }\n    }\n\nTopoSort figures out the nodes that have no dependence with each other\nin the linear order of the topological sequence and groups them together as subsets.\nThis allows you to run/process the nodes of each subset in parallel.\nThe subsets themselves are in topological order. If there's no need for\nparallel processing, the nodes in each subset can be processed sequentially,\nwhich fit in the overall topological order of all the nodes.\nMemory Ownership\nNodes are passed in by value in add() and are stored by value in the TopoSort's Data struct.\nFor simple type like integer (e.g. u16, u32), the node values are simply copied.\nFor slice and pointer node type (e.g. []const u8), the memory for the nodes\nare not duplicated. Memory is owned and managed by the caller.\nConfiguration\nThe Toposort.init() function takes in optional configurations. E.g.\n    const T = usize;  // node data type\n    var tsort = try TopoSort(T).init(allocator, .{\n        verbose = true,\n        max_range = 4000,\n    });\n\nSetting the verbose flag prints internal messages while sorting.\nThe max_range property sets the maximum value of the node item value.\nE.g. For node values ranging from 1, 2, 3, 20, 75, ... 100, 100 is the\nmaximum value. If all your node values are positive integers,\npassing in a number type (u16, u32, u64, etc) for the node data type and\nsetting the max_range let TopoSort use a simpler data structure with\nfaster performance.  Building a dependency tree can be more than 3X or 4X faster.\nCompare the 3rd benchmark and 4th benchmark in tests.zig.\nMore Usage\nTo use a slice/string for the node type,\n    const T = []const u8;\n    var tsort = try TopoSort(T).init(allocator, .{});\n\nTo get a list of topologically sorted nodes.\n    const T = []const u8;\n    var list = ArrayList(T).init(allocator);    // list to hold the returned nodes.\n    defer list.deinit();\n    for ((try result.get_sorted_list(&list)).items) |node| {\n        ...\n    }\n\nTo add dependency similar to the Makefile rule format,\nAdd the dependent node A to the leading B node.  A: B\nAdd the dependent node B to the leading C node.  B: C\nAdd the dependent node B to a list of leading nodes.  B: E F G\n    const T = []const u8;\n    var tsort = try TopoSort(T).init(allocator, .{});\n    try tsort.add_dep(\"A\", \"B\");    // A: B\n    try tsort.add_dep(\"B\", \"C\");    // B: C\n    try tsort.add_dep(\"B\", \"D\");    // B: D\n    try tsort.add_deps(\"B\", &[_]T{ \"E\", \"F\", \"G\" });    // B: E F G\n\n    var nodes = ArrayList(T).init(allocator);\n    try nodes.append(\"E\");\n    try nodes.append(\"F\");\n    try nodes.append(\"G\");\n    try tsort.add_deps(\"B\", nodes.items);\n\nTo add a graph in one shot in text string,\n    var tsort = try TopoSort([]const u8).init(allocator, .{});\n    try tsort.add_graph(\"(a b) (a c) (d) (c e f g)\");\n\nThe format of the graph data is a series of \"(dep lead)\" rules in a string.\nIn the example above, a depends on b, a depends on c, d depends on none,\nand c depends on e, f, and g.\nThis can be called multiple times with different parts of the graphs to build the whole thing.\nTo traverse the list of nodes in the graph,\n    for (result.get_nodes().items) |node| {\n        ...\n    }\n\nTo traverse the dependency graph recursively,\n    const T = usize;  // node data type\n    var tsort = try TopoSort(T).init(allocator, .{});\n    ...\n    const result = try tsort.sort();\n    visit_tree(result, null, result.get_root_set());\n\n    fn visit_tree(result: SortResult(T), lead_id: ?u32, dependent_ids: ArrayList(u32)) {\n        if (lead_id) |id| { // lead_id is optional since the root nodes have no leading nodes.\n            const lead_node = result.get_node(id);\n            ...\n        }\n        for (dependent_ids.items) |node_id| {\n            const dependent_node = result.get_node(node_id);\n            ...\n            visit_tree(result, node_id, result.get_dependents(node_id));\n        }\n    }\n\nCommand Line Tool\nTopoSort comes with a command line interface (CLI) tool toposort-cli,\nwhich uses the TopoSort library internally.  The data file it used follows\nthe simple dependent rule format of Makefile. E.g.\n  A: B\n  B: C D\n  C: E F G\n\nSample invocations on the test data:\n  zig-out/bin/toposort-cli --data data/data.txt\n  zig-out/bin/toposort-cli --data data/data.txt --verbose\n  zig-out/bin/toposort-cli --data data/data2.txt\n  zig-out/bin/toposort-cli --data data/data_cycle1.txt\n  zig-out/bin/toposort-cli --data data/data_cycle2.txt\n  zig-out/bin/toposort-cli --data data/data_num.txt --int\n\nSpecify the whole graph in the command line.\n  zig-out/bin/toposort-cli --graph \"(a b) (b d) (b c) (c d)\"\n  zig-out/bin/toposort-cli --graph \"(a b) (a c) (d) (c e f g)\"\n\nBenchmarks\nTopoSort comes with some benchmark tests.\nRun zig build test -Doptimize=ReleaseFast to run the benchmarks.\nLicense\nTopoSort is MIT licensed.\nFurther Reading\nFor more information on the Zig build system, check out these resources:\n\nZig Build System\nBuild System Tricks",
    "summary": {
      "en": "**TopoSort Library Summary**\n\nTopoSort is a Zig library designed for efficiently performing topological sorting on dependency graphs. Key features include:\n\n- **Dependency Graph Building**: Create graphs from dependency data.\n- **Topological Sorting**: Sort nodes based on their dependencies.\n- **Parallel Processing**: Generate subsets of nodes that can be processed simultaneously.\n- **Cycle Detection**: Identify and report cycles in the graph.\n- **Node Type Support**: Handle various types of nodes.\n\n**Installation Instructions**:\n1. Access the Releases page and choose a version.\n2. Use `zig fetch` to add TopoSort to your project with the command:\n   ```\n   zig fetch --save <URL>\n   ```\n\n**Basic Usage**:\n1. Import the library in your Zig source file.\n2. Initialize TopoSort with your desired node type.\n3. Add dependencies between nodes.\n4. Perform the sort and check for cycles.\n5. If no cycles are detected, process the sorted nodes.\n\n**Memory Management**:\n- Node values are stored by value or managed by the caller, depending on the type.\n\n**Configuration Options**:\n- Customize initialization with options like verbosity and maximum node value to optimize performance.\n\n**Command Line Tool**:\nTopoSort includes a CLI tool to sort dependencies using a simple rule format similar to Makefile.\n\n**Benchmarks**:\nThe library includes benchmarks that can be run to test performance.\n\n**License**: \nTopoSort is licensed under MIT.\n\nFor more information on using the Zig build system, refer to relevant resources.",
      "ko": "TopoSort는 의존성 그래프에서 효율적으로 위상 정렬을 수행하기 위해 설계된 Zig 라이브러리입니다. 주요 기능으로는 의존성 데이터를 기반으로 그래프를 생성하는 의존성 그래프 구축, 노드를 의존성에 따라 정렬하는 위상 정렬, 동시에 처리할 수 있는 노드의 하위 집합을 생성하는 병렬 처리, 그래프 내의 사이클을 식별하고 보고하는 사이클 탐지, 다양한 유형의 노드를 처리하는 노드 유형 지원이 있습니다.\n\n설치 방법은 다음과 같습니다. 먼저 Releases 페이지에 접속하여 원하는 버전을 선택합니다. 그런 다음 `zig fetch` 명령어를 사용하여 TopoSort를 프로젝트에 추가할 수 있습니다. 명령어는 다음과 같습니다: `zig fetch --save <URL>`.\n\n기본 사용법은 다음과 같습니다. Zig 소스 파일에 라이브러리를 가져옵니다. 원하는 노드 유형으로 TopoSort를 초기화합니다. 노드 간의 의존성을 추가한 후, 정렬을 수행하고 사이클을 확인합니다. 사이클이 발견되지 않으면 정렬된 노드를 처리합니다.\n\n메모리 관리 측면에서 노드 값은 유형에 따라 값으로 저장되거나 호출자가 관리합니다. 초기화를 사용자 정의할 수 있는 구성 옵션도 제공되며, 여기에는 성능을 최적화하기 위한 상세 수준과 최대 노드 값 설정이 포함됩니다.\n\nTopoSort는 Makefile과 유사한 간단한 규칙 형식을 사용하여 의존성을 정렬할 수 있는 CLI 도구도 포함하고 있습니다. 이 라이브러리에는 성능을 테스트할 수 있는 벤치마크도 포함되어 있습니다. TopoSort는 MIT 라이선스 하에 배포됩니다. Zig 빌드 시스템 사용에 대한 더 많은 정보는 관련 자료를 참고하시기 바랍니다.",
      "ja": "TopoSortは、依存関係グラフに対して効率的にトポロジカルソートを行うために設計されたZigライブラリです。このライブラリの主な機能には、依存関係データからグラフを作成する「依存関係グラフの構築」、ノードを依存関係に基づいて並べ替える「トポロジカルソート」、同時に処理できるノードのサブセットを生成する「並列処理」、グラフ内のサイクルを特定して報告する「サイクル検出」、さまざまなタイプのノードを扱う「ノードタイプのサポート」が含まれます。\n\nインストール手順は以下の通りです。まず、リリースページにアクセスしてバージョンを選択します。次に、`zig fetch`コマンドを使用してプロジェクトにTopoSortを追加します。\n\n基本的な使用方法は、まずZigのソースファイルにライブラリをインポートし、希望するノードタイプでTopoSortを初期化します。その後、ノード間に依存関係を追加し、ソートを実行してサイクルをチェックします。サイクルが検出されなければ、ソートされたノードを処理します。\n\nメモリ管理については、ノードの値はタイプに応じて値として保存されるか、呼び出し元によって管理されます。\n\n設定オプションとしては、初期化時に冗長性や最大ノード値などのオプションをカスタマイズしてパフォーマンスを最適化できます。\n\nTopoSortには、Makefileに似たシンプルなルール形式を使用して依存関係をソートするためのCLIツールも含まれています。また、ライブラリにはパフォーマンスをテストするためのベンチマークも含まれています。\n\nライセンスはMITライセンスの下で提供されています。Zigビルドシステムの使用に関する詳細は、関連するリソースを参照してください。"
    }
  },
  {
    "id": "0ac1c6b3f99477e8",
    "title": {
      "en": "Forking Work Simplification – Let's Bring Back Eisenhower's Process Improvement",
      "ko": "작업 단순화: 아이젠하워의 개선법",
      "ja": "仕事簡素化の極意"
    },
    "type": "story",
    "url": "https://www.governance.fyi/p/forking-work-simplification-and-more",
    "score": 55,
    "by": "RetiredRichard",
    "time": 1743555162,
    "content": "Share this postGovernance CyberneticsForking Work Simplification & MoreCopy linkFacebookEmailNotesMoreDiscover more from Governance CyberneticsSubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.Already have an account? Sign inForking Work Simplification & MoreLet's Bring Back Truman's and Eisenhower's Front Line Process Improvement ProgramDave DeekApr 01, 20255Share this postGovernance CyberneticsForking Work Simplification & MoreCopy linkFacebookEmailNotesMore23ShareHappy April Fool’s, but frankly we are all (well almost all) business today with a neat little update. Kevin Hawickhorst’s article talked about Eisenhower-era process improvement tools that transformed federal efficiency during the 1940s-1960s, when government trust reached 80% and major national projects succeeded. You know what? Why keep such a gem hidden away in old dusty books and digital pdf scans? We should create a documentation site with the intent of fully recreating, updating, and creating alternative versions of the Work Simplification program and the artifacts (training materials, art, etc)—not just for governments but for civic organizations, political campaigns, and businesses. The idea is to make it easier for today's organizations (or at least the common political actor local government employee) identify unnecessary procedural steps and eliminate bottlenecks that waste resources, maybe even spot loopholes and poison pills in bills and plans to spot for. Right now we converted the associated manuals into a docs site called Standards, with our end of April plans is recreate the other materials like Process Chart forms and the such Standards - Fork of Work SimplificationOn that note, thanks for reading Governance Cybernetics! Subscribe for free to receive new posts and support our work.SubscribeWhy Does This Matter? Process improvement isn't intuitive for most people. As stated in earlier articles, even big boy corporations with dedicated resources often struggle to implement it effectively (don’t believe me, ask McKinsey who is less trustworthy than I am but for some reason, you trust these guys). And while think tanks occasionally tackle implementation issues, they're typically far removed from the ground-level reality where policies succeed or fail.We need to make process improvement accessible to ordinary people in local groups who have the most direct experience with broken processes. They don't need MBA jargon or complex methodologies—just practical tools to document what's happening in their communities. Practical tools like what the Work Simplification Program has to offer. A hypothetical example is a local YIMBY volunteer who's helped ten homeowners navigate ADU permits and has invaluable knowledge that no policy expert in Sacramento possesses (which is a good idea in its own right, instead of an accounting student helping people file taxes, law or real estate students helping people navigate ADU permits with the understanding they are not real estate lawyers). Adjusting Work Simplification’s Process Charting (making a list of all the steps one by one and making them like a more verbose flowchart), so that volunteer with basic process mapping skills increases the chances that the volunteer might spot something everyone else overlooks!Unlike corporate “process improvement” which focuses on short-term cost-cutting or (in very rare cases since Deming’s death) efficiency, the WW2-era political process improvement focuses on effectiveness—making the government deliver what laws promise just like what happened during the 40s and 50s. This distinction matters because it centers on citizen experience rather than administrative convenience.Even something as esoteric as increasing fertility rates, there is a growing mountain of research and case studies showcasing the value of better implementation or even better local government can provide, let’s say “booming” results (*sounds of crickets chirping* yes I want to make that joke again).Recreating Work Simplification will help us deal with failures aren't about policy intent but process implementation. On That NoteThe consolidated training manual is called “Work Simplification as Exemplified by the Work Simplification Program of the U. S. Bureau of the Budget.” It is online on Hathitrust. To read it, you have to be located in the US. To download it, you have to have an institutional subscription. Or, you can reach out to me and I be glad to give you a digital pdf. Thanks for reading Governance Cybernetics! Subscribe for free to receive new posts and support our work.Subscribe5Share this postGovernance CyberneticsForking Work Simplification & MoreCopy linkFacebookEmailNotesMore23SharePrevious",
    "summary": {
      "en": "The article discusses the need to revive and modernize the Work Simplification Program, originally developed during the Eisenhower era, which improved government efficiency. The author, Dave Deek, highlights that these process improvement tools helped achieve high levels of trust and success in national projects in the mid-20th century. \n\nDeek proposes creating a documentation site to update and share these tools not only with government entities but also with civic organizations and businesses, aiming to help them identify and eliminate unnecessary steps in their processes. This is important because many organizations, even large corporations, struggle with process improvement due to its complexity.\n\nThe goal is to make these resources accessible to local volunteers and groups who have firsthand experience with inefficient processes, rather than relying on complicated corporate methods. By focusing on the effectiveness of government services and enhancing citizen experiences, the revived program could address implementation failures rather than just policy intentions. \n\nDeek encourages readers to explore the original Work Simplification materials, which are available online, and to reach out for access if needed.",
      "ko": "이 기사는 아이젠하워 시대에 개발된 작업 간소화 프로그램을 현대화하고 부활시킬 필요성에 대해 다루고 있습니다. 이 프로그램은 정부의 효율성을 높이는 데 기여했습니다. 저자 데이브 디크는 이러한 프로세스 개선 도구들이 20세기 중반의 국가 프로젝트에서 높은 신뢰와 성공을 달성하는 데 도움을 주었다고 강조합니다.\n\n디크는 정부 기관뿐만 아니라 시민 단체와 기업과도 이 도구들을 업데이트하고 공유할 수 있는 문서화 사이트를 만들 것을 제안합니다. 이를 통해 조직들이 불필요한 단계를 식별하고 제거하는 데 도움을 주고자 합니다. 많은 조직, 심지어 대기업조차도 프로세스 개선의 복잡성 때문에 어려움을 겪고 있기 때문에 이러한 접근이 중요합니다.\n\n목표는 복잡한 기업 방법에 의존하기보다는 비효율적인 프로세스를 직접 경험한 지역 자원봉사자와 그룹이 이 자원에 접근할 수 있도록 하는 것입니다. 정부 서비스의 효과성을 높이고 시민의 경험을 개선하는 데 집중함으로써 부활한 프로그램은 단순히 정책 의도에 그치지 않고 실행 실패를 해결할 수 있을 것입니다.\n\n디크는 독자들에게 원래의 작업 간소화 자료를 온라인에서 찾아보도록 권장하며, 필요할 경우 접근할 수 있도록 연락하라고 합니다.",
      "ja": "この記事では、アイゼンハワー政権時代に開発された「作業簡素化プログラム」を復活させ、現代に合わせて改良する必要性が論じられています。このプログラムは、政府の効率を向上させるために役立ちました。著者のデイブ・ディーク氏は、これらのプロセス改善ツールが20世紀中頃の国家プロジェクトにおいて高い信頼と成功をもたらしたことを強調しています。\n\nディーク氏は、これらのツールを更新し、政府機関だけでなく市民団体や企業とも共有するためのドキュメンテーションサイトを作成することを提案しています。これにより、組織がプロセスの中で不要なステップを特定し、排除する手助けをすることが目的です。多くの組織、特に大企業でもプロセス改善が難しいのは、その複雑さに起因しています。\n\nこのリソースを、非効率なプロセスに直接関わっている地域のボランティアやグループが利用できるようにすることが目指されています。複雑な企業の手法に頼るのではなく、政府サービスの効果を重視し、市民の体験を向上させることで、復活したプログラムは政策の意図だけでなく、実施の失敗にも対処できる可能性があります。\n\nディーク氏は、オンラインで入手可能な元の作業簡素化資料を探求し、必要に応じてアクセスを求めるよう読者に呼びかけています。"
    }
  },
  {
    "id": "fe6ab7e585c71ac1",
    "title": {
      "en": "Systems Correctness Practices at AWS: Leveraging Formal and Semi-Formal Methods",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://queue.acm.org/detail.cfm?id=3712057",
    "score": 157,
    "by": "yarapavan",
    "time": 1743519582,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "f2e62f1021a29850",
    "title": {
      "en": "RubyUI (Former PhlexUI): Ruby Gem for RubyUI Components",
      "ko": "루비UI: 컴포넌트의 혁신",
      "ja": "ルビーUI: コンポーネントの宝石"
    },
    "type": "story",
    "url": "https://github.com/ruby-ui/ruby_ui",
    "score": 163,
    "by": "ksec",
    "time": 1743521998,
    "content": "RubyUI (former PhlexUI) 🚀\nBeautifully designed components that you can copy and paste into your apps. Accessible. Customizable. Open Source.\nThis is NOT a component library. It's a collection of re-usable components that you can generate or copy and paste into your apps.\nPick the components you need. Copy and paste the code into your project and customize to your needs. The code is yours.\nUse this as a reference to build your own component libraries.\nKey Features:\n\nBuilt for Speed ⚡: RubyUI leverages Phlex, which is up to 12x faster than traditional Rails ERB templates.\nStunning UI 🎨: Design beautiful, streamlined, and customizable UIs that sell your app effortlessly.\nStay Organized 📁: Keep your UI components well-organized and easy to manage.\nCustomer-Centric UX 🧑‍💼: Create memorable app experiences for your users.\nCompletely Customizable 🔧: Full control over the design of all components.\nMinimal Dependencies 🍃: Uses custom-built Stimulus.js controllers to keep your app lean.\nReuse with Ease ♻️: Build components once and use them seamlessly across your project.\n\nHow to Use:\n\nFind the perfect component 🔍: Browse live-embedded components on our documentation page.\nCopy the snippet 📋: Easily copy code snippets for quick implementation.\nMake it yours 🎨: Customize components using Tailwind utility classes to fit your specific needs.\n\nInstallation 🚀\n1. Install the gem\nbundle add ruby_ui --group development --require false\n\nor add it to your Gemfile:\ngem \"ruby_ui\", group: :development, require: false\n\n2. Run the installer:\nbin/rails g ruby_ui:install\n\n3. Done! 🎉\nYou can generate your components using ruby_ui:component generator.\nbin/rails g ruby_ui:component Accordion\n\nDocumentation 📖\nVisit https://rubyui.com/docs/introduction to view the full documentation, including:\n\nDetailed component guides\nThemes\nLookbook\nGetting started guide\n\nSpeed Comparison 🏎️\nRubyUI, powered by Phlex, outperforms alternative methods:\n\nPhlex: Baseline 🏁\nViewComponent: ~1.5x slower 🚙\nERB Templates: ~5x slower 🐢\n\nSee the original view layers benchmark by @KonnorRogers and its variations.\nImportmap notes:\nIf you run into importmap issues this stackoverflow question might help:\nhttps://stackoverflow.com/questions/70548841/how-to-add-custom-js-file-to-new-rails-7-project/72855705\nLicense 📜\nLicensed under the MIT license.\n\n© 2024 RubyUI. All rights reserved. 🔒",
    "summary": {
      "en": "**RubyUI Overview**\n\nRubyUI (formerly PhlexUI) offers a collection of beautifully designed, reusable components that you can easily copy and paste into your apps. It's open-source, accessible, and customizable. Unlike a traditional component library, you can choose and modify components to fit your project.\n\n**Key Features:**\n- **Fast Performance:** RubyUI is built on Phlex, making it up to 12 times faster than standard Rails ERB templates.\n- **Beautiful Design:** Create stunning and customizable user interfaces.\n- **Organization:** Keep your UI components well-structured and manageable.\n- **User Experience:** Focus on creating memorable experiences for your users.\n- **Customization:** You have full control over the design of all components.\n- **Lightweight:** Minimal dependencies with custom Stimulus.js controllers.\n- **Reusability:** Build components once and reuse them throughout your project.\n\n**How to Use:**\n1. **Find Components:** Browse live components on the documentation page.\n2. **Copy Code:** Easily copy snippets for quick use.\n3. **Customize:** Use Tailwind utility classes to match your needs.\n\n**Installation Steps:**\n1. Install the gem with:\n   - `bundle add ruby_ui --group development --require false` \n   - or add to your Gemfile: `gem \"ruby_ui\", group: :development, require: false`\n2. Run the installer: `bin/rails g ruby_ui:install`\n3. Generate your components: `bin/rails g ruby_ui:component Accordion`\n\n**Documentation:** Full documentation is available at [rubyui.com/docs/introduction](https://rubyui.com/docs/introduction), including guides, themes, and a getting started guide.\n\n**Performance Comparison:** RubyUI is significantly faster than alternatives like ViewComponent and ERB templates.\n\n**License:** RubyUI is licensed under the MIT license.",
      "ko": "RubyUI는 이전에 PhlexUI로 알려졌던 오픈소스 프로젝트로, 아름답게 디자인된 재사용 가능한 컴포넌트 모음을 제공합니다. 이 컴포넌트들은 앱에 쉽게 복사하여 붙여넣을 수 있으며, 접근성이 뛰어나고 사용자 맞춤형으로 수정할 수 있습니다. 전통적인 컴포넌트 라이브러리와 달리, 프로젝트에 맞게 원하는 컴포넌트를 선택하고 수정할 수 있는 점이 특징입니다.\n\nRubyUI의 주요 특징 중 하나는 빠른 성능입니다. Phlex 기반으로 구축되어 있어, 일반적인 Rails ERB 템플릿보다 최대 12배 더 빠릅니다. 또한, 사용자 인터페이스를 아름답고 맞춤형으로 디자인할 수 있는 기능을 제공합니다. UI 컴포넌트를 잘 구조화하여 관리할 수 있으며, 사용자에게 기억에 남는 경험을 제공하는 데 중점을 두고 있습니다. 모든 컴포넌트의 디자인을 완전히 제어할 수 있으며, 최소한의 종속성과 함께 사용자 정의 Stimulus.js 컨트롤러를 사용하여 가볍게 유지됩니다. 한 번 만든 컴포넌트는 프로젝트 전반에 걸쳐 재사용할 수 있습니다.\n\n사용 방법은 간단합니다. 먼저 문서 페이지에서 라이브 컴포넌트를 찾아보고, 필요한 코드 스니펫을 쉽게 복사하여 사용할 수 있습니다. Tailwind 유틸리티 클래스를 활용해 필요에 맞게 커스터마이즈할 수 있습니다.\n\n설치 과정은 다음과 같습니다. 먼저, gem을 설치합니다. `bundle add ruby_ui --group development --require false` 명령어를 사용하거나 Gemfile에 `gem \"ruby_ui\", group: :development, require: false`를 추가합니다. 그 다음, 설치 프로그램을 실행합니다: `bin/rails g ruby_ui:install`. 마지막으로, 컴포넌트를 생성합니다: `bin/rails g ruby_ui:component Accordion`.\n\n자세한 문서는 [rubyui.com/docs/introduction](https://rubyui.com/docs/introduction)에서 확인할 수 있으며, 가이드, 테마, 시작 가이드 등이 포함되어 있습니다. RubyUI는 ViewComponent나 ERB 템플릿과 비교했을 때 성능이 상당히 우수합니다. 라이센스는 MIT 라이센스에 따라 제공됩니다.",
      "ja": "RubyUI（以前のPhlexUI）は、美しくデザインされた再利用可能なコンポーネントのコレクションを提供します。これらのコンポーネントは、アプリに簡単にコピー＆ペーストできるようになっています。オープンソースで、アクセスしやすく、カスタマイズも可能です。従来のコンポーネントライブラリとは異なり、プロジェクトに合わせてコンポーネントを選んだり、変更したりすることができます。\n\nRubyUIの主な特徴には、まず高速なパフォーマンスがあります。Phlexに基づいて構築されているため、標準のRails ERBテンプレートよりも最大12倍速く動作します。また、美しいデザインを実現し、カスタマイズ可能なユーザーインターフェースを作成できます。UIコンポーネントを整理して管理しやすく保つことができ、ユーザーにとって記憶に残る体験を提供することに重点を置いています。すべてのコンポーネントのデザインを完全にコントロールでき、依存関係が最小限の軽量な構造を持っています。コンポーネントを一度作成すれば、プロジェクト全体で再利用することが可能です。\n\n使用方法は簡単です。まず、ドキュメントページでライブコンポーネントを探します。次に、必要なコードを簡単にコピーして使用できます。Tailwindのユーティリティクラスを使って、自分のニーズに合わせてカスタマイズすることもできます。\n\nインストール手順は以下の通りです。まず、gemをインストールします。コマンドは「bundle add ruby_ui --group development --require false」またはGemfileに「gem \"ruby_ui\", group: :development, require: false」と追加します。次に、インストーラーを実行します。「bin/rails g ruby_ui:install」と入力します。そして、コンポーネントを生成します。「bin/rails g ruby_ui:component Accordion」と入力します。\n\n完全なドキュメントは、ガイドやテーマ、入門ガイドを含む[rubyui.com/docs/introduction](https://rubyui.com/docs/introduction)で入手できます。RubyUIは、ViewComponentやERBテンプレートなどの代替品と比較して、かなり高速です。ライセンスはMITライセンスの下で提供されています。"
    }
  },
  {
    "id": "95bfd065245189dc",
    "title": {
      "en": "Chromophobia",
      "ko": "색깔 공포증",
      "ja": "色恐怖症"
    },
    "type": "story",
    "url": "https://press.uchicago.edu/ucp/books/book/distributed/C/bo3536650.html",
    "score": 3,
    "by": "handfuloflight",
    "time": 1743616500,
    "content": "Reviews\n          Previous Slide\n\n                  “A thorough and witty cultural history of color.”\n\n                Karen Rosenberg | New York Times\n\n                  “A provocative contribution to the discourse of color theory.”\n\n                James Meyer | Artforum\n\n                  “Full of good writing, good anecdotes, devastating quotes, deft arguments, and just the sort of mysterious anomalies one would expect from an artist writing about the enemies of his practice.”\n\n                Dave Hickey | Bookforum\n\n                  “This beautifully produced book is an intelligent and provocative essay on why Western culture hates and fears colour. The prose is cumulative and passionate in its effect and widely referential—from Barthes to Melville, Wim Wenders to Huysmans. . . . You cannot fail to be stimulated by his thoughts”\n\n                RA (Royal Academy Magazine)\n\n                  “Batchelor has found an irresistible selection of anecdotes and quotes relating to the experience of color. . . . Thoughtful and entertaining.”\n\n                Tema Celeste\n\n                  “A hugely entertaining guide to our ongoing obsession with white.”\n\n                Time Out London\n\n                  “Switching from novels and movies to art and architecture, Batchelor clearly and cleverly traces the cultural implications of the 100 year-plus Colour War between Chromophobes like Le Corbusier, with their hosannas to whiteness, and Chromophiliacs like Warhol, the great artist of cosmetics. A succinct book of art theory which goes down smoothly.”\n\n                iD Magazine\n\n                  “A theoretical and cultural banquet. . . . The book’s narrative quality goes beyond the telling of color theory’s history and other approaches to color, coming to read like a psychological thriller: how the West crushed color—or at least thought it did so.”\n\n                New Art Examiner\n              Next SlideSlide 1Slide 2Slide 3\n\nBack to top\n\n                Table of Contents\n\n                Table of Contents\n\n                         1. Whitescapes2. Chromophobia3. Apocalypstick4. Hanunoo5. ChromophiliaReferencesSelect Bibliography and FilmographyList of Illustrations Acknowledgements\n\n                      Read Moreabout table of contents\n\n                      Read Lessabout table of contents\n\nTable of Contents\n\n                         1. Whitescapes2. Chromophobia3. Apocalypstick4. Hanunoo5. ChromophiliaReferencesSelect Bibliography and FilmographyList of Illustrations Acknowledgements\n\n                      Read Moreabout table of contents\n\n                      Read Lessabout table of contents\n\nBe the first to know\n\n            Get the latest updates on new releases, special offers, and media highlights when you subscribe to our email lists!\n\n            Sign up here for updates about the Press",
    "summary": {
      "en": "The text provides a collection of reviews for a book about color. Key points include:\n\n- The book is praised for its thorough and witty exploration of color in culture, as noted by the New York Times.\n- It is described as a provocative addition to color theory discussions by Artforum.\n- Reviewers commend the engaging writing style, interesting anecdotes, and sharp arguments, highlighting its appeal to both art and color enthusiasts.\n- The Royal Academy Magazine calls it a thoughtful and stimulating essay on Western culture's complex relationship with color.\n- The book contrasts \"Chromophobes,\" who favor whiteness, with \"Chromophiliacs,\" who embrace vibrant colors, illustrating a cultural conflict over color.\n- Overall, it is characterized as an entertaining and insightful read, blending theory with narrative elements, akin to a psychological thriller.\n\nThe table of contents lists the main topics discussed in the book.",
      "ko": "이 책은 색깔에 대한 내용을 다룬 리뷰 모음집이다. 뉴욕 타임스는 이 책이 문화 속 색깔을 철저하고 재치 있게 탐구했다고 칭찬했다. 아트포럼에서는 색채 이론 논의에 도발적인 기여를 했다고 평가했다. 리뷰어들은 매력적인 글쓰기 스타일과 흥미로운 일화, 날카로운 주장을 높이 평가하며, 예술과 색깔에 관심 있는 독자들에게 모두 매력적이라고 강조했다. 로열 아카데미 매거진은 이 책이 서양 문화와 색깔 간의 복잡한 관계에 대한 사려 깊고 자극적인 에세이라고 언급했다. \n\n책에서는 '크로모포브'(흰색을 선호하는 사람들)와 '크로모필리아크'(선명한 색을 좋아하는 사람들)를 대조하며 색깔에 대한 문화적 갈등을 보여준다. 전반적으로 이 책은 이론과 서사적 요소를 결합하여 심리 스릴러와 같은 재미있고 통찰력 있는 읽을거리를 제공한다고 평가된다. \n\n목차에는 책에서 다루는 주요 주제들이 나열되어 있다.",
      "ja": "このテキストは、色に関する書籍のレビューをまとめたものです。ニューヨークタイムズでは、この本が文化における色の徹底的かつウィットに富んだ探求を行っていると高く評価されています。また、アートフォーラムでは、色彩理論に対する挑発的な視点を提供する作品として紹介されています。レビュアーたちは、魅力的な文体や興味深い逸話、鋭い議論を称賛し、アートや色に興味を持つ人々にとっての魅力を強調しています。ロイヤルアカデミーマガジンは、西洋文化と色との複雑な関係についての考えさせられる刺激的なエッセイだと評しています。\n\nこの本では、「クロモフォーブ（色を嫌う人）」と「クロモフィリアク（色を愛する人）」という対比を通じて、色に関する文化的な対立を描いています。全体として、理論と物語の要素を融合させた、エンターテインメント性と洞察に満ちた読み物として特徴づけられています。心理的スリラーに似たスタイルで展開されているのが魅力です。\n\n目次には、本書で取り上げられている主要なトピックが記載されています。"
    }
  },
  {
    "id": "40cb3d44157bc448",
    "title": {
      "en": "Converting a Go-Kart into a Deathtrap",
      "ko": "죽음의 고카트 변신",
      "ja": "デスカート改造術"
    },
    "type": "story",
    "url": "https://matto.io/posts/converting-a-go-kart-into-a-deathtrap/",
    "score": 106,
    "by": "mattogodoy",
    "time": 1743350186,
    "content": "Converting a go-kart into a deathtrap  Posted Mar 21, 2025   Updated Mar 22, 2025    By  Matto Godoy    11 min readEste artículo también está disponible en Español 🇪🇸As a child, I always dreamed of having vehicles that I could drive myself. I remember putting a chair with a fan in front of it and pretending to fly an airplane.The PlanNow that my son is about to turn 3, I thought I could do something similar for him. I came up with the idea of transforming a boring pedal go-kart into an incredibly powerful and dangerous, and therefore, super fun electric car.RequirementsBefore starting the project, I defined a series of basic requirements:Motor: It has to be powered by an electric motor, because we no longer live in 1940.Size: It has to be small, so it’s easy to drive and fits in the trunk for transportation.Power: There are many models of electric go-karts for children. A good example is those that can be rented at shopping centers. These, in addition to being of very poor quality, have very little power, very little final speed, and their controls are usually all-or-nothing, meaning you can’t control speed or direction proportionally. They either accelerate or not.Speed Limiter: As it’s a car for children, it has to be safe (within reason). For this, the car must have some means to limit the maximum speed it can reach. This speed can be set very low at first and increased as the child gains experience.Driving Modes: Continuing with safety measures, my idea is that the car should have 2 driving modes:Manual: Speed, brakes, direction (forward or reverse), steering, and speed limiter are controlled by the driver themselves. This will be ideal when the child is a bit older and has some driving experience.Radio-controlled: Speed, brakes, direction, and speed limiter are controlled with a radio like those used for radio-controlled cars. The steering control will remain manual (at least for now).All-terrain: Cars that can only go on asphalt are boring. Nothing better than going through dirt, stones, and mud on a difficult terrain. This kart has to have good ground clearance and be able to handle any obstacle.Tires: They have to be of the inflatable kind to counteract the lack of suspension. They are also necessary for the car to be all-terrain.ComponentsThe go-kartThe kart had to be the right size for children between 3 and 6 years old. After looking around, I found a used pedal go-kart in very good condition that would serve as the base structure for the project:With this, I solved important things like the steering mechanism and the seat, in addition to the overall structure.The floorThe original pedal kart structure is quite flimsy and didn’t have anywhere to attach the motors or support the pedals and feet. I decided to use a 16mm thick MDF wood base cut to a specific size and shape to fit the kart. On this base, the motors, pedals, battery, and electronics box will be mounted. It will also give the car good rigidity.The motorsIn the past, Hoverboards (another deadly trap for kids) became very popular. Today, the Hoverboard craze has passed, and most are abandoned, gathering dust. This means we can get them almost for free by buying them second-hand from some parent who’s tired of tripping over them in their kids’ bedroom.A great advantage of these motors is that they fit inside the wheel, so they don’t take up any space. The motor IS the wheel. You just add a tire and go. Additionally, they are extremely powerful for their size. A Hoverboard can carry up to a 100 kg person uphill without issues.   Motor controllersThe Hoverboard motors are brushless, and they have magnetic sensors to know the motor’s position at all times. This allows for very good speed regulation. There are variants of these motors without sensors, but they don’t work well at low speeds.After searching a bit, I found a controller called ZS-X11H that is incredibly cheap (costs about €12), especially compared to more decent controllers.In addition to its price, it has the advantage of allowing you to change the direction of rotation, adjust the speed, and even has a braking function (more on this later). Additionally, their control inputs are analog by default, but by soldering a jumper, you can use PWM inputs. This will allow us to control them using an Arduino Nano.The batteryThe Hoverboard I took the motors from had a 36v battery, but it was very small and very dead. I wanted it to have plenty of autonomy, so I bought a 36v, 10Ah battery. With this, it should have enough autonomy for a while (I haven’t checked how long yet, but in my short experience, it lasts a lot).A problem with most batteries is that they get ruined if they are over-discharged. That’s why I installed a discharge protection system that simply shuts off the kart when the battery voltage is too low:And I also installed a small screen that shows the remaining charge and current voltage:Voltage convertersI said that the battery is 36 volts, but we also need 5v to power the Arduino and the RC receiver, and 12v for the lights. For this, we use two voltage converters like this one:The wheelsAs I mentioned earlier, I wanted the wheels to be inflatable to cushion the bumps a bit. The original pedal go-kart wheels were plastic, so in addition to being noisy, they were extremely hard.For the front wheels, I bought the typical wheelbarrow wheels that come with a tire, tube, and bearings:For the rear wheels, after researching a bit, I found that there’s a tire size that’s perfect for Hoverboard motors. The specific size is 4.10/3.50-5. They’re not cheap or easy to find, but luckily I got them on AliExpress.With a rubber mallet and a bit of effort, I managed to put the tires on the motors. They fit perfectly. The only problem is that the motors don’t have a hole for the air valve to come out, so I had to make a hole in the tire itself and take it out from there. It’s not pretty, but I couldn’t think of another solution: The Radio ControlI was looking for something cheap but functional. After reading a bit online, I found a very affordable model (around €20 with receiver included) that has 6 channels and many functions. It’s the HotRC CT-6A:Six channels are more than enough for the functionalities I want to control:Mode (manual or radio-controlled)AccelerationBrakingSteeringSpeed limitI use 5 of the 6 available channels, and one is left free in case I want to add steering control in the future.LightsEvery worthy car has to have lights. I installed a fairly powerful front white light and a rear red position light. Both turn on and off with a switch located behind the steering wheel. PedalsFor manual mode, I installed pedals that allow progressive acceleration and braking. I’m not sure what they were originally designed for, but they’re exactly what I needed:Something I added was keeping the brake functionality with the pedal, even if the car is in radio-controlled mode. This way, if the driver wants to brake at any moment for any reason, they can do so.Speed limiterThe kart’s maximum speed can be limited using a potentiometer installed in the electronics box when the kart is in manual mode, or with a 3-position switch on the radio control for that mode.Direction controlIn manual mode, the control for forward or reverse direction is a switch located behind the steering wheel. With it, you can select forward or reverse. In radio-controlled mode, it’s a button on the remote itself that’s used to change direction.The other switch you see in the photo is for the lights.The brainAs the car’s main processor, I used an Arduino Nano. It’s a very simple microcontroller, but the reality is that you don’t need much more.Its functions are:Define the driving modeReceive radio control commandsDecode the current position of the pedalsDefine the direction of rotation of the motorsControl the speed limitSend the output signal to the motor controllersTo learn more, see the source code section.DiagramThis is a simplified diagram I made of all the components that make up the car:Source codeThe source code is based on Arduino using PlatformIO for development. It’s open-source and can be found here.Integrating All ComponentsHere are some photos and videos of the entire process.This is an early test of the motor controller. The battery hadn’t arrived yet, so I was using a regulated power source for the test: Here are some integration tests with the Arduino and the RC. The code has a debugging function that allows you to see the inputs and outputs on the computer screen:  This is the waterproof box that contains all the electronic components. It fits perfectly under the seat. Unfortunately, I forgot to take a photo with all the components inside:    Here is the installation of the rear motors. I was thinking about how to make an adapter to hold them, but it occurred to me that cutting the Hoverboard’s original supports could be an easy and robust way:    Here you can see it’s almost finished. I installed the pedal go-kart structure on the wooden floor and the front wheels:  These are the first tests with everything integrated: And this is the final result:      Pending tasksSteering controlOne of the things I initially considered adding, but later discarded, is controlling the steering using the remote control. This functionality would be a full project in itself because it has a couple of aspects that are difficult to solve:The motor used to change direction needs to be very powerful. Sometimes turning the steering wheel requires a lot of force, and a model servo, no matter how large, wouldn’t work. I’ve seen that for some robotics projects, windshield wiper motors are used, adapted with a potentiometer to turn them into super-powerful servos. This would be a good solution, but it requires a lot of work.The mechanism has to allow for both manual and radio-controlled steering. This is more difficult than it seems. Having the aforementioned motor installed would make it almost impossible to manually turn the wheels using the steering wheel, because this motor uses a gearbox. A solution could be to manually “engage” the radio-controlled system when needed, and “disengage” it when the driver is controlling the direction. This would require a lot of work and isn’t a solution I particularly like.Because of these problems, I decided that (for now) there will be no way to control the steering of the car using the remote control. However, it’s a good way for the child to learn to drive on their own.Regenerative brakingI mentioned earlier that the motor controller I used for this project had a braking function. For this particular controller, that function is not progressive. That is, the brake can be activated or not. This isn’t a big problem in itself, but the real problem is that when I first tested the car, I realized that the braking function of these controllers only serves to reduce the speed at which the wheels are turning, but they don’t apply considerable force. In short, this means that the car currently HAS NO BRAKES.The reason is that for the motors to brake, you have to change their mode and make them work as generators. This way, the motors offer resistance, but they generate energy, and that energy has to go somewhere. Ideally, back to the battery. Decent motor controllers have this functionality, and it’s called “regenerative braking.” It’s very good because you can recover some of the battery’s energy during braking or when going downhill.This explains why the controllers I used are so cheap; they don’t have this functionality (among others). I’ve already ordered others that cost six times more but have many more features, including regenerative braking. I made the investment because they can be used later for other projects, but mainly so that the driver can brake, since this is already a deadly trap as it is.Loss of remote signalThis remote control has a signal loss feature. I need to investigate a bit more and implement it. I don’t want it to happen that at some point, whether due to distance or any other reason, the signal from the remote control is lost and the child is trapped in a speeding kart with the accelerator stuck. This is another reason why I made sure the driver can always use the brake pedal in any mode.That’s all. See you next time!  electronics This post is licensed under  CC BY 4.0  by the author. Share",
    "summary": {
      "en": "**Summary:**\n\nThe article discusses a project where the author, inspired by childhood dreams, transforms a basic pedal go-kart into a powerful electric car for his young son. Key points include:\n\n1. **Project Goals**: The aim is to create an electric go-kart that is fun yet safe for children, featuring a speed limiter and two driving modes—manual and radio-controlled.\n\n2. **Components**:\n   - **Go-Kart Base**: A used pedal go-kart is repurposed for the structure.\n   - **Motor**: Hoverboard motors are used for their power and compactness.\n   - **Battery**: A 36v, 10Ah battery is installed for good autonomy.\n   - **Control Systems**: An Arduino Nano controls the motors and driving modes, while a cheap radio control unit manages the functions.\n\n3. **Safety Features**: The kart includes a speed limiter and a manual brake pedal that can be used in both driving modes.\n\n4. **Pending Improvements**: Future enhancements may include adding steering control via remote and implementing regenerative braking for better safety.\n\n5. **Concerns**: The author expresses worries about losing remote signal control, emphasizing the importance of having manual braking available at all times.\n\nOverall, the project aims to create an exciting yet safe driving experience for young children.",
      "ko": "이 글에서는 어린 시절의 꿈에서 영감을 받아 기본 페달 고카트를 강력한 전기차로 변형하는 프로젝트에 대해 이야기합니다. 주요 내용은 다음과 같습니다.\n\n프로젝트의 목표는 아이들이 즐겁고 안전하게 탈 수 있는 전기 고카트를 만드는 것입니다. 이 고카트는 속도 제한 장치와 수동 및 원격 조정 두 가지 주행 모드를 갖추고 있습니다.\n\n구성 요소로는 중고 페달 고카트를 구조물로 재활용하고, 파워와 컴팩트함을 고려하여 호버보드 모터를 사용합니다. 36v, 10Ah 배터리를 설치해 좋은 자율주행 시간을 확보하며, 아두이노 나노를 통해 모터와 주행 모드를 제어하고 저렴한 무선 조정 장치로 기능을 관리합니다.\n\n안전 기능으로는 속도 제한 장치와 두 가지 주행 모드에서 사용할 수 있는 수동 브레이크 페달이 포함되어 있습니다. 향후 개선 사항으로는 원격으로 조향 제어를 추가하고, 안전성을 높이기 위해 회생 제동 시스템을 도입할 계획입니다.\n\n저자는 원격 신호 제어가 끊길까 걱정하며, 항상 수동 브레이크를 사용할 수 있는 것이 중요하다고 강조합니다. 이 프로젝트는 어린 아이들에게 흥미롭고 안전한 주행 경험을 제공하는 것을 목표로 하고 있습니다.",
      "ja": "この記事では、著者が子供の頃の夢に触発されて、基本的なペダルゴーカートを息子のために強力な電動車に変えるプロジェクトについて説明しています。主なポイントは以下の通りです。\n\nプロジェクトの目標は、子供たちが楽しめる安全な電動ゴーカートを作ることです。このゴーカートには、スピードリミッターと手動運転モード、ラジオコントロールモードの2つの運転モードが搭載されています。\n\n使用する部品については、まず構造には中古のペダルゴーカートを再利用します。モーターには、パワーとコンパクトさを兼ね備えたハゥバーボードのモーターを使用します。バッテリーは36V、10Ahのものを取り付けて、十分な走行距離を確保します。制御システムにはArduino Nanoを使い、モーターや運転モードを制御します。また、安価なラジオコントロールユニットが機能を管理します。\n\n安全機能としては、スピードリミッターと手動ブレーキペダルがあり、どちらの運転モードでも使用可能です。今後の改善点としては、リモコンによるハンドル操作の追加や、より安全性を高めるための回生ブレーキの実装が考えられています。\n\n著者は、リモート信号の制御が失われることへの懸念を表明しており、常に手動ブレーキが利用できることの重要性を強調しています。このプロジェクトは、若い子供たちにとって刺激的でありながら安全な運転体験を提供することを目指しています。"
    }
  },
  {
    "id": "f42a7c25b492336d",
    "title": {
      "en": "UCSD: Large Language Models Pass the Turing Test",
      "ko": "UCSD: 튜링 테스트 통과!",
      "ja": "UCSD: 言語モデルがチューリングテスト突破"
    },
    "type": "story",
    "url": "https://arxiv.org/abs/2503.23674",
    "score": 77,
    "by": "Mossy9",
    "time": 1743589088,
    "content": "We evaluated 4 systems (ELIZA, GPT-4o, LLaMa-3.1-405B, and GPT-4.5) in two randomised, controlled, and pre-registered Turing tests on independent populations. Participants had 5 minute conversations simultaneously with another human participant and one of these systems before judging which conversational partner they thought was human. When prompted to adopt a humanlike persona, GPT-4.5 was judged to be the human 73% of the time: significantly more often than interrogators selected the real human participant. LLaMa-3.1, with the same prompt, was judged to be the human 56% of the time -- not significantly more or less often than the humans they were being compared to -- while baseline models (ELIZA and GPT-4o) achieved win rates significantly below chance (23% and 21% respectively). The results constitute the first empirical evidence that any artificial system passes a standard three-party Turing test. The results have implications for debates about what kind of intelligence is exhibited by Large Language Models (LLMs), and the social and economic impacts these systems are likely to have.",
    "summary": {
      "en": "In a study, researchers tested four AI systems (ELIZA, GPT-4o, LLaMa-3.1-405B, and GPT-4.5) in two Turing tests with participants having 5-minute conversations with both a human and an AI. Participants then had to decide which one they thought was human. GPT-4.5 was identified as human 73% of the time, significantly more than the actual human participant, while LLaMa-3.1 was identified as human 56% of the time. The other models (ELIZA and GPT-4o) performed poorly, with only 23% and 21% accuracy respectively. This study provides the first strong evidence that an AI system can pass a Turing test, raising important questions about the intelligence of Large Language Models (LLMs) and their potential social and economic effects.",
      "ko": "연구에서 연구자들은 네 가지 인공지능 시스템(ELIZA, GPT-4o, LLaMa-3.1-405B, GPT-4.5)을 두 번의 튜링 테스트에서 시험했습니다. 참가자들은 인간과 인공지능과 각각 5분간 대화한 후, 어떤 쪽이 인간이라고 생각하는지 결정해야 했습니다. GPT-4.5는 73%의 확률로 인간으로 인식되었으며, 이는 실제 인간 참가자보다 훨씬 높은 수치입니다. 반면 LLaMa-3.1은 56%의 확률로 인간으로 인식되었습니다. 나머지 모델인 ELIZA와 GPT-4o는 각각 23%와 21%의 낮은 정확도로 성과가 좋지 않았습니다. 이 연구는 인공지능 시스템이 튜링 테스트를 통과할 수 있다는 강력한 첫 번째 증거를 제공하며, 대형 언어 모델(LLM)의 지능과 이들이 사회적, 경제적에 미칠 잠재적 영향에 대한 중요한 질문을 제기합니다.",
      "ja": "研究者たちは、4つのAIシステム（ELIZA、GPT-4o、LLaMa-3.1-405B、GPT-4.5）を使って、2回のチューリングテストを実施しました。参加者は、人間とAIの両方と5分間の会話を行い、その後、どちらが人間だと思うかを判断しました。GPT-4.5は73%の確率で人間として認識され、実際の人間参加者よりもかなり高い割合でした。一方、LLaMa-3.1は56%の確率で人間と見なされました。残りのモデルであるELIZAとGPT-4oは、23%と21%という低い精度でした。この研究は、AIシステムがチューリングテストを通過できるという初めての強い証拠を提供しており、大規模言語モデル（LLM）の知能や、それがもたらす社会的・経済的影響について重要な疑問を提起しています。"
    }
  },
  {
    "id": "800a230fe98c3307",
    "title": {
      "en": "Tell HN: Camelgate NPM Outage (Cloudflare)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": null,
    "score": 114,
    "by": "bavarianbob",
    "time": 1743524368,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "4b305ddd0ff0ea02",
    "title": {
      "en": "We can, must, and will simulate nematode brains",
      "ko": "선충 뇌 시뮬레이션!",
      "ja": "線虫脳をシミュレーション！"
    },
    "type": "story",
    "url": "https://asteriskmag.com/issues/09/we-can-must-and-will-simulate-nematode-brains",
    "score": 120,
    "by": "l1n",
    "time": 1743520581,
    "content": "We Can, Must, and Will Simulate Nematode Brains\n\n\t\t\t\t\tMichael Skuhersky\n\nScientists have spent over 25 years trying — and failing — to build computer simulations of the smallest brain we know. Today, we finally have the tools to pull it off.\n\n\t\t\t\t\t\t\t\t\t\t\tA near-perfect simulation of the human brain would have profound implications for humanity. It could offer a pathway for us to transcend the biological limitations that have constrained human potential, and enable unimaginable new forms of intelligence, creativity, and exploration. This represents the next phase in human evolution, freeing our cognition and memory from the limits of our organic structure.Unfortunately, it’s also a long way off. The human brain contains on the order of one hundred billion neurons — interconnected by up to a quadrillion synapses. Reverse-engineering this vast network would require computational resources far exceeding what’s currently available. Scientists seeking a proof of concept for whole brain emulation have had to turn to simpler model organisms. And by far the simplest available brain — at just 300 neurons — belongs to the nematode Caenorhabditis elegans.Scientists have been working on the problem of simulating C. elegans in some form or another for over 25 years. So far, they’ve been met with little success. But with today’s technology, the task is finally possible, and —as I’ll argue —necessary.\n\n    Motion patterns of C. elegans. Credit: Hiroshima University, Osaka University\n\n\t\t\t\t\t\t\t\t\t\t\tA brief history of worm brains\n\n\t\t\t\t\t\t\t\t\t\t\tThe biologist Sydney Brenner became interested in C. elegans as a model organism for developmental biology in the 1970s. Its simplicity and small size made it an ideal lab subject. In 1986, John C. White, a scientist in Brenner’s research group, produced a nearly complete map of the neural connections that make up the C. elegans brain — what scientists now call the connectome. As computers became more accessible, other scientists started building on Brenner’s work. Ernst Neibur and Paul Erdös kicked things off with a biophysical model of nematode locomotion in 1991. Two different teams (one at the University of Oregon and the other in Japan) published plans for building more ambitious models in the late 1990s. Both would have utilized White’s work on neural circuitry. Unfortunately, neither got off the ground.In 2004, the Virtual C. elegans project at Hiroshima University got somewhat farther: they released two papers describing their model, which simulated the nematode’s motor control circuits. The simulated nematode could respond to virtual pokes on its head, but it didn’t do much else. And even this was, arguably, not a true simulation. Although the researchers had a map of the nematode’s neurons, they didn’t know their innate biophysical parameter — that is, the precise electrical characteristics of the connections between them. Instead, the researchers used machine learning to produce a set of values for each neuron that made their simulated nematode respond to a poke like a real one would. As a result, this approach was not entirely grounded in biological reality — a recurring theme that would surface in several future simulation attempts.That is where things stood at the dawn of the 2010s. While work continued on simulating nematode locomotion, there was no progress on simulating a nematode’s brain —let alone a realistic one. Then, on January 1st, 2010, the engineer Giovanni Idili tweeted at the official account of the Whole Brain Catalogue, a project to consolidate data from mouse brains: “new year's resolution: simulate the whole C.Elegans brain (302 neurons)!” U.C. San Diego neuroscience grad student Stephen Larson noticed the tweet and, by August, Larson was pitching the idea at conferences. By early 2011, Larson and Idili had put together a team to start work on what would become the OpenWorm project —the efforts of a decentralized group of academics with the goal of creating a complete, realistic, and open source model of C. elegans.This was a heady time to be interested in simulating extremely tiny brains. Over the next few years, OpenWorm published a series of papers and model updates. In 2013, they hosted their first conference in Paris and landed an optimistic story in The Atlantic (title: “Is This Virtual Worm The First Sign of the Singularity?”). Meanwhile, the researcher David Dalrymple was working on a parallel project at MIT, which he dubbed Nemaload. OpenWorm scientists largely used data from dead nematodes but Dalrymple wanted to use the then-new technique of optogenetics to study living specimens. Optogenetics allows scientists to control neurons and other cells with light. In this case, the technique could be used to collect data on how a nematode’s brain responds to different states by perturbing it thousands-upon-thousands of times. In a 2011 comment on LessWrong, Dalrymple wrote “I would be Extremely Surprised, for whatever that's worth, if this is still an open problem in 2020.”It’s now 2025, and nematode simulation remains an open problem. Dalrymple abandoned Nemaload in 2012. OpenWorm still exists but has not made substantial progress over the past ten years towards creating a truly scientific whole brain simulation, due to a lack of available data. Occasionally, more modern (though still heavily assumption-based) simulations are published, including integrative models that strive to make fewer assumptions. We’re not quite back where we were in the 2010s: we have much better data on the C. elegans nervous system and — as I’ll discuss later — much better tools to study it. But we aren’t much closer to simulating a whole brain.What went wrong? Why has it taken over 25 years to build a working computer simulation of one of the simplest brains known to mankind? And, more importantly, why do I think that this time we can actually pull it off?\n\t\t\t\t\t\t\t\t\t\t\tWhy we got stuck\n\n\t\t\t\t\t\t\t\t\t\t\tBefore explaining what happened, we should ask a more fundamental question: what does it mean to successfully simulate a brain? This is a topic where it’s important to be specific. The term \"simulation\" in academic neuroscience often evokes the notorious failures of the Human Brain Project. In 2013, neuroscientist Henry Markram secured about 1 billion euros from the European Union to \"simulate the human brain\" — a proposal widely deemed unrealistic even at the time. The project faced significant challenges and ultimately did not meet its ambitious yet vague goals. These events cast something of a stigma on brain simulation research, making it especially important for those in the field to set clearer, more realistic goals with concrete milestones along the way.What makes a good simulation is a debate in itself, so I’ll just share my view: a good simulation of a nervous system is one that both accurately replicates its functionality and reliably predicts the future activity of a real system under the same initial conditions. That is, a simulated nematode in a simulated plate of agar should behave the same way as a real nematode in a real plate of agar. If we disturb the simulation —say, by poking or shining a light on it — it should respond the same way the real nematode would. And it should keep acting like a real nematode over time, instead of accumulating more error as time goes on.This definition can help us clarify what is and isn’t simulation. Last October, a consortium of scientists across 127 institutions published the complete connectome of the fruit fly, Drosophila melanogaster. This is a massive accomplishment by any objective standard: it is only the second complete connectome assembled, after that of C. elegans, and contains over 140,000 neurons (as compared to C. elegans’s 300). The success of the project, called FlyWire, has rekindled interest in brain simulation. And, in a sense, the FlyWire connectome can be used to simulate a fruit fly. When Philip Shiu, a researcher on the project, test-‘fired’ the neurons responsible for sensing sugar, the model predicted that other neurons that extend the fly’s proboscis would fire, as they would in a real fly. Other researchers have since used Shiu’s model to accurately predict neural patterns involved in the fly’s sense of taste, grooming, and locomotion.Shiu’s model represents an important advance in our understanding of fruit fly brains, but it isn’t really a simulation. (Nor is it trying to be;Shiu himself has been clear that the model is extremely simplified and makes assumptions about key parameters governing how neurons behave). While the model can successfully predict the behavior of particular groups of neurons, it cannot mimic the exact functionality of an entire fly brain. That’s because the FlyWire model is missing the same thing as OpenWorm (and other attempts to simulate nematodes) did: good data on the relationship between neural structure and neural function.Think of the connectome as a map of the brain. It can tell us how neurons connect to each other through electrical and chemical synapses. But despite revealing which neurons connect to one another, it doesn’t tell us anything about how those connections work. To fully model a brain, we need to understand the biophysical parameters governing each neuron’s behavior. This includes not only the variable strength of synapses (in neuroscience, these are called weights) but also the cells’ membrane properties, such as capacitance and the shapes of dendrites and axons, which affect how electrical signals propagate. We need to know both a neuron’s firing threshold as well as how that threshold changes as the animal learns new things (learning involves shifts in both synaptic weights and the intrinsic properties of neurons themselves). A simulation based only on a static connectome can’t learn —so it won’t behave very much like the real creature it’s trying to simulate.Unfortunately, learning the dynamic biophysical features of a living brain is much harder than understanding its structure (which, as we’ve seen, is hard enough). The primary technique used to map a connectome is electron microscopy. Because electrons have a wavelength up to one hundred thousand times smaller than that of visible light, they can be used to produce images at a much higher resolution than light microscopes. But electron microscopy has a serious disadvantage. It can only be used on sliced brain tissue, so it can’t tell us how a living brain responds to stimuli or changes over time. The technique can give us extremely detailed, high quality images, but can’t tell us a neuron’s electrical characteristics, like the strength of its synapses or how its membranes store electrical charge.For decades, the only way to learn such things was through a technique called patch clamping. The advantage of patch clamping is that it is highly accurate. The disadvantage is that it requires the painstaking placement of electrodes on each individual neuron. With effort, it’s feasible to patch clamp about three neurons at once, making it a less-than-ideal choice for capturing information about neural activity throughout the whole brain. This is where things stood when earlier attempts to simulate C. elegans stalled out. It was a problem of timing: In 2013, the tools that would let us understand what happens inside neurons either didn’t exist, or weren’t ready for practical use.\n\t\t\t\t\t\t\t\t\t\t\tNew ways to see\n\n\t\t\t\t\t\t\t\t\t\t\tAs C. elegans simulation research was losing steam, other researchers pushed forward in advancing the ability to observe cells. First, advances in optical microscopy made it possible to capture fast, relatively sharp images of living cells without destroying them. Since the late 1950s, biologists have relied on confocal microscopes, which use a tiny pinhole to block out-of-focus light. This creates higher resolution images, but the method is also slow, since capturing a whole sample means scanning it point-by-point. This is a serious problem for studying traits that change rapidly (like neuronal activity). This is where modern techniques like light sheet microscopy prove particularly useful. Instead of focusing light through a point, light sheet microscopes use a laser sheet to illuminate an entire 2D cross-section of a sample.The process is dramatically faster and gentler on tissue than traditional confocal methods.Light sheet microscopes have existed since the 1990s, but early versions of the technology struggled to capture fast intracellular processes. That changed with a series of innovations in the early 2010s. First, new techniques were developed to allow optical microscopy below the diffraction limit (the smallest distance between two points at which they can still be distinguished by an optical system). For visible light, this distance is between 200 and 250 nanometers — too big to distinguish most cellular features. That changed with the introduction of super-resolution microscopy which featured resolutions of 100 nanometers and below. Another major advance was DiSPIM,\n\n        1\n\n invented in 2014. In light sheet microscopes, the light illuminating an image has to be perpendicular to the camera picking it up. Originally, this meant that the camera and the light sheet were part of separate assemblies. DiSPIM microscopes use two perpendicular lens assemblies, each equipped with a light source and a camera. This approach doubled the speed with which the microscope could capture images of living samples, and ensured that images could be reconstructed at the same resolution across all three dimensions. In 2015, a group at Columbia University developed a method called SCAPE,\n\n        2\n\n which used an oblique sheet of light to scan and image a sample using a single lens assembly. SCAPE is even faster than earlier light sheet techniques, making it particularly useful for tracking rapid neuronal activity. Another set of innovations has to do with what the microscopes are looking at. All the methods we’ve discussed depend on fluorescent reporters — engineered proteins that fluoresce under certain conditions, such as the presence of a specific protein or the expression of a particular gene. In our case, that trigger is calcium. When a neuron fires, calcium ions flood into the cell, making calcium influx a reliable proxy for neuronal activity. The key breakthrough here was the development of the GCaMP6 family of reporters by a team at the Janelia Research Campus between 2013 and 2015. This new generation of calcium indicators were brighter and more sensitive than earlier versions, quickly becoming the go-to tool for imaging neuronal circuits in living organisms. While GCaMP6 revolutionized calcium-based imaging, even more precise measurements could come from fluorescent reporters that respond to voltage directly. These already exist for larger organisms and are actively being developed for use in C. elegans.Today, the combination of calcium imaging and microscopy techniques like DiSPIM and SCAPE means that we can see how neurons behave throughout the entire C. elegans brain — in real time. The next challenge is to actually do it. And to do it a lot. Our understanding of the C. elegans connectome has improved significantly since White’s groundbreaking work in 1986. White’s connectome was a mosaic of five individual worms. However, the same neuron in different animals might differ in size or capacity for electric charge. To fully understand the C. elegans brain and its operation during a broad range of behaviors, we need to collect data from thousands of individuals.There’s the question of what to do with the data once we have it. This is another area where recent advances –this time, in machine learning — make the process much more feasible. For all its biological complexity, the C. elegans brain still consists of just 300 neurons — tiny compared to state-of-the-art large language models. Using symbolic regression, a machine learning technique for discovering mathematical formulas that explain observed data, we can take our data on neuronal activity and use it to derive key parameters like capacitance and synaptic strength for every single neuron and every single neuronal connection. These equations would likely resemble the biophysical models that scientists have already derived from patch-clamp experiments, but inferred directly from whole-brain data.\n\t\t\t\t\t\t\t\t\t\t\tFish, flies, and beyond\n\n\t\t\t\t\t\t\t\t\t\t\tI don’t mean to suggest that building an accurate C. elegans simulation will be easy. There are many considerations that the technologies I’ve described may not account for, from extra-synaptic signalling to the role of specific neuron morphology (not to mention the fact that neurons and synapses change over the course of a nematode’s life). But with modern techniques, which continue to rapidly improve, I do believe that it is possible.And if we want to one day build simulations of larger animals — including humans — I also believe that it is necessary. The optical microscopy techniques that let us observe the neural activity of living organisms have one key limitation: depth. Light can only penetrate so far into tissue. With current techniques, that limit is roughly 750 microns, a bit less than a millimeter. To build an accurate whole brain simulation, we need activity data from a whole brain — which means that we’re currently limited to brains less than a millimeter deep. In other words, C. elegans, larval zebrafish, and fly brains are our only options. By investigating small organisms, we can develop new methods that allow us to predict neural activity by looking at the brain’s structure and other indirect forms of data. These techniques will make it possible for us to model more complex brains, including those that are too large for us to image their activity directly.My research focuses on creating a scientifically-grounded simulation of C. elegans by integrating these recently developed microscopy, fluorescent reporter, and machine learning methods into a cohesive pipeline and methodological framework. The idea is to create a proven simulation creation blueprint that can then be applied to more complex brains. But achieving a successful simulation of C. elegans would be a remarkable scientific accomplishment on its own. More importantly, it would help us begin to decipher how the structure of a brain relates to the dynamic processes unfolding within it. Over time, this understanding will open the doors to simulating more complex organisms, ultimately including humans. We have a long journey ahead of us, but now is the best time to begin — expeditiously, and with tractable, well-defined milestones along the way.\n\n    Sign up for our newsletter to get Asterisk’s latest interviews, essays, and more.\n\n        Subscribe\n\n    Dual-view Plane Illumination Microscopy\n\n                ↩\n\n    Swept, Confocally Aligned Planar Excitation\n\n                ↩\n\nMichael Skuhersky holds a PhD in neuroscience from MIT and is currently founding a nonprofit research institute focused on brain simulation.\n\nPublished March 2025\n\n\t\tShare with email\n\n\t\tShare on Twitter\n\n\t\tShare on Facebook\n\n\t\tShare on LinkedIn\n\n\t\t\tHave something to say? Email us at letters@asteriskmag.com.\n\nPrevious\n\t\t\t\t\tThe Unbearable Loudness of Chewing\n\n\t\t\t\t\t\t\tNextDeros and the Ur-Abduction\n\nFurther Reading\n\n\t\t\t\tMore:\n\t\t\t\t\t\t\t\t\tscience\n\t\t\t\t\t\t\t\t\ttechnology\n\n\t\t\t\t\t\tA Defense of Weird Research\n\n\t\t\t\t\t\t\t\t\tDeena Mousa\n\n\t\t\t\t\t\t\t\t\tLauren Gilbert\n\n\t\t\t\t\t\tAutomating Math\n\n\t\t\t\t\t\t\t\t\tAdam Marblestone\n\n\t\t\t\t\t\tCan We Build a Five Gigawatt Data Center?\n\n\t\t\t\t\t\t\t\t\tLynette Bye\n\n\t\t\t\t\t\tGreening the Solar System\n\n\t\t\t\t\t\t\t\t\tEdwin Kite\n\n\t\t\t\t\t\t\t\t\tRobin Wordsworth\n\n\t\t\t\t\t\tThe Case for Insect Consciousness\n\n\t\t\t\t\t\t\t\t\tBob Fischer\n\n\t\t\t\t\t\tThe Unbearable Loudness of Chewing\n\n\t\t\t\t\t\t\t\t\tJake Eaton\n\n\t\t\t\t\t\tYes, Shrimp Matter\n\n\t\t\t\t\t\t\t\t\tAndrés Jiménez Zorrilla\n\nManage Consent PreferencesStrictly Necessary CookiesAlways ActiveThese cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.Performance Cookies  Performance Cookies These cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site. All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.Functional Cookies  Functional Cookies These cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages. If you do not allow these cookies then some or all of these services may not function properly.Targeting Cookies  Targeting Cookies These cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\n\nBack ButtonPerformance Cookies  Search IconFilter IconClear checkbox label labelApply CancelConsent Leg.Interest checkbox label label checkbox label label checkbox label label\n\nClear checkbox label labelApply Cancel\n\nConsent Leg.Interest checkbox label label checkbox label label checkbox label label",
    "summary": {
      "en": "Scientists have been trying for over 25 years to create computer simulations of the simple brain of the nematode worm, C. elegans. Recent advancements in technology make this goal achievable and necessary. A successful simulation could lead to breakthroughs in understanding brain function and potentially aid in simulating larger brains, including humans.\n\nThe C. elegans brain consists of only 300 neurons, making it a good candidate for simulation. Past attempts have failed mainly due to a lack of detailed data on how neurons function and respond to stimuli. While researchers have mapped the connections between neurons (the connectome), they still need to understand the electrical properties and behaviors of these neurons for an accurate simulation.\n\nRecent developments in microscopy and machine learning have improved our ability to observe living neurons in real-time. These advancements allow researchers to gather more detailed data about neuronal activity, which is crucial for building an effective simulation. The goal is to integrate these new methods into a cohesive framework to simulate C. elegans accurately, which could eventually lead to simulating more complex brains.\n\nOverall, while challenges remain, the current technological landscape provides a promising opportunity to make significant progress in brain simulation research.",
      "ko": "과학자들은 25년 넘게 선충인 C. elegans의 간단한 뇌를 컴퓨터로 시뮬레이션하려고 노력해왔습니다. 최근 기술의 발전 덕분에 이 목표가 가능해지고 필요해졌습니다. 성공적인 시뮬레이션은 뇌 기능 이해에 큰 진전을 가져오고, 더 큰 뇌, 즉 인간의 뇌를 시뮬레이션하는 데도 도움이 될 수 있습니다.\n\nC. elegans의 뇌는 단 300개의 뉴런으로 구성되어 있어 시뮬레이션에 적합한 대상입니다. 과거의 시도들은 주로 뉴런이 어떻게 작동하고 자극에 반응하는지에 대한 상세한 데이터 부족으로 실패했습니다. 연구자들은 뉴런 간의 연결망인 커넥톰을 매핑했지만, 정확한 시뮬레이션을 위해서는 뉴런의 전기적 특성과 행동을 이해해야 합니다.\n\n최근 현미경 기술과 기계 학습의 발전은 살아있는 뉴런을 실시간으로 관찰하는 능력을 향상시켰습니다. 이러한 발전은 연구자들이 뉴런 활동에 대한 더 상세한 데이터를 수집할 수 있게 해주며, 이는 효과적인 시뮬레이션 구축에 매우 중요합니다. 연구자들은 이러한 새로운 방법들을 통합하여 C. elegans를 정확하게 시뮬레이션하는 일관된 프레임워크를 만드는 것을 목표로 하고 있으며, 이는 궁극적으로 더 복잡한 뇌의 시뮬레이션으로 이어질 수 있습니다.\n\n전반적으로 도전 과제가 남아 있지만, 현재의 기술 환경은 뇌 시뮬레이션 연구에서 중요한 진전을 이룰 수 있는 유망한 기회를 제공합니다.",
      "ja": "科学者たちは、線虫のシンプルな脳であるC. elegansのコンピュータシミュレーションを25年以上にわたって試みてきました。最近の技術の進歩により、この目標が達成可能かつ必要になっています。成功したシミュレーションは、脳の機能を理解するためのブレークスルーにつながり、さらには人間を含むより大きな脳のシミュレーションにも役立つ可能性があります。\n\nC. elegansの脳はわずか300個のニューロンで構成されており、シミュレーションに適した対象です。過去の試みは、ニューロンがどのように機能し、刺激に反応するかについての詳細なデータが不足していたために失敗しました。研究者たちはニューロン間の接続をマッピングしましたが、正確なシミュレーションを行うためには、これらのニューロンの電気的特性や挙動を理解する必要があります。\n\n最近の顕微鏡技術や機械学習の進展により、リアルタイムで生きたニューロンを観察する能力が向上しました。これにより、ニューロンの活動に関するより詳細なデータを収集できるようになり、効果的なシミュレーションを構築するために重要です。研究者たちは、これらの新しい方法を統合してC. elegansを正確にシミュレーションするための一貫した枠組みを作ることを目指しています。これが最終的には、より複雑な脳のシミュレーションにつながる可能性があります。\n\n全体として、課題は残っていますが、現在の技術環境は脳シミュレーション研究において重要な進展を遂げるための有望な機会を提供しています。"
    }
  },
  {
    "id": "d60819311b10fd24",
    "title": {
      "en": "The state of binary compatibility on Linux and how to address it",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://jangafx.com/insights/linux-binary-compatibility",
    "score": 184,
    "by": "generichuman",
    "time": 1743546166,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "10e4994d548a520c",
    "title": {
      "en": "SSLyze – SSL configuration scanning library and CLI tool",
      "ko": "SSL 구성 스캐너",
      "ja": "SSLyzeでSSL診断"
    },
    "type": "story",
    "url": "https://github.com/nabla-c0d3/sslyze",
    "score": 42,
    "by": "Brysonbw",
    "time": 1743552211,
    "content": "SSLyze\n\nSSLyze is a fast and powerful SSL/TLS scanning tool and Python library.\nSSLyze can analyze the SSL/TLS configuration of a server by connecting to it, in order to ensure that it uses strong\nencryption settings (certificate, cipher suites, elliptic curves, etc.), and that it is not vulnerable to known TLS\nattacks (Heartbleed, ROBOT, OpenSSL CCS injection, etc.).\nKey features\n\nFocus on speed and reliability: SSLyze is a battle-tested tool that is used to reliably scan hundreds of thousands\nof servers every day.\nEasy to operationalize: SSLyze can be directly run from CI/CD, in order to continuously check a server against\nMozilla's recommended TLS configuration.\nFully documented Python API to run scans directly from any\nPython application, such as a function deployed to AWS Lambda.\nSupport for scanning non-HTTP servers including SMTP, XMPP, LDAP, POP, IMAP, RDP, Postgres and FTP servers.\nResults of a scan can easily be saved to a JSON file for later processing.\nAnd much more!\n\nQuick start\nOn Windows, Linux (x86 or x64) and macOS, SSLyze can be installed directly via pip:\n$ pip install --upgrade pip setuptools wheel\n$ pip install --upgrade sslyze\n$ python -m sslyze www.yahoo.com www.google.com \"[2607:f8b0:400a:807::2004]:443\"\n\nIt can also be used via Docker:\n$ docker run --rm -it nablac0d3/sslyze:6.1.0 www.google.com\n\nLastly, a pre-compiled Windows executable can be downloaded from the Releases\npage.\nPython API Documentation\nA sample script describing how to use the SSLyze's Python API is available at ./api_sample.py.\nFull documentation for SSLyze's Python API is available here.\nUsage as a CI/CD step\nBy default, SSLyze will check the server's scan results against Mozilla's recommended \"intermediate\" TLS\nconfiguration, and will return a non-zero exit code if the server\nis not compliant.\n$ python -m sslyze mozilla.com\n\nChecking results against Mozilla's \"intermediate\" configuration. See https://ssl-config.mozilla.org/ for more details.\n\nmozilla.com:443: OK - Compliant.\n\nThe Mozilla configuration to check against can be configured via --mozilla_config={old, intermediate, modern}:\n$ python -m sslyze --mozilla_config=modern mozilla.com\n\nChecking results against Mozilla's \"modern\" configuration. See https://ssl-config.mozilla.org/ for more details.\n\nmozilla.com:443: FAILED - Not compliant.\n    * certificate_types: Deployed certificate types are {'rsa'}, should have at least one of {'ecdsa'}.\n    * certificate_signatures: Deployed certificate signatures are {'sha256WithRSAEncryption'}, should have at least one of {'ecdsa-with-SHA512', 'ecdsa-with-SHA256', 'ecdsa-with-SHA384'}.\n    * tls_versions: TLS versions {'TLSv1.2'} are supported, but should be rejected.\n    * ciphers: Cipher suites {'TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384', 'TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256', 'TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256'} are supported, but should be rejected.\n\nThis can be used to easily run an SSLyze scan as a CI/CD step.\nDevelopment environment\nTo setup a development environment:\n$ pip install --upgrade pip setuptools wheel\n$ pip install -e .\n$ pip install -r requirements-dev.txt\n\nThe tests can then be run using:\n$ invoke test\n\nLicense\nCopyright (c) 2025 Alban Diquet\nSSLyze is made available under the terms of the GNU Affero General Public License (AGPL). See LICENSE.txt for details and exceptions.",
    "summary": {
      "en": "**Summary of SSLyze**\n\nSSLyze is a fast and effective tool for scanning SSL/TLS configurations on servers. It checks for strong encryption settings and identifies vulnerabilities to known attacks, such as Heartbleed.\n\n**Key Features:**\n- **Speed and Reliability:** SSLyze is widely used to scan many servers daily.\n- **Easy Integration:** It can be easily used in CI/CD pipelines to monitor server compliance with Mozilla's TLS configuration recommendations.\n- **Python API:** Users can run scans from Python applications, with documentation available.\n- **Support for Various Protocols:** It can scan non-HTTP servers like SMTP, LDAP, and more.\n- **Results Storage:** Scan results can be saved in JSON format.\n\n**Quick Start Instructions:**\n- Install SSLyze on Windows, Linux, or macOS using pip, or run it via Docker.\n- A pre-compiled Windows executable is also available.\n\n**Using SSLyze in CI/CD:**\n- By default, it checks against Mozilla's \"intermediate\" TLS settings and returns a status code based on compliance.\n- Users can specify different Mozilla configurations for more stringent checks.\n\n**Development Setup:**\n- To set up a development environment, install the necessary requirements and run tests easily.\n\n**License:**\nSSLyze is licensed under the GNU Affero General Public License (AGPL).",
      "ko": "SSLyze는 서버의 SSL/TLS 설정을 빠르고 효과적으로 검사하는 도구입니다. 이 도구는 강력한 암호화 설정을 확인하고 Heartbleed와 같은 알려진 공격에 대한 취약점을 식별합니다.\n\nSSLyze의 주요 특징은 속도와 신뢰성입니다. 이 도구는 매일 많은 서버를 검사하는 데 널리 사용됩니다. 또한 CI/CD 파이프라인에 쉽게 통합할 수 있어, 서버가 Mozilla의 TLS 설정 권장 사항을 준수하는지 모니터링할 수 있습니다. 사용자는 Python 애플리케이션에서 스캔을 실행할 수 있으며, 관련 문서도 제공됩니다. SSLyze는 SMTP, LDAP 등과 같은 비HTTP 서버도 검사할 수 있는 다양한 프로토콜을 지원합니다. 스캔 결과는 JSON 형식으로 저장할 수 있습니다.\n\nSSLyze를 시작하려면 Windows, Linux 또는 macOS에서 pip를 사용해 설치하거나 Docker를 통해 실행할 수 있습니다. 미리 컴파일된 Windows 실행 파일도 제공됩니다.\n\nCI/CD에서 SSLyze를 사용할 경우, 기본적으로 Mozilla의 \"중간\" TLS 설정을 기준으로 검사하며, 준수 여부에 따라 상태 코드를 반환합니다. 사용자는 더 엄격한 검사를 위해 다른 Mozilla 설정을 지정할 수 있습니다.\n\n개발 환경을 설정하려면 필요한 요구 사항을 설치하고 쉽게 테스트를 실행할 수 있습니다.\n\nSSLyze는 GNU Affero General Public License(AGPL) 하에 라이선스가 부여됩니다.",
      "ja": "SSLyzeは、サーバーのSSL/TLS設定をスキャンするための迅速かつ効果的なツールです。このツールは、強力な暗号化設定を確認し、Heartbleedのような既知の攻撃に対する脆弱性を特定します。\n\nSSLyzeの主な特徴には、スピードと信頼性があります。多くのサーバーを日々スキャンするために広く利用されています。また、CI/CDパイプラインに簡単に統合でき、MozillaのTLS設定に準拠しているかを監視するのに役立ちます。Python APIも提供されており、ユーザーはPythonアプリケーションからスキャンを実行できるほか、ドキュメントも用意されています。さらに、SMTPやLDAPなどの非HTTPサーバーもスキャン可能です。スキャン結果はJSON形式で保存できます。\n\nSSLyzeのインストールは、Windows、Linux、macOSでpipを使って行うか、Dockerを介して実行できます。また、事前にコンパイルされたWindows用の実行ファイルも利用可能です。\n\nCI/CDでの使用においては、デフォルトでMozillaの「中間」TLS設定に対してチェックを行い、準拠状況に基づいてステータスコードを返します。ユーザーは、より厳格なチェックのために異なるMozillaの設定を指定することもできます。\n\n開発環境を設定するには、必要な要件をインストールし、簡単にテストを実行できます。\n\nSSLyzeは、GNU Affero General Public License（AGPL）の下でライセンスされています。"
    }
  },
  {
    "id": "865daef6d59c5341",
    "title": {
      "en": "AR Computers to Terminate Eyestrain and Myopia",
      "ko": "눈 건강 혁명! AR 컴퓨터",
      "ja": "ARで目の疲れ解消！"
    },
    "type": "story",
    "url": "https://eyewiki.org/AR_Computers_To_Terminate_Eyestrain_And_Myopia",
    "score": 43,
    "by": "plun9",
    "time": 1743550271,
    "content": "AR Computers To Terminate Eyestrain And Myopia\n\n\t\t\t\t\t\tFrom EyeWikiJump to:navigation, search\n\nAll content on Eyewiki is protected by copyright law and the Terms of Service. This content may not be reproduced, copied, or put into any artificial intelligence program, including large language and generative AI models, without permission from the Academy.\n\nArticle initiated by:\n\nChing Lai Tsai, MD\n\nAll contributors:\n\n Benjamin Buckner, MD,Vandana Reddy, MD,Ching Lai Tsai, MD,Nichelle Warren MD\n\nAssigned editor:\n\n Nichelle Warren MD\n\nReview:\n\nAssigned status Up to Date\n\nby Nichelle Warren MD on December 28, 2024.\n\nadd\n\nContributing Editors:\n\nadd\n\nContents\n\n1 Introduction\n2 Factors contributing to eyestrain and myopia\n3 Disease prevention\n\n3.1 Termination of asthenopia\n3.2 Prevention of myopia\n\n4 Further advantages\n\n4.1 No reading glasses required\n4.2 No dizziness\n4.3 No back pain\n4.4 No neck stiffness\n4.5 No vertebrae overlap\n4.6 Less physical fatigue\n4.7 Dynamic reading instead of static reading\n4.8 No need to turn on the lights\n4.9 No glare\n4.10 No isolation\n4.11 No physical screen needed\n4.12 Dual-display\n4.13 Relieve eye fatigue\n4.14 Unaffected by vehicle vibration\n4.15 Sunlight instead of artificial light\n4.16 Outdoors\n4.17 Environmentallyfriendly\n\n5 Conclusion\n6 References\n\nIntroduction\nIn humans, prolonged contraction of the ciliary and medial rectus muscles during close reading will result in eye strain.\n\nOn the other hand, eye strain will not occur if the ciliary and medial rectus muscles do not contract during close reading.\nAbout 2cm in front of the eye, the Near-Eye Display (NED)[1] technology of Augmented Reality smart glasses (AR glasses) projects computer generated images/informations(CGIs) directly onto the retina, and this provides a passive way for our eyes to acquire information.\n\n  Figure 1. Projection of CGI onto the retina\nFactors contributing to eyestrain and myopia\nIn the medical community, the main factors considered to causing eye fatigue and myopia are as follows:\n1. Long-term contraction of the ciliary muscle.\n2. Long-term contraction of the medial rectus muscle.\n3. Insufficient exposure of the retina to sunlight.[2]\n4. The eye not exhibiting peripheral myopia.[3]\n5. Peripheral visual field deprivation.[4]\n\nDisease prevention\n\"Evidence is mounting that myopia is growing around the world, with a recent study estimating that on average, 30% of the world is currently myopic and by 2050, almost 50% will be myopic, that’s a staggering 5 billion people.\"\nhttps://myopiainstitute.org/myopia/\nAugmented reality smart glasses have the potential to play an important role in the prevention of asthenopia and myopia.\n\nTermination of asthenopia\nWhen using AR glasses, its translucent display allows your eyes to receive two light sources at the same time, one is ambient light from the real world, and the other is projected by the NED.\nWhen the display becomes opaque, light from the real world will not penetrate, and your eye will only receive the light from the NED.\n\nSimilar in principle to an ophthalmoscope shining light into the eye, the NED of AR glasses actively projects CGI onto the retina.\n①. Once any refractive errors are corrected, people of any age can receive the CGI clearly via the retina; this indicates that the light projected from the NED is indeed parallel light.\n②. Once the refractive error is corrected, the CGI projected through the parallel light is naturally focused on the etina(=macula) without any accommodation.\n③. By extension of the accommodation-convergence reflex, no convergence.\nTherefore, there is no contraction of the ciliary and medial rectus muscle.\nSo by using a modified augmented reality glasses, humans can read at close range with both the ciliary and medial rectus muscles relaxed, and the eyes will never get tired.\n\nPrevention of myopia\nAR glasses can be turned into AR computers by appending a piece of opaque material on the front of the screen to turn the see-through display into a non see-through one and installing the software required in its host.\n\nThus, an AR computer can be called an ophthalmoscope with a computer host.\n\n  Figure 2. An augmented reality personal computer (An AR PC)\n\n  Figure 3. Attaching an opaque material on the front of the screen\n\n• The AR computer is equipped with a light-transmittable part around the opaque display. The opaque display allows the user to face the sun and use sunlight as the background light source. The opaque display protects the eyeball and the macula, while the light-transmittable part allows the peripheral retina to come into contact with sunlight.\n\n• The AR computer can be equipped with convex lenses around the opaque display. The convex lens can shorten the focal length of the light around the opaque display (i.e. the macula area) and change the light that is originally focused on the outside of the retina to the inside, turning the relative peripheral hyperopia into peripheral myopia.  Figure 4. An AR computer without convex lens  Figure 5. An AR computer with convex lens\n\n•  With the head raised, the light-transmittable part of an AR computer provides a wide field of view, eliminating the phenomenon where the peripheral visual field is deprived when reading with the head down.\n\n  Figure 6. No peripheral visual field deprivation\n\nThus, the AR computer can simultaneously overcome all the major factors contributing to myopia.\n\n  An AR computer overcomes currently known factors\nFurther advantages\nThe AR computer can complete all tasks a traditional PC is capable of, such as editing documents, browsing the web, emails, media playing etc. It also has the unique ability of AR glasses.\n\nNo reading glasses required\nAs long as the refractive error is corrected, the parallel light will naturally focus on the retina, people of any age can get a clear picture, so the elderly do not need reading  glasses when using AR computers.\n\nNo dizziness\nAs both the ciliary and medial rectus muscles are relaxed, there is no vergence-accommodation conflict (VAC), so there is neither dizziness nor VR motion sickness experienced by the user.\n\nNo back pain\nYou can take a supine position while using the AR computer. Being able to lie down means you can relax most of the muscles in your body and the intervertebral discs won't be compressed, so you won't have lower back pain.\n\nNo neck stiffness\nThe virtual image moves with the line of sight, so users can move their head and neck freely without having to keep looking down.  Therefore, the shoulder and neck will not be stiff.\n\nNo vertebrae overlap\nWhen using the AR computer lying down, the spine can be stretched out, the vertebrae no longer overlap each other, and the intervertebral discs are not compressed.\n\nLess physical fatigue\nBeing able to lie down and use the AR computer means that most of the muscles in your body, including the ciliary and medial rectus muscles, are in a relaxed state, so your body will no longer suffer from soreness and fatigue and also save more energy than any other working position.\n\nDynamic reading instead of static reading\nThe light-transmitting part allows users to see the surrounding environment when using the AR computer, so you can change your posture and move your body at any time.  Therefore, it encourages dynamic reading rather than static reading to avoid complications of a sedentary lifestyle.  Users can even move around within the confines of a secure environment.\n\nNo need to turn on the lights\nWhen we are using AR computers during the day, as long as we are facing a sunny place, we don’t need to turn on the lights.\n\nNo glare\nThe non see-through display blocks out light sources from the real world, and the retina only receives parallel light from the NED technology. So generally speaking there is no glare.\n\nNo isolation\nThe light-transmittable part of the AR computer allows users to contact the surroundings, avoiding isolation from the environment and other users.\n\nNo physical screen needed\nWhen CGI is projected onto the retina, a virtual screen will appear in front of you and move freely with your line of sight. You can put it on the wall, on the ceiling, in mid-air, whereever you like.\n\nDual-display\nA binocular AR computer is equipped with two screens.\nInstead of sharing a screen, each eye has its own screen.\n\nRelieve eye fatigue\nOnce the refractive error is corrected, both the ciliary and medial rectus muscles are relaxed. Therefore, the use of AR computer may relieve eye fatigue caused by prolonged contraction of the ciliary and medial rectus muscles due to long-term close reading.\n\nUnaffected by vehicle vibration\nThe display of the AR Computer moves synchronously with the user’s eyes, and the images remain clear and stable even on moving vehicles.\n\nSunlight instead of artificial light\nThe opaque display of the AR computer protects the eyeball and the macula, allowing the user to face the sun with sunlight as the background light source.\n\nOutdoors\nThe opaque display of the AR computer  protects the eyeball and the macula, keeps the clarify and contrast of the CGI and encourages users to go outdoors,e.g. in the woods, by the river, etc.\n\nEnvironmentallyfriendly\nWhen AR Computer users go outdoors or face the outside, the use of artificial light is reduced, saving energy and being environmentally friendly.\n\nConclusion\nEye fatigue and myopia have endangered human beings for thousands of years, and despite various treatments, the myopic population continues to rise.\nThe NED technology provides a passive way for our eyes to receive informations which is completely different from the way humans have been using their eyes to actively find objects and read information since ancient time. This has revolutionized the mechanism for reading at close range, and is undoubtedly worthy of further exploration.\n\nAlthough its results still need to be supported by clinical trial data, the fact that AR computer overcomes the currently known factors of myopia is promising.\n\nReferences\n\n↑ http://www.kessleroptics.com/portfolio/near-to-eye-displays\n\n↑ Erica G. Landis, Victoria Yang, Dillon M. Brown, Machelle T. Pardue, Scott A. Read; Dim Light Exposure and Myopia in Children. Invest. Ophthalmol. Vis. Sci. 2018;59(12):4804-4811. doi: https://doi.org/10.1167/iovs.18-24415.\n\n↑ Alexandra Benavente-Pérez, Ann Nour, David Troilo; Axial Eye Growth and Refractive Error Development Can Be Modified by Exposing the Peripheral Retina to Relative Myopic or Hyperopic Defocus. Invest. Ophthalmol. Vis. Sci. 2014;55(10):6765-6773. doi: https://doi.org/10.1167/iovs.14-14524.\n\n↑ Smith EL 3rd, Hung LF, Arumugam B. Visual regulation of refractive development: insights from animal studies. Eye (Lond). 2014 Feb;28(2):180-8. doi: 10.1038/eye.2013.277. Epub 2013 Dec 13. PMID: 24336296; PMCID: PMC3930279.\n\nRetrieved from \"https://eyewiki.org/w/index.php?title=AR_Computers_To_Terminate_Eyestrain_And_Myopia&oldid=114410\"\n\n\tThe Academy uses cookies to analyze performance and provide relevant personalized content to users of our website.Learn moreAccept\n\n\t\t\t\t\tCategories: ArticlesRefractive Management/Intervention",
    "summary": {
      "en": "**Summary: AR Computers to Alleviate Eyestrain and Myopia**\n\nAugmented Reality (AR) smart glasses may help reduce eyestrain and myopia (nearsightedness). Prolonged close reading causes muscle strain in the eyes, leading to discomfort and vision problems. AR glasses project images directly onto the retina, allowing the eyes to focus without straining.\n\n**Key Factors Causing Eyestrain and Myopia:**\n1. Long-term muscle contraction in the eyes.\n2. Lack of sunlight exposure on the retina.\n3. Limited peripheral vision.\n\n**Benefits of AR Glasses:**\n- **Prevents Eyestrain**: Allows the use of both ambient and projected light, reducing muscle contraction.\n- **Reduces Myopia**: With modifications, AR glasses can create a computer display that uses sunlight and protects the eyes while still providing clear images.\n- **Comfortable Use**: Users can lie down, reducing back and neck pain, and avoid dizziness since the eye muscles remain relaxed.\n- **Dynamic Reading**: Users can move freely without being confined to a static position, promoting a more active lifestyle.\n- **Environmentally Friendly**: Encourages outdoor use and reduces reliance on artificial lighting.\n\n**Conclusion**: AR technology offers a promising new approach to addressing eyestrain and myopia, although further clinical studies are needed to confirm its effectiveness.",
      "ko": "증강 현실(AR) 스마트 안경이 눈의 피로와 근시를 줄이는 데 도움을 줄 수 있습니다. 가까운 거리에서 오랜 시간 독서를 하면 눈의 근육이 긴장하게 되어 불편함과 시력 문제를 초래할 수 있습니다. AR 안경은 이미지를 망막에 직접 투사하여 눈이 긴장하지 않고도 초점을 맞출 수 있게 합니다.\n\n눈의 피로와 근시를 유발하는 주요 요인은 다음과 같습니다. 첫째, 눈의 근육이 장기간 수축 상태에 있는 것입니다. 둘째, 망막에 햇빛이 부족한 경우입니다. 셋째, 주변 시야가 제한되는 것입니다.\n\nAR 안경의 장점은 여러 가지가 있습니다. 첫째, 눈의 피로를 예방합니다. 주변 빛과 투사된 빛을 함께 사용하여 근육의 긴장을 줄입니다. 둘째, 근시를 감소시킬 수 있습니다. AR 안경은 햇빛을 활용하는 컴퓨터 화면을 생성하여 눈을 보호하면서도 선명한 이미지를 제공합니다. 셋째, 편안한 사용이 가능합니다. 사용자는 누워서 사용할 수 있어 허리와 목의 통증을 줄이고, 눈의 근육이 이완되어 어지러움을 피할 수 있습니다. 넷째, 동적인 독서가 가능합니다. 사용자는 정적인 자세에 구애받지 않고 자유롭게 움직일 수 있어 더 활동적인 생활을 촉진합니다. 마지막으로, 환경 친화적입니다. 야외에서의 사용을 장려하고 인공 조명에 대한 의존도를 줄입니다.\n\nAR 기술은 눈의 피로와 근시 문제를 해결하는 새로운 접근 방식을 제시하지만, 그 효과를 확인하기 위해서는 추가적인 임상 연구가 필요합니다.",
      "ja": "拡張現実（AR）スマートグラスは、目の疲れや近視を軽減するのに役立つ可能性があります。長時間の近くでの読書は、目の筋肉に負担をかけ、不快感や視力の問題を引き起こします。ARグラスは画像を網膜に直接投影するため、目は無理なく焦点を合わせることができます。\n\n目の疲れや近視を引き起こす主な要因には、目の筋肉の長時間の収縮、網膜への日光の不足、周辺視野の制限があります。\n\nARグラスの利点には、目の疲れを防ぐことが挙げられます。周囲の光と投影された光の両方を使用できるため、筋肉の収縮を減らします。また、ARグラスは改良を加えることで、太陽光を利用したコンピュータ表示を作成し、目を保護しながらも鮮明な画像を提供することができます。使用者は横になることができるため、背中や首の痛みを軽減し、目の筋肉がリラックスした状態を保つことでめまいを避けることができます。さらに、使用者は静止した位置に制約されることなく自由に動けるため、よりアクティブなライフスタイルを促進します。ARグラスは屋外での使用を促し、人工照明への依存を減らすため、環境にも優しいです。\n\nAR技術は目の疲れや近視に対処する新しいアプローチを提供する可能性がありますが、その効果を確認するためにはさらなる臨床研究が必要です。"
    }
  },
  {
    "id": "f03c8aec4c513998",
    "title": {
      "en": "How AI is creating a rift at McKinsey, Bain, and BCG",
      "ko": "AI가 만든 경영 컨설팅의 갈등",
      "ja": "AIが生む格差"
    },
    "type": "story",
    "url": "https://the-ken.com/story/bcg-and-mckinsey-sell-speed-as-ai-shakes-up-consulting-so-why-arent-consultants-buying-it/",
    "score": 90,
    "by": "rustoo",
    "time": 1743526984,
    "content": "Dang, you’re out of gift subscriptions\n                             We currently limit the number of free gift subscriptions per subscriber. You’ll get more when you renew your subscription.\n                            In the meantime, if you want to tell someone about The Ken, gift them a story.\n\n                              Not now, thanks.\n\nYour email is on its way!\n\n                To track your gifted stories, go to your Account Settings\n                We’ve emailed your gift link. Want us to send some more?\n\n                Yes, Send more\n\n                 0  of 0  Subscription available\n\n                Not now, thanks\n\nOn the fast track\n\n            How AI is creating a rift at McKinsey, Bain, and BCG\n\n            By\n                            Abhirami G\n\n            Instead of making consultants’ lives easier, generative AI has led to shorter deadlines and vanishing creativity\n\n              1 Apr 2025/11 min  read\n\nAbhirami writes about how the rise of AI is affecting society in varied and unexpected ways.\n\n          Read Summary\n\n                            Clients’ increasing access to AI tools is transforming the way consulting firms operate\n                            Apart from moving towards an implementation-based model, the Big 3—McKinsey, BCG and Bain—are now incorporating AI in a major chunk of their projects\n                            Consultants at the juniormost level are encouraged to use AI for research, strategy and ideation. Albeit on considerably shortened timelines\n                            Thoughts on AI vary across the firms’ hierarchy—while managers and partners love it, consultants who are closer to the ground view things differently\n\n                    Enter your email address to receive a daily summary of all our stories.\n\n                “Why are you solving this problem from first principles? You should’ve used ChatGPT to help you.”\nA consultant at Boston Consulting Group (BCG), one of the “Big 3s” in the sector, recounted their manager’s reaction when they turned in a preliminary report on a certain topic a few months ago. “The funniest part is that I had used ChatGPT to solve the problem. I just hadn’t told my manager,” they said, not wanting to be named as they weren’t authorised to speak with the media.\nWelcome to management consulting in 2025, where much of what you see on a consultant’s deck has been outsourced to generative AI tools—including, funnily enough, the deck itself. You’d think firms would be wary of this new technology that threatens to make them obsolete. After all, AI has considerablynarrowedInnoleadThe End of Consulting as We Know it: Client Power and the AI Revolution the knowledge gap between firms and clients, as one McKinsey consultant admitted. Instead, firms are enthusiastically adopting AI across the board.\nThose on the “frontlines” of the industry’s AI pivot are the consultants—the junior-most employees at firms like BCG tasked with research and number crunching. And it seems like they’re not happy about it. Instead of making their life easier, AI has resulted in drastically reduced timelines and ebbing appreciation for creativity.\n“A lot of senior partners and partners think of AI as a magic bullet,” said a former manager at Bain, another of the Big 3 consultancies. For instance, a senior partner who assigned his juniors to work on a client assessmentclient assessmenta systematic process of gathering and analysing information about a client's business, goals, and challenges to determine the best course of action thought they could use an AI chatbot to parse data and ready the assessment in a day, said the former manager. The same thing would have earlier taken two to three weeks.\nBut here’s the thing: even with AI, a day is an unrealistic deadline for an assignment like this. So, who bears the brunt of these outsized expectations? The runt of the litter: the consultants. When rushed, they end up turning in subpar, AI-generated copy that sounds nice but doesn’t mean much.\n\n    Credits\n\n          Written by\n                Abhirami G\n\n          Edited by\n                Neha Mehrotra\n\n\t\t\t\t\tGift\n\n\t\t\t\t\t\tPremium Benefit\n\n\t\t\tShare\n\n\t\t\tShare this article with your network\n\t\t\tSend the article link to friends or colleagues who might find this story interesting or insightful.\n\t\t\thttps://the-ken.com/story/bcg-and-mckinsey-sell-speed-as-ai-shakes-up-consulting-so-why-arent-consultants-buying-it/\n\n\t\t\t\t\tLink Copied!\n\n\t\t\tSend the article link to friends or colleagues who might find this story interesting or insightful.\n\n\t\t\thttps://the-ken.com/story/bcg-and-mckinsey-sell-speed-as-ai-shakes-up-consulting-so-why-arent-consultants-buying-it/\n\n\t\t\t\t\tLink Copied!\n\n                Topics\n\n                    Bain & Co\n                    Boston Consulting Group (BCG)\n                    generative AI\n                    large language model\n                    Management Consulting\n                    McKinsey & Company\n\nCredits\n\n          Written by\n                Abhirami G\n\n          Edited by\n                Neha Mehrotra\n\n\t\t\t\t\tGift\n\n\t\t\t\t\t\tPremium Benefit\n\n\t\t\tShare\n\n\t\t\tShare this article with your network\n\t\t\tSend the article link to friends or colleagues who might find this story interesting or insightful.\n\t\t\thttps://the-ken.com/story/bcg-and-mckinsey-sell-speed-as-ai-shakes-up-consulting-so-why-arent-consultants-buying-it/\n\n\t\t\t\t\tLink Copied!\n\n\t\t\tSend the article link to friends or colleagues who might find this story interesting or insightful.\n\n\t\t\thttps://the-ken.com/story/bcg-and-mckinsey-sell-speed-as-ai-shakes-up-consulting-so-why-arent-consultants-buying-it/\n\n\t\t\t\t\tLink Copied!\n\n                Topics\n\n                    Bain & Co\n                    Boston Consulting Group (BCG)\n                    generative AI\n                    large language model\n                    Management Consulting\n                    McKinsey & Company\n\nThis story is only available to subscribers of The Ken.\n\t\t\t\t\t\t\t\t\tAlready a subscriber?  Log in\n\n\t\t\t\t\t\t\t\t\tThe only business subscription you need\n\t\t\t\t\t\t\t\t\tUnrivaled analysis and powerful stories about businesses from award-winning journalists. Read by 5,00,000+ subscribers globally who want to be prepared for what comes next.\n\n\t\t\t\t\t\t\t\t\t\tBasic\n\n\t\t\t\t\t\t\t\t\t\t\t$  120 / year\n\n\t\t\t\t\t\t\t\t\t\tMOST POPULAR\n\n\t\t\t\t\t\t\t\t\t\tPremium\n\t\t\t\t\t\t\t\t\t\t $  199/ year\n\n\t\t\t\t\t\t\t\t\t\tPremium Duo\n\t\t\t\t\t\t\t\t\t\t $  299 / year\n\n\t\t\t\t\t\t\t                       Only Free\n\t\t\t\t\t\t\t                        + Premium Podcasts\n\n\t\t\t\t\t\t\t                          Only Free Podcasts\n\t\t\t\t\t\t\t                          Access to podcasts covering the hottest business and startup trends in India\n\n\t\t\t\t\t\t\t                      Partner Account Access\n\n\t\t\t\t\t\t\t                          Partner Account Access\n\t\t\t\t\t\t\t                          Get two independent Premium accounts\n\n\t\t\t\t\t\t\t                      Gift a 1 Year Subscription for Free\n\n\t\t\t\t\t\t\t                          1+1 subscription\n\t\t\t\t\t\t\t                          Gift a subscription to The Ken to anyone you like\n\n\t\t\t\t\t\t\t                      App Access\n\n\t\t\t\t\t\t\t                      iPad App Access\n\n\t\t\t\t\t\t\t                        Priority Access\n\n\t\t\t\t\t\t\t                      To new product offerings, features, community features, and events\n\n\t\t\t\t\t\t\t                          \tDaily Long-form Stories\n\n\t\t\t\t\t\t\t                              Daily Long-form Stories\n\t\t\t\t\t\t\t                              Daily long-form stories from India’s startup ecosystem, internet economy, and publicly-listed tech and consumer companies.\n\n\t\t\t\t\t\t\t                          \tOnly Subscriber\n\t\t\t\t\t\t\t                        \t+ Premium\n\t\t\t\t\t\t\t                            Newsletters\n\n\t\t\t\t\t\t\t                              Only Subscriber Newsletters\n\t\t\t\t\t\t\t                              Topical and sharp weekly industry newsletters covering e-commerce, retail, fintech, personal finance, edtech. Fresh and original weekend newsletters like The Nutgraf and First Principles.\n\n\t\t\t\t\t\t\t                            Visual Stories\n\n\t\t\t\t\t\t\t                           Only Free\n\t\t\t\t\t\t\t                            + Premium Podcasts\n\n\t\t\t\t\t\t\t                              Free + Premium Podcasts\n\t\t\t\t\t\t\t                              Covering the most interesting workplaces, career and business trends, along with candid conversations with founders and business leaders.\n\n\t\t\t\t\t\t\t                            2 Years\n\n\t\t\t\t\t\t\t                          Archive access to the last two years of stories\n\n\t\t\t\t\t\t\t                            5\n\n\t\t\t\t\t\t\t                          Premium story gift credits per month\n\n\t\t\t\t\t\t\t                            3\n\n\t\t\t\t\t\t\t                          Monthly subscription gift credits per year\n\n\t\t\t\t\t\t\t                          2\n\n\t\t\t\t\t\t\t                          Upto two devices allowed\n\n\t\t\t\t\t\t\t\t\t\t\t\tSubscribe to Basic - $  120 / year\n\n\t\t\t\t\t\t\t\t\t\tCompare all plans\n\n\t\t\t\t\t\t\t                        Free + Premium Podcasts\n\n\t\t\t\t\t\t\t                          Free + Premium Podcasts\n\t\t\t\t\t\t\t                          Access to our entire collection of premium podcasts covering the most interesting business, career, and startup trends in India\n\n\t\t\t\t\t\t\t                        Partner Account Access\n\n\t\t\t\t\t\t\t                          Partner Account Access\n\t\t\t\t\t\t\t                          Get two independent Premium accounts\n\n\t\t\t\t\t\t\t                      Gift a 1 Year Subscription for Free\n\n\t\t\t\t\t\t\t                          1+1 subscription\n\t\t\t\t\t\t\t                          Gift a subscription to The Ken to anyone you like\n\n\t\t\t\t\t\t\t                      App Access\n\n\t\t\t\t\t\t\t                      iPad App Access\n\n\t\t\t\t\t\t\t                      Priority Access\n\n\t\t\t\t\t\t\t                      To new product offerings, features, community features, and events\n\n\t\t\t\t\t\t\t                            Daily Long-form Stories\n\n\t\t\t\t\t\t\t                              Daily Long-form Stories\n\t\t\t\t\t\t\t                              Daily long-form stories from India’s startup ecosystem, internet economy, and publicly-listed tech and consumer companies.\n\n\t\t\t\t\t\t\t                            Subscriber + Premium Newsletters\n\n\t\t\t\t\t\t\t                              Subscriber + Premium Newsletters\n\t\t\t\t\t\t\t                              Topical and sharp weekly industry newsletters covering e-commerce, retail, fintech, personal finance, edtech. Fresh and original weekend newsletters like The Nutgraf and First Principles.\n\n\t\t\t\t\t\t\t                            Visual Stories\n\n\t\t\t\t\t\t\t                              Visual stories\n\t\t\t\t\t\t\t                              Understand business the way it’s meant to be seen, through charts, graphs, and infographics.\n\n\t\t\t\t\t\t\t                          Free + Premium Podcasts\n\n\t\t\t\t\t\t\t                              Free + Premium Podcasts\n\t\t\t\t\t\t\t                              Access to the collection of premium podcasts that cover the most interesting workplaces, career, and business trends, along with candid conversations with founders and business leaders.\n\n\t\t\t\t\t\t\t                            Unlimited\n\n\t\t\t\t\t\t\t                          Access to all The Ken's Archives\n\n\t\t\t\t\t\t\t                            10\n\n\t\t\t\t\t\t\t                          Premium story gift  credits per month\n\n\t\t\t\t\t\t\t                            5\n\n\t\t\t\t\t\t\t                          Monthly subscription gift credits per year\n\n\t\t\t\t\t\t\t                          3\n\n\t\t\t\t\t\t\t                          Upto three devices allowed\n\n\t\t\t\t\t\t\t\t\t\t\t\tSubscribe to Premium - $  199 / year\n\n\t\t\t\t\t\t\t\t\t\tCompare all plans\n\n\t\t\t\t\t\t\t                      Free + Premium Podcasts\n\n\t\t\t\t\t\t\t                          Free + Premium Podcasts\n\t\t\t\t\t\t\t                          Access to our entire collection of premium podcasts covering the most interesting business, career, and startup trends in India\n\n\t\t\t\t\t\t\t                        Partner Account Access\n\n\t\t\t\t\t\t\t                          Partner Account Access\n\t\t\t\t\t\t\t                          Get two independent Premium accounts\n\n\t\t\t\t\t\t\t                      Gift a 1 Year Subscription for Free\n\n\t\t\t\t\t\t\t                          1+1 subscription\n\t\t\t\t\t\t\t                          Gift a subscription to The Ken to anyone you like\n\n\t\t\t\t\t\t\t                      App Access\n\n\t\t\t\t\t\t\t                      iPad App Access\n\n\t\t\t\t\t\t\t                      Priority Access\n\n\t\t\t\t\t\t\t                      To new product offerings, features, community features, and events\n\n\t\t\t\t\t\t\t                            Daily Long-form Stories\n\n\t\t\t\t\t\t\t                              Daily Long-form stories\n\t\t\t\t\t\t\t                              Daily long-form stories from India’s startup ecosystem, internet economy, and publicly-listed tech and consumer companies.\n\n\t\t\t\t\t\t\t                            Subscriber + Premium Newsletters\n\n\t\t\t\t\t\t\t                              Subscriber + Premium Newsletters\n\t\t\t\t\t\t\t                              Topical and sharp weekly industry newsletters covering e-commerce, retail, fintech, personal finance, edtech. Fresh and original weekend newsletters like The Nutgraf and First Principles.\n\n\t\t\t\t\t\t\t                            Visual Stories\n\n\t\t\t\t\t\t\t                              Visual Stories\n\t\t\t\t\t\t\t                              Understand business the way it’s meant to be seen, through charts, graphs, and infographics.\n\n\t\t\t\t\t\t\t                          Free + Premium Podcasts\n\n\t\t\t\t\t\t\t                              Free + Premium Podcasts\n\t\t\t\t\t\t\t                              Access to the collection of premium podcasts that cover the most interesting workplaces, careers, and business trends, along with candid conversations with founders and business leaders.\n\n\t\t\t\t\t\t\t                            Unlimited\n\n\t\t\t\t\t\t\t                          Access to all The Ken's Archives\n\n\t\t\t\t\t\t\t                            20\n\n\t\t\t\t\t\t\t                          Premium story gift credits per month, with 10 credits per account\n\n\t\t\t\t\t\t\t                            10\n\n\t\t\t\t\t\t\t                          Monthly subscription gift credits per year, with 5 credits per account\n\n\t\t\t\t\t\t\t                          3 + 3\n\n\t\t\t\t\t\t\t                          Upto three devices allowed per account\n\n\t\t\t\t\t\t\t\t\t\t\t\tSubscribe to Premium Duo - $  299 / year\n\n\t\t\t\t\t\t\t\t\t\tCompare all plans\n\n\t\t\t\t\t\t            Not ready to subscribe? Sign up for a free account.\n\n\t\t\t\t\t\t              Get complete access to select stories, free newsletters and podcasts.\n\n\t\t\t\t\t\t            Sign up for free\n\t\t\t\t\t\t            Learn more\n\n\t\t\t\t\t          \tOR\n\n\t\t\t\t\t\t            Buy this story\n\t\t\t\t\t\t            Get 1-year access to just this story and nothing else\n\t\t\t\t\t\t            Buy for $  20\n\n\t\t\t\t\tTrusted by 5,00,000+ executives and leaders from the world’s most successful organisations and students at top post-graduate campuses\n\n\t\t\t\t\t\t\t\tSharp, Original, Insightful, Analytical, Handcrafted\n\t\t\t\t\t\t\t\tUnrivaled analysis and powerful stories about businesses in India and abroad from award-winning journalists. Includes access to long-form articles, premium newsletters, and our top-ranked podcasts.\n\n\t\t\t\t\t\t\t\tCoverage across Sectors, Companies, and Geographies\n\t\t\t\t\t\t\t\tDecode the most significant shifts in business, technology, startups, and healthcare. All told through a combination of original reporting, beautifully visualised data, and compelling narratives.\n\n\t\t\t\t\t\t\t\tMade even better by a 5,00,000+ community\n\t\t\t\t\t\t\t\tOur subscribers include leaders from the world’s most successful companies, students at top post-graduate campuses, and smart, curious people who want to understand how business is shaping the future.\n\n\t\t\t\t\t\t\t                The Ken has proven naysayers wrong by successfully running a digital news publication on a pure-subscription business model in India. They have shown that discerning readers are willing to pay for well-researched, well-written, in-dept news articles.\n\t\t\t\t\t\t\t                Kiran Mazumdar Shaw\n\t\t\t\t\t\t\t                Executive Chairperson, Biocon Limited\n\n\t\t\t\t\t\t\t                As a designer, it’s easy to get lost in the craft of building products. As a business owner however, keeping up with a rapidly changing landscape is key to saying relevant. The Ken doesn’t just help me stay on top of what’s happening in India(and beyond), but makes it fun to do so.\n\t\t\t\t\t\t\t                Rahul Gonsalves\n\t\t\t\t\t\t\t                Co-founder and CEO, Obvious Ventures\n\n\t\t\t\t\t\t\t\t\t\t\tI enjoy reading The Ken because it is informative, the articles are well researched, well written, without the spin and bias. I admire The Ken team for their dedication to getting closer to the true picture.\n\t\t\t\t\t\t\t\t\t\t\tHari Buggana\n                  \t\t\t\t\t\t\tChairman and MD, InvAscent\n\n\t\t\t\t\t\t\t\t\t\t\tTransparent, Honest, Detailed. To me, The Ken has been this since the day I subscribed to them. The research that they put into each story and the way it is presented is thoroughly interesting. Personally, I’ve always had a great time interacting with the publication and reading the stories.\n                  \t\t\t\t\t\t\tHarshil Mathur\n                  \t\t\t\t\t\t\tCEO and Co-Founder, Razorpay\n\n\t\t\t\t\t\t\t                The Ken has proven naysayers wrong by successfully running a digital news publication on a pure-subscription business model in India. They have shown that discerning readers are willing to pay for well-researched, well-written, in-dept news articles.\n\t\t\t\t\t\t\t                Kiran Mazumdar Shaw\n\t\t\t\t\t\t\t                Executive Chairperson, Biocon Limited\n\n\t\t\t\t\t\t\t                As a designer, it’s easy to get lost in the craft of building products. As a business owner however, keeping up with a rapidly changing landscape is key to saying relevant. The Ken doesn’t just help me stay on top of what’s happening in India(and beyond), but makes it fun to do so.\n\t\t\t\t\t\t\t                Rahul Gonsalves\n\t\t\t\t\t\t\t                Co-founder and CEO, Obvious Ventures\n\n\t\t\t\t\t\t\t\t\t\t\tI enjoy reading The Ken because it is informative, the articles are well researched, well written, without the spin and bias. I admire The Ken team for their dedication to getting closer to the true picture.\n\t\t\t\t\t\t\t\t\t\t\tHari Buggana\n                  \t\t\t\t\t\t\tChairman and MD, InvAscent\n\n\t\t\t\t\t\t\t\t\t\t\tTransparent, Honest, Detailed. To me, The Ken has been this since the day I subscribed to them. The research that they put into each story and the way it is presented is thoroughly interesting. Personally, I’ve always had a great time interacting with the publication and reading the stories.\n                  \t\t\t\t\t\t\tHarshil Mathur\n                  \t\t\t\t\t\t\tCEO and Co-Founder, Razorpay\n\t\t\t\t\t\t\t\t\t\t‹›\n\n\t\t\t\t\t\t\t\tFAQs\n\n\t\t\t\t\t                I would like to purchase a subscription, but I am not sure which plan to go for?\n\n\t\t\t\t\t                  We have three subscription plans - Basic, Premium, and Premium Duo.\n\n\t\t\t\t\t                  \t\tThe Basic plan only gives you access to daily long-form stories, subscriber newsletters, free podcasts, and archive access for the previous two years. You won’t get our premium newsletters, premium podcasts, visual stories, and the iPad app.\n\n\t\t\t\t\t                  \t\tThe Premium plan gives you complete access to all of The Ken’s products and features.\n\n\t\t\t\t\t                  \t\tThe Premium Duo plan gives you complete access to The Ken for two independent accounts.\n\n\t\t\t\t\t\t\t\t\t\tWhat topics do you usually write about?\n\n\t\t\t\t\t\t\t\t\t\tWe publish sharp, original, deeply reported long-form stories from India’s startup ecosystem, internet economy, and publicly listed tech and consumer companies. Our stories are forward-looking, analytical, and directional—supported by data, visualisations, and infographics. We use language and narrative that is accessible to even lay readers. And we optimise for quality over quantity, every single time.Our specialised subscriber-only newsletters are written by our expert, award-winning journalists and cover a range of topics across finance, retail, clean energy, and edtech, alongside weekend newsletters like The Nutgraf and First Principles.\n\n\t\t\t\t\t\t\t\t\t\tCan I upgrade to a higher plan?\n\n\t\t\t\t\t\t\t\t\t\tYes, you can upgrade to a higher plan while your current subscription is still active. Our system will automatically give you a discount against the remainder of your current subscription.\n\n\t\t\t\t\t\t\t\t\t\tWhat will happen to my existing subscription if I upgrade?\n\n\t\t\t\t\t\t\t\t\t\tYou will get a discount equivalent to the time remaining in your current subscription.\n\n\t\t\t\t\t\t\t\t\t\tWe want to purchase a subscription to The Ken as a group. Is it possible?\n\n\t\t\t\t\t\t\t\t\t\tYes, we do have corporate subscriptions, corporate gift subscriptions, and campus plans for larger groups that wish to read The Ken together. Write to us at corporate@the-ken.com and we'll help you with that.\n\n\t\t\t\t\t\t\t\t\t\tAre there any discounts for students?\n\n\t\t\t\t\t\t\t\t\t\tYes. You will have to upload a valid student ID card during checkout to avail the 50% discount. You can purchase a student subscription here.\n\n\t\t\t\t\t\t\t\t\t\tWill I be able to download the stories offline?\n\n\t\t\t\t\t\t\t\t\t\tWe do not allow downloading and distribution of our stories. The Ken is a digital subscription-based product and only those with an active subscription will have access to the stories, either on our app or the website.\n\n\t\t\t\t\t\t\t\t\t\tIf I purchase a subscription, how many devices can I read The Ken on?\n\n\t\t\t\t\t\t\t\t\t\tWe allow two simultaneous logins per subscription for the Basic plan, via only web and mobile.We allow three simultaneous logins per subscription for the Premium and Premium Duo plans, via web, mobile, and Android tablet/iPad.\n\n\t\t\t\t\t\t\t\t\t\tI recently purchased a subscription and would like to cancel it. Is it possible?\n\n\t\t\t\t\t\t\t\t\t\tWe do not offer any cancellations or refunds. If you are facing any issues with your subscription, you can write to us at support@the-ken.com.\n\n\t\t\t\t\t\t\t\t\t\tCan I stop / pause my subscription?\n\n\t\t\t\t\t\t\t\t\t\tNo\n\n\t\t\t\t\t\t\t\t\t\tDo you offer any discounts?\n\n\t\t\t\t\t\t\t\t\t\tSorry, no. Our journalism is funded completely by our subscribers. We believe that quality journalism comes at a price, and readers trust and pay us so that we can remain independent.\n\n\t\t\t\t\t\t\tLooking for Group Subscriptions?\n\n\t\t\t\t\t\t\t\tCorporate Subscriptions\n\t\t\t\t\t\t\t\tCampus Subscriptions\n\n                Newsletters\n\n                  Ed Set Go\n                  Trade Tricks\n                  Long and Short\n                  Ka-Ching!\n                  Inciting Incident\n                  Two By Two\n                  The Nutgraf\n                  First Principles\n                  The Collection\n                  Podcasts\n\n                      Two By Two\n                      Daybreak\n                      The Nutgraf\n                       One Billion in 10 minutes\n                      First Principles\n                      The First Two Years\n\n                Topics\n\n                  Startups\n                  Fintech\n                  Financial services\n                  Saas\n\n                  Careers\n                  Food delivery\n                  E-commerce\n                  Retail\n                   FMCG\n                   Venture Capital\n                  Public policy\n                  Mobility\n                  Climate\n                  B2B\n\n                  OTT\n                  Media\n                  Education\n                  Healthcare\n                  Big tech\n                  Manufacturing\n                  Smartphones\n\n                Companies\n\n                  Byjus\n                  Amazon\n                  Reliance\n                  Instagram\n                  Nykaa\n                  Shopee\n\n                   Jio\n                  Dunzo\n                  Softbank\n                  Uber\n                  Flipkart\n                  Ola\n                  Google\n                  GoJek\n                  Swiggy\n                  Zomato\n                  Netflix\n                  Bharti Airtel\n                   Apple\n                   Razorpay\n\n                  Facebook\n                  Phonepe\n                  Grab\n                  Hotstar\n                  Paytm\n                   Tiger Global\n                  SEBI\n                  WhatsApp\n                  TRAI\n                  RBI\n                  Sequoia Capital\n                  Tata\n                  Amazon Prime\n                  Cred\n\n                  Airtel\n                  Reliance Industries\n\n                  Microsoft\n                  Youtube\n                  Unacademy\n                  Bigbasket\n                  Niti Aayog\n                  HDFC Bank\n                  Xiaomi\n                  ICICI Bank\n                  Walmart\n                  NPCI\n                  Reliance Jio\n                  Pharmeasy\n\n              Delivering sharp, original, insightful, analytical journalism about business and start-ups across India since 2016.\n\n                      Subscribe\n\n                @media (min-width: 1024px){\n                  .main-footer .footer-main-menu ul li{\n                    margin-right: 7px;\n                  }\n                }\n\n                @media (max-width: 575px){\n                  .main-footer .footer-main-menu ul{\n                    flex-wrap: wrap;\n                    justify-content: flex-start;\n                    row-gap: 10px;\n                  }\n                }\n\n                About Us\n                Team\n                Culture\n                Careers\n                Help\n                Blog\n                Write For Us\n                Guest Writers\n\n                Subscriptions\n\n                      Individual Subscriptions\n\n                      Corporate Subscriptions\n\n                      Campus Subscriptions\n\n                      Gift a Subscription\n\n                      24-Learning\n\n                      The Ken Learning\n\n                Follow Us\n\n                  Download The Ken App\n\n                      Android\n\n                      Apple\n\n              Terms & Conditions\n              •\n              Privacy\n\n              © 2025 Kenrise Media Private Limited. all rights reserved\n\nThe only business subscription you need\n\t\t\t\t\t\t\t\t\tUnrivaled analysis and powerful stories about businesses from award-winning journalists. Read by 5,00,000+ subscribers globally who want to be prepared for what comes next.\n\nOnly Free\n\t\t\t\t\t\t\t                        + Premium Podcasts\n\n\t\t\t\t\t\t\t                          Only Free Podcasts\n\t\t\t\t\t\t\t                          Access to podcasts covering the hottest business and startup trends in India\n\n\t\t\t\t\t\t\t                      Partner Account Access\n\n\t\t\t\t\t\t\t                          Partner Account Access\n\t\t\t\t\t\t\t                          Get two independent Premium accounts\n\n\t\t\t\t\t\t\t                      Gift a 1 Year Subscription for Free\n\n\t\t\t\t\t\t\t                          1+1 subscription\n\t\t\t\t\t\t\t                          Gift a subscription to The Ken to anyone you like\n\n\t\t\t\t\t\t\t                      App Access\n\n\t\t\t\t\t\t\t                      iPad App Access\n\n\t\t\t\t\t\t\t                        Priority Access\n\n\t\t\t\t\t\t\t                      To new product offerings, features, community features, and events\n\n\t\t\t\t\t\t\t                          \tDaily Long-form Stories\n\n\t\t\t\t\t\t\t                              Daily Long-form Stories\n\t\t\t\t\t\t\t                              Daily long-form stories from India’s startup ecosystem, internet economy, and publicly-listed tech and consumer companies.\n\n\t\t\t\t\t\t\t                          \tOnly Subscriber\n\t\t\t\t\t\t\t                        \t+ Premium\n\t\t\t\t\t\t\t                            Newsletters\n\n\t\t\t\t\t\t\t                              Only Subscriber Newsletters\n\t\t\t\t\t\t\t                              Topical and sharp weekly industry newsletters covering e-commerce, retail, fintech, personal finance, edtech. Fresh and original weekend newsletters like The Nutgraf and First Principles.\n\n\t\t\t\t\t\t\t                            Visual Stories\n\n\t\t\t\t\t\t\t                           Only Free\n\t\t\t\t\t\t\t                            + Premium Podcasts\n\n\t\t\t\t\t\t\t                              Free + Premium Podcasts\n\t\t\t\t\t\t\t                              Covering the most interesting workplaces, career and business trends, along with candid conversations with founders and business leaders.\n\n\t\t\t\t\t\t\t                            2 Years\n\n\t\t\t\t\t\t\t                          Archive access to the last two years of stories\n\n\t\t\t\t\t\t\t                            5\n\n\t\t\t\t\t\t\t                          Premium story gift credits per month\n\n\t\t\t\t\t\t\t                            3\n\n\t\t\t\t\t\t\t                          Monthly subscription gift credits per year\n\n\t\t\t\t\t\t\t                          2\n\n\t\t\t\t\t\t\t                          Upto two devices allowed\n\nFree + Premium Podcasts\n\n\t\t\t\t\t\t\t                          Free + Premium Podcasts\n\t\t\t\t\t\t\t                          Access to our entire collection of premium podcasts covering the most interesting business, career, and startup trends in India\n\n\t\t\t\t\t\t\t                        Partner Account Access\n\n\t\t\t\t\t\t\t                          Partner Account Access\n\t\t\t\t\t\t\t                          Get two independent Premium accounts\n\n\t\t\t\t\t\t\t                      Gift a 1 Year Subscription for Free\n\n\t\t\t\t\t\t\t                          1+1 subscription\n\t\t\t\t\t\t\t                          Gift a subscription to The Ken to anyone you like\n\n\t\t\t\t\t\t\t                      App Access\n\n\t\t\t\t\t\t\t                      iPad App Access\n\n\t\t\t\t\t\t\t                      Priority Access\n\n\t\t\t\t\t\t\t                      To new product offerings, features, community features, and events\n\n\t\t\t\t\t\t\t                            Daily Long-form Stories\n\n\t\t\t\t\t\t\t                              Daily Long-form Stories\n\t\t\t\t\t\t\t                              Daily long-form stories from India’s startup ecosystem, internet economy, and publicly-listed tech and consumer companies.\n\n\t\t\t\t\t\t\t                            Subscriber + Premium Newsletters\n\n\t\t\t\t\t\t\t                              Subscriber + Premium Newsletters\n\t\t\t\t\t\t\t                              Topical and sharp weekly industry newsletters covering e-commerce, retail, fintech, personal finance, edtech. Fresh and original weekend newsletters like The Nutgraf and First Principles.\n\n\t\t\t\t\t\t\t                            Visual Stories\n\n\t\t\t\t\t\t\t                              Visual stories\n\t\t\t\t\t\t\t                              Understand business the way it’s meant to be seen, through charts, graphs, and infographics.\n\n\t\t\t\t\t\t\t                          Free + Premium Podcasts\n\n\t\t\t\t\t\t\t                              Free + Premium Podcasts\n\t\t\t\t\t\t\t                              Access to the collection of premium podcasts that cover the most interesting workplaces, career, and business trends, along with candid conversations with founders and business leaders.\n\n\t\t\t\t\t\t\t                            Unlimited\n\n\t\t\t\t\t\t\t                          Access to all The Ken's Archives\n\n\t\t\t\t\t\t\t                            10\n\n\t\t\t\t\t\t\t                          Premium story gift  credits per month\n\n\t\t\t\t\t\t\t                            5\n\n\t\t\t\t\t\t\t                          Monthly subscription gift credits per year\n\n\t\t\t\t\t\t\t                          3\n\n\t\t\t\t\t\t\t                          Upto three devices allowed\n\nFree + Premium Podcasts\n\n\t\t\t\t\t\t\t                          Free + Premium Podcasts\n\t\t\t\t\t\t\t                          Access to our entire collection of premium podcasts covering the most interesting business, career, and startup trends in India\n\n\t\t\t\t\t\t\t                        Partner Account Access\n\n\t\t\t\t\t\t\t                          Partner Account Access\n\t\t\t\t\t\t\t                          Get two independent Premium accounts\n\n\t\t\t\t\t\t\t                      Gift a 1 Year Subscription for Free\n\n\t\t\t\t\t\t\t                          1+1 subscription\n\t\t\t\t\t\t\t                          Gift a subscription to The Ken to anyone you like\n\n\t\t\t\t\t\t\t                      App Access\n\n\t\t\t\t\t\t\t                      iPad App Access\n\n\t\t\t\t\t\t\t                      Priority Access\n\n\t\t\t\t\t\t\t                      To new product offerings, features, community features, and events\n\n\t\t\t\t\t\t\t                            Daily Long-form Stories\n\n\t\t\t\t\t\t\t                              Daily Long-form stories\n\t\t\t\t\t\t\t                              Daily long-form stories from India’s startup ecosystem, internet economy, and publicly-listed tech and consumer companies.\n\n\t\t\t\t\t\t\t                            Subscriber + Premium Newsletters\n\n\t\t\t\t\t\t\t                              Subscriber + Premium Newsletters\n\t\t\t\t\t\t\t                              Topical and sharp weekly industry newsletters covering e-commerce, retail, fintech, personal finance, edtech. Fresh and original weekend newsletters like The Nutgraf and First Principles.\n\n\t\t\t\t\t\t\t                            Visual Stories\n\n\t\t\t\t\t\t\t                              Visual Stories\n\t\t\t\t\t\t\t                              Understand business the way it’s meant to be seen, through charts, graphs, and infographics.\n\n\t\t\t\t\t\t\t                          Free + Premium Podcasts\n\n\t\t\t\t\t\t\t                              Free + Premium Podcasts\n\t\t\t\t\t\t\t                              Access to the collection of premium podcasts that cover the most interesting workplaces, careers, and business trends, along with candid conversations with founders and business leaders.\n\n\t\t\t\t\t\t\t                            Unlimited\n\n\t\t\t\t\t\t\t                          Access to all The Ken's Archives\n\n\t\t\t\t\t\t\t                            20\n\n\t\t\t\t\t\t\t                          Premium story gift credits per month, with 10 credits per account\n\n\t\t\t\t\t\t\t                            10\n\n\t\t\t\t\t\t\t                          Monthly subscription gift credits per year, with 5 credits per account\n\n\t\t\t\t\t\t\t                          3 + 3\n\n\t\t\t\t\t\t\t                          Upto three devices allowed per account\n\nTrusted by 5,00,000+ executives and leaders from the world’s most successful organisations and students at top post-graduate campuses\n\nSharp, Original, Insightful, Analytical, Handcrafted\n\t\t\t\t\t\t\t\tUnrivaled analysis and powerful stories about businesses in India and abroad from award-winning journalists. Includes access to long-form articles, premium newsletters, and our top-ranked podcasts.\n\n\t\t\t\t\t\t\t\tCoverage across Sectors, Companies, and Geographies\n\t\t\t\t\t\t\t\tDecode the most significant shifts in business, technology, startups, and healthcare. All told through a combination of original reporting, beautifully visualised data, and compelling narratives.\n\n\t\t\t\t\t\t\t\tMade even better by a 5,00,000+ community\n\t\t\t\t\t\t\t\tOur subscribers include leaders from the world’s most successful companies, students at top post-graduate campuses, and smart, curious people who want to understand how business is shaping the future.\n\n\t\t\t\t\t\t\t                The Ken has proven naysayers wrong by successfully running a digital news publication on a pure-subscription business model in India. They have shown that discerning readers are willing to pay for well-researched, well-written, in-dept news articles.\n\t\t\t\t\t\t\t                Kiran Mazumdar Shaw\n\t\t\t\t\t\t\t                Executive Chairperson, Biocon Limited\n\n\t\t\t\t\t\t\t                As a designer, it’s easy to get lost in the craft of building products. As a business owner however, keeping up with a rapidly changing landscape is key to saying relevant. The Ken doesn’t just help me stay on top of what’s happening in India(and beyond), but makes it fun to do so.\n\t\t\t\t\t\t\t                Rahul Gonsalves\n\t\t\t\t\t\t\t                Co-founder and CEO, Obvious Ventures\n\n\t\t\t\t\t\t\t\t\t\t\tI enjoy reading The Ken because it is informative, the articles are well researched, well written, without the spin and bias. I admire The Ken team for their dedication to getting closer to the true picture.\n\t\t\t\t\t\t\t\t\t\t\tHari Buggana\n                  \t\t\t\t\t\t\tChairman and MD, InvAscent\n\n\t\t\t\t\t\t\t\t\t\t\tTransparent, Honest, Detailed. To me, The Ken has been this since the day I subscribed to them. The research that they put into each story and the way it is presented is thoroughly interesting. Personally, I’ve always had a great time interacting with the publication and reading the stories.\n                  \t\t\t\t\t\t\tHarshil Mathur\n                  \t\t\t\t\t\t\tCEO and Co-Founder, Razorpay\n\n\t\t\t\t\t\t\t                The Ken has proven naysayers wrong by successfully running a digital news publication on a pure-subscription business model in India. They have shown that discerning readers are willing to pay for well-researched, well-written, in-dept news articles.\n\t\t\t\t\t\t\t                Kiran Mazumdar Shaw\n\t\t\t\t\t\t\t                Executive Chairperson, Biocon Limited\n\n\t\t\t\t\t\t\t                As a designer, it’s easy to get lost in the craft of building products. As a business owner however, keeping up with a rapidly changing landscape is key to saying relevant. The Ken doesn’t just help me stay on top of what’s happening in India(and beyond), but makes it fun to do so.\n\t\t\t\t\t\t\t                Rahul Gonsalves\n\t\t\t\t\t\t\t                Co-founder and CEO, Obvious Ventures\n\n\t\t\t\t\t\t\t\t\t\t\tI enjoy reading The Ken because it is informative, the articles are well researched, well written, without the spin and bias. I admire The Ken team for their dedication to getting closer to the true picture.\n\t\t\t\t\t\t\t\t\t\t\tHari Buggana\n                  \t\t\t\t\t\t\tChairman and MD, InvAscent\n\n\t\t\t\t\t\t\t\t\t\t\tTransparent, Honest, Detailed. To me, The Ken has been this since the day I subscribed to them. The research that they put into each story and the way it is presented is thoroughly interesting. Personally, I’ve always had a great time interacting with the publication and reading the stories.\n                  \t\t\t\t\t\t\tHarshil Mathur\n                  \t\t\t\t\t\t\tCEO and Co-Founder, Razorpay\n\t\t\t\t\t\t\t\t\t\t‹›\n\nFAQs\n\n\t\t\t\t\t                I would like to purchase a subscription, but I am not sure which plan to go for?\n\n\t\t\t\t\t                  We have three subscription plans - Basic, Premium, and Premium Duo.\n\n\t\t\t\t\t                  \t\tThe Basic plan only gives you access to daily long-form stories, subscriber newsletters, free podcasts, and archive access for the previous two years. You won’t get our premium newsletters, premium podcasts, visual stories, and the iPad app.\n\n\t\t\t\t\t                  \t\tThe Premium plan gives you complete access to all of The Ken’s products and features.\n\n\t\t\t\t\t                  \t\tThe Premium Duo plan gives you complete access to The Ken for two independent accounts.\n\n\t\t\t\t\t\t\t\t\t\tWhat topics do you usually write about?\n\n\t\t\t\t\t\t\t\t\t\tWe publish sharp, original, deeply reported long-form stories from India’s startup ecosystem, internet economy, and publicly listed tech and consumer companies. Our stories are forward-looking, analytical, and directional—supported by data, visualisations, and infographics. We use language and narrative that is accessible to even lay readers. And we optimise for quality over quantity, every single time.Our specialised subscriber-only newsletters are written by our expert, award-winning journalists and cover a range of topics across finance, retail, clean energy, and edtech, alongside weekend newsletters like The Nutgraf and First Principles.\n\n\t\t\t\t\t\t\t\t\t\tCan I upgrade to a higher plan?\n\n\t\t\t\t\t\t\t\t\t\tYes, you can upgrade to a higher plan while your current subscription is still active. Our system will automatically give you a discount against the remainder of your current subscription.\n\n\t\t\t\t\t\t\t\t\t\tWhat will happen to my existing subscription if I upgrade?\n\n\t\t\t\t\t\t\t\t\t\tYou will get a discount equivalent to the time remaining in your current subscription.\n\n\t\t\t\t\t\t\t\t\t\tWe want to purchase a subscription to The Ken as a group. Is it possible?\n\n\t\t\t\t\t\t\t\t\t\tYes, we do have corporate subscriptions, corporate gift subscriptions, and campus plans for larger groups that wish to read The Ken together. Write to us at corporate@the-ken.com and we'll help you with that.\n\n\t\t\t\t\t\t\t\t\t\tAre there any discounts for students?\n\n\t\t\t\t\t\t\t\t\t\tYes. You will have to upload a valid student ID card during checkout to avail the 50% discount. You can purchase a student subscription here.\n\n\t\t\t\t\t\t\t\t\t\tWill I be able to download the stories offline?\n\n\t\t\t\t\t\t\t\t\t\tWe do not allow downloading and distribution of our stories. The Ken is a digital subscription-based product and only those with an active subscription will have access to the stories, either on our app or the website.\n\n\t\t\t\t\t\t\t\t\t\tIf I purchase a subscription, how many devices can I read The Ken on?\n\n\t\t\t\t\t\t\t\t\t\tWe allow two simultaneous logins per subscription for the Basic plan, via only web and mobile.We allow three simultaneous logins per subscription for the Premium and Premium Duo plans, via web, mobile, and Android tablet/iPad.\n\n\t\t\t\t\t\t\t\t\t\tI recently purchased a subscription and would like to cancel it. Is it possible?\n\n\t\t\t\t\t\t\t\t\t\tWe do not offer any cancellations or refunds. If you are facing any issues with your subscription, you can write to us at support@the-ken.com.\n\n\t\t\t\t\t\t\t\t\t\tCan I stop / pause my subscription?\n\n\t\t\t\t\t\t\t\t\t\tNo\n\n\t\t\t\t\t\t\t\t\t\tDo you offer any discounts?\n\n\t\t\t\t\t\t\t\t\t\tSorry, no. Our journalism is funded completely by our subscribers. We believe that quality journalism comes at a price, and readers trust and pay us so that we can remain independent.\n\nLooking for Group Subscriptions?\n\n\t\t\t\t\t\t\t\tCorporate Subscriptions\n\t\t\t\t\t\t\t\tCampus Subscriptions",
    "summary": {
      "en": "The text discusses the impact of AI on consulting firms like McKinsey, Bain, and BCG. It notes that although AI is meant to simplify tasks for consultants, it has instead led to tighter deadlines and less creative work. Junior consultants, who are responsible for research and data analysis, are expected to use AI tools but often find the new demands overwhelming. Senior partners may view AI as a solution to improve efficiency, but this has resulted in unrealistic expectations for quick turnarounds on complex tasks. Consequently, junior consultants may submit low-quality work generated by AI under pressure.\n\nAdditionally, the text briefly mentions subscription options for a publication called The Ken, which includes various plans offering access to in-depth articles and newsletters on business topics.",
      "ko": "AI가 맥킨지, 베인, BCG와 같은 컨설팅 회사에 미치는 영향을 다루고 있습니다. AI는 컨설턴트의 업무를 간소화하기 위해 도입되었지만, 오히려 마감 기한이 촉박해지고 창의적인 작업이 줄어드는 결과를 초래했습니다. 연구와 데이터 분석을 담당하는 주니어 컨설턴트들은 AI 도구를 사용해야 하지만, 새로운 요구 사항이 부담스러워지는 경우가 많습니다. 고위 파트너들은 AI를 효율성을 높이는 해결책으로 바라보지만, 이로 인해 복잡한 작업에 대한 빠른 결과를 기대하는 비현실적인 상황이 발생하고 있습니다. 결과적으로 주니어 컨설턴트들은 압박감 속에서 AI가 생성한 저품질의 작업물을 제출할 수밖에 없습니다.\n\n또한, '더 켄'이라는 출판물의 구독 옵션에 대해서도 간단히 언급하고 있습니다. 이 출판물은 비즈니스 주제에 대한 심층 기사와 뉴스레터에 접근할 수 있는 다양한 구독 계획을 제공합니다.",
      "ja": "AIの導入がマッキンゼー、ベイン、BCGといったコンサルティング会社に与える影響について述べています。AIはコンサルタントの業務を簡素化することを目的としていますが、実際には締切が厳しくなり、創造的な仕事が減少しています。リサーチやデータ分析を担当するジュニアコンサルタントはAIツールを使うことが求められていますが、新たな要求に圧倒されることが多いです。シニアパートナーはAIを効率化の手段と見なすことがありますが、これにより複雑な業務に対する迅速な対応を期待されるようになっています。その結果、ジュニアコンサルタントはプレッシャーの中でAIが生成した低品質な成果物を提出することになりがちです。\n\nまた、The Kenという出版物のサブスクリプションオプションについても触れています。さまざまなプランが用意されており、ビジネスに関する詳細な記事やニュースレターにアクセスできる内容になっています。"
    }
  },
  {
    "id": "fc106686a4dcdb96",
    "title": {
      "en": "Show HN: Nue – Apps lighter than a React button",
      "ko": "Nue: 리액트 버튼보다 가벼운 앱",
      "ja": "Nue: 軽量アプリの革命"
    },
    "type": "story",
    "url": "https://nuejs.org/blog/large-scale-apps/",
    "score": 719,
    "by": "tipiirai",
    "time": 1743486461,
    "content": "April 1, 2025\n\n  Apps lighter than a React button\n\n    Tero Piirainen\n    @tipiirai\n\n      On this release, we’re showing what happens when you push modern web standards — HTML, CSS, and JS — to their peak:\n\n    <video type=\"video/mp4\" controls src=\"https://video.nuejs.org/39b76cca-e55b-4e9b-8583-b053f9dbd55d/play_720p.mp4\">\n  </video>\n\n  {\"videoid\":\"39b76cca-e55b-4e9b-8583-b053f9dbd55d\",\"poster\":\"thumbnail_70d8de32.jpg\",\"width\":\"704\",\"height\":\"407\"}\n\nThis entire app is lighter than a React/ShadCN button:\n\nSee benchmark and details here ›\nGoing large-scale\nHere’s the same app, now with a Rust computation engine and Event Sourcing for instant search and other operations over 150,000 records — far past where JS-version of the engine crashed with a maximum call stack exception.\n\n    <video type=\"video/mp4\" controls src=\"https://video.nuejs.org/eb65fcdd-5be4-4923-a783-f41efafe58a7/play_720p.mp4\">\n  </video>\n\n  {\"videoid\":\"eb65fcdd-5be4-4923-a783-f41efafe58a7\",\"poster\":\"/img/rust-splash.png\",\"width\":\"704\",\"height\":\"440\"}\n\n    Instant operations across 150.000 records with Rust/WASM\n\nThis demo is here ›\nTooling\nNue crushes HMR and build speed records and sets you up with a millisecond feedback loop for your everyday VSCode/Sublime file-save operations:\n\n    <video type=\"video/mp4\" controls src=\"https://video.nuejs.org/ffbb6d40-5b74-4176-a115-d0ed040edca5/play_720p.mp4\">\n  </video>\n\n  {\"videoid\":\"ffbb6d40-5b74-4176-a115-d0ed040edca5\",\"poster\":\"\",\"width\":\"\",\"height\":\"\"}\n\n    Immediate feedback for design and component updates, preserving app state\n\nHere's what this means:\nFor Rust, Go, and JS engineers\nThis is a game-changer for Rust, Go, and JS engineers stuck wrestling with React idioms instead of leaning on timeless software patterns. Nue emphasizes a model-first approach, delivering modular design with simple, testable functions, true static typing, and minimal dependencies. Nue is a liberating experience for system devs whose skills can finally shine in a separated model layer.\nFor Design Engineers\nThis is an important shift for design engineers bogged down by React patterns and 40,000+ line design systems. Build radically simpler systems with modern CSS (@layers, variables, calc()) and take control of your typography and whitespace.\nFor UX Engineers\nThis is a wake-up call for UX engineers tangled in React hooks and utility class walls instead of owning the user experience. Build apps as light as a React button to push the web — and your skills — forward.\nFAQ: WTH is Nue?\nNue is a web framework focused on web standards, currently in active development. I'm aiming to reveal the hidden complexity that’s become normalized in modern web development. When a single button outweighs an entire application, something’s fundamentally broken.\nNue drives the inevitable shift. We’re rebuilding tools and frameworks from the ground up with a cleaner, more robust architecture. Our goal is to bring back the joy of web development for everyone—whether you’re focused on performance, design, or UX.\n\n  What's next\n  We're improving the developer experience in three distinct phases:\n\n  Join the mailing list to follow our progress and see how our vision unfolds:",
    "summary": {
      "en": "**Summary: Nue Framework Release Announcement**\n\nOn April 1, 2025, Tero Piirainen introduced Nue, a web framework designed to optimize web development by focusing on modern standards like HTML, CSS, and JavaScript. \n\nKey Highlights:\n- Nue enables the creation of lightweight apps, even lighter than a typical React button.\n- A demonstration showed a Rust computation engine handling over 150,000 records efficiently, surpassing the limitations of JavaScript.\n- Nue offers rapid feedback for developers, enhancing build speed and improving the workflow in tools like VSCode and Sublime.\n\n**Target Audiences:**\n- **Rust, Go, and JS Engineers:** Nue promotes a model-first approach, simplifying design and allowing for more efficient coding practices.\n- **Design Engineers:** It encourages building simpler design systems with modern CSS techniques.\n- **UX Engineers:** It allows for more control over user experience by moving away from complex React patterns.\n\n**Nue's Philosophy:**\nThe framework aims to reduce the complexity of modern web development, bringing back the joy of creating applications without the burdens of heavy frameworks. \n\n**Next Steps:**\nThe development of Nue will follow three phases, and interested individuals can join a mailing list to stay updated on progress.",
      "ko": "2025년 4월 1일, 테로 피이라이넨은 HTML, CSS, JavaScript와 같은 현대 표준에 중점을 둔 웹 개발 최적화를 위한 웹 프레임워크인 Nue를 소개했습니다. \n\nNue의 주요 특징 중 하나는 일반적인 React 버튼보다도 가벼운 앱을 만들 수 있다는 점입니다. 시연에서는 Rust 기반의 계산 엔진이 150,000개 이상의 기록을 효율적으로 처리하는 모습을 보여주었으며, 이는 JavaScript의 한계를 뛰어넘는 성능을 자랑합니다. 또한, Nue는 개발자에게 빠른 피드백을 제공하여 빌드 속도를 높이고 VSCode와 Sublime과 같은 도구에서의 작업 흐름을 개선합니다.\n\nNue는 Rust, Go, JavaScript 엔지니어를 대상으로 모델 우선 접근 방식을 채택하여 디자인을 단순화하고 더 효율적인 코딩 관행을 가능하게 합니다. 디자인 엔지니어들에게는 현대 CSS 기법을 활용하여 더 간단한 디자인 시스템을 구축하도록 장려하며, UX 엔지니어들에게는 복잡한 React 패턴에서 벗어나 사용자 경험을 더 잘 제어할 수 있는 기회를 제공합니다.\n\nNue의 철학은 현대 웹 개발의 복잡성을 줄이고, 무거운 프레임워크의 부담 없이 애플리케이션을 만드는 즐거움을 되찾는 것입니다. Nue의 개발은 세 가지 단계로 진행될 예정이며, 관심 있는 사람들은 메일링 리스트에 가입하여 진행 상황을 확인할 수 있습니다.",
      "ja": "2025年4月1日、テロ・ピイライネン氏がNueというウェブフレームワークを発表しました。このフレームワークは、HTML、CSS、JavaScriptといった現代の標準に焦点を当て、ウェブ開発を最適化することを目的としています。\n\nNueの主な特徴として、軽量なアプリケーションを作成できる点が挙げられます。これは、一般的なReactボタンよりもさらに軽いものです。また、デモではRustの計算エンジンが15万件以上のデータを効率的に処理する様子が示され、JavaScriptの限界を超える性能を発揮しました。さらに、Nueは開発者に迅速なフィードバックを提供し、VSCodeやSublimeといったツールでのビルド速度を向上させ、作業の流れを改善します。\n\nNueは、Rust、Go、JavaScriptのエンジニアを主な対象とし、モデルファーストのアプローチを推進しています。これにより、設計が簡素化され、より効率的なコーディングが可能になります。また、デザインエンジニアには、現代のCSS技術を活用してシンプルなデザインシステムを構築することを促します。UXエンジニアにとっては、複雑なReactパターンから離れることで、ユーザー体験をよりコントロールできるようになります。\n\nNueの哲学は、現代のウェブ開発の複雑さを減らし、重いフレームワークの負担を感じることなくアプリケーションを作成する楽しさを取り戻すことです。\n\n今後の展開として、Nueの開発は3つのフェーズに分かれて進められます。興味のある方は、進捗を知るためのメールリストに参加することができます。"
    }
  },
  {
    "id": "f8cbed25db13b6e4",
    "title": {
      "en": "Nintendo Switch 2 Launches June 5 at $449.99",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.businesswire.com/news/home/20250402229347/en/Nintendo-Switch-2-Launches-June-5-at-%24449.99-Bringing-New-Forms-of-Game-Communication-to-Life",
    "score": 6,
    "by": "Willingham",
    "time": 1743619772,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "e8f72cb3837ef0d2",
    "title": {
      "en": "Where does air pollution come from?",
      "ko": "대기오염의 원인",
      "ja": "大気汚染の源"
    },
    "type": "story",
    "url": "https://ourworldindata.org/air-pollution-sources",
    "score": 221,
    "by": "kamaraju",
    "time": 1743567903,
    "content": "HomeAir PollutionAir pollution kills millions every year— where does it come from?A breakdown of the sources of many air pollutants that damage our health and ecosystems.By: Hannah Ritchie and Pablo RosadoMarch 31, 2025Cite this articleReuse our work freelyMillions of people die prematurely from air pollution every year. This problem has existed since humans started burning materials for fuel — first wood and biomass, then fossil fuels.But it’s an environmental and public health problem that we can make progress on. We know this because the world has already been successful in reducing air pollutants, and many countries that used to be highly polluted now have much cleaner air than they used to.To tackle air pollution effectively — tofocus our efforts on the interventions that will have the biggest impact — we need to understand where it’s coming from.That’s why we wrote this article.1A note on data and definitionsThe main data source we rely on is the Community Emissions Data System (CEDS). There are a couple of reasons why we think it’s an incredibly valuable resource:It has long-term global and national data extending back to the 18th century and is frequently updated with the latest estimates for 2022.It’s published with an open-access license and transparent methodology and inputs, which you can find on GitHub. The peer-reviewed paper describing the methodology is here.2This online data resource is open to user comments and feedback, so errors or issues can be easily reported.CEDS provides clear documentation of data improvements and detailed comparisons of recent updates against previous versions.To be clear, CEDS does not have high-quality measurements of emissions of air pollutants — certainly not dating back to the 18th century. These figures are calculated and modeled based on inputs, such as the quantity of different fuels that were burned, technological advancements and pollution controls, fertilizer use, and agricultural production. You can, for example, estimate the amount of sulfur dioxide produced from burning one tonne of coal in a power plant (with or without pollution filters).Of course, this means the data comes with some uncertainty, especially in earlier periods. However, it gives us a reasonable and consistent global dataset to understand how trends in emissions of air pollutants have changed over time.This article will focus on the breakdown of pollutants by their source. For this, we’ll use a categorization based on CEDS’s classification. In the table below, we summarize what is included in each category.CategorySub-categoriesAgricultureEnteric fermentationFuel use in agriculture, forestry, and fishingIndirect N₂O emissionsManure managementRice cultivationSoil emissionsOther agricultural emissionsBuildingsCommercial and institutional buildingsResidential buildingsDomestic aviationDomestic aviationEnergyElectricity production (autoproducer)Electricity production (public)Fossil fuel firesFugitive emissions from natural gas distributionFugitive emissions from natural gas productionFugitive emissions from other energy sourcesFugitive emissions from petroleumFugitive emissions from solid fuelsHeat productionOther energy transformationOther fuel use (unspecified)IndustryAdipic acid productionAluminum productionCement productionChemical industryIndustrial combustion (chemicals)Industrial combustion (construction)Industrial combustion (food and tobacco)Industrial combustion (iron and steel)Industrial combustion (machinery)Industrial combustion (mining and quarrying)Industrial combustion (non-ferrous metals)Industrial combustion (non-metallic minerals)Industrial combustion (other)Industrial combustion (pulp and paper)Industrial combustion (textile and leather)Industrial combustion (transport equipment)Industrial combustion (wood products)Iron and steel alloy productionLime productionNitric acid productionOther mineral productionOther non-ferrous metal productionPulp and paper, food, beverage, and wood processingInternational aviationInternational aviationInternational shippingInternational shippingOil tanker loadingSolventsChemical products manufacture and processingDegreasing and cleaningOther product usePaint applicationTransportDomestic navigationRail transportationRoad transportationOther transportWasteSolid waste disposalWaste combustionWastewater handlingOther waste handlingOther waste sourcesUnspecified waste sourcesShow moreHow air pollution damages our healthBefore we discuss the sources of some of the key air pollutants, we should briefly explain how air pollution affects human health and how each of these pollutants contributes to this.There are three key pathways by which a pollutant can cause harm3:Direct exposure: Some gases are toxic and can have an acute effect on health. These acute impacts are more common for people with existing respiratory conditions such as asthma or chronic obstructive pulmonary disease (COPD). This direct exposure does cost lives, but the total number is relatively small compared to the millions that die from chronic exposure to air pollution.Formation of particulate matter: Many of the pollutants we’ll look at contribute to health impacts indirectly by breaking down to form secondary smaller particles. These particles are called “particulate matter”. Typically, the smaller the particles are, the worse they can be for human health because they can enter our lungs and airways — and, in some cases, the bloodstream. Particulate matter can cause respiratory and cardiovascular problems, including cancer, strokes, and heart attacks.Formation of ozone: Another indirect way these pollutants can affect our health is by forming a gas called ozone (O3). Ozone can cause breathing problems and worsen acute conditions like asthma and COPD4. However, it also affects our health through chronic exposure by causing inflammation of the lungs, increasing the risk of respiratory diseases, and reducing our cardiovascular health.As we discuss each pollutant, we’ll briefly explain how it affects health through one or several pathways. While it’s difficult to pinpoint exactly how many deaths each pollutant causes, wherever possible, we’ll also try to give a rough order of magnitude estimate.To give some sense of scale, here is the Global Burden of Disease’s breakdown of global deaths from air pollution.5In 2021, this totaled around 8 million deaths. Note that there are also natural sources of particulate matter, so not all of these pollution deaths resulted from human emissions. But most did. For more on this, see our colleague Max Roser’s article, which looks at estimates from various sources.3.1 million came from household air pollution, a combination of direct toxicity and particulate matter. 4.7 million came from outdoor particulate matter, and another half a million from outdoor ozone pollution.6Breaking down the sources of different air pollutantsSulfur dioxide: the source of acid rainSulfur dioxide (SO2) is the main pollutant that causes acid rain. This has been a major environmental problem because acid rain can change the chemistry of rivers and lakes, affecting fish populations, soils, and the extent and quality of forests. You can also see the effects of acid rain on older limestone and marble buildings and statues, where the acidity dissolves parts of the structure.There are two ways that SO2 can threaten human health. First, direct inhalation of SO2 can exacerbate respiratory problems such as asthma and bronchitis. But its main contribution is by breaking down to form small particulate matter. While an exact figure is hard to pin down, given that sulfur dioxide is a substantial contributor to particulate matter and that at least 4 million deaths are linked to these small particles yearly, we would estimate that hundreds of thousands of deaths per year are linked to SO2.SO2 is formed when we burn fuels that contain sulfur.The charts below show where global emissions come from and how these sectors have changed over time. In 2022, energy production was the biggest contributor by far. This is predominantly due to power from coal, which has sulfur impurities that are released when it’s burned.The main contribution of the industry is the metal smelting process.7 This is because many of the ores that are used to produce metals – such as pyrite – contain large amounts of sulfur, which is released when they are roasted at high temperatures.Oil also contains sulfur, which is why road transport, shipping, and aviation all contribute. Shipping emissions have received a lot of attention in the last few years because they dropped by more than 70% in 2020 after the introduction of tight regulations on maritime fuels.You’ll notice that global emissions of SO2 peaked in 1979 and have almost halved since then, thanks to the introduction of pollution controls, particularly in Europe, North America, and China. SO2 can be removed from smokestacks in coal plants using technologies that filter or “scrub” the sulfur away before it’s emitted into the atmosphere. This, combined with a move away from coal in Europe and North America, has led to a rapid reduction in emissions. Nitrogen oxides (NOₓ): the reason car exhaust fumes are so damagingNitrogen oxides (NOₓ) are a group of gases, mostly made up of nitric oxide (NO) and nitrogen dioxide (NO2). They are formed when fuels containing nitrogen (again, mostly fossil fuels) are burned, causing them to react with oxygen.Like sulfur dioxide, which we just looked at, NOₓ can cause acid rain, threatening wildlife and ecosystems. NOₓ has a particularly large impact on human health because it acts through all three mechanisms we looked at earlier. It can be acutely toxic, inflaming the lungs. It reacts with other gases to form particulate matter, and it also forms ozone. NOₓ, therefore, causes smog and the thick haze you often see in highly polluted cities.Again, we don’t have exact estimates for how many deaths it contributes to. But, given that it’s a main source of ozone (which kills around half a million) and a substantial fraction of particulate matter (which kills several million), it’s reasonable to expect that NOₓ is linked to over a million deaths yearly.Since coal, oil, and gas all contain nitrogen, NOₓ is produced in various sectors, as the chart below shows. The biggest source is transport — mostly from road vehicles — where NOₓ is emitted from the exhaust of cars and trucks. This is almost matched by the burning of coal and gas for electricity production, shown as “energy” in the chart. Like road transport, burning fuel for shipping emits significant amounts of NOₓ, making it a leading source, too.And industrial processes such as metal smelting, cement production, and petroleum refining contribute a lot.A smaller but still important source is agriculture. When nitrogen is applied to crops as synthetic fertilizer or manure, some of this nitrogen is converted to nitrogen oxides (and ammonia, which we’ll come on to later) in the soil.8 While emissions of NOₓ from processes such as transport and electricity production have declined a lot globally, progress on agriculture has been much slower: emissions have flatlined but have not fallen much.Some countries have been successful in drastically reducing emissions of NOₓ – with huge benefits for human health. Moving away from fossil fuels – particularly coal in electricity production – has led to a large decline in “energy” emissions (have a look at the United Kingdom as an example). Setting pollution control standards for automakers has also played a crucial role in reducing emissions from road transport. NOₓ emissions from exhausts can be dramatically reduced through the use of catalytic converters, which are devices that split the NOₓ compounds into nitrogen and oxygen before they are released into the atmosphere.Black carbon: the soot that fills our skies and lungsBlack carbon (BC) are the small particles that many of us know as “soot”.As most of us know from experience — such as lighting a bonfire — soot is formed when we burn materials such as wood and biomass or fossil fuels like coal. It’s the incomplete combustion of these materials that leads to the formation of these BC particles.These particles are black because they absorb light, and this absorption of sunlight contributes to climate change. However, when it comes to health, it contributes through its direct toxicity and the formation of small particulates.Black carbon can be a major issue for household and outdoor particulate matter and probably contributes to several million deaths per year.In the charts below, you can see the sources of black carbon globally.Unsurprisingly, the biggest contributor to BC is energy production, mostly coal and biomass burning for electricity and heat. Black carbon is a big problem in industrialized countries with large-scale electricity production, but also in lower-income countries where people rely on burning biomass and charcoal for cooking and heating.Road transport is another major contributor since black carbon is also formed from diesel engines and exhausts. The open burning of waste also plays a surprisingly large role, particularly in low-to-middle-income countries, where this is often used for waste disposal.Some countries — particularly richer ones — have seen a huge drop in black carbon emissions over the last 50 years due to moving away from biomass and coal burning and introducing cleaner cars.Methane: burping cows, rice paddies, and gas leaksMethane (CH4) is a greenhouse gas, so it’s mostly discussed regarding contributions to climate change. However, methane can also affect health when it breaks down to form ozone, a gas that’s hazardous to human health. In fact, methane is the biggest precursor to ozone in many places.It’s estimated that methane can lead to up to half a million premature deaths a year.9The charts below show where it comes from.Agriculture, specifically livestock and rice production, is the biggest source of methane. Ruminant livestock — mostly cows — produce methane in their digestive systems and release it into the atmosphere by burping. That’s why beef and lamb tend to have a high carbon footprint.10 Rice also produces methane because it’s often grown in flooded paddy fields with low oxygen levels. This means methane is produced rather than carbon dioxide. Eating less beef, lamb, and dairy could dramatically reduce emissions from agriculture. Finding innovative ways to reduce the amount of methane produced per cow by changing their diets could also help.Energy generation is another large source of methane. Most of it comes from leaks — which we call “fugitive emissions” — from oil and gas wells. If these are not properly managed, some methane escapes into the atmosphere. Another key source is coal mining. Monitoring oil and gas wells for methane leaks and enforcing regulations to ensure that limits are not breached can reduce these emissions. New drone and satellite technologies are already being developed to provide a global map of where these leaks are coming from.The third sector that contributes a lot is waste. Methane is produced when organic material — like food waste or paper — rots in conditions without much oxygen (like in a landfill). Securely sealing landfills or capturing this methane for energy can effectively reduce these emissions. Methane from waste has been falling in many richer countries — like the United Kingdom — that have implemented these strategies.Ammonia (NH3): it’s all about farmingAs the chart below shows, nearly all human emissions of ammonia (NH3) come from agriculture. When we add nitrogen to crops as synthetic fertilizers or manure, some of this nitrogen is converted into NH3 in the soil. Other smaller sources include decomposing organic waste in landfills and energy production.Although NH3 doesn’t stay in the atmosphere for long — typically hours to days — it can react with other gases to form small particulates that harm human health.Some studies suggest ammonia could drive several hundred thousand (up to 385,000) premature deaths from particulate matter.11In the charts below, you can see that unlike most other air pollutants, where emissions have peaked globally, emissions of NH3 have continued to rise as livestock production and the use of synthetic fertilizers have increased.Some countries — particularly those in Europe — have achieved small reductions in emissions because they use less fertilizers than a few decades ago.Non-methane volatile organic compounds (NMVOCs)Non-methane volatile organic compounds (NMVOCs) can threaten human health through all three of the pathways we looked at earlier: they can be directly toxic in high concentrations and mix with other gases to form ozone and small particulates.You can see the global sources of NMVOCs in the chart below.NMVOCs are produced by traditional pollution sources like burning fossil fuels and car exhausts. However, unlike most other pollutants, solvents such as paints, cleaning products, and chemical plants are also major sources.In addition to switching to low-carbon energy and phasing out gasoline cars, we also need to reduce the use of volatile organic compounds (VOCs) in personal care products and solvents. Setting emission limits on the chemicals industry will also be key to lowering our exposure to non-methane VOCs (NMVOCs).While pollution sources are diverse, the solutions are often notGoing through so many pollutants, one by one — as we just did — can seem overwhelming. We don’t need to just tackle one or two; we need to tackle more than six.12The good news is that the solutions we need often cut across several gases at the same time.Burning stuff for energy — whether that’s fossil fuels or biomass — is the root source for many of these gases. Moving to clean energy — deploying renewable or nuclear electricity, electrifying our cars, our industry, and home heating — and ensuring that people worldwide have access to modern energy sources would simultaneously cut many of these pollutants.Reducing meat production and consumption by shifting to more plant-based diets would reduce methane and ammonia emissions at the same time, too.These transitions come with large health benefits, just from reducing air pollution alone.And we know that it can be done. Many countries have dramatically reduced levels of air pollution, and as you can see in the chart below, they’ve prevented hundreds of thousands of early deaths as a result.The total number of deaths has declined in these countries despite much larger older populations. The decline in death rates has been even larger.AcknowledgmentsMany thanks to Max Roser and Edouard Mathieu for their feedback and comments on this article.Continue reading on Our World in DataIn many countries, people breathe the cleanest air in centuries. What can the rest of the world learn from this?Air pollution tends to get worse before it gets better, but how can we accelerate this transition? Data ExplorerData review: how many people die from air pollution?This data review presents published estimates of the global death toll from air pollution and provides the context that makes them understandable.EndnotesHere, we’re not talking about greenhouse gases that drive climate change — which we cover in great detail elsewhere — although we will include a few greenhouse gases, such as methane, which can also act as a precursor to local air pollutants.Hoesly, R. M., Smith, S. J., Feng, L., Klimont, Z., Janssens-Maenhout, G., Pitkanen, T., ... & Zhang, Q. (2018). Historical (1750–2014) anthropogenic emissions of reactive gases and aerosols from the Community Emissions Data System (CEDS). Geoscientific Model Development, 11(1), 369-408.World Health Organization (2021). WHO global air quality guidelines: particulate matter (‎PM2.5 and PM10)‎, ozone, nitrogen dioxide, sulfur dioxide and carbon monoxide.Note that we’re talking about ground-level, or tropospheric, ozone in the lower atmosphere. At this level, it’s considered a pollutant. This differs from stratospheric ozone, which is high in the atmosphere and crucial for protecting us from ultraviolet radiation. We cover this in our work on the Ozone Layer.* Mar, K. A., Unger, C., Walderdorff, L., & Butler, T. (2022). Beyond CO2 equivalence: The impacts of methane on climate, ecosystems, and health. Environmental science & policy.The Global Burden of Disease is published by the Institute for Health Metrics and Evaluation (IHME).Note that when we add all of these individual risk factors — indoor particulates, outdoor particulates, and outdoor ozone — the total comes to 8.3 million, which is higher than the Global Burden of Disease reports on aggregate. This is because different risk factors can combine to increase health problems and the risk of premature death. In this article, our colleague, Saloni Dattani, examines how risk factors are estimated and why they can’t be summed up to give the total number of premature deaths.Fioletov, V. E., McLinden, C. A., Krotkov, N., Li, C., Joiner, J., Theys, N., ... & Moran, M. D. (2016). A global catalogue of large SO2 sources and emissions derived from the Ozone Monitoring Instrument. Atmospheric Chemistry and Physics, 16(18), 11497-11519.Pan, S. Y., He, K. H., Lin, K. T., Fan, C., & Chang, C. T. (2022). Addressing nitrogenous gases from croplands toward low-emission agriculture. Npj Climate and Atmospheric Science.This estimate comes from the UN Environment Programme and Climate and Clean Air Coalition:\u000b\u000bUNEP and Climate and Clean Air Coalition (2021) Global Methane Assessment: Benefits and Costs of Mitigating Methane Emissions.Although this is not the only reason they have a high carbon footprint, even when we ignore methane, they still emit much carbon through land use and manure.Wyer, K. E., Kelleghan, D. B., Blanes-Vidal, V., Schauberger, G., & Curran, T. P. (2022). Ammonia emissions from agriculture and their contribution to fine particulate matter: A review of implications for human health. Journal of Environmental Management, 323, 116285.We included six of the big ones in this article, but it’s not a complete list.Cite this workOur articles and data visualizations rely on work from many different people and organizations. When citing this article, please also cite the underlying data sources. This article can be cited as:Hannah Ritchie and Pablo Rosado (2025) - “Air pollution kills millions every year— where does it come from?” Published online at OurWorldinData.org. Retrieved from: 'https://ourworldindata.org/air-pollution-sources' [Online Resource]BibTeX citation@article{owid-air-pollution-sources,\n    author = {Hannah Ritchie and Pablo Rosado},\n    title = {Air pollution kills millions every year— where does it come from?},\n    journal = {Our World in Data},\n    year = {2025},\n    note = {https://ourworldindata.org/air-pollution-sources}\n}Reuse this work freelyAll visualizations, data, and code produced by Our World in Data are completely open access under the Creative Commons BY license. You have the permission to use, distribute, and reproduce these in any medium, provided the source and authors are credited.The data produced by third parties and made available by Our World in Data is subject to the license terms from the original third-party authors. We will always indicate the original source of the data in our documentation, so you should always check the license of any such third-party data before use and redistribution.All of our charts can be embedded in any site.",
    "summary": {
      "en": "Air pollution is a major global health crisis, causing millions of premature deaths each year. It originates from burning fuels like wood, biomass, and fossil fuels. Despite this ongoing issue, progress has been made in reducing air pollution in many countries.\n\nTo effectively address air pollution, it's crucial to understand its sources. The Community Emissions Data System (CEDS) provides valuable data on air pollutants, going back to the 18th century. This data helps identify the main sources of pollution, categorized into agriculture, energy, industry, transport, and waste.\n\nKey pollutants include sulfur dioxide (SO2), nitrogen oxides (NOₓ), black carbon, methane (CH4), ammonia (NH3), and non-methane volatile organic compounds (NMVOCs). Each of these pollutants impacts health through various pathways, such as direct toxicity, forming particulate matter, or generating ozone, which can lead to serious respiratory and cardiovascular issues.\n\nFor instance, SO2 mainly comes from energy production and industrial processes, while NOₓ is primarily emitted from transport and energy generation. Black carbon is produced from the incomplete burning of fossil fuels and biomass. Methane is largely released through agriculture, especially from livestock and rice cultivation, as well as leaks in oil and gas production.\n\nAddressing air pollution requires a multi-faceted approach, such as transitioning to clean energy, reducing meat consumption, and improving agricultural practices. Many countries have successfully reduced pollution levels, leading to significant health benefits. \n\nOverall, while air pollution remains a critical challenge, there are effective strategies available to reduce its impact on health and the environment.",
      "ko": "대기 오염은 전 세계적으로 심각한 건강 위기로, 매년 수백만 명의 조기 사망을 초래하고 있습니다. 이 문제는 나무, 바이오매스, 화석 연료와 같은 연료를 태우면서 발생합니다. 이러한 지속적인 문제에도 불구하고, 많은 국가에서 대기 오염을 줄이기 위한 진전이 이루어지고 있습니다.\n\n대기 오염을 효과적으로 해결하기 위해서는 그 원인을 이해하는 것이 중요합니다. 커뮤니티 배출 데이터 시스템(CEDS)은 18세기까지 거슬러 올라가는 대기 오염 물질에 대한 귀중한 데이터를 제공합니다. 이 데이터는 농업, 에너지, 산업, 교통, 폐기물 등으로 분류된 주요 오염원들을 식별하는 데 도움을 줍니다.\n\n주요 오염 물질로는 이산화황(SO2), 질소산화물(NOₓ), 검은 탄소, 메탄(CH4), 암모니아(NH3), 비메탄 휘발성 유기 화합물(NMVOCs)이 있습니다. 이들 각각의 오염 물질은 직접적인 독성, 미세먼지 형성, 오존 생성 등을 통해 건강에 영향을 미치며, 이는 심각한 호흡기 및 심혈관 문제를 유발할 수 있습니다.\n\n예를 들어, SO2는 주로 에너지 생산과 산업 공정에서 발생하며, NOₓ는 주로 교통과 에너지 생성에서 배출됩니다. 검은 탄소는 화석 연료와 바이오매스의 불완전한 연소로 생성됩니다. 메탄은 주로 농업, 특히 가축 사육과 쌀 재배에서 방출되며, 석유 및 가스 생산 과정에서도 누출됩니다.\n\n대기 오염 문제를 해결하기 위해서는 청정 에너지로의 전환, 육류 소비 감소, 농업 관행 개선 등 다양한 접근 방식이 필요합니다. 많은 국가들이 오염 수준을 성공적으로 줄여 건강에 큰 이점을 가져왔습니다.\n\n전반적으로 대기 오염은 여전히 중요한 도전 과제로 남아 있지만, 건강과 환경에 미치는 영향을 줄이기 위한 효과적인 전략들이 존재합니다.",
      "ja": "大気汚染は、毎年何百万もの早期死亡を引き起こす重大な健康危機です。これは、木材やバイオマス、化石燃料の燃焼から発生します。この問題は依然として続いていますが、多くの国で大気汚染の削減に向けた進展が見られています。\n\n大気汚染に効果的に対処するためには、その原因を理解することが重要です。コミュニティ排出データシステム（CEDS）は、18世紀まで遡る大気汚染物質に関する貴重なデータを提供しています。このデータは、農業、エネルギー、産業、交通、廃棄物といった主要な汚染源を特定するのに役立ちます。\n\n主要な汚染物質には、二酸化硫黄（SO2）、窒素酸化物（NOₓ）、黒炭、メタン（CH4）、アンモニア（NH3）、および非メタン揮発性有機化合物（NMVOCs）が含まれます。これらの汚染物質は、直接的な毒性や微小粒子状物質の形成、オゾンの生成を通じて健康に影響を与え、深刻な呼吸器や心血管の問題を引き起こす可能性があります。\n\n例えば、SO2は主にエネルギー生産や産業プロセスから発生し、NOₓは主に交通やエネルギー生成から排出されます。黒炭は化石燃料やバイオマスの不完全燃焼から生じます。メタンは主に農業、特に家畜や稲作から放出され、石油やガスの生産過程での漏れも原因となります。\n\n大気汚染に対処するためには、クリーンエネルギーへの移行、肉の消費削減、農業の改善など、多面的なアプローチが必要です。多くの国が汚染レベルを成功裏に低下させ、健康に大きな利益をもたらしています。\n\n全体として、大気汚染は依然として重要な課題ですが、その健康や環境への影響を減少させるための効果的な戦略が存在します。"
    }
  },
  {
    "id": "f1a7032b6d99dba2",
    "title": {
      "en": "Why resume writing is snake oil",
      "ko": "이력서 작성의 허상",
      "ja": "履歴書のウソ"
    },
    "type": "story",
    "url": "https://interviewing.io/blog/why-resume-writing-is-snake-oil",
    "score": 12,
    "by": "leeny",
    "time": 1743613351,
    "content": "Footnotes\n\nWill side projects help you get a job? Good question and one that should be teased apart a bit. Getting a job has two components: getting in the door and doing well in interviews. In general, side projects are useless for getting in the door. Yes, every once in a while, a side project goes viral. Or if you build something really cool with your target company’s API, it can get some attention. But that’s pretty rare. Most side projects that adorn resumes go completely unnoticed.  When it comes to performing well in interviews, it depends. If the companies you’re interviewing at test you on practical skills, then they can be a great use of time. They can also be a great use of time to help you understand how APIs work, how the internet works, how clients and servers talk to each other, and so on. But if the companies you’re targeting primarily ask algorithmic questions, then side projects probably aren’t the best use of time. Finally, will side projects make you a better engineer? Absolutely. And that’s the best reason to do them. But that’s not quite the same as getting a job, is it? Once you're actively looking for a job, your time is better spent on interview prep and outreach. ↩",
    "summary": {
      "en": "Side projects can be helpful, but their impact on job hunting is mixed. They usually don't help you get noticed by employers, as most side projects go unnoticed on resumes. However, if you're interviewing at companies that focus on practical skills, side projects can be beneficial and deepen your understanding of technology. Conversely, if the companies you’re targeting mainly ask algorithm-related questions, then side projects may not be the best use of your time. Ultimately, while side projects can make you a better engineer, when you're job hunting, it's more effective to focus on interview preparation and networking.",
      "ko": "부업은 도움이 될 수 있지만, 구직에 미치는 영향은 다양합니다. 대부분의 부업은 이력서에서 눈에 띄지 않기 때문에 고용주에게 주목받는 데 큰 도움이 되지 않습니다. 그러나 실용적인 기술에 중점을 두는 회사에서 면접을 볼 경우, 부업은 유익할 수 있으며 기술에 대한 이해도를 높이는 데 도움이 됩니다. 반면, 목표로 하는 회사가 주로 알고리즘 관련 질문을 한다면, 부업에 시간을 투자하는 것이 최선의 선택이 아닐 수 있습니다. 결국, 부업이 엔지니어로서의 능력을 향상시킬 수는 있지만, 구직 활동을 할 때는 면접 준비와 네트워킹에 집중하는 것이 더 효과적입니다.",
      "ja": "副業は役立つことがありますが、就職活動への影響はさまざまです。多くの場合、副業は履歴書に記載しても雇用主に目を留めてもらえないことが多いです。しかし、実践的なスキルを重視する企業で面接を受ける場合、副業は有益であり、技術への理解を深める手助けになります。一方で、対象とする企業が主にアルゴリズムに関する質問をする場合、副業に時間を使うのはあまり効果的ではないかもしれません。結局のところ、副業はエンジニアとしてのスキルを向上させることができますが、就職活動中は面接の準備やネットワーキングに集中する方が効果的です。"
    }
  },
  {
    "id": "162f395fbe9e58fa",
    "title": {
      "en": "Dave Täht has died",
      "ko": "데이브 타흐트 별세",
      "ja": "デイブ・テハト死去"
    },
    "type": "story",
    "url": "https://libreqos.io/2025/04/01/in-loving-memory-of-dave/",
    "score": 229,
    "by": "mhandley",
    "time": 1743532847,
    "content": "About\n\n\tLibreQoS\n\tAuthors\n\nLTS\nGitHub\nNews\nChat\nSocial\n\n\tYouTube\n\tFosstodon\n\tFacebook\n\tTwitter\n\tLinkedIn\n\n\t\t\t\tAbout\n\n\tLibreQoS\n\tAuthors\n\nLTS\nGitHub\nNews\nChat\nSocial\n\n\tYouTube\n\tFosstodon\n\tFacebook\n\tTwitter\n\tLinkedIn\n\n04/01/2025We are devastated to report that Dave Täht has passed away.Dave was an amazing person, whose work on FQ-CoDel, CAKE, and LibreQoS changed the internet forever. He and Jim Gettys championed the fight against bufferbloat, working to improve the global internet, and to make smooth real-time communication viable for everyone, everywhere.Because of Dave’s persistence and advocacy, millions of devices worldwide now ship algorithms like FQ-CoDel by default, to smooth connectivity. This has allowed for reliable video calling in places where it would otherwise be impossible – and in turn, enabled millions to have access to their loved ones, healthcare, and community. One of Robert’s ISP customers is a kind paraplegic woman who lives in a remote rural community. She is able to access medical care via telemedicine, and to Facetime video call her grandchildren – all because of Dave’s contributions to field of network engineering. There are millions of people like her, whose lives are shaped by the technology and contributions Dave made available to the world, even though most will never know it. The code Dave contributed was always free and open source. He turned down numerous lucrative contracts because he always put his principles first – he valued the impact his code could make globally, not just prestige or money.\n\nDave is the reason that Starlink was able to tackle its latency issues – enabling a generation of young entrepreneurs across the developing world, such as these young folks in the Philippines (pictured below), to start their own ISPs – to expand internet access to their communities.\n\n\t\t\t\t\t\t\t\t\tDave started work on bufferbloat in part because of his own journey working to expand internet access in Nicaragua. Over more than a decade, his hard work had come full-circle, and helped to pave the way for the next generation of network engineers to improve connectivity where it’s needed most.\n\nWe are incredibly grateful to have Dave as our friend, mentor, and as someone who continuously inspired us – showing us that we could do better for each other in the world, and leverage technology to make that happen. He will be dearly missed.PS: Dave is forever in our hearts and souls, in our routers and… in production. https://github.com/LibreQoE/LibreQoS/pull/684 We will miss you so much, Dave.–Robert, Herbert, and FrankLibreQoS\n\nGallery\n\n2 Responses\n\n\t\t\t\tPingback: Remembering Dave Taht – Doc Searls Weblog\n\n\t\t\t\tPingback: Fallece Dave Täht, reconocido Ingeniero de Redes y Activista de Internet, DEP - CIBERNINJAS\n\nAbout\n\n\tLibreQoS\n\tAuthors\n\nLTS\nGitHub\nNews\nChat\nSocial\n\n\tYouTube\n\tFosstodon\n\tFacebook\n\tTwitter\n\tLinkedIn\n\n\t\t\t\tAbout\n\n\tLibreQoS\n\tAuthors\n\nLTS\nGitHub\nNews\nChat\nSocial\n\n\tYouTube\n\tFosstodon\n\tFacebook\n\tTwitter\n\tLinkedIn\n\n\t\t\t\t\tSearch\n\n\t\t\t\t\t\t\t\t\tCopyright © 2024 LibreQoE, LLCP.O. Box 222111, El Paso, TX 79913 USA",
    "summary": {
      "en": "Dave Täht, a significant figure in network engineering, has passed away. He was known for his contributions to important technologies like FQ-CoDel and CAKE, which helped combat bufferbloat, a problem affecting internet performance. His work has greatly improved internet connectivity, enabling reliable video calls and access to services for many people, including those in remote areas.\n\nDave believed in open-source principles, often turning down lucrative contracts to prioritize the positive impact of his work. His efforts also helped companies like Starlink reduce latency, benefiting young entrepreneurs in developing countries.\n\nHe was not only a mentor and friend to many but also an inspiration for improving global internet access. He will be deeply missed by his colleagues and the community he impacted.",
      "ko": "네트워크 엔지니어링 분야의 중요한 인물인 데이브 태흐트가 세상을 떠났습니다. 그는 인터넷 성능에 영향을 미치는 문제인 버퍼블로트를 해결하기 위해 FQ-CoDel과 CAKE와 같은 중요한 기술에 기여한 것으로 잘 알려져 있습니다. 그의 연구는 많은 사람들에게 신뢰할 수 있는 화상 통화와 서비스 접근을 가능하게 하여 인터넷 연결성을 크게 향상시켰습니다. 특히, 외딴 지역에 사는 사람들에게도 큰 도움이 되었습니다.\n\n데이브는 오픈 소스 원칙을 믿었으며, 긍정적인 영향을 우선시하기 위해 수익성이 높은 계약을 자주 거절했습니다. 그의 노력은 스타링크와 같은 기업들이 지연 시간을 줄이는 데도 기여하여 개발도상국의 젊은 기업가들에게 혜택을 주었습니다.\n\n그는 많은 사람들에게 멘토이자 친구였으며, 전 세계 인터넷 접근성을 개선하는 데 영감을 주었습니다. 그의 동료들과 그가 영향을 미친 커뮤니티는 그를 깊이 그리워할 것입니다.",
      "ja": "ネットワークエンジニアリングの重要な人物であるデイブ・タハト氏が亡くなりました。彼は、インターネットのパフォーマンスに影響を与えるバッファーブロート問題に対処するための技術、FQ-CoDelやCAKEなどに貢献したことで知られています。彼の業績はインターネット接続を大幅に改善し、遠隔地に住む人々を含む多くの人々が信頼できるビデオ通話やサービスにアクセスできるようにしました。\n\nデイブはオープンソースの原則を信じており、利益の大きい契約を断ることもありました。彼は自らの仕事がもたらすポジティブな影響を優先していました。また、彼の努力はスターレンクのような企業が遅延を減少させるのにも寄与し、発展途上国の若い起業家たちにとっても恩恵をもたらしました。\n\n彼は多くの人々にとってメンターであり友人であり、世界のインターネットアクセスを改善するためのインスピレーションでもありました。彼の存在は同僚や彼が影響を与えたコミュニティにとって深く惜しまれることでしょう。"
    }
  },
  {
    "id": "c8b96ce927b43869",
    "title": {
      "en": "Show HN: Textcase: A Python Library for Text Case Conversion",
      "ko": "텍스트케이스: 파이썬 텍스트 변환기",
      "ja": "テキストケース変換ライブラリ"
    },
    "type": "story",
    "url": "https://github.com/zobweyt/textcase",
    "score": 67,
    "by": "zobweyt",
    "time": 1743545975,
    "content": "textcase\n\nA feature-rich Python text case conversion library.\nDocumentation: https://zobweyt.github.io/textcase\nPyPi: https://pypi.org/project/textcase\nFeatures\n\nText case conversion: Convert strings between various text cases (e.g., snake_case, kebab-case, camelCase, etc.).\nExtensible Design: Easily extend the library with custom cases and boundaries.\nAcronym Handling: Properly detects and formats acronyms in strings (as in HTTPRequest).\nNon-ASCII Support: Handles non-ASCII characters seamlessly (no inferences on the input language itself is made).\n100% Test Coverage: Comprehensive tests ensure reliability and correctness.\nWell-Documented: Clean documentation with usage examples for easy understanding.\nPerformant: Efficient implementation without the use of regular expressions.\nZero Dependencies: The library has no external dependencies, making it lightweight and easy to integrate.\n\nInstallation\nCreate and activate a virtual environment and then install textcase:\npip install textcase\n\nUsage\nYou can convert strings into a case using the convert function:\nfrom textcase import case, convert\n\nprint(convert(\"ronnie james dio\", case.SNAKE))     # ronnie_james_dio\nprint(convert(\"Ronnie_James_dio\", case.CONSTANT))  # RONNIE_JAMES_DIO\nprint(convert(\"RONNIE_JAMES_DIO\", case.KEBAB))     # ronnie-james-dio\nprint(convert(\"RONNIE-JAMES-DIO\", case.CAMEL))     # ronnieJamesDio\nprint(convert(\"ronnie-james-dio\", case.PASCAL))    # RonnieJamesDio\nprint(convert(\"RONNIE JAMES DIO\", case.LOWER))     # ronnie james dio\nprint(convert(\"ronnie james dio\", case.UPPER))     # RONNIE JAMES DIO\nprint(convert(\"ronnie-james-dio\", case.TITLE))     # Ronnie James Dio\nprint(convert(\"ronnie james dio\", case.SENTENCE))  # Ronnie james dio\n\nSee documentation for more usage examples.",
    "summary": {
      "en": "**Summary of Textcase Library**\n\nTextcase is a Python library designed for converting strings into different text formats, such as snake_case, kebab-case, and camelCase. \n\n**Key Features:**\n- **Text Case Conversion:** Easily switch between various text formats.\n- **Extensible:** You can add custom cases and boundaries.\n- **Acronym Handling:** Formats acronyms correctly (e.g., HTTPRequest).\n- **Non-ASCII Support:** Works with non-ASCII characters without issues.\n- **Reliability:** Fully tested to ensure accuracy.\n- **Documentation:** Clear guides and examples for easy use.\n- **Performance:** Fast and efficient, without using regular expressions.\n- **Lightweight:** No external dependencies needed.\n\n**Installation:**\nTo install Textcase, create a virtual environment and run:\n```bash\npip install textcase\n```\n\n**Usage:**\nYou can convert strings with the `convert` function. Here are a few examples:\n- Convert to snake_case: `convert(\"ronnie james dio\", case.SNAKE)` returns `ronnie_james_dio`.\n- Convert to CONSTANT: `convert(\"Ronnie_James_dio\", case.CONSTANT)` returns `RONNIE_JAMES_DIO`.\n\nFor more examples and details, refer to the documentation.",
      "ko": "Textcase는 문자열을 다양한 텍스트 형식으로 변환하기 위해 설계된 파이썬 라이브러리입니다. 지원하는 형식으로는 snake_case, kebab-case, camelCase 등이 있습니다.\n\n이 라이브러리의 주요 기능으로는 여러 텍스트 형식 간의 변환이 간편하다는 점이 있습니다. 사용자는 자신만의 형식과 경계를 추가할 수 있으며, 약어를 올바르게 포맷하는 기능도 제공합니다. 또한 비ASCII 문자도 문제없이 처리할 수 있습니다. Textcase는 정확성을 보장하기 위해 철저하게 테스트되었으며, 사용이 간편하도록 명확한 가이드와 예제가 제공됩니다. 성능 또한 뛰어나며 정규 표현식을 사용하지 않고도 빠르고 효율적으로 작동합니다. 외부 의존성이 없기 때문에 가볍게 사용할 수 있습니다.\n\nTextcase를 설치하려면 가상 환경을 만들고 다음 명령어를 실행하면 됩니다. pip install textcase\n\n문자열 변환은 `convert` 함수를 사용하여 수행할 수 있습니다. 예를 들어, `convert(\"ronnie james dio\", case.SNAKE)`를 실행하면 `ronnie_james_dio`가 반환됩니다. 또 다른 예로, `convert(\"Ronnie_James_dio\", case.CONSTANT)`를 사용하면 `RONNIE_JAMES_DIO`가 반환됩니다.\n\n더 많은 예제와 자세한 내용은 문서를 참고하세요.",
      "ja": "Textcaseは、文字列をさまざまなテキスト形式に変換するためのPythonライブラリです。具体的には、snake_case、kebab-case、camelCaseなどの形式に対応しています。\n\nこのライブラリの主な特徴には、さまざまなテキスト形式への簡単な切り替えが可能なテキストケース変換機能があります。また、カスタムケースや境界を追加できる拡張性も備えています。さらに、略語を正しくフォーマットする機能もあり、例えば「HTTPRequest」のような略語にも対応しています。非ASCII文字にも問題なく対応し、信頼性が高く、正確性を確保するために十分にテストされています。使いやすさを考慮した明確なガイドや例も用意されており、パフォーマンスも優れていて、正規表現を使用せずに高速かつ効率的に動作します。外部依存関係も必要ありません。\n\nTextcaseをインストールするには、仮想環境を作成し、以下のコマンドを実行します。\npip install textcase\n\n文字列を変換するには、`convert`関数を使用します。いくつかの例を挙げると、snake_caseに変換する場合は、`convert(\"ronnie james dio\", case.SNAKE)`と入力すると、`ronnie_james_dio`が返されます。また、CONSTANT形式に変換する場合は、`convert(\"Ronnie_James_dio\", case.CONSTANT)`と入力すると、`RONNIE_JAMES_DIO`が得られます。さらに多くの例や詳細については、ドキュメントを参照してください。"
    }
  },
  {
    "id": "5283fb8bc85abb09",
    "title": {
      "en": "Tesla's Global Vehicle Deliveries Sank 13% in First Quarter",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.wsj.com/business/autos/teslas-global-vehicle-deliveries-sink-13-in-first-quarter-a1d6c1d4",
    "score": 26,
    "by": "JumpCrisscross",
    "time": 1743619990,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "f337720bc0d21257",
    "title": {
      "en": "Nushell 0.103 released, with background jobs support",
      "ko": "누쉘 0.103 출시! 백그라운드 작업 지원",
      "ja": "Nushell 0.103 発表！バックグラウンド対応"
    },
    "type": "story",
    "url": "https://www.nushell.sh/blog/2025-03-18-nushell_0_103_0.html#support-for-background-jobs-toc",
    "score": 12,
    "by": "sunshine-o",
    "time": 1743601624,
    "content": "Nushell 0.103.0Today, we're releasing version 0.103.0 of Nu. This release adds support for spawning background jobs, attaching attributes to custom commands, official .deb, .apk, and .rpm packages, a number of Vi-mode enhancements, more than 20 new proposed commands in the std-rfc module, and much more.Where to get itNu 0.103.0 is available as pre-built binaries or from crates.io. If you have Rust installed you can install it using cargo install nu.As part of this release, we also publish a set of optional plugins you can install and use with Nu. To install, use cargo install nu_plugin_<plugin name>.Table of contentsHighlights and themes of this releaseSupport for Background JobsOfficial .deb, .rpm, and .apk packages are now availableCustom Command Attributesstd-rfc ModuleCompletion and LSP enhancementsReedline Vi-mode enhancementsChangesAdditionsattr commandsrandom uuiddefault --emptyPlatform-specific char eolinto string --group-digitsfrom xml --allow-dtdto ymloverlay use completions$env.config.filesize.show_unitBacktraces for Nushell codeStartup banner themingNumeric range padding support in str expandBreaking changessplit list now keeps empty sublistsmatch no longer executes closures as if they were a block_PROMPT_\\* environment variables are no longer inherited_Closure serialization changesFilesize unit formattingExternal completers are no longer used for internal commandsRemovalsrangeinto bitsfmtBug fixes and other changesNotes for plugin developersHall of fameFull changelogHighlights and themes of this release [toc]Support for Background Jobs [toc]Thanks to @cosineblast in #14883, Nushell now has long awaited support for background jobs!The following commands have been added to all platforms:job spawnjob listjob killAnd on Unix platforms:Ctrl+Z to move a currently running external command into a frozen background jobjob unfreezeJob support is considered \"experimental\" at the moment. We do expect that this feature will be enhanced in the near future and that some functionality will change, likely with breaking changes.Official .deb, .rpm, and .apk packages are now available [toc]Over in our Integrations repository, PR #5 adds official support for Debian/Ubuntu (.deb), Red Hat/Fedora/Rocky (.rpm), and Alpine (.apk). See the Installation Guide for details.Custom Command Attributes [toc]With #14906, attributes can now be attached to custom commands. This release includes two built-in attributes:@example: Adds an example usage for the command.@search-terms: Adds search terms that can help users discover the command with help --find (or help -f).Users can add their own attributes which will be available in the structured-data documentation accessible via help commands and scope commands.Also, the Standard Library and new std-rfc (below) have been updated to use Custom Attributes for their examples. And the test harness has been updated to use a @test attribute to define test cases.We expect that attributes will be used to enable additional features in future releases.std-rfc Module [toc]The Standard Library (std) is a collection of useful additions written in native Nu. #15042 also adds std-rfc, a \"proving ground\" for proposed additions to the Standard Library. Features in the std-rfc module should be considered experimental while we determine if and when they should be promoted to the Standard Library in a future release. We welcome your feedback on these commands to help us make that decision.In this first release, std-rfc includes the following proposals:std-rfc/conversions: A set of helper conversions, including into list, columns-into-table, name-values, record-into-columns, and table-into-columns.std-rfc/tables: New commands to select and reject columns and rows based on slices, and a new aggregate command for running calculations on aggregated table data. aggregate is even more useful when paired with group-by --to-table results.std-rfc/kv (key-value store): a module which can easily store and retrieve pipeline (or other) data in an in-memory (session-based) or on-disk (universal) SQLite database. While we expect that Job control will be extended at some point with messaging support, kv can currently be used to retrieve results from a background job (and for many other purposes).std-rfc/clip: clip copy and clip paste commands for interacting with the system clipboard (requires terminal support).std-rfc/str: dedent and unindent commands to remove common (or specified) indentation from multi-line strings.std-rfc/path: Helpers for working with the extension, parent, or stem of a path.Completion and LSP enhancements [toc]Thanks to PRs from @blindFS, this release includes quite a few LSP and completion fixes and enhancements.Reedline Vi-mode enhancements [toc]This release also includes a number of Vi-mode enhancements with additions to Reedline:From @deephbzImprovements to Visual Mode Selection and Command Consistency (#867)Atomic unified commands for ChangeInside/DeleteInside (#874)\"Yank\" (copy): (#868)From @thomasschafero command to swap anchor and cursor (#877)Clear selection when exiting visual mode (#878)Changes [toc]Additions [toc]attr commands [toc]As part of the new custom command attributes introduced in this release, several attribute commands have been added:attr example: Attach an example to a command's help text.@example \"double an int\"  { 5 | double }   --result 10\n@example \"double a float\" { 0.5 | double } --result 1.0\ndef double []: [number -> number] {\n  $in * 2\n}\n# The examples above will be shown in `help double` or `double --help`.attr search-terms: Attach search terms to a command so that it is easier to find using help --find.attr category: Set the command category for a command.random uuid [toc]Thanks to @ayamdobhal in #15239, the random uuid command can now also generate v1, v3, v5, and v7 uuids. Previously, only v4 uuids could be generated. To specify the version, pass the -v/--version flag (default is v4). Note that version 1 requires an additional --mac/-m flag to be provided, and versions 3 and 5 require both a --namespace/-n flag and a --name/-s flag.default --empty [toc]This release introduces a new --empty flag for the default command (#15223). With this new flag, if the input value is \"empty\", then the provided default value will be returned. A value is considering \"empty\" according to the is-empty command, which currently returns true for any of the following:an empty string: \"\"an empty list: []an empty record: {}an empty binary value: 0x[]null: nullPlatform-specific char eol [toc]In #15059, several additional escape characters were added to the char command:eollsepline_sepThese will output the newline character(s) for the current platform (i.e., \\n on Unix and \\r\\n on Windows). This is different from newline, nl, line_feed, and lf which all output \\n on all platforms.into string --group-digits [toc]With #15265, providing the --group-digits flag to the into string command will format integers by grouping several digits together and using the system locale digit separator.from xml --allow-dtd [toc]After #15272, document type declarations (DTD) can now parsed by from xml if you pass the new --allow-dtd flag.to yml [toc]As pointed out in #15240, it was previously possible to load data from YAML files using the from yaml or from yml commands, but only possible to save data using to yaml. Now, you can use either to yaml or to yml to save data into YAML files thanks to @LoicRiegel in #15254.> [[foo bar]; [\"1\" \"2\"]] | to yml\n- foo: '1'\n  bar: '2'Thanks to this, the save command can also be used with the .yml extension:> [[foo bar]; [\"1\" \"2\"]] | save test.ymloverlay use completions [toc]Thanks to @hongquan in #15057 with input from @blindFS, autocomplete now lists more directories for overlay use.$env.config.filesize.show_unit [toc]A new config option, $env.config.filesize.show_unit, was introduced in #15276. show_unit is true by default, but when it is set to false, file sizes will be displayed without units (e.g., in table). This can be useful if you have set filesize.unit to a fixed unit, and you do not want the same unit to be shown over and over again.Backtraces for Nushell code [toc]Some errors in Nushell code can be \"chained\", where one error condition creates another (and potentially another).Previously, it could be difficult to find the source of the actual error, but with #14945, it is now possible to enable backtraces in Nushell code. To do so:$env.NU_BACKTRACE = 1... and re-run the failing code.Startup banner theming [toc]With #15095, it is now possible to set the styles used in the Welcome banner at startup using new $env.config.color_config settings:banner_foreground: The primary color of the banner textbanner_highlight1: Used for the first set of highlights, e.g., Nushell, nu, GitHub, et. albanner_highlight2: Used for the second set of highlights, e.g. Discord, Documentation, et. al.Numeric range padding support in str expand [toc]With #15125, @atahabaki added support for numeric ranges with padded zeros in str expand. For example:'{00..10}' | str expandBreaking changes [toc]split list now keeps empty sublists [toc]In #15161, if split list is used on a list with consecutive separators, it now returns an empty list. For example:[1 1 0 0 3 3 0 4 4] | split list 0\n# => [[1 1] [] [2 2] [3 3]]match no longer executes closures as if they were a block [toc]Prior to #15032, match would attempt to execute a closure returned from a match arm. Now, it returns the closure instead. For example:Before #15032:match 1 { _ => {|| print hi} }\n# => hiAfter #15032:match 1 { _ => {|| print hi} }\n# => closure_1090PROMPT_* environment variables are no longer inherited [toc]With #15130, the PROMPT_* environments are no longer inherited from the calling process. This prevents issues where some systems would set a POSIX-compatible PROMPT_COMMAND.If a prompt setting is not defined in the startup config, a Nushell-compatible default is applied instead of allowing inheritance from the parent process.Closure serialization changes [toc]Prior to #15285, to json, to msgpack, and to msgpackz would serialize closures as a null. With this PR, these commands are aligned with the existing behavior of to nuon. Attempting to pass a closure (or data structure containing a closure) to one of these commands will result in an error. Instead, you can now use the to <format> --serialize option with all of these commands, in which case a string-representation of the closure will be serialized.Filesize unit formatting [toc]When setting a specific filesize unit via $env.config.filesize.unit, sizes will now be formatted using the default locale (e.g., separators and decimals). This returns a similar 0.98 behavior.External completers are no longer used for internal commands [toc]Prior to #15086, internal commands (including custom commands and aliases) without custom completions would fall back to the external completer. After this PR, internal commands will not use the external completer.NoteThe zoxide completer previously listed in the cookbook relied on this functionality. To update your zoxide completer to be compatible, you can (optionally) remove your external zoxide completer, add this to your config:def \"nu-complete zoxide path\" [context: string] {\n  let parts = $context | split row \" \" | skip 1\n  {\n    options: {\n      sort: false\n      completion_algorithm: prefix\n      positional: false\n      case_sensitive: false\n    }\n    completions: (zoxide query --list --exclude $env.PWD -- ...$parts | lines)\n  }\n}\n\ndef --env --wrapped z [...rest: string@\"nu-complete zoxide path\"] {\n  __zoxide_z ...$rest\n}Note, if your zoxide configuration from zoxide init is in a vendor autoload, you should also add the above snippet to either a vendor autoload or user autoload, so that the __zoxide_z command is defined.Removals [toc]range [toc]The range command has been removed in #15038 following it's deprecation in 0.102.0. Please use the slice command as one-to-one replacement.into bits [toc]The into bits command has been removed in #15039 following it's deprecation in 0.102.0. Please use the format bits command as one-to-one replacement.fmt [toc]The fmt command has been removed in #15040 following it's deprecation in 0.102.0. Please use the format number command as one-to-one replacement.Bug fixes and other changes [toc]authortitlelink@Bahexmake echo const#14997@Bahexfeat(std/dirs): retain state in subshells or with exec-restarts#15080@Bahexfeat(overlay): expose constants with overlay use#15081@Bahexfeat(const): implement run_const for const#15082@IanManskeRework operator type errors#14429@KAAtheWiseGitFix an integer overflow bug in into duration#15031@LoicRiegelbugfix: math commands now return error with infinite range [#15135]#15236@MMukundiFix insert/upsert creation for nested lists (#15131)#15133@NotTheDr01dsFix improper application of local timezone offset to Unix epochs#15283@WindSoilderallow export alias in repl#15054@WindSoilderFix missing required overlay error#15058@WindSoilderfix $env.FILE_PWD and $env.CURRENT_FILE inside overlay use#15126@cosineblastTranspose now rejects streams with non-record values#15151@dam4rusfeat(explore): Allow expanding selected cell with 'e'#15000@hardfau1tfix(compact): compact empty list or record in column#15213@sgvictorinocheck signals while printing values#14980@sgvictorinofix ranges over zero-length input#15062@sgvictorinocheck signals in nu-glob and ls#15140@sgvictorinoprevent panic when parsing incomplete multi-expr matches#15230@sholderbachBump bytesize to fix into filesize#15088@sholderbachClose find handle in ls windows unsafe code#15314@ysthakurFix spread operator lexing in records#15023@ysthakurFix unterminated loop in parse_record#15246@ysthakurInclude symlinks in directory completions#15268Notes for plugin developers [toc]None this release.Hall of fame [toc]Thanks to all the contributors below for helping us solve issues, improve documentation, refactor code, and more! 🙏authortitlelink@LoicRiegeldoc: clarify trailing line ending in 'to json -r' documentation#15234@LoicRiegelrefactor: rename subcommand structs#15309@MMeschadd polars str strip chars (with --end / --start options)#15118@MMeschadds And and Or operators to polars plugin nu_expressions#15248@MMeschAdd Xor to polars plugin nu_expressions#15249@MMeschEnhance polars plugin documentation#15250@MMeschAdds polars list-contains command#15304@atahabakiFeature+: Bracoxide Zero Padding for Numeric Ranges#15125@eggcakerfix polars save example typo#15008@hardfau1tfix(test-support): use CARGO_BUILD_TARGET_DIR env var#15212@hongquanImprove documentation for each command#15172@sgvictorinoremove nu-check examples with the --all flag#15047@tmillrfeat(cli): add vi solidus / keybinding#14908Full changelog [toc]authortitlelink@132iklAdd search terms for hide and hide-env#15017@132iklFix match running closures as block#15032@132iklRun-time pipeline input type checking performance optimizations#15192@132iklParse XML documents with DTDs by default, and add --disallow-dtd flag#15272@132iklUnify closure serializing logic for to nuon, to msgpack, and to json#15285@132iklDisallow DTD by default in from xml#15325@BahexCustom command attributes#14906@Bahexfix extern commands' extra description#14996@Bahexmake echo const#14997@Bahexfix block spans for the module keyword#15078@Bahexuse 0-indexing in explore#15079@Bahexfeat(std/dirs): retain state in subshells or with exec-restarts#15080@Bahexfeat(overlay): expose constants with overlay use#15081@Bahexfeat(const): implement run_const for const#15082@Bahexdocs(std-rfc): use actual examples rather than doc comments#15097@Bahexfix(test stdlib): scanning tests shouldn't be affected by user config#15113@Bahexdocs(chunks): make chunks easier to discover for binary data#15117@Bahexsplit list: add streaming, closure argument, and splitting before/after a separator#15161@IanManskeRework operator type errors#14429@IanManskeAdd insert benchmarks#15166@IanManskeRespect system locale when formatting file sizes via config#15271@IanManskeAdd filesize.show_unit config option#15276@IanManskeinto string should not modify strings#15320@KAAtheWiseGitFix an integer overflow bug in into duration#15031@LoicRiegeldoc: clarify trailing line ending in 'to json -r' documentation#15234@LoicRiegelbugfix: math commands now return error with infinite range [#15135]#15236@LoicRiegelbugfix: add \"to yml\" command#15254@LoicRiegelrefactor: rename subcommand structs#15309@MMeschadd polars str strip chars (with --end / --start options)#15118@MMeschadds And and Or operators to polars plugin nu_expressions#15248@MMeschAdd Xor to polars plugin nu_expressions#15249@MMeschEnhance polars plugin documentation#15250@MMeschAdds polars list-contains command#15304@MMukundiFix insert/upsert creation for nested lists (#15131)#15133@NotTheDr01dsRemove --no-default-features for std-lib-and-python-virtualenv CI#15045@NotTheDr01dsAdds platform agnostic EoL separator to char command#15059@NotTheDr01dsFix char lsep assignment#15065@NotTheDr01dsAdd std-rfc README#15066@NotTheDr01dsMove std-rfc into Nushell#15042@NotTheDr01dsEnable theming the Welcome Banner#15095@NotTheDr01dsUpdate std-rfc tests to use @test attributes#15098@NotTheDr01dsAdd buffer_editor example with arguments in config nu --doc#15122@NotTheDr01dsFix failing test when using man version 2.13.0#15123@NotTheDr01dsReplace \"function\" with \"command\" in several user-facing doc#15129@NotTheDr01dsRemove inheritance for PROMPT variables created in default_env.nu#15130@NotTheDr01dsRemove BACKTRACE message for non-panic errors#15143@NotTheDr01dsAdd default --empty to handle empty values#15223@NotTheDr01dsFix improper application of local timezone offset to Unix epochs#15283@WindSoilderEnable nushell error with backtrace#14945@WindSoilderupdate miette to 7.5#15014@WindSoilderremove duplicate code in math/log.rs#15022@WindSoilderallow export alias in repl#15054@WindSoilderFix missing required overlay error#15058@WindSoildermake plugin compatible with nightly nushell version#15084@WindSoilderfix $env.FILE_PWD and $env.CURRENT_FILE inside overlay use#15126@WindSoilderadd a helpful msg to indicate a job has been frozen#15206@app/dependabotbuild(deps): bump crate-ci/typos from 1.29.4 to 1.29.5#15006@app/dependabotbuild(deps): bump bytes from 1.9.0 to 1.10.0#15010@app/dependabotbuild(deps): bump data-encoding from 2.7.0 to 2.8.0#15101@app/dependabotbuild(deps): bump actions-rust-lang/setup-rust-toolchain from 1.10.1 to 1.11.0#15179@app/dependabotbuild(deps): bump crate-ci/typos from 1.29.5 to 1.29.10#15180@app/dependabotbuild(deps): bump rust-embed from 8.5.0 to 8.6.0#15183@app/dependabotbuild(deps): bump scraper from 0.22.0 to 0.23.1#15294@app/dependabotbuild(deps): bump titlecase from 3.3.0 to 3.4.0#15295@app/dependabotbuild(deps): bump zip from 2.2.1 to 2.4.1#15335@atahabakiFeature+: Bracoxide Zero Padding for Numeric Ranges#15125@ayamdobhalfeat(random uuid): add support for uuid versions other than 4.#15239@ayax79Added S3 support for polars save#15005@ayax79Polars: Minor code cleanup#15144@ayax79Expose flag to not maintain order on polars concat#15145@ayax79move to polars bigidx#15177@ayax79polars strip-chars: Allow any polars expression for pattern argument#15178@ayax79polars open: exposing the ability to configure hive settings.#15255@ayax79Polars: Map pq extension to parquet files#15284@ayax79Support for reading Categorical and Enum types#15292@blindFSrefactor(completion): AST traverse to find the inner-most expression to complete#14973@blindFSrefactor(completion): expression based variable/cell_path completion#15033@blindFSfix: clippy warnings with --all-features#15035@blindFSfix(lsp): exit on null root_dir#15051@blindFSfix(lsp): inlay hints span issue with user config scripts#15071@blindFSrefactor(completion): flatten_shape -> expression for internal/external/operator#15086@blindFSfeat(lsp): hover on external command shows manpage#15115@blindFSfix(completion): edge cases of operator completions#15169@blindFSfix(completion): prefix_str should be trimmed to element_expression#15171@blindFSfeat(lsp): semantic tokens for highlighting internal commands with spaces#15173@blindFSfix(lsp): completion of commands defined after the cursor#15188@blindFSfix: new clippy warnings from rust 1.85.0#15203@blindFSfix(lsp): completion label descriptions for cell_path and external values#15226@blindFSfeat(lsp): signature help (manually triggered)#15233@blindFSfix(lsp): completion on command with following text#15238@blindFSfeat(lsp): completion items now respect the append_whitespace flag#15247@blindFSfix: security_audit, bump ring from 0.17.8 to 0.17.13#15263@blindFSrefactor(lsp): span fix made easy by bumping lsp-textdocument to 0.4.2#15287@blindFSfix(lsp): find_id for custom def in custom def#15289@blindFSfix(completion): more quoting for file_completion/directory_completion#15299@blindFSrefactor: tree-sitter-nu friendly alternative expressions#15301@blindFSfix(completion): full set of operators for type any#15303@blindFSfix(lsp): ansi strip on hover text#15331@cosineblastJobs#14883@cosineblastTranspose now rejects streams with non-record values#15151@cptpiepmatzReplaced IoError::new_with_additional_context calls that still had Span::unknown()#15056@cptpiepmatzUse proc-macro-error2 instead of proc-macro-error#15093@cptpiepmatzMore precise ErrorKind::NotFound errors#15149@cptpiepmatzTest on Beta Toolchain#15280@dam4rusfeat(explore): Allow expanding selected cell with 'e'#15000@eggcakerfix polars save example typo#15008@fdncredadd attr category @category to custom command attributes#15137@fdncredupdate to the latest reedline#15139@fdncredbump the rust toolchain to 1.83.0#15148@fdncredallow bench to handle larger numbers#15162@fdncredupdate query json help and examples#15190@fdncredupdate reedline editcommands in nushell#15191@fdncredallow --group-digits to be used in into string#15265@fdncredremove mimalloc allocator#15317@hardfau1tfix(test-support): use CARGO_BUILD_TARGET_DIR env var#15212@hardfau1tfix(compact): compact empty list or record in column#15213@hongquanProvide more directories autocomplete for \"overlay use\"#15057@hongquanImprove documentation for each command#15172@hustcerFix tests of docker image and Update Nu LICENSE#15015@hustcerRefactor kv commands: replace inline params in SQL queries#15108@hustcerAdd ansi codes to move cursor position#15221@hustcerUpdate toolkit.nu add nu_plugin_polars plugin for build and install#15324@hustcerAdd category to pwd and banner commands#15330@kubouchRemove Twitter from README#15026@sgvictorinocheck signals while printing values#14980@sgvictorinoremove nu-check examples with the --all flag#15047@sgvictorinofix ranges over zero-length input#15062@sgvictorinocheck signals in nu-glob and ls#15140@sgvictorinoprevent panic when parsing incomplete multi-expr matches#15230@sholderbachFuzz more realistically with keyword const eval#15036@sholderbachTrigger tests for patch release branch pushes#15037@sholderbachRemove range command after deprecation#15038@sholderbachRemove into bits after deprecation#15039@sholderbachRemove fmt after deprecation#15040@sholderbachFix usages of fmt to format number#15041@sholderbachUse build_target information in startup banner#15046@sholderbachRefactor/fix tests affecting the whole command set#15073@sholderbachBump bytesize to fix into filesize#15088@sholderbachFix match blocks in std-rfc/kv implementation#15089@sholderbachBump yanked dependencies#15090@sholderbachBump Ubuntu runners to 22.04 LTS for tests#15109@sholderbachRevert / vi binding due to priority bug#15111@sholderbachBump ratatui to 0.29.0#15187@sholderbachBump reedline to latest commit#15189@sholderbachBump reedline for recent completion fix#15310@sholderbachClose find handle in ls windows unsafe code#15314@sholderbachPin reedline to 0.39.0 for release#15338@tmillrfeat(cli): add vi solidus / keybinding#14908@ysthakurBump to 0.102.1 dev version#15012@ysthakurFix spread operator lexing in records#15023@ysthakurFix unterminated loop in parse_record#15246@ysthakurInclude symlinks in directory completions#15268@ysthakurFeature-gate job unfreeze behind \"os\"#15339Edit this page on GitHubContributors: Yash Thakur, NotTheDr01ds, Ian Manske, Loïc Riegel, Jan Klass, 132ikl",
    "summary": {
      "en": "Nushell has released version 0.103.0, introducing several new features and improvements:\n\n1. **Background Jobs**: Users can now manage background jobs with commands like `job spawn`, `job list`, and `job kill`. Support for moving running commands to the background is also added, but this feature is still experimental.\n\n2. **Package Support**: Official packages for Debian (.deb), Red Hat (.rpm), and Alpine (.apk) systems are now available.\n\n3. **Custom Command Attributes**: Users can attach attributes to custom commands to improve documentation and discoverability. Two built-in attributes are `@example` and `@search-terms`.\n\n4. **New Commands in std-rfc Module**: This experimental module includes new helper commands for data conversion, table manipulation, key-value storage, and clipboard interactions.\n\n5. **Vi-mode Enhancements**: Improvements have been made to the Vi-mode, including better visual selection and command consistency.\n\n6. **Completion and LSP Enhancements**: Several fixes and improvements for Language Server Protocol (LSP) and command completion have been added.\n\n7. **Bug Fixes and Changes**: Various bug fixes, changes to existing commands, and removal of deprecated features have been implemented to improve overall functionality.\n\nTo get this version, users can download pre-built binaries or install it via Rust’s package manager, Cargo. Optional plugins are also available for additional features.",
      "ko": "Nushell이 0.103.0 버전을 출시하며 여러 가지 새로운 기능과 개선 사항을 도입했습니다.\n\n첫째, 사용자들은 이제 `job spawn`, `job list`, `job kill`와 같은 명령어를 사용해 백그라운드 작업을 관리할 수 있습니다. 실행 중인 명령어를 백그라운드로 이동하는 기능도 추가되었지만, 이 기능은 아직 실험 단계입니다.\n\n둘째, Debian(.deb), Red Hat(.rpm), Alpine(.apk) 시스템을 위한 공식 패키지가 제공됩니다.\n\n셋째, 사용자들은 커스텀 명령어에 속성을 추가하여 문서화와 검색 가능성을 향상시킬 수 있습니다. 두 가지 내장 속성으로는 `@example`과 `@search-terms`가 있습니다.\n\n넷째, 새로운 std-rfc 모듈에서는 데이터 변환, 테이블 조작, 키-값 저장소, 클립보드 상호작용을 위한 새로운 도우미 명령어가 포함되어 있습니다.\n\n다섯째, Vi 모드가 개선되어 시각적 선택과 명령어 일관성이 향상되었습니다.\n\n여섯째, 언어 서버 프로토콜(LSP)과 명령어 자동 완성을 위한 여러 가지 수정 및 개선 사항이 추가되었습니다.\n\n마지막으로, 다양한 버그 수정과 기존 명령어의 변경, 더 이상 사용되지 않는 기능의 제거가 이루어져 전반적인 기능이 향상되었습니다.\n\n이 버전을 얻으려면 사용자는 미리 빌드된 바이너리를 다운로드하거나 Rust의 패키지 관리자 Cargo를 통해 설치할 수 있습니다. 추가 기능을 위한 선택적 플러그인도 제공됩니다.",
      "ja": "Nushellはバージョン0.103.0をリリースし、いくつかの新機能と改善を導入しました。\n\nまず、背景ジョブの管理が可能になりました。ユーザーは`job spawn`、`job list`、`job kill`といったコマンドを使って背景ジョブを管理できます。また、実行中のコマンドを背景に移動させる機能も追加されましたが、これはまだ実験的な機能です。\n\n次に、Debian（.deb）、Red Hat（.rpm）、Alpine（.apk）システム向けの公式パッケージが利用可能になりました。\n\nカスタムコマンドに属性を追加できるようになり、ドキュメントの充実や発見性の向上が図られています。組み込みの属性として`@example`と`@search-terms`が用意されています。\n\n新しい実験的なモジュールであるstd-rfcには、データ変換、テーブル操作、キーと値の保存、クリップボードとのやり取りのための新しいヘルパーコマンドが含まれています。\n\nViモードも改善され、視覚的な選択やコマンドの一貫性が向上しました。\n\n言語サーバープロトコル（LSP）やコマンド補完に関しても、いくつかの修正と改善が行われました。\n\n最後に、さまざまなバグ修正や既存コマンドの変更、非推奨機能の削除が実施され、全体的な機能性が向上しています。\n\nこのバージョンを入手するには、事前にビルドされたバイナリをダウンロードするか、RustのパッケージマネージャーであるCargoを通じてインストールできます。また、追加機能のためのオプションのプラグインも利用可能です。"
    }
  },
  {
    "id": "289b30c9c07d7bf4",
    "title": {
      "en": "Show HN: I vibecoded a 35k LoC recipe app",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.recipeninja.ai",
    "score": 110,
    "by": "tomblomfield",
    "time": 1743559033,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "b80791c80f24c436",
    "title": {
      "en": "The April Fools joke that might have got me fired",
      "ko": "해고 위기 장난!",
      "ja": "解雇の危機!?エイプリルフールジョーク"
    },
    "type": "story",
    "url": "http://oldvcr.blogspot.com/2025/04/the-april-fools-joke-that-might-have.html",
    "score": 496,
    "by": "goldenskye",
    "time": 1743491490,
    "content": "Old Vintage Computing Research\n\nREWIND and PLAY\n\nTuesday, April 1, 2025\n\nThe April Fools joke that might have got me fired\n\nEveryone should pull one great practical joke in their lifetimes. This one was mine, and I think it's past the statute of limitations. The story is true. Only the names are redacted to protect the guilty.\n\nMy first job out of college was a database programmer, even though my undergraduate degree had nothing to do with computers and my current profession still mostly doesn't. The reason was that the University I worked for couldn't afford competitive wages, but they did offer various fringe benefits, and they were willing to train someone who at least had decent working knowledge. I, as a newly minted graduate of the august University of California system, had decent working knowledge at least of BSD/386 and SunOS, but more importantly also had the glowing recommendation of my predecessor who was being promoted into a new position. I was hired, which was their first mistake.\n\nThe system I was hired to work on was an HP 9000 K250, one of Hewlett-Packard's big PA-RISC servers. I wish I had a photograph of it, but all I have are a couple bad scans of some bad Polaroids of my office and none of the server room. The server room was downstairs from my office back in the days when server rooms were on-premises, complete with a swipe card lock and a halon system that would give you a few seconds of grace before it flooded everything. The K250 hulked in there where it had recently replaced what I think was an Encore mini of some sort (probably a Multimax, since it was a few years old and the 88K Encores would have been too new for the University), along with the AIX RS/6000s that provided student and faculty shell accounts and E-mail, the bonded T1 lines, some of the terminal servers, the massive Cabletron routers and a lot of the telco stuff. One of the tape reels from the Encore hangs on my wall today as a memento.\n\nThe K250 and the Encore it replaced (as well as the L-Class that later replaced the K250 when I was a consultant) ran an all-singing, all-dancing student information system called CARS. CARS is still around, renamed Jenzabar, though I suspect that many of its underpinnings remain if you look under the table. In those days CARS was a massive overlay that was loaded atop the operating system and database, which when I started were, respectively, HP/UX 10.20 and Informix. (I'm old.) It used Informix tables, screens and stored procedures plus its own text UI libraries to run code written variously as Perform screens, SQL, C-shell scripts and plain old C or ESQL/C. Everything was tracked in RCS using overgrown Makefiles. I had the admin side (resource management, financials, attendance trackers, etc.) and my office partner had the academic side (mostly grades and faculty tracking). My job was to write and maintain this code and shortly after to help the University create custom applications in CARS' brand-spanking new web module, which chose the new hotness in scripting languages, i.e., Perl. Fortuitously I had learned Perl in, appropriately enough, a computational linguistics course.\n\nCARS also managed most of the printers on campus except for the few that the RS/6000s controlled directly. Most of the campus admin printers were HP LaserJet 4 units of some derivation equipped with JetDirect cards for networking. These are great warhorse printers, some of the best laser printers HP ever made. I suspect there were line printers other places, but those printers were largely what existed in the University's offices.\n\nIt turns out that the READY message these printers show on their VFD panels is changeable. I don't remember where I read this, probably idly paging through the manual over a lunch break, but initially the only fun things I could think of to do was to have the printer say hi to my boss when she sent jobs to it, stuff like that (whereupon she would tell me to get back to work). Then it dawned on me: because I had access to the printer spools on the K250, and the spool directories were conveniently named the same as their hostnames, I knew where each and every networked LaserJet on campus was. I was young, rash and motivated. This was a hack I just couldn't resist. It would be even better than what had been my favourite joke at my alma mater, where campus services, notable for posting various service suspension notices, posted one April Fools' Day that gravity itself would be suspended to various buildings. I felt sure this hack would eclipse that too.\n\nThe plan on April Fools' Day was to get into work at OMG early o'clock and iterate over every entry in the spool, sending it a sequence that would change the READY message to INSERT 5 CENTS. This would cause every networked LaserJet on campus to appear to ask for a nickel before you printed anything. The script was very simple (this is the actual script, I saved it):\n\n#!/bin/csh -f\n\ncd /opt/carsi/spool\nforeach i (*)\n        echo '^[%-12345X@PJL RDYMSG DISPLAY=\"INSERT 5 CENTS\"' | netto $i 9100\nend\n\nThe ^[ was a literal ASCII 27 ESCape character, and netto was a simple netcat-like script I had written in these days before netcat was widely used. That's it.\n\nNow, let me be clear: the printer was still ready! The effect was merely cosmetic! It would still print if you sent jobs to it! Nevertheless, to complete the effect, this message was sent out on the campus-wide administration mailing list (which I also saved):\n\nTo: xxx@xxx.xxx\nDate: xxx, 1 Apr xxxx 05:41:34 -0800 (PST)\nSubject: IMPORTANT NOTE ON PRINTER POLICY\n\nDue to the increasing costs of service commitments for campus printers,\nall printers on campus will be reprogrammed for pay-per-page service\nto defray these mounting expenses, effective immediately.\n\nMost printers will now require a 5 cent deposit per page for printing. This\nmay be paid on account or through special coin acceptors to be installed\non the unit by technicians through the end of this week. If your office has\nnot yet established an account, your printer will automatically request you to\ninsert 5 cents into the slot per page to be printed. Please check your\nprinter's LCD [sic] display to see if your printer requires the 5 cents per\npage before using your printer.\n\nAdditional printers will be retrofitted as soon as possible. Technicians\nwill be contacting departments with specific details.\n\nAll accounts will be maintained on CARS. Do not call the Helpdesk. To\nestablish or verify your department's printer account, please call me at\nxxxx.\n\nPlease also direct all questions regarding this new policy to me as well.\n\nWe apologise for the inconvenience and hope that the new cost requirement\nwill not adversely affect your department's productivity.\n\nAt the end of the day I would reset everything back to READY, smile smugly, and continue with my menial existence. That was the plan.\n\nHaving sent this out, I fielded a few anxious calls, who laughed uproariously when they realized, and I reset their printers manually afterwards. The people who knew me, knew I was a practical joker, took note of the date, and sent approving replies. One of the best was sent to me later in the day by intercampus mail, printed on their laser printer, with a nickel taped to it.\n\nUnfortunately, not everybody on campus knew me, and those who did not not only did not call me, but instead called university administration directly. By 8:30am it was chaos in the main office and this filtered up to the head of HR, who most definitely did know me, and told me I'd better send a retraction before the CFO got in or I was in big trouble. That went wrong also, because my retraction said that campus administration was not considering charging per-page fees when in fact they actually were, so I had to retract it and send a new retraction that didn't call attention to that fact. I also ran the script to reset everything early. Eventually the hubbub finally settled down around noon. Everybody in the office thought it was very funny. Even my boss, who officially disapproved, thought it was somewhat funny.\n\nThe other thing that went wrong, as if all that weren't enough, was that the director of IT — which is to say, my boss's boss — was away on vacation when all this took place. (Read E-mail remotely? Who does that?) I compounded this situation with the tactical error of going skiing over the coming weekend and part of the next week, most of which I spent snowplowing down the bunny slopes face first, so that he discovered all the angry E-mail in his box without me around to explain myself. (My office partner remembers him coming in wide-eyed asking, \"what did he do??\") When I returned, it was icier in the office than it had been on the mountain. The assistant director, who thought it was funny, was in trouble for not putting a lid on it, and I was in really big trouble for doing it in the first place. I was appropriately contrite and made various apologies and was an uncharacteristically model employee for an unnaturally long period of time.\n\nThe Ice Age eventually thawed and the incident was officially dropped except for a \"poor judgment\" on my next performance review and the satisfaction of what was then considered the best practical joke ever pulled on campus. Indeed, everyone agreed it was much more technically accomplished than the previous award winner, where someone had supposedly gotten it around the grounds that the security guards at the entrance would be charging a nominal admission fee per head. Years later they still said it was legendary.\n\nI like to think they still do.\n\nPosted by\n\nClassicHasClass\n\nat\n\n12:03 AM\n\nEmail ThisBlogThis!Share to XShare to FacebookShare to Pinterest\n\nLabels:\nhp,\nprotip\n\n2 comments:\n\n    (function() {\n      var items = null;\n      var msgs = null;\n      var config = {};\n\n// <![CDATA[\n      var cursor = null;\n      if (items && items.length > 0) {\n        cursor = parseInt(items[items.length - 1].timestamp) + 1;\n      }\n\n      var bodyFromEntry = function(entry) {\n        var text = (entry &&\n                    ((entry.content && entry.content.$t) ||\n                     (entry.summary && entry.summary.$t))) ||\n            '';\n        if (entry && entry.gd$extendedProperty) {\n          for (var k in entry.gd$extendedProperty) {\n            if (entry.gd$extendedProperty[k].name == 'blogger.contentRemoved') {\n              return '<span class=\"deleted-comment\">' + text + '</span>';\n            }\n          }\n        }\n        return text;\n      }\n\n      var parse = function(data) {\n        cursor = null;\n        var comments = [];\n        if (data && data.feed && data.feed.entry) {\n          for (var i = 0, entry; entry = data.feed.entry[i]; i++) {\n            var comment = {};\n            // comment ID, parsed out of the original id format\n            var id = /blog-(\\d+).post-(\\d+)/.exec(entry.id.$t);\n            comment.id = id ? id[2] : null;\n            comment.body = bodyFromEntry(entry);\n            comment.timestamp = Date.parse(entry.published.$t) + '';\n            if (entry.author && entry.author.constructor === Array) {\n              var auth = entry.author[0];\n              if (auth) {\n                comment.author = {\n                  name: (auth.name ? auth.name.$t : undefined),\n                  profileUrl: (auth.uri ? auth.uri.$t : undefined),\n                  avatarUrl: (auth.gd$image ? auth.gd$image.src : undefined)\n                };\n              }\n            }\n            if (entry.link) {\n              if (entry.link[2]) {\n                comment.link = comment.permalink = entry.link[2].href;\n              }\n              if (entry.link[3]) {\n                var pid = /.*comments\\/default\\/(\\d+)\\?.*/.exec(entry.link[3].href);\n                if (pid && pid[1]) {\n                  comment.parentId = pid[1];\n                }\n              }\n            }\n            comment.deleteclass = 'item-control blog-admin';\n            if (entry.gd$extendedProperty) {\n              for (var k in entry.gd$extendedProperty) {\n                if (entry.gd$extendedProperty[k].name == 'blogger.itemClass') {\n                  comment.deleteclass += ' ' + entry.gd$extendedProperty[k].value;\n                } else if (entry.gd$extendedProperty[k].name == 'blogger.displayTime') {\n                  comment.displayTime = entry.gd$extendedProperty[k].value;\n                }\n              }\n            }\n            comments.push(comment);\n          }\n        }\n        return comments;\n      };\n\n      var paginator = function(callback) {\n        if (hasMore()) {\n          var url = config.feed + '?alt=json&v=2&orderby=published&reverse=false&max-results=50';\n          if (cursor) {\n            url += '&published-min=' + new Date(cursor).toISOString();\n          }\n          window.bloggercomments = function(data) {\n            var parsed = parse(data);\n            cursor = parsed.length < 50 ? null\n                : parseInt(parsed[parsed.length - 1].timestamp) + 1\n            callback(parsed);\n            window.bloggercomments = null;\n          }\n          url += '&callback=bloggercomments';\n          var script = document.createElement('script');\n          script.type = 'text/javascript';\n          script.src = url;\n          document.getElementsByTagName('head')[0].appendChild(script);\n        }\n      };\n      var hasMore = function() {\n        return !!cursor;\n      };\n      var getMeta = function(key, comment) {\n        if ('iswriter' == key) {\n          var matches = !!comment.author\n              && comment.author.name == config.authorName\n              && comment.author.profileUrl == config.authorUrl;\n          return matches ? 'true' : '';\n        } else if ('deletelink' == key) {\n          return config.baseUri + '/comment/delete/'\n               + config.blogId + '/' + comment.id;\n        } else if ('deleteclass' == key) {\n          return comment.deleteclass;\n        }\n        return '';\n      };\n\n      var replybox = null;\n      var replyUrlParts = null;\n      var replyParent = undefined;\n\n      var onReply = function(commentId, domId) {\n        if (replybox == null) {\n          // lazily cache replybox, and adjust to suit this style:\n          replybox = document.getElementById('comment-editor');\n          if (replybox != null) {\n            replybox.height = '250px';\n            replybox.style.display = 'block';\n            replyUrlParts = replybox.src.split('#');\n          }\n        }\n        if (replybox && (commentId !== replyParent)) {\n          replybox.src = '';\n          document.getElementById(domId).insertBefore(replybox, null);\n          replybox.src = replyUrlParts[0]\n              + (commentId ? '&parentID=' + commentId : '')\n              + '#' + replyUrlParts[1];\n          replyParent = commentId;\n        }\n      };\n\n      var hash = (window.location.hash || '#').substring(1);\n      var startThread, targetComment;\n      if (/^comment-form_/.test(hash)) {\n        startThread = hash.substring('comment-form_'.length);\n      } else if (/^c[0-9]+$/.test(hash)) {\n        targetComment = hash.substring(1);\n      }\n\n      // Configure commenting API:\n      var configJso = {\n        'maxDepth': config.maxThreadDepth\n      };\n      var provider = {\n        'id': config.postId,\n        'data': items,\n        'loadNext': paginator,\n        'hasMore': hasMore,\n        'getMeta': getMeta,\n        'onReply': onReply,\n        'rendered': true,\n        'initComment': targetComment,\n        'initReplyThread': startThread,\n        'config': configJso,\n        'messages': msgs\n      };\n\n      var render = function() {\n        if (window.goog && window.goog.comments) {\n          var holder = document.getElementById('comment-holder');\n          window.goog.comments.render(holder, provider);\n        }\n      };\n\n      // render now, or queue to render when library loads:\n      if (window.goog && window.goog.comments) {\n        render();\n      } else {\n        window.goog = window.goog || {};\n        window.goog.comments = window.goog.comments || {};\n        window.goog.comments.loadQueue = window.goog.comments.loadQueue || [];\n        window.goog.comments.loadQueue.push(render);\n      }\n    })();\n// ]]>\n\nDanApril 1, 2025 at 5:39 AMMany, many years ago the Computer Science students of a certain UK university used a PostScript flaw to hack into the departmental printers (which were different from the main Uni printers) on Friday evening. They had earlier obtained the fingerprints of the head of department by devious means, and for one whole weekend, printed as faintly as possible were the fingerprints of the head of department on every sheet of output.On Sunday night they re-ran the hack and removed this \"feature\".ReplyDeleteRepliesClassicHasClassApril 1, 2025 at 9:34 AMAnd, of course, I'm sure you are reporting as a completely uninvolved observer. ;)DeleteRepliesReplyReplyAdd commentLoad more...\n\nComments are subject to moderation. Be nice.\n\n      BLOG_CMT_createIframe('https://www.blogger.com/rpc_relay.html');\n\nOlder Post\n\nHome\n\nSubscribe to:\nPost Comments (Atom)\n\nWelcome to Old VCR\n\nMy general vintage computing projects, mostly microcomputers, 6502, PalmOS, 68K/Power Mac and Unix workstations, but that's not all you'll see. While over the decades I've written for publications like COMPUTE, TidBITS and Ars Technica, these articles are all original and just for you. My promise: No AI-generated article text, ever. Be kind, REWIND and PLAY. -- Cameron Kaiser\n\nOld VCR is advertisement- and donation-funded, and what I get goes to maintaining the hardware here at Floodgap. I don't drink coffee, but the Mr Pibb doesn't buy itself. :-) Thanks for reading.\n\n(adsbygoogle = window.adsbygoogle || []).push({});\n\n(adsbygoogle = window.adsbygoogle || []).push({});\n\nGreatest hits\n\nThe April Fools joke that might have got me fired\n\nThe MIPS ThinkPad, kind of\n\nMeet your new two-factor authenticator: your Commodore 64\n\nSo thieves broke into your storage unit - again\n\nDusting off Dreamcast Linux\n\nRefurb weekend: Canon Cat\n\nThe Apple Network Server's all-too-secret weapon (featuring PPC Toolbox)\n\nIf one GUI's not enough for your SPARC workstation, try four\n\nSo long, home T1 line; hello, hacking the T1 router\n\nWhen you have too much memory for SheepShaver\n\nOther stuff I write\n\nOther classic computing posts from TenFourFox Development\nTalospace: OpenPOWER news and experiences from the free computing frontier\nJerk Music Critic: music reviews worth what you paid for them\n\nAbout Me\n\nClassicHasClass\n\nView my complete profile\n\nBlog Archive\n\n        ▼\n\n2025\n\n(4)\n\n        ▼\n\nApril\n\n(1)\n\nThe April Fools joke that might have got me fired\n\n        ►\n\nMarch\n\n(1)\n\n        ►\n\nJanuary\n\n(2)\n\n        ►\n\n2024\n\n(25)\n\n        ►\n\nDecember\n\n(2)\n\n        ►\n\nNovember\n\n(2)\n\n        ►\n\nOctober\n\n(4)\n\n        ►\n\nSeptember\n\n(1)\n\n        ►\n\nJuly\n\n(1)\n\n        ►\n\nJune\n\n(2)\n\n        ►\n\nMay\n\n(3)\n\n        ►\n\nApril\n\n(2)\n\n        ►\n\nMarch\n\n(4)\n\n        ►\n\nFebruary\n\n(2)\n\n        ►\n\nJanuary\n\n(2)\n\n        ►\n\n2023\n\n(39)\n\n        ►\n\nDecember\n\n(1)\n\n        ►\n\nNovember\n\n(2)\n\n        ►\n\nOctober\n\n(3)\n\n        ►\n\nSeptember\n\n(5)\n\n        ►\n\nAugust\n\n(4)\n\n        ►\n\nJuly\n\n(4)\n\n        ►\n\nJune\n\n(4)\n\n        ►\n\nMay\n\n(2)\n\n        ►\n\nApril\n\n(2)\n\n        ►\n\nMarch\n\n(3)\n\n        ►\n\nFebruary\n\n(3)\n\n        ►\n\nJanuary\n\n(6)\n\n        ►\n\n2022\n\n(36)\n\n        ►\n\nDecember\n\n(5)\n\n        ►\n\nNovember\n\n(4)\n\n        ►\n\nOctober\n\n(6)\n\n        ►\n\nSeptember\n\n(4)\n\n        ►\n\nAugust\n\n(3)\n\n        ►\n\nJuly\n\n(3)\n\n        ►\n\nJune\n\n(2)\n\n        ►\n\nMay\n\n(2)\n\n        ►\n\nApril\n\n(1)\n\n        ►\n\nMarch\n\n(2)\n\n        ►\n\nFebruary\n\n(3)\n\n        ►\n\nJanuary\n\n(1)\n\n        ►\n\n2021\n\n(26)\n\n        ►\n\nDecember\n\n(2)\n\n        ►\n\nNovember\n\n(1)\n\n        ►\n\nOctober\n\n(1)\n\n        ►\n\nSeptember\n\n(3)\n\n        ►\n\nAugust\n\n(5)\n\n        ►\n\nJune\n\n(2)\n\n        ►\n\nApril\n\n(3)\n\n        ►\n\nMarch\n\n(4)\n\n        ►\n\nFebruary\n\n(4)\n\n        ►\n\nJanuary\n\n(1)\n\n        ►\n\n2020\n\n(25)\n\n        ►\n\nDecember\n\n(2)\n\n        ►\n\nNovember\n\n(5)\n\n        ►\n\nOctober\n\n(2)\n\n        ►\n\nSeptember\n\n(3)\n\n        ►\n\nAugust\n\n(2)\n\n        ►\n\nJuly\n\n(3)\n\n        ►\n\nJune\n\n(3)\n\n        ►\n\nMay\n\n(2)\n\n        ►\n\nApril\n\n(3)\n\nLabels\n\n3d\n(3)\n\n6502\n(29)\n\n65816\n(9)\n\n6800\n(2)\n\n68000\n(18)\n\n8051\n(2)\n\n9995\n(1)\n\na/ux\n(2)\n\nadministrivia\n(1)\n\naix\n(3)\n\nalpha micro\n(2)\n\namiga\n(1)\n\napple ii\n(4)\n\nappletalk\n(3)\n\narduino\n(1)\n\natari 8-bit\n(1)\n\natari st\n(2)\n\natarilab\n(1)\n\nbebox\n(3)\n\nbeos\n(6)\n\nbrowser\n(7)\n\nbucketlist\n(1)\n\nc128\n(7)\n\nc264\n(1)\n\nc64\n(14)\n\ncanon\n(2)\n\ncap-x comp-x\n(1)\n\ncasio\n(1)\n\nclassilla\n(1)\n\ncobalt\n(1)\n\ncommodore\n(20)\n\nconsole\n(2)\n\ncp/m\n(1)\n\ncp1600\n(1)\n\ncray\n(1)\n\ncrypto ancienne\n(4)\n\ndata general\n(2)\n\ndec\n(2)\n\ndec alpha\n(1)\n\ndecpro\n(1)\n\ndectalk\n(1)\n\ndick smith\n(2)\n\ndos\n(2)\n\ndreamcast\n(2)\n\nemulation\n(1)\n\nfirewire\n(1)\n\nforth\n(2)\n\nfouo\n(1)\n\nfuture\n(1)\n\ngeos\n(1)\n\ngopher\n(7)\n\ngraphics\n(6)\n\nhohoho\n(1)\n\nhp\n(2)\n\nhpux\n(1)\n\nhumour\n(2)\n\niannetta\n(1)\n\nibm\n(1)\n\nintellivision\n(1)\n\ninty\n(1)\n\nitanium\n(1)\n\nivory\n(1)\n\nkim-1\n(8)\n\nlinux\n(1)\n\nlisp\n(1)\n\nlynx\n(3)\n\nmac\n(14)\n\nmagic cap\n(2)\n\nmattel\n(1)\n\nmechanical\n(1)\n\nmemorials\n(12)\n\nmips\n(5)\n\nnetware\n(1)\n\nnetworking\n(13)\n\nnextstep\n(1)\n\nnubus\n(1)\n\npalm\n(7)\n\npanasonic\n(1)\n\nparisc\n(3)\n\npdp-11\n(1)\n\npocket handheld\n(3)\n\npong\n(7)\n\npower mac\n(20)\n\npowerpc\n(8)\n\nprior art\n(3)\n\nprotip\n(3)\n\nrefurb weekend\n(21)\n\nreview\n(6)\n\nscience\n(1)\n\nsega\n(1)\n\nsharp\n(1)\n\nsmartwatch\n(2)\n\nsoftware\n(51)\n\nsolbourne\n(3)\n\nsparc\n(3)\n\nspreadsheet\n(1)\n\nsun\n(2)\n\nsunray\n(2)\n\nsuperh\n(3)\n\nsymbolics\n(1)\n\ntadpole\n(1)\n\ntandy radio shack\n(2)\n\nterminal\n(11)\n\nti\n(4)\n\ntomy tutor\n(4)\n\ntoshiba\n(1)\n\nunboxing\n(1)\n\nunix\n(15)\n\nunscreenshotable\n(5)\n\nusb\n(4)\n\nusenet\n(1)\n\nvenix\n(1)\n\nvtech\n(1)\n\nwince\n(2)\n\nwindows\n(2)\n\nworkslate\n(2)\n\nx86\n(3)\n\nyaddayaddayadda\n(1)\n\nz80\n(3)\n\nCopyright 2020-25 Cameron Kaiser. CC BY-NC-ND 4.0. Powered by Blogger.",
    "summary": {
      "en": "In a humorous April Fools' prank in 2025, the author, a database programmer at a university, modified the READY message on campus printers to say \"INSERT 5 CENTS.\" This was done as a joke to make it seem like printing would require a fee. The author sent an email announcement about this new policy, which caused panic among staff who didn't know it was a prank. \n\nWhile many found it funny, the university administration panicked, leading to a chaotic situation. The author had to send multiple retractions to calm things down, and the incident attracted attention from higher-ups, resulting in trouble for him. Despite the initial backlash, the prank was eventually seen as a legendary joke, and the author learned to be more cautious in the future.",
      "ko": "2025년 만우절에 한 대학의 데이터베이스 프로그래머가 캠퍼스 프린터의 READY 메시지를 \"INSERT 5 CENTS\"로 변경하는 유머러스한 장난을 쳤다. 이 장난은 인쇄를 하려면 요금을 내야 하는 것처럼 보이게 하려는 의도로 진행되었다. 저자는 이 새로운 정책에 대한 이메일 공지를 보냈고, 이를 모르는 직원들 사이에서 혼란이 일어났다.\n\n많은 사람들이 이 장난을 재미있게 여겼지만, 대학 행정 측은 당황하여 혼란스러운 상황이 발생했다. 저자는 상황을 진정시키기 위해 여러 차례 사과 이메일을 보내야 했고, 이 사건은 상급자들의 주목을 받으면서 그에게 문제가 생겼다. 초기의 반발에도 불구하고, 이 장난은 결국 전설적인 농담으로 여겨지게 되었고, 저자는 앞으로 더 조심해야겠다는 교훈을 얻었다.",
      "ja": "2025年のエイプリルフールに、大学のデータベースプログラマーである著者が、キャンパスのプリンターのREADYメッセージを「5セントを挿入してください」と変更しました。これは、印刷に料金がかかるように見せかけるジョークでした。著者はこの新しいポリシーについてのメールを送信し、スタッフの中にはそれがジョークだと知らずにパニックになった人もいました。\n\n多くの人がこのジョークを面白いと感じた一方で、大学の管理者たちは混乱し、状況は混沌としました。著者は事態を収拾するために何度も訂正のメールを送らなければなりませんでした。この出来事は上層部の注目を集め、著者にとってトラブルの原因となりました。最初は反発がありましたが、このジョークは最終的に伝説的なものとして受け入れられ、著者は今後より慎重になることを学びました。"
    }
  },
  {
    "id": "ac5ed5d4df827b90",
    "title": {
      "en": "CERN scientists find evidence of quantum entanglement in sheep",
      "ko": "양에서 발견된 양자 얽힘 증거",
      "ja": "羊の量子もつれ発見！"
    },
    "type": "story",
    "url": "https://home.cern/news/news/physics/cern-scientists-find-evidence-quantum-entanglement-sheep",
    "score": 331,
    "by": "mackopes",
    "time": 1743505714,
    "content": "Toggle navigation\n\n                                                                               About\n\n\t\t\t\t\t\tCERN\n\nAt CERN, we probe the fundamental structure of particles that make up everything around us. We do so using the world's largest and most complex scientific instruments.\n\nKnow more\n\n              Who we are\n\n                Our Mission\n\n                Our Governance\n\n                Our Member States\n\n                Our History\n\n                Our People\n\n              What we do\n\n                Fundamental research\n\n                Contribute to society\n\n                Environmentally responsible research\n\n                Bring nations together\n\n                Inspire and educate\n\n                Fast facts and FAQs\n\n                Key Achievements\n\n\t\t\t\t\t\t  Key achievements submenu\n\n\tThe Higgs Boson\n\n\tThe W boson\n\n\tThe Z boson\n\n\tThe Large Hadron Collider\n\n\tThe Birth of the web\n\n\tAntimatter\n\n                                                                               News\n\n      Featured news, updates, stories, opinions, announcements\n\n                CERN releases report on the feasibility of a ...\n\n            Accelerators\n\n            News\n\n            31 March, 2025\n\n                Symmetry between up and down quarks is more b...\n\n            Physics\n\n            News\n\n            28 March, 2025\n\n                A new piece in the matter–antimatter puzzle\n\n            Physics\n\n            Press release\n\n            25 March, 2025\n\n                CERN announces winner of third Collide Copenh...\n\n            At CERN\n\n            News\n\n            18 March, 2025\n\n      Latest news\n\n              News\n\n                Accelerators\n\n                At CERN\n\n                Computing\n\n                Engineering\n\n                Experiments\n\n                Knowledge sharing\n\n                Physics\n\n                Events\n\n              CERN Community\n\n                News and announcements\n\n                Official communications\n\n                Events\n\n              Scientists\n\n                News\n\n                Press Room\n\n\t\t\t\t\t\t  Press Room submenu\n\n\tMedia News\n\n\tResources\n\n\tContact\n\n                                                                               Science\n\n\t\t\t\t\t\tScience\n\nThe research programme at CERN covers topics from kaons to cosmic rays, and from the Standard Model to supersymmetry\n\nKnow more\n\n              Physics\n\n                Antimatter\n\n                Dark matter\n\n                The early universe\n\n                The Higgs boson\n\n                The Standard Model\n\n                + More\n\n              Accelerators\n\n                CERN's accelerators\n\n                The Antiproton Decelerator\n\n                The Large Hadron Collider\n\n                High-Luminosity LHC\n\n                + More\n\n              Engineering\n\n                Accelerating: radiofrequency cavities\n\n                Steering and focusing: magnets and superconductivity\n\n                Circulating: ultra-high vacuum\n\n                Cooling: cryogenic systems\n\n                Powering: energy at CERN\n\n                + More\n\n              Computing\n\n                The CERN Data Centre\n\n                The Worldwide LHC Computing Grid\n\n                CERN openlab\n\n                Open source for open science\n\n                The birth of the web\n\n                + More\n\n              Experiments\n\n                ALICE\n\n                ATLAS\n\n                CMS\n\n                LHCb\n\n                + More\n\n                                                                               Resources\n\n          Featured resources\n\n                CERN Courier Jan/Feb 2025\n\n            Courier\n\n            Physics\n\n            1 January, 2025\n\n                High-Luminosity LHC images\n\n            Image\n\n            Accelerators\n\n            20 June, 2018\n\n                LHC Facts and Figures\n\n            Brochure\n\n            Knowledge sharing\n\n            10 May, 2022\n\n      See all resources\n\n              By Topic\n\n                Accelerators\n\n                At CERN\n\n                Computing\n\n                Engineering\n\n                Experiments\n\n                Knowledge sharing\n\n                Physics\n\n              By format\n\n                360 image\n\n                Annual report\n\n                Brochure\n\n                Bulletin\n\n                Courier\n\n                Image\n\n                Video\n\n                + More\n\n              By audience\n\n                CERN community\n\n                Educators\n\n                General public\n\n                Industry\n\n                Media\n\n                Scientists\n\n                Students\n\n                + More\n\n              search\n\n                  E.G. BIRTH OF WEB, LHC PAGE 1, BULLETIN...\n                  E.G. BIRTH OF WEB, LHC...\n\n      Search\n\n  Search\n\n                |\n                en\n\n      enfr\n\nenfr\n\nRelated Articles\n\n                CERN to change name for 70th Anniversary\n\n            At CERN\n\n            News\n\n            1 April, 2024\n\n                Time to change: CERN scientists propose 25-ho...\n\n            At CERN\n\n            News\n\n            1 April, 2023\n\n                CERN proposes “space elevator” accelerator\n\n            At CERN\n\n            News\n\n            1 April, 2021\n\n      View all news\n\nAlso On At CERN\n\n                CERN community: Join our spring scavenger hun...\n\n            At CERN\n\n            News\n\n            27 March, 2025\n\n                A trailblazer at 90\n\n            At CERN\n\n            News\n\n            27 March, 2025\n\n                CERN mobility: driven by data, naturally\n\n            At CERN\n\n            Opinion\n\n                                              Mar Capeans\n\n            27 March, 2025\n\n                CERN announces winner of third Collide Copenh...\n\n            At CERN\n\n            News\n\n            18 March, 2025\n\n                Prime Minister of Italy visits CERN\n\n            At CERN\n\n            News\n\n            13 March, 2025\n\n                The Prime Minister of Luxembourg visits CERN\n\n            At CERN\n\n            News\n\n            28 February, 2025\n\n                CERN invites photographers to explore the col...\n\n            At CERN\n\n            News\n\n            26 February, 2025\n\n                CERN Science Gateway welcomes its 500 000th v...\n\n            At CERN\n\n            News\n\n            24 February, 2025\n\n                International Day of Women and Girls in Scien...\n\n            At CERN\n\n            News\n\n            11 February, 2025\n\n      View all news\n\nFollow Us\n\nMore Social Media Accounts\n\nCERN\n\tEsplanade des Particules 1\n\tP.O. Box\n\t1211Geneva 23\n\tSwitzerland",
    "summary": {
      "en": "CERN is a leading research organization that studies the basic structure of particles that make up the universe using advanced scientific instruments. Its mission includes conducting fundamental research, contributing to society, and promoting international collaboration. CERN has made significant discoveries, such as the Higgs boson, and is known for the Large Hadron Collider, the world's largest particle accelerator. The organization also focuses on educating and inspiring the public about science. In addition to various experiments and projects, CERN emphasizes responsible environmental practices and knowledge sharing.",
      "ko": "CERN은 우주를 구성하는 기본 입자의 구조를 연구하는 선도적인 연구 기관입니다. 이곳은 첨단 과학 기기를 사용하여 기초 연구를 수행하고, 사회에 기여하며, 국제 협력을 촉진하는 것을 목표로 하고 있습니다. CERN은 힉스 보존과 같은 중요한 발견을 했으며, 세계에서 가장 큰 입자 가속기인 대형 하드론 충돌기로 잘 알려져 있습니다. 이 기관은 과학에 대한 대중의 교육과 영감을 주는 데에도 힘쓰고 있습니다. 다양한 실험과 프로젝트 외에도 CERN은 책임 있는 환경 관행과 지식 공유를 강조하고 있습니다.",
      "ja": "CERNは、宇宙を構成する粒子の基本的な構造を研究する先進的な研究機関です。CERNの使命は、基礎研究を行い、社会に貢献し、国際的な協力を促進することです。ヒッグス粒子の発見など、重要な成果を上げており、世界最大の粒子加速器である大型ハドロン衝突型加速器でも知られています。また、CERNは科学について一般の人々を教育し、インスパイアすることにも力を入れています。さまざまな実験やプロジェクトに加えて、環境に配慮した責任ある行動や知識の共有も重視しています。"
    }
  },
  {
    "id": "59c7916b52dac2fe",
    "title": {
      "en": "Netflix’s Media Production Suite",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://netflixtechblog.com/globalizing-productions-with-netflixs-media-production-suite-fc3c108c0a22",
    "score": 251,
    "by": "MattSayar",
    "time": 1743469353,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "23485dcaa52c0775",
    "title": {
      "en": "Simulated Economy Tutorial",
      "ko": "모의 경제 강좌",
      "ja": "シミュ経済入門"
    },
    "type": "story",
    "url": "https://jasonfantl.com/posts/Simulated-Economy-(1)/",
    "score": 99,
    "by": "jfantl",
    "time": 1743552347,
    "content": "Simulated Economy (1)  Posted Apr 7, 2023   Updated Oct 7, 2024    By  Jason Fantl    7 min readConceptMotivationImagine an open world RPG where your actions affect the price of goods, the markets reacting to anything the player may do (burn down wheat fields, cost of food increases; kill the merchants, prices differentiate between cities; sell the many swords you’ve collected on your adventures, tank the sword market). What would it take to have such an adaptive simulated economy? You could take a very simple approach and define a rule like The cost of a good is inversely proportional to the amount of that good in the game. But this will inevitable fail to capture the complex behavior we know economies to have. In order to create the desired emergent behavior, we must think at the level of the individual. By the end of this project we’ll have markets that converge to optimal prices, multiple coupled markets, inflation, geographically distinct economies, and merchants connecting cities, all of which adapts to any possible change in the environment. This work is in part inspired by Simulating Supply and Demand and Emergent Economies for Role Playing Games, both great resources if you want to explore more.We want a complex economy to emerge from simple actions taken by individuals, so how do people make economic decisions? This is an unimaginably deep question, so we need to start somewhere simple. Below is the motivating example to start off our economic model, but keep in mind there are many other approaches.You’re checking out a new super market in the neighborhood and see your favorite cereal, but then you see that it costs $10. “This is madness!” you think. You know that just down the road your usual super market sells the same cereal for $5, so you don’t buy the cereal. But the next day, at your usual super market, you find the price of cereal is now $10 as well. “This is unfortunate, but it seems the price of cereal has gone up, darn.” You still decide to buy the cereal since you really like it.This story outlines a very simple decision making algorithm, which although incomplete, gives us a great place to start. People seem to track two numbers when it comes to the price of a good: How much they personally value a good, and how much they expect that good to cost in the market. In the above story, our individual personally valued the cereal at more then $10, which we know since they eventually bought the cereal for $10. But this value alone is not enough to explain our story, if it were, then our individual would have immediately bought the cereal from the first market. Their expected market value of the cereal was much lower then the price they saw, so they knew they could probably buy it somewhere else for much cheaper, and that’s why they didn’t buy the first cereal. These two numbers are where we begin.ImplementationWe will begin with a single market. Each actor will keep track of how much they personally value a good and how much they expect that good to cost. From here we can tell if they are a buyer or seller: A buyer is someone who personally values a good more then they expect it to cost (for example, if they value a good at $10 and expect it to cost $8 in the market, then they are a buyer of that good), and a seller is someone who values a good less then what they expect it to cost in the market.This very first simulation will be as simple as possible, no money is given in a trade, no limited goods, no transaction costs, no diminishing returns, nothing except transaction offers. They will attempt to buy and sell with each other at random. But how does the expected market value change over time? In order to have a convergence of prices, we will have the buyer decrease their expected price after a transaction, and sellers increase their price. Essentially, the buyer is thinking “I bought this good for $10, next time I’ll try and buy it for $9”, while the seller thinks “I sold this for $10, next time I’ll try and sell it for $11”. The opposite happens on a failed transaction, the buyer thinking “I need to offer more next time if I want the good”. Perhaps at some point the buyer even becomes a seller when the expected price overcomes their personal value.The core of the code is shown below, but I will link the full repository (with commented code if you want to see the finer details.)  1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n// how quickly we should update our beliefs about the market\nbeliefVolatility := 0.1\n\n// find all buyers and sellers\nsellers := make([]*Actor, 0)\nbuyers := make([]*Actor, 0)\nfor actor := range actors {\n\tif actor.expectedMarketValue < actor.personalValue {\n\t\tbuyers = append(buyers, actor)\n\t} else {\n\t\tsellers = append(sellers, actor)\n\t}\n}\n\n// try to buy and sell\nmatchedCount := intMin(len(buyers), len(sellers))\nfor i := 0; i < matchedCount; i++ {\n\n\t// buyers and sellers are randomly matched up\n\tbuyer := buyers[i]\n\tseller := sellers[i]\n\n\t// attempt to transact\n\twillingSellPrice := seller.expectedMarketValue\n\twillingBuyPrice := buyer.expectedMarketValue\n\tif willingBuyPrice >= willingSellPrice {\n\t\t// transaction made, each should try to get an even better deal next time\n\t\tbuyer.expectedMarketValue -= beliefVolatility\n\t\tseller.expectedMarketValue += beliefVolatility\n\t} else {\n\t\t// transaction failed, each should make a better offer next time\n\t\tbuyer.expectedMarketValue += beliefVolatility\n\t\tseller.expectedMarketValue -= beliefVolatility\n\t}\n}\n\n// if you didn't get matched with anyone, offer a better deal next time\nfor i := matchedCount; i < len(buyers); i++ {\n\t// failed buyers should offer to buy at a higher price next time\n\tbuyers[i].expectedMarketValue += beliefVolatility\n}\nfor i := matchedCount; i < len(sellers); i++ {\n\t// failed sellers should offer to sell at a lower price next time\n\tsellers[i].expectedMarketValue -= beliefVolatility\n}\nWith this very simple decision making process we can run our first simulation. We will have 200 actors in this market, each starting with a random personal value and expected value. Below is the graph of expected values (green and red for buyers and sellers respectively) and personal values (pink). I modify actors personal values at different points in time to see how it effects the market.We see a quick convergence of expected values to what looks like the average of the personal values. Actually, its converging to the median. Our market applies forces that try and balance the number of buyers and sellers, penalizing those who don’t get matched up. Another perspective we can take is to consider the supply demand curves.Instead of being given the supply and demand curves, we need to derive them. Given that we know peoples personal values, we can determine for some hypothetical price how many people will be buyers and how many sellers. Graphing for every price the number of buyers will give us a demand curve, and similarly with sellers the supply curve. By finding at what price the two curves are equal, we find the theoretical optimal price. Below is again 200 people interacting every frame, some personal values changed at points in time. We add in the theoretical price to the graph (blue), as well as the supply and demand curves.The basic principal works well! We haven’t set a global price for a good, or set who should buy or sell, and yet we get a functioning economy that converges to the best possible market price and adapts to changes in actors personal values.NextCurrently we rely on a round based approach, but economies don’t function in these discrete rounds, people buy and sell at random times. People also don’t transact anything at the moment, they just exchange information, but no goods. Both these issues can be easily addressed, but the second will create a new and interesting problem: Scarcity.  Simulated Economy This post is licensed under  CC BY 4.0  by the author. Share             Recently UpdatedRediscovering QuaternionsDistributed Swarm Group AssignmentDynamic Decentralized Cluster Size EstimationDynamic Decentralized Cluster IdentificationSimulated Economy (1)ContentsConceptMotivationImplementationNextFurther Reading  Apr 7, 2023Simulated Economy (2) Roundless Currently the economy uses buy/sell rounds in order to match up buyers and sellers, but this won’t work in our final simulation. A simulated RPG economy will have buyers that somewhat ran...  Apr 7, 2023Simulated Economy (3) Multiple Markets Lets start with just two markets. We’ll have wood and chairs, with wood being used to build chairs. But first lets just look at the two markets uncoupled, each with personal values...  Apr 7, 2023Simulated Economy (4) Inflation Everyone seems to know that when the government prints too much money it causes inflation, but why? If everyone suddenly doubled their money, they would be willing to buy more, increasing... Peer to Peer Chat Network in Go (6) Simulated Economy (2)",
    "summary": {
      "en": "The text discusses the creation of a simulated economy for an open-world RPG where players' actions influence market prices. The aim is to develop a complex economic system that reflects real-world behaviors through simple individual actions. \n\nKey points include:\n\n1. **Concept**: The economy will adapt to player actions, such as destroying resources or trading items, affecting prices and market dynamics.\n\n2. **Motivation**: An example illustrates how individuals make purchasing decisions based on personal value and expected market price. If a player values an item more than its expected price, they are likely to buy it.\n\n3. **Implementation**: The initial simulation involves a simple market where actors (buyers and sellers) adjust their expected prices based on transaction outcomes. Buyers lower their expected price after a purchase, while sellers raise theirs. This process continues until the market reaches a balance.\n\n4. **Results**: The simulation shows that expected prices converge towards the average of individual personal values, creating a functional economy without predefined prices.\n\n5. **Future Improvements**: The simulation currently uses rounds for transactions but will evolve to allow for random buying and selling, introducing more complexity and the concept of scarcity.\n\nOverall, the goal is to create a dynamic and responsive economic system that mimics real-world markets through player interactions.",
      "ko": "이 텍스트는 플레이어의 행동이 시장 가격에 영향을 미치는 오픈 월드 RPG의 시뮬레이션 경제 시스템 구축에 대해 설명합니다. 목표는 간단한 개별 행동을 통해 실제 세계의 행동을 반영하는 복잡한 경제 시스템을 개발하는 것입니다.\n\n주요 내용은 다음과 같습니다. 경제는 자원을 파괴하거나 아이템을 거래하는 등의 플레이어 행동에 따라 적응하며, 이는 가격과 시장 역학에 영향을 미칩니다. 예를 들어, 개인이 개인적인 가치와 예상 시장 가격에 따라 구매 결정을 내리는 과정을 보여줍니다. 만약 플레이어가 어떤 아이템을 예상 가격보다 더 높게 평가한다면, 그 아이템을 구매할 가능성이 높습니다.\n\n초기 시뮬레이션은 거래 결과에 따라 구매자와 판매자가 예상 가격을 조정하는 간단한 시장을 포함합니다. 구매자는 구매 후 예상 가격을 낮추고, 판매자는 가격을 높입니다. 이 과정은 시장이 균형에 도달할 때까지 계속됩니다. 시뮬레이션 결과, 예상 가격이 개인의 가치 평균으로 수렴하며, 미리 정해진 가격 없이도 기능하는 경제가 형성됩니다.\n\n현재 시뮬레이션은 거래를 위한 라운드를 사용하지만, 앞으로는 무작위 구매와 판매를 허용하여 더 복잡한 시스템과 희소성 개념을 도입할 예정입니다. 전반적으로 목표는 플레이어의 상호작용을 통해 실제 시장을 모방하는 동적이고 반응적인 경제 시스템을 만드는 것입니다.",
      "ja": "オープンワールドRPGのために、プレイヤーの行動が市場価格に影響を与えるシミュレーション経済の構築について説明しています。このシステムの目的は、個々の簡単な行動を通じて現実の行動を反映する複雑な経済システムを開発することです。\n\nこの経済は、プレイヤーの行動に応じて変化します。例えば、資源を破壊したりアイテムを取引したりすることで、価格や市場の動向に影響を与えます。\n\n個人が購入決定を下す際の動機付けの例として、個人の価値観や予想される市場価格に基づいて判断することが挙げられます。プレイヤーがアイテムの価値を予想価格よりも高く評価する場合、そのアイテムを購入する可能性が高くなります。\n\n初期のシミュレーションでは、買い手と売り手が取引結果に基づいて期待価格を調整するシンプルな市場を使用しています。買い手は購入後に期待価格を下げ、売り手はそれを上げます。このプロセスは市場が均衡に達するまで続きます。\n\nシミュレーションの結果、期待価格は個々の価値の平均に収束し、あらかじめ設定された価格がない機能的な経済が形成されることが示されました。\n\n今後の改善点として、現在は取引にラウンドを使用していますが、ランダムな売買を可能にすることで、より複雑で希少性の概念を導入する予定です。\n\n全体として、プレイヤーの相互作用を通じて現実の市場を模倣する動的で応答性のある経済システムの構築を目指しています。"
    }
  },
  {
    "id": "e490058bd8d0d384",
    "title": {
      "en": "New in Gmail: Making E2E encrypted emails easy to use for all organizations",
      "ko": "Gmail의 E2E 암호화, 모두를 위한 간편함!",
      "ja": "Gmailの新機能：簡単E2E暗号化メール"
    },
    "type": "story",
    "url": "https://workspace.google.com/blog/identity-and-security/gmail-easy-end-to-end-encryption-all-businesses",
    "score": 68,
    "by": "skim",
    "time": 1743520826,
    "content": "Identity and SecurityNew in Gmail: Making end-to-end encrypted emails easy to use for all organizationsApril 2, 2025Johney BurkeSenior Product Manager, Google WorkspaceJulien DuplantProduct Manager, Google WorkspaceTry Google Workspace at No CostGet a business email, all the storage you need, video conferencing, and more.SIGN UP At Google, we believe that secure, confidential communication should be available for organizations of all sizes. However, end-to-end encrypted (E2EE) email was historically a privilege reserved for organizations with significant IT resources, due to the complexity of S/MIME and proprietary solutions. Over the past two years, we've made progress in breaking down these barriers, simplifying E2EE, to help customers address their critical compliance and data sovereignty needs. But we knew there was more work to do to truly democratize it.Today is Gmail’s birthday, and we wanted to do something special — enable enterprise users to send E2EE messages to any user on any email inbox with just a few clicks. This capability, requiring minimal efforts for both IT teams and end users, abstracts away the traditional IT complexity and substandard user experiences of existing solutions, while preserving enhanced data sovereignty, privacy, and security controls. We’re rolling this out in a phased approach, starting today, in beta, with the ability to send E2EE emails to Gmail users in your own organization. In the coming weeks, users will be able to send E2EE emails to any Gmail inbox, and, later this year, to any email inbox. Let’s take a closer look. Sending an E2EE email to a non-Gmail userThe current state of encrypted email — the good, the bad, and the unpleasantMost enterprise email providers encrypt customer data at rest and in transit. Gmail does it by default. The Secure/Multipurpose Internet Mail Extensions (S/MIME) is a protocol that enables sending digitally signed and encrypted messages. It is typically used for highly-sensitive emails among regulated organizations, such as government agencies and businesses that work with them.While more organizations have real needs for E2EE emails, few have the resources to implement S/MIME. IT teams need to acquire and manage certificates and deploy them to each user, resulting in additional efforts and costs. And end users have to figure out whether they and the recipient have S/MIME configured (few do), and then go through the hassle of exchanging certificates before the encrypted emails can be exchanged. This often results in frustration and the inability to send encrypted emails.Alternatives to S/MIME, such as encryption features from email providers or proprietary point solutions, present significant drawbacks as well: the former requires sharing encryption keys, increasing data privacy and sovereignty risks, while the latter complicates end user experiences with custom applications, portals, or browser extensions. We think there should be a simpler and more efficient way.Sending end-to-end encrypted emails to any inbox with GmailThe idea here is simple. Email messages are encrypted with just a few clicks in Gmail regardless of who they are being sent to — no need for end users to exchange certificates or use custom software. The emails are protected using encryption keys controlled by the customer and not available to Google servers, providing enhanced data privacy and security. And the IT team no longer needs to go through the complex S/MIME setup or certificate management. This is how it works behind the scenes:When the recipient is a Gmail user (enterprise or personal), Gmail sends an E2EE email. The email is automatically decrypted in the recipient's inbox, and the recipient can use Gmail in a familiar way.When the recipient is not a Gmail user, Gmail sends them an invitation to view the E2EE email in a restricted version of Gmail. The recipient can then use a guest Google Workspace account to securely view and reply to the email.When the recipient has S/MIME configured, Gmail sends an E2EE email via S/MIME (just like it does today).Securely viewing an E2EE email in a restricted version of GmailIT teams also have the option to require all external recipients (even if they are Gmail users) to use the restricted version of Gmail. This helps ensure that their organization’s data does not end up stored on third-party servers and devices. It also makes it easier for organizations to protect their data by having the ability to apply security policies and revoke access to emails, no matter how long ago they were sent. Essentially, the E2EE email becomes like a document in Google Drive, allowing the IT team to control its access.This new capability is powered by client-side encryption (CSE), a technical control in Workspace, that helps organizations to protect their sensitive emails, documents, calendar events, and meetings using encryption keys that are under their sole control and stored outside of Google’s infrastructure in a location of their choice. Data gets encrypted on the client before it is transmitted or stored in Google's cloud-based storage, rendering it indecipherable to Google and other third-party entities and helping to meet regulatory requirements, such as data sovereignty, HIPAA, and export controls.Additional security and sovereignty enhancements in GmailBeyond E2EE emails, we’re also making a number of security capabilities in Gmail generally available to help organizations keep their data secure and compliant, including:CSE default mode — allows IT admins to set a policy that makes E2EE messages a default setting in Gmail for teams that regularly deal with sensitive data.Classification labels — help users in your organization understand message sensitivity and handle messages accordingly.Data loss prevention (DLP) — allows IT teams to leverage rules to automatically apply classification labels to messages and take action on messages based on their labels, such as blocking email message delivery.A new threat protection AI model — we added a new holistic AI model in Gmail that acts as a supervisor to our existing AI/ML and heuristic defenses. It evaluates thousands of combined signals from billions of endpoints based on the actor, behavior, and content to catch more spam and phishing before they reach users.Getting startedWith data security and sovereignty, the job is never done. We are always at work to help our customers — from small businesses and large enterprises to schools and government agencies — strengthen their security and compliance posture. To get early access for E2EE emails in Gmail, let us know. To learn more, check out the documentation, understand what’s included in Assured Controls plans, and attend our upcoming sessions at Cloud Next ‘25.Posted inIdentity and SecurityGmailRelated articlesIdentity and SecurityGemini in Workspace apps and the Gemini app are first to achieve FedRAMP High authorizationBy Alice Rison • 3-minute readIdentity and SecurityGoogle Cloud named a Leader in the 2025 Forrester Data Security Platforms WaveBy Archana Ramamoorthy • 2-minute readIdentity and SecurityNew webinar and global survey reveal why incremental cyber security fixes don’t workBy Andy Wen • 4-minute readIdentity and SecurityFrom checkboxes to checkmates: How Google Workspace can help you achieve CMMC 2.0 complianceBy Ashleigh Laone • 3-minute read",
    "summary": {
      "en": "Google is making it easier for organizations to use end-to-end encrypted (E2EE) emails in Gmail. Historically, E2EE required complex setups like S/MIME, which many organizations couldn't manage. Now, Gmail allows users to send E2EE emails simply, without needing to exchange certificates or use complicated software. \n\nStarting today, enterprise users can send E2EE emails to anyone within their organization, with plans to expand to all Gmail users soon and eventually to all email inboxes. When sending an E2EE email, if the recipient is a Gmail user, the email is automatically decrypted. If the recipient is not a Gmail user, they receive an invitation to view the email securely in a restricted version of Gmail.\n\nThis new feature enhances privacy, as the encryption keys are controlled by the customer and not Google. IT teams also benefit by avoiding complex certificate management. Additional security features being introduced include default E2EE settings for sensitive data, classification labels for emails, data loss prevention tools, and a new AI model to enhance threat protection.\n\nOverall, Google is committed to making secure communication accessible for all organizations, helping them meet compliance and security needs.",
      "ko": "구글이 Gmail에서 종단 간 암호화(End-to-End Encryption, E2EE) 이메일을 사용하는 것을 더 쉽게 만들고 있습니다. 이전에는 E2EE를 설정하기 위해 S/MIME과 같은 복잡한 절차가 필요했지만, 많은 조직이 이를 관리하기 어려웠습니다. 이제 Gmail은 사용자들이 인증서를 교환하거나 복잡한 소프트웨어를 사용할 필요 없이 간단하게 E2EE 이메일을 보낼 수 있도록 하고 있습니다.\n\n오늘부터 기업 사용자는 조직 내의 누구에게나 E2EE 이메일을 보낼 수 있으며, 곧 모든 Gmail 사용자로 확대될 예정이고 궁극적으로 모든 이메일 수신함으로 확장될 계획입니다. E2EE 이메일을 보낼 때, 수신자가 Gmail 사용자라면 이메일이 자동으로 복호화됩니다. 만약 수신자가 Gmail 사용자가 아닐 경우, 제한된 버전의 Gmail에서 안전하게 이메일을 볼 수 있도록 초대장이 전송됩니다.\n\n이 새로운 기능은 고객이 암호화 키를 관리하게 되어 개인정보 보호를 강화합니다. 또한 IT 팀은 복잡한 인증서 관리를 피할 수 있어 이점이 있습니다. 추가로 도입되는 보안 기능으로는 민감한 데이터에 대한 기본 E2EE 설정, 이메일 분류 레이블, 데이터 손실 방지 도구, 위협 보호를 강화하는 새로운 AI 모델이 포함됩니다.\n\n전반적으로 구글은 모든 조직이 안전한 커뮤니케이션을 이용할 수 있도록 지원하며, 이들이 규정 준수와 보안 요구를 충족할 수 있도록 돕고 있습니다.",
      "ja": "Googleは、組織がGmailでエンドツーエンド暗号化（E2EE）メールを簡単に利用できるようにしています。これまで、E2EEを利用するにはS/MIMEのような複雑な設定が必要で、多くの組織が対応できませんでした。しかし、今ではGmailを使えば、証明書の交換や複雑なソフトウェアを使わずにE2EEメールを送信できるようになりました。\n\n本日から、企業のユーザーは自組織内の誰にでもE2EEメールを送信できるようになり、近い将来にはすべてのGmailユーザー、最終的にはすべてのメールボックスに拡大する予定です。E2EEメールを送信する際、受信者がGmailユーザーであれば、メールは自動的に復号されます。Gmailユーザーでない受信者には、制限されたバージョンのGmailで安全にメールを閲覧するための招待が届きます。\n\nこの新機能はプライバシーを強化します。暗号化キーは顧客が管理し、Googleが管理することはありません。また、ITチームは複雑な証明書管理を避けることができるため、利便性も向上します。さらに、機密データに対するデフォルトのE2EE設定、メールの分類ラベル、データ損失防止ツール、新しいAIモデルによる脅威保護の強化など、追加のセキュリティ機能も導入されます。\n\n全体として、Googleはすべての組織が安全なコミュニケーションを利用できるように努めており、コンプライアンスやセキュリティのニーズに応える手助けをしています。"
    }
  },
  {
    "id": "9187c52f129e4146",
    "title": {
      "en": "Open source, 3D-printable smart chess board",
      "ko": "오픈소스 스마트 체스판",
      "ja": "オープンソース3Dチェスボード"
    },
    "type": "story",
    "url": "https://thangs.com/designer/Concept%20Bytes/3d-model/Open%20Chess%20-%20Smart%20Chess%20Board-1300202",
    "score": 61,
    "by": "bdcravens",
    "time": 1743357542,
    "content": "Concept Bytes178 followersFollowConcept Bytes|Image 1 of 42Concept Bytes|Image 1 of 4222Add Make78ShareAdd MakeOpen Chess - Smart Chess Board155downloads·6 days ago·Print. Click. Play.\nOpenChess is a fully open source smart chessboard designed to make interactive, DownloadPrint. Click. Play.\nOpenChess is a fully open source smart chessboard designed to make interactive, intelligent gameplay accessible to everyone. By combining low-cost electronics, 3D printing, and customizable software, OpenChess empowers makers, educators, and chess lovers to build their own connected chess experience — without the high price tag.\n💡 Why OpenChess?\nMost smart chessboards are expensive and closed-source. OpenChess changes that. It’s built from the ground up to be:\nAffordable – Uses a custom PCB with embedded magnetic sensors and LEDs for real-time move detection and feedback.\nDIY-Friendly – Print your own board, your own pieces, and assemble everything at home.\nProgrammable – Based on Arduino, you can customize behavior, create new game modes, or build learning tools and challenges.\nModular & Open – Everything is open source. Remix the board, design your own pieces, or upgrade the electronics however you want.\n🔧 What You’ll Need:\nOpenChess PCB – Detects pieces and lights up squares.\nhttps://concept-bytes.com/products/openchess-pcb?variant=50601021833517\nOr make your own! Gerbers are attached below! Just send them to JLCPCB!\nArduino Nano ESP32/RP2040/33IOT – Controls the logic and communication.\nhttps://www.stemsolutionsmart.com/products/arduino-nano-rp2040-connect\n3D Printed Board & Pieces – Print everything from scratch or customize it to your style.\nAttached below or over at Maker world and Thangs!\nhttps://makerworld.com/en/models/1256302-openchess-smart-chess-board#profileId-1279762\nWhether you’re building a smart chessboard for your classroom, designing a game with custom rules, or just want a cool weekend project — OpenChess puts the power in your hands.\n🎓 Print it. Click it. Play it.\nWelcome to the future of chessTags:ChessgamesdiyfunAdd a comment...More modelsConcept BytesTipsy The Ai Cocktail MakerOpen Chess - Smart Chess Board155downloads·6 days agoinToys & GamesandBambu Lab78ShareAdd MakeConcept Bytes178 followersFollowDownloadMore modelsConcept BytesTipsy The Ai Cocktail Maker",
    "summary": {
      "en": "OpenChess is an open-source smart chessboard that makes interactive chess games affordable and accessible for everyone. It uses inexpensive electronics and 3D printing, allowing users to create their own chess experiences without spending a lot of money.\n\nKey Features:\n- **Affordable**: Uses a custom circuit board with magnetic sensors and lights for real-time move detection.\n- **DIY-Friendly**: Users can print their own board and pieces at home.\n- **Programmable**: Built on Arduino, it can be customized for different game modes or educational tools.\n- **Modular & Open**: Everything is open-source, enabling users to modify the board and design their own pieces.\n\nTo build OpenChess, you'll need:\n- An OpenChess PCB for detecting pieces.\n- An Arduino Nano for controlling the board.\n- 3D printed components, which can be customized.\n\nOpenChess is great for classrooms, custom games, or fun projects, putting creative control in the hands of the user.",
      "ko": "OpenChess는 누구나 저렴하게 이용할 수 있는 오픈 소스 스마트 체스판입니다. 이 체스판은 저렴한 전자기기와 3D 프린팅 기술을 활용하여 사용자가 많은 비용을 들이지 않고도 자신만의 체스 경험을 만들 수 있도록 돕습니다.\n\n주요 특징으로는 첫째, 저렴한 가격에 맞춰 설계된 맞춤형 회로 기판이 있어 자석 센서와 조명을 이용해 실시간으로 수를 감지합니다. 둘째, 사용자가 집에서 직접 체스판과 말을 인쇄할 수 있어 DIY에 적합합니다. 셋째, 아두이노 기반으로 제작되어 다양한 게임 모드나 교육 도구로 커스터마이즈할 수 있습니다. 마지막으로, 모든 것이 오픈 소스이기 때문에 사용자가 체스판을 수정하고 자신만의 말을 디자인할 수 있습니다.\n\nOpenChess를 만들기 위해서는 체스 말을 감지할 수 있는 OpenChess PCB와 보드를 제어할 아두이노 나노, 그리고 사용자 맞춤형으로 제작할 수 있는 3D 프린팅 부품이 필요합니다. OpenChess는 교실, 맞춤형 게임, 또는 재미있는 프로젝트에 적합하며, 사용자에게 창의적인 제어권을 제공합니다.",
      "ja": "OpenChessは、インタラクティブなチェスゲームを手頃な価格で楽しめるオープンソースのスマートチェスボードです。安価な電子部品と3Dプリントを活用しており、ユーザーは少ない費用で自分だけのチェス体験を作り出すことができます。\n\n主な特徴としては、まず手頃な価格であることが挙げられます。カスタム回路基板に磁気センサーとライトを搭載し、リアルタイムでの手の動きを検出します。また、DIYに適しており、ユーザーは自宅で自分のボードや駒を印刷することができます。さらに、Arduinoを基にしているため、さまざまなゲームモードや教育用ツールにカスタマイズ可能です。すべてがオープンソースであるため、ユーザーはボードを改造したり、自分の駒をデザインしたりすることができます。\n\nOpenChessを作るには、駒を検出するためのOpenChess PCB、ボードを制御するためのArduino Nano、そしてカスタマイズ可能な3Dプリント部品が必要です。OpenChessは教室やカスタムゲーム、楽しいプロジェクトに最適で、ユーザーに創造的なコントロールを提供します。"
    }
  },
  {
    "id": "1bcd23019c816f35",
    "title": {
      "en": "US Marines to get high-speed, radar-evading electric seagliders for rescue ops",
      "ko": "전투용 전기 해양 글라이더 도입",
      "ja": "米海兵隊、電動シーグライダー導入！"
    },
    "type": "story",
    "url": "https://interestingengineering.com/military/us-marines-seagliders-for-rescue-ops",
    "score": 132,
    "by": "jdmark",
    "time": 1743199017,
    "content": "ShareMilitaryUS Marines to get high-speed, radar-evading electric seagliders for rescue opsThe Viceroy seaglider can soar up to 180 mph over approximately 180 miles.\nUpdated: Mar 27, 2025 06:03 PM EST1Transportation🚀715 mph top speed: Bombardier to debut fastest civilian jet since ConcordeChris Younga day ago2Space🚀Mars may hold a massive water reservoir, enough to flood the planet up to nine feetNeetika Walter2 days ago3Health🚀Men’s turn: US scientists unveil a hormone-free male birth control pill!Aamir Khollam2 days ago4Science🚀Mysterious magnetic anomaly over Earth puzzles scientists, risks space techNeetika Walter2 days ago5Culture🚀OpenAI’s biggest surge since ChatGPT: AI Ghibli art “melts” GPUs as 1M users flood inAamir Khollam2 days ago6Military🚀US starts producing nuclear warhead '24 times more powerful than Hiroshima bomb'Chris Young2 days ago7Innovation🚀World’s smallest: Bee-mimicking flying robot uses magnets to aid in search, rescueJijo Malayil2 days ago8Science🚀Quantum camera safely captures first image of mammal embryo; can aid in IVFMaria Mocerino2 days ago9Energy🚀China hits jackpot with massive 110-million-ton offshore oil discoveryMaria Mocerino2 days ago10Science🚀407-million-year-old: Fossil of 1st land-dwelling giant breaks the tree of lifeMrigakshi Dixit2 days ago1Space🌟Humans orbit Earth over north, south poles for first time with SpaceX Fram2 missionChris Young43 minutes ago2Science🌟Alligator-sized amphibians found buried together in 230 million-year-old mass graveGeorgina Jedikovskaan hour ago3Culture🌟Alcohol makes males more attractive to females flies, mating chances rise dramaticallyGeorgina Jedikovskaan hour ago4Energy🌟GreenMet, Tanbreez strike deal for Greenland rare earths amid Trump’s takeover pushNeetika Walter2 hours ago5Science🌟North American continent slowly losing rock from its underside, discover scientistsMrigakshi Dixit2 hours ago6Transportation🌟Historic Tesla collapse: Worst sales drop ever as Musk faces political backlashAamir Khollam3 hours ago7Energy🌟China's next-gen HL-3 tokamak achieves dual 210 million °F milestone in a firstPrabhat Ranjan Mishra3 hours ago8Health🌟World's smallest: Injectable pacemaker uses body fluids for power, dissolves post useSrishti Gupta4 hours ago9Innovation🌟Quantum sandwich: Scientists merge 'impossible' materials for next-gen computingKaif Shaikh6 hours ago10Innovation🌟Tuna-inspired robotic fin can help deep-sea drones swim better than fishJijo Malayil6 hours agoKapil Kajal6 days ago2ShareViceroy seaglider Regent Craft\nRegent Craft, a developer specializing in all-electric seagliders based in Rhode Island, has completed its initial contract with the U.S. Marine Corps Warfighting Lab (MCWL).\n\nIn a significant progression, the company has secured a follow-up agreement, estimated at $10 million, which offers opportunities for further extensions.\n\nThe initial phase of Regent’s partnership with MCWL, valued at $4.75 million, encompassed twelve key deliverables that validated the technical feasibility of seagliders.\n\nThis phase began with testing a quarter-scale prototype and culminated in the recently successful sea trials of the full-scale Viceroy prototype.\n\nHigh-speed, radar-proof electric seagliders\n\nThe second phase of the agreement aims to deepen the evaluation of the Viceroy’s technical capabilities.\n\nThis will involve demonstrations relevant to specific defense operations, a crucial aspect as military needs evolve.\n\nTom Huntley, Vice President of Government Relations and Defense at Regent, expressed pride in continuing collaboration with the Marine Corps.\n\nHe highlighted the role of Regent’s seagliders in meeting national security requirements, specifically in logistics operations that face challenges in contested maritime environments.\n\nThe advantages of seagliders are particularly noteworthy for defense operations, where speed and efficiency can determine the success of missions.\n\nThe Viceroy seaglider can soar up to 180 mph over approximately 180 miles.\n\nOne of the standout features of sea gliders is their ability to take off and land on water.\n\nThis capability eliminates reliance on traditional runways, which can be vulnerable in conflict situations.\n\nFurthermore, these electric-powered vessels can be recharged from shore and ship resources, ensuring a dependable energy source during operations.\n\nAnother significant benefit is their low radar and sonar signatures. By flying close to the water’s surface, seagliders avoid radar detection while minimizing heat and infrared visibility due to their electric propulsion systems.\n\nFor rescue ops\n\nTheir design simplicity and fewer components translate into lower operational and maintenance costs when compared to traditional aviation and maritime vehicles.\n\nEarlier this year, Regent made strides in its manufacturing capabilities, breaking ground on a facility in the Quonset Business Park in North Kingstown, RI.\n\nSet to be operational by 2026, this facility will focus on component manufacturing, vehicle assembly, and testing for the Viceroy seaglider.\n\nRecent progress has included initiating sea trials for the Viceroy prototype in Narragansett Bay, representing a key milestone in advancing the seaglider’s maritime certification.\n\nFurthermore, Regent has submitted its Viceroy Design Basis Agreement to the U.S. Coast Guard, a necessary step in the certification process.\n\nAs geopolitical dynamics shift focus from land-based conflicts to maritime challenges, the complexities of operations in coastal and contested areas become increasingly pronounced.\n\nThe “tyranny of distance” concept describes the difficulty in efficiently transporting personnel and supplies between combat zones and resupply points.\n\nSituations such as island-hopping strategies in the Indo-Pacific or urgent rescue missions in open waters highlight the necessity for innovative solutions.RECOMMENDED ARTICLES\n\nHuntley emphasized the strategic advantages sea gliders could provide in maritime operations.\n\nDrawing from his experience with humanitarian missions as a former U.S. Coast Guard pilot, he underscored the potential of these vessels to enhance military operations in challenging environments.\n\nSeagliders could play a pivotal role in modern defense strategies, offering new options in increasingly complex operational landscapes.\n2COMMENTSABOUT THE EDITORKapil Kajal Kapil Kajal is an award-winning journalist with a diverse portfolio spanning defense, politics, technology, crime, environment, human rights, and foreign policy. His work has been featured in publications such as Janes, National Geographic, Al Jazeera, Rest of World, Mongabay, and Nikkei. Kapil holds a dual bachelor's degree in Electrical, Electronics, and Communication Engineering and a master’s diploma in journalism from the Institute of Journalism and New Media in Bangalore.NEWSLETTERThe Blueprint DailyStay up-to-date on engineering, tech, space, and science news with The Blueprint.Mail Me  By clicking sign up, you confirm that you accept this site's Terms of Use and Privacy PolicyNewsmilitaryPOPULAR ARTICLES1healthImmune changes during pregnancy may protect against long COVID, study showsAamir Khollam20 hours ago2militaryNorthrop Grumman boosts F-16’s combat edge with $14 million radar upgradeNeetika Walter21 hours ago3innovationNew metamaterial stores 160x more energy, paving the way for smarter robotsAamir Khollama day ago4science50,000-year-old Neanderthal-like tools found in China challenge evolution theoriesNeetika Waltera day agoRELATED ARTICLESinnovationTuna-inspired robotic fin can help deep-sea drones swim better than fishtransportationRussia eyes more 120 MW nuclear icebreakers to dominate icy Northern Sea RouteinnovationNew tech for multiple security robot control unveiled, allows 24/7 surveillanceenergyUS: 2500 MWe nuclear plant that can power over 1.9 million homes to work till 2050sJOBSLoading opportunities...",
    "summary": {
      "en": "The U.S. Marines are set to receive high-speed, electric seagliders called the Viceroy, which can travel up to 180 mph for about 180 miles. Regent Craft, the company behind the seagliders, has completed a contract with the Marine Corps and secured an additional $10 million agreement to further develop the technology. \n\nThese seagliders are designed to assist in military operations, particularly in challenging maritime environments, by taking off and landing on water without the need for runways. They are electric-powered, can be recharged from shore, and have low radar and sonar visibility, making them ideal for stealth operations.\n\nRegent Craft is enhancing its manufacturing capabilities with a new facility in Rhode Island, expected to be operational by 2026, to support the production and testing of the Viceroy. The seagliders could play a crucial role in modern defense strategies, especially in complicated coastal operations and rescue missions.",
      "ko": "미국 해병대는 Viceroy라는 고속 전기 해상 글라이더를 도입할 예정입니다. 이 글라이더는 시속 180마일로 약 180마일을 이동할 수 있습니다. 해상 글라이더를 개발한 Regent Craft는 해병대와 계약을 체결하고 기술 개발을 위한 추가 1천만 달러 계약을 확보했습니다.\n\n이 해상 글라이더는 특히 어려운 해양 환경에서 군사 작전을 지원하도록 설계되었습니다. 이들은 수면에서 이착륙할 수 있어 활주로가 필요하지 않습니다. 전기로 작동하며, 육지에서 충전할 수 있고 레이더와 소나에 잘 탐지되지 않아 은밀한 작전에 적합합니다.\n\nRegent Craft는 로드아일랜드에 새로운 시설을 세우고 있으며, 이 시설은 2026년까지 가동될 예정입니다. 이를 통해 Viceroy의 생산과 테스트를 지원할 계획입니다. 이 해상 글라이더는 현대 방어 전략에서 중요한 역할을 할 수 있으며, 특히 복잡한 해안 작전과 구조 임무에서 큰 도움이 될 것입니다.",
      "ja": "アメリカ海兵隊は、時速180マイルで約180マイルの距離を移動できる高速電動シーグライダー「バイサロイ」を導入することになりました。このシーグライダーを開発したレジェントクラフト社は、海兵隊との契約を完了し、技術のさらなる開発のために追加で1,000万ドルの契約を獲得しました。\n\nこのシーグライダーは、特に厳しい海洋環境での軍事作戦を支援するために設計されており、滑走路なしで水上からの離着陸が可能です。電動で動き、岸から充電できるため、低いレーダーおよびソナーの可視性を持ち、隠密作戦に最適です。\n\nレジェントクラフト社は、バイサロイの生産とテストを支えるために、2026年までに稼働予定の新しい工場をロードアイランド州に設立し、製造能力を強化しています。このシーグライダーは、特に複雑な沿岸作戦や救助ミッションにおいて、現代の防衛戦略において重要な役割を果たす可能性があります。"
    }
  },
  {
    "id": "ab0513d7954bbd01",
    "title": {
      "en": "Show HN: JavaScript PubSub in 163 Bytes",
      "ko": "163바이트 자바스크립트 PubSub",
      "ja": "163バイトのPubSub"
    },
    "type": "story",
    "url": "https://github.com/hassanshaikley/pico-pubsub",
    "score": 100,
    "by": "hmmokidk",
    "time": 1743385075,
    "content": "pico-pubsub\nThe smallest PubSub library possible. Zero Dependencies. 149 bytes.\nI wrote this article a while back. But I realized...why not just publish the code?\nSmaller than the competition.\n\nnano-pubsub\ntiny-pubusb\n\nBuilt with JS13K games in mind. Such as cred which is unfortunately in need of some weight loss soon, it is almost 25KB now.\nIf you have any ideas that may trim off even one single byte please share it. Create an issue! I don't mind.\nThe Source\nThis is the entire source (index.js).\nlet t = new EventTarget();\n\nsub = (e, c) => (t.addEventListener(e, c), () => t.removeEventListener(e, c));\npub = (n, d) => t.dispatchEvent(new CustomEvent(n, { detail: d }));\n\nUsage\nnpm install pico-pubsub\n\nimport \"pico-pubsub\"\n\nconst unsub = sub('jump', function (anything) {\n  console.log(\"someone jumped - \" + anything.detail)\n});\n\npub('jump', \"a_user_id\")\n>> \"someone jumped - a_user_id\"\n\nunsub()\n\npub('jump', \"another_user_id\")\n>> Nothing happens now\n\nTroubleshoot\n\nMight add TS support in the future. For now you can use the following snippet.\n\ndeclare global {\n  function pub(event: string, data: any): VoidFunction;\n  function sub(event: string, callback: (data: CustomEvent) => void): void;\n}\n\nIf you have export issues just copy paste and change export type.\n\nProve it\nThe following command will produce a 149b file:\nnpx esbuild index.js --bundle --minify --format=esm --outfile=bundle.js\nThe Competition\nComing in at #2 we have nano-pubsub which slims down to an impressive 194b...Not bad at all! Only ~30% larger.\n/**\n * @public\n */\nexport interface Subscriber<Event> {\n  (event: Event): void;\n}\n/**\n * @public\n */\nexport interface PubSub<Message> {\n  publish: (message: Message) => void;\n  subscribe: (subscriber: Subscriber<Message>) => () => void;\n}\n\n/**\n * @public\n */\nexport default function createPubSub<Message = void>(): PubSub<Message> {\n  const subscribers: { [id: string]: Subscriber<Message> } =\n    Object.create(null);\n  let nextId = 0;\n  function subscribe(subscriber: Subscriber<Message>) {\n    const id = nextId++;\n    subscribers[id] = subscriber;\n    return function unsubscribe() {\n      delete subscribers[id];\n    };\n  }\n\n  function publish(event: Message) {\n    for (const id in subscribers) {\n      subscribers[id](event);\n    }\n  }\n\n  return {\n    publish,\n    subscribe,\n  };\n}\n\nAnd at #3 we have tiny-pubsub which brings a non critical function to the table as well as an extra function with the way it handles unsubscribing! The agony! This comes in at a whopping 401b, more than twice nano-pubsub!\nlet subscriptions = Object.create(null);\n\nfunction subscribe(evt, func) {\n  if (typeof func !== \"function\") {\n    throw \"Subscribers must be functions\";\n  }\n  const oldSubscriptions = subscriptions[evt] || [];\n  oldSubscriptions.push(func);\n  subscriptions[evt] = oldSubscriptions;\n}\n\nfunction publish(evt) {\n  let args = Array.prototype.slice.call(arguments, 1);\n  const subFunctions = subscriptions[evt] || [];\n  for (let i = 0; i < subFunctions.length; i++) {\n    subFunctions[i].apply(null, args);\n  }\n}\n\nfunction unsubscribe(evt, func) {\n  const oldSubscriptions = subscriptions[evt] || [];\n  const newSubscriptions = oldSubscriptions.filter((item) => item !== func);\n  subscriptions[evt] = newSubscriptions;\n}\n\nfunction cancel(evt) {\n  delete subscriptions[evt];\n}\n\nmodule.exports = { subscribe, publish, unsubscribe, cancel };\n\nNotes\nIf you don't want to use the window object just do this, just know it'll cost ya 7 bytes:\nlet t = new EventTarget();\n\nexport default {\n  s: (e, c) => (t.addEventListener(e, c), () => t.removeEventListener(e, c)),\n  p: (n, d) => t.dispatchEvent(new CustomEvent(n, { detail: d })),\n};",
    "summary": {
      "en": "### Summary of Pico-PubSub\n\n**Pico-PubSub** is a very small Pub/Sub library, designed to be as lightweight as possible with no dependencies, and its code is only 149 bytes. It is intended for use in small applications, like JS13K games.\n\n**Key Features:**\n- **Zero Dependencies:** The library is entirely self-contained.\n- **Minimal Size:** At 149 bytes, it is smaller than other similar libraries.\n- **Basic Functions:**\n  - `sub(event, callback)`: Subscribes to an event.\n  - `pub(event, data)`: Publishes an event with related data.\n  \n**Usage Example:**\n1. Install with `npm install pico-pubsub`.\n2. Import the library and use the `sub` and `pub` functions to subscribe to and publish events.\n\n**Troubleshooting:**\n- TypeScript support may be added in the future, but a simple TypeScript snippet is provided for now.\n\n**Competition:**\n- **Nano-PubSub:** Slightly larger at 194 bytes.\n- **Tiny-PubSub:** Even larger at 401 bytes and includes more features.\n\nIn conclusion, Pico-PubSub is a compact and efficient choice for handling Pub/Sub patterns in JavaScript applications.",
      "ko": "Pico-PubSub는 매우 작은 Pub/Sub 라이브러리로, 의존성이 전혀 없고 코드 크기가 단 149바이트입니다. 이 라이브러리는 JS13K 게임과 같은 소규모 애플리케이션에서 사용하기 위해 설계되었습니다.\n\n주요 특징으로는 완전한 독립성을 자랑하는 제로 의존성과 149바이트라는 최소 크기가 있습니다. 기본 기능으로는 이벤트에 구독하는 `sub(event, callback)`와 관련 데이터를 포함한 이벤트를 발행하는 `pub(event, data)`가 있습니다.\n\n사용 방법은 간단합니다. 먼저 `npm install pico-pubsub` 명령어로 설치한 후, 라이브러리를 가져와 `sub`와 `pub` 함수를 사용하여 이벤트를 구독하고 발행할 수 있습니다.\n\n문제 해결을 위해 TypeScript 지원이 향후 추가될 수 있지만, 현재는 간단한 TypeScript 코드 예시가 제공됩니다.\n\n경쟁 라이브러리로는 194바이트 크기의 Nano-PubSub와 401바이트로 더 많은 기능을 포함한 Tiny-PubSub가 있습니다. Pico-PubSub는 JavaScript 애플리케이션에서 Pub/Sub 패턴을 처리하는 데 있어 작고 효율적인 선택입니다.",
      "ja": "Pico-PubSubは、非常に小さなPub/Subライブラリで、依存関係がなく、コードサイズはわずか149バイトです。このライブラリは、JS13Kゲームのような小規模なアプリケーションでの使用を目的としています。\n\n主な特徴としては、完全に自己完結型であるため依存関係がゼロであること、149バイトというサイズの小ささが挙げられます。また、基本的な機能として、イベントにサブスクライブする`sub(event, callback)`と、関連データを伴うイベントを発行する`pub(event, data)`があります。\n\n使用例としては、まず`npm install pico-pubsub`でインストールし、ライブラリをインポートして`sub`と`pub`関数を使ってイベントのサブスクライブや発行を行います。\n\nトラブルシューティングについては、将来的にTypeScriptのサポートが追加される可能性がありますが、現時点では簡単なTypeScriptのコード例が提供されています。\n\n競合としては、サイズが194バイトのNano-PubSubや、401バイトでより多くの機能を持つTiny-PubSubがあります。Pico-PubSubは、JavaScriptアプリケーションにおけるPub/Subパターンを扱うためのコンパクトで効率的な選択肢です。"
    }
  },
  {
    "id": "1d3a19f7e6fb09ac",
    "title": {
      "en": "Experimental Tauri Verso Integration",
      "ko": "타우리 버소 실험 통합",
      "ja": "実験的タウリ統合"
    },
    "type": "story",
    "url": "https://v2.tauri.app/blog/tauri-verso-integration/",
    "score": 136,
    "by": "stareatgoats",
    "time": 1743280249,
    "content": "(() => {\n\t\t\ttry {\n\t\t\t\tif (!matchMedia('(min-width: 50em)').matches) return;\n\t\t\t\t/** @type {HTMLElement | null} */\n\t\t\t\tconst target = document.querySelector('sl-sidebar-state-persist');\n\t\t\t\tconst state = JSON.parse(sessionStorage.getItem('sl-sidebar-state') || '0');\n\t\t\t\tif (!target || !state || target.dataset.hash !== state.hash) return;\n\t\t\t\twindow._starlightScrollRestore = state.scroll;\n\t\t\t\tcustomElements.define(\n\t\t\t\t\t'sl-sidebar-restore',\n\t\t\t\t\tclass SidebarRestore extends HTMLElement {\n\t\t\t\t\t\tconnectedCallback() {\n\t\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t\tconst idx = parseInt(this.dataset.index || '');\n\t\t\t\t\t\t\t\tconst details = this.closest('details');\n\t\t\t\t\t\t\t\tif (details && typeof state.open[idx] === 'boolean') details.open = state.open[idx];\n\t\t\t\t\t\t\t} catch {}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t);\n\t\t\t} catch {}\n\t\t})();\n\t        Quick Start         What is Tauri?     Prerequisites     Create a Project        Frontend Configuration         Overview     Leptos     Next.js     Nuxt     Qwik     SvelteKit     Trunk     Vite            Upgrade & Migrate         Overview     Upgrade from Tauri 1.0     Upgrade from Tauri 2.0 Beta                Core Concepts         Overview     Tauri Architecture     Process Model     App Size        Inter-Process Communication         Overview     Brownfield Pattern     Isolation Pattern                Security         Overview     Permissions     Command Scopes     Capabilities     Content Security Policy (CSP)     HTTP Headers New     Tauri Ecosystem Security     Application Lifecycle Threats     Future Work     Runtime Authority            Develop         Overview     Calling the Frontend from Rust     Calling Rust from the Frontend     Configuration Files     Embedding Additional Files     Embedding External Binaries     State Management     Updating Dependencies        Debug         Overview     CrabNebula DevTools New     Debug in Neovim     Debug in JetBrains IDEs     Debug in VS Code            Plugins         Overview     Mobile Plugin Development            Tests         Overview     Mock Tauri APIs        WebDriver         Overview     Continuous Integration        Example         Setup     Selenium     WebdriverIO                        Distribute         Overview     App Store     AppImage     AUR     CrabNebula Cloud     Debian     DMG     Flathub     Google Play     macOS Application Bundle     Microsoft Store     RPM     Snapcraft     Windows Installer        Sign         macOS     Windows     Linux     iOS     Android            Pipelines         CrabNebula Cloud     GitHub                Learn         Overview     Node.js as a sidecar     Splashscreen     System Tray     Window Customization        Security         Using Plugin Permissions     Capabilities for Different Windows and Platforms     Writing Plugin Permissions         Window Menu            Plugins         Overview     Autostart     Barcode Scanner New     Biometric New     Command Line Interface (CLI)     Clipboard     Deep Linking New     Dialog     File System     Global Shortcut     HTTP Client     Localhost     Logging     NFC New     Notifications     Opener     OS Information     Persisted Scope     Positioner     Process     Shell     Single Instance     SQL     Store     Stronghold     Updater     Upload     Websocket     Window State            About         About Tauri     The Tauri Book     Tauri Governance     Tauri Philosophy     Trademark Guidelines\n\t\t(() => {\n\t\t\tconst scroller = document.getElementById('starlight__sidebar');\n\t\t\tif (!window._starlightScrollRestore || !scroller) return;\n\t\t\tscroller.scrollTop = window._starlightScrollRestore;\n\t\t\tdelete window._starlightScrollRestore;\n\t\t})();\n\t      GitHub Discord Twitter Mastodon RSS            Select language    EnglishFrançaisEspañol简体中文日本語\n\n(() => {\n\t\t\ttry {\n\t\t\t\tif (!matchMedia('(min-width: 50em)').matches) return;\n\t\t\t\t/** @type {HTMLElement | null} */\n\t\t\t\tconst target = document.querySelector('sl-sidebar-state-persist');\n\t\t\t\tconst state = JSON.parse(sessionStorage.getItem('sl-sidebar-state') || '0');\n\t\t\t\tif (!target || !state || target.dataset.hash !== state.hash) return;\n\t\t\t\twindow._starlightScrollRestore = state.scroll;\n\t\t\t\tcustomElements.define(\n\t\t\t\t\t'sl-sidebar-restore',\n\t\t\t\t\tclass SidebarRestore extends HTMLElement {\n\t\t\t\t\t\tconnectedCallback() {\n\t\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t\tconst idx = parseInt(this.dataset.index || '');\n\t\t\t\t\t\t\t\tconst details = this.closest('details');\n\t\t\t\t\t\t\t\tif (details && typeof state.open[idx] === 'boolean') details.open = state.open[idx];\n\t\t\t\t\t\t\t} catch {}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t);\n\t\t\t} catch {}\n\t\t})();\n\t        Security         Capability     Permission     Scope     Core Permissions         Command Line Interface     Configuration     Environment Variables     Webview Versions        Releases         Overview        @tauri-apps            api         2.4.1     2.4.0     2.3.0     2.2.0     2.1.1     2.1.0     2.0.3     2.0.2     2.0.1     2.0.0     2.0.0-rc.6     2.0.0-rc.5     2.0.0-rc.4     2.0.0-rc.3     2.0.0-rc.2     2.0.0-rc.1     2.0.0-rc.0     2.0.0-beta.16     2.0.0-beta.15     2.0.0-beta.14     2.0.0-beta.13     2.0.0-beta.12     2.0.0-beta.11     2.0.0-beta.10     2.0.0-beta.9     2.0.0-beta.8     2.0.0-beta.7     2.0.0-beta.6     2.0.0-beta.5     2.0.0-beta.4     2.0.0-beta.3     2.0.0-beta.2     2.0.0-beta.1     2.0.0-beta.0     2.0.0-alpha.14     2.0.0-alpha.13     2.0.0-alpha.12     2.0.0-alpha.11     2.0.0-alpha.10     2.0.0-alpha.9     2.0.0-alpha.8     2.0.0-alpha.7     2.0.0-alpha.6     2.0.0-alpha.5     2.0.0-alpha.4     2.0.0-alpha.3     2.0.0-alpha.2     2.0.0-alpha.1     2.0.0-alpha.0     1.5.3     1.5.2     1.5.1     1.5.0     1.4.0     1.3.0     1.2.0     1.1.0     1.0.2     1.0.1     1.0.0     1.0.0-rc.7     1.0.0-rc.6     1.0.0-rc.5     1.0.0-rc.4     1.0.0-rc.3     1.0.0-rc.2     1.0.0-rc.1     1.0.0-rc.0     1.0.0-beta-rc.3     1.0.0-beta-rc.2     1.0.0-beta-rc.1     1.0.0-beta-rc.0     1.0.0-beta.8     1.0.0-beta.7     1.0.0-beta.6     1.0.0-beta.5     1.0.0-beta.4     1.0.0-beta.3     1.0.0-beta.2     1.0.0-beta.1     1.0.0-beta.0            cli         2.4.1     2.4.0     2.3.1     2.3.0     2.2.7     2.2.6     2.2.5     2.2.4     2.2.3     2.2.2     2.2.1     2.2.0     2.1.0     2.0.4     2.0.3     2.0.2     2.0.1     2.0.0     2.0.0-rc.18     2.0.0-rc.17     2.0.0-rc.16     2.0.0-rc.15     2.0.0-rc.14     2.0.0-rc.13     2.0.0-rc.12     2.0.0-rc.11     2.0.0-rc.10     2.0.0-rc.9     2.0.0-rc.8     2.0.0-rc.7     2.0.0-rc.6     2.0.0-rc.5     2.0.0-rc.4     2.0.0-rc.3     2.0.0-rc.2     2.0.0-rc.1     2.0.0-rc.0     2.0.0-beta.23     2.0.0-beta.22     2.0.0-beta.21     2.0.0-beta.20     2.0.0-beta.19     2.0.0-beta.18     2.0.0-beta.17     2.0.0-beta.16     2.0.0-beta.15     2.0.0-beta.14     2.0.0-beta.13     2.0.0-beta.12     2.0.0-beta.11     2.0.0-beta.10     2.0.0-beta.9     2.0.0-beta.8     2.0.0-beta.7     2.0.0-beta.6     2.0.0-beta.5     2.0.0-beta.4     2.0.0-beta.3     2.0.0-beta.2     2.0.0-beta.1     2.0.0-beta.0     2.0.0-alpha.21     2.0.0-alpha.20     2.0.0-alpha.19     2.0.0-alpha.18     2.0.0-alpha.17     2.0.0-alpha.16     2.0.0-alpha.15     2.0.0-alpha.14     2.0.0-alpha.13     2.0.0-alpha.12     2.0.0-alpha.11     2.0.0-alpha.10     2.0.0-alpha.9     2.0.0-alpha.8     2.0.0-alpha.7     2.0.0-alpha.6     2.0.0-alpha.5     2.0.0-alpha.4     2.0.0-alpha.3     2.0.0-alpha.2     2.0.0-alpha.1     2.0.0-alpha.0     1.5.11     1.5.10     1.5.9     1.5.8     1.5.7     1.5.6     1.5.5     1.5.4     1.5.3     1.5.2     1.5.1     1.5.0     1.4.0     1.3.1     1.3.0     1.2.3     1.2.2     1.2.1     1.2.0     1.1.1     1.1.0     1.0.5     1.0.4     1.0.3     1.0.2     1.0.1     1.0.0     1.0.0-rc.16     1.0.0-rc.15     1.0.0-rc.14     1.0.0-rc.13     1.0.0-rc.12     1.0.0-rc.11     1.0.0-rc.10     1.0.0-rc.9     1.0.0-rc.8     1.0.0-rc.6     1.0.0-rc.5     1.0.0-rc.4     1.0.0-rc.3     1.0.0-rc.2     1.0.0-rc.1     1.0.0-rc.0                tao         0.32.8     0.32.7     0.32.6     0.32.5     0.32.4     0.32.3     0.32.2     0.32.1     0.32.0     0.31.1     0.31.0     0.30.7     0.30.6     0.30.5     0.30.4     0.30.3     0.30.2     0.30.1     0.30.0     0.29.1     0.29.0     0.28.1     0.28.0     0.27.1     0.27.0     0.26.2     0.26.1     0.26.0     0.25.0     0.24.1     0.24.0     0.23.0     0.22.3     0.22.2     0.22.1     0.22.0     0.21.1     0.21.0     0.20.0     0.19.1     0.19.0     0.18.3     0.18.2     0.18.1     0.18.0     0.17.0     0.16.0     0.15.9     0.15.8     0.15.7     0.15.6     0.15.5     0.15.4     0.15.3     0.15.2     0.15.1     0.15.0     0.14.0     0.13.3     0.13.2     0.13.1     0.13.0     0.12.2     0.12.1     0.12.0     0.11.2     0.11.1     0.11.0     0.10.0     0.9.1     0.9.0     0.8.5     0.8.4     0.8.3     0.8.2     0.8.1     0.8.0     0.7.0     0.6.4     0.6.3     0.6.2     0.6.1     0.6.0     0.5.2     0.5.1     0.5.0     0.4.0     0.3.1     0.3.0     0.2.6     0.2.5     0.2.4     0.2.3     0.2.2     0.2.1     0.2.0            tauri         2.4.1     2.4.0     2.3.1     2.3.0     2.2.5     2.2.4     2.2.3     2.2.2     2.2.1     2.2.0     2.1.1     2.1.0     2.0.6     2.0.5     2.0.4     2.0.3     2.0.2     2.0.1     2.0.0     2.0.0-rc.17     2.0.0-rc.16     2.0.0-rc.15     2.0.0-rc.14     2.0.0-rc.13     2.0.0-rc.12     2.0.0-rc.11     2.0.0-rc.10     2.0.0-rc.9     2.0.0-rc.8     2.0.0-rc.7     2.0.0-rc.6     2.0.0-rc.5     2.0.0-rc.4     2.0.0-rc.3     2.0.0-rc.2     2.0.0-rc.1     2.0.0-rc.0     2.0.0-beta.25     2.0.0-beta.24     2.0.0-beta.23     2.0.0-beta.22     2.0.0-beta.21     2.0.0-beta.20     2.0.0-beta.19     2.0.0-beta.18     2.0.0-beta.17     2.0.0-beta.16     2.0.0-beta.15     2.0.0-beta.14     2.0.0-beta.13     2.0.0-beta.12     2.0.0-beta.11     2.0.0-beta.10     2.0.0-beta.9     2.0.0-beta.8     2.0.0-beta.7     2.0.0-beta.6     2.0.0-beta.5     2.0.0-beta.4     2.0.0-beta.3     2.0.0-beta.2     2.0.0-beta.1     2.0.0-beta.0     2.0.0-alpha.21     2.0.0-alpha.20     2.0.0-alpha.19     2.0.0-alpha.18     2.0.0-alpha.17     2.0.0-alpha.16     2.0.0-alpha.15     2.0.0-alpha.14     2.0.0-alpha.13     2.0.0-alpha.12     2.0.0-alpha.11     2.0.0-alpha.10     2.0.0-alpha.9     2.0.0-alpha.8     2.0.0-alpha.7     2.0.0-alpha.6     2.0.0-alpha.5     2.0.0-alpha.4     2.0.0-alpha.3     2.0.0-alpha.2     2.0.0-alpha.1     2.0.0-alpha.0     1.6.0     1.5.4     1.5.3     1.5.2     1.5.1     1.5.0     1.4.1     1.4.0     1.3.0     1.2.5     1.2.4     1.2.3     1.2.2     1.2.1     1.2.0     1.1.4     1.1.3     1.1.2     1.1.1     1.1.0     1.0.9     1.0.8     1.0.7     1.0.6     1.0.5     1.0.4     1.0.3     1.0.2     1.0.1     1.0.0     1.0.0-rc.17     1.0.0-rc.16     1.0.0-rc.15     1.0.0-rc.14     1.0.0-rc.13     1.0.0-rc.12     1.0.0-rc.11     1.0.0-rc.10     1.0.0-rc.9     1.0.0-rc.8     1.0.0-rc.7     1.0.0-rc.6     1.0.0-rc.5     1.0.0-rc.4     1.0.0-rc.3     1.0.0-rc.2     1.0.0-rc.1     1.0.0-rc.0     1.0.0-beta-rc.4     1.0.0-beta-rc.3     1.0.0-beta-rc.2     1.0.0-beta-rc.1     1.0.0-beta-rc.0     1.0.0-beta.8     1.0.0-beta.7     1.0.0-beta.6     1.0.0-beta.5     1.0.0-beta.4     1.0.0-beta.3     1.0.0-beta.2     1.0.0-beta.1     1.0.0-beta.0     0.11.1     0.11.0     0.10.0     0.9.2     0.9.1     0.9.0     0.8.0     0.7.5     0.7.4     0.7.3     0.7.2     0.7.1     0.7.0     0.6.2     0.6.0            tauri-bundler         2.3.1     2.3.0     2.2.4     2.2.3     2.2.2     2.2.1     2.2.0     2.1.0     2.0.4     2.0.3     2.0.2     2.0.1     2.0.1-rc.15     2.0.1-rc.14     2.0.1-rc.13     2.0.1-rc.12     2.0.1-rc.11     2.0.1-rc.10     2.0.1-rc.9     2.0.1-rc.8     2.0.1-rc.7     2.0.1-rc.6     2.0.1-rc.5     2.0.1-rc.4     2.0.1-rc.3     2.0.1-rc.2     2.0.1-rc.1     2.0.1-rc.0     2.0.1-beta.19     2.0.1-beta.18     2.0.1-beta.17     2.0.1-beta.16     2.0.1-beta.15     2.0.1-beta.14     2.0.1-beta.13     2.0.1-beta.12     2.0.1-beta.11     2.0.1-beta.10     2.0.1-beta.9     2.0.1-beta.8     2.0.1-beta.7     2.0.1-beta.6     2.0.1-beta.5     2.0.1-beta.4     2.0.1-beta.3     2.0.1-beta.2     2.0.1-beta.1     2.0.1-beta.0     2.0.0     2.0.0-rc.0     2.0.0-beta.3     2.0.0-beta.2     2.0.0-beta.1     2.0.0-beta.0     2.0.0-alpha.14     2.0.0-alpha.13     2.0.0-alpha.12     2.0.0-alpha.11     2.0.0-alpha.10     2.0.0-alpha.9     2.0.0-alpha.8     2.0.0-alpha.7     2.0.0-alpha.6     2.0.0-alpha.5     2.0.0-alpha.4     2.0.0-alpha.3     2.0.0-alpha.2     2.0.0-alpha.1     2.0.0-alpha.0     1.5.1     1.5.0     1.4.8     1.4.7     1.4.6     1.4.5     1.4.4     1.4.3     1.4.2     1.4.1     1.4.0     1.3.0     1.2.1     1.2.0     1.1.2     1.1.1     1.1.0     1.0.7     1.0.6     1.0.5     1.0.4     1.0.3     1.0.2     1.0.1     1.0.0     1.0.0-rc.10     1.0.0-rc.9     1.0.0-rc.8     1.0.0-rc.7     1.0.0-rc.6     1.0.0-rc.5     1.0.0-rc.4     1.0.0-rc.3     1.0.0-rc.2     1.0.0-rc.1     1.0.0-rc.0     1.0.0-beta-rc.1     1.0.0-beta-rc.0     1.0.0-beta.4     1.0.0-beta.3     1.0.0-beta.2     1.0.0-beta.1     1.0.0-beta.0     0.9.4     0.9.3     0.9.2     0.9.1     0.9.0     0.8.5     0.8.4     0.8.3     0.8.2     0.8.1     0.8.0     0.7.0            tauri-cli         2.4.1     2.4.0     2.3.1     2.3.0     2.2.7     2.2.6     2.2.5     2.2.4     2.2.3     2.2.2     2.2.1     2.2.0     2.1.0     2.0.4     2.0.3     2.0.2     2.0.1     2.0.0     2.0.0-rc.18     2.0.0-rc.17     2.0.0-rc.16     2.0.0-rc.15     2.0.0-rc.13     2.0.0-rc.12     2.0.0-rc.11     2.0.0-rc.10     2.0.0-rc.9     2.0.0-rc.8     2.0.0-rc.7     2.0.0-rc.6     2.0.0-rc.5     2.0.0-rc.4     2.0.0-rc.3     2.0.0-rc.2     2.0.0-rc.1     2.0.0-rc.0     2.0.0-beta.23     2.0.0-beta.22     2.0.0-beta.21     2.0.0-beta.20     2.0.0-beta.19     2.0.0-beta.18     2.0.0-beta.17     2.0.0-beta.16     2.0.0-beta.15     2.0.0-beta.14     2.0.0-beta.13     2.0.0-beta.12     2.0.0-beta.11     2.0.0-beta.10     2.0.0-beta.9     2.0.0-beta.8     2.0.0-beta.7     2.0.0-beta.6     2.0.0-beta.5     2.0.0-beta.4     2.0.0-beta.3     2.0.0-beta.2     2.0.0-beta.1     2.0.0-beta.0     2.0.0-alpha.21     2.0.0-alpha.20     2.0.0-alpha.19     2.0.0-alpha.18     2.0.0-alpha.17     2.0.0-alpha.16     2.0.0-alpha.15     2.0.0-alpha.14     2.0.0-alpha.13     2.0.0-alpha.12     2.0.0-alpha.11     2.0.0-alpha.10     2.0.0-alpha.9     2.0.0-alpha.8     2.0.0-alpha.7     2.0.0-alpha.6     2.0.0-alpha.5     2.0.0-alpha.4     2.0.0-alpha.3     2.0.0-alpha.2     2.0.0-alpha.1     2.0.0-alpha.0     1.5.11     1.5.10     1.5.9     1.5.8     1.5.7     1.5.6     1.5.5     1.5.4     1.5.3     1.5.2     1.5.1     1.5.0     1.4.0     1.3.1     1.3.0     1.2.3     1.2.2     1.2.1     1.2.0     1.1.1     1.1.0     1.0.5     1.0.4     1.0.3     1.0.2     1.0.1     1.0.0     1.0.0-rc.16     1.0.0-rc.15     1.0.0-rc.14     1.0.0-rc.13     1.0.0-rc.12     1.0.0-rc.11     1.0.0-rc.10     1.0.0-rc.9     1.0.0-rc.8     1.0.0-rc.7     1.0.0-rc.6     1.0.0-rc.5     1.0.0-rc.4     1.0.0-rc.3     1.0.0-rc.2     1.0.0-rc.1     1.0.0-rc.0     1.0.0-beta-rc.4     1.0.0-beta-rc.3     1.0.0-beta-rc.2     1.0.0-beta-rc.1     1.0.0-beta-rc.0     1.0.0-beta.7     1.0.0-beta.6     1.0.0-beta.5     1.0.0-beta.4     1.0.0-beta.3     1.0.0-beta.2     1.0.0-beta.1     1.0.0-beta.0            wry         0.50.5     0.50.4     0.50.3     0.50.2     0.50.1     0.50.0     0.49.0     0.48.1     0.48.0     0.47.2     0.47.1     0.47.0     0.46.3     0.46.2     0.46.1     0.46.0     0.45.0     0.44.1     0.44.0     0.43.1     0.43.0     0.42.0     0.41.0     0.40.1     0.40.0     0.39.5     0.39.4     0.39.3     0.39.2     0.39.1     0.39.0     0.38.2     0.38.1     0.38.0     0.37.0     0.36.0     0.35.2     0.35.1     0.35.0     0.34.2     0.34.1     0.34.0     0.33.1     0.33.0     0.32.0     0.31.0     0.30.0     0.29.0     0.28.3     0.28.2     0.28.1     0.28.0     0.27.3     0.27.2     0.27.1     0.27.0     0.26.0     0.25.0     0.24.1     0.24.0     0.23.4     0.23.3     0.23.2     0.23.1     0.23.0     0.22.6     0.22.5     0.22.4     0.22.3     0.22.2     0.22.1     0.22.0     0.21.1     0.21.0     0.20.2     0.20.1     0.20.0     0.19.0     0.18.3     0.18.2     0.18.1     0.18.0     0.17.0     0.16.2     0.16.1     0.16.0     0.15.1     0.15.0     0.14.0     0.13.3     0.13.2     0.13.1     0.13.0     0.12.2     0.12.1     0.12.0     0.11.0     0.10.3     0.10.2     0.10.1     0.10.0     0.9.4     0.9.3     0.9.2     0.9.1     0.9.0     0.8.0     0.7.0     0.6.2     0.6.1     0.6.0                JavaScript            api         @tauri-apps/api     app     core     dpi     event     image     menu     mocks     path     tray     webview     webviewWindow     window         barcode-scanner     biometric     cli     clipboard-manager     deep-link     dialog     fs     global-shortcut     http     log     nfc     notification     opener     os     positioner     process     shell     sql     store     stronghold     updater     upload     websocket     window-state         Rust (docs.rs)\n\t\t(() => {\n\t\t\tconst scroller = document.getElementById('starlight__sidebar');\n\t\t\tif (!window._starlightScrollRestore || !scroller) return;\n\t\t\tscroller.scrollTop = window._starlightScrollRestore;\n\t\t\tdelete window._starlightScrollRestore;\n\t\t})();\n\t      GitHub Discord Twitter Mastodon RSS            Select language    EnglishFrançaisEspañol简体中文日本語\n\n(() => {\n\t\t\ttry {\n\t\t\t\tif (!matchMedia('(min-width: 50em)').matches) return;\n\t\t\t\t/** @type {HTMLElement | null} */\n\t\t\t\tconst target = document.querySelector('sl-sidebar-state-persist');\n\t\t\t\tconst state = JSON.parse(sessionStorage.getItem('sl-sidebar-state') || '0');\n\t\t\t\tif (!target || !state || target.dataset.hash !== state.hash) return;\n\t\t\t\twindow._starlightScrollRestore = state.scroll;\n\t\t\t\tcustomElements.define(\n\t\t\t\t\t'sl-sidebar-restore',\n\t\t\t\t\tclass SidebarRestore extends HTMLElement {\n\t\t\t\t\t\tconnectedCallback() {\n\t\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t\tconst idx = parseInt(this.dataset.index || '');\n\t\t\t\t\t\t\t\tconst details = this.closest('details');\n\t\t\t\t\t\t\t\tif (details && typeof state.open[idx] === 'boolean') details.open = state.open[idx];\n\t\t\t\t\t\t\t} catch {}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t);\n\t\t\t} catch {}\n\t\t})();\n\t     All posts        Recent posts         Experimental Tauri Verso Integration     Tauri 2.0 Stable Release     Tauri 2.0 Release Candidate     Announcing Tauri 1.7.0     Tauri Board Elections 2024     Rust Security Advisory CVE-2024-24576     Announcing Tauri 1.6.0     Announcing the Tauri v2 Beta Release     Strengthening Tauri: Our Partnership with CrabNebula     Announcing Tauri 1.5.0     Roadmap to Tauri 2.0     Tauri Board Elections & Governance Update     Announcing Tauri 1.4.0     Announcing Tauri 1.3.0     Tauri 2.0.0-alpha.4 Released     create-tauri-app Version 3 Released     Tauri Community Growth & Feedback     Migration to webkit2gtk-4.1 on Linux port     Announcing the Tauri Mobile Alpha Release     Announcing Tauri 1.2.0     Announcing tauri-egui 0.1.0     Announcing Tauri 1.1.0     Tauri Programme Turns 1 and Board Elections     Tauri 1.0 Release\n\t\t(() => {\n\t\t\tconst scroller = document.getElementById('starlight__sidebar');\n\t\t\tif (!window._starlightScrollRestore || !scroller) return;\n\t\t\tscroller.scrollTop = window._starlightScrollRestore;\n\t\t\tdelete window._starlightScrollRestore;\n\t\t})();\n\t      GitHub Discord Twitter Mastodon RSS            Select language    EnglishFrançaisEspañol简体中文日本語",
    "summary": {
      "en": "The text describes a JavaScript function that manages the restoration of a sidebar's state in a web application. \n\nKey points include:\n\n1. **Media Query Check**: The script only runs if the screen width is at least 50em.\n2. **State Management**: It retrieves the sidebar's state (like scroll position and open/closed details) from the session storage.\n3. **Element Selection**: The script looks for a specific HTML element to update based on the retrieved state.\n4. **Sidebar Restore Class**: A custom HTML element is defined to handle restoring the sidebar's open/closed state based on saved data.\n5. **Error Handling**: The code includes try-catch blocks to prevent crashes from potential errors.\n\nAdditionally, the text includes a table of contents for Tauri documentation, covering topics like security, development, debugging, plugins, and distribution methods. It lists different versions of Tauri and its components, indicating ongoing updates and improvements.",
      "ko": "이 텍스트는 웹 애플리케이션에서 사이드바 상태를 복원하는 자바스크립트 함수에 대해 설명합니다. 주요 내용은 다음과 같습니다.\n\n스크립트는 화면 너비가 최소 50em일 때만 실행됩니다. 사이드바의 상태, 즉 스크롤 위치와 열림/닫힘 상태를 세션 스토리지에서 가져옵니다. 이 상태에 따라 업데이트할 특정 HTML 요소를 찾습니다. 저장된 데이터를 기반으로 사이드바의 열림/닫힘 상태를 복원하는 사용자 정의 HTML 요소가 정의됩니다. 코드에는 잠재적인 오류로 인한 충돌을 방지하기 위해 try-catch 블록이 포함되어 있습니다.\n\n또한, 텍스트에는 Tauri 문서의 목차가 포함되어 있으며, 보안, 개발, 디버깅, 플러그인, 배포 방법과 같은 주제를 다룹니다. Tauri와 그 구성 요소의 다양한 버전이 나열되어 있으며, 지속적인 업데이트와 개선 사항을 나타냅니다.",
      "ja": "このテキストは、ウェブアプリケーションにおけるサイドバーの状態を管理するJavaScriptの関数について説明しています。\n\nまず、メディアクエリのチェックがあります。このスクリプトは、画面の幅が少なくとも50emである場合にのみ実行されます。次に、状態管理の部分では、サイドバーの状態（スクロール位置や開閉の詳細など）をセッションストレージから取得します。続いて、特定のHTML要素を選択し、取得した状態に基づいて更新します。\n\nサイドバーの開閉状態を保存されたデータに基づいて復元するためのカスタムHTML要素も定義されています。また、エラーハンドリングのために、潜在的なエラーからのクラッシュを防ぐためにtry-catchブロックが含まれています。\n\nさらに、テキストにはTauriのドキュメントの目次が含まれており、セキュリティ、開発、デバッグ、プラグイン、配布方法などのトピックをカバーしています。Tauriとそのコンポーネントの異なるバージョンがリストされており、継続的な更新と改善が行われていることを示しています。"
    }
  },
  {
    "id": "51367c7e5215f98e",
    "title": {
      "en": "Show HN: Terminal dashboard that throttles my PC during peak electricity rates",
      "ko": "전력 절약 터미널 대시보드",
      "ja": "ピーク電力管理ダッシュボード"
    },
    "type": "story",
    "url": "https://www.naveen.ing/cli-for-smartplugs/",
    "score": 95,
    "by": "naveen_k",
    "time": 1743520679,
    "content": "WattWise: Terminal-Based Power Monitoring Using Smart Plugs    March 31, 2025\n\n  CLI dashboard showing real-time and historical power draw readings from a Kasa smart plug\n\nThe Challenge: Performance vs. Power Costs\nHigh-performance computing often means high electricity bills, especially in regions with time-of-use pricing. Finding the balance between computational power and energy efficiency becomes critical when running resource-intensive workloads.\nCurrent Workstation Specs\n\nCPUs: Dual EPYC 7C13 (base 2GHz, boost 3.7GHz) - 128 cores total (256 threads)\nMotherboard: Gigabyte MZ72-HB2\nRAM: 512GB DDR4\nGPUs: 2x A4000 16GB\nGPUs: 2x 4090 48GB (Upcoming)\nStorage: 10TB NVMe\nPSU: Coolmaster 1600W Platinum\n\nUnderstanding Power Usage\nI have been setting up a workstation for off-loading compute-intensive LLM workflows from my desktop. The build includes dual Epyc CPUs with plans to add 4 GPUs for up to 128GB VRAM to run multiple agents in parallel. A key constraint was ensuring the system could run comfortably on a standard household 120V outlet.\nSince I already use smart plugs throughout my home, I added one to monitor the workstation’s power consumption. However, checking power statistics using the kasa phone app or Home Assistant dashboard proved cumbersome, especially when I already maintain a terminal window with monitoring tools like htop, nvtop, and nload in a 2×2 grid on my secondary display.\n\n  TP-link Kasa EP25 Smart Plug\n\nBuilding a Terminal-Based Solution\nAfter sketching a simple wireframe for what I wanted: a clean, terminal-based UI that would display power consumption data already being collected by my Home Assistant instance through the TP-Link integration. After getting the data access sorted, I spent some time with ‘Claude 3.7 Sonnet Thinking’ iterating on the CLI structure and features.\nThe result: WattWise, a lightweight CLI tool that pulls power usage data from smart plugs (either directly or through Home Assistant) and presents it in a clean, information-dense dashboard right in the terminal.\n\n  WattWise Demo\n\nKey Features\nMonitoring Features\n\nReal-time power monitoring with wattage and current display\nColor-coded power values (green for low usage, yellow for medium, red for high)\nHistorical consumption charts directly in the terminal\nWhile smart plug measurements aren’t lab-grade (typically ±1-3% accuracy), they’re more than sufficient for practical usage monitoring\n\nPower Management Features\n\nAutomatic CPU/GPU throttling based on time-of-use electricity pricing\nConfigurable power thresholds and performance profiles\nSimple configuration through an interactive setup process\n\nDeployment Options\n\nDirect installation from source code\nDocker support for containerized deployment\nConnection options for both direct Kasa smart plug access and Home Assistant integration\n\nDynamic Power Management\nSince my utility provider uses Time of Use (ToU) pricing. Over the years, I’ve built various home automations to minimize electricity usage during peak hours. With my workstation potentially drawing up to 1400W at full load, it made sense to add automatic CPU and GPU throttling during expensive rate periods.\n\n  Power Optimizer: System architecture\n\nMy testing showed that reducing CPU frequency from 3700MHz to 1500MHz saved approximately 225W under load on my specific setup, though results will vary by hardware. The power optimizer service dynamically adjusts clock speeds based on:\n\nSystem load (from os.getloadavg())\nCurrent power consumption (from the smart plug)\nTime of day (to account for ToU periods)\n\n  Power Optimizer: ToU adaption chart\n\nAdaptive Performance Control\nInitially, I explored a full PID (Proportional-Integral-Derivative) controller, but a simpler PI (Proportional-Integral) approach proved more suitable for power management.\nThe PI controller focuses on two key aspects:\n\nProportional (P) term: Provides immediate, proportional response to current errors\nIntegral (I) term: Accumulates past errors to eliminate long-term steady-state deviations\n\nBy removing the Derivative term, we:\n\nSimplify the control logic\nReduce computational overhead\nMaintain smooth performance transitions\nAvoid potential over-optimization from the derivative calculation\n\nFor systems with gradual state changes like power management, the derivative term often introduces unnecessary complexity without meaningful benefits.\nThe controller dynamically adapts system parameters by considering:\n\nCurrent system utilization (CPU/GPU load)\nInstantaneous power consumption\nTime-of-use electricity rate periods\n\nThis approach ensures energy-efficient transitions without the complexity of a full PID implementation.\n\n  Power Optimizer: Control flow (with PID)\n\nImplementation and Limitations\nTerminal Interface\nThe dashboard design is simple with:\n\nA large, easy-to-read current wattage display\nColor-coded elements based on consumption thresholds\nReal-time updates with configurable refresh intervals\nHistorical graph using Unicode block characters for compatibility across terminals\n\nData Sources\nThe tool supports two main data sources:\n\nDirect Kasa Connection: Communicates directly with TP-Link Kasa smart plugs\nHome Assistant Integration: Uses the existing HA setup with authentication tokens\n\nThe code is modular enough that adding support for other smart plug systems shouldn’t be too difficult.\nCurrent Limitations\nThis is very much a personal project with some constraints:\n\nCurrently supports only one smart plug at a time\nWorks only with Kasa smart plugs that have energy monitoring capabilities (Ex: EP25)\nRequires either direct network access to the plug or Home Assistant integration\nPower management features require Linux systems with appropriate CPU/GPU frequency control capabilities\n\nHow TO Use\nBasic setup:\ngit clone https://github.com/naveenkul/WattWise.git\ncd WattWise\npip install -r requirements.txt\npip install .\n\nBasic usage:\n# Quick power view (single reading)\nwattwise\n\n# Continuous monitoring with charts\nwattwise --watch\n\nFuture Improvements\n\nSupport for multiple plugs with aggregated power statistics\nAdditional smart plug brands/models compatibility\nEnhanced visualization options and exports\nIntegration with other power management tools\nBetter predictive algorithms for consumption forecasting\n\nFinal Thoughts\nWattWise started as a simple utility to solve my specific need: monitoring a power-hungry workstation from the terminal I already have open. The addition of automatic power management during peak ToU hours has already saved me from manually adjusting system performance throughout the day.\nThe dashboard part of the project is open-sourced under the MIT license. Documentation and installation instructions are available in the repository. Contributions and feedback are welcome!\n\n        naveenkul/WattWise\n\nFeel free to fork it and adapt it to your specific needs – I’d be curious to see what others build with it.\n\nUpdate (04/01/25): Added Workstation specs",
    "summary": {
      "en": "**Summary of WattWise: Terminal-Based Power Monitoring Using Smart Plugs**\n\n**Overview:**  \nWattWise is a command-line interface (CLI) tool designed to monitor the power consumption of high-performance computing workstations using TP-Link Kasa smart plugs. It aims to help users manage electricity costs, especially during peak pricing periods.\n\n**Challenge:**  \nHigh-performance computing can lead to significant electricity bills. Balancing performance and energy efficiency is essential, especially when running demanding workloads.\n\n**Workstation Setup:**  \nThe workstation features dual EPYC CPUs and plans for multiple GPUs, requiring careful power management to operate on a standard household outlet.\n\n**WattWise Development:**  \nWattWise was created to display power consumption data in a user-friendly terminal format. It pulls data from smart plugs or Home Assistant, offering real-time monitoring and historical data.\n\n**Key Features:**\n- **Monitoring:** Real-time wattage display, color-coded power levels, and historical charts.\n- **Power Management:** Automatic CPU/GPU throttling based on electricity pricing, customizable performance profiles, and easy setup.\n- **Deployment:** Can be installed directly from source code or via Docker, with options for different connection methods.\n\n**Dynamic Power Management:**  \nWattWise adjusts CPU and GPU performance based on system load and electricity rates, helping to save power during peak times. A simpler PI control strategy is used for effective power management.\n\n**Implementation:**  \nThe interface is straightforward, showing current power usage and historical data. It supports direct connections to Kasa plugs or integration with Home Assistant. \n\n**Limitations:**  \nCurrently, WattWise works with a single Kasa smart plug and requires Linux systems for power management features.\n\n**Usage:**  \nUsers can install and run WattWise with simple commands to monitor power usage continuously or check quick readings.\n\n**Future Plans:**  \nImprovements include support for multiple plugs, compatibility with other brands, enhanced visualizations, and better predictive algorithms.\n\n**Conclusion:**  \nWattWise began as a personal project to monitor power usage effectively and has evolved to include automated power management features. The project is open-source, inviting contributions and adaptations from the community.",
      "ko": "WattWise는 TP-Link Kasa 스마트 플러그를 사용하여 고성능 컴퓨팅 워크스테이션의 전력 소비를 모니터링하는 명령줄 인터페이스(CLI) 도구입니다. 이 도구는 특히 전기 요금이 높은 시간대에 사용자들이 전기 비용을 관리할 수 있도록 돕는 것을 목표로 합니다.\n\n고성능 컴퓨팅은 상당한 전기 요금을 초래할 수 있습니다. 따라서 성능과 에너지 효율성을 균형 있게 유지하는 것이 중요합니다. 특히 요구되는 작업을 수행할 때 더욱 그렇습니다.\n\n이 워크스테이션은 듀얼 EPYC CPU와 여러 GPU를 계획하고 있어, 일반 가정용 콘센트에서 작동하기 위해서는 세심한 전력 관리가 필요합니다. WattWise는 사용자 친화적인 터미널 형식으로 전력 소비 데이터를 표시하도록 설계되었습니다. 스마트 플러그나 홈 어시스턴트에서 데이터를 가져와 실시간 모니터링과 과거 데이터를 제공합니다.\n\nWattWise의 주요 기능으로는 실시간 전력량 표시, 색상으로 구분된 전력 수준, 그리고 과거 차트가 있습니다. 전기 요금에 따라 CPU와 GPU의 성능을 자동으로 조절하고, 사용자 맞춤형 성능 프로필을 제공하며, 설치가 간편합니다. 소스 코드에서 직접 설치하거나 Docker를 통해 설치할 수 있으며, 다양한 연결 방법을 지원합니다.\n\nWattWise는 시스템 부하와 전기 요금에 따라 CPU와 GPU 성능을 조정하여 피크 시간대에 전력을 절약하는 데 도움을 줍니다. 효과적인 전력 관리를 위해 간단한 PI 제어 전략을 사용합니다. 인터페이스는 직관적이며 현재 전력 사용량과 과거 데이터를 보여줍니다. Kasa 플러그에 직접 연결하거나 홈 어시스턴트와 통합할 수 있습니다.\n\n현재 WattWise는 단일 Kasa 스마트 플러그와 함께 작동하며, 전력 관리 기능을 위해서는 리눅스 시스템이 필요합니다. 사용자는 간단한 명령어로 WattWise를 설치하고 실행하여 전력 사용량을 지속적으로 모니터링하거나 빠른 측정을 할 수 있습니다.\n\n앞으로의 계획으로는 여러 플러그 지원, 다른 브랜드와의 호환성, 시각화 개선, 그리고 더 나은 예측 알고리즘이 포함됩니다. WattWise는 개인 프로젝트로 시작하여 전력 사용량을 효과적으로 모니터링하는 기능을 포함하게 되었으며, 자동화된 전력 관리 기능도 추가되었습니다. 이 프로젝트는 오픈 소스이며, 커뮤니티의 기여와 적응을 환영합니다.",
      "ja": "WattWiseは、TP-Link Kasaのスマートプラグを使用して高性能コンピューティングワークステーションの電力消費を監視するためのコマンドラインインターフェース（CLI）ツールです。このツールは、特に電気料金が高くなる時間帯において、ユーザーが電気代を管理する手助けをすることを目的としています。\n\n高性能コンピューティングは、かなりの電気料金を引き起こす可能性があります。そのため、パフォーマンスとエネルギー効率のバランスを取ることが重要です。特に、負荷の高い作業を行う際には注意が必要です。\n\nこのワークステーションは、デュアルEPYC CPUを搭載し、複数のGPUを使用する予定であり、標準的な家庭用コンセントで動作させるためには慎重な電力管理が求められます。\n\nWattWiseは、ユーザーフレンドリーな端末形式で電力消費データを表示するために開発されました。スマートプラグやHome Assistantからデータを取得し、リアルタイムの監視と過去のデータを提供します。\n\n主な機能には、リアルタイムのワット数表示、色分けされた電力レベル、過去のデータを示すチャートがあります。また、電気料金に基づいてCPUやGPUの性能を自動的に調整する機能や、カスタマイズ可能なパフォーマンスプロファイル、簡単なセットアップも特徴です。インストールはソースコードから直接行うことも、Dockerを通じて行うこともでき、さまざまな接続方法に対応しています。\n\nWattWiseは、システムの負荷や電気料金に応じてCPUとGPUの性能を調整し、ピーク時の電力を節約します。効果的な電力管理のために、シンプルなPI制御戦略が用いられています。\n\nインターフェースは直感的で、現在の電力使用量や過去のデータを表示します。Kasaプラグへの直接接続やHome Assistantとの統合もサポートしています。\n\n現在のところ、WattWiseは単一のKasaスマートプラグでのみ動作し、電力管理機能にはLinuxシステムが必要です。\n\nユーザーは、簡単なコマンドを使ってWattWiseをインストールし、電力使用量を継続的に監視したり、迅速な読み取りを行ったりできます。\n\n今後の計画には、複数のプラグのサポート、他のブランドとの互換性、視覚化の向上、より良い予測アルゴリズムの開発が含まれています。\n\nWattWiseは、効果的な電力使用の監視を目的とした個人プロジェクトとして始まり、自動化された電力管理機能を含むように進化しました。このプロジェクトはオープンソースであり、コミュニティからの貢献や適応を歓迎しています。"
    }
  },
  {
    "id": "64f593cb399426b9",
    "title": {
      "en": "The Guardian flourishes without a paywall",
      "ko": "가디언, 유료 없이 번창!",
      "ja": "ガーディアンの成功"
    },
    "type": "story",
    "url": "https://nymag.com/intelligencer/article/how-the-guardian-us-flourishes-without-a-paywall.html",
    "score": 601,
    "by": "bookofjoe",
    "time": 1743208287,
    "content": "stop the presses\n\n          The Newspaper Flourishing Without a Paywall\n\n            By\n        Charlotte Klein,\n          a features writer and media columnist at New York Magazine\n\n              Mar. 27, 2025\n\n              saved\n\n              Save this article to read it later.\n\n              Find this story in your account’s ‘Saved for Later’ section.\n\n                Comment\n\n                  Photo-Illustration: Intelligencer; Photos: Getty Images\n\n        There was a time in media when having a billionaire owner was an asset. For many outlets, this is still the case, particularly those publishing super-secretive group chats of powerful government officials who seem eager to trample on the rights of the press. But for others, like the Washington Post and the Los Angeles Times, both of which have seen their benefactors bend a knee to Donald Trump and sacrifice the integrity of their newspapers in the process, ownership has become a problem. A problem that the Guardian US, the American arm of the British newspaper, has exploited.\n\n  “All around us, media organizations have begun to capitulate,” editor Betsy Reed writes in the current fundraising appeal at the bottom of each Guardian article. “The Guardian has neither a self-interested billionaire owner nor profit-seeking corporate henchmen pressuring us to appease the rich and powerful. With the new administration boasting about its desire to punish journalists, and Trump and his allies already pursuing lawsuits against newspapers whose stories they don’t like, it has never been more urgent, or more perilous, to pursue fair, accurate reporting. Can you support The Guardian today?”\n\n  The asks themselves are not new.The Guardian for the past eight years has asked readers to donate in support of its journalism, a “reader revenue” program that helps the paper exist without a paywall. But the appeals have gotten more strident — and more Trump-focused — since the Post and the L.A. Times withdrew expected endorsements for Kamala Harris last fall, and to great effect. The last week in February was the second-biggest week in U.S. reader-revenue history for The Guardian; the biggest was the week of the 2024 presidential election.\n\n  The Guardian US expects to hit $44 million in voluntary reader donations in the U.S. and Canada this year, up 33 percent over last year, with some people supporting on a recurring basis and others giving one-time contributions. A first-person account by a Canadian citizen detained by ICE, for example, netted $105,000 in pledged donations. These contributions account for more than 60 percent of the American operation’s total revenue, which last year exceeded costs by $16 million (the rest of its revenue comes from advertising and philanthropic support from foundations). The amount that people are contributing on a pledged basis doubled in October —the month that Jeff Bezos pulled the Post’s endorsement for Harris — and has stayed at that level every month since, according to internal data provided by The Guardian, which just surpassed 350,000 recurring supporters in the U.S.\n\n  “Our messaging does appeal to people to support our work as a kind of cause, and I think that works for a number of reasons,” Reed said in a recent interview in her downtown office, “but one is that there is a real crisis of access to reliable information for people who don’t want or have the means to subscribe to the New York Times. That is a real problem that we have an answer to.”\n\n  Guardian managing director Steve Sachs added that The Guardian’s global perspective — the internal catchphrase of the U.S. operation is that it covers America for the world and the world for America — is also a draw at such a volatile moment in geopolitics. “People are even more interested now than they were six months ago,” he said. The Guardian US’ audience is split 50-50 between American and global readers.\n\n  The Guardian launched its U.S. operation more than a decade ago and seemed to have a promising start, hiring high-profile columnists like Michael Wolff and Glenn Greenwald and winning a Pulitzer for reporting on widespread secret surveillance by the National Security Agency. But as the Obama era gave way to the Trump era, the close association of the Guardian US with surveillance-state reporting made it less relevant to American readers. Annual losses led to restructuring that hit the U.S. operation, once housed in an airy Soho loft, particularly hard.\n\n  But during the pandemic, the Guardian US, like other news outlets, experienced a surge in reader interest, readying it for an expansion in 2022 that included hiring Reed, formerly of The Nation and the Intercept, and Sachs, who’d spent years on the business side of Time Inc. The audience has continued to grow, with Sachs noting that the site typically sees between 40 million to 50 million unique visitors a month. “We’re now at a place where our audience is actually bigger in the U.S. than The Wall Street Journal’s audience in the U.S.,” he said.\n\n  Leaders and organizations across the media industry have been trying to innovate in the way people can pay for news. Sachs says he’s been increasingly asked about the no-paywall model, with others in the industry wondering whether they should adopt the same. “If you’re creative enough, you can figure out some new things,” he tells people. “But the question is, does it scale?” If it’s “another half-million dollars, a million dollars, whatever, it’s not worth the effort,” he said. Other publications that were donations-focused, including Vox.com (which is owned by the same parent company as New York), have moved away to membership models as a result. “The reason I think that it works for us is we cover so much breaking news and it drives a lot of traffic, and we have the scale to make it work,” Reed said. “Even if we only monetize one percent, it’s still a lot.” Still, even scaled-up publications like the Guardian US have to contend with the fact that donations are very boom-and-bust.\n\n  Despite The Guardian’s strident anti-Trump fundraising pushes, its broader audience is less partisan, as is the tone of its news coverage. It’s a weird line to straddle. “The appeals that you see at the bottom of articles are really framed around issues of press freedom and our identity and our structure of ownership,” Reed said. “They are not appeals that say, ‘Trump is bad, you need to support The Guardian, we are against Trump.’” Maybe not explicitly. But they are clearly benefitting from this moment and using the new money to hire, with expectations to continue growing its staff in the U.S. this year.\n\n  The big question is: Will it last? “I mean, unfortunately, there’s no shortage of global crises going on, from what’s happening in the Middle East to Ukraine to what’s happening in this country,” Reed said. “The world is incredibly interested in the shitshow here. God, it’s like they’re rubbernecking at a car crash.”\n\n          Sign Up for the IntelligencerNewsletter\n          Daily news about the politics, business, and technology shaping our world.\n\n            Email\n\n          This site is protected by reCAPTCHA and the Google\n          Privacy Policy and\n          Terms of Service apply.\n\n        Vox Media, LLC Terms and Privacy Notice\n        By submitting your email, you agree to our Terms and Privacy Notice and to receive email correspondence from us.\n\n    More From This Series\n\n              The New Substack Universe\n\n              MSNBC’s Post–Joy Reid, Post-NBC Future Is Kind of Bright?\n\n              How Trump Is Dividing and Conquering the White House Press Corps\n\n        See All\n\n    Tags:\n\n            stop the presses\n\n            media\n\n            the guardian\n\n        Show\n\n    Leave a Comment\n\n      The Newspaper Flourishing Without a Paywall\n\n    const freeLayoutsInstances = [\n      'ecom-article',\n      'ecom-products',\n      'non-monetizable'\n    ];\n    const paywalledLayoutsInstances = ['paywalled-article'];\n    const layoutInstance = document.querySelector('html').getAttribute('data-layout-uri').split('/instances/')[1].replace('@published', '');\n    const siteSlug = 'intelligencer';\n    const keywords = [\"stop the presses\",\"media\",\"the guardian\"]; // This is set by handlebars in the server.\n    const featureTypes = window._nymPermutive?.article?.featureTypes;\n    const freeConditions = {\n      isStrategist: !paywalledLayoutsInstances.includes(layoutInstance) && siteSlug === 'strategist',\n      isFreeLayout: freeLayoutsInstances.includes(layoutInstance),\n      hasExcludePaywallTags: /paywall exclude/i.test(keywords.join(',')),\n      isEcomm: featureTypes && featureTypes.includes('ecomm')\n    };\n    const structuredData = {\n      '@context': 'http://schema.org',\n      '@id': '#articleSchema',\n      hasPart: {\n        '@type': 'WebPageElement',\n        cssSelector: '.article-content',\n        isAccessibleForFree: false\n      },\n      isAccessibleForFree: false\n    };\n    for (const condition of Object.keys(freeConditions)) {\n      if (!freeConditions[condition]) continue;\n      structuredData.isAccessibleForFree = true;\n      structuredData.hasPart.isAccessibleForFree = true\n      break;\n    }\n    const ldJsonScript = document.createElement('script');\n    ldJsonScript.type = \"application/ld+json\";\n    ldJsonScript.innerHTML = JSON.stringify(structuredData);\n    document.head.appendChild(ldJsonScript);",
    "summary": {
      "en": "**Summary:**\n\nThe Guardian US has successfully operated without a paywall by relying on reader donations, which have increased significantly, especially during politically charged times. Unlike other newspapers that have compromised their integrity under billionaire ownership, The Guardian promotes itself as independent and committed to fair reporting. \n\nIn recent months, donations have surged to over $44 million, comprising more than 60% of its revenue. This growth was partly fueled by a strong response to fundraising appeals highlighting press freedom and the importance of reliable information, especially amid political unrest. \n\nThe Guardian's unique approach, which includes covering global perspectives, has attracted a diverse audience of around 40 to 50 million monthly visitors, surpassing even the Wall Street Journal in the U.S. However, the sustainability of this model remains uncertain, as it relies heavily on fluctuating donation patterns.",
      "ko": "가디언 US는 독자들의 기부에 의존하여 성공적으로 운영되고 있으며, 특히 정치적으로 긴장된 시기에 기부가 크게 증가했습니다. 다른 신문들이 억만장자의 소유 아래에서 신뢰성을 저버린 것과 달리, 가디언은 독립적이고 공정한 보도를 지향한다고 홍보하고 있습니다.\n\n최근 몇 달 동안 기부금은 4천4백만 달러를 넘어서며 수익의 60% 이상을 차지하게 되었습니다. 이러한 성장은 언론 자유와 신뢰할 수 있는 정보의 중요성을 강조한 모금 캠페인에 대한 강력한 반응 덕분입니다. 특히 정치적 불안정 속에서 이러한 메시지가 많은 사람들에게 공감을 얻었습니다.\n\n가디언의 독특한 접근 방식은 전 세계적인 관점을 다루며, 매달 약 4천만에서 5천만 명의 다양한 독자를 끌어모으고 있습니다. 이는 미국 내에서 월스트리트 저널을 초과하는 수치입니다. 그러나 이 모델의 지속 가능성은 기부 패턴의 변동성에 크게 의존하고 있어 불확실한 상황입니다.",
      "ja": "ガーディアンUSは、読者からの寄付に依存することで、成功裏にペイウォールなしで運営されています。特に政治的に緊張した時期には、寄付が大幅に増加しました。億万長者の所有下で信頼性を損なった他の新聞とは異なり、ガーディアンは独立したメディアとして、公正な報道を重視しています。\n\n最近数ヶ月で、寄付は4400万ドルを超え、収益の60%以上を占めるようになりました。この成長は、報道の自由や信頼できる情報の重要性を強調した資金調達の呼びかけに対する強い反応によって部分的に促進されました。特に政治的な混乱の中で、こうしたメッセージが響いたのです。\n\nガーディアンの独自のアプローチは、世界的な視点を取り入れた報道を行っており、毎月約4000万から5000万人の多様な読者を惹きつけています。この数字は、アメリカのウォール・ストリート・ジャーナルをも上回っています。しかし、このモデルの持続可能性は不確かであり、寄付の動向に大きく依存しています。"
    }
  },
  {
    "id": "52c2f371e86332fe",
    "title": {
      "en": "Silicon Valley, Halt and Catch Fire, and How Microserfdom Ate the World (2015)",
      "ko": "실리콘 밸리의 불꽃",
      "ja": "シリコンバレーの逆襲"
    },
    "type": "story",
    "url": "https://grantland.com/hollywood-prospectus/silicon-valley-halt-catch-fire-microserfs-douglas-coupland/",
    "score": 105,
    "by": "Apocryphon",
    "time": 1743541574,
    "content": "Start-up Costs: ‘Silicon Valley,’ ‘Halt and Catch Fire,’ and How Microserfdom Ate the World\n\n\t\t\t\t\t\t\t\t\tMario Zucca\n\n\t\t\t\t\t\t\tTV\n\n\t\t\t\t\t\t\t\tJune 19, 2015\n\n\t\t\t\t\t\t\tby Alex Pappademas\n\n\t\t\t\t\t\t\t\t\tFacebook\n\t\t\t\t\t\t\t\t\tTwitter\n\n\t\t\t\t\t\t\t\tPrint\n\n\t\t\t\t\t\t\tDouglas Coupland’s novel Microserfs is about the spiritual yearnings and time-frittering activities of youngish coders immersed in the drudgery of the software-development process, and how those activities become an expression of those yearnings. It was published 20years ago this month, which as far as I’m aware makes it the earliest significant stab by a fiction writer at the Great North American Tech-Company/Start-up Novel. It predates Po Bronson’s The First $20 Million Is Always the Hardest, Thomas Pynchon’s Bleeding Edge, Dave Eggers’sThe Circle, numerous other neuroman-à-clefs, score-settling pseudomemoirs and murder-dot-com whodunits,1 as well as tech-sector TV shows like Silicon Valley and Halt and Catch Fire, serials that pick up where the novels leave off.\nCoupland’s evocation of the inner lives of programmers turning entrepreneur in mid-’90s Redmond and Palo Alto should, in theory, enjoy the reputational equivalent of what marketers call first-mover advantage. But I’m not sure it does. Nobody’s talked about it much in the last two decades. Coupland’s most famous book is still his first one, which named a generation (or two) and made Coupland (b. 1961) the most visible and visibly reluctant spokesman for his peer group pre–Kurt Cobain. Universal Pictures bought the film rights to Microserfs two years after it was published, and IMDb insists a low-budget movie did get made in 2011, but there’s no trace of it elsewhere on the Internet.2 Nor did it become a Fox TV show, despite a mention in Wired that one was in the works. Maybe the overall bagginess of the book’s epistolary narrative stymied potential screenwriters; maybe adapting a book whose first and arguably most important section is about the office culture of a nonfictional household-name software company then run by one of the richest men in America presented insurmountable legal challenges. Had the TV show taken place among the cubicles of “GloboCom” and featured characters speaking in reverent tones of their omniscient CEO “Gil Yates,” something would probably have been lost. As it stands, the only extant Microserfs adaptation I know of is the abridged audiobook, read by Friends star (and soon-to-be Windows 95 shill) Matthew Perry. I have it on cassette tape; it is maybe the most ’90s object I have ever owned, a curio as totemic as a lock of Alanis Morissette’s hair preserved in a flannel-swaddled vial of Crystal Pepsi.\nMicroserfs is written as a series of entries from the PowerBook journal of a 26-year-old named Daniel, who when the story begins is making $26,000 a year as a “bug checker” in Building Seven3 of Microsoft’s Redmond campus, living in a $235-a-month group house with other Microsoft employees, and dealing with anxieties professional and existential:\n[F]ear of not producing enough; fear of not finding a little white-with-red-printing stock option envelope in the pigeonhole; fear of losing the sensation of actually making something anymore; fear about the slow erosion of perks within the company; fear that the growth years will never return again; fear that the bottom line is the only thing that really drives the process; fear of disposability…\nHe has trouble sleeping and maintaining relationships and struggles with a sense of alienation from the physical world. “I feel like my body is a station wagon in which I drive my brain around,” he writes, “like a suburban mother taking the kids to hockey practice.” There’s a case to be made that this — the disconnect between the mental and the physical as experienced by members of a caste of golden-handcuffed knowledge workers, and the possibility of greater harmony between car and driver, consciousness-wise — is the book’s real subject, rather than Microsoft or “tech” or start-up-house life. When Daniel and most of his housemates leave Washington and Microsoft for Silicon Valley to work on a Next Big Thing hatched by their programming-genius coworker Michael, the move precipitates a kind of spring thaw within the crew. Everyone renegotiates their relationship with their body in one way or another: characters take up shiatsu massage, come out of the closet, come to grips with childhood eating disorders, dress experimentally, grow their hair out, fuck and procreate, and work up the courage to meet their Net-chat crushes offline. The post-corporate work environment becomes a context in which these former brains-in-a-jar learn to feel the sun on their skin again — while building a revolutionary new software application called Oop!, which allows users to do object-oriented programming by manipulating Lego-like bricks and seems to anticipate Minecraft more than a little bit.\nMicroserfs was fiction grounded in embedded reporting; it began its life as a magazine story for Wired. More than one outlet had offered to send Coupland to Redmond, ostensibly to write about its burgeoning population of Gen-X-aged techies. “They really just wanted me to spy on Bill Gates and write about that,” Coupland told an interviewer in 1994. “I said that I wouldn’t do it … I got Wired and John Battelle to write it into the contract that I was to write a piece about Microsoft and not Bill Gates.” Speaking to the New York Times that same year, he described his sojourn among the code-monkeys as “a ‘Gorillas in the Mist’ kind of observation. What do they put in their glove compartments? What snack foods do they eat? What posters are on their bedroom walls?” Wired ran Coupland’s first Microsoft piece — which would become the opening chapter of the novel — in its January 1994 issue. Coupland himself appears on the cover, fronting a quintet of serious-faced men and women presumably representing Generation Microsoft. He’s wearing a yellow spandex cycling shirt and a few days’ worth of stubble and looks a little like a bike messenger who’s been asked to stand in for a celebrity. The background is blue sky and everyone’s looking off into the middle distance; it’s like a Leninist Sears portrait.\n\nThe story inside presents Coupland’s anthropological notes on young Microsoft, right down to the snack-food question — when Michael suffers a professional setback and sequesters himself in his office on page two, Daniel and his gym-rat coworker Todd procure flat food (Kraft Singles, fruit leather) to slide under his door. The ironic thing about the story is that while it’s not the de facto Gates profile Coupland felt other magazines pressuring him to do, it’s certainly about Bill Gates, in that it’s about Microsoft employees for whom the boss is a kind of holy ghost. One of the faux–Barbara Kruger–isms punctuating the hardcover of Coupland’s previous book, Life After God, was, “You are the first generation to be raised without religion”; in Microserfs,those unused belief-muscles find purpose again through faith in Bill. Here’s Daniel, stopping between buildings to consider the mist above the company soccer field: “I had this weird feeling — of how the presence of Bill floats about the Campus, semi-visible, at all times, kind of like the dead grandfather in the Family Circus cartoons. Bill is a moral force, a spectral force, a force that shapes, a force that molds. A force with thick, thick glasses.”\nOne reason why Microserfs is a strange read that feels epochs and not just decades old today is that its vision of Gates has been superseded in the culture at least twice — first by the image of Gates as a Hank Scorpio–esque corporate shark that emerged from the Senate hearings into the Microsoft–Department of Justice antitrust settlementin the early ’00s, then by Gates’s rebirth as benevolent mega-philanthropist, underwriter of NPR programming, and provider of clean water to the developing world. Another reason is timing. The novel was published in 1995, but Coupland did his reporting (several weeks at Microsoft, and later several more in the Bay Area tech-start-up scene) in 1993 and 1994. Rather than an on-the-ground account of the first tech boom, then, Microserfs isan inadvertent time capsule of the moment just before the explosive growth of the consumer-facing Internet transformed society’s relationship to technology.\nIn that sense, the story and the subsequent book are thematically consistent with the earliest issues of Wired, which are notable both for their blue-sky, smart-drink futurism and for what they don’t see coming. The magazine was a year old when the Microserfs cover story ran; the “Net Surf” column in that same issue makes note of the growing popularity of something called the “World Wide Web.” It’s described as “a distributed system that presents the user with documents full of hypermedia links to other documents or information systems. Selecting one of these links, the user can then access more information about a particular topic.” Reading an explanation like this in 2015 is not unlike reading the Wikipedia description for water, “a transparent fluid which forms the world’s streams, lakes, oceans and rain, and is the major constituent of the fluids of living things”; the fact that the Web needed describing at all tells you something about the extent to which the future was still being negotiated as Coupland researched and wrote this book. In the old Net Surf columns, the future you’re living in by reading this article on your computer or phone glimmers on the horizon, but long-dead jargon words like finger and telnet and point your gopher are still in everyday use; in Microserfs, Daniel dismisses the whole concept of the “information superhighway” as a manufactured trend. “This highway — is it a joke? You hear so much about it, but really, what is it … The media has gone berserk with Net-this and Net-that. It’s a bit much. The Net is cool, but not that cool.”\nMicroserfs hit stores in 1995, which turned out to be a pretty big year for Net-this and Net-that. Yahoo, Amazon, and Craigslist were founded; Javascript, the MP3 compression standard, cost-per-click and cost-per-impression advertising, the first “wiki” site, and the Internet Explorer browser were introduced. Netscape went public; Bill Gates wrote the infamous “Internet Tidal Wave” memo to Microsoft executives, proclaiming in the course of 5,000-plus words that the Internet was “the most important single development to come along since the IBM PC was introduced in 1981.” Meanwhile, at any time between May and September, you could walk into a multiplex not yet driven out of business by Netflix and watch a futuristic thriller like Hackers or Johnny Mnemonic or Virtuosity or The Net, movies that capitalized on the culture’s tech obsession as if it were a dance craze, spinning (mostly absurd) visions of the (invariably sinister) ways technology would soon pervade our lives. Microserfs isn’t as hysterical as those movies, and its vision of the coming world is much brighter, but in its own way it’s just as wrongheaded and nailed-to-its-context.\n\n“What is the search for the next great compelling application,” Daniel asks at one point, “but a search for the human identity?” Microserfs argues that the entrepreneurial fantasy of ditching a big corporation to work at a cool start-up with your friends can actually be part of that search — that there’s a way to reinvent work in your own image and according to your own values, that you can find the same transcendence within the sphere of commerce that the slackers in Coupland’s own Generation X4eschewed McJobs in order to chase. The notion that cutting the corporate cord to work for a start-up often just means busting out of a cubicle in order to shackle oneself to a laptop in a slightly funkier room goes unexamined; the possibility that work within a capitalist system, no matter how creative and freeform and unlike what your parents did, might be fundamentally incompatible with self-actualization and spiritual fulfillment is not on the table.\nThis, to paraphrase Portlandia, is one of the dreams of the ’90s — that our work selves and our true natures could be one and the same. 1995 also marked the debut of Fast Company magazine, whose first issue shouted “WORK IS PERSONAL”in type as big and bold as the publication’s name. Inside that first issue, ideas about boom-time productivity and hipness and the search for meaning hung out and did whatever.\n“As far as I’m concerned, having to change your life when you arrive at work each morning is tantamount to slavery,” says the head of an Intel microprocessor fabrication plant, who adjustedhis hard-charging management style after suffering a heart attack at 36. (“To this day he visits cardiac units every six months,” we’re told, “‘just to look at the gray faces and remember.’”) Kathy Ryan, then AOL’s “Vice President of Cool,” explains her title: “Often you’ll see it spelled k-e-w-e-l. It’s used when the whole is greater than the sum of the parts, when interesting graphics, low prices, innovative concepts, and interactivity all come together. That’s kewel.” She then names a bunch of kewel websites that no one will ever hear of again. The only vision of the future of work that doesn’t sound like a New Economy opium dream comes courtesy of Hatim Tyabji, then the CEO of VeriFone, who answers a question about his demanding and (back then) unusually email-driven management style as follows:\nAll I can say is that every person has to come to terms with himself or herself in the context of this new environment. Let’s say it’s Sunday, and you’re at home. You walk past the den or bedroom, wherever your computer is. Are VeriFone people more likely than other people to log on? Absolutely. Am I expecting that? To some extent, yes. But I’m not demanding that. You have to decide … Now the reality is, if you are a global company, you can’t say ‘It’s Sunday in the United States so I’m not going to think about work.’ If it’s Sunday here, it’s Monday in Australia, and people there may need you. So it’s a never-ending cycle. I make no bones about that.\nWe are all more like VeriFone people than we used to be. Mobile-device commercials appropriate the rhetoric of freedom to sell us the devices that will enable our jobs to reach into our lives at any moment, long after we’ve left the office, for the day or for life. More often than not, WORK IS PERSONAL not because we’ve turned it into a space where we can be our best selvesbut because it bleeds into and colonizes the part of the day we’re supposed to spend living and loving and finding ourselves. Microserfs isn’t without cynicism about Silicon Valley and the new paradigms that were being birthed there circa 1993-94. “I suppose,” Daniel says, “that this is the birthplace of the new postindustrial economy here amid the ghosts of apricot orchards, spinach farms and horse ranches … Here, where sexy new technologies are being blueprinted, CAD’ed, engineered, imagineered and modeled — post-machines making countless millions of people obsolete overnight.”\nFrom across a cultural divide, the nerds from Redmond regard the breezily profligate supernerds of Palo Alto with a mix of envy and horror: “They’re immune to money. They just sort of assume it’ll appear like rain.” With Steve Jobs in exile and the Web’s billionaire boys’ club still a few years away, the Valley in the book is “a bland anarchy,” a kingdom “with a thousand princes but no kings.” A few of those princes, of course, will grow up. They’ll disrupt industries for disruption’s sake, tank the economy at least once, vigorously defend their right to treat employees like contractors, and turn a new generation of coder-dreamers into serfs. Coupland’s characters can’t conceive of any of this yet. Capitalism still seems like it can be saved from within; no matter how much it takes over your life, work never seems like work in the traditional sense, as long as there’s a trampoline in the backyard that you can jump on while thinking about God. And that, to coin a phrase, is how they get you.\nNowadays, of course, serious thinkers addressing tech culture and its promises of innovation/liberation are expected to wear their suspicion like a sidearm. Adapting Ben Mezrich’s nonfiction book The Accidental Billionaires for the screen as The Social Network, Aaron Sorkin looked at Facebook and saw junior Jacobins preparing to guillotine their social and intellectual betters. The movie is close to perfect as cinema and deeply suspect as commentary. It’s too busy judging Mark Zuckerberg and his motives to reflect on the real spiritual consequences of rampant technologization. Sorkin’s gravestone as a thinking person will have HE READ THE COMMENTS engraved on it; aggrieved by the existence of an Internet where anyone, no matter how uninformed, can just go online and say bad things about Aaron Sorkin, he turns Zuckerberg’s invention of Facebook into the vengeance of a lovelorn loser. In the movie’s last scene, Jesse Eisenberg’s Zuckerberg hits refresh in vain on the friend request that Rooney Mara’s Erica won’t accept, while the soundtrack sticks the Beatles’ “Baby You’re a Rich Man” in his guts like a shiv. The moment reads two ways. Either it’s about Zuckerberg, for all his billions, being Just Like Us — a slave to the same digital toy to which we’ve subcontracted management of our memories, our personal interactions, and our sense of self-worth — or it’s about how Facebook, born of Zuckerberg’s sense of exclusion, has made us just like him. The notion that the tech visionaries whose inventions colonize our daily lives are actually uploading the virus of their personalities to the global unconscious is an idea that floats through The Social Network; it’s the engine that powers Alex Gibney’s forthcoming documentary SteveJobs: The Man in the Machine, which stops just short of postulating that every iPhone includes a fragment of poor angry uncharitable emotionally stunted Steve’s immortal soul, like a sliver of wormwood from Mordor.\n♦♦♦\nEven Coupland would never be this credulous again about the tech sector and its promises; nothing he’s written since could be mistaken for propaganda. He undertook a grueling book tour for Microserfs that left him exhausted and suffering from depression, and his fiction in the ensuing years went to a pretty apocalyptic and Ballardian place. He even took a hammer to his own back catalogue in 2006’s JPod, a bearded-Spock reimagining of Microserfs set at a video-game company, with side trips via human trafficking to the industrial wastes of China, where one character acquires a heroin habit after a stint in a bootleg-Nike factory. When another character mentions Coupland’s name, someone groans, “That asshole.” Coupland himself shows up later in the book as a cynical and foul-mouthed would-be entrepreneur angling to poach programmers for a weather-prediction start-up. It’s a guilty meta-meltdown, the closest thing to a Deconstructing Harry or Yeezus in Coupland’s bibliography. JPod is a tech-culture story informed by a decade of evidence that the tech revolution has disproportionately benefited terrible people while doing nothing in particular for our evolution as a species into beings of pure light.\nHBO\nMike Judge’s Silicon Valley, which just wrapped its second season on HBO this week, turns that dispiriting reality into the premise for a sitcom. Judge made his way to the real Silicon Valley after graduating from UC San Diego with a physics degree in 1985. He got a tester-engineer job at an interface-card company in Sunnyvale, called in sick on many a Monday, and quit after two and a half months to play upright bass in a blues band. Eventually he discovered animation, and went on to create Beavis and Butt-Head and King of the Hill. But he also wrote and directed 1999’s Office Space, about white-collar drones toiling in the kind of soul-desiccating corporate cubicle-warren from which the Internet was supposed to liberate us. One of the dark truths underlying Silicon Valley is that on some level workers might have had it easier under the Bill Lumberghs of the world; better a boss who asks you to come in on weekends to fill out TPS reports than a job that annihilates the idea of weekends entirely.\nOur hero, if that’s the right word here, is Richard Hendricks (Thomas Middleditch), a socially awkward and flop-sweaty programmer at the Google-esque software giant Hooli. Richard almost always wears a hoodie and a look of skin-crawling embarrassment; he’s the kind of gamma male to whom even Mark Zuckerberg might have administered atomic wedgies. And that’s the joke: No matter how much it still thinks of itself as the Wild West, the Valley is now an entrenched and hierarchical culture, in which power accrues to anyone willing to do the necessary bullying. Richard lives and works at a start-up incubator run by Erlich Bachman, played by T.J. Miller, who expertly conjures both Val Kilmer in Real Genius and Garfield sniffing for lasagna, underlining each gem of highly suspect entrepreneurial wisdom with contrails of pot smoke. Richard’s not a visionary; the project he’s incubating at Erlich’s place is a music-player app that even Erlich can’t see the point of, until someone else looks at Richard’s code and realizes he’s accidentally created a potentially revolutionary new data-compression algorithm. When he chooses to start his own company, Pied Piper, and develop the algorithm with funding from a venture capitalist (the late Christopher Evan Welch) instead of taking a $10 million buyout from his hard-charging Hooli boss, Gavin Belson, he becomes a CEO overnight, and realizes almost as quickly that the job comes with requirements he’s not equipped to handle. “If you’re not an asshole,” Erlich tells him, “it creates this kind of asshole vacuum, and that void is filled by other assholes.” The tech sector is no longer a space in which you can be your best self — it’s a space that demands your worst, and Richard has spent the rest of the series fighting that reality, usually in vain.\nTwo seasons in, Silicon Valley is a good show but not yet a great one. The pacing is awkward, as if the actors are leaving room after each zinger for a laugh track we can’t hear. The cameos by real-life Valley royalty (Snapchat’s Evan Spiegel, Cameron and Tyler Winklevoss, tech journalists Kara Swisher and Walter Mossberg, Google’s Eric Schmidt) are as winkingly self-serving as Entourage’s famous-actor walk-ons always were, and add just as little in the way of verisimilitude — plus actors, unlike billionaires, can usually act. We get more than enough reality from the opening credits, which depict the Valley as an ever-changing SimCity-as-Westeros circuit board of animated tech-company logos signifying fortunes in perpetual flux. Napster is a balloon that inflates and collapses; Yahoo’s logo is elbowed into a subordinate position by Alibaba’s; Facebook’s logo just gets bigger. The show is as cynical about tech entrepreneurs and venture capitalists as its Sunday-night lead-out Veep is about Washington.5 There’s a discussion about whether a crappy piece of software is “Apple Maps bad, or just Zune bad”; VCs in a pitch meeting calmly explore the possibility that a new playground-finder app for parents might also have “pedophile-facing” implications.\nHBO\nThe other residents of Erlich’s incubator become Pied Piper’s first employees, including Gilfoyle (Martin Starr) and Dinesh (Kumail Nanjiani), who communicate through scatological insults and references to jerking off. “Our corporate culture is that we don’t have a corporate culture,” Erlich says, not unproudly. The dude-clubhouse sets are an achievement in production design; when you look at them, you can practically smell the energy-drink burps and the trash can that everyone has decided is someone else’s problem. We’re meant to root for these guys against Hooli the way we rooted for the Deltas against the Omegas in Animal House; they don’t stand for anything in particular, but their loutishness establishes them as de facto counterculture heroes, standing in opposition to the pretensions of ruling-class phonies like Gavin Belson, who says things like, “I don’t know about you people, but I don’t want to live in a world where someone else makes the world a better place better than we do.”\nBut the smartest thing about Silicon Valley is that — at least so far — it’s not a show about a scrappy new start-up taking on the big guys and winning; it’s a show about a scrappy new start-up taking on the big guys and barely surviving. Better-lawyered and deeper-pocketed, Hooli keeps finding new ways to knock them around; every ally they find on the VC side turns out to have an ego-driven agenda of their own. Richard finds out that Welch’s Peter Gregory funded Pied Piper only as part of a feud with Belson; when he expresses incredulity to Gregory’s assistant (Amanda Crew), she responds, “That’s nothing. Peter would spend millions just to mildly annoy Gavin. These are billionaires, Richard. Annoying each other means more to them than we’ll make in a lifetime.” They’re utterly at the mercy of the behemoths whose hegemony they’re supposed to be disrupting, pawns moved about the board by capricious gods. And in that sense, they’re just like us — there’s something cathartically funny about watching the Pied Piper guys suit up for a game that’s already been decided because our own interactions with Apple, Google, and Facebook tend to leave us feeling just as powerless.\nThe quintessential Silicon Valley character isn’t Richard or Erlich or Belson; it’s Richard’s old Hooli coworker Nelson “Big Head” Bighetti, who stays behind when Richard strikes out on his own. Big Head has nothing to offer the company or the world; his one invention is a lousy and morally deplorable app called “Nip Alert.” But like Being There’s Chauncey Gardner or the hypnotized Ron Livingston in Office Space, his blankness registers with management as the sphinxlike affect of a star. Belson, obsessed with re-creating the Pied Piper algorithm, repeatedly promotes Big Head all the way to a position as “head dreamer.” In Judge’s universe, money is destruction, success just makes you a target, cynicism blankets Northern California like Wi-Fi, and the only morally sound way to win is by not trying. Forget the search for human identity, self-reinvention, post-humanity, God in the machine, art, emotion, making a faith out of commercial detritus, or building the future; Richard and the rest of the Pied Piper crew are too busy trying to avoid being bulldozed into a landfill.\n♦♦♦\nIn Microserfs, Daniel’s Microsoft coworker and eventual girlfriend Karla describes coders as “the fabricators of the human dream’s next REM cycle … building the center from which all else will be held.” Nobody talks that way about computers anymore; maybe it was always a sales pitch when they did. Our relationship to technology and our sense of the people who profit from putting it in our hands has changed dramatically since 1995; we’re no longer convinced that the engineers of our way of life have our best interests at heart. (The apple.com page where you can buy the $17,000 yellow-gold Apple Watch actually says the watch “represents a new chapter in the relationship people have with technology” — a chapter in which Apple stops pretending it isn’t as much a luxury-goods concern as LVMH, I guess.) So The Social Network works because it villainizes Mark Zuckerberg, and Ex Machina’s twists feel telegraphed because of course Oscar Isaac’s health-conscious tech billionaire is a creep, and Silicon Valley connects because it makes its tech bros hapless and goofy enough to be sympathetic, and the only place where the old notion of programmers as creatives and inventors of new paradigms can still live and breathe is on a low-rated TV show that’s also a period piece set 30years in the past.\nAMC’s Halt and Catch Fire takes its name from a mythical command that would supposedly cause a computer to accelerate and multitask to the point of self-destruction; in Halt, ex-IBM executive Joe MacMillan (Lee Pace) does approximately the same thing to a pokey early-’80s Dallas software company called Cardiff Electric, hot-wiring its business model by cloning an IBM machine and launching Cardiff into the personal-computer market before his bosses can object. The part about the cloning is more or less fact-based; Joe and engineer Gordon Clark (Scoot McNairy) are essentially doing what the founders of Texas-born Compaq did in 1982. This was the dawn of the PC wars, a hugely important moment in personal-computing history, but it’s hard to look at these guys and imagine their story striking anyone as a worthy successor to Mad Men.6 You can understand why Halt’s creators felt the need or the pressure to overload Pace’s Joe with a whole soap opera’s worth of plot-sparking character traits — a murky employment history, hidden agendas, sexual ambiguity, a propensity for stagey outbursts of violence. The A.V. Club smugly rechristened him “Tron Draper.”\nFor a while, you could assume you knew where all this was headed — to Joe and Gordon changing the world with their amazing knockoff computer, to an Apple parallel with Joe as Steve Jobs and Gordon as Steve Wozniak, to the truism that innovation in technology requires perfect sharks as well as perfect geeks. And yet from the beginning, you could feel the show trying to escape the arc suggested by its premise, to pull the rug out on itself. In the pilot, Joe meets Cameron, a punk computer science major who between games of Centipede has come up with some oddly prescient ideas about a worldwide network of linked computers. They have sex; later, when Joe needs to produce an engineer who’s never seen IBM’s code in order to evade legal action from Big Blue, she becomes Cardiff Electric’s newest employee.\nAMC\nIn the beginning, Cameron, like Joe, is a strung-together collection of traits and ideas — about women in technology, about women in general, about punk —that the actress Mackenzie Davis managed to render consistent by sheer force of will. But Cameron becomes the character whose presence bends the story away from what “really happened.” Joe has instructed Gordon and Cardiff’s engineers to build a computer, the Cardiff Giant, that’s twice as fast as anything on the market and costs half as much; they’re close to accomplishing this goal when Cameron comes up with the idea to give the Giant a personality, via an interface that asks questions like, “What do you want to do today?” She’s essentially invented Siri a generation ahead of schedule, dreaming up a future when computers aren’t mere office supplies. Joe loves the idea, because he’s in love with Cameron; Gordon objects, on the grounds that giving the machine a soul will be hard to implement and cost them in speed, although Gordon’s wife, Donna, who’s also an engineer, understands the idea immediately. Suddenly we were watching a show about artistic collaboration and compromise, and while we all knew Gordonwaswrong when he groused that a personable OS will be like “having your mother inside your computer,” we couldsee where he was coming from. The story twists again: Joe loses his nerve. The Giant goes to market as a regular old fast/cheap PC. Then, in a Comdex hotel room darkened as if for a séance, Joe comes face-to-face with his first Macintosh, and realizes he’s made the wrong call: “It speaks,” he says, his voice full of wonder and dread. We realize we’ve spent the better part of a season watching these characters fail — that Gordon and Joe aren’t going to become the Jobs and Wozniak of this world because Jobs and Wozniak are the Jobs and Wozniak of this world.\nAnd it quickly became a show with no comfort zone to return to; Joe became a frustrated artist, Gordon ended up a businessman, and Cameron took off with Donna to start an online-gaming company called Mutiny out of a suburban Dallas punk-pad. A third of the way into Season 2, Halt’s vision of start-up life at Mutiny looks a little like the early days of Nolan Bushnell’s Atari, back when the company consisted of T-shirted potheads building Pong machines in an old roller rink. If there’s a substantive Mad Men parallel to be drawn here, it’s that these shows share a willingness to shift the ground under their characters’ feet and understand that sometimes the best stories involve people on the wrong side of history. Recall that for all his brilliance as an ad man, Don Draper could often be as blinkered and resistant to change as anyone else at Sterling Cooper. He started out in Season 1 sneering at Doyle Dane Bernbach’s iconic “Lemon” ad for Volkswagen, which sparked advertising’s so-called Creative Revolution; it took him a decade and no small amount of suffering to get around to buying the world a Coke. There’s no story to be told about someone who’s always right.\n“Something’s coming, and it’s gonna be big, and it won’t include this place, and it won’t include you,” Joe MacMillan tells Cardiff Electric’s founder, on what might be the last day of their acquaintance. But the compelling thing about this show as a portrait of an industry destined to transform our world is that the change in the air might not include Joe or Gordon or even Cameron, who in a recent episode was presented with something that looked an awful lot like a social network and failed to see the point of it. She’s trying to make online gaming happen in a world that’s about a year away from going cartridge-crazy again thanks to Nintendo. We’ll see. The important thing is the sense of manic excitement that pervades the Mutiny scenes this season, and the way it bleeds into the show itself. We know exactly where computers are and aren’t going, but it still feels like anything is possible.\n\n\t\t\t\t\t\tFiled Under: TV, Silicon Valley, Microserfs, Mike Judge, halt and catch fire, Douglas Coupland, Internet, Microsoft, apple, Steve Jobs, Bill Gates, Wired, Fast Company, Internet Explorer\n\n\t\t\tAlex Pappademas is a staff writer for Grantland.\n\n\t\t\t\tArchive\n\t\t\t\t\t\t\t\t\t@ PAPPADEMAS\n\n\t\t\t\t\t\t\tMore From Alex Pappademas\n\t\t\t\t\t\t\t\tMore TV\n\t\t\t\t\t\t\tMore Hollywood Prospectus\n\n\t\t\t\tMore from Alex Pappademas\n\n\t\t\t\t\t\t\tBrand Echh: Sandra Bullock and Billy Bob Thornton Can’t Save the Lame ‘Our Brand Is Crisis’  October 30, 2015\n\n\t\t\t\t\t\t\tWitch, Please: Vin Diesel’s ‘The Last Witch Hunter’ Is Both Supremely Nerdy and Not Nerdy Enough  October 23, 2015\n\n\t\t\t\t\t\t\tHard to Be a God: ‘Steve Jobs’ Thinks Different About Steve Jobs  October 9, 2015\n\n\t\t\t\t\t\t\tThe Light Stuff: Ridley Scott’s Fun, Remote ‘The Martian’  October 2, 2015\n\n\t\t\t\t\t\t\tKook Skywalker: ‘The Walk’ Takes Viewers Through the Wire  October 1, 2015\n\n\t\t\t\tSee all from Alex Pappademas\n\n\t\t\t\tMore TV\n\n\t\t\t\t\t\t\t50 Scenes That Do Not Appear in the Fox ‘X-Files’ Revival  October 30, 2015\n\n\t\t\t\t\t\t\tThe State of Scary TV: ‘The Returned’ and ‘Ash vs Evil Dead’ Join ‘The Walking Dead’ in Offering High-Quality Horror  October 29, 2015\n\n\t\t\t\t\t\t\t‘The Andy Greenwald Podcast’: Aya Cash of ‘You’re the Worst’  October 29, 2015\n\n\t\t\t\t\t\t\tAsh vs. Bruce Campbell: The B Movie Legend Returns to ‘The Evil Dead’  October 28, 2015\n\n\t\t\t\t\t\t\t‘The Andy Greenwald Podcast’: ‘Fargo’ Showrunner Noah Hawley  October 27, 2015\n\n\t\t\t\tSee all TV\n\n\t\t\t\tMore Hollywood Prospectus\n\n\t\t\t\t\t\t\tBrand Echh: Sandra Bullock and Billy Bob Thornton Can’t Save the Lame ‘Our Brand Is Crisis’  October 30, 2015\n\n\t\t\t\t\t\t\t50 Scenes That Do Not Appear in the Fox ‘X-Files’ Revival  October 30, 2015\n\n\t\t\t\t\t\t\tIn Praise of Beach Slang, 2015’s Best, Most Sincere Rock Band  October 29, 2015\n\n\t\t\t\t\t\t\tWho Was Missing From Taylor Swift’s Miami Squad?  October 29, 2015\n\n\t\t\t\t\t\t\tHappy ‘Halloween’: The Best Horror-Movie Monsters  October 29, 2015\n\n\t\t\t\tSee all Hollywood Prospectus\n\n\t\t\t\t\t\t\t\t\t\t\tPrevious Story\n\t\t\t\t\t\t\tGrantland Q&A: Talking With Director Pete Docter About Pixar’s Terrific ‘Inside Out’\n\t\t\t\t\t\t➤\n\n\t\t\t\t\t\t\t\t\t\t\tNext Story\n\t\t\t\t\t\t\tSongs of the Week: Welcome Back, Janet Jackson\n\t\t\t\t\t\t\t➤\n\n\t\t\t\t\t\t\t\t\tThe And Greenwald Podcast – Aya Cash\n\n\t\t\t\t\t\t\t\tOctober 28, 2015\n\n\t\t\t\t\t\t\t\t\tThe Andy Greenwald Podcast – 'Fargo' Showrunner Noah Hawley\n\n\t\t\t\t\t\t\t\tOctober 27, 2015\n\n\t\t\t\t\t\t\t\t\tGrantland Pop Culture – Aaron Sorkin\n\n\t\t\t\t\t\t\t\tOctober 23, 2015",
    "summary": {
      "en": "The article discusses Douglas Coupland's novel *Microserfs*, which was published 20 years ago and explores the lives of young coders working at Microsoft in the mid-1990s. The book is significant as an early fictional representation of the tech industry and its culture, predating other notable works like *The Circle* and popular TV shows like *Silicon Valley* and *Halt and Catch Fire*.\n\n*Microserfs* follows a character named Daniel, who works as a \"bug checker\" at Microsoft. He and his friends experience anxiety and existential fears related to their jobs and the tech industry. The story highlights their transition from corporate life to a more free-spirited start-up environment in Silicon Valley, where they seek personal fulfillment and a reconnection with their bodies and identities. The book reflects the cultural moment just before the rise of the internet and the tech boom, capturing the hopes and disillusionments of the time.\n\nCoupland's work is seen as a blend of fiction and journalism, drawing from his experiences observing Microsoft employees. The narrative captures the early 90s tech culture and the complex relationship between workers and their corporate identities, suggesting that the dream of finding personal meaning in work is often undermined by the realities of capitalism.\n\nThe article also compares *Microserfs* with contemporary shows like *Silicon Valley*, which portrays a more cynical view of the tech industry, highlighting the struggles of start-ups against larger corporations. It notes that the idealistic vision of tech workers as creators of meaningful change has shifted, as evidenced by the darker tones in modern portrayals of Silicon Valley and its entrepreneurs. \n\nIn summary, the piece reflects on how *Microserfs* provided an early lens into the tech culture that has evolved into a more complex and often disillusioned narrative in today’s media.",
      "ko": "이 글은 더글라스 쿠플랜드의 소설 *마이크로서프스*에 대해 다루고 있습니다. 이 소설은 20년 전에 출간되었으며, 1990년대 중반 마이크로소프트에서 일하는 젊은 프로그래머들의 삶을 탐구합니다. 이 책은 기술 산업과 그 문화를 초기 소설적으로 표현한 중요한 작품으로, *더 서클*이나 인기 TV 프로그램인 *실리콘 밸리*, *홀트 앤드 캐치 파이어*보다 앞서 출간되었습니다.\n\n*마이크로서프스*는 마이크로소프트에서 \"버그 검사기\"로 일하는 다니엘이라는 캐릭터를 중심으로 전개됩니다. 그는 친구들과 함께 직업과 기술 산업과 관련된 불안과 존재론적 두려움을 경험합니다. 이야기는 그들이 기업 생활에서 실리콘 밸리의 보다 자유로운 스타트업 환경으로 전환하는 과정을 보여주며, 개인적인 성취와 신체 및 정체성과의 재연결을 추구합니다. 이 책은 인터넷과 기술 붐이 일어나기 직전의 문화적 순간을 반영하며, 그 시기의 희망과 실망을 포착합니다.\n\n쿠플랜드의 작품은 허구와 저널리즘이 결합된 형태로, 마이크로소프트 직원들을 관찰한 경험에서 영감을 받았습니다. 이 서사는 90년대 초 기술 문화를 담고 있으며, 노동자와 그들의 기업 정체성 간의 복잡한 관계를 보여줍니다. 개인적인 의미를 찾으려는 꿈이 자본주의의 현실에 의해 종종 훼손된다는 점을 시사합니다.\n\n이 글은 또한 *마이크로서프스*와 현대의 *실리콘 밸리* 같은 프로그램을 비교합니다. 후자는 기술 산업에 대한 보다 냉소적인 시각을 제시하며, 스타트업이 대기업과 맞서는 고난을 강조합니다. 기술 근로자들이 의미 있는 변화를 창출하는 이상적인 비전이 변화했음을 보여주며, 현대의 실리콘 밸리와 그 기업가들에 대한 어두운 묘사가 이를 뒷받침합니다.\n\n결론적으로, 이 글은 *마이크로서프스*가 어떻게 기술 문화에 대한 초기 시각을 제공했는지를 반영하며, 오늘날의 미디어에서 더욱 복잡하고 종종 실망스러운 서사로 발전했음을 보여줍니다.",
      "ja": "この記事では、ダグラス・クープランドの小説『マイクロサーフス』について取り上げています。この作品は20年前に出版され、1990年代中頃のマイクロソフトで働く若いプログラマーたちの生活を描いています。この本は、テクノロジー業界とその文化を早期にフィクションとして表現した重要な作品であり、他の著名な作品やテレビ番組、例えば『ザ・サークル』や『シリコンバレー』、さらには『ホルト・アンド・キャッチ・ファイア』よりも先に発表されました。\n\n『マイクロサーフス』は、マイクロソフトで「バグチェッカー」として働くダニエルというキャラクターを中心に展開します。彼と彼の友人たちは、仕事やテクノロジー業界に関連する不安や存在的な恐怖を抱えています。物語は、彼らが企業生活からより自由なスタートアップ環境へと移行する様子を描き、個人的な充実感や身体とアイデンティティとの再接続を求める姿を示しています。この本は、インターネットとテクノロジーブームが始まる前の文化的な瞬間を反映し、その時代の希望や失望を捉えています。\n\nクープランドの作品は、フィクションとジャーナリズムの融合と見なされており、彼自身がマイクロソフトの従業員を観察した経験からインスピレーションを得ています。物語は90年代初頭のテクノロジー文化や、労働者と企業アイデンティティとの複雑な関係を描写し、仕事において個人的な意味を見出す夢が資本主義の現実によってしばしば損なわれることを示唆しています。\n\nこの記事では、現代の『シリコンバレー』のような番組と『マイクロサーフス』を比較し、テクノロジー業界に対するよりシニカルな見方を強調しています。スタートアップが大企業に対抗する苦労を描写し、テクノロジー労働者が意味のある変化を創造する理想的なビジョンが変化してきたことを指摘しています。現代のシリコンバレーやその起業家たちの描写には、より暗いトーンが見られることがその証拠です。\n\n要するに、この記事は『マイクロサーフス』がどのようにテクノロジー文化を早期に捉え、今日のメディアにおけるより複雑でしばしば失望に満ちた物語へと進化してきたかを反映しています。"
    }
  },
  {
    "id": "cd33a492c7138a97",
    "title": {
      "en": "Debts, Tech and Otherwise",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://blogs.newardassociates.com/blog/2025/debts-tech-and-otherwise.html",
    "score": 43,
    "by": "BerislavLopac",
    "time": 1743407574,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "67cbf2ab0359f132",
    "title": {
      "en": "Mithril.js: small (8.96 KB gzipped) modern client-side JavaScript framework",
      "ko": "미쓰릴.js: 경량 JS 프레임워크",
      "ja": "ミスリル.js: 軽量モダンJSフレームワーク"
    },
    "type": "story",
    "url": "https://github.com/MithrilJS/mithril.js",
    "score": 34,
    "by": "Brysonbw",
    "time": 1743534428,
    "content": "Mithril.js\n\nWhat is Mithril.js?\nInstallation\nDocumentation\nGetting Help\nContributing\n\nWhat is Mithril.js?\nA modern client-side JavaScript framework for building Single Page Applications. It's small (8.96 KB gzipped), fast and provides routing and XHR utilities out of the box.\nMithril.js is used by companies like Vimeo and Nike, and open source platforms like Lichess 👍.\nMithril.js supports IE11, Firefox ESR, and the last two versions of Firefox, Edge, Safari, and Chrome. No polyfills required. 👌\nInstallation\nCDN\n<!-- Development: whichever you prefer -->\n<script src=\"https://unpkg.com/mithril/mithril.js\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/mithril/mithril.js\"></script>\n\n<!-- Production: whichever you prefer -->\n<script src=\"https://unpkg.com/mithril/mithril.min.js\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/mithril/mithril.min.js\"></script>\n\nnpm\nnpm install mithril --save\n\nThe \"Getting started\" guide is a good place to start learning how to use Mithril.js.\nTypeScript type definitions are available from DefinitelyTyped. They can be installed with:\n$ npm install @types/mithril --save-dev\n\nDocumentation\nDocumentation lives on mithril.js.org.\nYou may be interested in the API Docs, a Simple Application, or perhaps some Examples.\nGetting Help\nMithril.js has an active & welcoming community on Zulip, or feel free to ask questions on Stack Overflow using the mithril.js tag.\nContributing\nThere's a Contributing FAQ on the Mithril.js site that hopefully helps, but if not definitely hop into the Zulip stream and ask away!\n\nThanks for reading!\n🎁",
    "summary": {
      "en": "**Mithril.js Summary**\n\n- **What is Mithril.js?**\n  - Mithril.js is a lightweight JavaScript framework (8.96 KB gzipped) for building Single Page Applications (SPAs). It is fast and includes built-in routing and XHR utilities. It is used by companies like Vimeo and Nike and supports major browsers without the need for polyfills.\n\n- **Installation**\n  - You can install Mithril.js via CDN or npm:\n    - **CDN:** Use script tags from unpkg or jsDelivr for both development and production.\n    - **npm:** Run `npm install mithril --save`.\n\n- **Documentation**\n  - Visit mithril.js.org for documentation, including API docs, a guide to building a simple application, and examples.\n\n- **Getting Help**\n  - Join the Mithril.js community on Zulip or ask questions on Stack Overflow with the mithril.js tag.\n\n- **Contributing**\n  - Check the Contributing FAQ on the Mithril.js site for guidance, and feel free to ask questions in the Zulip community.\n\nThanks for reading!",
      "ko": "Mithril.js는 경량의 자바스크립트 프레임워크로, 단일 페이지 애플리케이션(SPA)을 만드는 데 사용됩니다. 크기는 8.96 KB로 압축되어 있으며, 빠른 속도를 자랑합니다. 내장된 라우팅 기능과 XHR 유틸리티를 포함하고 있어, Vimeo와 Nike와 같은 기업에서도 사용되고 있습니다. 주요 브라우저를 지원하며, 폴리필 없이도 작동합니다.\n\nMithril.js는 CDN이나 npm을 통해 설치할 수 있습니다. CDN을 사용할 경우, unpkg나 jsDelivr의 스크립트 태그를 이용하면 됩니다. npm을 통해 설치하려면 `npm install mithril --save` 명령어를 실행하면 됩니다.\n\n문서화는 mithril.js.org에서 확인할 수 있으며, API 문서, 간단한 애플리케이션 구축 가이드, 예제 등이 포함되어 있습니다.\n\n도움이 필요하다면 Zulip에서 Mithril.js 커뮤니티에 참여하거나 Stack Overflow에서 mithril.js 태그를 사용해 질문할 수 있습니다.\n\n기여를 원하신다면 Mithril.js 사이트의 기여 FAQ를 참고하시고, Zulip 커뮤니티에서 질문해도 좋습니다.",
      "ja": "Mithril.jsは、軽量なJavaScriptフレームワークで、主にシングルページアプリケーション（SPA）を構築するために使用されます。サイズは8.96 KB（gzip圧縮時）で、非常に高速です。ルーティング機能やXHR（XMLHttpRequest）ユーティリティが組み込まれており、VimeoやNikeなどの企業でも利用されています。また、主要なブラウザに対応しており、ポリフィルを必要としません。\n\nMithril.jsのインストールは、CDNまたはnpmを通じて行えます。CDNを利用する場合は、unpkgやjsDelivrのスクリプトタグを使用して、開発環境と本番環境の両方で利用できます。npmを使う場合は、コマンドラインで「npm install mithril --save」と入力します。\n\nドキュメントは、mithril.js.orgで確認できます。APIドキュメントやシンプルなアプリケーションの構築ガイド、実例などが掲載されています。\n\nサポートが必要な場合は、ZulipのMithril.jsコミュニティに参加するか、Stack Overflowで「mithril.js」タグを使って質問を投稿できます。\n\n貢献を希望する方は、Mithril.jsのサイトにある貢献に関するFAQをチェックし、Zulipコミュニティで質問をすることもできます。"
    }
  },
  {
    "id": "64fec81d4aa198b2",
    "title": {
      "en": "Why Kagi launched \"no use, no pay\"",
      "ko": "카기, \"사용 안 하면 요금 없다\" 출시!",
      "ja": "カギの新提案「使わなければ払わない」"
    },
    "type": "story",
    "url": "https://getlago.substack.com/p/why-kagi-launched-no-use-no-pay",
    "score": 73,
    "by": "AnhTho_FR",
    "time": 1743505532,
    "content": "Share this postThe Bill, PleaseWhy Kagi launched “no use, no pay”Copy linkFacebookEmailNotesMoreDiscover more from The Bill, PleaseThe Bill, Please tells the stories and learnings from building Lago, a YC-backed open-source billing & metering product.Over 1,000 subscribersSubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.Already have an account? Sign inWhy Kagi launched “no use, no pay”FinnApr 01, 20253Share this postThe Bill, PleaseWhy Kagi launched “no use, no pay”Copy linkFacebookEmailNotesMoreSharePaying for a SaaS subscription you don’t use sucks. Sure, I don’t use that shameful cupboard with the ice cream maker, sous-vide device and electric ham-cutter either, but at least those things don’t charge me again!In SaaS, billing is an underrated aspect of the customer experience. Money brings the customer-vendor relationship down to brass tacks. Even the friendliest customer support reps lose their charm if you feel like you’re getting ripped off.The ad-free subscription search engine Kagi recently did the opposite: Buried in their changelog, they invested in their customers with a simple billing change that unexpectedly went viral. The change was simple: Kagi wouldn’t bill you if you didn’t use the product in a given month. No use, no pay.The change didn’t even get its own blog article:But the decision rippled outward from Kagi’s own Discord to social media and then to the top of Hacker news, from which the word spread to media sites including the Verge.It caught our attention since we’re in the billing/pricing/monetization space and it was the first time we heard about something like this. It’s also a great example of how billing practices can make a real impact on your brand and positioning.I sat down with Brandon Saltalamacchia who leads marketing at Kagi to chat about this update and its reception. Here’s what I learned from him:Why Kagi decided to not charge usersAs Brandon told me, the idea first came about from an idea session: “Our Founder Vlad is friends with an incredible marketer in the UK known as Rory Sutherland. They were on a call together brainstorming ideas and talking about their businesses when Rory suggested refunding users who don’t use our product in a month. A no use, no pay scheme.”Generous schemes like this often look like marketing ploys. Any “no questions asked 100% money back guarantee” to me just reads like “refunding anyone who asks is cheaper than doing customer support”.But I don’t have that cynicism with Kagi’s fair pricing. Had this been for marketing, it would have come with a launch, not buried in a changelog along with minor improvements.No read, no pay. Subscribe to our newsletter and don’t pay, even if you do read.SubscribeI asked Brandon for the motivation behind this. He told me: “This billing strategy aligns with treating our customers like neighbors and there was no intention except for making customers happy.We had zero conversations about how this would impact revenue, if it could reactivate dormant users or if it could increase new subscriber growth.”There’s an interesting insight here: Business teams often start with the goal of increasing activation or mitigating churn or attract more users. Fair pricing surely did all three for Kagi, yet it wasn’t the intention. Sometimes, just doing something people will obviously like is the right thing to do—whether or not there’s a business justification for it.This would be harder to sell in a VC-funded startup (the only outside investors in Kagi are members of its own community). Kagi doesn’t need to grow at breakneck speed and show increasing numbers every quarter.A caveat is that this is uniquely suited for search. It’s a product category that creates value through active usage, so this policy would be pointless for things that run in the background. You’d never expect to be refunded if your cybersecurity threat monitoring doesn’t detect anything in a month. You pay for that peace of mind!Is this all a marketing scheme?Then we were a little overwhelmed with the responses from around the world. We did not expect to see this reaction. In my mind this was a “small” feature, even though it took a few weeks to plan and implement, but looking back at it, it’s a huge feature and I wish we did it earlier.It wasn’t a small project by any means, lots had to be done to make it a fluid experience for our users, down to the production of email copy, to making sure there are no bugs on our end with crediting back the cost.”That’s what Brandon told me when I asked him about the marketing aspect of all this.This last part is my favorite part of the interview. The idea sounds easy to build. But when building this, these things often run into issues with the billing system:Do you want to refund the money or issue a credit for a free month?If you want to issue credits, do you have a credit system?With 3 paid plans, how do we account for credits for users on different plans?What if someone upgrades their plan while still having a credit?If a customer is on a yearly plan, do they pay less the next year, do they get a free month after their plan or can they withdraw the money?These are just the first 5 things that come to mind—and I’m sure the team had to accommodate more edge cases. Building all of this is why billing is important, but underrated. Flexibility to ship pricing/monetization updates requires a billing system that can accommodate all the cases without breaking anything. Not having a flexible system means these kinds of updates take months to ship and require multiple engineers.(Lago is that system and makes it easy to ship billing changes like that)How Kagi benefitted from its billing changeWhen I asked Brandon about the results of this, he told me that “Most importantly, our current customers are very happy that this has been implemented. A very tiny % of our users forget to use our product in a single month, so it affects very little of our community, but it’s there as a safety net for our customers so that they can relax, knowing that should in the future they forget to use Kagi Search, we will look after them and credit their accounts.”That’s in addition to the virality of this “little” update also brought in a lot of new customers who heard about Kagi for the first time because of the buzz. It also brought in customers who were previously on the fence who now felt safe that they weren’t taking a risk.The day of the announcement marked their biggest single day of growth in 6 months.Will this catch on?I wondered whether this kind of practice could become more popular.“A part of me wants to be optimistic about this, but unfortunately for a typical SAAS company it goes against their very nature to implement something like this, as it risks losing reliable recurring revenue, so I can’t see it being implemented by many anytime soon.”I agree here. Most startups are struggling because they’re burning VC money or are   barely kept afloat by ramen profitability. They won’t reject money a user is contractually obligated to give them.That’s short-term thinking, of course. A user who pays for a product they don’t use will be frustrated and churn anyway, while someone whose account is more paused (they’re not paying) might come back. If you’re focused on hypergrowth and report in 3-month intervals, the long-term trust gains don’t really matter.Kagi can afford this because they go further than being bootstrapped and profitable:  As a Public Benefit Corporation, not beholden to maximizing shareholder value.That doesn’t mean that no use, no pay is a bad business decision. Ultimately, it sacrifices short-term revenue for long-term trust. And that matters most when you’re not looking for a quick acquisition or eyeing an IPO.You made it all the way here? Looks like you enjoyed this newsletter. You know what that means. Time to subscribe!Subscribe3Share this postThe Bill, PleaseWhy Kagi launched “no use, no pay”Copy linkFacebookEmailNotesMoreShare",
    "summary": {
      "en": "Kagi, an ad-free subscription search engine, recently introduced a \"no use, no pay\" billing policy, meaning users won't be charged if they don't use the service in a given month. This change, suggested during a brainstorming session, quickly gained attention and went viral on social media, demonstrating the importance of fair billing practices in customer satisfaction.\n\nBrandon Saltalamacchia, Kagi's marketing lead, explained that the decision was made to enhance customer happiness rather than to boost revenue. Although this approach could potentially lead to increased user engagement and new subscribers, Kagi prioritized customer care over immediate financial gain. \n\nImplementing this policy required significant planning to address various billing complexities, but it ultimately resulted in a positive response from current customers and attracted new ones. The day of the announcement marked Kagi's most substantial growth in six months.\n\nWhile this practice may not be widely adopted by other SaaS companies due to concerns about steady revenue, Kagi's model allows them to prioritize long-term trust over short-term profits, as they are a Public Benefit Corporation not focused on maximizing shareholder value.",
      "ko": "광고 없는 구독 기반 검색 엔진인 카기는 최근 \"사용하지 않으면 요금 없음\"이라는 새로운 청구 정책을 도입했습니다. 이 정책은 사용자가 특정 월에 서비스를 이용하지 않으면 요금이 부과되지 않는다는 의미입니다. 이 변화는 브레인스토밍 세션에서 제안되었으며, 빠르게 주목을 받으며 소셜 미디어에서 화제가 되었습니다. 이는 고객 만족을 위한 공정한 청구 관행의 중요성을 보여줍니다.\n\n카기의 마케팅 책임자인 브랜든 살타라마키아는 이 결정이 수익 증대보다는 고객의 행복을 증진하기 위해 이루어졌다고 설명했습니다. 이 접근 방식은 사용자 참여와 신규 구독자를 늘릴 가능성이 있지만, 카기는 즉각적인 재정적 이익보다 고객 관리에 더 중점을 두었습니다.\n\n이 정책을 시행하기 위해서는 다양한 청구 복잡성을 해결하기 위한 상당한 계획이 필요했지만, 결과적으로 현재 고객들로부터 긍정적인 반응을 얻었고 새로운 고객도 유치할 수 있었습니다. 발표 당일은 카기에게 지난 6개월 동안 가장 큰 성장의 날이었습니다.\n\n이러한 관행은 다른 SaaS 기업들이 안정적인 수익에 대한 우려로 인해 널리 채택하지 않을 수 있지만, 카기의 모델은 단기 이익보다 장기적인 신뢰를 우선시할 수 있게 해줍니다. 카기는 주주 가치를 극대화하는 데 초점을 맞추지 않는 공익 기업이기 때문입니다.",
      "ja": "Kagiは、広告なしのサブスクリプション検索エンジンで、最近「使わなければ支払わない」という新しい請求ポリシーを導入しました。これにより、ユーザーは特定の月にサービスを利用しなければ料金が発生しません。この変更はブレインストーミングセッションで提案され、すぐに注目を集め、SNSで話題になりました。これは、公正な請求慣行が顧客満足において重要であることを示しています。\n\nKagiのマーケティング責任者であるブランドン・サルタラマッキアは、この決定は収益を増やすためではなく、顧客の幸福を高めるために行われたと説明しました。このアプローチは、ユーザーのエンゲージメントや新しい加入者の増加につながる可能性がありますが、Kagiは短期的な利益よりも顧客ケアを優先しました。\n\nこのポリシーを実施するには、さまざまな請求の複雑さに対処するための計画が必要でしたが、最終的には既存の顧客から好意的な反応を得て、新しい顧客も引き寄せる結果となりました。発表の日は、Kagiにとって過去6ヶ月で最も大きな成長を遂げた日となりました。\n\nこのような慣行は、安定した収益に対する懸念から他のSaaS企業には広く採用されないかもしれませんが、Kagiのモデルは、株主価値の最大化を目指さない公共利益法人として、短期的な利益よりも長期的な信頼を優先することを可能にしています。"
    }
  },
  {
    "id": "59f729020f7f8a58",
    "title": {
      "en": "Launch HN: Augento (YC W25) – Fine-tune your agents with reinforcement learning",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": null,
    "score": 100,
    "by": "lmeierhoefer",
    "time": 1743442144,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "b9a96ce4b7be1628",
    "title": {
      "en": "Show HN: Duolingo-style exercises but with real-world content like the news",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://app.fluentsubs.com/exercises/daily",
    "score": 450,
    "by": "ph4evers",
    "time": 1743486394,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "a7254282b1f18d4f",
    "title": {
      "en": "The Fifth Kind of Optimisation",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://tratt.net/laurie/blog/2025/the_fifth_kind_of_optimisation.html",
    "score": 3,
    "by": "todsacerdoti",
    "time": 1743589786,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "d35dafb1526d22a4",
    "title": {
      "en": "Sales Compensation Simulator",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.exec.com/sales-comp",
    "score": 107,
    "by": "seanlinehan",
    "time": 1743262447,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "9d2cbf18cd4e0897",
    "title": {
      "en": "Notes on the Pentium's microcode circuitry",
      "ko": "펜티엄 마이크로코드 비밀",
      "ja": "ペンティアムの微細回路"
    },
    "type": "story",
    "url": "https://www.righto.com/2025/03/pentium-microcde-rom-circuitry.html",
    "score": 179,
    "by": "leotravis10",
    "time": 1743446137,
    "content": "Ken Shirriff's blog\n\nComputer history, restoring vintage computers, IC reverse engineering, and whatever\n\nNotes on the Pentium's microcode circuitry\n\nMost people think of machine instructions as the fundamental steps that a computer performs.\nHowever, many processors have another layer of software underneath: microcode.\nWith microcode, instead of building the processor's control circuitry from complex logic gates, the control logic is\nimplemented with code known as microcode, stored in the microcode ROM.\nTo execute a machine instruction, the computer internally executes several simpler micro-instructions, specified by the microcode.\nIn this post, I examine the microcode ROM in the original Pentium, looking at the low-level circuitry.\nThe photo below shows the Pentium's thumbnail-sized silicon die under a microscope.\nI've labeled the main functional blocks.\nThe microcode ROM is highlighted at the right.\nIf you look closely, you can see that the microcode ROM consists of two rectangular banks, one above the other.\nThis die photo of the Pentium shows the location of the microcode ROM. Click this image (or any other) for a larger version.\nThe image below shows a closeup of the two microcode ROM banks.\nEach bank provides 45 bits of output; together they implement a micro-instruction that is 90 bits long.\nEach bank consists of a grid of transistors arranged into 288 rows and 720 columns.\nThe microcode ROM holds 4608 micro-instructions,\n414,720 bits in total.\nAt this magnification, the ROM appears featureless, but it is covered with horizontal wires, each just 1.5 µm\nthick.\nThe 90 output lines from the ROM, with a closeup of six lines exiting the ROM.\nThe ROM's 90 output lines are collected into a bundle of wires between the banks, as shown above.\nThe detail shows how six of the bits exit from the banks and join the bundle.\nThis bundle exits the ROM to the left, travels to various parts of the chip, and controls the chip's circuitry.\nThe output lines are in the chip's top metal layer (M3):\nthe Pentium has three layers of metal wiring with M1 on the bottom, M2 in the middle, and M3 on top.\nThe Pentium has a large number of bits in its micro-instruction, 90 bits compared to 21 bits in the 8086.\nPresumably, the Pentium has a \"horizontal\" microcode architecture, where the microcode bits correspond to low-level control signals,\nas opposed to \"vertical\" microcode, where the bits are encoded into denser micro-instructions.\nI don't have any information on the Pentium's encoding of microcode; unlike the 8086, the Pentium's patents don't provide any clues.\nThe 8086's microcode ROM holds 512 micro-instructions, much less than the Pentium's 4608 micro-instructions.\nThis makes sense, given the much greater complexity of the Pentium's instruction set, including the floating-point unit on the chip.\n\nThe image below shows a closeup of the Pentium's microcode ROM.\nFor this image, I removed the three layers of metal and the polysilicon layer\nto expose the chip's underlying silicon.\nThe pattern of silicon doping is visible, showing the transistors and thus the data stored in the ROM.\nIf you have enough time, you can extract the bits from the ROM by examining the silicon and seeing where transistors are present.\nA closeup of the ROM showing how bits are encoded in the layout of transistors.\nBefore explaining the ROM's circuitry, I'll review how an NMOS transistor is constructed.\nA transistor can be considered a switch between the source and drain, controlled by the gate.\nThe source and drain regions (green) consist of silicon doped with impurities to change its semiconductor properties, forming N+ silicon.\n(These regions are visible in the photo above.)\nThe gate consists of a layer of polysilicon (red), separated from the silicon by a very thin insulating oxide layer. Whenever polysilicon crosses active silicon, a transistor is formed.\nDiagram showing the structure of an NMOS transistor.\nBits are stored in the ROM through the pattern of transistors in the grid.\nThe presence or absence of a transistor stores a 0 or 1 bit.1\nThe closeup below shows eight bits of the microcode ROM. There are four transistors present and four gaps where transistors are\nmissing.\nThus, this part of the ROM holds four 0 bits and four 1 bits.\nFor the diagram below, I removed the three metal layers and the polysilicon to show the underlying silicon.\nI colored doped (active) silicon regions green, and drew in the horizontal polysilicon lines in red.\nAs explained above, a transistor is created if polysilicon crosses doped silicon.\nThus, the contents of the ROM are defined by the pattern of silicon regions, which creates the transistors.\nEight bits of the microcode ROM, with four transistors present.\nThe horizontal silicon lines are used as wiring to provide ground to the transistors, while the horizontal polysilicon lines select one of the\nrows in the ROM.\nThe transistors in that row will turn on, pulling the associated output lines low.\nThat is, the presence of a transistor in a row causes the output to be pulled low, while the absence of a transistor causes\nthe output line to remain high.\nA schematic corresponding to the eight bits above.\nThe diagram below shows the silicon, polysilicon, and bottom metal (M1) layers. I removed the metal from the left to reveal the silicon and polysilicon underneath, but the pattern of vertical metal lines continues there.\nAs shown earlier, the silicon pattern forms transistors. Each horizontal metal line has a connection\nto ground through a metal line (not shown).\nThe horizontal polysilicon lines select a row.\nWhen polysilicon lines cross doped silicon, the gate of a transistor is formed.\nTwo transistors may share the drain, as in the transistor pair on the left.\nDiagram showing the silicon, polysilicon, and M1 layers.\nThe vertical metal wires form the outputs. The circles are contacts between the metal wire and the silicon of a transistor.2\nShort metal jumpers connect the polysilicon lines to the metal layer above, which will be described next.\nThe image below shows the upper left corner of the ROM. The yellowish metal lines are the top metal layer (M3), while the\nreddish metal lines are the middle metal layer (M2).\nThe thick yellowish M3 lines distribute ground to the ROM. Underneath the horizontal M3 line, a horizontal M2 line also\ndistributes ground.\nThe grids of black dots are numerous contacts between the M3 line and the M2 line, providing a low-resistance connection.\nThe M2 line, in turn, connects to vertical M1 ground lines underneath—these wide vertical lines are faintly visible.\nThese M1 lines connect to the silicon, as shown earlier, providing ground to each transistor.\nThis illustrates the complexity of power distribution in the Pentium: the thick top metal (M3) is the primary distribution of\n+5 volts and ground through the chip, but power must be passed down through M2 and M1 to reach the transistors.\nThe upper left corner of the ROM.\nThe other important feature above is the horizontal metal lines, which help distribute the row-select signals.\nAs shown earlier, horizontal polysilicon lines provide the row-select signals to the transistors.\nHowever, polysilicon is not as good a conductor as metal, so long polysilicon lines have too much resistance.\nThe solution is to run metal lines in parallel, periodically connected to the underlying polysilicon lines and\nreducing the overall resistance.\nSince the vertical metal output lines are in the M1 layer, the horizontal row-select lines run in the M2 layer so they don't collide.\nShort \"jumpers\" in the M1 layer connect the M2 lines to the polysilicon lines.\nTo summarize, each ROM bank contains a grid of transistors and transistor vacancies to define the bits of the ROM.\nThe ROM is carefully designed so the different layers—silicon, polysilicon, M1, and M2—work together to maximize the\nROM's performance and density.\nMicrocode Address Register\nAs the Pentium executes an instruction, it provides the address of each micro-instruction to the microcode ROM.\nThe Pentium holds this address—the micro-address—in the Microcode Address Register (MAR).\nThe MAR is a 13-bit register located above the microcode ROM.\nThe diagram below shows the Microcode Address Register above the upper ROM bank.\nIt consists of 13 bits; each bit has multiple latches to hold the value as well as any pushed subroutine micro-addresses.\nBetween bits 7 and 8, some buffer circuitry amplifies the control signals that go to each bit's circuitry.\nAt the right, drivers amplify the outputs from the MAR, sending the signals to the row drivers and column-select circuitry that\nI will discuss below.\nTo the left of the MAR is a 32-bit register that is apparently unrelated to the microcode ROM, although I haven't determined its function.\nThe Microcode Address Register is located above the upper ROM bank.\nThe outputs from the Microcode Address Register select rows and columns in the microcode ROM, as I'll explain\nbelow.\nBits 12 through 7 of the MAR select a block of 8 rows, while bits 6 through 4 select a row in this block.\nBits 3 through 0 select one column out of each group of 16 columns to select an output bit.\nThus, the microcode address controls what word is provided by the ROM.\nSeveral different operations can be performed on the Microcode Address Register.\nWhen executing a machine instruction, the MAR must be loaded with the address of the corresponding\nmicrocode routine.\n(I haven't determined how this address is generated.)\nAs microcode is executed, the MAR is usually incremented to move to the next micro-instruction.\nHowever, the MAR can branch to a new micro-address as required.\nThe MAR also supports microcode subroutine calls; it will push the current micro-address and jump to the new micro-address.\nAt the end of the micro-subroutine, the micro-address is popped so execution returns to the previous location.\nThe MAR supports three levels of subroutine calls, as it contains three registers to hold the stack of pushed micro-addresses.\nThe MAR receives control signals and addresses from standard-cell logic\nlocated above the MAR.\nStrangely, in Intel's published floorplans for the Pentium, this standard-cell logic is\nlabeled as part of the branch prediction logic, which is above it.\nHowever, carefully tracing the signals from the standard-cell logic shows that is connected to the Microcode Address Register, not\nthe branch predictor.\nRow-select drivers\nAs explained above, each ROM bank has 288 rows of transistors, with polysilicon lines to select one of the rows.\nTo the right of the ROM is circuitry that activates one of these row-select lines, based on the micro-address.\nEach row matches a different 9-bit address. A straightforward implementation would use a 9-input AND gate for each\nrow, matching a particular pattern of 9 address bits or their complements.\nHowever, this implementation would require 576 very large AND gates, so it is impractical.\nInstead, the Pentium uses an optimized implementation with one 6-input AND gate for each group of 8 rows.\nThe remaining three address bits are decoded once at the top of the ROM.\nAs a result, each row only needs one gate, detecting if its group of eight rows is selected and if the particular one of eight\nis selected.\nSimplified schematic of the row driver circuitry.\nThe schematic above shows the circuitry for a group of eight rows, slightly simplified.3\nAt the top, three address bits are decoded, generating eight output lines with one active at a time.\nThe remaining six address bits are inverted, providing the bit and its complement to the decoding circuitry.\nThus, the 9 bits are converted into 20 signals that flow through the decoders, a large number of wires, but not unmanageable.\nEach group of eight rows has a 6-input AND gate that matches a particular 6-bit address, determined by which inputs are\ncomplemented and which are not.4\nThe NAND gate and inverter at the left combine the 3-bit decoding and the 6-bit decoding, activating the appropriate row.\nSince there are up to 720 transistors in each row, the row-select lines need to be driven with high current.\nThus, the row-select drivers use large transistors, roughly 25 times the size of a regular transistor.\nTo fit these transistors into the same vertical spacing as the rest of the decoding circuitry, a tricky packing is used.\nThe drivers for each group of 8 rows are packed into a 3×3 grid, except the first column has two drivers (since there\nare 8 drivers in the group, not 9).\nTo avoid a gap, the drivers in the first column are larger vertically and squashed horizontally.\nOutput circuitry\nThe schematic below shows the multiplexer circuit that selects one of 16 columns for a microcode output bit.\nThe first stage has four 4-to-1 multiplexers. Next, another 4-to-1 multiplexer selects one of the outputs.\nFinally, a BiCMOS driver amplifies the output for transmission to the rest of the processor.\nThe 16-to-1 multiplexer/output driver.\nIn more detail, the ROM and the first multiplexer are essentially NMOS circuits, rather than CMOS. Specifically, the ROM's\ngrid of transistors is constructed from NMOS transistors that can pull a column line low, but there are no PMOS transistors in\nthe grid to pull the line high (since that would double the size of the ROM).\nInstead, the multiplexer includes precharge transistors to pull the lines high, presumably in the clock phase before the\nROM is read.\nThe capacitance of the lines will keep the line high unless it is pulled low by a transistor in the grid.\nOne of the four transistors in the multiplexer is activated (by control signal a, b, c, or d) to select the desired line.\nThe output goes to a \"keeper\" circuit, which keeps the output high unless it is pulled low.\nThe keeper uses an inverter with a weak PMOS transistor that can only provide a small pull-up current.\nA stronger low input will overpower this transistor, switching the state of the keeper.\nThe output of this multiplexer, along with the outputs of three other multiplexers, goes to the second-stage multiplexer,5\nwhich selects one of its four inputs, based on control signals e, f, g, and h.\nThe output of this multiplexer is held in a latch built from two inverters. The second latch has weak transistors so the latch\ncan be easily forced into the desired state.\nThe output from the first latch goes through a CMOS switch into a second latch, creating a flip-flop.\nThe output from the second latch goes to a BiCMOS driver, which drives one of the 90 microcode output lines.\nMost processors are built from CMOS circuitry (i.e. NMOS and PMOS transistors), but the Pentium is built from BiCMOS circuitry:\nbipolar transistors as well as CMOS.\nAt the time, bipolar transistors improved performance for high-current drivers; see my\narticle on\nthe Pentium's BiCMOS circuitry.\nThe diagram below shows three bits of the microcode output. This circuitry is for the upper ROM bank; the circuitry is\nmirrored for the lower bank.\nThe circuitry matches the schematic above. Each of the three blocks has 16 input lines from the ROM grid.\nFour 4-to-1 multiplexers reduce this to 4 lines, and the second multiplexer selects a single line. The result is latched\nand amplified by the output driver.\n(Note the large square shape of the bipolar transistors.)\nNext is the shift register that processes the microcode ROM outputs for testing.\nThe shift register uses XOR logic for its feedback; unlike the rest of the circuitry, the XOR logic is irregular since\nonly some bits are fed into XOR gates.\nThree bits of output from the microcode, I removed the three metal layers to show the polysilicon and silicon.\nCircuitry for testing\nWhy does the microcode ROM have shift registers and XOR gates?\nThe reason is that a chip such as the Pentium is very difficult to test: if one out of 3.1 million transistors goes bad, how do you detect it? For a simple processor like the 8086, you can run through the instruction set and be fairly confident that any problem would turn up.\nBut with a complex chip, it is almost impossible to design an instruction sequence that would test every bit of the microcode ROM, every bit of the cache, and so forth.\nStarting with the 386, Intel added circuitry to the processor solely to make testing easier; about 2.7% of the transistors in the 386 were for testing.\nThe Pentium has this testing circuitry for many ROMs and PLAs, including the division PLA that caused the infamous FDIV bug.\nTo test a ROM inside the processor, Intel added circuitry to scan the entire ROM and checksum its contents.\nSpecifically, a pseudo-random number generator runs through each address, while another circuit computes a checksum of the ROM output, forming a \"signature\" word.\nAt the end, if the signature word has the right value, the ROM is almost certainly correct.\nBut if there is even a single bit error, the checksum will be wrong and the chip will be rejected.\nThe pseudo-random numbers and the checksum are both implemented with linear feedback shift registers (LFSR), a shift register along with a few XOR gates to feed the output back to the input.\nFor more information on testing circuitry in the 386, see Design and Test of the 80386, written by Pat Gelsinger, who became Intel's CEO years later.\nConclusions\nYou'd think that implementing a ROM would be straightforward, but the Pentium's microcode ROM is surprisingly complex due to\nits optimized structure and its circuitry for testing.\nI haven't been able to determine much about how the microcode works, except that the micro-instruction is 90 bits wide and\nthe ROM holds 4608 micro-instructions in total.\nBut hopefully you've found this look at the circuitry interesting.\nDisclaimer: this should all be viewed as slightly speculative and there are probably some errors.\nI didn't want to prefix every statement with \"I think that...\" but you should pretend it is there.\nI plan to write more about the implementation of the Pentium, so\nfollow me on Bluesky (@righto.com) or RSS for updates.\nPeter Bosch has done some reverse engineering of the Pentium II microcode; his information is here.\nFootnotes and references\n\nIt is arbitrary if a transistor corresponds to a 0 bit or a 1 bit. A transistor will pull the output line low (i.e. a 0 bit),\nbut the signal could be inverted before it is used.\nMore analysis of the circuitry or ROM contents would clear this up.↩\n\nWhen looking at a ROM like this, the contact pattern seems like it should tell you the contents of the ROM.\nUnfortunately, this doesn't work. Since a contact can be attached to one or two transistors, the contact\npattern doesn't give you enough information.\nYou need to see the silicon to determine the transistor pattern and thus the bits.↩\n\nI simplified the row driver schematic. The most interesting difference is that the NAND gates are optimized to use three\ntransistors each, instead of four transistors. The trick is that one of the NMOS transistors is essentially shared across\nthe group of 8 drivers; an inverter drives the low side of all eight gates.\nThe second simplification is that the 6-input AND gate is implemented with two 3-input NAND gates and a NOR gate for\nelectrical reasons.\nAlso, the decoder that converts 3 bits into 8 select lines is located between the banks, at the right, not at\nthe top of the ROM as I showed in the schematic.\nLikewise, the inverters for the 6 row-select bits are not at the top.\nInstead, there are 6 inverters and 6 buffers arranged in a column to the right of the ROM, which works better for the layout.\nThese are BiCMOS drivers so they can provide the high-current outputs necessary for the long wires and numerous\ntransistor gates that they must drive.↩\n\nThe inputs to the 6-input AND gate are arranged in a binary counting pattern, selecting each row in sequence.\nThis binary arrangment is standard for a ROM's decoder circuitry and is a good way to recognize a ROM on a die.\nThe Pentium has 36 row decoders, rather than the 64 that you'd expect from a 6-bit input. The ROM was made to the size\nnecessary, rather than a full power of two.\nIn most ROMs, it's difficult to determine if the ROM is addressed bottom-to-top or top-to-bottom.\nHowever, because the microcode ROM's counting pattern is truncated, one can see that the top bank starts with 0 at the top\nand counts downward, while the bottom bank is reversed, starting with 0 at the bottom and counting upward.↩\n\nA note to anyone trying to read the ROM contents: it appears that the order of entries in a group of 16 is inconsistent,\nso a straightforward attempt to visually read the ROM will end up with scrambled data.\nThat is, some of the groups are reversed. I don't see any obvious pattern in which groups are reversed.\nA closeup of the first stage output mux. This image shows the M1 metal layer.\nIn the diagram above, look at the contacts from the select lines, connecting the select lines to the mux transistors.\nThe contacts on the left are the mirror image of the contacts on the right, so the columns will be accessed in the opposite\norder.\nThis mirroring pattern isn't consistent, though; sometimes neighboring groups are mirrored and sometimes they aren't.\nI don't know why the circuitry has this layout. Sometimes mirroring adjacent groups makes the layout more efficient, but\nthe inconsistent mirroring argues against this. Maybe an automated layout system decided this was the best way.\nOr maybe Intel did this to provide a bit of obfuscation against reverse engineering.↩\n\nEmail ThisBlogThis!Share to XShare to FacebookShare to Pinterest\n\nLabels:\nintel,\nmicrocode,\nPentium,\nreverse-engineering\n\n3 comments:\n\n<img src=\"//blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEidhui65uLGfWs8Xj2rz9wRCVb0LWNazI4frey6bcRlS8V32KNycnQ0qTGBnvsfxP7yWYIBNOVOaZFECkj0FXu9GF-6k-DmEfGg--kPJ7QjBqjWNNFCZi89lpSI9dtU8g/s45-c/*\" width=\"35\" height=\"35\" class=\"photo\" alt=\"\">\nKen Boak\nsaid...\n\nKen, I know that you like a challenge. Are you familiar with the Ferranti F100-L Cpu from 1977?https://revaldinho.github.io/f100l/doc/F100CPU.html\n\nApril 1, 2025 at 6:19 AM\n\nAnonymous\nsaid...\n\nI've never really understood why everyone went the micro-code way. It seems to me that building a very simple instruction set in the machine like \"select register 5\", \"latch register array to address bus\", \"set memory read pin\", \"select register 4\", \"latch data bus to register array\", and other very low-level things, then letting the assembler convert higher level instruction like \"load r4, [r5]\" into those lower-level operations, would result in a much simpler architecture.This would be similar to certain ARM pseudo-instructions which don't actually exist but are converted into other instructions that do.\n\nApril 1, 2025 at 8:08 PM\n\nAnonymous\nsaid...\n\nWell, the RISC vs. CISC debate was basically about that. You are proposing a \"super-RISC\" that, I suppose, is beyond the point of diminishing returns for different reasons (in the old days, memory space and bandwidth; for current projects, transistors are super cheap).\n\nApril 2, 2025 at 4:08 AM\n\nPost a Comment\n\nOlder Post\n\nHome\n\n      @import url('https://fonts.googleapis.com/css?family=Montserrat:300,400,500,700');\n      .form-preview {\n      display: flex;\n      flex-direction: column;\n      justify-content: center;\n      margin-top: 30px;\n      padding: clamp(17px, 5%, 40px) clamp(17px, 7%, 50px);\n      max-width: 350px;\n      min-height: 200px;\n      border-radius: 6px;\n      box-shadow: 0 5px 25px rgba(34, 60, 47, 0.25);\n      }\n      .form-preview,\n      .form-preview *{\n        box-sizing: border-box;\n      }\n      .form-preview .preview-heading {\n      width: 100%;\n      }\n      .form-preview .preview-heading h5{\n        margin-top: 0;\n        margin-bottom: 0;\n      }\n      .form-preview .preview-input-field {\n      margin-top: 20px;\n      width: 100%;\n      }\n      .form-preview .preview-input-field input {\n      width: 100%;\n      height: 40px;\n      border-radius: 6px;\n      border: 2px solid #e9e8e8;\n      background-color: #fff;\n      outline: none;\n      }\n      .form-preview .preview-input-field input::placeholder, .form-preview .preview-input-field input {\n      opacity: 0.5;\n      color: #000;\n      font-family: \"Montserrat\";\n      font-size: 14px;\n      font-weight: 500;\n      line-height: 20px;\n      text-align: center;\n      }\n      .form-preview .preview-submit-button {\n      margin-top: 10px;\n      width: 100%;\n      }\n      .form-preview .preview-submit-button button {\n      width: 100%;\n      height: 40px;\n      border: 0;\n      border-radius: 6px;\n      line-height: 0px;\n      }\n      .form-preview .preview-submit-button button:hover {\n      cursor: pointer;\n      }\n\n      Get new posts by email:  Subscribe\n\nAbout the site\n\nContact info and site index\n\nPopular Posts\n\nA USB interface to the \"Mother of All Demos\" keyset\n\nThe Pentium contains a complicated circuit to multiply by three\n\nNotes on the Pentium's microcode circuitry\n\nInside a vintage aerospace navigation computer of uncertain purpose\n\nA Multi-Protocol Infrared Remote Library for the Arduino\n\nApple iPhone charger teardown: quality in a tiny expensive package\n\nA dozen USB chargers in the lab: Apple is very good, but not quite the best\n\nMining Bitcoin with pencil and paper: 0.67 hashes per day\n\nSearch This Blog\n\nLabels\n\n386\n\n6502\n\n8008\n\n8085\n\n8086\n\n8087\n\n8088\n\naerospace\n\nalto\n\nanalog\n\nApollo\n\napple\n\narc\n\narduino\n\narm\n\nbeaglebone\n\nbitcoin\n\nc#\n\ncadc\n\ncalculator\n\nchips\n\ncss\n\ndatapoint\n\ndx7\n\nelectronics\n\nf#\n\nfairchild\n\nfpga\n\nfractals\n\ngenome\n\nglobus\n\nhaskell\n\nHP\n\nhtml5\n\nibm\n\nibm1401\n\nibm360\n\nintel\n\nipv6\n\nir\n\njava\n\njavascript\n\nmath\n\nmicrocode\n\noscilloscope\n\nPentium\n\nphoto\n\npower supply\n\nrandom\n\nreverse-engineering\n\nsheevaplug\n\nsnark\n\nspace\n\nspanish\n\nsynth\n\nteardown\n\ntheory\n\nunicode\n\nZ-80\n\nBlog Archive\n\n        ▼\n\n2025\n\n(8)\n\n        ▼\n\nMarch\n\n(3)\n\nNotes on the Pentium's microcode circuitry\nA USB interface to the \"Mother of All Demos\" keyset\nThe Pentium contains a complicated circuit to mult...\n\n        ►\n\nFebruary\n\n(1)\n\n        ►\n\nJanuary\n\n(4)\n\n        ►\n\n2024\n\n(21)\n\n        ►\n\nDecember\n\n(1)\n\n        ►\n\nNovember\n\n(1)\n\n        ►\n\nOctober\n\n(1)\n\n        ►\n\nSeptember\n\n(3)\n\n        ►\n\nAugust\n\n(2)\n\n        ►\n\nJuly\n\n(2)\n\n        ►\n\nJune\n\n(1)\n\n        ►\n\nMay\n\n(1)\n\n        ►\n\nApril\n\n(1)\n\n        ►\n\nMarch\n\n(2)\n\n        ►\n\nFebruary\n\n(3)\n\n        ►\n\nJanuary\n\n(3)\n\n        ►\n\n2023\n\n(35)\n\n        ►\n\nDecember\n\n(4)\n\n        ►\n\nNovember\n\n(2)\n\n        ►\n\nOctober\n\n(3)\n\n        ►\n\nSeptember\n\n(1)\n\n        ►\n\nAugust\n\n(2)\n\n        ►\n\nJuly\n\n(3)\n\n        ►\n\nMay\n\n(1)\n\n        ►\n\nApril\n\n(2)\n\n        ►\n\nMarch\n\n(4)\n\n        ►\n\nFebruary\n\n(5)\n\n        ►\n\nJanuary\n\n(8)\n\n        ►\n\n2022\n\n(18)\n\n        ►\n\nNovember\n\n(3)\n\n        ►\n\nAugust\n\n(1)\n\n        ►\n\nJuly\n\n(1)\n\n        ►\n\nJune\n\n(1)\n\n        ►\n\nMay\n\n(1)\n\n        ►\n\nApril\n\n(4)\n\n        ►\n\nMarch\n\n(2)\n\n        ►\n\nFebruary\n\n(3)\n\n        ►\n\nJanuary\n\n(2)\n\n        ►\n\n2021\n\n(26)\n\n        ►\n\nDecember\n\n(4)\n\n        ►\n\nNovember\n\n(2)\n\n        ►\n\nSeptember\n\n(1)\n\n        ►\n\nAugust\n\n(1)\n\n        ►\n\nJuly\n\n(2)\n\n        ►\n\nJune\n\n(2)\n\n        ►\n\nMay\n\n(1)\n\n        ►\n\nApril\n\n(2)\n\n        ►\n\nMarch\n\n(4)\n\n        ►\n\nFebruary\n\n(4)\n\n        ►\n\nJanuary\n\n(3)\n\n        ►\n\n2020\n\n(33)\n\n        ►\n\nDecember\n\n(2)\n\n        ►\n\nNovember\n\n(3)\n\n        ►\n\nOctober\n\n(2)\n\n        ►\n\nSeptember\n\n(4)\n\n        ►\n\nAugust\n\n(5)\n\n        ►\n\nJuly\n\n(2)\n\n        ►\n\nJune\n\n(3)\n\n        ►\n\nMay\n\n(4)\n\n        ►\n\nApril\n\n(2)\n\n        ►\n\nMarch\n\n(5)\n\n        ►\n\nJanuary\n\n(1)\n\n        ►\n\n2019\n\n(18)\n\n        ►\n\nNovember\n\n(3)\n\n        ►\n\nOctober\n\n(2)\n\n        ►\n\nSeptember\n\n(3)\n\n        ►\n\nAugust\n\n(1)\n\n        ►\n\nJuly\n\n(4)\n\n        ►\n\nApril\n\n(2)\n\n        ►\n\nFebruary\n\n(1)\n\n        ►\n\nJanuary\n\n(2)\n\n        ►\n\n2018\n\n(17)\n\n        ►\n\nDecember\n\n(1)\n\n        ►\n\nSeptember\n\n(4)\n\n        ►\n\nAugust\n\n(1)\n\n        ►\n\nJune\n\n(1)\n\n        ►\n\nMay\n\n(1)\n\n        ►\n\nApril\n\n(1)\n\n        ►\n\nMarch\n\n(3)\n\n        ►\n\nFebruary\n\n(1)\n\n        ►\n\nJanuary\n\n(4)\n\n        ►\n\n2017\n\n(21)\n\n        ►\n\nDecember\n\n(5)\n\n        ►\n\nNovember\n\n(2)\n\n        ►\n\nOctober\n\n(3)\n\n        ►\n\nAugust\n\n(1)\n\n        ►\n\nJuly\n\n(2)\n\n        ►\n\nJune\n\n(2)\n\n        ►\n\nApril\n\n(2)\n\n        ►\n\nMarch\n\n(2)\n\n        ►\n\nFebruary\n\n(1)\n\n        ►\n\nJanuary\n\n(1)\n\n        ►\n\n2016\n\n(34)\n\n        ►\n\nDecember\n\n(2)\n\n        ►\n\nOctober\n\n(5)\n\n        ►\n\nSeptember\n\n(8)\n\n        ►\n\nAugust\n\n(2)\n\n        ►\n\nJuly\n\n(3)\n\n        ►\n\nJune\n\n(4)\n\n        ►\n\nMay\n\n(1)\n\n        ►\n\nApril\n\n(1)\n\n        ►\n\nMarch\n\n(1)\n\n        ►\n\nFebruary\n\n(4)\n\n        ►\n\nJanuary\n\n(3)\n\n        ►\n\n2015\n\n(12)\n\n        ►\n\nDecember\n\n(2)\n\n        ►\n\nNovember\n\n(1)\n\n        ►\n\nOctober\n\n(3)\n\n        ►\n\nAugust\n\n(1)\n\n        ►\n\nMay\n\n(2)\n\n        ►\n\nMarch\n\n(2)\n\n        ►\n\nFebruary\n\n(1)\n\n        ►\n\n2014\n\n(13)\n\n        ►\n\nDecember\n\n(1)\n\n        ►\n\nOctober\n\n(1)\n\n        ►\n\nSeptember\n\n(3)\n\n        ►\n\nMay\n\n(2)\n\n        ►\n\nMarch\n\n(1)\n\n        ►\n\nFebruary\n\n(5)\n\n        ►\n\n2013\n\n(24)\n\n        ►\n\nNovember\n\n(2)\n\n        ►\n\nSeptember\n\n(4)\n\n        ►\n\nAugust\n\n(4)\n\n        ►\n\nJuly\n\n(4)\n\n        ►\n\nJune\n\n(2)\n\n        ►\n\nApril\n\n(1)\n\n        ►\n\nMarch\n\n(2)\n\n        ►\n\nFebruary\n\n(2)\n\n        ►\n\nJanuary\n\n(3)\n\n        ►\n\n2012\n\n(10)\n\n        ►\n\nDecember\n\n(1)\n\n        ►\n\nNovember\n\n(5)\n\n        ►\n\nOctober\n\n(1)\n\n        ►\n\nMay\n\n(1)\n\n        ►\n\nMarch\n\n(1)\n\n        ►\n\nFebruary\n\n(1)\n\n        ►\n\n2011\n\n(11)\n\n        ►\n\nDecember\n\n(2)\n\n        ►\n\nJuly\n\n(2)\n\n        ►\n\nMay\n\n(2)\n\n        ►\n\nApril\n\n(1)\n\n        ►\n\nMarch\n\n(1)\n\n        ►\n\nFebruary\n\n(3)\n\n        ►\n\n2010\n\n(22)\n\n        ►\n\nDecember\n\n(2)\n\n        ►\n\nNovember\n\n(4)\n\n        ►\n\nOctober\n\n(3)\n\n        ►\n\nAugust\n\n(1)\n\n        ►\n\nJune\n\n(1)\n\n        ►\n\nMay\n\n(2)\n\n        ►\n\nApril\n\n(3)\n\n        ►\n\nMarch\n\n(4)\n\n        ►\n\nJanuary\n\n(2)\n\n        ►\n\n2009\n\n(22)\n\n        ►\n\nDecember\n\n(2)\n\n        ►\n\nNovember\n\n(5)\n\n        ►\n\nSeptember\n\n(1)\n\n        ►\n\nAugust\n\n(3)\n\n        ►\n\nJuly\n\n(1)\n\n        ►\n\nJune\n\n(3)\n\n        ►\n\nApril\n\n(1)\n\n        ►\n\nMarch\n\n(3)\n\n        ►\n\nFebruary\n\n(2)\n\n        ►\n\nJanuary\n\n(1)\n\n        ►\n\n2008\n\n(27)\n\n        ►\n\nJuly\n\n(3)\n\n        ►\n\nJune\n\n(1)\n\n        ►\n\nMay\n\n(3)\n\n        ►\n\nApril\n\n(4)\n\n        ►\n\nMarch\n\n(10)\n\n        ►\n\nFebruary\n\n(6)\n\nPowered by Blogger.",
    "summary": {
      "en": "Ken Shirriff's blog explores the intricacies of computer history, vintage computer restoration, and integrated circuit reverse engineering. In a recent post, he delves into the microcode circuitry of the original Pentium processor. \n\nKey points include:\n\n1. **Microcode vs. Machine Instructions**: Microcode is a layer of software that operates beneath machine instructions. Instead of using complex logic gates, processors like the Pentium use microcode stored in ROM (Read-Only Memory) to execute simpler micro-instructions.\n\n2. **Pentium Microcode ROM**: The Pentium features a microcode ROM that is intricate in design. It consists of two banks, each outputting 45 bits for a combined total of 90 bits per micro-instruction, allowing for 4608 micro-instructions in total.\n\n3. **Transistor Arrangement**: The microcode ROM is built from a grid of transistors, where the presence or absence of a transistor represents binary data (0 or 1). The design is optimized to minimize space while maximizing performance.\n\n4. **Microcode Address Register (MAR)**: The MAR holds the address for the micro-instructions, allowing the Pentium to fetch and execute the necessary microcode for machine instructions. It supports operations like incrementing addresses and branching to new micro-addresses.\n\n5. **Row-Select Drivers**: The circuitry uses an efficient design to select rows in the microcode ROM using a combination of address decoding and optimized gates, allowing for effective activation of the necessary rows.\n\n6. **Output Circuitry**: The microcode's output is managed by multiplexer circuits that select the appropriate bits for processing, ensuring that the right microcode is delivered to control the processor's operations.\n\n7. **Testing Circuitry**: To ensure reliability, the Pentium incorporates testing circuits that check the integrity of the microcode ROM, utilizing pseudo-random number generators and checksums to detect any errors.\n\nIn conclusion, the microcode ROM in the Pentium is a complex but crucial component, showcasing advanced design techniques that allow for efficient processing and testing. Shirriff's insights provide a detailed look at the inner workings of this historic processor.",
      "ko": "켄 시리프의 블로그는 컴퓨터 역사, 빈티지 컴퓨터 복원, 집적 회로 역설계의 복잡한 내용을 탐구합니다. 최근 게시물에서는 원래 펜티엄 프로세서의 마이크로코드 회로에 대해 다루고 있습니다.\n\n마이크로코드와 기계 명령어의 차이를 설명하자면, 마이크로코드는 기계 명령어 아래에서 작동하는 소프트웨어의 한 층입니다. 펜티엄과 같은 프로세서는 복잡한 논리 게이트 대신 ROM(읽기 전용 메모리)에 저장된 마이크로코드를 사용하여 간단한 마이크로 명령어를 실행합니다.\n\n펜티엄에는 복잡한 설계를 가진 마이크로코드 ROM이 있습니다. 이 ROM은 두 개의 뱅크로 구성되어 있으며, 각 뱅크는 45비트를 출력하여 총 90비트의 마이크로 명령어를 생성합니다. 이를 통해 총 4608개의 마이크로 명령어를 처리할 수 있습니다.\n\n마이크로코드 ROM은 트랜지스터의 격자로 구성되어 있으며, 트랜지스터의 존재 여부에 따라 이진 데이터(0 또는 1)를 나타냅니다. 이 설계는 공간을 최소화하면서 성능을 극대화하도록 최적화되어 있습니다.\n\n마이크로코드 주소 레지스터(MAR)는 마이크로 명령어의 주소를 저장하여 펜티엄이 기계 명령어에 필요한 마이크로코드를 가져오고 실행할 수 있도록 합니다. 이 레지스터는 주소를 증가시키거나 새로운 마이크로 주소로 분기하는 작업을 지원합니다.\n\n회로는 주소 디코딩과 최적화된 게이트를 조합하여 마이크로코드 ROM의 행을 선택하는 효율적인 설계를 사용합니다. 이를 통해 필요한 행을 효과적으로 활성화할 수 있습니다.\n\n마이크로코드의 출력은 멀티플렉서 회로에 의해 관리되어 적절한 비트를 선택하여 처리합니다. 이를 통해 올바른 마이크로코드가 프로세서의 작동을 제어하는 데 전달됩니다.\n\n신뢰성을 보장하기 위해 펜티엄은 마이크로코드 ROM의 무결성을 검사하는 테스트 회로를 포함하고 있습니다. 이 회로는 의사 난수 생성기와 체크섬을 활용하여 오류를 감지합니다.\n\n결론적으로, 펜티엄의 마이크로코드 ROM은 복잡하지만 중요한 구성 요소로, 효율적인 처리와 테스트를 가능하게 하는 고급 설계 기술을 보여줍니다. 시리프의 통찰력은 이 역사적인 프로세서의 내부 작동을 자세히 살펴볼 수 있게 해줍니다.",
      "ja": "ケン・シリフのブログでは、コンピュータの歴史やヴィンテージコンピュータの復元、集積回路の逆アセンブルについて詳しく探求しています。最近の投稿では、オリジナルのペンティウムプロセッサのマイクロコード回路について深く掘り下げています。\n\nまず、マイクロコードと機械命令の違いについて説明します。マイクロコードは、機械命令の下に位置するソフトウェアの層です。ペンティウムのようなプロセッサは、複雑な論理ゲートを使う代わりに、ROM（読み出し専用メモリ）に保存されたマイクロコードを利用して、より単純なマイクロ命令を実行します。\n\nペンティウムには、複雑な設計のマイクロコードROMがあります。このROMは二つのバンクから成り、それぞれが45ビットを出力し、合計で90ビットのマイクロ命令を生成します。これにより、合計で4608のマイクロ命令を扱うことができます。\n\nマイクロコードROMはトランジスタのグリッドから構成されており、トランジスタの有無が二進数データ（0または1）を表します。この設計は、スペースを最小限に抑えつつ、性能を最大化するよう最適化されています。\n\nマイクロコードアドレスレジスタ（MAR）は、マイクロ命令のアドレスを保持し、ペンティウムが機械命令に必要なマイクロコードを取得して実行できるようにします。これにより、アドレスのインクリメントや新しいマイクロアドレスへの分岐といった操作が可能になります。\n\n行選択ドライバは、アドレスデコーディングと最適化されたゲートの組み合わせを使用して、マイクロコードROM内の行を効率的に選択します。これにより、必要な行を効果的にアクティブにすることができます。\n\nマイクロコードの出力は、適切なビットを処理するためにマルチプレクサ回路によって管理され、正しいマイクロコードがプロセッサの動作を制御するために供給されます。\n\nペンティウムは、信頼性を確保するために、マイクロコードROMの整合性をチェックするテスト回路を組み込んでいます。これには、擬似乱数生成器やチェックサムを利用してエラーを検出する仕組みが含まれています。\n\nペンティウムのマイクロコードROMは、複雑でありながら重要なコンポーネントであり、効率的な処理とテストを可能にする高度な設計技術を示しています。シリフの洞察は、この歴史的なプロセッサの内部動作を詳細に紹介しています。"
    }
  },
  {
    "id": "3afded431ef90be5",
    "title": {
      "en": "Show HN: Make SVGs interactive in React with 1 line",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://svggles.vercel.app/",
    "score": 49,
    "by": "shantingHou",
    "time": 1743532111,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "f8f53137243edea7",
    "title": {
      "en": "Everything is Ghibli",
      "ko": "모두가 지브리",
      "ja": "すべてはジブリ"
    },
    "type": "story",
    "url": "https://carly.substack.com/p/everything-is-ghibli",
    "score": 204,
    "by": "ghuntley",
    "time": 1743457494,
    "content": "Share this postGood Graf!Everything is GhibliCopy linkFacebookEmailNotesMoreDiscover more from Good Graf!Design, tech, and writing with substance and style.Over 4,000 subscribersSubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.Already have an account? Sign inEverything is GhibliWhen we got “create anything” and all made the same thingCarly AyresMar 28, 2025188Share this postGood Graf!Everything is GhibliCopy linkFacebookEmailNotesMore1634ShareOpenAI unleashed its native image generation in ChatGPT on Tuesday. By Wednesday morning, every social feed was drowning in Studio Ghibli-style portraits. (Linkedin, check back next week.) What happened—and why—is another signal of where AI, art, and our attention are headed.SubscribeHow it started: Just some bros and a selfieOn March 25, OpenAI livestreamed GPT-4o’s image generation features—blending text and visuals, rendering flawless typography, and handling complex prompts with ease. At one point, the team turned a group selfie into an anime-style portrait.Sure, the technical updates were significant: improved text rendering (a longtime challenge), better attribute binding (complex prompts, handled), and context carried across conversation. But what really matters is what we did with it.How it’s going: Full-on GhiblificationIt started with one tweet: “Tremendous alpha right now in sending your wife photos of yall converted to studio ghibli anime,” wrote Grant Slatton. Soon everyone—even Mike Tyson—was stylizing themselves, their kids, 9/11, and memes into the soft, pastel aesthetic of Japan’s beloved animation house.“only a few times in my life i got to see days like today when technological marvels bring together the timeline in rapturous joy,” tweeted roon. The vibes got so high, OpenAI had to delay rollout to free users.When Grant tweeted about the trend, CEO Sam Altman replied: “Believe it or not we put a lot of thought into the initial examples we show.” (Note the Ghibli-fied avatar.)@sama claps backMiyazaki vs. the machineAs billions of pixels were being Ghibli-fied, a 2016 clip resurfaced: Hayao Miyazaki, Studio Ghibli’s legendary founder, watching an AI demo. His take? “I strongly feel that this is an insult to life itself. I am utterly disgusted.”Trung Phan called out the math: each Ghibli film contains 60–70,000 hand-drawn, watercolored frames. A four-second scene in The Wind Rises took one animator 15 months. Now, anyone summon the style in seconds.“Imagine being Miyazaki,” tweeted Nabeel S. Qureshi, “pouring decades of heart and soul into this transcendent, tender style… and seeing it sloppified by linear algebra.”Slop, aesthetics, and valueThis tension—between delight and devaluation—sparked a broader conversation. Reggie James dubbed the trend “Ghibli slop,” not as dismissal, but as critique of what happens when distinctive artistic vision becomes infinitely reproducible at scale. “What was once valuable in the awareness of painstaking labor, beautiful stories, and coherent aesthetic,” he writes, “is now valuable PURELY in our reception to, and reproduction of, the aesthetic.”When anyone can generate a passable Ghibli homage in seconds, the scarcity shifts—from execution to conception, from craft to taste. As signüll put it: “we’re exiting the scarcity economy of visuals & entering something weirder—where aesthetics become ambient infrastructure, like wifi.”Chris Paik also captured this paradox: “This commodification turns meaningful, handcrafted narratives—filled with emotional depth and human struggle—into hollow memes, diluting their artistic significance.”The everything generatorWhat’s most telling isn’t just what people made, but what they didn’t. “They gave us an everything generator but everyone is obsessed with making it do the same thing,” noted Luke Miles. Given infinite possibility, we chose repetition. Ad nauseum.Still, some interesting (dare I say useful?) applications are emerging: Sherwin Wu visualized a home renovation. Sam Dape spun up a sticker pack. Ken Wheeler personalized a kid’s book. EP made a YouTube title card.@sherwinwu on XStealing Google’s thunder: Vibes > benchmarksAnother subplot: the hullabaloo completely eclipsed Google’s announcement of Gemini 2.5 Pro—their most advanced AI model to date—released the same day. It also came on the heels of image generation upstart Reve. As signüll observed: “Google dropped their biggest upgrade ever & ghibli core completely hijacked the zeitgeist. Peak example of how vibes now override tech merit in the attention economy.” signüll vs. noisehow to lose the internet in few hoursgemini 2.5 was supposed to be google’s chatgpt moment. smarter reasoning. faster context. tighter multimodal fusion. years of infrastructure bets finally lining up…Read more6 days ago · 37 likes · 1 comment · signüllThe takeaway? If you’re launching a consumer product, technical superiority doesn’t automatically translate to cultural impact. In the battle for attention, a memorable vibe often beats a better benchmark.What stays valuableAs Ghibli clones flooded the feed, “we’re cooked” tweets flew. But the flood revealed something else: when execution becomes trivial, direction becomes essential.“This is a huge signal that design won’t disappear,” argued Joseph Alessio. “Everyone is doing ghiblicore because that’s exactly what OpenAI demonstrated. People still need someone to shape concepts—lower the floor, raise the ceiling.”Runway’s co-founder Cristóbal Valenzuela made a bet: “If you are thinking about a new career, I would strongly suggest Art Direction. Being a good art director might end up being the most important creative role of the next decade.”Will Manidis predicted: “The most valuable items in the future will be the old and beautiful. all of text will become ai slop, all of art will become ai slop or a reaction to ai slop. The vast majority of genuine human beauty that will exist has already been created.” As Willem Van Lancker put it in January: “Our children will romanticize and fetishize the last era of pure human creation in ways we can’t imagine.”The policy responseBy Wednesday night, OpenAI appeared to be adjusting its approach. Some users reported being blocked from generating Ghibli-style images. An spokesperson clarified that while the company prevents “generations in the style of individual living artists,” it permits “broader studio styles.” Still, they added, “We’re always learning from real-world use and feedback.”Where do we go from here?The Ghibli-fication of the internet made the abstract tangible. It showed us AI’s capacity to delight and its power to flatten. What comes next will depend on how we respond, not just to the tools, but to the values they surface. We’re not just generating images. We’re generating norms.—CarlyShare188Share this postGood Graf!Everything is GhibliCopy linkFacebookEmailNotesMore1634SharePreviousNext",
    "summary": {
      "en": "OpenAI recently introduced image generation in ChatGPT, allowing users to create stunning images, including Studio Ghibli-style portraits. This feature quickly led to a flood of Ghibli-themed content across social media, showcasing both the excitement and challenges of AI in art.\n\nKey points include:\n\n1. **Launch and Impact**: OpenAI's new image generation sparked a trend where many users, including celebrities, created Ghibli-style art, highlighting the capability of AI to blend text and visuals effectively.\n\n2. **Artistic Concerns**: This trend reignited discussions about the value of art. Legendary animator Hayao Miyazaki criticized AI art as a devaluation of true artistic effort, contrasting the painstaking work behind hand-drawn animations with the ease of AI generation.\n\n3. **Cultural Shift**: The ease of replicating an aesthetic raises questions about the meaning of art and creativity. As unique artistic styles become widely accessible, the focus may shift from craftsmanship to the ideas behind the art.\n\n4. **Market Dynamics**: The overwhelming popularity of Ghibli-themed creations overshadowed significant tech announcements, indicating that emotional appeal often outweighs technical advancements in capturing public attention.\n\n5. **Future of Creativity**: Experts suggest that while AI can produce art, the role of human creativity and direction will remain crucial. As AI-generated content rises, the value of authentic, handcrafted work may become even more appreciated.\n\n6. **Policy Adjustments**: In response to the trend, OpenAI is reportedly refining its content generation policies to balance creativity with respect for individual artists.\n\nOverall, the Ghibli-fication trend illustrates both the potential and pitfalls of AI in art, raising important questions about creativity, value, and the future landscape of artistic expression.",
      "ko": "OpenAI는 최근 ChatGPT에서 이미지 생성 기능을 도입하여 사용자들이 스튜디오 지브리 스타일의 멋진 이미지를 만들 수 있게 되었습니다. 이 기능은 소셜 미디어에서 지브리 테마의 콘텐츠가 급증하는 계기가 되었으며, AI가 예술에서 가지는 흥미로운 가능성과 도전 과제를 보여주고 있습니다.\n\nOpenAI의 새로운 이미지 생성 기능은 많은 사용자, 특히 유명인들이 지브리 스타일의 예술을 창작하는 트렌드를 촉발했습니다. 이는 AI가 텍스트와 이미지를 효과적으로 결합할 수 있는 능력을 강조합니다. 그러나 이 트렌드는 예술의 가치에 대한 논의를 다시 불러일으켰습니다. 전설적인 애니메이터 미야자키 하야오가 AI 예술을 비판하며, 손으로 그린 애니메이션의 고된 작업과 AI 생성의 용이함을 대조했습니다.\n\n미적 감각을 쉽게 복제할 수 있게 되면서 예술과 창의성의 의미에 대한 질문이 제기되고 있습니다. 독특한 예술 스타일이 널리 접근 가능해짐에 따라, 예술의 가치는 기술적인 숙련도에서 아이디어로 이동할 수 있습니다. 지브리 테마의 창작물이 폭발적인 인기를 끌면서 중요한 기술 발표들이 가려지는 현상도 나타났습니다. 이는 감정적인 매력이 종종 기술적 발전보다 대중의 관심을 끌기 때문입니다.\n\n전문가들은 AI가 예술을 생성할 수 있지만, 인간의 창의성과 방향성이 여전히 중요하다고 말합니다. AI가 생성한 콘텐츠가 증가함에 따라, 진정한 수작업의 가치는 더욱 높이 평가될 가능성이 있습니다. 이러한 트렌드에 대응하여 OpenAI는 창의성과 개별 예술가에 대한 존중을 균형 있게 조정하기 위해 콘텐츠 생성 정책을 개선하고 있는 것으로 알려졌습니다.\n\n전반적으로 지브리화 트렌드는 예술에서 AI의 가능성과 한계를 보여주며, 창의성, 가치, 그리고 예술적 표현의 미래에 대한 중요한 질문을 제기하고 있습니다.",
      "ja": "OpenAIは最近、ChatGPTに画像生成機能を追加し、ユーザーがスタジオジブリ風のポートレートなどの美しい画像を作成できるようになりました。この機能はすぐにソーシャルメディアでジブリをテーマにしたコンテンツの洪水を引き起こし、AIがアートに与える興奮と課題を浮き彫りにしました。\n\nこの新機能の導入は、多くのユーザーや著名人がジブリスタイルのアートを制作するトレンドを生み出しました。これは、AIがテキストとビジュアルを効果的に融合させる能力を示しています。しかし、このトレンドはアートの価値についての議論を再燃させました。伝説的なアニメーターである宮崎駿は、AIアートを真の芸術的努力の価値を損なうものとして批判し、手描きアニメーションの背後にある苦労とAI生成の容易さを対比させました。\n\n美的感覚を簡単に再現できることは、アートや創造性の意味についての疑問を引き起こします。独自の芸術スタイルが広くアクセス可能になるにつれ、職人技からアートの背後にあるアイデアへと焦点が移る可能性があります。また、ジブリをテーマにした作品の圧倒的な人気は、重要な技術発表を影に隠し、感情的な魅力が技術的な進歩よりも公衆の関心を引くことが多いことを示しています。\n\n専門家は、AIがアートを生み出すことができる一方で、人間の創造性や指導の役割は依然として重要であると指摘しています。AI生成のコンテンツが増える中で、真の手作り作品の価値がさらに評価されるようになるかもしれません。これに応じて、OpenAIは個々のアーティストへの敬意と創造性のバランスを取るために、コンテンツ生成ポリシーを見直しているとのことです。\n\nジブリ風の作品の流行は、AIがアートにおいて持つ可能性と課題を示しており、創造性や価値、今後の芸術表現の風景について重要な疑問を提起しています。"
    }
  },
  {
    "id": "caa90419d1ee2f66",
    "title": {
      "en": "Go Optimization Guide",
      "ko": "최적화 가이드",
      "ja": "最適化ガイド"
    },
    "type": "story",
    "url": "https://goperf.dev/",
    "score": 472,
    "by": "jedeusus",
    "time": 1743452998,
    "content": "Patterns and Techniques for Writing High-Performance Applications with Go¶\nThe Go App Optimization Guide is a collection of technical articles aimed at helping developers write faster, more efficient Go applications. Whether you're building high-throughput APIs, microservices, or distributed systems, this series offers practical patterns, real-world use cases, and low-level performance insights to guide your optimization efforts.\nWhile Go doesn’t expose as many knobs for performance tuning as languages like C++ or Rust, it still provides plenty of opportunities to make your applications significantly faster. From memory reuse and allocation control to efficient networking and concurrency patterns, Go offers a pragmatic set of tools for writing high-performance code.\nWe focus on concrete techniques with measurable impact you can apply immediately—covering everything from core language features to advanced networking strategies.\n What’s Covered So Far¶\nCommon Go Patterns for Performance¶\nIn this first article, we explore a curated set of high-impact performance patterns every Go developer should know:\n\nUsing sync.Pool effectively\nAvoiding unnecessary allocations\nStruct layout and memory alignment\nEfficient error handling\nZero-cost abstractions with interfaces\nIn-place sorting and slices reuse\n\nEach pattern is grounded in practical use cases, with benchmarks and examples you can copy into your own codebase.\n\n What’s Coming Next¶\nHigh-Performance Networking in Go¶\nIn our upcoming deep dive into networking, we'll focus on building high-throughput network services with Go’s standard library and beyond. This includes:\n\nEfficient use of net/http and net.Conn\nManaging large volumes of concurrent connections\nPerformance tuning with epoll/kqueue and GOMAXPROCS\nLoad testing techniques and bottleneck diagnostics\nTBD...\n\nWe'll also explore when to drop down to lower-level libraries like fasthttp, and how to balance performance with maintainability.\n\n Who This Is For¶\nThis series is ideal for:\n\nBackend engineers optimizing Go services in production\nDevelopers working on latency-sensitive systems\nTeams migrating to Go and building performance-critical paths\nAnyone curious about Go’s performance model and trade-offs\n\nStay tuned—more articles, code samples, and tools are on the way. You can bookmark this page to follow the series as it evolves.\n\n    1 day ago2025-03-31\n\n    1 week ago2025-03-20",
    "summary": {
      "en": "The Go App Optimization Guide is a series of articles designed to help developers create faster and more efficient applications using the Go programming language. It provides practical techniques and insights for optimizing performance in various types of applications, such as APIs and microservices.\n\nKey Points:\n- The guide emphasizes that while Go has fewer performance tuning options compared to languages like C++ or Rust, it still offers many ways to enhance application speed.\n- It covers essential patterns for performance, including:\n  - Effective use of sync.Pool\n  - Avoiding unnecessary memory allocations\n  - Proper struct layout and memory alignment\n  - Efficient error handling\n  - Using interfaces without extra costs\n  - Reusing slices and in-place sorting\n\nEach pattern includes practical examples and benchmarks for immediate application.\n\nUpcoming topics will include high-performance networking strategies, such as:\n- Utilizing Go’s net/http and net.Conn effectively\n- Handling many concurrent connections\n- Performance tuning techniques\n- Load testing and identifying bottlenecks\n\nThe guide is aimed at backend engineers, developers focused on reducing latency, teams transitioning to Go, and anyone interested in Go’s performance capabilities.\n\nMore articles and resources will be added, so readers can stay updated.",
      "ko": "Go 앱 최적화 가이드는 개발자들이 Go 프로그래밍 언어를 사용하여 더 빠르고 효율적인 애플리케이션을 만들 수 있도록 돕기 위해 작성된 일련의 기사입니다. 이 가이드는 API와 마이크로서비스와 같은 다양한 유형의 애플리케이션에서 성능을 최적화하기 위한 실용적인 기술과 통찰력을 제공합니다.\n\n이 가이드는 Go가 C++나 Rust와 같은 언어에 비해 성능 조정 옵션이 적지만, 여전히 애플리케이션 속도를 향상시킬 수 있는 여러 방법이 있다는 점을 강조합니다. 성능을 위한 필수 패턴으로는 효과적인 sync.Pool 사용, 불필요한 메모리 할당 피하기, 적절한 구조체 레이아웃과 메모리 정렬, 효율적인 오류 처리, 추가 비용 없이 인터페이스 사용, 슬라이스 재사용 및 제자리 정렬 등이 포함됩니다. 각 패턴은 즉시 적용할 수 있는 실용적인 예제와 벤치마크를 제공합니다.\n\n앞으로 다룰 주제에는 고성능 네트워킹 전략이 포함됩니다. 여기에는 Go의 net/http와 net.Conn을 효과적으로 활용하는 방법, 많은 동시 연결 처리, 성능 조정 기술, 부하 테스트 및 병목 현상 식별 등이 포함됩니다.\n\n이 가이드는 백엔드 엔지니어, 지연 시간을 줄이는 데 집중하는 개발자, Go로 전환하는 팀, 그리고 Go의 성능 기능에 관심이 있는 모든 사람을 대상으로 합니다. 더 많은 기사와 자료가 추가될 예정이니 독자들은 계속해서 업데이트를 받을 수 있습니다.",
      "ja": "Goアプリ最適化ガイドは、開発者がGoプログラミング言語を使用して、より速く効率的なアプリケーションを作成するための一連の記事です。このガイドでは、APIやマイクロサービスなど、さまざまなタイプのアプリケーションのパフォーマンスを最適化するための実践的な技術や洞察を提供します。\n\nこのガイドの重要なポイントは、GoはC++やRustのような言語に比べてパフォーマンス調整の選択肢が少ないものの、アプリケーションの速度を向上させるための多くの方法があるということです。パフォーマンス向上のための基本的なパターンには、以下のようなものがあります。sync.Poolの効果的な使用、不要なメモリ割り当ての回避、適切な構造体のレイアウトとメモリアラインメント、効率的なエラーハンドリング、追加コストなしでのインターフェースの使用、スライスの再利用とインプレースソートなどです。\n\n各パターンには、すぐに実践できる具体的な例やベンチマークが含まれています。\n\n今後のトピックには、高性能なネットワーキング戦略が含まれます。具体的には、Goのnet/httpやnet.Connを効果的に活用する方法、多数の同時接続の処理、パフォーマンス調整技術、負荷テストやボトルネックの特定などが取り上げられます。\n\nこのガイドは、バックエンドエンジニア、レイテンシを削減することに焦点を当てた開発者、Goに移行中のチーム、そしてGoのパフォーマンス機能に興味のある人々を対象としています。\n\n今後も記事やリソースが追加される予定で、読者は最新情報を得ることができます。"
    }
  },
  {
    "id": "543ca928e3a6efbc",
    "title": {
      "en": "The Egg (2009)",
      "ko": null,
      "ja": null
    },
    "type": "story",
    "url": "https://www.galactanet.com/oneoff/theegg.html",
    "score": 220,
    "by": "jxmorris12",
    "time": 1743421532,
    "content": null,
    "summary": {
      "en": null,
      "ko": null,
      "ja": null
    }
  },
  {
    "id": "160a5f2825696717",
    "title": {
      "en": "A deliberate practice app for guitar players who want to level up",
      "ko": "기타 실력 향상 앱",
      "ja": "ギター上達アプリ"
    },
    "type": "story",
    "url": "https://www.captrice.io/",
    "score": 451,
    "by": "adityaathalye",
    "time": 1743218851,
    "content": "Practice using a smart metronome that captures metrics and\n        turns them into actionable insights; paired with an effective\n        practice method focusing on speed, endurance, accuracy, and\n        adaptability.\n\n        No knowledge of music theory required\n        A library of exercises to import from, or create your own\n        Free to use. No ads. No signup\n        Works in any browser on PC & smartphone. No app download needed\n        Data is stored on your device. Export/Delete anytime.\n        Works in \"Offline mode\" (if the browser supports it)",
    "summary": {
      "en": "Use a smart metronome that tracks your practice and gives you useful insights. It focuses on helping you improve speed, endurance, accuracy, and adaptability.\n\nKey points:\n- No music theory knowledge needed.\n- Access a library of exercises or create your own.\n- Free to use with no ads and no signup required.\n- Works in any web browser on PC and smartphone; no app download necessary.\n- Your data is stored on your device, and you can export or delete it anytime.\n- It can work offline if your browser supports it.",
      "ko": "스마트 메트로놈을 사용하면 연습을 추적하고 유용한 통찰력을 제공합니다. 이 도구는 속도, 지구력, 정확성, 적응력을 향상시키는 데 중점을 둡니다.\n\n음악 이론에 대한 지식이 없어도 사용할 수 있습니다. 다양한 연습 자료를 이용하거나 자신만의 연습을 만들 수 있습니다. 광고 없이 무료로 제공되며, 회원 가입도 필요하지 않습니다. PC와 스마트폰의 모든 웹 브라우저에서 작동하며, 앱을 다운로드할 필요가 없습니다. \n\n사용자의 데이터는 기기에 저장되며, 언제든지 내보내거나 삭제할 수 있습니다. 브라우저가 지원하는 경우 오프라인에서도 사용할 수 있습니다.",
      "ja": "練習を追跡し、有益な洞察を提供するスマートメトロノームを使いましょう。これは、スピード、持久力、正確性、適応力を向上させることに重点を置いています。\n\n音楽理論の知識は必要ありません。エクササイズのライブラリにアクセスすることも、自分でエクササイズを作成することもできます。広告はなく、サインアップも不要で、無料で利用できます。PCやスマートフォンの任意のウェブブラウザで動作し、アプリのダウンロードは必要ありません。\n\nデータはデバイスに保存され、いつでもエクスポートや削除が可能です。また、ブラウザが対応していれば、オフラインでも使用できます。"
    }
  },
  {
    "id": "e259e78386318098",
    "title": {
      "en": "The demoscene as a UNESCO heritage in Sweden",
      "ko": "스웨덴, 데모신의 유산",
      "ja": "スウェーデンのデモシーン遺産"
    },
    "type": "story",
    "url": "https://www.goto80.com/the-demoscene-as-a-unesco-heritage-in-sweden",
    "score": 631,
    "by": "robin_reala",
    "time": 1743417597,
    "content": "The demoscene as a UNESCO heritage in Sweden\n\n\t\t\t\t\t\tMar 31, 2025 | ramblings\n\n\t\t\t\t\tThe demoscene has become a national UNESCO-heritage in Sweden, thanks to an application that Ziphoid and me did last year. This has already happened in several European countries, as part of the international Art of Coding initiative to make the demoscene a global UNESCO heritage. I think this makes plenty of sense, since the demoscene is arguablythe oldest creative digital subculture around. It has largely stuck to its own values and traditions throughout the world’s technological and economical shifts, and that sort of consistency is quite unusual in the digital world.\nThe main idea of the demoscene is to compete with productions that maximize a certain hardware, but that’s not what all demosceners like to do. My demogroup Hack n’ Trade for example, cares more about making weird stuff, and there are plenty of other groups like that. Some demosceners don’t release anything at all, but might do important work to keep the scene alive (BBS-trading, organizing parties, preserving software…).\nI’ve written plenty of papers and blog posts about the demoscene, and I’ve often felt a gap between the stuff I write as a researcher and my personal experience of the demoscene. There is certainly an international demoscene with big events and huge releases that can be described in general terms, but what has mattered more to me is the local scenes, the small parties and the people you hang out with. Meeting up with a bunch of friends and making weird computer stuff “for no reason, really” is a great setting. That’s what I enjoy the most, in the end. For other sceners, it’s different.\nThere is a sort of diversity in the scene that is difficult to capture and generalize. The Swedish coder with a well-paid programming job and a busy family life might consider the demoscene as an escape to his teenage years, while the LSD-munching raver from France who trades illegal warez on BBSs and makes weird pixel art considers the scene as a free culture without corporate or art world bullshit. There’s room for both in the scene, because it is werdly conservative and open at the same time. And perhaps that is one of the reasons why it should be considered an intangible heritage.",
    "summary": {
      "en": "**Summary: The Demoscene as a UNESCO Heritage in Sweden**\n\nThe demoscene has been recognized as a national UNESCO heritage in Sweden, following an application by Ziphoid and the author. This movement is part of a broader initiative to establish the demoscene as a global UNESCO heritage, similar to efforts in other European countries. The demoscene is one of the oldest creative digital subcultures, maintaining its values and traditions despite technological changes.\n\nThe main focus of the demoscene is to create productions that push hardware limits, but not all participants share this goal. Some, like the author's group Hack n’ Trade, prioritize making unique creations, while others contribute by organizing events or preserving software without producing releases themselves.\n\nThe author reflects on the difference between their research and personal experiences within the demoscene. While there are large international events, they value local gatherings and the friendships formed there, emphasizing the joy of creating for fun. The scene is diverse, with participants ranging from professionals seeking nostalgia to those embracing a counter-culture. This blend of conservatism and openness is part of what makes the demoscene worthy of recognition as an intangible heritage.",
      "ko": "스웨덴에서 데모신이 국가 유네스코 유산으로 인정받았다. 이는 Ziphoid와 저자의 신청에 따른 결과로, 데모신을 글로벌 유네스코 유산으로 자리매김하려는 더 넓은 노력의 일환이다. 데모신은 가장 오래된 창의적인 디지털 서브컬처 중 하나로, 기술 변화에도 불구하고 그 가치와 전통을 유지하고 있다.\n\n데모신의 주요 초점은 하드웨어의 한계를 뛰어넘는 작품을 만드는 것이지만, 모든 참여자가 이 목표를 공유하는 것은 아니다. 저자의 그룹인 Hack n’ Trade와 같은 일부는 독특한 창작물을 만드는 데 중점을 두고, 다른 이들은 이벤트를 조직하거나 소프트웨어를 보존하는 방식으로 기여한다.\n\n저자는 데모신 내에서의 연구와 개인적인 경험의 차이에 대해 생각한다. 대규모 국제 행사도 있지만, 그들은 지역 모임과 그곳에서 형성된 우정을 더 소중히 여기며, 재미를 위해 창작하는 기쁨을 강조한다. 이 장면은 향수를 찾는 전문가부터 반문화에 동참하는 사람들까지 다양한 참여자로 구성되어 있다. 이러한 보수성과 개방성의 조화가 데모신이 무형 유산으로 인정받을 가치가 있는 이유 중 하나이다.",
      "ja": "デモシーンがスウェーデンで国のユネスコ遺産として認定されました。これは、Ziphoidと著者の申請によるもので、デモシーンを世界的なユネスコ遺産として確立しようとする広範な取り組みの一環です。デモシーンは、技術の変化にもかかわらず、その価値観や伝統を維持している最も古いクリエイティブなデジタルサブカルチャーの一つです。\n\nデモシーンの主な焦点は、ハードウェアの限界を押し広げる作品を作ることですが、すべての参加者がこの目標を共有しているわけではありません。著者のグループであるHack n’ Tradeのように、独自の作品を作ることを優先する人もいれば、イベントを開催したり、ソフトウェアを保存したりすることで貢献する人もいます。\n\n著者は、デモシーンにおける自身の研究と個人的な経験の違いについて考えています。大規模な国際イベントもありますが、彼らは地元の集まりやそこで築かれる友情を大切にし、楽しむために創作する喜びを強調しています。このシーンは多様で、懐かしさを求めるプロからカウンターカルチャーを受け入れる人まで、さまざまな参加者がいます。この保守性と開放性の融合こそが、デモシーンが無形文化遺産として認められるに値する理由の一部です。"
    }
  },
  {
    "id": "a3bcc343dd1bdc3b",
    "title": {
      "en": "Facebook Is Hiding Heather Cox Richardson's Posts",
      "ko": "페이스북의 숨겨진 글",
      "ja": "フェイスブックの陰謀"
    },
    "type": "story",
    "url": "https://closertotheedge.substack.com/p/facebook-is-hiding-heather-cox-richardsons",
    "score": 26,
    "by": "nicpottier",
    "time": 1743596882,
    "content": "Share this postCloser to the EdgeFACEBOOK IS HIDING HEATHER COX RICHARDSON’S POSTSCopy linkFacebookEmailNotesMoreDiscover more from Closer to the EdgeRes Ipsa LoquiturSubscribeBy subscribing,  I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.Already have an account? Sign inFACEBOOK IS HIDING HEATHER COX RICHARDSON’S POSTSApr 02, 20253,701Share this postCloser to the EdgeFACEBOOK IS HIDING HEATHER COX RICHARDSON’S POSTSCopy linkFacebookEmailNotesMore5101,490ShareSOMETHING ISN’T RIGHTOn March 31, 2025, one of the most trusted historians in America watched her own words disappear from the internet. Heather Cox Richardson confirmed that two of her Facebook posts were no longer visible — not just to her followers, but to herself and her husband.“Hi Folks: my two posts from last night have disappeared for many of us, including me and Buddy, but appear to be here for others. I’m trying to figure it out but if anyone has any ideas, do let me know in the comments. Eager to see if this post shows up.”Then, in a follow-up:“OK, this is now gone for me, as well.”Not flagged. Not disputed. Not taken down with explanation. Just gone — silently, and without warning. This is what erasure looks like in the age of algorithmic control.USERS CONFIRM THE SILENCEAcross the platform, her readers confirmed what she feared: the posts were no longer on her wall:“Seeing this but no other posts, not even on your wall.”“No posts after March 26. Gone.”“I can’t see anything on her page past March 26.”The rhythm of her voice — so consistent, so reliable — had been broken. Not by choice. Not by glitch. By force.Heather Cox Richardson’s reply?“Oh, this is SO not good….”THE POSTS THAT VANISHEDOne of the missing posts dealt with allegations that senior members of Donald Trump’s administration had discussed military strikes on Yemen over unsecured channels. It was already spreading fast — more than 130,000 likes, 60,000 shares — before it disappeared.That matters. Because this is not a celebrity being muted. This is a historian documenting power. And that history — our history — is now being selectively hidden.This isn’t content moderation. This is strategic memory loss.HER MESSAGE: GET OFF FACEBOOKRichardson, sounding the alarm as best she could, urged readers to escape the platform altogether:“Please remember you can get these letters from Substack at Letters from an American, and can sign up to get them by email there, too. No paywall, and all free.”Her Substack: Letters from an AmericanA newsletter about the history behind today's politics.By Heather Cox RichardsonTHIS IS NOT A DRILLIf Facebook can make the voice of a presidential historian vanish — not just from your feed, but from her own timeline — then what else is being filtered? What else is being buried? What else are we being kept from seeing?This isn’t about one post. This is about the infrastructure of truth itself.And the scariest part?There will be no notification when history is deleted.Only silence.Subscribe3,701Share this postCloser to the EdgeFACEBOOK IS HIDING HEATHER COX RICHARDSON’S POSTSCopy linkFacebookEmailNotesMore5101,490SharePreviousNext",
    "summary": {
      "en": "Heather Cox Richardson, a respected historian, reported that two of her Facebook posts disappeared without explanation on March 31, 2025. She noticed that not only could she not see her posts, but many of her followers also confirmed that they were missing from her page. One deleted post discussed serious allegations about Trump's administration, which had gained significant attention before vanishing. Richardson expressed concern that this erasure represented a larger issue of controlling historical narratives, urging her readers to follow her on Substack, where her writings are available without restrictions. She emphasized that if Facebook can remove a historian's work, it raises questions about what else might be hidden from the public, highlighting the dangers of selective visibility on social media.",
      "ko": "존경받는 역사학자인 헤더 콕스 리차드슨은 2025년 3월 31일 자신의 페이스북 게시물 두 개가 이유 없이 사라졌다고 보고했습니다. 그녀는 자신의 게시물을 볼 수 없을 뿐만 아니라 많은 팔로워들도 그녀의 페이지에서 게시물이 사라졌다고 확인했다고 전했습니다. 삭제된 게시물 중 하나는 트럼프 행정부에 대한 심각한 혐의에 대해 다루었으며, 이 게시물은 사라지기 전 상당한 주목을 받았습니다. 리차드슨은 이러한 삭제가 역사적 서사를 통제하는 더 큰 문제를 나타낸다고 우려하며, 독자들에게 자신의 서면 자료를 제한 없이 볼 수 있는 서브스택에서 그녀를 팔로우할 것을 권장했습니다. 그녀는 페이스북이 역사학자의 작업을 삭제할 수 있다면, 대중에게 숨겨질 수 있는 다른 것들에 대한 의문이 제기된다고 강조하며, 소셜 미디어에서 선택적 가시성의 위험성을 부각시켰습니다.",
      "ja": "著名な歴史家であるヘザー・コックス・リチャードソンは、2025年3月31日に自分のフェイスブックの投稿が理由もなく消えたと報告しました。彼女は、自分だけでなく、多くのフォロワーもその投稿がページから消えていることを確認したと述べています。削除された投稿の一つは、トランプ政権に関する深刻な告発について触れており、消える前には大きな注目を集めていました。リチャードソンは、この消去が歴史的な物語をコントロールするというより大きな問題を示していると懸念を表明し、読者に対して自分のサブスタックをフォローするよう呼びかけました。彼女は、フェイスブックが歴史家の作品を削除できるのなら、他に何が公に隠されているのかという疑問が生じると強調し、ソーシャルメディアにおける選択的な可視性の危険性を指摘しました。"
    }
  },
  {
    "id": "e041accba2a99e11",
    "title": {
      "en": "Neuralatex: A machine learning library written in pure LATEX",
      "ko": "뉴럴라텍스: 순수 라텍스 머신러닝 라이브러리",
      "ja": "ニューラテックス"
    },
    "type": "story",
    "url": "https://neuralatex.com/",
    "score": 22,
    "by": "Anon84",
    "time": 1743544314,
    "content": "Neuralatex is a scalar values-based auto-grad library similar to MicroGrad\n            but written entirely in latex! As part of your latex\n            document you can specify the architecture of a neural network and\n            loss functions, how to generate or load training data, and specify\n            training hyperparameters and experiments. When the document\n            is compiled, the latex compiler will generate or load training data,\n            train the network, run experiments and generate figures. Training\n            debug output can be written to the latex compiler log or included\n            as part of the paper itself.",
    "summary": {
      "en": "Neuralatex is an auto-grad library for neural networks that works with LaTeX. It allows you to define your neural network's architecture, loss functions, training data, and hyperparameters directly in your LaTeX document. When you compile your document, it automatically generates or loads data, trains the network, conducts experiments, and creates figures. You can also see training debug information in the LaTeX log or include it in your paper.",
      "ko": "Neuralatex는 LaTeX와 함께 작동하는 신경망을 위한 자동 그래디언트 라이브러리입니다. 이 라이브러리를 사용하면 LaTeX 문서에서 신경망의 구조, 손실 함수, 훈련 데이터 및 하이퍼파라미터를 직접 정의할 수 있습니다. 문서를 컴파일할 때, Neuralatex는 자동으로 데이터를 생성하거나 불러오고, 네트워크를 훈련시키며, 실험을 수행하고, 결과를 시각화합니다. 또한 훈련 중 발생하는 디버그 정보를 LaTeX 로그에서 확인하거나 논문에 포함시킬 수 있습니다.",
      "ja": "Neuralatexは、LaTeXと連携するニューラルネットワーク用の自動微分ライブラリです。このライブラリを使うと、ニューラルネットワークのアーキテクチャ、損失関数、トレーニングデータ、ハイパーパラメータを直接LaTeX文書内で定義できます。文書をコンパイルすると、自動的にデータを生成または読み込み、ネットワークをトレーニングし、実験を行い、図を作成します。また、トレーニングのデバッグ情報をLaTeXのログで確認したり、論文に含めたりすることも可能です。"
    }
  }
]